Salient Object Detection Driven by Fixation Prediction

Wenguan Wang 1, Jianbing Shen∗ 1,2, Xingping Dong 1, Ali Borji3
Beijing Lab of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, China

1

2

3

Inception Institute of Artiﬁcial Intelligence, Abu Dhabi, UAE

Department of Computer Science, University of Central Florida, USA

wenguanwang.ai@gmail.com, {shenjianbing, dongxingping}@bit.edu.cn, aborji@crcv.ucf.edu

Abstract

Research in visual saliency has been focused on two ma-
jor types of models namely ﬁxation prediction and salient
object detection. The relationship between the two, how-
ever, has been less explored.
In this paper, we propose
to employ the former model type to identify and segment
salient objects in scenes. We build a novel neural network
called Attentive Saliency Network (ASNet)1 that learns to
detect salient objects from ﬁxation maps. The ﬁxation map,
derived at the upper network layers, captures a high-level
understanding of the scene. Salient object detection is then
viewed as ﬁne-grained object-level saliency segmentation
and is progressively optimized with the guidance of the ﬁx-
ation map in a top-down manner. ASNet is based on a hier-
archy of convolutional LSTMs (convLSTMs) that offers an
efﬁcient recurrent mechanism for sequential reﬁnement of
the segmentation map. Several loss functions are introduced
for boosting the performance of the ASNet. Extensive exper-
imental evaluation shows that our proposed ASNet is capa-
ble of generating accurate segmentation maps with the help
of the computed ﬁxation map. Our work offers a deeper in-
sight into the mechanisms of attention and narrows the gap
between salient object detection and ﬁxation prediction.

1. Introduction

Salient object detection (SOD) has been studied exten-
sively for more than a decade (since [37]). It has several
applications such as segmentation [52, 10], object proposal
generation [2], and image resizing [57, 50]. Recently, the
use of deep neural networks for saliency detection has been
trending. Although promising results have been achieved,
they occasionally fail to detect the most salient object in

∗Corresponding author: Jianbing Shen. This work was supported in
part by the Beijing Natural Science Foundation under Grant 4182056, the
Fok Ying Tung Education Foundation under Grant 141067, and the Spe-
cialized Fund for Joint Building Program of Beijing Municipal Education
Commission.

1Available at: https://github.com/wenguanwang/ASNet.

(a)

(b)
Figure 1. Given complex scenes like (a), what are the salient
objects? We propose the Attentive Saliency Network (ASNet)
that infers the object saliency (b) from predicted ﬁxation maps (c),
which is consistent with human attention mechanisms.

(c)

complex cluttered scenes containing several objects (such
as the ones shown in Fig. 1 (a)). Additionally, for current
computational saliency models, their connection with how
humans explicitly choose salient objects or watch natural
scenes are less clear (as discussed in [3, 6]).

In this paper, we take a further step towards a more bi-
ologically plausible SOD model equipped with high-level
prior of ﬁxation map. The human attention prior, rep-
resented by eye movements or from a ﬁxation prediction
model, is instinctive and more consistent with visual pro-
cessing of human visual system. The suggested model not
only generates high-quality object saliency maps, but also
pushes the boundary of SOD research by building a close
connection to human ﬁxation prediction (FP). As shown in
Fig. 1, our model infers object saliency using the ﬁxation
prior, where this prior acts as a selective mechanism to en-
hance the saliency representation for the purpose of accu-
rate object saliency inference. Our algorithm has bias on
the assumption that aligns with the core views of previous
studies [34, 4] that explored the relationship between eye
movements (implicit saliency) and explicit object saliency.
These studies conﬁrmed a strong correlation between ﬁx-
ations and salient objects. Further beyond treating FP and
SOD as two separate tasks that are learned in a branched

11711

network (as in [26]), these two tasks are more deeply inter-
connected in our model. The ﬁxation map provides a high-
level signal, which is learned from upper layers of our neu-
ral network. It is then used for salient object detection in a
top-down manner. This process is straightforward and sim-
ilar to how humans process a scene sequentially (i.e., ﬁrst
paying attention to important areas of a scene quickly, and
then taking more efforts for precise segmentation). Further,
leveraging the rich information from exiting large-scale eye
movement datasets can improve the robustness and gener-
alization ability of SOD models.

The proposed Attentive Saliency Network (ASNet) is
based on convolutional LSTM (convLSTM) [59], which has
convolutional structures in both the input-to-state and state-
to-state transitions. Further, beyond the fully connected
LSTM models, convLSTM encodes the spatial information
via convolution operations to facilitate pixel-wise saliency
labeling. The key advantage of LSTM is iterative removal
of irrelevant information and learning powerful represen-
tations through updating the memory cell. This allows the
ASNet to progressively optimize features for better saliency
estimation in a feed-forward strategy. By stacking multiple
convLSTMs, the ASNet is trainable for gradually rendering
object saliency from ﬁxation map in a top-down manner.

Our contributions are manifold:

• We aim to infer salient objects (captured in lower net-
work layers) from the ﬁxation map (encoded in higher lay-
ers) within a uniﬁed neural network. This goes one step
beyond previous deep learning based saliency models and
offers a deep insight into the conﬂuence between ﬁxation
prediction and salient object detection.
• We present the Attentive Saliency Network (ASNet)
which is a hierarchy of convLSTMs for step-wise inference
of object saliency. ConvLSTM has the advantage of the
improved ﬂow of information with recurrent connections,
which results in more powerful saliency representation.
• We introduce novel loss functions for SOD, derived from
exiting SOD evaluation metrics, for capturing several qual-
ity factors. As we will show empirically, these new loss
functions lead to higher performance.

2. Related Work

In this section, we ﬁrst brieﬂy review the ﬁxation pre-
diction (§ 2.1) and salient object detection literature (§ 2.2).
Then, in § 2.3, we discuss studies exploring the relationship
between the above two tasks.

2.1. Fixation Prediction (FP)

Fixation Prediction (FP) aims to identify the ﬁxation
points that human viewers would focus on at ﬁrst glance.
It has a long history from [19] and is still active in vision
research. From the view of mechanism to obtain atten-
tion, early attention models can be classiﬁed into different

schools [5], such as cognitive model [19, 28, 40, 42, 41],
Bayesian model [63], decision theoretic model [12], infor-
mation theoretic model [7], graphical model [13], spectral
analysis model [16], pattern classiﬁcation model [23], etc.
We refer the readers to [5] for more detailed overview.

More recently, many deep learning based attention
models have been proposed. The eDN model [46] repre-
sented an early architecture that automatically learns deep
representations for FP. After that, DeepFix [25], SALICON
net [18], Mr-CNN [36], Shallow and Deep [43], attentive
LSTM [11], DVA net [51], were successively proposed,
with the use of deeper networks and more complex architec-
tures. Jetley et al. [20] tested several loss functions based on
probability distance measures and found Bhattacharyya dis-
tance could give the best performance. Those deep learning
solutions generally achieved better performance, compared
with traditional non-deep learning techniques.

2.2. Salient Object Detection (SOD)

Salient object detection (SOD) aims at highlighting
salient object regions in images. Different from FP that is
originated from cognition and psychology research commu-
nities, SOD is more a computer vision task that is driven by
object-level applications [56]. The history of SOD is rela-
tively more recent and dates back to the works of Liu et al.
[37] and Achanta et al. [1]. Due to lack of the knowledge
of image content, most non-deep learning SOD models
[10, 58, 60, 21, 67, 53, 55] are based on low-level features
and certain heuristic hypothesizes (such as contrast, back-
ground prior). Please see [4] for detailed overviews.

In more recent work, deep learning based SOD mod-
els have made substantial improvement. These methods
mainly utilize multi-scale and multi-level representations
[31, 64, 38], integrate both local estimation and global
search [47], explore global and local context information
[66], combine pixel- and segment-level features [32], de-
velop level set [17], or consider short connections with skip-
layer structures [15], based on neural network. Some other
methods try to integrate deep learning models with hand-
crafted features [30], study saliency prior [48], or exploit
various deep learning architectures [27, 35, 65, 49, 54].

2.3. The Relationship between FP and SOD

Although SOD has been extensively studied in computer
vision research, only few studies (e.g., [39, 4, 34]) have ex-
plored how humans explicitly choose salient objects. They
have quantitatively conﬁrmed that object saliency judg-
ments agree with human ﬁxations. According to the analy-
sis in [4], there exists a strong correlation between explicit
saliency judgments and free-viewing ﬁxations, which can
be viewed as two proxies of visual attention. Li et al. [34]
have demonstrated that, unlike FP datasets, there exists a
heavy bias in many widely used SOD datasets. Most of the

1712

Figure 2. Typical network architectures used in previous FP or SOD models and our ASNet. (a) Single-stream network, (b) Multi-
stream network, and (c) Skip-layer network. (d) Branched network adopted in [26], where FP and SOD are achieved via two branches
sharing several bottom layers. (e) The adopted ASNet captures ﬁxation map from upper layer, which is indicative of the inference of object
saliency from lower layers. Stack of convLSTMs are adopted for iteratively optimizing features, while preserving spatial information.

SOD datasets have only a few obvious objects in the scene.
Next, we discuss several representative deep models in
SOD or FP from the view of network architecture. This
would better situate our work with respect to previous works
and help to highlight our contributions. As shown in Fig. 2,
most deep learning models for FP or SOD only consider a
single task. Typical architectures include: (a) single-stream
network [25, 20, 43, 65, 17] (standard architecture), (b)
multi-stream network [18, 66, 31, 36] (training with multi-
scale inputs), and (c) skip-layer network [15, 32, 64, 38]
(concatenate multi-layer responses for ﬁnal output). As
seen, previous deep learning based works often treated FP
and SOD as two unrelated tasks.

Instead of performing FP or SOD separately, we ex-
ploit the correlation between ﬁxations and salient objects
via tightly coupling these two tasks in a uniﬁed deep learn-
ing architecture. There are only few methods consider FP
and SOD tasks together. In [9], ﬁxation map from a pre-
trained FP model is used as an extra cue for guiding SOD,
while it didn’t emphasize learning both FP and SOD simul-
taneously. In [26], FP and SOD are achieved via two sep-
arate network branches, which only share weights in sev-
eral lower layers (Fig. 2 (d)). In our method, as illustrated
in Fig. 2 (e), ﬁxation map (high-level knowledge captured
in top layers) is used for guiding accurate object saliency
detection in lower layers. Thus our approach goes beyond
above work by learning FP and SOD within a uniﬁed net-
work and in a top-down end-to-end manner.

3. Our Approach

Given an input image, the goal is to produce a pixel-wise
saliency map to highlight salient object regions. As demon-
strated in Fig. 3, the proposed ASNet ﬁrst captures a global
and high-level understanding of a scene in its higher layers,
by learning to predict human ﬁxations (§3.1). Then, it uses
a stack of convLSTMs to progressively infer object saliency
from the ﬁxation map in a top-down and coarse-to-ﬁne man-
ner (§3.2). The whole network is simultaneously trained to
predict ﬁxation locations and to detect salient objects in an
end-to-end way (§3.3).

3.1. Fixation Predicting

At the bottom of ASNet resides a stack of convolutional
layers where the lower layers respond to primitive image
features such as edges, corners and shared common pat-
terns, and the higher layers extract semantic information
like objects or faces. The ASNet learns the FP as a high-
level task towards modeling human ﬁxation locations with
the utilization of features from higher layers, and achieves
the SOD by optimizing the ﬁxation prior with the features
from the lower layers.

The lower convolutional layers are borrowed from the
ﬁrst ﬁve convolutional blocks of VGGNet [45] (13 convolu-
tional layers in total). We omit the last pooling layer (pool5)
for preserving more spatial information. For a training im-
age, with a typical resolution of 224 × 224 × 3, we com-
pute a convolutional layer by applying a 3×3 kernel with
sigmoid activation function, to the last convolutional fea-
ture map (14 × 14 × 512). The result is a probability map
P ∈ [0, 1]14×14 which is used as a ﬁxation prior from global
and high-level image context. The model for the task of
FP is trained via minimizing the following Kullback-Leibler
Divergence (KL-Div) loss function:

LAtt(G, P ) =

1

14×14

14 × 14 X

x

gx log(

),

(1)

gx
px

where G denotes the resized ground-truth attention map
G ∈ [0, 1]14×14 and gx ∈ G, px ∈ P . The gray-scale ﬁx-
ation map is obtained via ﬁltering the binary ﬁxation map
using a Gaussian ﬁlter with small variance. The KL-Div
measure, the minimization of which is equivalent to cross-
entropy minimization, is widely used in visual saliency pre-
diction [18, 51]. In the next section, we will leverage such
ﬁxation map as the prior for producing object saliency.

3.2. Detecting Object Saliency with Fixation Prior

The ﬁxation map P gives a coarse but informative prior
regarding visually salient regions. A number of previous
studies for pixel-labeling tasks such as semantic segmenta-
tion [44], and salient object detection [35, 48], have shown
that neural networks are capable of producing ﬁne-gained

1713

gates i, f, c. At each step t, as a new input Xt arrives, its in-
formation will be accumulated to the cell if the input gate it
is activated. Also, the past cell status Ct−1 could be “forgot-
ten” in this process if the forget gate ft is on. Whether the
latest cell status Ct should be propagated to the ﬁnal state Ht
is further controlled by the output gate ot. Formally, above
memory update process at step t is driven by the following
equations:

i ∗ Xt + W H
f ∗ Xt + W H
o ∗ Xt + W H

it = σ(W X
ft = σ(W X
ot = σ(W X
Ct = ft ◦ Ct−1 +it ◦tanh(W X
Ht = ot ◦ tanh(Ct),

i ∗ Ht−1 + bi),
f ∗ Ht−1 + bf ),
o ∗ Ht−1 + bo),
c ∗Xt +W H

(2)

(3)

(4)

c ∗Ht−1 +bc), (5)
(6)

where ‘∗’ denotes the convolution operator and ‘◦’ repre-
sents element-wise product. σ and tanh are the activa-
tion functions of logistic sigmoid and hyperbolic tangent.
The inputs Xt, cell memory Ct, hidden states Ht and gates
it, ft, ct are 3D tensors whose spatial dimensions are the
same. W s and bs are the learned weights and biases.

In our case, the convLSTM takes the features X ex-
tracted from the convolutional neural network (from the last
convolutional layers prior to pooling layers) as input, and
produces reﬁned saliency features for ﬁnal saliency estima-
tion. Since it operates on static images, the input features in
all steps are the same: X1 = . . . = Xt = X (see Fig. 4).
Here, we take the advantages of recurrent natures of LSTM
for iteratively optimizing the saliency features of static im-
ages, instead of using LSTM for modeling the temporal de-
pendency of sequential data.

With the learned ﬁxation prior P ∈ [0, 1]14×14, we ﬁrst
combine P with the convolutional features from conv5-3
and feed them into a convLSTM. In each time step, the con-
vLSTM is trained for inferring the salient object with the
knowledge of ﬁxation information, and sequentially opti-
mizes the features with the updated memory cell and hidden
states (see Fig. 4 (b)). Thus, the features are reorganized
towards better representation of object saliency. More spe-
cially, we ﬁrst compress the feature responses from conv5-3
layer via a convolutional layer with 64 ﬁlters to lower com-
putational costs and adopt sigmoid activation for regulariz-
ing the response from features to lie within the same range
([0, 1]) of P . Then, the attention prior map P is concate-
nated with the compressed features and fed into the con-
vLSTM. We apply a 1×1 convolution kernel to the ﬁnal
convLSTM output H for obtaining an object saliency map
Q ∈ [0, 1]14×14.

Several different metrics have been proposed for evaluat-
ing saliency models and no single metric can fully summa-
rize the performance of a model. This motivates us to com-
bine the classical weighted cross-entropy loss function with

1714

Figure 3. Architecture of the proposed ASNet. The ﬁxation map
is learned from the upper layers and is used by the ASNet to locate
the salient objects. Then, the ﬁne-grained object saliency is grad-
ually inferred from lower layers and is successively optimized via
the recurrent architecture of convLSTM. Zoom-in for details.

labeling results via incorporating high-level information en-
coded in upper network layers. Here, we desire our model
to be able to infer precise object saliency from the ﬁxation
map predicted in the upper network layers.

The network is trained for detecting and successively re-
ﬁning the salient object via aggregating information from
high-level ﬁxation map and the spatially rich information
from low-level network features. As shown in Fig. 3, the
SOD is computed in a top-down fashion, successively in-
tegrating information from earlier layers. Multiple convL-
STM networks [59] (the yellow blocks in Fig. 3) are stacked
for building more meaningful feature representations with
recurrent connections. We leverage the sequential nature
of LSTM to process features in an iterative way. For a
certain layer, convLSTM discards less informative features
while enhances informative features, thus generating grad-
ually improved saliency maps.

ConvLSTM extends traditional fully connected LSTM
[14] to consume spatial features. Basically, this is achieved
by substituting dot products with convolutional operations
in the LSTM equations. ConvLSTM has convolutional
structures in both the input-to-state and state-to-state transi-
tions, which can preserve the spatial information of convo-
lutional feature map, thus enabling our network to produce
a pixel-wise labeling.

A schematic diagram of convLSTM is presented in Fig.
4. Similar to traditional gated LSTMs, the convLSTM uses
the memory cells and gates to control information ﬂow.
It works by sequentially updating an internal state H and
memory cell C, according to the values of three sigmoid

(a)

(b)

Figure 4. Illustration of our convLSTM based object saliency optimization, where (b) shows detailed architecture of our convLSTM
optimization module in (a). Zoom-in for details.

precision, recall, F-measure, and MAE metrics for more ef-
ﬁcient training. Given the ground-truth salient object an-
notation S (here S ∈ {0, 1}14×14 for conv5-3 layer), the
overall loss function is deﬁned as:

LSal(S, Q) = LC(S, Q)+α1LP (S, Q)+α2LR(S, Q)

+α3LF (S, Q)+α4LM AE(S, Q),

where αs are balance parameters and are empirically set as
α1 = α2 = α3 = α4 = 0.1. LC is the weighted cross-
entropy loss function, which is widely adopted for training
SOD models and opted as the primary loss in our case:

After obtaining the object saliency map Q ∈ [0, 1]14×14
inferred from the ﬁxation map P , we upsample (×2) Q
and feed it to the next convLSTM with the compressed
features (28×28×64) from conv4-3 layer for more de-
tailed reﬁnement. Above process is iteratively applied for
conv4-3, conv3-3, conv2-2 and conv1-2 layers, respectively.
Finally, the ASNet outputs a high-quality object saliency
mask (224×224×1). In sum, the ASNet is able to effec-
tively infer the object saliency thanks to 1) a learnable ﬁxa-
tion prior, 2) iteratively updating saliency features with re-
current architecture and 3) efﬁciently merging spatially rich
information from lower layers in a top-down manner.

LC(S, Q) =

γ · (1 − sx) · log(1 − qx)

3.3. Implementation Details

(7)

(8)

1
N Xx(cid:0)

+ (1 − γ) · sx · log qx

,

(cid:1)

where N is the total number of pixels and sk ∈ S, qk ∈
Q. γ refers to the ratio of salient pixels in ground truth S.
Weighted cross-entropy loss handles the imbalance between
number of salient and non-salient pixels.

LP , LR and LF are computed similar to precision, recall

and F-measure scores:

LP (S, Q) = −

Xx

sx · qx.(

Xx

qk + ǫ),

LR(S, Q) = −

LF (S, Q) = −

sk + ǫ),

sx · qx.(

Xx
Xx
(1 + β2) · LP (S, Q) · LR(S, Q)
β2 · LP (S, Q) + LR(S, Q) + ǫ

.

(9)

(10)

(11)

where β2 = 0.3 as suggested by [1], and ǫ is a regular-
ization constant. Since precision, recall and F-measure are
similarity metrics and higher values are better, negative val-
ues are used for minimizing.

LM AE is derived from the mean absolute error (MAE)
measure that computes the discrepancy between the
saliency map Q and the ground-truth map S:

LM AE(S, Q) =

|sx − qx|.

(12)

1
N Xx

Overall loss: Let I = {Ik, k = 1, . . . , K} denote all the
training images (resized into 224 × 224) borrowed from ex-
iting SOD or FP datasets. Since there are only few datasets
that offer annotations for both SOD and FP tasks, most of
the training images are either labeled with human ﬁxation
annotation or object saliency mask. Let yA
k ∈ {0, 1} and
yS
k ∈ {0, 1} indicate whether we have the attention anno-
tation Gk and object saliency mask Sk for the k-th training
image. Our ﬁnal loss function can be expressed as:

L =

yA
k ·LAtt(Gk, Pk)+

yS
k ·

LSal(Sℓ

k, Qℓ

k), (13)

K

X
k=1

K

X
k=1

5

X
ℓ=1

k and yS

where the loss functions LAtt and LSal are deﬁned in Eqn.
1 and Eqn. 7, respectively. The indicators yA
k are
employed to remedy missing ground truth in correspond-
ing tasks. That is the error is not propagated back when
the annotations are not offered. The ℓ ∈ {1, . . . , 5} refers
to the ℓ-th convLSTM with conv-ℓ block in ASNet. With
the hierarchical loss functions, each layer in ASNet has di-
rect access to the gradients from the loss function leading
to an implicit deep supervision [29]. We set the time steps
to three in our convLSTM and employ 3 × 3 kernels for
convolution operations.

1715

Dataset

#Images

Resolution

Annotation
FP SOD

Training

Testing

640 × 480

SALICON [22] 15,000 X
THUS10K [10] 10,000

PASCAL-S[34]

X max(w, h) = 400
DUT-OMRON [61] 5,168 X X max(w, h) = 400
X X max(w, h) = 500
max(w, h) = 1024
X max(w, h) = 400
X max(w, h) = 400

ECCSD[60] 1,000
HKU-IS[31] 4,447

MIT1003[23] 1,004 X

850

Table 1. Datasets used for training and testing the ASNet.

Training datasets: Another advantage of ASNet is that it
can use data from both SOD and FP benchmarks. We con-
sider three large-scale saliency datasets: SALICON [22],
THUS10K [10], and DUT-OMRON [61]. The SALICON
dataset is widely used in the domain of FP, while the
THUS10K dataset is commonly used for SOD. These two
datasets have annotations for ﬁxations and salient objects,
respectively. We further utilize the DUT-OMRON dataset
which offers annotations for both FP and SOD. Detailed de-
scriptions of employed datasets can be found in Table 1.
Training settings: In each training iteration, we use a min-
batch of 10 images, which are sampled from above three
datasets and ensure data balance between SOD and FP. Data
augmentation techniques (e.g., ﬂipping, rotation) are also
adopted. Our model is implemented in Python on Keras,
and trained with the Adam optimizer [24]. During the train-
ing phase, the learning rate is set to 0.0001 and is decreased
by a factor of 10 every two epochs. The networks were
trained for 12 epochs with early stopping strategy.
Computation load: ASNet is simultaneously trained for
FP and SOD tasks in an end-to-end manner. The entire
training procedure takes about 10 hours with a NVIDIA TI-
TAN X GPU and a 4.0GHz Intel processor. It takes about
0.08s to process an image of 224 × 224 size, which is faster
than most deep learning based competitors.

4. Experimental results

In this section, we ﬁrst examine the performance of AS-
Net for the FP task. The goal of this experiment is to in-
vestigate the effectiveness of the learned ﬁxation map prior,
instead of comparing it with the state-of-the-art FP models.
Then we evaluate the performance of the ASNet for the pri-
mary SOD task. Finally, an ablation study is performed to
gain a deeper insight into the proposed ASNet.
Testing datasets: Four datasets including PASCAL-S [34],
MIT1003 [23], ECCSD [60] and HKU-IS [31] are used for
testing our model. PASCAL-S offers both annotations for
FP and SOD, MIT1003 is a representative benchmark for
FP, and ECCSD and HKU-IS are two typical datasets which
are widely used for SOD. We report the evaluation results
for both tasks over the PASCAL-S dataset. More details of
above datasets can be found in Table 1.
Evaluation metrics: For the FP task, there are several ways

Methods AUC-Judd ↑ SIM ↑ shufﬂed AUC ↑ CC ↑ NSS ↑
1.36
1.86
1.60
1.73
2.08
1.29
1.25
0.82
1.38
1.10
2.01
2.30

Mr-CNN [36]
SALICON [18]
Shallow-Net [43]
Deep-Net [43]
SU [26]
eDN [46]
BMS [62]
AIM [8]
GBVS [13]
ITTI [19]
ASNet-14×14
ASNet-28×28

0.38
0.53
-
0.51
-
0.41
0.36
0.26
0.42
0.33
0.60
0.65

0.35
0.42
-
0.40
-
0.30
0.33
0.27
0.36
0.32
0.49
0.52

0.80
0.85
-
0.86
-
0.85
0.79
0.79
0.83
0.77
0.87
0.88

0.73
0.74
0.68
0.73
0.73
0.66
0.69
0.68
0.66
0.66
0.73
0.75

- The authors in [26, 43] have not released detailed results.

Table 2. Quantitative comparison of different FP models on the
MIT1003 [23] dataset.

Methods AUC-Judd ↑ SIM ↑ shufﬂed AUC ↑ CC ↑ NSS ↑
1.35
1.88
1.90
1.74
2.22
1.42
1.28
0.97
1.36
1.30
2.26
2.43

Mr-CNN [36]
SALICON [18]
Shallow-Net [43]
Deep-Net [43]
SU [26]
eDN [46]
BMS [62]
AIM [8]
GBVS [13]
ITTI [19]
ASNet-14×14
ASNet-28×28

0.40
0.58
-
0.55
-
-
-
0.32
0.45
0.42
0.70
0.73

0.34
0.46
-
0.42
-
-
-
0.30
0.36
0.36
0.55
0.59

0.79
0.86
-
0.87
-
-
-
0.77
0.84
0.82
0.90
0.90

0.71
0.72
0.69
0.71
0.73
0.65
0.67
0.65
0.65
0.64
0.74
0.74

- The authors in [43, 26, 46, 62] have not released detailed results.
Table 3. Quantitative comparison of different FP models on the
PASCAL-S [23] dataset.

to measure the agreement between model predictions and
human eye movements. Here, we employ ﬁve typical met-
rics, namely Normalized Scanpath Saliency (NSS), Simi-
larity Metric (SIM), Linear Correlation Coefﬁcient (CC),
AUC-Judd, and shufﬂed AUC. Please refer to [5, 51] for
detailed descriptions of these metrics. For the SOD task,
three standard metrics, namely PR-curve, F-measure, and
MAE, are employed for evaluation. See [55] for details.

4.1. Performance of ASNet

Performance on FP task: We evaluated the ﬁxation prior
map generated by ASNet compared to 10 state-of-the-art
ﬁxation models, including 4 classical models: ITTI [19],
GBVS [13], AIM [8], BMS [62], and 6 deep learning based
models: eDN [46], SALICON [18], SU [26], Mr-CNN [36],
Shallow-Net [43] and Deep-Net[43]. Results are reported
over PASCAL-S [34] and MIT1003 [23] datasets.

Our ASNet is able to generate a ﬁxation prediction map
P from top layer, which is relatively rough, and much
smaller (only 14×14) compared to exiting ﬁxation models.
For the sake of a fairer comparison and deeper insight into
the advantage of our ASNet, we further generate a larger
ﬁxation map (28×28) via feeding P into an additional con-

1716

(a) ECCSD

(b) HKU-IS

(c) PASCAL-S

Figure 5. SOD results with PR-curve on three widely used benchmarks: ECCSD [60], HKU-IS [31] and PASCAL-S [34], where the
scores from non-deep learning models are indicated by dashed lines. Best viewed in color.

Figure 6. Qualitative results of ASNet. From top to bottom: example images, ﬁxation maps, and object saliency results.

vLSTM with the features from conv4-3 layer. We therefore
derive two baselines: ASNet-14×14 and ASNet-28×28,
corresponding to the rough prediction and the reﬁned at-
tention map with a more detailed spatial information.

As shown in Table 2 and Table 3, ASNet-14×14 per-
forms better than previous non-deep learning models and
is competitive with current top-performing deep learning
contenders. Considering our relatively simple network ar-
chitecture, and smaller output resolution (14×14), the sug-
gested ASNet is much favorable and effective. We at-
tribute this primarily to the extra generality and powerful
saliency representations learned from the SOD task. Ad-
ditionally, ASNet-28×28 produces further better results,
which demonstrates the proposed ASNet has potential of
obtaining better FP results with considering more detailed
spatial information.
Performance on SOD task: Here we evaluate the per-
formance of ASNet on its primary task: SOD. We per-
form quantitative study on 3 widely used datasets, namely
ECCSD [60], HKU-IS [31] and PASCAL-S [34]. We com-
pare ASNet against 14 recent deep learning based alterna-
tives: LEGS [47], MDF [31], DS [33], SU [26], DCL [32],
ELD [30], RFCN [48], DHS [35], HEDS [15], NLDF [38],
DLS [17], AMU [64], UCF [65], and SRM [49]. We also

SU [26]

Methods

ECCSD [60]

HS [60] 0.730
DRFI [21] 0.787
wCtr [67] 0.672
MDF [31] 0.831
LEGS [47] 0.831
DS [33] 0.810
DCL [32] 0.898
ELD [30] 0.865
0.88
RFCN [48] 0.898
DHS [35] 0.905
HEDS [15] 0.915
NLDF [38] 0.905
DLS [17] 0.825
AMU [64] 0.889
UCF [65] 0.868
SRM [49] 0.910
ASNet 0.928

HKU-IS [31] PASCAL-S [34]
F-score↑ MAE↓ F-score↑ MAE↓ F-score↑ MAE↓
0.259
0.196
0.193
0.145
0.155
0.170
0.108
0.121
0.10
0.118
0.091
0.080
0.099
0.136
0.098
0.116
0.127
0.072

0.223
0.166
0.178
0.108
0.119
0.160
0.071
0.080
0.06
0.097
0.061
0.052
0.063
0.090
0.058
0.068
0.056
0.043
- The authors in [26] have not released detailed results.
∗ MDF [31] is trained on a subset of HKU-IS, and evaluated on the
remaining images.

0.636
0.215
0.710
0.692
0.143
0.783
0.611
0.138
0.694
0.860* 0.129* 0.764
0.749
0.101
0.812
0.818
0.078
0.848
0.822
0.048
0.907
0.767
0.071
0.844
0.77
-
-
0.827
0.079
0.895
0.820
0.052
0.892
0.830
0.039
0.913
0.831
0.048
0.902
0.719
0.072
0.806
0.834
0.052
0.918
0.771
0.062
0.905
0.783
0.046
0.892
0.857
0.035
0.920

Table 4. The F-measure and MAE scores of SOD on three pop-
ular datasets. ASNet gains the best performance with the assis-
tance of visual attention prior. See text for details.

1717

Aspects

Methods

ECCSD [60]

HKU-IS [31] PASCAL-S [34]
F-score↑ MAE↓ F-score↑ MAE↓ F-score↑ MAE↓

0.928

0.043

0.920

0.035

0.857

0.072

variants

archi-
tecture

loss

ASNet
(conv1-output)

w/o ﬁxation 0.913
w/o convLSTM 0.891
conv5-output 0.853
conv4-output 0.875
conv3-output 0.903
conv2-output 0.919
w/o LP
0.923
w/o LR 0.924
0.924
w/o LF
w/o LM AE 0.921
w/o extra loss 0.917

0.051
0.068
0.093
0.076
0.061
0.049
0.045
0.046
0.047
0.044
0.048

0.915
0.887
0.830
0.844
0.892
0.912
0.917
0.915
0.916
0.914
0.912

0.040
0.069
0.079
0.058
0.049
0.040
0.038
0.039
0.040
0.037
0.040

0.831
0.797
0.739
0.749
0.794
0.847
0.852
0.854
0.854
0.850
0.847

0.083
0.112
0.117
0.092
0.086
0.078
0.075
0.074
0.074
0.072
0.075

Table 5. Ablation study of ASNet. We change one component at
a time, to assess individual contributions. See § 4.2 for details.

consider 3 classical non-deep learning models: HS [60],
DRFI [21] and wCtr [67]. The results are obtained from
the authors or by running their public implementations with
original settings. The precision-recall curves of all methods
are plotted in Fig. 5. As can be seen, the ASNet outper-
forms other competitors2. We report maximum F-measure
and MAE scores in Table 4. Overall, the proposed method
achieves better performance over three datasets using all
evaluation metrics. Qualitative results over example images
from above datasets are depicted in Fig. 6; showing that
the proposed ASNet is well applicable to various complex
scenes.

4.2. Validation of the Proposed Algorithm

We now conduct a more detailed examination of our pro-
posed approach. We assess 1) contribution of the ﬁxation
prior for the SOD task, 2) the effects of convLSTM archi-
tecture, 3) the inﬂuence of stacked convLSTMs structure,
and 4) the importance of the introduced loss functions.
1. Does ﬁxation prior contribute to SOD? To answer this
question, we directly remove the ﬁxation prediction layer
and the corresponding loss function LAtt in Eqn. 13. Then,
we retrain ASNet with SOD data and obtain a baseline:
w/o ﬁxation. From Table 4, we ﬁnd that ﬁxation map is
indeed informative to SOD over all three datasets. The im-
provement is more pronounced on PASCAL-S [34] dataset,
which is collected from PASCAL challenge with more gen-
eral scenes and less center-bias. These results demonstrate
that a strong correlation exists between SOD and FP tasks,
and our ASNet achieves better performance with the guid-
ance from the ﬁxation map. This also demonstrate that the
leverage of large-scale FP data could improve the general-
ization ability of ASNet.

2Here we do not include the results from SU [26], since the authors

have not released code or PR-curve results.

2. What is the effect of convLSTM? Here, we study the
contribution of the convLSTM architecture, which consti-
tutes a building block of our ASNet. To this end, we replace
the convLSTMs with 5 convolution layers, which have 3×3
kernels and inputs/outputs with original dimensions. Thus
we have a baseline: w/o convLSTM. Such network has sim-
ilar architecture with previous bottom-up/top-down deep
learning models [44, 35]. From Table 4, we observe a drop
in F-score and MAE scores over three datasets which im-
plies the effectiveness of the convLSTM.
3. Is the hierarchical architecture meaningful? We also
study the effect of our hierarchical architecture with a stack
of several convLSTMs and top-down saliency inference.
We test 4 baselines: conv5-output, conv4-output, conv3-
output, and conv2-output, which correspond to the outputs
from the intermediate layers of ASNet. Note that the ﬁ-
nal prediction of ASNEt can be viewed as the output from
conv1 layer. We ﬁnd that the saliency results are gradually
optimized by adding more details from lower layers.
4. Are the extra loss functions necessary? ASNet is
equipped with 4 extra loss functions: LP , LR, LF , LM AE,
which are derived from four widely used SOD metrics.
For testing their effects, we retrain ASNet with different
loss functions separately and thus we have four baselines:
w/o LP , w/o LR, w/o LF , w/o LM AE. Another base-
line w/o extra loss indicates the results without consider-
ing all the extra loss functions. We show their F-measure
and MAE scores in Table 5. We observe that those loss
functions would boost the ﬁnal performance with about 1%
improvement in F-score.

5. Conclusions

We proposed a deep learning network, ASNet, towards a
better interpretable and efﬁcient SOD model, which lever-
ages ﬁxation prediction for detecting salient objects. The
ﬁxation map, as high-level knowledge of a scene, was
learned from upper layers of ASNet. Such prior was fur-
ther utilized for teaching the network where the salient ob-
ject is and the detailed object saliency was rendered step by
step by considering ﬁner and ﬁner features in a top-down
manner. ConvLSTM was equipped for iteratively dropping
useless features and enhancing the features for better repre-
sentation. A set of loss functions derived from SOD metrics
were introduced for boosting model predictions. Extensive
experimental results demonstrate that our approach outper-
forms state-of-the-art methods and conﬁrm our view that
ﬁxation map is valuable and indicative for SOD.

This paper points out two potential directions of follow-
ing works. The ﬁrst one regards exploring the rationale
behind SOD from the ﬁxation prediction viewpoint. The
second one is to seek better loss functions for boosting the
performance of deep learning based SOD models.

1718

References

[1] R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk.
In CVPR, 2009.

Frequency-tuned salient region detection.
2, 5

[2] B. Alexe, T. Deselaers, and V. Ferrari. Measuring the object-
ness of image windows. IEEE TPAMI, 34(11):2189–2202,
2012. 1

[3] A. Borji. What is a salient object? A dataset and a baseline
model for salient object detection. IEEE TIP, 24(2):742–756,
2015. 1

[4] A. Borji, M.-M. Cheng, H. Jiang, and J. Li. Salient object de-
tection: A benchmark. IEEE TIP, 24(12):5706–5722, 2015.
1, 2

[5] A. Borji and L. Itti. State-of-the-art in visual attention mod-

eling. IEEE TPAMI, 35(1):185–207, 2013. 2, 6

[6] A. Borji, D. N. Sihite, and L. Itti. What stands out in a
scene? A study of human explicit saliency judgment. Vision
research, 91:62–77, 2013. 1

[7] N. Bruce and J. Tsotsos. Saliency based on information max-

imization. In NIPS, 2006. 2

[8] N. D. Bruce and J. K. Tsotsos. Saliency, attention, and vi-
sual search: An information theoretic approach. Journal of
Vision, 9(3):5–5, 2009. 6

[9] X. Chen, A. Zheng, J. Li, and F. Lu. Look, perceive and seg-
ment: Finding the salient objects in images via two-stream
ﬁxation-semantic cnns. In ICCV, 2017. 3

[10] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S.-M.
IEEE
Hu. Global contrast based salient region detection.
TPAMI, 37(3):569–582, 2015. 1, 2, 6

[11] M. Cornia, L. Baraldi, G. Serra, and R. Cucchiara. Predict-
ing human eye ﬁxations via an lstm-based saliency attentive
model. arXiv preprint arXiv:1611.09571, 2016. 2

[12] D. Gao and N. Vasconcelos. Discriminant saliency for visual

recognition from cluttered scenes. In NIPS, 2005. 2

[13] J. Harel, C. Koch, and P. Perona. Graph-based visual

saliency. In NIPS, 2007. 2, 6

[14] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997. 4

[15] Q. Hou, M.-M. Cheng, X. Hu, A. Borji, Z. Tu, and P. Torr.
Deeply supervised salient object detection with short con-
nections. In CVPR, 2017. 2, 3, 7

[16] X. Hou and L. Zhang. Saliency detection: A spectral residual

approach. In CVPR, 2007. 2

[17] P. Hu, B. Shuai, J. Liu, and G. Wang. Deep level sets for

salient object detection. In CVPR, 2017. 2, 3, 7

[18] X. Huang, C. Shen, X. Boix, and Q. Zhao. SALICON: Re-
ducing the semantic gap in saliency prediction by adapting
deep neural networks. In ICCV, 2015. 2, 3, 6

[19] L. Itti, C. Koch, and E. Niebur. A model of saliency-based
IEEE TPAMI,

visual attention for rapid scene analysis.
20(11):1254–1259, 1998. 2, 6

[20] S. Jetley, N. Murray, and E. Vig. End-to-end saliency map-
ping via probability distribution prediction. In CVPR, 2016.
2, 3

[21] H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li.
Salient object detection: A discriminative regional feature
integration approach. In CVPR, 2013. 2, 7, 8

[22] M. Jiang, S. Huang, J. Duan, and Q. Zhao. SALICON:

Saliency in context. In CVPR, 2015. 6

[23] T. Judd, K. Ehinger, F. Durand, and A. Torralba. Learning to

predict where humans look. In ICCV, 2009. 2, 6

[24] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. In ICLR, 2015. 6

[25] S. S. Kruthiventi, K. Ayush, and R. V. Babu. Deepﬁx: A
fully convolutional neural network for predicting human eye
ﬁxations. IEEE TIP, 2017. 2, 3

[26] S. S. Kruthiventi, V. Gudisa,

J. H. Dholakiya, and
R. Venkatesh Babu. Saliency uniﬁed: A deep architecture
for simultaneous eye ﬁxation prediction and salient object
segmentation. In CVPR, 2016. 2, 3, 6, 7, 8

[27] J. Kuen, Z. Wang, and G. Wang. Recurrent attentional net-

works for saliency detection. In CVPR, 2016. 2

[28] O. Le Meur, P. Le Callet, D. Barba, and D. Thoreau. A coher-
ent computational approach to model bottom-up visual atten-
tion. IEEE TPAMI, 28(5):802–817, 2006. 2

[29] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-

supervised nets. In AISTATS, 2015. 5

[30] G. Lee, Y.-W. Tai, and J. Kim. Deep saliency with encoded
In CVPR,

low level distance map and high level features.
2016. 2, 7

[31] G. Li and Y. Yu. Visual saliency based on multiscale deep

features. In CVPR, 2015. 2, 3, 6, 7, 8

[32] G. Li and Y. Yu. Deep contrast learning for salient object

detection. In CVPR, 2016. 2, 3, 7

[33] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y. Zhuang,
H. Ling, and J. Wang. Deepsaliency: Multi-task deep neu-
IEEE TIP,
ral network model for salient object detection.
25(8):3919 – 3930, 2016. 7

[34] Y. Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille. The
secrets of salient object segmentation. In CVPR, 2014. 1, 2,
6, 7, 8

[35] N. Liu and J. Han. DHSNet: Deep hierarchical saliency net-
work for salient object detection. In CVPR, 2016. 2, 3, 7,
8

[36] N. Liu, J. Han, T. Liu, and X. Li. Learning to predict eye
ﬁxations via multiresolution convolutional neural networks.
IEEE TNNLS, 2016. 2, 3, 6

[37] T. Liu, J. Sun, N.-N. Zheng, X. Tang, and H.-Y. Shum.
Learning to detect a salient object. In CVPR, 2007. 1, 2
[38] Z. Luo, A. Mishra, A. Achkar, J. Eichel, S. Li, and P.-M.
Jodoin. Non-local deep features for salient object detection.
In CVPR, 2017. 2, 3, 7

[39] C. M. Masciocchi, S. Mihalas, D. Parkhurst, and E. Niebur.
Everyone knows what is interesting: Salient locations which
should be ﬁxated. Journal of vision, 9(11):25–25, 2009. 2

[40] S. Mathe and C. Sminchisescu. Dynamic eye movement
datasets and learnt saliency models for visual action recog-
nition. In ECCV. 2012. 2

[41] S. Mathe and C. Sminchisescu. Action from still image
dataset and inverse optimal control to learn task speciﬁc vi-
sual scanpaths. In NIPS, 2013. 2

[42] S. Mathe and C. Sminchisescu. Actions in the eye: Dynamic
gaze datasets and learnt saliency models for visual recogni-
tion. IEEE TPAMI, 37(7):1408–1424, 2015. 2

1719

[64] P. Zhang, D. Wang, H. Lu, H. Wang, and X. Ruan. Amulet:
Aggregating multi-level convolutional features for salient
object detection. In ICCV, 2017. 2, 3, 7

[65] P. Zhang, D. Wang, H. Lu, H. Wang, and B. Yin. Learning
uncertain convolutional features for accurate saliency detec-
tion. In ICCV, 2017. 2, 3, 7

[66] R. Zhao, W. Ouyang, H. Li, and X. Wang. Saliency detection
by multi-context deep learning. In CVPR, 2015. 2, 3
[67] W. Zhu, S. Liang, Y. Wei, and J. Sun. Saliency optimization

from robust background detection. In CVPR, 2014. 2, 7, 8

[43] J. Pan, E. Sayrol, X. Giro-i Nieto, K. McGuinness, and N. E.
O’Connor. Shallow and deep convolutional networks for
saliency prediction. In CVPR, 2016. 2, 3, 6

[44] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Doll´ar. Learn-

ing to reﬁne object segments. In ECCV, 2016. 3, 8

[45] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
3

[46] E. Vig, M. Dorr, and D. Cox. Large-scale optimization of hi-
erarchical features for saliency prediction in natural images.
In CVPR, 2014. 2, 6

[47] L. Wang, H. Lu, X. Ruan, and M.-H. Yang. Deep networks
for saliency detection via local estimation and global search.
In CVPR, 2015. 2, 7

[48] L. Wang, L. Wang, H. Lu, P. Zhang, and X. Ruan. Saliency
detection with recurrent fully convolutional networks.
In
ECCV, 2016. 2, 3, 7

[49] T. Wang, A. Borji, L. Zhang, P. Zhang, and H. Lu. A stage-
wise reﬁnement model for detecting salient objects in im-
ages. In ICCV, 2017. 2, 7

[50] W. Wang and J. Shen. Deep cropping via attention box pre-

diction and aesthetics assessment. In ICCV, 2017. 1

[51] W. Wang and J. Shen. Deep visual attention prediction. IEEE

TIP, 27(5):2368–2378, 2018. 2, 3, 6

[52] W. Wang, J. Shen, and F. Porikli. Saliency-aware geodesic

video object segmentation. In CVPR, 2015. 1

[53] W. Wang, J. Shen, and L. Shao. Consistent video saliency
using local gradient ﬂow optimization and global reﬁnement.
IEEE TIP, 24(11):4185–4196, 2015. 2

[54] W. Wang, J. Shen, and L. Shao. Video salient object detec-
tion via fully convolutional networks. IEEE TIP, 27(1):38–
49, 2018. 2

[55] W. Wang, J. Shen, L. Shao, and F. Porikli. Correspondence
driven saliency transfer. IEEE TIP, 25(11):5025–5034, 2016.
2, 6

[56] W. Wang, J. Shen, R. Yang, and F. Porikli.

Saliency-
aware video object segmentation. IEEE TPAMI, 40(1):20–
33, 2018. 2

[57] W. Wang, J. Shen, Y. Yu, and K.-L. Ma. Stereoscopic thumb-
IEEE
nail creation via efﬁcient stereo saliency detection.
TVCG, 23(8):2014–2027, 2017. 1

[58] Y. Wei, F. Wen, W. Zhu, and J. Sun. Geodesic saliency using

background priors. ECCV, 2012. 2

[59] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong,
and W.-C. Woo. Convolutional LSTM network: A machine
In NIPS,
learning approach for precipitation nowcasting.
2015. 2, 4

[60] Q. Yan, L. Xu, J. Shi, and J. Jia. Hierarchical saliency detec-

tion. In CVPR, 2013. 2, 6, 7, 8

[61] C. Yang, L. Zhang, H. Lu, X. Ruan, and M.-H. Yang.
Saliency detection via graph-based manifold ranking.
In
CVPR, 2013. 6

[62] J. Zhang and S. Sclaroff. Saliency detection: A boolean map

approach. In ICCV, 2013. 6

[63] L. Zhang, M. H. Tong, T. K. Marks, H. Shan, and G. W. Cot-
trell. SUN: A bayesian framework for saliency using natural
statistics. Journal of vision, 8(7):32–32, 2008. 2

1720

Salient Object Detection Driven by Fixation Prediction

Wenguan Wang 1, Jianbing Shen∗ 1,2, Xingping Dong 1, Ali Borji3
Beijing Lab of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, China

1

2

3

Inception Institute of Artiﬁcial Intelligence, Abu Dhabi, UAE

Department of Computer Science, University of Central Florida, USA

wenguanwang.ai@gmail.com, {shenjianbing, dongxingping}@bit.edu.cn, aborji@crcv.ucf.edu

Abstract

Research in visual saliency has been focused on two ma-
jor types of models namely ﬁxation prediction and salient
object detection. The relationship between the two, how-
ever, has been less explored.
In this paper, we propose
to employ the former model type to identify and segment
salient objects in scenes. We build a novel neural network
called Attentive Saliency Network (ASNet)1 that learns to
detect salient objects from ﬁxation maps. The ﬁxation map,
derived at the upper network layers, captures a high-level
understanding of the scene. Salient object detection is then
viewed as ﬁne-grained object-level saliency segmentation
and is progressively optimized with the guidance of the ﬁx-
ation map in a top-down manner. ASNet is based on a hier-
archy of convolutional LSTMs (convLSTMs) that offers an
efﬁcient recurrent mechanism for sequential reﬁnement of
the segmentation map. Several loss functions are introduced
for boosting the performance of the ASNet. Extensive exper-
imental evaluation shows that our proposed ASNet is capa-
ble of generating accurate segmentation maps with the help
of the computed ﬁxation map. Our work offers a deeper in-
sight into the mechanisms of attention and narrows the gap
between salient object detection and ﬁxation prediction.

1. Introduction

Salient object detection (SOD) has been studied exten-
sively for more than a decade (since [37]). It has several
applications such as segmentation [52, 10], object proposal
generation [2], and image resizing [57, 50]. Recently, the
use of deep neural networks for saliency detection has been
trending. Although promising results have been achieved,
they occasionally fail to detect the most salient object in

∗Corresponding author: Jianbing Shen. This work was supported in
part by the Beijing Natural Science Foundation under Grant 4182056, the
Fok Ying Tung Education Foundation under Grant 141067, and the Spe-
cialized Fund for Joint Building Program of Beijing Municipal Education
Commission.

1Available at: https://github.com/wenguanwang/ASNet.

(a)

(b)
Figure 1. Given complex scenes like (a), what are the salient
objects? We propose the Attentive Saliency Network (ASNet)
that infers the object saliency (b) from predicted ﬁxation maps (c),
which is consistent with human attention mechanisms.

(c)

complex cluttered scenes containing several objects (such
as the ones shown in Fig. 1 (a)). Additionally, for current
computational saliency models, their connection with how
humans explicitly choose salient objects or watch natural
scenes are less clear (as discussed in [3, 6]).

In this paper, we take a further step towards a more bi-
ologically plausible SOD model equipped with high-level
prior of ﬁxation map. The human attention prior, rep-
resented by eye movements or from a ﬁxation prediction
model, is instinctive and more consistent with visual pro-
cessing of human visual system. The suggested model not
only generates high-quality object saliency maps, but also
pushes the boundary of SOD research by building a close
connection to human ﬁxation prediction (FP). As shown in
Fig. 1, our model infers object saliency using the ﬁxation
prior, where this prior acts as a selective mechanism to en-
hance the saliency representation for the purpose of accu-
rate object saliency inference. Our algorithm has bias on
the assumption that aligns with the core views of previous
studies [34, 4] that explored the relationship between eye
movements (implicit saliency) and explicit object saliency.
These studies conﬁrmed a strong correlation between ﬁx-
ations and salient objects. Further beyond treating FP and
SOD as two separate tasks that are learned in a branched

11711

network (as in [26]), these two tasks are more deeply inter-
connected in our model. The ﬁxation map provides a high-
level signal, which is learned from upper layers of our neu-
ral network. It is then used for salient object detection in a
top-down manner. This process is straightforward and sim-
ilar to how humans process a scene sequentially (i.e., ﬁrst
paying attention to important areas of a scene quickly, and
then taking more efforts for precise segmentation). Further,
leveraging the rich information from exiting large-scale eye
movement datasets can improve the robustness and gener-
alization ability of SOD models.

The proposed Attentive Saliency Network (ASNet) is
based on convolutional LSTM (convLSTM) [59], which has
convolutional structures in both the input-to-state and state-
to-state transitions. Further, beyond the fully connected
LSTM models, convLSTM encodes the spatial information
via convolution operations to facilitate pixel-wise saliency
labeling. The key advantage of LSTM is iterative removal
of irrelevant information and learning powerful represen-
tations through updating the memory cell. This allows the
ASNet to progressively optimize features for better saliency
estimation in a feed-forward strategy. By stacking multiple
convLSTMs, the ASNet is trainable for gradually rendering
object saliency from ﬁxation map in a top-down manner.

Our contributions are manifold:

• We aim to infer salient objects (captured in lower net-
work layers) from the ﬁxation map (encoded in higher lay-
ers) within a uniﬁed neural network. This goes one step
beyond previous deep learning based saliency models and
offers a deep insight into the conﬂuence between ﬁxation
prediction and salient object detection.
• We present the Attentive Saliency Network (ASNet)
which is a hierarchy of convLSTMs for step-wise inference
of object saliency. ConvLSTM has the advantage of the
improved ﬂow of information with recurrent connections,
which results in more powerful saliency representation.
• We introduce novel loss functions for SOD, derived from
exiting SOD evaluation metrics, for capturing several qual-
ity factors. As we will show empirically, these new loss
functions lead to higher performance.

2. Related Work

In this section, we ﬁrst brieﬂy review the ﬁxation pre-
diction (§ 2.1) and salient object detection literature (§ 2.2).
Then, in § 2.3, we discuss studies exploring the relationship
between the above two tasks.

2.1. Fixation Prediction (FP)

Fixation Prediction (FP) aims to identify the ﬁxation
points that human viewers would focus on at ﬁrst glance.
It has a long history from [19] and is still active in vision
research. From the view of mechanism to obtain atten-
tion, early attention models can be classiﬁed into different

schools [5], such as cognitive model [19, 28, 40, 42, 41],
Bayesian model [63], decision theoretic model [12], infor-
mation theoretic model [7], graphical model [13], spectral
analysis model [16], pattern classiﬁcation model [23], etc.
We refer the readers to [5] for more detailed overview.

More recently, many deep learning based attention
models have been proposed. The eDN model [46] repre-
sented an early architecture that automatically learns deep
representations for FP. After that, DeepFix [25], SALICON
net [18], Mr-CNN [36], Shallow and Deep [43], attentive
LSTM [11], DVA net [51], were successively proposed,
with the use of deeper networks and more complex architec-
tures. Jetley et al. [20] tested several loss functions based on
probability distance measures and found Bhattacharyya dis-
tance could give the best performance. Those deep learning
solutions generally achieved better performance, compared
with traditional non-deep learning techniques.

2.2. Salient Object Detection (SOD)

Salient object detection (SOD) aims at highlighting
salient object regions in images. Different from FP that is
originated from cognition and psychology research commu-
nities, SOD is more a computer vision task that is driven by
object-level applications [56]. The history of SOD is rela-
tively more recent and dates back to the works of Liu et al.
[37] and Achanta et al. [1]. Due to lack of the knowledge
of image content, most non-deep learning SOD models
[10, 58, 60, 21, 67, 53, 55] are based on low-level features
and certain heuristic hypothesizes (such as contrast, back-
ground prior). Please see [4] for detailed overviews.

In more recent work, deep learning based SOD mod-
els have made substantial improvement. These methods
mainly utilize multi-scale and multi-level representations
[31, 64, 38], integrate both local estimation and global
search [47], explore global and local context information
[66], combine pixel- and segment-level features [32], de-
velop level set [17], or consider short connections with skip-
layer structures [15], based on neural network. Some other
methods try to integrate deep learning models with hand-
crafted features [30], study saliency prior [48], or exploit
various deep learning architectures [27, 35, 65, 49, 54].

2.3. The Relationship between FP and SOD

Although SOD has been extensively studied in computer
vision research, only few studies (e.g., [39, 4, 34]) have ex-
plored how humans explicitly choose salient objects. They
have quantitatively conﬁrmed that object saliency judg-
ments agree with human ﬁxations. According to the analy-
sis in [4], there exists a strong correlation between explicit
saliency judgments and free-viewing ﬁxations, which can
be viewed as two proxies of visual attention. Li et al. [34]
have demonstrated that, unlike FP datasets, there exists a
heavy bias in many widely used SOD datasets. Most of the

1712

Figure 2. Typical network architectures used in previous FP or SOD models and our ASNet. (a) Single-stream network, (b) Multi-
stream network, and (c) Skip-layer network. (d) Branched network adopted in [26], where FP and SOD are achieved via two branches
sharing several bottom layers. (e) The adopted ASNet captures ﬁxation map from upper layer, which is indicative of the inference of object
saliency from lower layers. Stack of convLSTMs are adopted for iteratively optimizing features, while preserving spatial information.

SOD datasets have only a few obvious objects in the scene.
Next, we discuss several representative deep models in
SOD or FP from the view of network architecture. This
would better situate our work with respect to previous works
and help to highlight our contributions. As shown in Fig. 2,
most deep learning models for FP or SOD only consider a
single task. Typical architectures include: (a) single-stream
network [25, 20, 43, 65, 17] (standard architecture), (b)
multi-stream network [18, 66, 31, 36] (training with multi-
scale inputs), and (c) skip-layer network [15, 32, 64, 38]
(concatenate multi-layer responses for ﬁnal output). As
seen, previous deep learning based works often treated FP
and SOD as two unrelated tasks.

Instead of performing FP or SOD separately, we ex-
ploit the correlation between ﬁxations and salient objects
via tightly coupling these two tasks in a uniﬁed deep learn-
ing architecture. There are only few methods consider FP
and SOD tasks together. In [9], ﬁxation map from a pre-
trained FP model is used as an extra cue for guiding SOD,
while it didn’t emphasize learning both FP and SOD simul-
taneously. In [26], FP and SOD are achieved via two sep-
arate network branches, which only share weights in sev-
eral lower layers (Fig. 2 (d)). In our method, as illustrated
in Fig. 2 (e), ﬁxation map (high-level knowledge captured
in top layers) is used for guiding accurate object saliency
detection in lower layers. Thus our approach goes beyond
above work by learning FP and SOD within a uniﬁed net-
work and in a top-down end-to-end manner.

3. Our Approach

Given an input image, the goal is to produce a pixel-wise
saliency map to highlight salient object regions. As demon-
strated in Fig. 3, the proposed ASNet ﬁrst captures a global
and high-level understanding of a scene in its higher layers,
by learning to predict human ﬁxations (§3.1). Then, it uses
a stack of convLSTMs to progressively infer object saliency
from the ﬁxation map in a top-down and coarse-to-ﬁne man-
ner (§3.2). The whole network is simultaneously trained to
predict ﬁxation locations and to detect salient objects in an
end-to-end way (§3.3).

3.1. Fixation Predicting

At the bottom of ASNet resides a stack of convolutional
layers where the lower layers respond to primitive image
features such as edges, corners and shared common pat-
terns, and the higher layers extract semantic information
like objects or faces. The ASNet learns the FP as a high-
level task towards modeling human ﬁxation locations with
the utilization of features from higher layers, and achieves
the SOD by optimizing the ﬁxation prior with the features
from the lower layers.

The lower convolutional layers are borrowed from the
ﬁrst ﬁve convolutional blocks of VGGNet [45] (13 convolu-
tional layers in total). We omit the last pooling layer (pool5)
for preserving more spatial information. For a training im-
age, with a typical resolution of 224 × 224 × 3, we com-
pute a convolutional layer by applying a 3×3 kernel with
sigmoid activation function, to the last convolutional fea-
ture map (14 × 14 × 512). The result is a probability map
P ∈ [0, 1]14×14 which is used as a ﬁxation prior from global
and high-level image context. The model for the task of
FP is trained via minimizing the following Kullback-Leibler
Divergence (KL-Div) loss function:

LAtt(G, P ) =

1

14×14

14 × 14 X

x

gx log(

),

(1)

gx
px

where G denotes the resized ground-truth attention map
G ∈ [0, 1]14×14 and gx ∈ G, px ∈ P . The gray-scale ﬁx-
ation map is obtained via ﬁltering the binary ﬁxation map
using a Gaussian ﬁlter with small variance. The KL-Div
measure, the minimization of which is equivalent to cross-
entropy minimization, is widely used in visual saliency pre-
diction [18, 51]. In the next section, we will leverage such
ﬁxation map as the prior for producing object saliency.

3.2. Detecting Object Saliency with Fixation Prior

The ﬁxation map P gives a coarse but informative prior
regarding visually salient regions. A number of previous
studies for pixel-labeling tasks such as semantic segmenta-
tion [44], and salient object detection [35, 48], have shown
that neural networks are capable of producing ﬁne-gained

1713

gates i, f, c. At each step t, as a new input Xt arrives, its in-
formation will be accumulated to the cell if the input gate it
is activated. Also, the past cell status Ct−1 could be “forgot-
ten” in this process if the forget gate ft is on. Whether the
latest cell status Ct should be propagated to the ﬁnal state Ht
is further controlled by the output gate ot. Formally, above
memory update process at step t is driven by the following
equations:

i ∗ Xt + W H
f ∗ Xt + W H
o ∗ Xt + W H

it = σ(W X
ft = σ(W X
ot = σ(W X
Ct = ft ◦ Ct−1 +it ◦tanh(W X
Ht = ot ◦ tanh(Ct),

i ∗ Ht−1 + bi),
f ∗ Ht−1 + bf ),
o ∗ Ht−1 + bo),
c ∗Xt +W H

(2)

(3)

(4)

c ∗Ht−1 +bc), (5)
(6)

where ‘∗’ denotes the convolution operator and ‘◦’ repre-
sents element-wise product. σ and tanh are the activa-
tion functions of logistic sigmoid and hyperbolic tangent.
The inputs Xt, cell memory Ct, hidden states Ht and gates
it, ft, ct are 3D tensors whose spatial dimensions are the
same. W s and bs are the learned weights and biases.

In our case, the convLSTM takes the features X ex-
tracted from the convolutional neural network (from the last
convolutional layers prior to pooling layers) as input, and
produces reﬁned saliency features for ﬁnal saliency estima-
tion. Since it operates on static images, the input features in
all steps are the same: X1 = . . . = Xt = X (see Fig. 4).
Here, we take the advantages of recurrent natures of LSTM
for iteratively optimizing the saliency features of static im-
ages, instead of using LSTM for modeling the temporal de-
pendency of sequential data.

With the learned ﬁxation prior P ∈ [0, 1]14×14, we ﬁrst
combine P with the convolutional features from conv5-3
and feed them into a convLSTM. In each time step, the con-
vLSTM is trained for inferring the salient object with the
knowledge of ﬁxation information, and sequentially opti-
mizes the features with the updated memory cell and hidden
states (see Fig. 4 (b)). Thus, the features are reorganized
towards better representation of object saliency. More spe-
cially, we ﬁrst compress the feature responses from conv5-3
layer via a convolutional layer with 64 ﬁlters to lower com-
putational costs and adopt sigmoid activation for regulariz-
ing the response from features to lie within the same range
([0, 1]) of P . Then, the attention prior map P is concate-
nated with the compressed features and fed into the con-
vLSTM. We apply a 1×1 convolution kernel to the ﬁnal
convLSTM output H for obtaining an object saliency map
Q ∈ [0, 1]14×14.

Several different metrics have been proposed for evaluat-
ing saliency models and no single metric can fully summa-
rize the performance of a model. This motivates us to com-
bine the classical weighted cross-entropy loss function with

1714

Figure 3. Architecture of the proposed ASNet. The ﬁxation map
is learned from the upper layers and is used by the ASNet to locate
the salient objects. Then, the ﬁne-grained object saliency is grad-
ually inferred from lower layers and is successively optimized via
the recurrent architecture of convLSTM. Zoom-in for details.

labeling results via incorporating high-level information en-
coded in upper network layers. Here, we desire our model
to be able to infer precise object saliency from the ﬁxation
map predicted in the upper network layers.

The network is trained for detecting and successively re-
ﬁning the salient object via aggregating information from
high-level ﬁxation map and the spatially rich information
from low-level network features. As shown in Fig. 3, the
SOD is computed in a top-down fashion, successively in-
tegrating information from earlier layers. Multiple convL-
STM networks [59] (the yellow blocks in Fig. 3) are stacked
for building more meaningful feature representations with
recurrent connections. We leverage the sequential nature
of LSTM to process features in an iterative way. For a
certain layer, convLSTM discards less informative features
while enhances informative features, thus generating grad-
ually improved saliency maps.

ConvLSTM extends traditional fully connected LSTM
[14] to consume spatial features. Basically, this is achieved
by substituting dot products with convolutional operations
in the LSTM equations. ConvLSTM has convolutional
structures in both the input-to-state and state-to-state transi-
tions, which can preserve the spatial information of convo-
lutional feature map, thus enabling our network to produce
a pixel-wise labeling.

A schematic diagram of convLSTM is presented in Fig.
4. Similar to traditional gated LSTMs, the convLSTM uses
the memory cells and gates to control information ﬂow.
It works by sequentially updating an internal state H and
memory cell C, according to the values of three sigmoid

(a)

(b)

Figure 4. Illustration of our convLSTM based object saliency optimization, where (b) shows detailed architecture of our convLSTM
optimization module in (a). Zoom-in for details.

precision, recall, F-measure, and MAE metrics for more ef-
ﬁcient training. Given the ground-truth salient object an-
notation S (here S ∈ {0, 1}14×14 for conv5-3 layer), the
overall loss function is deﬁned as:

LSal(S, Q) = LC(S, Q)+α1LP (S, Q)+α2LR(S, Q)

+α3LF (S, Q)+α4LM AE(S, Q),

where αs are balance parameters and are empirically set as
α1 = α2 = α3 = α4 = 0.1. LC is the weighted cross-
entropy loss function, which is widely adopted for training
SOD models and opted as the primary loss in our case:

After obtaining the object saliency map Q ∈ [0, 1]14×14
inferred from the ﬁxation map P , we upsample (×2) Q
and feed it to the next convLSTM with the compressed
features (28×28×64) from conv4-3 layer for more de-
tailed reﬁnement. Above process is iteratively applied for
conv4-3, conv3-3, conv2-2 and conv1-2 layers, respectively.
Finally, the ASNet outputs a high-quality object saliency
mask (224×224×1). In sum, the ASNet is able to effec-
tively infer the object saliency thanks to 1) a learnable ﬁxa-
tion prior, 2) iteratively updating saliency features with re-
current architecture and 3) efﬁciently merging spatially rich
information from lower layers in a top-down manner.

LC(S, Q) =

γ · (1 − sx) · log(1 − qx)

3.3. Implementation Details

(7)

(8)

1
N Xx(cid:0)

+ (1 − γ) · sx · log qx

,

(cid:1)

where N is the total number of pixels and sk ∈ S, qk ∈
Q. γ refers to the ratio of salient pixels in ground truth S.
Weighted cross-entropy loss handles the imbalance between
number of salient and non-salient pixels.

LP , LR and LF are computed similar to precision, recall

and F-measure scores:

LP (S, Q) = −

Xx

sx · qx.(

Xx

qk + ǫ),

LR(S, Q) = −

LF (S, Q) = −

sk + ǫ),

sx · qx.(

Xx
Xx
(1 + β2) · LP (S, Q) · LR(S, Q)
β2 · LP (S, Q) + LR(S, Q) + ǫ

.

(9)

(10)

(11)

where β2 = 0.3 as suggested by [1], and ǫ is a regular-
ization constant. Since precision, recall and F-measure are
similarity metrics and higher values are better, negative val-
ues are used for minimizing.

LM AE is derived from the mean absolute error (MAE)
measure that computes the discrepancy between the
saliency map Q and the ground-truth map S:

LM AE(S, Q) =

|sx − qx|.

(12)

1
N Xx

Overall loss: Let I = {Ik, k = 1, . . . , K} denote all the
training images (resized into 224 × 224) borrowed from ex-
iting SOD or FP datasets. Since there are only few datasets
that offer annotations for both SOD and FP tasks, most of
the training images are either labeled with human ﬁxation
annotation or object saliency mask. Let yA
k ∈ {0, 1} and
yS
k ∈ {0, 1} indicate whether we have the attention anno-
tation Gk and object saliency mask Sk for the k-th training
image. Our ﬁnal loss function can be expressed as:

L =

yA
k ·LAtt(Gk, Pk)+

yS
k ·

LSal(Sℓ

k, Qℓ

k), (13)

K

X
k=1

K

X
k=1

5

X
ℓ=1

k and yS

where the loss functions LAtt and LSal are deﬁned in Eqn.
1 and Eqn. 7, respectively. The indicators yA
k are
employed to remedy missing ground truth in correspond-
ing tasks. That is the error is not propagated back when
the annotations are not offered. The ℓ ∈ {1, . . . , 5} refers
to the ℓ-th convLSTM with conv-ℓ block in ASNet. With
the hierarchical loss functions, each layer in ASNet has di-
rect access to the gradients from the loss function leading
to an implicit deep supervision [29]. We set the time steps
to three in our convLSTM and employ 3 × 3 kernels for
convolution operations.

1715

Dataset

#Images

Resolution

Annotation
FP SOD

Training

Testing

640 × 480

SALICON [22] 15,000 X
THUS10K [10] 10,000

PASCAL-S[34]

X max(w, h) = 400
DUT-OMRON [61] 5,168 X X max(w, h) = 400
X X max(w, h) = 500
max(w, h) = 1024
X max(w, h) = 400
X max(w, h) = 400

ECCSD[60] 1,000
HKU-IS[31] 4,447

MIT1003[23] 1,004 X

850

Table 1. Datasets used for training and testing the ASNet.

Training datasets: Another advantage of ASNet is that it
can use data from both SOD and FP benchmarks. We con-
sider three large-scale saliency datasets: SALICON [22],
THUS10K [10], and DUT-OMRON [61]. The SALICON
dataset is widely used in the domain of FP, while the
THUS10K dataset is commonly used for SOD. These two
datasets have annotations for ﬁxations and salient objects,
respectively. We further utilize the DUT-OMRON dataset
which offers annotations for both FP and SOD. Detailed de-
scriptions of employed datasets can be found in Table 1.
Training settings: In each training iteration, we use a min-
batch of 10 images, which are sampled from above three
datasets and ensure data balance between SOD and FP. Data
augmentation techniques (e.g., ﬂipping, rotation) are also
adopted. Our model is implemented in Python on Keras,
and trained with the Adam optimizer [24]. During the train-
ing phase, the learning rate is set to 0.0001 and is decreased
by a factor of 10 every two epochs. The networks were
trained for 12 epochs with early stopping strategy.
Computation load: ASNet is simultaneously trained for
FP and SOD tasks in an end-to-end manner. The entire
training procedure takes about 10 hours with a NVIDIA TI-
TAN X GPU and a 4.0GHz Intel processor. It takes about
0.08s to process an image of 224 × 224 size, which is faster
than most deep learning based competitors.

4. Experimental results

In this section, we ﬁrst examine the performance of AS-
Net for the FP task. The goal of this experiment is to in-
vestigate the effectiveness of the learned ﬁxation map prior,
instead of comparing it with the state-of-the-art FP models.
Then we evaluate the performance of the ASNet for the pri-
mary SOD task. Finally, an ablation study is performed to
gain a deeper insight into the proposed ASNet.
Testing datasets: Four datasets including PASCAL-S [34],
MIT1003 [23], ECCSD [60] and HKU-IS [31] are used for
testing our model. PASCAL-S offers both annotations for
FP and SOD, MIT1003 is a representative benchmark for
FP, and ECCSD and HKU-IS are two typical datasets which
are widely used for SOD. We report the evaluation results
for both tasks over the PASCAL-S dataset. More details of
above datasets can be found in Table 1.
Evaluation metrics: For the FP task, there are several ways

Methods AUC-Judd ↑ SIM ↑ shufﬂed AUC ↑ CC ↑ NSS ↑
1.36
1.86
1.60
1.73
2.08
1.29
1.25
0.82
1.38
1.10
2.01
2.30

Mr-CNN [36]
SALICON [18]
Shallow-Net [43]
Deep-Net [43]
SU [26]
eDN [46]
BMS [62]
AIM [8]
GBVS [13]
ITTI [19]
ASNet-14×14
ASNet-28×28

0.38
0.53
-
0.51
-
0.41
0.36
0.26
0.42
0.33
0.60
0.65

0.35
0.42
-
0.40
-
0.30
0.33
0.27
0.36
0.32
0.49
0.52

0.80
0.85
-
0.86
-
0.85
0.79
0.79
0.83
0.77
0.87
0.88

0.73
0.74
0.68
0.73
0.73
0.66
0.69
0.68
0.66
0.66
0.73
0.75

- The authors in [26, 43] have not released detailed results.

Table 2. Quantitative comparison of different FP models on the
MIT1003 [23] dataset.

Methods AUC-Judd ↑ SIM ↑ shufﬂed AUC ↑ CC ↑ NSS ↑
1.35
1.88
1.90
1.74
2.22
1.42
1.28
0.97
1.36
1.30
2.26
2.43

Mr-CNN [36]
SALICON [18]
Shallow-Net [43]
Deep-Net [43]
SU [26]
eDN [46]
BMS [62]
AIM [8]
GBVS [13]
ITTI [19]
ASNet-14×14
ASNet-28×28

0.40
0.58
-
0.55
-
-
-
0.32
0.45
0.42
0.70
0.73

0.34
0.46
-
0.42
-
-
-
0.30
0.36
0.36
0.55
0.59

0.79
0.86
-
0.87
-
-
-
0.77
0.84
0.82
0.90
0.90

0.71
0.72
0.69
0.71
0.73
0.65
0.67
0.65
0.65
0.64
0.74
0.74

- The authors in [43, 26, 46, 62] have not released detailed results.
Table 3. Quantitative comparison of different FP models on the
PASCAL-S [23] dataset.

to measure the agreement between model predictions and
human eye movements. Here, we employ ﬁve typical met-
rics, namely Normalized Scanpath Saliency (NSS), Simi-
larity Metric (SIM), Linear Correlation Coefﬁcient (CC),
AUC-Judd, and shufﬂed AUC. Please refer to [5, 51] for
detailed descriptions of these metrics. For the SOD task,
three standard metrics, namely PR-curve, F-measure, and
MAE, are employed for evaluation. See [55] for details.

4.1. Performance of ASNet

Performance on FP task: We evaluated the ﬁxation prior
map generated by ASNet compared to 10 state-of-the-art
ﬁxation models, including 4 classical models: ITTI [19],
GBVS [13], AIM [8], BMS [62], and 6 deep learning based
models: eDN [46], SALICON [18], SU [26], Mr-CNN [36],
Shallow-Net [43] and Deep-Net[43]. Results are reported
over PASCAL-S [34] and MIT1003 [23] datasets.

Our ASNet is able to generate a ﬁxation prediction map
P from top layer, which is relatively rough, and much
smaller (only 14×14) compared to exiting ﬁxation models.
For the sake of a fairer comparison and deeper insight into
the advantage of our ASNet, we further generate a larger
ﬁxation map (28×28) via feeding P into an additional con-

1716

(a) ECCSD

(b) HKU-IS

(c) PASCAL-S

Figure 5. SOD results with PR-curve on three widely used benchmarks: ECCSD [60], HKU-IS [31] and PASCAL-S [34], where the
scores from non-deep learning models are indicated by dashed lines. Best viewed in color.

Figure 6. Qualitative results of ASNet. From top to bottom: example images, ﬁxation maps, and object saliency results.

vLSTM with the features from conv4-3 layer. We therefore
derive two baselines: ASNet-14×14 and ASNet-28×28,
corresponding to the rough prediction and the reﬁned at-
tention map with a more detailed spatial information.

As shown in Table 2 and Table 3, ASNet-14×14 per-
forms better than previous non-deep learning models and
is competitive with current top-performing deep learning
contenders. Considering our relatively simple network ar-
chitecture, and smaller output resolution (14×14), the sug-
gested ASNet is much favorable and effective. We at-
tribute this primarily to the extra generality and powerful
saliency representations learned from the SOD task. Ad-
ditionally, ASNet-28×28 produces further better results,
which demonstrates the proposed ASNet has potential of
obtaining better FP results with considering more detailed
spatial information.
Performance on SOD task: Here we evaluate the per-
formance of ASNet on its primary task: SOD. We per-
form quantitative study on 3 widely used datasets, namely
ECCSD [60], HKU-IS [31] and PASCAL-S [34]. We com-
pare ASNet against 14 recent deep learning based alterna-
tives: LEGS [47], MDF [31], DS [33], SU [26], DCL [32],
ELD [30], RFCN [48], DHS [35], HEDS [15], NLDF [38],
DLS [17], AMU [64], UCF [65], and SRM [49]. We also

SU [26]

Methods

ECCSD [60]

HS [60] 0.730
DRFI [21] 0.787
wCtr [67] 0.672
MDF [31] 0.831
LEGS [47] 0.831
DS [33] 0.810
DCL [32] 0.898
ELD [30] 0.865
0.88
RFCN [48] 0.898
DHS [35] 0.905
HEDS [15] 0.915
NLDF [38] 0.905
DLS [17] 0.825
AMU [64] 0.889
UCF [65] 0.868
SRM [49] 0.910
ASNet 0.928

HKU-IS [31] PASCAL-S [34]
F-score↑ MAE↓ F-score↑ MAE↓ F-score↑ MAE↓
0.259
0.196
0.193
0.145
0.155
0.170
0.108
0.121
0.10
0.118
0.091
0.080
0.099
0.136
0.098
0.116
0.127
0.072

0.223
0.166
0.178
0.108
0.119
0.160
0.071
0.080
0.06
0.097
0.061
0.052
0.063
0.090
0.058
0.068
0.056
0.043
- The authors in [26] have not released detailed results.
∗ MDF [31] is trained on a subset of HKU-IS, and evaluated on the
remaining images.

0.636
0.215
0.710
0.692
0.143
0.783
0.611
0.138
0.694
0.860* 0.129* 0.764
0.749
0.101
0.812
0.818
0.078
0.848
0.822
0.048
0.907
0.767
0.071
0.844
0.77
-
-
0.827
0.079
0.895
0.820
0.052
0.892
0.830
0.039
0.913
0.831
0.048
0.902
0.719
0.072
0.806
0.834
0.052
0.918
0.771
0.062
0.905
0.783
0.046
0.892
0.857
0.035
0.920

Table 4. The F-measure and MAE scores of SOD on three pop-
ular datasets. ASNet gains the best performance with the assis-
tance of visual attention prior. See text for details.

1717

Aspects

Methods

ECCSD [60]

HKU-IS [31] PASCAL-S [34]
F-score↑ MAE↓ F-score↑ MAE↓ F-score↑ MAE↓

0.928

0.043

0.920

0.035

0.857

0.072

variants

archi-
tecture

loss

ASNet
(conv1-output)

w/o ﬁxation 0.913
w/o convLSTM 0.891
conv5-output 0.853
conv4-output 0.875
conv3-output 0.903
conv2-output 0.919
w/o LP
0.923
w/o LR 0.924
0.924
w/o LF
w/o LM AE 0.921
w/o extra loss 0.917

0.051
0.068
0.093
0.076
0.061
0.049
0.045
0.046
0.047
0.044
0.048

0.915
0.887
0.830
0.844
0.892
0.912
0.917
0.915
0.916
0.914
0.912

0.040
0.069
0.079
0.058
0.049
0.040
0.038
0.039
0.040
0.037
0.040

0.831
0.797
0.739
0.749
0.794
0.847
0.852
0.854
0.854
0.850
0.847

0.083
0.112
0.117
0.092
0.086
0.078
0.075
0.074
0.074
0.072
0.075

Table 5. Ablation study of ASNet. We change one component at
a time, to assess individual contributions. See § 4.2 for details.

consider 3 classical non-deep learning models: HS [60],
DRFI [21] and wCtr [67]. The results are obtained from
the authors or by running their public implementations with
original settings. The precision-recall curves of all methods
are plotted in Fig. 5. As can be seen, the ASNet outper-
forms other competitors2. We report maximum F-measure
and MAE scores in Table 4. Overall, the proposed method
achieves better performance over three datasets using all
evaluation metrics. Qualitative results over example images
from above datasets are depicted in Fig. 6; showing that
the proposed ASNet is well applicable to various complex
scenes.

4.2. Validation of the Proposed Algorithm

We now conduct a more detailed examination of our pro-
posed approach. We assess 1) contribution of the ﬁxation
prior for the SOD task, 2) the effects of convLSTM archi-
tecture, 3) the inﬂuence of stacked convLSTMs structure,
and 4) the importance of the introduced loss functions.
1. Does ﬁxation prior contribute to SOD? To answer this
question, we directly remove the ﬁxation prediction layer
and the corresponding loss function LAtt in Eqn. 13. Then,
we retrain ASNet with SOD data and obtain a baseline:
w/o ﬁxation. From Table 4, we ﬁnd that ﬁxation map is
indeed informative to SOD over all three datasets. The im-
provement is more pronounced on PASCAL-S [34] dataset,
which is collected from PASCAL challenge with more gen-
eral scenes and less center-bias. These results demonstrate
that a strong correlation exists between SOD and FP tasks,
and our ASNet achieves better performance with the guid-
ance from the ﬁxation map. This also demonstrate that the
leverage of large-scale FP data could improve the general-
ization ability of ASNet.

2Here we do not include the results from SU [26], since the authors

have not released code or PR-curve results.

2. What is the effect of convLSTM? Here, we study the
contribution of the convLSTM architecture, which consti-
tutes a building block of our ASNet. To this end, we replace
the convLSTMs with 5 convolution layers, which have 3×3
kernels and inputs/outputs with original dimensions. Thus
we have a baseline: w/o convLSTM. Such network has sim-
ilar architecture with previous bottom-up/top-down deep
learning models [44, 35]. From Table 4, we observe a drop
in F-score and MAE scores over three datasets which im-
plies the effectiveness of the convLSTM.
3. Is the hierarchical architecture meaningful? We also
study the effect of our hierarchical architecture with a stack
of several convLSTMs and top-down saliency inference.
We test 4 baselines: conv5-output, conv4-output, conv3-
output, and conv2-output, which correspond to the outputs
from the intermediate layers of ASNet. Note that the ﬁ-
nal prediction of ASNEt can be viewed as the output from
conv1 layer. We ﬁnd that the saliency results are gradually
optimized by adding more details from lower layers.
4. Are the extra loss functions necessary? ASNet is
equipped with 4 extra loss functions: LP , LR, LF , LM AE,
which are derived from four widely used SOD metrics.
For testing their effects, we retrain ASNet with different
loss functions separately and thus we have four baselines:
w/o LP , w/o LR, w/o LF , w/o LM AE. Another base-
line w/o extra loss indicates the results without consider-
ing all the extra loss functions. We show their F-measure
and MAE scores in Table 5. We observe that those loss
functions would boost the ﬁnal performance with about 1%
improvement in F-score.

5. Conclusions

We proposed a deep learning network, ASNet, towards a
better interpretable and efﬁcient SOD model, which lever-
ages ﬁxation prediction for detecting salient objects. The
ﬁxation map, as high-level knowledge of a scene, was
learned from upper layers of ASNet. Such prior was fur-
ther utilized for teaching the network where the salient ob-
ject is and the detailed object saliency was rendered step by
step by considering ﬁner and ﬁner features in a top-down
manner. ConvLSTM was equipped for iteratively dropping
useless features and enhancing the features for better repre-
sentation. A set of loss functions derived from SOD metrics
were introduced for boosting model predictions. Extensive
experimental results demonstrate that our approach outper-
forms state-of-the-art methods and conﬁrm our view that
ﬁxation map is valuable and indicative for SOD.

This paper points out two potential directions of follow-
ing works. The ﬁrst one regards exploring the rationale
behind SOD from the ﬁxation prediction viewpoint. The
second one is to seek better loss functions for boosting the
performance of deep learning based SOD models.

1718

References

[1] R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk.
In CVPR, 2009.

Frequency-tuned salient region detection.
2, 5

[2] B. Alexe, T. Deselaers, and V. Ferrari. Measuring the object-
ness of image windows. IEEE TPAMI, 34(11):2189–2202,
2012. 1

[3] A. Borji. What is a salient object? A dataset and a baseline
model for salient object detection. IEEE TIP, 24(2):742–756,
2015. 1

[4] A. Borji, M.-M. Cheng, H. Jiang, and J. Li. Salient object de-
tection: A benchmark. IEEE TIP, 24(12):5706–5722, 2015.
1, 2

[5] A. Borji and L. Itti. State-of-the-art in visual attention mod-

eling. IEEE TPAMI, 35(1):185–207, 2013. 2, 6

[6] A. Borji, D. N. Sihite, and L. Itti. What stands out in a
scene? A study of human explicit saliency judgment. Vision
research, 91:62–77, 2013. 1

[7] N. Bruce and J. Tsotsos. Saliency based on information max-

imization. In NIPS, 2006. 2

[8] N. D. Bruce and J. K. Tsotsos. Saliency, attention, and vi-
sual search: An information theoretic approach. Journal of
Vision, 9(3):5–5, 2009. 6

[9] X. Chen, A. Zheng, J. Li, and F. Lu. Look, perceive and seg-
ment: Finding the salient objects in images via two-stream
ﬁxation-semantic cnns. In ICCV, 2017. 3

[10] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S.-M.
IEEE
Hu. Global contrast based salient region detection.
TPAMI, 37(3):569–582, 2015. 1, 2, 6

[11] M. Cornia, L. Baraldi, G. Serra, and R. Cucchiara. Predict-
ing human eye ﬁxations via an lstm-based saliency attentive
model. arXiv preprint arXiv:1611.09571, 2016. 2

[12] D. Gao and N. Vasconcelos. Discriminant saliency for visual

recognition from cluttered scenes. In NIPS, 2005. 2

[13] J. Harel, C. Koch, and P. Perona. Graph-based visual

saliency. In NIPS, 2007. 2, 6

[14] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997. 4

[15] Q. Hou, M.-M. Cheng, X. Hu, A. Borji, Z. Tu, and P. Torr.
Deeply supervised salient object detection with short con-
nections. In CVPR, 2017. 2, 3, 7

[16] X. Hou and L. Zhang. Saliency detection: A spectral residual

approach. In CVPR, 2007. 2

[17] P. Hu, B. Shuai, J. Liu, and G. Wang. Deep level sets for

salient object detection. In CVPR, 2017. 2, 3, 7

[18] X. Huang, C. Shen, X. Boix, and Q. Zhao. SALICON: Re-
ducing the semantic gap in saliency prediction by adapting
deep neural networks. In ICCV, 2015. 2, 3, 6

[19] L. Itti, C. Koch, and E. Niebur. A model of saliency-based
IEEE TPAMI,

visual attention for rapid scene analysis.
20(11):1254–1259, 1998. 2, 6

[20] S. Jetley, N. Murray, and E. Vig. End-to-end saliency map-
ping via probability distribution prediction. In CVPR, 2016.
2, 3

[21] H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li.
Salient object detection: A discriminative regional feature
integration approach. In CVPR, 2013. 2, 7, 8

[22] M. Jiang, S. Huang, J. Duan, and Q. Zhao. SALICON:

Saliency in context. In CVPR, 2015. 6

[23] T. Judd, K. Ehinger, F. Durand, and A. Torralba. Learning to

predict where humans look. In ICCV, 2009. 2, 6

[24] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. In ICLR, 2015. 6

[25] S. S. Kruthiventi, K. Ayush, and R. V. Babu. Deepﬁx: A
fully convolutional neural network for predicting human eye
ﬁxations. IEEE TIP, 2017. 2, 3

[26] S. S. Kruthiventi, V. Gudisa,

J. H. Dholakiya, and
R. Venkatesh Babu. Saliency uniﬁed: A deep architecture
for simultaneous eye ﬁxation prediction and salient object
segmentation. In CVPR, 2016. 2, 3, 6, 7, 8

[27] J. Kuen, Z. Wang, and G. Wang. Recurrent attentional net-

works for saliency detection. In CVPR, 2016. 2

[28] O. Le Meur, P. Le Callet, D. Barba, and D. Thoreau. A coher-
ent computational approach to model bottom-up visual atten-
tion. IEEE TPAMI, 28(5):802–817, 2006. 2

[29] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-

supervised nets. In AISTATS, 2015. 5

[30] G. Lee, Y.-W. Tai, and J. Kim. Deep saliency with encoded
In CVPR,

low level distance map and high level features.
2016. 2, 7

[31] G. Li and Y. Yu. Visual saliency based on multiscale deep

features. In CVPR, 2015. 2, 3, 6, 7, 8

[32] G. Li and Y. Yu. Deep contrast learning for salient object

detection. In CVPR, 2016. 2, 3, 7

[33] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y. Zhuang,
H. Ling, and J. Wang. Deepsaliency: Multi-task deep neu-
IEEE TIP,
ral network model for salient object detection.
25(8):3919 – 3930, 2016. 7

[34] Y. Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille. The
secrets of salient object segmentation. In CVPR, 2014. 1, 2,
6, 7, 8

[35] N. Liu and J. Han. DHSNet: Deep hierarchical saliency net-
work for salient object detection. In CVPR, 2016. 2, 3, 7,
8

[36] N. Liu, J. Han, T. Liu, and X. Li. Learning to predict eye
ﬁxations via multiresolution convolutional neural networks.
IEEE TNNLS, 2016. 2, 3, 6

[37] T. Liu, J. Sun, N.-N. Zheng, X. Tang, and H.-Y. Shum.
Learning to detect a salient object. In CVPR, 2007. 1, 2
[38] Z. Luo, A. Mishra, A. Achkar, J. Eichel, S. Li, and P.-M.
Jodoin. Non-local deep features for salient object detection.
In CVPR, 2017. 2, 3, 7

[39] C. M. Masciocchi, S. Mihalas, D. Parkhurst, and E. Niebur.
Everyone knows what is interesting: Salient locations which
should be ﬁxated. Journal of vision, 9(11):25–25, 2009. 2

[40] S. Mathe and C. Sminchisescu. Dynamic eye movement
datasets and learnt saliency models for visual action recog-
nition. In ECCV. 2012. 2

[41] S. Mathe and C. Sminchisescu. Action from still image
dataset and inverse optimal control to learn task speciﬁc vi-
sual scanpaths. In NIPS, 2013. 2

[42] S. Mathe and C. Sminchisescu. Actions in the eye: Dynamic
gaze datasets and learnt saliency models for visual recogni-
tion. IEEE TPAMI, 37(7):1408–1424, 2015. 2

1719

[64] P. Zhang, D. Wang, H. Lu, H. Wang, and X. Ruan. Amulet:
Aggregating multi-level convolutional features for salient
object detection. In ICCV, 2017. 2, 3, 7

[65] P. Zhang, D. Wang, H. Lu, H. Wang, and B. Yin. Learning
uncertain convolutional features for accurate saliency detec-
tion. In ICCV, 2017. 2, 3, 7

[66] R. Zhao, W. Ouyang, H. Li, and X. Wang. Saliency detection
by multi-context deep learning. In CVPR, 2015. 2, 3
[67] W. Zhu, S. Liang, Y. Wei, and J. Sun. Saliency optimization

from robust background detection. In CVPR, 2014. 2, 7, 8

[43] J. Pan, E. Sayrol, X. Giro-i Nieto, K. McGuinness, and N. E.
O’Connor. Shallow and deep convolutional networks for
saliency prediction. In CVPR, 2016. 2, 3, 6

[44] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Doll´ar. Learn-

ing to reﬁne object segments. In ECCV, 2016. 3, 8

[45] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
3

[46] E. Vig, M. Dorr, and D. Cox. Large-scale optimization of hi-
erarchical features for saliency prediction in natural images.
In CVPR, 2014. 2, 6

[47] L. Wang, H. Lu, X. Ruan, and M.-H. Yang. Deep networks
for saliency detection via local estimation and global search.
In CVPR, 2015. 2, 7

[48] L. Wang, L. Wang, H. Lu, P. Zhang, and X. Ruan. Saliency
detection with recurrent fully convolutional networks.
In
ECCV, 2016. 2, 3, 7

[49] T. Wang, A. Borji, L. Zhang, P. Zhang, and H. Lu. A stage-
wise reﬁnement model for detecting salient objects in im-
ages. In ICCV, 2017. 2, 7

[50] W. Wang and J. Shen. Deep cropping via attention box pre-

diction and aesthetics assessment. In ICCV, 2017. 1

[51] W. Wang and J. Shen. Deep visual attention prediction. IEEE

TIP, 27(5):2368–2378, 2018. 2, 3, 6

[52] W. Wang, J. Shen, and F. Porikli. Saliency-aware geodesic

video object segmentation. In CVPR, 2015. 1

[53] W. Wang, J. Shen, and L. Shao. Consistent video saliency
using local gradient ﬂow optimization and global reﬁnement.
IEEE TIP, 24(11):4185–4196, 2015. 2

[54] W. Wang, J. Shen, and L. Shao. Video salient object detec-
tion via fully convolutional networks. IEEE TIP, 27(1):38–
49, 2018. 2

[55] W. Wang, J. Shen, L. Shao, and F. Porikli. Correspondence
driven saliency transfer. IEEE TIP, 25(11):5025–5034, 2016.
2, 6

[56] W. Wang, J. Shen, R. Yang, and F. Porikli.

Saliency-
aware video object segmentation. IEEE TPAMI, 40(1):20–
33, 2018. 2

[57] W. Wang, J. Shen, Y. Yu, and K.-L. Ma. Stereoscopic thumb-
IEEE
nail creation via efﬁcient stereo saliency detection.
TVCG, 23(8):2014–2027, 2017. 1

[58] Y. Wei, F. Wen, W. Zhu, and J. Sun. Geodesic saliency using

background priors. ECCV, 2012. 2

[59] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong,
and W.-C. Woo. Convolutional LSTM network: A machine
In NIPS,
learning approach for precipitation nowcasting.
2015. 2, 4

[60] Q. Yan, L. Xu, J. Shi, and J. Jia. Hierarchical saliency detec-

tion. In CVPR, 2013. 2, 6, 7, 8

[61] C. Yang, L. Zhang, H. Lu, X. Ruan, and M.-H. Yang.
Saliency detection via graph-based manifold ranking.
In
CVPR, 2013. 6

[62] J. Zhang and S. Sclaroff. Saliency detection: A boolean map

approach. In ICCV, 2013. 6

[63] L. Zhang, M. H. Tong, T. K. Marks, H. Shan, and G. W. Cot-
trell. SUN: A bayesian framework for saliency using natural
statistics. Journal of vision, 8(7):32–32, 2008. 2

1720

Salient Object Detection Driven by Fixation Prediction

Wenguan Wang 1, Jianbing Shen∗ 1,2, Xingping Dong 1, Ali Borji3
Beijing Lab of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, China

1

2

3

Inception Institute of Artiﬁcial Intelligence, Abu Dhabi, UAE

Department of Computer Science, University of Central Florida, USA

wenguanwang.ai@gmail.com, {shenjianbing, dongxingping}@bit.edu.cn, aborji@crcv.ucf.edu

Abstract

Research in visual saliency has been focused on two ma-
jor types of models namely ﬁxation prediction and salient
object detection. The relationship between the two, how-
ever, has been less explored.
In this paper, we propose
to employ the former model type to identify and segment
salient objects in scenes. We build a novel neural network
called Attentive Saliency Network (ASNet)1 that learns to
detect salient objects from ﬁxation maps. The ﬁxation map,
derived at the upper network layers, captures a high-level
understanding of the scene. Salient object detection is then
viewed as ﬁne-grained object-level saliency segmentation
and is progressively optimized with the guidance of the ﬁx-
ation map in a top-down manner. ASNet is based on a hier-
archy of convolutional LSTMs (convLSTMs) that offers an
efﬁcient recurrent mechanism for sequential reﬁnement of
the segmentation map. Several loss functions are introduced
for boosting the performance of the ASNet. Extensive exper-
imental evaluation shows that our proposed ASNet is capa-
ble of generating accurate segmentation maps with the help
of the computed ﬁxation map. Our work offers a deeper in-
sight into the mechanisms of attention and narrows the gap
between salient object detection and ﬁxation prediction.

1. Introduction

Salient object detection (SOD) has been studied exten-
sively for more than a decade (since [37]). It has several
applications such as segmentation [52, 10], object proposal
generation [2], and image resizing [57, 50]. Recently, the
use of deep neural networks for saliency detection has been
trending. Although promising results have been achieved,
they occasionally fail to detect the most salient object in

∗Corresponding author: Jianbing Shen. This work was supported in
part by the Beijing Natural Science Foundation under Grant 4182056, the
Fok Ying Tung Education Foundation under Grant 141067, and the Spe-
cialized Fund for Joint Building Program of Beijing Municipal Education
Commission.

1Available at: https://github.com/wenguanwang/ASNet.

(a)

(b)
Figure 1. Given complex scenes like (a), what are the salient
objects? We propose the Attentive Saliency Network (ASNet)
that infers the object saliency (b) from predicted ﬁxation maps (c),
which is consistent with human attention mechanisms.

(c)

complex cluttered scenes containing several objects (such
as the ones shown in Fig. 1 (a)). Additionally, for current
computational saliency models, their connection with how
humans explicitly choose salient objects or watch natural
scenes are less clear (as discussed in [3, 6]).

In this paper, we take a further step towards a more bi-
ologically plausible SOD model equipped with high-level
prior of ﬁxation map. The human attention prior, rep-
resented by eye movements or from a ﬁxation prediction
model, is instinctive and more consistent with visual pro-
cessing of human visual system. The suggested model not
only generates high-quality object saliency maps, but also
pushes the boundary of SOD research by building a close
connection to human ﬁxation prediction (FP). As shown in
Fig. 1, our model infers object saliency using the ﬁxation
prior, where this prior acts as a selective mechanism to en-
hance the saliency representation for the purpose of accu-
rate object saliency inference. Our algorithm has bias on
the assumption that aligns with the core views of previous
studies [34, 4] that explored the relationship between eye
movements (implicit saliency) and explicit object saliency.
These studies conﬁrmed a strong correlation between ﬁx-
ations and salient objects. Further beyond treating FP and
SOD as two separate tasks that are learned in a branched

11711

network (as in [26]), these two tasks are more deeply inter-
connected in our model. The ﬁxation map provides a high-
level signal, which is learned from upper layers of our neu-
ral network. It is then used for salient object detection in a
top-down manner. This process is straightforward and sim-
ilar to how humans process a scene sequentially (i.e., ﬁrst
paying attention to important areas of a scene quickly, and
then taking more efforts for precise segmentation). Further,
leveraging the rich information from exiting large-scale eye
movement datasets can improve the robustness and gener-
alization ability of SOD models.

The proposed Attentive Saliency Network (ASNet) is
based on convolutional LSTM (convLSTM) [59], which has
convolutional structures in both the input-to-state and state-
to-state transitions. Further, beyond the fully connected
LSTM models, convLSTM encodes the spatial information
via convolution operations to facilitate pixel-wise saliency
labeling. The key advantage of LSTM is iterative removal
of irrelevant information and learning powerful represen-
tations through updating the memory cell. This allows the
ASNet to progressively optimize features for better saliency
estimation in a feed-forward strategy. By stacking multiple
convLSTMs, the ASNet is trainable for gradually rendering
object saliency from ﬁxation map in a top-down manner.

Our contributions are manifold:

• We aim to infer salient objects (captured in lower net-
work layers) from the ﬁxation map (encoded in higher lay-
ers) within a uniﬁed neural network. This goes one step
beyond previous deep learning based saliency models and
offers a deep insight into the conﬂuence between ﬁxation
prediction and salient object detection.
• We present the Attentive Saliency Network (ASNet)
which is a hierarchy of convLSTMs for step-wise inference
of object saliency. ConvLSTM has the advantage of the
improved ﬂow of information with recurrent connections,
which results in more powerful saliency representation.
• We introduce novel loss functions for SOD, derived from
exiting SOD evaluation metrics, for capturing several qual-
ity factors. As we will show empirically, these new loss
functions lead to higher performance.

2. Related Work

In this section, we ﬁrst brieﬂy review the ﬁxation pre-
diction (§ 2.1) and salient object detection literature (§ 2.2).
Then, in § 2.3, we discuss studies exploring the relationship
between the above two tasks.

2.1. Fixation Prediction (FP)

Fixation Prediction (FP) aims to identify the ﬁxation
points that human viewers would focus on at ﬁrst glance.
It has a long history from [19] and is still active in vision
research. From the view of mechanism to obtain atten-
tion, early attention models can be classiﬁed into different

schools [5], such as cognitive model [19, 28, 40, 42, 41],
Bayesian model [63], decision theoretic model [12], infor-
mation theoretic model [7], graphical model [13], spectral
analysis model [16], pattern classiﬁcation model [23], etc.
We refer the readers to [5] for more detailed overview.

More recently, many deep learning based attention
models have been proposed. The eDN model [46] repre-
sented an early architecture that automatically learns deep
representations for FP. After that, DeepFix [25], SALICON
net [18], Mr-CNN [36], Shallow and Deep [43], attentive
LSTM [11], DVA net [51], were successively proposed,
with the use of deeper networks and more complex architec-
tures. Jetley et al. [20] tested several loss functions based on
probability distance measures and found Bhattacharyya dis-
tance could give the best performance. Those deep learning
solutions generally achieved better performance, compared
with traditional non-deep learning techniques.

2.2. Salient Object Detection (SOD)

Salient object detection (SOD) aims at highlighting
salient object regions in images. Different from FP that is
originated from cognition and psychology research commu-
nities, SOD is more a computer vision task that is driven by
object-level applications [56]. The history of SOD is rela-
tively more recent and dates back to the works of Liu et al.
[37] and Achanta et al. [1]. Due to lack of the knowledge
of image content, most non-deep learning SOD models
[10, 58, 60, 21, 67, 53, 55] are based on low-level features
and certain heuristic hypothesizes (such as contrast, back-
ground prior). Please see [4] for detailed overviews.

In more recent work, deep learning based SOD mod-
els have made substantial improvement. These methods
mainly utilize multi-scale and multi-level representations
[31, 64, 38], integrate both local estimation and global
search [47], explore global and local context information
[66], combine pixel- and segment-level features [32], de-
velop level set [17], or consider short connections with skip-
layer structures [15], based on neural network. Some other
methods try to integrate deep learning models with hand-
crafted features [30], study saliency prior [48], or exploit
various deep learning architectures [27, 35, 65, 49, 54].

2.3. The Relationship between FP and SOD

Although SOD has been extensively studied in computer
vision research, only few studies (e.g., [39, 4, 34]) have ex-
plored how humans explicitly choose salient objects. They
have quantitatively conﬁrmed that object saliency judg-
ments agree with human ﬁxations. According to the analy-
sis in [4], there exists a strong correlation between explicit
saliency judgments and free-viewing ﬁxations, which can
be viewed as two proxies of visual attention. Li et al. [34]
have demonstrated that, unlike FP datasets, there exists a
heavy bias in many widely used SOD datasets. Most of the

1712

Figure 2. Typical network architectures used in previous FP or SOD models and our ASNet. (a) Single-stream network, (b) Multi-
stream network, and (c) Skip-layer network. (d) Branched network adopted in [26], where FP and SOD are achieved via two branches
sharing several bottom layers. (e) The adopted ASNet captures ﬁxation map from upper layer, which is indicative of the inference of object
saliency from lower layers. Stack of convLSTMs are adopted for iteratively optimizing features, while preserving spatial information.

SOD datasets have only a few obvious objects in the scene.
Next, we discuss several representative deep models in
SOD or FP from the view of network architecture. This
would better situate our work with respect to previous works
and help to highlight our contributions. As shown in Fig. 2,
most deep learning models for FP or SOD only consider a
single task. Typical architectures include: (a) single-stream
network [25, 20, 43, 65, 17] (standard architecture), (b)
multi-stream network [18, 66, 31, 36] (training with multi-
scale inputs), and (c) skip-layer network [15, 32, 64, 38]
(concatenate multi-layer responses for ﬁnal output). As
seen, previous deep learning based works often treated FP
and SOD as two unrelated tasks.

Instead of performing FP or SOD separately, we ex-
ploit the correlation between ﬁxations and salient objects
via tightly coupling these two tasks in a uniﬁed deep learn-
ing architecture. There are only few methods consider FP
and SOD tasks together. In [9], ﬁxation map from a pre-
trained FP model is used as an extra cue for guiding SOD,
while it didn’t emphasize learning both FP and SOD simul-
taneously. In [26], FP and SOD are achieved via two sep-
arate network branches, which only share weights in sev-
eral lower layers (Fig. 2 (d)). In our method, as illustrated
in Fig. 2 (e), ﬁxation map (high-level knowledge captured
in top layers) is used for guiding accurate object saliency
detection in lower layers. Thus our approach goes beyond
above work by learning FP and SOD within a uniﬁed net-
work and in a top-down end-to-end manner.

3. Our Approach

Given an input image, the goal is to produce a pixel-wise
saliency map to highlight salient object regions. As demon-
strated in Fig. 3, the proposed ASNet ﬁrst captures a global
and high-level understanding of a scene in its higher layers,
by learning to predict human ﬁxations (§3.1). Then, it uses
a stack of convLSTMs to progressively infer object saliency
from the ﬁxation map in a top-down and coarse-to-ﬁne man-
ner (§3.2). The whole network is simultaneously trained to
predict ﬁxation locations and to detect salient objects in an
end-to-end way (§3.3).

3.1. Fixation Predicting

At the bottom of ASNet resides a stack of convolutional
layers where the lower layers respond to primitive image
features such as edges, corners and shared common pat-
terns, and the higher layers extract semantic information
like objects or faces. The ASNet learns the FP as a high-
level task towards modeling human ﬁxation locations with
the utilization of features from higher layers, and achieves
the SOD by optimizing the ﬁxation prior with the features
from the lower layers.

The lower convolutional layers are borrowed from the
ﬁrst ﬁve convolutional blocks of VGGNet [45] (13 convolu-
tional layers in total). We omit the last pooling layer (pool5)
for preserving more spatial information. For a training im-
age, with a typical resolution of 224 × 224 × 3, we com-
pute a convolutional layer by applying a 3×3 kernel with
sigmoid activation function, to the last convolutional fea-
ture map (14 × 14 × 512). The result is a probability map
P ∈ [0, 1]14×14 which is used as a ﬁxation prior from global
and high-level image context. The model for the task of
FP is trained via minimizing the following Kullback-Leibler
Divergence (KL-Div) loss function:

LAtt(G, P ) =

1

14×14

14 × 14 X

x

gx log(

),

(1)

gx
px

where G denotes the resized ground-truth attention map
G ∈ [0, 1]14×14 and gx ∈ G, px ∈ P . The gray-scale ﬁx-
ation map is obtained via ﬁltering the binary ﬁxation map
using a Gaussian ﬁlter with small variance. The KL-Div
measure, the minimization of which is equivalent to cross-
entropy minimization, is widely used in visual saliency pre-
diction [18, 51]. In the next section, we will leverage such
ﬁxation map as the prior for producing object saliency.

3.2. Detecting Object Saliency with Fixation Prior

The ﬁxation map P gives a coarse but informative prior
regarding visually salient regions. A number of previous
studies for pixel-labeling tasks such as semantic segmenta-
tion [44], and salient object detection [35, 48], have shown
that neural networks are capable of producing ﬁne-gained

1713

gates i, f, c. At each step t, as a new input Xt arrives, its in-
formation will be accumulated to the cell if the input gate it
is activated. Also, the past cell status Ct−1 could be “forgot-
ten” in this process if the forget gate ft is on. Whether the
latest cell status Ct should be propagated to the ﬁnal state Ht
is further controlled by the output gate ot. Formally, above
memory update process at step t is driven by the following
equations:

i ∗ Xt + W H
f ∗ Xt + W H
o ∗ Xt + W H

it = σ(W X
ft = σ(W X
ot = σ(W X
Ct = ft ◦ Ct−1 +it ◦tanh(W X
Ht = ot ◦ tanh(Ct),

i ∗ Ht−1 + bi),
f ∗ Ht−1 + bf ),
o ∗ Ht−1 + bo),
c ∗Xt +W H

(2)

(3)

(4)

c ∗Ht−1 +bc), (5)
(6)

where ‘∗’ denotes the convolution operator and ‘◦’ repre-
sents element-wise product. σ and tanh are the activa-
tion functions of logistic sigmoid and hyperbolic tangent.
The inputs Xt, cell memory Ct, hidden states Ht and gates
it, ft, ct are 3D tensors whose spatial dimensions are the
same. W s and bs are the learned weights and biases.

In our case, the convLSTM takes the features X ex-
tracted from the convolutional neural network (from the last
convolutional layers prior to pooling layers) as input, and
produces reﬁned saliency features for ﬁnal saliency estima-
tion. Since it operates on static images, the input features in
all steps are the same: X1 = . . . = Xt = X (see Fig. 4).
Here, we take the advantages of recurrent natures of LSTM
for iteratively optimizing the saliency features of static im-
ages, instead of using LSTM for modeling the temporal de-
pendency of sequential data.

With the learned ﬁxation prior P ∈ [0, 1]14×14, we ﬁrst
combine P with the convolutional features from conv5-3
and feed them into a convLSTM. In each time step, the con-
vLSTM is trained for inferring the salient object with the
knowledge of ﬁxation information, and sequentially opti-
mizes the features with the updated memory cell and hidden
states (see Fig. 4 (b)). Thus, the features are reorganized
towards better representation of object saliency. More spe-
cially, we ﬁrst compress the feature responses from conv5-3
layer via a convolutional layer with 64 ﬁlters to lower com-
putational costs and adopt sigmoid activation for regulariz-
ing the response from features to lie within the same range
([0, 1]) of P . Then, the attention prior map P is concate-
nated with the compressed features and fed into the con-
vLSTM. We apply a 1×1 convolution kernel to the ﬁnal
convLSTM output H for obtaining an object saliency map
Q ∈ [0, 1]14×14.

Several different metrics have been proposed for evaluat-
ing saliency models and no single metric can fully summa-
rize the performance of a model. This motivates us to com-
bine the classical weighted cross-entropy loss function with

1714

Figure 3. Architecture of the proposed ASNet. The ﬁxation map
is learned from the upper layers and is used by the ASNet to locate
the salient objects. Then, the ﬁne-grained object saliency is grad-
ually inferred from lower layers and is successively optimized via
the recurrent architecture of convLSTM. Zoom-in for details.

labeling results via incorporating high-level information en-
coded in upper network layers. Here, we desire our model
to be able to infer precise object saliency from the ﬁxation
map predicted in the upper network layers.

The network is trained for detecting and successively re-
ﬁning the salient object via aggregating information from
high-level ﬁxation map and the spatially rich information
from low-level network features. As shown in Fig. 3, the
SOD is computed in a top-down fashion, successively in-
tegrating information from earlier layers. Multiple convL-
STM networks [59] (the yellow blocks in Fig. 3) are stacked
for building more meaningful feature representations with
recurrent connections. We leverage the sequential nature
of LSTM to process features in an iterative way. For a
certain layer, convLSTM discards less informative features
while enhances informative features, thus generating grad-
ually improved saliency maps.

ConvLSTM extends traditional fully connected LSTM
[14] to consume spatial features. Basically, this is achieved
by substituting dot products with convolutional operations
in the LSTM equations. ConvLSTM has convolutional
structures in both the input-to-state and state-to-state transi-
tions, which can preserve the spatial information of convo-
lutional feature map, thus enabling our network to produce
a pixel-wise labeling.

A schematic diagram of convLSTM is presented in Fig.
4. Similar to traditional gated LSTMs, the convLSTM uses
the memory cells and gates to control information ﬂow.
It works by sequentially updating an internal state H and
memory cell C, according to the values of three sigmoid

(a)

(b)

Figure 4. Illustration of our convLSTM based object saliency optimization, where (b) shows detailed architecture of our convLSTM
optimization module in (a). Zoom-in for details.

precision, recall, F-measure, and MAE metrics for more ef-
ﬁcient training. Given the ground-truth salient object an-
notation S (here S ∈ {0, 1}14×14 for conv5-3 layer), the
overall loss function is deﬁned as:

LSal(S, Q) = LC(S, Q)+α1LP (S, Q)+α2LR(S, Q)

+α3LF (S, Q)+α4LM AE(S, Q),

where αs are balance parameters and are empirically set as
α1 = α2 = α3 = α4 = 0.1. LC is the weighted cross-
entropy loss function, which is widely adopted for training
SOD models and opted as the primary loss in our case:

After obtaining the object saliency map Q ∈ [0, 1]14×14
inferred from the ﬁxation map P , we upsample (×2) Q
and feed it to the next convLSTM with the compressed
features (28×28×64) from conv4-3 layer for more de-
tailed reﬁnement. Above process is iteratively applied for
conv4-3, conv3-3, conv2-2 and conv1-2 layers, respectively.
Finally, the ASNet outputs a high-quality object saliency
mask (224×224×1). In sum, the ASNet is able to effec-
tively infer the object saliency thanks to 1) a learnable ﬁxa-
tion prior, 2) iteratively updating saliency features with re-
current architecture and 3) efﬁciently merging spatially rich
information from lower layers in a top-down manner.

LC(S, Q) =

γ · (1 − sx) · log(1 − qx)

3.3. Implementation Details

(7)

(8)

1
N Xx(cid:0)

+ (1 − γ) · sx · log qx

,

(cid:1)

where N is the total number of pixels and sk ∈ S, qk ∈
Q. γ refers to the ratio of salient pixels in ground truth S.
Weighted cross-entropy loss handles the imbalance between
number of salient and non-salient pixels.

LP , LR and LF are computed similar to precision, recall

and F-measure scores:

LP (S, Q) = −

Xx

sx · qx.(

Xx

qk + ǫ),

LR(S, Q) = −

LF (S, Q) = −

sk + ǫ),

sx · qx.(

Xx
Xx
(1 + β2) · LP (S, Q) · LR(S, Q)
β2 · LP (S, Q) + LR(S, Q) + ǫ

.

(9)

(10)

(11)

where β2 = 0.3 as suggested by [1], and ǫ is a regular-
ization constant. Since precision, recall and F-measure are
similarity metrics and higher values are better, negative val-
ues are used for minimizing.

LM AE is derived from the mean absolute error (MAE)
measure that computes the discrepancy between the
saliency map Q and the ground-truth map S:

LM AE(S, Q) =

|sx − qx|.

(12)

1
N Xx

Overall loss: Let I = {Ik, k = 1, . . . , K} denote all the
training images (resized into 224 × 224) borrowed from ex-
iting SOD or FP datasets. Since there are only few datasets
that offer annotations for both SOD and FP tasks, most of
the training images are either labeled with human ﬁxation
annotation or object saliency mask. Let yA
k ∈ {0, 1} and
yS
k ∈ {0, 1} indicate whether we have the attention anno-
tation Gk and object saliency mask Sk for the k-th training
image. Our ﬁnal loss function can be expressed as:

L =

yA
k ·LAtt(Gk, Pk)+

yS
k ·

LSal(Sℓ

k, Qℓ

k), (13)

K

X
k=1

K

X
k=1

5

X
ℓ=1

k and yS

where the loss functions LAtt and LSal are deﬁned in Eqn.
1 and Eqn. 7, respectively. The indicators yA
k are
employed to remedy missing ground truth in correspond-
ing tasks. That is the error is not propagated back when
the annotations are not offered. The ℓ ∈ {1, . . . , 5} refers
to the ℓ-th convLSTM with conv-ℓ block in ASNet. With
the hierarchical loss functions, each layer in ASNet has di-
rect access to the gradients from the loss function leading
to an implicit deep supervision [29]. We set the time steps
to three in our convLSTM and employ 3 × 3 kernels for
convolution operations.

1715

Dataset

#Images

Resolution

Annotation
FP SOD

Training

Testing

640 × 480

SALICON [22] 15,000 X
THUS10K [10] 10,000

PASCAL-S[34]

X max(w, h) = 400
DUT-OMRON [61] 5,168 X X max(w, h) = 400
X X max(w, h) = 500
max(w, h) = 1024
X max(w, h) = 400
X max(w, h) = 400

ECCSD[60] 1,000
HKU-IS[31] 4,447

MIT1003[23] 1,004 X

850

Table 1. Datasets used for training and testing the ASNet.

Training datasets: Another advantage of ASNet is that it
can use data from both SOD and FP benchmarks. We con-
sider three large-scale saliency datasets: SALICON [22],
THUS10K [10], and DUT-OMRON [61]. The SALICON
dataset is widely used in the domain of FP, while the
THUS10K dataset is commonly used for SOD. These two
datasets have annotations for ﬁxations and salient objects,
respectively. We further utilize the DUT-OMRON dataset
which offers annotations for both FP and SOD. Detailed de-
scriptions of employed datasets can be found in Table 1.
Training settings: In each training iteration, we use a min-
batch of 10 images, which are sampled from above three
datasets and ensure data balance between SOD and FP. Data
augmentation techniques (e.g., ﬂipping, rotation) are also
adopted. Our model is implemented in Python on Keras,
and trained with the Adam optimizer [24]. During the train-
ing phase, the learning rate is set to 0.0001 and is decreased
by a factor of 10 every two epochs. The networks were
trained for 12 epochs with early stopping strategy.
Computation load: ASNet is simultaneously trained for
FP and SOD tasks in an end-to-end manner. The entire
training procedure takes about 10 hours with a NVIDIA TI-
TAN X GPU and a 4.0GHz Intel processor. It takes about
0.08s to process an image of 224 × 224 size, which is faster
than most deep learning based competitors.

4. Experimental results

In this section, we ﬁrst examine the performance of AS-
Net for the FP task. The goal of this experiment is to in-
vestigate the effectiveness of the learned ﬁxation map prior,
instead of comparing it with the state-of-the-art FP models.
Then we evaluate the performance of the ASNet for the pri-
mary SOD task. Finally, an ablation study is performed to
gain a deeper insight into the proposed ASNet.
Testing datasets: Four datasets including PASCAL-S [34],
MIT1003 [23], ECCSD [60] and HKU-IS [31] are used for
testing our model. PASCAL-S offers both annotations for
FP and SOD, MIT1003 is a representative benchmark for
FP, and ECCSD and HKU-IS are two typical datasets which
are widely used for SOD. We report the evaluation results
for both tasks over the PASCAL-S dataset. More details of
above datasets can be found in Table 1.
Evaluation metrics: For the FP task, there are several ways

Methods AUC-Judd ↑ SIM ↑ shufﬂed AUC ↑ CC ↑ NSS ↑
1.36
1.86
1.60
1.73
2.08
1.29
1.25
0.82
1.38
1.10
2.01
2.30

Mr-CNN [36]
SALICON [18]
Shallow-Net [43]
Deep-Net [43]
SU [26]
eDN [46]
BMS [62]
AIM [8]
GBVS [13]
ITTI [19]
ASNet-14×14
ASNet-28×28

0.38
0.53
-
0.51
-
0.41
0.36
0.26
0.42
0.33
0.60
0.65

0.35
0.42
-
0.40
-
0.30
0.33
0.27
0.36
0.32
0.49
0.52

0.80
0.85
-
0.86
-
0.85
0.79
0.79
0.83
0.77
0.87
0.88

0.73
0.74
0.68
0.73
0.73
0.66
0.69
0.68
0.66
0.66
0.73
0.75

- The authors in [26, 43] have not released detailed results.

Table 2. Quantitative comparison of different FP models on the
MIT1003 [23] dataset.

Methods AUC-Judd ↑ SIM ↑ shufﬂed AUC ↑ CC ↑ NSS ↑
1.35
1.88
1.90
1.74
2.22
1.42
1.28
0.97
1.36
1.30
2.26
2.43

Mr-CNN [36]
SALICON [18]
Shallow-Net [43]
Deep-Net [43]
SU [26]
eDN [46]
BMS [62]
AIM [8]
GBVS [13]
ITTI [19]
ASNet-14×14
ASNet-28×28

0.40
0.58
-
0.55
-
-
-
0.32
0.45
0.42
0.70
0.73

0.34
0.46
-
0.42
-
-
-
0.30
0.36
0.36
0.55
0.59

0.79
0.86
-
0.87
-
-
-
0.77
0.84
0.82
0.90
0.90

0.71
0.72
0.69
0.71
0.73
0.65
0.67
0.65
0.65
0.64
0.74
0.74

- The authors in [43, 26, 46, 62] have not released detailed results.
Table 3. Quantitative comparison of different FP models on the
PASCAL-S [23] dataset.

to measure the agreement between model predictions and
human eye movements. Here, we employ ﬁve typical met-
rics, namely Normalized Scanpath Saliency (NSS), Simi-
larity Metric (SIM), Linear Correlation Coefﬁcient (CC),
AUC-Judd, and shufﬂed AUC. Please refer to [5, 51] for
detailed descriptions of these metrics. For the SOD task,
three standard metrics, namely PR-curve, F-measure, and
MAE, are employed for evaluation. See [55] for details.

4.1. Performance of ASNet

Performance on FP task: We evaluated the ﬁxation prior
map generated by ASNet compared to 10 state-of-the-art
ﬁxation models, including 4 classical models: ITTI [19],
GBVS [13], AIM [8], BMS [62], and 6 deep learning based
models: eDN [46], SALICON [18], SU [26], Mr-CNN [36],
Shallow-Net [43] and Deep-Net[43]. Results are reported
over PASCAL-S [34] and MIT1003 [23] datasets.

Our ASNet is able to generate a ﬁxation prediction map
P from top layer, which is relatively rough, and much
smaller (only 14×14) compared to exiting ﬁxation models.
For the sake of a fairer comparison and deeper insight into
the advantage of our ASNet, we further generate a larger
ﬁxation map (28×28) via feeding P into an additional con-

1716

(a) ECCSD

(b) HKU-IS

(c) PASCAL-S

Figure 5. SOD results with PR-curve on three widely used benchmarks: ECCSD [60], HKU-IS [31] and PASCAL-S [34], where the
scores from non-deep learning models are indicated by dashed lines. Best viewed in color.

Figure 6. Qualitative results of ASNet. From top to bottom: example images, ﬁxation maps, and object saliency results.

vLSTM with the features from conv4-3 layer. We therefore
derive two baselines: ASNet-14×14 and ASNet-28×28,
corresponding to the rough prediction and the reﬁned at-
tention map with a more detailed spatial information.

As shown in Table 2 and Table 3, ASNet-14×14 per-
forms better than previous non-deep learning models and
is competitive with current top-performing deep learning
contenders. Considering our relatively simple network ar-
chitecture, and smaller output resolution (14×14), the sug-
gested ASNet is much favorable and effective. We at-
tribute this primarily to the extra generality and powerful
saliency representations learned from the SOD task. Ad-
ditionally, ASNet-28×28 produces further better results,
which demonstrates the proposed ASNet has potential of
obtaining better FP results with considering more detailed
spatial information.
Performance on SOD task: Here we evaluate the per-
formance of ASNet on its primary task: SOD. We per-
form quantitative study on 3 widely used datasets, namely
ECCSD [60], HKU-IS [31] and PASCAL-S [34]. We com-
pare ASNet against 14 recent deep learning based alterna-
tives: LEGS [47], MDF [31], DS [33], SU [26], DCL [32],
ELD [30], RFCN [48], DHS [35], HEDS [15], NLDF [38],
DLS [17], AMU [64], UCF [65], and SRM [49]. We also

SU [26]

Methods

ECCSD [60]

HS [60] 0.730
DRFI [21] 0.787
wCtr [67] 0.672
MDF [31] 0.831
LEGS [47] 0.831
DS [33] 0.810
DCL [32] 0.898
ELD [30] 0.865
0.88
RFCN [48] 0.898
DHS [35] 0.905
HEDS [15] 0.915
NLDF [38] 0.905
DLS [17] 0.825
AMU [64] 0.889
UCF [65] 0.868
SRM [49] 0.910
ASNet 0.928

HKU-IS [31] PASCAL-S [34]
F-score↑ MAE↓ F-score↑ MAE↓ F-score↑ MAE↓
0.259
0.196
0.193
0.145
0.155
0.170
0.108
0.121
0.10
0.118
0.091
0.080
0.099
0.136
0.098
0.116
0.127
0.072

0.223
0.166
0.178
0.108
0.119
0.160
0.071
0.080
0.06
0.097
0.061
0.052
0.063
0.090
0.058
0.068
0.056
0.043
- The authors in [26] have not released detailed results.
∗ MDF [31] is trained on a subset of HKU-IS, and evaluated on the
remaining images.

0.636
0.215
0.710
0.692
0.143
0.783
0.611
0.138
0.694
0.860* 0.129* 0.764
0.749
0.101
0.812
0.818
0.078
0.848
0.822
0.048
0.907
0.767
0.071
0.844
0.77
-
-
0.827
0.079
0.895
0.820
0.052
0.892
0.830
0.039
0.913
0.831
0.048
0.902
0.719
0.072
0.806
0.834
0.052
0.918
0.771
0.062
0.905
0.783
0.046
0.892
0.857
0.035
0.920

Table 4. The F-measure and MAE scores of SOD on three pop-
ular datasets. ASNet gains the best performance with the assis-
tance of visual attention prior. See text for details.

1717

Aspects

Methods

ECCSD [60]

HKU-IS [31] PASCAL-S [34]
F-score↑ MAE↓ F-score↑ MAE↓ F-score↑ MAE↓

0.928

0.043

0.920

0.035

0.857

0.072

variants

archi-
tecture

loss

ASNet
(conv1-output)

w/o ﬁxation 0.913
w/o convLSTM 0.891
conv5-output 0.853
conv4-output 0.875
conv3-output 0.903
conv2-output 0.919
w/o LP
0.923
w/o LR 0.924
0.924
w/o LF
w/o LM AE 0.921
w/o extra loss 0.917

0.051
0.068
0.093
0.076
0.061
0.049
0.045
0.046
0.047
0.044
0.048

0.915
0.887
0.830
0.844
0.892
0.912
0.917
0.915
0.916
0.914
0.912

0.040
0.069
0.079
0.058
0.049
0.040
0.038
0.039
0.040
0.037
0.040

0.831
0.797
0.739
0.749
0.794
0.847
0.852
0.854
0.854
0.850
0.847

0.083
0.112
0.117
0.092
0.086
0.078
0.075
0.074
0.074
0.072
0.075

Table 5. Ablation study of ASNet. We change one component at
a time, to assess individual contributions. See § 4.2 for details.

consider 3 classical non-deep learning models: HS [60],
DRFI [21] and wCtr [67]. The results are obtained from
the authors or by running their public implementations with
original settings. The precision-recall curves of all methods
are plotted in Fig. 5. As can be seen, the ASNet outper-
forms other competitors2. We report maximum F-measure
and MAE scores in Table 4. Overall, the proposed method
achieves better performance over three datasets using all
evaluation metrics. Qualitative results over example images
from above datasets are depicted in Fig. 6; showing that
the proposed ASNet is well applicable to various complex
scenes.

4.2. Validation of the Proposed Algorithm

We now conduct a more detailed examination of our pro-
posed approach. We assess 1) contribution of the ﬁxation
prior for the SOD task, 2) the effects of convLSTM archi-
tecture, 3) the inﬂuence of stacked convLSTMs structure,
and 4) the importance of the introduced loss functions.
1. Does ﬁxation prior contribute to SOD? To answer this
question, we directly remove the ﬁxation prediction layer
and the corresponding loss function LAtt in Eqn. 13. Then,
we retrain ASNet with SOD data and obtain a baseline:
w/o ﬁxation. From Table 4, we ﬁnd that ﬁxation map is
indeed informative to SOD over all three datasets. The im-
provement is more pronounced on PASCAL-S [34] dataset,
which is collected from PASCAL challenge with more gen-
eral scenes and less center-bias. These results demonstrate
that a strong correlation exists between SOD and FP tasks,
and our ASNet achieves better performance with the guid-
ance from the ﬁxation map. This also demonstrate that the
leverage of large-scale FP data could improve the general-
ization ability of ASNet.

2Here we do not include the results from SU [26], since the authors

have not released code or PR-curve results.

2. What is the effect of convLSTM? Here, we study the
contribution of the convLSTM architecture, which consti-
tutes a building block of our ASNet. To this end, we replace
the convLSTMs with 5 convolution layers, which have 3×3
kernels and inputs/outputs with original dimensions. Thus
we have a baseline: w/o convLSTM. Such network has sim-
ilar architecture with previous bottom-up/top-down deep
learning models [44, 35]. From Table 4, we observe a drop
in F-score and MAE scores over three datasets which im-
plies the effectiveness of the convLSTM.
3. Is the hierarchical architecture meaningful? We also
study the effect of our hierarchical architecture with a stack
of several convLSTMs and top-down saliency inference.
We test 4 baselines: conv5-output, conv4-output, conv3-
output, and conv2-output, which correspond to the outputs
from the intermediate layers of ASNet. Note that the ﬁ-
nal prediction of ASNEt can be viewed as the output from
conv1 layer. We ﬁnd that the saliency results are gradually
optimized by adding more details from lower layers.
4. Are the extra loss functions necessary? ASNet is
equipped with 4 extra loss functions: LP , LR, LF , LM AE,
which are derived from four widely used SOD metrics.
For testing their effects, we retrain ASNet with different
loss functions separately and thus we have four baselines:
w/o LP , w/o LR, w/o LF , w/o LM AE. Another base-
line w/o extra loss indicates the results without consider-
ing all the extra loss functions. We show their F-measure
and MAE scores in Table 5. We observe that those loss
functions would boost the ﬁnal performance with about 1%
improvement in F-score.

5. Conclusions

We proposed a deep learning network, ASNet, towards a
better interpretable and efﬁcient SOD model, which lever-
ages ﬁxation prediction for detecting salient objects. The
ﬁxation map, as high-level knowledge of a scene, was
learned from upper layers of ASNet. Such prior was fur-
ther utilized for teaching the network where the salient ob-
ject is and the detailed object saliency was rendered step by
step by considering ﬁner and ﬁner features in a top-down
manner. ConvLSTM was equipped for iteratively dropping
useless features and enhancing the features for better repre-
sentation. A set of loss functions derived from SOD metrics
were introduced for boosting model predictions. Extensive
experimental results demonstrate that our approach outper-
forms state-of-the-art methods and conﬁrm our view that
ﬁxation map is valuable and indicative for SOD.

This paper points out two potential directions of follow-
ing works. The ﬁrst one regards exploring the rationale
behind SOD from the ﬁxation prediction viewpoint. The
second one is to seek better loss functions for boosting the
performance of deep learning based SOD models.

1718

References

[1] R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk.
In CVPR, 2009.

Frequency-tuned salient region detection.
2, 5

[2] B. Alexe, T. Deselaers, and V. Ferrari. Measuring the object-
ness of image windows. IEEE TPAMI, 34(11):2189–2202,
2012. 1

[3] A. Borji. What is a salient object? A dataset and a baseline
model for salient object detection. IEEE TIP, 24(2):742–756,
2015. 1

[4] A. Borji, M.-M. Cheng, H. Jiang, and J. Li. Salient object de-
tection: A benchmark. IEEE TIP, 24(12):5706–5722, 2015.
1, 2

[5] A. Borji and L. Itti. State-of-the-art in visual attention mod-

eling. IEEE TPAMI, 35(1):185–207, 2013. 2, 6

[6] A. Borji, D. N. Sihite, and L. Itti. What stands out in a
scene? A study of human explicit saliency judgment. Vision
research, 91:62–77, 2013. 1

[7] N. Bruce and J. Tsotsos. Saliency based on information max-

imization. In NIPS, 2006. 2

[8] N. D. Bruce and J. K. Tsotsos. Saliency, attention, and vi-
sual search: An information theoretic approach. Journal of
Vision, 9(3):5–5, 2009. 6

[9] X. Chen, A. Zheng, J. Li, and F. Lu. Look, perceive and seg-
ment: Finding the salient objects in images via two-stream
ﬁxation-semantic cnns. In ICCV, 2017. 3

[10] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S.-M.
IEEE
Hu. Global contrast based salient region detection.
TPAMI, 37(3):569–582, 2015. 1, 2, 6

[11] M. Cornia, L. Baraldi, G. Serra, and R. Cucchiara. Predict-
ing human eye ﬁxations via an lstm-based saliency attentive
model. arXiv preprint arXiv:1611.09571, 2016. 2

[12] D. Gao and N. Vasconcelos. Discriminant saliency for visual

recognition from cluttered scenes. In NIPS, 2005. 2

[13] J. Harel, C. Koch, and P. Perona. Graph-based visual

saliency. In NIPS, 2007. 2, 6

[14] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997. 4

[15] Q. Hou, M.-M. Cheng, X. Hu, A. Borji, Z. Tu, and P. Torr.
Deeply supervised salient object detection with short con-
nections. In CVPR, 2017. 2, 3, 7

[16] X. Hou and L. Zhang. Saliency detection: A spectral residual

approach. In CVPR, 2007. 2

[17] P. Hu, B. Shuai, J. Liu, and G. Wang. Deep level sets for

salient object detection. In CVPR, 2017. 2, 3, 7

[18] X. Huang, C. Shen, X. Boix, and Q. Zhao. SALICON: Re-
ducing the semantic gap in saliency prediction by adapting
deep neural networks. In ICCV, 2015. 2, 3, 6

[19] L. Itti, C. Koch, and E. Niebur. A model of saliency-based
IEEE TPAMI,

visual attention for rapid scene analysis.
20(11):1254–1259, 1998. 2, 6

[20] S. Jetley, N. Murray, and E. Vig. End-to-end saliency map-
ping via probability distribution prediction. In CVPR, 2016.
2, 3

[21] H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li.
Salient object detection: A discriminative regional feature
integration approach. In CVPR, 2013. 2, 7, 8

[22] M. Jiang, S. Huang, J. Duan, and Q. Zhao. SALICON:

Saliency in context. In CVPR, 2015. 6

[23] T. Judd, K. Ehinger, F. Durand, and A. Torralba. Learning to

predict where humans look. In ICCV, 2009. 2, 6

[24] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. In ICLR, 2015. 6

[25] S. S. Kruthiventi, K. Ayush, and R. V. Babu. Deepﬁx: A
fully convolutional neural network for predicting human eye
ﬁxations. IEEE TIP, 2017. 2, 3

[26] S. S. Kruthiventi, V. Gudisa,

J. H. Dholakiya, and
R. Venkatesh Babu. Saliency uniﬁed: A deep architecture
for simultaneous eye ﬁxation prediction and salient object
segmentation. In CVPR, 2016. 2, 3, 6, 7, 8

[27] J. Kuen, Z. Wang, and G. Wang. Recurrent attentional net-

works for saliency detection. In CVPR, 2016. 2

[28] O. Le Meur, P. Le Callet, D. Barba, and D. Thoreau. A coher-
ent computational approach to model bottom-up visual atten-
tion. IEEE TPAMI, 28(5):802–817, 2006. 2

[29] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-

supervised nets. In AISTATS, 2015. 5

[30] G. Lee, Y.-W. Tai, and J. Kim. Deep saliency with encoded
In CVPR,

low level distance map and high level features.
2016. 2, 7

[31] G. Li and Y. Yu. Visual saliency based on multiscale deep

features. In CVPR, 2015. 2, 3, 6, 7, 8

[32] G. Li and Y. Yu. Deep contrast learning for salient object

detection. In CVPR, 2016. 2, 3, 7

[33] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y. Zhuang,
H. Ling, and J. Wang. Deepsaliency: Multi-task deep neu-
IEEE TIP,
ral network model for salient object detection.
25(8):3919 – 3930, 2016. 7

[34] Y. Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille. The
secrets of salient object segmentation. In CVPR, 2014. 1, 2,
6, 7, 8

[35] N. Liu and J. Han. DHSNet: Deep hierarchical saliency net-
work for salient object detection. In CVPR, 2016. 2, 3, 7,
8

[36] N. Liu, J. Han, T. Liu, and X. Li. Learning to predict eye
ﬁxations via multiresolution convolutional neural networks.
IEEE TNNLS, 2016. 2, 3, 6

[37] T. Liu, J. Sun, N.-N. Zheng, X. Tang, and H.-Y. Shum.
Learning to detect a salient object. In CVPR, 2007. 1, 2
[38] Z. Luo, A. Mishra, A. Achkar, J. Eichel, S. Li, and P.-M.
Jodoin. Non-local deep features for salient object detection.
In CVPR, 2017. 2, 3, 7

[39] C. M. Masciocchi, S. Mihalas, D. Parkhurst, and E. Niebur.
Everyone knows what is interesting: Salient locations which
should be ﬁxated. Journal of vision, 9(11):25–25, 2009. 2

[40] S. Mathe and C. Sminchisescu. Dynamic eye movement
datasets and learnt saliency models for visual action recog-
nition. In ECCV. 2012. 2

[41] S. Mathe and C. Sminchisescu. Action from still image
dataset and inverse optimal control to learn task speciﬁc vi-
sual scanpaths. In NIPS, 2013. 2

[42] S. Mathe and C. Sminchisescu. Actions in the eye: Dynamic
gaze datasets and learnt saliency models for visual recogni-
tion. IEEE TPAMI, 37(7):1408–1424, 2015. 2

1719

[64] P. Zhang, D. Wang, H. Lu, H. Wang, and X. Ruan. Amulet:
Aggregating multi-level convolutional features for salient
object detection. In ICCV, 2017. 2, 3, 7

[65] P. Zhang, D. Wang, H. Lu, H. Wang, and B. Yin. Learning
uncertain convolutional features for accurate saliency detec-
tion. In ICCV, 2017. 2, 3, 7

[66] R. Zhao, W. Ouyang, H. Li, and X. Wang. Saliency detection
by multi-context deep learning. In CVPR, 2015. 2, 3
[67] W. Zhu, S. Liang, Y. Wei, and J. Sun. Saliency optimization

from robust background detection. In CVPR, 2014. 2, 7, 8

[43] J. Pan, E. Sayrol, X. Giro-i Nieto, K. McGuinness, and N. E.
O’Connor. Shallow and deep convolutional networks for
saliency prediction. In CVPR, 2016. 2, 3, 6

[44] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Doll´ar. Learn-

ing to reﬁne object segments. In ECCV, 2016. 3, 8

[45] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
3

[46] E. Vig, M. Dorr, and D. Cox. Large-scale optimization of hi-
erarchical features for saliency prediction in natural images.
In CVPR, 2014. 2, 6

[47] L. Wang, H. Lu, X. Ruan, and M.-H. Yang. Deep networks
for saliency detection via local estimation and global search.
In CVPR, 2015. 2, 7

[48] L. Wang, L. Wang, H. Lu, P. Zhang, and X. Ruan. Saliency
detection with recurrent fully convolutional networks.
In
ECCV, 2016. 2, 3, 7

[49] T. Wang, A. Borji, L. Zhang, P. Zhang, and H. Lu. A stage-
wise reﬁnement model for detecting salient objects in im-
ages. In ICCV, 2017. 2, 7

[50] W. Wang and J. Shen. Deep cropping via attention box pre-

diction and aesthetics assessment. In ICCV, 2017. 1

[51] W. Wang and J. Shen. Deep visual attention prediction. IEEE

TIP, 27(5):2368–2378, 2018. 2, 3, 6

[52] W. Wang, J. Shen, and F. Porikli. Saliency-aware geodesic

video object segmentation. In CVPR, 2015. 1

[53] W. Wang, J. Shen, and L. Shao. Consistent video saliency
using local gradient ﬂow optimization and global reﬁnement.
IEEE TIP, 24(11):4185–4196, 2015. 2

[54] W. Wang, J. Shen, and L. Shao. Video salient object detec-
tion via fully convolutional networks. IEEE TIP, 27(1):38–
49, 2018. 2

[55] W. Wang, J. Shen, L. Shao, and F. Porikli. Correspondence
driven saliency transfer. IEEE TIP, 25(11):5025–5034, 2016.
2, 6

[56] W. Wang, J. Shen, R. Yang, and F. Porikli.

Saliency-
aware video object segmentation. IEEE TPAMI, 40(1):20–
33, 2018. 2

[57] W. Wang, J. Shen, Y. Yu, and K.-L. Ma. Stereoscopic thumb-
IEEE
nail creation via efﬁcient stereo saliency detection.
TVCG, 23(8):2014–2027, 2017. 1

[58] Y. Wei, F. Wen, W. Zhu, and J. Sun. Geodesic saliency using

background priors. ECCV, 2012. 2

[59] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong,
and W.-C. Woo. Convolutional LSTM network: A machine
In NIPS,
learning approach for precipitation nowcasting.
2015. 2, 4

[60] Q. Yan, L. Xu, J. Shi, and J. Jia. Hierarchical saliency detec-

tion. In CVPR, 2013. 2, 6, 7, 8

[61] C. Yang, L. Zhang, H. Lu, X. Ruan, and M.-H. Yang.
Saliency detection via graph-based manifold ranking.
In
CVPR, 2013. 6

[62] J. Zhang and S. Sclaroff. Saliency detection: A boolean map

approach. In ICCV, 2013. 6

[63] L. Zhang, M. H. Tong, T. K. Marks, H. Shan, and G. W. Cot-
trell. SUN: A bayesian framework for saliency using natural
statistics. Journal of vision, 8(7):32–32, 2008. 2

1720

