7
1
0
2
 
n
u
J
 
8
2
 
 
]

V
C
.
s
c
[
 
 
3
v
8
4
1
5
0
.
1
1
6
1
:
v
i
X
r
a

Variational Deep Embedding:
An Unsupervised and Generative Approach to
Clustering∗

Zhuxi Jiang1, Yin Zheng2, Huachun Tan1, Bangsheng Tang3, Hanning Zhou3
1Beijing Institute of Technology, Beijing, China
2Tencent AI Lab, Shenzhen, China
3Hulu LLC., Beijing, China
{zjiang, tanhc}@bit.edu.cn, yinzheng@tencent.com,
bangsheng.tang@gmail.com, eric.zhou@hulu.com

June 29, 2017

Abstract

Clustering is among the most fundamental tasks in machine learning and artiﬁ-
cial intelligence. In this paper, we propose Variational Deep Embedding (VaDE), a
novel unsupervised generative clustering approach within the framework of Varia-
tional Auto-Encoder (VAE). Speciﬁcally, VaDE models the data generative proce-
dure with a Gaussian Mixture Model (GMM) and a deep neural network (DNN): 1)
the GMM picks a cluster; 2) from which a latent embedding is generated; 3) then
the DNN decodes the latent embedding into an observable. Inference in VaDE is
done in a variational way: a different DNN is used to encode observables to latent
embeddings, so that the evidence lower bound (ELBO) can be optimized using the
Stochastic Gradient Variational Bayes (SGVB) estimator and the reparameteriza-
tion trick. Quantitative comparisons with strong baselines are included in this pa-
per, and experimental results show that VaDE signiﬁcantly outperforms the state-
of-the-art clustering methods on 5 benchmarks from various modalities. Moreover,
by VaDE’s generative nature, we show its capability of generating highly realis-
tic samples for any speciﬁed cluster, without using supervised information during
training.

1

Introduction

Clustering is the process of grouping similar objects together, which is one of the
most fundamental tasks in machine learning and artiﬁcial intelligence. Over the past
decades, a large family of clustering algorithms have been developed and successfully

∗This paper is accepted by IJCAI 2017, http://ijcai-17.org/accepted-papers.html

Figure 1: The diagram of VaDE. The data generative process of VaDE is done as follows: 1) a
cluster is picked from a GMM model; 2) a latent embedding is generated based on the picked
cluster; 3) DNN f (z; θ) decodes the latent embedding into an observable x. A encoder network
g(x; φ) is used to maximize the ELBO of VaDE.

applied in enormous real world tasks Ng et al. [2002]; Xie et al. [2016]; Yang et al.
[2010]; Ye et al. [2008]. Generally speaking, there is a dichotomy of clustering meth-
ods: Similarity-based clustering and Feature-based clustering. Similarity-based clus-
tering builds models upon a distance matrix, which is a N ×N matrix that measures the
distance between each pair of the N samples. One of the most famous similarity-based
clustering methods is Spectral Clustering (SC) Von Luxburg [2007], which leverages
the Laplacian spectra of the distance matrix to reduce dimensionality before clustering.
Similarity-based clustering methods have the advantage that domain-speciﬁc similar-
ity or kernel functions can be easily incorporated into the models. But these methods
suffer scalability issue due to super-quadratic running time for computing spectra.

Different from similarity-based methods, a feature-based method takes a N × D
matrix as input, where N is the number of samples and D is the feature dimension.
One popular feature-based clustering method is K-means, which aims to partition
the samples into K clusters so as to minimize the within-cluster sum of squared er-
rors. Another representative feature-based clustering model is Gaussian Mixture Model
(GMM), which assumes that the data points are generated from a Mixture-of-Gaussians
(MoG), and the parameters of GMM are optimized by the Expectation Maximization
(EM) algorithm. One advantage of GMM over K-means is that a GMM can generate
samples by estimation of data density. Although K-means, GMM and their variants Liu
et al. [2010]; Ye et al. [2008] have been extensively used, learning good representations
most suitable for clustering tasks is left largely unexplored.

Recently, deep learning has achieved widespread success in numerous machine
learning tasks He et al. [2016]; Krizhevsky et al. [2012]; Szegedy et al. [2015]; Zheng

et al. [2014a,b, 2015, 2016], where learning good representations by deep neural net-
works (DNN) lies in the core. Taking a similar approach, it is conceivable to conduct
clustering analysis on good representations, instead of raw data points.
In a recent
work, Deep Embedded Clustering (DEC) Xie et al. [2016] was proposed to simultane-
ously learn feature representations and cluster assignments by deep neural networks.
Although DEC performs well in clustering, similar to K-means, DEC cannot model
the generative process of data, hence is not able to generate samples. Some recent
works, e.g. VAE Kingma and Welling [2014], GAN Goodfellow et al. [2014] , Pixel-
RNN Oord et al. [2016], InfoGAN Chen et al. [2016] and PPGN Nguyen et al. [2016],
have shown that neural networks can be trained to generate meaningful samples. The
motivation of this work is to develop a clustering model based on neural networks that
1) learns good representations that capture the statistical structure of the data, and 2) is
capable of generating samples.

In this paper, we propose a clustering framework, Variational Deep Embedding
(VaDE), that combines VAE Kingma and Welling [2014] and a Gaussian Mixture
Model for clustering tasks. VaDE models the data generative process by a GMM and
a DNN f : 1) a cluster is picked up by the GMM; 2) from which a latent representation
z is sampled; 3) DNN f decodes z to an observation x. Moreover, VaDE is optimized
by using another DNN g to encode observed data x into latent embedding z, so that
the Stochastic Gradient Variational Bayes (SGVB) estimator and the reparameteriza-
tion trick Kingma and Welling [2014] can be used to maximize the evidence lower
bound (ELBO). VaDE generalizes VAE in that a Mixture-of-Gaussians prior replaces
the single Gaussian prior. Hence, VaDE is by design more suitable for clustering tasks1.
Speciﬁcally, the main contributions of the paper are:

• We propose an unsupervised generative clustering framework, VaDE, that com-

bines VAE and GMM together.

• We show how to optimize VaDE by maximizing the ELBO using the SGVB

estimator and the reparameterization trick;

• Experimental results show that VaDE outperforms the state-of-the-art clustering

models on 5 datasets from various modalities by a large margin;

• We show that VaDE can generate highly realistic samples for any speciﬁed clus-

ter, without using supervised information during training.

The diagram of VaDE is illustrated in Figure 1.

2 Related Work

Recently, people ﬁnd that learning good representations plays an important role in
clustering tasks. For example, DEC Xie et al. [2016] was proposed to learn feature
representations and cluster assignments simultaneously by deep neural networks. In

1Although people can use VaDE to do unsupervised feature learning or semi-supervised learning tasks,

we only focus on clustering tasks in this work.

fact, DEC learns a mapping from the observed space to a lower-dimensional latent
space, where it iteratively optimizes the KL divergence to minimize the within-cluster
distance of each cluster. DEC achieved impressive performances on clustering tasks.
However, the feature embedding in DEC is designed speciﬁcally for clustering and fails
to uncover the real underlying structure of the data, which makes the model lack of the
ability to extend itself to other tasks beyond clustering, such as generating samples.

The deep generative models have recently attracted much attention in that they
can capture the data distribution by neural networks, from which unseen samples can
be generated. GAN and VAE are among the most successful deep generative mod-
els in recent years. Both of them are appealing unsupervised generative models, and
their variants have been extensively studied and applied in various tasks such as semi-
supervised classiﬁcation Abbasnejad et al. [2016]; Kingma et al. [2014]; Maaløe et
al. [2016]; Makhzani et al. [2016]; Salimans et al. [2016], clustering Makhzani et al.
[2016] and image generation Dosovitskiy and Brox [2016]; Radford et al. [2016].

For example, Abbasnejad et al. [2016] proposed to use a mixture of VAEs for semi-
supervised classiﬁcation tasks, where the mixing coefﬁcients of these VAEs are mod-
eled by a Dirichlet process to adapt its capacity to the input data. SB-VAE Nalisnick
and Smyth [2016] also applied Bayesian nonparametric techniques on VAE, which de-
rived a stochastic latent dimensionality by a stick-breaking prior and achieved good
performance on semi-supervised classiﬁcation tasks. VaDE differs with SB-VAE in
that the cluster assignment and the latent representation are jointly considered in the
Gaussian mixture prior, whereas SB-VAE separately models the latent representation
and the class variable, which fails to capture the dependence between them. Addi-
tionally, VaDE does not need the class label during training, while the labels of data
are required by SB-VAE due to its semi-supervised setting. Among the variants of
VAE, Adversarial Auto-Encoder(AAE) Makhzani et al. [2016] can also do unsuper-
vised clustering tasks. Different from VaDE, AAE uses GAN to match the aggregated
posterior with the prior of VAE, which is much more complex than VaDE on the train-
ing procedure. We will compare AAE with VaDE in the experiments part.

Similar to VaDE, Nalisnick et al. [2016] proposed DLGMM to combine VAE and
GMM together. The crucial difference, however, is that VaDE uses a mixture of Gaus-
sian prior to replace the single Gaussian prior of VAE, which is suitable for clustering
tasks by nature, while DLGMM uses a mixture of Gaussian distribution as the approxi-
mate posterior of VAE and does not model the class variable. Hence, VaDE generalizes
VAE to clustering tasks, whereas DLGMM is used to improve the capacity of the orig-
inal VAE and is not suitable for clustering tasks by design. The recently proposed
GM-CVAE Shu et al. [2016] also combines VAE with GMM together. However, the
GMM in GM-CVAE is used to model the transitions between video frames, which is
the main difference with VaDE.

3 Variational Deep Embedding

In this section, we describe Variational Deep Embedding (VaDE), a model for proba-
bilistic clustering problem within the framework of Variational Auto-Encoder (VAE).

3.1 The Generative Process

Since VaDE is a kind of unsupervised generative approach to clustering, we herein ﬁrst
describe the generative process of VaDE. Speciﬁcally, suppose there are K clusters, an
observed sample x ∈ RD is generated by the following process:

1. Choose a cluster c ∼ Cat(π)
2. Choose a latent vector z ∼ N (cid:0)µc, σ2

c I(cid:1)

3. Choose a sample x:

(a) If x is binary

i. Compute the expectation vector µx

µx = f (z; θ)

ii. Choose a sample x ∼ Ber(µx)

(b) If x is real-valued

i. Compute µx and σ2
x

[µx; log σ2
x] = f (z; θ)
xI(cid:1)
ii. Choose a sample x ∼ N (cid:0)µx, σ2

where K is a predeﬁned parameter, πk is the prior probability for cluster k, π ∈ RK
+ ,
1 = (cid:80)K
k=1 πk, Cat(π) is the categorical distribution parametrized by π, µc and σ2
c
are the mean and the variance of the Gaussian distribution corresponding to cluster c, I
is an identity matrix, f (z; θ) is a neural network whose input is z and is parametrized
by θ, Ber(µx) and N (µx, σ2
x) are multivariate Bernoulli distribution and Gaussian
distribution parametrized by µx and µx, σx, respectively. The generative process is
depicted in Figure 1.

According to the generative process above, the joint probability p(x, z, c) can be

factorized as:

p(x, z, c) = p(x|z)p(z|c)p(c),

since x and c are independent conditioned on z. And the probabilities are deﬁned as:

p(c) = Cat(c|π)
p(z|c) = N (cid:0)z|µc, σ2
p(x|z) = Ber(x|µx)

c I(cid:1)
or N (x|µx, σ2

xI)

3.2 Variational Lower Bound

A VaDE instance is tuned to maximize the likelihood of the given data points. Given
the generative process in Section 3.1, by using Jensen’s inequality, the log-likelihood
of VaDE can be written as:

log p(x) = log

p(x, z, c)dz

(cid:90)

(cid:88)

z

c

(1)

(2)

(3)

(4)

(5)

(6)

≥ Eq(z,c|x)[log

] = LELBO(x)

(7)

p(x, z, c)
q(z, c|x)

where LELBO is the evidence lower bound (ELBO), q(z, c|x) is the variational posterior
to approximate the true posterior p(z, c|x). In VaDE, we assume q(z, c|x) to be a mean-
ﬁeld distribution and can be factorized as:

q(z, c|x) = q(z|x)q(c|x).

(8)

Then, according to Equation 3 and 8, the LELBO(x) in Equation 7 can be rewritten

as:

LELBO(x) = Eq(z,c|x)

log

(cid:20)

(cid:21)

p(x, z, c)
q(z, c|x)

= Eq(z,c|x) [log p(x, z, c) − log q(z, c|x)]
= Eq(z,c|x)[log p(x|z) + log p(z|c)

+ log p(c) − log q(z|x) − log q(c|x)]

In VaDE, similar to VAE, we use a neural network g to model q(z|x):

[˜µ; log ˜σ2] = g(x; φ)

q(z|x) = N (z; ˜µ, ˜σ2I)

where φ is the parameter of network g.

By substituting the terms in Equation 9 with Equations 4, 5, 6 and 11, and using
the SGVB estimator and the reparameterization trick, the LELBO(x) can be rewritten
as: 2

LELBO(x) =

xi log µ(l)

x |i + (1 − xi) log(1 − µ(l)

x |i)

L
(cid:88)

D
(cid:88)

l=1

i=1

1
L

−

1
2

K
(cid:88)

J
(cid:88)

γc

c=1

j=1

(log σ2

c |j +

˜σ2|j
σ2
c |j

+

( ˜µ|j − µc|j)2
σ2
c |j

)

+

γc log

+

(1 + log ˜σ2|j)

K
(cid:88)

c=1

πc
γc

1
2

J
(cid:88)

j=1

where L is the number of Monte Carlo samples in the SGVB estimator, D is the
dimensionality of x and µ(l)
x , xi is the ith element of x, J is the dimensionality of µc,
c , ˜µ and ˜σ2, and ∗|j denotes the jth element of ∗, K is the number of clusters, πc is
σ2
the prior probability of cluster c, and γc denotes q(c|x) for simplicity.

In Equation 12, we compute µ(l)

x as

µ(l)

x = f (z(l); θ),

2This is the case when the observation x is binary. For the real-valued situation, the ELBO can be

obtained in a similar way.

(9)

(10)

(11)

(12)

(13)

where z(l) is the lth sample from q(z|x) by Equation 11 to produce the Monte Carlo
samples. According to the reparameterization trick, z(l) is obtained by

z(l) = ˜µ + ˜σ ◦ (cid:15)(l),

(14)

where (cid:15)(l) ∼ N (0, I), ◦ is element-wise multiplication, and ˜µ, ˜σ are derived by Equa-
tion 10.

We now describe how to formulate γc (cid:44) q(c|x) in Equation 12 to maximize the

ELBO. Speciﬁcally, LELBO(x) can be rewritten as:

LELBO(x) = Eq(z,c|x)

log

=

q(c|x)q(z|x)

log

(cid:20)

(cid:20)

(cid:21)

p(x, z, c)
q(z, c|x)

p(x|z)p(z)
q(z|x)
(cid:90)

z

p(x|z)p(z)
q(z|x)

(cid:88)

c

(cid:90)

z
(cid:90)

z

+ log

(cid:21)

dz

p(c|z)
q(c|x)

=

q(z|x) log

dz −

q(z|x)DKL(q(c|x)||p(c|z))dz

(15)

In Equation 15, the ﬁrst term has no relationship with c and the second term is
non-negative. Hence, to maximize LELBO(x), DKL(q(c|x)||p(c|z)) ≡ 0 should be
satisﬁed. As a result, we use the following equation to compute q(c|x) in VaDE:

q(c|x) = p(c|z) ≡

p(c)p(z|c)
c(cid:48)=1 p(c(cid:48))p(z|c(cid:48))

(cid:80)K

(16)

By using Equation 16, the information loss induced by the mean-ﬁeld approxima-
tion can be mitigated, since p(c|z) captures the relationship between c and z.
It is
worth noting that p(c|z) is only an approximation to q(c|x), and we ﬁnd it works well
in practice3.

Once the training is done by maximizing the ELBO w.r.t the parameters of {π, µc, σc, θ, φ},

c ∈ {1, · · · , K}, a latent representation z can be extracted for each observed sample
x by Equation 10 and Equation 11, and the clustering assignments can be obtained by
Equation 16.

3.3 Understanding the ELBO of VaDE

This section, we provide some intuitions of the ELBO of VaDE. More speciﬁcally, the
ELBO in Equation 7 can be further rewritten as:

LELBO(x) = Eq(z,c|x)[log p(x|z)] − DKL(q(z, c|x)||p(z, c))

(17)

The ﬁrst term in Equation 17 is the reconstruction term, which encourages VaDE to
explain the dataset well. And the second term is the Kullback-Leibler divergence from
the Mixture-of-Gaussians (MoG) prior p(z, c) to the variational posterior q(z, c|x),
which regularizes the latent embedding z to lie on a MoG manifold.

3We approximate q(c|x) by: 1) sampling a z(i) ∼ q(z|x); 2) computing q(c|x) = p(c|z(i)) according

to Equation 16

Figure 2: Clustering accuracy over number of epochs during training on MNIST. We also illus-
trate the best performances of DEC, AAE, LDMGI and GMM. It is better to view the ﬁgure in
color.

To demonstrate the importance of the KL term in Equation 17, we train an Auto-
Encoder (AE) with the same network architecture as VaDE ﬁrst, and then apply GMM
on the latent representations from the learned AE, since a VaDE model without the KL
term is almost equivalent to an AE. We refer to this model as AE+GMM. We also show
the performance of using GMM directly on the observed space (GMM), using VAE on
the observed space and then using GMM on the latent space from VAE (VAE+GMM)4,
as well as the performances of LDMGI Yang et al. [2010], AAE Makhzani et al. [2016]
and DEC Xie et al. [2016], in Figure 2. The fact that VaDE outperforms AE+GMM
(without KL term) and VAE+GMM signiﬁcantly conﬁrms the importance of the regu-
larization term and the advantage of jointly optimizing VAE and GMM by VaDE. We
also present the illustrations of clusters and the way they are changed w.r.t. training
epochs on MNIST dataset in Figure 3, where we map the latent representations z into
2D space by t-SNE Maaten and Hinton [2008].

4By doing this, VAE and GMM are optimized separately.

(a) Epoch 0 (11.35%)

(b) Epoch 1 (55.63%)

(c) Epoch 5 (72.40%)

(d) Epoch 50 (84.59%)

(e) Epoch 120 (90.76%)

(f) Epoch End (94.46%)

Figure 3: The illustration about how data is clustered in the latent space learned by VaDE during
training on MNIST. Different colors indicate different ground-truth classes and the clustering
accuracy at the corresponding epoch is reported in the bracket. It is clear to see that the latent
representations become more and more suitable for clustering during training, which can also be
proved by the increasing clustering accuracy.

4 Experiments

In this section, we evaluate the performance of VaDE on 5 benchmarks from differ-
ent modalities: MNIST LeCun et al. [1998], HHAR Stisen et al. [2015], Reuters-
10K Lewis et al. [2004], Reuters Lewis et al. [2004] and STL-10 Coates et al. [2011].
We provide quantitative comparisons of VaDE with other clustering methods includ-
ing GMM, AE+GMM, VAE+GMM, LDGMI Yang et al. [2010], AAE Makhzani et
al. [2016] and the strong baseline DEC Xie et al. [2016]. We use the same net-
work architecture as DEC for a fair comparison. The experimental results show that
VaDE achieves the state-of-the-art performance on all these benchmarks. Addition-
ally, we also provide quantitatively comparisons with other variants of VAE on the
discriminative quality of the latent representations. The code of VaDE is available at
https://github.com/slim1017/VaDE.

4.1 Datasets Description

The following datasets are used in our empirical experiments.

Input Dim # Clusters

Dataset
MNIST
HHAR
REUTERS-10K
REUTERS
STL-10

# Samples
70000
10299
10000
685071
13000

784
561
2000
2000
2048

10
6
4
4
10

Table 1: Datasets statistics

• MNIST: The MNIST dataset consists of 70000 handwritten digits. The images
are centered and of size 28 by 28 pixels. We reshaped each image to a 784-
dimensional vector.

• HHAR: The Heterogeneity Human Activity Recognition (HHAR) dataset con-
tains 10299 sensor records from smart phones and smart watches. All samples
are partitioned into 6 categories of human activities and each sample is of 561
dimensions.

• REUTERS: There are around 810000 English news stories labeled with a cate-
gory tree in original Reuters dataset. Following DEC, we used 4 root categories:
corporate/industrial, government/social, markets, and economics as labels and
discarded all documents with multiple labels, which results in a 685071-article
dataset. We computed tf-idf features on the 2000 most frequent words to repre-
sent all articles. Similar to DEC, a random subset of 10000 documents is sam-
pled, which is referred to as Reuters-10K, since some spectral clustering methods
(e.g. LDMGI) cannot scale to full Reuters dataset.

• STL-10: The STL-10 dataset consists of color images of 96-by-96 pixel size.
There are 10 classes with 1300 examples each. Since clustering directly from
raw pixels of high resolution images is rather difﬁcult, we extracted features of
images of STL-10 by ResNet-50 He et al. [2016], which were then used to test
the performance of VaDE and all baselines. More speciﬁcally, we applied a 3 × 3
average pooling over the last feature map of ResNet-50 and the dimensionality
of the features is 2048.

4.2 Experimental Setup

As mentioned before, the same network architecture as DEC is adopted by VaDE for
a fair comparison. Speciﬁcally, the architectures of f and g in Equation 1 and Equa-
tion 10 are 10-2000-500-500-D and D-500-500-2000-10, respectively, where D is the
input dimensionality. All layers are fully connected. Adam optimizer Kingma and Ba
[2015] is used to maximize the ELBO of Equation 9, and the mini-batch size is 100.
The learning rate for MNIST, HHAR, Reuters-10K and STL-10 is 0.002 and decreases
every 10 epochs with a decay rate of 0.9, and the learning rate for Reuters is 0.0005
with a decay rate of 0.5 for every epoch. As for the generative process in Section 3.1,

MNIST HHAR REUTERS-10K REUTERS
Method
60.34
53.73
54.72
GMM
77.67
82.18
70.13
AE+GMM
68.02
VAE+GMM 72.94
69.56
84.09†
63.43
65.62
LDMGI
83.77
83.48
69.82
AAE
84.30†
79.86
74.32
DEC
94.46
84.46
79.83
VaDE
†: Taken from Xie et al. [2016].

55.81
70.98
60.89
N/A
75.12
75.63†
79.38

STL-10
72.44
79.83
78.86
79.22
80.01
80.62
84.45

Table 2: Clustering accuracy (%) performance comparison on all datasets.

k=3
Method
18.43
VAE
DLGMM 9.14
7.64
SB-VAE
2.20
VaDE

k=5
15.69
8.38
7.25
2.14

k=10
14.19
8.42
7.31
2.22

Table 3: MNIST test error-rate (%) for kNN on latent space.

the multivariate Bernoulli distribution is used for MNIST dataset, and the multivariate
Gaussian distribution is used for the others. The number of clusters is ﬁxed to the num-
ber of classes for each dataset, similar to DEC. We will vary the number of clusters in
Section 4.6.

Similar to other VAE-based models Kingma and Salimans [2016]; Sønderby et al.
[2016], VaDE suffers from the problem that the reconstruction term in Equation 17
would be so weak in the beginning of training that the model might get stuck in an
undesirable local minima or saddle point, from which it is hard to escape.
In this
work, pretraining is used to avoid this problem. Speciﬁcally, we use a Stacked Auto-
Encoder to pretrain the networks f and g. Then all data points are projected into the
latent space z by the pretrained network g, where a GMM is applied to initialize the
parameters of {π, µc, σc}, c ∈ {1, · · · , K}. In practice, few epochs of pretraining are
enough to provide a good initialization of VaDE. We ﬁnd that VaDE is not sensitive to
hyperparameters after pretraining. Hence, we did not spend a lot of effort to tune them.

4.3 Quantitative Comparison

Following DEC, the performance of VaDE is measured by unsupervised clustering
accuracy (ACC), which is deﬁned as:

(cid:80)N

i=1

1{li = m(ci)}

ACC = max
m∈M

N

where N is the total number of samples, li is the ground-truth label, ci is the cluster as-
signment obtained by the model, and M is the set of all possible one-to-one mappings

between cluster assignments and labels. The best mapping can be obtained by using
the KuhnMunkres algorithm Munkres [1957]. Similar to DEC, we perform 10 random
restarts when initializing all clustering models and pick the result with the best objec-
tive value. As for LDMGI, AAE and DEC, we use the same conﬁgurations as their
original papers. Table 2 compares the performance of VaDE with other baselines over
all datasets. It can be seen that VaDE outperforms all these baselines by a large mar-
gin on all datasets. Speciﬁcally, on MNIST, HHAR, Reuters-10K, Reuters and STL-10
dataset, VaDE achieves ACC of 94.46%, 84.46%, 79.83%, 79.38% and 84.45%, which
outperforms DEC with a relative increase ratio of 12.05%, 5.76%, 7.41%, 4.96% and
4.75%, respectively.

We also compare VaDE with SB-VAE Nalisnick and Smyth [2016] and DLGMM Nal-

isnick et al. [2016] on the discriminative power of the latent representations, since these
two baselines cannot do clustering tasks. Following SB-VAE, the discriminative pow-
ers of the models’ latent representations are assessed by running a k-Nearest Neighbors
classiﬁer (kNN) on the latent representations of MNIST. Table 3 shows the error rate of
the kNN classiﬁer on the latent representations. It can be seen that VaDE outperforms
SB-VAE and DLGMM signiﬁcantly5.

Note that although VaDE can learn discriminative representations of samples, the
training of VaDE is in a totally unsupervised way. Hence, we did not compare VaDE
with other supervised models.

4.4 Generating Samples by VaDE

One major advantage of VaDE over DEC Xie et al. [2016] is that it is by nature a
generative clustering model and can generate highly realistic samples for any speciﬁed
cluster (class). In this section, we provide some qualitative comparisons on generat-
ing samples among VaDE, GMM, VAE and the state-of-art generative method Info-
GAN Chen et al. [2016].

Figure 4 illustrates the generated samples for class 0 to 9 of MNIST by GMM, VAE,
InfoGAN and VaDE, respectively. It can be seen that the digits generated by VaDE are
smooth and diverse. Note that the classes of the samples from VAE cannot be speciﬁed.
We can also see that the performance of VaDE is comparable with InfoGAN.

4.5 Visualization of Learned Embeddings

In this section, we visualize the learned representations of VAE, DEC and VaDE on
MNIST dataset. To this end, we use t-SNE Maaten and Hinton [2008] to reduce the
dimensionality of the latent representation z from 10 to 2, and plot 2000 randomly sam-
pled digits in Figure 5. The ﬁrst row of Figure 5 illustrates the ground-truth labels for
each digit, where different colors indicate different labels. The second row of Figure 5
demonstrates the clustering results, where correctly clustered samples are colored with
green and incorrect ones with red.

5We use the same network architecture for VaDE, SB-VAE in Table 3 for fair comparisons. Since there
is no code available for DLGMM, we take the number of DLGMM directly from Nalisnick et al. [2016].
Note that Nalisnick and Smyth [2016] has already shown that the performance of SB-VAE is comparable to
DLGMM.

(a) GMM

(b) VAE

(c) InfoGAN

(d) VaDE

Figure 4: The digits generated by GMM, VAE, InfoGAN and VaDE. Except (b), digits in the
same row come from the same cluster.

From Figure 5 we can see that the original VAE which used a single Gaussian prior
does not perform well in clustering tasks.
It can also be observed that the embed-
dings learned by VaDE are better than those by VAE and DEC, since the number of
incorrectly clustered samples is smaller. Furthermore, incorrectly clustered samples by
VaDE are mostly located at the border of each cluster, where confusing samples usu-
ally appear. In contrast, a lot of the incorrectly clustered samples of DEC appear in the
interior of the clusters, which indicates that DEC fails to preserve the inherent structure
of the data. Some mistakes made by DEC and VaDE are also marked in Figure 5.

4.6 The Impact of the Number of Clusters

So far, the number of clusters for VaDE is set to the number of classes for each dataset,
which is a prior knowledge. To demonstrate VaDE’s representation power as an un-
supervised clustering model, we deliberately choose different numbers of clusters K.
Each row in Figure 6 illustrates the samples from a cluster grouped by VaDE on MNIST
dataset, where K is set to 7 and 14 in Figure 6(a) and Figure 6(b), respectively. We

Figure 5: Visualization of the embeddings learned by VAE, DEC and VaDE on MNIST, re-
spectively. The ﬁrst row illustrates the ground-truth labels for each digit, where different colors
indicate different labels. The second row demonstrates the clustering results, where correctly
clustered samples are colored with green and, incorrect ones with red. GT:4 means the ground-
truth label of the digit is 4, DEC:4 means DEC assigns the digit to the cluster of 4, and VaDE:4
denotes the assignment by VaDE is 4, and so on. It is better to view the ﬁgure in color.

can see that, if K is smaller than the number of classes, digits with similar appearances
will be clustered together, such as 9 and 4, 3 and 8 in Figure 6(a). On the other hand, if
K is larger than the number of classes, some digits will fall into sub-classes by VaDE,
such as the fatter 0 and thinner 0, and the upright 1 and oblique 1 in Figure 6(b).

5 Conclusion

In this paper, we proposed Variational Deep Embedding (VaDE) which embeds the
probabilistic clustering problems into a Variational Auto-Encoder (VAE) framework.
VaDE models the data generative procedure by a GMM model and a neural network,
and is optimized by maximizing the evidence lower bound (ELBO) of the log-likelihood
of data by the SGVB estimator and the reparameterization trick. We compared the
clustering performance of VaDE with strong baselines on 5 benchmarks from different
modalities, and the experimental results showed that VaDE outperforms the state-of-
the-art methods by a large margin. We also showed that VaDE could generate highly
realistic samples conditioned on cluster information without using any supervised in-
formation during training. Note that although we use a MoG prior for VaDE in this

(a) 7 clusters

(b) 14 clusters

Figure 6: Clustering MNIST with different numbers of clusters. We illustrate samples belonging
to each cluster by rows.

paper, other mixture models can also be adopted in this framework ﬂexibly, which will
be our future work.

Acknowledgments

We thank the School of Mechanical Engineering of BIT (Beijing Institute of Tech-
nology) and Collaborative Innovation Center of Electric Vehicles in Beijing for their
support. This work was supported by the National Natural Science Foundation of
China (61620106002, 61271376). We also thank the anonymous reviewers.

References

Ehsan Abbasnejad, Anthony Dick, and Anton van den Hengel.

Inﬁnite variational
autoencoder for semi-supervised learning. arXiv preprint arXiv:1611.07800, 2016.

Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter
Abbeel. Infogan: Interpretable representation learning by information maximizing
generative adversarial nets. In NIPS, 2016.

Adam Coates, Andrew Y Ng, and Honglak Lee. An analysis of single-layer networks in
unsupervised feature learning. In International Conference on Artiﬁcial Intelligence
and Statistics, 2011.

Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity

metrics based on deep networks. In NIPS, 2016.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In
NIPS, 2014.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for

image recognition. In CVPR, 2016.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In

ICLR, 2015.

2014.

Diederik P Kingma and Tim Salimans. Improving variational autoencoders with in-

verse autoregressive ﬂow. In NIPS, 2016.

Diederik P Kingma and Max Welling. Auto-encoding variational bayes.

In ICLR,

Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, and Max Welling. Semi-

supervised learning with deep generative models. In NIPS, 2014.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with

deep convolutional neural networks. In NIPS, 2012.

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learn-

ing applied to document recognition. Proceedings of the IEEE, 1998.

David D Lewis, Yiming Yang, Tony G Rose, and Fan Li. Rcv1: A new benchmark
collection for text categorization research. Journal of machine learning research,
2004.

Jialu Liu, Deng Cai, and Xiaofei He. Gaussian mixture model with local consistency.

In AAAI, 2010.

Lars Maaløe, Casper Kaae Sønderby, Søren Kaae Sønderby, and Ole Winther. Auxil-

iary deep generative models. In ICML, 2016.

Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of

Machine Learning Research, 2008.

Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan

Frey. Adversarial autoencoders. In NIPS, 2016.

James Munkres. Algorithms for the assignment and transportation problems. Journal

of the society for industrial and applied mathematics, 1957.

Eric Nalisnick and Padhraic Smyth. Stick-breaking variational autoencoders. arXiv

preprint arXiv:1605.06197, 2016.

Eric Nalisnick, Lars Hertel, and Padhraic Smyth. Approximate inference for deep

latent gaussian mixtures. 2016.

Andrew Y Ng, Michael I Jordan, Yair Weiss, et al. On spectral clustering: Analysis

and an algorithm. In NIPS, 2002.

Anh Nguyen, Jason Yosinski, Yoshua Bengio, Alexey Dosovitskiy, and Jeff Clune.
Plug & play generative networks: Conditional iterative generation of images in latent
space. arXiv preprint arXiv:1612.00005, 2016.

Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neu-

ral networks. In ICML, 2016.

Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning

with deep convolutional generative adversarial networks. In ICLR, 2016.

Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and

Xi Chen. Improved techniques for training gans. In NIPS, 2016.

Rui Shu, James Brofos, Frank Zhang, Hung Hai Bui, Mohammad Ghavamzadeh, and
Mykel Kochenderfer. Stochastic video prediction with conditional density estima-
tion. In ECCV Workshop on Action and Anticipation for Visual Learning, 2016.

Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole

Winther. Ladder variational autoencoders. In NIPS, 2016.

Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow, Mikkel Baun
Kjærgaard, Anind Dey, Tobias Sonne, and Mads Møller Jensen. Smart devices are
different: Assessing and mitigatingmobile sensing heterogeneities for activity recog-
nition. In Proceedings of the 13th ACM Conference on Embedded Networked Sensor
Systems, 2015.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going
In Proceedings of the IEEE Conference on Computer
deeper with convolutions.
Vision and Pattern Recognition, pages 1–9, 2015.

Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 2007.

Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clus-

tering analysis. In ICML, 2016.

Yi Yang, Dong Xu, Feiping Nie, Shuicheng Yan, and Yueting Zhuang. Image clustering
using local discriminant models and global integration. IEEE Transactions on Image
Processing, 2010.

Jieping Ye, Zheng Zhao, and Mingrui Wu. Discriminative k-means for clustering. In

NIPS, 2008.

Yin Zheng, Richard S Zemel, Yu-Jin Zhang, and Hugo Larochelle. A neural autore-
gressive approach to attention-based recognition. International Journal of Computer
Vision, 113(1):67–79, 2014.

Yin Zheng, Yu-Jin Zhang, and H. Larochelle. Topic modeling of multimodal data: An
autoregressive approach. In Computer Vision and Pattern Recognition (CVPR), 2014
IEEE Conference on, pages 1370–1377, June 2014.

Y. Zheng, Yu-Jin Zhang, and H. Larochelle. A deep and autoregressive approach for
topic modeling of multimodal data. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, PP(99):1–1, 2015.

Yin Zheng, Bangsheng Tang, Wenkui Ding, and Hanning Zhou. A neural autoregres-
In Proceedings of the 33nd International

sive approach to collaborative ﬁltering.
Conference on Machine Learning, pages 764–773, 2016.

Appendix A

In this section, we provide the derivation of q(c|x) = Eq(z|x) [p(c|z)].

The evidence lower bound LELBO(x) can be rewritten as:

LELBO(x) = Eq(z,c|x)

log

(cid:20)

(cid:21)

p(x, z, c)
q(z, c|x)

q(z, c|x) log

p(x|z)p(z|c)p(c)
q(z, c|x)

dz

q(c|x)q(z|x) log

(cid:20)

q(c|x)q(z|x)

log

p(x|z)p(c|z)p(z)
q(c|x)q(z|x)

dz

+ log

(cid:21)

dz

p(c|z)
q(c|x)

p(x|z)p(z)
q(z|x)
(cid:90)

(cid:88)

c
(cid:88)

c
(cid:88)

c

=

=

=

(cid:90)

z
(cid:90)

z
(cid:90)

z
(cid:90)

z
(cid:90)

z

=

q(z|x) log

dz −

q(z|x)

q(c|x) log

(cid:88)

c

q(c|x)
p(c|z)

dz

p(x|z)p(z)
q(z|x)

p(x|z)p(z)
q(z|x)

z
(cid:90)

z

=

q(z|x) log

dz −

q(z|x)DKL(q(c|x)||p(c|z))dz (18)

In Equation 18, the ﬁrst term does not depend on c and the second term is non-
negative. Thus, maximizing the lower bound LELBO(x) with respect to q(c|x) requires
that DKL(q(c|x)||p(c|z)) = 0. Thus, we have

where ν is a constant.

Since (cid:80)

c q(c|x) = 1 and (cid:80)

c p(c|z) = 1, we have:

q(c|x)
p(c|z)

= ν

q(c|x)
p(c|z)

= 1

Taking the expectation on both sides, we can obtain:

q(c|x) = Eq(z|x)[p(c|z)]

Appendix B

Lemma 1 Given two multivariate Gaussian distributions q(z) = N (z; ˜µ, ˜σ2I) and
p(z) = N (z; µ, σ2I), we have:

(cid:90)

q(z) log p(z) dz =

log (2πσ2

j ) −

J
(cid:88)

j=1

−

1
2

˜σ2
j
2σ2
j

−

(˜µj − µj)2
2σ2
j

(19)

where µj, σj, ˜µj and ˜σj simply denote the jth element of µ, σ, ˜µ and ˜σ, respectively,
and J is the dimensionality of z.

Proof (of Lemma 1).

(cid:90)

(cid:90)

q(z) log p(z) dz =

N (z; ˜µ, ˜σ2I) log N (z; µ, σ2I) dz

(cid:90) J
(cid:89)

=

(cid:113)

2π˜σ2
j

j=1

exp(−

(zj − ˜µj)2
2˜σ2
j

) log

1

(cid:113)

2πσ2
j

exp(−

(zj − µj)2
2σ2
j



 dz
)

exp(−

(zj − ˜µj)2
2˜σ2
j

(cid:113)

2π˜σ2
j

) log



(cid:113)

exp(−

1

2πσ2
j



 dzj
)

(zj − µj)2
2σ2
j

J
(cid:89)

j=1







1

1

1

J
(cid:88)

(cid:90)

=

j=1

J
(cid:88)

(cid:90)

=

j=1

J
(cid:88)

j=1

=

−

1
2

=C −

=C −

=C −

=

J
(cid:88)

j=1

−

1
2

(cid:90)

(cid:90)

(cid:90)

(cid:34)

˜σ2
j
σ2
j

˜σ2
j
σ2
j
˜σ2
j
σ2
j
˜σ2
j
σ2
j

exp(−

(cid:113)

2π˜σ2
j

log(2πσ2

j ) −

(zj − ˜µj)2
2˜σ2
j

(cid:20)

)

−

1
2

(cid:21)
log(2πσ2
j )

(cid:90)

dzj −

1

(cid:113)

2π˜σ2
j

exp(−

(zj − ˜µj)2
2˜σ2
j

)

(zj − µj)2
2σ2
j

dzj

(cid:90)

1

(cid:113)

2π˜σ2
j

exp(−

(zj − ˜µj)2
2˜σ2
j

)

(zj − ˜µj)2 + 2(zj − ˜µj)(˜µj − µj) + (˜µj − µj)2
2˜σ2
j

˜σ2
j
σ2
j

dzj

1

(cid:113)

2π˜σ2
j

exp(−

(zj − ˜µj)2
2˜σ2
j

)

(zj − ˜µj)2
2˜σ2
j

(cid:90)

dzj −

1

(cid:113)

2π˜σ2
j

exp(−

(zj − ˜µj)2
2˜σ2
j

)

(˜µj − µj)2
2σ2
j

dzj

exp(−

dxj −

x2
j
2

x2
j
2

)

(˜µj − µj)2
2σ2
j

1
√
2π

1
√
2π

xj
2

=C −

(−

) d(exp(−

)) −

x2
j
2

(˜µj − µj)2
2σ2
j

log (2πσ2

j ) −

˜σ2
j
2σ2
j

−

(˜µj − µj)2
2σ2
j

where C denotes (cid:80)J

j=1 − 1

2 log(2πσ2

j ) for simplicity.

1
√
2π

xj
2

(−

) exp(−

∞

x2
j
2

(cid:12)
(cid:12)
)
(cid:12)

−∞

(cid:90)

−

1
√
2π

exp(−

) d(−

x2
j
2

(cid:35)
)

−

xj
2

(˜µj − µj)2
2σ2
j

Appendix C

In this section, we describe how to compute the evidence lower bound of VaDE. Specif-
ically, the evidence lower bound can be rewritten as:

LELBO(x) =Eq(z,c|x) [log p(x|z)]

+ Eq(z,c|x) [log p(z|c)]
+ Eq(z,c|x) [log p(c)]
− Eq(z,c|x) [log q(z|x)]
− Eq(z,c|x) [log q(c|x)]

(20)

The Equation 20 can be computed by substituting Equation 4, 5, 6, 11 and 16 into
Equation 20 and using Lemma 1 in Appendix B. Speciﬁcally, each item of Equation 20
can be obtained as follows:

• Eq(z,c|x) [log p(x|z)]:

Recall that the observation x can be modeled as either a multivariate Bernoulli
distribution or a multivariate Gaussian distribution. We provide the derivation of
Eq(z,c|x) [log p(x|z)] for the multivariate Bernoulli distribution, and the deriva-
tion for the multivariate Gaussian case can be obtained in a similar way.

Using the SGVB estimator, we can approximate the Eq(z,c|x) [log p(x|z)] as:

Eq(z,c|x) [log p(x|z)] =

log p(x|z(l))

1
L

1
L

L
(cid:88)

l=1

L
(cid:88)

D
(cid:88)

l=1

i=1

=

xi log µ(l)

x i + (1 − xi) log(1 − µ(l)

x i)

(l) = f (z(l); θ), z(l) ∼ N (˜µ, ˜σ2I) and (cid:2)˜µ; log ˜σ2(cid:3) = g(x; φ). L is
where µx
the number of Monte Carlo samples in the SGVB estimator and can be set to 1.
D is the dimensionality of x.

Since the Monte Carlo estimate of the expectation above is non-differentiable
w.r.t φ when z(l) is directly sampled from z ∼ N (˜µ, ˜σ2I), we use the reparam-
eterization trick to obtain a differentiable estimation:

z(l) = ˜µ + ˜σ ◦ (cid:15)(l)

and (cid:15)(l) ∼ N (0, I)

where ◦ denotes the element-wise product.

• Eq(z,c|x) [log p(z|c)]:

Eq(z,c|x) [log p(z|c)] =

q(c|x)q(z|x) log p(z|c) dz

(cid:90)

K
(cid:88)

z

c=1

=

q(c|x)

N (z|˜µ, ˜σ2I) log N (z|µc, σ2

c I) dz

K
(cid:88)

c=1

(cid:90)

z

According to Lemma 1 in Appendix B, we have:

Eq(z,c|x) [log p(z|c)] = −

q(c|x)

log(2π) +

K
(cid:88)

c=1





J
2

1
2

J
(cid:88)
(

j=1

log σ2

cj +

J
(cid:88)

j=1

˜σ2
j
σ2
cj

+

J
(cid:88)

j=1

(˜µj − µcj)2
σ2
cj



)



• Eq(z,c|x) [log p(c)]:

Eq(z,c|x)[log p(c)] =

q(z|x)q(c|x) log p(c) dz

q(z|x)

q(c|x) log πc dz

K
(cid:88)

c=1

q(c|x) log πc

(cid:90)

K
(cid:88)

z

c=1

=

=

(cid:90)

z

K
(cid:88)

c=1

(cid:90)

K
(cid:88)

c=1

z
(cid:90)

z

• Eq(z,c|x) [log q(z|x)]:

Eq(z,c|x) [log q(z|x)] =

q(c|x)q(z|x) log q(z|x) dz

=

N (z; ˜µ, ˜σ2I) log N (z; ˜µ, ˜σ2I) dz

According to Lemma 1 in Appendix B, we have:

Eq(z,c|x) [log q(z|x)] = −

log(2π) −

(1 + log ˜σ2
j )

J
2

1
2

J
(cid:88)

j=1

• Eq(z,c|x) [log q(c|x)]:

Eq(z,c|x) [log q(c|x)] =

q(z|x)q(c|x) log q(c|x) dz

(cid:90)

K
(cid:88)

z

c=1

=

=

(cid:90)

z

K
(cid:88)

c=1

q(z|x)

q(c|x) log q(c|x) dz

K
(cid:88)

c=1

q(c|x) log q(c|x)

where µcj, σcj, ˜µj and ˜σj simply denote the jth element of µc, σc, ˜µ and ˜σ described
in Section 3 respectively. J is the dimensionality of z and K is the number of clusters.
For all the above equations, q(c|x) is computed by Appendix A and can be approx-

imated by the SGVB estimator and the reparameterization trick as follows:

q(c|x) = Eq(z|x) [p(c|z)] =

1
L

L
(cid:88)

l=1

p(c)p(z(l)|c)
c(cid:48)=1 p(c(cid:48))p(z(l)|c(cid:48))

(cid:80)K

where z(l) ∼ N (˜µ, ˜σ2I), (cid:2)˜µ; log ˜σ2(cid:3) = g(x; φ), z(l) = ˜µ + ˜σ ◦ (cid:15)(l) and (cid:15)(l) ∼
N (0, I).

7
1
0
2
 
n
u
J
 
8
2
 
 
]

V
C
.
s
c
[
 
 
3
v
8
4
1
5
0
.
1
1
6
1
:
v
i
X
r
a

Variational Deep Embedding:
An Unsupervised and Generative Approach to
Clustering∗

Zhuxi Jiang1, Yin Zheng2, Huachun Tan1, Bangsheng Tang3, Hanning Zhou3
1Beijing Institute of Technology, Beijing, China
2Tencent AI Lab, Shenzhen, China
3Hulu LLC., Beijing, China
{zjiang, tanhc}@bit.edu.cn, yinzheng@tencent.com,
bangsheng.tang@gmail.com, eric.zhou@hulu.com

June 29, 2017

Abstract

Clustering is among the most fundamental tasks in machine learning and artiﬁ-
cial intelligence. In this paper, we propose Variational Deep Embedding (VaDE), a
novel unsupervised generative clustering approach within the framework of Varia-
tional Auto-Encoder (VAE). Speciﬁcally, VaDE models the data generative proce-
dure with a Gaussian Mixture Model (GMM) and a deep neural network (DNN): 1)
the GMM picks a cluster; 2) from which a latent embedding is generated; 3) then
the DNN decodes the latent embedding into an observable. Inference in VaDE is
done in a variational way: a different DNN is used to encode observables to latent
embeddings, so that the evidence lower bound (ELBO) can be optimized using the
Stochastic Gradient Variational Bayes (SGVB) estimator and the reparameteriza-
tion trick. Quantitative comparisons with strong baselines are included in this pa-
per, and experimental results show that VaDE signiﬁcantly outperforms the state-
of-the-art clustering methods on 5 benchmarks from various modalities. Moreover,
by VaDE’s generative nature, we show its capability of generating highly realis-
tic samples for any speciﬁed cluster, without using supervised information during
training.

1

Introduction

Clustering is the process of grouping similar objects together, which is one of the
most fundamental tasks in machine learning and artiﬁcial intelligence. Over the past
decades, a large family of clustering algorithms have been developed and successfully

∗This paper is accepted by IJCAI 2017, http://ijcai-17.org/accepted-papers.html

Figure 1: The diagram of VaDE. The data generative process of VaDE is done as follows: 1) a
cluster is picked from a GMM model; 2) a latent embedding is generated based on the picked
cluster; 3) DNN f (z; θ) decodes the latent embedding into an observable x. A encoder network
g(x; φ) is used to maximize the ELBO of VaDE.

applied in enormous real world tasks Ng et al. [2002]; Xie et al. [2016]; Yang et al.
[2010]; Ye et al. [2008]. Generally speaking, there is a dichotomy of clustering meth-
ods: Similarity-based clustering and Feature-based clustering. Similarity-based clus-
tering builds models upon a distance matrix, which is a N ×N matrix that measures the
distance between each pair of the N samples. One of the most famous similarity-based
clustering methods is Spectral Clustering (SC) Von Luxburg [2007], which leverages
the Laplacian spectra of the distance matrix to reduce dimensionality before clustering.
Similarity-based clustering methods have the advantage that domain-speciﬁc similar-
ity or kernel functions can be easily incorporated into the models. But these methods
suffer scalability issue due to super-quadratic running time for computing spectra.

Different from similarity-based methods, a feature-based method takes a N × D
matrix as input, where N is the number of samples and D is the feature dimension.
One popular feature-based clustering method is K-means, which aims to partition
the samples into K clusters so as to minimize the within-cluster sum of squared er-
rors. Another representative feature-based clustering model is Gaussian Mixture Model
(GMM), which assumes that the data points are generated from a Mixture-of-Gaussians
(MoG), and the parameters of GMM are optimized by the Expectation Maximization
(EM) algorithm. One advantage of GMM over K-means is that a GMM can generate
samples by estimation of data density. Although K-means, GMM and their variants Liu
et al. [2010]; Ye et al. [2008] have been extensively used, learning good representations
most suitable for clustering tasks is left largely unexplored.

Recently, deep learning has achieved widespread success in numerous machine
learning tasks He et al. [2016]; Krizhevsky et al. [2012]; Szegedy et al. [2015]; Zheng

et al. [2014a,b, 2015, 2016], where learning good representations by deep neural net-
works (DNN) lies in the core. Taking a similar approach, it is conceivable to conduct
clustering analysis on good representations, instead of raw data points.
In a recent
work, Deep Embedded Clustering (DEC) Xie et al. [2016] was proposed to simultane-
ously learn feature representations and cluster assignments by deep neural networks.
Although DEC performs well in clustering, similar to K-means, DEC cannot model
the generative process of data, hence is not able to generate samples. Some recent
works, e.g. VAE Kingma and Welling [2014], GAN Goodfellow et al. [2014] , Pixel-
RNN Oord et al. [2016], InfoGAN Chen et al. [2016] and PPGN Nguyen et al. [2016],
have shown that neural networks can be trained to generate meaningful samples. The
motivation of this work is to develop a clustering model based on neural networks that
1) learns good representations that capture the statistical structure of the data, and 2) is
capable of generating samples.

In this paper, we propose a clustering framework, Variational Deep Embedding
(VaDE), that combines VAE Kingma and Welling [2014] and a Gaussian Mixture
Model for clustering tasks. VaDE models the data generative process by a GMM and
a DNN f : 1) a cluster is picked up by the GMM; 2) from which a latent representation
z is sampled; 3) DNN f decodes z to an observation x. Moreover, VaDE is optimized
by using another DNN g to encode observed data x into latent embedding z, so that
the Stochastic Gradient Variational Bayes (SGVB) estimator and the reparameteriza-
tion trick Kingma and Welling [2014] can be used to maximize the evidence lower
bound (ELBO). VaDE generalizes VAE in that a Mixture-of-Gaussians prior replaces
the single Gaussian prior. Hence, VaDE is by design more suitable for clustering tasks1.
Speciﬁcally, the main contributions of the paper are:

• We propose an unsupervised generative clustering framework, VaDE, that com-

bines VAE and GMM together.

• We show how to optimize VaDE by maximizing the ELBO using the SGVB

estimator and the reparameterization trick;

• Experimental results show that VaDE outperforms the state-of-the-art clustering

models on 5 datasets from various modalities by a large margin;

• We show that VaDE can generate highly realistic samples for any speciﬁed clus-

ter, without using supervised information during training.

The diagram of VaDE is illustrated in Figure 1.

2 Related Work

Recently, people ﬁnd that learning good representations plays an important role in
clustering tasks. For example, DEC Xie et al. [2016] was proposed to learn feature
representations and cluster assignments simultaneously by deep neural networks. In

1Although people can use VaDE to do unsupervised feature learning or semi-supervised learning tasks,

we only focus on clustering tasks in this work.

fact, DEC learns a mapping from the observed space to a lower-dimensional latent
space, where it iteratively optimizes the KL divergence to minimize the within-cluster
distance of each cluster. DEC achieved impressive performances on clustering tasks.
However, the feature embedding in DEC is designed speciﬁcally for clustering and fails
to uncover the real underlying structure of the data, which makes the model lack of the
ability to extend itself to other tasks beyond clustering, such as generating samples.

The deep generative models have recently attracted much attention in that they
can capture the data distribution by neural networks, from which unseen samples can
be generated. GAN and VAE are among the most successful deep generative mod-
els in recent years. Both of them are appealing unsupervised generative models, and
their variants have been extensively studied and applied in various tasks such as semi-
supervised classiﬁcation Abbasnejad et al. [2016]; Kingma et al. [2014]; Maaløe et
al. [2016]; Makhzani et al. [2016]; Salimans et al. [2016], clustering Makhzani et al.
[2016] and image generation Dosovitskiy and Brox [2016]; Radford et al. [2016].

For example, Abbasnejad et al. [2016] proposed to use a mixture of VAEs for semi-
supervised classiﬁcation tasks, where the mixing coefﬁcients of these VAEs are mod-
eled by a Dirichlet process to adapt its capacity to the input data. SB-VAE Nalisnick
and Smyth [2016] also applied Bayesian nonparametric techniques on VAE, which de-
rived a stochastic latent dimensionality by a stick-breaking prior and achieved good
performance on semi-supervised classiﬁcation tasks. VaDE differs with SB-VAE in
that the cluster assignment and the latent representation are jointly considered in the
Gaussian mixture prior, whereas SB-VAE separately models the latent representation
and the class variable, which fails to capture the dependence between them. Addi-
tionally, VaDE does not need the class label during training, while the labels of data
are required by SB-VAE due to its semi-supervised setting. Among the variants of
VAE, Adversarial Auto-Encoder(AAE) Makhzani et al. [2016] can also do unsuper-
vised clustering tasks. Different from VaDE, AAE uses GAN to match the aggregated
posterior with the prior of VAE, which is much more complex than VaDE on the train-
ing procedure. We will compare AAE with VaDE in the experiments part.

Similar to VaDE, Nalisnick et al. [2016] proposed DLGMM to combine VAE and
GMM together. The crucial difference, however, is that VaDE uses a mixture of Gaus-
sian prior to replace the single Gaussian prior of VAE, which is suitable for clustering
tasks by nature, while DLGMM uses a mixture of Gaussian distribution as the approxi-
mate posterior of VAE and does not model the class variable. Hence, VaDE generalizes
VAE to clustering tasks, whereas DLGMM is used to improve the capacity of the orig-
inal VAE and is not suitable for clustering tasks by design. The recently proposed
GM-CVAE Shu et al. [2016] also combines VAE with GMM together. However, the
GMM in GM-CVAE is used to model the transitions between video frames, which is
the main difference with VaDE.

3 Variational Deep Embedding

In this section, we describe Variational Deep Embedding (VaDE), a model for proba-
bilistic clustering problem within the framework of Variational Auto-Encoder (VAE).

3.1 The Generative Process

Since VaDE is a kind of unsupervised generative approach to clustering, we herein ﬁrst
describe the generative process of VaDE. Speciﬁcally, suppose there are K clusters, an
observed sample x ∈ RD is generated by the following process:

1. Choose a cluster c ∼ Cat(π)
2. Choose a latent vector z ∼ N (cid:0)µc, σ2

c I(cid:1)

3. Choose a sample x:

(a) If x is binary

i. Compute the expectation vector µx

µx = f (z; θ)

ii. Choose a sample x ∼ Ber(µx)

(b) If x is real-valued

i. Compute µx and σ2
x

[µx; log σ2
x] = f (z; θ)
xI(cid:1)
ii. Choose a sample x ∼ N (cid:0)µx, σ2

where K is a predeﬁned parameter, πk is the prior probability for cluster k, π ∈ RK
+ ,
1 = (cid:80)K
k=1 πk, Cat(π) is the categorical distribution parametrized by π, µc and σ2
c
are the mean and the variance of the Gaussian distribution corresponding to cluster c, I
is an identity matrix, f (z; θ) is a neural network whose input is z and is parametrized
by θ, Ber(µx) and N (µx, σ2
x) are multivariate Bernoulli distribution and Gaussian
distribution parametrized by µx and µx, σx, respectively. The generative process is
depicted in Figure 1.

According to the generative process above, the joint probability p(x, z, c) can be

factorized as:

p(x, z, c) = p(x|z)p(z|c)p(c),

since x and c are independent conditioned on z. And the probabilities are deﬁned as:

p(c) = Cat(c|π)
p(z|c) = N (cid:0)z|µc, σ2
p(x|z) = Ber(x|µx)

c I(cid:1)
or N (x|µx, σ2

xI)

3.2 Variational Lower Bound

A VaDE instance is tuned to maximize the likelihood of the given data points. Given
the generative process in Section 3.1, by using Jensen’s inequality, the log-likelihood
of VaDE can be written as:

log p(x) = log

p(x, z, c)dz

(cid:90)

(cid:88)

z

c

(1)

(2)

(3)

(4)

(5)

(6)

≥ Eq(z,c|x)[log

] = LELBO(x)

(7)

p(x, z, c)
q(z, c|x)

where LELBO is the evidence lower bound (ELBO), q(z, c|x) is the variational posterior
to approximate the true posterior p(z, c|x). In VaDE, we assume q(z, c|x) to be a mean-
ﬁeld distribution and can be factorized as:

q(z, c|x) = q(z|x)q(c|x).

(8)

Then, according to Equation 3 and 8, the LELBO(x) in Equation 7 can be rewritten

as:

LELBO(x) = Eq(z,c|x)

log

(cid:20)

(cid:21)

p(x, z, c)
q(z, c|x)

= Eq(z,c|x) [log p(x, z, c) − log q(z, c|x)]
= Eq(z,c|x)[log p(x|z) + log p(z|c)

+ log p(c) − log q(z|x) − log q(c|x)]

In VaDE, similar to VAE, we use a neural network g to model q(z|x):

[˜µ; log ˜σ2] = g(x; φ)

q(z|x) = N (z; ˜µ, ˜σ2I)

where φ is the parameter of network g.

By substituting the terms in Equation 9 with Equations 4, 5, 6 and 11, and using
the SGVB estimator and the reparameterization trick, the LELBO(x) can be rewritten
as: 2

LELBO(x) =

xi log µ(l)

x |i + (1 − xi) log(1 − µ(l)

x |i)

L
(cid:88)

D
(cid:88)

l=1

i=1

1
L

−

1
2

K
(cid:88)

J
(cid:88)

γc

c=1

j=1

(log σ2

c |j +

˜σ2|j
σ2
c |j

+

( ˜µ|j − µc|j)2
σ2
c |j

)

+

γc log

+

(1 + log ˜σ2|j)

K
(cid:88)

c=1

πc
γc

1
2

J
(cid:88)

j=1

where L is the number of Monte Carlo samples in the SGVB estimator, D is the
dimensionality of x and µ(l)
x , xi is the ith element of x, J is the dimensionality of µc,
c , ˜µ and ˜σ2, and ∗|j denotes the jth element of ∗, K is the number of clusters, πc is
σ2
the prior probability of cluster c, and γc denotes q(c|x) for simplicity.

In Equation 12, we compute µ(l)

x as

µ(l)

x = f (z(l); θ),

2This is the case when the observation x is binary. For the real-valued situation, the ELBO can be

obtained in a similar way.

(9)

(10)

(11)

(12)

(13)

where z(l) is the lth sample from q(z|x) by Equation 11 to produce the Monte Carlo
samples. According to the reparameterization trick, z(l) is obtained by

z(l) = ˜µ + ˜σ ◦ (cid:15)(l),

(14)

where (cid:15)(l) ∼ N (0, I), ◦ is element-wise multiplication, and ˜µ, ˜σ are derived by Equa-
tion 10.

We now describe how to formulate γc (cid:44) q(c|x) in Equation 12 to maximize the

ELBO. Speciﬁcally, LELBO(x) can be rewritten as:

LELBO(x) = Eq(z,c|x)

log

=

q(c|x)q(z|x)

log

(cid:20)

(cid:20)

(cid:21)

p(x, z, c)
q(z, c|x)

p(x|z)p(z)
q(z|x)
(cid:90)

z

p(x|z)p(z)
q(z|x)

(cid:88)

c

(cid:90)

z
(cid:90)

z

+ log

(cid:21)

dz

p(c|z)
q(c|x)

=

q(z|x) log

dz −

q(z|x)DKL(q(c|x)||p(c|z))dz

(15)

In Equation 15, the ﬁrst term has no relationship with c and the second term is
non-negative. Hence, to maximize LELBO(x), DKL(q(c|x)||p(c|z)) ≡ 0 should be
satisﬁed. As a result, we use the following equation to compute q(c|x) in VaDE:

q(c|x) = p(c|z) ≡

p(c)p(z|c)
c(cid:48)=1 p(c(cid:48))p(z|c(cid:48))

(cid:80)K

(16)

By using Equation 16, the information loss induced by the mean-ﬁeld approxima-
tion can be mitigated, since p(c|z) captures the relationship between c and z.
It is
worth noting that p(c|z) is only an approximation to q(c|x), and we ﬁnd it works well
in practice3.

Once the training is done by maximizing the ELBO w.r.t the parameters of {π, µc, σc, θ, φ},

c ∈ {1, · · · , K}, a latent representation z can be extracted for each observed sample
x by Equation 10 and Equation 11, and the clustering assignments can be obtained by
Equation 16.

3.3 Understanding the ELBO of VaDE

This section, we provide some intuitions of the ELBO of VaDE. More speciﬁcally, the
ELBO in Equation 7 can be further rewritten as:

LELBO(x) = Eq(z,c|x)[log p(x|z)] − DKL(q(z, c|x)||p(z, c))

(17)

The ﬁrst term in Equation 17 is the reconstruction term, which encourages VaDE to
explain the dataset well. And the second term is the Kullback-Leibler divergence from
the Mixture-of-Gaussians (MoG) prior p(z, c) to the variational posterior q(z, c|x),
which regularizes the latent embedding z to lie on a MoG manifold.

3We approximate q(c|x) by: 1) sampling a z(i) ∼ q(z|x); 2) computing q(c|x) = p(c|z(i)) according

to Equation 16

Figure 2: Clustering accuracy over number of epochs during training on MNIST. We also illus-
trate the best performances of DEC, AAE, LDMGI and GMM. It is better to view the ﬁgure in
color.

To demonstrate the importance of the KL term in Equation 17, we train an Auto-
Encoder (AE) with the same network architecture as VaDE ﬁrst, and then apply GMM
on the latent representations from the learned AE, since a VaDE model without the KL
term is almost equivalent to an AE. We refer to this model as AE+GMM. We also show
the performance of using GMM directly on the observed space (GMM), using VAE on
the observed space and then using GMM on the latent space from VAE (VAE+GMM)4,
as well as the performances of LDMGI Yang et al. [2010], AAE Makhzani et al. [2016]
and DEC Xie et al. [2016], in Figure 2. The fact that VaDE outperforms AE+GMM
(without KL term) and VAE+GMM signiﬁcantly conﬁrms the importance of the regu-
larization term and the advantage of jointly optimizing VAE and GMM by VaDE. We
also present the illustrations of clusters and the way they are changed w.r.t. training
epochs on MNIST dataset in Figure 3, where we map the latent representations z into
2D space by t-SNE Maaten and Hinton [2008].

4By doing this, VAE and GMM are optimized separately.

(a) Epoch 0 (11.35%)

(b) Epoch 1 (55.63%)

(c) Epoch 5 (72.40%)

(d) Epoch 50 (84.59%)

(e) Epoch 120 (90.76%)

(f) Epoch End (94.46%)

Figure 3: The illustration about how data is clustered in the latent space learned by VaDE during
training on MNIST. Different colors indicate different ground-truth classes and the clustering
accuracy at the corresponding epoch is reported in the bracket. It is clear to see that the latent
representations become more and more suitable for clustering during training, which can also be
proved by the increasing clustering accuracy.

4 Experiments

In this section, we evaluate the performance of VaDE on 5 benchmarks from differ-
ent modalities: MNIST LeCun et al. [1998], HHAR Stisen et al. [2015], Reuters-
10K Lewis et al. [2004], Reuters Lewis et al. [2004] and STL-10 Coates et al. [2011].
We provide quantitative comparisons of VaDE with other clustering methods includ-
ing GMM, AE+GMM, VAE+GMM, LDGMI Yang et al. [2010], AAE Makhzani et
al. [2016] and the strong baseline DEC Xie et al. [2016]. We use the same net-
work architecture as DEC for a fair comparison. The experimental results show that
VaDE achieves the state-of-the-art performance on all these benchmarks. Addition-
ally, we also provide quantitatively comparisons with other variants of VAE on the
discriminative quality of the latent representations. The code of VaDE is available at
https://github.com/slim1017/VaDE.

4.1 Datasets Description

The following datasets are used in our empirical experiments.

Input Dim # Clusters

Dataset
MNIST
HHAR
REUTERS-10K
REUTERS
STL-10

# Samples
70000
10299
10000
685071
13000

784
561
2000
2000
2048

10
6
4
4
10

Table 1: Datasets statistics

• MNIST: The MNIST dataset consists of 70000 handwritten digits. The images
are centered and of size 28 by 28 pixels. We reshaped each image to a 784-
dimensional vector.

• HHAR: The Heterogeneity Human Activity Recognition (HHAR) dataset con-
tains 10299 sensor records from smart phones and smart watches. All samples
are partitioned into 6 categories of human activities and each sample is of 561
dimensions.

• REUTERS: There are around 810000 English news stories labeled with a cate-
gory tree in original Reuters dataset. Following DEC, we used 4 root categories:
corporate/industrial, government/social, markets, and economics as labels and
discarded all documents with multiple labels, which results in a 685071-article
dataset. We computed tf-idf features on the 2000 most frequent words to repre-
sent all articles. Similar to DEC, a random subset of 10000 documents is sam-
pled, which is referred to as Reuters-10K, since some spectral clustering methods
(e.g. LDMGI) cannot scale to full Reuters dataset.

• STL-10: The STL-10 dataset consists of color images of 96-by-96 pixel size.
There are 10 classes with 1300 examples each. Since clustering directly from
raw pixels of high resolution images is rather difﬁcult, we extracted features of
images of STL-10 by ResNet-50 He et al. [2016], which were then used to test
the performance of VaDE and all baselines. More speciﬁcally, we applied a 3 × 3
average pooling over the last feature map of ResNet-50 and the dimensionality
of the features is 2048.

4.2 Experimental Setup

As mentioned before, the same network architecture as DEC is adopted by VaDE for
a fair comparison. Speciﬁcally, the architectures of f and g in Equation 1 and Equa-
tion 10 are 10-2000-500-500-D and D-500-500-2000-10, respectively, where D is the
input dimensionality. All layers are fully connected. Adam optimizer Kingma and Ba
[2015] is used to maximize the ELBO of Equation 9, and the mini-batch size is 100.
The learning rate for MNIST, HHAR, Reuters-10K and STL-10 is 0.002 and decreases
every 10 epochs with a decay rate of 0.9, and the learning rate for Reuters is 0.0005
with a decay rate of 0.5 for every epoch. As for the generative process in Section 3.1,

MNIST HHAR REUTERS-10K REUTERS
Method
60.34
53.73
54.72
GMM
77.67
82.18
70.13
AE+GMM
68.02
VAE+GMM 72.94
69.56
84.09†
63.43
65.62
LDMGI
83.77
83.48
69.82
AAE
84.30†
79.86
74.32
DEC
94.46
84.46
79.83
VaDE
†: Taken from Xie et al. [2016].

55.81
70.98
60.89
N/A
75.12
75.63†
79.38

STL-10
72.44
79.83
78.86
79.22
80.01
80.62
84.45

Table 2: Clustering accuracy (%) performance comparison on all datasets.

k=3
Method
18.43
VAE
DLGMM 9.14
7.64
SB-VAE
2.20
VaDE

k=5
15.69
8.38
7.25
2.14

k=10
14.19
8.42
7.31
2.22

Table 3: MNIST test error-rate (%) for kNN on latent space.

the multivariate Bernoulli distribution is used for MNIST dataset, and the multivariate
Gaussian distribution is used for the others. The number of clusters is ﬁxed to the num-
ber of classes for each dataset, similar to DEC. We will vary the number of clusters in
Section 4.6.

Similar to other VAE-based models Kingma and Salimans [2016]; Sønderby et al.
[2016], VaDE suffers from the problem that the reconstruction term in Equation 17
would be so weak in the beginning of training that the model might get stuck in an
undesirable local minima or saddle point, from which it is hard to escape.
In this
work, pretraining is used to avoid this problem. Speciﬁcally, we use a Stacked Auto-
Encoder to pretrain the networks f and g. Then all data points are projected into the
latent space z by the pretrained network g, where a GMM is applied to initialize the
parameters of {π, µc, σc}, c ∈ {1, · · · , K}. In practice, few epochs of pretraining are
enough to provide a good initialization of VaDE. We ﬁnd that VaDE is not sensitive to
hyperparameters after pretraining. Hence, we did not spend a lot of effort to tune them.

4.3 Quantitative Comparison

Following DEC, the performance of VaDE is measured by unsupervised clustering
accuracy (ACC), which is deﬁned as:

(cid:80)N

i=1

1{li = m(ci)}

ACC = max
m∈M

N

where N is the total number of samples, li is the ground-truth label, ci is the cluster as-
signment obtained by the model, and M is the set of all possible one-to-one mappings

between cluster assignments and labels. The best mapping can be obtained by using
the KuhnMunkres algorithm Munkres [1957]. Similar to DEC, we perform 10 random
restarts when initializing all clustering models and pick the result with the best objec-
tive value. As for LDMGI, AAE and DEC, we use the same conﬁgurations as their
original papers. Table 2 compares the performance of VaDE with other baselines over
all datasets. It can be seen that VaDE outperforms all these baselines by a large mar-
gin on all datasets. Speciﬁcally, on MNIST, HHAR, Reuters-10K, Reuters and STL-10
dataset, VaDE achieves ACC of 94.46%, 84.46%, 79.83%, 79.38% and 84.45%, which
outperforms DEC with a relative increase ratio of 12.05%, 5.76%, 7.41%, 4.96% and
4.75%, respectively.

We also compare VaDE with SB-VAE Nalisnick and Smyth [2016] and DLGMM Nal-

isnick et al. [2016] on the discriminative power of the latent representations, since these
two baselines cannot do clustering tasks. Following SB-VAE, the discriminative pow-
ers of the models’ latent representations are assessed by running a k-Nearest Neighbors
classiﬁer (kNN) on the latent representations of MNIST. Table 3 shows the error rate of
the kNN classiﬁer on the latent representations. It can be seen that VaDE outperforms
SB-VAE and DLGMM signiﬁcantly5.

Note that although VaDE can learn discriminative representations of samples, the
training of VaDE is in a totally unsupervised way. Hence, we did not compare VaDE
with other supervised models.

4.4 Generating Samples by VaDE

One major advantage of VaDE over DEC Xie et al. [2016] is that it is by nature a
generative clustering model and can generate highly realistic samples for any speciﬁed
cluster (class). In this section, we provide some qualitative comparisons on generat-
ing samples among VaDE, GMM, VAE and the state-of-art generative method Info-
GAN Chen et al. [2016].

Figure 4 illustrates the generated samples for class 0 to 9 of MNIST by GMM, VAE,
InfoGAN and VaDE, respectively. It can be seen that the digits generated by VaDE are
smooth and diverse. Note that the classes of the samples from VAE cannot be speciﬁed.
We can also see that the performance of VaDE is comparable with InfoGAN.

4.5 Visualization of Learned Embeddings

In this section, we visualize the learned representations of VAE, DEC and VaDE on
MNIST dataset. To this end, we use t-SNE Maaten and Hinton [2008] to reduce the
dimensionality of the latent representation z from 10 to 2, and plot 2000 randomly sam-
pled digits in Figure 5. The ﬁrst row of Figure 5 illustrates the ground-truth labels for
each digit, where different colors indicate different labels. The second row of Figure 5
demonstrates the clustering results, where correctly clustered samples are colored with
green and incorrect ones with red.

5We use the same network architecture for VaDE, SB-VAE in Table 3 for fair comparisons. Since there
is no code available for DLGMM, we take the number of DLGMM directly from Nalisnick et al. [2016].
Note that Nalisnick and Smyth [2016] has already shown that the performance of SB-VAE is comparable to
DLGMM.

(a) GMM

(b) VAE

(c) InfoGAN

(d) VaDE

Figure 4: The digits generated by GMM, VAE, InfoGAN and VaDE. Except (b), digits in the
same row come from the same cluster.

From Figure 5 we can see that the original VAE which used a single Gaussian prior
does not perform well in clustering tasks.
It can also be observed that the embed-
dings learned by VaDE are better than those by VAE and DEC, since the number of
incorrectly clustered samples is smaller. Furthermore, incorrectly clustered samples by
VaDE are mostly located at the border of each cluster, where confusing samples usu-
ally appear. In contrast, a lot of the incorrectly clustered samples of DEC appear in the
interior of the clusters, which indicates that DEC fails to preserve the inherent structure
of the data. Some mistakes made by DEC and VaDE are also marked in Figure 5.

4.6 The Impact of the Number of Clusters

So far, the number of clusters for VaDE is set to the number of classes for each dataset,
which is a prior knowledge. To demonstrate VaDE’s representation power as an un-
supervised clustering model, we deliberately choose different numbers of clusters K.
Each row in Figure 6 illustrates the samples from a cluster grouped by VaDE on MNIST
dataset, where K is set to 7 and 14 in Figure 6(a) and Figure 6(b), respectively. We

Figure 5: Visualization of the embeddings learned by VAE, DEC and VaDE on MNIST, re-
spectively. The ﬁrst row illustrates the ground-truth labels for each digit, where different colors
indicate different labels. The second row demonstrates the clustering results, where correctly
clustered samples are colored with green and, incorrect ones with red. GT:4 means the ground-
truth label of the digit is 4, DEC:4 means DEC assigns the digit to the cluster of 4, and VaDE:4
denotes the assignment by VaDE is 4, and so on. It is better to view the ﬁgure in color.

can see that, if K is smaller than the number of classes, digits with similar appearances
will be clustered together, such as 9 and 4, 3 and 8 in Figure 6(a). On the other hand, if
K is larger than the number of classes, some digits will fall into sub-classes by VaDE,
such as the fatter 0 and thinner 0, and the upright 1 and oblique 1 in Figure 6(b).

5 Conclusion

In this paper, we proposed Variational Deep Embedding (VaDE) which embeds the
probabilistic clustering problems into a Variational Auto-Encoder (VAE) framework.
VaDE models the data generative procedure by a GMM model and a neural network,
and is optimized by maximizing the evidence lower bound (ELBO) of the log-likelihood
of data by the SGVB estimator and the reparameterization trick. We compared the
clustering performance of VaDE with strong baselines on 5 benchmarks from different
modalities, and the experimental results showed that VaDE outperforms the state-of-
the-art methods by a large margin. We also showed that VaDE could generate highly
realistic samples conditioned on cluster information without using any supervised in-
formation during training. Note that although we use a MoG prior for VaDE in this

(a) 7 clusters

(b) 14 clusters

Figure 6: Clustering MNIST with different numbers of clusters. We illustrate samples belonging
to each cluster by rows.

paper, other mixture models can also be adopted in this framework ﬂexibly, which will
be our future work.

Acknowledgments

We thank the School of Mechanical Engineering of BIT (Beijing Institute of Tech-
nology) and Collaborative Innovation Center of Electric Vehicles in Beijing for their
support. This work was supported by the National Natural Science Foundation of
China (61620106002, 61271376). We also thank the anonymous reviewers.

References

Ehsan Abbasnejad, Anthony Dick, and Anton van den Hengel.

Inﬁnite variational
autoencoder for semi-supervised learning. arXiv preprint arXiv:1611.07800, 2016.

Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter
Abbeel. Infogan: Interpretable representation learning by information maximizing
generative adversarial nets. In NIPS, 2016.

Adam Coates, Andrew Y Ng, and Honglak Lee. An analysis of single-layer networks in
unsupervised feature learning. In International Conference on Artiﬁcial Intelligence
and Statistics, 2011.

Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity

metrics based on deep networks. In NIPS, 2016.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In
NIPS, 2014.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for

image recognition. In CVPR, 2016.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In

ICLR, 2015.

2014.

Diederik P Kingma and Tim Salimans. Improving variational autoencoders with in-

verse autoregressive ﬂow. In NIPS, 2016.

Diederik P Kingma and Max Welling. Auto-encoding variational bayes.

In ICLR,

Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, and Max Welling. Semi-

supervised learning with deep generative models. In NIPS, 2014.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with

deep convolutional neural networks. In NIPS, 2012.

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learn-

ing applied to document recognition. Proceedings of the IEEE, 1998.

David D Lewis, Yiming Yang, Tony G Rose, and Fan Li. Rcv1: A new benchmark
collection for text categorization research. Journal of machine learning research,
2004.

Jialu Liu, Deng Cai, and Xiaofei He. Gaussian mixture model with local consistency.

In AAAI, 2010.

Lars Maaløe, Casper Kaae Sønderby, Søren Kaae Sønderby, and Ole Winther. Auxil-

iary deep generative models. In ICML, 2016.

Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of

Machine Learning Research, 2008.

Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan

Frey. Adversarial autoencoders. In NIPS, 2016.

James Munkres. Algorithms for the assignment and transportation problems. Journal

of the society for industrial and applied mathematics, 1957.

Eric Nalisnick and Padhraic Smyth. Stick-breaking variational autoencoders. arXiv

preprint arXiv:1605.06197, 2016.

Eric Nalisnick, Lars Hertel, and Padhraic Smyth. Approximate inference for deep

latent gaussian mixtures. 2016.

Andrew Y Ng, Michael I Jordan, Yair Weiss, et al. On spectral clustering: Analysis

and an algorithm. In NIPS, 2002.

Anh Nguyen, Jason Yosinski, Yoshua Bengio, Alexey Dosovitskiy, and Jeff Clune.
Plug & play generative networks: Conditional iterative generation of images in latent
space. arXiv preprint arXiv:1612.00005, 2016.

Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neu-

ral networks. In ICML, 2016.

Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning

with deep convolutional generative adversarial networks. In ICLR, 2016.

Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and

Xi Chen. Improved techniques for training gans. In NIPS, 2016.

Rui Shu, James Brofos, Frank Zhang, Hung Hai Bui, Mohammad Ghavamzadeh, and
Mykel Kochenderfer. Stochastic video prediction with conditional density estima-
tion. In ECCV Workshop on Action and Anticipation for Visual Learning, 2016.

Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole

Winther. Ladder variational autoencoders. In NIPS, 2016.

Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow, Mikkel Baun
Kjærgaard, Anind Dey, Tobias Sonne, and Mads Møller Jensen. Smart devices are
different: Assessing and mitigatingmobile sensing heterogeneities for activity recog-
nition. In Proceedings of the 13th ACM Conference on Embedded Networked Sensor
Systems, 2015.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going
In Proceedings of the IEEE Conference on Computer
deeper with convolutions.
Vision and Pattern Recognition, pages 1–9, 2015.

Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 2007.

Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clus-

tering analysis. In ICML, 2016.

Yi Yang, Dong Xu, Feiping Nie, Shuicheng Yan, and Yueting Zhuang. Image clustering
using local discriminant models and global integration. IEEE Transactions on Image
Processing, 2010.

Jieping Ye, Zheng Zhao, and Mingrui Wu. Discriminative k-means for clustering. In

NIPS, 2008.

Yin Zheng, Richard S Zemel, Yu-Jin Zhang, and Hugo Larochelle. A neural autore-
gressive approach to attention-based recognition. International Journal of Computer
Vision, 113(1):67–79, 2014.

Yin Zheng, Yu-Jin Zhang, and H. Larochelle. Topic modeling of multimodal data: An
autoregressive approach. In Computer Vision and Pattern Recognition (CVPR), 2014
IEEE Conference on, pages 1370–1377, June 2014.

Y. Zheng, Yu-Jin Zhang, and H. Larochelle. A deep and autoregressive approach for
topic modeling of multimodal data. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, PP(99):1–1, 2015.

Yin Zheng, Bangsheng Tang, Wenkui Ding, and Hanning Zhou. A neural autoregres-
In Proceedings of the 33nd International

sive approach to collaborative ﬁltering.
Conference on Machine Learning, pages 764–773, 2016.

Appendix A

In this section, we provide the derivation of q(c|x) = Eq(z|x) [p(c|z)].

The evidence lower bound LELBO(x) can be rewritten as:

LELBO(x) = Eq(z,c|x)

log

(cid:20)

(cid:21)

p(x, z, c)
q(z, c|x)

q(z, c|x) log

p(x|z)p(z|c)p(c)
q(z, c|x)

dz

q(c|x)q(z|x) log

(cid:20)

q(c|x)q(z|x)

log

p(x|z)p(c|z)p(z)
q(c|x)q(z|x)

dz

+ log

(cid:21)

dz

p(c|z)
q(c|x)

p(x|z)p(z)
q(z|x)
(cid:90)

(cid:88)

c
(cid:88)

c
(cid:88)

c

=

=

=

(cid:90)

z
(cid:90)

z
(cid:90)

z
(cid:90)

z
(cid:90)

z

=

q(z|x) log

dz −

q(z|x)

q(c|x) log

(cid:88)

c

q(c|x)
p(c|z)

dz

p(x|z)p(z)
q(z|x)

p(x|z)p(z)
q(z|x)

z
(cid:90)

z

=

q(z|x) log

dz −

q(z|x)DKL(q(c|x)||p(c|z))dz (18)

In Equation 18, the ﬁrst term does not depend on c and the second term is non-
negative. Thus, maximizing the lower bound LELBO(x) with respect to q(c|x) requires
that DKL(q(c|x)||p(c|z)) = 0. Thus, we have

where ν is a constant.

Since (cid:80)

c q(c|x) = 1 and (cid:80)

c p(c|z) = 1, we have:

q(c|x)
p(c|z)

= ν

q(c|x)
p(c|z)

= 1

Taking the expectation on both sides, we can obtain:

q(c|x) = Eq(z|x)[p(c|z)]

Appendix B

Lemma 1 Given two multivariate Gaussian distributions q(z) = N (z; ˜µ, ˜σ2I) and
p(z) = N (z; µ, σ2I), we have:

(cid:90)

q(z) log p(z) dz =

log (2πσ2

j ) −

J
(cid:88)

j=1

−

1
2

˜σ2
j
2σ2
j

−

(˜µj − µj)2
2σ2
j

(19)

where µj, σj, ˜µj and ˜σj simply denote the jth element of µ, σ, ˜µ and ˜σ, respectively,
and J is the dimensionality of z.

Proof (of Lemma 1).

(cid:90)

(cid:90)

q(z) log p(z) dz =

N (z; ˜µ, ˜σ2I) log N (z; µ, σ2I) dz

(cid:90) J
(cid:89)

=

(cid:113)

2π˜σ2
j

j=1

exp(−

(zj − ˜µj)2
2˜σ2
j

) log

1

(cid:113)

2πσ2
j

exp(−

(zj − µj)2
2σ2
j



 dz
)

exp(−

(zj − ˜µj)2
2˜σ2
j

(cid:113)

2π˜σ2
j

) log



(cid:113)

exp(−

1

2πσ2
j



 dzj
)

(zj − µj)2
2σ2
j

J
(cid:89)

j=1







1

1

1

J
(cid:88)

(cid:90)

=

j=1

J
(cid:88)

(cid:90)

=

j=1

J
(cid:88)

j=1

=

−

1
2

=C −

=C −

=C −

=

J
(cid:88)

j=1

−

1
2

(cid:90)

(cid:90)

(cid:90)

(cid:34)

˜σ2
j
σ2
j

˜σ2
j
σ2
j
˜σ2
j
σ2
j
˜σ2
j
σ2
j

exp(−

(cid:113)

2π˜σ2
j

log(2πσ2

j ) −

(zj − ˜µj)2
2˜σ2
j

(cid:20)

)

−

1
2

(cid:21)
log(2πσ2
j )

(cid:90)

dzj −

1

(cid:113)

2π˜σ2
j

exp(−

(zj − ˜µj)2
2˜σ2
j

)

(zj − µj)2
2σ2
j

dzj

(cid:90)

1

(cid:113)

2π˜σ2
j

exp(−

(zj − ˜µj)2
2˜σ2
j

)

(zj − ˜µj)2 + 2(zj − ˜µj)(˜µj − µj) + (˜µj − µj)2
2˜σ2
j

˜σ2
j
σ2
j

dzj

1

(cid:113)

2π˜σ2
j

exp(−

(zj − ˜µj)2
2˜σ2
j

)

(zj − ˜µj)2
2˜σ2
j

(cid:90)

dzj −

1

(cid:113)

2π˜σ2
j

exp(−

(zj − ˜µj)2
2˜σ2
j

)

(˜µj − µj)2
2σ2
j

dzj

exp(−

dxj −

x2
j
2

x2
j
2

)

(˜µj − µj)2
2σ2
j

1
√
2π

1
√
2π

xj
2

=C −

(−

) d(exp(−

)) −

x2
j
2

(˜µj − µj)2
2σ2
j

log (2πσ2

j ) −

˜σ2
j
2σ2
j

−

(˜µj − µj)2
2σ2
j

where C denotes (cid:80)J

j=1 − 1

2 log(2πσ2

j ) for simplicity.

1
√
2π

xj
2

(−

) exp(−

∞

x2
j
2

(cid:12)
(cid:12)
)
(cid:12)

−∞

(cid:90)

−

1
√
2π

exp(−

) d(−

x2
j
2

(cid:35)
)

−

xj
2

(˜µj − µj)2
2σ2
j

Appendix C

In this section, we describe how to compute the evidence lower bound of VaDE. Specif-
ically, the evidence lower bound can be rewritten as:

LELBO(x) =Eq(z,c|x) [log p(x|z)]

+ Eq(z,c|x) [log p(z|c)]
+ Eq(z,c|x) [log p(c)]
− Eq(z,c|x) [log q(z|x)]
− Eq(z,c|x) [log q(c|x)]

(20)

The Equation 20 can be computed by substituting Equation 4, 5, 6, 11 and 16 into
Equation 20 and using Lemma 1 in Appendix B. Speciﬁcally, each item of Equation 20
can be obtained as follows:

• Eq(z,c|x) [log p(x|z)]:

Recall that the observation x can be modeled as either a multivariate Bernoulli
distribution or a multivariate Gaussian distribution. We provide the derivation of
Eq(z,c|x) [log p(x|z)] for the multivariate Bernoulli distribution, and the deriva-
tion for the multivariate Gaussian case can be obtained in a similar way.

Using the SGVB estimator, we can approximate the Eq(z,c|x) [log p(x|z)] as:

Eq(z,c|x) [log p(x|z)] =

log p(x|z(l))

1
L

1
L

L
(cid:88)

l=1

L
(cid:88)

D
(cid:88)

l=1

i=1

=

xi log µ(l)

x i + (1 − xi) log(1 − µ(l)

x i)

(l) = f (z(l); θ), z(l) ∼ N (˜µ, ˜σ2I) and (cid:2)˜µ; log ˜σ2(cid:3) = g(x; φ). L is
where µx
the number of Monte Carlo samples in the SGVB estimator and can be set to 1.
D is the dimensionality of x.

Since the Monte Carlo estimate of the expectation above is non-differentiable
w.r.t φ when z(l) is directly sampled from z ∼ N (˜µ, ˜σ2I), we use the reparam-
eterization trick to obtain a differentiable estimation:

z(l) = ˜µ + ˜σ ◦ (cid:15)(l)

and (cid:15)(l) ∼ N (0, I)

where ◦ denotes the element-wise product.

• Eq(z,c|x) [log p(z|c)]:

Eq(z,c|x) [log p(z|c)] =

q(c|x)q(z|x) log p(z|c) dz

(cid:90)

K
(cid:88)

z

c=1

=

q(c|x)

N (z|˜µ, ˜σ2I) log N (z|µc, σ2

c I) dz

K
(cid:88)

c=1

(cid:90)

z

According to Lemma 1 in Appendix B, we have:

Eq(z,c|x) [log p(z|c)] = −

q(c|x)

log(2π) +

K
(cid:88)

c=1





J
2

1
2

J
(cid:88)
(

j=1

log σ2

cj +

J
(cid:88)

j=1

˜σ2
j
σ2
cj

+

J
(cid:88)

j=1

(˜µj − µcj)2
σ2
cj



)



• Eq(z,c|x) [log p(c)]:

Eq(z,c|x)[log p(c)] =

q(z|x)q(c|x) log p(c) dz

q(z|x)

q(c|x) log πc dz

K
(cid:88)

c=1

q(c|x) log πc

(cid:90)

K
(cid:88)

z

c=1

=

=

(cid:90)

z

K
(cid:88)

c=1

(cid:90)

K
(cid:88)

c=1

z
(cid:90)

z

• Eq(z,c|x) [log q(z|x)]:

Eq(z,c|x) [log q(z|x)] =

q(c|x)q(z|x) log q(z|x) dz

=

N (z; ˜µ, ˜σ2I) log N (z; ˜µ, ˜σ2I) dz

According to Lemma 1 in Appendix B, we have:

Eq(z,c|x) [log q(z|x)] = −

log(2π) −

(1 + log ˜σ2
j )

J
2

1
2

J
(cid:88)

j=1

• Eq(z,c|x) [log q(c|x)]:

Eq(z,c|x) [log q(c|x)] =

q(z|x)q(c|x) log q(c|x) dz

(cid:90)

K
(cid:88)

z

c=1

=

=

(cid:90)

z

K
(cid:88)

c=1

q(z|x)

q(c|x) log q(c|x) dz

K
(cid:88)

c=1

q(c|x) log q(c|x)

where µcj, σcj, ˜µj and ˜σj simply denote the jth element of µc, σc, ˜µ and ˜σ described
in Section 3 respectively. J is the dimensionality of z and K is the number of clusters.
For all the above equations, q(c|x) is computed by Appendix A and can be approx-

imated by the SGVB estimator and the reparameterization trick as follows:

q(c|x) = Eq(z|x) [p(c|z)] =

1
L

L
(cid:88)

l=1

p(c)p(z(l)|c)
c(cid:48)=1 p(c(cid:48))p(z(l)|c(cid:48))

(cid:80)K

where z(l) ∼ N (˜µ, ˜σ2I), (cid:2)˜µ; log ˜σ2(cid:3) = g(x; φ), z(l) = ˜µ + ˜σ ◦ (cid:15)(l) and (cid:15)(l) ∼
N (0, I).

7
1
0
2
 
n
u
J
 
8
2
 
 
]

V
C
.
s
c
[
 
 
3
v
8
4
1
5
0
.
1
1
6
1
:
v
i
X
r
a

Variational Deep Embedding:
An Unsupervised and Generative Approach to
Clustering∗

Zhuxi Jiang1, Yin Zheng2, Huachun Tan1, Bangsheng Tang3, Hanning Zhou3
1Beijing Institute of Technology, Beijing, China
2Tencent AI Lab, Shenzhen, China
3Hulu LLC., Beijing, China
{zjiang, tanhc}@bit.edu.cn, yinzheng@tencent.com,
bangsheng.tang@gmail.com, eric.zhou@hulu.com

June 29, 2017

Abstract

Clustering is among the most fundamental tasks in machine learning and artiﬁ-
cial intelligence. In this paper, we propose Variational Deep Embedding (VaDE), a
novel unsupervised generative clustering approach within the framework of Varia-
tional Auto-Encoder (VAE). Speciﬁcally, VaDE models the data generative proce-
dure with a Gaussian Mixture Model (GMM) and a deep neural network (DNN): 1)
the GMM picks a cluster; 2) from which a latent embedding is generated; 3) then
the DNN decodes the latent embedding into an observable. Inference in VaDE is
done in a variational way: a different DNN is used to encode observables to latent
embeddings, so that the evidence lower bound (ELBO) can be optimized using the
Stochastic Gradient Variational Bayes (SGVB) estimator and the reparameteriza-
tion trick. Quantitative comparisons with strong baselines are included in this pa-
per, and experimental results show that VaDE signiﬁcantly outperforms the state-
of-the-art clustering methods on 5 benchmarks from various modalities. Moreover,
by VaDE’s generative nature, we show its capability of generating highly realis-
tic samples for any speciﬁed cluster, without using supervised information during
training.

1

Introduction

Clustering is the process of grouping similar objects together, which is one of the
most fundamental tasks in machine learning and artiﬁcial intelligence. Over the past
decades, a large family of clustering algorithms have been developed and successfully

∗This paper is accepted by IJCAI 2017, http://ijcai-17.org/accepted-papers.html

Figure 1: The diagram of VaDE. The data generative process of VaDE is done as follows: 1) a
cluster is picked from a GMM model; 2) a latent embedding is generated based on the picked
cluster; 3) DNN f (z; θ) decodes the latent embedding into an observable x. A encoder network
g(x; φ) is used to maximize the ELBO of VaDE.

applied in enormous real world tasks Ng et al. [2002]; Xie et al. [2016]; Yang et al.
[2010]; Ye et al. [2008]. Generally speaking, there is a dichotomy of clustering meth-
ods: Similarity-based clustering and Feature-based clustering. Similarity-based clus-
tering builds models upon a distance matrix, which is a N ×N matrix that measures the
distance between each pair of the N samples. One of the most famous similarity-based
clustering methods is Spectral Clustering (SC) Von Luxburg [2007], which leverages
the Laplacian spectra of the distance matrix to reduce dimensionality before clustering.
Similarity-based clustering methods have the advantage that domain-speciﬁc similar-
ity or kernel functions can be easily incorporated into the models. But these methods
suffer scalability issue due to super-quadratic running time for computing spectra.

Different from similarity-based methods, a feature-based method takes a N × D
matrix as input, where N is the number of samples and D is the feature dimension.
One popular feature-based clustering method is K-means, which aims to partition
the samples into K clusters so as to minimize the within-cluster sum of squared er-
rors. Another representative feature-based clustering model is Gaussian Mixture Model
(GMM), which assumes that the data points are generated from a Mixture-of-Gaussians
(MoG), and the parameters of GMM are optimized by the Expectation Maximization
(EM) algorithm. One advantage of GMM over K-means is that a GMM can generate
samples by estimation of data density. Although K-means, GMM and their variants Liu
et al. [2010]; Ye et al. [2008] have been extensively used, learning good representations
most suitable for clustering tasks is left largely unexplored.

Recently, deep learning has achieved widespread success in numerous machine
learning tasks He et al. [2016]; Krizhevsky et al. [2012]; Szegedy et al. [2015]; Zheng

et al. [2014a,b, 2015, 2016], where learning good representations by deep neural net-
works (DNN) lies in the core. Taking a similar approach, it is conceivable to conduct
clustering analysis on good representations, instead of raw data points.
In a recent
work, Deep Embedded Clustering (DEC) Xie et al. [2016] was proposed to simultane-
ously learn feature representations and cluster assignments by deep neural networks.
Although DEC performs well in clustering, similar to K-means, DEC cannot model
the generative process of data, hence is not able to generate samples. Some recent
works, e.g. VAE Kingma and Welling [2014], GAN Goodfellow et al. [2014] , Pixel-
RNN Oord et al. [2016], InfoGAN Chen et al. [2016] and PPGN Nguyen et al. [2016],
have shown that neural networks can be trained to generate meaningful samples. The
motivation of this work is to develop a clustering model based on neural networks that
1) learns good representations that capture the statistical structure of the data, and 2) is
capable of generating samples.

In this paper, we propose a clustering framework, Variational Deep Embedding
(VaDE), that combines VAE Kingma and Welling [2014] and a Gaussian Mixture
Model for clustering tasks. VaDE models the data generative process by a GMM and
a DNN f : 1) a cluster is picked up by the GMM; 2) from which a latent representation
z is sampled; 3) DNN f decodes z to an observation x. Moreover, VaDE is optimized
by using another DNN g to encode observed data x into latent embedding z, so that
the Stochastic Gradient Variational Bayes (SGVB) estimator and the reparameteriza-
tion trick Kingma and Welling [2014] can be used to maximize the evidence lower
bound (ELBO). VaDE generalizes VAE in that a Mixture-of-Gaussians prior replaces
the single Gaussian prior. Hence, VaDE is by design more suitable for clustering tasks1.
Speciﬁcally, the main contributions of the paper are:

• We propose an unsupervised generative clustering framework, VaDE, that com-

bines VAE and GMM together.

• We show how to optimize VaDE by maximizing the ELBO using the SGVB

estimator and the reparameterization trick;

• Experimental results show that VaDE outperforms the state-of-the-art clustering

models on 5 datasets from various modalities by a large margin;

• We show that VaDE can generate highly realistic samples for any speciﬁed clus-

ter, without using supervised information during training.

The diagram of VaDE is illustrated in Figure 1.

2 Related Work

Recently, people ﬁnd that learning good representations plays an important role in
clustering tasks. For example, DEC Xie et al. [2016] was proposed to learn feature
representations and cluster assignments simultaneously by deep neural networks. In

1Although people can use VaDE to do unsupervised feature learning or semi-supervised learning tasks,

we only focus on clustering tasks in this work.

fact, DEC learns a mapping from the observed space to a lower-dimensional latent
space, where it iteratively optimizes the KL divergence to minimize the within-cluster
distance of each cluster. DEC achieved impressive performances on clustering tasks.
However, the feature embedding in DEC is designed speciﬁcally for clustering and fails
to uncover the real underlying structure of the data, which makes the model lack of the
ability to extend itself to other tasks beyond clustering, such as generating samples.

The deep generative models have recently attracted much attention in that they
can capture the data distribution by neural networks, from which unseen samples can
be generated. GAN and VAE are among the most successful deep generative mod-
els in recent years. Both of them are appealing unsupervised generative models, and
their variants have been extensively studied and applied in various tasks such as semi-
supervised classiﬁcation Abbasnejad et al. [2016]; Kingma et al. [2014]; Maaløe et
al. [2016]; Makhzani et al. [2016]; Salimans et al. [2016], clustering Makhzani et al.
[2016] and image generation Dosovitskiy and Brox [2016]; Radford et al. [2016].

For example, Abbasnejad et al. [2016] proposed to use a mixture of VAEs for semi-
supervised classiﬁcation tasks, where the mixing coefﬁcients of these VAEs are mod-
eled by a Dirichlet process to adapt its capacity to the input data. SB-VAE Nalisnick
and Smyth [2016] also applied Bayesian nonparametric techniques on VAE, which de-
rived a stochastic latent dimensionality by a stick-breaking prior and achieved good
performance on semi-supervised classiﬁcation tasks. VaDE differs with SB-VAE in
that the cluster assignment and the latent representation are jointly considered in the
Gaussian mixture prior, whereas SB-VAE separately models the latent representation
and the class variable, which fails to capture the dependence between them. Addi-
tionally, VaDE does not need the class label during training, while the labels of data
are required by SB-VAE due to its semi-supervised setting. Among the variants of
VAE, Adversarial Auto-Encoder(AAE) Makhzani et al. [2016] can also do unsuper-
vised clustering tasks. Different from VaDE, AAE uses GAN to match the aggregated
posterior with the prior of VAE, which is much more complex than VaDE on the train-
ing procedure. We will compare AAE with VaDE in the experiments part.

Similar to VaDE, Nalisnick et al. [2016] proposed DLGMM to combine VAE and
GMM together. The crucial difference, however, is that VaDE uses a mixture of Gaus-
sian prior to replace the single Gaussian prior of VAE, which is suitable for clustering
tasks by nature, while DLGMM uses a mixture of Gaussian distribution as the approxi-
mate posterior of VAE and does not model the class variable. Hence, VaDE generalizes
VAE to clustering tasks, whereas DLGMM is used to improve the capacity of the orig-
inal VAE and is not suitable for clustering tasks by design. The recently proposed
GM-CVAE Shu et al. [2016] also combines VAE with GMM together. However, the
GMM in GM-CVAE is used to model the transitions between video frames, which is
the main difference with VaDE.

3 Variational Deep Embedding

In this section, we describe Variational Deep Embedding (VaDE), a model for proba-
bilistic clustering problem within the framework of Variational Auto-Encoder (VAE).

3.1 The Generative Process

Since VaDE is a kind of unsupervised generative approach to clustering, we herein ﬁrst
describe the generative process of VaDE. Speciﬁcally, suppose there are K clusters, an
observed sample x ∈ RD is generated by the following process:

1. Choose a cluster c ∼ Cat(π)
2. Choose a latent vector z ∼ N (cid:0)µc, σ2

c I(cid:1)

3. Choose a sample x:

(a) If x is binary

i. Compute the expectation vector µx

µx = f (z; θ)

ii. Choose a sample x ∼ Ber(µx)

(b) If x is real-valued

i. Compute µx and σ2
x

[µx; log σ2
x] = f (z; θ)
xI(cid:1)
ii. Choose a sample x ∼ N (cid:0)µx, σ2

where K is a predeﬁned parameter, πk is the prior probability for cluster k, π ∈ RK
+ ,
1 = (cid:80)K
k=1 πk, Cat(π) is the categorical distribution parametrized by π, µc and σ2
c
are the mean and the variance of the Gaussian distribution corresponding to cluster c, I
is an identity matrix, f (z; θ) is a neural network whose input is z and is parametrized
by θ, Ber(µx) and N (µx, σ2
x) are multivariate Bernoulli distribution and Gaussian
distribution parametrized by µx and µx, σx, respectively. The generative process is
depicted in Figure 1.

According to the generative process above, the joint probability p(x, z, c) can be

factorized as:

p(x, z, c) = p(x|z)p(z|c)p(c),

since x and c are independent conditioned on z. And the probabilities are deﬁned as:

p(c) = Cat(c|π)
p(z|c) = N (cid:0)z|µc, σ2
p(x|z) = Ber(x|µx)

c I(cid:1)
or N (x|µx, σ2

xI)

3.2 Variational Lower Bound

A VaDE instance is tuned to maximize the likelihood of the given data points. Given
the generative process in Section 3.1, by using Jensen’s inequality, the log-likelihood
of VaDE can be written as:

log p(x) = log

p(x, z, c)dz

(cid:90)

(cid:88)

z

c

(1)

(2)

(3)

(4)

(5)

(6)

≥ Eq(z,c|x)[log

] = LELBO(x)

(7)

p(x, z, c)
q(z, c|x)

where LELBO is the evidence lower bound (ELBO), q(z, c|x) is the variational posterior
to approximate the true posterior p(z, c|x). In VaDE, we assume q(z, c|x) to be a mean-
ﬁeld distribution and can be factorized as:

q(z, c|x) = q(z|x)q(c|x).

(8)

Then, according to Equation 3 and 8, the LELBO(x) in Equation 7 can be rewritten

as:

LELBO(x) = Eq(z,c|x)

log

(cid:20)

(cid:21)

p(x, z, c)
q(z, c|x)

= Eq(z,c|x) [log p(x, z, c) − log q(z, c|x)]
= Eq(z,c|x)[log p(x|z) + log p(z|c)

+ log p(c) − log q(z|x) − log q(c|x)]

In VaDE, similar to VAE, we use a neural network g to model q(z|x):

[˜µ; log ˜σ2] = g(x; φ)

q(z|x) = N (z; ˜µ, ˜σ2I)

where φ is the parameter of network g.

By substituting the terms in Equation 9 with Equations 4, 5, 6 and 11, and using
the SGVB estimator and the reparameterization trick, the LELBO(x) can be rewritten
as: 2

LELBO(x) =

xi log µ(l)

x |i + (1 − xi) log(1 − µ(l)

x |i)

L
(cid:88)

D
(cid:88)

l=1

i=1

1
L

−

1
2

K
(cid:88)

J
(cid:88)

γc

c=1

j=1

(log σ2

c |j +

˜σ2|j
σ2
c |j

+

( ˜µ|j − µc|j)2
σ2
c |j

)

+

γc log

+

(1 + log ˜σ2|j)

K
(cid:88)

c=1

πc
γc

1
2

J
(cid:88)

j=1

where L is the number of Monte Carlo samples in the SGVB estimator, D is the
dimensionality of x and µ(l)
x , xi is the ith element of x, J is the dimensionality of µc,
c , ˜µ and ˜σ2, and ∗|j denotes the jth element of ∗, K is the number of clusters, πc is
σ2
the prior probability of cluster c, and γc denotes q(c|x) for simplicity.

In Equation 12, we compute µ(l)

x as

µ(l)

x = f (z(l); θ),

2This is the case when the observation x is binary. For the real-valued situation, the ELBO can be

obtained in a similar way.

(9)

(10)

(11)

(12)

(13)

where z(l) is the lth sample from q(z|x) by Equation 11 to produce the Monte Carlo
samples. According to the reparameterization trick, z(l) is obtained by

z(l) = ˜µ + ˜σ ◦ (cid:15)(l),

(14)

where (cid:15)(l) ∼ N (0, I), ◦ is element-wise multiplication, and ˜µ, ˜σ are derived by Equa-
tion 10.

We now describe how to formulate γc (cid:44) q(c|x) in Equation 12 to maximize the

ELBO. Speciﬁcally, LELBO(x) can be rewritten as:

LELBO(x) = Eq(z,c|x)

log

=

q(c|x)q(z|x)

log

(cid:20)

(cid:20)

(cid:21)

p(x, z, c)
q(z, c|x)

p(x|z)p(z)
q(z|x)
(cid:90)

z

p(x|z)p(z)
q(z|x)

(cid:88)

c

(cid:90)

z
(cid:90)

z

+ log

(cid:21)

dz

p(c|z)
q(c|x)

=

q(z|x) log

dz −

q(z|x)DKL(q(c|x)||p(c|z))dz

(15)

In Equation 15, the ﬁrst term has no relationship with c and the second term is
non-negative. Hence, to maximize LELBO(x), DKL(q(c|x)||p(c|z)) ≡ 0 should be
satisﬁed. As a result, we use the following equation to compute q(c|x) in VaDE:

q(c|x) = p(c|z) ≡

p(c)p(z|c)
c(cid:48)=1 p(c(cid:48))p(z|c(cid:48))

(cid:80)K

(16)

By using Equation 16, the information loss induced by the mean-ﬁeld approxima-
tion can be mitigated, since p(c|z) captures the relationship between c and z.
It is
worth noting that p(c|z) is only an approximation to q(c|x), and we ﬁnd it works well
in practice3.

Once the training is done by maximizing the ELBO w.r.t the parameters of {π, µc, σc, θ, φ},

c ∈ {1, · · · , K}, a latent representation z can be extracted for each observed sample
x by Equation 10 and Equation 11, and the clustering assignments can be obtained by
Equation 16.

3.3 Understanding the ELBO of VaDE

This section, we provide some intuitions of the ELBO of VaDE. More speciﬁcally, the
ELBO in Equation 7 can be further rewritten as:

LELBO(x) = Eq(z,c|x)[log p(x|z)] − DKL(q(z, c|x)||p(z, c))

(17)

The ﬁrst term in Equation 17 is the reconstruction term, which encourages VaDE to
explain the dataset well. And the second term is the Kullback-Leibler divergence from
the Mixture-of-Gaussians (MoG) prior p(z, c) to the variational posterior q(z, c|x),
which regularizes the latent embedding z to lie on a MoG manifold.

3We approximate q(c|x) by: 1) sampling a z(i) ∼ q(z|x); 2) computing q(c|x) = p(c|z(i)) according

to Equation 16

Figure 2: Clustering accuracy over number of epochs during training on MNIST. We also illus-
trate the best performances of DEC, AAE, LDMGI and GMM. It is better to view the ﬁgure in
color.

To demonstrate the importance of the KL term in Equation 17, we train an Auto-
Encoder (AE) with the same network architecture as VaDE ﬁrst, and then apply GMM
on the latent representations from the learned AE, since a VaDE model without the KL
term is almost equivalent to an AE. We refer to this model as AE+GMM. We also show
the performance of using GMM directly on the observed space (GMM), using VAE on
the observed space and then using GMM on the latent space from VAE (VAE+GMM)4,
as well as the performances of LDMGI Yang et al. [2010], AAE Makhzani et al. [2016]
and DEC Xie et al. [2016], in Figure 2. The fact that VaDE outperforms AE+GMM
(without KL term) and VAE+GMM signiﬁcantly conﬁrms the importance of the regu-
larization term and the advantage of jointly optimizing VAE and GMM by VaDE. We
also present the illustrations of clusters and the way they are changed w.r.t. training
epochs on MNIST dataset in Figure 3, where we map the latent representations z into
2D space by t-SNE Maaten and Hinton [2008].

4By doing this, VAE and GMM are optimized separately.

(a) Epoch 0 (11.35%)

(b) Epoch 1 (55.63%)

(c) Epoch 5 (72.40%)

(d) Epoch 50 (84.59%)

(e) Epoch 120 (90.76%)

(f) Epoch End (94.46%)

Figure 3: The illustration about how data is clustered in the latent space learned by VaDE during
training on MNIST. Different colors indicate different ground-truth classes and the clustering
accuracy at the corresponding epoch is reported in the bracket. It is clear to see that the latent
representations become more and more suitable for clustering during training, which can also be
proved by the increasing clustering accuracy.

4 Experiments

In this section, we evaluate the performance of VaDE on 5 benchmarks from differ-
ent modalities: MNIST LeCun et al. [1998], HHAR Stisen et al. [2015], Reuters-
10K Lewis et al. [2004], Reuters Lewis et al. [2004] and STL-10 Coates et al. [2011].
We provide quantitative comparisons of VaDE with other clustering methods includ-
ing GMM, AE+GMM, VAE+GMM, LDGMI Yang et al. [2010], AAE Makhzani et
al. [2016] and the strong baseline DEC Xie et al. [2016]. We use the same net-
work architecture as DEC for a fair comparison. The experimental results show that
VaDE achieves the state-of-the-art performance on all these benchmarks. Addition-
ally, we also provide quantitatively comparisons with other variants of VAE on the
discriminative quality of the latent representations. The code of VaDE is available at
https://github.com/slim1017/VaDE.

4.1 Datasets Description

The following datasets are used in our empirical experiments.

Input Dim # Clusters

Dataset
MNIST
HHAR
REUTERS-10K
REUTERS
STL-10

# Samples
70000
10299
10000
685071
13000

784
561
2000
2000
2048

10
6
4
4
10

Table 1: Datasets statistics

• MNIST: The MNIST dataset consists of 70000 handwritten digits. The images
are centered and of size 28 by 28 pixels. We reshaped each image to a 784-
dimensional vector.

• HHAR: The Heterogeneity Human Activity Recognition (HHAR) dataset con-
tains 10299 sensor records from smart phones and smart watches. All samples
are partitioned into 6 categories of human activities and each sample is of 561
dimensions.

• REUTERS: There are around 810000 English news stories labeled with a cate-
gory tree in original Reuters dataset. Following DEC, we used 4 root categories:
corporate/industrial, government/social, markets, and economics as labels and
discarded all documents with multiple labels, which results in a 685071-article
dataset. We computed tf-idf features on the 2000 most frequent words to repre-
sent all articles. Similar to DEC, a random subset of 10000 documents is sam-
pled, which is referred to as Reuters-10K, since some spectral clustering methods
(e.g. LDMGI) cannot scale to full Reuters dataset.

• STL-10: The STL-10 dataset consists of color images of 96-by-96 pixel size.
There are 10 classes with 1300 examples each. Since clustering directly from
raw pixels of high resolution images is rather difﬁcult, we extracted features of
images of STL-10 by ResNet-50 He et al. [2016], which were then used to test
the performance of VaDE and all baselines. More speciﬁcally, we applied a 3 × 3
average pooling over the last feature map of ResNet-50 and the dimensionality
of the features is 2048.

4.2 Experimental Setup

As mentioned before, the same network architecture as DEC is adopted by VaDE for
a fair comparison. Speciﬁcally, the architectures of f and g in Equation 1 and Equa-
tion 10 are 10-2000-500-500-D and D-500-500-2000-10, respectively, where D is the
input dimensionality. All layers are fully connected. Adam optimizer Kingma and Ba
[2015] is used to maximize the ELBO of Equation 9, and the mini-batch size is 100.
The learning rate for MNIST, HHAR, Reuters-10K and STL-10 is 0.002 and decreases
every 10 epochs with a decay rate of 0.9, and the learning rate for Reuters is 0.0005
with a decay rate of 0.5 for every epoch. As for the generative process in Section 3.1,

MNIST HHAR REUTERS-10K REUTERS
Method
60.34
53.73
54.72
GMM
77.67
82.18
70.13
AE+GMM
68.02
VAE+GMM 72.94
69.56
84.09†
63.43
65.62
LDMGI
83.77
83.48
69.82
AAE
84.30†
79.86
74.32
DEC
94.46
84.46
79.83
VaDE
†: Taken from Xie et al. [2016].

55.81
70.98
60.89
N/A
75.12
75.63†
79.38

STL-10
72.44
79.83
78.86
79.22
80.01
80.62
84.45

Table 2: Clustering accuracy (%) performance comparison on all datasets.

k=3
Method
18.43
VAE
DLGMM 9.14
7.64
SB-VAE
2.20
VaDE

k=5
15.69
8.38
7.25
2.14

k=10
14.19
8.42
7.31
2.22

Table 3: MNIST test error-rate (%) for kNN on latent space.

the multivariate Bernoulli distribution is used for MNIST dataset, and the multivariate
Gaussian distribution is used for the others. The number of clusters is ﬁxed to the num-
ber of classes for each dataset, similar to DEC. We will vary the number of clusters in
Section 4.6.

Similar to other VAE-based models Kingma and Salimans [2016]; Sønderby et al.
[2016], VaDE suffers from the problem that the reconstruction term in Equation 17
would be so weak in the beginning of training that the model might get stuck in an
undesirable local minima or saddle point, from which it is hard to escape.
In this
work, pretraining is used to avoid this problem. Speciﬁcally, we use a Stacked Auto-
Encoder to pretrain the networks f and g. Then all data points are projected into the
latent space z by the pretrained network g, where a GMM is applied to initialize the
parameters of {π, µc, σc}, c ∈ {1, · · · , K}. In practice, few epochs of pretraining are
enough to provide a good initialization of VaDE. We ﬁnd that VaDE is not sensitive to
hyperparameters after pretraining. Hence, we did not spend a lot of effort to tune them.

4.3 Quantitative Comparison

Following DEC, the performance of VaDE is measured by unsupervised clustering
accuracy (ACC), which is deﬁned as:

(cid:80)N

i=1

1{li = m(ci)}

ACC = max
m∈M

N

where N is the total number of samples, li is the ground-truth label, ci is the cluster as-
signment obtained by the model, and M is the set of all possible one-to-one mappings

between cluster assignments and labels. The best mapping can be obtained by using
the KuhnMunkres algorithm Munkres [1957]. Similar to DEC, we perform 10 random
restarts when initializing all clustering models and pick the result with the best objec-
tive value. As for LDMGI, AAE and DEC, we use the same conﬁgurations as their
original papers. Table 2 compares the performance of VaDE with other baselines over
all datasets. It can be seen that VaDE outperforms all these baselines by a large mar-
gin on all datasets. Speciﬁcally, on MNIST, HHAR, Reuters-10K, Reuters and STL-10
dataset, VaDE achieves ACC of 94.46%, 84.46%, 79.83%, 79.38% and 84.45%, which
outperforms DEC with a relative increase ratio of 12.05%, 5.76%, 7.41%, 4.96% and
4.75%, respectively.

We also compare VaDE with SB-VAE Nalisnick and Smyth [2016] and DLGMM Nal-

isnick et al. [2016] on the discriminative power of the latent representations, since these
two baselines cannot do clustering tasks. Following SB-VAE, the discriminative pow-
ers of the models’ latent representations are assessed by running a k-Nearest Neighbors
classiﬁer (kNN) on the latent representations of MNIST. Table 3 shows the error rate of
the kNN classiﬁer on the latent representations. It can be seen that VaDE outperforms
SB-VAE and DLGMM signiﬁcantly5.

Note that although VaDE can learn discriminative representations of samples, the
training of VaDE is in a totally unsupervised way. Hence, we did not compare VaDE
with other supervised models.

4.4 Generating Samples by VaDE

One major advantage of VaDE over DEC Xie et al. [2016] is that it is by nature a
generative clustering model and can generate highly realistic samples for any speciﬁed
cluster (class). In this section, we provide some qualitative comparisons on generat-
ing samples among VaDE, GMM, VAE and the state-of-art generative method Info-
GAN Chen et al. [2016].

Figure 4 illustrates the generated samples for class 0 to 9 of MNIST by GMM, VAE,
InfoGAN and VaDE, respectively. It can be seen that the digits generated by VaDE are
smooth and diverse. Note that the classes of the samples from VAE cannot be speciﬁed.
We can also see that the performance of VaDE is comparable with InfoGAN.

4.5 Visualization of Learned Embeddings

In this section, we visualize the learned representations of VAE, DEC and VaDE on
MNIST dataset. To this end, we use t-SNE Maaten and Hinton [2008] to reduce the
dimensionality of the latent representation z from 10 to 2, and plot 2000 randomly sam-
pled digits in Figure 5. The ﬁrst row of Figure 5 illustrates the ground-truth labels for
each digit, where different colors indicate different labels. The second row of Figure 5
demonstrates the clustering results, where correctly clustered samples are colored with
green and incorrect ones with red.

5We use the same network architecture for VaDE, SB-VAE in Table 3 for fair comparisons. Since there
is no code available for DLGMM, we take the number of DLGMM directly from Nalisnick et al. [2016].
Note that Nalisnick and Smyth [2016] has already shown that the performance of SB-VAE is comparable to
DLGMM.

(a) GMM

(b) VAE

(c) InfoGAN

(d) VaDE

Figure 4: The digits generated by GMM, VAE, InfoGAN and VaDE. Except (b), digits in the
same row come from the same cluster.

From Figure 5 we can see that the original VAE which used a single Gaussian prior
does not perform well in clustering tasks.
It can also be observed that the embed-
dings learned by VaDE are better than those by VAE and DEC, since the number of
incorrectly clustered samples is smaller. Furthermore, incorrectly clustered samples by
VaDE are mostly located at the border of each cluster, where confusing samples usu-
ally appear. In contrast, a lot of the incorrectly clustered samples of DEC appear in the
interior of the clusters, which indicates that DEC fails to preserve the inherent structure
of the data. Some mistakes made by DEC and VaDE are also marked in Figure 5.

4.6 The Impact of the Number of Clusters

So far, the number of clusters for VaDE is set to the number of classes for each dataset,
which is a prior knowledge. To demonstrate VaDE’s representation power as an un-
supervised clustering model, we deliberately choose different numbers of clusters K.
Each row in Figure 6 illustrates the samples from a cluster grouped by VaDE on MNIST
dataset, where K is set to 7 and 14 in Figure 6(a) and Figure 6(b), respectively. We

Figure 5: Visualization of the embeddings learned by VAE, DEC and VaDE on MNIST, re-
spectively. The ﬁrst row illustrates the ground-truth labels for each digit, where different colors
indicate different labels. The second row demonstrates the clustering results, where correctly
clustered samples are colored with green and, incorrect ones with red. GT:4 means the ground-
truth label of the digit is 4, DEC:4 means DEC assigns the digit to the cluster of 4, and VaDE:4
denotes the assignment by VaDE is 4, and so on. It is better to view the ﬁgure in color.

can see that, if K is smaller than the number of classes, digits with similar appearances
will be clustered together, such as 9 and 4, 3 and 8 in Figure 6(a). On the other hand, if
K is larger than the number of classes, some digits will fall into sub-classes by VaDE,
such as the fatter 0 and thinner 0, and the upright 1 and oblique 1 in Figure 6(b).

5 Conclusion

In this paper, we proposed Variational Deep Embedding (VaDE) which embeds the
probabilistic clustering problems into a Variational Auto-Encoder (VAE) framework.
VaDE models the data generative procedure by a GMM model and a neural network,
and is optimized by maximizing the evidence lower bound (ELBO) of the log-likelihood
of data by the SGVB estimator and the reparameterization trick. We compared the
clustering performance of VaDE with strong baselines on 5 benchmarks from different
modalities, and the experimental results showed that VaDE outperforms the state-of-
the-art methods by a large margin. We also showed that VaDE could generate highly
realistic samples conditioned on cluster information without using any supervised in-
formation during training. Note that although we use a MoG prior for VaDE in this

(a) 7 clusters

(b) 14 clusters

Figure 6: Clustering MNIST with different numbers of clusters. We illustrate samples belonging
to each cluster by rows.

paper, other mixture models can also be adopted in this framework ﬂexibly, which will
be our future work.

Acknowledgments

We thank the School of Mechanical Engineering of BIT (Beijing Institute of Tech-
nology) and Collaborative Innovation Center of Electric Vehicles in Beijing for their
support. This work was supported by the National Natural Science Foundation of
China (61620106002, 61271376). We also thank the anonymous reviewers.

References

Ehsan Abbasnejad, Anthony Dick, and Anton van den Hengel.

Inﬁnite variational
autoencoder for semi-supervised learning. arXiv preprint arXiv:1611.07800, 2016.

Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter
Abbeel. Infogan: Interpretable representation learning by information maximizing
generative adversarial nets. In NIPS, 2016.

Adam Coates, Andrew Y Ng, and Honglak Lee. An analysis of single-layer networks in
unsupervised feature learning. In International Conference on Artiﬁcial Intelligence
and Statistics, 2011.

Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity

metrics based on deep networks. In NIPS, 2016.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In
NIPS, 2014.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for

image recognition. In CVPR, 2016.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In

ICLR, 2015.

2014.

Diederik P Kingma and Tim Salimans. Improving variational autoencoders with in-

verse autoregressive ﬂow. In NIPS, 2016.

Diederik P Kingma and Max Welling. Auto-encoding variational bayes.

In ICLR,

Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, and Max Welling. Semi-

supervised learning with deep generative models. In NIPS, 2014.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with

deep convolutional neural networks. In NIPS, 2012.

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learn-

ing applied to document recognition. Proceedings of the IEEE, 1998.

David D Lewis, Yiming Yang, Tony G Rose, and Fan Li. Rcv1: A new benchmark
collection for text categorization research. Journal of machine learning research,
2004.

Jialu Liu, Deng Cai, and Xiaofei He. Gaussian mixture model with local consistency.

In AAAI, 2010.

Lars Maaløe, Casper Kaae Sønderby, Søren Kaae Sønderby, and Ole Winther. Auxil-

iary deep generative models. In ICML, 2016.

Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of

Machine Learning Research, 2008.

Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan

Frey. Adversarial autoencoders. In NIPS, 2016.

James Munkres. Algorithms for the assignment and transportation problems. Journal

of the society for industrial and applied mathematics, 1957.

Eric Nalisnick and Padhraic Smyth. Stick-breaking variational autoencoders. arXiv

preprint arXiv:1605.06197, 2016.

Eric Nalisnick, Lars Hertel, and Padhraic Smyth. Approximate inference for deep

latent gaussian mixtures. 2016.

Andrew Y Ng, Michael I Jordan, Yair Weiss, et al. On spectral clustering: Analysis

and an algorithm. In NIPS, 2002.

Anh Nguyen, Jason Yosinski, Yoshua Bengio, Alexey Dosovitskiy, and Jeff Clune.
Plug & play generative networks: Conditional iterative generation of images in latent
space. arXiv preprint arXiv:1612.00005, 2016.

Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neu-

ral networks. In ICML, 2016.

Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning

with deep convolutional generative adversarial networks. In ICLR, 2016.

Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and

Xi Chen. Improved techniques for training gans. In NIPS, 2016.

Rui Shu, James Brofos, Frank Zhang, Hung Hai Bui, Mohammad Ghavamzadeh, and
Mykel Kochenderfer. Stochastic video prediction with conditional density estima-
tion. In ECCV Workshop on Action and Anticipation for Visual Learning, 2016.

Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole

Winther. Ladder variational autoencoders. In NIPS, 2016.

Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow, Mikkel Baun
Kjærgaard, Anind Dey, Tobias Sonne, and Mads Møller Jensen. Smart devices are
different: Assessing and mitigatingmobile sensing heterogeneities for activity recog-
nition. In Proceedings of the 13th ACM Conference on Embedded Networked Sensor
Systems, 2015.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going
In Proceedings of the IEEE Conference on Computer
deeper with convolutions.
Vision and Pattern Recognition, pages 1–9, 2015.

Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 2007.

Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clus-

tering analysis. In ICML, 2016.

Yi Yang, Dong Xu, Feiping Nie, Shuicheng Yan, and Yueting Zhuang. Image clustering
using local discriminant models and global integration. IEEE Transactions on Image
Processing, 2010.

Jieping Ye, Zheng Zhao, and Mingrui Wu. Discriminative k-means for clustering. In

NIPS, 2008.

Yin Zheng, Richard S Zemel, Yu-Jin Zhang, and Hugo Larochelle. A neural autore-
gressive approach to attention-based recognition. International Journal of Computer
Vision, 113(1):67–79, 2014.

Yin Zheng, Yu-Jin Zhang, and H. Larochelle. Topic modeling of multimodal data: An
autoregressive approach. In Computer Vision and Pattern Recognition (CVPR), 2014
IEEE Conference on, pages 1370–1377, June 2014.

Y. Zheng, Yu-Jin Zhang, and H. Larochelle. A deep and autoregressive approach for
topic modeling of multimodal data. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, PP(99):1–1, 2015.

Yin Zheng, Bangsheng Tang, Wenkui Ding, and Hanning Zhou. A neural autoregres-
In Proceedings of the 33nd International

sive approach to collaborative ﬁltering.
Conference on Machine Learning, pages 764–773, 2016.

Appendix A

In this section, we provide the derivation of q(c|x) = Eq(z|x) [p(c|z)].

The evidence lower bound LELBO(x) can be rewritten as:

LELBO(x) = Eq(z,c|x)

log

(cid:20)

(cid:21)

p(x, z, c)
q(z, c|x)

q(z, c|x) log

p(x|z)p(z|c)p(c)
q(z, c|x)

dz

q(c|x)q(z|x) log

(cid:20)

q(c|x)q(z|x)

log

p(x|z)p(c|z)p(z)
q(c|x)q(z|x)

dz

+ log

(cid:21)

dz

p(c|z)
q(c|x)

p(x|z)p(z)
q(z|x)
(cid:90)

(cid:88)

c
(cid:88)

c
(cid:88)

c

=

=

=

(cid:90)

z
(cid:90)

z
(cid:90)

z
(cid:90)

z
(cid:90)

z

=

q(z|x) log

dz −

q(z|x)

q(c|x) log

(cid:88)

c

q(c|x)
p(c|z)

dz

p(x|z)p(z)
q(z|x)

p(x|z)p(z)
q(z|x)

z
(cid:90)

z

=

q(z|x) log

dz −

q(z|x)DKL(q(c|x)||p(c|z))dz (18)

In Equation 18, the ﬁrst term does not depend on c and the second term is non-
negative. Thus, maximizing the lower bound LELBO(x) with respect to q(c|x) requires
that DKL(q(c|x)||p(c|z)) = 0. Thus, we have

where ν is a constant.

Since (cid:80)

c q(c|x) = 1 and (cid:80)

c p(c|z) = 1, we have:

q(c|x)
p(c|z)

= ν

q(c|x)
p(c|z)

= 1

Taking the expectation on both sides, we can obtain:

q(c|x) = Eq(z|x)[p(c|z)]

Appendix B

Lemma 1 Given two multivariate Gaussian distributions q(z) = N (z; ˜µ, ˜σ2I) and
p(z) = N (z; µ, σ2I), we have:

(cid:90)

q(z) log p(z) dz =

log (2πσ2

j ) −

J
(cid:88)

j=1

−

1
2

˜σ2
j
2σ2
j

−

(˜µj − µj)2
2σ2
j

(19)

where µj, σj, ˜µj and ˜σj simply denote the jth element of µ, σ, ˜µ and ˜σ, respectively,
and J is the dimensionality of z.

Proof (of Lemma 1).

(cid:90)

(cid:90)

q(z) log p(z) dz =

N (z; ˜µ, ˜σ2I) log N (z; µ, σ2I) dz

(cid:90) J
(cid:89)

=

(cid:113)

2π˜σ2
j

j=1

exp(−

(zj − ˜µj)2
2˜σ2
j

) log

1

(cid:113)

2πσ2
j

exp(−

(zj − µj)2
2σ2
j



 dz
)

exp(−

(zj − ˜µj)2
2˜σ2
j

(cid:113)

2π˜σ2
j

) log



(cid:113)

exp(−

1

2πσ2
j



 dzj
)

(zj − µj)2
2σ2
j

J
(cid:89)

j=1







1

1

1

J
(cid:88)

(cid:90)

=

j=1

J
(cid:88)

(cid:90)

=

j=1

J
(cid:88)

j=1

=

−

1
2

=C −

=C −

=C −

=

J
(cid:88)

j=1

−

1
2

(cid:90)

(cid:90)

(cid:90)

(cid:34)

˜σ2
j
σ2
j

˜σ2
j
σ2
j
˜σ2
j
σ2
j
˜σ2
j
σ2
j

exp(−

(cid:113)

2π˜σ2
j

log(2πσ2

j ) −

(zj − ˜µj)2
2˜σ2
j

(cid:20)

)

−

1
2

(cid:21)
log(2πσ2
j )

(cid:90)

dzj −

1

(cid:113)

2π˜σ2
j

exp(−

(zj − ˜µj)2
2˜σ2
j

)

(zj − µj)2
2σ2
j

dzj

(cid:90)

1

(cid:113)

2π˜σ2
j

exp(−

(zj − ˜µj)2
2˜σ2
j

)

(zj − ˜µj)2 + 2(zj − ˜µj)(˜µj − µj) + (˜µj − µj)2
2˜σ2
j

˜σ2
j
σ2
j

dzj

1

(cid:113)

2π˜σ2
j

exp(−

(zj − ˜µj)2
2˜σ2
j

)

(zj − ˜µj)2
2˜σ2
j

(cid:90)

dzj −

1

(cid:113)

2π˜σ2
j

exp(−

(zj − ˜µj)2
2˜σ2
j

)

(˜µj − µj)2
2σ2
j

dzj

exp(−

dxj −

x2
j
2

x2
j
2

)

(˜µj − µj)2
2σ2
j

1
√
2π

1
√
2π

xj
2

=C −

(−

) d(exp(−

)) −

x2
j
2

(˜µj − µj)2
2σ2
j

log (2πσ2

j ) −

˜σ2
j
2σ2
j

−

(˜µj − µj)2
2σ2
j

where C denotes (cid:80)J

j=1 − 1

2 log(2πσ2

j ) for simplicity.

1
√
2π

xj
2

(−

) exp(−

∞

x2
j
2

(cid:12)
(cid:12)
)
(cid:12)

−∞

(cid:90)

−

1
√
2π

exp(−

) d(−

x2
j
2

(cid:35)
)

−

xj
2

(˜µj − µj)2
2σ2
j

Appendix C

In this section, we describe how to compute the evidence lower bound of VaDE. Specif-
ically, the evidence lower bound can be rewritten as:

LELBO(x) =Eq(z,c|x) [log p(x|z)]

+ Eq(z,c|x) [log p(z|c)]
+ Eq(z,c|x) [log p(c)]
− Eq(z,c|x) [log q(z|x)]
− Eq(z,c|x) [log q(c|x)]

(20)

The Equation 20 can be computed by substituting Equation 4, 5, 6, 11 and 16 into
Equation 20 and using Lemma 1 in Appendix B. Speciﬁcally, each item of Equation 20
can be obtained as follows:

• Eq(z,c|x) [log p(x|z)]:

Recall that the observation x can be modeled as either a multivariate Bernoulli
distribution or a multivariate Gaussian distribution. We provide the derivation of
Eq(z,c|x) [log p(x|z)] for the multivariate Bernoulli distribution, and the deriva-
tion for the multivariate Gaussian case can be obtained in a similar way.

Using the SGVB estimator, we can approximate the Eq(z,c|x) [log p(x|z)] as:

Eq(z,c|x) [log p(x|z)] =

log p(x|z(l))

1
L

1
L

L
(cid:88)

l=1

L
(cid:88)

D
(cid:88)

l=1

i=1

=

xi log µ(l)

x i + (1 − xi) log(1 − µ(l)

x i)

(l) = f (z(l); θ), z(l) ∼ N (˜µ, ˜σ2I) and (cid:2)˜µ; log ˜σ2(cid:3) = g(x; φ). L is
where µx
the number of Monte Carlo samples in the SGVB estimator and can be set to 1.
D is the dimensionality of x.

Since the Monte Carlo estimate of the expectation above is non-differentiable
w.r.t φ when z(l) is directly sampled from z ∼ N (˜µ, ˜σ2I), we use the reparam-
eterization trick to obtain a differentiable estimation:

z(l) = ˜µ + ˜σ ◦ (cid:15)(l)

and (cid:15)(l) ∼ N (0, I)

where ◦ denotes the element-wise product.

• Eq(z,c|x) [log p(z|c)]:

Eq(z,c|x) [log p(z|c)] =

q(c|x)q(z|x) log p(z|c) dz

(cid:90)

K
(cid:88)

z

c=1

=

q(c|x)

N (z|˜µ, ˜σ2I) log N (z|µc, σ2

c I) dz

K
(cid:88)

c=1

(cid:90)

z

According to Lemma 1 in Appendix B, we have:

Eq(z,c|x) [log p(z|c)] = −

q(c|x)

log(2π) +

K
(cid:88)

c=1





J
2

1
2

J
(cid:88)
(

j=1

log σ2

cj +

J
(cid:88)

j=1

˜σ2
j
σ2
cj

+

J
(cid:88)

j=1

(˜µj − µcj)2
σ2
cj



)



• Eq(z,c|x) [log p(c)]:

Eq(z,c|x)[log p(c)] =

q(z|x)q(c|x) log p(c) dz

q(z|x)

q(c|x) log πc dz

K
(cid:88)

c=1

q(c|x) log πc

(cid:90)

K
(cid:88)

z

c=1

=

=

(cid:90)

z

K
(cid:88)

c=1

(cid:90)

K
(cid:88)

c=1

z
(cid:90)

z

• Eq(z,c|x) [log q(z|x)]:

Eq(z,c|x) [log q(z|x)] =

q(c|x)q(z|x) log q(z|x) dz

=

N (z; ˜µ, ˜σ2I) log N (z; ˜µ, ˜σ2I) dz

According to Lemma 1 in Appendix B, we have:

Eq(z,c|x) [log q(z|x)] = −

log(2π) −

(1 + log ˜σ2
j )

J
2

1
2

J
(cid:88)

j=1

• Eq(z,c|x) [log q(c|x)]:

Eq(z,c|x) [log q(c|x)] =

q(z|x)q(c|x) log q(c|x) dz

(cid:90)

K
(cid:88)

z

c=1

=

=

(cid:90)

z

K
(cid:88)

c=1

q(z|x)

q(c|x) log q(c|x) dz

K
(cid:88)

c=1

q(c|x) log q(c|x)

where µcj, σcj, ˜µj and ˜σj simply denote the jth element of µc, σc, ˜µ and ˜σ described
in Section 3 respectively. J is the dimensionality of z and K is the number of clusters.
For all the above equations, q(c|x) is computed by Appendix A and can be approx-

imated by the SGVB estimator and the reparameterization trick as follows:

q(c|x) = Eq(z|x) [p(c|z)] =

1
L

L
(cid:88)

l=1

p(c)p(z(l)|c)
c(cid:48)=1 p(c(cid:48))p(z(l)|c(cid:48))

(cid:80)K

where z(l) ∼ N (˜µ, ˜σ2I), (cid:2)˜µ; log ˜σ2(cid:3) = g(x; φ), z(l) = ˜µ + ˜σ ◦ (cid:15)(l) and (cid:15)(l) ∼
N (0, I).

