8
1
0
2
 
b
e
F
 
5
2
 
 
]

G
L
.
s
c
[
 
 
4
v
0
9
6
2
0
.
6
0
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

ENHANCING THE RELIABILITY OF
OUT-OF-DISTRIBUTION IMAGE DETECTION IN
NEURAL NETWORKS

Shiyu Liang
Coordinated Science Lab, Department of ECE
University of Illinois at Urbana-Champaign
sliang26@illinois.edu

R. Srikant
Coordinated Science Lab, Department of ECE
University of Illinois at Urbana-Champaign
rsrikant@illinois.edu

Yixuan Li
Facebook Research
yixuanl@fb.com

ABSTRACT

We consider the problem of detecting out-of-distribution images in neural networks.
We propose ODIN, a simple and effective method that does not require any change
to a pre-trained neural network. Our method is based on the observation that using
temperature scaling and adding small perturbations to the input can separate the
softmax score distributions between in- and out-of-distribution images, allowing
for more effective detection. We show in a series of experiments that ODIN
is compatible with diverse network architectures and datasets. It consistently
outperforms the baseline approach (Hendrycks & Gimpel, 2017) by a large margin,
establishing a new state-of-the-art performance on this task. For example, ODIN
reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet
(applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%.

1

INTRODUCTION

Modern neural networks are known to generalize well when the training and testing data are sampled
from the same distribution (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016;
Cho et al., 2014; Zhang et al., 2017). However, when deploying neural networks in real-world
applications, there is often very little control over the testing data distribution. Recent works
have shown that neural networks tend to make high conﬁdence predictions even for completely
unrecognizable (Nguyen et al., 2015) or irrelevant inputs (Hendrycks & Gimpel, 2017; Szegedy et al.,
2014; Moosavi-Dezfooli et al., 2017). It has been well documented (Amodei et al., 2016) that it is
important for classiﬁers to be aware of uncertainty when shown new kinds of inputs, i.e., out-of-
distribution examples. Therefore, being able to accurately detect out-of-distribution examples can
be practically important for visual recognition tasks (Krizhevsky et al., 2012; Farabet et al., 2013; Ji
et al., 2013).

A seemingly straightforward approach of detecting out-of-distribution images is to enlarge the training
set of both in- and out-of-distribution examples. However, the number of out-of-distribution examples
can be inﬁnitely many, making the re-training approach computationally expensive and intractable.
Moreover, to ensure that a neural network accurately classiﬁes in-distribution samples into correct
classes while correctly detecting out-of-distribution samples, one might need to employ exceedingly
large neural network architectures, which further complicates the training process.

Hendrycks & Gimpel (2017) proposed a baseline method to detect out-of-distribution examples
without further re-training networks. The method is based on an observation that a well-trained neural
network tends to assign higher softmax scores to in-distribution examples than out-of-distribution
examples. In this paper, we go further. We observe that after using temperature scaling in the softmax
function (Hinton et al., 2015; Pereyra et al., 2017) and adding small controlled perturbations to inputs,

1

Published as a conference paper at ICLR 2018

the softmax score gap between in - and out-of-distribution examples is further enlarged. We will
show that the combination of these two techniques (temperature scaling and input perturbation) can
lead to better detection performance. For example, provided with a pre-trained DenseNet (Huang
et al., 2016) on CIFAR-10 dataset (positive samples), we test against images from TinyImageNet
dataset (negative samples). Our method reduces the False Positive Rate (FPR), i.e., the fraction of
misclassiﬁed out-of-distribution samples, from 34.7% to 4.3%, when 95% of in-distribution images
are correctly classiﬁed. We summarize the main contributions of this paper as the following:

•

•

We propose a simple and effective method, ODIN (Out-of-DIstribution detector for Neural net-
works), for detecting out-of-distribution examples in neural networks. Our method does not require
re-training the neural network and is easily implementable on any modern neural architecture.

We test ODIN on state-of-the-art network architectures (e.g., DenseNet (Huang et al., 2016) and
Wide ResNet (Zagoruyko & Komodakis, 2016)) under a diverse set of in- and out-distribution
dataset pairs. We show ODIN can signiﬁcantly improve the detection performance, and consistently
outperforms the state-of-the-art method (Hendrycks & Gimpel, 2017) by a large margin.

We empirically analyze how parameter settings affect the performance, and further provide simple
analysis that provides some intuition behind our method.

•

The outline of this paper is as follows. In Section 2, we present the necessary deﬁnitions and the
problem statement. In Section 3, we introduce ODIN and present performance results in Section 4.
We experimentally analyze the proposed method and provide some justiﬁcation for our method in
Section 5. We summarize the related works and future directions in Section 6 and conclude the paper
in Section 7.

2 PROBLEM STATEMENT

X

In this paper, we consider the problem of distinguishing in- and out-of-distribution images on a pre-
trained neural network. Let PX and QX denote two distinct data distributions deﬁned on the image
. Assume that a neural network f is trained on a dataset drawn from the distribution PX . Thus,
space
we call PX the in-distribution and QX the out-distribution, respectively. In testing, we draw new
images from a mixture distribution PX×Z deﬁned on
, where the conditional probability
distributions PX|Z=0 = PX and PX|Z=1 = QX denote in- and out-distribution respectively. Now
we focus on the following problem: Given an image X drawn from the mixture distribution PX×Z,
can we distinguish whether the image is from in-distribution PX or not?

0, 1
}

X × {

In this paper, we focus on detecting out-of-distribution images. However, it is equally important to
correctly classify an image into the right class if it is an in-distribution image. But this can be easily
done: once it has been detected that an image is in-distribution, we can simply use the original image
and run it through the neural network to classify it. Thus, we do not change the predictions of the
neural network for in-distribution images and only focus on improving the detection performance for
out-of-distribution images.

3 ODIN: OUT-OF-DISTRIBUTION DETECTOR

In this section, we present our method, ODIN, for detecting out-of-distribution samples. The detector
is built on two components: temperature scaling and input preprocessing. We describe the details of
both components below.
Temperature Scaling. Assume that the neural network f = (f1, ..., fN ) is trained to classify N
classes. For each input x, the neural network assigns a label ˆy(x) = arg maxi Si(x; T ) by computing
the softmax output for each class. Speciﬁcally,

Si(x; T ) =

exp (fi(x)/T )
j=1 exp (fj(x)/T )
R+ is the temperature scaling parameter and set to 1 during the training. For a given input
where T
x, we call the maximum softmax probability, i.e., Sˆy(x; T ) = maxi Si(x; T ) the softmax score. In
this paper, we use notations Sˆy(x; T ) and S(x; T ) interchangeably. Prior works have established
the use of temperature scaling to distill the knowledge in neural networks (Hinton et al., 2015) and

(cid:80)N

(1)

∈

,

2

Published as a conference paper at ICLR 2018

−

εsign(

x log Sˆy(x; T )),

calibrate the prediction conﬁdence in classiﬁcation tasks (Guo et al., 2017). As we shall see later,
a good manipulation of temperature T can push the softmax scores of in- and out-of-distribution
images further apart from each other, making the out-of-distribution images distinguishable.
Input Preprocessing. Before feeding the image x into the neural network, we preprocess the input
by adding small perturbations to it. The preprocessed image is given by
˜x = x

−∇
where the parameter ε can be interpreted as the perturbation magnitude. The method is inspired by the
idea in the reference (Goodfellow et al., 2015), where small perturbations are added to decrease the
softmax score for the true label and force the neural network to make a wrong prediction. Here, our
goal and setting are rather different: we aim to increase the softmax score of any given input, without
the need for a class label at all. As we shall see later, the perturbation can have stronger effect on the
in- distribution images than that on out-of-distribution images, making them more separable. Note
that the perturbations can be easily computed by back-propagating the gradient of the cross-entropy
loss w.r.t the input.
Out-of-distribution Detector. The proposed approach works as follows. For each image x, we ﬁrst
calculate the preprocessed image ˜x according to the equation (2). Next, we feed the preprocessed
image ˜x into the neural network, calculate its softmax score S( ˜x; T ) and compare the score to the
threshold δ. We say that the image x is an in-distribution example if the softmax score is above
the threshold and that the image x is an out-of-distribution example, otherwise. Therefore, the
out-of-distribution detector is given by

(2)

g(x; δ, T, ε) =

(cid:26)1
0

if maxi p( ˜x; T )
δ,
if maxi p( ˜x; T ) > δ.

≤

The parameters T, ε and δ are chosen so that the true positive rate (i.e., the fraction of in-distribution
images correctly classiﬁed as in-distribution images) under some out-of-distribution image data set
is 95%. (The choice of the out-of-distribution images to tune the parameters T, ε and δ appears to
be unimportant, as demonstrated in the appendix H.) Having chosen the parameters as above, we
evaluate the performance of our algorithm using various metrics in the next section.

4 EXPERIMENTS

4.1 TRAINING SETUP

In this section, we demonstrate the effectiveness of ODIN on several computer vision benchmark
datasets. We run all experiments with PyTorch1 and we will release the code to reproduce all
experimental results2.

Architectures and training conﬁgurations. We adopt two state-of-the-art neural network architec-
tures, including DenseNet (Huang et al., 2016) and Wide ResNet (Zagoruyko & Komodakis, 2016).
For DenseNet, our model follows the same setup as in (Huang et al., 2016), with depth L = 100,
growth rate k = 12 (Dense-BC) and dropout rate 0. In addition, we evaluate the method on a Wide
ResNet, with depth 28, width 10 (WRN-28-10) and dropout rate 0. Furthermore, in Appendix A.1, we
provide additional experimental results on another Wide ResNet with depth 40, width 4 (WRN-40-4).
The hyper-parameters of neural networks are set identical to the original Wide ResNet (Zagoruyko
& Komodakis, 2016) and DenseNet (Huang et al., 2016) implementations. All neural networks are
trained with stochastic gradient descent with Nesterov momentum (Duchi et al., 2011; Kingma & Ba,
2014). Speciﬁcally, we train Dense-BC for 300 epochs with batch size 64 and momentum 0.9; and
Wide ResNet for 200 epochs with batch size 128 and momentum 0.9. The learning rate starts at 0.1,
and is dropped by a factor of 10 at 50% and 75% of the training progress, respectively.

Accuracy.
Each neural network architecture is
trained on CIFAR-10 (C-10) and CIFAR-100 (C-100)
datasets (Krizhevsky & Hinton, 2009),
respectively.
CIFAR-10 and CIFAR-100 images are drawn from 10
and 100 classes, respectively. Both datasets consist of

1http://pytorch.org
2https://github.com/facebookresearch/odin

Architecture C-10 C-100

Dense-BC
WRN-28-10

4.81
3.71

22.37
19.86

Table 1: Test error rates on CIFAR-10
and CIFAR-100 datasets.

3

Published as a conference paper at ICLR 2018

50,000 training images and 10,000 test images. The test
error on CIFAR datasets are summarized in Table 1.

4.2 OUT-OF-DISTRIBUTION DATASETS

At test time, the test images from CIFAR-10 (CIFAR-100) datasets can be viewed as the in-distribution
(positive) examples. For out-of-distribution (negative) examples, we follow the setting in (Hendrycks
& Gimpel, 2017) and test on several different natural image datasets and synthetic noise datasets. All
the datasets considered are listed below.

(1) TinyImageNet. The Tiny ImageNet dataset3 consists of a subset of ImageNet images (Deng
et al., 2009). It contains 10,000 test images from 200 different classes. We construct two datasets,
TinyImageNet (crop) and TinyImageNet (resize), by either randomly cropping image patches of
size 32

32 or downsampling each image to size 32

32.

(2) LSUN. The Large-scale Scene UNderstanding dataset (LSUN) has a testing set of 10,000 images
of 10 different scenes (Yu et al., 2015). Similar to TinyImageNet, we construct two datasets,
LSUN (crop) and LSUN (resize), by randomly cropping and downsampling the LSUN testing set,
respectively.

×

×

(3) iSUN. The iSUN (Xu et al., 2015) consists of a subset of SUN images. We include the entire

collection of 8925 images in iSUN and downsample each image to size 32 by 32.

(4) Gaussian Noise. The synthetic Gaussian noise dataset consists of 10,000 random 2D Gaussian
noise images, where each RGB value of every pixel is sampled from an i.i.d Gaussian distribution
with mean 0.5 and unit variance. We further clip each pixel value into the range [0, 1].

(5) Uniform Noise. The synthetic uniform noise dataset consists of 10,000 images where each RGB
value of every pixel is independently and identically sampled from a uniform distribution on [0, 1].

4.3 EVALUATION METRICS

We adopt the following four different metrics to measure the effectiveness of a neural network in
distinguishing in- and out-of-distribution images.

(1) FPR at 95% TPR can be interpreted as the probability that a negative (out-of-distribution)
example is misclassiﬁed as positive (in-distribution) when the true positive rate (TPR) is as high as
95%. True positive rate can be computed by TPR = TP / (TP+FN), where TP and FN denote true
positives and false negatives respectively. The false positive rate (FPR) can be computed by FPR =
FP / (FP+TN), where FP and TN denote false positives and true negatives respectively.

(2) Detection Error, i.e., Pe measures the misclassiﬁcation probability when TPR is 95%. The
TPR) + 0.5FPR, where we assume that both positive

deﬁnition of Pe is given by Pe = 0.5(1
and negative examples have the equal probability of appearing in the test set.

−

(3) AUROC is the Area Under the Receiver Operating Characteristic curve, which is also a threshold-
independent metric (Davis & Goadrich, 2006). The ROC curve depicts the relationship between
TPR and FPR. The AUROC can be interpreted as the probability that a positive example is assigned
a higher detection score than a negative example (Fawcett, 2006). A perfect detector corresponds
to an AUROC score of 100%.

(4) AUPR is the Area under the Precision-Recall curve, which is another threshold independent
metric (Manning et al., 1999; Saito & Rehmsmeier, 2015). The PR curve is a graph showing
the precision=TP/(TP+FP) and recall=TP/(TP+FN) against each other. The metric AUPR-In and
AUPR-Out in Table 2 denote the area under the precision-recall curve where in-distribution and
out-of-distribution images are speciﬁed as positives, respectively.

4.4 EXPERIMENTAL RESULTS

Comparison with baseline. In Figure 1, we show the ROC curves when DenseNet-BC-100 is
evaluated on CIFAR-10 (positive) images against TinyImageNet (negative) test examples. The red
curve corresponds to the ROC curve when using baseline method (Hendrycks & Gimpel, 2017),

3https://tiny-imagenet.herokuapp.com

4

Published as a conference paper at ICLR 2018

Out-of-distribution
dataset

FPR
(95% TPR)
↓

Detection
Error
↓

AUROC

↑

AUPR
In
↑

AUPR
Out
↑

Baseline (Hendrycks & Gimpel, 2017) / ODIN

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

34.7/4.3
40.8/7.5
39.3/8.7
33.6/3.8
37.2/6.3
23.5/0.0
12.3/0.0

67.8/17.3
82.2/44.3
69.4/17.6
83.3/44.0
84.8/49.5
88.3/0.5
95.4/0.2

38.9/23.4
45.6/25.5
35.0/21.8
35.0/17.6
40.6/21.3
1.6/0.0
0.3/0.0

66.6/43.9
79.2/55.9
74.0/39.6
82.2/56.5
82.7/57.3
98.2/0.1
99.2/1.0

19.9/4.7
22.9/6.3
22.2/6.9
19.3/4.4
21.1/5.7
14.3/2.5
8.7/2.5

36.4/11.2
43.6/24.6
37.2/11.3
44.1/24.5
44.7/27.2
46.6/2.8
50.2/2.6

21.9/14.2
25.3/15.2
20.0/13.4
20.0/11.3
22.8/13.2
3.3/2.5
2.6/2.5

35.8/24.4
42.1/30.4
39.5/22.3
43.6/30.8
43.9/31.1
51.6/2.5
52.1/3.0

95.3/99.1
94.1/98.5
94.8/98.2
95.4/99.2
94.8/98.8
96.5/99.9
97.5/100.0

83.0/97.1
70.4/90.7
83.7/96.8
70.6/91.5
69.9/90.1
83.2/99.5
81.8/99.6

92.9/94.2
91.0/92.1
94.5/95.9
93.9/95.4
92.5/93.7
99.2/100.0
99.5/100.0

82.0/90.8
72.2/84.0
80.3/92.0
73.9/86.0
72.8/85.6
84.1/99.1
84.3/98.5

96.4/99.1
95.1/98.6
96.0/98.5
96.4/99.3
95.9/98.9
97.8/100.0
98.3/100.0

85.3/97.4
71.4/91.4
86.2/97.1
72.5/92.4
71.9/91.1
88.1/99.6
87.6/99.7

92.5/92.8
89.7/89.0
95.1/95.8
93.8/93.8
91.7/91.2
99.3/100.0
99.6/100.0

83.3/91.4
70.4/82.8
83.4/92.4
75.7/86.2
74.2/85.9
89.9/99.4
90.2/99.1

93.8/99.1
92.4/98.5
93.1/97.8
94.0/99.2
93.1/98.8
93.0/99.9
95.9/100.0

80.8/96.8
68.6/90.1
80.9/96.5
68.0/90.6
67.0/88.9
73.1/99.0
70.1/99.1

91.9/94.7
89.9/93.6
93.1/95.5
92.8/96.1
91.5/94.9
98.9/100.0
99.3/100.0

80.2/90.0
70.8/84.4
77.0/91.6
70.1/84.9
69.2/84.8
71.0/97.5
70.9/95.9

Dense-BC
CIFAR-10

Dense-BC
CIFAR-100

WRN-28-10
CIFAR-10

WRN-28-10
CIFAR-100

Table 2: Distinguishing in- and out-of-distribution test set data for image classiﬁcation. All values are
percentages. ↑ indicates larger value is better, and ↓ indicates lower value is better. All parameter settings are
shown in Appendix A.2. Additional results on WRN-40-4 and MNIST dataset are reported in Appendix A.1.

whereas the blue curve corresponds to our method with temperature T = 1000 and perturbation
magnitude ε = 0.0012. We observe a strikingly large gap between the blue and red ROC curves. For
example, when TPR= 95%, the FPR can be reduced from 34% to 4.2% by using our approach.
Choosing parameters. For each out-of-distribution dataset,
we randomly hold out 1,000 images for tuning the parameters
T and ε. For temperature T , we select among 1, 2, 5, 10,
20, 50, 100, 200, 500, 1000; and for perturbation magnitude
ε we choose from 21 evenly spaced numbers starting from 0
and ending at 0.004. The optimal parameters are chosen to
minimize the FPR at TPR 95% on the holdout set. We evaluate
the our approach on the remaining test images. All parameter
settings are reported in the Appendix A. We provide additional
details on the effect of parameters in Section 5.

Main results. The main results are summarized in Table 2. For
each in- and out-of-distribution dataset pair, we report both the
performance of the baseline (Hendrycks & Gimpel, 2017) and
our approach using temperature scaling and input preprocessing.
In Table 2, we observe improved performance across all neural
architectures and all dataset pairs. Noticeably, our method
consistently outperforms the baseline by a large margin when
measured by FPR at 95% TPR and detection error.

5

Figure 1:
(a) ROC curves of base-
line (red) and our method (blue)
on DenseNet-BC-100 network, where
CIFAR-10 and TinyImageNet (crop)
are in- and out-of-distribution dataset,
respectively.

Published as a conference paper at ICLR 2018

Figure 2: (a)-(d) Performance of our method vs. MMD between in- and out-of-distribution datasets. Neural
networks are trained on CIFAR-100 and CIFAR-80, respectively. The out-of-distribution datasets are 1: LSUN
(cop), 2: TinyImageNet (crop), 3: LSUN (resize), 4:
is iSUN (resize), 5: TinyImageNet (resize) and 6:
CIFAR-20.

4.5 EXTENSIONS

In this subsection, we analyze how the statistical distance be-
tween in- and out-of-distribution natural image dataset affects the detection performance of the
proposed method.
Data distribution distance vs. Detection performance. To measure the statistical distance between
in- and out-of-distribution datasets, we adopt a commonly used metric, maximum mean discrepancy
(MMD) with Gaussian RBF kernel (Sriperumbudur et al., 2010; Gretton et al., 2012; Sutherland et al.,
2016). Speciﬁcally, given two image sets, V =
, the maximum
v1, ..., vm
}
{
mean discrepancy between V and Q is deﬁned as

w1, ..., wm
{

and W =

}

2

(cid:92)MMD

(V, W ) =

1
(cid:0)m
2

(cid:1)

(cid:88)

i(cid:54)=j

k(vi, vj) +

1
(cid:1)
(cid:0)m
2

(cid:88)

i(cid:54)=j

k(wi, wj)

−

k(vi, wj),

(cid:88)

i(cid:54)=j

(cid:1)

2
(cid:0)m
2
(cid:17)

∪

,
·

) is the Gaussian RBF kernel, i.e., k(x, x(cid:48)) = exp
·

. We use the same method
where k(
used by Sutherland et al. (2016) to choose σ, where 2σ2 is set to the median of all Euclidean distances
between all images in the aggregate set V

W .

−

(cid:16)

(cid:107)x−x(cid:48)(cid:107)2
2
2σ2

In Figure 2 (a)(b), we show how the performance of ODIN varies against the MMD distances between
in- and out-of-distribution datasets4. The datasets (on x-axis) are ranked in the descending order of
MMD distances with CIFAR-100. There are two interesting observations can be drawn from these
ﬁgures. First, we ﬁnd that the MMD distances between the cropped datasets and CIFAR-100 tend
to be larger. This is likely due to the fact that cropped images only contain local image context and
are therefore more distinct from CIFAR-100 images, while resized images contain global patterns
and are thus similar to images in CIFAR-100. Second, we observe that the MMD distance tends to
be negatively correlated with the detection performance. This suggests, not surprisingly, that the
detection task becomes harder as in and out-of-distribution images are more similar to each other.
Same-manifold datasets. Furthermore, we investigate the extreme scenario when in- and out-of-
distribution datasets are on the same manifold. In experiment, we randomly split CIFAR-100 into
two disjoint datasets containing 80 and 20 classes each. We name them CIFAR-80 and CIFAR-
20, respectively. We train both DenseNet and Wide ResNet-28-10 on the CIFAR-80 dataset (in-
distribution) and evaluate the detection performance on the CIFAR-20 dataset (out-distribution). All
hyperparameters used here are exactly the same as in Section 4.1. The MMD distance between
CIFAR-20 and CIFAR-80 is much smaller than other dataset pairs. In Figure 2 (c)(d), we observe that
both FPR at TPR 95% and detection error become larger on the CIFAR-20 dataset. This coincides
with our expectation that the detection task becomes extremely hard when in- and out-of-distribution
dataset locate on the same manifold. We provide additional experimental results in Appendix A.1
and Appendix G.

5 DISCUSSIONS

5.1 EFFECTS OF PARAMETERS

In this subsection, we empirically show how temperature T and perturbation magnitude ε affect
FPR at TPR 95% and AUROC on DenseNet and Wide ResNet-28-10. Additional results on other

4All distances are provided in Appendix G.

6

Published as a conference paper at ICLR 2018

Figure 3: (a)(b) Effects of temperature T when ε = 0. (c)(d) Effects of perturbation magnitude ε when T = 1.
All networks are trained on CIFAR-10 (in-distribution). Additional results on other metrics and Wide ResNet-40
are provided in Appendix B.

Figure 4: (a)(b) Effects of perturbation magnitude ε on DenseNet when T is large (e.g., T = 1000). (c)(d)
Effects of perturbation magnitude of ε on Wide-ResNet-28-10 when T is large (e.g., T = 1000). All networks
are trained on CIFAR-10. Additional results on other metrics and Wide ResNet-40 are provided in Appendix B.

metrics and architectures are provided in Appendix B. We show the detection performance when
using only the temperature scaling method (see Figure 3(a)(b), ε = 0), or the input preprocessing
method (see Figure 3(c)(d), T = 1). In Figure 4, we show the detection performance w.r.t ε when T
is optimal (e.g., T =1000). First, from Figure 3 (a)(b), we observe that increasing the temperature
can improve the detection performance, although the effects diminish when T is sufﬁciently large
(e.g., T > 100). Next, from Figure 3(c)(d) and Figure 4, we observe that we can further improve
the detection performance by appropriately choosing the perturbation magnitudes. We can achieve
overall better performance by combining both (1) temperature scaling and (2) input preprocessing.

5.2 ANALYSIS ON TEMPERATURE SCALING

In this subsection, we analyze the effectiveness of the temperature scaling method. As shown
in Figure 3 (a) and (b), we observe that a sufﬁciently large temperature yields better detection
performance although the effects diminish when T is too large. To gain insight, we can use the Taylor
expansion of the softmax score (details provided in Appendix D). When T is sufﬁciently large, we
have

Sˆy(x; T )

≈

N

(cid:80)

1
T

i[fˆy(x)

1
fi(x)] + 1
2T 2

(cid:80)

i[fˆy(x)

,

fi(x)]2

−
by omitting the third and higher orders. For simplicity of notation, we deﬁne

−

−

(3)

U1(x) =

[fˆy(x)

fi(x)]

and U2(x) =

1

N

1

−

(cid:88)

i(cid:54)=ˆy

−

1

N

1

−

(cid:88)

i(cid:54)=ˆy

[fˆy(x)

fi(x)]2.

(4)

−

Interpretations of U1 and U2. By deﬁnition, U1 measures the extent to which the largest unnormal-
ized output of the neural network deviates from the remaining outputs; while U2 measures the extent
to which the remaining smaller outputs deviate from each other. We provide formal mathematical
derivations in Appendix F. In Figure 5(a), we show the distribution of U1 for each out-of-distribution
dataset vs. the in-distribution dataset (in red). We observe that the largest outputs of the neural
network on in-distribution images deviate more from the remaining outputs. This is likely due to the
fact that neural networks tend to make more conﬁdent predictions on in-distribution images.
U1], for each
Further, we show in Figure 5(b) the expectation of U2 conditioned on U1, i.e., E[U2|
dataset. The red curve (in-distribution images) has overall higher expectation. This indicates that,

7

Published as a conference paper at ICLR 2018

Figure 5: (a) Probability density of U1 under different datasets on DenseNet. (b) Expectations of U2 conditioned
on U1 on DenseNet. (c) Probability density of the norm of gradient on DenseNet under temperature 1, 000.
(c)(d) Expectation of the norm of gradient conditioned on the softmax scores on DenseNet under temperature
T = 1000 and T = 1, respectively. (f)(g) Outputs of DenseNet on each class for an image of dog from
CIFAR-10 and an image from TinyImageNet (crop). The DenseNet is trained on CIFAR-10. Additional results
on other architectures are provided in Appendix C.

when two images have similar values on U1, the in-distribution image tends to have a much higher
value of U2 than the out-of-distribution image.
In other words, for in-distribution images, the
remaining outputs (excluding the largest output) tend to be more separated from each other compared
to out-of-distribution datasets. This may happen when some classes in the in-distribution dataset
share common features while others differ signiﬁcantly. To illustrate this, in Figure 5 (f)(g), we show
the outputs of each class using a DenseNet (trained on CIFAR-10) on a dog image from CIFAR-10,
and another image from TinyImageNet (crop). For the image of dog, we can observe that the largest
output for the label dog is close to the output for the label cat but is quite separated from the outputs
for the label car and truck. This is likely due to the fact that, in CIFAR-10, images of dogs are very
similar to the images of cats but are quite distinct from images of car and truck. For the image from
TinyImageNet (crop), despite having one large output, the remaining outputs are close to each other
and thus have a smaller deviation.
The effects of T . To see the usefulness of adopting a large T , we can ﬁrst rewrite the softmax score
function in Equation (3) as S
U2/2T )/T . Hence the softmax score is largely determined
(U1 −
by U1 and U2/2T . As noted earlier, U1 makes in-distribution images produce larger softmax scores
than out-of-distribution images since S
U2.
Therefore, by choosing a sufﬁciently large temperature, we can compensate the negative impacts of
U2/2T on the detection performance, making the softmax scores between in- and out-of-distribution
images more separable. Eventually, when T is sufﬁciently large, the distribution of softmax score is
almost dominated by the distribution of U1 and thus increasing the temperature further is no longer
effective. This explains why we see in Figure 3 (a)(b) that the performance does not change when T
is too large (e.g., T > 100). In Appendix E, we provide a formal proof showing that the detection
error eventually converges to a constant number when T goes to inﬁnity.

U1, while U2 has the exact opposite effect since S

∝ −

∝

∝

5.3 ANALYSIS ON INPUT PREPROCESSING
As noted previously, using the temperature scaling method by itself can be effective in improving the
detection performance. However, the effectiveness quickly diminishes as T becomes very large. In
order to make further improvement, we complement temperature scaling with input preprocessing.
This has already been seen in Figure 4, where the detection performance is improved by a large
margin on most datasets when T = 1000, provided with an appropriate perturbation magnitude ε is
chosen. In this subsection, we provide some intuition behind this.

To explain, we can look into the ﬁrst order Taylor expansion of the log-softmax function for the
perturbed image ˜x, which is given by

log Sˆy( ˜x; T ) = log Sˆy(x; T ) + ε

x log Sˆy(x; T )

(cid:107)∇

(cid:107)1 + o(ε),

where x is the original input.
The effects of gradient.
(cid:107)1 — the
In Figure 5 (c), we present the distribution of
1-norm of gradient of log-softmax with respect to the input x — for all datasets. A salient observation

x log S(x; T )

(cid:107)∇

8

Published as a conference paper at ICLR 2018

≈

is that CIFAR-10 images (in-distribution) tend to have larger values on the norm of gradient than
most out-of-distribution images. To further see the effects of the norm of gradient on the softmax
score, we provide in Figures 5 (d) the conditional expectation E[
S]. We can
observe that, when an in-distribution image and an out-of-distribution image have the same softmax
score, the value of

x log S(x; T )

x log S(x; T )

(cid:107)1 for in-distribution image tends to be larger.

(cid:107)1|

(cid:107)∇

(cid:107)∇

We illustrate the effects of the norm of gradient in Figure 6. Suppose
that an in-distribution image x1 (blue) and an out-of-distribution
image x2 (red) have similar softmax scores, i.e., S(x1)
S(x2).
After input processing, the in-distribution image can have a much
larger softmax score than the out-of-distribution image x2 since x1
results in a much larger value on the norm of softmax gradient than
that of x2. Therefore, in- and out-of-distribution images are more
separable from each other after input preprocessing5.
The effect of ε. When the magnitude ε is sufﬁciently small, adding
perturbations does not change the predictions of the neural network,
i.e., ˆy( ˜x) = ˆy(x). However, when ε is not negligible, the gap of
softmax scores between in- and out-of-distribution images can be
(cid:107)1. Our observation is consistent with
affected by
that in (Szegedy et al., 2014; Goodfellow et al., 2015; Moosavi-
Dezfooli et al., 2017), which show that the softmax scores tend to change signiﬁcantly if small
perturbations are added to the in-distribution images. It is also worth noting that using a very large ε
can lead to performance degradation, as seen in Figure 4. This is likely due to the fact that the second
and higher order terms in the Taylor expansion are no longer insigniﬁcant when the perturbation
magnitude is too large.

Figure 6: Illustration of effects
of the input preprocessing.

x log S(x; T )

(cid:107)∇

6 RELATED WORKS AND FUTURE DIRECTIONS

The problem of detecting out-of-distribution examples in low-dimensional space has been well-studied
in various contexts (see the survey by Pimentel et al. (2014)). Conventional methods such as density
estimation, nearest neighbor and clustering analysis are widely used in detecting low-dimensional out-
of-distribution examples (Chow, 1970; Vincent & Bengio, 2003; Ghoting et al., 2008; Devroye et al.,
2013), . The density estimation approach uses probabilistic models to estimate the in-distribution
density and declares a test example to be out-of-distribution if it locates in the low-density areas.
The clustering method is based on the statistical distance, and declares an example to be out-of-
distribution if it locates far from its neighborhood. Despite various applications in low-dimensional
spaces, unfortunately, these methods are known to be unreliable in high-dimensional space such as
image space (Wasserman, 2006; Theis et al., 2015). In recent years, out-of-distribution detectors based
on deep models have been proposed. Schlegl et al. (2017) train a generative adversarial networks to
detect out-of-distribution examples in clinical scenario. Sabokrou et al. (2016) train a convolutional
network to detect anomaly in scenes. Andrews et al. (2016) adopt transfer representation-learning
for anomaly detection. All these works require enlarging or modifying the neural networks. In
a more recent work, Hendrycks & Gimpel (2017) found that pre-trained neural networks can be
overconﬁdent to out-of-distribution example, limiting the effectiveness of detection. Our paper aims
to improve the performance of detecting out-of-distribution examples, without requiring any change
to an existing well-trained model.

Our approach leverages the following two interesting observations to help better distinguish between
in- and out-of-distribution examples: (1) On in-distribution images, modern neural networks tend to
produce outputs with larger variance across class labels, and (2) neural networks have larger norm
of gradient of log-softmax scores when applied on in-distribution images. We believe that having a
better understanding of these phenomenon can lead to further insights into this problem.

7 CONCLUSIONS

In this paper, we propose a simple and effective method to detect out-of-distribution data samples
in neural networks. Our method does not require retraining the neural network and signiﬁcantly

5Similar observation can be seen when T = 1, where we present the conditional expectation of the norm of

softmax gradient in Figure 5 (e).

9

Published as a conference paper at ICLR 2018

improves on the baseline (state-of-the-art) on different neural architectures across various in and
out-distribution dataset pairs. We empirically analyze the method under different parameter settings,
and provide some insights behind the approach. Future work involves exploring our method in other
applications such as speech recognition and natural language processing.

The research reported here was supported by NSF Grant CPS ECCS 1739189.

ACKNOWLEDGMENTS

REFERENCES

Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané.

Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.

Jerone T.A Andrews, Thomas Tanay, Edward J. Morton, and Lewis D. Grifﬁn. Transfer representation-

learning for anomaly detection. In ICML, 2016.

Yaroslav Bulatov. notmnist dataset. 2011.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for
statistical machine translation. EMNLP, 2014.

C Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on information theory,

Jesse Davis and Mark Goadrich. The relationship between precision-recall and roc curves. In ICML.

16(1):41–46, 1970.

ACM, 2006.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale

hierarchical image database. In CVPR, 2009.

Luc Devroye, László Györﬁ, and Gábor Lugosi. A probabilistic theory of pattern recognition,

volume 31. Springer Science & Business Media, 2013.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.

Clement Farabet, Camille Couprie, Laurent Najman, and Yann LeCun. Learning hierarchical features
for scene labeling. IEEE transactions on pattern analysis and machine intelligence, 35(8):1915–
1929, 2013.

Tom Fawcett. An introduction to roc analysis. Pattern recognition letters, 2006.

Amol Ghoting, Srinivasan Parthasarathy, and Matthew Eric Otey. Fast mining of distance-based
outliers in high-dimensional datasets. Data Mining and Knowledge Discovery, 16(3):349–364,
2008.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. ICLR, 2015.

Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola. A

kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723–773, 2012.

Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural

networks. arXiv preprint arXiv:1706.04599, 2017.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image

recognition. In CVPR, 2016.

Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution

examples in neural networks. ICLR, 2017.

10

Published as a conference paper at ICLR 2018

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv

preprint arXiv:1503.02531, 2015.

preprint arXiv:1608.06993, 2016.

Gao Huang, Zhuang Liu, and Kilian Q Weinberger. Densely connected convolutional networks. arXiv

Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolutional neural networks for human action
recognition. IEEE transactions on pattern analysis and machine intelligence, 35(1):221–231,
2013.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097–1105,
2012.

Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning

through probabilistic program induction. Science, 2015.

Christopher D Manning, Hinrich Schütze, et al. Foundations of statistical natural language processing,

volume 999. MIT Press, 1999.

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal

adversarial perturbations. CVPR, 2017.

Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High conﬁdence

predictions for unrecognizable images. 2015.

Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, and Geoffrey Hinton. Regularizing

neural networks by penalizing conﬁdent output distributions. ICLR, 2017.

Marco AF Pimentel, David A Clifton, Lei Clifton, and Lionel Tarassenko. A review of novelty

detection. Signal Processing, 99:215–249, 2014.

Mohammad Sabokrou, Mohsen Fayyaz, Mahmood Fathy, et al. Fully convolutional neural network

for fast anomaly detection in crowded scenes. arXiv preprint arXiv:1609.00866, 2016.

Takaya Saito and Marc Rehmsmeier. The precision-recall plot is more informative than the roc plot
when evaluating binary classiﬁers on imbalanced datasets. PloS one, 10(3):e0118432, 2015.

Thomas Schlegl, Philipp Seeböck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs.
Unsupervised anomaly detection with generative adversarial networks to guide marker discovery.
In International Conference on Information Processing in Medical Imaging, pp. 146–157. Springer,
2017.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image

recognition. ICLR, 2015.

Bharath K Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Schölkopf, and Gert RG
Lanckriet. Hilbert space embeddings and metrics on probability measures. Journal of Machine
Learning Research, 11(Apr):1517–1561, 2010.

Dougal J Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alex
Smola, and Arthur Gretton. Generative models and model criticism via optimized maximum mean
discrepancy. ICLR, 2016.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,

and Rob Fergus. Intriguing properties of neural networks. NIPS, 2014.

Lucas Theis, Aäron van den Oord, and Matthias Bethge. A note on the evaluation of generative

models. ICLR, 2015.

11

Published as a conference paper at ICLR 2018

Pascal Vincent and Yoshua Bengio. Manifold parzen windows. In Advances in neural information

processing systems, pp. 849–856, 2003.

Larry Wasserman. All of Nonparametric Statistics. Springer, 2006.

Pingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R Kulkarni, and Jianxiong
Xiao. Turkergaze: Crowdsourcing saliency with webcam based eye tracking. arXiv preprint
arXiv:1504.06755, 2015.

Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-
scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365,
2015.

Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,

2016.

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding

deep learning requires rethinking generalization. ICLR, 2017.

12

Published as a conference paper at ICLR 2018

A SUPPLEMENTARY RESULTS IN SECTION 4.4
A.1 EXPERIMENTAL RESULTS

Out-of-distribution
dataset

FPR
(95% TPR)
↓

Detection
Error
↓

AUROC

↑

AUPR
In
↑

AUPR
Out
↑

Baseline (Hendrycks & Gimpel, 2017) / Ours

WRN-40-4
CIFAR-10

WRN-40-4
CIFAR-100

Dense-BC
CIFAR-80

WRN-28-10
CIFAR-80

WRN-40-4
CIFAR-80

MNIST

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

CIFAR-20
TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

CIFAR-20
TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

CIFAR-20
TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

Omniglot
notMNIST
CIFAR-10bw
Gaussian
Uniform

49.8/36.7
62.3/49.1
34.6/23.0
54.5/35.1
58.6/41.0
26.6/3.2
21.8/0.9

66.9/43.3
78.1/55.1
74.9/35.9
77.9/50.0
79.5/52.9
84.7/3.3
77.2/3.1

84.1/81.1
72.9/22.7
84.4/46.3
67.1/20.9
84.9/45.9
86.1/50.2
100.0/0.9
98.5/1.2

80.4/78.3
71.3/46.7
81.0/48.8
74.4/45.5
81.9/49.0
82.7/51.1
99.6/1.4
100.0/0.4

82.4/78.4
68.3/34.3
80.6/53.5
72.2/33.2
79.1/51.2
81.2/53.2
99.7/48.6
99.7/10.7

0.2/0.0
10.3/8.7
0.1/0.0
0.0/0.0
0.0/0.0

27.4/20.9
33.6/27.1
19.8/14.0
29.8/20.1
31.8/23.0
15.8/4.1
13.4/3.0

36.0/24.1
41.5/30.1
40.0/20.4
41.5/27.5
42.2/28.9
44.9/4.2
41.1/4.0

44.9/43.0
39.0/13.8
44.7/25.6
36.0/12.9
45.0/25.4
50.5/27.6
52.5/3.0
51.8/3.1

42.7/41.6
38.1/25.9
43.0/26.9
39.7/25.2
43.5/27.0
43.9/28.1
52.3/3.2
52.5/2.7

43.7/41.7
36.6/19.6
42.8/29.2
38.6/19.1
42.0/28.1
43.1/29.1
52.4/26.8
52.4/7.8

2.6/2.5
7.7/6.8
2.5/2.5
2.5/2.5
2.5/2.5

87.3/89.3
79.3/81.6
93.4/95.1
84.7/87.0
82.1/84.9
96.1/99.2
96.5/99.7

81.3/88.5
72.6/81.6
79.1/90.8
75.2/85.6
74.3/84.3
86.3/98.8
86.4/99.0

76.6/77.8
83.4/96.2
76.8/91.7
84.6/96.2
77.5/91.8
76.1/90.5
64.3/98.6
80.4/99.6

79.2/80.4
83.1/91.9
77.1/89.2
82.0/92.9
78.8/90.1
78.3/89.4
80.6/98.9
79.7/99.1

76.8/78.1
83.6/93.4
76.2/87.7
83.1/93.4
77.6/88.8
76.2/87.7
65.6/93.8
74.3/97.7

85.1/86.7
73.5/76.9
93.1/94.3
79.8/82.6
76.4/80.2
97.0/99.2
97.5/99.7

80.6/87.2
69.4/78.0
81.4/89.9
73.1/83.5
72.9/81.9
90.5/99.1
90.2/99.2

79.4/80.6
86.3/96.6
80.3/92.7
86.9/96.4
81.4/92.9
79.8/91.3
78.4/99.1
86.7/99.6

81.5/82.2
85.9/92.6
80.0/89.5
84.4/93.0
82.2/90.8
81.5/90.0
87.7/99.2
87.4/99.4

78.9/79.1
85.9/94.0
78.5/88.3
86.3/93.7
80.0/89.4
78.7/88.3
77.3/95.7
83.0/98.4

87.2/90.7
80.6/84.8
92.4/95.2
85.3/89.7
83.2/87.8
94.8/99.2
94.7/99.7

80.1/89.1
71.6/83.4
76.3/91.5
73.3/86.4
71.9/85.1
77.0/97.9
78.6/98.6

71.6/73.6
79.9/95.8
71.5/90.4
82.1/96.0
71.6/90.2
69.9/88.8
52.2/96.6
68.0/99.1

74.2/76.2
79.7/90.7
72.6/88.5
78.2/91.5
73.4/88.8
72.6/88.0
66.8/97.6
65.5/98.0

72.2/75.0
81.2/92.5
72.5/86.1
79.7/93.1
73.9/87.3
72.2/86.1
53.7/88.7
61.2/95.2

99.6/100.0
97.2/98.2
99.7/100.0
99.7/100.0
99.9/100.0

99.7/100.0
97.5/98.4
99.8/100.0
99.8/100.0
99.9/100.0

99.5/100.0
97.4/98.0
99.7/100.0
99.7/100.0
99.9/100.0

Table 3: Distinguishing in- and out-of-distribution test set data for image classiﬁcation. All values are
percentages. ↑ indicates larger value is better, and ↓ indicates lower value is better.

MNIST: We used the same MNIST classiﬁer used by Hendrycks & Gimpel (2017), which is a
three-layer, 256 neuron-wide, fully connected network trained for 30 epochs with Adam (Kingma
& Ba, 2014). The classiﬁer achieve 99.34% test accuracy on the MNIST test set. We compare our
method with the baseline (Hendrycks & Gimpel, 2017) on ﬁve different out-of-distribution datasets:
(1) Omniglot dataset (Lake et al., 2015) contains images of handwritten characters in stead of the
handwritten digits in MNIST; (2) notMNIST (Bulatov, 2011) dataset contains typeface characters;

13

Published as a conference paper at ICLR 2018

(3) CIFAR-10bw contains black and white rescaled CIFAR-10 images; (4)(5) Gaussian and Uniform
image set contains the synthetic Gaussian and Uniform noise images used in Section 4.2.
Wide ResNet-40-4: We use the same architecture used by Hendrycks & Gimpel (2017) to evaluate
the baseline and our method. The Wide ResNet-40-4 achieves 95.7% test accuracy on CIFAR-10
dataset and achieve 79.27% test accuracy on CIFAR-100.
CIFAR-80: DenseNet-BC-100 achieves 78.94% test accuracy on CIFAR-80, while Wide ResNet-
28-10 achieves 81.71% test accuracy and Wide ResNet-40-4 achieves 79.53% test accuracy on
CIFAR-80.

A.2 PARAMETER SETTINGS

For MNIST, we set T = 1000 and ε = 0. The parameter settings for other structures are shown as
follows.

DenseNet-BC-100

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

0.0014
0.0014
0
0.0014
0.0014
0.0014
0.0014
-

0.002
0.0022
0.0036
0.002
0.002
0.0028
0.0026
0.0002

0.002
0.0022
0.0038
0.0018
0.002
0.0024
0.0028
-

Table 4: Optimal perturbation magnitude ε for reproducing main results in Table 2 and 3.

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

Table 5: Optimal Temperature T for reproducing main results in Table 2 and 3.

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

DenseNet-BC-100

1000
1000
1000
1000
1000
1000
1000
-

1000
1000
1000
1000
1000
1
1
1

1000
1000
1000
1000
1000
1
1
-

Wide-ResNet-28-10

0.0005
0.0011
0
0.0006
0.0008
0.0014
0.0014
-

0.0002
0.0004
0.0002
0.0002
0.0002
0.0002
0.0002
5e-05

0.0026
0.0024
0.0038
0.0026
0.0026
0.0032
0.0032
-

14

Table 6: Optimal perturbation magnitude ε for reproducing main results in Table 2 and 3.

Published as a conference paper at ICLR 2018

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

Wide-ResNet-28-10

1000
1000
1000
1000
1000
1000
1000
-

1000
1000
1000
1000
1000
1
1
1

1000
1000
1000
1000
1000
1000
1000
-

Table 7: Optimal Temperature T for reproducing main results in Table 2 and 3.

Wide-ResNet-40-4

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

0.0004
0.0008
0
0.001
0.0008
0.0016
0.0016
-

0.0002
0.0004
0.0002
0.0002
0.0002
0.0002
0.0002
0.0002

0.0014
0.0016
0.0038
0.0014
0.0016
0.0024
0.0026
-

Table 8: Optimal perturbation magnitude ε for reproducing main results in Table 2 and 3.

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

Wide-ResNet-40-4

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

1000
1000
1000
1000
1000
1
1
1

1000
1000
1000
1000
1000
1000
1000
-

Table 9: Optimal Temperature T for reproducing main results in Table 2 and 3.

1000
1000
1000
1000
1000
1000
1000
-

15

Published as a conference paper at ICLR 2018

B SUPPLEMENTARY RESULTS IN SECTION 5.1

Figure 7: Detection performance on DenseNet, Wide ResNet-28-10 and Wide ResNet-40-4 under different
temperature, when input preprocessing is not used, i.e., ε = 0. All networks are trained on CIFAR-10.

16

Published as a conference paper at ICLR 2018

Figure 8: Detection performance on DenseNet, Wide ResNet-28-10 and Wide ResNet-40-4 under different
perturbation magnitude, when temperature scaling is not used, i.e., T = 1. All networks are trained on
CIFAR-10.

17

Published as a conference paper at ICLR 2018

Figure 9: Detection performance on DenseNet, Wide ResNet-28-10 and Wide ResNet-40-4 under different
perturbation magnitude, when the optimal temperature is used, i.e., T = 1000. All networks are trained on
CIFAR-10.

18

Published as a conference paper at ICLR 2018

C SUPPLEMENTARY RESULTS IN SECTION 5.2 AND 5.3

Figure 10: Expectation of the second order term U2 conditioned on the ﬁrst order term U1 under DenseNet,
Wide-ResNet-28-10 and Wide ResNet-40-4. All networks are trained on CIFAR-10.

Figure 11: Expectation of gradient norms conditioned on the softmax scores under DenseNet, Wide-ResNet-28-
10 and Wide ResNet-40-4, where the temperature scaling is not used. All networks are trained on CIFAR-10.

Figure 12: Expectation of gradient norms conditioned on the softmax scores under DenseNet, Wide-ResNet-28-
10 and Wide ResNet-40-4, where the optimal temperature is used, i.e., T = 1000. All networks are trained on
CIFAR-10.

19

In this section, we present the Taylor expansion of the soft-max score function:

Published as a conference paper at ICLR 2018

D TAYLOR EXPANSION

Sˆy(x; T ) =

(cid:80)N

exp (fˆy(x)/T )
i=1 exp(fi(x)/T )
1
(cid:16) fi(x)−f ˆy(x)
T

i=1 exp

(cid:17)

(cid:80)N

=

=

1
+ 1
2!

1
fi(x)] + 1
2T 2

(cid:104)

(cid:80)N

i=1

1 + fi(x)−f ˆy(x)

T

(fi(x)−f ˆy(x))2
T 2

+ o (cid:0) 1
T 2

(cid:1)(cid:105)

by Taylor expansion

≈

N

−

1
T

(cid:80)N

i=1[fˆy(x)

−

(cid:80)N

i=1[fi(x)

fˆy(x)]2

−

E PROPOSITION 1

The following proposition 1 shows that the detection error Pe(T, 0)
Thus, increasing the temperature further can only slightly improve the detection performance.
Proposition 1. There exists a constant c only depending on function U1, in-distribution PX and
out-of-distribution QX such that limT →∞ Pe(T, ε) = c, when ε = 0 (i.e., no input preprocessing).
Proof. Since

c if T is sufﬁciently large.

≈

Therefore, for any X,
(cid:18)

Sˆy(X; T ) =

exp(fˆy(X)/T )
i=1 exp(fi(X)/T )

(cid:80)N

=

1 + (cid:80)

1

i(cid:54)=ˆy exp([fi(X)

fˆy(X)]/T )

−

lim
T →∞

T

1
Sˆy(X; T )

−

(cid:19)

+ N

= lim
T →∞

(cid:20)
1

T

(cid:88)

i(cid:54)=ˆy

exp

−

(cid:18) fi(X)

fˆy(X)

(cid:19)(cid:21)

−
T

=

[fˆy(X)

fi(X)] = (N

1)U1(X)

−

−

(cid:88)

i(cid:54)=ˆy

This indicates that the random variable

(cid:18)

T

1
Sˆy(X; T )

−

(cid:19)

+ N

(N

1)U1(X) a.s.

→

−

as T
the false positive rate

→ ∞

. This means that for a speciﬁc α > 0, choosing the threshold δT = 1/(N

α/T ), then

FPR(T ) = QX (Sˆy(X; T ) > 1/(N

α/T )) = QX

T

N

−

−

−

(cid:18)

(cid:18)

1
Sˆy(X; T )

−

(cid:19)

(cid:19)

> α

QX ((N

1)U1(X) > α) ,

(cid:18)

(cid:18)

1
Sˆy(X; T )

−

(cid:19)

(cid:19)

> α

PX ((N

1)U1(X) > α) .

−

−

T →∞
−−−−→

T →∞
−−−−→

and the true positive rate

TPR(T ) = PX (Sˆy(X; T ) > 1/(N

α/T )) = PX

T

N

Choosing α∗ such that PX ((N
at the same time FPR(T )
depending on U1, PX , QX and PZ, such that

QX ((N

→

−

−

1)U1(X) > α∗) = 0.95, then TPR(T )
1)U1(X) > α∗) as T

0.95 as T

and
. There exists a constant c

→ ∞

→

→ ∞

lim
T →∞

Pe(T, 0) = 0.05P (Z = 0) + P (Z = 1)QX ((N

1)U1(X) > α∗) = c.

−

20

Published as a conference paper at ICLR 2018

F ANALYSIS OF TEMPERATURE

For simplicity of the notations, let ∆i = fˆy
mean of the set ∆. Therefore,

−

fi and thus ∆ =

∆i

i(cid:54)=ˆy. Besides, let ¯∆ denote the
}

{

¯∆ =

1

N

1

−

(cid:88)

i(cid:54)=ˆy

∆i =

1

N

1

−

(cid:88)

i(cid:54)=ˆy

[fˆy

fi] = U1.

−

Equivalently,

Next, we will show

U1 = Mean(∆).

U2 =

1

N

1

−

(cid:88)

i(cid:54)=ˆy

[fˆy

−

(cid:122)

1

fi]2 =

Variance2(∆)
(cid:125)(cid:124)
(cid:88)

[∆i

Mean2(∆)
(cid:122)(cid:125)(cid:124)(cid:123)
¯∆2

.

(cid:123)
¯∆]2 +

N

1

−

i(cid:54)=ˆy

−

by∆i = fˆy

fi

−

Since

then

U2 =

∆2
i

(cid:88)

i(cid:54)=ˆy
(cid:88)

i(cid:54)=ˆy
(cid:88)

i(cid:54)=ˆy

(cid:88)

1

−
1

−
1

−
1

−

1

1

1

1

N

N

N

N

(cid:124)

=

=

=

¯∆ + ¯∆)2

(∆i

−

[(∆i

¯∆)2

−

−

2(∆i

−

¯∆) ¯∆ + ¯∆2]

¯∆]2

[∆i

−

i(cid:54)=ˆy
(cid:123)(cid:122)
Variance2(∆)

−

N

1

−

(cid:125)

(cid:124)

i(cid:54)=ˆy
(cid:123)(cid:122)
=0

2 ¯∆

(cid:88)

(∆i

−

¯∆)

+ ¯∆2

(cid:125)

(cid:124)(cid:123)(cid:122)(cid:125)
Mean2(∆)

U2 = Variance2(∆) + Mean2(∆)

G ADDITIONAL RESULTS IN SECTION 4.5

Apart from the Maximum Mean Discrepancy, we as well calculate the Energy distance between in-
and out-of-distribution datasets. Let P and Q denote two different distributions. Then the energy
distance between distributions P and Q is deﬁned as

D2

energy(P, Q) = 2EV ∼P,W ∼Q

X
(cid:107)

Y

−

(cid:107) −

EV,V (cid:48)∼P

X (cid:48)

X
(cid:107)

−

(cid:107) −

EW,W (cid:48)∼Q

Y
(cid:107)

Y (cid:48)

.

(cid:107)

−

Therefore, the energy distance between two datasets V =
W1, ..., Wm
{

Q is deﬁned as

iid
∼

}

V1, ..., Vm
{

}

iid
∼

P and W =

2

(cid:92)Denergy

(P, Q) =

2
m2

m
(cid:88)

m
(cid:88)

i=1

j=1

Vi
(cid:107)

−

Wj

(cid:107) −

1
(cid:1)
(cid:0)m
2

(cid:88)

i(cid:54)=j

Vi

(cid:107)

−

Vj

(cid:107) −

1
(cid:1)
(cid:0)m
2

(cid:88)

i(cid:54)=j

Wi
(cid:107)

Wj

.
(cid:107)

−

In the experiment, we use the 2-norm

(cid:107) · (cid:107)2.

21

Published as a conference paper at ICLR 2018

In-distribution Out-of-distribution
datasets

Datasets

MMD
Distance Distance

Energy

CIFAR-100

CIFAR-80

Tiny-ImageNet (crop)
LSUN (crop)
Tiny-ImageNet (resize)
LSUN (resize)
iSUN (resize)

Tiny-ImageNet (crop)
LSUN (crop)
Tiny-ImageNet (resize)
LSUN (resize)
iSUN (resize)
CIFAR-20

0.41
0.43
0.088
0.12
0.11

0.4
0.43
0.095
0.120
0.116
0.057

2.25
2.31
0.54
0.63
0.56

2.22
2.29
0.57
0.62
0.61
0.35

22

Published as a conference paper at ICLR 2018

Figure 13: False positive rate (FPR) and true positive rate (TPR) under different thresholds (δ) when the
temperature (T ) is set to 1, 000 and the perturbation magnitude (ε) is set to 0.0014. The DenseNet is trained on
CIFAR-10.

Figure 14: Detection performance on Tiny-ImageNet (resize), LSUN (resize) and iSUN (resize) when parame-
ters are tuned on six different out-of-distribution datasets. Each tuning set contains 1,000 images and each test
set contains 9,000 images. Both DenseNet and Wide-ResNet are trained on CIFAR-10. Additional results on
other datasets are provide in Table 10 and 11.

H ADDITIONAL DISCUSSIONS

In this section, we present additional discussion on the proposed method. We ﬁrst empirically show
how the threshold δ affects the detection performance. We next show how the proposed method
performs when the parameters are tuned on a certain out-of-distribution dataset and are evaluated on
other out-of-distribution datasets. Finally, we show how the size of dataset for choosing parameters
affects the detection performance.
Effects of the threshold. We analyze how the threshold affects the following metrics: (1) FPR, i.e.,
the fraction of out-of-distribution images misclassiﬁed as in-distribution images; (2) TPR, i.e, the
fraction of in-distribution images correctly classiﬁed as in-distribution images. In Figure 13, we
show how the thresholds affect FPR and TPR when the temperature and perturbation magnitude are
chosen optimally (i.e., T = 1, 000, ε = 0.0014). From the ﬁgure, we can observe that the threshold
corresponding to 95% TPR can produce small FPRs on all out-of-distribution datasets.
Performance across datasets. To investigate how the parameters generalize across datasets, we
tune the parameters using one out-of-distribution dataset and then evaluate on a different one. Given
an out-of-distribution dataset, we ﬁrst split the dataset into two disjoint subsets: tuning set and
test set. The tuning set contains 1,000 images and the test set contains 9,000 images. We tune the
parameters on the tuning set and evaluate the detection performance on the test set. We ﬁrst choose
the temperature T and the perturbation magnitude ε such that the FPR at TPR 95% is minimized on
the tuning set of one out-of-distribution dataset. Next, we set δ to the threshold corresponding to 95%
TPR and calculate the false positive rates on the test sets of other out-of-distribution datasets.

In Figure 14, we show the detection performance on three out-of-distribution datasets when the
parameters are tuned on six different datasets. From Figure 14, we can observe that the parameters
tuned on different tuning sets can have quite similar detection FPRs on all of three out-of-distribution
image sets. This may be due to the fact, shown in Figure 13, that the threshold corresponding to 95%
TPR can produce small FPRs on all datasets.

23

Published as a conference paper at ICLR 2018

Figure 15: FPR at TPR 95% under different tuning set sizes. The DenseNet is trained on CIFAR-10 and each
test set contains 8,000 out-of-distribution images.

Performance vs.
tuning set size. To show the effects of the tuning set size on the detection
performance, we devise the following experiment. For each out-of-distribution dataset, we choose the
tuning set size from 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000. For each set size, we
tune the temperature T and perturbation magnitude ε to minimize the FPR at TPR 95% and calculate
the FPR. In Figure 15, we show the detection performance of ODIN under different tuning set size.
From Figure 15, we can observe that the FPR at TPR 95% tends to stabilize when the set size grows
above 1,000.

24

Published as a conference paper at ICLR 2018

DenseNet-BC-100

Test set

ImgNet (c)

ImgNet (r) LSUN (c) LSUN (c)

iSUN

Gaussian Uniform

Baseline (Hendrycks & Gimpel, 2017) / Ours

ImgNet (c)
ImgNet (r)
LSUN (c)
LSUN (r)
iSUN
Gaussian
Uniform

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

34.7/6.6
40.7/14.9
39.3/8.1
33.6/10.4
37.2/12.6
23.5/0.4
12.3/4.5

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

Table 10: Detection performance across different datasets. Each row corresponds to the FPR at
TPR 95% on the same test set where parameters are tuned under different tuning sets. Each column
corresponds to the FPR at TPR 95% on different test sets where parameters are tuned under the same
tuning set. The DenseNet is trained on CIFAR-10.

Wide-ResNet- 28-10

Test set

ImgNet (c)

ImgNet (r) LSUN (c)

LSUN (c)

iSUN

Gaussian Uniform

Baseline (Hendrycks & Gimpel, 2017) / Ours

ImgNet (c)
ImgNet (r)
LSUN (c)
LSUN (r)
iSUN
Gaussian
Uniform

38.9/23.4
45.6/25.5
35.0/28.1
35.0/18.9
40.6/21.8
1.6/0.0
0.3/0.0

38.9/24.5
45.6/25.5
35.0/31.27
35.0/18.0
40.6/21.7
1.6/0.0
0.3/0.0

38.9/27.1
45.6/32.9
35.0/21.8
35.0/25.6
40.6/28.9
1.6/0.4
0.3/0.0

38.9/23.4
45.6/25.8
35.0/28.2
35.0/17.6
40.6/ 21.9
1.6/0.0
0.3/0.0

38.9/24.1
45.6/25.8
35.0/28.9
35.0/18.0
40.6/21.3
1.6/0.0
0.3/0.0

38.9/26.5
45.6/27.9
35.0/29.7
35.0/19.1
40.6/22.8
1.6/0.0
0.3/0.0

38.9/26.5
45.6/27.9
35.0/29.7
35.0/19.1
40.6/22.8
1.6/0.0
0.3/0.0

Table 11: Detection performance across different datasets. Each row corresponds to the FPR at
TPR 95% on the same test set where parameters are tuned under different tuning sets. Each column
corresponds to the FPR at TPR 95% on different test sets where parameters are tuned under the same
tuning set. The Wide-ResNet is trained on CIFAR-10.

25

Published as a conference paper at ICLR 2018

Figure 16: (a) The test accuracy on the images having softmax scores above the threshold corresponding to
a certain true positive rate. (b) The test accuracy on the images having softmax scores below the threshold
corresponding to a certain true positive rate. All networks are trained on CIFAR-10.

Figure 17: Outputs of DenseNet on thirty classes for an image of apple from CIFAR-80 and an image of red
pepper from CIFAR-20. The label “0” denotes the class “apple” and the label “49" denotes the class “orange".

I ADDITIONAL ANALYSIS

Difﬁcult-to-classify images and difﬁcult-to-detect images. We analyze the correlation between
the images that tend to be out-of-distribution and images on which the neural network tend to make
incorrect predictions. To understand the correlation, we devise the following experiment. For the
ﬁxed temperature T and perturbation magnitude ε, we ﬁrst set δ to the softmax score threshold
corresponding to a certain true positive rate. Next, we calculate the test accuracy on the images with
softmax scores above δ and the test accuracy on the images with softmax score below δ, respectively.
We report the results in Figure 16(a) and (b). From these two ﬁgures, we can observe that the images
that are difﬁcult to detect are more likely to be the images that are difﬁcult to classify. For example,
the DenseNet can achieve up to 98.5% test accuracy on the images having softmax scores above the
threshold corresponding to 80% TPR, but can only achieve around 82% test accuracy on the images
having softmax scores below the threshold corresponding to 80% TPR.
Same manifold datasets. We provide additional empirical results showing how the term E[U2|
U1]
affects the detection performance when in- and out-of-distribution datasets locate on the same
manifold. In Figure 17, we show the outputs of DenseNet on thirty classes for an image of apple
from CIFAR-80 (in-distribution) and an image of red pepper of CIFAR-20 (out-distribution). We
can observe that the outputs of DenseNet for both images are quite similar to each other. In addition,
we can observe that for both images, the second and third largest output are quite close to the
largest output. This may be due the fact the image of red pepper shares some common features
with the images in CIFAR-80. Furthermore, the similarity between the outputs for the images from

26

Published as a conference paper at ICLR 2018

Out-of-distribution
dataset

FPR
(95% TPR)
↓

Detection AUROC

Error
↓

↑

AUPR
In
↑

AUPR
Out
↑

Baseline (Hendrycks & Gimpel, 2017) / Ours

CIFAR-10
CIFAR-100

CIFAR-100
CIFAR-10

57.1/47.2
81.8/81.4

31.1/26.1
43.4/43.2

89.0/89.8
76.1/76.7

91.2/91.4
79.9/80.4

86.8/88.7
71.3/72.6

Table 12: Distinguishing in- and out-of-distribution test set data for image classiﬁcation. All values are
percentages. ↑ indicates larger value is better, and ↓ indicates lower value is better. The architecture is DenseNet.

CIFAR-80 and CIFAR-20 can help explain that the detection task becomes harder when in- and
out-of-distribution datasets locate on the same manifold.

Reciprocal results between datasets. In Table 12, we show the reciprocal results between datasets.
First, we train the DenseNet on the CIFAR-10 dataset (in-distribution) and evaluate the detection
performance on the CIFAR-100 dataset (out-distribution). Next, we train the DenseNet on the
CIFAR-100 dataset (in-distribution) and evaluate the detection performance on the CIFAR-10 dataset
(out-distribution). From Table 12, we can observe that the performance of the DenseNet trained on
CIFAR-10 is better than the performance of the DenseNet trained on CIFAR-100. This may be due to
the fact that the DenseNet has a higher test accuracy on CIFAR-10 (around 95%) compared to the
test accuracy on CIFAR-100 (around 77%).

27

8
1
0
2
 
b
e
F
 
5
2
 
 
]

G
L
.
s
c
[
 
 
4
v
0
9
6
2
0
.
6
0
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

ENHANCING THE RELIABILITY OF
OUT-OF-DISTRIBUTION IMAGE DETECTION IN
NEURAL NETWORKS

Shiyu Liang
Coordinated Science Lab, Department of ECE
University of Illinois at Urbana-Champaign
sliang26@illinois.edu

R. Srikant
Coordinated Science Lab, Department of ECE
University of Illinois at Urbana-Champaign
rsrikant@illinois.edu

Yixuan Li
Facebook Research
yixuanl@fb.com

ABSTRACT

We consider the problem of detecting out-of-distribution images in neural networks.
We propose ODIN, a simple and effective method that does not require any change
to a pre-trained neural network. Our method is based on the observation that using
temperature scaling and adding small perturbations to the input can separate the
softmax score distributions between in- and out-of-distribution images, allowing
for more effective detection. We show in a series of experiments that ODIN
is compatible with diverse network architectures and datasets. It consistently
outperforms the baseline approach (Hendrycks & Gimpel, 2017) by a large margin,
establishing a new state-of-the-art performance on this task. For example, ODIN
reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet
(applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%.

1

INTRODUCTION

Modern neural networks are known to generalize well when the training and testing data are sampled
from the same distribution (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016;
Cho et al., 2014; Zhang et al., 2017). However, when deploying neural networks in real-world
applications, there is often very little control over the testing data distribution. Recent works
have shown that neural networks tend to make high conﬁdence predictions even for completely
unrecognizable (Nguyen et al., 2015) or irrelevant inputs (Hendrycks & Gimpel, 2017; Szegedy et al.,
2014; Moosavi-Dezfooli et al., 2017). It has been well documented (Amodei et al., 2016) that it is
important for classiﬁers to be aware of uncertainty when shown new kinds of inputs, i.e., out-of-
distribution examples. Therefore, being able to accurately detect out-of-distribution examples can
be practically important for visual recognition tasks (Krizhevsky et al., 2012; Farabet et al., 2013; Ji
et al., 2013).

A seemingly straightforward approach of detecting out-of-distribution images is to enlarge the training
set of both in- and out-of-distribution examples. However, the number of out-of-distribution examples
can be inﬁnitely many, making the re-training approach computationally expensive and intractable.
Moreover, to ensure that a neural network accurately classiﬁes in-distribution samples into correct
classes while correctly detecting out-of-distribution samples, one might need to employ exceedingly
large neural network architectures, which further complicates the training process.

Hendrycks & Gimpel (2017) proposed a baseline method to detect out-of-distribution examples
without further re-training networks. The method is based on an observation that a well-trained neural
network tends to assign higher softmax scores to in-distribution examples than out-of-distribution
examples. In this paper, we go further. We observe that after using temperature scaling in the softmax
function (Hinton et al., 2015; Pereyra et al., 2017) and adding small controlled perturbations to inputs,

1

Published as a conference paper at ICLR 2018

the softmax score gap between in - and out-of-distribution examples is further enlarged. We will
show that the combination of these two techniques (temperature scaling and input perturbation) can
lead to better detection performance. For example, provided with a pre-trained DenseNet (Huang
et al., 2016) on CIFAR-10 dataset (positive samples), we test against images from TinyImageNet
dataset (negative samples). Our method reduces the False Positive Rate (FPR), i.e., the fraction of
misclassiﬁed out-of-distribution samples, from 34.7% to 4.3%, when 95% of in-distribution images
are correctly classiﬁed. We summarize the main contributions of this paper as the following:

•

•

We propose a simple and effective method, ODIN (Out-of-DIstribution detector for Neural net-
works), for detecting out-of-distribution examples in neural networks. Our method does not require
re-training the neural network and is easily implementable on any modern neural architecture.

We test ODIN on state-of-the-art network architectures (e.g., DenseNet (Huang et al., 2016) and
Wide ResNet (Zagoruyko & Komodakis, 2016)) under a diverse set of in- and out-distribution
dataset pairs. We show ODIN can signiﬁcantly improve the detection performance, and consistently
outperforms the state-of-the-art method (Hendrycks & Gimpel, 2017) by a large margin.

We empirically analyze how parameter settings affect the performance, and further provide simple
analysis that provides some intuition behind our method.

•

The outline of this paper is as follows. In Section 2, we present the necessary deﬁnitions and the
problem statement. In Section 3, we introduce ODIN and present performance results in Section 4.
We experimentally analyze the proposed method and provide some justiﬁcation for our method in
Section 5. We summarize the related works and future directions in Section 6 and conclude the paper
in Section 7.

2 PROBLEM STATEMENT

X

In this paper, we consider the problem of distinguishing in- and out-of-distribution images on a pre-
trained neural network. Let PX and QX denote two distinct data distributions deﬁned on the image
. Assume that a neural network f is trained on a dataset drawn from the distribution PX . Thus,
space
we call PX the in-distribution and QX the out-distribution, respectively. In testing, we draw new
images from a mixture distribution PX×Z deﬁned on
, where the conditional probability
distributions PX|Z=0 = PX and PX|Z=1 = QX denote in- and out-distribution respectively. Now
we focus on the following problem: Given an image X drawn from the mixture distribution PX×Z,
can we distinguish whether the image is from in-distribution PX or not?

0, 1
}

X × {

In this paper, we focus on detecting out-of-distribution images. However, it is equally important to
correctly classify an image into the right class if it is an in-distribution image. But this can be easily
done: once it has been detected that an image is in-distribution, we can simply use the original image
and run it through the neural network to classify it. Thus, we do not change the predictions of the
neural network for in-distribution images and only focus on improving the detection performance for
out-of-distribution images.

3 ODIN: OUT-OF-DISTRIBUTION DETECTOR

In this section, we present our method, ODIN, for detecting out-of-distribution samples. The detector
is built on two components: temperature scaling and input preprocessing. We describe the details of
both components below.
Temperature Scaling. Assume that the neural network f = (f1, ..., fN ) is trained to classify N
classes. For each input x, the neural network assigns a label ˆy(x) = arg maxi Si(x; T ) by computing
the softmax output for each class. Speciﬁcally,

Si(x; T ) =

exp (fi(x)/T )
j=1 exp (fj(x)/T )
R+ is the temperature scaling parameter and set to 1 during the training. For a given input
where T
x, we call the maximum softmax probability, i.e., Sˆy(x; T ) = maxi Si(x; T ) the softmax score. In
this paper, we use notations Sˆy(x; T ) and S(x; T ) interchangeably. Prior works have established
the use of temperature scaling to distill the knowledge in neural networks (Hinton et al., 2015) and

(cid:80)N

(1)

∈

,

2

Published as a conference paper at ICLR 2018

−

εsign(

x log Sˆy(x; T )),

calibrate the prediction conﬁdence in classiﬁcation tasks (Guo et al., 2017). As we shall see later,
a good manipulation of temperature T can push the softmax scores of in- and out-of-distribution
images further apart from each other, making the out-of-distribution images distinguishable.
Input Preprocessing. Before feeding the image x into the neural network, we preprocess the input
by adding small perturbations to it. The preprocessed image is given by
˜x = x

−∇
where the parameter ε can be interpreted as the perturbation magnitude. The method is inspired by the
idea in the reference (Goodfellow et al., 2015), where small perturbations are added to decrease the
softmax score for the true label and force the neural network to make a wrong prediction. Here, our
goal and setting are rather different: we aim to increase the softmax score of any given input, without
the need for a class label at all. As we shall see later, the perturbation can have stronger effect on the
in- distribution images than that on out-of-distribution images, making them more separable. Note
that the perturbations can be easily computed by back-propagating the gradient of the cross-entropy
loss w.r.t the input.
Out-of-distribution Detector. The proposed approach works as follows. For each image x, we ﬁrst
calculate the preprocessed image ˜x according to the equation (2). Next, we feed the preprocessed
image ˜x into the neural network, calculate its softmax score S( ˜x; T ) and compare the score to the
threshold δ. We say that the image x is an in-distribution example if the softmax score is above
the threshold and that the image x is an out-of-distribution example, otherwise. Therefore, the
out-of-distribution detector is given by

(2)

g(x; δ, T, ε) =

(cid:26)1
0

if maxi p( ˜x; T )
δ,
if maxi p( ˜x; T ) > δ.

≤

The parameters T, ε and δ are chosen so that the true positive rate (i.e., the fraction of in-distribution
images correctly classiﬁed as in-distribution images) under some out-of-distribution image data set
is 95%. (The choice of the out-of-distribution images to tune the parameters T, ε and δ appears to
be unimportant, as demonstrated in the appendix H.) Having chosen the parameters as above, we
evaluate the performance of our algorithm using various metrics in the next section.

4 EXPERIMENTS

4.1 TRAINING SETUP

In this section, we demonstrate the effectiveness of ODIN on several computer vision benchmark
datasets. We run all experiments with PyTorch1 and we will release the code to reproduce all
experimental results2.

Architectures and training conﬁgurations. We adopt two state-of-the-art neural network architec-
tures, including DenseNet (Huang et al., 2016) and Wide ResNet (Zagoruyko & Komodakis, 2016).
For DenseNet, our model follows the same setup as in (Huang et al., 2016), with depth L = 100,
growth rate k = 12 (Dense-BC) and dropout rate 0. In addition, we evaluate the method on a Wide
ResNet, with depth 28, width 10 (WRN-28-10) and dropout rate 0. Furthermore, in Appendix A.1, we
provide additional experimental results on another Wide ResNet with depth 40, width 4 (WRN-40-4).
The hyper-parameters of neural networks are set identical to the original Wide ResNet (Zagoruyko
& Komodakis, 2016) and DenseNet (Huang et al., 2016) implementations. All neural networks are
trained with stochastic gradient descent with Nesterov momentum (Duchi et al., 2011; Kingma & Ba,
2014). Speciﬁcally, we train Dense-BC for 300 epochs with batch size 64 and momentum 0.9; and
Wide ResNet for 200 epochs with batch size 128 and momentum 0.9. The learning rate starts at 0.1,
and is dropped by a factor of 10 at 50% and 75% of the training progress, respectively.

Accuracy.
Each neural network architecture is
trained on CIFAR-10 (C-10) and CIFAR-100 (C-100)
datasets (Krizhevsky & Hinton, 2009),
respectively.
CIFAR-10 and CIFAR-100 images are drawn from 10
and 100 classes, respectively. Both datasets consist of

1http://pytorch.org
2https://github.com/facebookresearch/odin

Architecture C-10 C-100

Dense-BC
WRN-28-10

4.81
3.71

22.37
19.86

Table 1: Test error rates on CIFAR-10
and CIFAR-100 datasets.

3

Published as a conference paper at ICLR 2018

50,000 training images and 10,000 test images. The test
error on CIFAR datasets are summarized in Table 1.

4.2 OUT-OF-DISTRIBUTION DATASETS

At test time, the test images from CIFAR-10 (CIFAR-100) datasets can be viewed as the in-distribution
(positive) examples. For out-of-distribution (negative) examples, we follow the setting in (Hendrycks
& Gimpel, 2017) and test on several different natural image datasets and synthetic noise datasets. All
the datasets considered are listed below.

(1) TinyImageNet. The Tiny ImageNet dataset3 consists of a subset of ImageNet images (Deng
et al., 2009). It contains 10,000 test images from 200 different classes. We construct two datasets,
TinyImageNet (crop) and TinyImageNet (resize), by either randomly cropping image patches of
size 32

32 or downsampling each image to size 32

32.

(2) LSUN. The Large-scale Scene UNderstanding dataset (LSUN) has a testing set of 10,000 images
of 10 different scenes (Yu et al., 2015). Similar to TinyImageNet, we construct two datasets,
LSUN (crop) and LSUN (resize), by randomly cropping and downsampling the LSUN testing set,
respectively.

×

×

(3) iSUN. The iSUN (Xu et al., 2015) consists of a subset of SUN images. We include the entire

collection of 8925 images in iSUN and downsample each image to size 32 by 32.

(4) Gaussian Noise. The synthetic Gaussian noise dataset consists of 10,000 random 2D Gaussian
noise images, where each RGB value of every pixel is sampled from an i.i.d Gaussian distribution
with mean 0.5 and unit variance. We further clip each pixel value into the range [0, 1].

(5) Uniform Noise. The synthetic uniform noise dataset consists of 10,000 images where each RGB
value of every pixel is independently and identically sampled from a uniform distribution on [0, 1].

4.3 EVALUATION METRICS

We adopt the following four different metrics to measure the effectiveness of a neural network in
distinguishing in- and out-of-distribution images.

(1) FPR at 95% TPR can be interpreted as the probability that a negative (out-of-distribution)
example is misclassiﬁed as positive (in-distribution) when the true positive rate (TPR) is as high as
95%. True positive rate can be computed by TPR = TP / (TP+FN), where TP and FN denote true
positives and false negatives respectively. The false positive rate (FPR) can be computed by FPR =
FP / (FP+TN), where FP and TN denote false positives and true negatives respectively.

(2) Detection Error, i.e., Pe measures the misclassiﬁcation probability when TPR is 95%. The
TPR) + 0.5FPR, where we assume that both positive

deﬁnition of Pe is given by Pe = 0.5(1
and negative examples have the equal probability of appearing in the test set.

−

(3) AUROC is the Area Under the Receiver Operating Characteristic curve, which is also a threshold-
independent metric (Davis & Goadrich, 2006). The ROC curve depicts the relationship between
TPR and FPR. The AUROC can be interpreted as the probability that a positive example is assigned
a higher detection score than a negative example (Fawcett, 2006). A perfect detector corresponds
to an AUROC score of 100%.

(4) AUPR is the Area under the Precision-Recall curve, which is another threshold independent
metric (Manning et al., 1999; Saito & Rehmsmeier, 2015). The PR curve is a graph showing
the precision=TP/(TP+FP) and recall=TP/(TP+FN) against each other. The metric AUPR-In and
AUPR-Out in Table 2 denote the area under the precision-recall curve where in-distribution and
out-of-distribution images are speciﬁed as positives, respectively.

4.4 EXPERIMENTAL RESULTS

Comparison with baseline. In Figure 1, we show the ROC curves when DenseNet-BC-100 is
evaluated on CIFAR-10 (positive) images against TinyImageNet (negative) test examples. The red
curve corresponds to the ROC curve when using baseline method (Hendrycks & Gimpel, 2017),

3https://tiny-imagenet.herokuapp.com

4

Published as a conference paper at ICLR 2018

Out-of-distribution
dataset

FPR
(95% TPR)
↓

Detection
Error
↓

AUROC

↑

AUPR
In
↑

AUPR
Out
↑

Baseline (Hendrycks & Gimpel, 2017) / ODIN

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

34.7/4.3
40.8/7.5
39.3/8.7
33.6/3.8
37.2/6.3
23.5/0.0
12.3/0.0

67.8/17.3
82.2/44.3
69.4/17.6
83.3/44.0
84.8/49.5
88.3/0.5
95.4/0.2

38.9/23.4
45.6/25.5
35.0/21.8
35.0/17.6
40.6/21.3
1.6/0.0
0.3/0.0

66.6/43.9
79.2/55.9
74.0/39.6
82.2/56.5
82.7/57.3
98.2/0.1
99.2/1.0

19.9/4.7
22.9/6.3
22.2/6.9
19.3/4.4
21.1/5.7
14.3/2.5
8.7/2.5

36.4/11.2
43.6/24.6
37.2/11.3
44.1/24.5
44.7/27.2
46.6/2.8
50.2/2.6

21.9/14.2
25.3/15.2
20.0/13.4
20.0/11.3
22.8/13.2
3.3/2.5
2.6/2.5

35.8/24.4
42.1/30.4
39.5/22.3
43.6/30.8
43.9/31.1
51.6/2.5
52.1/3.0

95.3/99.1
94.1/98.5
94.8/98.2
95.4/99.2
94.8/98.8
96.5/99.9
97.5/100.0

83.0/97.1
70.4/90.7
83.7/96.8
70.6/91.5
69.9/90.1
83.2/99.5
81.8/99.6

92.9/94.2
91.0/92.1
94.5/95.9
93.9/95.4
92.5/93.7
99.2/100.0
99.5/100.0

82.0/90.8
72.2/84.0
80.3/92.0
73.9/86.0
72.8/85.6
84.1/99.1
84.3/98.5

96.4/99.1
95.1/98.6
96.0/98.5
96.4/99.3
95.9/98.9
97.8/100.0
98.3/100.0

85.3/97.4
71.4/91.4
86.2/97.1
72.5/92.4
71.9/91.1
88.1/99.6
87.6/99.7

92.5/92.8
89.7/89.0
95.1/95.8
93.8/93.8
91.7/91.2
99.3/100.0
99.6/100.0

83.3/91.4
70.4/82.8
83.4/92.4
75.7/86.2
74.2/85.9
89.9/99.4
90.2/99.1

93.8/99.1
92.4/98.5
93.1/97.8
94.0/99.2
93.1/98.8
93.0/99.9
95.9/100.0

80.8/96.8
68.6/90.1
80.9/96.5
68.0/90.6
67.0/88.9
73.1/99.0
70.1/99.1

91.9/94.7
89.9/93.6
93.1/95.5
92.8/96.1
91.5/94.9
98.9/100.0
99.3/100.0

80.2/90.0
70.8/84.4
77.0/91.6
70.1/84.9
69.2/84.8
71.0/97.5
70.9/95.9

Dense-BC
CIFAR-10

Dense-BC
CIFAR-100

WRN-28-10
CIFAR-10

WRN-28-10
CIFAR-100

Table 2: Distinguishing in- and out-of-distribution test set data for image classiﬁcation. All values are
percentages. ↑ indicates larger value is better, and ↓ indicates lower value is better. All parameter settings are
shown in Appendix A.2. Additional results on WRN-40-4 and MNIST dataset are reported in Appendix A.1.

whereas the blue curve corresponds to our method with temperature T = 1000 and perturbation
magnitude ε = 0.0012. We observe a strikingly large gap between the blue and red ROC curves. For
example, when TPR= 95%, the FPR can be reduced from 34% to 4.2% by using our approach.
Choosing parameters. For each out-of-distribution dataset,
we randomly hold out 1,000 images for tuning the parameters
T and ε. For temperature T , we select among 1, 2, 5, 10,
20, 50, 100, 200, 500, 1000; and for perturbation magnitude
ε we choose from 21 evenly spaced numbers starting from 0
and ending at 0.004. The optimal parameters are chosen to
minimize the FPR at TPR 95% on the holdout set. We evaluate
the our approach on the remaining test images. All parameter
settings are reported in the Appendix A. We provide additional
details on the effect of parameters in Section 5.

Main results. The main results are summarized in Table 2. For
each in- and out-of-distribution dataset pair, we report both the
performance of the baseline (Hendrycks & Gimpel, 2017) and
our approach using temperature scaling and input preprocessing.
In Table 2, we observe improved performance across all neural
architectures and all dataset pairs. Noticeably, our method
consistently outperforms the baseline by a large margin when
measured by FPR at 95% TPR and detection error.

5

Figure 1:
(a) ROC curves of base-
line (red) and our method (blue)
on DenseNet-BC-100 network, where
CIFAR-10 and TinyImageNet (crop)
are in- and out-of-distribution dataset,
respectively.

Published as a conference paper at ICLR 2018

Figure 2: (a)-(d) Performance of our method vs. MMD between in- and out-of-distribution datasets. Neural
networks are trained on CIFAR-100 and CIFAR-80, respectively. The out-of-distribution datasets are 1: LSUN
(cop), 2: TinyImageNet (crop), 3: LSUN (resize), 4:
is iSUN (resize), 5: TinyImageNet (resize) and 6:
CIFAR-20.

4.5 EXTENSIONS

In this subsection, we analyze how the statistical distance be-
tween in- and out-of-distribution natural image dataset affects the detection performance of the
proposed method.
Data distribution distance vs. Detection performance. To measure the statistical distance between
in- and out-of-distribution datasets, we adopt a commonly used metric, maximum mean discrepancy
(MMD) with Gaussian RBF kernel (Sriperumbudur et al., 2010; Gretton et al., 2012; Sutherland et al.,
2016). Speciﬁcally, given two image sets, V =
, the maximum
v1, ..., vm
}
{
mean discrepancy between V and Q is deﬁned as

w1, ..., wm
{

and W =

}

2

(cid:92)MMD

(V, W ) =

1
(cid:0)m
2

(cid:1)

(cid:88)

i(cid:54)=j

k(vi, vj) +

1
(cid:1)
(cid:0)m
2

(cid:88)

i(cid:54)=j

k(wi, wj)

−

k(vi, wj),

(cid:88)

i(cid:54)=j

(cid:1)

2
(cid:0)m
2
(cid:17)

∪

,
·

) is the Gaussian RBF kernel, i.e., k(x, x(cid:48)) = exp
·

. We use the same method
where k(
used by Sutherland et al. (2016) to choose σ, where 2σ2 is set to the median of all Euclidean distances
between all images in the aggregate set V

W .

−

(cid:16)

(cid:107)x−x(cid:48)(cid:107)2
2
2σ2

In Figure 2 (a)(b), we show how the performance of ODIN varies against the MMD distances between
in- and out-of-distribution datasets4. The datasets (on x-axis) are ranked in the descending order of
MMD distances with CIFAR-100. There are two interesting observations can be drawn from these
ﬁgures. First, we ﬁnd that the MMD distances between the cropped datasets and CIFAR-100 tend
to be larger. This is likely due to the fact that cropped images only contain local image context and
are therefore more distinct from CIFAR-100 images, while resized images contain global patterns
and are thus similar to images in CIFAR-100. Second, we observe that the MMD distance tends to
be negatively correlated with the detection performance. This suggests, not surprisingly, that the
detection task becomes harder as in and out-of-distribution images are more similar to each other.
Same-manifold datasets. Furthermore, we investigate the extreme scenario when in- and out-of-
distribution datasets are on the same manifold. In experiment, we randomly split CIFAR-100 into
two disjoint datasets containing 80 and 20 classes each. We name them CIFAR-80 and CIFAR-
20, respectively. We train both DenseNet and Wide ResNet-28-10 on the CIFAR-80 dataset (in-
distribution) and evaluate the detection performance on the CIFAR-20 dataset (out-distribution). All
hyperparameters used here are exactly the same as in Section 4.1. The MMD distance between
CIFAR-20 and CIFAR-80 is much smaller than other dataset pairs. In Figure 2 (c)(d), we observe that
both FPR at TPR 95% and detection error become larger on the CIFAR-20 dataset. This coincides
with our expectation that the detection task becomes extremely hard when in- and out-of-distribution
dataset locate on the same manifold. We provide additional experimental results in Appendix A.1
and Appendix G.

5 DISCUSSIONS

5.1 EFFECTS OF PARAMETERS

In this subsection, we empirically show how temperature T and perturbation magnitude ε affect
FPR at TPR 95% and AUROC on DenseNet and Wide ResNet-28-10. Additional results on other

4All distances are provided in Appendix G.

6

Published as a conference paper at ICLR 2018

Figure 3: (a)(b) Effects of temperature T when ε = 0. (c)(d) Effects of perturbation magnitude ε when T = 1.
All networks are trained on CIFAR-10 (in-distribution). Additional results on other metrics and Wide ResNet-40
are provided in Appendix B.

Figure 4: (a)(b) Effects of perturbation magnitude ε on DenseNet when T is large (e.g., T = 1000). (c)(d)
Effects of perturbation magnitude of ε on Wide-ResNet-28-10 when T is large (e.g., T = 1000). All networks
are trained on CIFAR-10. Additional results on other metrics and Wide ResNet-40 are provided in Appendix B.

metrics and architectures are provided in Appendix B. We show the detection performance when
using only the temperature scaling method (see Figure 3(a)(b), ε = 0), or the input preprocessing
method (see Figure 3(c)(d), T = 1). In Figure 4, we show the detection performance w.r.t ε when T
is optimal (e.g., T =1000). First, from Figure 3 (a)(b), we observe that increasing the temperature
can improve the detection performance, although the effects diminish when T is sufﬁciently large
(e.g., T > 100). Next, from Figure 3(c)(d) and Figure 4, we observe that we can further improve
the detection performance by appropriately choosing the perturbation magnitudes. We can achieve
overall better performance by combining both (1) temperature scaling and (2) input preprocessing.

5.2 ANALYSIS ON TEMPERATURE SCALING

In this subsection, we analyze the effectiveness of the temperature scaling method. As shown
in Figure 3 (a) and (b), we observe that a sufﬁciently large temperature yields better detection
performance although the effects diminish when T is too large. To gain insight, we can use the Taylor
expansion of the softmax score (details provided in Appendix D). When T is sufﬁciently large, we
have

Sˆy(x; T )

≈

N

(cid:80)

1
T

i[fˆy(x)

1
fi(x)] + 1
2T 2

(cid:80)

i[fˆy(x)

,

fi(x)]2

−
by omitting the third and higher orders. For simplicity of notation, we deﬁne

−

−

(3)

U1(x) =

[fˆy(x)

fi(x)]

and U2(x) =

1

N

1

−

(cid:88)

i(cid:54)=ˆy

−

1

N

1

−

(cid:88)

i(cid:54)=ˆy

[fˆy(x)

fi(x)]2.

(4)

−

Interpretations of U1 and U2. By deﬁnition, U1 measures the extent to which the largest unnormal-
ized output of the neural network deviates from the remaining outputs; while U2 measures the extent
to which the remaining smaller outputs deviate from each other. We provide formal mathematical
derivations in Appendix F. In Figure 5(a), we show the distribution of U1 for each out-of-distribution
dataset vs. the in-distribution dataset (in red). We observe that the largest outputs of the neural
network on in-distribution images deviate more from the remaining outputs. This is likely due to the
fact that neural networks tend to make more conﬁdent predictions on in-distribution images.
U1], for each
Further, we show in Figure 5(b) the expectation of U2 conditioned on U1, i.e., E[U2|
dataset. The red curve (in-distribution images) has overall higher expectation. This indicates that,

7

Published as a conference paper at ICLR 2018

Figure 5: (a) Probability density of U1 under different datasets on DenseNet. (b) Expectations of U2 conditioned
on U1 on DenseNet. (c) Probability density of the norm of gradient on DenseNet under temperature 1, 000.
(c)(d) Expectation of the norm of gradient conditioned on the softmax scores on DenseNet under temperature
T = 1000 and T = 1, respectively. (f)(g) Outputs of DenseNet on each class for an image of dog from
CIFAR-10 and an image from TinyImageNet (crop). The DenseNet is trained on CIFAR-10. Additional results
on other architectures are provided in Appendix C.

when two images have similar values on U1, the in-distribution image tends to have a much higher
value of U2 than the out-of-distribution image.
In other words, for in-distribution images, the
remaining outputs (excluding the largest output) tend to be more separated from each other compared
to out-of-distribution datasets. This may happen when some classes in the in-distribution dataset
share common features while others differ signiﬁcantly. To illustrate this, in Figure 5 (f)(g), we show
the outputs of each class using a DenseNet (trained on CIFAR-10) on a dog image from CIFAR-10,
and another image from TinyImageNet (crop). For the image of dog, we can observe that the largest
output for the label dog is close to the output for the label cat but is quite separated from the outputs
for the label car and truck. This is likely due to the fact that, in CIFAR-10, images of dogs are very
similar to the images of cats but are quite distinct from images of car and truck. For the image from
TinyImageNet (crop), despite having one large output, the remaining outputs are close to each other
and thus have a smaller deviation.
The effects of T . To see the usefulness of adopting a large T , we can ﬁrst rewrite the softmax score
function in Equation (3) as S
U2/2T )/T . Hence the softmax score is largely determined
(U1 −
by U1 and U2/2T . As noted earlier, U1 makes in-distribution images produce larger softmax scores
than out-of-distribution images since S
U2.
Therefore, by choosing a sufﬁciently large temperature, we can compensate the negative impacts of
U2/2T on the detection performance, making the softmax scores between in- and out-of-distribution
images more separable. Eventually, when T is sufﬁciently large, the distribution of softmax score is
almost dominated by the distribution of U1 and thus increasing the temperature further is no longer
effective. This explains why we see in Figure 3 (a)(b) that the performance does not change when T
is too large (e.g., T > 100). In Appendix E, we provide a formal proof showing that the detection
error eventually converges to a constant number when T goes to inﬁnity.

U1, while U2 has the exact opposite effect since S

∝ −

∝

∝

5.3 ANALYSIS ON INPUT PREPROCESSING
As noted previously, using the temperature scaling method by itself can be effective in improving the
detection performance. However, the effectiveness quickly diminishes as T becomes very large. In
order to make further improvement, we complement temperature scaling with input preprocessing.
This has already been seen in Figure 4, where the detection performance is improved by a large
margin on most datasets when T = 1000, provided with an appropriate perturbation magnitude ε is
chosen. In this subsection, we provide some intuition behind this.

To explain, we can look into the ﬁrst order Taylor expansion of the log-softmax function for the
perturbed image ˜x, which is given by

log Sˆy( ˜x; T ) = log Sˆy(x; T ) + ε

x log Sˆy(x; T )

(cid:107)∇

(cid:107)1 + o(ε),

where x is the original input.
The effects of gradient.
(cid:107)1 — the
In Figure 5 (c), we present the distribution of
1-norm of gradient of log-softmax with respect to the input x — for all datasets. A salient observation

x log S(x; T )

(cid:107)∇

8

Published as a conference paper at ICLR 2018

≈

is that CIFAR-10 images (in-distribution) tend to have larger values on the norm of gradient than
most out-of-distribution images. To further see the effects of the norm of gradient on the softmax
score, we provide in Figures 5 (d) the conditional expectation E[
S]. We can
observe that, when an in-distribution image and an out-of-distribution image have the same softmax
score, the value of

x log S(x; T )

x log S(x; T )

(cid:107)1 for in-distribution image tends to be larger.

(cid:107)1|

(cid:107)∇

(cid:107)∇

We illustrate the effects of the norm of gradient in Figure 6. Suppose
that an in-distribution image x1 (blue) and an out-of-distribution
image x2 (red) have similar softmax scores, i.e., S(x1)
S(x2).
After input processing, the in-distribution image can have a much
larger softmax score than the out-of-distribution image x2 since x1
results in a much larger value on the norm of softmax gradient than
that of x2. Therefore, in- and out-of-distribution images are more
separable from each other after input preprocessing5.
The effect of ε. When the magnitude ε is sufﬁciently small, adding
perturbations does not change the predictions of the neural network,
i.e., ˆy( ˜x) = ˆy(x). However, when ε is not negligible, the gap of
softmax scores between in- and out-of-distribution images can be
(cid:107)1. Our observation is consistent with
affected by
that in (Szegedy et al., 2014; Goodfellow et al., 2015; Moosavi-
Dezfooli et al., 2017), which show that the softmax scores tend to change signiﬁcantly if small
perturbations are added to the in-distribution images. It is also worth noting that using a very large ε
can lead to performance degradation, as seen in Figure 4. This is likely due to the fact that the second
and higher order terms in the Taylor expansion are no longer insigniﬁcant when the perturbation
magnitude is too large.

Figure 6: Illustration of effects
of the input preprocessing.

x log S(x; T )

(cid:107)∇

6 RELATED WORKS AND FUTURE DIRECTIONS

The problem of detecting out-of-distribution examples in low-dimensional space has been well-studied
in various contexts (see the survey by Pimentel et al. (2014)). Conventional methods such as density
estimation, nearest neighbor and clustering analysis are widely used in detecting low-dimensional out-
of-distribution examples (Chow, 1970; Vincent & Bengio, 2003; Ghoting et al., 2008; Devroye et al.,
2013), . The density estimation approach uses probabilistic models to estimate the in-distribution
density and declares a test example to be out-of-distribution if it locates in the low-density areas.
The clustering method is based on the statistical distance, and declares an example to be out-of-
distribution if it locates far from its neighborhood. Despite various applications in low-dimensional
spaces, unfortunately, these methods are known to be unreliable in high-dimensional space such as
image space (Wasserman, 2006; Theis et al., 2015). In recent years, out-of-distribution detectors based
on deep models have been proposed. Schlegl et al. (2017) train a generative adversarial networks to
detect out-of-distribution examples in clinical scenario. Sabokrou et al. (2016) train a convolutional
network to detect anomaly in scenes. Andrews et al. (2016) adopt transfer representation-learning
for anomaly detection. All these works require enlarging or modifying the neural networks. In
a more recent work, Hendrycks & Gimpel (2017) found that pre-trained neural networks can be
overconﬁdent to out-of-distribution example, limiting the effectiveness of detection. Our paper aims
to improve the performance of detecting out-of-distribution examples, without requiring any change
to an existing well-trained model.

Our approach leverages the following two interesting observations to help better distinguish between
in- and out-of-distribution examples: (1) On in-distribution images, modern neural networks tend to
produce outputs with larger variance across class labels, and (2) neural networks have larger norm
of gradient of log-softmax scores when applied on in-distribution images. We believe that having a
better understanding of these phenomenon can lead to further insights into this problem.

7 CONCLUSIONS

In this paper, we propose a simple and effective method to detect out-of-distribution data samples
in neural networks. Our method does not require retraining the neural network and signiﬁcantly

5Similar observation can be seen when T = 1, where we present the conditional expectation of the norm of

softmax gradient in Figure 5 (e).

9

Published as a conference paper at ICLR 2018

improves on the baseline (state-of-the-art) on different neural architectures across various in and
out-distribution dataset pairs. We empirically analyze the method under different parameter settings,
and provide some insights behind the approach. Future work involves exploring our method in other
applications such as speech recognition and natural language processing.

The research reported here was supported by NSF Grant CPS ECCS 1739189.

ACKNOWLEDGMENTS

REFERENCES

Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané.

Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.

Jerone T.A Andrews, Thomas Tanay, Edward J. Morton, and Lewis D. Grifﬁn. Transfer representation-

learning for anomaly detection. In ICML, 2016.

Yaroslav Bulatov. notmnist dataset. 2011.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for
statistical machine translation. EMNLP, 2014.

C Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on information theory,

Jesse Davis and Mark Goadrich. The relationship between precision-recall and roc curves. In ICML.

16(1):41–46, 1970.

ACM, 2006.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale

hierarchical image database. In CVPR, 2009.

Luc Devroye, László Györﬁ, and Gábor Lugosi. A probabilistic theory of pattern recognition,

volume 31. Springer Science & Business Media, 2013.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.

Clement Farabet, Camille Couprie, Laurent Najman, and Yann LeCun. Learning hierarchical features
for scene labeling. IEEE transactions on pattern analysis and machine intelligence, 35(8):1915–
1929, 2013.

Tom Fawcett. An introduction to roc analysis. Pattern recognition letters, 2006.

Amol Ghoting, Srinivasan Parthasarathy, and Matthew Eric Otey. Fast mining of distance-based
outliers in high-dimensional datasets. Data Mining and Knowledge Discovery, 16(3):349–364,
2008.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. ICLR, 2015.

Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola. A

kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723–773, 2012.

Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural

networks. arXiv preprint arXiv:1706.04599, 2017.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image

recognition. In CVPR, 2016.

Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution

examples in neural networks. ICLR, 2017.

10

Published as a conference paper at ICLR 2018

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv

preprint arXiv:1503.02531, 2015.

preprint arXiv:1608.06993, 2016.

Gao Huang, Zhuang Liu, and Kilian Q Weinberger. Densely connected convolutional networks. arXiv

Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolutional neural networks for human action
recognition. IEEE transactions on pattern analysis and machine intelligence, 35(1):221–231,
2013.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097–1105,
2012.

Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning

through probabilistic program induction. Science, 2015.

Christopher D Manning, Hinrich Schütze, et al. Foundations of statistical natural language processing,

volume 999. MIT Press, 1999.

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal

adversarial perturbations. CVPR, 2017.

Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High conﬁdence

predictions for unrecognizable images. 2015.

Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, and Geoffrey Hinton. Regularizing

neural networks by penalizing conﬁdent output distributions. ICLR, 2017.

Marco AF Pimentel, David A Clifton, Lei Clifton, and Lionel Tarassenko. A review of novelty

detection. Signal Processing, 99:215–249, 2014.

Mohammad Sabokrou, Mohsen Fayyaz, Mahmood Fathy, et al. Fully convolutional neural network

for fast anomaly detection in crowded scenes. arXiv preprint arXiv:1609.00866, 2016.

Takaya Saito and Marc Rehmsmeier. The precision-recall plot is more informative than the roc plot
when evaluating binary classiﬁers on imbalanced datasets. PloS one, 10(3):e0118432, 2015.

Thomas Schlegl, Philipp Seeböck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs.
Unsupervised anomaly detection with generative adversarial networks to guide marker discovery.
In International Conference on Information Processing in Medical Imaging, pp. 146–157. Springer,
2017.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image

recognition. ICLR, 2015.

Bharath K Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Schölkopf, and Gert RG
Lanckriet. Hilbert space embeddings and metrics on probability measures. Journal of Machine
Learning Research, 11(Apr):1517–1561, 2010.

Dougal J Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alex
Smola, and Arthur Gretton. Generative models and model criticism via optimized maximum mean
discrepancy. ICLR, 2016.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,

and Rob Fergus. Intriguing properties of neural networks. NIPS, 2014.

Lucas Theis, Aäron van den Oord, and Matthias Bethge. A note on the evaluation of generative

models. ICLR, 2015.

11

Published as a conference paper at ICLR 2018

Pascal Vincent and Yoshua Bengio. Manifold parzen windows. In Advances in neural information

processing systems, pp. 849–856, 2003.

Larry Wasserman. All of Nonparametric Statistics. Springer, 2006.

Pingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R Kulkarni, and Jianxiong
Xiao. Turkergaze: Crowdsourcing saliency with webcam based eye tracking. arXiv preprint
arXiv:1504.06755, 2015.

Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-
scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365,
2015.

Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,

2016.

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding

deep learning requires rethinking generalization. ICLR, 2017.

12

Published as a conference paper at ICLR 2018

A SUPPLEMENTARY RESULTS IN SECTION 4.4
A.1 EXPERIMENTAL RESULTS

Out-of-distribution
dataset

FPR
(95% TPR)
↓

Detection
Error
↓

AUROC

↑

AUPR
In
↑

AUPR
Out
↑

Baseline (Hendrycks & Gimpel, 2017) / Ours

WRN-40-4
CIFAR-10

WRN-40-4
CIFAR-100

Dense-BC
CIFAR-80

WRN-28-10
CIFAR-80

WRN-40-4
CIFAR-80

MNIST

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

CIFAR-20
TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

CIFAR-20
TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

CIFAR-20
TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

Omniglot
notMNIST
CIFAR-10bw
Gaussian
Uniform

49.8/36.7
62.3/49.1
34.6/23.0
54.5/35.1
58.6/41.0
26.6/3.2
21.8/0.9

66.9/43.3
78.1/55.1
74.9/35.9
77.9/50.0
79.5/52.9
84.7/3.3
77.2/3.1

84.1/81.1
72.9/22.7
84.4/46.3
67.1/20.9
84.9/45.9
86.1/50.2
100.0/0.9
98.5/1.2

80.4/78.3
71.3/46.7
81.0/48.8
74.4/45.5
81.9/49.0
82.7/51.1
99.6/1.4
100.0/0.4

82.4/78.4
68.3/34.3
80.6/53.5
72.2/33.2
79.1/51.2
81.2/53.2
99.7/48.6
99.7/10.7

0.2/0.0
10.3/8.7
0.1/0.0
0.0/0.0
0.0/0.0

27.4/20.9
33.6/27.1
19.8/14.0
29.8/20.1
31.8/23.0
15.8/4.1
13.4/3.0

36.0/24.1
41.5/30.1
40.0/20.4
41.5/27.5
42.2/28.9
44.9/4.2
41.1/4.0

44.9/43.0
39.0/13.8
44.7/25.6
36.0/12.9
45.0/25.4
50.5/27.6
52.5/3.0
51.8/3.1

42.7/41.6
38.1/25.9
43.0/26.9
39.7/25.2
43.5/27.0
43.9/28.1
52.3/3.2
52.5/2.7

43.7/41.7
36.6/19.6
42.8/29.2
38.6/19.1
42.0/28.1
43.1/29.1
52.4/26.8
52.4/7.8

2.6/2.5
7.7/6.8
2.5/2.5
2.5/2.5
2.5/2.5

87.3/89.3
79.3/81.6
93.4/95.1
84.7/87.0
82.1/84.9
96.1/99.2
96.5/99.7

81.3/88.5
72.6/81.6
79.1/90.8
75.2/85.6
74.3/84.3
86.3/98.8
86.4/99.0

76.6/77.8
83.4/96.2
76.8/91.7
84.6/96.2
77.5/91.8
76.1/90.5
64.3/98.6
80.4/99.6

79.2/80.4
83.1/91.9
77.1/89.2
82.0/92.9
78.8/90.1
78.3/89.4
80.6/98.9
79.7/99.1

76.8/78.1
83.6/93.4
76.2/87.7
83.1/93.4
77.6/88.8
76.2/87.7
65.6/93.8
74.3/97.7

85.1/86.7
73.5/76.9
93.1/94.3
79.8/82.6
76.4/80.2
97.0/99.2
97.5/99.7

80.6/87.2
69.4/78.0
81.4/89.9
73.1/83.5
72.9/81.9
90.5/99.1
90.2/99.2

79.4/80.6
86.3/96.6
80.3/92.7
86.9/96.4
81.4/92.9
79.8/91.3
78.4/99.1
86.7/99.6

81.5/82.2
85.9/92.6
80.0/89.5
84.4/93.0
82.2/90.8
81.5/90.0
87.7/99.2
87.4/99.4

78.9/79.1
85.9/94.0
78.5/88.3
86.3/93.7
80.0/89.4
78.7/88.3
77.3/95.7
83.0/98.4

87.2/90.7
80.6/84.8
92.4/95.2
85.3/89.7
83.2/87.8
94.8/99.2
94.7/99.7

80.1/89.1
71.6/83.4
76.3/91.5
73.3/86.4
71.9/85.1
77.0/97.9
78.6/98.6

71.6/73.6
79.9/95.8
71.5/90.4
82.1/96.0
71.6/90.2
69.9/88.8
52.2/96.6
68.0/99.1

74.2/76.2
79.7/90.7
72.6/88.5
78.2/91.5
73.4/88.8
72.6/88.0
66.8/97.6
65.5/98.0

72.2/75.0
81.2/92.5
72.5/86.1
79.7/93.1
73.9/87.3
72.2/86.1
53.7/88.7
61.2/95.2

99.6/100.0
97.2/98.2
99.7/100.0
99.7/100.0
99.9/100.0

99.7/100.0
97.5/98.4
99.8/100.0
99.8/100.0
99.9/100.0

99.5/100.0
97.4/98.0
99.7/100.0
99.7/100.0
99.9/100.0

Table 3: Distinguishing in- and out-of-distribution test set data for image classiﬁcation. All values are
percentages. ↑ indicates larger value is better, and ↓ indicates lower value is better.

MNIST: We used the same MNIST classiﬁer used by Hendrycks & Gimpel (2017), which is a
three-layer, 256 neuron-wide, fully connected network trained for 30 epochs with Adam (Kingma
& Ba, 2014). The classiﬁer achieve 99.34% test accuracy on the MNIST test set. We compare our
method with the baseline (Hendrycks & Gimpel, 2017) on ﬁve different out-of-distribution datasets:
(1) Omniglot dataset (Lake et al., 2015) contains images of handwritten characters in stead of the
handwritten digits in MNIST; (2) notMNIST (Bulatov, 2011) dataset contains typeface characters;

13

Published as a conference paper at ICLR 2018

(3) CIFAR-10bw contains black and white rescaled CIFAR-10 images; (4)(5) Gaussian and Uniform
image set contains the synthetic Gaussian and Uniform noise images used in Section 4.2.
Wide ResNet-40-4: We use the same architecture used by Hendrycks & Gimpel (2017) to evaluate
the baseline and our method. The Wide ResNet-40-4 achieves 95.7% test accuracy on CIFAR-10
dataset and achieve 79.27% test accuracy on CIFAR-100.
CIFAR-80: DenseNet-BC-100 achieves 78.94% test accuracy on CIFAR-80, while Wide ResNet-
28-10 achieves 81.71% test accuracy and Wide ResNet-40-4 achieves 79.53% test accuracy on
CIFAR-80.

A.2 PARAMETER SETTINGS

For MNIST, we set T = 1000 and ε = 0. The parameter settings for other structures are shown as
follows.

DenseNet-BC-100

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

0.0014
0.0014
0
0.0014
0.0014
0.0014
0.0014
-

0.002
0.0022
0.0036
0.002
0.002
0.0028
0.0026
0.0002

0.002
0.0022
0.0038
0.0018
0.002
0.0024
0.0028
-

Table 4: Optimal perturbation magnitude ε for reproducing main results in Table 2 and 3.

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

Table 5: Optimal Temperature T for reproducing main results in Table 2 and 3.

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

DenseNet-BC-100

1000
1000
1000
1000
1000
1000
1000
-

1000
1000
1000
1000
1000
1
1
1

1000
1000
1000
1000
1000
1
1
-

Wide-ResNet-28-10

0.0005
0.0011
0
0.0006
0.0008
0.0014
0.0014
-

0.0002
0.0004
0.0002
0.0002
0.0002
0.0002
0.0002
5e-05

0.0026
0.0024
0.0038
0.0026
0.0026
0.0032
0.0032
-

14

Table 6: Optimal perturbation magnitude ε for reproducing main results in Table 2 and 3.

Published as a conference paper at ICLR 2018

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

Wide-ResNet-28-10

1000
1000
1000
1000
1000
1000
1000
-

1000
1000
1000
1000
1000
1
1
1

1000
1000
1000
1000
1000
1000
1000
-

Table 7: Optimal Temperature T for reproducing main results in Table 2 and 3.

Wide-ResNet-40-4

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

0.0004
0.0008
0
0.001
0.0008
0.0016
0.0016
-

0.0002
0.0004
0.0002
0.0002
0.0002
0.0002
0.0002
0.0002

0.0014
0.0016
0.0038
0.0014
0.0016
0.0024
0.0026
-

Table 8: Optimal perturbation magnitude ε for reproducing main results in Table 2 and 3.

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

Wide-ResNet-40-4

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

1000
1000
1000
1000
1000
1
1
1

1000
1000
1000
1000
1000
1000
1000
-

Table 9: Optimal Temperature T for reproducing main results in Table 2 and 3.

1000
1000
1000
1000
1000
1000
1000
-

15

Published as a conference paper at ICLR 2018

B SUPPLEMENTARY RESULTS IN SECTION 5.1

Figure 7: Detection performance on DenseNet, Wide ResNet-28-10 and Wide ResNet-40-4 under different
temperature, when input preprocessing is not used, i.e., ε = 0. All networks are trained on CIFAR-10.

16

Published as a conference paper at ICLR 2018

Figure 8: Detection performance on DenseNet, Wide ResNet-28-10 and Wide ResNet-40-4 under different
perturbation magnitude, when temperature scaling is not used, i.e., T = 1. All networks are trained on
CIFAR-10.

17

Published as a conference paper at ICLR 2018

Figure 9: Detection performance on DenseNet, Wide ResNet-28-10 and Wide ResNet-40-4 under different
perturbation magnitude, when the optimal temperature is used, i.e., T = 1000. All networks are trained on
CIFAR-10.

18

Published as a conference paper at ICLR 2018

C SUPPLEMENTARY RESULTS IN SECTION 5.2 AND 5.3

Figure 10: Expectation of the second order term U2 conditioned on the ﬁrst order term U1 under DenseNet,
Wide-ResNet-28-10 and Wide ResNet-40-4. All networks are trained on CIFAR-10.

Figure 11: Expectation of gradient norms conditioned on the softmax scores under DenseNet, Wide-ResNet-28-
10 and Wide ResNet-40-4, where the temperature scaling is not used. All networks are trained on CIFAR-10.

Figure 12: Expectation of gradient norms conditioned on the softmax scores under DenseNet, Wide-ResNet-28-
10 and Wide ResNet-40-4, where the optimal temperature is used, i.e., T = 1000. All networks are trained on
CIFAR-10.

19

In this section, we present the Taylor expansion of the soft-max score function:

Published as a conference paper at ICLR 2018

D TAYLOR EXPANSION

Sˆy(x; T ) =

(cid:80)N

exp (fˆy(x)/T )
i=1 exp(fi(x)/T )
1
(cid:16) fi(x)−f ˆy(x)
T

i=1 exp

(cid:17)

(cid:80)N

=

=

1
+ 1
2!

1
fi(x)] + 1
2T 2

(cid:104)

(cid:80)N

i=1

1 + fi(x)−f ˆy(x)

T

(fi(x)−f ˆy(x))2
T 2

+ o (cid:0) 1
T 2

(cid:1)(cid:105)

by Taylor expansion

≈

N

−

1
T

(cid:80)N

i=1[fˆy(x)

−

(cid:80)N

i=1[fi(x)

fˆy(x)]2

−

E PROPOSITION 1

The following proposition 1 shows that the detection error Pe(T, 0)
Thus, increasing the temperature further can only slightly improve the detection performance.
Proposition 1. There exists a constant c only depending on function U1, in-distribution PX and
out-of-distribution QX such that limT →∞ Pe(T, ε) = c, when ε = 0 (i.e., no input preprocessing).
Proof. Since

c if T is sufﬁciently large.

≈

Therefore, for any X,
(cid:18)

Sˆy(X; T ) =

exp(fˆy(X)/T )
i=1 exp(fi(X)/T )

(cid:80)N

=

1 + (cid:80)

1

i(cid:54)=ˆy exp([fi(X)

fˆy(X)]/T )

−

lim
T →∞

T

1
Sˆy(X; T )

−

(cid:19)

+ N

= lim
T →∞

(cid:20)
1

T

(cid:88)

i(cid:54)=ˆy

exp

−

(cid:18) fi(X)

fˆy(X)

(cid:19)(cid:21)

−
T

=

[fˆy(X)

fi(X)] = (N

1)U1(X)

−

−

(cid:88)

i(cid:54)=ˆy

This indicates that the random variable

(cid:18)

T

1
Sˆy(X; T )

−

(cid:19)

+ N

(N

1)U1(X) a.s.

→

−

as T
the false positive rate

→ ∞

. This means that for a speciﬁc α > 0, choosing the threshold δT = 1/(N

α/T ), then

FPR(T ) = QX (Sˆy(X; T ) > 1/(N

α/T )) = QX

T

N

−

−

−

(cid:18)

(cid:18)

1
Sˆy(X; T )

−

(cid:19)

(cid:19)

> α

QX ((N

1)U1(X) > α) ,

(cid:18)

(cid:18)

1
Sˆy(X; T )

−

(cid:19)

(cid:19)

> α

PX ((N

1)U1(X) > α) .

−

−

T →∞
−−−−→

T →∞
−−−−→

and the true positive rate

TPR(T ) = PX (Sˆy(X; T ) > 1/(N

α/T )) = PX

T

N

Choosing α∗ such that PX ((N
at the same time FPR(T )
depending on U1, PX , QX and PZ, such that

QX ((N

→

−

−

1)U1(X) > α∗) = 0.95, then TPR(T )
1)U1(X) > α∗) as T

0.95 as T

and
. There exists a constant c

→ ∞

→

→ ∞

lim
T →∞

Pe(T, 0) = 0.05P (Z = 0) + P (Z = 1)QX ((N

1)U1(X) > α∗) = c.

−

20

Published as a conference paper at ICLR 2018

F ANALYSIS OF TEMPERATURE

For simplicity of the notations, let ∆i = fˆy
mean of the set ∆. Therefore,

−

fi and thus ∆ =

∆i

i(cid:54)=ˆy. Besides, let ¯∆ denote the
}

{

¯∆ =

1

N

1

−

(cid:88)

i(cid:54)=ˆy

∆i =

1

N

1

−

(cid:88)

i(cid:54)=ˆy

[fˆy

fi] = U1.

−

Equivalently,

Next, we will show

U1 = Mean(∆).

U2 =

1

N

1

−

(cid:88)

i(cid:54)=ˆy

[fˆy

−

(cid:122)

1

fi]2 =

Variance2(∆)
(cid:125)(cid:124)
(cid:88)

[∆i

Mean2(∆)
(cid:122)(cid:125)(cid:124)(cid:123)
¯∆2

.

(cid:123)
¯∆]2 +

N

1

−

i(cid:54)=ˆy

−

by∆i = fˆy

fi

−

Since

then

U2 =

∆2
i

(cid:88)

i(cid:54)=ˆy
(cid:88)

i(cid:54)=ˆy
(cid:88)

i(cid:54)=ˆy

(cid:88)

1

−
1

−
1

−
1

−

1

1

1

1

N

N

N

N

(cid:124)

=

=

=

¯∆ + ¯∆)2

(∆i

−

[(∆i

¯∆)2

−

−

2(∆i

−

¯∆) ¯∆ + ¯∆2]

¯∆]2

[∆i

−

i(cid:54)=ˆy
(cid:123)(cid:122)
Variance2(∆)

−

N

1

−

(cid:125)

(cid:124)

i(cid:54)=ˆy
(cid:123)(cid:122)
=0

2 ¯∆

(cid:88)

(∆i

−

¯∆)

+ ¯∆2

(cid:125)

(cid:124)(cid:123)(cid:122)(cid:125)
Mean2(∆)

U2 = Variance2(∆) + Mean2(∆)

G ADDITIONAL RESULTS IN SECTION 4.5

Apart from the Maximum Mean Discrepancy, we as well calculate the Energy distance between in-
and out-of-distribution datasets. Let P and Q denote two different distributions. Then the energy
distance between distributions P and Q is deﬁned as

D2

energy(P, Q) = 2EV ∼P,W ∼Q

X
(cid:107)

Y

−

(cid:107) −

EV,V (cid:48)∼P

X (cid:48)

X
(cid:107)

−

(cid:107) −

EW,W (cid:48)∼Q

Y
(cid:107)

Y (cid:48)

.

(cid:107)

−

Therefore, the energy distance between two datasets V =
W1, ..., Wm
{

Q is deﬁned as

iid
∼

}

V1, ..., Vm
{

}

iid
∼

P and W =

2

(cid:92)Denergy

(P, Q) =

2
m2

m
(cid:88)

m
(cid:88)

i=1

j=1

Vi
(cid:107)

−

Wj

(cid:107) −

1
(cid:1)
(cid:0)m
2

(cid:88)

i(cid:54)=j

Vi

(cid:107)

−

Vj

(cid:107) −

1
(cid:1)
(cid:0)m
2

(cid:88)

i(cid:54)=j

Wi
(cid:107)

Wj

.
(cid:107)

−

In the experiment, we use the 2-norm

(cid:107) · (cid:107)2.

21

Published as a conference paper at ICLR 2018

In-distribution Out-of-distribution
datasets

Datasets

MMD
Distance Distance

Energy

CIFAR-100

CIFAR-80

Tiny-ImageNet (crop)
LSUN (crop)
Tiny-ImageNet (resize)
LSUN (resize)
iSUN (resize)

Tiny-ImageNet (crop)
LSUN (crop)
Tiny-ImageNet (resize)
LSUN (resize)
iSUN (resize)
CIFAR-20

0.41
0.43
0.088
0.12
0.11

0.4
0.43
0.095
0.120
0.116
0.057

2.25
2.31
0.54
0.63
0.56

2.22
2.29
0.57
0.62
0.61
0.35

22

Published as a conference paper at ICLR 2018

Figure 13: False positive rate (FPR) and true positive rate (TPR) under different thresholds (δ) when the
temperature (T ) is set to 1, 000 and the perturbation magnitude (ε) is set to 0.0014. The DenseNet is trained on
CIFAR-10.

Figure 14: Detection performance on Tiny-ImageNet (resize), LSUN (resize) and iSUN (resize) when parame-
ters are tuned on six different out-of-distribution datasets. Each tuning set contains 1,000 images and each test
set contains 9,000 images. Both DenseNet and Wide-ResNet are trained on CIFAR-10. Additional results on
other datasets are provide in Table 10 and 11.

H ADDITIONAL DISCUSSIONS

In this section, we present additional discussion on the proposed method. We ﬁrst empirically show
how the threshold δ affects the detection performance. We next show how the proposed method
performs when the parameters are tuned on a certain out-of-distribution dataset and are evaluated on
other out-of-distribution datasets. Finally, we show how the size of dataset for choosing parameters
affects the detection performance.
Effects of the threshold. We analyze how the threshold affects the following metrics: (1) FPR, i.e.,
the fraction of out-of-distribution images misclassiﬁed as in-distribution images; (2) TPR, i.e, the
fraction of in-distribution images correctly classiﬁed as in-distribution images. In Figure 13, we
show how the thresholds affect FPR and TPR when the temperature and perturbation magnitude are
chosen optimally (i.e., T = 1, 000, ε = 0.0014). From the ﬁgure, we can observe that the threshold
corresponding to 95% TPR can produce small FPRs on all out-of-distribution datasets.
Performance across datasets. To investigate how the parameters generalize across datasets, we
tune the parameters using one out-of-distribution dataset and then evaluate on a different one. Given
an out-of-distribution dataset, we ﬁrst split the dataset into two disjoint subsets: tuning set and
test set. The tuning set contains 1,000 images and the test set contains 9,000 images. We tune the
parameters on the tuning set and evaluate the detection performance on the test set. We ﬁrst choose
the temperature T and the perturbation magnitude ε such that the FPR at TPR 95% is minimized on
the tuning set of one out-of-distribution dataset. Next, we set δ to the threshold corresponding to 95%
TPR and calculate the false positive rates on the test sets of other out-of-distribution datasets.

In Figure 14, we show the detection performance on three out-of-distribution datasets when the
parameters are tuned on six different datasets. From Figure 14, we can observe that the parameters
tuned on different tuning sets can have quite similar detection FPRs on all of three out-of-distribution
image sets. This may be due to the fact, shown in Figure 13, that the threshold corresponding to 95%
TPR can produce small FPRs on all datasets.

23

Published as a conference paper at ICLR 2018

Figure 15: FPR at TPR 95% under different tuning set sizes. The DenseNet is trained on CIFAR-10 and each
test set contains 8,000 out-of-distribution images.

Performance vs.
tuning set size. To show the effects of the tuning set size on the detection
performance, we devise the following experiment. For each out-of-distribution dataset, we choose the
tuning set size from 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000. For each set size, we
tune the temperature T and perturbation magnitude ε to minimize the FPR at TPR 95% and calculate
the FPR. In Figure 15, we show the detection performance of ODIN under different tuning set size.
From Figure 15, we can observe that the FPR at TPR 95% tends to stabilize when the set size grows
above 1,000.

24

Published as a conference paper at ICLR 2018

DenseNet-BC-100

Test set

ImgNet (c)

ImgNet (r) LSUN (c) LSUN (c)

iSUN

Gaussian Uniform

Baseline (Hendrycks & Gimpel, 2017) / Ours

ImgNet (c)
ImgNet (r)
LSUN (c)
LSUN (r)
iSUN
Gaussian
Uniform

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

34.7/6.6
40.7/14.9
39.3/8.1
33.6/10.4
37.2/12.6
23.5/0.4
12.3/4.5

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

Table 10: Detection performance across different datasets. Each row corresponds to the FPR at
TPR 95% on the same test set where parameters are tuned under different tuning sets. Each column
corresponds to the FPR at TPR 95% on different test sets where parameters are tuned under the same
tuning set. The DenseNet is trained on CIFAR-10.

Wide-ResNet- 28-10

Test set

ImgNet (c)

ImgNet (r) LSUN (c)

LSUN (c)

iSUN

Gaussian Uniform

Baseline (Hendrycks & Gimpel, 2017) / Ours

ImgNet (c)
ImgNet (r)
LSUN (c)
LSUN (r)
iSUN
Gaussian
Uniform

38.9/23.4
45.6/25.5
35.0/28.1
35.0/18.9
40.6/21.8
1.6/0.0
0.3/0.0

38.9/24.5
45.6/25.5
35.0/31.27
35.0/18.0
40.6/21.7
1.6/0.0
0.3/0.0

38.9/27.1
45.6/32.9
35.0/21.8
35.0/25.6
40.6/28.9
1.6/0.4
0.3/0.0

38.9/23.4
45.6/25.8
35.0/28.2
35.0/17.6
40.6/ 21.9
1.6/0.0
0.3/0.0

38.9/24.1
45.6/25.8
35.0/28.9
35.0/18.0
40.6/21.3
1.6/0.0
0.3/0.0

38.9/26.5
45.6/27.9
35.0/29.7
35.0/19.1
40.6/22.8
1.6/0.0
0.3/0.0

38.9/26.5
45.6/27.9
35.0/29.7
35.0/19.1
40.6/22.8
1.6/0.0
0.3/0.0

Table 11: Detection performance across different datasets. Each row corresponds to the FPR at
TPR 95% on the same test set where parameters are tuned under different tuning sets. Each column
corresponds to the FPR at TPR 95% on different test sets where parameters are tuned under the same
tuning set. The Wide-ResNet is trained on CIFAR-10.

25

Published as a conference paper at ICLR 2018

Figure 16: (a) The test accuracy on the images having softmax scores above the threshold corresponding to
a certain true positive rate. (b) The test accuracy on the images having softmax scores below the threshold
corresponding to a certain true positive rate. All networks are trained on CIFAR-10.

Figure 17: Outputs of DenseNet on thirty classes for an image of apple from CIFAR-80 and an image of red
pepper from CIFAR-20. The label “0” denotes the class “apple” and the label “49" denotes the class “orange".

I ADDITIONAL ANALYSIS

Difﬁcult-to-classify images and difﬁcult-to-detect images. We analyze the correlation between
the images that tend to be out-of-distribution and images on which the neural network tend to make
incorrect predictions. To understand the correlation, we devise the following experiment. For the
ﬁxed temperature T and perturbation magnitude ε, we ﬁrst set δ to the softmax score threshold
corresponding to a certain true positive rate. Next, we calculate the test accuracy on the images with
softmax scores above δ and the test accuracy on the images with softmax score below δ, respectively.
We report the results in Figure 16(a) and (b). From these two ﬁgures, we can observe that the images
that are difﬁcult to detect are more likely to be the images that are difﬁcult to classify. For example,
the DenseNet can achieve up to 98.5% test accuracy on the images having softmax scores above the
threshold corresponding to 80% TPR, but can only achieve around 82% test accuracy on the images
having softmax scores below the threshold corresponding to 80% TPR.
Same manifold datasets. We provide additional empirical results showing how the term E[U2|
U1]
affects the detection performance when in- and out-of-distribution datasets locate on the same
manifold. In Figure 17, we show the outputs of DenseNet on thirty classes for an image of apple
from CIFAR-80 (in-distribution) and an image of red pepper of CIFAR-20 (out-distribution). We
can observe that the outputs of DenseNet for both images are quite similar to each other. In addition,
we can observe that for both images, the second and third largest output are quite close to the
largest output. This may be due the fact the image of red pepper shares some common features
with the images in CIFAR-80. Furthermore, the similarity between the outputs for the images from

26

Published as a conference paper at ICLR 2018

Out-of-distribution
dataset

FPR
(95% TPR)
↓

Detection AUROC

Error
↓

↑

AUPR
In
↑

AUPR
Out
↑

Baseline (Hendrycks & Gimpel, 2017) / Ours

CIFAR-10
CIFAR-100

CIFAR-100
CIFAR-10

57.1/47.2
81.8/81.4

31.1/26.1
43.4/43.2

89.0/89.8
76.1/76.7

91.2/91.4
79.9/80.4

86.8/88.7
71.3/72.6

Table 12: Distinguishing in- and out-of-distribution test set data for image classiﬁcation. All values are
percentages. ↑ indicates larger value is better, and ↓ indicates lower value is better. The architecture is DenseNet.

CIFAR-80 and CIFAR-20 can help explain that the detection task becomes harder when in- and
out-of-distribution datasets locate on the same manifold.

Reciprocal results between datasets. In Table 12, we show the reciprocal results between datasets.
First, we train the DenseNet on the CIFAR-10 dataset (in-distribution) and evaluate the detection
performance on the CIFAR-100 dataset (out-distribution). Next, we train the DenseNet on the
CIFAR-100 dataset (in-distribution) and evaluate the detection performance on the CIFAR-10 dataset
(out-distribution). From Table 12, we can observe that the performance of the DenseNet trained on
CIFAR-10 is better than the performance of the DenseNet trained on CIFAR-100. This may be due to
the fact that the DenseNet has a higher test accuracy on CIFAR-10 (around 95%) compared to the
test accuracy on CIFAR-100 (around 77%).

27

8
1
0
2
 
b
e
F
 
5
2
 
 
]

G
L
.
s
c
[
 
 
4
v
0
9
6
2
0
.
6
0
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

ENHANCING THE RELIABILITY OF
OUT-OF-DISTRIBUTION IMAGE DETECTION IN
NEURAL NETWORKS

Shiyu Liang
Coordinated Science Lab, Department of ECE
University of Illinois at Urbana-Champaign
sliang26@illinois.edu

R. Srikant
Coordinated Science Lab, Department of ECE
University of Illinois at Urbana-Champaign
rsrikant@illinois.edu

Yixuan Li
Facebook Research
yixuanl@fb.com

ABSTRACT

We consider the problem of detecting out-of-distribution images in neural networks.
We propose ODIN, a simple and effective method that does not require any change
to a pre-trained neural network. Our method is based on the observation that using
temperature scaling and adding small perturbations to the input can separate the
softmax score distributions between in- and out-of-distribution images, allowing
for more effective detection. We show in a series of experiments that ODIN
is compatible with diverse network architectures and datasets. It consistently
outperforms the baseline approach (Hendrycks & Gimpel, 2017) by a large margin,
establishing a new state-of-the-art performance on this task. For example, ODIN
reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet
(applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%.

1

INTRODUCTION

Modern neural networks are known to generalize well when the training and testing data are sampled
from the same distribution (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016;
Cho et al., 2014; Zhang et al., 2017). However, when deploying neural networks in real-world
applications, there is often very little control over the testing data distribution. Recent works
have shown that neural networks tend to make high conﬁdence predictions even for completely
unrecognizable (Nguyen et al., 2015) or irrelevant inputs (Hendrycks & Gimpel, 2017; Szegedy et al.,
2014; Moosavi-Dezfooli et al., 2017). It has been well documented (Amodei et al., 2016) that it is
important for classiﬁers to be aware of uncertainty when shown new kinds of inputs, i.e., out-of-
distribution examples. Therefore, being able to accurately detect out-of-distribution examples can
be practically important for visual recognition tasks (Krizhevsky et al., 2012; Farabet et al., 2013; Ji
et al., 2013).

A seemingly straightforward approach of detecting out-of-distribution images is to enlarge the training
set of both in- and out-of-distribution examples. However, the number of out-of-distribution examples
can be inﬁnitely many, making the re-training approach computationally expensive and intractable.
Moreover, to ensure that a neural network accurately classiﬁes in-distribution samples into correct
classes while correctly detecting out-of-distribution samples, one might need to employ exceedingly
large neural network architectures, which further complicates the training process.

Hendrycks & Gimpel (2017) proposed a baseline method to detect out-of-distribution examples
without further re-training networks. The method is based on an observation that a well-trained neural
network tends to assign higher softmax scores to in-distribution examples than out-of-distribution
examples. In this paper, we go further. We observe that after using temperature scaling in the softmax
function (Hinton et al., 2015; Pereyra et al., 2017) and adding small controlled perturbations to inputs,

1

Published as a conference paper at ICLR 2018

the softmax score gap between in - and out-of-distribution examples is further enlarged. We will
show that the combination of these two techniques (temperature scaling and input perturbation) can
lead to better detection performance. For example, provided with a pre-trained DenseNet (Huang
et al., 2016) on CIFAR-10 dataset (positive samples), we test against images from TinyImageNet
dataset (negative samples). Our method reduces the False Positive Rate (FPR), i.e., the fraction of
misclassiﬁed out-of-distribution samples, from 34.7% to 4.3%, when 95% of in-distribution images
are correctly classiﬁed. We summarize the main contributions of this paper as the following:

•

•

We propose a simple and effective method, ODIN (Out-of-DIstribution detector for Neural net-
works), for detecting out-of-distribution examples in neural networks. Our method does not require
re-training the neural network and is easily implementable on any modern neural architecture.

We test ODIN on state-of-the-art network architectures (e.g., DenseNet (Huang et al., 2016) and
Wide ResNet (Zagoruyko & Komodakis, 2016)) under a diverse set of in- and out-distribution
dataset pairs. We show ODIN can signiﬁcantly improve the detection performance, and consistently
outperforms the state-of-the-art method (Hendrycks & Gimpel, 2017) by a large margin.

We empirically analyze how parameter settings affect the performance, and further provide simple
analysis that provides some intuition behind our method.

•

The outline of this paper is as follows. In Section 2, we present the necessary deﬁnitions and the
problem statement. In Section 3, we introduce ODIN and present performance results in Section 4.
We experimentally analyze the proposed method and provide some justiﬁcation for our method in
Section 5. We summarize the related works and future directions in Section 6 and conclude the paper
in Section 7.

2 PROBLEM STATEMENT

X

In this paper, we consider the problem of distinguishing in- and out-of-distribution images on a pre-
trained neural network. Let PX and QX denote two distinct data distributions deﬁned on the image
. Assume that a neural network f is trained on a dataset drawn from the distribution PX . Thus,
space
we call PX the in-distribution and QX the out-distribution, respectively. In testing, we draw new
images from a mixture distribution PX×Z deﬁned on
, where the conditional probability
distributions PX|Z=0 = PX and PX|Z=1 = QX denote in- and out-distribution respectively. Now
we focus on the following problem: Given an image X drawn from the mixture distribution PX×Z,
can we distinguish whether the image is from in-distribution PX or not?

0, 1
}

X × {

In this paper, we focus on detecting out-of-distribution images. However, it is equally important to
correctly classify an image into the right class if it is an in-distribution image. But this can be easily
done: once it has been detected that an image is in-distribution, we can simply use the original image
and run it through the neural network to classify it. Thus, we do not change the predictions of the
neural network for in-distribution images and only focus on improving the detection performance for
out-of-distribution images.

3 ODIN: OUT-OF-DISTRIBUTION DETECTOR

In this section, we present our method, ODIN, for detecting out-of-distribution samples. The detector
is built on two components: temperature scaling and input preprocessing. We describe the details of
both components below.
Temperature Scaling. Assume that the neural network f = (f1, ..., fN ) is trained to classify N
classes. For each input x, the neural network assigns a label ˆy(x) = arg maxi Si(x; T ) by computing
the softmax output for each class. Speciﬁcally,

Si(x; T ) =

exp (fi(x)/T )
j=1 exp (fj(x)/T )
R+ is the temperature scaling parameter and set to 1 during the training. For a given input
where T
x, we call the maximum softmax probability, i.e., Sˆy(x; T ) = maxi Si(x; T ) the softmax score. In
this paper, we use notations Sˆy(x; T ) and S(x; T ) interchangeably. Prior works have established
the use of temperature scaling to distill the knowledge in neural networks (Hinton et al., 2015) and

(cid:80)N

(1)

∈

,

2

Published as a conference paper at ICLR 2018

−

εsign(

x log Sˆy(x; T )),

calibrate the prediction conﬁdence in classiﬁcation tasks (Guo et al., 2017). As we shall see later,
a good manipulation of temperature T can push the softmax scores of in- and out-of-distribution
images further apart from each other, making the out-of-distribution images distinguishable.
Input Preprocessing. Before feeding the image x into the neural network, we preprocess the input
by adding small perturbations to it. The preprocessed image is given by
˜x = x

−∇
where the parameter ε can be interpreted as the perturbation magnitude. The method is inspired by the
idea in the reference (Goodfellow et al., 2015), where small perturbations are added to decrease the
softmax score for the true label and force the neural network to make a wrong prediction. Here, our
goal and setting are rather different: we aim to increase the softmax score of any given input, without
the need for a class label at all. As we shall see later, the perturbation can have stronger effect on the
in- distribution images than that on out-of-distribution images, making them more separable. Note
that the perturbations can be easily computed by back-propagating the gradient of the cross-entropy
loss w.r.t the input.
Out-of-distribution Detector. The proposed approach works as follows. For each image x, we ﬁrst
calculate the preprocessed image ˜x according to the equation (2). Next, we feed the preprocessed
image ˜x into the neural network, calculate its softmax score S( ˜x; T ) and compare the score to the
threshold δ. We say that the image x is an in-distribution example if the softmax score is above
the threshold and that the image x is an out-of-distribution example, otherwise. Therefore, the
out-of-distribution detector is given by

(2)

g(x; δ, T, ε) =

(cid:26)1
0

if maxi p( ˜x; T )
δ,
if maxi p( ˜x; T ) > δ.

≤

The parameters T, ε and δ are chosen so that the true positive rate (i.e., the fraction of in-distribution
images correctly classiﬁed as in-distribution images) under some out-of-distribution image data set
is 95%. (The choice of the out-of-distribution images to tune the parameters T, ε and δ appears to
be unimportant, as demonstrated in the appendix H.) Having chosen the parameters as above, we
evaluate the performance of our algorithm using various metrics in the next section.

4 EXPERIMENTS

4.1 TRAINING SETUP

In this section, we demonstrate the effectiveness of ODIN on several computer vision benchmark
datasets. We run all experiments with PyTorch1 and we will release the code to reproduce all
experimental results2.

Architectures and training conﬁgurations. We adopt two state-of-the-art neural network architec-
tures, including DenseNet (Huang et al., 2016) and Wide ResNet (Zagoruyko & Komodakis, 2016).
For DenseNet, our model follows the same setup as in (Huang et al., 2016), with depth L = 100,
growth rate k = 12 (Dense-BC) and dropout rate 0. In addition, we evaluate the method on a Wide
ResNet, with depth 28, width 10 (WRN-28-10) and dropout rate 0. Furthermore, in Appendix A.1, we
provide additional experimental results on another Wide ResNet with depth 40, width 4 (WRN-40-4).
The hyper-parameters of neural networks are set identical to the original Wide ResNet (Zagoruyko
& Komodakis, 2016) and DenseNet (Huang et al., 2016) implementations. All neural networks are
trained with stochastic gradient descent with Nesterov momentum (Duchi et al., 2011; Kingma & Ba,
2014). Speciﬁcally, we train Dense-BC for 300 epochs with batch size 64 and momentum 0.9; and
Wide ResNet for 200 epochs with batch size 128 and momentum 0.9. The learning rate starts at 0.1,
and is dropped by a factor of 10 at 50% and 75% of the training progress, respectively.

Accuracy.
Each neural network architecture is
trained on CIFAR-10 (C-10) and CIFAR-100 (C-100)
datasets (Krizhevsky & Hinton, 2009),
respectively.
CIFAR-10 and CIFAR-100 images are drawn from 10
and 100 classes, respectively. Both datasets consist of

1http://pytorch.org
2https://github.com/facebookresearch/odin

Architecture C-10 C-100

Dense-BC
WRN-28-10

4.81
3.71

22.37
19.86

Table 1: Test error rates on CIFAR-10
and CIFAR-100 datasets.

3

Published as a conference paper at ICLR 2018

50,000 training images and 10,000 test images. The test
error on CIFAR datasets are summarized in Table 1.

4.2 OUT-OF-DISTRIBUTION DATASETS

At test time, the test images from CIFAR-10 (CIFAR-100) datasets can be viewed as the in-distribution
(positive) examples. For out-of-distribution (negative) examples, we follow the setting in (Hendrycks
& Gimpel, 2017) and test on several different natural image datasets and synthetic noise datasets. All
the datasets considered are listed below.

(1) TinyImageNet. The Tiny ImageNet dataset3 consists of a subset of ImageNet images (Deng
et al., 2009). It contains 10,000 test images from 200 different classes. We construct two datasets,
TinyImageNet (crop) and TinyImageNet (resize), by either randomly cropping image patches of
size 32

32 or downsampling each image to size 32

32.

(2) LSUN. The Large-scale Scene UNderstanding dataset (LSUN) has a testing set of 10,000 images
of 10 different scenes (Yu et al., 2015). Similar to TinyImageNet, we construct two datasets,
LSUN (crop) and LSUN (resize), by randomly cropping and downsampling the LSUN testing set,
respectively.

×

×

(3) iSUN. The iSUN (Xu et al., 2015) consists of a subset of SUN images. We include the entire

collection of 8925 images in iSUN and downsample each image to size 32 by 32.

(4) Gaussian Noise. The synthetic Gaussian noise dataset consists of 10,000 random 2D Gaussian
noise images, where each RGB value of every pixel is sampled from an i.i.d Gaussian distribution
with mean 0.5 and unit variance. We further clip each pixel value into the range [0, 1].

(5) Uniform Noise. The synthetic uniform noise dataset consists of 10,000 images where each RGB
value of every pixel is independently and identically sampled from a uniform distribution on [0, 1].

4.3 EVALUATION METRICS

We adopt the following four different metrics to measure the effectiveness of a neural network in
distinguishing in- and out-of-distribution images.

(1) FPR at 95% TPR can be interpreted as the probability that a negative (out-of-distribution)
example is misclassiﬁed as positive (in-distribution) when the true positive rate (TPR) is as high as
95%. True positive rate can be computed by TPR = TP / (TP+FN), where TP and FN denote true
positives and false negatives respectively. The false positive rate (FPR) can be computed by FPR =
FP / (FP+TN), where FP and TN denote false positives and true negatives respectively.

(2) Detection Error, i.e., Pe measures the misclassiﬁcation probability when TPR is 95%. The
TPR) + 0.5FPR, where we assume that both positive

deﬁnition of Pe is given by Pe = 0.5(1
and negative examples have the equal probability of appearing in the test set.

−

(3) AUROC is the Area Under the Receiver Operating Characteristic curve, which is also a threshold-
independent metric (Davis & Goadrich, 2006). The ROC curve depicts the relationship between
TPR and FPR. The AUROC can be interpreted as the probability that a positive example is assigned
a higher detection score than a negative example (Fawcett, 2006). A perfect detector corresponds
to an AUROC score of 100%.

(4) AUPR is the Area under the Precision-Recall curve, which is another threshold independent
metric (Manning et al., 1999; Saito & Rehmsmeier, 2015). The PR curve is a graph showing
the precision=TP/(TP+FP) and recall=TP/(TP+FN) against each other. The metric AUPR-In and
AUPR-Out in Table 2 denote the area under the precision-recall curve where in-distribution and
out-of-distribution images are speciﬁed as positives, respectively.

4.4 EXPERIMENTAL RESULTS

Comparison with baseline. In Figure 1, we show the ROC curves when DenseNet-BC-100 is
evaluated on CIFAR-10 (positive) images against TinyImageNet (negative) test examples. The red
curve corresponds to the ROC curve when using baseline method (Hendrycks & Gimpel, 2017),

3https://tiny-imagenet.herokuapp.com

4

Published as a conference paper at ICLR 2018

Out-of-distribution
dataset

FPR
(95% TPR)
↓

Detection
Error
↓

AUROC

↑

AUPR
In
↑

AUPR
Out
↑

Baseline (Hendrycks & Gimpel, 2017) / ODIN

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

34.7/4.3
40.8/7.5
39.3/8.7
33.6/3.8
37.2/6.3
23.5/0.0
12.3/0.0

67.8/17.3
82.2/44.3
69.4/17.6
83.3/44.0
84.8/49.5
88.3/0.5
95.4/0.2

38.9/23.4
45.6/25.5
35.0/21.8
35.0/17.6
40.6/21.3
1.6/0.0
0.3/0.0

66.6/43.9
79.2/55.9
74.0/39.6
82.2/56.5
82.7/57.3
98.2/0.1
99.2/1.0

19.9/4.7
22.9/6.3
22.2/6.9
19.3/4.4
21.1/5.7
14.3/2.5
8.7/2.5

36.4/11.2
43.6/24.6
37.2/11.3
44.1/24.5
44.7/27.2
46.6/2.8
50.2/2.6

21.9/14.2
25.3/15.2
20.0/13.4
20.0/11.3
22.8/13.2
3.3/2.5
2.6/2.5

35.8/24.4
42.1/30.4
39.5/22.3
43.6/30.8
43.9/31.1
51.6/2.5
52.1/3.0

95.3/99.1
94.1/98.5
94.8/98.2
95.4/99.2
94.8/98.8
96.5/99.9
97.5/100.0

83.0/97.1
70.4/90.7
83.7/96.8
70.6/91.5
69.9/90.1
83.2/99.5
81.8/99.6

92.9/94.2
91.0/92.1
94.5/95.9
93.9/95.4
92.5/93.7
99.2/100.0
99.5/100.0

82.0/90.8
72.2/84.0
80.3/92.0
73.9/86.0
72.8/85.6
84.1/99.1
84.3/98.5

96.4/99.1
95.1/98.6
96.0/98.5
96.4/99.3
95.9/98.9
97.8/100.0
98.3/100.0

85.3/97.4
71.4/91.4
86.2/97.1
72.5/92.4
71.9/91.1
88.1/99.6
87.6/99.7

92.5/92.8
89.7/89.0
95.1/95.8
93.8/93.8
91.7/91.2
99.3/100.0
99.6/100.0

83.3/91.4
70.4/82.8
83.4/92.4
75.7/86.2
74.2/85.9
89.9/99.4
90.2/99.1

93.8/99.1
92.4/98.5
93.1/97.8
94.0/99.2
93.1/98.8
93.0/99.9
95.9/100.0

80.8/96.8
68.6/90.1
80.9/96.5
68.0/90.6
67.0/88.9
73.1/99.0
70.1/99.1

91.9/94.7
89.9/93.6
93.1/95.5
92.8/96.1
91.5/94.9
98.9/100.0
99.3/100.0

80.2/90.0
70.8/84.4
77.0/91.6
70.1/84.9
69.2/84.8
71.0/97.5
70.9/95.9

Dense-BC
CIFAR-10

Dense-BC
CIFAR-100

WRN-28-10
CIFAR-10

WRN-28-10
CIFAR-100

Table 2: Distinguishing in- and out-of-distribution test set data for image classiﬁcation. All values are
percentages. ↑ indicates larger value is better, and ↓ indicates lower value is better. All parameter settings are
shown in Appendix A.2. Additional results on WRN-40-4 and MNIST dataset are reported in Appendix A.1.

whereas the blue curve corresponds to our method with temperature T = 1000 and perturbation
magnitude ε = 0.0012. We observe a strikingly large gap between the blue and red ROC curves. For
example, when TPR= 95%, the FPR can be reduced from 34% to 4.2% by using our approach.
Choosing parameters. For each out-of-distribution dataset,
we randomly hold out 1,000 images for tuning the parameters
T and ε. For temperature T , we select among 1, 2, 5, 10,
20, 50, 100, 200, 500, 1000; and for perturbation magnitude
ε we choose from 21 evenly spaced numbers starting from 0
and ending at 0.004. The optimal parameters are chosen to
minimize the FPR at TPR 95% on the holdout set. We evaluate
the our approach on the remaining test images. All parameter
settings are reported in the Appendix A. We provide additional
details on the effect of parameters in Section 5.

Main results. The main results are summarized in Table 2. For
each in- and out-of-distribution dataset pair, we report both the
performance of the baseline (Hendrycks & Gimpel, 2017) and
our approach using temperature scaling and input preprocessing.
In Table 2, we observe improved performance across all neural
architectures and all dataset pairs. Noticeably, our method
consistently outperforms the baseline by a large margin when
measured by FPR at 95% TPR and detection error.

5

Figure 1:
(a) ROC curves of base-
line (red) and our method (blue)
on DenseNet-BC-100 network, where
CIFAR-10 and TinyImageNet (crop)
are in- and out-of-distribution dataset,
respectively.

Published as a conference paper at ICLR 2018

Figure 2: (a)-(d) Performance of our method vs. MMD between in- and out-of-distribution datasets. Neural
networks are trained on CIFAR-100 and CIFAR-80, respectively. The out-of-distribution datasets are 1: LSUN
(cop), 2: TinyImageNet (crop), 3: LSUN (resize), 4:
is iSUN (resize), 5: TinyImageNet (resize) and 6:
CIFAR-20.

4.5 EXTENSIONS

In this subsection, we analyze how the statistical distance be-
tween in- and out-of-distribution natural image dataset affects the detection performance of the
proposed method.
Data distribution distance vs. Detection performance. To measure the statistical distance between
in- and out-of-distribution datasets, we adopt a commonly used metric, maximum mean discrepancy
(MMD) with Gaussian RBF kernel (Sriperumbudur et al., 2010; Gretton et al., 2012; Sutherland et al.,
2016). Speciﬁcally, given two image sets, V =
, the maximum
v1, ..., vm
}
{
mean discrepancy between V and Q is deﬁned as

w1, ..., wm
{

and W =

}

2

(cid:92)MMD

(V, W ) =

1
(cid:0)m
2

(cid:1)

(cid:88)

i(cid:54)=j

k(vi, vj) +

1
(cid:1)
(cid:0)m
2

(cid:88)

i(cid:54)=j

k(wi, wj)

−

k(vi, wj),

(cid:88)

i(cid:54)=j

(cid:1)

2
(cid:0)m
2
(cid:17)

∪

,
·

) is the Gaussian RBF kernel, i.e., k(x, x(cid:48)) = exp
·

. We use the same method
where k(
used by Sutherland et al. (2016) to choose σ, where 2σ2 is set to the median of all Euclidean distances
between all images in the aggregate set V

W .

−

(cid:16)

(cid:107)x−x(cid:48)(cid:107)2
2
2σ2

In Figure 2 (a)(b), we show how the performance of ODIN varies against the MMD distances between
in- and out-of-distribution datasets4. The datasets (on x-axis) are ranked in the descending order of
MMD distances with CIFAR-100. There are two interesting observations can be drawn from these
ﬁgures. First, we ﬁnd that the MMD distances between the cropped datasets and CIFAR-100 tend
to be larger. This is likely due to the fact that cropped images only contain local image context and
are therefore more distinct from CIFAR-100 images, while resized images contain global patterns
and are thus similar to images in CIFAR-100. Second, we observe that the MMD distance tends to
be negatively correlated with the detection performance. This suggests, not surprisingly, that the
detection task becomes harder as in and out-of-distribution images are more similar to each other.
Same-manifold datasets. Furthermore, we investigate the extreme scenario when in- and out-of-
distribution datasets are on the same manifold. In experiment, we randomly split CIFAR-100 into
two disjoint datasets containing 80 and 20 classes each. We name them CIFAR-80 and CIFAR-
20, respectively. We train both DenseNet and Wide ResNet-28-10 on the CIFAR-80 dataset (in-
distribution) and evaluate the detection performance on the CIFAR-20 dataset (out-distribution). All
hyperparameters used here are exactly the same as in Section 4.1. The MMD distance between
CIFAR-20 and CIFAR-80 is much smaller than other dataset pairs. In Figure 2 (c)(d), we observe that
both FPR at TPR 95% and detection error become larger on the CIFAR-20 dataset. This coincides
with our expectation that the detection task becomes extremely hard when in- and out-of-distribution
dataset locate on the same manifold. We provide additional experimental results in Appendix A.1
and Appendix G.

5 DISCUSSIONS

5.1 EFFECTS OF PARAMETERS

In this subsection, we empirically show how temperature T and perturbation magnitude ε affect
FPR at TPR 95% and AUROC on DenseNet and Wide ResNet-28-10. Additional results on other

4All distances are provided in Appendix G.

6

Published as a conference paper at ICLR 2018

Figure 3: (a)(b) Effects of temperature T when ε = 0. (c)(d) Effects of perturbation magnitude ε when T = 1.
All networks are trained on CIFAR-10 (in-distribution). Additional results on other metrics and Wide ResNet-40
are provided in Appendix B.

Figure 4: (a)(b) Effects of perturbation magnitude ε on DenseNet when T is large (e.g., T = 1000). (c)(d)
Effects of perturbation magnitude of ε on Wide-ResNet-28-10 when T is large (e.g., T = 1000). All networks
are trained on CIFAR-10. Additional results on other metrics and Wide ResNet-40 are provided in Appendix B.

metrics and architectures are provided in Appendix B. We show the detection performance when
using only the temperature scaling method (see Figure 3(a)(b), ε = 0), or the input preprocessing
method (see Figure 3(c)(d), T = 1). In Figure 4, we show the detection performance w.r.t ε when T
is optimal (e.g., T =1000). First, from Figure 3 (a)(b), we observe that increasing the temperature
can improve the detection performance, although the effects diminish when T is sufﬁciently large
(e.g., T > 100). Next, from Figure 3(c)(d) and Figure 4, we observe that we can further improve
the detection performance by appropriately choosing the perturbation magnitudes. We can achieve
overall better performance by combining both (1) temperature scaling and (2) input preprocessing.

5.2 ANALYSIS ON TEMPERATURE SCALING

In this subsection, we analyze the effectiveness of the temperature scaling method. As shown
in Figure 3 (a) and (b), we observe that a sufﬁciently large temperature yields better detection
performance although the effects diminish when T is too large. To gain insight, we can use the Taylor
expansion of the softmax score (details provided in Appendix D). When T is sufﬁciently large, we
have

Sˆy(x; T )

≈

N

(cid:80)

1
T

i[fˆy(x)

1
fi(x)] + 1
2T 2

(cid:80)

i[fˆy(x)

,

fi(x)]2

−
by omitting the third and higher orders. For simplicity of notation, we deﬁne

−

−

(3)

U1(x) =

[fˆy(x)

fi(x)]

and U2(x) =

1

N

1

−

(cid:88)

i(cid:54)=ˆy

−

1

N

1

−

(cid:88)

i(cid:54)=ˆy

[fˆy(x)

fi(x)]2.

(4)

−

Interpretations of U1 and U2. By deﬁnition, U1 measures the extent to which the largest unnormal-
ized output of the neural network deviates from the remaining outputs; while U2 measures the extent
to which the remaining smaller outputs deviate from each other. We provide formal mathematical
derivations in Appendix F. In Figure 5(a), we show the distribution of U1 for each out-of-distribution
dataset vs. the in-distribution dataset (in red). We observe that the largest outputs of the neural
network on in-distribution images deviate more from the remaining outputs. This is likely due to the
fact that neural networks tend to make more conﬁdent predictions on in-distribution images.
U1], for each
Further, we show in Figure 5(b) the expectation of U2 conditioned on U1, i.e., E[U2|
dataset. The red curve (in-distribution images) has overall higher expectation. This indicates that,

7

Published as a conference paper at ICLR 2018

Figure 5: (a) Probability density of U1 under different datasets on DenseNet. (b) Expectations of U2 conditioned
on U1 on DenseNet. (c) Probability density of the norm of gradient on DenseNet under temperature 1, 000.
(c)(d) Expectation of the norm of gradient conditioned on the softmax scores on DenseNet under temperature
T = 1000 and T = 1, respectively. (f)(g) Outputs of DenseNet on each class for an image of dog from
CIFAR-10 and an image from TinyImageNet (crop). The DenseNet is trained on CIFAR-10. Additional results
on other architectures are provided in Appendix C.

when two images have similar values on U1, the in-distribution image tends to have a much higher
value of U2 than the out-of-distribution image.
In other words, for in-distribution images, the
remaining outputs (excluding the largest output) tend to be more separated from each other compared
to out-of-distribution datasets. This may happen when some classes in the in-distribution dataset
share common features while others differ signiﬁcantly. To illustrate this, in Figure 5 (f)(g), we show
the outputs of each class using a DenseNet (trained on CIFAR-10) on a dog image from CIFAR-10,
and another image from TinyImageNet (crop). For the image of dog, we can observe that the largest
output for the label dog is close to the output for the label cat but is quite separated from the outputs
for the label car and truck. This is likely due to the fact that, in CIFAR-10, images of dogs are very
similar to the images of cats but are quite distinct from images of car and truck. For the image from
TinyImageNet (crop), despite having one large output, the remaining outputs are close to each other
and thus have a smaller deviation.
The effects of T . To see the usefulness of adopting a large T , we can ﬁrst rewrite the softmax score
function in Equation (3) as S
U2/2T )/T . Hence the softmax score is largely determined
(U1 −
by U1 and U2/2T . As noted earlier, U1 makes in-distribution images produce larger softmax scores
than out-of-distribution images since S
U2.
Therefore, by choosing a sufﬁciently large temperature, we can compensate the negative impacts of
U2/2T on the detection performance, making the softmax scores between in- and out-of-distribution
images more separable. Eventually, when T is sufﬁciently large, the distribution of softmax score is
almost dominated by the distribution of U1 and thus increasing the temperature further is no longer
effective. This explains why we see in Figure 3 (a)(b) that the performance does not change when T
is too large (e.g., T > 100). In Appendix E, we provide a formal proof showing that the detection
error eventually converges to a constant number when T goes to inﬁnity.

U1, while U2 has the exact opposite effect since S

∝ −

∝

∝

5.3 ANALYSIS ON INPUT PREPROCESSING
As noted previously, using the temperature scaling method by itself can be effective in improving the
detection performance. However, the effectiveness quickly diminishes as T becomes very large. In
order to make further improvement, we complement temperature scaling with input preprocessing.
This has already been seen in Figure 4, where the detection performance is improved by a large
margin on most datasets when T = 1000, provided with an appropriate perturbation magnitude ε is
chosen. In this subsection, we provide some intuition behind this.

To explain, we can look into the ﬁrst order Taylor expansion of the log-softmax function for the
perturbed image ˜x, which is given by

log Sˆy( ˜x; T ) = log Sˆy(x; T ) + ε

x log Sˆy(x; T )

(cid:107)∇

(cid:107)1 + o(ε),

where x is the original input.
The effects of gradient.
(cid:107)1 — the
In Figure 5 (c), we present the distribution of
1-norm of gradient of log-softmax with respect to the input x — for all datasets. A salient observation

x log S(x; T )

(cid:107)∇

8

Published as a conference paper at ICLR 2018

≈

is that CIFAR-10 images (in-distribution) tend to have larger values on the norm of gradient than
most out-of-distribution images. To further see the effects of the norm of gradient on the softmax
score, we provide in Figures 5 (d) the conditional expectation E[
S]. We can
observe that, when an in-distribution image and an out-of-distribution image have the same softmax
score, the value of

x log S(x; T )

x log S(x; T )

(cid:107)1 for in-distribution image tends to be larger.

(cid:107)1|

(cid:107)∇

(cid:107)∇

We illustrate the effects of the norm of gradient in Figure 6. Suppose
that an in-distribution image x1 (blue) and an out-of-distribution
image x2 (red) have similar softmax scores, i.e., S(x1)
S(x2).
After input processing, the in-distribution image can have a much
larger softmax score than the out-of-distribution image x2 since x1
results in a much larger value on the norm of softmax gradient than
that of x2. Therefore, in- and out-of-distribution images are more
separable from each other after input preprocessing5.
The effect of ε. When the magnitude ε is sufﬁciently small, adding
perturbations does not change the predictions of the neural network,
i.e., ˆy( ˜x) = ˆy(x). However, when ε is not negligible, the gap of
softmax scores between in- and out-of-distribution images can be
(cid:107)1. Our observation is consistent with
affected by
that in (Szegedy et al., 2014; Goodfellow et al., 2015; Moosavi-
Dezfooli et al., 2017), which show that the softmax scores tend to change signiﬁcantly if small
perturbations are added to the in-distribution images. It is also worth noting that using a very large ε
can lead to performance degradation, as seen in Figure 4. This is likely due to the fact that the second
and higher order terms in the Taylor expansion are no longer insigniﬁcant when the perturbation
magnitude is too large.

Figure 6: Illustration of effects
of the input preprocessing.

x log S(x; T )

(cid:107)∇

6 RELATED WORKS AND FUTURE DIRECTIONS

The problem of detecting out-of-distribution examples in low-dimensional space has been well-studied
in various contexts (see the survey by Pimentel et al. (2014)). Conventional methods such as density
estimation, nearest neighbor and clustering analysis are widely used in detecting low-dimensional out-
of-distribution examples (Chow, 1970; Vincent & Bengio, 2003; Ghoting et al., 2008; Devroye et al.,
2013), . The density estimation approach uses probabilistic models to estimate the in-distribution
density and declares a test example to be out-of-distribution if it locates in the low-density areas.
The clustering method is based on the statistical distance, and declares an example to be out-of-
distribution if it locates far from its neighborhood. Despite various applications in low-dimensional
spaces, unfortunately, these methods are known to be unreliable in high-dimensional space such as
image space (Wasserman, 2006; Theis et al., 2015). In recent years, out-of-distribution detectors based
on deep models have been proposed. Schlegl et al. (2017) train a generative adversarial networks to
detect out-of-distribution examples in clinical scenario. Sabokrou et al. (2016) train a convolutional
network to detect anomaly in scenes. Andrews et al. (2016) adopt transfer representation-learning
for anomaly detection. All these works require enlarging or modifying the neural networks. In
a more recent work, Hendrycks & Gimpel (2017) found that pre-trained neural networks can be
overconﬁdent to out-of-distribution example, limiting the effectiveness of detection. Our paper aims
to improve the performance of detecting out-of-distribution examples, without requiring any change
to an existing well-trained model.

Our approach leverages the following two interesting observations to help better distinguish between
in- and out-of-distribution examples: (1) On in-distribution images, modern neural networks tend to
produce outputs with larger variance across class labels, and (2) neural networks have larger norm
of gradient of log-softmax scores when applied on in-distribution images. We believe that having a
better understanding of these phenomenon can lead to further insights into this problem.

7 CONCLUSIONS

In this paper, we propose a simple and effective method to detect out-of-distribution data samples
in neural networks. Our method does not require retraining the neural network and signiﬁcantly

5Similar observation can be seen when T = 1, where we present the conditional expectation of the norm of

softmax gradient in Figure 5 (e).

9

Published as a conference paper at ICLR 2018

improves on the baseline (state-of-the-art) on different neural architectures across various in and
out-distribution dataset pairs. We empirically analyze the method under different parameter settings,
and provide some insights behind the approach. Future work involves exploring our method in other
applications such as speech recognition and natural language processing.

The research reported here was supported by NSF Grant CPS ECCS 1739189.

ACKNOWLEDGMENTS

REFERENCES

Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané.

Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.

Jerone T.A Andrews, Thomas Tanay, Edward J. Morton, and Lewis D. Grifﬁn. Transfer representation-

learning for anomaly detection. In ICML, 2016.

Yaroslav Bulatov. notmnist dataset. 2011.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for
statistical machine translation. EMNLP, 2014.

C Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on information theory,

Jesse Davis and Mark Goadrich. The relationship between precision-recall and roc curves. In ICML.

16(1):41–46, 1970.

ACM, 2006.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale

hierarchical image database. In CVPR, 2009.

Luc Devroye, László Györﬁ, and Gábor Lugosi. A probabilistic theory of pattern recognition,

volume 31. Springer Science & Business Media, 2013.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.

Clement Farabet, Camille Couprie, Laurent Najman, and Yann LeCun. Learning hierarchical features
for scene labeling. IEEE transactions on pattern analysis and machine intelligence, 35(8):1915–
1929, 2013.

Tom Fawcett. An introduction to roc analysis. Pattern recognition letters, 2006.

Amol Ghoting, Srinivasan Parthasarathy, and Matthew Eric Otey. Fast mining of distance-based
outliers in high-dimensional datasets. Data Mining and Knowledge Discovery, 16(3):349–364,
2008.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. ICLR, 2015.

Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola. A

kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723–773, 2012.

Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural

networks. arXiv preprint arXiv:1706.04599, 2017.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image

recognition. In CVPR, 2016.

Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution

examples in neural networks. ICLR, 2017.

10

Published as a conference paper at ICLR 2018

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv

preprint arXiv:1503.02531, 2015.

preprint arXiv:1608.06993, 2016.

Gao Huang, Zhuang Liu, and Kilian Q Weinberger. Densely connected convolutional networks. arXiv

Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolutional neural networks for human action
recognition. IEEE transactions on pattern analysis and machine intelligence, 35(1):221–231,
2013.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097–1105,
2012.

Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning

through probabilistic program induction. Science, 2015.

Christopher D Manning, Hinrich Schütze, et al. Foundations of statistical natural language processing,

volume 999. MIT Press, 1999.

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal

adversarial perturbations. CVPR, 2017.

Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High conﬁdence

predictions for unrecognizable images. 2015.

Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, and Geoffrey Hinton. Regularizing

neural networks by penalizing conﬁdent output distributions. ICLR, 2017.

Marco AF Pimentel, David A Clifton, Lei Clifton, and Lionel Tarassenko. A review of novelty

detection. Signal Processing, 99:215–249, 2014.

Mohammad Sabokrou, Mohsen Fayyaz, Mahmood Fathy, et al. Fully convolutional neural network

for fast anomaly detection in crowded scenes. arXiv preprint arXiv:1609.00866, 2016.

Takaya Saito and Marc Rehmsmeier. The precision-recall plot is more informative than the roc plot
when evaluating binary classiﬁers on imbalanced datasets. PloS one, 10(3):e0118432, 2015.

Thomas Schlegl, Philipp Seeböck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs.
Unsupervised anomaly detection with generative adversarial networks to guide marker discovery.
In International Conference on Information Processing in Medical Imaging, pp. 146–157. Springer,
2017.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image

recognition. ICLR, 2015.

Bharath K Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Schölkopf, and Gert RG
Lanckriet. Hilbert space embeddings and metrics on probability measures. Journal of Machine
Learning Research, 11(Apr):1517–1561, 2010.

Dougal J Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alex
Smola, and Arthur Gretton. Generative models and model criticism via optimized maximum mean
discrepancy. ICLR, 2016.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,

and Rob Fergus. Intriguing properties of neural networks. NIPS, 2014.

Lucas Theis, Aäron van den Oord, and Matthias Bethge. A note on the evaluation of generative

models. ICLR, 2015.

11

Published as a conference paper at ICLR 2018

Pascal Vincent and Yoshua Bengio. Manifold parzen windows. In Advances in neural information

processing systems, pp. 849–856, 2003.

Larry Wasserman. All of Nonparametric Statistics. Springer, 2006.

Pingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R Kulkarni, and Jianxiong
Xiao. Turkergaze: Crowdsourcing saliency with webcam based eye tracking. arXiv preprint
arXiv:1504.06755, 2015.

Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-
scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365,
2015.

Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,

2016.

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding

deep learning requires rethinking generalization. ICLR, 2017.

12

Published as a conference paper at ICLR 2018

A SUPPLEMENTARY RESULTS IN SECTION 4.4
A.1 EXPERIMENTAL RESULTS

Out-of-distribution
dataset

FPR
(95% TPR)
↓

Detection
Error
↓

AUROC

↑

AUPR
In
↑

AUPR
Out
↑

Baseline (Hendrycks & Gimpel, 2017) / Ours

WRN-40-4
CIFAR-10

WRN-40-4
CIFAR-100

Dense-BC
CIFAR-80

WRN-28-10
CIFAR-80

WRN-40-4
CIFAR-80

MNIST

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

CIFAR-20
TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

CIFAR-20
TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

CIFAR-20
TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

Omniglot
notMNIST
CIFAR-10bw
Gaussian
Uniform

49.8/36.7
62.3/49.1
34.6/23.0
54.5/35.1
58.6/41.0
26.6/3.2
21.8/0.9

66.9/43.3
78.1/55.1
74.9/35.9
77.9/50.0
79.5/52.9
84.7/3.3
77.2/3.1

84.1/81.1
72.9/22.7
84.4/46.3
67.1/20.9
84.9/45.9
86.1/50.2
100.0/0.9
98.5/1.2

80.4/78.3
71.3/46.7
81.0/48.8
74.4/45.5
81.9/49.0
82.7/51.1
99.6/1.4
100.0/0.4

82.4/78.4
68.3/34.3
80.6/53.5
72.2/33.2
79.1/51.2
81.2/53.2
99.7/48.6
99.7/10.7

0.2/0.0
10.3/8.7
0.1/0.0
0.0/0.0
0.0/0.0

27.4/20.9
33.6/27.1
19.8/14.0
29.8/20.1
31.8/23.0
15.8/4.1
13.4/3.0

36.0/24.1
41.5/30.1
40.0/20.4
41.5/27.5
42.2/28.9
44.9/4.2
41.1/4.0

44.9/43.0
39.0/13.8
44.7/25.6
36.0/12.9
45.0/25.4
50.5/27.6
52.5/3.0
51.8/3.1

42.7/41.6
38.1/25.9
43.0/26.9
39.7/25.2
43.5/27.0
43.9/28.1
52.3/3.2
52.5/2.7

43.7/41.7
36.6/19.6
42.8/29.2
38.6/19.1
42.0/28.1
43.1/29.1
52.4/26.8
52.4/7.8

2.6/2.5
7.7/6.8
2.5/2.5
2.5/2.5
2.5/2.5

87.3/89.3
79.3/81.6
93.4/95.1
84.7/87.0
82.1/84.9
96.1/99.2
96.5/99.7

81.3/88.5
72.6/81.6
79.1/90.8
75.2/85.6
74.3/84.3
86.3/98.8
86.4/99.0

76.6/77.8
83.4/96.2
76.8/91.7
84.6/96.2
77.5/91.8
76.1/90.5
64.3/98.6
80.4/99.6

79.2/80.4
83.1/91.9
77.1/89.2
82.0/92.9
78.8/90.1
78.3/89.4
80.6/98.9
79.7/99.1

76.8/78.1
83.6/93.4
76.2/87.7
83.1/93.4
77.6/88.8
76.2/87.7
65.6/93.8
74.3/97.7

85.1/86.7
73.5/76.9
93.1/94.3
79.8/82.6
76.4/80.2
97.0/99.2
97.5/99.7

80.6/87.2
69.4/78.0
81.4/89.9
73.1/83.5
72.9/81.9
90.5/99.1
90.2/99.2

79.4/80.6
86.3/96.6
80.3/92.7
86.9/96.4
81.4/92.9
79.8/91.3
78.4/99.1
86.7/99.6

81.5/82.2
85.9/92.6
80.0/89.5
84.4/93.0
82.2/90.8
81.5/90.0
87.7/99.2
87.4/99.4

78.9/79.1
85.9/94.0
78.5/88.3
86.3/93.7
80.0/89.4
78.7/88.3
77.3/95.7
83.0/98.4

87.2/90.7
80.6/84.8
92.4/95.2
85.3/89.7
83.2/87.8
94.8/99.2
94.7/99.7

80.1/89.1
71.6/83.4
76.3/91.5
73.3/86.4
71.9/85.1
77.0/97.9
78.6/98.6

71.6/73.6
79.9/95.8
71.5/90.4
82.1/96.0
71.6/90.2
69.9/88.8
52.2/96.6
68.0/99.1

74.2/76.2
79.7/90.7
72.6/88.5
78.2/91.5
73.4/88.8
72.6/88.0
66.8/97.6
65.5/98.0

72.2/75.0
81.2/92.5
72.5/86.1
79.7/93.1
73.9/87.3
72.2/86.1
53.7/88.7
61.2/95.2

99.6/100.0
97.2/98.2
99.7/100.0
99.7/100.0
99.9/100.0

99.7/100.0
97.5/98.4
99.8/100.0
99.8/100.0
99.9/100.0

99.5/100.0
97.4/98.0
99.7/100.0
99.7/100.0
99.9/100.0

Table 3: Distinguishing in- and out-of-distribution test set data for image classiﬁcation. All values are
percentages. ↑ indicates larger value is better, and ↓ indicates lower value is better.

MNIST: We used the same MNIST classiﬁer used by Hendrycks & Gimpel (2017), which is a
three-layer, 256 neuron-wide, fully connected network trained for 30 epochs with Adam (Kingma
& Ba, 2014). The classiﬁer achieve 99.34% test accuracy on the MNIST test set. We compare our
method with the baseline (Hendrycks & Gimpel, 2017) on ﬁve different out-of-distribution datasets:
(1) Omniglot dataset (Lake et al., 2015) contains images of handwritten characters in stead of the
handwritten digits in MNIST; (2) notMNIST (Bulatov, 2011) dataset contains typeface characters;

13

Published as a conference paper at ICLR 2018

(3) CIFAR-10bw contains black and white rescaled CIFAR-10 images; (4)(5) Gaussian and Uniform
image set contains the synthetic Gaussian and Uniform noise images used in Section 4.2.
Wide ResNet-40-4: We use the same architecture used by Hendrycks & Gimpel (2017) to evaluate
the baseline and our method. The Wide ResNet-40-4 achieves 95.7% test accuracy on CIFAR-10
dataset and achieve 79.27% test accuracy on CIFAR-100.
CIFAR-80: DenseNet-BC-100 achieves 78.94% test accuracy on CIFAR-80, while Wide ResNet-
28-10 achieves 81.71% test accuracy and Wide ResNet-40-4 achieves 79.53% test accuracy on
CIFAR-80.

A.2 PARAMETER SETTINGS

For MNIST, we set T = 1000 and ε = 0. The parameter settings for other structures are shown as
follows.

DenseNet-BC-100

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

0.0014
0.0014
0
0.0014
0.0014
0.0014
0.0014
-

0.002
0.0022
0.0036
0.002
0.002
0.0028
0.0026
0.0002

0.002
0.0022
0.0038
0.0018
0.002
0.0024
0.0028
-

Table 4: Optimal perturbation magnitude ε for reproducing main results in Table 2 and 3.

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

Table 5: Optimal Temperature T for reproducing main results in Table 2 and 3.

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

DenseNet-BC-100

1000
1000
1000
1000
1000
1000
1000
-

1000
1000
1000
1000
1000
1
1
1

1000
1000
1000
1000
1000
1
1
-

Wide-ResNet-28-10

0.0005
0.0011
0
0.0006
0.0008
0.0014
0.0014
-

0.0002
0.0004
0.0002
0.0002
0.0002
0.0002
0.0002
5e-05

0.0026
0.0024
0.0038
0.0026
0.0026
0.0032
0.0032
-

14

Table 6: Optimal perturbation magnitude ε for reproducing main results in Table 2 and 3.

Published as a conference paper at ICLR 2018

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

Wide-ResNet-28-10

1000
1000
1000
1000
1000
1000
1000
-

1000
1000
1000
1000
1000
1
1
1

1000
1000
1000
1000
1000
1000
1000
-

Table 7: Optimal Temperature T for reproducing main results in Table 2 and 3.

Wide-ResNet-40-4

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

0.0004
0.0008
0
0.001
0.0008
0.0016
0.0016
-

0.0002
0.0004
0.0002
0.0002
0.0002
0.0002
0.0002
0.0002

0.0014
0.0016
0.0038
0.0014
0.0016
0.0024
0.0026
-

Table 8: Optimal perturbation magnitude ε for reproducing main results in Table 2 and 3.

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

Wide-ResNet-40-4

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

1000
1000
1000
1000
1000
1
1
1

1000
1000
1000
1000
1000
1000
1000
-

Table 9: Optimal Temperature T for reproducing main results in Table 2 and 3.

1000
1000
1000
1000
1000
1000
1000
-

15

Published as a conference paper at ICLR 2018

B SUPPLEMENTARY RESULTS IN SECTION 5.1

Figure 7: Detection performance on DenseNet, Wide ResNet-28-10 and Wide ResNet-40-4 under different
temperature, when input preprocessing is not used, i.e., ε = 0. All networks are trained on CIFAR-10.

16

Published as a conference paper at ICLR 2018

Figure 8: Detection performance on DenseNet, Wide ResNet-28-10 and Wide ResNet-40-4 under different
perturbation magnitude, when temperature scaling is not used, i.e., T = 1. All networks are trained on
CIFAR-10.

17

Published as a conference paper at ICLR 2018

Figure 9: Detection performance on DenseNet, Wide ResNet-28-10 and Wide ResNet-40-4 under different
perturbation magnitude, when the optimal temperature is used, i.e., T = 1000. All networks are trained on
CIFAR-10.

18

Published as a conference paper at ICLR 2018

C SUPPLEMENTARY RESULTS IN SECTION 5.2 AND 5.3

Figure 10: Expectation of the second order term U2 conditioned on the ﬁrst order term U1 under DenseNet,
Wide-ResNet-28-10 and Wide ResNet-40-4. All networks are trained on CIFAR-10.

Figure 11: Expectation of gradient norms conditioned on the softmax scores under DenseNet, Wide-ResNet-28-
10 and Wide ResNet-40-4, where the temperature scaling is not used. All networks are trained on CIFAR-10.

Figure 12: Expectation of gradient norms conditioned on the softmax scores under DenseNet, Wide-ResNet-28-
10 and Wide ResNet-40-4, where the optimal temperature is used, i.e., T = 1000. All networks are trained on
CIFAR-10.

19

In this section, we present the Taylor expansion of the soft-max score function:

Published as a conference paper at ICLR 2018

D TAYLOR EXPANSION

Sˆy(x; T ) =

(cid:80)N

exp (fˆy(x)/T )
i=1 exp(fi(x)/T )
1
(cid:16) fi(x)−f ˆy(x)
T

i=1 exp

(cid:17)

(cid:80)N

=

=

1
+ 1
2!

1
fi(x)] + 1
2T 2

(cid:104)

(cid:80)N

i=1

1 + fi(x)−f ˆy(x)

T

(fi(x)−f ˆy(x))2
T 2

+ o (cid:0) 1
T 2

(cid:1)(cid:105)

by Taylor expansion

≈

N

−

1
T

(cid:80)N

i=1[fˆy(x)

−

(cid:80)N

i=1[fi(x)

fˆy(x)]2

−

E PROPOSITION 1

The following proposition 1 shows that the detection error Pe(T, 0)
Thus, increasing the temperature further can only slightly improve the detection performance.
Proposition 1. There exists a constant c only depending on function U1, in-distribution PX and
out-of-distribution QX such that limT →∞ Pe(T, ε) = c, when ε = 0 (i.e., no input preprocessing).
Proof. Since

c if T is sufﬁciently large.

≈

Therefore, for any X,
(cid:18)

Sˆy(X; T ) =

exp(fˆy(X)/T )
i=1 exp(fi(X)/T )

(cid:80)N

=

1 + (cid:80)

1

i(cid:54)=ˆy exp([fi(X)

fˆy(X)]/T )

−

lim
T →∞

T

1
Sˆy(X; T )

−

(cid:19)

+ N

= lim
T →∞

(cid:20)
1

T

(cid:88)

i(cid:54)=ˆy

exp

−

(cid:18) fi(X)

fˆy(X)

(cid:19)(cid:21)

−
T

=

[fˆy(X)

fi(X)] = (N

1)U1(X)

−

−

(cid:88)

i(cid:54)=ˆy

This indicates that the random variable

(cid:18)

T

1
Sˆy(X; T )

−

(cid:19)

+ N

(N

1)U1(X) a.s.

→

−

as T
the false positive rate

→ ∞

. This means that for a speciﬁc α > 0, choosing the threshold δT = 1/(N

α/T ), then

FPR(T ) = QX (Sˆy(X; T ) > 1/(N

α/T )) = QX

T

N

−

−

−

(cid:18)

(cid:18)

1
Sˆy(X; T )

−

(cid:19)

(cid:19)

> α

QX ((N

1)U1(X) > α) ,

(cid:18)

(cid:18)

1
Sˆy(X; T )

−

(cid:19)

(cid:19)

> α

PX ((N

1)U1(X) > α) .

−

−

T →∞
−−−−→

T →∞
−−−−→

and the true positive rate

TPR(T ) = PX (Sˆy(X; T ) > 1/(N

α/T )) = PX

T

N

Choosing α∗ such that PX ((N
at the same time FPR(T )
depending on U1, PX , QX and PZ, such that

QX ((N

→

−

−

1)U1(X) > α∗) = 0.95, then TPR(T )
1)U1(X) > α∗) as T

0.95 as T

and
. There exists a constant c

→ ∞

→

→ ∞

lim
T →∞

Pe(T, 0) = 0.05P (Z = 0) + P (Z = 1)QX ((N

1)U1(X) > α∗) = c.

−

20

Published as a conference paper at ICLR 2018

F ANALYSIS OF TEMPERATURE

For simplicity of the notations, let ∆i = fˆy
mean of the set ∆. Therefore,

−

fi and thus ∆ =

∆i

i(cid:54)=ˆy. Besides, let ¯∆ denote the
}

{

¯∆ =

1

N

1

−

(cid:88)

i(cid:54)=ˆy

∆i =

1

N

1

−

(cid:88)

i(cid:54)=ˆy

[fˆy

fi] = U1.

−

Equivalently,

Next, we will show

U1 = Mean(∆).

U2 =

1

N

1

−

(cid:88)

i(cid:54)=ˆy

[fˆy

−

(cid:122)

1

fi]2 =

Variance2(∆)
(cid:125)(cid:124)
(cid:88)

[∆i

Mean2(∆)
(cid:122)(cid:125)(cid:124)(cid:123)
¯∆2

.

(cid:123)
¯∆]2 +

N

1

−

i(cid:54)=ˆy

−

by∆i = fˆy

fi

−

Since

then

U2 =

∆2
i

(cid:88)

i(cid:54)=ˆy
(cid:88)

i(cid:54)=ˆy
(cid:88)

i(cid:54)=ˆy

(cid:88)

1

−
1

−
1

−
1

−

1

1

1

1

N

N

N

N

(cid:124)

=

=

=

¯∆ + ¯∆)2

(∆i

−

[(∆i

¯∆)2

−

−

2(∆i

−

¯∆) ¯∆ + ¯∆2]

¯∆]2

[∆i

−

i(cid:54)=ˆy
(cid:123)(cid:122)
Variance2(∆)

−

N

1

−

(cid:125)

(cid:124)

i(cid:54)=ˆy
(cid:123)(cid:122)
=0

2 ¯∆

(cid:88)

(∆i

−

¯∆)

+ ¯∆2

(cid:125)

(cid:124)(cid:123)(cid:122)(cid:125)
Mean2(∆)

U2 = Variance2(∆) + Mean2(∆)

G ADDITIONAL RESULTS IN SECTION 4.5

Apart from the Maximum Mean Discrepancy, we as well calculate the Energy distance between in-
and out-of-distribution datasets. Let P and Q denote two different distributions. Then the energy
distance between distributions P and Q is deﬁned as

D2

energy(P, Q) = 2EV ∼P,W ∼Q

X
(cid:107)

Y

−

(cid:107) −

EV,V (cid:48)∼P

X (cid:48)

X
(cid:107)

−

(cid:107) −

EW,W (cid:48)∼Q

Y
(cid:107)

Y (cid:48)

.

(cid:107)

−

Therefore, the energy distance between two datasets V =
W1, ..., Wm
{

Q is deﬁned as

iid
∼

}

V1, ..., Vm
{

}

iid
∼

P and W =

2

(cid:92)Denergy

(P, Q) =

2
m2

m
(cid:88)

m
(cid:88)

i=1

j=1

Vi
(cid:107)

−

Wj

(cid:107) −

1
(cid:1)
(cid:0)m
2

(cid:88)

i(cid:54)=j

Vi

(cid:107)

−

Vj

(cid:107) −

1
(cid:1)
(cid:0)m
2

(cid:88)

i(cid:54)=j

Wi
(cid:107)

Wj

.
(cid:107)

−

In the experiment, we use the 2-norm

(cid:107) · (cid:107)2.

21

Published as a conference paper at ICLR 2018

In-distribution Out-of-distribution
datasets

Datasets

MMD
Distance Distance

Energy

CIFAR-100

CIFAR-80

Tiny-ImageNet (crop)
LSUN (crop)
Tiny-ImageNet (resize)
LSUN (resize)
iSUN (resize)

Tiny-ImageNet (crop)
LSUN (crop)
Tiny-ImageNet (resize)
LSUN (resize)
iSUN (resize)
CIFAR-20

0.41
0.43
0.088
0.12
0.11

0.4
0.43
0.095
0.120
0.116
0.057

2.25
2.31
0.54
0.63
0.56

2.22
2.29
0.57
0.62
0.61
0.35

22

Published as a conference paper at ICLR 2018

Figure 13: False positive rate (FPR) and true positive rate (TPR) under different thresholds (δ) when the
temperature (T ) is set to 1, 000 and the perturbation magnitude (ε) is set to 0.0014. The DenseNet is trained on
CIFAR-10.

Figure 14: Detection performance on Tiny-ImageNet (resize), LSUN (resize) and iSUN (resize) when parame-
ters are tuned on six different out-of-distribution datasets. Each tuning set contains 1,000 images and each test
set contains 9,000 images. Both DenseNet and Wide-ResNet are trained on CIFAR-10. Additional results on
other datasets are provide in Table 10 and 11.

H ADDITIONAL DISCUSSIONS

In this section, we present additional discussion on the proposed method. We ﬁrst empirically show
how the threshold δ affects the detection performance. We next show how the proposed method
performs when the parameters are tuned on a certain out-of-distribution dataset and are evaluated on
other out-of-distribution datasets. Finally, we show how the size of dataset for choosing parameters
affects the detection performance.
Effects of the threshold. We analyze how the threshold affects the following metrics: (1) FPR, i.e.,
the fraction of out-of-distribution images misclassiﬁed as in-distribution images; (2) TPR, i.e, the
fraction of in-distribution images correctly classiﬁed as in-distribution images. In Figure 13, we
show how the thresholds affect FPR and TPR when the temperature and perturbation magnitude are
chosen optimally (i.e., T = 1, 000, ε = 0.0014). From the ﬁgure, we can observe that the threshold
corresponding to 95% TPR can produce small FPRs on all out-of-distribution datasets.
Performance across datasets. To investigate how the parameters generalize across datasets, we
tune the parameters using one out-of-distribution dataset and then evaluate on a different one. Given
an out-of-distribution dataset, we ﬁrst split the dataset into two disjoint subsets: tuning set and
test set. The tuning set contains 1,000 images and the test set contains 9,000 images. We tune the
parameters on the tuning set and evaluate the detection performance on the test set. We ﬁrst choose
the temperature T and the perturbation magnitude ε such that the FPR at TPR 95% is minimized on
the tuning set of one out-of-distribution dataset. Next, we set δ to the threshold corresponding to 95%
TPR and calculate the false positive rates on the test sets of other out-of-distribution datasets.

In Figure 14, we show the detection performance on three out-of-distribution datasets when the
parameters are tuned on six different datasets. From Figure 14, we can observe that the parameters
tuned on different tuning sets can have quite similar detection FPRs on all of three out-of-distribution
image sets. This may be due to the fact, shown in Figure 13, that the threshold corresponding to 95%
TPR can produce small FPRs on all datasets.

23

Published as a conference paper at ICLR 2018

Figure 15: FPR at TPR 95% under different tuning set sizes. The DenseNet is trained on CIFAR-10 and each
test set contains 8,000 out-of-distribution images.

Performance vs.
tuning set size. To show the effects of the tuning set size on the detection
performance, we devise the following experiment. For each out-of-distribution dataset, we choose the
tuning set size from 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000. For each set size, we
tune the temperature T and perturbation magnitude ε to minimize the FPR at TPR 95% and calculate
the FPR. In Figure 15, we show the detection performance of ODIN under different tuning set size.
From Figure 15, we can observe that the FPR at TPR 95% tends to stabilize when the set size grows
above 1,000.

24

Published as a conference paper at ICLR 2018

DenseNet-BC-100

Test set

ImgNet (c)

ImgNet (r) LSUN (c) LSUN (c)

iSUN

Gaussian Uniform

Baseline (Hendrycks & Gimpel, 2017) / Ours

ImgNet (c)
ImgNet (r)
LSUN (c)
LSUN (r)
iSUN
Gaussian
Uniform

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

34.7/6.6
40.7/14.9
39.3/8.1
33.6/10.4
37.2/12.6
23.5/0.4
12.3/4.5

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

Table 10: Detection performance across different datasets. Each row corresponds to the FPR at
TPR 95% on the same test set where parameters are tuned under different tuning sets. Each column
corresponds to the FPR at TPR 95% on different test sets where parameters are tuned under the same
tuning set. The DenseNet is trained on CIFAR-10.

Wide-ResNet- 28-10

Test set

ImgNet (c)

ImgNet (r) LSUN (c)

LSUN (c)

iSUN

Gaussian Uniform

Baseline (Hendrycks & Gimpel, 2017) / Ours

ImgNet (c)
ImgNet (r)
LSUN (c)
LSUN (r)
iSUN
Gaussian
Uniform

38.9/23.4
45.6/25.5
35.0/28.1
35.0/18.9
40.6/21.8
1.6/0.0
0.3/0.0

38.9/24.5
45.6/25.5
35.0/31.27
35.0/18.0
40.6/21.7
1.6/0.0
0.3/0.0

38.9/27.1
45.6/32.9
35.0/21.8
35.0/25.6
40.6/28.9
1.6/0.4
0.3/0.0

38.9/23.4
45.6/25.8
35.0/28.2
35.0/17.6
40.6/ 21.9
1.6/0.0
0.3/0.0

38.9/24.1
45.6/25.8
35.0/28.9
35.0/18.0
40.6/21.3
1.6/0.0
0.3/0.0

38.9/26.5
45.6/27.9
35.0/29.7
35.0/19.1
40.6/22.8
1.6/0.0
0.3/0.0

38.9/26.5
45.6/27.9
35.0/29.7
35.0/19.1
40.6/22.8
1.6/0.0
0.3/0.0

Table 11: Detection performance across different datasets. Each row corresponds to the FPR at
TPR 95% on the same test set where parameters are tuned under different tuning sets. Each column
corresponds to the FPR at TPR 95% on different test sets where parameters are tuned under the same
tuning set. The Wide-ResNet is trained on CIFAR-10.

25

Published as a conference paper at ICLR 2018

Figure 16: (a) The test accuracy on the images having softmax scores above the threshold corresponding to
a certain true positive rate. (b) The test accuracy on the images having softmax scores below the threshold
corresponding to a certain true positive rate. All networks are trained on CIFAR-10.

Figure 17: Outputs of DenseNet on thirty classes for an image of apple from CIFAR-80 and an image of red
pepper from CIFAR-20. The label “0” denotes the class “apple” and the label “49" denotes the class “orange".

I ADDITIONAL ANALYSIS

Difﬁcult-to-classify images and difﬁcult-to-detect images. We analyze the correlation between
the images that tend to be out-of-distribution and images on which the neural network tend to make
incorrect predictions. To understand the correlation, we devise the following experiment. For the
ﬁxed temperature T and perturbation magnitude ε, we ﬁrst set δ to the softmax score threshold
corresponding to a certain true positive rate. Next, we calculate the test accuracy on the images with
softmax scores above δ and the test accuracy on the images with softmax score below δ, respectively.
We report the results in Figure 16(a) and (b). From these two ﬁgures, we can observe that the images
that are difﬁcult to detect are more likely to be the images that are difﬁcult to classify. For example,
the DenseNet can achieve up to 98.5% test accuracy on the images having softmax scores above the
threshold corresponding to 80% TPR, but can only achieve around 82% test accuracy on the images
having softmax scores below the threshold corresponding to 80% TPR.
Same manifold datasets. We provide additional empirical results showing how the term E[U2|
U1]
affects the detection performance when in- and out-of-distribution datasets locate on the same
manifold. In Figure 17, we show the outputs of DenseNet on thirty classes for an image of apple
from CIFAR-80 (in-distribution) and an image of red pepper of CIFAR-20 (out-distribution). We
can observe that the outputs of DenseNet for both images are quite similar to each other. In addition,
we can observe that for both images, the second and third largest output are quite close to the
largest output. This may be due the fact the image of red pepper shares some common features
with the images in CIFAR-80. Furthermore, the similarity between the outputs for the images from

26

Published as a conference paper at ICLR 2018

Out-of-distribution
dataset

FPR
(95% TPR)
↓

Detection AUROC

Error
↓

↑

AUPR
In
↑

AUPR
Out
↑

Baseline (Hendrycks & Gimpel, 2017) / Ours

CIFAR-10
CIFAR-100

CIFAR-100
CIFAR-10

57.1/47.2
81.8/81.4

31.1/26.1
43.4/43.2

89.0/89.8
76.1/76.7

91.2/91.4
79.9/80.4

86.8/88.7
71.3/72.6

Table 12: Distinguishing in- and out-of-distribution test set data for image classiﬁcation. All values are
percentages. ↑ indicates larger value is better, and ↓ indicates lower value is better. The architecture is DenseNet.

CIFAR-80 and CIFAR-20 can help explain that the detection task becomes harder when in- and
out-of-distribution datasets locate on the same manifold.

Reciprocal results between datasets. In Table 12, we show the reciprocal results between datasets.
First, we train the DenseNet on the CIFAR-10 dataset (in-distribution) and evaluate the detection
performance on the CIFAR-100 dataset (out-distribution). Next, we train the DenseNet on the
CIFAR-100 dataset (in-distribution) and evaluate the detection performance on the CIFAR-10 dataset
(out-distribution). From Table 12, we can observe that the performance of the DenseNet trained on
CIFAR-10 is better than the performance of the DenseNet trained on CIFAR-100. This may be due to
the fact that the DenseNet has a higher test accuracy on CIFAR-10 (around 95%) compared to the
test accuracy on CIFAR-100 (around 77%).

27

8
1
0
2
 
b
e
F
 
5
2
 
 
]

G
L
.
s
c
[
 
 
4
v
0
9
6
2
0
.
6
0
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

ENHANCING THE RELIABILITY OF
OUT-OF-DISTRIBUTION IMAGE DETECTION IN
NEURAL NETWORKS

Shiyu Liang
Coordinated Science Lab, Department of ECE
University of Illinois at Urbana-Champaign
sliang26@illinois.edu

R. Srikant
Coordinated Science Lab, Department of ECE
University of Illinois at Urbana-Champaign
rsrikant@illinois.edu

Yixuan Li
Facebook Research
yixuanl@fb.com

ABSTRACT

We consider the problem of detecting out-of-distribution images in neural networks.
We propose ODIN, a simple and effective method that does not require any change
to a pre-trained neural network. Our method is based on the observation that using
temperature scaling and adding small perturbations to the input can separate the
softmax score distributions between in- and out-of-distribution images, allowing
for more effective detection. We show in a series of experiments that ODIN
is compatible with diverse network architectures and datasets. It consistently
outperforms the baseline approach (Hendrycks & Gimpel, 2017) by a large margin,
establishing a new state-of-the-art performance on this task. For example, ODIN
reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet
(applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%.

1

INTRODUCTION

Modern neural networks are known to generalize well when the training and testing data are sampled
from the same distribution (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016;
Cho et al., 2014; Zhang et al., 2017). However, when deploying neural networks in real-world
applications, there is often very little control over the testing data distribution. Recent works
have shown that neural networks tend to make high conﬁdence predictions even for completely
unrecognizable (Nguyen et al., 2015) or irrelevant inputs (Hendrycks & Gimpel, 2017; Szegedy et al.,
2014; Moosavi-Dezfooli et al., 2017). It has been well documented (Amodei et al., 2016) that it is
important for classiﬁers to be aware of uncertainty when shown new kinds of inputs, i.e., out-of-
distribution examples. Therefore, being able to accurately detect out-of-distribution examples can
be practically important for visual recognition tasks (Krizhevsky et al., 2012; Farabet et al., 2013; Ji
et al., 2013).

A seemingly straightforward approach of detecting out-of-distribution images is to enlarge the training
set of both in- and out-of-distribution examples. However, the number of out-of-distribution examples
can be inﬁnitely many, making the re-training approach computationally expensive and intractable.
Moreover, to ensure that a neural network accurately classiﬁes in-distribution samples into correct
classes while correctly detecting out-of-distribution samples, one might need to employ exceedingly
large neural network architectures, which further complicates the training process.

Hendrycks & Gimpel (2017) proposed a baseline method to detect out-of-distribution examples
without further re-training networks. The method is based on an observation that a well-trained neural
network tends to assign higher softmax scores to in-distribution examples than out-of-distribution
examples. In this paper, we go further. We observe that after using temperature scaling in the softmax
function (Hinton et al., 2015; Pereyra et al., 2017) and adding small controlled perturbations to inputs,

1

Published as a conference paper at ICLR 2018

the softmax score gap between in - and out-of-distribution examples is further enlarged. We will
show that the combination of these two techniques (temperature scaling and input perturbation) can
lead to better detection performance. For example, provided with a pre-trained DenseNet (Huang
et al., 2016) on CIFAR-10 dataset (positive samples), we test against images from TinyImageNet
dataset (negative samples). Our method reduces the False Positive Rate (FPR), i.e., the fraction of
misclassiﬁed out-of-distribution samples, from 34.7% to 4.3%, when 95% of in-distribution images
are correctly classiﬁed. We summarize the main contributions of this paper as the following:

•

•

We propose a simple and effective method, ODIN (Out-of-DIstribution detector for Neural net-
works), for detecting out-of-distribution examples in neural networks. Our method does not require
re-training the neural network and is easily implementable on any modern neural architecture.

We test ODIN on state-of-the-art network architectures (e.g., DenseNet (Huang et al., 2016) and
Wide ResNet (Zagoruyko & Komodakis, 2016)) under a diverse set of in- and out-distribution
dataset pairs. We show ODIN can signiﬁcantly improve the detection performance, and consistently
outperforms the state-of-the-art method (Hendrycks & Gimpel, 2017) by a large margin.

We empirically analyze how parameter settings affect the performance, and further provide simple
analysis that provides some intuition behind our method.

•

The outline of this paper is as follows. In Section 2, we present the necessary deﬁnitions and the
problem statement. In Section 3, we introduce ODIN and present performance results in Section 4.
We experimentally analyze the proposed method and provide some justiﬁcation for our method in
Section 5. We summarize the related works and future directions in Section 6 and conclude the paper
in Section 7.

2 PROBLEM STATEMENT

X

In this paper, we consider the problem of distinguishing in- and out-of-distribution images on a pre-
trained neural network. Let PX and QX denote two distinct data distributions deﬁned on the image
. Assume that a neural network f is trained on a dataset drawn from the distribution PX . Thus,
space
we call PX the in-distribution and QX the out-distribution, respectively. In testing, we draw new
images from a mixture distribution PX×Z deﬁned on
, where the conditional probability
distributions PX|Z=0 = PX and PX|Z=1 = QX denote in- and out-distribution respectively. Now
we focus on the following problem: Given an image X drawn from the mixture distribution PX×Z,
can we distinguish whether the image is from in-distribution PX or not?

0, 1
}

X × {

In this paper, we focus on detecting out-of-distribution images. However, it is equally important to
correctly classify an image into the right class if it is an in-distribution image. But this can be easily
done: once it has been detected that an image is in-distribution, we can simply use the original image
and run it through the neural network to classify it. Thus, we do not change the predictions of the
neural network for in-distribution images and only focus on improving the detection performance for
out-of-distribution images.

3 ODIN: OUT-OF-DISTRIBUTION DETECTOR

In this section, we present our method, ODIN, for detecting out-of-distribution samples. The detector
is built on two components: temperature scaling and input preprocessing. We describe the details of
both components below.
Temperature Scaling. Assume that the neural network f = (f1, ..., fN ) is trained to classify N
classes. For each input x, the neural network assigns a label ˆy(x) = arg maxi Si(x; T ) by computing
the softmax output for each class. Speciﬁcally,

Si(x; T ) =

exp (fi(x)/T )
j=1 exp (fj(x)/T )
R+ is the temperature scaling parameter and set to 1 during the training. For a given input
where T
x, we call the maximum softmax probability, i.e., Sˆy(x; T ) = maxi Si(x; T ) the softmax score. In
this paper, we use notations Sˆy(x; T ) and S(x; T ) interchangeably. Prior works have established
the use of temperature scaling to distill the knowledge in neural networks (Hinton et al., 2015) and

(cid:80)N

(1)

∈

,

2

Published as a conference paper at ICLR 2018

−

εsign(

x log Sˆy(x; T )),

calibrate the prediction conﬁdence in classiﬁcation tasks (Guo et al., 2017). As we shall see later,
a good manipulation of temperature T can push the softmax scores of in- and out-of-distribution
images further apart from each other, making the out-of-distribution images distinguishable.
Input Preprocessing. Before feeding the image x into the neural network, we preprocess the input
by adding small perturbations to it. The preprocessed image is given by
˜x = x

−∇
where the parameter ε can be interpreted as the perturbation magnitude. The method is inspired by the
idea in the reference (Goodfellow et al., 2015), where small perturbations are added to decrease the
softmax score for the true label and force the neural network to make a wrong prediction. Here, our
goal and setting are rather different: we aim to increase the softmax score of any given input, without
the need for a class label at all. As we shall see later, the perturbation can have stronger effect on the
in- distribution images than that on out-of-distribution images, making them more separable. Note
that the perturbations can be easily computed by back-propagating the gradient of the cross-entropy
loss w.r.t the input.
Out-of-distribution Detector. The proposed approach works as follows. For each image x, we ﬁrst
calculate the preprocessed image ˜x according to the equation (2). Next, we feed the preprocessed
image ˜x into the neural network, calculate its softmax score S( ˜x; T ) and compare the score to the
threshold δ. We say that the image x is an in-distribution example if the softmax score is above
the threshold and that the image x is an out-of-distribution example, otherwise. Therefore, the
out-of-distribution detector is given by

(2)

g(x; δ, T, ε) =

(cid:26)1
0

if maxi p( ˜x; T )
δ,
if maxi p( ˜x; T ) > δ.

≤

The parameters T, ε and δ are chosen so that the true positive rate (i.e., the fraction of in-distribution
images correctly classiﬁed as in-distribution images) under some out-of-distribution image data set
is 95%. (The choice of the out-of-distribution images to tune the parameters T, ε and δ appears to
be unimportant, as demonstrated in the appendix H.) Having chosen the parameters as above, we
evaluate the performance of our algorithm using various metrics in the next section.

4 EXPERIMENTS

4.1 TRAINING SETUP

In this section, we demonstrate the effectiveness of ODIN on several computer vision benchmark
datasets. We run all experiments with PyTorch1 and we will release the code to reproduce all
experimental results2.

Architectures and training conﬁgurations. We adopt two state-of-the-art neural network architec-
tures, including DenseNet (Huang et al., 2016) and Wide ResNet (Zagoruyko & Komodakis, 2016).
For DenseNet, our model follows the same setup as in (Huang et al., 2016), with depth L = 100,
growth rate k = 12 (Dense-BC) and dropout rate 0. In addition, we evaluate the method on a Wide
ResNet, with depth 28, width 10 (WRN-28-10) and dropout rate 0. Furthermore, in Appendix A.1, we
provide additional experimental results on another Wide ResNet with depth 40, width 4 (WRN-40-4).
The hyper-parameters of neural networks are set identical to the original Wide ResNet (Zagoruyko
& Komodakis, 2016) and DenseNet (Huang et al., 2016) implementations. All neural networks are
trained with stochastic gradient descent with Nesterov momentum (Duchi et al., 2011; Kingma & Ba,
2014). Speciﬁcally, we train Dense-BC for 300 epochs with batch size 64 and momentum 0.9; and
Wide ResNet for 200 epochs with batch size 128 and momentum 0.9. The learning rate starts at 0.1,
and is dropped by a factor of 10 at 50% and 75% of the training progress, respectively.

Accuracy.
Each neural network architecture is
trained on CIFAR-10 (C-10) and CIFAR-100 (C-100)
datasets (Krizhevsky & Hinton, 2009),
respectively.
CIFAR-10 and CIFAR-100 images are drawn from 10
and 100 classes, respectively. Both datasets consist of

1http://pytorch.org
2https://github.com/facebookresearch/odin

Architecture C-10 C-100

Dense-BC
WRN-28-10

4.81
3.71

22.37
19.86

Table 1: Test error rates on CIFAR-10
and CIFAR-100 datasets.

3

Published as a conference paper at ICLR 2018

50,000 training images and 10,000 test images. The test
error on CIFAR datasets are summarized in Table 1.

4.2 OUT-OF-DISTRIBUTION DATASETS

At test time, the test images from CIFAR-10 (CIFAR-100) datasets can be viewed as the in-distribution
(positive) examples. For out-of-distribution (negative) examples, we follow the setting in (Hendrycks
& Gimpel, 2017) and test on several different natural image datasets and synthetic noise datasets. All
the datasets considered are listed below.

(1) TinyImageNet. The Tiny ImageNet dataset3 consists of a subset of ImageNet images (Deng
et al., 2009). It contains 10,000 test images from 200 different classes. We construct two datasets,
TinyImageNet (crop) and TinyImageNet (resize), by either randomly cropping image patches of
size 32

32 or downsampling each image to size 32

32.

(2) LSUN. The Large-scale Scene UNderstanding dataset (LSUN) has a testing set of 10,000 images
of 10 different scenes (Yu et al., 2015). Similar to TinyImageNet, we construct two datasets,
LSUN (crop) and LSUN (resize), by randomly cropping and downsampling the LSUN testing set,
respectively.

×

×

(3) iSUN. The iSUN (Xu et al., 2015) consists of a subset of SUN images. We include the entire

collection of 8925 images in iSUN and downsample each image to size 32 by 32.

(4) Gaussian Noise. The synthetic Gaussian noise dataset consists of 10,000 random 2D Gaussian
noise images, where each RGB value of every pixel is sampled from an i.i.d Gaussian distribution
with mean 0.5 and unit variance. We further clip each pixel value into the range [0, 1].

(5) Uniform Noise. The synthetic uniform noise dataset consists of 10,000 images where each RGB
value of every pixel is independently and identically sampled from a uniform distribution on [0, 1].

4.3 EVALUATION METRICS

We adopt the following four different metrics to measure the effectiveness of a neural network in
distinguishing in- and out-of-distribution images.

(1) FPR at 95% TPR can be interpreted as the probability that a negative (out-of-distribution)
example is misclassiﬁed as positive (in-distribution) when the true positive rate (TPR) is as high as
95%. True positive rate can be computed by TPR = TP / (TP+FN), where TP and FN denote true
positives and false negatives respectively. The false positive rate (FPR) can be computed by FPR =
FP / (FP+TN), where FP and TN denote false positives and true negatives respectively.

(2) Detection Error, i.e., Pe measures the misclassiﬁcation probability when TPR is 95%. The
TPR) + 0.5FPR, where we assume that both positive

deﬁnition of Pe is given by Pe = 0.5(1
and negative examples have the equal probability of appearing in the test set.

−

(3) AUROC is the Area Under the Receiver Operating Characteristic curve, which is also a threshold-
independent metric (Davis & Goadrich, 2006). The ROC curve depicts the relationship between
TPR and FPR. The AUROC can be interpreted as the probability that a positive example is assigned
a higher detection score than a negative example (Fawcett, 2006). A perfect detector corresponds
to an AUROC score of 100%.

(4) AUPR is the Area under the Precision-Recall curve, which is another threshold independent
metric (Manning et al., 1999; Saito & Rehmsmeier, 2015). The PR curve is a graph showing
the precision=TP/(TP+FP) and recall=TP/(TP+FN) against each other. The metric AUPR-In and
AUPR-Out in Table 2 denote the area under the precision-recall curve where in-distribution and
out-of-distribution images are speciﬁed as positives, respectively.

4.4 EXPERIMENTAL RESULTS

Comparison with baseline. In Figure 1, we show the ROC curves when DenseNet-BC-100 is
evaluated on CIFAR-10 (positive) images against TinyImageNet (negative) test examples. The red
curve corresponds to the ROC curve when using baseline method (Hendrycks & Gimpel, 2017),

3https://tiny-imagenet.herokuapp.com

4

Published as a conference paper at ICLR 2018

Out-of-distribution
dataset

FPR
(95% TPR)
↓

Detection
Error
↓

AUROC

↑

AUPR
In
↑

AUPR
Out
↑

Baseline (Hendrycks & Gimpel, 2017) / ODIN

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

34.7/4.3
40.8/7.5
39.3/8.7
33.6/3.8
37.2/6.3
23.5/0.0
12.3/0.0

67.8/17.3
82.2/44.3
69.4/17.6
83.3/44.0
84.8/49.5
88.3/0.5
95.4/0.2

38.9/23.4
45.6/25.5
35.0/21.8
35.0/17.6
40.6/21.3
1.6/0.0
0.3/0.0

66.6/43.9
79.2/55.9
74.0/39.6
82.2/56.5
82.7/57.3
98.2/0.1
99.2/1.0

19.9/4.7
22.9/6.3
22.2/6.9
19.3/4.4
21.1/5.7
14.3/2.5
8.7/2.5

36.4/11.2
43.6/24.6
37.2/11.3
44.1/24.5
44.7/27.2
46.6/2.8
50.2/2.6

21.9/14.2
25.3/15.2
20.0/13.4
20.0/11.3
22.8/13.2
3.3/2.5
2.6/2.5

35.8/24.4
42.1/30.4
39.5/22.3
43.6/30.8
43.9/31.1
51.6/2.5
52.1/3.0

95.3/99.1
94.1/98.5
94.8/98.2
95.4/99.2
94.8/98.8
96.5/99.9
97.5/100.0

83.0/97.1
70.4/90.7
83.7/96.8
70.6/91.5
69.9/90.1
83.2/99.5
81.8/99.6

92.9/94.2
91.0/92.1
94.5/95.9
93.9/95.4
92.5/93.7
99.2/100.0
99.5/100.0

82.0/90.8
72.2/84.0
80.3/92.0
73.9/86.0
72.8/85.6
84.1/99.1
84.3/98.5

96.4/99.1
95.1/98.6
96.0/98.5
96.4/99.3
95.9/98.9
97.8/100.0
98.3/100.0

85.3/97.4
71.4/91.4
86.2/97.1
72.5/92.4
71.9/91.1
88.1/99.6
87.6/99.7

92.5/92.8
89.7/89.0
95.1/95.8
93.8/93.8
91.7/91.2
99.3/100.0
99.6/100.0

83.3/91.4
70.4/82.8
83.4/92.4
75.7/86.2
74.2/85.9
89.9/99.4
90.2/99.1

93.8/99.1
92.4/98.5
93.1/97.8
94.0/99.2
93.1/98.8
93.0/99.9
95.9/100.0

80.8/96.8
68.6/90.1
80.9/96.5
68.0/90.6
67.0/88.9
73.1/99.0
70.1/99.1

91.9/94.7
89.9/93.6
93.1/95.5
92.8/96.1
91.5/94.9
98.9/100.0
99.3/100.0

80.2/90.0
70.8/84.4
77.0/91.6
70.1/84.9
69.2/84.8
71.0/97.5
70.9/95.9

Dense-BC
CIFAR-10

Dense-BC
CIFAR-100

WRN-28-10
CIFAR-10

WRN-28-10
CIFAR-100

Table 2: Distinguishing in- and out-of-distribution test set data for image classiﬁcation. All values are
percentages. ↑ indicates larger value is better, and ↓ indicates lower value is better. All parameter settings are
shown in Appendix A.2. Additional results on WRN-40-4 and MNIST dataset are reported in Appendix A.1.

whereas the blue curve corresponds to our method with temperature T = 1000 and perturbation
magnitude ε = 0.0012. We observe a strikingly large gap between the blue and red ROC curves. For
example, when TPR= 95%, the FPR can be reduced from 34% to 4.2% by using our approach.
Choosing parameters. For each out-of-distribution dataset,
we randomly hold out 1,000 images for tuning the parameters
T and ε. For temperature T , we select among 1, 2, 5, 10,
20, 50, 100, 200, 500, 1000; and for perturbation magnitude
ε we choose from 21 evenly spaced numbers starting from 0
and ending at 0.004. The optimal parameters are chosen to
minimize the FPR at TPR 95% on the holdout set. We evaluate
the our approach on the remaining test images. All parameter
settings are reported in the Appendix A. We provide additional
details on the effect of parameters in Section 5.

Main results. The main results are summarized in Table 2. For
each in- and out-of-distribution dataset pair, we report both the
performance of the baseline (Hendrycks & Gimpel, 2017) and
our approach using temperature scaling and input preprocessing.
In Table 2, we observe improved performance across all neural
architectures and all dataset pairs. Noticeably, our method
consistently outperforms the baseline by a large margin when
measured by FPR at 95% TPR and detection error.

5

Figure 1:
(a) ROC curves of base-
line (red) and our method (blue)
on DenseNet-BC-100 network, where
CIFAR-10 and TinyImageNet (crop)
are in- and out-of-distribution dataset,
respectively.

Published as a conference paper at ICLR 2018

Figure 2: (a)-(d) Performance of our method vs. MMD between in- and out-of-distribution datasets. Neural
networks are trained on CIFAR-100 and CIFAR-80, respectively. The out-of-distribution datasets are 1: LSUN
(cop), 2: TinyImageNet (crop), 3: LSUN (resize), 4:
is iSUN (resize), 5: TinyImageNet (resize) and 6:
CIFAR-20.

4.5 EXTENSIONS

In this subsection, we analyze how the statistical distance be-
tween in- and out-of-distribution natural image dataset affects the detection performance of the
proposed method.
Data distribution distance vs. Detection performance. To measure the statistical distance between
in- and out-of-distribution datasets, we adopt a commonly used metric, maximum mean discrepancy
(MMD) with Gaussian RBF kernel (Sriperumbudur et al., 2010; Gretton et al., 2012; Sutherland et al.,
2016). Speciﬁcally, given two image sets, V =
, the maximum
v1, ..., vm
}
{
mean discrepancy between V and Q is deﬁned as

w1, ..., wm
{

and W =

}

2

(cid:92)MMD

(V, W ) =

1
(cid:0)m
2

(cid:1)

(cid:88)

i(cid:54)=j

k(vi, vj) +

1
(cid:1)
(cid:0)m
2

(cid:88)

i(cid:54)=j

k(wi, wj)

−

k(vi, wj),

(cid:88)

i(cid:54)=j

(cid:1)

2
(cid:0)m
2
(cid:17)

∪

,
·

) is the Gaussian RBF kernel, i.e., k(x, x(cid:48)) = exp
·

. We use the same method
where k(
used by Sutherland et al. (2016) to choose σ, where 2σ2 is set to the median of all Euclidean distances
between all images in the aggregate set V

W .

−

(cid:16)

(cid:107)x−x(cid:48)(cid:107)2
2
2σ2

In Figure 2 (a)(b), we show how the performance of ODIN varies against the MMD distances between
in- and out-of-distribution datasets4. The datasets (on x-axis) are ranked in the descending order of
MMD distances with CIFAR-100. There are two interesting observations can be drawn from these
ﬁgures. First, we ﬁnd that the MMD distances between the cropped datasets and CIFAR-100 tend
to be larger. This is likely due to the fact that cropped images only contain local image context and
are therefore more distinct from CIFAR-100 images, while resized images contain global patterns
and are thus similar to images in CIFAR-100. Second, we observe that the MMD distance tends to
be negatively correlated with the detection performance. This suggests, not surprisingly, that the
detection task becomes harder as in and out-of-distribution images are more similar to each other.
Same-manifold datasets. Furthermore, we investigate the extreme scenario when in- and out-of-
distribution datasets are on the same manifold. In experiment, we randomly split CIFAR-100 into
two disjoint datasets containing 80 and 20 classes each. We name them CIFAR-80 and CIFAR-
20, respectively. We train both DenseNet and Wide ResNet-28-10 on the CIFAR-80 dataset (in-
distribution) and evaluate the detection performance on the CIFAR-20 dataset (out-distribution). All
hyperparameters used here are exactly the same as in Section 4.1. The MMD distance between
CIFAR-20 and CIFAR-80 is much smaller than other dataset pairs. In Figure 2 (c)(d), we observe that
both FPR at TPR 95% and detection error become larger on the CIFAR-20 dataset. This coincides
with our expectation that the detection task becomes extremely hard when in- and out-of-distribution
dataset locate on the same manifold. We provide additional experimental results in Appendix A.1
and Appendix G.

5 DISCUSSIONS

5.1 EFFECTS OF PARAMETERS

In this subsection, we empirically show how temperature T and perturbation magnitude ε affect
FPR at TPR 95% and AUROC on DenseNet and Wide ResNet-28-10. Additional results on other

4All distances are provided in Appendix G.

6

Published as a conference paper at ICLR 2018

Figure 3: (a)(b) Effects of temperature T when ε = 0. (c)(d) Effects of perturbation magnitude ε when T = 1.
All networks are trained on CIFAR-10 (in-distribution). Additional results on other metrics and Wide ResNet-40
are provided in Appendix B.

Figure 4: (a)(b) Effects of perturbation magnitude ε on DenseNet when T is large (e.g., T = 1000). (c)(d)
Effects of perturbation magnitude of ε on Wide-ResNet-28-10 when T is large (e.g., T = 1000). All networks
are trained on CIFAR-10. Additional results on other metrics and Wide ResNet-40 are provided in Appendix B.

metrics and architectures are provided in Appendix B. We show the detection performance when
using only the temperature scaling method (see Figure 3(a)(b), ε = 0), or the input preprocessing
method (see Figure 3(c)(d), T = 1). In Figure 4, we show the detection performance w.r.t ε when T
is optimal (e.g., T =1000). First, from Figure 3 (a)(b), we observe that increasing the temperature
can improve the detection performance, although the effects diminish when T is sufﬁciently large
(e.g., T > 100). Next, from Figure 3(c)(d) and Figure 4, we observe that we can further improve
the detection performance by appropriately choosing the perturbation magnitudes. We can achieve
overall better performance by combining both (1) temperature scaling and (2) input preprocessing.

5.2 ANALYSIS ON TEMPERATURE SCALING

In this subsection, we analyze the effectiveness of the temperature scaling method. As shown
in Figure 3 (a) and (b), we observe that a sufﬁciently large temperature yields better detection
performance although the effects diminish when T is too large. To gain insight, we can use the Taylor
expansion of the softmax score (details provided in Appendix D). When T is sufﬁciently large, we
have

Sˆy(x; T )

≈

N

(cid:80)

1
T

i[fˆy(x)

1
fi(x)] + 1
2T 2

(cid:80)

i[fˆy(x)

,

fi(x)]2

−
by omitting the third and higher orders. For simplicity of notation, we deﬁne

−

−

(3)

U1(x) =

[fˆy(x)

fi(x)]

and U2(x) =

1

N

1

−

(cid:88)

i(cid:54)=ˆy

−

1

N

1

−

(cid:88)

i(cid:54)=ˆy

[fˆy(x)

fi(x)]2.

(4)

−

Interpretations of U1 and U2. By deﬁnition, U1 measures the extent to which the largest unnormal-
ized output of the neural network deviates from the remaining outputs; while U2 measures the extent
to which the remaining smaller outputs deviate from each other. We provide formal mathematical
derivations in Appendix F. In Figure 5(a), we show the distribution of U1 for each out-of-distribution
dataset vs. the in-distribution dataset (in red). We observe that the largest outputs of the neural
network on in-distribution images deviate more from the remaining outputs. This is likely due to the
fact that neural networks tend to make more conﬁdent predictions on in-distribution images.
U1], for each
Further, we show in Figure 5(b) the expectation of U2 conditioned on U1, i.e., E[U2|
dataset. The red curve (in-distribution images) has overall higher expectation. This indicates that,

7

Published as a conference paper at ICLR 2018

Figure 5: (a) Probability density of U1 under different datasets on DenseNet. (b) Expectations of U2 conditioned
on U1 on DenseNet. (c) Probability density of the norm of gradient on DenseNet under temperature 1, 000.
(c)(d) Expectation of the norm of gradient conditioned on the softmax scores on DenseNet under temperature
T = 1000 and T = 1, respectively. (f)(g) Outputs of DenseNet on each class for an image of dog from
CIFAR-10 and an image from TinyImageNet (crop). The DenseNet is trained on CIFAR-10. Additional results
on other architectures are provided in Appendix C.

when two images have similar values on U1, the in-distribution image tends to have a much higher
value of U2 than the out-of-distribution image.
In other words, for in-distribution images, the
remaining outputs (excluding the largest output) tend to be more separated from each other compared
to out-of-distribution datasets. This may happen when some classes in the in-distribution dataset
share common features while others differ signiﬁcantly. To illustrate this, in Figure 5 (f)(g), we show
the outputs of each class using a DenseNet (trained on CIFAR-10) on a dog image from CIFAR-10,
and another image from TinyImageNet (crop). For the image of dog, we can observe that the largest
output for the label dog is close to the output for the label cat but is quite separated from the outputs
for the label car and truck. This is likely due to the fact that, in CIFAR-10, images of dogs are very
similar to the images of cats but are quite distinct from images of car and truck. For the image from
TinyImageNet (crop), despite having one large output, the remaining outputs are close to each other
and thus have a smaller deviation.
The effects of T . To see the usefulness of adopting a large T , we can ﬁrst rewrite the softmax score
function in Equation (3) as S
U2/2T )/T . Hence the softmax score is largely determined
(U1 −
by U1 and U2/2T . As noted earlier, U1 makes in-distribution images produce larger softmax scores
than out-of-distribution images since S
U2.
Therefore, by choosing a sufﬁciently large temperature, we can compensate the negative impacts of
U2/2T on the detection performance, making the softmax scores between in- and out-of-distribution
images more separable. Eventually, when T is sufﬁciently large, the distribution of softmax score is
almost dominated by the distribution of U1 and thus increasing the temperature further is no longer
effective. This explains why we see in Figure 3 (a)(b) that the performance does not change when T
is too large (e.g., T > 100). In Appendix E, we provide a formal proof showing that the detection
error eventually converges to a constant number when T goes to inﬁnity.

U1, while U2 has the exact opposite effect since S

∝ −

∝

∝

5.3 ANALYSIS ON INPUT PREPROCESSING
As noted previously, using the temperature scaling method by itself can be effective in improving the
detection performance. However, the effectiveness quickly diminishes as T becomes very large. In
order to make further improvement, we complement temperature scaling with input preprocessing.
This has already been seen in Figure 4, where the detection performance is improved by a large
margin on most datasets when T = 1000, provided with an appropriate perturbation magnitude ε is
chosen. In this subsection, we provide some intuition behind this.

To explain, we can look into the ﬁrst order Taylor expansion of the log-softmax function for the
perturbed image ˜x, which is given by

log Sˆy( ˜x; T ) = log Sˆy(x; T ) + ε

x log Sˆy(x; T )

(cid:107)∇

(cid:107)1 + o(ε),

where x is the original input.
The effects of gradient.
(cid:107)1 — the
In Figure 5 (c), we present the distribution of
1-norm of gradient of log-softmax with respect to the input x — for all datasets. A salient observation

x log S(x; T )

(cid:107)∇

8

Published as a conference paper at ICLR 2018

≈

is that CIFAR-10 images (in-distribution) tend to have larger values on the norm of gradient than
most out-of-distribution images. To further see the effects of the norm of gradient on the softmax
score, we provide in Figures 5 (d) the conditional expectation E[
S]. We can
observe that, when an in-distribution image and an out-of-distribution image have the same softmax
score, the value of

x log S(x; T )

x log S(x; T )

(cid:107)1 for in-distribution image tends to be larger.

(cid:107)1|

(cid:107)∇

(cid:107)∇

We illustrate the effects of the norm of gradient in Figure 6. Suppose
that an in-distribution image x1 (blue) and an out-of-distribution
image x2 (red) have similar softmax scores, i.e., S(x1)
S(x2).
After input processing, the in-distribution image can have a much
larger softmax score than the out-of-distribution image x2 since x1
results in a much larger value on the norm of softmax gradient than
that of x2. Therefore, in- and out-of-distribution images are more
separable from each other after input preprocessing5.
The effect of ε. When the magnitude ε is sufﬁciently small, adding
perturbations does not change the predictions of the neural network,
i.e., ˆy( ˜x) = ˆy(x). However, when ε is not negligible, the gap of
softmax scores between in- and out-of-distribution images can be
(cid:107)1. Our observation is consistent with
affected by
that in (Szegedy et al., 2014; Goodfellow et al., 2015; Moosavi-
Dezfooli et al., 2017), which show that the softmax scores tend to change signiﬁcantly if small
perturbations are added to the in-distribution images. It is also worth noting that using a very large ε
can lead to performance degradation, as seen in Figure 4. This is likely due to the fact that the second
and higher order terms in the Taylor expansion are no longer insigniﬁcant when the perturbation
magnitude is too large.

Figure 6: Illustration of effects
of the input preprocessing.

x log S(x; T )

(cid:107)∇

6 RELATED WORKS AND FUTURE DIRECTIONS

The problem of detecting out-of-distribution examples in low-dimensional space has been well-studied
in various contexts (see the survey by Pimentel et al. (2014)). Conventional methods such as density
estimation, nearest neighbor and clustering analysis are widely used in detecting low-dimensional out-
of-distribution examples (Chow, 1970; Vincent & Bengio, 2003; Ghoting et al., 2008; Devroye et al.,
2013), . The density estimation approach uses probabilistic models to estimate the in-distribution
density and declares a test example to be out-of-distribution if it locates in the low-density areas.
The clustering method is based on the statistical distance, and declares an example to be out-of-
distribution if it locates far from its neighborhood. Despite various applications in low-dimensional
spaces, unfortunately, these methods are known to be unreliable in high-dimensional space such as
image space (Wasserman, 2006; Theis et al., 2015). In recent years, out-of-distribution detectors based
on deep models have been proposed. Schlegl et al. (2017) train a generative adversarial networks to
detect out-of-distribution examples in clinical scenario. Sabokrou et al. (2016) train a convolutional
network to detect anomaly in scenes. Andrews et al. (2016) adopt transfer representation-learning
for anomaly detection. All these works require enlarging or modifying the neural networks. In
a more recent work, Hendrycks & Gimpel (2017) found that pre-trained neural networks can be
overconﬁdent to out-of-distribution example, limiting the effectiveness of detection. Our paper aims
to improve the performance of detecting out-of-distribution examples, without requiring any change
to an existing well-trained model.

Our approach leverages the following two interesting observations to help better distinguish between
in- and out-of-distribution examples: (1) On in-distribution images, modern neural networks tend to
produce outputs with larger variance across class labels, and (2) neural networks have larger norm
of gradient of log-softmax scores when applied on in-distribution images. We believe that having a
better understanding of these phenomenon can lead to further insights into this problem.

7 CONCLUSIONS

In this paper, we propose a simple and effective method to detect out-of-distribution data samples
in neural networks. Our method does not require retraining the neural network and signiﬁcantly

5Similar observation can be seen when T = 1, where we present the conditional expectation of the norm of

softmax gradient in Figure 5 (e).

9

Published as a conference paper at ICLR 2018

improves on the baseline (state-of-the-art) on different neural architectures across various in and
out-distribution dataset pairs. We empirically analyze the method under different parameter settings,
and provide some insights behind the approach. Future work involves exploring our method in other
applications such as speech recognition and natural language processing.

The research reported here was supported by NSF Grant CPS ECCS 1739189.

ACKNOWLEDGMENTS

REFERENCES

Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané.

Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.

Jerone T.A Andrews, Thomas Tanay, Edward J. Morton, and Lewis D. Grifﬁn. Transfer representation-

learning for anomaly detection. In ICML, 2016.

Yaroslav Bulatov. notmnist dataset. 2011.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for
statistical machine translation. EMNLP, 2014.

C Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on information theory,

Jesse Davis and Mark Goadrich. The relationship between precision-recall and roc curves. In ICML.

16(1):41–46, 1970.

ACM, 2006.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale

hierarchical image database. In CVPR, 2009.

Luc Devroye, László Györﬁ, and Gábor Lugosi. A probabilistic theory of pattern recognition,

volume 31. Springer Science & Business Media, 2013.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.

Clement Farabet, Camille Couprie, Laurent Najman, and Yann LeCun. Learning hierarchical features
for scene labeling. IEEE transactions on pattern analysis and machine intelligence, 35(8):1915–
1929, 2013.

Tom Fawcett. An introduction to roc analysis. Pattern recognition letters, 2006.

Amol Ghoting, Srinivasan Parthasarathy, and Matthew Eric Otey. Fast mining of distance-based
outliers in high-dimensional datasets. Data Mining and Knowledge Discovery, 16(3):349–364,
2008.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. ICLR, 2015.

Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola. A

kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723–773, 2012.

Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural

networks. arXiv preprint arXiv:1706.04599, 2017.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image

recognition. In CVPR, 2016.

Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution

examples in neural networks. ICLR, 2017.

10

Published as a conference paper at ICLR 2018

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv

preprint arXiv:1503.02531, 2015.

preprint arXiv:1608.06993, 2016.

Gao Huang, Zhuang Liu, and Kilian Q Weinberger. Densely connected convolutional networks. arXiv

Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolutional neural networks for human action
recognition. IEEE transactions on pattern analysis and machine intelligence, 35(1):221–231,
2013.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097–1105,
2012.

Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning

through probabilistic program induction. Science, 2015.

Christopher D Manning, Hinrich Schütze, et al. Foundations of statistical natural language processing,

volume 999. MIT Press, 1999.

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal

adversarial perturbations. CVPR, 2017.

Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High conﬁdence

predictions for unrecognizable images. 2015.

Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, and Geoffrey Hinton. Regularizing

neural networks by penalizing conﬁdent output distributions. ICLR, 2017.

Marco AF Pimentel, David A Clifton, Lei Clifton, and Lionel Tarassenko. A review of novelty

detection. Signal Processing, 99:215–249, 2014.

Mohammad Sabokrou, Mohsen Fayyaz, Mahmood Fathy, et al. Fully convolutional neural network

for fast anomaly detection in crowded scenes. arXiv preprint arXiv:1609.00866, 2016.

Takaya Saito and Marc Rehmsmeier. The precision-recall plot is more informative than the roc plot
when evaluating binary classiﬁers on imbalanced datasets. PloS one, 10(3):e0118432, 2015.

Thomas Schlegl, Philipp Seeböck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs.
Unsupervised anomaly detection with generative adversarial networks to guide marker discovery.
In International Conference on Information Processing in Medical Imaging, pp. 146–157. Springer,
2017.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image

recognition. ICLR, 2015.

Bharath K Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Schölkopf, and Gert RG
Lanckriet. Hilbert space embeddings and metrics on probability measures. Journal of Machine
Learning Research, 11(Apr):1517–1561, 2010.

Dougal J Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alex
Smola, and Arthur Gretton. Generative models and model criticism via optimized maximum mean
discrepancy. ICLR, 2016.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,

and Rob Fergus. Intriguing properties of neural networks. NIPS, 2014.

Lucas Theis, Aäron van den Oord, and Matthias Bethge. A note on the evaluation of generative

models. ICLR, 2015.

11

Published as a conference paper at ICLR 2018

Pascal Vincent and Yoshua Bengio. Manifold parzen windows. In Advances in neural information

processing systems, pp. 849–856, 2003.

Larry Wasserman. All of Nonparametric Statistics. Springer, 2006.

Pingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R Kulkarni, and Jianxiong
Xiao. Turkergaze: Crowdsourcing saliency with webcam based eye tracking. arXiv preprint
arXiv:1504.06755, 2015.

Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-
scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365,
2015.

Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,

2016.

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding

deep learning requires rethinking generalization. ICLR, 2017.

12

Published as a conference paper at ICLR 2018

A SUPPLEMENTARY RESULTS IN SECTION 4.4
A.1 EXPERIMENTAL RESULTS

Out-of-distribution
dataset

FPR
(95% TPR)
↓

Detection
Error
↓

AUROC

↑

AUPR
In
↑

AUPR
Out
↑

Baseline (Hendrycks & Gimpel, 2017) / Ours

WRN-40-4
CIFAR-10

WRN-40-4
CIFAR-100

Dense-BC
CIFAR-80

WRN-28-10
CIFAR-80

WRN-40-4
CIFAR-80

MNIST

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

CIFAR-20
TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

CIFAR-20
TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

CIFAR-20
TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian

Omniglot
notMNIST
CIFAR-10bw
Gaussian
Uniform

49.8/36.7
62.3/49.1
34.6/23.0
54.5/35.1
58.6/41.0
26.6/3.2
21.8/0.9

66.9/43.3
78.1/55.1
74.9/35.9
77.9/50.0
79.5/52.9
84.7/3.3
77.2/3.1

84.1/81.1
72.9/22.7
84.4/46.3
67.1/20.9
84.9/45.9
86.1/50.2
100.0/0.9
98.5/1.2

80.4/78.3
71.3/46.7
81.0/48.8
74.4/45.5
81.9/49.0
82.7/51.1
99.6/1.4
100.0/0.4

82.4/78.4
68.3/34.3
80.6/53.5
72.2/33.2
79.1/51.2
81.2/53.2
99.7/48.6
99.7/10.7

0.2/0.0
10.3/8.7
0.1/0.0
0.0/0.0
0.0/0.0

27.4/20.9
33.6/27.1
19.8/14.0
29.8/20.1
31.8/23.0
15.8/4.1
13.4/3.0

36.0/24.1
41.5/30.1
40.0/20.4
41.5/27.5
42.2/28.9
44.9/4.2
41.1/4.0

44.9/43.0
39.0/13.8
44.7/25.6
36.0/12.9
45.0/25.4
50.5/27.6
52.5/3.0
51.8/3.1

42.7/41.6
38.1/25.9
43.0/26.9
39.7/25.2
43.5/27.0
43.9/28.1
52.3/3.2
52.5/2.7

43.7/41.7
36.6/19.6
42.8/29.2
38.6/19.1
42.0/28.1
43.1/29.1
52.4/26.8
52.4/7.8

2.6/2.5
7.7/6.8
2.5/2.5
2.5/2.5
2.5/2.5

87.3/89.3
79.3/81.6
93.4/95.1
84.7/87.0
82.1/84.9
96.1/99.2
96.5/99.7

81.3/88.5
72.6/81.6
79.1/90.8
75.2/85.6
74.3/84.3
86.3/98.8
86.4/99.0

76.6/77.8
83.4/96.2
76.8/91.7
84.6/96.2
77.5/91.8
76.1/90.5
64.3/98.6
80.4/99.6

79.2/80.4
83.1/91.9
77.1/89.2
82.0/92.9
78.8/90.1
78.3/89.4
80.6/98.9
79.7/99.1

76.8/78.1
83.6/93.4
76.2/87.7
83.1/93.4
77.6/88.8
76.2/87.7
65.6/93.8
74.3/97.7

85.1/86.7
73.5/76.9
93.1/94.3
79.8/82.6
76.4/80.2
97.0/99.2
97.5/99.7

80.6/87.2
69.4/78.0
81.4/89.9
73.1/83.5
72.9/81.9
90.5/99.1
90.2/99.2

79.4/80.6
86.3/96.6
80.3/92.7
86.9/96.4
81.4/92.9
79.8/91.3
78.4/99.1
86.7/99.6

81.5/82.2
85.9/92.6
80.0/89.5
84.4/93.0
82.2/90.8
81.5/90.0
87.7/99.2
87.4/99.4

78.9/79.1
85.9/94.0
78.5/88.3
86.3/93.7
80.0/89.4
78.7/88.3
77.3/95.7
83.0/98.4

87.2/90.7
80.6/84.8
92.4/95.2
85.3/89.7
83.2/87.8
94.8/99.2
94.7/99.7

80.1/89.1
71.6/83.4
76.3/91.5
73.3/86.4
71.9/85.1
77.0/97.9
78.6/98.6

71.6/73.6
79.9/95.8
71.5/90.4
82.1/96.0
71.6/90.2
69.9/88.8
52.2/96.6
68.0/99.1

74.2/76.2
79.7/90.7
72.6/88.5
78.2/91.5
73.4/88.8
72.6/88.0
66.8/97.6
65.5/98.0

72.2/75.0
81.2/92.5
72.5/86.1
79.7/93.1
73.9/87.3
72.2/86.1
53.7/88.7
61.2/95.2

99.6/100.0
97.2/98.2
99.7/100.0
99.7/100.0
99.9/100.0

99.7/100.0
97.5/98.4
99.8/100.0
99.8/100.0
99.9/100.0

99.5/100.0
97.4/98.0
99.7/100.0
99.7/100.0
99.9/100.0

Table 3: Distinguishing in- and out-of-distribution test set data for image classiﬁcation. All values are
percentages. ↑ indicates larger value is better, and ↓ indicates lower value is better.

MNIST: We used the same MNIST classiﬁer used by Hendrycks & Gimpel (2017), which is a
three-layer, 256 neuron-wide, fully connected network trained for 30 epochs with Adam (Kingma
& Ba, 2014). The classiﬁer achieve 99.34% test accuracy on the MNIST test set. We compare our
method with the baseline (Hendrycks & Gimpel, 2017) on ﬁve different out-of-distribution datasets:
(1) Omniglot dataset (Lake et al., 2015) contains images of handwritten characters in stead of the
handwritten digits in MNIST; (2) notMNIST (Bulatov, 2011) dataset contains typeface characters;

13

Published as a conference paper at ICLR 2018

(3) CIFAR-10bw contains black and white rescaled CIFAR-10 images; (4)(5) Gaussian and Uniform
image set contains the synthetic Gaussian and Uniform noise images used in Section 4.2.
Wide ResNet-40-4: We use the same architecture used by Hendrycks & Gimpel (2017) to evaluate
the baseline and our method. The Wide ResNet-40-4 achieves 95.7% test accuracy on CIFAR-10
dataset and achieve 79.27% test accuracy on CIFAR-100.
CIFAR-80: DenseNet-BC-100 achieves 78.94% test accuracy on CIFAR-80, while Wide ResNet-
28-10 achieves 81.71% test accuracy and Wide ResNet-40-4 achieves 79.53% test accuracy on
CIFAR-80.

A.2 PARAMETER SETTINGS

For MNIST, we set T = 1000 and ε = 0. The parameter settings for other structures are shown as
follows.

DenseNet-BC-100

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

0.0014
0.0014
0
0.0014
0.0014
0.0014
0.0014
-

0.002
0.0022
0.0036
0.002
0.002
0.0028
0.0026
0.0002

0.002
0.0022
0.0038
0.0018
0.002
0.0024
0.0028
-

Table 4: Optimal perturbation magnitude ε for reproducing main results in Table 2 and 3.

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

Table 5: Optimal Temperature T for reproducing main results in Table 2 and 3.

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

DenseNet-BC-100

1000
1000
1000
1000
1000
1000
1000
-

1000
1000
1000
1000
1000
1
1
1

1000
1000
1000
1000
1000
1
1
-

Wide-ResNet-28-10

0.0005
0.0011
0
0.0006
0.0008
0.0014
0.0014
-

0.0002
0.0004
0.0002
0.0002
0.0002
0.0002
0.0002
5e-05

0.0026
0.0024
0.0038
0.0026
0.0026
0.0032
0.0032
-

14

Table 6: Optimal perturbation magnitude ε for reproducing main results in Table 2 and 3.

Published as a conference paper at ICLR 2018

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

Wide-ResNet-28-10

1000
1000
1000
1000
1000
1000
1000
-

1000
1000
1000
1000
1000
1
1
1

1000
1000
1000
1000
1000
1000
1000
-

Table 7: Optimal Temperature T for reproducing main results in Table 2 and 3.

Wide-ResNet-40-4

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

0.0004
0.0008
0
0.001
0.0008
0.0016
0.0016
-

0.0002
0.0004
0.0002
0.0002
0.0002
0.0002
0.0002
0.0002

0.0014
0.0016
0.0038
0.0014
0.0016
0.0024
0.0026
-

Table 8: Optimal perturbation magnitude ε for reproducing main results in Table 2 and 3.

Out-of-distribution datasets CIFAR-10 CIFAR-80 CIFAR-100

Wide-ResNet-40-4

TinyImageNet (crop)
TinyImageNet (resize)
LSUN (crop)
LSUN (resize)
iSUN
Uniform
Gaussian
CIFAR-20

1000
1000
1000
1000
1000
1
1
1

1000
1000
1000
1000
1000
1000
1000
-

Table 9: Optimal Temperature T for reproducing main results in Table 2 and 3.

1000
1000
1000
1000
1000
1000
1000
-

15

Published as a conference paper at ICLR 2018

B SUPPLEMENTARY RESULTS IN SECTION 5.1

Figure 7: Detection performance on DenseNet, Wide ResNet-28-10 and Wide ResNet-40-4 under different
temperature, when input preprocessing is not used, i.e., ε = 0. All networks are trained on CIFAR-10.

16

Published as a conference paper at ICLR 2018

Figure 8: Detection performance on DenseNet, Wide ResNet-28-10 and Wide ResNet-40-4 under different
perturbation magnitude, when temperature scaling is not used, i.e., T = 1. All networks are trained on
CIFAR-10.

17

Published as a conference paper at ICLR 2018

Figure 9: Detection performance on DenseNet, Wide ResNet-28-10 and Wide ResNet-40-4 under different
perturbation magnitude, when the optimal temperature is used, i.e., T = 1000. All networks are trained on
CIFAR-10.

18

Published as a conference paper at ICLR 2018

C SUPPLEMENTARY RESULTS IN SECTION 5.2 AND 5.3

Figure 10: Expectation of the second order term U2 conditioned on the ﬁrst order term U1 under DenseNet,
Wide-ResNet-28-10 and Wide ResNet-40-4. All networks are trained on CIFAR-10.

Figure 11: Expectation of gradient norms conditioned on the softmax scores under DenseNet, Wide-ResNet-28-
10 and Wide ResNet-40-4, where the temperature scaling is not used. All networks are trained on CIFAR-10.

Figure 12: Expectation of gradient norms conditioned on the softmax scores under DenseNet, Wide-ResNet-28-
10 and Wide ResNet-40-4, where the optimal temperature is used, i.e., T = 1000. All networks are trained on
CIFAR-10.

19

In this section, we present the Taylor expansion of the soft-max score function:

Published as a conference paper at ICLR 2018

D TAYLOR EXPANSION

Sˆy(x; T ) =

(cid:80)N

exp (fˆy(x)/T )
i=1 exp(fi(x)/T )
1
(cid:16) fi(x)−f ˆy(x)
T

i=1 exp

(cid:17)

(cid:80)N

=

=

1
+ 1
2!

1
fi(x)] + 1
2T 2

(cid:104)

(cid:80)N

i=1

1 + fi(x)−f ˆy(x)

T

(fi(x)−f ˆy(x))2
T 2

+ o (cid:0) 1
T 2

(cid:1)(cid:105)

by Taylor expansion

≈

N

−

1
T

(cid:80)N

i=1[fˆy(x)

−

(cid:80)N

i=1[fi(x)

fˆy(x)]2

−

E PROPOSITION 1

The following proposition 1 shows that the detection error Pe(T, 0)
Thus, increasing the temperature further can only slightly improve the detection performance.
Proposition 1. There exists a constant c only depending on function U1, in-distribution PX and
out-of-distribution QX such that limT →∞ Pe(T, ε) = c, when ε = 0 (i.e., no input preprocessing).
Proof. Since

c if T is sufﬁciently large.

≈

Therefore, for any X,
(cid:18)

Sˆy(X; T ) =

exp(fˆy(X)/T )
i=1 exp(fi(X)/T )

(cid:80)N

=

1 + (cid:80)

1

i(cid:54)=ˆy exp([fi(X)

fˆy(X)]/T )

−

lim
T →∞

T

1
Sˆy(X; T )

−

(cid:19)

+ N

= lim
T →∞

(cid:20)
1

T

(cid:88)

i(cid:54)=ˆy

exp

−

(cid:18) fi(X)

fˆy(X)

(cid:19)(cid:21)

−
T

=

[fˆy(X)

fi(X)] = (N

1)U1(X)

−

−

(cid:88)

i(cid:54)=ˆy

This indicates that the random variable

(cid:18)

T

1
Sˆy(X; T )

−

(cid:19)

+ N

(N

1)U1(X) a.s.

→

−

as T
the false positive rate

→ ∞

. This means that for a speciﬁc α > 0, choosing the threshold δT = 1/(N

α/T ), then

FPR(T ) = QX (Sˆy(X; T ) > 1/(N

α/T )) = QX

T

N

−

−

−

(cid:18)

(cid:18)

1
Sˆy(X; T )

−

(cid:19)

(cid:19)

> α

QX ((N

1)U1(X) > α) ,

(cid:18)

(cid:18)

1
Sˆy(X; T )

−

(cid:19)

(cid:19)

> α

PX ((N

1)U1(X) > α) .

−

−

T →∞
−−−−→

T →∞
−−−−→

and the true positive rate

TPR(T ) = PX (Sˆy(X; T ) > 1/(N

α/T )) = PX

T

N

Choosing α∗ such that PX ((N
at the same time FPR(T )
depending on U1, PX , QX and PZ, such that

QX ((N

→

−

−

1)U1(X) > α∗) = 0.95, then TPR(T )
1)U1(X) > α∗) as T

0.95 as T

and
. There exists a constant c

→ ∞

→

→ ∞

lim
T →∞

Pe(T, 0) = 0.05P (Z = 0) + P (Z = 1)QX ((N

1)U1(X) > α∗) = c.

−

20

Published as a conference paper at ICLR 2018

F ANALYSIS OF TEMPERATURE

For simplicity of the notations, let ∆i = fˆy
mean of the set ∆. Therefore,

−

fi and thus ∆ =

∆i

i(cid:54)=ˆy. Besides, let ¯∆ denote the
}

{

¯∆ =

1

N

1

−

(cid:88)

i(cid:54)=ˆy

∆i =

1

N

1

−

(cid:88)

i(cid:54)=ˆy

[fˆy

fi] = U1.

−

Equivalently,

Next, we will show

U1 = Mean(∆).

U2 =

1

N

1

−

(cid:88)

i(cid:54)=ˆy

[fˆy

−

(cid:122)

1

fi]2 =

Variance2(∆)
(cid:125)(cid:124)
(cid:88)

[∆i

Mean2(∆)
(cid:122)(cid:125)(cid:124)(cid:123)
¯∆2

.

(cid:123)
¯∆]2 +

N

1

−

i(cid:54)=ˆy

−

by∆i = fˆy

fi

−

Since

then

U2 =

∆2
i

(cid:88)

i(cid:54)=ˆy
(cid:88)

i(cid:54)=ˆy
(cid:88)

i(cid:54)=ˆy

(cid:88)

1

−
1

−
1

−
1

−

1

1

1

1

N

N

N

N

(cid:124)

=

=

=

¯∆ + ¯∆)2

(∆i

−

[(∆i

¯∆)2

−

−

2(∆i

−

¯∆) ¯∆ + ¯∆2]

¯∆]2

[∆i

−

i(cid:54)=ˆy
(cid:123)(cid:122)
Variance2(∆)

−

N

1

−

(cid:125)

(cid:124)

i(cid:54)=ˆy
(cid:123)(cid:122)
=0

2 ¯∆

(cid:88)

(∆i

−

¯∆)

+ ¯∆2

(cid:125)

(cid:124)(cid:123)(cid:122)(cid:125)
Mean2(∆)

U2 = Variance2(∆) + Mean2(∆)

G ADDITIONAL RESULTS IN SECTION 4.5

Apart from the Maximum Mean Discrepancy, we as well calculate the Energy distance between in-
and out-of-distribution datasets. Let P and Q denote two different distributions. Then the energy
distance between distributions P and Q is deﬁned as

D2

energy(P, Q) = 2EV ∼P,W ∼Q

X
(cid:107)

Y

−

(cid:107) −

EV,V (cid:48)∼P

X (cid:48)

X
(cid:107)

−

(cid:107) −

EW,W (cid:48)∼Q

Y
(cid:107)

Y (cid:48)

.

(cid:107)

−

Therefore, the energy distance between two datasets V =
W1, ..., Wm
{

Q is deﬁned as

iid
∼

}

V1, ..., Vm
{

}

iid
∼

P and W =

2

(cid:92)Denergy

(P, Q) =

2
m2

m
(cid:88)

m
(cid:88)

i=1

j=1

Vi
(cid:107)

−

Wj

(cid:107) −

1
(cid:1)
(cid:0)m
2

(cid:88)

i(cid:54)=j

Vi

(cid:107)

−

Vj

(cid:107) −

1
(cid:1)
(cid:0)m
2

(cid:88)

i(cid:54)=j

Wi
(cid:107)

Wj

.
(cid:107)

−

In the experiment, we use the 2-norm

(cid:107) · (cid:107)2.

21

Published as a conference paper at ICLR 2018

In-distribution Out-of-distribution
datasets

Datasets

MMD
Distance Distance

Energy

CIFAR-100

CIFAR-80

Tiny-ImageNet (crop)
LSUN (crop)
Tiny-ImageNet (resize)
LSUN (resize)
iSUN (resize)

Tiny-ImageNet (crop)
LSUN (crop)
Tiny-ImageNet (resize)
LSUN (resize)
iSUN (resize)
CIFAR-20

0.41
0.43
0.088
0.12
0.11

0.4
0.43
0.095
0.120
0.116
0.057

2.25
2.31
0.54
0.63
0.56

2.22
2.29
0.57
0.62
0.61
0.35

22

Published as a conference paper at ICLR 2018

Figure 13: False positive rate (FPR) and true positive rate (TPR) under different thresholds (δ) when the
temperature (T ) is set to 1, 000 and the perturbation magnitude (ε) is set to 0.0014. The DenseNet is trained on
CIFAR-10.

Figure 14: Detection performance on Tiny-ImageNet (resize), LSUN (resize) and iSUN (resize) when parame-
ters are tuned on six different out-of-distribution datasets. Each tuning set contains 1,000 images and each test
set contains 9,000 images. Both DenseNet and Wide-ResNet are trained on CIFAR-10. Additional results on
other datasets are provide in Table 10 and 11.

H ADDITIONAL DISCUSSIONS

In this section, we present additional discussion on the proposed method. We ﬁrst empirically show
how the threshold δ affects the detection performance. We next show how the proposed method
performs when the parameters are tuned on a certain out-of-distribution dataset and are evaluated on
other out-of-distribution datasets. Finally, we show how the size of dataset for choosing parameters
affects the detection performance.
Effects of the threshold. We analyze how the threshold affects the following metrics: (1) FPR, i.e.,
the fraction of out-of-distribution images misclassiﬁed as in-distribution images; (2) TPR, i.e, the
fraction of in-distribution images correctly classiﬁed as in-distribution images. In Figure 13, we
show how the thresholds affect FPR and TPR when the temperature and perturbation magnitude are
chosen optimally (i.e., T = 1, 000, ε = 0.0014). From the ﬁgure, we can observe that the threshold
corresponding to 95% TPR can produce small FPRs on all out-of-distribution datasets.
Performance across datasets. To investigate how the parameters generalize across datasets, we
tune the parameters using one out-of-distribution dataset and then evaluate on a different one. Given
an out-of-distribution dataset, we ﬁrst split the dataset into two disjoint subsets: tuning set and
test set. The tuning set contains 1,000 images and the test set contains 9,000 images. We tune the
parameters on the tuning set and evaluate the detection performance on the test set. We ﬁrst choose
the temperature T and the perturbation magnitude ε such that the FPR at TPR 95% is minimized on
the tuning set of one out-of-distribution dataset. Next, we set δ to the threshold corresponding to 95%
TPR and calculate the false positive rates on the test sets of other out-of-distribution datasets.

In Figure 14, we show the detection performance on three out-of-distribution datasets when the
parameters are tuned on six different datasets. From Figure 14, we can observe that the parameters
tuned on different tuning sets can have quite similar detection FPRs on all of three out-of-distribution
image sets. This may be due to the fact, shown in Figure 13, that the threshold corresponding to 95%
TPR can produce small FPRs on all datasets.

23

Published as a conference paper at ICLR 2018

Figure 15: FPR at TPR 95% under different tuning set sizes. The DenseNet is trained on CIFAR-10 and each
test set contains 8,000 out-of-distribution images.

Performance vs.
tuning set size. To show the effects of the tuning set size on the detection
performance, we devise the following experiment. For each out-of-distribution dataset, we choose the
tuning set size from 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000. For each set size, we
tune the temperature T and perturbation magnitude ε to minimize the FPR at TPR 95% and calculate
the FPR. In Figure 15, we show the detection performance of ODIN under different tuning set size.
From Figure 15, we can observe that the FPR at TPR 95% tends to stabilize when the set size grows
above 1,000.

24

Published as a conference paper at ICLR 2018

DenseNet-BC-100

Test set

ImgNet (c)

ImgNet (r) LSUN (c) LSUN (c)

iSUN

Gaussian Uniform

Baseline (Hendrycks & Gimpel, 2017) / Ours

ImgNet (c)
ImgNet (r)
LSUN (c)
LSUN (r)
iSUN
Gaussian
Uniform

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

34.7/6.6
40.7/14.9
39.3/8.1
33.6/10.4
37.2/12.6
23.5/0.4
12.3/4.5

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

34.7/4.3
40.7/7.5
39.3/13.8
33.6/4.8
37.2/6.3
23.5/0.0
12.3/0.0

Table 10: Detection performance across different datasets. Each row corresponds to the FPR at
TPR 95% on the same test set where parameters are tuned under different tuning sets. Each column
corresponds to the FPR at TPR 95% on different test sets where parameters are tuned under the same
tuning set. The DenseNet is trained on CIFAR-10.

Wide-ResNet- 28-10

Test set

ImgNet (c)

ImgNet (r) LSUN (c)

LSUN (c)

iSUN

Gaussian Uniform

Baseline (Hendrycks & Gimpel, 2017) / Ours

ImgNet (c)
ImgNet (r)
LSUN (c)
LSUN (r)
iSUN
Gaussian
Uniform

38.9/23.4
45.6/25.5
35.0/28.1
35.0/18.9
40.6/21.8
1.6/0.0
0.3/0.0

38.9/24.5
45.6/25.5
35.0/31.27
35.0/18.0
40.6/21.7
1.6/0.0
0.3/0.0

38.9/27.1
45.6/32.9
35.0/21.8
35.0/25.6
40.6/28.9
1.6/0.4
0.3/0.0

38.9/23.4
45.6/25.8
35.0/28.2
35.0/17.6
40.6/ 21.9
1.6/0.0
0.3/0.0

38.9/24.1
45.6/25.8
35.0/28.9
35.0/18.0
40.6/21.3
1.6/0.0
0.3/0.0

38.9/26.5
45.6/27.9
35.0/29.7
35.0/19.1
40.6/22.8
1.6/0.0
0.3/0.0

38.9/26.5
45.6/27.9
35.0/29.7
35.0/19.1
40.6/22.8
1.6/0.0
0.3/0.0

Table 11: Detection performance across different datasets. Each row corresponds to the FPR at
TPR 95% on the same test set where parameters are tuned under different tuning sets. Each column
corresponds to the FPR at TPR 95% on different test sets where parameters are tuned under the same
tuning set. The Wide-ResNet is trained on CIFAR-10.

25

Published as a conference paper at ICLR 2018

Figure 16: (a) The test accuracy on the images having softmax scores above the threshold corresponding to
a certain true positive rate. (b) The test accuracy on the images having softmax scores below the threshold
corresponding to a certain true positive rate. All networks are trained on CIFAR-10.

Figure 17: Outputs of DenseNet on thirty classes for an image of apple from CIFAR-80 and an image of red
pepper from CIFAR-20. The label “0” denotes the class “apple” and the label “49" denotes the class “orange".

I ADDITIONAL ANALYSIS

Difﬁcult-to-classify images and difﬁcult-to-detect images. We analyze the correlation between
the images that tend to be out-of-distribution and images on which the neural network tend to make
incorrect predictions. To understand the correlation, we devise the following experiment. For the
ﬁxed temperature T and perturbation magnitude ε, we ﬁrst set δ to the softmax score threshold
corresponding to a certain true positive rate. Next, we calculate the test accuracy on the images with
softmax scores above δ and the test accuracy on the images with softmax score below δ, respectively.
We report the results in Figure 16(a) and (b). From these two ﬁgures, we can observe that the images
that are difﬁcult to detect are more likely to be the images that are difﬁcult to classify. For example,
the DenseNet can achieve up to 98.5% test accuracy on the images having softmax scores above the
threshold corresponding to 80% TPR, but can only achieve around 82% test accuracy on the images
having softmax scores below the threshold corresponding to 80% TPR.
Same manifold datasets. We provide additional empirical results showing how the term E[U2|
U1]
affects the detection performance when in- and out-of-distribution datasets locate on the same
manifold. In Figure 17, we show the outputs of DenseNet on thirty classes for an image of apple
from CIFAR-80 (in-distribution) and an image of red pepper of CIFAR-20 (out-distribution). We
can observe that the outputs of DenseNet for both images are quite similar to each other. In addition,
we can observe that for both images, the second and third largest output are quite close to the
largest output. This may be due the fact the image of red pepper shares some common features
with the images in CIFAR-80. Furthermore, the similarity between the outputs for the images from

26

Published as a conference paper at ICLR 2018

Out-of-distribution
dataset

FPR
(95% TPR)
↓

Detection AUROC

Error
↓

↑

AUPR
In
↑

AUPR
Out
↑

Baseline (Hendrycks & Gimpel, 2017) / Ours

CIFAR-10
CIFAR-100

CIFAR-100
CIFAR-10

57.1/47.2
81.8/81.4

31.1/26.1
43.4/43.2

89.0/89.8
76.1/76.7

91.2/91.4
79.9/80.4

86.8/88.7
71.3/72.6

Table 12: Distinguishing in- and out-of-distribution test set data for image classiﬁcation. All values are
percentages. ↑ indicates larger value is better, and ↓ indicates lower value is better. The architecture is DenseNet.

CIFAR-80 and CIFAR-20 can help explain that the detection task becomes harder when in- and
out-of-distribution datasets locate on the same manifold.

Reciprocal results between datasets. In Table 12, we show the reciprocal results between datasets.
First, we train the DenseNet on the CIFAR-10 dataset (in-distribution) and evaluate the detection
performance on the CIFAR-100 dataset (out-distribution). Next, we train the DenseNet on the
CIFAR-100 dataset (in-distribution) and evaluate the detection performance on the CIFAR-10 dataset
(out-distribution). From Table 12, we can observe that the performance of the DenseNet trained on
CIFAR-10 is better than the performance of the DenseNet trained on CIFAR-100. This may be due to
the fact that the DenseNet has a higher test accuracy on CIFAR-10 (around 95%) compared to the
test accuracy on CIFAR-100 (around 77%).

27

