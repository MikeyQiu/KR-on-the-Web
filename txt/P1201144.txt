6
1
0
2
 
p
e
S
 
4
2
 
 
]
L
C
.
s
c
[
 
 
1
v
5
8
5
7
0
.
9
0
6
1
:
v
i
X
r
a

An Investigation of Recurrent Neural Architectures
for Drug Name Recognition

Raghavendra Chalapathy
University of Sydney
J12/1 Cleveland St
Darlington NSW 2008
rcha9612@uni.sydney.edu.au

Ehsan Zare Borzeshi
Capital Markets CRC
3/55 Harrington St
Sydney NSW 2000
ezborzeshi@cmcrc.com

Massimo Piccardi
University of Technology Sydney
PO Box 123
Broadway NSW 2007
Massimo.Piccardi@uts.edu.au

Abstract

Drug name recognition (DNR) is an essential
step in the Pharmacovigilance (PV) pipeline.
DNR aims to ﬁnd drug name mentions in un-
structured biomedical texts and classify them
State-of-the-art
into predeﬁned categories.
DNR approaches heavily rely on hand-crafted
features and domain-speciﬁc resources which
are difﬁcult to collect and tune. For this rea-
son, this paper investigates the effectiveness of
contemporary recurrent neural architectures -
the Elman and Jordan networks and the bidi-
rectional LSTM with CRF decoding - at per-
forming DNR straight from the text. The
experimental results achieved on the author-
itative SemEval-2013 Task 9.1 benchmarks
show that the bidirectional LSTM-CRF ranks
closely to highly-dedicated, hand-crafted sys-
tems.

1 Introduction

Pharmacovigilance (PV) is deﬁned by the World
Health Organization as the science and activities
concerned with the detection, assessment, under-
standing and prevention of adverse effects of drugs
or any other drug-related problems. Drug name
recognition (DNR) is a fundamental step in the PV
pipeline, similarly to the well-studied Named En-
tity Recognition (NER) task for general natural lan-
guage processing (NLP). DNR aims to ﬁnd drug
mentions in unstructured biomedical texts and clas-
sify them into predeﬁned categories in order to link
drug names with their effects and explore drug-drug
interactions (DDIs). Conventional approaches to
DNR sub-divide as rule-based, dictionary-based and

machine learning-based.
Intrinsically, rule-based
systems are hard to scale, time-consuming to as-
semble and ineffective in the presence of infor-
mal sentences and abbreviated phrases. Dictionary-
based systems identify drug names by matching text
chunks against drug dictionaries. These systems
typically achieve high precision, but suffer from low
recall (i.e., they miss a signiﬁcant number of men-
tions) due to spelling errors or drug name variants
not present in the dictionaries (Liu et al., 2015a).
Conversely, machine-learning approaches have the
potential to overcome all these limitations since their
foundations are intrinsically robust to variants. The
current state-of-the-art machine learning approaches
follow a two-step process of
feature engineer-
ing and classiﬁcation (Segura-Bedmar et al., 2015;
Abacha et al., 2015; Rockt¨aschel et al., 2013). Fea-
ture engineering refers to the task of representing
text by dedicated numeric vectors using domain
knowledge. Similarly to the design of rule-based
systems, this task requires much expert knowledge,
is typically challenging and time-consuming, and
has a major impact on the ﬁnal accuracy. For this
reason, this paper explores the performance of con-
temporary recurrent neural networks (RNNs) at pro-
viding end-to-end DNR straight from text, without
any manual feature engineering stage. The tested
RNNs include the popular Elman and Jordan net-
works and the bidirectional long short-term memory
(LSTM) with decoding provided by a conditional
random ﬁeld (CRF) (Elman, 1990; Jordan, 1986;
Lample et al., 2016; Collobert et al., 2011). The ex-
perimental results over the SemEval-2013 Task 9.1
benchmarks show an interesting accuracy from the

LSTM-CRF that exceeds that of various manually-
engineered systems and approximates the best result
in the literature.

2 Related Work

very

recent

features

specialized

approaches

(Rockt¨aschel et al., 2012),

Most of the research on drug name recognition to
date has focussed on domain-dependent aspects and
specialized text features. The beneﬁt of leveraging
such tailored features was made evident by the re-
sults from the SemEval-2013 Task 9.1 (Recognition
and classiﬁcation of pharmacological substances,
known as DNR task) challenge. The system that
ranked ﬁrst, WBI-NER (Rockt¨aschel et al., 2013),
adopted
derived
the ChemSpot
from an improved version of
tool
a collection of
drug dictionaries and ontologies. Similarly, many
other
(Abacha et al., 2015;
Liu et al., 2015b; Segura-Bedmar et al., 2015) have
been based on various combinations of general
and domain-speciﬁc features.
In the broader
ﬁeld of machine learning,
the recent years have
witnessed a rapid proliferation of deep neural
networks, with unprecedented results in tasks as
diverse as visual, speech and named-entity recog-
nition (Hinton et al., 2012; Krizhevsky et al., 2012;
Lample et al., 2016). One of the main advantages
of neural networks is that they can learn the fea-
ture representations automatically from the data,
thus avoiding the laborious feature engineering
stage
Lample et al., 2016).
Given these promising results, the main goal of this
paper is to provide the ﬁrst performance investiga-
tion of popular RNNs such as the Elman and Jordan
networks and the bidirectional LSTM-CRF over
DNR tasks.

(Mesnil et al., 2015;

3 The Proposed Approach

DNR can be formulated as a joint segmentation and
classiﬁcation task over a predeﬁned set of classes.
As an example, consider the input sentence provided
in Table 1. The notation follows the widely adopted
in/out/begin (IOB) entity representation with, in this
instance, Cimetidine as the drug, ALFENTA as the
brand, and words volatile inhalation anesthetics to-
gether as the group.
In this paper, we approach
the DNR task by recurrent neural networks and we

In
therefore provide a brief description hereafter.
an RNN, each word in the input sentence is ﬁrst
mapped to a random real-valued vector of arbitrary
dimension, d. Then, a measurement for the word,
noted as x(t), is formed by concatenating the word’s
own vector with a window of preceding and follow-
ing vectors (the ”context”). An example of input
vector with a context window of size s = 3 is:
w3(t) = [Cimetidine, reduces, ef f ect],
→ xreduces ∈ Rd,
→ xCimetidine ∈ Rd,
‘ef f ect′
→ xef f ect ∈ Rd,
x(t) = [xCimetidine, xreduces, xef f ect] ∈ R3d

‘Cimetidine′

‘reduces′

(1)

where w3(t) is the context window centered around
the t-th word, ′reduces′, and xword represents the
numerical vector for word.

For the Elman network, both x(t) and the output
from the hidden layer at time t − 1, h(t − 1), are in-
put into the hidden layer for frame t. The recurrent
connection from the past time frame enables a short-
term memory, while hidden-to-hidden neuron con-
nections make the network Turing-complete. This
architecture, common in RNNs, is suitable for pre-
diction of sequences. Formally, the hidden layer is
described as:

h(t) = f (U • x(t) + V • h(t − 1))

(2)

where U and V are randomly-initialized weight ma-
trices between the input and the hidden layer, and
between the past and current hidden layers, respec-
tively. Function f (·) is the sigmoid function:

f (x) =

1
1 + e−x

(3)

that adds non-linearity to the layer. Eventually, h(t)
is input in the output layer:

y(t) = g(W • h(t)), with g(zm) =

ezm
k=1ezk

ΣK

(4)

and convolved with the output weight matrix, W .
The output is normalized by a multi-class logistic
function, g(·), to become a proper probability over
the class set. The output dimensionality is therefore
determined by the number of entity classes (i.e., 4
for the DNR task).The Jordan network is very sim-
ilar to the Elman network, except that the feedback

Sentence
Entity class

Cimetidine
B-drug

reduces
O

clearance
O

of
O

ALFENTA and
O
B-brand

volatile
B-group

inhalation
I-group

anesthetics
I-group

Table 1: Example sentence in a DNR task with entity classes represented in IOB format.

DDI-DrugBank

DDI-MedLine

Training+Test for DDI task Test for DNR Training+Test for DDI task Test for DNR

documents
sentences
drug n
group
brand
drug

730
6577
124
3832
1770
9715

54
145
6
65
53
180
Table 2: Statistics of training and test datasets used for SemEval-2013 Task 9.1.

175
1627
520
234
36
1574

58
520
115
90
6
171

is sourced from the output layer rather than the pre-
vious hidden layer:

h(t) = f (U • x(t) + V • y(t − 1)).

(5)

their

decay

recent

biases

them toward
inputs (Bengio et al., 1994).

Although the Elman and Jordan networks
ex-
can learn long-term dependencies,
their
ponential
The
most
LSTM was designed to overcome this
limi-
tation by incorporating a gated memory-cell
to capture long-range dependencies within the
In the
data (Hochreiter and Schmidhuber, 1997).
bidirectional LSTM, for any given sentence,
the
−→
h (t), and a right,
network computes both a left,
←−
h (t), representations of the sentence context at
every input, x(t). The ﬁnal representation is created
←−
h (t)]. All
by concatenating them as h(t) = [
these networks utilize the h(t) layer as an implicit
feature for entity class prediction: although this
model has proved effective in many cases, it is not
able to provide joint decoding of the outputs in a
Viterbi-style manner (e.g., an I-group cannot follow
a B-brand; etc). Thus, another modiﬁcation to the
bidirectional LSTM is the addition of a conditional
random ﬁeld (CRF) (Lafferty et al., 2001) as the
output layer to provide optimal sequential decoding.
The resulting network is commonly referred to as
the bidirectional LSTM-CRF (Lample et al., 2016).

−→
h (t);

4 Experiments

4.1 Datasets

DDIExtraction

from

The
challenge
9.1

(Segura-Bedmar et al., 2013)

shared

2013
SemEval-2013
has

task
Task
provided

a benchmark corpus for DNR and DDI extraction.
The corpus contains manually-annotated pharma-
cological substances and drug-drug interactions
for a total of 18, 502 pharmacological
(DDIs)
substances and 5, 028 DDIs.
It collates two
DDI-DrugBank and DDI-
distinct datasets:
MedLine (Herrero-Zazo et al., 2013).
Table 2
the training
summarizes the basic statistics of
and test datasets used in our experiments.
For
proper comparison, we follow the same settings
as (Segura-Bedmar et al., 2015), using the training
data of the DNR task along with the test data for the
DDI task for training and validation of DNR. We
split this joint dataset into a training and validation
sets with approximately 70% of sentences for
training and the remaining for validation.

4.2 Evaluation Methodology

Our models have been blindly evaluated on un-
seen DNR test data using the strict evaluation
the predicted en-
metrics. With this evaluation,
tities have to match the ground-truth entities ex-
actly, both in boundary and class.
To facili-
tate the replication of our experimental results,
we have used a publicly-available library for the
implementation1 (i.e., the Theano neural network
toolkit (Bergstra et al., 2010)).
The experiments
have been run over a range of values for the
hyper-parameters, using the validation set for se-
lection (Bergstra and Bengio, 2012). The hyper-
parameters include the number of hidden-layer
nodes, H ∈ {25, 50, 100}, the context window size,
s ∈ {1, 3, 5}, and the embedding dimension, d ∈

1https://github.com/raghavchalapathy/dnr

DDI-DrugBank

DDI-MedLine

Precision Recall F1 Score Precision Recall F1 Score

Methods
WBI-NER (Rockt¨aschel et al., 2013)
Hybrid-DDI (Abacha et al., 2015)
Word2Vec+DINTO (Segura-Bedmar et al., 2015)
Elman RNN
Jordan RNN
Bidirectional LSTM-CRF

88.00
93.00
69.00
79.91
77.59
87.07
Table 3: Performance comparison between the recurrent neural networks (bottom three lines) and state-of-the-art systems (top

58.10
37.00
57.00
37.78
40.06
52.75

87.80
80.00
75.00
69.13
68.25
85.19

87.00
70.00
82.00
60.91
60.91
83.39

61.00
74.00
65.00
43.23
59.47
52.93

56.00
25.00
51.00
33.56
30.20
52.57

three lines) over the SemEval-2013 Task 9.1.

Entities

DDI-DrugBank

DDI-MedLine

Precision Recall F1 Score Precision Recall F1 Score

Bidirectional LSTM-CRF

53.76
61.05
0.0
45.45
Table 4: SemEval-2013 Task 9.1 results by entity for the bidirectional LSTM-CRF.

group
drug
brand
drug n

59.52
65.22
0.0
40.20

83.33
87.50
84.85
0.0

76.92
90.59
91.30
0.0

90.91
84.62
79.25
0.0

56.50
63.06
0.0
42.67

{50, 100, 300, 500, 1000}. Two additional parame-
ters, the learning and drop-out rates, were sampled
from a uniform distribution in the range [0.05, 0.1].
The embedding and initial weight matrices were all
sampled from the uniform distribution within range
[−1, 1]. Early training stopping was set to 100
epochs to mollify over-ﬁtting, and the model that
gave the best performance on the validation set was
retained. The accuracy is reported in terms of micro-
average F1 score computed using the CoNLL score
function (Nadeau and Sekine, 2007).

4.3 Results and Analysis

Table 3 shows the performance comparison between
the explored RNNs and state-of-the-art DNR sys-
the RNNs have not
tems. As an overall note,
reached the same accuracy as the top system, WBI-
NER (Rockt¨aschel et al., 2013). However, the bidi-
rectional LSTM-CRF has achieved the second-best
score on DDI-DrugBank and the third-best on DDI-
MedLine. These results seem interesting on the
ground that the RNNs provide DNR straight from
text rather than from manually-engineered features.
Given that the RNNs learn entirely from the data, the
better performance over the DDI-DrugBank dataset
is very likely due to its larger size. Accordingly, it
is reasonable to expect higher relative performance
should larger corpora become available in the fu-
ture. Table 4 also breaks down the results by en-
tity class for the bidirectional LSTM-CRF. The low

score on the brand class for DDI-MedLine and on
the drug n class (i.e., active substances not ap-
proved for human use) for DDI-DrugBank are likely
attributable to the very small sample size (Table 2).
This issue is also shared by the state-of-the-art DNR
systems.

5 Conclusion

This paper has investigated the effectiveness of re-
current neural architectures, namely the Elman and
Jordan networks and the bidirectional LSTM-CRF,
for drug name recognition. The most appealing fea-
ture of these architectures is their ability to pro-
vide end-to-end recognition straight from text, spar-
ing effort from laborious feature construction. To
the best of our knowledge, ours is the ﬁrst pa-
per to explore RNNs for entity recognition from
pharmacological
The experimental results
over the SemEval-2013 Task 9.1 benchmarks look
promising, with the bidirectional LSTM-CRF rank-
ing closely to the state of the art. A potential way
to further improve its performance would be to ini-
tialize its training with unsupervised word embed-
dings such as Word2Vec (Mikolov et al., 2013) and
GloVe (Pennington et al., 2014). This approach has
proved effective in many other domains and still dis-
penses with expert annotation effort; we plan this
exploration for the near future.

text.

References

[Abacha et al.2015] Asma Ben Abacha, Md Faisal Mah-
bub Chowdhury, Aikaterini Karanasiou, Yassine Mra-
bet, Alberto Lavelli, and Pierre Zweigenbaum. 2015.
Text mining for pharmacovigilance: Using machine
learning for drug name recognition and drug–drug
interaction extraction and classiﬁcation.
Journal of
Biomedical Informatics, 58:122–132.

[Bengio et al.1994] Yoshua Bengio, Patrice Simard, and
Paolo Frasconi. 1994. Learning long-term dependen-
cies with gradient descent is difﬁcult. IEEE Transac-
tions on Neural Networks, 5(2):157–166.

[Bergstra and Bengio2012] James Bergstra and Yoshua
Bengio. 2012. Random search for hyper-parameter
optimization. Journal of Machine Learning Research,
13:281–305.

[Bergstra et al.2010] James Bergstra, Olivier Breuleux,
Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu,
Guillaume Desjardins, Joseph Turian, David Warde-
Farley, and Yoshua Bengio. 2010. Theano: A CPU
and GPU math compiler in Python. In The 9th Python
in Science Conference, pages 1–7.

[Collobert et al.2011] Ronan Collobert, Jason Weston,
L´eon Bottou, Michael Karlen, Koray Kavukcuoglu,
and Pavel Kuksa. 2011. Natural language processing
(almost) from scratch. Journal of Machine Learning
Research, 12:2493–2537.

[Elman1990] Jeffrey L Elman. 1990. Finding structure in

time. Cognitive Science, 14(2):179–211.
[Herrero-Zazo et al.2013] Mar´ıa Herrero-Zazo,

Isabel
Segura-Bedmar, Paloma Mart´ınez, and Thierry De-
clerck. 2013. The DDI corpus: An annotated corpus
with pharmacological
substances and drug–drug
Journal of Biomedical Informatics,
interactions.
46(5):914–920.

[Hinton et al.2012] Geoffrey Hinton, Li Deng, Dong Yu,
George E Dahl, Abdel-rahman Mohamed, Navdeep
Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick
Nguyen, Tara N Sainath, et al. 2012. Deep neural
networks for acoustic modeling in speech recognition:
The shared views of four research groups. IEEE Sig-
nal Processing Magazine, 29(6):82–97.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural Computation, 9(8):1735–1780.

[Jordan1986] Michael I. Jordan. 1986. Serial order: A
parallel distributed processing approach. Technical re-
port, San Diego: University of California, Institute for
Cognitive Science.

[Krizhevsky et al.2012] Alex Krizhevsky, Ilya Sutskever,
and Geoffrey E Hinton. 2012. Imagenet classiﬁcation
with deep convolutional neural networks.
In NIPS,
pages 1097–1105.

[Lafferty et al.2001] John Lafferty, Andrew McCallum,
and Fernando Pereira. 2001. Conditional random
ﬁelds: Probabilistic models for segmenting and label-
ing sequence data. In ICML, pages 282–289.

[Lample et al.2016] Guillaume Lample, Miguel Balles-
teros, Sandeep Subramanian, Kazuya Kawakami, and
Chris Dyer. 2016. Neural architectures for named en-
tity recognition. In NAACL-HLT.

[Liu et al.2015a] Shengyu Liu, Buzhou Tang, Qingcai
Chen, and Xiaolong Wang.
2015a. Drug name
recognition: Approaches and resources. Information,
6(4):790–810.

[Liu et al.2015b] Shengyu Liu, Buzhou Tang, Qingcai
Chen, Xiaolong Wang, and Xiaoming Fan. 2015b.
Feature engineering for drug name recognition in
biomedical texts: Feature conjunction and feature se-
lection. Computational and Mathematical Methods in
Medicine, 2015:1–9.

[Mesnil et al.2015] Gr´egoire Mesnil, Yann Dauphin,
Kaisheng Yao, Yoshua Bengio, Li Deng, Dilek
Hakkani-Tur, Xiaodong He, Larry Heck, Gokhan Tur,
Dong Yu, et al. 2015. Using recurrent neural net-
works for slot ﬁlling in spoken language understand-
ing. IEEE/ACM Transactions on Audio, Speech, and
Language Processing, 23(3):530–539.

[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever, Kai
Chen, Greg S Corrado, and Jeff Dean. 2013. Dis-
tributed representations of words and phrases and their
compositionality. In NIPS, pages 3111–3119.

[Nadeau and Sekine2007] David Nadeau and Satoshi
Sekine. 2007. A survey of named entity recogni-
tion and classiﬁcation. Linguisticae Investigationes,
30(1):3–26.

[Pennington et al.2014] Jeffrey

Richard
Socher, and Christopher D. Manning. 2014. GloVe:
Global vectors for word representation.
In EMNLP,
pages 1532–1543.

Pennington,

[Rockt¨aschel et al.2012] Tim Rockt¨aschel, Michael Wei-
dlich, and Ulf Leser. 2012. ChemSpot: A hybrid sys-
tem for chemical named entity recognition. Bioinfor-
matics, 28(12):1633–1640.

[Rockt¨aschel et al.2013] Tim Rockt¨aschel, Torsten Hu-
ber, Michael Weidlich, and Ulf Leser. 2013. WBI-
NER: The impact of domain-speciﬁc features on the
performance of identifying and classifying mentions
In The 7th International Workshop on Se-
of drugs.
mantic Evaluation, pages 356–363.

[Segura-Bedmar et al.2013] Isabel

Segura-Bedmar,
Paloma Mart´ınez, and Mar´ıa Herrero Zazo.
2013.
Semeval-2013 task 9: Extraction of drug-drug inter-
actions from biomedical texts (DDIExtraction 2013).
In The 7th International Workshop on Semantic
Evaluation.

[Segura-Bedmar et al.2015] Isabel

Segura-Bedmar,
Vıctor Su´arez-Paniagua, and Paloma Martınez. 2015.
Exploring word embedding for drug name recogni-
tion.
In The 6th International Workshop on Health
Text Mining and Information Analysis, page 64.

6
1
0
2
 
p
e
S
 
4
2
 
 
]
L
C
.
s
c
[
 
 
1
v
5
8
5
7
0
.
9
0
6
1
:
v
i
X
r
a

An Investigation of Recurrent Neural Architectures
for Drug Name Recognition

Raghavendra Chalapathy
University of Sydney
J12/1 Cleveland St
Darlington NSW 2008
rcha9612@uni.sydney.edu.au

Ehsan Zare Borzeshi
Capital Markets CRC
3/55 Harrington St
Sydney NSW 2000
ezborzeshi@cmcrc.com

Massimo Piccardi
University of Technology Sydney
PO Box 123
Broadway NSW 2007
Massimo.Piccardi@uts.edu.au

Abstract

Drug name recognition (DNR) is an essential
step in the Pharmacovigilance (PV) pipeline.
DNR aims to ﬁnd drug name mentions in un-
structured biomedical texts and classify them
State-of-the-art
into predeﬁned categories.
DNR approaches heavily rely on hand-crafted
features and domain-speciﬁc resources which
are difﬁcult to collect and tune. For this rea-
son, this paper investigates the effectiveness of
contemporary recurrent neural architectures -
the Elman and Jordan networks and the bidi-
rectional LSTM with CRF decoding - at per-
forming DNR straight from the text. The
experimental results achieved on the author-
itative SemEval-2013 Task 9.1 benchmarks
show that the bidirectional LSTM-CRF ranks
closely to highly-dedicated, hand-crafted sys-
tems.

1 Introduction

Pharmacovigilance (PV) is deﬁned by the World
Health Organization as the science and activities
concerned with the detection, assessment, under-
standing and prevention of adverse effects of drugs
or any other drug-related problems. Drug name
recognition (DNR) is a fundamental step in the PV
pipeline, similarly to the well-studied Named En-
tity Recognition (NER) task for general natural lan-
guage processing (NLP). DNR aims to ﬁnd drug
mentions in unstructured biomedical texts and clas-
sify them into predeﬁned categories in order to link
drug names with their effects and explore drug-drug
interactions (DDIs). Conventional approaches to
DNR sub-divide as rule-based, dictionary-based and

machine learning-based.
Intrinsically, rule-based
systems are hard to scale, time-consuming to as-
semble and ineffective in the presence of infor-
mal sentences and abbreviated phrases. Dictionary-
based systems identify drug names by matching text
chunks against drug dictionaries. These systems
typically achieve high precision, but suffer from low
recall (i.e., they miss a signiﬁcant number of men-
tions) due to spelling errors or drug name variants
not present in the dictionaries (Liu et al., 2015a).
Conversely, machine-learning approaches have the
potential to overcome all these limitations since their
foundations are intrinsically robust to variants. The
current state-of-the-art machine learning approaches
follow a two-step process of
feature engineer-
ing and classiﬁcation (Segura-Bedmar et al., 2015;
Abacha et al., 2015; Rockt¨aschel et al., 2013). Fea-
ture engineering refers to the task of representing
text by dedicated numeric vectors using domain
knowledge. Similarly to the design of rule-based
systems, this task requires much expert knowledge,
is typically challenging and time-consuming, and
has a major impact on the ﬁnal accuracy. For this
reason, this paper explores the performance of con-
temporary recurrent neural networks (RNNs) at pro-
viding end-to-end DNR straight from text, without
any manual feature engineering stage. The tested
RNNs include the popular Elman and Jordan net-
works and the bidirectional long short-term memory
(LSTM) with decoding provided by a conditional
random ﬁeld (CRF) (Elman, 1990; Jordan, 1986;
Lample et al., 2016; Collobert et al., 2011). The ex-
perimental results over the SemEval-2013 Task 9.1
benchmarks show an interesting accuracy from the

LSTM-CRF that exceeds that of various manually-
engineered systems and approximates the best result
in the literature.

2 Related Work

very

recent

features

specialized

approaches

(Rockt¨aschel et al., 2012),

Most of the research on drug name recognition to
date has focussed on domain-dependent aspects and
specialized text features. The beneﬁt of leveraging
such tailored features was made evident by the re-
sults from the SemEval-2013 Task 9.1 (Recognition
and classiﬁcation of pharmacological substances,
known as DNR task) challenge. The system that
ranked ﬁrst, WBI-NER (Rockt¨aschel et al., 2013),
adopted
derived
the ChemSpot
from an improved version of
tool
a collection of
drug dictionaries and ontologies. Similarly, many
other
(Abacha et al., 2015;
Liu et al., 2015b; Segura-Bedmar et al., 2015) have
been based on various combinations of general
and domain-speciﬁc features.
In the broader
ﬁeld of machine learning,
the recent years have
witnessed a rapid proliferation of deep neural
networks, with unprecedented results in tasks as
diverse as visual, speech and named-entity recog-
nition (Hinton et al., 2012; Krizhevsky et al., 2012;
Lample et al., 2016). One of the main advantages
of neural networks is that they can learn the fea-
ture representations automatically from the data,
thus avoiding the laborious feature engineering
stage
Lample et al., 2016).
Given these promising results, the main goal of this
paper is to provide the ﬁrst performance investiga-
tion of popular RNNs such as the Elman and Jordan
networks and the bidirectional LSTM-CRF over
DNR tasks.

(Mesnil et al., 2015;

3 The Proposed Approach

DNR can be formulated as a joint segmentation and
classiﬁcation task over a predeﬁned set of classes.
As an example, consider the input sentence provided
in Table 1. The notation follows the widely adopted
in/out/begin (IOB) entity representation with, in this
instance, Cimetidine as the drug, ALFENTA as the
brand, and words volatile inhalation anesthetics to-
gether as the group.
In this paper, we approach
the DNR task by recurrent neural networks and we

In
therefore provide a brief description hereafter.
an RNN, each word in the input sentence is ﬁrst
mapped to a random real-valued vector of arbitrary
dimension, d. Then, a measurement for the word,
noted as x(t), is formed by concatenating the word’s
own vector with a window of preceding and follow-
ing vectors (the ”context”). An example of input
vector with a context window of size s = 3 is:
w3(t) = [Cimetidine, reduces, ef f ect],
→ xreduces ∈ Rd,
→ xCimetidine ∈ Rd,
‘ef f ect′
→ xef f ect ∈ Rd,
x(t) = [xCimetidine, xreduces, xef f ect] ∈ R3d

‘Cimetidine′

‘reduces′

(1)

where w3(t) is the context window centered around
the t-th word, ′reduces′, and xword represents the
numerical vector for word.

For the Elman network, both x(t) and the output
from the hidden layer at time t − 1, h(t − 1), are in-
put into the hidden layer for frame t. The recurrent
connection from the past time frame enables a short-
term memory, while hidden-to-hidden neuron con-
nections make the network Turing-complete. This
architecture, common in RNNs, is suitable for pre-
diction of sequences. Formally, the hidden layer is
described as:

h(t) = f (U • x(t) + V • h(t − 1))

(2)

where U and V are randomly-initialized weight ma-
trices between the input and the hidden layer, and
between the past and current hidden layers, respec-
tively. Function f (·) is the sigmoid function:

f (x) =

1
1 + e−x

(3)

that adds non-linearity to the layer. Eventually, h(t)
is input in the output layer:

y(t) = g(W • h(t)), with g(zm) =

ezm
k=1ezk

ΣK

(4)

and convolved with the output weight matrix, W .
The output is normalized by a multi-class logistic
function, g(·), to become a proper probability over
the class set. The output dimensionality is therefore
determined by the number of entity classes (i.e., 4
for the DNR task).The Jordan network is very sim-
ilar to the Elman network, except that the feedback

Sentence
Entity class

Cimetidine
B-drug

reduces
O

clearance
O

of
O

ALFENTA and
O
B-brand

volatile
B-group

inhalation
I-group

anesthetics
I-group

Table 1: Example sentence in a DNR task with entity classes represented in IOB format.

DDI-DrugBank

DDI-MedLine

Training+Test for DDI task Test for DNR Training+Test for DDI task Test for DNR

documents
sentences
drug n
group
brand
drug

730
6577
124
3832
1770
9715

54
145
6
65
53
180
Table 2: Statistics of training and test datasets used for SemEval-2013 Task 9.1.

175
1627
520
234
36
1574

58
520
115
90
6
171

is sourced from the output layer rather than the pre-
vious hidden layer:

h(t) = f (U • x(t) + V • y(t − 1)).

(5)

their

decay

recent

biases

them toward
inputs (Bengio et al., 1994).

Although the Elman and Jordan networks
ex-
can learn long-term dependencies,
their
ponential
The
most
LSTM was designed to overcome this
limi-
tation by incorporating a gated memory-cell
to capture long-range dependencies within the
In the
data (Hochreiter and Schmidhuber, 1997).
bidirectional LSTM, for any given sentence,
the
−→
h (t), and a right,
network computes both a left,
←−
h (t), representations of the sentence context at
every input, x(t). The ﬁnal representation is created
←−
h (t)]. All
by concatenating them as h(t) = [
these networks utilize the h(t) layer as an implicit
feature for entity class prediction: although this
model has proved effective in many cases, it is not
able to provide joint decoding of the outputs in a
Viterbi-style manner (e.g., an I-group cannot follow
a B-brand; etc). Thus, another modiﬁcation to the
bidirectional LSTM is the addition of a conditional
random ﬁeld (CRF) (Lafferty et al., 2001) as the
output layer to provide optimal sequential decoding.
The resulting network is commonly referred to as
the bidirectional LSTM-CRF (Lample et al., 2016).

−→
h (t);

4 Experiments

4.1 Datasets

DDIExtraction

from

The
challenge
9.1

(Segura-Bedmar et al., 2013)

shared

2013
SemEval-2013
has

task
Task
provided

a benchmark corpus for DNR and DDI extraction.
The corpus contains manually-annotated pharma-
cological substances and drug-drug interactions
for a total of 18, 502 pharmacological
(DDIs)
substances and 5, 028 DDIs.
It collates two
DDI-DrugBank and DDI-
distinct datasets:
MedLine (Herrero-Zazo et al., 2013).
Table 2
the training
summarizes the basic statistics of
and test datasets used in our experiments.
For
proper comparison, we follow the same settings
as (Segura-Bedmar et al., 2015), using the training
data of the DNR task along with the test data for the
DDI task for training and validation of DNR. We
split this joint dataset into a training and validation
sets with approximately 70% of sentences for
training and the remaining for validation.

4.2 Evaluation Methodology

Our models have been blindly evaluated on un-
seen DNR test data using the strict evaluation
the predicted en-
metrics. With this evaluation,
tities have to match the ground-truth entities ex-
actly, both in boundary and class.
To facili-
tate the replication of our experimental results,
we have used a publicly-available library for the
implementation1 (i.e., the Theano neural network
toolkit (Bergstra et al., 2010)).
The experiments
have been run over a range of values for the
hyper-parameters, using the validation set for se-
lection (Bergstra and Bengio, 2012). The hyper-
parameters include the number of hidden-layer
nodes, H ∈ {25, 50, 100}, the context window size,
s ∈ {1, 3, 5}, and the embedding dimension, d ∈

1https://github.com/raghavchalapathy/dnr

DDI-DrugBank

DDI-MedLine

Precision Recall F1 Score Precision Recall F1 Score

Methods
WBI-NER (Rockt¨aschel et al., 2013)
Hybrid-DDI (Abacha et al., 2015)
Word2Vec+DINTO (Segura-Bedmar et al., 2015)
Elman RNN
Jordan RNN
Bidirectional LSTM-CRF

88.00
93.00
69.00
79.91
77.59
87.07
Table 3: Performance comparison between the recurrent neural networks (bottom three lines) and state-of-the-art systems (top

58.10
37.00
57.00
37.78
40.06
52.75

87.80
80.00
75.00
69.13
68.25
85.19

87.00
70.00
82.00
60.91
60.91
83.39

56.00
25.00
51.00
33.56
30.20
52.57

61.00
74.00
65.00
43.23
59.47
52.93

three lines) over the SemEval-2013 Task 9.1.

Entities

DDI-DrugBank

DDI-MedLine

Precision Recall F1 Score Precision Recall F1 Score

Bidirectional LSTM-CRF

53.76
61.05
0.0
45.45
Table 4: SemEval-2013 Task 9.1 results by entity for the bidirectional LSTM-CRF.

group
drug
brand
drug n

59.52
65.22
0.0
40.20

83.33
87.50
84.85
0.0

76.92
90.59
91.30
0.0

90.91
84.62
79.25
0.0

56.50
63.06
0.0
42.67

{50, 100, 300, 500, 1000}. Two additional parame-
ters, the learning and drop-out rates, were sampled
from a uniform distribution in the range [0.05, 0.1].
The embedding and initial weight matrices were all
sampled from the uniform distribution within range
[−1, 1]. Early training stopping was set to 100
epochs to mollify over-ﬁtting, and the model that
gave the best performance on the validation set was
retained. The accuracy is reported in terms of micro-
average F1 score computed using the CoNLL score
function (Nadeau and Sekine, 2007).

4.3 Results and Analysis

Table 3 shows the performance comparison between
the explored RNNs and state-of-the-art DNR sys-
the RNNs have not
tems. As an overall note,
reached the same accuracy as the top system, WBI-
NER (Rockt¨aschel et al., 2013). However, the bidi-
rectional LSTM-CRF has achieved the second-best
score on DDI-DrugBank and the third-best on DDI-
MedLine. These results seem interesting on the
ground that the RNNs provide DNR straight from
text rather than from manually-engineered features.
Given that the RNNs learn entirely from the data, the
better performance over the DDI-DrugBank dataset
is very likely due to its larger size. Accordingly, it
is reasonable to expect higher relative performance
should larger corpora become available in the fu-
ture. Table 4 also breaks down the results by en-
tity class for the bidirectional LSTM-CRF. The low

score on the brand class for DDI-MedLine and on
the drug n class (i.e., active substances not ap-
proved for human use) for DDI-DrugBank are likely
attributable to the very small sample size (Table 2).
This issue is also shared by the state-of-the-art DNR
systems.

5 Conclusion

This paper has investigated the effectiveness of re-
current neural architectures, namely the Elman and
Jordan networks and the bidirectional LSTM-CRF,
for drug name recognition. The most appealing fea-
ture of these architectures is their ability to pro-
vide end-to-end recognition straight from text, spar-
ing effort from laborious feature construction. To
the best of our knowledge, ours is the ﬁrst pa-
per to explore RNNs for entity recognition from
pharmacological
The experimental results
over the SemEval-2013 Task 9.1 benchmarks look
promising, with the bidirectional LSTM-CRF rank-
ing closely to the state of the art. A potential way
to further improve its performance would be to ini-
tialize its training with unsupervised word embed-
dings such as Word2Vec (Mikolov et al., 2013) and
GloVe (Pennington et al., 2014). This approach has
proved effective in many other domains and still dis-
penses with expert annotation effort; we plan this
exploration for the near future.

text.

References

[Abacha et al.2015] Asma Ben Abacha, Md Faisal Mah-
bub Chowdhury, Aikaterini Karanasiou, Yassine Mra-
bet, Alberto Lavelli, and Pierre Zweigenbaum. 2015.
Text mining for pharmacovigilance: Using machine
learning for drug name recognition and drug–drug
interaction extraction and classiﬁcation.
Journal of
Biomedical Informatics, 58:122–132.

[Bengio et al.1994] Yoshua Bengio, Patrice Simard, and
Paolo Frasconi. 1994. Learning long-term dependen-
cies with gradient descent is difﬁcult. IEEE Transac-
tions on Neural Networks, 5(2):157–166.

[Bergstra and Bengio2012] James Bergstra and Yoshua
Bengio. 2012. Random search for hyper-parameter
optimization. Journal of Machine Learning Research,
13:281–305.

[Bergstra et al.2010] James Bergstra, Olivier Breuleux,
Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu,
Guillaume Desjardins, Joseph Turian, David Warde-
Farley, and Yoshua Bengio. 2010. Theano: A CPU
and GPU math compiler in Python. In The 9th Python
in Science Conference, pages 1–7.

[Collobert et al.2011] Ronan Collobert, Jason Weston,
L´eon Bottou, Michael Karlen, Koray Kavukcuoglu,
and Pavel Kuksa. 2011. Natural language processing
(almost) from scratch. Journal of Machine Learning
Research, 12:2493–2537.

[Elman1990] Jeffrey L Elman. 1990. Finding structure in

time. Cognitive Science, 14(2):179–211.
[Herrero-Zazo et al.2013] Mar´ıa Herrero-Zazo,

Isabel
Segura-Bedmar, Paloma Mart´ınez, and Thierry De-
clerck. 2013. The DDI corpus: An annotated corpus
with pharmacological
substances and drug–drug
Journal of Biomedical Informatics,
interactions.
46(5):914–920.

[Hinton et al.2012] Geoffrey Hinton, Li Deng, Dong Yu,
George E Dahl, Abdel-rahman Mohamed, Navdeep
Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick
Nguyen, Tara N Sainath, et al. 2012. Deep neural
networks for acoustic modeling in speech recognition:
The shared views of four research groups. IEEE Sig-
nal Processing Magazine, 29(6):82–97.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural Computation, 9(8):1735–1780.

[Jordan1986] Michael I. Jordan. 1986. Serial order: A
parallel distributed processing approach. Technical re-
port, San Diego: University of California, Institute for
Cognitive Science.

[Krizhevsky et al.2012] Alex Krizhevsky, Ilya Sutskever,
and Geoffrey E Hinton. 2012. Imagenet classiﬁcation
with deep convolutional neural networks.
In NIPS,
pages 1097–1105.

[Lafferty et al.2001] John Lafferty, Andrew McCallum,
and Fernando Pereira. 2001. Conditional random
ﬁelds: Probabilistic models for segmenting and label-
ing sequence data. In ICML, pages 282–289.

[Lample et al.2016] Guillaume Lample, Miguel Balles-
teros, Sandeep Subramanian, Kazuya Kawakami, and
Chris Dyer. 2016. Neural architectures for named en-
tity recognition. In NAACL-HLT.

[Liu et al.2015a] Shengyu Liu, Buzhou Tang, Qingcai
Chen, and Xiaolong Wang.
2015a. Drug name
recognition: Approaches and resources. Information,
6(4):790–810.

[Liu et al.2015b] Shengyu Liu, Buzhou Tang, Qingcai
Chen, Xiaolong Wang, and Xiaoming Fan. 2015b.
Feature engineering for drug name recognition in
biomedical texts: Feature conjunction and feature se-
lection. Computational and Mathematical Methods in
Medicine, 2015:1–9.

[Mesnil et al.2015] Gr´egoire Mesnil, Yann Dauphin,
Kaisheng Yao, Yoshua Bengio, Li Deng, Dilek
Hakkani-Tur, Xiaodong He, Larry Heck, Gokhan Tur,
Dong Yu, et al. 2015. Using recurrent neural net-
works for slot ﬁlling in spoken language understand-
ing. IEEE/ACM Transactions on Audio, Speech, and
Language Processing, 23(3):530–539.

[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever, Kai
Chen, Greg S Corrado, and Jeff Dean. 2013. Dis-
tributed representations of words and phrases and their
compositionality. In NIPS, pages 3111–3119.

[Nadeau and Sekine2007] David Nadeau and Satoshi
Sekine. 2007. A survey of named entity recogni-
tion and classiﬁcation. Linguisticae Investigationes,
30(1):3–26.

[Pennington et al.2014] Jeffrey

Richard
Socher, and Christopher D. Manning. 2014. GloVe:
Global vectors for word representation.
In EMNLP,
pages 1532–1543.

Pennington,

[Rockt¨aschel et al.2012] Tim Rockt¨aschel, Michael Wei-
dlich, and Ulf Leser. 2012. ChemSpot: A hybrid sys-
tem for chemical named entity recognition. Bioinfor-
matics, 28(12):1633–1640.

[Rockt¨aschel et al.2013] Tim Rockt¨aschel, Torsten Hu-
ber, Michael Weidlich, and Ulf Leser. 2013. WBI-
NER: The impact of domain-speciﬁc features on the
performance of identifying and classifying mentions
In The 7th International Workshop on Se-
of drugs.
mantic Evaluation, pages 356–363.

[Segura-Bedmar et al.2013] Isabel

Segura-Bedmar,
Paloma Mart´ınez, and Mar´ıa Herrero Zazo.
2013.
Semeval-2013 task 9: Extraction of drug-drug inter-
actions from biomedical texts (DDIExtraction 2013).
In The 7th International Workshop on Semantic
Evaluation.

[Segura-Bedmar et al.2015] Isabel

Segura-Bedmar,
Vıctor Su´arez-Paniagua, and Paloma Martınez. 2015.
Exploring word embedding for drug name recogni-
tion.
In The 6th International Workshop on Health
Text Mining and Information Analysis, page 64.

6
1
0
2
 
p
e
S
 
4
2
 
 
]
L
C
.
s
c
[
 
 
1
v
5
8
5
7
0
.
9
0
6
1
:
v
i
X
r
a

An Investigation of Recurrent Neural Architectures
for Drug Name Recognition

Raghavendra Chalapathy
University of Sydney
J12/1 Cleveland St
Darlington NSW 2008
rcha9612@uni.sydney.edu.au

Ehsan Zare Borzeshi
Capital Markets CRC
3/55 Harrington St
Sydney NSW 2000
ezborzeshi@cmcrc.com

Massimo Piccardi
University of Technology Sydney
PO Box 123
Broadway NSW 2007
Massimo.Piccardi@uts.edu.au

Abstract

Drug name recognition (DNR) is an essential
step in the Pharmacovigilance (PV) pipeline.
DNR aims to ﬁnd drug name mentions in un-
structured biomedical texts and classify them
State-of-the-art
into predeﬁned categories.
DNR approaches heavily rely on hand-crafted
features and domain-speciﬁc resources which
are difﬁcult to collect and tune. For this rea-
son, this paper investigates the effectiveness of
contemporary recurrent neural architectures -
the Elman and Jordan networks and the bidi-
rectional LSTM with CRF decoding - at per-
forming DNR straight from the text. The
experimental results achieved on the author-
itative SemEval-2013 Task 9.1 benchmarks
show that the bidirectional LSTM-CRF ranks
closely to highly-dedicated, hand-crafted sys-
tems.

1 Introduction

Pharmacovigilance (PV) is deﬁned by the World
Health Organization as the science and activities
concerned with the detection, assessment, under-
standing and prevention of adverse effects of drugs
or any other drug-related problems. Drug name
recognition (DNR) is a fundamental step in the PV
pipeline, similarly to the well-studied Named En-
tity Recognition (NER) task for general natural lan-
guage processing (NLP). DNR aims to ﬁnd drug
mentions in unstructured biomedical texts and clas-
sify them into predeﬁned categories in order to link
drug names with their effects and explore drug-drug
interactions (DDIs). Conventional approaches to
DNR sub-divide as rule-based, dictionary-based and

machine learning-based.
Intrinsically, rule-based
systems are hard to scale, time-consuming to as-
semble and ineffective in the presence of infor-
mal sentences and abbreviated phrases. Dictionary-
based systems identify drug names by matching text
chunks against drug dictionaries. These systems
typically achieve high precision, but suffer from low
recall (i.e., they miss a signiﬁcant number of men-
tions) due to spelling errors or drug name variants
not present in the dictionaries (Liu et al., 2015a).
Conversely, machine-learning approaches have the
potential to overcome all these limitations since their
foundations are intrinsically robust to variants. The
current state-of-the-art machine learning approaches
follow a two-step process of
feature engineer-
ing and classiﬁcation (Segura-Bedmar et al., 2015;
Abacha et al., 2015; Rockt¨aschel et al., 2013). Fea-
ture engineering refers to the task of representing
text by dedicated numeric vectors using domain
knowledge. Similarly to the design of rule-based
systems, this task requires much expert knowledge,
is typically challenging and time-consuming, and
has a major impact on the ﬁnal accuracy. For this
reason, this paper explores the performance of con-
temporary recurrent neural networks (RNNs) at pro-
viding end-to-end DNR straight from text, without
any manual feature engineering stage. The tested
RNNs include the popular Elman and Jordan net-
works and the bidirectional long short-term memory
(LSTM) with decoding provided by a conditional
random ﬁeld (CRF) (Elman, 1990; Jordan, 1986;
Lample et al., 2016; Collobert et al., 2011). The ex-
perimental results over the SemEval-2013 Task 9.1
benchmarks show an interesting accuracy from the

LSTM-CRF that exceeds that of various manually-
engineered systems and approximates the best result
in the literature.

2 Related Work

very

recent

features

specialized

approaches

(Rockt¨aschel et al., 2012),

Most of the research on drug name recognition to
date has focussed on domain-dependent aspects and
specialized text features. The beneﬁt of leveraging
such tailored features was made evident by the re-
sults from the SemEval-2013 Task 9.1 (Recognition
and classiﬁcation of pharmacological substances,
known as DNR task) challenge. The system that
ranked ﬁrst, WBI-NER (Rockt¨aschel et al., 2013),
adopted
derived
the ChemSpot
from an improved version of
tool
a collection of
drug dictionaries and ontologies. Similarly, many
other
(Abacha et al., 2015;
Liu et al., 2015b; Segura-Bedmar et al., 2015) have
been based on various combinations of general
and domain-speciﬁc features.
In the broader
ﬁeld of machine learning,
the recent years have
witnessed a rapid proliferation of deep neural
networks, with unprecedented results in tasks as
diverse as visual, speech and named-entity recog-
nition (Hinton et al., 2012; Krizhevsky et al., 2012;
Lample et al., 2016). One of the main advantages
of neural networks is that they can learn the fea-
ture representations automatically from the data,
thus avoiding the laborious feature engineering
stage
Lample et al., 2016).
Given these promising results, the main goal of this
paper is to provide the ﬁrst performance investiga-
tion of popular RNNs such as the Elman and Jordan
networks and the bidirectional LSTM-CRF over
DNR tasks.

(Mesnil et al., 2015;

3 The Proposed Approach

DNR can be formulated as a joint segmentation and
classiﬁcation task over a predeﬁned set of classes.
As an example, consider the input sentence provided
in Table 1. The notation follows the widely adopted
in/out/begin (IOB) entity representation with, in this
instance, Cimetidine as the drug, ALFENTA as the
brand, and words volatile inhalation anesthetics to-
gether as the group.
In this paper, we approach
the DNR task by recurrent neural networks and we

In
therefore provide a brief description hereafter.
an RNN, each word in the input sentence is ﬁrst
mapped to a random real-valued vector of arbitrary
dimension, d. Then, a measurement for the word,
noted as x(t), is formed by concatenating the word’s
own vector with a window of preceding and follow-
ing vectors (the ”context”). An example of input
vector with a context window of size s = 3 is:
w3(t) = [Cimetidine, reduces, ef f ect],
→ xreduces ∈ Rd,
→ xCimetidine ∈ Rd,
‘ef f ect′
→ xef f ect ∈ Rd,
x(t) = [xCimetidine, xreduces, xef f ect] ∈ R3d

‘Cimetidine′

‘reduces′

(1)

where w3(t) is the context window centered around
the t-th word, ′reduces′, and xword represents the
numerical vector for word.

For the Elman network, both x(t) and the output
from the hidden layer at time t − 1, h(t − 1), are in-
put into the hidden layer for frame t. The recurrent
connection from the past time frame enables a short-
term memory, while hidden-to-hidden neuron con-
nections make the network Turing-complete. This
architecture, common in RNNs, is suitable for pre-
diction of sequences. Formally, the hidden layer is
described as:

h(t) = f (U • x(t) + V • h(t − 1))

(2)

where U and V are randomly-initialized weight ma-
trices between the input and the hidden layer, and
between the past and current hidden layers, respec-
tively. Function f (·) is the sigmoid function:

f (x) =

1
1 + e−x

(3)

that adds non-linearity to the layer. Eventually, h(t)
is input in the output layer:

y(t) = g(W • h(t)), with g(zm) =

ezm
k=1ezk

ΣK

(4)

and convolved with the output weight matrix, W .
The output is normalized by a multi-class logistic
function, g(·), to become a proper probability over
the class set. The output dimensionality is therefore
determined by the number of entity classes (i.e., 4
for the DNR task).The Jordan network is very sim-
ilar to the Elman network, except that the feedback

Sentence
Entity class

Cimetidine
B-drug

reduces
O

clearance
O

of
O

ALFENTA and
O
B-brand

volatile
B-group

inhalation
I-group

anesthetics
I-group

Table 1: Example sentence in a DNR task with entity classes represented in IOB format.

DDI-DrugBank

DDI-MedLine

Training+Test for DDI task Test for DNR Training+Test for DDI task Test for DNR

documents
sentences
drug n
group
brand
drug

730
6577
124
3832
1770
9715

54
145
6
65
53
180
Table 2: Statistics of training and test datasets used for SemEval-2013 Task 9.1.

175
1627
520
234
36
1574

58
520
115
90
6
171

is sourced from the output layer rather than the pre-
vious hidden layer:

h(t) = f (U • x(t) + V • y(t − 1)).

(5)

their

decay

recent

biases

them toward
inputs (Bengio et al., 1994).

Although the Elman and Jordan networks
ex-
can learn long-term dependencies,
their
ponential
The
most
LSTM was designed to overcome this
limi-
tation by incorporating a gated memory-cell
to capture long-range dependencies within the
In the
data (Hochreiter and Schmidhuber, 1997).
bidirectional LSTM, for any given sentence,
the
−→
h (t), and a right,
network computes both a left,
←−
h (t), representations of the sentence context at
every input, x(t). The ﬁnal representation is created
←−
h (t)]. All
by concatenating them as h(t) = [
these networks utilize the h(t) layer as an implicit
feature for entity class prediction: although this
model has proved effective in many cases, it is not
able to provide joint decoding of the outputs in a
Viterbi-style manner (e.g., an I-group cannot follow
a B-brand; etc). Thus, another modiﬁcation to the
bidirectional LSTM is the addition of a conditional
random ﬁeld (CRF) (Lafferty et al., 2001) as the
output layer to provide optimal sequential decoding.
The resulting network is commonly referred to as
the bidirectional LSTM-CRF (Lample et al., 2016).

−→
h (t);

4 Experiments

4.1 Datasets

DDIExtraction

from

The
challenge
9.1

(Segura-Bedmar et al., 2013)

shared

2013
SemEval-2013
has

task
Task
provided

a benchmark corpus for DNR and DDI extraction.
The corpus contains manually-annotated pharma-
cological substances and drug-drug interactions
for a total of 18, 502 pharmacological
(DDIs)
substances and 5, 028 DDIs.
It collates two
DDI-DrugBank and DDI-
distinct datasets:
MedLine (Herrero-Zazo et al., 2013).
Table 2
the training
summarizes the basic statistics of
and test datasets used in our experiments.
For
proper comparison, we follow the same settings
as (Segura-Bedmar et al., 2015), using the training
data of the DNR task along with the test data for the
DDI task for training and validation of DNR. We
split this joint dataset into a training and validation
sets with approximately 70% of sentences for
training and the remaining for validation.

4.2 Evaluation Methodology

Our models have been blindly evaluated on un-
seen DNR test data using the strict evaluation
the predicted en-
metrics. With this evaluation,
tities have to match the ground-truth entities ex-
actly, both in boundary and class.
To facili-
tate the replication of our experimental results,
we have used a publicly-available library for the
implementation1 (i.e., the Theano neural network
toolkit (Bergstra et al., 2010)).
The experiments
have been run over a range of values for the
hyper-parameters, using the validation set for se-
lection (Bergstra and Bengio, 2012). The hyper-
parameters include the number of hidden-layer
nodes, H ∈ {25, 50, 100}, the context window size,
s ∈ {1, 3, 5}, and the embedding dimension, d ∈

1https://github.com/raghavchalapathy/dnr

DDI-DrugBank

DDI-MedLine

Precision Recall F1 Score Precision Recall F1 Score

Methods
WBI-NER (Rockt¨aschel et al., 2013)
Hybrid-DDI (Abacha et al., 2015)
Word2Vec+DINTO (Segura-Bedmar et al., 2015)
Elman RNN
Jordan RNN
Bidirectional LSTM-CRF

88.00
93.00
69.00
79.91
77.59
87.07
Table 3: Performance comparison between the recurrent neural networks (bottom three lines) and state-of-the-art systems (top

58.10
37.00
57.00
37.78
40.06
52.75

87.80
80.00
75.00
69.13
68.25
85.19

87.00
70.00
82.00
60.91
60.91
83.39

61.00
74.00
65.00
43.23
59.47
52.93

56.00
25.00
51.00
33.56
30.20
52.57

three lines) over the SemEval-2013 Task 9.1.

Entities

DDI-DrugBank

DDI-MedLine

Precision Recall F1 Score Precision Recall F1 Score

Bidirectional LSTM-CRF

53.76
61.05
0.0
45.45
Table 4: SemEval-2013 Task 9.1 results by entity for the bidirectional LSTM-CRF.

group
drug
brand
drug n

59.52
65.22
0.0
40.20

83.33
87.50
84.85
0.0

76.92
90.59
91.30
0.0

90.91
84.62
79.25
0.0

56.50
63.06
0.0
42.67

{50, 100, 300, 500, 1000}. Two additional parame-
ters, the learning and drop-out rates, were sampled
from a uniform distribution in the range [0.05, 0.1].
The embedding and initial weight matrices were all
sampled from the uniform distribution within range
[−1, 1]. Early training stopping was set to 100
epochs to mollify over-ﬁtting, and the model that
gave the best performance on the validation set was
retained. The accuracy is reported in terms of micro-
average F1 score computed using the CoNLL score
function (Nadeau and Sekine, 2007).

4.3 Results and Analysis

Table 3 shows the performance comparison between
the explored RNNs and state-of-the-art DNR sys-
the RNNs have not
tems. As an overall note,
reached the same accuracy as the top system, WBI-
NER (Rockt¨aschel et al., 2013). However, the bidi-
rectional LSTM-CRF has achieved the second-best
score on DDI-DrugBank and the third-best on DDI-
MedLine. These results seem interesting on the
ground that the RNNs provide DNR straight from
text rather than from manually-engineered features.
Given that the RNNs learn entirely from the data, the
better performance over the DDI-DrugBank dataset
is very likely due to its larger size. Accordingly, it
is reasonable to expect higher relative performance
should larger corpora become available in the fu-
ture. Table 4 also breaks down the results by en-
tity class for the bidirectional LSTM-CRF. The low

score on the brand class for DDI-MedLine and on
the drug n class (i.e., active substances not ap-
proved for human use) for DDI-DrugBank are likely
attributable to the very small sample size (Table 2).
This issue is also shared by the state-of-the-art DNR
systems.

5 Conclusion

This paper has investigated the effectiveness of re-
current neural architectures, namely the Elman and
Jordan networks and the bidirectional LSTM-CRF,
for drug name recognition. The most appealing fea-
ture of these architectures is their ability to pro-
vide end-to-end recognition straight from text, spar-
ing effort from laborious feature construction. To
the best of our knowledge, ours is the ﬁrst pa-
per to explore RNNs for entity recognition from
pharmacological
The experimental results
over the SemEval-2013 Task 9.1 benchmarks look
promising, with the bidirectional LSTM-CRF rank-
ing closely to the state of the art. A potential way
to further improve its performance would be to ini-
tialize its training with unsupervised word embed-
dings such as Word2Vec (Mikolov et al., 2013) and
GloVe (Pennington et al., 2014). This approach has
proved effective in many other domains and still dis-
penses with expert annotation effort; we plan this
exploration for the near future.

text.

References

[Abacha et al.2015] Asma Ben Abacha, Md Faisal Mah-
bub Chowdhury, Aikaterini Karanasiou, Yassine Mra-
bet, Alberto Lavelli, and Pierre Zweigenbaum. 2015.
Text mining for pharmacovigilance: Using machine
learning for drug name recognition and drug–drug
interaction extraction and classiﬁcation.
Journal of
Biomedical Informatics, 58:122–132.

[Bengio et al.1994] Yoshua Bengio, Patrice Simard, and
Paolo Frasconi. 1994. Learning long-term dependen-
cies with gradient descent is difﬁcult. IEEE Transac-
tions on Neural Networks, 5(2):157–166.

[Bergstra and Bengio2012] James Bergstra and Yoshua
Bengio. 2012. Random search for hyper-parameter
optimization. Journal of Machine Learning Research,
13:281–305.

[Bergstra et al.2010] James Bergstra, Olivier Breuleux,
Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu,
Guillaume Desjardins, Joseph Turian, David Warde-
Farley, and Yoshua Bengio. 2010. Theano: A CPU
and GPU math compiler in Python. In The 9th Python
in Science Conference, pages 1–7.

[Collobert et al.2011] Ronan Collobert, Jason Weston,
L´eon Bottou, Michael Karlen, Koray Kavukcuoglu,
and Pavel Kuksa. 2011. Natural language processing
(almost) from scratch. Journal of Machine Learning
Research, 12:2493–2537.

[Elman1990] Jeffrey L Elman. 1990. Finding structure in

time. Cognitive Science, 14(2):179–211.
[Herrero-Zazo et al.2013] Mar´ıa Herrero-Zazo,

Isabel
Segura-Bedmar, Paloma Mart´ınez, and Thierry De-
clerck. 2013. The DDI corpus: An annotated corpus
with pharmacological
substances and drug–drug
Journal of Biomedical Informatics,
interactions.
46(5):914–920.

[Hinton et al.2012] Geoffrey Hinton, Li Deng, Dong Yu,
George E Dahl, Abdel-rahman Mohamed, Navdeep
Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick
Nguyen, Tara N Sainath, et al. 2012. Deep neural
networks for acoustic modeling in speech recognition:
The shared views of four research groups. IEEE Sig-
nal Processing Magazine, 29(6):82–97.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural Computation, 9(8):1735–1780.

[Jordan1986] Michael I. Jordan. 1986. Serial order: A
parallel distributed processing approach. Technical re-
port, San Diego: University of California, Institute for
Cognitive Science.

[Krizhevsky et al.2012] Alex Krizhevsky, Ilya Sutskever,
and Geoffrey E Hinton. 2012. Imagenet classiﬁcation
with deep convolutional neural networks.
In NIPS,
pages 1097–1105.

[Lafferty et al.2001] John Lafferty, Andrew McCallum,
and Fernando Pereira. 2001. Conditional random
ﬁelds: Probabilistic models for segmenting and label-
ing sequence data. In ICML, pages 282–289.

[Lample et al.2016] Guillaume Lample, Miguel Balles-
teros, Sandeep Subramanian, Kazuya Kawakami, and
Chris Dyer. 2016. Neural architectures for named en-
tity recognition. In NAACL-HLT.

[Liu et al.2015a] Shengyu Liu, Buzhou Tang, Qingcai
Chen, and Xiaolong Wang.
2015a. Drug name
recognition: Approaches and resources. Information,
6(4):790–810.

[Liu et al.2015b] Shengyu Liu, Buzhou Tang, Qingcai
Chen, Xiaolong Wang, and Xiaoming Fan. 2015b.
Feature engineering for drug name recognition in
biomedical texts: Feature conjunction and feature se-
lection. Computational and Mathematical Methods in
Medicine, 2015:1–9.

[Mesnil et al.2015] Gr´egoire Mesnil, Yann Dauphin,
Kaisheng Yao, Yoshua Bengio, Li Deng, Dilek
Hakkani-Tur, Xiaodong He, Larry Heck, Gokhan Tur,
Dong Yu, et al. 2015. Using recurrent neural net-
works for slot ﬁlling in spoken language understand-
ing. IEEE/ACM Transactions on Audio, Speech, and
Language Processing, 23(3):530–539.

[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever, Kai
Chen, Greg S Corrado, and Jeff Dean. 2013. Dis-
tributed representations of words and phrases and their
compositionality. In NIPS, pages 3111–3119.

[Nadeau and Sekine2007] David Nadeau and Satoshi
Sekine. 2007. A survey of named entity recogni-
tion and classiﬁcation. Linguisticae Investigationes,
30(1):3–26.

[Pennington et al.2014] Jeffrey

Richard
Socher, and Christopher D. Manning. 2014. GloVe:
Global vectors for word representation.
In EMNLP,
pages 1532–1543.

Pennington,

[Rockt¨aschel et al.2012] Tim Rockt¨aschel, Michael Wei-
dlich, and Ulf Leser. 2012. ChemSpot: A hybrid sys-
tem for chemical named entity recognition. Bioinfor-
matics, 28(12):1633–1640.

[Rockt¨aschel et al.2013] Tim Rockt¨aschel, Torsten Hu-
ber, Michael Weidlich, and Ulf Leser. 2013. WBI-
NER: The impact of domain-speciﬁc features on the
performance of identifying and classifying mentions
In The 7th International Workshop on Se-
of drugs.
mantic Evaluation, pages 356–363.

[Segura-Bedmar et al.2013] Isabel

Segura-Bedmar,
Paloma Mart´ınez, and Mar´ıa Herrero Zazo.
2013.
Semeval-2013 task 9: Extraction of drug-drug inter-
actions from biomedical texts (DDIExtraction 2013).
In The 7th International Workshop on Semantic
Evaluation.

[Segura-Bedmar et al.2015] Isabel

Segura-Bedmar,
Vıctor Su´arez-Paniagua, and Paloma Martınez. 2015.
Exploring word embedding for drug name recogni-
tion.
In The 6th International Workshop on Health
Text Mining and Information Analysis, page 64.

6
1
0
2
 
p
e
S
 
4
2
 
 
]
L
C
.
s
c
[
 
 
1
v
5
8
5
7
0
.
9
0
6
1
:
v
i
X
r
a

An Investigation of Recurrent Neural Architectures
for Drug Name Recognition

Raghavendra Chalapathy
University of Sydney
J12/1 Cleveland St
Darlington NSW 2008
rcha9612@uni.sydney.edu.au

Ehsan Zare Borzeshi
Capital Markets CRC
3/55 Harrington St
Sydney NSW 2000
ezborzeshi@cmcrc.com

Massimo Piccardi
University of Technology Sydney
PO Box 123
Broadway NSW 2007
Massimo.Piccardi@uts.edu.au

Abstract

Drug name recognition (DNR) is an essential
step in the Pharmacovigilance (PV) pipeline.
DNR aims to ﬁnd drug name mentions in un-
structured biomedical texts and classify them
State-of-the-art
into predeﬁned categories.
DNR approaches heavily rely on hand-crafted
features and domain-speciﬁc resources which
are difﬁcult to collect and tune. For this rea-
son, this paper investigates the effectiveness of
contemporary recurrent neural architectures -
the Elman and Jordan networks and the bidi-
rectional LSTM with CRF decoding - at per-
forming DNR straight from the text. The
experimental results achieved on the author-
itative SemEval-2013 Task 9.1 benchmarks
show that the bidirectional LSTM-CRF ranks
closely to highly-dedicated, hand-crafted sys-
tems.

1 Introduction

Pharmacovigilance (PV) is deﬁned by the World
Health Organization as the science and activities
concerned with the detection, assessment, under-
standing and prevention of adverse effects of drugs
or any other drug-related problems. Drug name
recognition (DNR) is a fundamental step in the PV
pipeline, similarly to the well-studied Named En-
tity Recognition (NER) task for general natural lan-
guage processing (NLP). DNR aims to ﬁnd drug
mentions in unstructured biomedical texts and clas-
sify them into predeﬁned categories in order to link
drug names with their effects and explore drug-drug
interactions (DDIs). Conventional approaches to
DNR sub-divide as rule-based, dictionary-based and

machine learning-based.
Intrinsically, rule-based
systems are hard to scale, time-consuming to as-
semble and ineffective in the presence of infor-
mal sentences and abbreviated phrases. Dictionary-
based systems identify drug names by matching text
chunks against drug dictionaries. These systems
typically achieve high precision, but suffer from low
recall (i.e., they miss a signiﬁcant number of men-
tions) due to spelling errors or drug name variants
not present in the dictionaries (Liu et al., 2015a).
Conversely, machine-learning approaches have the
potential to overcome all these limitations since their
foundations are intrinsically robust to variants. The
current state-of-the-art machine learning approaches
follow a two-step process of
feature engineer-
ing and classiﬁcation (Segura-Bedmar et al., 2015;
Abacha et al., 2015; Rockt¨aschel et al., 2013). Fea-
ture engineering refers to the task of representing
text by dedicated numeric vectors using domain
knowledge. Similarly to the design of rule-based
systems, this task requires much expert knowledge,
is typically challenging and time-consuming, and
has a major impact on the ﬁnal accuracy. For this
reason, this paper explores the performance of con-
temporary recurrent neural networks (RNNs) at pro-
viding end-to-end DNR straight from text, without
any manual feature engineering stage. The tested
RNNs include the popular Elman and Jordan net-
works and the bidirectional long short-term memory
(LSTM) with decoding provided by a conditional
random ﬁeld (CRF) (Elman, 1990; Jordan, 1986;
Lample et al., 2016; Collobert et al., 2011). The ex-
perimental results over the SemEval-2013 Task 9.1
benchmarks show an interesting accuracy from the

LSTM-CRF that exceeds that of various manually-
engineered systems and approximates the best result
in the literature.

2 Related Work

very

recent

features

specialized

approaches

(Rockt¨aschel et al., 2012),

Most of the research on drug name recognition to
date has focussed on domain-dependent aspects and
specialized text features. The beneﬁt of leveraging
such tailored features was made evident by the re-
sults from the SemEval-2013 Task 9.1 (Recognition
and classiﬁcation of pharmacological substances,
known as DNR task) challenge. The system that
ranked ﬁrst, WBI-NER (Rockt¨aschel et al., 2013),
adopted
derived
the ChemSpot
from an improved version of
tool
a collection of
drug dictionaries and ontologies. Similarly, many
other
(Abacha et al., 2015;
Liu et al., 2015b; Segura-Bedmar et al., 2015) have
been based on various combinations of general
and domain-speciﬁc features.
In the broader
ﬁeld of machine learning,
the recent years have
witnessed a rapid proliferation of deep neural
networks, with unprecedented results in tasks as
diverse as visual, speech and named-entity recog-
nition (Hinton et al., 2012; Krizhevsky et al., 2012;
Lample et al., 2016). One of the main advantages
of neural networks is that they can learn the fea-
ture representations automatically from the data,
thus avoiding the laborious feature engineering
stage
Lample et al., 2016).
Given these promising results, the main goal of this
paper is to provide the ﬁrst performance investiga-
tion of popular RNNs such as the Elman and Jordan
networks and the bidirectional LSTM-CRF over
DNR tasks.

(Mesnil et al., 2015;

3 The Proposed Approach

DNR can be formulated as a joint segmentation and
classiﬁcation task over a predeﬁned set of classes.
As an example, consider the input sentence provided
in Table 1. The notation follows the widely adopted
in/out/begin (IOB) entity representation with, in this
instance, Cimetidine as the drug, ALFENTA as the
brand, and words volatile inhalation anesthetics to-
gether as the group.
In this paper, we approach
the DNR task by recurrent neural networks and we

In
therefore provide a brief description hereafter.
an RNN, each word in the input sentence is ﬁrst
mapped to a random real-valued vector of arbitrary
dimension, d. Then, a measurement for the word,
noted as x(t), is formed by concatenating the word’s
own vector with a window of preceding and follow-
ing vectors (the ”context”). An example of input
vector with a context window of size s = 3 is:
w3(t) = [Cimetidine, reduces, ef f ect],
→ xreduces ∈ Rd,
→ xCimetidine ∈ Rd,
‘ef f ect′
→ xef f ect ∈ Rd,
x(t) = [xCimetidine, xreduces, xef f ect] ∈ R3d

‘Cimetidine′

‘reduces′

(1)

where w3(t) is the context window centered around
the t-th word, ′reduces′, and xword represents the
numerical vector for word.

For the Elman network, both x(t) and the output
from the hidden layer at time t − 1, h(t − 1), are in-
put into the hidden layer for frame t. The recurrent
connection from the past time frame enables a short-
term memory, while hidden-to-hidden neuron con-
nections make the network Turing-complete. This
architecture, common in RNNs, is suitable for pre-
diction of sequences. Formally, the hidden layer is
described as:

h(t) = f (U • x(t) + V • h(t − 1))

(2)

where U and V are randomly-initialized weight ma-
trices between the input and the hidden layer, and
between the past and current hidden layers, respec-
tively. Function f (·) is the sigmoid function:

f (x) =

1
1 + e−x

(3)

that adds non-linearity to the layer. Eventually, h(t)
is input in the output layer:

y(t) = g(W • h(t)), with g(zm) =

ezm
k=1ezk

ΣK

(4)

and convolved with the output weight matrix, W .
The output is normalized by a multi-class logistic
function, g(·), to become a proper probability over
the class set. The output dimensionality is therefore
determined by the number of entity classes (i.e., 4
for the DNR task).The Jordan network is very sim-
ilar to the Elman network, except that the feedback

Sentence
Entity class

Cimetidine
B-drug

reduces
O

clearance
O

of
O

ALFENTA and
O
B-brand

volatile
B-group

inhalation
I-group

anesthetics
I-group

Table 1: Example sentence in a DNR task with entity classes represented in IOB format.

DDI-DrugBank

DDI-MedLine

Training+Test for DDI task Test for DNR Training+Test for DDI task Test for DNR

documents
sentences
drug n
group
brand
drug

730
6577
124
3832
1770
9715

54
145
6
65
53
180
Table 2: Statistics of training and test datasets used for SemEval-2013 Task 9.1.

175
1627
520
234
36
1574

58
520
115
90
6
171

is sourced from the output layer rather than the pre-
vious hidden layer:

h(t) = f (U • x(t) + V • y(t − 1)).

(5)

their

decay

recent

biases

them toward
inputs (Bengio et al., 1994).

Although the Elman and Jordan networks
ex-
can learn long-term dependencies,
their
ponential
The
most
LSTM was designed to overcome this
limi-
tation by incorporating a gated memory-cell
to capture long-range dependencies within the
In the
data (Hochreiter and Schmidhuber, 1997).
bidirectional LSTM, for any given sentence,
the
−→
h (t), and a right,
network computes both a left,
←−
h (t), representations of the sentence context at
every input, x(t). The ﬁnal representation is created
←−
h (t)]. All
by concatenating them as h(t) = [
these networks utilize the h(t) layer as an implicit
feature for entity class prediction: although this
model has proved effective in many cases, it is not
able to provide joint decoding of the outputs in a
Viterbi-style manner (e.g., an I-group cannot follow
a B-brand; etc). Thus, another modiﬁcation to the
bidirectional LSTM is the addition of a conditional
random ﬁeld (CRF) (Lafferty et al., 2001) as the
output layer to provide optimal sequential decoding.
The resulting network is commonly referred to as
the bidirectional LSTM-CRF (Lample et al., 2016).

−→
h (t);

4 Experiments

4.1 Datasets

DDIExtraction

from

The
challenge
9.1

(Segura-Bedmar et al., 2013)

shared

2013
SemEval-2013
has

task
Task
provided

a benchmark corpus for DNR and DDI extraction.
The corpus contains manually-annotated pharma-
cological substances and drug-drug interactions
for a total of 18, 502 pharmacological
(DDIs)
substances and 5, 028 DDIs.
It collates two
DDI-DrugBank and DDI-
distinct datasets:
MedLine (Herrero-Zazo et al., 2013).
Table 2
the training
summarizes the basic statistics of
and test datasets used in our experiments.
For
proper comparison, we follow the same settings
as (Segura-Bedmar et al., 2015), using the training
data of the DNR task along with the test data for the
DDI task for training and validation of DNR. We
split this joint dataset into a training and validation
sets with approximately 70% of sentences for
training and the remaining for validation.

4.2 Evaluation Methodology

Our models have been blindly evaluated on un-
seen DNR test data using the strict evaluation
the predicted en-
metrics. With this evaluation,
tities have to match the ground-truth entities ex-
actly, both in boundary and class.
To facili-
tate the replication of our experimental results,
we have used a publicly-available library for the
implementation1 (i.e., the Theano neural network
toolkit (Bergstra et al., 2010)).
The experiments
have been run over a range of values for the
hyper-parameters, using the validation set for se-
lection (Bergstra and Bengio, 2012). The hyper-
parameters include the number of hidden-layer
nodes, H ∈ {25, 50, 100}, the context window size,
s ∈ {1, 3, 5}, and the embedding dimension, d ∈

1https://github.com/raghavchalapathy/dnr

DDI-DrugBank

DDI-MedLine

Precision Recall F1 Score Precision Recall F1 Score

Methods
WBI-NER (Rockt¨aschel et al., 2013)
Hybrid-DDI (Abacha et al., 2015)
Word2Vec+DINTO (Segura-Bedmar et al., 2015)
Elman RNN
Jordan RNN
Bidirectional LSTM-CRF

88.00
93.00
69.00
79.91
77.59
87.07
Table 3: Performance comparison between the recurrent neural networks (bottom three lines) and state-of-the-art systems (top

58.10
37.00
57.00
37.78
40.06
52.75

87.80
80.00
75.00
69.13
68.25
85.19

87.00
70.00
82.00
60.91
60.91
83.39

61.00
74.00
65.00
43.23
59.47
52.93

56.00
25.00
51.00
33.56
30.20
52.57

three lines) over the SemEval-2013 Task 9.1.

Entities

DDI-DrugBank

DDI-MedLine

Precision Recall F1 Score Precision Recall F1 Score

Bidirectional LSTM-CRF

53.76
61.05
0.0
45.45
Table 4: SemEval-2013 Task 9.1 results by entity for the bidirectional LSTM-CRF.

group
drug
brand
drug n

59.52
65.22
0.0
40.20

83.33
87.50
84.85
0.0

76.92
90.59
91.30
0.0

90.91
84.62
79.25
0.0

56.50
63.06
0.0
42.67

{50, 100, 300, 500, 1000}. Two additional parame-
ters, the learning and drop-out rates, were sampled
from a uniform distribution in the range [0.05, 0.1].
The embedding and initial weight matrices were all
sampled from the uniform distribution within range
[−1, 1]. Early training stopping was set to 100
epochs to mollify over-ﬁtting, and the model that
gave the best performance on the validation set was
retained. The accuracy is reported in terms of micro-
average F1 score computed using the CoNLL score
function (Nadeau and Sekine, 2007).

4.3 Results and Analysis

Table 3 shows the performance comparison between
the explored RNNs and state-of-the-art DNR sys-
the RNNs have not
tems. As an overall note,
reached the same accuracy as the top system, WBI-
NER (Rockt¨aschel et al., 2013). However, the bidi-
rectional LSTM-CRF has achieved the second-best
score on DDI-DrugBank and the third-best on DDI-
MedLine. These results seem interesting on the
ground that the RNNs provide DNR straight from
text rather than from manually-engineered features.
Given that the RNNs learn entirely from the data, the
better performance over the DDI-DrugBank dataset
is very likely due to its larger size. Accordingly, it
is reasonable to expect higher relative performance
should larger corpora become available in the fu-
ture. Table 4 also breaks down the results by en-
tity class for the bidirectional LSTM-CRF. The low

score on the brand class for DDI-MedLine and on
the drug n class (i.e., active substances not ap-
proved for human use) for DDI-DrugBank are likely
attributable to the very small sample size (Table 2).
This issue is also shared by the state-of-the-art DNR
systems.

5 Conclusion

This paper has investigated the effectiveness of re-
current neural architectures, namely the Elman and
Jordan networks and the bidirectional LSTM-CRF,
for drug name recognition. The most appealing fea-
ture of these architectures is their ability to pro-
vide end-to-end recognition straight from text, spar-
ing effort from laborious feature construction. To
the best of our knowledge, ours is the ﬁrst pa-
per to explore RNNs for entity recognition from
pharmacological
The experimental results
over the SemEval-2013 Task 9.1 benchmarks look
promising, with the bidirectional LSTM-CRF rank-
ing closely to the state of the art. A potential way
to further improve its performance would be to ini-
tialize its training with unsupervised word embed-
dings such as Word2Vec (Mikolov et al., 2013) and
GloVe (Pennington et al., 2014). This approach has
proved effective in many other domains and still dis-
penses with expert annotation effort; we plan this
exploration for the near future.

text.

References

[Abacha et al.2015] Asma Ben Abacha, Md Faisal Mah-
bub Chowdhury, Aikaterini Karanasiou, Yassine Mra-
bet, Alberto Lavelli, and Pierre Zweigenbaum. 2015.
Text mining for pharmacovigilance: Using machine
learning for drug name recognition and drug–drug
interaction extraction and classiﬁcation.
Journal of
Biomedical Informatics, 58:122–132.

[Bengio et al.1994] Yoshua Bengio, Patrice Simard, and
Paolo Frasconi. 1994. Learning long-term dependen-
cies with gradient descent is difﬁcult. IEEE Transac-
tions on Neural Networks, 5(2):157–166.

[Bergstra and Bengio2012] James Bergstra and Yoshua
Bengio. 2012. Random search for hyper-parameter
optimization. Journal of Machine Learning Research,
13:281–305.

[Bergstra et al.2010] James Bergstra, Olivier Breuleux,
Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu,
Guillaume Desjardins, Joseph Turian, David Warde-
Farley, and Yoshua Bengio. 2010. Theano: A CPU
and GPU math compiler in Python. In The 9th Python
in Science Conference, pages 1–7.

[Collobert et al.2011] Ronan Collobert, Jason Weston,
L´eon Bottou, Michael Karlen, Koray Kavukcuoglu,
and Pavel Kuksa. 2011. Natural language processing
(almost) from scratch. Journal of Machine Learning
Research, 12:2493–2537.

[Elman1990] Jeffrey L Elman. 1990. Finding structure in

time. Cognitive Science, 14(2):179–211.
[Herrero-Zazo et al.2013] Mar´ıa Herrero-Zazo,

Isabel
Segura-Bedmar, Paloma Mart´ınez, and Thierry De-
clerck. 2013. The DDI corpus: An annotated corpus
with pharmacological
substances and drug–drug
Journal of Biomedical Informatics,
interactions.
46(5):914–920.

[Hinton et al.2012] Geoffrey Hinton, Li Deng, Dong Yu,
George E Dahl, Abdel-rahman Mohamed, Navdeep
Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick
Nguyen, Tara N Sainath, et al. 2012. Deep neural
networks for acoustic modeling in speech recognition:
The shared views of four research groups. IEEE Sig-
nal Processing Magazine, 29(6):82–97.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural Computation, 9(8):1735–1780.

[Jordan1986] Michael I. Jordan. 1986. Serial order: A
parallel distributed processing approach. Technical re-
port, San Diego: University of California, Institute for
Cognitive Science.

[Krizhevsky et al.2012] Alex Krizhevsky, Ilya Sutskever,
and Geoffrey E Hinton. 2012. Imagenet classiﬁcation
with deep convolutional neural networks.
In NIPS,
pages 1097–1105.

[Lafferty et al.2001] John Lafferty, Andrew McCallum,
and Fernando Pereira. 2001. Conditional random
ﬁelds: Probabilistic models for segmenting and label-
ing sequence data. In ICML, pages 282–289.

[Lample et al.2016] Guillaume Lample, Miguel Balles-
teros, Sandeep Subramanian, Kazuya Kawakami, and
Chris Dyer. 2016. Neural architectures for named en-
tity recognition. In NAACL-HLT.

[Liu et al.2015a] Shengyu Liu, Buzhou Tang, Qingcai
Chen, and Xiaolong Wang.
2015a. Drug name
recognition: Approaches and resources. Information,
6(4):790–810.

[Liu et al.2015b] Shengyu Liu, Buzhou Tang, Qingcai
Chen, Xiaolong Wang, and Xiaoming Fan. 2015b.
Feature engineering for drug name recognition in
biomedical texts: Feature conjunction and feature se-
lection. Computational and Mathematical Methods in
Medicine, 2015:1–9.

[Mesnil et al.2015] Gr´egoire Mesnil, Yann Dauphin,
Kaisheng Yao, Yoshua Bengio, Li Deng, Dilek
Hakkani-Tur, Xiaodong He, Larry Heck, Gokhan Tur,
Dong Yu, et al. 2015. Using recurrent neural net-
works for slot ﬁlling in spoken language understand-
ing. IEEE/ACM Transactions on Audio, Speech, and
Language Processing, 23(3):530–539.

[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever, Kai
Chen, Greg S Corrado, and Jeff Dean. 2013. Dis-
tributed representations of words and phrases and their
compositionality. In NIPS, pages 3111–3119.

[Nadeau and Sekine2007] David Nadeau and Satoshi
Sekine. 2007. A survey of named entity recogni-
tion and classiﬁcation. Linguisticae Investigationes,
30(1):3–26.

[Pennington et al.2014] Jeffrey

Richard
Socher, and Christopher D. Manning. 2014. GloVe:
Global vectors for word representation.
In EMNLP,
pages 1532–1543.

Pennington,

[Rockt¨aschel et al.2012] Tim Rockt¨aschel, Michael Wei-
dlich, and Ulf Leser. 2012. ChemSpot: A hybrid sys-
tem for chemical named entity recognition. Bioinfor-
matics, 28(12):1633–1640.

[Rockt¨aschel et al.2013] Tim Rockt¨aschel, Torsten Hu-
ber, Michael Weidlich, and Ulf Leser. 2013. WBI-
NER: The impact of domain-speciﬁc features on the
performance of identifying and classifying mentions
In The 7th International Workshop on Se-
of drugs.
mantic Evaluation, pages 356–363.

[Segura-Bedmar et al.2013] Isabel

Segura-Bedmar,
Paloma Mart´ınez, and Mar´ıa Herrero Zazo.
2013.
Semeval-2013 task 9: Extraction of drug-drug inter-
actions from biomedical texts (DDIExtraction 2013).
In The 7th International Workshop on Semantic
Evaluation.

[Segura-Bedmar et al.2015] Isabel

Segura-Bedmar,
Vıctor Su´arez-Paniagua, and Paloma Martınez. 2015.
Exploring word embedding for drug name recogni-
tion.
In The 6th International Workshop on Health
Text Mining and Information Analysis, page 64.

