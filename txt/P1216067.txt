Unsupervised Learning of Syntactic Structure
with Invertible Neural Projections

Junxian He Graham Neubig Taylor Berg-Kirkpatrick
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
{junxianh, gneubig, tberg}@cs.cmu.edu

8
1
0
2
 
g
u
A
 
8
2
 
 
]
L
C
.
s
c
[
 
 
1
v
1
1
1
9
0
.
8
0
8
1
:
v
i
X
r
a

Abstract

Unsupervised learning of syntactic structure is
typically performed using generative models
with discrete latent variables and multinomial
parameters. In most cases, these models have
not
leveraged continuous word representa-
tions. In this work, we propose a novel gener-
ative model that jointly learns discrete syntac-
tic structure and continuous word representa-
tions in an unsupervised fashion by cascading
an invertible neural network with a structured
generative prior. We show that the invertibility
condition allows for efﬁcient exact inference
and marginal likelihood computation in our
model so long as the prior is well-behaved. In
experiments we instantiate our approach with
both Markov and tree-structured priors, eval-
uating on two tasks: part-of-speech (POS) in-
duction, and unsupervised dependency parsing
without gold POS annotation. On the Penn
Treebank, our Markov-structured model sur-
passes state-of-the-art results on POS induc-
tion. Similarly, we ﬁnd that our tree-structured
model achieves state-of-the-art performance
on unsupervised dependency parsing for the
difﬁcult training condition where neither gold
POS annotation nor punctuation-based con-
straints are available.1

1

Introduction

Data annotation is a major bottleneck for the appli-
cation of supervised learning approaches to many
problems. As a result, unsupervised methods that
learn directly from unlabeled data are increasingly
important. For tasks related to unsupervised syn-
tactic analysis, discrete generative models have
dominated in recent years – for example, for both
part-of-speech (POS) induction
(Blunsom and
Cohn, 2011; Stratos et al., 2016) and unsuper-
vised dependency parsing (Klein and Manning,

1Code is available at https://github.com/jxhe/struct-

learning-with-ﬂow.

(a) Traditional pre-trained
skip-gram embeddings

(b) Learned latent embedd-
ings from our approach

Figure 1: Visualization (t-SNE) of skip-gram embeddings
(trained on one billion words with context window size equal
to 1) and latent embeddings learned by our approach with a
Markov-structured prior. Each node represents a word and is
colored according to the most likely gold POS tag from the
Penn Treebank (best seen in color).

2004; Cohen and Smith, 2009; Pate and Johnson,
2016). While similar models have had success on
a range of unsupervised tasks, they have mostly ig-
nored the apparent utility of continuous word rep-
resentations evident from supervised NLP appli-
cations (He et al., 2017; Peters et al., 2018).
In
this work, we focus on leveraging and explicitly
representing continuous word embeddings within
unsupervised models of syntactic structure.

Pre-trained word embeddings from massive un-
labeled corpora offer a compact way of inject-
ing a prior notion of word similarity into mod-
els that would otherwise treat words as discrete,
isolated categories. However, the speciﬁc prop-
erties of language captured by any particular em-
bedding scheme can be difﬁcult to control, and,
further, may not be ideally suited to the task at
hand. For example, pre-trained skip-gram em-
beddings (Mikolov et al., 2013) with small con-
text window size are found to capture the syntac-
tic properties of language well (Bansal et al., 2014;
Lin et al., 2015). However, if our goal is to sepa-
rate syntactic categories, this embedding space is
not ideal – POS categories correspond to overlap-

Figure 2: Depiction of proposed generative model. The syntax model is composed of discrete random variables, zi. Each ei
is a latent continuous embeddings sampled from Gaussian distribution conditioned on zi, while xi is the observed embedding,
deterministically derived from ei. The left portion depicts how the neural projector maps the simple Gaussian to a more
complex distribution in the output space. The right portion depicts two instantiations of the syntax model in our approach: one
is Markov-structured and the other is DMV-structured. For DMV, ztree is the latent dependency tree structure.

ping interspersed regions in the embedding space,
evident in Figure 1(a).

In our approach, we propose to learn a new
latent embedding space as a projection of pre-
trained embeddings (depicted in Figure 1(b)),
while jointly learning latent syntactic structure –
for example, POS categories or syntactic depen-
dencies. To this end, we introduce a new gener-
ative model (shown in Figure 2) that ﬁrst gener-
ates a latent syntactic representation (e.g. a de-
pendency parse) from a discrete structured prior
(which we also call the “syntax model”), then,
conditioned on this representation, generates a se-
quence of latent embedding random variables cor-
responding to each word, and ﬁnally produces the
observed (pre-trained) word embeddings by pro-
jecting these latent vectors through a parameter-
ized non-linear function. The latent embeddings
can be jointly learned with the structured syntax
model in a completely unsupervised fashion.

By choosing an invertible neural network as
our non-linear projector, and then parameterizing
our model in terms of the projection’s inverse,
we are able to derive tractable exact inference
and marginal likelihood computation procedures
so long as inference is tractable in the underlying
syntax model. In §3.1 we show that this derivation
corresponds to an alternate view of our approach
whereby we jointly learn a mapping of observed
word embeddings to a new embedding space that
is more suitable for the syntax model, but include
an additional Jacobian regularization term to pre-
vent information loss.

Recent work has sought

to take advantage
of word embeddings in unsupervised generative

models with alternate approaches (Lin et al., 2015;
Tran et al., 2016; Jiang et al., 2016; Han et al.,
2017). Lin et al. (2015) build an HMM with Gaus-
sian emissions on observed word embeddings, but
they do not attempt to learn new embeddings. Tran
et al. (2016), Jiang et al. (2016), and Han et al.
(2017) extend HMM or dependency model with
valence (DMV) (Klein and Manning, 2004) with
multinomials that use word (or tag) embeddings
in their parameterization. However, they do not
represent the embeddings as latent variables.

In experiments, we instantiate our approach us-
ing both a Markov-structured syntax model and
a tree-structured syntax model – speciﬁcally, the
DMV. We evaluate on two tasks: part-of-speech
(POS) induction and unsupervised dependency
parsing without gold POS tags. Experimental re-
sults on the Penn Treebank (Marcus et al., 1993)
demonstrate that our approach improves the ba-
sic HMM and DMV by a large margin,
lead-
ing to the state-of-the-art results on POS induc-
tion, and state-of-the-art results on unsupervised
dependency parsing in the difﬁcult training sce-
nario where neither gold POS annotation nor
punctuation-based constraints are available.

2 Model

As an illustrative example, we ﬁrst present a base-
line model for Markov syntactic structure (POS in-
duction) that treats a sequence of pre-trained word
embeddings as observations. Then, we propose
our novel approach, again using Markov structure,
that introduces latent word embedding variables
and a neural projector. Lastly, we extend our ap-
proach to more general syntactic structures.

2.1 Example: Gaussian HMM

fφ(ei). The joint distribution of our model is:

We start by describing the Gaussian hidden
Markov model introduced by Lin et al. (2015),
which is a locally normalized model with multi-
nomial transitions and Gaussian emissions. Given
a sentence of length (cid:96), we denote the latent POS
tags as z = {zi}(cid:96)
i=1, observed (pre-trained) word
embeddings as x = {xi}(cid:96)
i=1, transition parame-
ters as θ, and Gaussian emission parameters as η.
The joint distribution of data and latent variables
factors as:

p(z, x; θ, η) =

pθ(zi|zi−1)pη(xi|zi),

(1)

(cid:89)(cid:96)

i=1

where pθ(zi|zi−1) is the multinomial transition
probability and pη(xi|zi) is the multivariate Gaus-
sian emission probability.

While the observed word embeddings do inform
this model with a notion of word similarity – lack-
ing in the basic multinomial HMM – the Gaussian
emissions may not be sufﬁciently ﬂexible to sepa-
rate some syntactic categories in the complex pre-
trained embedding space – for example the skip-
gram embedding space as visualized in Figure 1(a)
where different POS categories overlap. Next we
introduce a new approach that adds ﬂexibility to
the emission distribution by incorporating new la-
tent embedding variables.

2.2 Markov Structure with Neural Projector

To ﬂexibly model observed embeddings and yield
a new representation space that is more suitable
for the syntax model, we propose to cascade a neu-
ral network as a projection function, deterministi-
cally transforming the simple space deﬁned by the
Gaussian HMM to the observed embedding space.
We denote the latent embedding of the ith word in
a sentence as ei ∈ Rde, and the neural projection
function as f , parameterized by φ. In the case of
sequential Markov structure, our new model cor-
responds to the following generative process:

For each time step i = 1, 2, · · · , (cid:96),

• Draw the latent state zi ∼ pθ(zi|zi−1)
• Draw the latent embedding ei ∼ N (µzi, Σzi)
• Deterministically produce embedding

xi = fφ(ei)

The graphical model is depicted in Figure 2. The
deterministic projection can also be viewed as
sampling each observation from a point mass at

p(z, e, x; θ, η, φ)

(cid:89)(cid:96)

=

i=1

[pθ(zi|zi−1)pη(ei|zi)pφ(xi|ei)],

(2)

where pη(·|zi) is a conditional Gaussian distribu-
tion, and pφ(xi|ei) is the Dirac delta function cen-
tered at fφ(ei):

pφ(xi|ei) = δ(xi −fφ(ei)) =

(cid:40)

∞ xi = fφ(ei)
0 otherwise

(3)

2.3 General Structure with Neural Projector

Our approach can be applied to a broad family of
structured syntax models. We denote latent em-
bedding variables as e = {ei}(cid:96)
i=1, discrete latent
variables in the syntax model as z = {zk}K
k=1
(K (cid:62) (cid:96)), where z1, z2, . . . , z(cid:96) are conditioned to
generate e1, e2, . . . , e(cid:96). The joint probability of
our model factors as:

p(z, e, x; θ, η, φ) =

(cid:89)(cid:96)

i=1

(cid:2)pη(ei|zi)pφ(xi|ei)(cid:3)
(4)

· psyntax(z; θ),

where psyntax(z; θ) represents the probability of
the syntax model, and can encode any syntactic
structure – though, its factorization structure will
determine whether inference is tractable in our full
model. As shown in Figure 2, we focus on two
syntax models for syntactic analysis in this paper.
The ﬁrst is Markov-structured, which we use for
POS induction, and the second is DMV-structured,
which we use to learn dependency parses without
supervision.

The marginal data likelihood of our model is:

p(x) =

psyntax(z; θ)

(cid:88)

(cid:16)

z
(cid:89)(cid:96)

·

i=1

(cid:90)

(cid:2)

ei

(cid:124)

pη(ei|zi)pφ(xi|ei)dei

(cid:123)(cid:122)
p(xi|zi)

(cid:3)(cid:17)

.

(cid:125)

(5)

While the discrete variables z can be marginal-
ized out with dynamic program in many cases, it
is generally intractable to marginalize out the la-
tent continuous variables, ei, for an arbitrary pro-
jection f in Eq. (5), which means inference and
learning may be difﬁcult. In §3, we address this
issue by constraining f to be invertible, and show
that this constraint enables tractable exact infer-
ence and marginal likelihood computation.

∂f −1
φ
∂xi

(cid:12)det

at xi, and (cid:12)
(cid:12)
(cid:12) represents the absolute value
of its determinant. This Jacobian term is nonzero
and differentiable if and only if f −1

φ exists.

Eq. (6) shows that we can directly calculate the
marginal emission distribution p(xi|zi). Denote
the marginal data likelihood of Gaussian HMM as
pHMM(x), then the log marginal data likelihood of
our model can be directly written as:
log p(x) = log pHMM(f −1

φ (x))
(cid:12)
(cid:12)
(cid:12)det

log

∂f −1
φ
∂xi

(cid:12)
(cid:12)
(cid:12),

(cid:88)(cid:96)

+

i=1

(7)

where f −1
φ (x) represents the new sequence of em-
beddings after applying f −1
φ to each xi. Eq. (7)
shows that the training objective of our model is
simply the Gaussian HMM log likelihood with an
additional Jacobian regularization term. From this
view, our approach can be seen as equivalent to
reversely projecting the data through f −1
φ to an-
other manifold e that is directly modeled by the
Gaussian HMM, with a regularization term.
In-
tuitively, we optimize the reverse projection f −1
φ
to modify the e space, making it more appropri-
ate for the syntax model. The Jacobian regular-
ization term accounts for the volume expansion or
contraction behavior of the projection. Maximiz-
ing it can be thought of as preventing information
loss. In the extreme case, the Jacobian determi-
nant is equal to zero, which means the projection
is non-invertible and thus information is being lost
through the projection. Such “information pre-
serving” regularization is crucial during optimiza-
tion, otherwise the trivial solution of always pro-
jecting data to the same single point to maximize
likelihood is viable.2

More generally, for an arbitrary syntax model

the data likelihood of our approach is:

p(x) =

psyntax(z)

(cid:88)

(cid:16)

z

(cid:89)(cid:96)

·

i=1

pη(f −1

(cid:12)
(cid:12)
φ (xi)|zi)
(cid:12)det

(8)

∂f −1
φ
∂xi

(cid:17)

.

(cid:12)
(cid:12)
(cid:12)

3 Learning & Inference

φ exists.

In this section, we introduce an invertibility con-
dition for our neural projector to tackle the op-
timization challenge. Speciﬁcally, we constrain
our neural projector with two requirements: (1)
dim(x) = dim(e) and (2) f −1
Invert-
ible transformations have been explored before
in independent components analysis (Hyv¨arinen
et al., 2004), gaussianization (Chen and Gopinath,
2001), and deep density models (Dinh et al., 2014,
2016; Kingma and Dhariwal, 2018), for unstruc-
tured data. Here, we generalize this style of ap-
proach to structured learning, and augment it with
discrete latent variables (zi). Under the invertibil-
ity condition, we derive a learning algorithm and
give another view of our approach revealed by the
objective function. Then, we present the architec-
ture of a neural projector we use in experiments: a
volume-preserving invertible neural network pro-
posed by Dinh et al. (2014) for independent com-
ponents estimation.

3.1 Learning with Invertibility

For ease of exposition, we explain the learning
algorithm in terms of Markov structure without
loss of generality. As shown in Eq. (5), the op-
timization challenge in our approach comes from
the intractability of the marginalized emission fac-
tor p(xi|zi).
If we can marginalize out ei and
compute p(xi|zi), then the posterior and marginal
likelihood of our Markov-structured model can be
computed with the forward-backward algorithm.
We can apply Eq. (3) and obtain :

p(xi|zi; η, φ) =

pη(ei|zi)δ(xi − fφ(ei))dei.

(cid:90)

ei

By using the change of variable rule to the integra-
tion, which allows the integration variable ei to be
replaced by x(cid:48)
i = fφ(ei), the marginal emission
factor can be computed in closed-form when the
invertibility condition is satisﬁed:

p(xi|zi; η, φ)

(cid:90)

=

x(cid:48)
i

pη(f −1

φ (x(cid:48)

(cid:12)
i)|zi)δ(xi − x(cid:48)
(cid:12)
i)
(cid:12)det

∂f −1
φ
∂x(cid:48)
i

(cid:12)
(cid:12)dx(cid:48)
(cid:12)

i

= pη(f −1

(cid:12)
(cid:12)
φ (xi)|zi)
(cid:12)det

∂f −1
φ
∂xi

(cid:12)
(cid:12)
(cid:12),

If the syntax model itself allows for tractable in-
ference and marginal likelihood computation, the
same dynamic program can be used to marginal-
ize out z. Therefore, our joint model inherits the
tractability of the underlying syntax model.

(6)

where pη(·|z) is a conditional Gaussian distribu-
is the Jacobian matrix of function f −1
φ

tion,

∂f −1
φ
∂xi

2For example, all ei could learn to be zero vectors, lead-
ing to the trivial solution of learning zero mean and zero vari-
ance Gaussian emissions achieving inﬁnite data likelihood.

angular with all ones on the main diagonal. Thus
the Jacobian determinant is always equal to one
(i.e. volume-preserving) and the invertibility con-
dition is naturally satisﬁed.

To be sufﬁciently expressive, we compose mul-
tiple coupling layers as suggested in Dinh et al.
(2014). Speciﬁcally, we exchange the role of
left and right half vectors at each layer as shown
in Figure 3. For instance, from xi to h(1)
the
left subset xi,l is unchanged, while from h(1)
to
h(2)
i,r remains the same. Also
i
note that composing multiple coupling layers does
not change the volume-preserving and invertibility
properties. Such a sequence of invertible transfor-
mations from the data space x to e is also called
normalizing ﬂow (Rezende and Mohamed, 2015).

the right subset h(1)

i

i

4 Experiments

In this section, we ﬁrst describe our datasets and
experimental setup. We then instantiate our ap-
proach with Markov and DMV-structured syntax
models, and report results on POS tagging and de-
pendency grammar induction respectively. Lastly,
we analyze the learned latent embeddings.

4.1 Data

For both POS tagging and dependency parsing, we
run experiments on the Wall Street Journal (WSJ)
portion of the Penn Treebank.3 To create the ob-
served data embeddings, we train skip-gram word
embeddings (Mikolov et al., 2013) that are found
to capture syntactic properties well when trained
with small context window (Bansal et al., 2014;
Lin et al., 2015). Following Lin et al. (2015), the
dimensionality dx is set to 100, and the training
context window size is set to 1 to encode more
syntactic information. The skip-gram embeddings
are trained on the one billion word language mod-
eling benchmark dataset (Chelba et al., 2013) in
addition to the WSJ corpus.

4.2 General Experimental Setup

For the neural projector, we employ rectiﬁed net-
works as coupling function g following Dinh et al.
(2014). We use a rectiﬁed network with an input
layer, one hidden layer, and linear output units,
the number of hidden units is set to the same as
the number of input units. The number of cou-
pling layers are varied as 4, 8, 16 for both tasks.

3Preprocessing is different for the two tasks, we describe

the details in the following subsections.

Figure 3: Depiction of the architecture of the inverse pro-
jection f −1
φ that composes multiple volume-preserving cou-
pling layers, with which we parameterize our model. On
the right, we schematically depict how the inverse projection
transforms the observed word embedding xi to a point ei in
a new embedding space.

3.2

Invertible Volume-Preserving Neural Net

For the projection we can use an arbitrary invert-
ible function, and given the representational power
of neural networks they seem a natural choice.
However, calculating the inverse and Jacobian of
an arbitrary neural network can be difﬁcult, as it
requires that all component functions be invert-
ible and also requires storage of large Jacobian
matrices, which is memory intensive. To address
this issue, several recent papers propose specially
designed invertible networks that are easily train-
able yet still powerful (Dinh et al., 2014, 2016;
Jacobsen et al., 2018). Inspired by these works,
we use the invertible transformation proposed by
Dinh et al. (2014), which consists of a series of
“coupling layers”. This architecture is specially
designed to guarantee a unit Jacobian determinant
(and thus the invertibility property).

From Eq. (8) we know that only f −1

φ is re-
quired for accomplishing learning and inference;
we never need to explicitly construct fφ. Thus, we
directly deﬁne the architecture of f −1
φ . As shown
in Figure 3, the nonlinear transformation from the
observed embedding xi to h(1)
represents the ﬁrst
coupling layer. The input in this layer is parti-
tioned into left and right halves of dimensions, xi,l
and xi,r, respectively. A single coupling layer is
deﬁned as:

i

h(1)

i,l = xi,l,

h(1)

i,r = xi,r + g(xi,l),

(9)

where g : Rdx/2 → Rdx/2 is the coupling func-
tion and can be any nonlinear form. This transfor-
mation satisﬁes dim(h(1)) = dim(x), and Dinh
et al. (2014) show that its Jacobian matrix is tri-

We optimize marginal data likelihood directly us-
ing Adam (Kingma and Ba, 2014). For both tasks
in the fully unsupervised setting, we do not tune
the hyper-parameters using supervised data.

4.3 Unsupervised POS tagging

For unsupervised POS tagging, we use a Markov-
structured syntax model in our approach, which
is a popular structure for unsupervised tagging
tasks (Lin et al., 2015; Tran et al., 2016).

Setup. Following existing literature, we train
and test on the entire WSJ corpus (49208 sen-
tences, 1M tokens). We use 45 tag clusters, the
number of POS tags that appear in WSJ cor-
pus. We train the discrete HMM and the Gaus-
sian HMM (Lin et al., 2015) as baselines. For the
Gaussian HMM, mean vectors of Gaussian emis-
sions are initialized with the empirical mean of all
word vectors with an additive noise. We assume
diagonal covariance matrix for p(ei|zi) and initial-
ize it with the empirical variance of the word vec-
tors. Following Lin et al. (2015), the covariance
matrix is ﬁxed during training. The multinomial
probabilities are initialized as θkv ∝ exp(ukv),
where ukv ∼ U [0, 1]. For our approach, we
initialize the syntax model and Gaussian param-
eters with the pre-trained Gaussian HMM. The
weights of layers in the rectiﬁed network are ini-
tialized from a uniform distribution with mean
zero and a standard deviation of (cid:112)1/nin, where
nin is the input dimension.4 We evaluate the per-
formance of POS tagging with both Many-to-One
(M-1) accuracy (Johnson, 2007) and V-Measure
(VM) (Rosenberg and Hirschberg, 2007). Given
a model we found that the tagging performance is
well-correlated with the training data likelihood,
thus we use training data likelihood as a unsuper-
vised criterion to select the trained model over 10
random restarts after training 50 epochs. We re-
peat this process 5 times and report the mean and
standard deviation of performance.

Results. We compare our approach with ba-
sic HMM, Gaussian HMM, and several state-
of-the-art systems, including sophisticated HMM
variants and clustering techniques with hand-
engineered features. The results are presented in
Table 1. Through the introduced latent embed-
dings and additional neural projection, our ap-
proach improves over the Gaussian HMM by 5.4
points in M-1 and 5.6 points in VM. Neural HMM

4This is the default parameter initialization in PyTorch.

System

M-1
w/o hand-engineered features
62.7
77.5
59.8
74.1
79.1
75.4 (1.0)
79.5 (0.9)
80.8 (1.3)
73.2 (4.3)

Discrete HMM
PYP-HMM (Blunsom and Cohn, 2011)
NHMM (basic) (Tran et al., 2016)
NHMM (+ Conv) (Tran et al., 2016)
NHMM (+ Conv & LSTM) (Tran et al., 2016)
Gaussian HMM (Lin et al., 2015)
Ours (4 layers)
Ours (8 layers)
Ours (16 layers)

VM

53.8
69.8
54.2
66.1
71.7
68.5 (0.5)
73.0 (0.7)
74.1 (0.7)
70.5 (2.1)

Feature HMM (Berg-Kirkpatrick et al., 2010)
Brown (+ proto) (Christodoulopoulos et al., 2010)
Cluster (word-based) (Yatbaz et al., 2012)
Cluster (token-based) (Yatbaz et al., 2014)

w/ hand-engineered features
75.5
76.1
80.2
79.5

–
68.8
72.1
69.1

Table 1: Unsupervised POS tagging results on entire WSJ,
compared with other baselines and state-of-the-art systems.
Standard deviation is given in parentheses when available.

(a) Gaussian HMM

(b) Our approach

Figure 4: Normalized Confusion matrix for POS tagging ex-
periments, row label represents the gold tag.

(NHMM) (Tran et al., 2016) is a baseline that also
learns word representation jointly. Both their ba-
sic model and extended Conv version does not
outperform the Gaussian HMM. Their best model
incorporates another LSTM to model long dis-
tance dependency and breaks the Markov assump-
tion, yet our approach still achieves substantial im-
provement over it without considering more con-
text information. Moreover, our method outper-
forms the best published result that beneﬁts from
hand-engineered features (Yatbaz et al., 2012) by
2.0 points on VM.

Confusion Matrix. We found that most tagging
errors happen in noun subcategories. Therefore,
we do the one-to-one mapping between gold POS
tags and induced clusters and plot the normalized
confusion matrix of noun subcategories in Fig-
ure 4. The Gaussian HMM fails to identify “NN”
and “NNS” correctly for most cases, and it often
recognizes “NNPS” as “NNP”. In contrast, our ap-
proach corrects these errors well.

4.4 Unsupervised Dependency Parsing

System

(cid:54) 10

all

without gold POS tags

For the task of unsupervised dependency parse in-
duction, we employ the Dependency Model with
Valence (DMV) (Klein and Manning, 2004) as the
syntax model in our approach. DMV is a genera-
tive model that deﬁnes a probability distribution
over dependency parse trees and syntactic cate-
gories, generating tokens and dependencies in a
head-outward fashion. While, traditionally, DMV
is trained using gold POS tags as observed syntac-
tic categories, in our approach, we treat each tag
as a latent variable, as described in §2.3.

Most existing approaches to this task are not
fully unsupervised since they rely on gold POS
tags following the original experimental setup for
DMV. This is partially because automatically pars-
ing from words is difﬁcult even when using un-
supervised syntactic categories (Spitkovsky et al.,
2011a). However, inducing dependencies from
words alone represents a more realistic exper-
imental condition since gold POS tags are of-
ten unavailable in practice. Previous work that
has trained from words alone often requires ad-
ditional linguistic constraints (like sentence inter-
nal boundaries) (Spitkovsky et al., 2011a,b, 2012,
2013), acoustic cues (Pate and Goldwater, 2013),
additional training data (Pate and Johnson, 2016),
or annotated data from related languages (Cohen
et al., 2011). Our approach is naturally designed
to train on word embeddings directly, thus we at-
tempt to induce dependencies without using gold
POS tags or other extra linguistic information.

Setup. Like previous work we use sections 02-
21 of WSJ corpus as training data and evaluate
on section 23, we remove punctuations and train
the models on sentences of length (cid:54) 10, “head-
percolation” rules (Collins, 1999) are applied to
obtain gold dependencies for evaluation. We train
basic DMV, extended DMV (E-DMV) (Head-
den III et al., 2009) and Gaussian DMV (which
treats POS tag as unknown latent variables and
generates observed word embeddings directly
conditioned on them following Gaussian distri-
bution) as baselines. Basic DMV and E-DMV
are trained with Viterbi EM (Spitkovsky et al.,
2010) on unsupervised POS tags induced from
our Markov-structured model described in §4.3.
Multinomial parameters of the syntax model in
both Gaussian DMV and our model are initial-
ized with the pre-trained DMV baseline. Other

w/o gold POS tags

(Spitkovsky et al., 2013)

DMV (Klein and Manning, 2004)
E-DMV (Headden III et al., 2009)
UR-A E-DMV (Tu and Honavar, 2012)
CS∗
Neural E-DMV (Jiang et al., 2016)
CRFAE (Cai et al., 2017)
Gaussian DMV
Ours (4 layers)
Ours (8 layers)
Ours (16 layers)

49.6
52.1
58.9
72.0∗
55.3
37.2
55.4 (1.3)
58.4 (1.9)
60.2 (1.3)
54.1 (8.5)

35.8
38.2
46.1
64.4∗
42.7
29.5
43.1 (1.2)
46.2 (2.3)
47.9 (1.2)
43.9 (5.7)

w/ gold POS tags (for reference only)

DMV (Klein and Manning, 2004)
UR-A E-DMV (Tu and Honavar, 2012)
MaxEnc (Le and Zuidema, 2015)
Neural E-DMV (Jiang et al., 2016)
CRFAE (Cai et al., 2017)
L-NDMV (Big training data) (Han et al., 2017)

55.1
71.4
73.2
72.5
71.7
77.2

39.7
57.0
65.8
57.6
55.7
63.2

Table 2: Directed dependency accuracy on section 23 of
WSJ, evaluating on sentences of length (cid:54) 10 and all lengths.
Starred entries (∗) denote that the system beneﬁts from ad-
ditional punctuation-based constraints. Standard deviation is
given in parentheses when available.
parameters are initialized in the same way as in
the POS tagging experiment. The directed depen-
dency accuracy (DDA) is used for evaluation and
we report accuracy on sentences of length (cid:54) 10
and all lengths. We train the parser until training
data likelihood converges, and report the mean and
standard deviation over 20 random restarts.

Comparison with other related work. Our
model directly observes word embeddings and
does not require gold POS tags during training.
Thus, results from related work trained on gold
tags are not directly comparable. However, to
measure how these systems might perform with-
out gold tags, we run three recent state-of-the-
art systems in our experimental setting: UR-
A E-DMV (Tu and Honavar, 2012), Neural E-
DMV (Jiang et al., 2016), and CRF Autoencoder
(CRFAE) (Cai et al., 2017).5 We use unsupervised
POS tags (induced from our Markov-structured
model) in place of gold tags.6 We also train ba-
sic DMV on gold tags and include several state-
of-the-art results on gold tags as reference points.

Results. As shown in Table 2, our approach
is able to improve over the Gaussian DMV by
4.8 points on length (cid:54) 10 and 4.8 points on all

5For the three systems, we use implementations from the
original papers (via personal correspondence with the au-
thors), and tune their hyperparameters on section 22 of WSJ.
6Using words directly is not practical because these sys-
tems often require a transition probability matrix between in-
put symbols, which requires too much memory.

System
Ours (4 layers)
Ours (8 layers)
Ours (16 layers)

M-1
78.2
72.5
67.2

VM
71.2
69.7
69.2

Table 3: Unsupervised POS tagging results of our approach
on WSJ, with random initialization of syntax model.

lengths, which suggests the additional latent em-
bedding layer and neural projector are helpful.
The proposed approach yields, to the best of our
knowledge,7 state-of-the-art performance with-
out gold POS annotation and without sentence-
internal boundary information. DMV, UR-A E-
DMV, Neural E-DMV, and CRFAE suffer a large
decrease in performance when trained on unsu-
pervised tags – an effect also seen in previous
work (Spitkovsky et al., 2011a; Cohen et al.,
2011). Since our approach induces latent POS
tags jointly with dependency trees, it may be able
to learn POS clusters that are more amenable to
grammar induction than the unsupervised tags.
We observe that CRFAE underperforms its gold-
tag counterpart substantially. This may largely be
a result of the model’s reliance on prior linguistic
rules that become unavailable when gold POS tag
types are unknown. Many extensions to DMV can
be considered orthogonal to our approach – they
essentially focus on improving the syntax model.
It is possible that incorporating these more sophis-
ticated syntax models into our approach may lead
to further improvements.

4.5 Sensitivity Analysis

Impact of Initialization.
In the above experi-
ments we initialize the structured syntax compo-
nents with the pre-trained Gaussian or discrete
baseline, which is shown as a useful technique
to help train our deep models. We further study
the results with fully random initialization. In the
POS tagging experiment, we report the results in
Table 3. While the performance with 4 layers is
comparable to the pre-trained Gaussian initializa-
tion, deeper projections (8 or 16 layers) result in a
dramatic drop in performance. This suggests that
the structured syntax model with very deep projec-
tions is difﬁcult to train from scratch, and a simpler
projection might be a good compromise in the ran-
dom initialization setting.

Different from the Markov prior in POS tag-

7We tried to be as thorough as possible in evaluation
by running top performing systems using our more difﬁcult
training setup when this was feasible – but it was not possible
to evaluate them all.

Table 4: Unsupervised POS tagging results on WSJ, with
fastText vectors as the observed embeddings.

System
Gaussian HMM
Ours (4 layers)
Ours (8 layers)
Ours (16 layers)

System
Gaussian DMV
Ours (4 layers)
Ours (8 layers)
Ours (16 layers)

M-1
72.0
76.4
76.8
67.3

(cid:54) 10
53.6
56.9
57.1
52.9

VM
65.0
69.3
69.4
62.0

all
41.3
43.9
42.3
39.5

Table 5: Directed dependency accuracy on section 23 of
WSJ, with fastText vectors as the observed embeddings.

ging experiments, our parsing model seems to be
quite sensitive to the initialization. For example,
directed accuracy of our approach on sentences of
length (cid:54) 10 is below 40.0 with random initializa-
tion. This is consistent with previous work that has
noted the importance of careful initialization for
DMV-based models such as the commonly used
harmonic initializer (Klein and Manning, 2004).
However, it is not straightforward to apply the har-
monic initializer for DMV directly in our model
without using some kind of pre-training since we
do not observe gold POS.

Impact of Observed Embeddings. We investi-
gate the effect of the choice of pre-trained embed-
ding on performance while using our approach.
To this end, we additionally include results us-
ing fastText embeddings (Bojanowski et al., 2017)
– which, in contrast with skip-gram embeddings,
include character-level information. We set the
context windows size to 1 and the dimension size
to 100 as in the skip-gram training, while keep-
ing other parameters set to their defaults. These
results are summarized in Table 4 and Table 5.
While fastText embeddings lead to reduced perfor-
mance with our model, our approach still yields an
improvement over the Gaussian baseline with the
new observed embeddings space.

4.6 Qualitative Analysis of Embeddings

We perform qualitative analysis to understand how
the latent embeddings help induce syntactic struc-
tures. First we ﬁlter out low-frequency words and
punctuations in WSJ, and visualize the rest words
(10k) with t-SNE (Maaten and Hinton, 2008) un-
der different embeddings. We assign each word
with its most likely gold POS tags in WSJ and
color them according to the gold POS tags.

Target
come

singing

cigars

newer

fanciest

Skip-gram
go came follow
coming sit
dancing sing
drumming dance
dances
cigarettes sodas
champagne cigar
rum
ﬂashier fancier
conventional low-end
new-generation
priciest up-scale
loveliest fancier
high-end

Markov Structure
be go do give
follow
dancing drumming
marching playing
recording
sodas bottles
drinks pills
cigarettes
softer lighter
thinner darker
smoother
liveliest priciest
smartest best-run
fastest-growing

Table 6: Target words and their 5 nearest neighbors, based
on skip-gram embeddings and our learned latent embeddings
with Markov-structured syntax model.

Figure 5: Visualization (t-SNE) of learned latent embed-
dings with DMV-structured syntax model. Each node rep-
resents a word and is colored according to the most likely
gold POS tag in the Penn Treebank (best seen in color).

For our Markov-structured model, we have dis-
played the embedding space in Figure 1(b), where
the gold POS clusters are well-formed. Further,
we present ﬁve example target words and their ﬁve
nearest neighbors in terms of cosine similarity. As
shown in Table 6, the skip-gram embedding cap-
tures both semantic and syntactic aspects to some
degree, yet our embeddings are able to focus es-
pecially on the syntactic aspects of words, in an
unsupervised fashion without using any extra mor-
phological information.

In Figure 5 we depict the learned latent em-
beddings with the DMV-structured syntax model.
Unlike the Markov structure, the DMV structure
maps a large subset of singular and plural nouns to
the same overlapping region. However, two clus-
ters of singular and plural nouns are actually sepa-
rated. We inspect the two clusters and the overlap-
ping region in Figure 5, it turns out that the nouns
in the separated clusters are words that can appear
as subjects and, therefore, for which verb agree-
ment is important to model. In contrast, the nouns

in the overlapping region are typically objects.
This demonstrates that the latent embeddings are
focusing on aspects of language that are speciﬁ-
cally important for modeling dependency without
ever having seen examples of dependency parses.
Some previous work has deliberately created
embeddings to capture different notions of sim-
ilarity (Levy and Goldberg, 2014; Cotterell and
Sch¨utze, 2015), while they use extra morphol-
ogy or dependency annotations to guide the em-
bedding learning, our approach provides a poten-
tial alternative to create new embeddings that are
guided by structured syntax model, only using un-
labeled text corpora.

5 Related Work

Our approach is related to ﬂow-based generative
models, which are ﬁrst described in NICE (Dinh
et al., 2014) and have recently received more at-
tention (Dinh et al., 2016; Jacobsen et al., 2018;
Kingma and Dhariwal, 2018).
This relevant
work mostly adopts simple (e.g. Gaussian) and
ﬁxed priors and does not attempt to learn inter-
pretable latent structures. Another related gen-
erative model class is variational auto-encoders
(VAEs) (Kingma and Welling, 2013) that opti-
mize a lower bound on the marginal data likeli-
hood, and can be extended to learn latent struc-
tures (Miao and Blunsom, 2016; Yin et al., 2018).
Against the ﬂow-based models, VAEs remove the
invertibility constraint but sacriﬁce the merits of
exact inference and exact log likelihood compu-
tation, which potentially results in optimization
challenges (Kingma et al., 2016). Our approach
can also be viewed in connection with generative
adversarial networks (GANs) (Goodfellow et al.,
2014) that is a likelihood-free framework to learn
implicit generative models. However, it is non-
trivial for a gradient-based method like GANs to
propagate gradients through discrete structures.

6 Conclusion

In this work, we deﬁne a novel generative ap-
proach to leverage continuous word representa-
tions for unsupervised learning of syntactic struc-
ture. Experiments on both POS induction and un-
supervised dependency parsing tasks demonstrate
the effectiveness of our proposed approach. Fu-
ture work might explore more sophisticated in-
vertible projections, or recurrent projections that
jointly transform the entire input sequence.

References

Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proceedings of ACL.

Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings of
HLT-NAACL, pages 582–590. Association for Com-
putational Linguistics.

Phil Blunsom and Trevor Cohn. 2011. A hierarchi-
cal pitman-yor process hmm for unsupervised part
of speech induction. In Proceedings of ACL.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion of Computational Linguistics.

Jiong Cai, Yong Jiang, and Kewei Tu. 2017. Crf au-
toencoder for unsupervised dependency parsing. In
Proceedings of EMNLP.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for measur-
ing progress in statistical language modeling. arXiv
preprint arXiv:1312.3005.

Scott Saobing Chen and Ramesh A Gopinath. 2001.
Gaussianization. In Advances in neural information
processing systems.

Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsuper-
vised pos induction: How far have we come?
In
Proceedings of EMNLP.

Shay B Cohen, Dipanjan Das, and Noah A Smith.
2011. Unsupervised structure prediction with non-
In Proceedings of
parallel multilingual guidance.
EMNLP.

Shay B Cohen and Noah A Smith. 2009. Shared logis-
tic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
HLT-NAACL.

Michael Collins. 1999. HEAD-DRIVEN STATISTICAL
MODELS FOR NATURAL LANGUAGE PARSING.
Ph.D. thesis, University of Pennsylvania.

Ryan Cotterell and Hinrich Sch¨utze. 2015. Mor-
In Proceedings of

phological word-embeddings.
NAACL-HLT.

Laurent Dinh, David Krueger, and Yoshua Bengio.
2014. Nice: Non-linear independent components
estimation. arXiv preprint arXiv:1410.8516.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In Proceedings of NIPS.

Wenjuan Han, Yong Jiang, and Kewei Tu. 2017. De-
pendency grammar induction with neural lexical-
In Proceedings of
ization and big training data.
EMNLP.

Luheng He, Kenton Lee, Mike Lewis, and Luke Zettle-
moyer. 2017. Deep semantic role labeling: What
works and whats next. In Proceedings of ACL.

William P Headden III, Mark Johnson, and David Mc-
Closky. 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Pro-
ceedings of HLT-NAACL.

Aapo Hyv¨arinen, Juha Karhunen, and Erkki Oja. 2004.
Independent component analysis, volume 46. John
Wiley & Sons.

J¨orn-Henrik Jacobsen, Arnold Smeulders, and Edouard
i-revnet: Deep invertible networks.

Oyallon. 2018.
arXiv preprint arXiv:1802.07088.

Yong Jiang, Wenjuan Han, and Kewei Tu. 2016. Unsu-
pervised neural dependency parsing. In Proceedings
of EMNLP.

Mark Johnson. 2007. Why doesnt em ﬁnd good
hmm pos-taggers? In Proceedings of the EMNLP-
CoNLL.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Diederik P Kingma and Prafulla Dhariwal. 2018.
Glow: Generative ﬂow with invertible 1x1 convo-
lutions. arXiv preprint arXiv:1807.03039.

Diederik P Kingma, Tim Salimans, Rafal Jozefowicz,
Xi Chen, Ilya Sutskever, and Max Welling. 2016.
Improved variational inference with inverse autore-
In Advances in Neural Information
gressive ﬂow.
Processing Systems, pages 4743–4751.

Diederik P Kingma and Max Welling. 2013. Auto-
arXiv preprint

encoding variational bayes.
arXiv:1312.6114.

Dan Klein and Christopher D Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL.

Phong Le and Willem Zuidema. 2015. Unsupervised
dependency parsing: Let’s use supervised parsers.
In Proceedings of NAACL-HLT.

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of ACL.

Laurent Dinh, Jascha Sohl-Dickstein, and Samy Ben-
gio. 2016. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803.

Chu-Cheng Lin, Waleed Ammar, Chris Dyer, and Lori
Levin. 2015. Unsupervised pos induction with word
embeddings. In Proceedings of the NAACL-HLT.

Karl Stratos, Michael Collins, and Daniel Hsu. 2016.
Unsupervised part-of-speech tagging with anchor
hidden markov models. Transactions of the Asso-
ciation for Computational Linguistics.

Ke M Tran, Yonatan Bisk, Ashish Vaswani, Daniel
Marcu, and Kevin Knight. 2016. Unsupervised neu-
In Proceedings of the
ral hidden markov models.
Workshop on Structured Prediction for NLP.

Kewei Tu and Vasant Honavar. 2012. Unambiguity
regularization for unsupervised learning of prob-
In Proceedings of EMNLP-
abilistic grammars.
CoNLL.

Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012.
Learning syntactic categories using paradigmatic
representations of word context. In Proceedings of
EMNLP-CoNLL.

Mehmet Ali Yatbaz, Enis Rıfat Sert, and Deniz Yuret.
2014. Unsupervised instance-based part of speech
In Proceed-
induction using probable substitutes.
ings of COLING.

Pengcheng Yin, Chunting Zhou, Junxian He, and Gra-
ham Neubig. 2018. Structvae: Tree-structured latent
variable models for semi-supervised semantic pars-
ing. In Proceedings of ACL.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics, 19(2):313–330.

Yishu Miao and Phil Blunsom. 2016. Language as a
latent variable: Discrete generative models for sen-
tence compression. In Proceedings of EMNLP.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
Efﬁcient estimation of word
arXiv preprint

frey Dean. 2013.
representations in vector space.
arXiv:1301.3781.

John K Pate and Sharon Goldwater. 2013. Unsu-
pervised dependency parsing with acoustic cues.
Transactions of the Association for Computational
Linguistics.

John K Pate and Mark Johnson. 2016. Grammar in-
duction from (lots of) words alone. In Proceedings
of COLING.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of HLT-NAACL.

Danilo Jimenez Rezende and Shakir Mohamed. 2015.
Variational inference with normalizing ﬂows.
In
Proceedings of ICML.

Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of EMNLP-
CoNLL.

Valentin I Spitkovsky, Hiyan Alshawi, Angel X Chang,
and Daniel Jurafsky. 2011a. Unsupervised depen-
dency parsing without gold part-of-speech tags. In
Proceedings of EMNLP.

Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2011b. Punctuation: Making a point in un-
supervised dependency parsing. In Proceedings of
CoNLL.

Valentin I Spitkovsky, Hiyan Alshawi, and Daniel
Jurafsky. 2012. Capitalization cues improve de-
In Proceedings of
pendency grammar induction.
NAACL-HLT Workshop on the Induction of Linguis-
tic Structure.

Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2013. Breaking out of local optima with
count transforms and model recombination: A study
in grammar induction. In Proceedings of EMNLP.

Valentin I Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,
and Christopher D Manning. 2010. Viterbi training
improves unsupervised dependency parsing. In Pro-
ceedings of CoNLL.

