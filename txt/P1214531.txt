9
1
0
2
 
n
u
J
 
3
 
 
]

V
C
.
s
c
[
 
 
1
v
5
0
8
0
0
.
6
0
9
1
:
v
i
X
r
a

GazeCorrection:Self-Guided Eye Manipulation in the
wild using Self-Supervised Generative Adversarial
Networks

Jichao Zhang ∗
Shandong University

Meng Sun ∗
Shandong University

Jingjing Chen∗
Shandong University

Hao Tang
University of Trento

Yan Yan
Texas State University

Xueying Qin
Shandong University

Nicu Sebe
University of Trento

Abstract

Gaze correction aims to redirect the person’s gaze into the camera by manipulating
the eye region, and it can be considered as a speciﬁc image resynthesis problem.
Gaze correction has a wide range of applications in real life, such as taking a
picture with staring at the camera. In this paper, we propose a novel method that is
based on the inpainting model to learn from the face image to ﬁll in the missing
eye regions with new contents representing corrected eye gaze. Moreover, our
model does not require the training dataset labeled with the speciﬁc head pose
and eye angle information, thus, the training data is easy to collect. To retain
the identity information of the eye region in the original input, we propose a self-
guided pretrained model to learn the angle-invariance feature. Experiments show
our model achieves very compelling gaze-corrected results in the wild dataset
which is collected from the website and will be introduced in details. Code is
available at https://github.com/zhangqianhui/GazeCorrection.

1

Introduction

Gaze correction, a speciﬁc task of gaze redirection which aims to adjust the eye gaze to any directions,
just changes the gaze into a single new direction. In this paper, we focus on the task of gaze correction
which is to manipulate the eye gaze to stare at the camera.

In some important life scenarios, these are a need to alter the appearance of eyes in a way that
digitally manipulates the person’s gaze into the camera. In same cases, it rarely happens for everyone
staring at the camera while taking a group photo. Another scenario is in desktop videoconferencing
systems where the eye contact is extremely important and the gaze also can express attributes such as
attentiveness, conﬁdence, and requirement. Unfortunately, eye contact and gaze awareness are lost in
most videoconferencing systems. Because videoconferencing participants look at their monitors and
not directly into the camera. In general, eye correction has practical importance in these application
scenarios.

The early research for gaze correction depends on special hardware, for example stereocameras [6, 30],
kinect sensor [14] or transparent mirrors [13, 18]. Recently, some methods based on machine learning
have attained very high-quality synthesized images with corrected gaze, such as DeepWarp [8].
DeepWarp uses a deep architecture to directly predict an image-warping ﬂow ﬁeld with the coarse-
to-ﬁne learning process. However, it requires the paired face dataset labeled with accurate, speciﬁc
information for head pose and eye angles while the collection of training image depends on particular

∗Equal Contribution

Preprint. Under review.

hardware. Moreover, DeepWarp would suffer from the failure in the wild images with large variations
in head pose. Another major type is based on the 3D model, such as GazeDirector [28]. The main
idea of GazeDirector is to model the eye region in 3D instead of trying to predict a ﬂow ﬁeld directly
from an input image. However, it is not easy to model realistically in details.

Compared with the previous methods, our model is simple and novel. We leverage the image
inpainting model with a fully convolutional network as the basis of our model to learn to ﬁll in the
missing eye regions with new contents representing corrected eye gaze. In the process of training
and testing, our inpainting model does not require the data with the speciﬁc eye angle and head pose
information. Thus, the training data is easy to collect and use. Similar to the previous inpainting
model [11], we use the global and local discriminator architecture D for performing the adversarial
loss to improve the quality of the inpainted region. Moreover, to preserve the identity information
in eye region of the original input, we propose a Self-Guided Pretrained Model to extract the angle-
invariance features which would be as the input of the network to guide the learning process of
the generator and discriminator. Finally, to improve the quality of inpainted result and stabilize the
adversarial learning, we add a self-supervised module for discriminator and generator. We compare
our approach with existing baseline methods for correcting the gaze in the wild image. Qualitative
and quantitative evaluations demonstrate that our model can achieve more compelling results.

Overall, our main contributions include:(1) A simple and novel image inpainting model for gaze
correction is proposed. Qualitative and quantitative assessment demonstrate the effectiveness and
superiority of the proposed model. (2) A Self-Guided Pretrained Model to extract the angle-invariance
features is proposed and the features would be as an input of the inpainting model to preserve identity
information of the original eye region. (3) A novel self-supervised learning module to improve the
quality of inpainted result and stabilize the adversarial training.

2 Related Work

Generative Adversarial Networks: Generative Adversarial Networks [9] is a powerful implicit
generative model to produce a model distribution that mimics a given target distribution, and it has
been applied to many ﬁelds, such as low-level image processing tasks (image in-painting [20, 12],
image super-resolution [15, 16, 27]), high-level semantic or style transfer [21, 33, 24, 32], video
prediction and generation [25, 3] and even classical computer vision problem (Object detection [26]).

Image Inpainting: Image inpainting, an important task in computer vision and graphics, aims to ﬁll
the missing pixels of an image with plausibly synthesized contents. Most of the previous methods
for image inpainting can be mainly divided into two classes. One is based on traditional diffusion or
patch methods with low-level features. For example, PatchMatch [2], a fast nearest neighbor ﬁeld
algorithm, which allows for real-time, high-level image inpainting has been proposed. In summary,
based on low-level features, these traditional methods are not effective to ﬁll in holes on complicated
semantic structures and unable to generate novel objects not found in the source image. The other is
the learning-based method. Recently, CNN-based and GAN-based methods have shown promising
performance on image inpainting [19, 11]. However, we argue that these methods would suffer the
limitations for face inpainting, where the inpainting result does not preserve the identity information
of the original input. Similar to the previous paper [7] which depends on the reference image, we
propose a self-guided method to learn the angle-invariance features which would be the input of the
inpainting network to guide the inpainting process for preserving the identity information.

Eye Gaze Manipulation: The previous methods for gaze manipulation can be divided into three
classes: 1) hardware-driven, 2) rendering and synthesis, 3) learning-based.

The hardware support is indispensable in early research. Kollarits et al. [13] tried to make use
of half-silvered mirrors to allow the camera to be placed on the optical path of the display. Yang
et al. [31] aimed to address the eye contract problem with a novel view synthesis, and they use a
pair of calibrated stereo cameras and a face model to track the head pose in 3D. Generally, these
hardware-based methods are costly.

Some researchers try to render the eye region based on 3D ﬁtting model, which replaces the original
eye with synthesizing new eyeballs. Banf et al. [1] used an example-based approach for deforming
the eyelids and slide the iris across the model surface with texture-coordinate interpolation. To ﬁx
the limitation caused by the use of a mesh where the face and eyes are joined, GazeDirector [28]

2

Figure 1: Overview of our network architecture.

regards the face and eyeballs as separate parts, synthesizes more high-quality images especially for
large redirection angles. These methods for the common gaze manipulation can be applied for eye
correction, but they suffer from the problem where rendering eyes realistically is a challenge.

The core idea for most of the method based on machine learning is to use large training set labeled
with speciﬁc eye angle and head pose information to learn the warping ﬁeld. The warping ﬁeld is
used to relocate eye pixels in the original image, thus realizing gaze redirection. Ganin et al. [8]
proposed a novel deep model based on the convolutional network as a predictor, which helps them to
achieve a high-quality result. We present a novel method for gaze correction. With an encode-decode
architecture as a generator and adversarial process, we can learn from the face image to ﬁll in the
missing regions with new contents representing corrected eye gaze. Compared with those learning-
based methods, our training dataset is unpaired and collected from the website, where the dataset
is not labeled with speciﬁc eye angle and head pose information, thus, is easy to collect and use.
Furthermore, these models are hard to achieve high-quality results for gaze correction in the wild
images.

3 Methods

As shown in Fig. 1, our model consists of two parts. One is the generator G is used for inpainting
network. We leverage one global discriminator Dg with using the entire face as input and one local
discriminator Dl with using the local eye region as input to perform the adversarial learning with
generator G. Additionally, Dl would also perform a self-supervised learning by classifying the
right and left eyes. The other is a Self-Guided Pretrained model which is based on autoencoder to
reconstruct and translate image between different domains to learn the angle-invariance features l.
We will introduce two parts of our model in details.

1 ∈ X and {yi}m

Generator G Trained with Unpaired Data for the Inpainting: We begin with introducing our
training data which consists of two domains {xi}n
1 ∈ Y . Domain X: 256 × 256
face images with eyes staring at the camera; domain Y : 256 × 256 face images with eyes staring at
somewhere else. Given a masking function M : remove eye region of face data (M −: crop out eye
region) and denote the eye-masked face as X c and Y c, thus, X c = M (X) and Y c = M (Y ). When
n → ∞ with the same masking function R, the distribution PX c for X c and PY c for Y c should
be identical, i.e. PX c = PY c . Given a function G to map X c to X, we will have X = G(X c),
and G(Y c) ⊆ X. This is the theoretical basis of our method. Therefore, we can use the data from
domain X as training data, the data from domain Y as test data. With this mapping function G
trained on domain X, we can correct the gaze direction of the test data in the domain Y to stare at
the camera. Note that, our training data is unpaired where every instance of domain X does not have
the corresponding groundtruth in the domain Y . While the theoretical analysis is still approximately
reasonable when the training data is very ample in X.

Next, we introduce the learning of generator G. As shown in Fig. 1, G is designed to be an
autoencoder to ﬁll in the missing regions with the contents representing corrected eye gaze. Different
from the architecture of [11], we use the fully-connected networks instead of dilated convolution in
the bottleneck. For the objective function of generator G, a reconstruction loss Lrec is necessary,

3

where we use L1 distance between the output ˜X (i.e., G(X c)) and groundtruth X. It is deﬁned as:
(cid:96)rec = EX [(cid:107) ˜X − X(cid:107)1].

(1)

Self-Supervised Adversarial Training: To avoid the inpainted results tending to be blurry with
(cid:96)rec only as an objective function, we employ two discriminators to perform adversarial loss for
improving the visual quality of inpainted result. Additionally, we add a self-supervised learning
module for discriminator to stabilize the training of adversarial networks and further improve the
quality of inpainted results. As shown in Fig. 1, Dg and Dl are based on ConvNets. Dg takes the
entire image X and ˜X as the input to ensure the generated contents are semantically coherent on the
global region, while Dl uses the local patch M −(X) and M −( ˜X) as inputs for training to achieve
more realistic and sharper contents. We concatenate the ﬁnal fully-connected output of Dg and Dl
into one output which is as the input of sigmoid to predict the probability of the image being real.
The objective function for discriminator D (including Dg and Dl) and generator G is deﬁned as:

min
G

max
D

(cid:96)adv = EX [logD(X, M −(X))] + E ˜X [log(1 − D( ˜X, M −( ˜X)))].

(2)

Inspired by the previous paper [4], we propose a simple and novel self-supervised module. In details,
we simple add a classiﬁcation learning to divide the left eye and right eye for local discriminator. We
desire this additional loss on the image classiﬁcation task could improve the ability of representation
for local discriminator and stabilize its training. As shown in Fig. 1, we augment the discriminator
with a left-right-eye classiﬁcation loss, which results in the ﬁnal adversarial loss functions:
(cid:96)adv = EX [logD(X, M −(X))] + E ˜X [log(1 − D( ˜X, M −( ˜X)))]
+ EX [logQD(P = p|X)] − E ˜X [logQD(P = p| ˜X)],

max
D

min
G

(3)

where p ∈ P is the position of eye where p = 0 means left eyes, p = 1 means right eyes. Q(P |X) is
the local discriminator’s predictive distribution over the position of eyes.

Self-Guided Pretrained Model for disentangled representation: Obviously, this inpainting pro-
cess is hard to preserve the consistency of the identity information (e.g., iris color, eye shape) between
X and ˜X. Recently, ExemplarGAN [7], which is a novel method for the task of closed-to-open
inpainting in natural images, tried to use the exemplar information to produce high-quality, personal-
ized inpainting results. However, the reference image, generally speaking, is not always available
in reality. Inspired by their idea, we can pretrain a model to attain the angle-invariance features
which are as inputs of G and D to guide their training process for preserving the identity informa-
tion of the eye region. To learn the angle-invariance features, we denote a small training data as
{za
i is from the same person, captured with different
angle information in the same scene. As shown in Fig. 1, our encoder network Een takes M −(za)
and M −(zb) as inputs respectively, the latent codes as the output are ca, la = Een(M −(za)) and
cb, lb = Een(M −(zb)) (c = [c1, c2] ∈ R2, l ∈ R128). With the decoder network Dde, we ﬁrstly
reconstruct the original input. The reconstruction objective function (cid:96)autoencoder for Een and Dde is
deﬁned as:

1 ∈ Z, where every image pair za

i and zb

i , zb

i }n

(cid:96)autoencoder = Eza,zb [(cid:107)M −(za) − Dde(ca, la)(cid:107)1 + (cid:107)M −(zb) − Dde(cb, lb)(cid:107)1].
Then, to learn the translation between M −(za) and M −(zb), we swap their latent vector c which is
similar to the previous paper [29]. The objective function for the translation is deﬁned as:

(4)

(cid:96)translation = Eza,zb [(cid:107)M −(zb) − Dde(cb, la)(cid:107)1 + (cid:107)M −(za) − Dde(ca, lb)(cid:107)1].
With (cid:96)autoencoder + (cid:96)translation as the overall objective function for Een and Dde trained on paired
data, we can disentangle vector c for representing the angle information in the latent space while
vector l can be as the angle-invariance features. Note that we use the existing gaze dataset to pretrain
this model for disentangled representation.
When training the inpainting model with taking M −(X) as an input of Een for attaining the angle-
invariance features to guide the inpainting of X, we call this pretrained model as Self-Guided
Pretrained Model and refer to this autoencoder with swapping the latent code as SAE.

(5)

Overall Objective function for G and D: As shown in the Fig. 1, the angle-invariance features
l is as the input of generator G and discriminator D. Taking local eye region M −(X) as the

4

input of Een, we have l = Een
l means outputing l). Then, we would have new
˜X (i.e., G(X c, l)). Moreover, to measure the distance between the inpainted eye region M −( ˜X) and
M −(X) in perceptual space, we try to minimize L2 distance of angle-invariance features l encoded
from M −( ˜X) and M −(X). In summary, the overall objective functions for G and D are shown as:

l (M −(X)) (Een

min
G

max
D

(cid:96)overall = EX [logD(X, M −(X), l)] + EX [log(1 − D( ˜X, M −( ˜X), l))]
+ λsEX [logQD(P = p|X)] − λsE ˜X [logQD(P = p| ˜X)]
l (M −( ˜X)) − l)(cid:107)2
+ λrEX [(cid:107) ˜X − X(cid:107)1] + λpEX [(cid:107)Een
2].

(6)

The more details about the network architecture have been shown in the supplementary material B.

In this section, we ﬁrst introduce the details of our dataset, network training and baseline models.
Next, we demonstrate the validity of Self-Guided Pretrained Model for learning the angle-invariance
features. Then, we compare the proposed model with baselines by qualitative and quantitative
assessment. Finally, the proposed self-supervised learning module would be investigated. For brevity,
we refer to our method as GazeGAN.

4 Experiments

4.1 Dataset

NewGaze Dataset: To evaluate the proposed model, we have investigated the benchmark datasets.
However, none of them meet our task for eye correction in the wild. Thus, we collected a new dataset
called NewGaze dataset. NewGaze consists of 30000 images. These unpaired data, which is collected
from the CelebA-ID and the website, consists of two domains. Domain X: 25000 face images with
eyes staring at the camera; domain Y : 5000 face images with eyes staring at somewhere else. We
crop all images (256 × 256) with face detection algorithm and compute the eye mask region by using
facial landmarks detection algorithm. As described above, we use all data of domain X to train our
model, while all data in domain Y is just as the test dataset. Note that no matter paired data is not
labeled with the speciﬁc eye angle or head pose information, thus, is very easy to collect.

Columbia Dataset: The Columbia Gaze [23] is a publicly available gaze data as the benchmark in
gaze locking an gaze tracking, where it has 5880 images of 56 people over 5 head pose and 21 gaze
directions. For each subjects, they are labelled with three information: head poses (0o, ±15o, ±30o),
seven horizontal gaze directions (0o, ±5o, ±10o, ±15o) and three vertical gaze directions (0o, ±10o).
The collecting details can be found in the their project page2. Similar to the pre-precessing methods
for NewGaze dataset, we would crop and resize them to 256 × 256 for training and testing. The
training set would be used for training our self-guided pretraining model.

4.2 Training Details

The proposed Self-Guided Pretrained model is trained on 1-sized batches with learning rate 0.01.
For the training of main model, λs, λr and λp is 1. To stable the training process, we use spectral
normalization [17] for every layer in discriminator D. The optimizer is Adam with β1 = 0.5 and
β2 = 0.999. The training batch is set to 16. The learning rate of our inpainting model is 0.0001 for
the ﬁrst 20000 iterations, and it will be linearly decayed to 0 over the remaining iterations.

4.3 Baseline Models

Image Inpainting: GazeGAN can be regarded as an inpainting model. Thus, we adopt the classical
deep inpainting model GLGAN [11] as a baseline and train it on the NewGaze dataset.

Image Translation: Image translation model, such as, StarGAN [5] has achieved high-quality results
in facial attribute manipulation. We train StarGAN on the NewGaze dataset to learn the translation
between the domain X and domain Y .

2http://www.cs.columbia.edu/ brian/projects/columbia_gaze.html

5

Figure 2: Introduction and validation of disentangled angle representation. (a) Comparisons of the
correction result between the GazeGAN(W/O) (2nd row) and GazeGAN (3rd row) with the same
input (1st row). The zoomed-in right eyes are shown in the bottom-right for every image. (b) Top: 10
images of eye region with different angles for each person. Bottom: curves of the latent code c1 over
eyes with 7 different horizontal direction and curves of the latent code c2 over eyes with 3 different
vertical direction. The dotted curves are corresponding to Person 1 (P1) and the solid curves are for
Person 2 (P2).

Figure 3: Comparisons with existing works tested in NewGaze dataset. The ﬁrst row shows the input
faces without staring at the camera. The rest rows show the eye-correction results from different
models.

Eye Manipulation: DeepWarp [8] has achieved the state-of-the-art gaze manipulation results based
on the deep model which uses convolution neural network to predict the ﬂow ﬁeld for modifying the
direction of gaze. We would compare our model with DeepWarp.
We use the public code of GLGAN3, and StarGAN4. For DeepWarp without published code, we
implement it by ourselves. DeepWarp requires the paired training image labeled with eye angle and
head pose information as inputs. Thus, we train it using the Columbia gaze dataset [23] and test it in
the NewGaze and Columbia dataset.

4.4 Learning the angle-invariance features

We use paired data to train SAE to learn the angle-invariance features l, which would be as an input of
generator G and discriminator D to guide the inpainting for retaining the identity information of the
eye region in the original input. We take an opposite approach to validate if l is angle-invariant: when
the disentangled features c represents the angle features in the latent space, the remaining features
l should be angle-invariant. Thus, we should validate if the feature ci is a linear correlation with
horizontal or vertical eye angle. As shown in left of Fig. 2(b), we show 7 samples for each person
with different horizontal angles in the top, and give the curve of c1 over these 7 samples in the bottom.

3https://github.com/shinseung428/GlobalLocalImageCompletion_tf
4https://github.com/yunjey/StarGAN

6

Figure 4: Comparisons of Gaze correction on Columbia test dataset. The rows from top to down are
input, results of DeepWarp and GazeGAN.

Figure 5: More correction results of GazeGAN in different head pose.

For original Autoencoder(OAE), we also train it using the paired dataset for comparison. We can
observe that c1 encoded from SAE is nearly linear with different eye angles as input while the c1
has the similar value for the same angle from different person (See SAE for P1 and P2). The other
curves (See OAE for P1 and P2) are very irregular means that OAE can not disentangle angle feature
in the latent space. As shown in right of Fig. 2(b), a similar conclusion could be attained.

To further demonstrate the effectiveness of Self-Guided Pretrained model for learning the angle-
invariance features, we train GazeGAN without concatenating the features l as the input of D and
G, and refer to this variant as GazeGAN(W/O). As shown in the Fig. 2(a), the inpainted face for
GazeGAN can better preserve the identity information (iris color and eye shape) of the original face
compared with the results of GazeGAN(W/O).

4.5 Eye correction

In this section, we provide the comparison experiments with baseline methods on the NewGaze dataset
for the task of eye correction, and use some metrics to evaluate the inpainting results quantitatively.
Note that we do not use any post-processing algorithm for these models, including GazeGAN.

Qualitative results: As shown in the last row of Fig. 3, GazeGAN can succeed in correcting the eyes
into looking at the camera, which validates the effectiveness of the proposed method. In comparison
to StarGAN, GazeGAN attains more obvious correction results. StarGAN can obtain compelling
results in style or texture translation, but it is hard to get the natural geometric translation. It is
explained that StarGAN is based on cycle-consistency loss, which requires the mapping between two
domains should be continuous and inverse to each other. According to the invariance of Domain
Theorem, the intrinsic dimensions of the two domains should be the same. However, the intrinsic
dimensions of domain Y are much higher than domain X. Furthermore, compared with GLGAN,
GazeGAN demonstrates its superior advantage in preserving the identity information (iris color and
eye shape) of the original input. For example, as shown in the 2nd column, the correction result of
GazeGAN has a more identical iris color than GLGAN. We can explain that the angle-invariance
features from the proposed Self-Guided Pretrained model contain the identity information from
the original input which could guide the inpainting for preserving the consistency of the color and
structure. As shown in the 4th row, the results of DeepWarp are very blurry and do not show an
obvious correction for these inputs compared with our model. Additionally, we test our model and

Table 1: Inception and FID scores on the eye region of gaze-corrected results from different models.

Methods
StarGAN
GLGAN
DeepWarp
GazeGAN
GT

Inception Scores
2.99±0.08
2.87±0.07
2.89±0.12
3.10±0.12
3.19±0.09

User Studies (Average)
28.90%
21.87%
13.13%
35.40%
100%

FID
28.34
34.33
106.53
30.21
27.64

7

Table 2: Ablation study of self-supervised learning model. The scores are IS and FID metric on 1000
inpainted results generated from models on different training iterations.

Model(104)

GazeGAN(W/O)
GazeGAN

2

2.69
3.01

4

2.97
3.12

6

2.93
2.98

8

3.20
3.04

10

2.92
3.12

12

3.04
3.10

14

2.96
3.10

16

3.02
2.98

28

3.02
3.04

20

3.07
2.99

GazeGAN(W/O)
GazeGAN

83.48
53.12

60.50
37.56

44.23
37.51

36.39
33.36

30.72
27.86

28.88
29.15

29.30
35.21

34.90
30.00

34.89
30.19

34.55
31.74

IS

FID

DeepWarp on Columbia test data. The test result has been shown in Fig. 4 and GazeGAN could attain
much sharper result than DeepWarp.

As shown in Fig. 5, we show more high-quality gaze correction results with different head poses.
More results can be found in the supplementary material A or an anonymized project page 5.

Quantitative Evaluation Protocol: The qualitative evaluation has validated the effectiveness and
advantage of our proposed GazeGAN in correcting the eye angle. To further demonstrate the previous
evaluation with quantitative methods, we employ two tactics: Inception scores and FID to evaluate
the generated sample quality in the eye region.

Inception Scores(IS) proposed by [22] uses the pretrained inception model to compute the scores
on all test results to evaluate the maintenance of meaningful objects and the variety. Higher IS
corresponds to higher quality images. Compared to IS, Fréchet Inception distance (FID) is more
consistent with human evaluation in assessing the realism and variation of input samples. Lower FID
means that the input samples are of better quality.

In addition to these two evaluations mentioned above, we conduct a user study in a survey to assess
the results of the eye correction from different models. In details, given an input face image in
NewGaze test dataset, we would show the corrected results from different models to 30 respondents
who were asked to select the best one based on perceptual realism and the ability of gaze correction.
This study prepares 50 questions for every respondent.

Quantitative results: Table 1 shows the evaluation results of IS and FID experiments on the corrected
images from different models. The proposed model GazeGAN attains a higher IS and FID scores
comparing with GLGAN, e.g., 3.10 ± 0.12 for GazeGAN and 2.87 ± 0.07 for GLGAN which
demonstrates GazeGAN can produce more photo-realistic inpainting results. We can have a similar
conclusion comparing with DeepWarp. The qualitative evaluation shows StarGAN is hard to learn
the apparent translation, but StarGAN attains a very high score, e.g., 28.34 for FID scores, higher
than other methods. We explain that StarGAN can generate high-quality samples and tends to learn
the reconstruction instead of translation by using the Wasserstein GAN with gradient penalty [10] as
the objective. The last column of Table 1 shows the evaluation results of the user study, and it shows
the voting for GazeGAN is 35.40%, higher than other models, e.g., 29.60% for StarGAN, 21.87%
for GLGAN, 13.13% for DeepWarp.

In general, GazeGAN can achieve very compelling and competitive gaze correction results in the
wild image.

4.6 Ablation Study for Self-Supervised Learning

We propose a self-supervised adversarial learning module to improve the quality of inpainted result
and stabilize the networks for adversarial learning. Our method is very simple and just augment
the discriminator and generator with a classiﬁcation loss for left-and-right eyes. For validating the
effectiveness of this self-supervised learning, we conduct the quantitative experiments using IS and
FID scores to evaluate the quality of inpainted samples.

As shown in Table. 2, we compare GazeGAN with self-supervised learning and GazeGAN without
self-supervised learning module (GazeGAN(W/O)) over multiple different training iterations. The
scores of GazeGAN with self-supervised learning attain higher IS and FID scores in most of cases,
which demonstrates the effectiveness of self-supervised learning to improve the quality of inpainted
results. In general, our self-supervised learning is very simple, but effective. Moreover, this module
is very easy to be augmented into other facial image tasks with adversarial learning.

5https://nips2019.wixsite.com/gazegan

8

5 Conclusion

In this paper, we have presented a simple and novel model, GazeGAN, for gaze correction in the wild
image. The novelty is we leverage the inpainting model with self-supervised generative adversarial
networks to learn from the face image to ﬁll in the missing eye regions with new contents representing
corrected eye gaze. Moreover, our method is very simple, the proposed GazeGAN does not require
the training data been labeled with speciﬁc eye angle and head information, even the majority of
training data without the corresponding groundtruth between different domains. To preserve the
identity information of the original input, Self-Guided Pretrained model proposed could learn the
angle-invariance features for guiding the inpainting process in the training and testing.

References

[1] Michael Banf and Volker Blanz. Example-based rendering of eye movements. In Computer

Graphics Forum, volume 28, pages 659–666. Wiley Online Library, 2009.

[2] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan B Goldman. Patchmatch: A
randomized correspondence algorithm for structural image editing. ACM Transactions on
Graphics (ToG), 28(3):24, 2009.

[3] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. Everybody dance now. arXiv

preprint arXiv:1808.07371, 2018.

[4] Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, and Neil Houlsby. Self-supervised

generative adversarial networks. arXiv preprint arXiv:1811.11212, 2018.

[5] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo.
Stargan: Uniﬁed generative adversarial networks for multi-domain image-to-image translation.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.

[6] Antonio Criminisi, Jamie Shotton, Andrew Blake, and Philip HS Torr. Gaze manipulation for

one-to-one teleconferencing. In ICCV, volume 3, pages 13–16, 2003.

[7] Brian Dolhansky and Cristian Canton Ferrer. Eye in-painting with exemplar generative adver-

sarial networks. In CVPR, pages 7902–7911, 2018.

[8] Yaroslav Ganin, Daniil Kononenko, Diana Sungatullina, and Victor Lempitsky. Deepwarp:
Photorealistic image resynthesis for gaze manipulation. In ECCV, pages 311–326. Springer,
2016.

[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pages 2672–2680,
2014.

[10] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of wasserstein gans. In Advances in Neural Information Processing Systems,
pages 5767–5777, 2017.

[11] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. Globally and locally consistent image

completion. TOG, 36(4):107, 2017.

[12] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. Globally and Locally Consistent

Image Completion. ACM Transactions on Graphics, 36(4):107:1–107:14, 2017.

[13] R Kollarits, C Woodworth, J Ribera, and R Gitlin. 34.4: An eye contact camera/display system
for videophone applications using a conventional direct-view lcd. In Society for Information
Display, International Symposium, pages 765–768, 1996.

[14] Claudia Kuster, Tiberiu Popa, Jean-Charles Bazin, Craig Gotsman, and Markus Gross. Gaze
correction for home video conferencing. ACM Transactions on Graphics (TOG), 31(6):174,
2012.

[15] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro
Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. Photo-
realistic single image super-resolution using a generative adversarial network. July 2017.

[16] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Kumar Singh, and Ming-Hsuan Yang.

Diverse image-to-image translation via disentangled representations. In ECCV, 2018.

9

[17] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization

for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.

[18] Ken-Ichi Okada, Fumihiko Maeda, Yusuke Ichikawaa, and Yutaka Matsushita. Multiparty
videoconferencing at virtual social distance: Majic design. In Proceedings of the 1994 ACM
conference on Computer supported cooperative work, pages 385–393. ACM, 1994.

[19] Deepak Pathak, Philipp Krähenbühl, Jeff Donahue, Trevor Darrell, and Alexei Efros. Context

encoders: Feature learning by inpainting. In CVPR, 2016.

[20] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context

encoders: Feature learning by inpainting. pages 2536–2544, 2016.

[21] Albert Pumarola, Antonio Agudo, Aleix M Martinez, Alberto Sanfeliu, and Francesc Moreno-
Noguer. Ganimation: Anatomically-aware facial animation from a single image. In ECCV,
pages 818–833, 2018.

[22] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Proceedings of the 30th International Conference
on Neural Information Processing Systems, NIPS’16, pages 2234–2242, USA, 2016. Curran
Associates Inc.

[23] Brian A. Smith, Qi Yin, Steven K. Feiner, and Shree K. Nayar. Gaze locking: Passive eye contact
detection for human-object interaction. In Proceedings of the 26th Annual ACM Symposium on
User Interface Software and Technology, UIST ’13, pages 271–280, New York, NY, USA, 2013.
ACM.

[24] Hao Tang, Dan Xu, Nicu Sebe, Yanzhi Wang, Jason J. Corso, and Yan Yan. Multi-channel
attention selection gan with cascaded semantic guidance for cross-view image translation. In
CVPR, 2019.

[25] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro.
High-resolution image synthesis and semantic manipulation with conditional gans. In CVPR,
2018.

[26] Xiaolong Wang, Abhinav Shrivastava, and Abhinav Gupta. A-fast-rcnn: Hard positive genera-

tion via adversary for object detection. July 2017.

[27] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and
Chen Change Loy. Esrgan: Enhanced super-resolution generative adversarial networks. In
ECCVW, September 2018.

[28] Erroll Wood, Tadas Baltrušaitis, Louis-Philippe Morency, Peter Robinson, and Andreas Bulling.
Gazedirector: Fully articulated eye gaze redirection in video. In Computer Graphics Forum,
volume 37, pages 217–225. Wiley Online Library, 2018.

[29] Taihong Xiao, Jiapeng Hong, and Jinwen Ma. Elegant: Exchanging latent encodings with
gan for transferring multiple face attributes. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 168–184, 2018.

[30] Ruigang Yang and Zhengyou Zhang. Eye gaze correction with stereovision for video-
teleconferencing. In European Conference on Computer Vision, pages 479–494. Springer,
2002.

[31] Ruigang Yang and Zhengyou Zhang. Eye gaze correction with stereovision for video-
teleconferencing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(7):956–
960, 2004.

[32] Jichao Zhang, Yezhi Shu, Songhua Xu, Gongze Cao, Fan Zhong, Meng Liu, and Xueying Qin.
Sparsely grouped multi-task generative adversarial networks for facial attribute manipulation.
In Proceedings of the 26th ACM International Conference on Multimedia, MM ’18, pages
392–401, New York, NY, USA, 2018. ACM.

[33] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image

translation using cycle-consistent adversarial networks. Oct 2017.

10

GazeCorrection:Self-Guided Eye Manipulation in the wild using
Self-Supervised Generative Adversarial Networks
Supplementary materials are shown below. The network architectures of GazeGAN are shown in
Table 3 ,Table 4 and Table 5. And more qualitative results are shown below.

A Network Architecture

The Self-Guided network, which is employed to preserve the identity information ,takes as an input
the local image rescaled to 128 × 128 pixels. For the completion network, we use an encoder-decoder
architecture,which will add the angle-invariance feature learned by Self-Guided network.And these
features are used in discriminator as additional information when determining if the generated image
is real or fake. Here are some notations should be noted. h: the height of input images. w: the width
of input images. C:the number of output channels. K: the size of kernel. S: the size of stride. P : the
padding method. IN:instance normalization. FC:Fully-Connected layers.

Layer

Input Shape

Layer Information

Output Shape

encoder

content

rotation

decoder

( h
2 , w
2 , w
( h
( h
4 , w
8 , w
( h

2 , 3)
2 , 32)
4 , 64)
8 , 128)

16 , w
( h
16 , w
( h

16 , 128)
16 , 128)

(128,1),(1,1)
16 , w
( h
16 ,128)
( h
8 , w
8 , 128)
4 , w
( h
4 , 128)
( h
2 , w
2 , 64)
(h,w,32)

CONV-(C64, K4×4, S2×2,Psame),IN,ReLU

CONV-(C32, K7×7, S1×1,Psame),IN,ReLU

( h
2 , w
4 , w
( h
( h
8 , w
CONV-(C128, K4×4, S2×2,Psame),IN,ReLU
16 , w
CONV-(C128, K4×4, S2×2,Psame),IN,ReLU ( h

2 , 32)
4 , 64)
8 , 128)
16 , 128)

FC-128

FC-1

Concatenation,FC-( h
16 × w
DECONV-(C128,K4×4,S 1
DECONV-(C128,K4×4,S 1
DECONV-(C64,K4×4,S 1
DECONV-(C32,K4×4,S 1

16 × 128),reshape
2 × 1
2 ),IN,ReLU)
2 × 1
2 ),IN,ReLU)
2 × 1
2 ),IN,ReLU)
2 × 1
2 ),IN,ReLU)

(128,1)

(1,1)

( h
16 , w
16 ,128)
8 , w
( h
8 , 128)
4 , w
( h
4 , 128)
2 , w
( h
2 , 64)
(h,w,32)

CONV-(C3, K7×7, S1×1,Psame),Tanh

(h,w,3)

Table 3: Self-Guided Pretraining architecture.

11

Part

encoder

bottleneck

decoder

Part

Dg

Input Shape

(h, w, 6)
(h, w, 16)
( h
2 , w
2 , 32)
4 , w
( h
4 , 64)
8 , w
( h
8 , 128)
16 , w
( h
16 , 256)
32 , w
( h

32 , 256)

( h
32 , w
32 , 256)
( h
16 , w
16 , 256)
8 , w
( h
8 , 128)
( h
4 , w
4 , 64)
2 , w
( h
2 , 32)
(h, w, 16)

Input Shape

(h, w, 3)
( h
2 , w
4 , w
( h
8 , w
( h
( h
16 , w
( h
32 , w
64 , w
( h

2 , 32)
4 , 64)
8 , 128)
16 , 256)
32 , 256)
64 , 256)

2 , 3),( h
( h
2 , w
( h
4 , w
8 , w
( h
( h
16 , w
32 , w
( h
( h
64 , w
64 , w
( h

2 , w
2 , 6)
4 , 32)
8 , 64)
16 , 128)
32 , 256)
64 , 256)
64 , 256)

(256,1)

Dl

2 , w
( h

2 , 3)

(256, guidedlef t, guidedright)
(512, 1)

Layer Information

Output Shape

(h, w, 16)
CONV-(C16, K7×7, S1×1,Psame),IN,ReLU
( h
2 , w
2 , 32)
CONV-(C32, K4×4, S2×2,Psame),IN,ReLU
( h
4 , w
4 , 64)
CONV-(C64, K4×4, S2×2,Psame),IN,ReLU
8 , w
( h
CONV-(C128, K4×4, S2×2,Psame),IN,ReLU
8 , 128)
16 , w
CONV-(C256, K4×4, S2×2,Psame),IN,ReLU ( h
16 , 256)
32 , w
CONV-(C256, K4×4, S2×2,Psame),IN,ReLU ( h
32 , 256)

(FC-256× h

FC-256
Concatenation
32 × w
DECONV-(C256,K3×3,S 1
DECONV-(C128,K3×3,S 1
DECONV-(C64,K3×3,S 1
DECONV-(C32,K3×3,S 1
DECONV-(C16,K3×3,S 1

32 ,ReLU)
2 × 1
2 × 1
2 × 1
2 × 1
2 × 1

2 ),IN,ReLU
2 ),IN,ReLU
2 ),IN,ReLU
2 ),IN,ReLU
2 ),IN,ReLU

CONV-(C3, K7×7, S1×1,Psame),Tanh

32 , 256)

(256, 1)
(512, 1)
32 , w
( h
16 , w
( h
16 , 256)
( h
8 , w
8 , 128)
4 , w
( h
4 , 64)
( h
2 , w
2 , 32)
(h, w, 16)
(h, w, 3)

Table 4: Generator architecture.

Layer Information

output

CONV-(C32, K5×5, S2×2,Psame), LReLU

CONV-(C64, K5×5, S2×2,Psame),LReLU

CONV-(C128, K5×5, S2×2,Psame),LReLU

CONV-(C256, K5×5, S2×2,Psame), LReLU

CONV-(C256, K5×5, S2×2,Psame), LReLU

CONV-(C256, K5×5, S2×2,Psame), LReLU

FC-256

Concatenation

CONV-(C32, K5×5, S2×2,Psame), Leaky ReLU

CONV-(C64, K5×5, S2×2,Psame),LReLU

CONV-(C128, K5×5, S2×2,Psame),LReLU

CONV-(C256, K5×5, S2×2,Psame), LReLU

CONV-(C256, K5×5, S2×2,Psame), LReLU

FC-256

FC-2

FC-1

( h
2 , w
( h
4 , w
8 , w
( h
( h
16 , w
( h
32 , w
64 , w
( h

2 , 32)
4 , 64)
8 , 128)
16 , 256)
32 , 256)
64 , 256)

(256,1)

( h
2 , w
( h
4 , w
( h
8 , w
16 , w
( h
( h
32 , w
64 , w
( h

2 , 6)
4 , 32)
8 , 64)
16 , 128)
32 , 256)
64 , 256)

(256,1)

(2,1)

(256,1)

(1,1)

(global, locallef t, localright, guidedlef t, guidedlef t)

Concatenation,FC-256,ReLU

Table 5: Global discriminator Dg and Local discriminator Dl architecture.

12

B Additional Qualitative Results

Figure 6: More results of GazeGAN for gaze correction in the wild.

13

Figure 7: More results of GazeGAN for gaze correction in the wild with large pose.

14

