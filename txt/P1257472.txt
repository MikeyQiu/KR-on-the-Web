Uniﬁed Embedding and Metric Learning for Zero-Exemplar Event Detection

Noureldien Hussein, Efstratios Gavves, Arnold W.M. Smeulders
QUVA Lab, University of Amsterdam
{nhussein,egavves,a.w.m.smeulders}@uva.nl

7
1
0
2
 
y
a
M
 
5
 
 
]

V
C
.
s
c
[
 
 
1
v
8
4
1
2
0
.
5
0
7
1
:
v
i
X
r
a

Abstract

Event detection in unconstrained videos is conceived
as a content-based video retrieval with two modalities:
textual and visual. Given a text describing a novel event,
the goal is to rank related videos accordingly. This task is
zero-exemplar, no video examples are given to the novel
event.
Related works train a bank of concept detectors on external
data sources. These detectors predict conﬁdence scores for
test videos, which are ranked and retrieved accordingly. In
contrast, we learn a joint space in which the visual and
textual representations are embedded. The space casts a
novel event as a probability of pre-deﬁned events. Also, it
learns to measure the distance between an event and its
related videos.
Our model is trained end-to-end on publicly available
EventNet. When applied to TRECVID Multimedia Event
Detection dataset, it outperforms the state-of-the-art by a
considerable margin.

1. Introduction

TRECVID Multimedia Event Detection (MED) [1, 2] is
a retrieval task for event videos, with the reputation of being
realistic. It comes in two ﬂavors: few-exemplar and zero-
exemplar, where the latter means that no video example is
known to the model. Although expecting a few examples
seems reasonable, in practice this implies that the user must
already have an index of any possible query, making it very
limited. In this paper, we focus on event video search with
zero exemplars.

Retrieving videos of never-seen events, such as “reno-
vating home”, without any video exemplar poses several
challenges. One challenge is how to bridge the gap be-
tween the visual and the textual semantics [3, 4, 5]. One
approach [3, 6, 7, 8, 9, 10] is to learn a dictionary of con-
cept detectors on external data source. Then, scores for test
videos are predicted using these detectors. Test videos are
then ranked and retrieved accordingly. The inherent weak-

Figure 1. We pose the problem of zero-exemplar event detection as
learning from a repository of pre-deﬁned events. Given video ex-
emplars of events “removing drywall” or “ﬁt wall times”, one may
detect a novel event “renovate home” as a probability distribution
over the predeﬁned events.

ness of this approach is that the presentation of a test video
is reduced to a limited vocabulary from the concept dictio-
nary. Another challenge is how to overcome the domain
difference between training and test events. While Seman-
tic Query Generation (SQG) [3, 8, 9, 11] mitigates this chal-
lenge by extracting keywords from the event query, it does
not address how relevant these keywords to the event itself.
For example, keyword “person” is not relevant to event “car
repair” as it is to “ﬂash mob gathering”.

Our entry to zero-exemplar events is that they gener-
ally have strong semantic correlations [12, 13] with other
possibly seen events. For instance, the novel event “ren-
ovating home” is related to “ﬁt wall tiles”, “remove dry-
wall”, or even to “paint door”. Novel events can, there-
fore, be casted on a repository of prior events, for which
knowledge sources in various forms are available before-
hand, such as the videos, as in EventNet [14], or articles,
as in WikiHow [15]. Not only do these sources provide
video examples of a large –but still limited– set of events,
but also they provide an association of text description of
events with their corresponding videos. A text article can
describe the event in words: what is it about, what are the

1

details and what are the semantics. We note that such a
visual-textual repository of events may serve as a knowl-
edge source, by which we can interpret novel event queries.
For Zero-exemplar Event Detection (ZED), we propose

a neural model with the following novelties:
1. We formulate a uniﬁed embedding for multiple modal-
ities (e.g. visual and textual) that enables a contrastive
metric for maximum discrimination between events.
2. A textual embedding poses the representation of a novel
event as a probability of predeﬁned events, such that it
spans a much larger space of admissible expressions.
3. We exploit a single data source, comprising pairs of event
articles and related videos. A single source rather enables
end-to-end learning from multi-modal individual pairs.

We empirically shows that our novelties result in perfor-
mance improvement. We evaluate the model on TRECVID
Multimedia Event Detection (MED) 2013 [1] and 2014 [2].
Our results show signiﬁcant improvement over the state-of-
the-art.

2. Related Work

Figure 2. Three families of methods for zero-exemplar event de-
tection: (a), (b) and (c). They build on top of feature represen-
tations learned a priori (i.e. initial representations), such as CNN
features x for a video v or word2vec features y for event text query
t. In a post-processing step, the distance θ is measured between
the embedded features. In contrast, our model rather falls in a new
family, depicted in (d), for it learns uniﬁed embedding with metric
loss using single data source.

We identify three families of methods for ZED, as in ﬁg-

ure 2 (a), (b) and (c).
Visual Embedding and Textual Retrieval. As in ﬁg-
ure 2(a), given a video vi represented as x ∈ X and a re-
lated text t represented as y ∈ Y. Then, a visual model fV
is trained to project x as yv ∈ Y such that the distance is
minimized between (yv, y). In test time, video ranking and

retrieval is done using distance metric between the projected
test video yt and test query representation y.

[16, 17] project the visual feature x of a web video v
into term-vector representation y of the video’s textual title
t. However, during training, the model makes use of the
text query of the test events to learn better term-vector rep-
resentation. Consequently, this limits the generalization for
novel event queries.
Textual Embedding and Visual Retrieval. As in ﬁg-
ure 2(b), a given text query t is projected into xt ∈ X using
pre-trained or learned language model fT .

[18] makes use of freely-available weekly-tagged web
videos. Then it propagates tags to test videos from its near-
est neighbors. Methods [7, 8, 9, 10, 3] have similar ap-
proach. Given a text query t, Semantic Query Generation
(SQG) extracts N most related concepts {ci, i ∈ N } to the
test query. Then, pre-trained concept detectors predict prob-
ability scores {si, i ∈ N } for a test video v. Aggregating
these probabilities results in the ﬁnal video score sv, upon
which videos are ranked and retrieved. [9] learns weighted
averaging.

The shortcoming of this family is that expressing a
video as probability scores of few concepts is under-
representation. Any concept that exists in the video but is
missing in the concept dictionary is thus unrepresented.
Visual-Textual Embedding and Semantic Retrieval. As
in ﬁgure 2(c), visual fV and textual fT models are trained
to project both of the visual x and textual y features into a
semantic space Z. During test, ranking score is the distance
between the projections zv, zt in the semantic space Z.

[19] projects video concepts into a high-dimensional
lexicon space. Separately, it projects concept-based features
to the space, which overcomes the lexicon mismatch be-
tween the query and the video concepts. [20] embeds a fu-
sion of low and mid-level visual features into distributional
semantic manifold [21, 22]. In a separate step, it embeds
text-based concepts into the manifold.

The third family, see ﬁgure 2(c), is superior to the others,
see ﬁgure 2(a), (b). However, one drawback of [19, 20] is
separately embedding both the visual and textual features
zv, zt. This leads to another drawback, having to measure
the distance between (zv, zt) in a post-processing step (e.g.
cosine similarity).
Uniﬁed Embedding and Metric Learning Retrieval Our
method rather falls into a new family, see ﬁgure 2(d), and it
overcomes the shortcomings of [19, 20] by the following. It
is trained on a single data source, enabling a uniﬁed embed-
ding for features of multiple modalities into a metric space.
Consequently, the distance between the embedded features
is measured by the model using the learned metric space.

Auxiliary Methods Independent to the previous works, the
following techniques have been used to improve the results:

Figure 3. Model overview. Using dataset Dz of
M event categories and N videos. Each event has
a text article and a few videos. Given a video x
with text title k, belonging to an event with arti-
cle t, we extract features x, yk, yt respectively.
At the top, network fT learns to classify the title
feature yk into one of M event categories. In the
middle, we borrow the network fT to embed the
event article’s feature yt as zt ∈ Z. Then, at the
bottom, the network fV learns to embed the video
feature x as zv ∈ Z such that the distance be-
tween (cid:0)zv, zt(cid:1) is minimized, in the learned metric
space Z.

self-paced reranking [23], pseudo-relevance feedback [24],
event query manual intervention [25], early fusion of fea-
tures (action [26, 27, 28, 29, 30] or acoustic [31, 32, 33]) or
late fusion of concept scores [17]. All these contributions
may be applied to our method.

Visual Representation. ConvNets [34, 35, 36, 37] pro-
vide frame-level representation. To tame them into video-
level counterpart, literature use: i- frame-level ﬁltering [38]
ii- vector encoding [39, 40] iii- learned pooling and re-
counting [10, 41] iv- average pooling [16, 17]. Also, low-
level action [28, 29], mid-level action [26, 27] or acous-
tic [31, 32, 33] features can be used. Textual Represen-
tation. To represent text, literature use: i- sequential mod-
els [42] ii- continuous word-space representations [22, 43]
iii- topic models [44, 45] iv- dictionary-space representa-
tion [17].

3. Method

3.1. Overview

Our goal is zero-exemplar retrieval of event videos with
respect to their relevance to a novel textual description of
an event. More speciﬁcally, for the zero-exemplar video
dataset Dz = {vz
i }, i = 1, . . . , L and given any future, tex-
tual event description tz, we want to learn a model f (·) that
i according to the relevance to tz, namely:
ranks the videos vz

tz : vz

i (cid:31) vz

j → f (vz

i , tz) > f (vz

j , tz).

(1)

3.2. Model

Since we focus on zero-exemplar setting, we cannot ex-
pect any training data directly relevant to the test queries.
As such, we cannot directly optimize our model for the
parameters WT , WV in eq. (3). In the absence of any di-
rect data, we resort to external knowledge databases. More
speciﬁcally, we propose to cast future novel query descrip-
tions as a convex combination of known query descriptions
in external databases, where we can measure their relevance
the database videos.

We start from a dataset Dz = {vi, ki, lj, tj} ,

i =
1, . . . , N, j = 1, . . . , M organized by an event taxonomy,
where we do not neither expect nor require the events to
overlap with any future event queries. The dataset is com-
posed of M events. Each event is associated with a textual,
article description of the event, analyzing different aspects
of it, such as: (i) the typical appearance of subjects and
objects (ii) it’s procedures (iii) the steps towards complet-
ing task associated with it. The dataset contains in total N
videos, with vi denoting the i-th video in the dataset with
metadata ki, e.g. the title of the video. A video is associ-
ated with an event label li and the article description ti of
the event it belongs to. Since multiple videos belong to the
same event, they share the article description of such event.
The ultimate goal of our model is zero-exemplar search
for event videos. Namely, provided unknown text queries
by the user, we want to retrieve those videos that are rele-
vant. We illustrate our proposed model during training in
ﬁgure 3. The model is composed of two components, a
textual embedding fT (·), a visual embedding fV (·). Our
ultimate goal is the ranking of videos, vi (cid:31) vj (cid:31) vk with
respect to their relevance to a query description, or in pair-
wise terms vi (cid:31) vj, vj (cid:31) vk and vi (cid:31) vk.

Let us assume a pair of videos vi, vj and query descrip-
tion t, where video vi is more relevant to the query t than vj.
Our goal is a model that learns to put videos in the correct
relative order, namely (vi, t) (cid:31) (vj, t). This is equivalent
to a model that learns visual-textual embeddings such that
j , where dtv
i < dtv
dtv
is the distance between visual-textual
i
embeddings of (vi, t), dtv
is the same for (vj, t). Since we
j
want to compare distances between pairs (vi, t), (vj, t), we
pose the learning of our model as the minimization of a con-
trastive loss [46]:

Lcon =

hi · d2

i + (1 − hi) max(1 − di, 0)2,

(2)

1
2N

N
(cid:88)

i=1

di = (cid:107)fT (ti; WT ) − fV (vi; WV )(cid:107)2,

(3)

where fT (ti; WT ) is the projection of the query descrip-
tion ti into the uniﬁed metric space Z parameterized by

WT , fV (vi; WV ) is the projection of a video vi onto the
same space Z parameterized by WV and hi a target vari-
able that equals to 1 when the i-th video is relevant to the
query description ti and 0 otherwise. Naturally, to optimize
eq. (2), we ﬁrst need to deﬁne the projections fT (·; WT )
and fV (·; WV ) in eq. (3).

Textual Embedding. The textual embedding component of
our model, fT (·; WT ), is illustrated in ﬁgure 3 (top). This
component is dedicated to learn a projection of a textual
input –including any future event queries t– on to the uni-
ﬁed space Z. Before detailing our model fT , however, we
note that that the textual embedding can be employed not
only with event article descriptions, but also with any other
textual information that might be associated to the dataset
videos, such as textual metadata. Although we expect the
video title not to be as descriptive as the associated article,
they may still be able to offer some discriminative informa-
tion as previously shown [16, 17] which can be associated
to the event category.

We model the textual embedding as a shallow (two lay-
ers) multi-layer perceptron (MLP). For the ﬁrst layer we
employ a ReLU nonlinearity. The second layer serves a
dual purpose. First, it projects the article description of an
event on the uniﬁed space Z. This projection is category-
speciﬁc, namely different videos that belong to the same
event will share the projection. Second, it can project any
video-speciﬁc textual metadata into the uniﬁed space. We,
therefore, propose to embed the title metadata ki, which is
uniquely associated with a video, not an event category. To
this end, we opt for softmax nonlinearity for the second
layer, followed by an additional logistic loss term to penal-
ized misprediction of titles mi with respect to the video’s
event label yj

i , namely

Llog =

−yj

i log f j

T (ki; WT ).

(4)

N
(cid:88)

M
(cid:88)

i=1

j=1

Overall, the textual embedding f j

T is trained with a dual
loss in mind. The ﬁrst loss term, see eq. (2) (3) takes care
that the ﬁnal network learns event-relevant textual projec-
tions. The second loss term, see eq. (4), takes care that the
ﬁnal network does not overﬁt to the particular event article
descriptions. The latter is crucial because the event article
descriptions in Dz will not overlap with the future event
queries, since we are in a zero-exemplar retrieval setting.
As such, training the textual embedding to be optimal only
for these event descriptions will likely result in severe over-
ﬁtting. Our goal and hope is that the ﬁnal textual embed-
ding model fT will capture both event-aware and video-
discriminative textual features.

Visual Embedding. The visual embedding component of
our model, fV (·; WV ), is illustrated in ﬁgure 3 (bottom).

This component is dedicated to learn a projection from the
visual input, namely the videos in our zero-exemplar dataset
Dz, into the uniﬁed metric space Z. The goal is to project
the videos belonging to semantically similar events; project
them into a similar region in the space. We model the visual
embedding fV (vi; WV ) using a shallow (two layers) multi-
layer perceptron with tanh nonlinearities, applied to any
visual feature for video vi.

End-to-End Training. At each training forward-pass, the
model is given a triplet of data inputs, an event description
ti, a related video vi and video title ki. From eq. (3) we
observe that the visual embedding fV (vi; WV ) is encour-
aged to minimize its distance with the output of the textual
embedding fT (ti; WT ). In the end, all the modules of the
proposed model are differentiable. Therefore, we train our
model in an end-to-end manner by minimizing the follow-
ing objective

LU ,

arg min
WV ,WT
LU = Lcon + Llog.

(5)

input vi

For the triplet input (vi, ti, ki), we rely on external repre-
sentations, since our ultimate goal is zero-exemplar search.
Strictly speaking, a visual
is represented as
CNN [35] feature vector, while textual inputs ti, ki are rep-
resented as LSI [45] or Doc2Vec [43] feature vectors. How-
ever, given that these external representations rely on neural
network architectures, if needed, they could also be further
ﬁne-tuned. We choose to freeze CNN and Doc2Vec mod-
ules to speed up training. Finally, in this paper we refer to
our main model with uniﬁed embedding, as modelU .

Inference. After training, we ﬁx the parameters (WV , WT ).
At test time, we set our function f (·) from eq. (1) to be
equivalent to the distance function from eq. (??). Hence, at
test time, we compute the Euclidean distance in the learned
metric space Z between the embeddings (zv, zt) of test
video v and novel event description t, respectively.

4. Experiments

4.1. Datasets

Before delving into the details of our experiments, ﬁrst

we describe the external knowledge sources we use.

Training dataset. We leverage videos and articles from
publicly available datasets. EvenNet [14] is a dataset of
∼90k event videos, harvested from YouTube and catego-
rized into 500 events in hierarchical form according to the
events’ ontology. Each event category contains around 180
videos. Each video is coupled with a text title, few tags and
related event’s ontology.

We exploit the fact that all events in EventNet are har-
vested from WikiHow [15] – a website for How-To articles
covering a wide spectrum of human activities. For instance:
“How to Feed a Dog” or “How to Arrange Flowers”. Thus,
we crawl WikiHow to get the articles related to all the events
in EventNet.

Test dataset. As the task is zero-exemplar, the test sets are
different from the training. While EventNet serves as the
training, the following serve as the test: TRECVID MED-
13 [1] and MED-14 [1].
In details, they are datasets of
videos for events. They comprise 27k videos. There are two
versions, MED-13 and MED-14 with 20 events for each.
Since 10 events overlap, the result is 30 different events in
total. Each event is coupled with short textual description
(title and deﬁnition).

4.2. Implementation Details

Video Features. To represent a video v, we uniformly sam-
ple a frame every one second. Then, using ResNet [35], we
extract pool5 CNN features for the sampled frames. Then,
we average pool the frame-level features to get the video-
level feature xv. We experiment different features from dif-
ferent CNN models: ResNet (prob, fc1000), VGG [37]
(fc6, fc7), GoogLeNet [47] (pool5, fc1024), and
Places365 [48] (fc6, fc7,fc8) except we ﬁnd ResNet
pool5 to be the best. We only use ResNet pool5 and we
don’t fuse multiple CNN features.

Text Features. We choose topic modeling [44, 45], as it
is well-suited for long (and sometimes noisy) text articles.
We train LSI topic model [45] on Wikipedia corpus [49].
We experiment different latent topics ranging from 300
to 6000, expect we found 2500 to be the best. Also,
we experiment other textual representations as LDA [44],
SkipThoughts [50] and Doc2Vec [43]. To extract a feature
from an event article k or video title t, ﬁrst we preprocess
the text using standard MLP steps: tokenization, lemmati-
zation and stemming. Then, for k, t we extract 2500-D LSI
features yk, yt, respectively. The same steps apply to MED
text queries.

Model Details.
Our visual and textual embeddings
fV (·), fT (·) are learned on top of the aforementioned visual
and textual features (xv, yk, yt). fT (·) is a 1-hidden layer
MLP classiﬁer with ReLU for hidden, softmax for out-
put, logistic loss and 2500-2500-500 neurons for
the input, hidden, and output layers, respectively. Similarly,
fV (·) is a 1-hidden layer MLP regressor with ReLU for hid-
den, contrastive loss and 2048-2048-500 neurons
for the input, hidden, and output layers, respectively. Our
code is made public1 to support further research.

1github.com/noureldien/unified_embedding

4.3. Textual Embedding

(a) LSI Features

(b) Embedded Features

Figure 4. Our textual embedding (b) maps MED to EventNet
events better than LSI features. Each dot in the matrix shows the
similarity between MED and EventNet events.

Here, we qualitatively demonstrate the beneﬁt of the tex-
tual embedding fT (·). Figure 4 shows the similarity matrix
between MED and EventNet events. Each dot represents
how a MED event is similar to EventNet events. It shows
that our embedding (right) is better than LSI (left) in map-
ping MED to EventNet events. For example, LSI wrongly
maps “9: getting a vehicle unstuck” to “256: launch a boat”
while our embedding correctly maps it to “170: drive a car”.
Also, our embedding maps with higher conﬁdence than LSI,
as in “16: doing homework or study”.

(a) LSI Features

(b) Embedded Features

Figure 5. For 20 events of MED-14, our textual embedding (right)
is more discriminant than the LSI feature representation (left).
Each dot in the matrix shows how similar an event to all the others.

Figure 5 shows the similarity matrix for MED events,
where each dot represents how related any MED event to
all the others. Our textual embedding (right) is more dis-
criminant than on the LSI feature representation (left). For
example, LSI representation shows high semantic correla-
tion between events “34: ﬁxing musical instrument” and
“40: tuning musical instrument”, while our embedding dis-
criminate them.

Next, we quantitatively demonstrate the beneﬁt of the
textual embedding fT (·).
In contrast to the main model,
see section 3, we investigate baseline modelV , where we
discard the textual embedding fT (·) and consider only the
visual embedding fV (·). We project a video v on the LSI

(a) MED-13, visual embedding (modelV ).

(b) MED-13, separate embedding (modelS ).

(c) MED-13, uniﬁed embedding (modelU ).

(d) MED-14, visual embedding (modelV ).

(e) MED-14 separate embedding (modelS ).
Figure 6. We visualize the results of video embedding using the uniﬁed embedding modelU and baselines modelV , modelS . Each sub-ﬁgure
shows how discriminant the representation of the embedded videos. Each dot represents a projected video, while each pentagon-shape
represents a projected event description. We use t-SNE to visualize the result.

(f) MED-14, uniﬁed embedding (modelU ).

representation y of the related event t. Thus, this baseline
It is
falls in the ﬁrst family of methods, see ﬁgure 2(a).
optimized using mean-squared error (MSE) loss LV
mse, see
eq. 6. The result of this baseline is reported in section 5,
table 1.

LV

mse =

(cid:107)yi − fV (vi; WV )(cid:107)2
2.

(6)

1
N

N
(cid:88)

i=1

Also, we train another baseline modelC, which is similar
to the aforementioned V except instead of using MSE loss
LV
con, as follows:

mse, see eq. (6), it uses contrastive loss LC

LC

con =

hi · d2

i + (1 − hi) max(1 − di, 0)2,

(7)

1
2N

N
(cid:88)

i=1

di = (cid:107)yi − fV (vi; WV )(cid:107)2.

4.4. Uniﬁed Embedding and Metric Learning

In this experiment, we demonstrate the beneﬁt of the uni-
ﬁed embedding. In contrast to our model presented in sec-
tion 3, we investigate baseline modelS , where this baseline
does not learn joint embedding. Instead, it separately learns
visual fV (·) and textual fT (·) projections. We model these

projections as a shallow (2-layer) MLP trained to classify
the data input into 500 event categories, using logistic loss,
same as eq. (4).

We conduct another experiment to demonstrate the ben-
eﬁt of learning metric space. In contrast to our model pre-
sented in section 3, we investigate baseline modelN , where
we discard the metric learning layer. Consequently, this
baseline learns the visual embedding is a shallow (2 lay-
ers) multi-layer perceptron with tanh non linearities. Also,
we replace the contrastive loss Lc, see eq. (2) with mean-
squared error loss Lmse, namely

LN

mse =

(cid:107)fT (ti; WT ) − fV (vi; WV )(cid:107)2
2.

(8)

1
N

N
(cid:88)

i=1

During retrieval, this baseline embeds a test video vi and
novel text query ti as features zv, zt onto the common space
Z using textual and visual embeddings fT (·), fV (·), respec-
tively. However, in a post-processing step, retrieval score
si for the video vi is the cosine distance between (zv, zt).
Similarly, all test videos are scored, ranked and retrieved.
The results of the aforementioned baselines modelS and
modelN are reported in table 1.

Comparing Different Embeddings.

In the previous ex-

periments, we investigated several baselines of the uni-
ﬁed embedding (modelU ), namely visual-only embedding
(modelV ), separate visual-textual embedding (modelS ) and
non-metric visual-textual embedding (modelN ). In a quali-
tative manner, we compare the results of such embeddings.
As shown in ﬁgure 6, we use these baselines to embed event
videos of MED-13 and MED-14 datasets into the corre-
sponding spaces. At the same time, we project the textual
description of the events on the same space. Then, we use
t-SNE [51] to visualize the result on 2D manifold. As seen,
the uniﬁed embedding, see sub-ﬁgures 6(e), 6(f) learns
more discriminant representations than the other baselines,
see sub-ﬁgures 6(a), 6(b), 6(c) and 6(d). The same obser-
vation holds for both for MED-13 and MED-14 datasets.

4.5. Mitigating Noise in EventNet

Based of quantitative and qualitative analysis, we con-
clude that EventNet is noisy. Not only videos are uncon-
strained, but also some of the video samples are irrelevant
to their event categories. EvenNet dataset [14] is accom-
panied by 500-category CNN classiﬁer. It achieves top-1
and top-5 accuracies of 30.67% and 53.27%, respectively.
Since events in EventNet are structured as an ontological
hierarchy, there is a total of 19 high-level categories. The
classiﬁer achieves top-1 and top-5 accuracies of 38.91% and
57.67%, respectively, over these high-level categories.

Based on these observations, we prune EventNet to re-
move noisy videos. To this end, ﬁrst we represent each
video as average pooling of ResNet pool5 features. Then,
we follow the conventional 5-fold cross validation with 5
rounds. For each round, we split the dataset into 5 sub-
sets, 4 subsets Vt for training and the last Vp for pruning.
Then we train a 2-layer MLP for classiﬁcation. After train-
ing, we forward-pass the videos of Vp and rule-out the mis-
classiﬁed ones.

The intuition behind pruning is that we rather learn
salient event concepts using less video samples than learn
noisy concepts with more samples. Pruning reduced the to-
tal number of videos by 26%, from 90.2k to 66.7k. This
pruned dataset is all what we use in our experiments.

4.6. Latent Topics in LSI

When training LSI topic model on Wikipedia corpus, a
crucial parameter is the number of latent topics K the model
constructs. We observe improvements in the performance
directly proportional to increasing K. The main reason that
the bigger the value of K, the more discriminant the LSI
feature is. Figure 7 conﬁrms our understanding.

5. Results

Evaluation metric. Since we are addressing, in essence,
an information retrieval task, we rely on the average preci-
sion (AP) per event, and mean average precision (mAP) per

Figure 7. Similarity matrix between LSI features of MED-14
events. The more the latent topics (K) in LSI model, the higher
the feature dimension, and the more discriminant the feature.

dataset. We follow the standard evaluation method as in the
relevant literature [1, 2, 52].

Comparing against model baselines. In table 1, we re-
port the mAP score of our model baselines, previously dis-
cussed in the experiments, see section 4. The table clearly
shows the marginal contribution of each of novelty for the
proposed method.

Baseline Loss
modelV LV
modelC LC
modelS Llog
modelN LN
modelU LU

Metric fV (·) fT (·) MED13 MED14
10.76
12.31
13.49
14.36
16.67

11.90
13.29
15.60
15.92
17.86

(cid:55)
mse (6)
con (7) (cid:51)
(cid:55)
(4)
(cid:55)
mse (8)
(5) (cid:51)

(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)

Table 1. Comparison between the uniﬁed embedding and other
baselines. The uniﬁed embedding modelU achieves the best re-
sults on MED-13 and MED-14 datasets.

Comparing against related work. We report the perfor-
mance of our method, the uniﬁed embedding modelU on
TRECVID MED-13 and MED-14 datasets. When com-
pared with the related works, our method improves over
the state-of-the-art by a considerable margin, as shown in
table 2 and ﬁgure 8.

Method
TagBook [18]
Discovary [7]
Composition [8]
Classiﬁers [9]
VideoStory† [17]
VideoStory∗ [17]
This Paper (modelU )

ToM '15 12.90
ICAI '15 09.60
AAAI '16 12.64
CVPR '16 13.46
PAMI '16 15.90
PAMI '16 20.00
17.86

MED13 MED14
05.90
–
13.37
14.32
05.20
08.00
16.67

Table 2. Performance comparison between our model and related
works. We report the mean average precision (mAP%) for MED-
13 and MED-14 datasets.

It is important to point out that VideoStory† uses only ob-
ject feature representation, so its comparable to our method.

(a) MED-13 Dataset

Figure 8. Event detection accuracies: per-event average precision (AP%) and per-dataset mean average precision (mAP%) for MED-13
and MED-14 datasets. We compare our results against TagBook [18], Discovary [7], Composition [8], Classiﬁers [9] and VideoStory [17].

(b) MED-14 Dataset

However, VideoStory∗ uses motion feature representation
and expert text query (i.e. using term-importance matrix
H in [17]). To rule out the marginal effect of using dif-
ferent datasets and features, we train VideoStory and report
results in table 3. Clearly, CNN features and video exem-
plars in the training set can improve the model accuracy, but
our method improves against VideoStory when trained on
the same dataset and using the same features. Other works
(Classiﬁers [9], Composition [8]) use both image and ac-
tion concept classiﬁers. Nonetheless, our method improves
over them using only object-centric CNN feature represen-
tations.

Training Set

CNN Feat. MED14
Method
VideoStory VideoStory46k [17] GoogleNet 08.00
GoogleNet 11.84
VideoStory FCVID [53]
GoogleNet 14.52
VideoStory EventNet [14]
15.80
ResNet
VideoStory EventNet [14]
16.67
ResNet
This Paper EventNet [14]

proposed to learn metric space. This enables measuring
the similarities between the embedded modalities using this
very space.

We experimented the novelties and demonstrated how
they contribute to improving the performance. We comple-
mented this by improvements over the state-of-the-art by
considerable margin on MED-13 and MED-14 datasets.

However, the question still remains, how can we discrim-
inate between these two MED events “34: ﬁxing musical in-
strument” and “40: tuning musical instrument”. We would
like to argue that temporal modeling for human actions in
videos is of absolute necessity to achieve such ﬁne-grained
event recognition. In future research, we would like to focus
on human-object interaction in videos and how to model it
temporally.

Acknowledgment

We thank Dennis Koelma, Masoud Mazloom and Cees
Snoek2 for lending their insights and technical support for
this work.

Table 3. Our method improves over VideoStory when trained on
the same dataset and using the same feature representation.

References

6. Conclusion

In this paper, we presented a novel approach for detect-
ing events in unconstrained web videos, in a zero-exemplar
fashion. Rather than learning separate embeddings form
cross-modal datasets, we proposed a uniﬁed embedding
where several cross-modalities are jointly projected. This
enables end-to-end learning. On top of this, we exploited
the fact that zero-exemplar is posed as retrieval task and

[1] Paul Over, George Awad, Jon Fiscus, Greg Sanders, and Bar-
bara Shaw. Trecvid 2013–an introduction to the goals, tasks,
In TRECVID
data, evaluation mechanisms, and metrics.
Workshop, 2013. 1, 2, 5, 7

[2] Paul Over, Jon Fiscus, Greg Sanders, David Joy, Mar-
tial Michel, George Awad, Alan Smeaton, Wessel Kraaij,
and Georges Qu´enot. Trecvid 2014–an overview of the
In
goals, tasks, data, evaluation mechanisms and metrics.
TRECVID Workshop, 2014. 1, 2, 7

2{kolema,m.mazloom,cgmsnoek}@uva.nl

[3] Lu Jiang, Shoou-I Yu, Deyu Meng, Teruko Mitamura, and
Alexander G Hauptmann. Bridging the ultimate semantic
gap: A semantic search engine for internet videos. In ICMR,
2015. 1, 2

[19] Shuang Wu, Sravanthi Bondugula, Florian Luisier, Xiaodan
Zhuang, and Pradeep Natarajan. Zero-shot event detection
using multi-modal fusion of weakly supervised concepts. In
IEEE CVPR, 2014. 2

[4] Amirhossein Habibian, Thomas Mensink, and Cees GM
Snoek. Composite concept discovery for zero-shot video
event detection. In ICMR, 2014. 1

[5] Amirhossein Habibian, Thomas Mensink, and Cees GM
Snoek. Discovering semantic vocabularies for cross-media
retrieval. In ICMR, 2015. 1

[6] Masoud Mazloom, Efstrastios Gavves, and Cees G. M.
Snoek. Conceptlets: Selective semantics for classifying
video events. In IEEE TMM, 2014. 1

[7] Xiaojun Chang, Yi Yang, Alexander G Hauptmann, Eric P
Xing, and Yao-Liang Yu. Semantic concept discovery for
large-scale zero-shot event detection. In IJCAI, 2015. 1, 2,
7, 8

[8] Xiaojun Chang, Yi Yang, Guodong Long, Chengqi Zhang,
and Alexander G Hauptmann. Dynamic concept composi-
tion for zero-example event detection. In arXiv, 2016. 1, 2,
7, 8

[9] Xiaojun Chang, Yao-Liang Yu, Yi Yang, and Eric P Xing.
They are not equally reliable: Semantic event search using
differentiated concept classiﬁers. In IEEE CVPR, 2016. 1, 2,
7, 8

[10] Yi-Jie Lu. Zero-example multimedia event detection and re-
counting with unsupervised evidence localization. In ACM
MM, 2016. 1, 2, 3

[11] Lu Jiang, Shoou-I Yu, Deyu Meng, Yi Yang, Teruko Mi-
tamura, and Alexander G Hauptmann. Fast and accurate
content-based semantic search in 100m internet videos. In
ACM MM, 2015. 1

[12] Thomas Mensink, Efstratios Gavves, and Cees Snoek. Costa:
Co-occurrence statistics for zero-shot classiﬁcation. In IEEE
CVPR, 2014. 1

[13] E. Gavves, T. E. J. Mensink, T. Tommasi, C. G. M. Snoek,
and T Tuytelaars. Active transfer learning with zero-shot
priors: Reusing past datasets for future tasks. In IEEE ICCV,
2015. 1

[14] Guangnan Ye, Yitong Li, Hongliang Xu, Dong Liu, and
Shih-Fu Chang. Eventnet: A large scale structured concept
library for complex event detection in video. In ACM MM,
2015. 1, 4, 7, 8

[15] Wikihow. http://wikihow.com. 1, 5

[16] Amirhossein Habibian, Thomas Mensink, and Cees GM
Snoek. Videostory: A new multimedia embedding for few-
example recognition and translation of events. In ACM MM,
2014. 2, 3, 4

[17] Amirhossein Habibian, Thomas Mensink, and Cees GM
Snoek. Videostory embeddings recognize events when ex-
amples are scarce. In IEEE TPAMI, 2016. 2, 3, 4, 7, 8

[20] Mohamed Elhoseiny, Jingen Liu, Hui Cheng, Harpreet
Sawhney, and Ahmed Elgammal. Zero-shot event detection
by multimodal distributional semantic embedding of videos.
In arXiv, 2015. 2

[21] Tomas Mikolov, Quoc V Le, and Ilya Sutskever. Exploit-
ing similarities among languages for machine translation. In
arXiv, 2013. 2

[22] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
and Jeff Dean. Distributed representations of words and
phrases and their compositionality. In NIPS, 2013. 2, 3

[23] Lu Jiang, Deyu Meng, Teruko Mitamura, and Alexander G
Hauptmann. Easy samples ﬁrst: Self-paced reranking for
zero-example multimedia search. In ACM MM, 2014. 3

[24] Lu Jiang, Teruko Mitamura, Shoou-I Yu, and Alexander G
Hauptmann. Zero-example event search using multimodal
pseudo relevance feedback. In ICMR, 2014. 3

[25] Arnav Agharwal, Rama Kovvuri, Ram Nevatia,

and
Cees GM Snoek. Tag-based video retrieval by embedding se-
mantic content in a continuous word space. In IEEE WACV,
2016. 3

[26] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,
and Manohar Paluri. C3d: generic features for video analy-
sis. In arXiv, 2014. 3

[27] Karen Simonyan and Andrew Zisserman. Two-stream con-
volutional networks for action recognition in videos.
In
NIPS, 2014. 3

[28] Heng Wang, Alexander Kl¨aser, Cordelia Schmid, and
Cheng-Lin Liu. Action recognition by dense trajectories. In
IEEE CVPR, 2011. 3

[29] J Uijlings, IC Duta, Enver Sangineto, and Nicu Sebe. Video
classiﬁcation with densely extracted hog/hof/mbh features:
an evaluation of the accuracy/computational efﬁciency trade-
off. In IJMIR, 2015. 3

[30] B. Fernando, E. Gavves, J. Oramas, A. Ghodrati, and
T. Tuytelaars. Rank pooling for action recognition. In IEEE
TPAMI, 2016. 3

[31] Lindasalwa Muda, Mumtaj Begam, and I Elamvazuthi. Voice
recognition algorithms using mel frequency cepstral coefﬁ-
cient (mfcc) and dynamic time warping (dtw) techniques. In
arXiv, 2010. 3

[32] Anurag Kumar and Bhiksha Raj. Audio event detection us-

ing weakly labeled data. In arXiv, 2016. 3

[33] Liping Jing, Bo Liu, Jaeyoung Choi, Adam Janin, Julia
Bernd, Michael W Mahoney, and Gerald Friedland. A dis-
criminative and compact audio representation for event de-
tection. In ACM MM, 2016. 3

[18] Masoud Mazloom, Xirong Li, and Cees Snoek. Tagbook: A
semantic video representation without supervision for event
detection. In IEEE TMM, 2015. 2, 7, 8

[34] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document recog-
nition. In IEEE. 3

[53] Yu-Gang Jiang, Zuxuan Wu, Jun Wang, Xiangyang Xue, and
Shih-Fu Chang. Exploiting feature and class relationships in
video categorization with regularized deep neural networks.
In IEEE TPAMI, 2017. 8

[35] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In arXiv, 2015.
3, 4, 5

[36] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NIPS, 2012. 3

[37] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In arXiv,
2014. 3, 5

[38] Chuang Gan, Ting Yao, Kuiyuan Yang, Yi Yang, and Tao
Mei. You lead, we exceed: Labor-free video concept learning
by jointly exploiting web videos and images. In IEEE CVPR,
2016. 3

[39] Karen Simonyan, Omkar M Parkhi, Andrea Vedaldi, and An-
drew Zisserman. Fisher vector faces in the wild. In BMVC,
2013. 3

[40] Relja Arandjelovic and Andrew Zisserman. All about vlad.

In IEEE CVPR, 2013. 3

[41] Pascal Mettes, Jan C van Gemert, Spencer Cappallo, Thomas
Mensink, and Cees GM Snoek. Bag-of-fragments: Select-
ing and encoding video fragments for event detection and
recounting. In ICMR, 2015. 3

[42] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to

sequence learning with neural networks. In NIPS, 2014. 3

[43] Quoc V Le and Tomas Mikolov. Distributed representations
of sentences and documents. In ICML, 2014. 3, 4, 5

[44] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent

dirichlet allocation. In JMLR, 2003. 3, 5

[45] Scott Deerwester, Susan T Dumais, George W Furnas,
Thomas K Landauer, and Richard Harshman. Indexing by
latent semantic analysis. In JACS, 1990. 3, 4, 5

[46] Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning
a similarity metric discriminatively, with application to face
veriﬁcation. In IEEE CVPR, 2005. 3

[47] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In IEEE CVPR, 2015. 5

[48] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Antonio Tor-
ralba, and Aude Oliva. Places: An image database for deep
scene understanding. In arXiv, 2016. 5

[49] Wikipedia, 2016. http://wikipedia.com. 5

[50] Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard
Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.
Skip-thought vectors. In NIPS, 2015. 5

[51] Laurens van der Maaten and Geoffrey Hinton. Visualizing

data using t-sne. In JMLR, 2008. 7

[52] Yu-Gang Jiang, Guangnan Ye, Shih-Fu Chang, Daniel El-
lis, and Alexander C Loui. Consumer video understanding:
A benchmark database and an evaluation of human and ma-
chine performance. In ICMR, 2011. 7

Uniﬁed Embedding and Metric Learning for Zero-Exemplar Event Detection

Noureldien Hussein, Efstratios Gavves, Arnold W.M. Smeulders
QUVA Lab, University of Amsterdam
{nhussein,egavves,a.w.m.smeulders}@uva.nl

7
1
0
2
 
y
a
M
 
5
 
 
]

V
C
.
s
c
[
 
 
1
v
8
4
1
2
0
.
5
0
7
1
:
v
i
X
r
a

Abstract

Event detection in unconstrained videos is conceived
as a content-based video retrieval with two modalities:
textual and visual. Given a text describing a novel event,
the goal is to rank related videos accordingly. This task is
zero-exemplar, no video examples are given to the novel
event.
Related works train a bank of concept detectors on external
data sources. These detectors predict conﬁdence scores for
test videos, which are ranked and retrieved accordingly. In
contrast, we learn a joint space in which the visual and
textual representations are embedded. The space casts a
novel event as a probability of pre-deﬁned events. Also, it
learns to measure the distance between an event and its
related videos.
Our model is trained end-to-end on publicly available
EventNet. When applied to TRECVID Multimedia Event
Detection dataset, it outperforms the state-of-the-art by a
considerable margin.

1. Introduction

TRECVID Multimedia Event Detection (MED) [1, 2] is
a retrieval task for event videos, with the reputation of being
realistic. It comes in two ﬂavors: few-exemplar and zero-
exemplar, where the latter means that no video example is
known to the model. Although expecting a few examples
seems reasonable, in practice this implies that the user must
already have an index of any possible query, making it very
limited. In this paper, we focus on event video search with
zero exemplars.

Retrieving videos of never-seen events, such as “reno-
vating home”, without any video exemplar poses several
challenges. One challenge is how to bridge the gap be-
tween the visual and the textual semantics [3, 4, 5]. One
approach [3, 6, 7, 8, 9, 10] is to learn a dictionary of con-
cept detectors on external data source. Then, scores for test
videos are predicted using these detectors. Test videos are
then ranked and retrieved accordingly. The inherent weak-

Figure 1. We pose the problem of zero-exemplar event detection as
learning from a repository of pre-deﬁned events. Given video ex-
emplars of events “removing drywall” or “ﬁt wall times”, one may
detect a novel event “renovate home” as a probability distribution
over the predeﬁned events.

ness of this approach is that the presentation of a test video
is reduced to a limited vocabulary from the concept dictio-
nary. Another challenge is how to overcome the domain
difference between training and test events. While Seman-
tic Query Generation (SQG) [3, 8, 9, 11] mitigates this chal-
lenge by extracting keywords from the event query, it does
not address how relevant these keywords to the event itself.
For example, keyword “person” is not relevant to event “car
repair” as it is to “ﬂash mob gathering”.

Our entry to zero-exemplar events is that they gener-
ally have strong semantic correlations [12, 13] with other
possibly seen events. For instance, the novel event “ren-
ovating home” is related to “ﬁt wall tiles”, “remove dry-
wall”, or even to “paint door”. Novel events can, there-
fore, be casted on a repository of prior events, for which
knowledge sources in various forms are available before-
hand, such as the videos, as in EventNet [14], or articles,
as in WikiHow [15]. Not only do these sources provide
video examples of a large –but still limited– set of events,
but also they provide an association of text description of
events with their corresponding videos. A text article can
describe the event in words: what is it about, what are the

1

details and what are the semantics. We note that such a
visual-textual repository of events may serve as a knowl-
edge source, by which we can interpret novel event queries.
For Zero-exemplar Event Detection (ZED), we propose

a neural model with the following novelties:
1. We formulate a uniﬁed embedding for multiple modal-
ities (e.g. visual and textual) that enables a contrastive
metric for maximum discrimination between events.
2. A textual embedding poses the representation of a novel
event as a probability of predeﬁned events, such that it
spans a much larger space of admissible expressions.
3. We exploit a single data source, comprising pairs of event
articles and related videos. A single source rather enables
end-to-end learning from multi-modal individual pairs.

We empirically shows that our novelties result in perfor-
mance improvement. We evaluate the model on TRECVID
Multimedia Event Detection (MED) 2013 [1] and 2014 [2].
Our results show signiﬁcant improvement over the state-of-
the-art.

2. Related Work

Figure 2. Three families of methods for zero-exemplar event de-
tection: (a), (b) and (c). They build on top of feature represen-
tations learned a priori (i.e. initial representations), such as CNN
features x for a video v or word2vec features y for event text query
t. In a post-processing step, the distance θ is measured between
the embedded features. In contrast, our model rather falls in a new
family, depicted in (d), for it learns uniﬁed embedding with metric
loss using single data source.

We identify three families of methods for ZED, as in ﬁg-

ure 2 (a), (b) and (c).
Visual Embedding and Textual Retrieval. As in ﬁg-
ure 2(a), given a video vi represented as x ∈ X and a re-
lated text t represented as y ∈ Y. Then, a visual model fV
is trained to project x as yv ∈ Y such that the distance is
minimized between (yv, y). In test time, video ranking and

retrieval is done using distance metric between the projected
test video yt and test query representation y.

[16, 17] project the visual feature x of a web video v
into term-vector representation y of the video’s textual title
t. However, during training, the model makes use of the
text query of the test events to learn better term-vector rep-
resentation. Consequently, this limits the generalization for
novel event queries.
Textual Embedding and Visual Retrieval. As in ﬁg-
ure 2(b), a given text query t is projected into xt ∈ X using
pre-trained or learned language model fT .

[18] makes use of freely-available weekly-tagged web
videos. Then it propagates tags to test videos from its near-
est neighbors. Methods [7, 8, 9, 10, 3] have similar ap-
proach. Given a text query t, Semantic Query Generation
(SQG) extracts N most related concepts {ci, i ∈ N } to the
test query. Then, pre-trained concept detectors predict prob-
ability scores {si, i ∈ N } for a test video v. Aggregating
these probabilities results in the ﬁnal video score sv, upon
which videos are ranked and retrieved. [9] learns weighted
averaging.

The shortcoming of this family is that expressing a
video as probability scores of few concepts is under-
representation. Any concept that exists in the video but is
missing in the concept dictionary is thus unrepresented.
Visual-Textual Embedding and Semantic Retrieval. As
in ﬁgure 2(c), visual fV and textual fT models are trained
to project both of the visual x and textual y features into a
semantic space Z. During test, ranking score is the distance
between the projections zv, zt in the semantic space Z.

[19] projects video concepts into a high-dimensional
lexicon space. Separately, it projects concept-based features
to the space, which overcomes the lexicon mismatch be-
tween the query and the video concepts. [20] embeds a fu-
sion of low and mid-level visual features into distributional
semantic manifold [21, 22]. In a separate step, it embeds
text-based concepts into the manifold.

The third family, see ﬁgure 2(c), is superior to the others,
see ﬁgure 2(a), (b). However, one drawback of [19, 20] is
separately embedding both the visual and textual features
zv, zt. This leads to another drawback, having to measure
the distance between (zv, zt) in a post-processing step (e.g.
cosine similarity).
Uniﬁed Embedding and Metric Learning Retrieval Our
method rather falls into a new family, see ﬁgure 2(d), and it
overcomes the shortcomings of [19, 20] by the following. It
is trained on a single data source, enabling a uniﬁed embed-
ding for features of multiple modalities into a metric space.
Consequently, the distance between the embedded features
is measured by the model using the learned metric space.

Auxiliary Methods Independent to the previous works, the
following techniques have been used to improve the results:

Figure 3. Model overview. Using dataset Dz of
M event categories and N videos. Each event has
a text article and a few videos. Given a video x
with text title k, belonging to an event with arti-
cle t, we extract features x, yk, yt respectively.
At the top, network fT learns to classify the title
feature yk into one of M event categories. In the
middle, we borrow the network fT to embed the
event article’s feature yt as zt ∈ Z. Then, at the
bottom, the network fV learns to embed the video
feature x as zv ∈ Z such that the distance be-
tween (cid:0)zv, zt(cid:1) is minimized, in the learned metric
space Z.

self-paced reranking [23], pseudo-relevance feedback [24],
event query manual intervention [25], early fusion of fea-
tures (action [26, 27, 28, 29, 30] or acoustic [31, 32, 33]) or
late fusion of concept scores [17]. All these contributions
may be applied to our method.

Visual Representation. ConvNets [34, 35, 36, 37] pro-
vide frame-level representation. To tame them into video-
level counterpart, literature use: i- frame-level ﬁltering [38]
ii- vector encoding [39, 40] iii- learned pooling and re-
counting [10, 41] iv- average pooling [16, 17]. Also, low-
level action [28, 29], mid-level action [26, 27] or acous-
tic [31, 32, 33] features can be used. Textual Represen-
tation. To represent text, literature use: i- sequential mod-
els [42] ii- continuous word-space representations [22, 43]
iii- topic models [44, 45] iv- dictionary-space representa-
tion [17].

3. Method

3.1. Overview

Our goal is zero-exemplar retrieval of event videos with
respect to their relevance to a novel textual description of
an event. More speciﬁcally, for the zero-exemplar video
dataset Dz = {vz
i }, i = 1, . . . , L and given any future, tex-
tual event description tz, we want to learn a model f (·) that
i according to the relevance to tz, namely:
ranks the videos vz

tz : vz

i (cid:31) vz

j → f (vz

i , tz) > f (vz

j , tz).

(1)

3.2. Model

Since we focus on zero-exemplar setting, we cannot ex-
pect any training data directly relevant to the test queries.
As such, we cannot directly optimize our model for the
parameters WT , WV in eq. (3). In the absence of any di-
rect data, we resort to external knowledge databases. More
speciﬁcally, we propose to cast future novel query descrip-
tions as a convex combination of known query descriptions
in external databases, where we can measure their relevance
the database videos.

We start from a dataset Dz = {vi, ki, lj, tj} ,

i =
1, . . . , N, j = 1, . . . , M organized by an event taxonomy,
where we do not neither expect nor require the events to
overlap with any future event queries. The dataset is com-
posed of M events. Each event is associated with a textual,
article description of the event, analyzing different aspects
of it, such as: (i) the typical appearance of subjects and
objects (ii) it’s procedures (iii) the steps towards complet-
ing task associated with it. The dataset contains in total N
videos, with vi denoting the i-th video in the dataset with
metadata ki, e.g. the title of the video. A video is associ-
ated with an event label li and the article description ti of
the event it belongs to. Since multiple videos belong to the
same event, they share the article description of such event.
The ultimate goal of our model is zero-exemplar search
for event videos. Namely, provided unknown text queries
by the user, we want to retrieve those videos that are rele-
vant. We illustrate our proposed model during training in
ﬁgure 3. The model is composed of two components, a
textual embedding fT (·), a visual embedding fV (·). Our
ultimate goal is the ranking of videos, vi (cid:31) vj (cid:31) vk with
respect to their relevance to a query description, or in pair-
wise terms vi (cid:31) vj, vj (cid:31) vk and vi (cid:31) vk.

Let us assume a pair of videos vi, vj and query descrip-
tion t, where video vi is more relevant to the query t than vj.
Our goal is a model that learns to put videos in the correct
relative order, namely (vi, t) (cid:31) (vj, t). This is equivalent
to a model that learns visual-textual embeddings such that
j , where dtv
i < dtv
dtv
is the distance between visual-textual
i
embeddings of (vi, t), dtv
is the same for (vj, t). Since we
j
want to compare distances between pairs (vi, t), (vj, t), we
pose the learning of our model as the minimization of a con-
trastive loss [46]:

Lcon =

hi · d2

i + (1 − hi) max(1 − di, 0)2,

(2)

1
2N

N
(cid:88)

i=1

di = (cid:107)fT (ti; WT ) − fV (vi; WV )(cid:107)2,

(3)

where fT (ti; WT ) is the projection of the query descrip-
tion ti into the uniﬁed metric space Z parameterized by

WT , fV (vi; WV ) is the projection of a video vi onto the
same space Z parameterized by WV and hi a target vari-
able that equals to 1 when the i-th video is relevant to the
query description ti and 0 otherwise. Naturally, to optimize
eq. (2), we ﬁrst need to deﬁne the projections fT (·; WT )
and fV (·; WV ) in eq. (3).

Textual Embedding. The textual embedding component of
our model, fT (·; WT ), is illustrated in ﬁgure 3 (top). This
component is dedicated to learn a projection of a textual
input –including any future event queries t– on to the uni-
ﬁed space Z. Before detailing our model fT , however, we
note that that the textual embedding can be employed not
only with event article descriptions, but also with any other
textual information that might be associated to the dataset
videos, such as textual metadata. Although we expect the
video title not to be as descriptive as the associated article,
they may still be able to offer some discriminative informa-
tion as previously shown [16, 17] which can be associated
to the event category.

We model the textual embedding as a shallow (two lay-
ers) multi-layer perceptron (MLP). For the ﬁrst layer we
employ a ReLU nonlinearity. The second layer serves a
dual purpose. First, it projects the article description of an
event on the uniﬁed space Z. This projection is category-
speciﬁc, namely different videos that belong to the same
event will share the projection. Second, it can project any
video-speciﬁc textual metadata into the uniﬁed space. We,
therefore, propose to embed the title metadata ki, which is
uniquely associated with a video, not an event category. To
this end, we opt for softmax nonlinearity for the second
layer, followed by an additional logistic loss term to penal-
ized misprediction of titles mi with respect to the video’s
event label yj

i , namely

Llog =

−yj

i log f j

T (ki; WT ).

(4)

N
(cid:88)

M
(cid:88)

i=1

j=1

Overall, the textual embedding f j

T is trained with a dual
loss in mind. The ﬁrst loss term, see eq. (2) (3) takes care
that the ﬁnal network learns event-relevant textual projec-
tions. The second loss term, see eq. (4), takes care that the
ﬁnal network does not overﬁt to the particular event article
descriptions. The latter is crucial because the event article
descriptions in Dz will not overlap with the future event
queries, since we are in a zero-exemplar retrieval setting.
As such, training the textual embedding to be optimal only
for these event descriptions will likely result in severe over-
ﬁtting. Our goal and hope is that the ﬁnal textual embed-
ding model fT will capture both event-aware and video-
discriminative textual features.

Visual Embedding. The visual embedding component of
our model, fV (·; WV ), is illustrated in ﬁgure 3 (bottom).

This component is dedicated to learn a projection from the
visual input, namely the videos in our zero-exemplar dataset
Dz, into the uniﬁed metric space Z. The goal is to project
the videos belonging to semantically similar events; project
them into a similar region in the space. We model the visual
embedding fV (vi; WV ) using a shallow (two layers) multi-
layer perceptron with tanh nonlinearities, applied to any
visual feature for video vi.

End-to-End Training. At each training forward-pass, the
model is given a triplet of data inputs, an event description
ti, a related video vi and video title ki. From eq. (3) we
observe that the visual embedding fV (vi; WV ) is encour-
aged to minimize its distance with the output of the textual
embedding fT (ti; WT ). In the end, all the modules of the
proposed model are differentiable. Therefore, we train our
model in an end-to-end manner by minimizing the follow-
ing objective

LU ,

arg min
WV ,WT
LU = Lcon + Llog.

(5)

input vi

For the triplet input (vi, ti, ki), we rely on external repre-
sentations, since our ultimate goal is zero-exemplar search.
Strictly speaking, a visual
is represented as
CNN [35] feature vector, while textual inputs ti, ki are rep-
resented as LSI [45] or Doc2Vec [43] feature vectors. How-
ever, given that these external representations rely on neural
network architectures, if needed, they could also be further
ﬁne-tuned. We choose to freeze CNN and Doc2Vec mod-
ules to speed up training. Finally, in this paper we refer to
our main model with uniﬁed embedding, as modelU .

Inference. After training, we ﬁx the parameters (WV , WT ).
At test time, we set our function f (·) from eq. (1) to be
equivalent to the distance function from eq. (??). Hence, at
test time, we compute the Euclidean distance in the learned
metric space Z between the embeddings (zv, zt) of test
video v and novel event description t, respectively.

4. Experiments

4.1. Datasets

Before delving into the details of our experiments, ﬁrst

we describe the external knowledge sources we use.

Training dataset. We leverage videos and articles from
publicly available datasets. EvenNet [14] is a dataset of
∼90k event videos, harvested from YouTube and catego-
rized into 500 events in hierarchical form according to the
events’ ontology. Each event category contains around 180
videos. Each video is coupled with a text title, few tags and
related event’s ontology.

We exploit the fact that all events in EventNet are har-
vested from WikiHow [15] – a website for How-To articles
covering a wide spectrum of human activities. For instance:
“How to Feed a Dog” or “How to Arrange Flowers”. Thus,
we crawl WikiHow to get the articles related to all the events
in EventNet.

Test dataset. As the task is zero-exemplar, the test sets are
different from the training. While EventNet serves as the
training, the following serve as the test: TRECVID MED-
13 [1] and MED-14 [1].
In details, they are datasets of
videos for events. They comprise 27k videos. There are two
versions, MED-13 and MED-14 with 20 events for each.
Since 10 events overlap, the result is 30 different events in
total. Each event is coupled with short textual description
(title and deﬁnition).

4.2. Implementation Details

Video Features. To represent a video v, we uniformly sam-
ple a frame every one second. Then, using ResNet [35], we
extract pool5 CNN features for the sampled frames. Then,
we average pool the frame-level features to get the video-
level feature xv. We experiment different features from dif-
ferent CNN models: ResNet (prob, fc1000), VGG [37]
(fc6, fc7), GoogLeNet [47] (pool5, fc1024), and
Places365 [48] (fc6, fc7,fc8) except we ﬁnd ResNet
pool5 to be the best. We only use ResNet pool5 and we
don’t fuse multiple CNN features.

Text Features. We choose topic modeling [44, 45], as it
is well-suited for long (and sometimes noisy) text articles.
We train LSI topic model [45] on Wikipedia corpus [49].
We experiment different latent topics ranging from 300
to 6000, expect we found 2500 to be the best. Also,
we experiment other textual representations as LDA [44],
SkipThoughts [50] and Doc2Vec [43]. To extract a feature
from an event article k or video title t, ﬁrst we preprocess
the text using standard MLP steps: tokenization, lemmati-
zation and stemming. Then, for k, t we extract 2500-D LSI
features yk, yt, respectively. The same steps apply to MED
text queries.

Model Details.
Our visual and textual embeddings
fV (·), fT (·) are learned on top of the aforementioned visual
and textual features (xv, yk, yt). fT (·) is a 1-hidden layer
MLP classiﬁer with ReLU for hidden, softmax for out-
put, logistic loss and 2500-2500-500 neurons for
the input, hidden, and output layers, respectively. Similarly,
fV (·) is a 1-hidden layer MLP regressor with ReLU for hid-
den, contrastive loss and 2048-2048-500 neurons
for the input, hidden, and output layers, respectively. Our
code is made public1 to support further research.

1github.com/noureldien/unified_embedding

4.3. Textual Embedding

(a) LSI Features

(b) Embedded Features

Figure 4. Our textual embedding (b) maps MED to EventNet
events better than LSI features. Each dot in the matrix shows the
similarity between MED and EventNet events.

Here, we qualitatively demonstrate the beneﬁt of the tex-
tual embedding fT (·). Figure 4 shows the similarity matrix
between MED and EventNet events. Each dot represents
how a MED event is similar to EventNet events. It shows
that our embedding (right) is better than LSI (left) in map-
ping MED to EventNet events. For example, LSI wrongly
maps “9: getting a vehicle unstuck” to “256: launch a boat”
while our embedding correctly maps it to “170: drive a car”.
Also, our embedding maps with higher conﬁdence than LSI,
as in “16: doing homework or study”.

(a) LSI Features

(b) Embedded Features

Figure 5. For 20 events of MED-14, our textual embedding (right)
is more discriminant than the LSI feature representation (left).
Each dot in the matrix shows how similar an event to all the others.

Figure 5 shows the similarity matrix for MED events,
where each dot represents how related any MED event to
all the others. Our textual embedding (right) is more dis-
criminant than on the LSI feature representation (left). For
example, LSI representation shows high semantic correla-
tion between events “34: ﬁxing musical instrument” and
“40: tuning musical instrument”, while our embedding dis-
criminate them.

Next, we quantitatively demonstrate the beneﬁt of the
textual embedding fT (·).
In contrast to the main model,
see section 3, we investigate baseline modelV , where we
discard the textual embedding fT (·) and consider only the
visual embedding fV (·). We project a video v on the LSI

(a) MED-13, visual embedding (modelV ).

(b) MED-13, separate embedding (modelS ).

(c) MED-13, uniﬁed embedding (modelU ).

(d) MED-14, visual embedding (modelV ).

(e) MED-14 separate embedding (modelS ).
Figure 6. We visualize the results of video embedding using the uniﬁed embedding modelU and baselines modelV , modelS . Each sub-ﬁgure
shows how discriminant the representation of the embedded videos. Each dot represents a projected video, while each pentagon-shape
represents a projected event description. We use t-SNE to visualize the result.

(f) MED-14, uniﬁed embedding (modelU ).

representation y of the related event t. Thus, this baseline
It is
falls in the ﬁrst family of methods, see ﬁgure 2(a).
optimized using mean-squared error (MSE) loss LV
mse, see
eq. 6. The result of this baseline is reported in section 5,
table 1.

LV

mse =

(cid:107)yi − fV (vi; WV )(cid:107)2
2.

(6)

1
N

N
(cid:88)

i=1

Also, we train another baseline modelC, which is similar
to the aforementioned V except instead of using MSE loss
LV
con, as follows:

mse, see eq. (6), it uses contrastive loss LC

LC

con =

hi · d2

i + (1 − hi) max(1 − di, 0)2,

(7)

1
2N

N
(cid:88)

i=1

di = (cid:107)yi − fV (vi; WV )(cid:107)2.

4.4. Uniﬁed Embedding and Metric Learning

In this experiment, we demonstrate the beneﬁt of the uni-
ﬁed embedding. In contrast to our model presented in sec-
tion 3, we investigate baseline modelS , where this baseline
does not learn joint embedding. Instead, it separately learns
visual fV (·) and textual fT (·) projections. We model these

projections as a shallow (2-layer) MLP trained to classify
the data input into 500 event categories, using logistic loss,
same as eq. (4).

We conduct another experiment to demonstrate the ben-
eﬁt of learning metric space. In contrast to our model pre-
sented in section 3, we investigate baseline modelN , where
we discard the metric learning layer. Consequently, this
baseline learns the visual embedding is a shallow (2 lay-
ers) multi-layer perceptron with tanh non linearities. Also,
we replace the contrastive loss Lc, see eq. (2) with mean-
squared error loss Lmse, namely

LN

mse =

(cid:107)fT (ti; WT ) − fV (vi; WV )(cid:107)2
2.

(8)

1
N

N
(cid:88)

i=1

During retrieval, this baseline embeds a test video vi and
novel text query ti as features zv, zt onto the common space
Z using textual and visual embeddings fT (·), fV (·), respec-
tively. However, in a post-processing step, retrieval score
si for the video vi is the cosine distance between (zv, zt).
Similarly, all test videos are scored, ranked and retrieved.
The results of the aforementioned baselines modelS and
modelN are reported in table 1.

Comparing Different Embeddings.

In the previous ex-

periments, we investigated several baselines of the uni-
ﬁed embedding (modelU ), namely visual-only embedding
(modelV ), separate visual-textual embedding (modelS ) and
non-metric visual-textual embedding (modelN ). In a quali-
tative manner, we compare the results of such embeddings.
As shown in ﬁgure 6, we use these baselines to embed event
videos of MED-13 and MED-14 datasets into the corre-
sponding spaces. At the same time, we project the textual
description of the events on the same space. Then, we use
t-SNE [51] to visualize the result on 2D manifold. As seen,
the uniﬁed embedding, see sub-ﬁgures 6(e), 6(f) learns
more discriminant representations than the other baselines,
see sub-ﬁgures 6(a), 6(b), 6(c) and 6(d). The same obser-
vation holds for both for MED-13 and MED-14 datasets.

4.5. Mitigating Noise in EventNet

Based of quantitative and qualitative analysis, we con-
clude that EventNet is noisy. Not only videos are uncon-
strained, but also some of the video samples are irrelevant
to their event categories. EvenNet dataset [14] is accom-
panied by 500-category CNN classiﬁer. It achieves top-1
and top-5 accuracies of 30.67% and 53.27%, respectively.
Since events in EventNet are structured as an ontological
hierarchy, there is a total of 19 high-level categories. The
classiﬁer achieves top-1 and top-5 accuracies of 38.91% and
57.67%, respectively, over these high-level categories.

Based on these observations, we prune EventNet to re-
move noisy videos. To this end, ﬁrst we represent each
video as average pooling of ResNet pool5 features. Then,
we follow the conventional 5-fold cross validation with 5
rounds. For each round, we split the dataset into 5 sub-
sets, 4 subsets Vt for training and the last Vp for pruning.
Then we train a 2-layer MLP for classiﬁcation. After train-
ing, we forward-pass the videos of Vp and rule-out the mis-
classiﬁed ones.

The intuition behind pruning is that we rather learn
salient event concepts using less video samples than learn
noisy concepts with more samples. Pruning reduced the to-
tal number of videos by 26%, from 90.2k to 66.7k. This
pruned dataset is all what we use in our experiments.

4.6. Latent Topics in LSI

When training LSI topic model on Wikipedia corpus, a
crucial parameter is the number of latent topics K the model
constructs. We observe improvements in the performance
directly proportional to increasing K. The main reason that
the bigger the value of K, the more discriminant the LSI
feature is. Figure 7 conﬁrms our understanding.

5. Results

Evaluation metric. Since we are addressing, in essence,
an information retrieval task, we rely on the average preci-
sion (AP) per event, and mean average precision (mAP) per

Figure 7. Similarity matrix between LSI features of MED-14
events. The more the latent topics (K) in LSI model, the higher
the feature dimension, and the more discriminant the feature.

dataset. We follow the standard evaluation method as in the
relevant literature [1, 2, 52].

Comparing against model baselines. In table 1, we re-
port the mAP score of our model baselines, previously dis-
cussed in the experiments, see section 4. The table clearly
shows the marginal contribution of each of novelty for the
proposed method.

Baseline Loss
modelV LV
modelC LC
modelS Llog
modelN LN
modelU LU

Metric fV (·) fT (·) MED13 MED14
10.76
12.31
13.49
14.36
16.67

11.90
13.29
15.60
15.92
17.86

(cid:55)
mse (6)
con (7) (cid:51)
(cid:55)
(4)
(cid:55)
mse (8)
(5) (cid:51)

(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)

Table 1. Comparison between the uniﬁed embedding and other
baselines. The uniﬁed embedding modelU achieves the best re-
sults on MED-13 and MED-14 datasets.

Comparing against related work. We report the perfor-
mance of our method, the uniﬁed embedding modelU on
TRECVID MED-13 and MED-14 datasets. When com-
pared with the related works, our method improves over
the state-of-the-art by a considerable margin, as shown in
table 2 and ﬁgure 8.

Method
TagBook [18]
Discovary [7]
Composition [8]
Classiﬁers [9]
VideoStory† [17]
VideoStory∗ [17]
This Paper (modelU )

ToM '15 12.90
ICAI '15 09.60
AAAI '16 12.64
CVPR '16 13.46
PAMI '16 15.90
PAMI '16 20.00
17.86

MED13 MED14
05.90
–
13.37
14.32
05.20
08.00
16.67

Table 2. Performance comparison between our model and related
works. We report the mean average precision (mAP%) for MED-
13 and MED-14 datasets.

It is important to point out that VideoStory† uses only ob-
ject feature representation, so its comparable to our method.

(a) MED-13 Dataset

Figure 8. Event detection accuracies: per-event average precision (AP%) and per-dataset mean average precision (mAP%) for MED-13
and MED-14 datasets. We compare our results against TagBook [18], Discovary [7], Composition [8], Classiﬁers [9] and VideoStory [17].

(b) MED-14 Dataset

However, VideoStory∗ uses motion feature representation
and expert text query (i.e. using term-importance matrix
H in [17]). To rule out the marginal effect of using dif-
ferent datasets and features, we train VideoStory and report
results in table 3. Clearly, CNN features and video exem-
plars in the training set can improve the model accuracy, but
our method improves against VideoStory when trained on
the same dataset and using the same features. Other works
(Classiﬁers [9], Composition [8]) use both image and ac-
tion concept classiﬁers. Nonetheless, our method improves
over them using only object-centric CNN feature represen-
tations.

Training Set

CNN Feat. MED14
Method
VideoStory VideoStory46k [17] GoogleNet 08.00
GoogleNet 11.84
VideoStory FCVID [53]
GoogleNet 14.52
VideoStory EventNet [14]
15.80
ResNet
VideoStory EventNet [14]
16.67
ResNet
This Paper EventNet [14]

proposed to learn metric space. This enables measuring
the similarities between the embedded modalities using this
very space.

We experimented the novelties and demonstrated how
they contribute to improving the performance. We comple-
mented this by improvements over the state-of-the-art by
considerable margin on MED-13 and MED-14 datasets.

However, the question still remains, how can we discrim-
inate between these two MED events “34: ﬁxing musical in-
strument” and “40: tuning musical instrument”. We would
like to argue that temporal modeling for human actions in
videos is of absolute necessity to achieve such ﬁne-grained
event recognition. In future research, we would like to focus
on human-object interaction in videos and how to model it
temporally.

Acknowledgment

We thank Dennis Koelma, Masoud Mazloom and Cees
Snoek2 for lending their insights and technical support for
this work.

Table 3. Our method improves over VideoStory when trained on
the same dataset and using the same feature representation.

References

6. Conclusion

In this paper, we presented a novel approach for detect-
ing events in unconstrained web videos, in a zero-exemplar
fashion. Rather than learning separate embeddings form
cross-modal datasets, we proposed a uniﬁed embedding
where several cross-modalities are jointly projected. This
enables end-to-end learning. On top of this, we exploited
the fact that zero-exemplar is posed as retrieval task and

[1] Paul Over, George Awad, Jon Fiscus, Greg Sanders, and Bar-
bara Shaw. Trecvid 2013–an introduction to the goals, tasks,
In TRECVID
data, evaluation mechanisms, and metrics.
Workshop, 2013. 1, 2, 5, 7

[2] Paul Over, Jon Fiscus, Greg Sanders, David Joy, Mar-
tial Michel, George Awad, Alan Smeaton, Wessel Kraaij,
and Georges Qu´enot. Trecvid 2014–an overview of the
In
goals, tasks, data, evaluation mechanisms and metrics.
TRECVID Workshop, 2014. 1, 2, 7

2{kolema,m.mazloom,cgmsnoek}@uva.nl

[3] Lu Jiang, Shoou-I Yu, Deyu Meng, Teruko Mitamura, and
Alexander G Hauptmann. Bridging the ultimate semantic
gap: A semantic search engine for internet videos. In ICMR,
2015. 1, 2

[19] Shuang Wu, Sravanthi Bondugula, Florian Luisier, Xiaodan
Zhuang, and Pradeep Natarajan. Zero-shot event detection
using multi-modal fusion of weakly supervised concepts. In
IEEE CVPR, 2014. 2

[4] Amirhossein Habibian, Thomas Mensink, and Cees GM
Snoek. Composite concept discovery for zero-shot video
event detection. In ICMR, 2014. 1

[5] Amirhossein Habibian, Thomas Mensink, and Cees GM
Snoek. Discovering semantic vocabularies for cross-media
retrieval. In ICMR, 2015. 1

[6] Masoud Mazloom, Efstrastios Gavves, and Cees G. M.
Snoek. Conceptlets: Selective semantics for classifying
video events. In IEEE TMM, 2014. 1

[7] Xiaojun Chang, Yi Yang, Alexander G Hauptmann, Eric P
Xing, and Yao-Liang Yu. Semantic concept discovery for
large-scale zero-shot event detection. In IJCAI, 2015. 1, 2,
7, 8

[8] Xiaojun Chang, Yi Yang, Guodong Long, Chengqi Zhang,
and Alexander G Hauptmann. Dynamic concept composi-
tion for zero-example event detection. In arXiv, 2016. 1, 2,
7, 8

[9] Xiaojun Chang, Yao-Liang Yu, Yi Yang, and Eric P Xing.
They are not equally reliable: Semantic event search using
differentiated concept classiﬁers. In IEEE CVPR, 2016. 1, 2,
7, 8

[10] Yi-Jie Lu. Zero-example multimedia event detection and re-
counting with unsupervised evidence localization. In ACM
MM, 2016. 1, 2, 3

[11] Lu Jiang, Shoou-I Yu, Deyu Meng, Yi Yang, Teruko Mi-
tamura, and Alexander G Hauptmann. Fast and accurate
content-based semantic search in 100m internet videos. In
ACM MM, 2015. 1

[12] Thomas Mensink, Efstratios Gavves, and Cees Snoek. Costa:
Co-occurrence statistics for zero-shot classiﬁcation. In IEEE
CVPR, 2014. 1

[13] E. Gavves, T. E. J. Mensink, T. Tommasi, C. G. M. Snoek,
and T Tuytelaars. Active transfer learning with zero-shot
priors: Reusing past datasets for future tasks. In IEEE ICCV,
2015. 1

[14] Guangnan Ye, Yitong Li, Hongliang Xu, Dong Liu, and
Shih-Fu Chang. Eventnet: A large scale structured concept
library for complex event detection in video. In ACM MM,
2015. 1, 4, 7, 8

[15] Wikihow. http://wikihow.com. 1, 5

[16] Amirhossein Habibian, Thomas Mensink, and Cees GM
Snoek. Videostory: A new multimedia embedding for few-
example recognition and translation of events. In ACM MM,
2014. 2, 3, 4

[17] Amirhossein Habibian, Thomas Mensink, and Cees GM
Snoek. Videostory embeddings recognize events when ex-
amples are scarce. In IEEE TPAMI, 2016. 2, 3, 4, 7, 8

[20] Mohamed Elhoseiny, Jingen Liu, Hui Cheng, Harpreet
Sawhney, and Ahmed Elgammal. Zero-shot event detection
by multimodal distributional semantic embedding of videos.
In arXiv, 2015. 2

[21] Tomas Mikolov, Quoc V Le, and Ilya Sutskever. Exploit-
ing similarities among languages for machine translation. In
arXiv, 2013. 2

[22] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
and Jeff Dean. Distributed representations of words and
phrases and their compositionality. In NIPS, 2013. 2, 3

[23] Lu Jiang, Deyu Meng, Teruko Mitamura, and Alexander G
Hauptmann. Easy samples ﬁrst: Self-paced reranking for
zero-example multimedia search. In ACM MM, 2014. 3

[24] Lu Jiang, Teruko Mitamura, Shoou-I Yu, and Alexander G
Hauptmann. Zero-example event search using multimodal
pseudo relevance feedback. In ICMR, 2014. 3

[25] Arnav Agharwal, Rama Kovvuri, Ram Nevatia,

and
Cees GM Snoek. Tag-based video retrieval by embedding se-
mantic content in a continuous word space. In IEEE WACV,
2016. 3

[26] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,
and Manohar Paluri. C3d: generic features for video analy-
sis. In arXiv, 2014. 3

[27] Karen Simonyan and Andrew Zisserman. Two-stream con-
volutional networks for action recognition in videos.
In
NIPS, 2014. 3

[28] Heng Wang, Alexander Kl¨aser, Cordelia Schmid, and
Cheng-Lin Liu. Action recognition by dense trajectories. In
IEEE CVPR, 2011. 3

[29] J Uijlings, IC Duta, Enver Sangineto, and Nicu Sebe. Video
classiﬁcation with densely extracted hog/hof/mbh features:
an evaluation of the accuracy/computational efﬁciency trade-
off. In IJMIR, 2015. 3

[30] B. Fernando, E. Gavves, J. Oramas, A. Ghodrati, and
T. Tuytelaars. Rank pooling for action recognition. In IEEE
TPAMI, 2016. 3

[31] Lindasalwa Muda, Mumtaj Begam, and I Elamvazuthi. Voice
recognition algorithms using mel frequency cepstral coefﬁ-
cient (mfcc) and dynamic time warping (dtw) techniques. In
arXiv, 2010. 3

[32] Anurag Kumar and Bhiksha Raj. Audio event detection us-

ing weakly labeled data. In arXiv, 2016. 3

[33] Liping Jing, Bo Liu, Jaeyoung Choi, Adam Janin, Julia
Bernd, Michael W Mahoney, and Gerald Friedland. A dis-
criminative and compact audio representation for event de-
tection. In ACM MM, 2016. 3

[18] Masoud Mazloom, Xirong Li, and Cees Snoek. Tagbook: A
semantic video representation without supervision for event
detection. In IEEE TMM, 2015. 2, 7, 8

[34] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document recog-
nition. In IEEE. 3

[53] Yu-Gang Jiang, Zuxuan Wu, Jun Wang, Xiangyang Xue, and
Shih-Fu Chang. Exploiting feature and class relationships in
video categorization with regularized deep neural networks.
In IEEE TPAMI, 2017. 8

[35] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In arXiv, 2015.
3, 4, 5

[36] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NIPS, 2012. 3

[37] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In arXiv,
2014. 3, 5

[38] Chuang Gan, Ting Yao, Kuiyuan Yang, Yi Yang, and Tao
Mei. You lead, we exceed: Labor-free video concept learning
by jointly exploiting web videos and images. In IEEE CVPR,
2016. 3

[39] Karen Simonyan, Omkar M Parkhi, Andrea Vedaldi, and An-
drew Zisserman. Fisher vector faces in the wild. In BMVC,
2013. 3

[40] Relja Arandjelovic and Andrew Zisserman. All about vlad.

In IEEE CVPR, 2013. 3

[41] Pascal Mettes, Jan C van Gemert, Spencer Cappallo, Thomas
Mensink, and Cees GM Snoek. Bag-of-fragments: Select-
ing and encoding video fragments for event detection and
recounting. In ICMR, 2015. 3

[42] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to

sequence learning with neural networks. In NIPS, 2014. 3

[43] Quoc V Le and Tomas Mikolov. Distributed representations
of sentences and documents. In ICML, 2014. 3, 4, 5

[44] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent

dirichlet allocation. In JMLR, 2003. 3, 5

[45] Scott Deerwester, Susan T Dumais, George W Furnas,
Thomas K Landauer, and Richard Harshman. Indexing by
latent semantic analysis. In JACS, 1990. 3, 4, 5

[46] Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning
a similarity metric discriminatively, with application to face
veriﬁcation. In IEEE CVPR, 2005. 3

[47] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In IEEE CVPR, 2015. 5

[48] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Antonio Tor-
ralba, and Aude Oliva. Places: An image database for deep
scene understanding. In arXiv, 2016. 5

[49] Wikipedia, 2016. http://wikipedia.com. 5

[50] Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard
Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.
Skip-thought vectors. In NIPS, 2015. 5

[51] Laurens van der Maaten and Geoffrey Hinton. Visualizing

data using t-sne. In JMLR, 2008. 7

[52] Yu-Gang Jiang, Guangnan Ye, Shih-Fu Chang, Daniel El-
lis, and Alexander C Loui. Consumer video understanding:
A benchmark database and an evaluation of human and ma-
chine performance. In ICMR, 2011. 7

Uniﬁed Embedding and Metric Learning for Zero-Exemplar Event Detection

Noureldien Hussein, Efstratios Gavves, Arnold W.M. Smeulders
QUVA Lab, University of Amsterdam
{nhussein,egavves,a.w.m.smeulders}@uva.nl

7
1
0
2
 
y
a
M
 
5
 
 
]

V
C
.
s
c
[
 
 
1
v
8
4
1
2
0
.
5
0
7
1
:
v
i
X
r
a

Abstract

Event detection in unconstrained videos is conceived
as a content-based video retrieval with two modalities:
textual and visual. Given a text describing a novel event,
the goal is to rank related videos accordingly. This task is
zero-exemplar, no video examples are given to the novel
event.
Related works train a bank of concept detectors on external
data sources. These detectors predict conﬁdence scores for
test videos, which are ranked and retrieved accordingly. In
contrast, we learn a joint space in which the visual and
textual representations are embedded. The space casts a
novel event as a probability of pre-deﬁned events. Also, it
learns to measure the distance between an event and its
related videos.
Our model is trained end-to-end on publicly available
EventNet. When applied to TRECVID Multimedia Event
Detection dataset, it outperforms the state-of-the-art by a
considerable margin.

1. Introduction

TRECVID Multimedia Event Detection (MED) [1, 2] is
a retrieval task for event videos, with the reputation of being
realistic. It comes in two ﬂavors: few-exemplar and zero-
exemplar, where the latter means that no video example is
known to the model. Although expecting a few examples
seems reasonable, in practice this implies that the user must
already have an index of any possible query, making it very
limited. In this paper, we focus on event video search with
zero exemplars.

Retrieving videos of never-seen events, such as “reno-
vating home”, without any video exemplar poses several
challenges. One challenge is how to bridge the gap be-
tween the visual and the textual semantics [3, 4, 5]. One
approach [3, 6, 7, 8, 9, 10] is to learn a dictionary of con-
cept detectors on external data source. Then, scores for test
videos are predicted using these detectors. Test videos are
then ranked and retrieved accordingly. The inherent weak-

Figure 1. We pose the problem of zero-exemplar event detection as
learning from a repository of pre-deﬁned events. Given video ex-
emplars of events “removing drywall” or “ﬁt wall times”, one may
detect a novel event “renovate home” as a probability distribution
over the predeﬁned events.

ness of this approach is that the presentation of a test video
is reduced to a limited vocabulary from the concept dictio-
nary. Another challenge is how to overcome the domain
difference between training and test events. While Seman-
tic Query Generation (SQG) [3, 8, 9, 11] mitigates this chal-
lenge by extracting keywords from the event query, it does
not address how relevant these keywords to the event itself.
For example, keyword “person” is not relevant to event “car
repair” as it is to “ﬂash mob gathering”.

Our entry to zero-exemplar events is that they gener-
ally have strong semantic correlations [12, 13] with other
possibly seen events. For instance, the novel event “ren-
ovating home” is related to “ﬁt wall tiles”, “remove dry-
wall”, or even to “paint door”. Novel events can, there-
fore, be casted on a repository of prior events, for which
knowledge sources in various forms are available before-
hand, such as the videos, as in EventNet [14], or articles,
as in WikiHow [15]. Not only do these sources provide
video examples of a large –but still limited– set of events,
but also they provide an association of text description of
events with their corresponding videos. A text article can
describe the event in words: what is it about, what are the

1

details and what are the semantics. We note that such a
visual-textual repository of events may serve as a knowl-
edge source, by which we can interpret novel event queries.
For Zero-exemplar Event Detection (ZED), we propose

a neural model with the following novelties:
1. We formulate a uniﬁed embedding for multiple modal-
ities (e.g. visual and textual) that enables a contrastive
metric for maximum discrimination between events.
2. A textual embedding poses the representation of a novel
event as a probability of predeﬁned events, such that it
spans a much larger space of admissible expressions.
3. We exploit a single data source, comprising pairs of event
articles and related videos. A single source rather enables
end-to-end learning from multi-modal individual pairs.

We empirically shows that our novelties result in perfor-
mance improvement. We evaluate the model on TRECVID
Multimedia Event Detection (MED) 2013 [1] and 2014 [2].
Our results show signiﬁcant improvement over the state-of-
the-art.

2. Related Work

Figure 2. Three families of methods for zero-exemplar event de-
tection: (a), (b) and (c). They build on top of feature represen-
tations learned a priori (i.e. initial representations), such as CNN
features x for a video v or word2vec features y for event text query
t. In a post-processing step, the distance θ is measured between
the embedded features. In contrast, our model rather falls in a new
family, depicted in (d), for it learns uniﬁed embedding with metric
loss using single data source.

We identify three families of methods for ZED, as in ﬁg-

ure 2 (a), (b) and (c).
Visual Embedding and Textual Retrieval. As in ﬁg-
ure 2(a), given a video vi represented as x ∈ X and a re-
lated text t represented as y ∈ Y. Then, a visual model fV
is trained to project x as yv ∈ Y such that the distance is
minimized between (yv, y). In test time, video ranking and

retrieval is done using distance metric between the projected
test video yt and test query representation y.

[16, 17] project the visual feature x of a web video v
into term-vector representation y of the video’s textual title
t. However, during training, the model makes use of the
text query of the test events to learn better term-vector rep-
resentation. Consequently, this limits the generalization for
novel event queries.
Textual Embedding and Visual Retrieval. As in ﬁg-
ure 2(b), a given text query t is projected into xt ∈ X using
pre-trained or learned language model fT .

[18] makes use of freely-available weekly-tagged web
videos. Then it propagates tags to test videos from its near-
est neighbors. Methods [7, 8, 9, 10, 3] have similar ap-
proach. Given a text query t, Semantic Query Generation
(SQG) extracts N most related concepts {ci, i ∈ N } to the
test query. Then, pre-trained concept detectors predict prob-
ability scores {si, i ∈ N } for a test video v. Aggregating
these probabilities results in the ﬁnal video score sv, upon
which videos are ranked and retrieved. [9] learns weighted
averaging.

The shortcoming of this family is that expressing a
video as probability scores of few concepts is under-
representation. Any concept that exists in the video but is
missing in the concept dictionary is thus unrepresented.
Visual-Textual Embedding and Semantic Retrieval. As
in ﬁgure 2(c), visual fV and textual fT models are trained
to project both of the visual x and textual y features into a
semantic space Z. During test, ranking score is the distance
between the projections zv, zt in the semantic space Z.

[19] projects video concepts into a high-dimensional
lexicon space. Separately, it projects concept-based features
to the space, which overcomes the lexicon mismatch be-
tween the query and the video concepts. [20] embeds a fu-
sion of low and mid-level visual features into distributional
semantic manifold [21, 22]. In a separate step, it embeds
text-based concepts into the manifold.

The third family, see ﬁgure 2(c), is superior to the others,
see ﬁgure 2(a), (b). However, one drawback of [19, 20] is
separately embedding both the visual and textual features
zv, zt. This leads to another drawback, having to measure
the distance between (zv, zt) in a post-processing step (e.g.
cosine similarity).
Uniﬁed Embedding and Metric Learning Retrieval Our
method rather falls into a new family, see ﬁgure 2(d), and it
overcomes the shortcomings of [19, 20] by the following. It
is trained on a single data source, enabling a uniﬁed embed-
ding for features of multiple modalities into a metric space.
Consequently, the distance between the embedded features
is measured by the model using the learned metric space.

Auxiliary Methods Independent to the previous works, the
following techniques have been used to improve the results:

Figure 3. Model overview. Using dataset Dz of
M event categories and N videos. Each event has
a text article and a few videos. Given a video x
with text title k, belonging to an event with arti-
cle t, we extract features x, yk, yt respectively.
At the top, network fT learns to classify the title
feature yk into one of M event categories. In the
middle, we borrow the network fT to embed the
event article’s feature yt as zt ∈ Z. Then, at the
bottom, the network fV learns to embed the video
feature x as zv ∈ Z such that the distance be-
tween (cid:0)zv, zt(cid:1) is minimized, in the learned metric
space Z.

self-paced reranking [23], pseudo-relevance feedback [24],
event query manual intervention [25], early fusion of fea-
tures (action [26, 27, 28, 29, 30] or acoustic [31, 32, 33]) or
late fusion of concept scores [17]. All these contributions
may be applied to our method.

Visual Representation. ConvNets [34, 35, 36, 37] pro-
vide frame-level representation. To tame them into video-
level counterpart, literature use: i- frame-level ﬁltering [38]
ii- vector encoding [39, 40] iii- learned pooling and re-
counting [10, 41] iv- average pooling [16, 17]. Also, low-
level action [28, 29], mid-level action [26, 27] or acous-
tic [31, 32, 33] features can be used. Textual Represen-
tation. To represent text, literature use: i- sequential mod-
els [42] ii- continuous word-space representations [22, 43]
iii- topic models [44, 45] iv- dictionary-space representa-
tion [17].

3. Method

3.1. Overview

Our goal is zero-exemplar retrieval of event videos with
respect to their relevance to a novel textual description of
an event. More speciﬁcally, for the zero-exemplar video
dataset Dz = {vz
i }, i = 1, . . . , L and given any future, tex-
tual event description tz, we want to learn a model f (·) that
i according to the relevance to tz, namely:
ranks the videos vz

tz : vz

i (cid:31) vz

j → f (vz

i , tz) > f (vz

j , tz).

(1)

3.2. Model

Since we focus on zero-exemplar setting, we cannot ex-
pect any training data directly relevant to the test queries.
As such, we cannot directly optimize our model for the
parameters WT , WV in eq. (3). In the absence of any di-
rect data, we resort to external knowledge databases. More
speciﬁcally, we propose to cast future novel query descrip-
tions as a convex combination of known query descriptions
in external databases, where we can measure their relevance
the database videos.

We start from a dataset Dz = {vi, ki, lj, tj} ,

i =
1, . . . , N, j = 1, . . . , M organized by an event taxonomy,
where we do not neither expect nor require the events to
overlap with any future event queries. The dataset is com-
posed of M events. Each event is associated with a textual,
article description of the event, analyzing different aspects
of it, such as: (i) the typical appearance of subjects and
objects (ii) it’s procedures (iii) the steps towards complet-
ing task associated with it. The dataset contains in total N
videos, with vi denoting the i-th video in the dataset with
metadata ki, e.g. the title of the video. A video is associ-
ated with an event label li and the article description ti of
the event it belongs to. Since multiple videos belong to the
same event, they share the article description of such event.
The ultimate goal of our model is zero-exemplar search
for event videos. Namely, provided unknown text queries
by the user, we want to retrieve those videos that are rele-
vant. We illustrate our proposed model during training in
ﬁgure 3. The model is composed of two components, a
textual embedding fT (·), a visual embedding fV (·). Our
ultimate goal is the ranking of videos, vi (cid:31) vj (cid:31) vk with
respect to their relevance to a query description, or in pair-
wise terms vi (cid:31) vj, vj (cid:31) vk and vi (cid:31) vk.

Let us assume a pair of videos vi, vj and query descrip-
tion t, where video vi is more relevant to the query t than vj.
Our goal is a model that learns to put videos in the correct
relative order, namely (vi, t) (cid:31) (vj, t). This is equivalent
to a model that learns visual-textual embeddings such that
j , where dtv
i < dtv
dtv
is the distance between visual-textual
i
embeddings of (vi, t), dtv
is the same for (vj, t). Since we
j
want to compare distances between pairs (vi, t), (vj, t), we
pose the learning of our model as the minimization of a con-
trastive loss [46]:

Lcon =

hi · d2

i + (1 − hi) max(1 − di, 0)2,

(2)

1
2N

N
(cid:88)

i=1

di = (cid:107)fT (ti; WT ) − fV (vi; WV )(cid:107)2,

(3)

where fT (ti; WT ) is the projection of the query descrip-
tion ti into the uniﬁed metric space Z parameterized by

WT , fV (vi; WV ) is the projection of a video vi onto the
same space Z parameterized by WV and hi a target vari-
able that equals to 1 when the i-th video is relevant to the
query description ti and 0 otherwise. Naturally, to optimize
eq. (2), we ﬁrst need to deﬁne the projections fT (·; WT )
and fV (·; WV ) in eq. (3).

Textual Embedding. The textual embedding component of
our model, fT (·; WT ), is illustrated in ﬁgure 3 (top). This
component is dedicated to learn a projection of a textual
input –including any future event queries t– on to the uni-
ﬁed space Z. Before detailing our model fT , however, we
note that that the textual embedding can be employed not
only with event article descriptions, but also with any other
textual information that might be associated to the dataset
videos, such as textual metadata. Although we expect the
video title not to be as descriptive as the associated article,
they may still be able to offer some discriminative informa-
tion as previously shown [16, 17] which can be associated
to the event category.

We model the textual embedding as a shallow (two lay-
ers) multi-layer perceptron (MLP). For the ﬁrst layer we
employ a ReLU nonlinearity. The second layer serves a
dual purpose. First, it projects the article description of an
event on the uniﬁed space Z. This projection is category-
speciﬁc, namely different videos that belong to the same
event will share the projection. Second, it can project any
video-speciﬁc textual metadata into the uniﬁed space. We,
therefore, propose to embed the title metadata ki, which is
uniquely associated with a video, not an event category. To
this end, we opt for softmax nonlinearity for the second
layer, followed by an additional logistic loss term to penal-
ized misprediction of titles mi with respect to the video’s
event label yj

i , namely

Llog =

−yj

i log f j

T (ki; WT ).

(4)

N
(cid:88)

M
(cid:88)

i=1

j=1

Overall, the textual embedding f j

T is trained with a dual
loss in mind. The ﬁrst loss term, see eq. (2) (3) takes care
that the ﬁnal network learns event-relevant textual projec-
tions. The second loss term, see eq. (4), takes care that the
ﬁnal network does not overﬁt to the particular event article
descriptions. The latter is crucial because the event article
descriptions in Dz will not overlap with the future event
queries, since we are in a zero-exemplar retrieval setting.
As such, training the textual embedding to be optimal only
for these event descriptions will likely result in severe over-
ﬁtting. Our goal and hope is that the ﬁnal textual embed-
ding model fT will capture both event-aware and video-
discriminative textual features.

Visual Embedding. The visual embedding component of
our model, fV (·; WV ), is illustrated in ﬁgure 3 (bottom).

This component is dedicated to learn a projection from the
visual input, namely the videos in our zero-exemplar dataset
Dz, into the uniﬁed metric space Z. The goal is to project
the videos belonging to semantically similar events; project
them into a similar region in the space. We model the visual
embedding fV (vi; WV ) using a shallow (two layers) multi-
layer perceptron with tanh nonlinearities, applied to any
visual feature for video vi.

End-to-End Training. At each training forward-pass, the
model is given a triplet of data inputs, an event description
ti, a related video vi and video title ki. From eq. (3) we
observe that the visual embedding fV (vi; WV ) is encour-
aged to minimize its distance with the output of the textual
embedding fT (ti; WT ). In the end, all the modules of the
proposed model are differentiable. Therefore, we train our
model in an end-to-end manner by minimizing the follow-
ing objective

LU ,

arg min
WV ,WT
LU = Lcon + Llog.

(5)

input vi

For the triplet input (vi, ti, ki), we rely on external repre-
sentations, since our ultimate goal is zero-exemplar search.
Strictly speaking, a visual
is represented as
CNN [35] feature vector, while textual inputs ti, ki are rep-
resented as LSI [45] or Doc2Vec [43] feature vectors. How-
ever, given that these external representations rely on neural
network architectures, if needed, they could also be further
ﬁne-tuned. We choose to freeze CNN and Doc2Vec mod-
ules to speed up training. Finally, in this paper we refer to
our main model with uniﬁed embedding, as modelU .

Inference. After training, we ﬁx the parameters (WV , WT ).
At test time, we set our function f (·) from eq. (1) to be
equivalent to the distance function from eq. (??). Hence, at
test time, we compute the Euclidean distance in the learned
metric space Z between the embeddings (zv, zt) of test
video v and novel event description t, respectively.

4. Experiments

4.1. Datasets

Before delving into the details of our experiments, ﬁrst

we describe the external knowledge sources we use.

Training dataset. We leverage videos and articles from
publicly available datasets. EvenNet [14] is a dataset of
∼90k event videos, harvested from YouTube and catego-
rized into 500 events in hierarchical form according to the
events’ ontology. Each event category contains around 180
videos. Each video is coupled with a text title, few tags and
related event’s ontology.

We exploit the fact that all events in EventNet are har-
vested from WikiHow [15] – a website for How-To articles
covering a wide spectrum of human activities. For instance:
“How to Feed a Dog” or “How to Arrange Flowers”. Thus,
we crawl WikiHow to get the articles related to all the events
in EventNet.

Test dataset. As the task is zero-exemplar, the test sets are
different from the training. While EventNet serves as the
training, the following serve as the test: TRECVID MED-
13 [1] and MED-14 [1].
In details, they are datasets of
videos for events. They comprise 27k videos. There are two
versions, MED-13 and MED-14 with 20 events for each.
Since 10 events overlap, the result is 30 different events in
total. Each event is coupled with short textual description
(title and deﬁnition).

4.2. Implementation Details

Video Features. To represent a video v, we uniformly sam-
ple a frame every one second. Then, using ResNet [35], we
extract pool5 CNN features for the sampled frames. Then,
we average pool the frame-level features to get the video-
level feature xv. We experiment different features from dif-
ferent CNN models: ResNet (prob, fc1000), VGG [37]
(fc6, fc7), GoogLeNet [47] (pool5, fc1024), and
Places365 [48] (fc6, fc7,fc8) except we ﬁnd ResNet
pool5 to be the best. We only use ResNet pool5 and we
don’t fuse multiple CNN features.

Text Features. We choose topic modeling [44, 45], as it
is well-suited for long (and sometimes noisy) text articles.
We train LSI topic model [45] on Wikipedia corpus [49].
We experiment different latent topics ranging from 300
to 6000, expect we found 2500 to be the best. Also,
we experiment other textual representations as LDA [44],
SkipThoughts [50] and Doc2Vec [43]. To extract a feature
from an event article k or video title t, ﬁrst we preprocess
the text using standard MLP steps: tokenization, lemmati-
zation and stemming. Then, for k, t we extract 2500-D LSI
features yk, yt, respectively. The same steps apply to MED
text queries.

Model Details.
Our visual and textual embeddings
fV (·), fT (·) are learned on top of the aforementioned visual
and textual features (xv, yk, yt). fT (·) is a 1-hidden layer
MLP classiﬁer with ReLU for hidden, softmax for out-
put, logistic loss and 2500-2500-500 neurons for
the input, hidden, and output layers, respectively. Similarly,
fV (·) is a 1-hidden layer MLP regressor with ReLU for hid-
den, contrastive loss and 2048-2048-500 neurons
for the input, hidden, and output layers, respectively. Our
code is made public1 to support further research.

1github.com/noureldien/unified_embedding

4.3. Textual Embedding

(a) LSI Features

(b) Embedded Features

Figure 4. Our textual embedding (b) maps MED to EventNet
events better than LSI features. Each dot in the matrix shows the
similarity between MED and EventNet events.

Here, we qualitatively demonstrate the beneﬁt of the tex-
tual embedding fT (·). Figure 4 shows the similarity matrix
between MED and EventNet events. Each dot represents
how a MED event is similar to EventNet events. It shows
that our embedding (right) is better than LSI (left) in map-
ping MED to EventNet events. For example, LSI wrongly
maps “9: getting a vehicle unstuck” to “256: launch a boat”
while our embedding correctly maps it to “170: drive a car”.
Also, our embedding maps with higher conﬁdence than LSI,
as in “16: doing homework or study”.

(a) LSI Features

(b) Embedded Features

Figure 5. For 20 events of MED-14, our textual embedding (right)
is more discriminant than the LSI feature representation (left).
Each dot in the matrix shows how similar an event to all the others.

Figure 5 shows the similarity matrix for MED events,
where each dot represents how related any MED event to
all the others. Our textual embedding (right) is more dis-
criminant than on the LSI feature representation (left). For
example, LSI representation shows high semantic correla-
tion between events “34: ﬁxing musical instrument” and
“40: tuning musical instrument”, while our embedding dis-
criminate them.

Next, we quantitatively demonstrate the beneﬁt of the
textual embedding fT (·).
In contrast to the main model,
see section 3, we investigate baseline modelV , where we
discard the textual embedding fT (·) and consider only the
visual embedding fV (·). We project a video v on the LSI

(a) MED-13, visual embedding (modelV ).

(b) MED-13, separate embedding (modelS ).

(c) MED-13, uniﬁed embedding (modelU ).

(d) MED-14, visual embedding (modelV ).

(e) MED-14 separate embedding (modelS ).
Figure 6. We visualize the results of video embedding using the uniﬁed embedding modelU and baselines modelV , modelS . Each sub-ﬁgure
shows how discriminant the representation of the embedded videos. Each dot represents a projected video, while each pentagon-shape
represents a projected event description. We use t-SNE to visualize the result.

(f) MED-14, uniﬁed embedding (modelU ).

representation y of the related event t. Thus, this baseline
It is
falls in the ﬁrst family of methods, see ﬁgure 2(a).
optimized using mean-squared error (MSE) loss LV
mse, see
eq. 6. The result of this baseline is reported in section 5,
table 1.

LV

mse =

(cid:107)yi − fV (vi; WV )(cid:107)2
2.

(6)

1
N

N
(cid:88)

i=1

Also, we train another baseline modelC, which is similar
to the aforementioned V except instead of using MSE loss
LV
con, as follows:

mse, see eq. (6), it uses contrastive loss LC

LC

con =

hi · d2

i + (1 − hi) max(1 − di, 0)2,

(7)

1
2N

N
(cid:88)

i=1

di = (cid:107)yi − fV (vi; WV )(cid:107)2.

4.4. Uniﬁed Embedding and Metric Learning

In this experiment, we demonstrate the beneﬁt of the uni-
ﬁed embedding. In contrast to our model presented in sec-
tion 3, we investigate baseline modelS , where this baseline
does not learn joint embedding. Instead, it separately learns
visual fV (·) and textual fT (·) projections. We model these

projections as a shallow (2-layer) MLP trained to classify
the data input into 500 event categories, using logistic loss,
same as eq. (4).

We conduct another experiment to demonstrate the ben-
eﬁt of learning metric space. In contrast to our model pre-
sented in section 3, we investigate baseline modelN , where
we discard the metric learning layer. Consequently, this
baseline learns the visual embedding is a shallow (2 lay-
ers) multi-layer perceptron with tanh non linearities. Also,
we replace the contrastive loss Lc, see eq. (2) with mean-
squared error loss Lmse, namely

LN

mse =

(cid:107)fT (ti; WT ) − fV (vi; WV )(cid:107)2
2.

(8)

1
N

N
(cid:88)

i=1

During retrieval, this baseline embeds a test video vi and
novel text query ti as features zv, zt onto the common space
Z using textual and visual embeddings fT (·), fV (·), respec-
tively. However, in a post-processing step, retrieval score
si for the video vi is the cosine distance between (zv, zt).
Similarly, all test videos are scored, ranked and retrieved.
The results of the aforementioned baselines modelS and
modelN are reported in table 1.

Comparing Different Embeddings.

In the previous ex-

periments, we investigated several baselines of the uni-
ﬁed embedding (modelU ), namely visual-only embedding
(modelV ), separate visual-textual embedding (modelS ) and
non-metric visual-textual embedding (modelN ). In a quali-
tative manner, we compare the results of such embeddings.
As shown in ﬁgure 6, we use these baselines to embed event
videos of MED-13 and MED-14 datasets into the corre-
sponding spaces. At the same time, we project the textual
description of the events on the same space. Then, we use
t-SNE [51] to visualize the result on 2D manifold. As seen,
the uniﬁed embedding, see sub-ﬁgures 6(e), 6(f) learns
more discriminant representations than the other baselines,
see sub-ﬁgures 6(a), 6(b), 6(c) and 6(d). The same obser-
vation holds for both for MED-13 and MED-14 datasets.

4.5. Mitigating Noise in EventNet

Based of quantitative and qualitative analysis, we con-
clude that EventNet is noisy. Not only videos are uncon-
strained, but also some of the video samples are irrelevant
to their event categories. EvenNet dataset [14] is accom-
panied by 500-category CNN classiﬁer. It achieves top-1
and top-5 accuracies of 30.67% and 53.27%, respectively.
Since events in EventNet are structured as an ontological
hierarchy, there is a total of 19 high-level categories. The
classiﬁer achieves top-1 and top-5 accuracies of 38.91% and
57.67%, respectively, over these high-level categories.

Based on these observations, we prune EventNet to re-
move noisy videos. To this end, ﬁrst we represent each
video as average pooling of ResNet pool5 features. Then,
we follow the conventional 5-fold cross validation with 5
rounds. For each round, we split the dataset into 5 sub-
sets, 4 subsets Vt for training and the last Vp for pruning.
Then we train a 2-layer MLP for classiﬁcation. After train-
ing, we forward-pass the videos of Vp and rule-out the mis-
classiﬁed ones.

The intuition behind pruning is that we rather learn
salient event concepts using less video samples than learn
noisy concepts with more samples. Pruning reduced the to-
tal number of videos by 26%, from 90.2k to 66.7k. This
pruned dataset is all what we use in our experiments.

4.6. Latent Topics in LSI

When training LSI topic model on Wikipedia corpus, a
crucial parameter is the number of latent topics K the model
constructs. We observe improvements in the performance
directly proportional to increasing K. The main reason that
the bigger the value of K, the more discriminant the LSI
feature is. Figure 7 conﬁrms our understanding.

5. Results

Evaluation metric. Since we are addressing, in essence,
an information retrieval task, we rely on the average preci-
sion (AP) per event, and mean average precision (mAP) per

Figure 7. Similarity matrix between LSI features of MED-14
events. The more the latent topics (K) in LSI model, the higher
the feature dimension, and the more discriminant the feature.

dataset. We follow the standard evaluation method as in the
relevant literature [1, 2, 52].

Comparing against model baselines. In table 1, we re-
port the mAP score of our model baselines, previously dis-
cussed in the experiments, see section 4. The table clearly
shows the marginal contribution of each of novelty for the
proposed method.

Baseline Loss
modelV LV
modelC LC
modelS Llog
modelN LN
modelU LU

Metric fV (·) fT (·) MED13 MED14
10.76
12.31
13.49
14.36
16.67

11.90
13.29
15.60
15.92
17.86

(cid:55)
mse (6)
con (7) (cid:51)
(cid:55)
(4)
(cid:55)
mse (8)
(5) (cid:51)

(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)

Table 1. Comparison between the uniﬁed embedding and other
baselines. The uniﬁed embedding modelU achieves the best re-
sults on MED-13 and MED-14 datasets.

Comparing against related work. We report the perfor-
mance of our method, the uniﬁed embedding modelU on
TRECVID MED-13 and MED-14 datasets. When com-
pared with the related works, our method improves over
the state-of-the-art by a considerable margin, as shown in
table 2 and ﬁgure 8.

Method
TagBook [18]
Discovary [7]
Composition [8]
Classiﬁers [9]
VideoStory† [17]
VideoStory∗ [17]
This Paper (modelU )

ToM '15 12.90
ICAI '15 09.60
AAAI '16 12.64
CVPR '16 13.46
PAMI '16 15.90
PAMI '16 20.00
17.86

MED13 MED14
05.90
–
13.37
14.32
05.20
08.00
16.67

Table 2. Performance comparison between our model and related
works. We report the mean average precision (mAP%) for MED-
13 and MED-14 datasets.

It is important to point out that VideoStory† uses only ob-
ject feature representation, so its comparable to our method.

(a) MED-13 Dataset

Figure 8. Event detection accuracies: per-event average precision (AP%) and per-dataset mean average precision (mAP%) for MED-13
and MED-14 datasets. We compare our results against TagBook [18], Discovary [7], Composition [8], Classiﬁers [9] and VideoStory [17].

(b) MED-14 Dataset

However, VideoStory∗ uses motion feature representation
and expert text query (i.e. using term-importance matrix
H in [17]). To rule out the marginal effect of using dif-
ferent datasets and features, we train VideoStory and report
results in table 3. Clearly, CNN features and video exem-
plars in the training set can improve the model accuracy, but
our method improves against VideoStory when trained on
the same dataset and using the same features. Other works
(Classiﬁers [9], Composition [8]) use both image and ac-
tion concept classiﬁers. Nonetheless, our method improves
over them using only object-centric CNN feature represen-
tations.

Training Set

CNN Feat. MED14
Method
VideoStory VideoStory46k [17] GoogleNet 08.00
GoogleNet 11.84
VideoStory FCVID [53]
GoogleNet 14.52
VideoStory EventNet [14]
15.80
ResNet
VideoStory EventNet [14]
16.67
ResNet
This Paper EventNet [14]

proposed to learn metric space. This enables measuring
the similarities between the embedded modalities using this
very space.

We experimented the novelties and demonstrated how
they contribute to improving the performance. We comple-
mented this by improvements over the state-of-the-art by
considerable margin on MED-13 and MED-14 datasets.

However, the question still remains, how can we discrim-
inate between these two MED events “34: ﬁxing musical in-
strument” and “40: tuning musical instrument”. We would
like to argue that temporal modeling for human actions in
videos is of absolute necessity to achieve such ﬁne-grained
event recognition. In future research, we would like to focus
on human-object interaction in videos and how to model it
temporally.

Acknowledgment

We thank Dennis Koelma, Masoud Mazloom and Cees
Snoek2 for lending their insights and technical support for
this work.

Table 3. Our method improves over VideoStory when trained on
the same dataset and using the same feature representation.

References

6. Conclusion

In this paper, we presented a novel approach for detect-
ing events in unconstrained web videos, in a zero-exemplar
fashion. Rather than learning separate embeddings form
cross-modal datasets, we proposed a uniﬁed embedding
where several cross-modalities are jointly projected. This
enables end-to-end learning. On top of this, we exploited
the fact that zero-exemplar is posed as retrieval task and

[1] Paul Over, George Awad, Jon Fiscus, Greg Sanders, and Bar-
bara Shaw. Trecvid 2013–an introduction to the goals, tasks,
In TRECVID
data, evaluation mechanisms, and metrics.
Workshop, 2013. 1, 2, 5, 7

[2] Paul Over, Jon Fiscus, Greg Sanders, David Joy, Mar-
tial Michel, George Awad, Alan Smeaton, Wessel Kraaij,
and Georges Qu´enot. Trecvid 2014–an overview of the
In
goals, tasks, data, evaluation mechanisms and metrics.
TRECVID Workshop, 2014. 1, 2, 7

2{kolema,m.mazloom,cgmsnoek}@uva.nl

[3] Lu Jiang, Shoou-I Yu, Deyu Meng, Teruko Mitamura, and
Alexander G Hauptmann. Bridging the ultimate semantic
gap: A semantic search engine for internet videos. In ICMR,
2015. 1, 2

[19] Shuang Wu, Sravanthi Bondugula, Florian Luisier, Xiaodan
Zhuang, and Pradeep Natarajan. Zero-shot event detection
using multi-modal fusion of weakly supervised concepts. In
IEEE CVPR, 2014. 2

[4] Amirhossein Habibian, Thomas Mensink, and Cees GM
Snoek. Composite concept discovery for zero-shot video
event detection. In ICMR, 2014. 1

[5] Amirhossein Habibian, Thomas Mensink, and Cees GM
Snoek. Discovering semantic vocabularies for cross-media
retrieval. In ICMR, 2015. 1

[6] Masoud Mazloom, Efstrastios Gavves, and Cees G. M.
Snoek. Conceptlets: Selective semantics for classifying
video events. In IEEE TMM, 2014. 1

[7] Xiaojun Chang, Yi Yang, Alexander G Hauptmann, Eric P
Xing, and Yao-Liang Yu. Semantic concept discovery for
large-scale zero-shot event detection. In IJCAI, 2015. 1, 2,
7, 8

[8] Xiaojun Chang, Yi Yang, Guodong Long, Chengqi Zhang,
and Alexander G Hauptmann. Dynamic concept composi-
tion for zero-example event detection. In arXiv, 2016. 1, 2,
7, 8

[9] Xiaojun Chang, Yao-Liang Yu, Yi Yang, and Eric P Xing.
They are not equally reliable: Semantic event search using
differentiated concept classiﬁers. In IEEE CVPR, 2016. 1, 2,
7, 8

[10] Yi-Jie Lu. Zero-example multimedia event detection and re-
counting with unsupervised evidence localization. In ACM
MM, 2016. 1, 2, 3

[11] Lu Jiang, Shoou-I Yu, Deyu Meng, Yi Yang, Teruko Mi-
tamura, and Alexander G Hauptmann. Fast and accurate
content-based semantic search in 100m internet videos. In
ACM MM, 2015. 1

[12] Thomas Mensink, Efstratios Gavves, and Cees Snoek. Costa:
Co-occurrence statistics for zero-shot classiﬁcation. In IEEE
CVPR, 2014. 1

[13] E. Gavves, T. E. J. Mensink, T. Tommasi, C. G. M. Snoek,
and T Tuytelaars. Active transfer learning with zero-shot
priors: Reusing past datasets for future tasks. In IEEE ICCV,
2015. 1

[14] Guangnan Ye, Yitong Li, Hongliang Xu, Dong Liu, and
Shih-Fu Chang. Eventnet: A large scale structured concept
library for complex event detection in video. In ACM MM,
2015. 1, 4, 7, 8

[15] Wikihow. http://wikihow.com. 1, 5

[16] Amirhossein Habibian, Thomas Mensink, and Cees GM
Snoek. Videostory: A new multimedia embedding for few-
example recognition and translation of events. In ACM MM,
2014. 2, 3, 4

[17] Amirhossein Habibian, Thomas Mensink, and Cees GM
Snoek. Videostory embeddings recognize events when ex-
amples are scarce. In IEEE TPAMI, 2016. 2, 3, 4, 7, 8

[20] Mohamed Elhoseiny, Jingen Liu, Hui Cheng, Harpreet
Sawhney, and Ahmed Elgammal. Zero-shot event detection
by multimodal distributional semantic embedding of videos.
In arXiv, 2015. 2

[21] Tomas Mikolov, Quoc V Le, and Ilya Sutskever. Exploit-
ing similarities among languages for machine translation. In
arXiv, 2013. 2

[22] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
and Jeff Dean. Distributed representations of words and
phrases and their compositionality. In NIPS, 2013. 2, 3

[23] Lu Jiang, Deyu Meng, Teruko Mitamura, and Alexander G
Hauptmann. Easy samples ﬁrst: Self-paced reranking for
zero-example multimedia search. In ACM MM, 2014. 3

[24] Lu Jiang, Teruko Mitamura, Shoou-I Yu, and Alexander G
Hauptmann. Zero-example event search using multimodal
pseudo relevance feedback. In ICMR, 2014. 3

[25] Arnav Agharwal, Rama Kovvuri, Ram Nevatia,

and
Cees GM Snoek. Tag-based video retrieval by embedding se-
mantic content in a continuous word space. In IEEE WACV,
2016. 3

[26] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,
and Manohar Paluri. C3d: generic features for video analy-
sis. In arXiv, 2014. 3

[27] Karen Simonyan and Andrew Zisserman. Two-stream con-
volutional networks for action recognition in videos.
In
NIPS, 2014. 3

[28] Heng Wang, Alexander Kl¨aser, Cordelia Schmid, and
Cheng-Lin Liu. Action recognition by dense trajectories. In
IEEE CVPR, 2011. 3

[29] J Uijlings, IC Duta, Enver Sangineto, and Nicu Sebe. Video
classiﬁcation with densely extracted hog/hof/mbh features:
an evaluation of the accuracy/computational efﬁciency trade-
off. In IJMIR, 2015. 3

[30] B. Fernando, E. Gavves, J. Oramas, A. Ghodrati, and
T. Tuytelaars. Rank pooling for action recognition. In IEEE
TPAMI, 2016. 3

[31] Lindasalwa Muda, Mumtaj Begam, and I Elamvazuthi. Voice
recognition algorithms using mel frequency cepstral coefﬁ-
cient (mfcc) and dynamic time warping (dtw) techniques. In
arXiv, 2010. 3

[32] Anurag Kumar and Bhiksha Raj. Audio event detection us-

ing weakly labeled data. In arXiv, 2016. 3

[33] Liping Jing, Bo Liu, Jaeyoung Choi, Adam Janin, Julia
Bernd, Michael W Mahoney, and Gerald Friedland. A dis-
criminative and compact audio representation for event de-
tection. In ACM MM, 2016. 3

[18] Masoud Mazloom, Xirong Li, and Cees Snoek. Tagbook: A
semantic video representation without supervision for event
detection. In IEEE TMM, 2015. 2, 7, 8

[34] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document recog-
nition. In IEEE. 3

[53] Yu-Gang Jiang, Zuxuan Wu, Jun Wang, Xiangyang Xue, and
Shih-Fu Chang. Exploiting feature and class relationships in
video categorization with regularized deep neural networks.
In IEEE TPAMI, 2017. 8

[35] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In arXiv, 2015.
3, 4, 5

[36] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NIPS, 2012. 3

[37] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In arXiv,
2014. 3, 5

[38] Chuang Gan, Ting Yao, Kuiyuan Yang, Yi Yang, and Tao
Mei. You lead, we exceed: Labor-free video concept learning
by jointly exploiting web videos and images. In IEEE CVPR,
2016. 3

[39] Karen Simonyan, Omkar M Parkhi, Andrea Vedaldi, and An-
drew Zisserman. Fisher vector faces in the wild. In BMVC,
2013. 3

[40] Relja Arandjelovic and Andrew Zisserman. All about vlad.

In IEEE CVPR, 2013. 3

[41] Pascal Mettes, Jan C van Gemert, Spencer Cappallo, Thomas
Mensink, and Cees GM Snoek. Bag-of-fragments: Select-
ing and encoding video fragments for event detection and
recounting. In ICMR, 2015. 3

[42] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to

sequence learning with neural networks. In NIPS, 2014. 3

[43] Quoc V Le and Tomas Mikolov. Distributed representations
of sentences and documents. In ICML, 2014. 3, 4, 5

[44] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent

dirichlet allocation. In JMLR, 2003. 3, 5

[45] Scott Deerwester, Susan T Dumais, George W Furnas,
Thomas K Landauer, and Richard Harshman. Indexing by
latent semantic analysis. In JACS, 1990. 3, 4, 5

[46] Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning
a similarity metric discriminatively, with application to face
veriﬁcation. In IEEE CVPR, 2005. 3

[47] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In IEEE CVPR, 2015. 5

[48] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Antonio Tor-
ralba, and Aude Oliva. Places: An image database for deep
scene understanding. In arXiv, 2016. 5

[49] Wikipedia, 2016. http://wikipedia.com. 5

[50] Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard
Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.
Skip-thought vectors. In NIPS, 2015. 5

[51] Laurens van der Maaten and Geoffrey Hinton. Visualizing

data using t-sne. In JMLR, 2008. 7

[52] Yu-Gang Jiang, Guangnan Ye, Shih-Fu Chang, Daniel El-
lis, and Alexander C Loui. Consumer video understanding:
A benchmark database and an evaluation of human and ma-
chine performance. In ICMR, 2011. 7

Uniﬁed Embedding and Metric Learning for Zero-Exemplar Event Detection

Noureldien Hussein, Efstratios Gavves, Arnold W.M. Smeulders
QUVA Lab, University of Amsterdam
{nhussein,egavves,a.w.m.smeulders}@uva.nl

7
1
0
2
 
y
a
M
 
5
 
 
]

V
C
.
s
c
[
 
 
1
v
8
4
1
2
0
.
5
0
7
1
:
v
i
X
r
a

Abstract

Event detection in unconstrained videos is conceived
as a content-based video retrieval with two modalities:
textual and visual. Given a text describing a novel event,
the goal is to rank related videos accordingly. This task is
zero-exemplar, no video examples are given to the novel
event.
Related works train a bank of concept detectors on external
data sources. These detectors predict conﬁdence scores for
test videos, which are ranked and retrieved accordingly. In
contrast, we learn a joint space in which the visual and
textual representations are embedded. The space casts a
novel event as a probability of pre-deﬁned events. Also, it
learns to measure the distance between an event and its
related videos.
Our model is trained end-to-end on publicly available
EventNet. When applied to TRECVID Multimedia Event
Detection dataset, it outperforms the state-of-the-art by a
considerable margin.

1. Introduction

TRECVID Multimedia Event Detection (MED) [1, 2] is
a retrieval task for event videos, with the reputation of being
realistic. It comes in two ﬂavors: few-exemplar and zero-
exemplar, where the latter means that no video example is
known to the model. Although expecting a few examples
seems reasonable, in practice this implies that the user must
already have an index of any possible query, making it very
limited. In this paper, we focus on event video search with
zero exemplars.

Retrieving videos of never-seen events, such as “reno-
vating home”, without any video exemplar poses several
challenges. One challenge is how to bridge the gap be-
tween the visual and the textual semantics [3, 4, 5]. One
approach [3, 6, 7, 8, 9, 10] is to learn a dictionary of con-
cept detectors on external data source. Then, scores for test
videos are predicted using these detectors. Test videos are
then ranked and retrieved accordingly. The inherent weak-

Figure 1. We pose the problem of zero-exemplar event detection as
learning from a repository of pre-deﬁned events. Given video ex-
emplars of events “removing drywall” or “ﬁt wall times”, one may
detect a novel event “renovate home” as a probability distribution
over the predeﬁned events.

ness of this approach is that the presentation of a test video
is reduced to a limited vocabulary from the concept dictio-
nary. Another challenge is how to overcome the domain
difference between training and test events. While Seman-
tic Query Generation (SQG) [3, 8, 9, 11] mitigates this chal-
lenge by extracting keywords from the event query, it does
not address how relevant these keywords to the event itself.
For example, keyword “person” is not relevant to event “car
repair” as it is to “ﬂash mob gathering”.

Our entry to zero-exemplar events is that they gener-
ally have strong semantic correlations [12, 13] with other
possibly seen events. For instance, the novel event “ren-
ovating home” is related to “ﬁt wall tiles”, “remove dry-
wall”, or even to “paint door”. Novel events can, there-
fore, be casted on a repository of prior events, for which
knowledge sources in various forms are available before-
hand, such as the videos, as in EventNet [14], or articles,
as in WikiHow [15]. Not only do these sources provide
video examples of a large –but still limited– set of events,
but also they provide an association of text description of
events with their corresponding videos. A text article can
describe the event in words: what is it about, what are the

1

details and what are the semantics. We note that such a
visual-textual repository of events may serve as a knowl-
edge source, by which we can interpret novel event queries.
For Zero-exemplar Event Detection (ZED), we propose

a neural model with the following novelties:
1. We formulate a uniﬁed embedding for multiple modal-
ities (e.g. visual and textual) that enables a contrastive
metric for maximum discrimination between events.
2. A textual embedding poses the representation of a novel
event as a probability of predeﬁned events, such that it
spans a much larger space of admissible expressions.
3. We exploit a single data source, comprising pairs of event
articles and related videos. A single source rather enables
end-to-end learning from multi-modal individual pairs.

We empirically shows that our novelties result in perfor-
mance improvement. We evaluate the model on TRECVID
Multimedia Event Detection (MED) 2013 [1] and 2014 [2].
Our results show signiﬁcant improvement over the state-of-
the-art.

2. Related Work

Figure 2. Three families of methods for zero-exemplar event de-
tection: (a), (b) and (c). They build on top of feature represen-
tations learned a priori (i.e. initial representations), such as CNN
features x for a video v or word2vec features y for event text query
t. In a post-processing step, the distance θ is measured between
the embedded features. In contrast, our model rather falls in a new
family, depicted in (d), for it learns uniﬁed embedding with metric
loss using single data source.

We identify three families of methods for ZED, as in ﬁg-

ure 2 (a), (b) and (c).
Visual Embedding and Textual Retrieval. As in ﬁg-
ure 2(a), given a video vi represented as x ∈ X and a re-
lated text t represented as y ∈ Y. Then, a visual model fV
is trained to project x as yv ∈ Y such that the distance is
minimized between (yv, y). In test time, video ranking and

retrieval is done using distance metric between the projected
test video yt and test query representation y.

[16, 17] project the visual feature x of a web video v
into term-vector representation y of the video’s textual title
t. However, during training, the model makes use of the
text query of the test events to learn better term-vector rep-
resentation. Consequently, this limits the generalization for
novel event queries.
Textual Embedding and Visual Retrieval. As in ﬁg-
ure 2(b), a given text query t is projected into xt ∈ X using
pre-trained or learned language model fT .

[18] makes use of freely-available weekly-tagged web
videos. Then it propagates tags to test videos from its near-
est neighbors. Methods [7, 8, 9, 10, 3] have similar ap-
proach. Given a text query t, Semantic Query Generation
(SQG) extracts N most related concepts {ci, i ∈ N } to the
test query. Then, pre-trained concept detectors predict prob-
ability scores {si, i ∈ N } for a test video v. Aggregating
these probabilities results in the ﬁnal video score sv, upon
which videos are ranked and retrieved. [9] learns weighted
averaging.

The shortcoming of this family is that expressing a
video as probability scores of few concepts is under-
representation. Any concept that exists in the video but is
missing in the concept dictionary is thus unrepresented.
Visual-Textual Embedding and Semantic Retrieval. As
in ﬁgure 2(c), visual fV and textual fT models are trained
to project both of the visual x and textual y features into a
semantic space Z. During test, ranking score is the distance
between the projections zv, zt in the semantic space Z.

[19] projects video concepts into a high-dimensional
lexicon space. Separately, it projects concept-based features
to the space, which overcomes the lexicon mismatch be-
tween the query and the video concepts. [20] embeds a fu-
sion of low and mid-level visual features into distributional
semantic manifold [21, 22]. In a separate step, it embeds
text-based concepts into the manifold.

The third family, see ﬁgure 2(c), is superior to the others,
see ﬁgure 2(a), (b). However, one drawback of [19, 20] is
separately embedding both the visual and textual features
zv, zt. This leads to another drawback, having to measure
the distance between (zv, zt) in a post-processing step (e.g.
cosine similarity).
Uniﬁed Embedding and Metric Learning Retrieval Our
method rather falls into a new family, see ﬁgure 2(d), and it
overcomes the shortcomings of [19, 20] by the following. It
is trained on a single data source, enabling a uniﬁed embed-
ding for features of multiple modalities into a metric space.
Consequently, the distance between the embedded features
is measured by the model using the learned metric space.

Auxiliary Methods Independent to the previous works, the
following techniques have been used to improve the results:

Figure 3. Model overview. Using dataset Dz of
M event categories and N videos. Each event has
a text article and a few videos. Given a video x
with text title k, belonging to an event with arti-
cle t, we extract features x, yk, yt respectively.
At the top, network fT learns to classify the title
feature yk into one of M event categories. In the
middle, we borrow the network fT to embed the
event article’s feature yt as zt ∈ Z. Then, at the
bottom, the network fV learns to embed the video
feature x as zv ∈ Z such that the distance be-
tween (cid:0)zv, zt(cid:1) is minimized, in the learned metric
space Z.

self-paced reranking [23], pseudo-relevance feedback [24],
event query manual intervention [25], early fusion of fea-
tures (action [26, 27, 28, 29, 30] or acoustic [31, 32, 33]) or
late fusion of concept scores [17]. All these contributions
may be applied to our method.

Visual Representation. ConvNets [34, 35, 36, 37] pro-
vide frame-level representation. To tame them into video-
level counterpart, literature use: i- frame-level ﬁltering [38]
ii- vector encoding [39, 40] iii- learned pooling and re-
counting [10, 41] iv- average pooling [16, 17]. Also, low-
level action [28, 29], mid-level action [26, 27] or acous-
tic [31, 32, 33] features can be used. Textual Represen-
tation. To represent text, literature use: i- sequential mod-
els [42] ii- continuous word-space representations [22, 43]
iii- topic models [44, 45] iv- dictionary-space representa-
tion [17].

3. Method

3.1. Overview

Our goal is zero-exemplar retrieval of event videos with
respect to their relevance to a novel textual description of
an event. More speciﬁcally, for the zero-exemplar video
dataset Dz = {vz
i }, i = 1, . . . , L and given any future, tex-
tual event description tz, we want to learn a model f (·) that
i according to the relevance to tz, namely:
ranks the videos vz

tz : vz

i (cid:31) vz

j → f (vz

i , tz) > f (vz

j , tz).

(1)

3.2. Model

Since we focus on zero-exemplar setting, we cannot ex-
pect any training data directly relevant to the test queries.
As such, we cannot directly optimize our model for the
parameters WT , WV in eq. (3). In the absence of any di-
rect data, we resort to external knowledge databases. More
speciﬁcally, we propose to cast future novel query descrip-
tions as a convex combination of known query descriptions
in external databases, where we can measure their relevance
the database videos.

We start from a dataset Dz = {vi, ki, lj, tj} ,

i =
1, . . . , N, j = 1, . . . , M organized by an event taxonomy,
where we do not neither expect nor require the events to
overlap with any future event queries. The dataset is com-
posed of M events. Each event is associated with a textual,
article description of the event, analyzing different aspects
of it, such as: (i) the typical appearance of subjects and
objects (ii) it’s procedures (iii) the steps towards complet-
ing task associated with it. The dataset contains in total N
videos, with vi denoting the i-th video in the dataset with
metadata ki, e.g. the title of the video. A video is associ-
ated with an event label li and the article description ti of
the event it belongs to. Since multiple videos belong to the
same event, they share the article description of such event.
The ultimate goal of our model is zero-exemplar search
for event videos. Namely, provided unknown text queries
by the user, we want to retrieve those videos that are rele-
vant. We illustrate our proposed model during training in
ﬁgure 3. The model is composed of two components, a
textual embedding fT (·), a visual embedding fV (·). Our
ultimate goal is the ranking of videos, vi (cid:31) vj (cid:31) vk with
respect to their relevance to a query description, or in pair-
wise terms vi (cid:31) vj, vj (cid:31) vk and vi (cid:31) vk.

Let us assume a pair of videos vi, vj and query descrip-
tion t, where video vi is more relevant to the query t than vj.
Our goal is a model that learns to put videos in the correct
relative order, namely (vi, t) (cid:31) (vj, t). This is equivalent
to a model that learns visual-textual embeddings such that
j , where dtv
i < dtv
dtv
is the distance between visual-textual
i
embeddings of (vi, t), dtv
is the same for (vj, t). Since we
j
want to compare distances between pairs (vi, t), (vj, t), we
pose the learning of our model as the minimization of a con-
trastive loss [46]:

Lcon =

hi · d2

i + (1 − hi) max(1 − di, 0)2,

(2)

1
2N

N
(cid:88)

i=1

di = (cid:107)fT (ti; WT ) − fV (vi; WV )(cid:107)2,

(3)

where fT (ti; WT ) is the projection of the query descrip-
tion ti into the uniﬁed metric space Z parameterized by

WT , fV (vi; WV ) is the projection of a video vi onto the
same space Z parameterized by WV and hi a target vari-
able that equals to 1 when the i-th video is relevant to the
query description ti and 0 otherwise. Naturally, to optimize
eq. (2), we ﬁrst need to deﬁne the projections fT (·; WT )
and fV (·; WV ) in eq. (3).

Textual Embedding. The textual embedding component of
our model, fT (·; WT ), is illustrated in ﬁgure 3 (top). This
component is dedicated to learn a projection of a textual
input –including any future event queries t– on to the uni-
ﬁed space Z. Before detailing our model fT , however, we
note that that the textual embedding can be employed not
only with event article descriptions, but also with any other
textual information that might be associated to the dataset
videos, such as textual metadata. Although we expect the
video title not to be as descriptive as the associated article,
they may still be able to offer some discriminative informa-
tion as previously shown [16, 17] which can be associated
to the event category.

We model the textual embedding as a shallow (two lay-
ers) multi-layer perceptron (MLP). For the ﬁrst layer we
employ a ReLU nonlinearity. The second layer serves a
dual purpose. First, it projects the article description of an
event on the uniﬁed space Z. This projection is category-
speciﬁc, namely different videos that belong to the same
event will share the projection. Second, it can project any
video-speciﬁc textual metadata into the uniﬁed space. We,
therefore, propose to embed the title metadata ki, which is
uniquely associated with a video, not an event category. To
this end, we opt for softmax nonlinearity for the second
layer, followed by an additional logistic loss term to penal-
ized misprediction of titles mi with respect to the video’s
event label yj

i , namely

Llog =

−yj

i log f j

T (ki; WT ).

(4)

N
(cid:88)

M
(cid:88)

i=1

j=1

Overall, the textual embedding f j

T is trained with a dual
loss in mind. The ﬁrst loss term, see eq. (2) (3) takes care
that the ﬁnal network learns event-relevant textual projec-
tions. The second loss term, see eq. (4), takes care that the
ﬁnal network does not overﬁt to the particular event article
descriptions. The latter is crucial because the event article
descriptions in Dz will not overlap with the future event
queries, since we are in a zero-exemplar retrieval setting.
As such, training the textual embedding to be optimal only
for these event descriptions will likely result in severe over-
ﬁtting. Our goal and hope is that the ﬁnal textual embed-
ding model fT will capture both event-aware and video-
discriminative textual features.

Visual Embedding. The visual embedding component of
our model, fV (·; WV ), is illustrated in ﬁgure 3 (bottom).

This component is dedicated to learn a projection from the
visual input, namely the videos in our zero-exemplar dataset
Dz, into the uniﬁed metric space Z. The goal is to project
the videos belonging to semantically similar events; project
them into a similar region in the space. We model the visual
embedding fV (vi; WV ) using a shallow (two layers) multi-
layer perceptron with tanh nonlinearities, applied to any
visual feature for video vi.

End-to-End Training. At each training forward-pass, the
model is given a triplet of data inputs, an event description
ti, a related video vi and video title ki. From eq. (3) we
observe that the visual embedding fV (vi; WV ) is encour-
aged to minimize its distance with the output of the textual
embedding fT (ti; WT ). In the end, all the modules of the
proposed model are differentiable. Therefore, we train our
model in an end-to-end manner by minimizing the follow-
ing objective

LU ,

arg min
WV ,WT
LU = Lcon + Llog.

(5)

input vi

For the triplet input (vi, ti, ki), we rely on external repre-
sentations, since our ultimate goal is zero-exemplar search.
Strictly speaking, a visual
is represented as
CNN [35] feature vector, while textual inputs ti, ki are rep-
resented as LSI [45] or Doc2Vec [43] feature vectors. How-
ever, given that these external representations rely on neural
network architectures, if needed, they could also be further
ﬁne-tuned. We choose to freeze CNN and Doc2Vec mod-
ules to speed up training. Finally, in this paper we refer to
our main model with uniﬁed embedding, as modelU .

Inference. After training, we ﬁx the parameters (WV , WT ).
At test time, we set our function f (·) from eq. (1) to be
equivalent to the distance function from eq. (??). Hence, at
test time, we compute the Euclidean distance in the learned
metric space Z between the embeddings (zv, zt) of test
video v and novel event description t, respectively.

4. Experiments

4.1. Datasets

Before delving into the details of our experiments, ﬁrst

we describe the external knowledge sources we use.

Training dataset. We leverage videos and articles from
publicly available datasets. EvenNet [14] is a dataset of
∼90k event videos, harvested from YouTube and catego-
rized into 500 events in hierarchical form according to the
events’ ontology. Each event category contains around 180
videos. Each video is coupled with a text title, few tags and
related event’s ontology.

We exploit the fact that all events in EventNet are har-
vested from WikiHow [15] – a website for How-To articles
covering a wide spectrum of human activities. For instance:
“How to Feed a Dog” or “How to Arrange Flowers”. Thus,
we crawl WikiHow to get the articles related to all the events
in EventNet.

Test dataset. As the task is zero-exemplar, the test sets are
different from the training. While EventNet serves as the
training, the following serve as the test: TRECVID MED-
13 [1] and MED-14 [1].
In details, they are datasets of
videos for events. They comprise 27k videos. There are two
versions, MED-13 and MED-14 with 20 events for each.
Since 10 events overlap, the result is 30 different events in
total. Each event is coupled with short textual description
(title and deﬁnition).

4.2. Implementation Details

Video Features. To represent a video v, we uniformly sam-
ple a frame every one second. Then, using ResNet [35], we
extract pool5 CNN features for the sampled frames. Then,
we average pool the frame-level features to get the video-
level feature xv. We experiment different features from dif-
ferent CNN models: ResNet (prob, fc1000), VGG [37]
(fc6, fc7), GoogLeNet [47] (pool5, fc1024), and
Places365 [48] (fc6, fc7,fc8) except we ﬁnd ResNet
pool5 to be the best. We only use ResNet pool5 and we
don’t fuse multiple CNN features.

Text Features. We choose topic modeling [44, 45], as it
is well-suited for long (and sometimes noisy) text articles.
We train LSI topic model [45] on Wikipedia corpus [49].
We experiment different latent topics ranging from 300
to 6000, expect we found 2500 to be the best. Also,
we experiment other textual representations as LDA [44],
SkipThoughts [50] and Doc2Vec [43]. To extract a feature
from an event article k or video title t, ﬁrst we preprocess
the text using standard MLP steps: tokenization, lemmati-
zation and stemming. Then, for k, t we extract 2500-D LSI
features yk, yt, respectively. The same steps apply to MED
text queries.

Model Details.
Our visual and textual embeddings
fV (·), fT (·) are learned on top of the aforementioned visual
and textual features (xv, yk, yt). fT (·) is a 1-hidden layer
MLP classiﬁer with ReLU for hidden, softmax for out-
put, logistic loss and 2500-2500-500 neurons for
the input, hidden, and output layers, respectively. Similarly,
fV (·) is a 1-hidden layer MLP regressor with ReLU for hid-
den, contrastive loss and 2048-2048-500 neurons
for the input, hidden, and output layers, respectively. Our
code is made public1 to support further research.

1github.com/noureldien/unified_embedding

4.3. Textual Embedding

(a) LSI Features

(b) Embedded Features

Figure 4. Our textual embedding (b) maps MED to EventNet
events better than LSI features. Each dot in the matrix shows the
similarity between MED and EventNet events.

Here, we qualitatively demonstrate the beneﬁt of the tex-
tual embedding fT (·). Figure 4 shows the similarity matrix
between MED and EventNet events. Each dot represents
how a MED event is similar to EventNet events. It shows
that our embedding (right) is better than LSI (left) in map-
ping MED to EventNet events. For example, LSI wrongly
maps “9: getting a vehicle unstuck” to “256: launch a boat”
while our embedding correctly maps it to “170: drive a car”.
Also, our embedding maps with higher conﬁdence than LSI,
as in “16: doing homework or study”.

(a) LSI Features

(b) Embedded Features

Figure 5. For 20 events of MED-14, our textual embedding (right)
is more discriminant than the LSI feature representation (left).
Each dot in the matrix shows how similar an event to all the others.

Figure 5 shows the similarity matrix for MED events,
where each dot represents how related any MED event to
all the others. Our textual embedding (right) is more dis-
criminant than on the LSI feature representation (left). For
example, LSI representation shows high semantic correla-
tion between events “34: ﬁxing musical instrument” and
“40: tuning musical instrument”, while our embedding dis-
criminate them.

Next, we quantitatively demonstrate the beneﬁt of the
textual embedding fT (·).
In contrast to the main model,
see section 3, we investigate baseline modelV , where we
discard the textual embedding fT (·) and consider only the
visual embedding fV (·). We project a video v on the LSI

(a) MED-13, visual embedding (modelV ).

(b) MED-13, separate embedding (modelS ).

(c) MED-13, uniﬁed embedding (modelU ).

(d) MED-14, visual embedding (modelV ).

(e) MED-14 separate embedding (modelS ).
Figure 6. We visualize the results of video embedding using the uniﬁed embedding modelU and baselines modelV , modelS . Each sub-ﬁgure
shows how discriminant the representation of the embedded videos. Each dot represents a projected video, while each pentagon-shape
represents a projected event description. We use t-SNE to visualize the result.

(f) MED-14, uniﬁed embedding (modelU ).

representation y of the related event t. Thus, this baseline
It is
falls in the ﬁrst family of methods, see ﬁgure 2(a).
optimized using mean-squared error (MSE) loss LV
mse, see
eq. 6. The result of this baseline is reported in section 5,
table 1.

LV

mse =

(cid:107)yi − fV (vi; WV )(cid:107)2
2.

(6)

1
N

N
(cid:88)

i=1

Also, we train another baseline modelC, which is similar
to the aforementioned V except instead of using MSE loss
LV
con, as follows:

mse, see eq. (6), it uses contrastive loss LC

LC

con =

hi · d2

i + (1 − hi) max(1 − di, 0)2,

(7)

1
2N

N
(cid:88)

i=1

di = (cid:107)yi − fV (vi; WV )(cid:107)2.

4.4. Uniﬁed Embedding and Metric Learning

In this experiment, we demonstrate the beneﬁt of the uni-
ﬁed embedding. In contrast to our model presented in sec-
tion 3, we investigate baseline modelS , where this baseline
does not learn joint embedding. Instead, it separately learns
visual fV (·) and textual fT (·) projections. We model these

projections as a shallow (2-layer) MLP trained to classify
the data input into 500 event categories, using logistic loss,
same as eq. (4).

We conduct another experiment to demonstrate the ben-
eﬁt of learning metric space. In contrast to our model pre-
sented in section 3, we investigate baseline modelN , where
we discard the metric learning layer. Consequently, this
baseline learns the visual embedding is a shallow (2 lay-
ers) multi-layer perceptron with tanh non linearities. Also,
we replace the contrastive loss Lc, see eq. (2) with mean-
squared error loss Lmse, namely

LN

mse =

(cid:107)fT (ti; WT ) − fV (vi; WV )(cid:107)2
2.

(8)

1
N

N
(cid:88)

i=1

During retrieval, this baseline embeds a test video vi and
novel text query ti as features zv, zt onto the common space
Z using textual and visual embeddings fT (·), fV (·), respec-
tively. However, in a post-processing step, retrieval score
si for the video vi is the cosine distance between (zv, zt).
Similarly, all test videos are scored, ranked and retrieved.
The results of the aforementioned baselines modelS and
modelN are reported in table 1.

Comparing Different Embeddings.

In the previous ex-

periments, we investigated several baselines of the uni-
ﬁed embedding (modelU ), namely visual-only embedding
(modelV ), separate visual-textual embedding (modelS ) and
non-metric visual-textual embedding (modelN ). In a quali-
tative manner, we compare the results of such embeddings.
As shown in ﬁgure 6, we use these baselines to embed event
videos of MED-13 and MED-14 datasets into the corre-
sponding spaces. At the same time, we project the textual
description of the events on the same space. Then, we use
t-SNE [51] to visualize the result on 2D manifold. As seen,
the uniﬁed embedding, see sub-ﬁgures 6(e), 6(f) learns
more discriminant representations than the other baselines,
see sub-ﬁgures 6(a), 6(b), 6(c) and 6(d). The same obser-
vation holds for both for MED-13 and MED-14 datasets.

4.5. Mitigating Noise in EventNet

Based of quantitative and qualitative analysis, we con-
clude that EventNet is noisy. Not only videos are uncon-
strained, but also some of the video samples are irrelevant
to their event categories. EvenNet dataset [14] is accom-
panied by 500-category CNN classiﬁer. It achieves top-1
and top-5 accuracies of 30.67% and 53.27%, respectively.
Since events in EventNet are structured as an ontological
hierarchy, there is a total of 19 high-level categories. The
classiﬁer achieves top-1 and top-5 accuracies of 38.91% and
57.67%, respectively, over these high-level categories.

Based on these observations, we prune EventNet to re-
move noisy videos. To this end, ﬁrst we represent each
video as average pooling of ResNet pool5 features. Then,
we follow the conventional 5-fold cross validation with 5
rounds. For each round, we split the dataset into 5 sub-
sets, 4 subsets Vt for training and the last Vp for pruning.
Then we train a 2-layer MLP for classiﬁcation. After train-
ing, we forward-pass the videos of Vp and rule-out the mis-
classiﬁed ones.

The intuition behind pruning is that we rather learn
salient event concepts using less video samples than learn
noisy concepts with more samples. Pruning reduced the to-
tal number of videos by 26%, from 90.2k to 66.7k. This
pruned dataset is all what we use in our experiments.

4.6. Latent Topics in LSI

When training LSI topic model on Wikipedia corpus, a
crucial parameter is the number of latent topics K the model
constructs. We observe improvements in the performance
directly proportional to increasing K. The main reason that
the bigger the value of K, the more discriminant the LSI
feature is. Figure 7 conﬁrms our understanding.

5. Results

Evaluation metric. Since we are addressing, in essence,
an information retrieval task, we rely on the average preci-
sion (AP) per event, and mean average precision (mAP) per

Figure 7. Similarity matrix between LSI features of MED-14
events. The more the latent topics (K) in LSI model, the higher
the feature dimension, and the more discriminant the feature.

dataset. We follow the standard evaluation method as in the
relevant literature [1, 2, 52].

Comparing against model baselines. In table 1, we re-
port the mAP score of our model baselines, previously dis-
cussed in the experiments, see section 4. The table clearly
shows the marginal contribution of each of novelty for the
proposed method.

Baseline Loss
modelV LV
modelC LC
modelS Llog
modelN LN
modelU LU

Metric fV (·) fT (·) MED13 MED14
10.76
12.31
13.49
14.36
16.67

11.90
13.29
15.60
15.92
17.86

(cid:55)
mse (6)
con (7) (cid:51)
(cid:55)
(4)
(cid:55)
mse (8)
(5) (cid:51)

(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)

Table 1. Comparison between the uniﬁed embedding and other
baselines. The uniﬁed embedding modelU achieves the best re-
sults on MED-13 and MED-14 datasets.

Comparing against related work. We report the perfor-
mance of our method, the uniﬁed embedding modelU on
TRECVID MED-13 and MED-14 datasets. When com-
pared with the related works, our method improves over
the state-of-the-art by a considerable margin, as shown in
table 2 and ﬁgure 8.

Method
TagBook [18]
Discovary [7]
Composition [8]
Classiﬁers [9]
VideoStory† [17]
VideoStory∗ [17]
This Paper (modelU )

ToM '15 12.90
ICAI '15 09.60
AAAI '16 12.64
CVPR '16 13.46
PAMI '16 15.90
PAMI '16 20.00
17.86

MED13 MED14
05.90
–
13.37
14.32
05.20
08.00
16.67

Table 2. Performance comparison between our model and related
works. We report the mean average precision (mAP%) for MED-
13 and MED-14 datasets.

It is important to point out that VideoStory† uses only ob-
ject feature representation, so its comparable to our method.

(a) MED-13 Dataset

Figure 8. Event detection accuracies: per-event average precision (AP%) and per-dataset mean average precision (mAP%) for MED-13
and MED-14 datasets. We compare our results against TagBook [18], Discovary [7], Composition [8], Classiﬁers [9] and VideoStory [17].

(b) MED-14 Dataset

However, VideoStory∗ uses motion feature representation
and expert text query (i.e. using term-importance matrix
H in [17]). To rule out the marginal effect of using dif-
ferent datasets and features, we train VideoStory and report
results in table 3. Clearly, CNN features and video exem-
plars in the training set can improve the model accuracy, but
our method improves against VideoStory when trained on
the same dataset and using the same features. Other works
(Classiﬁers [9], Composition [8]) use both image and ac-
tion concept classiﬁers. Nonetheless, our method improves
over them using only object-centric CNN feature represen-
tations.

Training Set

CNN Feat. MED14
Method
VideoStory VideoStory46k [17] GoogleNet 08.00
GoogleNet 11.84
VideoStory FCVID [53]
GoogleNet 14.52
VideoStory EventNet [14]
15.80
ResNet
VideoStory EventNet [14]
16.67
ResNet
This Paper EventNet [14]

proposed to learn metric space. This enables measuring
the similarities between the embedded modalities using this
very space.

We experimented the novelties and demonstrated how
they contribute to improving the performance. We comple-
mented this by improvements over the state-of-the-art by
considerable margin on MED-13 and MED-14 datasets.

However, the question still remains, how can we discrim-
inate between these two MED events “34: ﬁxing musical in-
strument” and “40: tuning musical instrument”. We would
like to argue that temporal modeling for human actions in
videos is of absolute necessity to achieve such ﬁne-grained
event recognition. In future research, we would like to focus
on human-object interaction in videos and how to model it
temporally.

Acknowledgment

We thank Dennis Koelma, Masoud Mazloom and Cees
Snoek2 for lending their insights and technical support for
this work.

Table 3. Our method improves over VideoStory when trained on
the same dataset and using the same feature representation.

References

6. Conclusion

In this paper, we presented a novel approach for detect-
ing events in unconstrained web videos, in a zero-exemplar
fashion. Rather than learning separate embeddings form
cross-modal datasets, we proposed a uniﬁed embedding
where several cross-modalities are jointly projected. This
enables end-to-end learning. On top of this, we exploited
the fact that zero-exemplar is posed as retrieval task and

[1] Paul Over, George Awad, Jon Fiscus, Greg Sanders, and Bar-
bara Shaw. Trecvid 2013–an introduction to the goals, tasks,
In TRECVID
data, evaluation mechanisms, and metrics.
Workshop, 2013. 1, 2, 5, 7

[2] Paul Over, Jon Fiscus, Greg Sanders, David Joy, Mar-
tial Michel, George Awad, Alan Smeaton, Wessel Kraaij,
and Georges Qu´enot. Trecvid 2014–an overview of the
In
goals, tasks, data, evaluation mechanisms and metrics.
TRECVID Workshop, 2014. 1, 2, 7

2{kolema,m.mazloom,cgmsnoek}@uva.nl

[3] Lu Jiang, Shoou-I Yu, Deyu Meng, Teruko Mitamura, and
Alexander G Hauptmann. Bridging the ultimate semantic
gap: A semantic search engine for internet videos. In ICMR,
2015. 1, 2

[19] Shuang Wu, Sravanthi Bondugula, Florian Luisier, Xiaodan
Zhuang, and Pradeep Natarajan. Zero-shot event detection
using multi-modal fusion of weakly supervised concepts. In
IEEE CVPR, 2014. 2

[4] Amirhossein Habibian, Thomas Mensink, and Cees GM
Snoek. Composite concept discovery for zero-shot video
event detection. In ICMR, 2014. 1

[5] Amirhossein Habibian, Thomas Mensink, and Cees GM
Snoek. Discovering semantic vocabularies for cross-media
retrieval. In ICMR, 2015. 1

[6] Masoud Mazloom, Efstrastios Gavves, and Cees G. M.
Snoek. Conceptlets: Selective semantics for classifying
video events. In IEEE TMM, 2014. 1

[7] Xiaojun Chang, Yi Yang, Alexander G Hauptmann, Eric P
Xing, and Yao-Liang Yu. Semantic concept discovery for
large-scale zero-shot event detection. In IJCAI, 2015. 1, 2,
7, 8

[8] Xiaojun Chang, Yi Yang, Guodong Long, Chengqi Zhang,
and Alexander G Hauptmann. Dynamic concept composi-
tion for zero-example event detection. In arXiv, 2016. 1, 2,
7, 8

[9] Xiaojun Chang, Yao-Liang Yu, Yi Yang, and Eric P Xing.
They are not equally reliable: Semantic event search using
differentiated concept classiﬁers. In IEEE CVPR, 2016. 1, 2,
7, 8

[10] Yi-Jie Lu. Zero-example multimedia event detection and re-
counting with unsupervised evidence localization. In ACM
MM, 2016. 1, 2, 3

[11] Lu Jiang, Shoou-I Yu, Deyu Meng, Yi Yang, Teruko Mi-
tamura, and Alexander G Hauptmann. Fast and accurate
content-based semantic search in 100m internet videos. In
ACM MM, 2015. 1

[12] Thomas Mensink, Efstratios Gavves, and Cees Snoek. Costa:
Co-occurrence statistics for zero-shot classiﬁcation. In IEEE
CVPR, 2014. 1

[13] E. Gavves, T. E. J. Mensink, T. Tommasi, C. G. M. Snoek,
and T Tuytelaars. Active transfer learning with zero-shot
priors: Reusing past datasets for future tasks. In IEEE ICCV,
2015. 1

[14] Guangnan Ye, Yitong Li, Hongliang Xu, Dong Liu, and
Shih-Fu Chang. Eventnet: A large scale structured concept
library for complex event detection in video. In ACM MM,
2015. 1, 4, 7, 8

[15] Wikihow. http://wikihow.com. 1, 5

[16] Amirhossein Habibian, Thomas Mensink, and Cees GM
Snoek. Videostory: A new multimedia embedding for few-
example recognition and translation of events. In ACM MM,
2014. 2, 3, 4

[17] Amirhossein Habibian, Thomas Mensink, and Cees GM
Snoek. Videostory embeddings recognize events when ex-
amples are scarce. In IEEE TPAMI, 2016. 2, 3, 4, 7, 8

[20] Mohamed Elhoseiny, Jingen Liu, Hui Cheng, Harpreet
Sawhney, and Ahmed Elgammal. Zero-shot event detection
by multimodal distributional semantic embedding of videos.
In arXiv, 2015. 2

[21] Tomas Mikolov, Quoc V Le, and Ilya Sutskever. Exploit-
ing similarities among languages for machine translation. In
arXiv, 2013. 2

[22] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
and Jeff Dean. Distributed representations of words and
phrases and their compositionality. In NIPS, 2013. 2, 3

[23] Lu Jiang, Deyu Meng, Teruko Mitamura, and Alexander G
Hauptmann. Easy samples ﬁrst: Self-paced reranking for
zero-example multimedia search. In ACM MM, 2014. 3

[24] Lu Jiang, Teruko Mitamura, Shoou-I Yu, and Alexander G
Hauptmann. Zero-example event search using multimodal
pseudo relevance feedback. In ICMR, 2014. 3

[25] Arnav Agharwal, Rama Kovvuri, Ram Nevatia,

and
Cees GM Snoek. Tag-based video retrieval by embedding se-
mantic content in a continuous word space. In IEEE WACV,
2016. 3

[26] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,
and Manohar Paluri. C3d: generic features for video analy-
sis. In arXiv, 2014. 3

[27] Karen Simonyan and Andrew Zisserman. Two-stream con-
volutional networks for action recognition in videos.
In
NIPS, 2014. 3

[28] Heng Wang, Alexander Kl¨aser, Cordelia Schmid, and
Cheng-Lin Liu. Action recognition by dense trajectories. In
IEEE CVPR, 2011. 3

[29] J Uijlings, IC Duta, Enver Sangineto, and Nicu Sebe. Video
classiﬁcation with densely extracted hog/hof/mbh features:
an evaluation of the accuracy/computational efﬁciency trade-
off. In IJMIR, 2015. 3

[30] B. Fernando, E. Gavves, J. Oramas, A. Ghodrati, and
T. Tuytelaars. Rank pooling for action recognition. In IEEE
TPAMI, 2016. 3

[31] Lindasalwa Muda, Mumtaj Begam, and I Elamvazuthi. Voice
recognition algorithms using mel frequency cepstral coefﬁ-
cient (mfcc) and dynamic time warping (dtw) techniques. In
arXiv, 2010. 3

[32] Anurag Kumar and Bhiksha Raj. Audio event detection us-

ing weakly labeled data. In arXiv, 2016. 3

[33] Liping Jing, Bo Liu, Jaeyoung Choi, Adam Janin, Julia
Bernd, Michael W Mahoney, and Gerald Friedland. A dis-
criminative and compact audio representation for event de-
tection. In ACM MM, 2016. 3

[18] Masoud Mazloom, Xirong Li, and Cees Snoek. Tagbook: A
semantic video representation without supervision for event
detection. In IEEE TMM, 2015. 2, 7, 8

[34] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document recog-
nition. In IEEE. 3

[53] Yu-Gang Jiang, Zuxuan Wu, Jun Wang, Xiangyang Xue, and
Shih-Fu Chang. Exploiting feature and class relationships in
video categorization with regularized deep neural networks.
In IEEE TPAMI, 2017. 8

[35] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In arXiv, 2015.
3, 4, 5

[36] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NIPS, 2012. 3

[37] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In arXiv,
2014. 3, 5

[38] Chuang Gan, Ting Yao, Kuiyuan Yang, Yi Yang, and Tao
Mei. You lead, we exceed: Labor-free video concept learning
by jointly exploiting web videos and images. In IEEE CVPR,
2016. 3

[39] Karen Simonyan, Omkar M Parkhi, Andrea Vedaldi, and An-
drew Zisserman. Fisher vector faces in the wild. In BMVC,
2013. 3

[40] Relja Arandjelovic and Andrew Zisserman. All about vlad.

In IEEE CVPR, 2013. 3

[41] Pascal Mettes, Jan C van Gemert, Spencer Cappallo, Thomas
Mensink, and Cees GM Snoek. Bag-of-fragments: Select-
ing and encoding video fragments for event detection and
recounting. In ICMR, 2015. 3

[42] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to

sequence learning with neural networks. In NIPS, 2014. 3

[43] Quoc V Le and Tomas Mikolov. Distributed representations
of sentences and documents. In ICML, 2014. 3, 4, 5

[44] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent

dirichlet allocation. In JMLR, 2003. 3, 5

[45] Scott Deerwester, Susan T Dumais, George W Furnas,
Thomas K Landauer, and Richard Harshman. Indexing by
latent semantic analysis. In JACS, 1990. 3, 4, 5

[46] Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning
a similarity metric discriminatively, with application to face
veriﬁcation. In IEEE CVPR, 2005. 3

[47] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In IEEE CVPR, 2015. 5

[48] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Antonio Tor-
ralba, and Aude Oliva. Places: An image database for deep
scene understanding. In arXiv, 2016. 5

[49] Wikipedia, 2016. http://wikipedia.com. 5

[50] Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard
Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.
Skip-thought vectors. In NIPS, 2015. 5

[51] Laurens van der Maaten and Geoffrey Hinton. Visualizing

data using t-sne. In JMLR, 2008. 7

[52] Yu-Gang Jiang, Guangnan Ye, Shih-Fu Chang, Daniel El-
lis, and Alexander C Loui. Consumer video understanding:
A benchmark database and an evaluation of human and ma-
chine performance. In ICMR, 2011. 7

