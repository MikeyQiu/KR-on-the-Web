Scalable Gaussian Process Classiﬁcation with
Additive Noise for Various Likelihoods

Haitao Liu, Yew-Soon Ong, Fellow, IEEE,, Ziwei Yu, Jianfei Cai, Senior Member, IEEE, and Xiaobo Shen

1

9
1
0
2
 
p
e
S
 
4
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
1
4
5
6
0
.
9
0
9
1
:
v
i
X
r
a

Abstract—Gaussian process classiﬁcation (GPC) provides a
ﬂexible and powerful statistical framework describing joint distri-
butions over function space. Conventional GPCs however suffer
from (i) poor scalability for big data due to the full kernel matrix,
and (ii) intractable inference due to the non-Gaussian likelihoods.
Hence, various scalable GPCs have been proposed through (i)
the sparse approximation built upon a small inducing set to
reduce the time complexity; and (ii) the approximate inference
to derive analytical evidence lower bound (ELBO). However,
these scalable GPCs equipped with analytical ELBO are limited
to speciﬁc likelihoods or additional assumptions. In this work,
we present a unifying framework which accommodates scalable
GPCs using various likelihoods. Analogous to GP regression
(GPR), we introduce additive noises to augment the probability
space for (i) the GPCs with step, (multinomial) probit and logit
likelihoods via the internal variables; and particularly, (ii) the
GPC using softmax likelihood via the noise variables themselves.
This leads to uniﬁed scalable GPCs with analytical ELBO by
using variational
inference. Empirically, our GPCs showcase
better results than state-of-the-art scalable GPCs for extensive
binary/multi-class classiﬁcation tasks with up to two million data
points.

Index Terms—Gaussian process classiﬁcation, large-scale, ad-

ditive noise, non-Gaussian likelihood, variational inference

I. INTRODUCTION

A S a non-parametric Bayesian model which is explainable

and provides conﬁdence in predictions, Gaussian process
(GP) has been widely investigated and used in various scenar-
ios, e.g., regression and classiﬁcation [1], active learning [2],
unsupervised learning [3], and multi-task learning [4], [5]. The
central task in GP is to infer the latent function f , which
follows a Gaussian process GP(0, k(.)) where the kernel k(.)
describes the covariance among inputs, from n observations
X = (x1, · · · , xn)T with labels y = (y1, · · · , yn)T. The
inference can be performed through the type-II maximum
likelihood which maximizes over the model evidence p(y).

We herein focus on GP classiﬁcation (GPC) [6], [7] with
discrete class labels. It
is more challenging than the GP
regression (GPR). Speciﬁcally, current GPC paradigms are
facing two main challenges. The ﬁrst is the poor scalability
to tackle massive datasets. The inversion and determinant of
the kernel matrix Knn = k(X, X) ∈ Rn×n incur O(n3)

Haitao Liu is with the Rolls-Royce@NTU Corporate Lab, Nanyang Tech-

nological University, Singapore, 637460. E-mail: htliu@ntu.edu.sg.

Yew-Soon Ong and Jianfei Cai are with School of Computer Science and
Engineering, Nanyang Technological University, Singapore, 639798. E-mail:
oyewsoon@gmail.com, asjfcai@ntu.edu.sg.

Ziwei Yu is with School of Computing, National University of Singapore,

Singapore, 117417. Email: yuziwei@u.nus.edu.

Xiaobo Shen is with School of Computer Science and Engineering, Nanjing
University of Science and Technology, China. Email: xbshen@njust.edu.cn.

time complexity for inference. This cubic complexity severely
limits the applicability of GP, especially in the era of big
data. It becomes more serious for multi-class classiﬁcation,
since we need to infer C latent functions for C classes. The
second is the intractable inference for the posterior p(f |y)
where f is the latent function values at data points. Due to
the commonly used non-Gaussian likelihoods p(y|f ), e.g., the
step likelihood, the (multinomial) probit/logit likelihoods and
the softmax likelihood, the Bayesian rule p(f |y) ∝ p(y|f )p(f )
incurs however intractable inference.

Inspired by the success of scalable GPR in recent years [8]–
[10], alternatively, we could address the two major issues in
GPC by regarding the classiﬁcation with discrete labels as a
regression task. For example, we could either directly treat
GPC as GPR, like [11]; or we interpret the class labels as the
outputs of a Dirichlet distribution to encourage the GPR-like
inference [12].

A more principled alternative is adopting the GPC frame-
work to handle binary/multi-class cases. To address the in-
ference issue, various approximate inference algorithms, e.g.,
laplace approximation (LA), expectation propagation (EP) and
variational
the core
inference (VI), have been developed,
of which is approximating the non-Gaussian p(f |y) with a
tractable Gaussian q(f ) [13].

As for the scalability issue, it has been extensively exploited
in the regime of GPR [14]. Particularly, the sparse approxi-
mations [9], [10], [15], [16] seek to distill the entire training
data through a global inducing set {Xm, u} comprising m
(m (cid:28) n) points, thus reducing the time complexity from
O(n3) to O(nm2). This is achieved either by modifying the
joint prior as p(f , f∗) ≈ q(f , f∗) where f∗ is the latent function
value at the test point x∗ [8]; or through directly approximating
the posterior p(f |y) ≈ q(f ) [9]. The time complexity can
be further reduced to O(m3) by recognizing an evidence
lower bound (ELBO) for log p(y). The ELBO factorizes over
data points [10], [17], [18], thus allowing efﬁcient stochastic
variational inference [19]. Moreover, by exploiting the speciﬁc
structures, e.g., the Kronecker and Toeplitz structures, in the
inducing set, the time complexity can be dramatically reduced
to O(n) [16], [20], [21].

Hence, scalable GPCs could inherit the sparse framework
from GPR, with the difﬁculty being that
the model evi-
dence or ELBO should be carefully built up in order to
overcome the intractable Gaussian integrals. To this end,
the fully independent training conditional (FITC) assumption
p(f |u) = (cid:81)n
i=1 p(fi|u) [15] is employed to build scalable

binary GPC [22].1 The scalability has been further improved
for binary/multi-class classiﬁcation through the stochastic vari-
ants [24], [25], which derive a closed-form ELBO factorized
over data points, thus supporting stochastic EP [26]. Differ-
ently, for binary classiﬁcation, variational inference is adopted
to derive a simple ELBO expressed as a one-dimensional
Gaussian integral which can be calculated through Gauss-
Hermite quadrature [27].2 This model has been further ex-
tended to multi-class classiﬁcation [28]. Particularly, when
using the logit likelihood, the P`olya-Gamma data augmenta-
tion [29] can be used such that the ELBO and the posterior
are analytical in the augmented probability space [30]. This
augmentation strategy has been recently extended to multi-
class classiﬁcation using logistic-softmax likelihood [31].3
Besides, a decoupled approach [32], [33] from the weight-
space view removes the coupling between the mean and the
covariance of a GP, resulting in lower complexity and an
expressive prediction mean.

Though showing high scalability for handling big data,
existing scalable GPCs derive analytical ELBO (i) using
additional assumptions which may limit the representational
capability [24], [25], and (ii) only for speciﬁc likelihoods, e.g.,
the step or probit likelihood [25], [27], [30], [31], [33].

Hence, this article proposes a unifying scalable GPC frame-
work which accommodates various likelihoods without addi-
tional assumptions. Speciﬁcally, by interpreting the GPC as
a noisy model analogous to GPR, we describe the step and
(multinomial) probit/logit likelihoods over a general Gaus-
sian error, and the softmax likelihood over a Gumbel error.
Thereafter, we augment the probability space for (i) the GPCs
using step and (multinomial) probit/logit likelihoods via the
internal variables; and particularly, (ii) the GPC using softmax
likelihood via the noises themselves. This leads to scalable
GPCs with analytical ELBO by using variational inference.
We empirically demonstrate the superiority of our GPCs on
extensive binary/multi-class classiﬁcation tasks with up to
two million data points. Python implementations built upon
the GPﬂow library [34] are available at https://github.com/
LiuHaiTao01/GPCnoise.

The reminder of this article is organized as follows. We
introduce the proposed scalable binary/multi-class GPCs with
additive noise in sections II and III, respectively. Thereafter,
section IV conducts numerical experiments to assess the
performance of proposed GPCs against state-of-the-art GPCs.
Finally, section V offers concluding remarks.

II. BINARY GPCS WITH ADDITIVE NOISE

A. Interpreting binary GPCs with additive noise

For the binary classiﬁcation with y ∈ {−1, 1}, the GPC

model in Fig. 1(a) is usually expressed as

f (x) ∼ GP(0, k(x, x(cid:48))),

p(y|f ) = π(yf ),

(1)

1For GPR, this assumption severally underestimates the noise variance and

worsens the prediction mean [23].

2Due to the fast Gauss-Hermite quadrature with high precision, this ELBO

is regarded as analytical.

3Different from the original softmax likelihood,

this hybrid likelihood

2

Fig. 1. (a) Illustration of the GPC where for binary case C = 1 and for multi-
class case C > 2. (b) The GPC with additive Gaussian noise (cid:15)c
i ∈ N (0, a).
This model is augmented by introducing the internal function g, which offers a
unifying description and helps derive closed-form ELBO. By marginalizing g
out and varying over a, we recover the GPC in (a) using step and (multinomial)
probit/logit likelihoods. (c) Particularly, for multi-class GPC using softmax
likelihood, instead of introducing g, we directly use the Gumbel error (cid:15)i to
augment the probability space for deriving analytical ELBO.

where the likelihood p(y|f ) employs an inverse link function
π(.) ∈ [0, 1] to squash the latent function f into the class prob-
ability space. Commonly used π(.) for binary GPC includes

step : π(z) = H(z),

(cid:90) z

probit : π(z) =

N (τ |0, 1)dτ,

−∞
logit : π(z) = (1 + exp(−z))−1,

(2)

where H(z) = 1 when z > 0; otherwise, H(z) = 0.

It is known that similar to the GPR y(x) = f (x) + (cid:15)(x),
the GPC in (1) can also be interpreted as a noisy model.
Speciﬁcally, we introduce a GPR-like internal latent function
g and the internal step likelihood, resulting in

g(x) = f (x) + (cid:15)(x),

p(y|g) = H(yg),

(3)

where (cid:15)(x) is an independent and identically distributed (i.i.d.)
noise which follows a distribution with probability density
function (PDF) φ(.) and cumulative distribution function
(CDF) Φ(.). The additional variable g in (3) augments the
probability space, and incurs the conditional independence
y ⊥ f |g, which is crucial for deriving the analytical ELBO
below.

The conventional likelihoods in (2) can be recovered by

marginalizing the internal g out as4

(cid:90)

p(y|f ) =

p(y|g)p(g|f )dg = Φ(yf ).

(4)

It is observed that (i) when (cid:15) follows the Dirac-delta distri-
bution φD((cid:15)) which is zero everywhere except at the origin
where it is inﬁnite, Eq. (4) recovers the step likelihood; (ii)
when (cid:15) follows the normal distribution φN ((cid:15)|0, 1), Eq. (4)
recovers the probit likelihood; and ﬁnally, (iii) when (cid:15) follows
the standard logistic distribution φL((cid:15)|0, 1), Eq. (4) recovers
the logit likelihood.

More interestingly, we could use a general Gaussian error
(cid:15) ∼ N (0, a) to describe the above three kinds of errors in
a unifying way, since all of them are symmetric, bell-shaped
distributions. It is observed that (i) when a → 0, the Gaussian
error degenerates to the Dirac-delta error;5 (ii) when a = 1,

4Eq. (4) holds only when (cid:15) follows a symmetric distribution, i.e., Φ(x) =
1 − Φ(−x). Otherwise, we have p(y = 1|f ) = 1 − Φ(−f ) and p(y =
−1|f ) = Φ(−f ).

5Directly using a = 0 will incur numerical issue for φN (.) and ΦN (.).

allows deriving analytical ELBO via a complex three-level augmentation.

But this issue can be sidestepped in the GPC model presented below.

the Gaussian error is equivalent to the normal error; and (iii)
when a = 2.897, the Gaussian error produces a well CDF
approximation to that of the logistic error with the maximum
difference as 0.009, see Fig. 5 in Appendix A. Bowling
and Khasawneh [35] proposed using the logistic function to
approximate the normal CDF ΦN . Inversely, we here use the
Gaussian error to approximate the logistic error. Note that the
optimal a for logistic error is derived in terms of CDF rather
than PDF since we focus on the approximation quality of the
likelihood (4).

Now the internal model g(x) = f (x) + N ((cid:15)|0, a) with
additive Gaussian noise, as depicted in Fig. 1(b), helps (i) de-
scribe the binary GPCs using different likelihoods uniformly;
and moreover, (ii) derive unifying and analytical ELBO and
prediction in section II-B.

B. Binary GPCs using step, probit and logit likelihoods

1) Evidence lower bound: Different from (3), we here
introduce a robust parameter δ like [36], resulting in the
likelihoods

p(y|g) = (1 − 2δ)H(yg) + δ,
(cid:19)

p(y|f ) = (1 − 2δ)ΦN

(cid:18) yf
√
a

+ δ.

(5)

The parameter δ, which is pretty small (e.g., 10−3), (i) prevents
the ELBO below from being inﬁnite, which is caused by the
logarithm form log p(y|g); and (ii) gives a degree of robustness
to outliers. Note that with δ, p(y|g) is still a distribution since
we have p(y|g) = {1 − δ, δ}.

Given the data {X, y} we have the joint prior p(f ) =
N (f |0, Knn) where f = (f1, · · · , fn)T and [Knn]ij =
k(xi, xj). Thereafter, we have the likelihoods p(g|f ) =
N (g|f , aI) and p(y|g) = (cid:81)n
i=1 p(yi|gi). To improve the
scalability for dealing with big data, we consider m induc-
ing variables u, which follow the same GP prior p(u) =
N (u|0, Kmm), for the latent variables f .6 In the statistical
model, a central task is deriving the posterior p(g, f , u|y) ∝
p(y|g)p(g|f )p(f |u) given the observations. It however is
intractable for GPC due to the non-Gaussian likelihood p(y|g).
Instead, we introduce a variational distribution q(g, f , u) =
p(g|f )p(f |u)q(u)7
exact posterior
p(g, f , u|y). We
their KL divergence
KL(q(g, f , u)||p(g, f , u|y)) = log p(y) − L, which is equiv-
alent to maximizing the evidence lower bound (ELBO) L
expressed as

to approximate
then minimize

the

(cid:28)

L =

log

(cid:29)

p(y, g, f , u)
q(g, f , u)

= (cid:104)log p(y|g)(cid:105)q(g) − KL(q(u)||p(u)),

q(g,f ,u)

(6)

where (cid:104).(cid:105)q(.) is the expectation over distribution q(.), and
q(g) = (cid:82) p(g|f )p(f |u)q(u)df du = (cid:82) p(g|f )q(f )df . For the
GPR-like internal model g(x) = f (x) + N ((cid:15)|0, a), we could

3

(7)

derive an analytical posterior q(g). Speciﬁcally, given that
q(u) = N (u|m, S), we have

(cid:90)

(cid:90)

q(f ) =

p(f |u)q(u)du = N (f |µf , Σf ),

q(g) =

p(g|f )q(f )df = N (g|µg, Σg),

mmm, Σf = Knn+KnmK−1

where µf = KnmK−1
mm−
I]Kmn, µg = µf and Σg = aI + Σf . Inserting (7) back
into (6), we obtain a closed-form ELBO factorized over data
points as

mm[SK−1

n
(cid:88)

(cid:20)

L =

log

(cid:19)

(cid:18) 1 − δ
δ

ΦN

(cid:19)

(cid:18) yiµfi
√

a + νfi

(cid:21)

+ log δ

(8)

i=1
− KL(q(u)||p(u)),

where µfi = [µf ]i and νfi = [Σf ]ii. The maximization of L
permits inferring the variational parameters and hyperparam-
eters simultaneously. Besides, the sum term in the right-hand
side of L allows using efﬁcient stochastic optimizer, e.g., the
Adam [37], for model training.8

It is observed from (8) that (i) when a = 0, we obtain the
bound L for the binary GPC using step likelihood; (i) when
a = 1, we obtain the binary GPC using probit likelihood;
and (i) when a = 2.897, we obtain the binary GPC using
logit likelihood. The unifying bound (8) reveals that the binary
GPCs using step, probit and logit likelihoods have similar
behavior.

f∗

for

The

at x∗

2) Prediction:

prediction
is
p(f∗|y) = (cid:82) p(f∗|f )q(f )df = N (f∗|µf∗ , νf∗ ), where µf∗ =
k∗mK−1
mm[SK−1
mmm, and νf∗ = k∗∗ + k∗mK−1
mm − I]km∗.
at x∗
Similarly, we obtain the prediction for g∗
as
p(g∗|y) = (cid:82) p(g∗|g)q(g)dg = N (g∗|µg∗ , νg∗ ), where
µg∗ = µf∗ and νg∗ = νf∗ + a. Finally, we have the
closed-form class probability for y∗ = 1 as

p(y∗ = 1|y) =

(cid:90)

p(y∗ = 1|g∗)p(g∗|y)df∗
(cid:18) µf∗√
(cid:19)

= (1 − 2δ)ΦN

a + νf∗

+ δ.

(9)

C. Discussions

[27]. Hensman et al.

We would like to emphasize that the binary GPCs pro-
posed in section II-B are different from that in section 3.2
[27] borrowed the bound Lg
of
for log p(g) in [10], which is equivalent
to maximizing
KL(q(f , u|g)||p(f , u|g)), and substituted it
into the aug-
mented joint distribution p(y, g, f ). This results in log p(y) ≥
log (cid:82) p(y|g) exp(Lg)dg = LHS. When using robust p(y|g),
we have

LHS =

log((1 − 2δ)ΦN (yiµfi) + δ) −

tr(Σf )

1
2

(10)

n
(cid:88)

i=1
− KL(q(u)||p(u)),

6u is assumed to be a sufﬁcient statistic for f , i.e., for any z we have

p(z|u, f ) = p(z|u).

7According to [9], we have p(z|u, y) = p(z|u). Hence, q(g, f , u) =

p(g, f |u, y)q(u) = p(g, f |u)q(u) = p(g|f )p(f |u)q(u).

8Particularly, the natural gradient descent (NGD) could be employed for
optimizing the variational parameters m and S. But in comparison to GPR,
the NGD+Adam optimizer brings little beneﬁts for classiﬁcation [33], [38].

4

(16)

(17)

(18)

which is different from our bound (8). Let Xm = X so
that u = f , LHS has a unique optimum where m∗ =
nn + I)−1 with α(m) being
Knn∂α(m)/∂m and S∗ = (K−1
the ﬁrst term in the right-hand side of LHS. The ineffective
estimations m∗ and S∗ are not due to the decoupling y⊥f |g.
They occur since LHS seeks to infer q(f |g) rather than the
interested q(f |y).

In contrast, our binary GPCs use VI to directly approximate
log p(y) via maximizing KL(q(g, f , u|y)||p(g, f , u|y). The
the internal variables g and the
improvement occurs that
general Gaussian error help derive completely analytical EL-
BOs for binary GPCs using step, probit and logit likelihoods.
Let Xm = X, our bound L in (8) has a unique optimum
where m∗ = Knn∂β(a, m, S)/∂m and S∗ = (K−1
nn − 2 ×
∂β(a, m, S)/∂S)−1 with β being the ﬁrst term in the right-
hand side of L. This optimum, which is similar to that of the
KL method rather than the Laplace approximation in [13], is
more informative than that of LHS.

III. MULTI-CLASS GPCS WITH ADDITIVE NOISE

A. Interpreting multi-class GPCs with additive noise

The more complicated multi-class GPC with the label y ∈
{1, · · · , C}, C > 2, is expressed by introducing the internals
{gc}C

c=1 for each of the C classes as

f c(x) ∼ GP(0, kc(x, x(cid:48))),
gc(x) = f c(x) + (cid:15)c(x),
gc(x),
y(x) = argmax

c
where f c and gc are independent latent functions, and (cid:15)c
is the i.i.d. noise for class c (1 ≤ c ≤ C). For nota-
tions, we ﬁrst deﬁne fi = (f 1(xi), · · · , f C(xi))T ∈ RC,
f c = (f c(x1), · · · , f c(xn))T ∈ Rn, f = (f 1, · · · , f C) ∈
Rn×C, gi = (g1(xi), · · · , gC(xi))T ∈ RC, gc =
(gc(x1), · · · , gc(xn))T ∈ Rn, and g = (g1, · · · , gC) ∈
Rn×C. We adopt the internal step likelihood

p(yi|gi) =

H(gyi

i − gc

i ) =

H(f yi

i + (cid:15)yi

i − f c

i − (cid:15)c

i ).

(cid:89)

c(cid:54)=yi

(cid:89)

c(cid:54)=yi

Thereafter, by integrating gi out, we obtain the conventional
likelihood9

p(yi|fi) =

p(yi|gi)p(gi|fi)dgi

(cid:90)

(cid:90)

=

φ((cid:15)i)

Φ((cid:15)i + f yi

i − f c

i )d(cid:15)i.

(cid:89)

c(cid:54)=yi

It is observed that (i) when (cid:15)i ∼ φD((cid:15)), Eq. (13) recovers the
multi-class step likelihood; (ii) when (cid:15)i ∼ φN ((cid:15)|0, 1), Eq. (13)
recovers the multinomial probit likelihood; (iii) when (cid:15)i ∼
φL((cid:15)|0, 1), Eq. (13) recovers the multinomial logit likelihood;
and ﬁnally, (iv) when using the Gumbel error (cid:15)i ∼ φG((cid:15)|0, 1),
Eq. (13) recovers the softmax likelihood, i.e.,

(12)

(13)

(14)

p(yi|fi) =

exp(f yi
i )
c=1 exp(f c
i )

.

(cid:80)C

9We omit the superscript in (cid:15)yi
i
i }c(cid:54)=yi .

on other error terms {(cid:15)c

since Eq. (13) has removed the dependency

Similarly, as depicted in Fig. 1(b), we can employ a Gaus-
sian error (cid:15) ∼ N (0, a) to describe the ﬁrst three symmetric
errors with a → 0, a = 1, and a = 2.897, respectively. But this
is not the case for the asymmetric Gumbel error, see Fig. 5 in
Appendix A. Hence, section III-B introduces the sparse multi-
class GPCs using step and multinomial probit/logit likelihoods
in a unifying framework; particularly, section III-D introduces
the sparse multi-class GPC using softmax likelihood.

B. Multi-class GPCs using step and multinomial probit/logit
likelihoods

1) Evidence lower bound: Similar to (5), we employ the

robust likelihoods as

(cid:18)

p(yi|gi) =

1 −

C
C − 1

δ

(cid:19) (cid:89)

c(cid:54)=yi

H(gyi

i − gc

i ) +

δ
C − 1

,

p(yi|fi) = (1 −

δ)S0 +

C
C − 1

δ
C − 1

,

(15)
where, given that p(gi|fi) = N (gi|fi, aI), S0 =
(cid:16) (cid:15)i+f yi
i −f c
E(cid:15)i∼N (0,a)
√
a
Again, by introducing the independent inducing set uc for

(cid:104)(cid:81)

ΦN

c(cid:54)=yi

(cid:17)(cid:105)

.

i

f c, 1 ≤ c ≤ C, the ELBO writes

(cid:28)

L =

log

(cid:29)

p(y, g, f , u)
q(g, f , u)

(11)

= (cid:104)log p(y|g)(cid:105)q(g) −

KL(q(uc)||p(uc)).

Let q(uc) = N (uc|mc, Sc), we have

(cid:90)

(cid:90)

q(f ) =

p(f |u)q(u)du =

N (f c|µc

f , Σc

f ),

q(g) =

p(g|f )q(f )df =

N (gc|µc

g, Σc

g),

q(g,f ,u)

C
(cid:88)

c=1

C
(cid:89)

c=1
C
(cid:89)

c=1

where µc
Kc
nm(Kc
aI + Σc
ELBO as

f = Kc
nn +
mm)−1[Sc(Kc
g =
f . Inserting (17) back into (16), we have a factorized

mm)−1mc, Σc
mn, µc

nm(Kc
mm)−1 − I]Kc

f = Kc
g = µc

f and Σc

n
(cid:88)

(cid:20)
log(1 − δ)Si + log

(cid:18) δ

L =

(cid:19)

(cid:21)
(1 − Si)

C − 1

i=1

−

C
(cid:88)

c=1

KL(q(uc)||p(uc)).

(cid:20)

(cid:81)

(cid:19)(cid:21)

(cid:18) gyi
i −µc
√
fi
a+νc
fi

c(cid:54)=yi

ΦN

,a+νyi
fi

i ∼N (µyi
gyi
fi

where Si = E
)
term Si is interpreted as the probability that the function value
corresponding to the observed class yi is larger than the others
at xi. Note that this one dimensional Gaussian integral can
be evaluated using fast Gauss-Hermite quadrature with high
precision.

. The

The unifying ELBO (18) herein describes the multi-class
GPCs using step likelihood (a = 0), multinomial probit like-
lihood (a = 1) and multinomial logit likelihood (a = 2.897).
Note that when a = 0, the model is equivalent to the one
presented in [36].

5

f∗

for

the

Finally,

prediction

2) Prediction:
(f 1
∗ , · · · , f C
(cid:81)C
c=1 N (f c
and νc
f∗
Similarly, we have
p(g∗|y) = (cid:82) p(g∗|g)q(g)dg = (cid:81)C
and νc
where µc
g∗
g∗

=
∗ )T at x∗ is p(f∗|y) = (cid:82) p(f∗|f )q(f )df =
mm)−1mc
∗ |µc
f∗
= kc
m∗.
as
),

), where µc
= kc
f∗
∗m(Kc
mm)−1[Sc(Kc
the prediction for g∗

at x∗
∗|µc
g∗
+ a. Finally, we have

∗m(Kc
mm)−1 − I]kc

, νc
f∗
∗∗ + kc

c=1 N (gc

= µc
f∗

= νc
f∗

, νc
g∗

By introducing the inducing set uc for f c, 1 ≤ c ≤ C, we

derive the ELBO as

(cid:28)

L =

log

(cid:29)

p(y, f , u, (cid:15))
q(f , u, (cid:15))

q(f ,u,(cid:15))

(cid:104)log p(yi|fi, (cid:15)i)(cid:105)q(fi)q((cid:15)i|fi) −

KL(q((cid:15)i|fi)||p((cid:15)i))

n
(cid:88)

i=1

(cid:90)

p(y∗|y) =

p(y∗|g∗)p(g∗|y)df∗

KL(q(uc|y)||p(uc)),

= (1 − δ)S∗ +

(1 − S∗),

δ
C − 1

(cid:20)

(cid:81)

where S∗ = Egy∗

∗ ∼N (µy∗
f∗

,a+νy∗
f∗

)

ΦN

c(cid:54)=y∗

(cid:18) gy∗
∗ −µc
√
f∗
a+νc
f∗

(cid:19)(cid:21)

.

(19)

(22)
where q(f , u, (cid:15)) = p(f |u)q(u)q((cid:15)|f ); q(fi) = (cid:81)C
i ) =
(cid:81)C
) with µc
f ]ii; and
fi
q((cid:15)i|fi) approximates the posterior p((cid:15)i|yi, fi), which follows
the exact expression as

c=1 q(f c
= [Σc

f ]i and νc
fi

c=1 N (f c

i |µc
fi

= [µc

, νc
fi

n
(cid:88)

=

i

−

C
(cid:88)

c=1

C. A possible generalization?

It is found that the unifying ELBOs in (8) and (18) re-
spectively recover the binary and multi-class GPCs using step
and (multinomial) probit/logit
likelihoods by varying over
the Gaussian noise variance a. It indicates that the GPCs
using these three symmetric likelihoods would produce similar
results, which will be veriﬁed in the numerical experiments in
section IV.

Besides, the parameter a inspires us to think of a more gen-
eral GPC model, like standard GPR, by treating the variance
a as a hyperparameter and inferring it from data. However, as
discussed in Appendix B, the ELBOs in (8) and (18) increase
with decreasing a. That means, L arrives at the maximum with
a = 0. This is because the internal likelihood p(y|g) does not
take into account the noise variance.

We name the GPCs using step and (multinomial) probit/logit
likelihoods as GPC-I, GPC-II and GPC-III, respectively. It is
found that given the same conﬁguration of hyperparameters,
the ELBO satisﬁes LI > LII > LIII, which again veriﬁes that
the noise variance a is not a proper hyperparameter.

D. Multi-class GPC using softmax likelihood

1) Evidence lower bound: According to (13), the softmax
likelihood used in the noisy multi-class GPC is equivalent to

p(yi|fi) =

φG((cid:15)i)

ΦG((cid:15)i + f yi

i − f c

i )d(cid:15)i,

(20)

(cid:90)

(cid:89)

c(cid:54)=yi

where for the standard Gumbel error, the PDF is φG(x) =
exp(−x − e−x) and the CDF is ΦG(x) = exp(−e−x). Instead
of introducing the internal g, we here directly use the noise
variables (cid:15) = {(cid:15)i}n
i=1 to augment the probability space due to
the expectation form in (20), as depicted in Fig. 1(c). Hence,
the augmented model with (cid:15)i has the following conditional
distributions as

p(yi, (cid:15)i|fi) = φG((cid:15)i)

(cid:89)

ΦG((cid:15)i + f yi

i − f c

i ),

p(yi|(cid:15)i, fi) =

(cid:89)

c(cid:54)=yi

c(cid:54)=yi
ΦG((cid:15)i + f yi

i − f c

i ).

(21)

p((cid:15)i|yi, fi) ∝ p(yi|fi, (cid:15)i)p((cid:15)i)

= φG((cid:15)i)

ΦG((cid:15)i + f yi

i − f c
i )

(cid:89)

c(cid:54)=yi





= exp

−(cid:15)i −

1 +


 e−(cid:15)i





(23)

(cid:88)

i −f yi

i

ef c

c(cid:54)=yi

c(cid:54)=yi

ef c

i = 1 + (cid:80)

i , 1) ,
i = (cid:80)C

c= Gumbel ((cid:15)i| log θ∗
i −f yi
where θ∗
. Note that
though the optimal p((cid:15)i|yi, fi) has an analytic form, directly
using p((cid:15)i|yi, fi) will make the ELBO intractable. Hence, we
adopt a general distribution q((cid:15)i|fi) = Gumbel((cid:15)i| log θi, 1)
which satisﬁes θi > 1 and already contains the optimal
distribution.

c=1 ef c

i −f yi

i

Thereafter, the closed-form ELBO, which is detailed in

Appendix C, is reorganized as,

L =

−

Pi − log θi −

+ 1

−

KL(q(uc|y)||p(uc)),

n
(cid:88)

(cid:26)

i=1

1
θi

(cid:27)

C
(cid:88)

c=1

1
θi

(cid:19)

(24)

where Pi = exp

(cid:18) νyi
fi

2 − µyi

fi

(cid:80)

exp

c(cid:54)=yi

(cid:16) νc
fi

2 + µc
fi

(cid:17)

. Fur-

thermore, in order to obtain a tight bound, let the derivative
of L w.r.t. θi

∂L
∂θi
to be zero, we have the optimal value θ∗
θ∗
i into L, we have

(Pi + 1) −

1
θ2
i

=

1
θi

i = Pi+1. Substituting

n
(cid:88)

C
(cid:88)

L = −

log(Pi + 1) −

KL(q(uc|y)||p(uc)).

(25)

i=1

c=1
We here name the GPC using softmax likelihood as GPCsm.
2) Prediction: To predict the latent function values f∗ at
the test point x∗, we substitute the approximate posteriors
into the predictive distribution p(f∗|y) = (cid:82) p(f∗|u)q(u)du =
(cid:81)C
). Thereafter, the distribution of the test

c=1 N (f c

, νc
f∗
label is computed as

∗ |µc
f∗

p(y∗|y) =

p(y∗|f∗)p(f∗|y)df∗

(cid:90)

(cid:90)

=

exp(f y∗
∗ )
c=1 exp(f c
∗ )

C
(cid:89)

c=1

(cid:80)C

N (f c

∗ |µc
f∗

, νc
f∗

)df∗.

(26)

The resulting integral however is intractable. Simply, we
estimate the mean prediction p(y∗|y) via markov chain monte
carlo (MCMC) by drawing samples from the Gaussian p(f∗|y).
Due to the independent latent functions, we draw samples from
the posterior p(f∗|y) as follows: let t ∼ N (0, I), we have the
νf∗ ◦ t where the symbol ◦ represents
sample x(cid:5) = µf∗ +
the point-wise product.

√

IV. NUMERICAL EXPERIMENTS

of

the

the

[25]

This

section

veriﬁes

including

(i)
[24],

performance

classiﬁcation

binary/multi-class
them against

state-of-the-art
binary/multi-class
(R codes

the
proposed GPCs (GPC-I, GPC-II, GPC-III and GPCsm)
tasks.
on multiple
scalable
We
compare
GPCs
GPC
at
using EP (GPCep)
http://proceedings.mlr.press/v51/hernandez-lobato16.html and
http://proceedings.mlr.press/v70/villacampa-calvo17a.html),
the binary/multi-class GPC using data augmentation
(ii)
at https:
(GPCaug)
//github.com/theogf/AugmentedGaussianProcesses.jl),
and
(iii) the binary/multi-class GPC using orthogonally decoupled
basis
available
[33]
at https://github.com/hughsalimbeni/orth decoupled var gps).
We run the experiments on a Linux workstation with eight
3.20 GHz cores, nvidia GTX1080Ti, and 32GB memory.

(Python codes

functions

available

available

(ORTH)

codes

(Julia

[30],

[31]

Table I summarizes the capabilities of existing scalable
GPCs for handling various likelihoods. Speciﬁcally, the GP-
Cep and ORTH employ the probit likelihood for binary case
and the step likelihood for multi-class case; and the GPCaug
uses the logit likelihood for binary case and the logistic-
softmax likelihood10 for multi-class case.

TABLE I
THE CAPABILITIES OF EXISTING SCALABLE GPCS FOR HANDLING
VARIOUS LIKELIHOODS. THE SYMBOL “b” REPRESENTS BINARY CASE
AND “m” REPRESENTS MULTI-CLASS CASE. NOTE THAT THE GPCaug
EMPLOYS A LOGISTIC-SOFTMAX LIKELIHOOD FOR MULTI-CLASS CASE.

GPCep [24], [25]
GPCaug [30], [31]
ORTH [33]
Ours

Step lik.
(cid:51)(m)
(cid:55)
(cid:51)(m)
(cid:51)(b, m)

Probit lik.
(cid:51)(b)
(cid:55)
(cid:51)(b)
(cid:51)(b, m)

Logit lik.
(cid:55)
(cid:51)(b)
(cid:55)
(cid:51)(b, m)

Softmax lik.
(cid:55)
(cid:51)(m)
(cid:55)
(cid:51)(m)

A. Comparative results

1) Toy examples: We showcase the proposed GPCs on two
illustrative binary and multi-class cases. The binary case is
the banana dataset used in [27]; the synthetic three-class case
samples three latent functions from a GP prior, and applies the
rule y(x) = argmaxcf c(x) to them. For the two classiﬁcation
datasets, we use m = 32 and the Mat´ern32 kernel for GPCs.
The models are trained using the Adam optimizer [37] over
5000 iterations.

As shown in Fig. 2, it is observed that (i) the proposed GPCs
well classify the two datasets; and (ii) the variational inference
framework pushes the optimized inducing points towards the

10Similar to the softmax (14), this likelihood expresses as p(yi|fi) =
i )/ (cid:80)C
i ) with σ(z) = (1 + exp(−z))−1.

c=1 σ(f c

σ(f yi

6

Illustration of the proposed GPCs on a binary case (top row) and
Fig. 2.
a three-class case (bottom row). The black curves represent the contours at
which the predicted class probability is 0.5. The black circles represent the
optimized locations of inducing points.

decision boundaries, which is similar to [27] but different
from [24], [25].11

2) UCI benchmarks: Similar to [33], we conduct the com-
parison on 19 UCI datasets with n ∈ [4897, 130064], d ∈
[7, 51], and C ∈ [2, 26]. The model conﬁgurations for com-
parison are detailed in Appendix D. Table II summarizes the
average classiﬁcation accuracy (acc) and negative log likeli-
hood (nll) results of scalable GPCs on the UCI benchmarks.
The complete results on each dataset are provided in Tables III
and IV in Appendix E. Note that for the fully-connected neural
networks using Selu activations, we use the results reported
in [39]. For ORTH, we use the results reported in [33].12

It is observed that for binary benchmarks, the proposed
three GPCs using different likelihoods outperform the others,
and they are competitive in terms of average acc. Besides,
compared to the GPC-I using step likelihood, GPC-II and
GPC-III have a slightly better performance in terms of nll. As
for multi-class benchmarks, all the proposed GPCs outperform
the competitors, and particularly, the GPCsm using softmax
likelihood provides remarkable performance in terms of nll.

As for GPCep [24], [25], in order to have scalable and
analytical ELBO, it employs (i) the FITC assumption p(f |u) =
(cid:81)n
i=1 p(fi|u), and (ii) an approximation to the integral in (6)
of [25]. It is found that EP has a better approximation than
VI for standard GPC [13]. The GPCep in our experiments
indeed is very competitive, especially in terms of nll and con-
vergence (see for example Fig. 4(b)-(d)), in comparison to the
proposed GPCs for almost all the datasets except mushroom,
nursery and wine-quality-white. The capability of
GPCep may be limited, because (i) the additional assumption
and approximation may worsen the prediction [23]; and (ii)
the R package sometimes is unstable, e.g., it fails in 4 out of
the 10 runs on the mushroom dataset.

The ORTH has acceptable acc results but provides relatively
poor nll results in comparison to GPCep. Finally, the GPCaug
exhibits competitive performance on the binary benchmarks.
But the multi-class results of GPCaug are not attractive. For

11The FITC assumption in [24], [25] incurs overlapped inducing points.

This has also been observed in regression tasks [23].

12 [33] employs the Mat´ern52+RBF kernel for ORTH. To be fair, the
proposed GPCs use the same combination kernel and produce the results
in Table V in Appendix E. It is found that the proposed scalable GPCs
outperform ORTH.

TABLE II
AVERAGE CLASSIFICATION ACCURACY (ACC) AND NEGATIVE LOG LIKELIHOOD (NLL) RESULTS ON UCI BENCHMARKS. THE VALUE OF GPCep IN THE
BRACKETS INDICATES THE AVERAGE RESULTS FOR ALL THE MULTI-CLASS UCI DATASETS EXCEPT N U R S E R Y. THE VALUE OF GPCaug IN THE
BRACKETS INDICATES THE AVERAGE RESULTS FOR ALL THE MULTI-CLASS UCI DATASETS EXCEPT W A V E F O R M-N O I S E.

binary

multi-class

Selu
92.6
NA
91.1
NA

ORTH
93.0
0.1639
89.2
0.5976

GPCep
94.0
0.1506
87.1 (90.4)
1.5637 (0.3168)

GPCaug
94.5
0.1354
83.0 (87.5)
0.3844 (0.3196)

GPC-I
95.1
0.1355
93.0
0.2374

GPC-II
95.1
0.1349
93.0
0.2331

GPC-III
95.1
0.1349
93.0
0.2353

GPCsm
NA
NA
92.6
0.1858

acc
nll
acc
nll

7

inﬁnite; and plays the role of noise, like GPR, to be robust to
outliers [40]. A ﬁxed δ, e.g., 10−3 is suggested by [28], [36]
for classiﬁcation. But we would like to argue that a ﬁxed δ will
worsen the predictive distribution, especially the predictive
variance.

dataset adult and

To verify this, we run the proposed GPCs on the
binary
dataset
waveform-noise using a ﬁxed δ and a free δ, respectively,
in Fig. 4(a)-(d). Particularly, Fig. 4(e) and (f) showcase the
estimations of latent predictive mean {µfi}n
i=1 and variance
{νfi}n
i=1 in GPC-I using ﬁxed or optimized δ on the adult
dataset.

the multi-class

example, it fails on the waveform-noise dataset. Besides,
the GPCaug sometimes suffers from slow convergence, see
for example Fig. 4(c) and (d). This may be caused by the
coordinate ascent optimizer deployed in the GPCaug package.
Finally, the proposed GPCs are much more efﬁcient than
GPCaug and GPCep due to the ﬂexible tensorﬂow framework
with GPU acceleration. For instance,
the proposed GPCs
require around 5 minutes to run on the miniboone dataset;
while GPCep requires 7.9 hours and GPCaug requires 2.3
hours.

3) The airline dataset: We ﬁnally assess the perfor-
mance of the GPCs on the large-scale airline dataset
containing the information of USA ﬂights between January
and April of 2008 [27]. The dataset has eight inputs including
age of the aircraft, distance covered, airtime, departure time,
arrival time, day of the week, day of the month and month.
According to [25], we treat the dataset as a classiﬁcation task
to predict the ﬂight delay with three statuses: on time, more
than 5 minutes of delay, or more than 5 minutes before time.
We partition the dataset into 2M training points and 10000
test points. We employ the RBF kernel, and use m = 200 for
the proposed GPCs, GPCep and GPCaug, and β = 200 and
γ = 500 for ORTH; we run the Adam optimizer over 100000
iterations using a mini-batch size of 1024 and the learning rate
of 0.01.

As shown in Fig. 3, the proposed GPCs, especially GPCsm,
converge with superior results in terms of both acc and nll.
The ORTH provides reasonable acc results, but suffers from
poor nll results. In contrast, the GPCep converges with the
second best nll results. Finally, the GPCaug has a stable but
relatively poor performance on this dataset.

Fig. 4. The accuracy (acc) and negative log likelihood (nll) of scalable GPCs
on the multi-class waveform-noise dataset (top) and the binary adult
dataset (medium); the bottom is the {µfi }n
i=1 estimated by
GPC-I with ﬁxed or optimized δ on the adult dataset. Note that the poor
results of GPCaug for waveform-noise are not included.

i=1 and {νfi }n

We observe that the ﬁxed δ brings a poor nll performance,
it even increases during the training on the adult
e.g.,
dataset. This phenomenon has also been observed in [25].
Taking the binary GPCs for example, we ﬁnd that they have
an optimum

δ∗ = 1 −

ΦN

1
n

n
(cid:88)

i=1

(cid:19)

(cid:18) yiµfi
√

a + νfi

(27)

Fig. 3. Comparative results of scalable GPCs on the large-scale airline
dataset.

B. Discussions of proposed GPCs

1) Can we use a ﬁxed δ?: The parameter δ is originally
introduced in the robust step likelihoods (5) and (15). It
prevents the logarithm in ELBOs (8) and (18) from being

for the bound (8). When using a ﬁxed δ, the model has to
over-/under-estimate µfi and νfi in order to maximize L, see
Fig. 4(e) and (f). These ineffective estimations, especially the
overestimated νfi , bring poor nll results. Hence, we treat δ > 0
as a free hyperparameter and infer it from data in order to (i)

relieve this issue, see Fig. 4(b) and (d); and (ii) keep the sum
formulation in (8), which is crucial for stochastic optimization.
2) Pros & cons of proposed GPCs: From the foregoing

classiﬁcation results, we have the following ﬁndings:

For the skewed Gumbel error, which relates to the speciﬁc
softmax likelihood for multi-class GPC, we should however
tackle it using another strategy, which is detailed in sec-
tion III-D.

8

• It is found that the bounds (8) and (18) for the proposed
GPCs using these three likelihoods (step, probit and
logit) are expressed uniformly. Hence, the symmetric and
bell-shaped error distributions help GPC-I, GPC-II and
GPC-III perform similarly. Besides, increasing the noise
variance a in Gaussian error (step→probit→logit) brings
softer likelihood and slightly better nll results;

• The introduction of δ helps derive analytical ELBOs
for GPC-I, GPC-II and GPC-III. And compared to the
usage of a ﬁxed δ, optimizing δ signiﬁcantly improves
the nll results. However, it is found that the GPCs using
δ have worse nll results than the counterparts on some
binary/multi-class datasets, see for example Fig. 4;

• Different from GPC-I, GPC-II and GPC-III, the asym-
metric and varying Gumbel errors {(cid:15)i}n
i=1 help GPCsm
describe the heteroscedasticity, thus outperforming the
others in terms of nll. But the GPCsm may suffer from
slow convergence in the early stage due to the compli-
cated model structure, see for example Fig. 4(b).

V. CONCLUSION

This work presents a novel unifying framework which
derives analytical ELBO for scalable GPCs using various
likelihoods. This is achieved by introducing additive noises to
interpret the GPCs in an augmented probability space through
the internal variables g or directly the noises themselves. The
superiority of our GPCs has been empirically demonstrated on
extensive binary/multi-class classiﬁcation tasks against state-
of-the-art scalable GPCs.

ACKNOWLEDGMENTS

This work was conducted within the Rolls-Royce@NTU
Corporate Lab with support from the National Research
Foundation (NRF) Singapore under the Corp Lab@University
Scheme. It is also partially supported by the Data Science
and Artiﬁcial Intelligence Research Center (DSAIR) and the
School of Computer Science and Engineering at Nanyang
Technological University.

APPENDIX A
THE GAUSSIAN APPROXIMATION TO VARIOUS ERROR
DISTRIBUTIONS

As discussed before, the step and (multinomial) probit/logit
likelihoods can be recovered by varying over different error
distributions in the GPC models (3) and (11). Particularly,
for the symmetric Dirac-delta, normal and logistic errors,
since they are similar, we can describe them using a uni-
fying Gaussian error N ((cid:15)|0, a), resulting in a unifying GPC
framework. As illustrated in Fig. 5, the Dirac-delta error can
be approximated by letting a → 0; the logistic error can
be approximated by having a = 2.897, which is derived by
minimizing their maximal CDF difference.

Fig. 5. The error distributions related to various GPC likelihoods. The top
row shows the PDF of error distributions, whereas the bottom row shows
the CDF. The dash curves represent the Gaussian approximation to the error
distributions.

APPENDIX B
TREATING THE NOISE VARIANCE a AS A
HYPERPARAMETER?

For the binary GPC with additive noise in section II-B,
by treating the noise variance a ≥ 0 as hyperparameters, we
obtain the derivative of L in (8) w.r.t a as

∂L
∂a

= −

log

1
2

(cid:18) 1 − δ
δ

(cid:34)

(cid:19) n
(cid:88)

i=1

φN

(cid:18) yiµfi
√

a + νfi

(cid:19) yiµfi

(a + νfi ) 3

2

(cid:35)

.

It is found that µfi represents the prediction mean at xi. When
the binary GPC provides sensible predictions, we have yiµfi >
0. Hence, we have ∂L/∂a < 0, which means that the ELBO
L arrives at the maximum when a = 0.

Similarly, for the multi-class GPC with additive noise in
section III-B, we obtain the derivative of L in (18) w.r.t a as

∂L
∂a

= log

(C − 1)

(cid:18) 1 − δ
δ

(cid:19) n
(cid:88)

i=1

∂Si
∂a

,

φN

(cid:33)

(cid:32) gyi

i − µc
fi
(cid:112)a + νc
fi

where

∂Si
∂a

= −

1
2

E

gyi
i





(cid:88)

c(cid:54)=yi


2

gyi
i − µc
fi
) 3
(a + νc
fi


i − µc(cid:48)
gyi
fi
(cid:113)
a + νc(cid:48)
fi
(cid:33)





(cid:32)

(cid:89)

c(cid:48)(cid:54)=yi,c

ΦN



∂φN

(cid:90)

+

i −µyi
gyi
fi
(cid:113)
a+νyi
fi

∂a

(cid:89)

c(cid:54)=yi

ΦN

(cid:33)

(cid:32) gyi

i − µc
fi
(cid:112)a + νc
fi

dgyi
i .

For the ﬁrst term in the right-hand side of ∂Si/∂a, since
ΦN (x) > ΦN (−x) and φN (x) = φN (−x) for x > 0, we

9

know that this is a negative term. For the second term in the
right-hand side of ∂Si/∂a, it satisﬁes

For the KL divergence (cid:80)C

c=1 KL(q(uc|y)||p(uc)) we have

∂φN

(cid:32)

(cid:33)

i −µyi
gyi
fi
(cid:113)
a+νyi
fi

(cid:90)

<

dgyi
i

∂a

(cid:34)

= −

1
2

E

gyi
i

1
a + νyi
fi

−

i − µyi
(gyi
)2
fi
(a + νyi
)2
fi

(cid:35)

= 0.

Hence, we again have ∂Si/∂a < 0 and furthermore ∂L/∂a <
0, which means that the ELBO L arrives at the maximum
when a = 0.

The above analysis reveals that a cannot play a role of

hyperparameter.

APPENDIX C
THE ELBO FOR MULTI-CLASS GPC USING SOFTMAX
LIKELIHOOD

The original ELBO of multi-class GPC using softmax

likelihood in section III-D is

L =

(cid:104)log p(yi|fi, (cid:15)i)(cid:105)q((cid:15)i|fi)q(fi) −

KL(q((cid:15)i|fi)||p((cid:15)i))

n
(cid:88)

i=1

n
(cid:88)

i

−

C
(cid:88)

c=1

KL(q(uc|y)||p(uc)).

For the ﬁrst double-expectation term in the right-hand side of
L, we ﬁrst calculate the inner expectation as

(cid:104)log p(yi|fi, (cid:15)i)(cid:105)q((cid:15)i|fi)
(cid:88)

(cid:90) +∞

q((cid:15)i|fi) log ΦG((cid:15)i + f yi

i − f c

i )d(cid:15)i

e(−((cid:15)i−log θi)−e−((cid:15)i−log θi))e−((cid:15)i+f yi

i −f c

i )d(cid:15)i

=

c(cid:54)=yi

= −

−∞
(cid:90) +∞

−∞

(cid:88)

c(cid:54)=yi

(cid:88)

c(cid:54)=yi

u=e−(cid:15)i

= −

θief c

i −f yi

i

ue−θudu

(cid:90) +∞

0

(1 + θu)e−θu
θ2

+∞

(cid:12)
(cid:12)
(cid:12)
(cid:12)

0

(cid:88)

=

θief c

i −f yi

i

c(cid:54)=yi
1
θi

= −

(cid:88)

i −f yi

i

ef c

.

c(cid:54)=yi

Then, the outside expectation is analytically expressed as

(cid:104)log p(yi|fi, (cid:15)i)(cid:105)q((cid:15)i|fi)q(fi)
i )df yi

i q(f yi

e−f yi

(cid:90)

i

= −

(cid:90)

(cid:88)

ef c

i q(f c

i )df c
i

= −

exp

(cid:18) νc
fi
2

− µc
fi

c(cid:54)=yi
(cid:19) (cid:88)

c(cid:54)=yi

exp

(cid:18) νc
fi
2

(cid:19)

.

+ µc
fi

1
θi

1
θi

For the KL divergence (cid:80)n
hand side of L, we have

i=1 KL(q((cid:15)i|fi)||p((cid:15)i)) in the right-

n
(cid:88)

i=1

KL(q((cid:15)i|fi)||p((cid:15)i)) =

log θi +

− 1

.

n
(cid:88)

(cid:18)

i=1

(cid:19)

1
θi

:=

1
2

C
(cid:88)

(cid:18)

log

c=1

+(mc)T(Kc

|Kc

mm|
|Sc|
mm)−1mc(cid:1) .

− m + tr[(Kc

mm)−1Sc]

According to the above computations, the ELBO is reorga-
nized as

L =

−

Pi − log θi −

+ 1

−

KL(q(uc|y)||p(uc)),

n
(cid:88)

(cid:26)

i=1

1
θi

(cid:27)

C
(cid:88)

c=1

1
θi

(cid:19)

(cid:18) νyi
fi

(cid:16) νc
fi

(cid:17)

(cid:80)

fi

2 − µyi

with Pi = exp
double-sum in the right-hand side of L over data points and
classes implies that we could obtain an unbiased estimation
of L via a subset of both data points and classes, which is
potential for large categorical cases [41].

. The

2 + µc
fi

exp

c(cid:54)=yi

APPENDIX D
DETAILS FOR UCI EXPERIMENTS

For the experiments on 19 UCI classiﬁcation benchmarks,
similar to [33], we employ the experimental settings detailed
as follows.

As for data preprocessing, we use the package Bayesian
Benchmarks13 which normalizes the inputs of datasets to
have zero mean and unit variance along each dimension, and
randomly chooses 10% of the data as the test set.

√

√

As for the kernel function, the results of ORTH in [33]
employ a combination of the Mat´ern52 kernel with length-
d and variance of 5.0, and the RBF kernel with
scale of 0.1
d and variance of 5.0. But for GPCep and
length-scale of
GPCaug, since their packages either only support the RBF
kernel or have no kernel combination module, we adopt the
RBF kernel with length-scale of 0.1
d and variance of 5.0
in the comparison. The proposed GPCs also use the RBF
kernel in Tables III and IV in Appendix E. Additionally, to
compare with ORTH, Table V in Appendix E offers the results
of proposed GPCs using the Mat´ern52+RBF kernel.

√

As for optimization, we use m = 300 inducing points
initialized through the k-means technique, and a mini-batch
size of 1024. We employ the Adam optimizer for all the
scalable GPCs except GPCaug,14 and run it over 10000
iterations with a learning rate of 0.01. We here do not employ
the natural gradient descent (NGD) strategy for optimizing the
variational parameters. Because in comparison to the regres-
sion task, the NGD+Adam optimizer brings little beneﬁts for
classiﬁcation [33], [38].

Finally, as for likelihoods, the ORTH and GPCep employ
the probit likelihood for binary case and the step likelihood for
multi-class case; the GPCaug adopts the logit likelihood for
binary case and the logistic-softmax likelihood for multi-class
case; and our GPCs have no limit and run with all likelihoods.

13https://github.com/hughsalimbeni/bayesian benchmarks.
14The Julia package of GPCaug employs a coordinate ascent optimizer.

10

[22] A. Naish-Guzman and S. Holden, “The generalized FITC approxima-
tion,” in Advances in Neural Information Processing Systems, 2008, pp.
1057–1064.

[23] M. Bauer, M. van der Wilk, and C. E. Rasmussen, “Understanding
probabilistic sparse Gaussian process approximations,” in Advances in
Neural Information Processing Systems, 2016, pp. 1533–1541.

[24] D. Hern´andez-Lobato and J. M. Hern´andez-Lobato, “Scalable Gaussian
process classiﬁcation via expectation propagation,” in Artiﬁcial Intelli-
gence and Statistics, 2016, pp. 168–176.

[25] C. Villacampa-Calvo and D. Hern´andez-Lobato, “Scalable multi-class
Gaussian process classiﬁcation using expectation propagation,” in In-
ternational Conference on Machine Learning.
JMLR. org, 2017, pp.
3550–3559.

[26] Y. Li, J. M. Hern´andez-Lobato, and R. E. Turner, “Stochastic expectation
propagation,” in Advances in Neural Information Processing Systems,
2015, pp. 2323–2331.

[27] J. Hensman, A. Matthews, and Z. Ghahramani, “Scalable variational
gaussian process classiﬁcation,” in Artiﬁcial Intelligence and Statistics,
2015, pp. 351–360.

[28] J. Hensman, A. G. Matthews, M. Filippone, and Z. Ghahramani,
“MCMC for variationally sparse Gaussian processes,” in Advances in
Neural Information Processing Systems, 2015, pp. 1648–1656.

[29] N. G. Polson, J. G. Scott, and J. Windle, “Bayesian inference for logistic
models using p´olya–Gamma latent variables,” Journal of the American
Statistical Association, vol. 108, no. 504, pp. 1339–1349, 2013.
[30] F. Wenzel, T. Galy-Fajou, C. Donner, M. Kloft, and M. Opper, “Efﬁcient
Gaussian process classiﬁcation using p`olya-gamma data augmentation,”
in International Conference on Machine Learning, 2018.

[31] T. Galy-Fajou, F. Wenzel, C. Donner, and M. Opper, “Multi-class gaus-
sian process classiﬁcation made conjugate: Efﬁcient inference via data
augmentation,” in International Conference on Artiﬁcial Intelligence and
Statistics, 2019.

[32] C.-A. Cheng and B. Boots, “Variational inference for Gaussian process
models with linear complexity,” in Advances in Neural Information
Processing Systems, 2017, pp. 5184–5194.

[33] H. Salimbeni, C.-A. Cheng, B. Boots, and M. Deisenroth, “Orthogo-
nally decoupled variational Gaussian processes,” in Advances in Neural
Information Processing Systems, 2018, pp. 8711–8720.

[34] D. G. Matthews, G. Alexander, M. Van Der Wilk, T. Nickson, K. Fujii,
A. Boukouvalas, P. Le´on-Villagr´a, Z. Ghahramani, and J. Hensman,
“GPﬂow: A Gaussian process library using tensorﬂow,” Journal of
Machine Learning Research, vol. 18, no. 1, pp. 1299–1304, 2017.
[35] S. R. Bowling, M. T. Khasawneh, S. Kaewkuekool, and B. R. Cho, “A
logistic approximation to the cumulative normal distribution,” Journal
of Industrial Engineering and Management, vol. 2, no. 1, 2009.
[36] A. G. de Garis Matthews, “Scalable Gaussian process inference using
variational methods,” Department of Engineering, University of Cam-
bridge, 2016.

[37] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014.

[38] H. Salimbeni, S. Eleftheriadis, and J. Hensman, “Natural gradients
in practice: Non-conjugate variational inference in Gaussian process
models,” in International Conference on Artiﬁcial Intelligence and
Statistics, 2018, pp. 689–697.

[39] G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter, “Self-
normalizing neural networks,” in Advances in Neural Information Pro-
cessing Systems, 2017, pp. 971–980.

[40] H.-C. Kim and Z. Ghahramani, “Outlier robust Gaussian process classiﬁ-
cation,” in Joint IAPR International Workshops on Statistical Techniques
in Pattern Recognition (SPR) and Structural and Syntactic Pattern
Recognition (SSPR). Springer, 2008, pp. 896–905.

[41] F. J. Ruiz, M. K. Titsias, A. B. Dieng, and D. M. Blei, “Augment
and reduce: Stochastic inference for large categorical distributions,” in
International Conference on Machine Learning, 2018, pp. 4400–4409.

APPENDIX E
DETAILED RESULTS

Tables III and IV provide the average results (over 10 runs)
of scalable binary/multi-class GPCs on 19 UCI benchmarks in
terms of accuracy and negative log likelihood. Additionally,
Table V shows the average results of proposed GPCs using
the Mat´ern52+RBF kernel on 19 UCI benchmarks.

REFERENCES

[1] C. E. Rasmussen and C. K. Williams, Gaussian processes for machine

[2] B. Settles, “Active learning literature survey,” Machine Learning, vol. 15,

learning. MIT Press, 2006.

no. 2, pp. 201–221, 1994.

[3] N. Lawrence, “Probabilistic non-linear principal component analysis
with Gaussian process latent variable models,” Journal of Machine
Learning Research, vol. 6, no. Nov, pp. 1783–1816, 2005.

[4] M. A. Alvarez, L. Rosasco, N. D. Lawrence et al., “Kernels for vector-
valued functions: A review,” Foundations and Trends R(cid:13) in Machine
Learning, vol. 4, no. 3, pp. 195–266, 2012.

[5] H. Liu, J. Cai, and Y.-S. Ong, “Remarks on multi-output Gaussian
process regression,” Knowledge-Based Systems, vol. 144, no. March,
pp. 102–121, 2018.

[6] H.-C. Kim and Z. Ghahramani, “Bayesian Gaussian process classiﬁca-
tion with the EM-EP algorithm,” IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 28, no. 12, pp. 1948–1959, 2006.

[7] L. Wang and C. Li, “Spectrum-based kernel

length estimation for
Gaussian process classiﬁcation,” IEEE Transactions on Cybernetics,
vol. 44, no. 6, pp. 805–816, 2013.

[8] J. Qui˜nonero-Candela and C. E. Rasmussen, “A unifying view of sparse
approximate Gaussian process regression,” Journal of Machine Learning
Research, vol. 6, no. Dec, pp. 1939–1959, 2005.

[9] M. K. Titsias, “Variational learning of inducing variables in sparse
Gaussian processes,” in Artiﬁcial Intelligence and Statistics, 2009, pp.
567–574.

[10] J. Hensman, N. Fusi, and N. D. Lawrence, “Gaussian processes for big
data,” in Uncertainty in Artiﬁcial Intelligence. Citeseer, 2013, pp. 282–
290.

[11] B. Fr¨ohlich, E. Rodner, M. Kemmler, and J. Denzler, “Large-scale
Gaussian process multi-class classiﬁcation for semantic segmentation
and facade recognition,” Machine Vision and Applications, vol. 24, no. 5,
pp. 1043–1053, 2013.

[12] D. Milios, R. Camoriano, P. Michiardi, L. Rosasco, and M. Filippone,
“Dirichlet-based Gaussian processes for large-scale calibrated classiﬁ-
cation,” in Advances in Neural Information Processing Systems, 2018,
pp. 6008–6018.

[13] H. Nickisch and C. E. Rasmussen, “Approximations for binary Gaussian
process classiﬁcation,” Journal of Machine Learning Research, vol. 9,
no. Oct, pp. 2035–2078, 2008.

[14] H. Liu, Y.-S. Ong, X. Shen, and J. Cai, “When Gaussian process meets
big data: A review of scalable gps,” arXiv preprint arXiv:1807.01065,
2018.

[15] E. Snelson and Z. Ghahramani, “Sparse Gaussian processes using
pseudo-inputs,” in Advances in Neural Information Processing Systems,
2006, pp. 1257–1264.

[16] A. Wilson and H. Nickisch, “Kernel interpolation for scalable struc-
tured Gaussian processes (KISS-GP),” in International Conference on
Machine Learning, 2015, pp. 1775–1784.

[17] T. N. Hoang, Q. M. Hoang, and B. K. H. Low, “A unifying framework
of anytime sparse Gaussian process regression models with stochastic
inference for big data,” in International Conference on
variational
Machine Learning, 2015, pp. 569–578.

[18] H. Peng, S. Zhe, X. Zhang, and Y. Qi, “Asynchronous distributed
variational Gaussian process for regression,” in International Conference
on Machine Learning, 2017, pp. 2788–2797.

[19] M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley, “Stochastic
variational inference,” Journal of Machine Learning Research, vol. 14,
no. 1, pp. 1303–1347, 2013.

[20] G. Pleiss, J. Gardner, K. Weinberger, and A. G. Wilson, “Constant-
time predictive distributions for Gaussian processes,” in International
Conference on Machine Learning, 2018, pp. 4111–4120.

[21] J. Gardner, G. Pleiss, R. Wu, K. Weinberger, and A. Wilson, “Prod-
uct kernel interpolation for scalable Gaussian processes,” in Artiﬁcial
Intelligence and Statistics, 2018, pp. 1407–1416.

TABLE III
AVERAGE CLASSIFICATION ACCURACY (ACC) RESULTS ON 19 UCI BENCHMARKS. NOTE THAT ALL THE GPCS EXCEPT ORTH USE THE RBF KERNEL IN
THIS COMPARISON. THE RESULTS OF ORTH REPORTED IN [33] EMPLOY THE MAT ´ERN52+RBF KERNEL.

11

TABLE IV
AVERAGE NEGATIVE LOG LIKELIHOOD (NLL) RESULTS ON 19 UCI BENCHMARKS. NOTE THAT ALL THE GPCS EXCEPT ORTH USE THE RBF KERNEL IN
THIS COMPARISON. THE RESULTS OF ORTH REPORTED IN [33] EMPLOY THE MAT ´ERN52+RBF KERNEL.

adult
connect-4
magic
miniboone
mushroom
ringnorm
twonorm
chess-krvk
letter
nursery
page-blocks
pendigits
statlog-landsat
statlog-shuttle
thyroid
wall-following
waveform
waveform-noise
wine-quality-white

n
48842
67557
19020
130064
8124
7400
7400
28056
20000
12960
5473
10992
6435
58000
7200
5456
5000
5000
4898

d
15
43
11
51
22
21
21
7
17
9
11
17
37
10
22
25
22
41
12

C
2
2
2
2
2
2
2
18
26
5
5
10
6
7
3
4
3
3
7

Selu
84.67
88.07
86.92
93.07
100.00
97.51
98.05
88.05
97.26
99.78
95.83
97.06
91.00
99.90
98.16
90.98
84.80
86.08
63.73

ORTH
85.65
85.99
89.35
93.49
100.00
98.78
97.65
67.76
95.77
97.30
97.21
99.66
91.28
99.90
99.47
95.56
86.13
82.93
57.05

GPCep
85.64
84.40
99.89
99.77
92.03
98.43
97.78
99.89
95.43
50.64
97.41
99.36
92.55
99.92
99.33
96.78
84.96
85.88
42.90

GPCaug
85.60
81.28
99.64
99.87
100.00
97.62
97.78
88.39
83.76
92.67
96.02
98.53
85.68
99.70
94.83
88.42
86.52
33.40
48.49

GPC-I
85.53
84.86
99.89
99.89
99.98
98.22
97.45
99.82
96.50
99.99
97.06
99.50
93.37
99.93
99.29
96.96
86.86
85.16
61.22

GPC-II
85.47
84.68
99.92
99.92
99.98
98.35
97.49
99.78
96.66
99.98
97.26
99.58
93.76
99.91
99.39
96.70
87.02
85.30
60.65

GPC-III
85.55
84.52
99.90
99.92
99.98
98.34
97.58
99.72
96.49
99.99
97.03
99.59
93.70
99.90
99.39
96.72
87.34
85.54
60.51

adult
connect-4
magic
miniboone
mushroom
ringnorm
twonorm
chess-krvk
letter
nursery
page-blocks
pendigits
statlog-landsat
statlog-shuttle
thyroid
wall-following
waveform
waveform-noise
wine-quality-white

n
48842
67557
19020
130064
8124
7400
7400
28056
20000
12960
5473
10992
6435
58000
7200
5456
5000
5000
4898

d
15
43
11
51
22
21
21
7
17
9
11
17
37
10
22
25
22
41
12

C
2
2
2
2
2
2
2
18
26
5
5
10
6
7
3
4
3
3
7

ORTH
0.3045
0.3086
0.2658
0.1618
0.0009
0.0466
0.0590
2.1625
0.2276
0.2225
0.1328
0.0209
0.3956
0.0049
0.0115
0.1514
0.5640
0.7096
2.5681

GPCep
0.3074
0.3822
0.0039
0.0031
0.2363
0.0507
0.0707
0.0236
0.1934
15.2799
0.0763
0.0258
0.2148
0.0035
0.0205
0.0839
0.4641
0.3170
2.0612

GPCaug
0.3306
0.4534
0.0153
0.0064
0.0054
0.0739
0.0625
0.1918
0.6852
0.1546
0.1343
0.1095
0.3356
0.0179
0.0893
0.3437
0.3342
1.0980
1.1191

GPC-I
0.3971
0.3958
0.0041
0.0046
0.0004
0.0568
0.0898
0.0141
0.1685
0.0042
0.1314
0.0225
0.2280
0.0036
0.0296
0.1124
0.4097
0.4437
1.2806

GPC-II
0.3956
0.3998
0.0030
0.0028
0.0004
0.0593
0.0831
0.0169
0.1699
0.0074
0.1165
0.0223
0.2164
0.0041
0.0285
0.1187
0.3987
0.4295
1.2684

GPC-III
0.3933
0.4029
0.0029
0.0029
0.0004
0.0619
0.0802
0.0223
0.1789
0.0070
0.1226
0.0232
0.2195
0.0047
0.0293
0.1242
0.3948
0.4223
1.2748

GPCsm
NA
NA
NA
NA
NA
NA
NA
99.85
96.25
99.63
97.10
99.45
91.91
99.88
99.21
96.50
87.84
86.24
57.24

GPCsm
NA
NA
NA
NA
NA
NA
NA
0.0072
0.1528
0.0299
0.0896
0.0282
0.2110
0.0045
0.0245
0.1033
0.2832
0.2999
0.9953

adult
connect-4
magic
miniboone
mushroom
ringnorm
twonorm
chess-krvk
letter
nursery
page-blocks
pendigits
statlog-landsat
statlog-shuttle
thyroid
wall-following
waveform
waveform-noise
wine-quality-white

n
48842
67557
19020
130064
8124
7400
7400
28056
20000
12960
5473
10992
6435
58000
7200
5456
5000
5000
4898

d
15
43
11
51
22
21
21
7
17
9
11
17
37
10
22
25
22
41
12

C
2
2
2
2
2
2
2
18
26
5
5
10
6
7
3
4
3
3
7

binary (average)
multi-class (average)

acc

nll

GPC-I
85.75
76.43
99.89
99.87
100.00
98.23
97.45
99.67
95.91
97.54
97.83
99.55
93.65
99.94
99.25
97.82
86.96
84.86
60.71
93.9
92.8

GPC-II
85.86
76.49
99.94
99.93
100.00
98.32
97.49
99.66
96.28
97.54
97.79
99.60
93.90
99.94
99.42
97.78
86.84
85.36
60.51
94.0
92.9

GPC-III
85.76
76.38
99.95
99.92
100.00
98.35
97.64
99.64
96.26
97.54
97.85
99.62
93.77
99.94
99.38
97.75
86.96
85.66
60.96
94.0
92.9

GPCsm GPC-I
0.3921
NA
0.5393
NA
0.0038
NA
0.0049
NA
0.0001
NA
0.0568
NA
0.0932
NA
0.0277
99.80
0.2101
96.36
0.1500
99.95
0.0990
97.66
0.0226
99.45
0.2189
92.80
0.0028
99.93
0.0269
99.46
0.0727
97.66
0.4130
88.06
0.4551
86.22
1.2662
58.20
0.1557
NA
0.2471
93.0

GPC-II
0.3892
0.5383
0.0029
0.0028
0.0001
0.0597
0.0830
0.0275
0.2008
0.1502
0.0934
0.0211
0.2043
0.0030
0.0236
0.0733
0.4058
0.4280
1.2623
0.1537
0.2411

GPC-III
0.3900
0.5396
0.0024
0.0030
0.0002
0.0624
0.0798
0.0290
0.2041
0.1504
0.0903
0.0218
0.2037
0.0033
0.0231
0.0763
0.4000
0.4224
1.2573
0.1539
0.2401

GPCsm
NA
NA
NA
NA
NA
NA
NA
0.0076
0.1433
0.0120
0.0745
0.0255
0.1900
0.0029
0.0178
0.0694
0.2828
0.2995
0.9868
NA
0.1760

TABLE V
AVERAGE RESULTS OF PROPOSED GPCS ON 19 UCI BENCHMARKS USING THE MAT ´ERN52+RBF KERNEL. NOTE THAT THE PROPOSED GPCS PROVIDE
RELATIVELY POOR RESULTS ON THE C O N N E C T-4 DATASET, WHICH SKEW THE AVERAGE PERFORMANCE.

