8
1
0
2
 
n
u
J
 
5
 
 
]

C
O
.
h
t
a
m

[
 
 
3
v
4
9
1
3
0
.
7
0
7
1
:
v
i
X
r
a

SENSITIVITY ANALYSIS
FOR MIRROR-STRATIFIABLE CONVEX FUNCTIONS

JALAL FADILI∗, J´ER ˆOME MALICK† , AND GABRIEL PEYR´E‡

Abstract. This paper provides a set of sensitivity analysis and activity identiﬁcation results for
a class of convex functions with a strong geometric structure, that we coined “mirror-stratiﬁable”.
These functions are such that there is a bijection between a primal and a dual stratiﬁcation of the
space into partitioning sets, called strata. This pairing is crucial to track the strata that are identi-
ﬁable by solutions of parametrized optimization problems or by iterates of optimization algorithms.
This class of functions encompasses all regularizers routinely used in signal and image processing,
machine learning, and statistics. We show that this “mirror-stratiﬁable” structure enjoys a nice
sensitivity theory, allowing us to study stability of solutions of optimization problems to small per-
turbations, as well as activity identiﬁcation of ﬁrst-order proximal splitting-type algorithms. Existing
results in the literature typically assume that, under a non-degeneracy condition, the active set asso-
ciated to a minimizer is stable to small perturbations and is identiﬁed in ﬁnite time by optimization
schemes. In contrast, our results do not require any non-degeneracy assumption:
in consequence,
the optimal active set is not necessarily stable anymore, but we are able to track precisely the set
of identiﬁable strata.We show that these results have crucial implications when solving challenging
ill-posed inverse problems via regularization, a typical scenario where the non-degeneracy condition
is not fulﬁlled. Our theoretical results, illustrated by numerical simulations, allow us to characterize
the instability behaviour of the regularized solutions, by locating the set of all low-dimensional strata
that can be potentially identiﬁed by these solutions.

Key words.

convex analysis, inverse problems, sensitivity, active sets, ﬁrst-order splitting

algorithms, applications in imaging and machine learning

AMS subject classiﬁcations. 65K05, 65K10, 90C25, 90C31.

1. Introduction. Variational methods and non-smooth optimization algorithms
are ubiquitous to solve large-scale inverse problems in various ﬁelds of science and
engineering, and in particular in data science. The non-smooth structure of the op-
timization problems promotes solutions conforming to some notion of simplicity or
low-complexity (e.g. sparsity, low-rank, etc.). The low-complexity structure is often
manifested in the form of a low-dimensional “active set”.
It is thus of prominent
interest to be able to quantitatively characterize the stability of these active sets to
perturbations of the objective function. Of crucial importance is also the identiﬁ-
cation in ﬁnite time of these active sets by iterates of optimization algorithms that
numerically minimize the objective function. This type of problems and results is
referred to as “activity identiﬁcation”; a review of the relevant literature will be pro-
vided in the sequel (at the beginning of each section).
In a nutshell, the existing
identiﬁcation results guarantee a perfect stability of the active set to perturbations,
or a ﬁnite identiﬁcation of this active set via algorithmic schemes, under some non-
degeneracy conditions (see in particular [39, 24, 58]). The crucial non-degeneracy
assumption, that takes generally the form (3.1), can be viewed as a geometric gen-
eralization of strict complementary in non-linear programming. However, as we will
illustrate shortly through some preliminary numerics, such a condition is too strin-
gent and is often barely veriﬁed. The goal of this paper is to investigate the situation
where this non-degeneracy assumption is violated.

∗Normandie Univ, ENSICAEN, CNRS, GREYC, France.
†CNRS and LJK, Grenoble, France
‡CNRS and DMA, ENS Paris, France.

1

1.1. Motivating Examples. In order to better grasp the relevance of our anal-
ysis, let us ﬁrst start with the setting of inverse problems that pervades various ﬁelds
including signal processing and machine learning. We will come back to this setting
in Section 4 with further discussions and references.

Regularized Inverse Problems. Assume one observes

y = ˚y + w ∈ RP where ˚y def.= Φ˚x

(1.1)

where w is some perturbation (called noise) and Φ ∈ RP ×N (called forward operator,
or design matrix in statistics). Solving an inverse problem amounts to recovering
˚x, to a good approximation, knowing y and Φ according to (1.1). Unfortunately, in
general, P can be much smaller than the ambient dimension N , and when P = N ,
the mapping Φ is in general ill-conditioned or even singular.

A classical approach is then to assume that ˚x has some ”low-complexity”, and
to use a prior regularization R promoting solutions with such low-complexity. This
leads to the following optimization problem

E(x, (λ, y)) def.= R(x) +

||y − Φx||2,

(P(λ, y))

min
x∈RN

1
2λ

Let us discuss two popular examples of regularizing functions promoting a low-complexity
structure. These two functions will be used in our numerical experiments.

Example 1 ((cid:96)1 norm). For x ∈ RN , its (cid:96)1 norm reads

R(x) = ||x||1

def.=

|xi|,

N
(cid:88)

i=1

(1.2)

where xi is the i-th entry of x. As advocated for instance by [15, 55], the (cid:96)1 enforces
to have a small number of non-zero
the solutions of (P(λ, y)) to be sparse, i.e.
components. Indeed, the (cid:96)1 norm can be shown to be the tightest convex relaxation
(in the sense of bi-conjugation) of the (cid:96)0 pseudo-norm restricted to the unit Euclidian
ball [37]. Recall that (cid:96)0 pseudo-norm of x ∈ RN measures its sparsity

||x||0

def.= (cid:93) (cid:8)i ∈ {1, . . . , N } : xi (cid:54)= 0(cid:9) .

Sparsity has witnessed a huge surge of interest in the last decades. For instance, in
signal and imaging sciences, one can approximate most natural signals and images
In statistics,
using sparse expansions in an appropriate dictionary (see e.g. [45]).
sparsity is a key toward model selection and interpretability [9].

Example 2 (Nuclear norm). For a matrix x ∈ Rn1×n2 ∼ RN , where N = n1n2,

the nuclear norm is deﬁned as

||x||∗

def.= ||σ(x)||1 =

σi(x),

n
(cid:88)

i=1

where n def.= min(n1, n2) and σ(x) = (σ1(x), . . . , σn(x)) ∈ Rn
+ is the vector of singular
values of x. The nuclear norm is the tightest convex relaxation of the rank (in the sense
of bi-conjugation) restricted to the unit Frobenius ball [36]. This underlies its wide use
to promote solutions of (P(λ, y)) with low rank, where we recall rank(x) def.= ||σ(x)||0.
Low-rank regularization has proved useful for a variety of applications, including con-
trol theory and machine learning; see e.g. [28, 1, 11]

2

Active set(s) identiﬁcation. The above discussed regularizers R are non-smooth
convex functions. This non-smoothness arises in a highly structured fashion and is
usually associated, locally, with some low-dimensional active subset of RN (in many
cases, such a subset is an aﬃne or a smooth manifold). Thus R will favor solutions
of (P(λ, y)) that lie in a low-dimensional active set and would allow the inversion of
the system (1.1) in a stable way. More precisely, one would like that under small per-
turbations w, the solutions of (P(λ, y)) move stably along the active set. A byproduct
of this behaviour is that from an algorithmic perspective, if an optimization algorithm
is used to solve (P(λ, y)), one would hope that the iterates of the scheme identify the
active set in ﬁnite time.

Identifying the low-dimensional active set in a stable way is highly desirable for
several reasons. One reason is that it is a fundamental property most practitioners are
looking for. Typical examples include neurosciences [27] where the goal is to recover
a spike train from neural activity, or astrophysics [53] where it is desired to separate
stars from a background.
In both examples, sparsity can be used as a modeling
hypothesis and the recovery method should comply with it in a stable way. A second
reason is algorithmic since one can also take advantage of the low-dimensionality of
the identiﬁed active set to reduce computational burden and memory storage, hence
opening the door to higher-order acceleration of optimization algorithms (see [38, 46]).
Unfortunately, this desirable behaviour rarely occurs in practical applications.
Yet one still observes some form of partial stability (to be given a rigorous meaning
in Section 3 and 4), as conﬁrmed by the numerical experiment of the next paragraph.

1.2. Illustrative numerical experiment. We consider a simple “compressed
sensing” scenario, with the (cid:96)1 norm R = || · ||1 [12] and the nuclear norm R = || · ||∗ [13]
as regularizers. The operator Φ ∈ RN ×P is drawn uniformly at random from the
standard Gaussian ensemble, i.e. the entries of Φ are independent and identically
distributed Gaussian random variables with zero-mean and unit variance. In the case
R = || · ||1, we set (N, P ) = (100, 50) and the vector to recover ˚x is drawn uniformly
at random among sparse vectors with R0(˚x) def.= ||˚x||0 = 10 and unit non-zero entries.
In the case R = || · ||∗, we set (N, P ) = (400, 300) (n1 = n2 = n = 20) and the
matrix to recover ˚x is drawn uniformly at random among low-rank matrices with
R0(˚x) def.= rank(˚x) = 4 and unit non-zero singular values. For each problem suite
(R, N, P ), 1000 realizations of (˚x, Φ, w) are drawn, and y is then generated according
to (1.1). The entries of the noise vector w are drawn uniformly from a Gaussian with
standard deviation 0.1, we set λ = 0.28 for R = || · ||1 and λ = 10 for R = || · ||∗.

For each realization of (˚x, Φ, w), we solve the associated problem (P(λ, y)) using
CVX [31] to get a high precision; denote x(cid:63)(λ, y) the obtained solutions. We also
solve (P(λ, y)) by the Forward-Backward (FB) scheme which reads in this case1

xk+1 = proxλγR(xk + γΦ∗(y − Φxk)).
In our setting, with γ = 1.8/σmax(Φ∗Φ) and non-emptiness of Argmin(E(·, λ, y), it is
well-known that the sequence (xk)k∈N converges to a point in Argmin(E(·, λ, y)).

The top row of Figure 1.1 displays the histogram of the complexity index excess
δ def.= R0(x(cid:63)(λ, y)) − R0(˚x) which clearly shows that we do not have exact stability

1The deﬁnition of the proximal mapping proxτ R (for τ > 0) is given in (5.3). The proximal

mappings of the (cid:96)1 and nuclear norms are

proxµ||·||1

(x) = (cid:0)sign(xi) max(0, |xi| − µ)(cid:1)

i , proxµ||·||∗ (x) = U diag(proxµ||·||1

(σ(x)))V ∗,

where x = U diag(σ(x))V ∗ is a SVD decomposition of x.

3

δ

f
o

s

m
a
r
g
o
t
s
i
H

k

n
o
i
t
a
r
e
t
i

s
v

)
k
x
(
0
R

R = || · ||1

R = || · ||∗

Fig. 1.1. Top row: Histogram of the complexity index excess δ = R0(x(cid:63)(λ, y)) − R0(˚x) where
x(cid:63)(λ, y) is the solution of (P(λ, y)) with noisy observation y = ˚y + w for a small perturbation w and
a well-chosen parameter value λ = c0||w||. Bottom row: Evolution of the complexity index R0(xk)
where (xk)k∈N is the FB iterates sequence converging to some x(cid:63)(λ, y) ∈ Argmin(E(·, λ, y)).

Indeed, although R0(x(cid:63)(λ, y)) is still rather small, its
under the perturbations w.
value is in most cases larger than the complexity index R0(˚x). The bottom row of
Figure 1.1 depicts the evolution with k of the complexity index R0(xk) for two random
instances of ˚x (in blue and red) corresponding to two diﬀerent values of δ. One can
observe that the iterates identify in ﬁnite time some low-dimensional active set with
a complexity index strictly larger than the one associated to ˚x (the red curves). We
will provide a more detailed discussion of this phenomenon in Section 6.

As we will emphasize in the review of previous work, existing results on sensitivity
analysis and low-complexity regularization only focus the case where R0(x(cid:63)(λ, y)) =
R0(˚x) (i.e. δ = 0), whose underlying claim is that the low-complexity model is per-
fectly stable to perturbations, which typically requires some non-degeneracy assump-
tion to hold. In many situations, however, including the compressed sensing scenario
described above, but also in super-resolution imaging (see [26]) and other challenging
inverse problems, non-degeneracy-type hypotheses are too restrictive. It is the goal
of this paper to develop a more general sensitivity analysis, that goes beyond the
non-degenerate case, to improve our understanding of stability to perturbations of
low-complexity regularized inverse problems.

1.3. Contributions and Outline. Our ﬁrst contribution consists in introduc-
ing, in Section 2, a class of proper lower-semicontinuous (lsc) convex functions that
we coin “mirror-stratiﬁable”. The subdiﬀerential of such a function induces a primal-
dual pairing of stratiﬁcations, which is pivotal to track identiﬁable strata. We discuss
several examples of functions enjoying such a structure. With this structure at hand,
we then turn to the main result of this paper, formalized in Theorem 1, and on which

4

all the others rely. This result shows ﬁnite enlarged activity identiﬁcation for a mirror-
stratiﬁable function without the need of any non-degeneracy condition. In addition,
the identiﬁable strata are precisely characterized in terms primal-dual optimal solu-
tions. Sections 3, 4 and 5 instantiate this abstract result for a set of concrete problems,
respectively: sensitivity of composite ”smooth+non-smooth” optimization problems
(Theorem 2), enlarged activity identiﬁcation by proximal splitting schemes such as
Forward-Backward and Douglas-Rachford algorithms (Theorems 4 and 5), and ﬁnally,
enlarged identiﬁcation for regularized inverse problems (Theorem 3). Before stating
these results, we make a short review of the associated literature. Finally Section 6
illustrates these theoretical ﬁndings with numerical experiments, in particular in a
compressed sensing scenario involving the (cid:96)1 and nuclear norms as regularizers. Fol-
lowing the philosophy of reproducible research, all the code to reproduce the ﬁgures
of this article is available online2.

2. Mirror-Stratiﬁable Functions. In this section, we introduce the class of
mirror-stratiﬁable convex functions, and present the sensitivity property they provide
(Theorem 1). We also illustrate that many popular regularizers used in data science
are mirror-stratiﬁable. Most the results of this section (in particular all the examples)
are easy to obtain by basic calculus; we therefore do not give these proofs in the text
and we gather some of them in Appendix A.

2.1. Stratiﬁcations and deﬁnitions. We start with recalling the following

standard deﬁnition of stratiﬁcation.

Definition 1 (Stratiﬁcation). A stratiﬁcation of a set D ⊂ RN is a ﬁnite
partition M = {Mi}i∈I such that for any partitioning sets (called strata) M and M (cid:48)
we have

M ∩ cl(M (cid:48)) (cid:54)= ∅ =⇒ M ⊂ cl(M (cid:48)).

If the strata are open polyhedra, then M is a polyhedral stratiﬁcation, and if they are
C 2-smooth manifolds then M entails a C 2-stratiﬁcation.

A stratiﬁcation is naturally endowed with the partial ordering (cid:54) in the sense that

M (cid:54) M (cid:48) ⇐⇒ M ⊂ cl(M (cid:48)) ⇐⇒ M ∩ cl(M (cid:48)) (cid:54)= ∅.

(2.1)

The relation is clearly reﬂexive and transitive. Furthermore, we have

cl(M ) =

(cid:91)

M (cid:48).

M (cid:48)(cid:54)M

(2.2)

An immediate consequence of Deﬁnition 1 is that for each point x ∈ D, there
is a unique stratum containing x, denoted Mx. Indeed, suppose that there are two
non-empty open strata M1 and M2 such that M1 ∩ M2 = {x}, and thus M1 ∩ M2 (cid:54)= ∅.
This implies, using (2.1), that M1 (cid:54) M2 and M2 (cid:54) M1, and thus M1 = M2.

At this stage, it is worth emphasizing that the strata are not needed to be mani-
folds in the rest of the paper. It is however the case that in many practical cases that
we will discuss, strata are indeed manifolds, and sometimes aﬃne manifolds.

Example 3 (Polyhedral sets and functions). A partition of a polyhedral set into
its open faces induces a natural ﬁnite polyhedral stratiﬁcation of it. In turn, let R :

2Code available at https://github.com/gpeyre/2017-SIOPT-stratification

5

RN → R def.= R∪{+∞} be a polyhedral function, and consider a polyhedral stratiﬁcation
of its epigraph, which is a polyhedral set in RN +1. Projecting all polyhedral strata onto
the ﬁrst N -coordinates one obtains a ﬁnite polyhedral stratiﬁcation of dom(R).

Remark 1. The previous example extends to semialgebraic sets and functions,
which are known to induce stratiﬁcations into ﬁnite disjoint unions of manifolds. In
fact, this holds for any tame class of sets/functions; see, e.g., [18].

We now single out a speciﬁc set of strata, called active strata, that will play a

central role for ﬁnite enlarged activity identiﬁcation purposes.

Definition 2 (Active strata). Given a stratiﬁcation M of D ⊂ RN and a point

x ∈ D, a stratum M ∈ M is said active at x if x ∈ cl(M ).

Proposition 1. Given a stratiﬁcation M of D and a point x ∈ D. Then there
exists δ > 0 such that set of strata Mx(cid:48) for any x(cid:48) such that ||x(cid:48) − x|| < δ, coincides
with the set of strata M (cid:62) Mx and with the set of active strata at x.

2.2. Mirror-Stratiﬁable functions. Following [19, Section 4], we deﬁne the

key correspondence operator whose role will become apparent shortly.

Definition 3. Let R : RN → R be a proper lsc convex function. The associated

correspondence operator JR : 2RN

→ 2RN

JR(S) def.=

is deﬁned as
(cid:91)

ri(∂R(x)),

x∈S

where ∂R is the subdiﬀerential of R.

Observe that, by deﬁnition, JR is increasing for set inclusion

S ⊂ S(cid:48) =⇒ JR(S) ⊂ JR(S(cid:48)).

(2.3)

For this operator to be useful in sensitivity analysis, we will further impose that JR
is decreasing for the partial ordering (cid:54) in (2.1), as captured in our main deﬁnition.
This is a key requirement that captures the intuitive idea that the larger a primal
stratum, the smaller its image by JR in the dual space.

In the following, we will denote R∗ the Legendre-Fenchel conjugate of R.
Definition 4 (Mirror-stratiﬁable functions). Let R : RN → R be a proper lsc
convex function; we say that R is mirror-stratiﬁable with respect to a (primal) strat-
iﬁcation M = {Mi}i∈I of dom(∂R) and a (dual) stratiﬁcation M∗ = {M ∗
i }i∈I of
dom(∂R∗) if the following holds:

(i) Conjugation induces a duality pairing between M and M∗, and JR : M →

M∗ is invertible with inverse JR∗ , i.e. ∀M ∈ M, M ∗ ∈ M∗ we have

M ∗ = JR(M ) ⇐⇒ JR∗ (M ∗) = M.

(ii) JR is decreasing for the relation (cid:54): for any M and M (cid:48) in M

M (cid:54) M (cid:48) ⇐⇒ JR(M ) (cid:62) JR(M (cid:48)).

The primal-dual stratiﬁcations that make R mirror-stratiﬁable are not unique, as
will be exempliﬁed in Remark 3. Though the deﬁnition is formal and the assumptions
looks restrictive, this class include many useful examples, as illustrated in the next
section. We ﬁnish this section with a remark, used in the sequel.

Remark 2 (Separability). For each m = 1, . . . , L, suppose that the proper lsc
convex function Rm : RNm → R is mirror-stratiﬁable with respect to stratiﬁcations
Mm and M∗
m. Then it is easy to show, using standard subdiﬀerential and conjugacy
calculus, that the function R : (xm)1(cid:54)m(cid:54)L ∈ RN1 × · · · × RNL (cid:55)→ (cid:80)L
m=1 Rm(xm) is
mirror-stratiﬁable with stratiﬁcations M1 × · · · × ML and M∗

1 × · · · × M∗
L.

6

2.3. Examples. The notion of a mirror-stratiﬁable function looks quite rigid.
However, many of the regularization functions routinely used in data science are
mirror-stratiﬁable. Let us provide some relevant examples in this section. In partic-
ular, the (cid:96)1-norm and the nuclear norm will be used in the numerical experiments.

2.3.1. Legendre functions. A lsc convex function R : RN → R is said to be a
Legendre function (see [50, Chapter 26]) if (i) it is diﬀerentiable and strictly convex
on the interior of its domain int(dom(R)) (cid:54)= ∅, and (ii) ||∇R(xk)|| → +∞ for every
sequence (xk)k∈N ⊂ int(dom(R)) converging to a boundary point of dom(R). Many
functions in convex optimization are Legendre; most notably, quadratic functions
and the log barrier of interior point methods. It was shown in [50, Theorem 26.5]
that R is Legendre if and only if its conjugate R∗ is Legendre, and that in this case
∇R is a bijection from int(dom(R)) to int(dom(R∗)) with ∇R∗ = (∇R)−1. As a
consequence, a Legendre function R is mirror-stratiﬁable with M = {int(dom(R))}
and M∗ = {int(dom(R∗))}.

2.3.2. (cid:96)1-norm. Let R : x ∈ R (cid:55)→ |x|, whose conjugate R∗ = ι[−1,1]. It follows
that R is mirror-stratiﬁable with M = { ] − ∞, 0[, {0}, ]0, +∞[} and M∗ = {{−1}, ] −
1, 1[, {+1}}. Using Remark 2, the next result is clear.

Lemma 1. The (cid:96)1-norm and its conjugate ι[−1,+1]N are mirror-stratiﬁable with re-
spect to the stratiﬁcations M = { ] − ∞, 0[, {0}, ]0, +∞[}N of RN and M∗ = {{−1}, ]−
1, 1[, {+1}}N of [−1, +1]N .

A graphical illustration of this lemma in dimension 2 is displayed in Figure 2.1(a).
Remark 3 (Non-uniqueness of stratiﬁcations).
In general, we do not have
uniqueness of stratiﬁcations inducing mirror-stratiﬁable functions. To illustrate this,
let us consider the (cid:96)1 norm. Take the partitions M = { ] − ∞, 0[∪]0, +∞[, {0}}N
of RN and M∗ = {{−1, +1}, ] − 1, 1[}N . These are valid stratiﬁcations though
the strata are not connected. Other possible valid stratiﬁcations are given by strata
Mj = (cid:8)x ∈ RN : ||x||0 = j(cid:9) and M ∗
j = J||·||1 (Mj), for j = 0, 1, . . . , N . It is immediate
to check that the (cid:96)1-norm is also mirror-stratiﬁable with respect to these stratiﬁcations.
However, as devised above, it is in general not wise to take such large strata as they
lead to less sharp localization and sensitivity results.

Remark 4 (Instability under the sum rule). The family of mirror-stratiﬁable
functions is unfortunately not stable under the sum. As a simple counter-example,
consider the pair of conjugate functions on R: R(x) = |x| + x2/2 and R∗(u) =
(max{|u| − 1, 0})2/2. We obviously have dom(∂R) = dom(∂R∗) = R. However we
observe that JR(R) = R \ {−1, 1}. This yields that JR cannot be a pairing between
any two stratiﬁcations of R, and therefore R cannot be mirror-stratiﬁable.

2.3.3. (cid:96)1,2 norm. The (cid:96)1,2 norm, also known as the group Lasso regularization,
has been advocated to promote group/block sparsity [60], i.e.
it drives all the coef-
ﬁcients in one group to zero together. Let B a non-overlapping uniform partition of
{1, . . . , N } into K blocks. The (cid:96)1,2 norm induced by the partition B reads

||x||B =

||xB||2

(cid:88)

B∈B

where xB is the restriction of x to the entries indexed by the block B. This is again
a separable function of the xB’s. One can easily show that the (cid:96)2 norm on R|B| and
its conjugate, the indicator of the unit (cid:96)2-ball B
in R|B|, are mirror-stratiﬁable
with respect to the stratiﬁcations {{0}, R|B| \ {0}} of RN and {S|B|−1

, int(B

(cid:96)|B|
2

)}

(cid:96)2

(cid:96)|B|
2

7

(a)

(b)

8

Fig. 2.1. Graphical illustration of mirror-stratiﬁcation for two norms: (a) (cid:96)1 norm in dimen-

sion 2; (b) (cid:96)1,2 norm in dimension 3 (with two blocks, of respectively sizes 1 and 2).

(cid:96)2

(cid:96)|B|
2

), where S|B|−1

of B
is the corresponding unit sphere. In turn, the (cid:96)1,2 norm is
mirror-stratiﬁable with respect to the stratiﬁcations M = {{0}, R|B| \ {0}}K and
M∗ = {S|B|−1, int(B
)}K. An illustration of this result in dimension 3 is portrayed
in Figure 2.1(b).

(cid:96)|B|
2

2.3.4. Nuclear norm. Let us come back to the nuclear norm deﬁned in Exam-
ple 2. For simplicity, we assume n1 = n2 = n. Let M be a stratum of the (cid:96)1 norm
stratiﬁcation. In the following, we denote by M sym its symmetrization. We observe
that σ−1(M sym) = {X ∈ Rn×n : rank(X) = ||z||0, z ∈ M }. So many inverse images
σ−1(M sym) of strata M sym coincide. This suggests the following n + 1 stratiﬁcations:
for a given i ∈ {0, . . . , N }

Mi = (cid:8)X ∈ Rn×n : rank(X) = i(cid:9)
i = {U ∈ Rn : σ1(U ) = · · · = σi(U ) = 1, ∀j > i, |σj(U )| < 1} .
M ∗

This yields that || · ||∗ is mirror-stratiﬁable with respect to these stratiﬁcations.

2.3.5. Polyhedral Functions. We here establish mirror-stratiﬁability of poly-
hedral functions, including the (cid:96)1 norm, the (cid:96)∞ norm and anisotropic TV semi-norm.

A polyhedral function R : RN → R can be expressed as

R(x) = max

{(cid:104)ai, x(cid:105) − αi} + ι(cid:84)

i=k+1,...,m{x : (cid:104)ai, x(cid:105)−αi(cid:54)0}(x).

(2.4)

i=1,...,k

For any x ∈ dom(R), introduce the two sets of indices

I max(x) = {i = 1, . . . , m : (cid:104)ai, x(cid:105) − αi = R(x)} ,
I feas(x) = {i = 1, . . . , m : (cid:104)ai, x(cid:105) = αi} .

For a given index set I ⊂ {1, . . . , m}, we consider the aﬃne manifold

MI

def.= (cid:8)x ∈ dom(R) : I max(x) ∩ I feas(x) = I(cid:9) .

(2.5)

We see that some MI may be empty and that M = {MI }I is a stratiﬁcation of
dom(R). The stratum MI is characterized by the optimality part I max = I ∩{1, . . . , k}
and the feasibility part I feas = I ∩ {k + 1, . . . , m} of I. Similarly we deﬁne

M ∗

I = ri(conv {ai

: i ∈ I max}) + ri(cone (cid:8)ai

: i ∈ I feas(cid:9)).

Let us formalize in the next proposition a result alluded to in [19].

Proposition 2. A polyhedral function R is mirror-stratiﬁable with respect to its

naturally induced stratiﬁcations {MI }I and {M ∗

I }I .

Remark 5 (Back to || · ||1). As we anticipated, Proposition 2 subsumes Lemma 1

as a special case. To see this, observe that

||x||1 = max
||u||∞(cid:54)1

(cid:104)u, x(cid:105) =

max
u∈{−1,+1}N

(cid:104)u, x(cid:105) ,

which is of the form (2.4) with k = m = 2N . Thus there are 22N
aﬃne manifolds as
deﬁned by (2.5). But many of them are empty: there is only 3N (non-empty, distinct)
manifolds in the stratiﬁcation of RN , and they coincide with those in Lemma 1.

2.3.6. Spectral Lifting of Polyhedral Functions. As we discussed in Re-
mark 4, JR may fail to induce a duality pairing between stratiﬁcations of R and R∗,
in which case R cannot be mirror-stratiﬁable. Hence, for this type of duality to hold,
one needs to impose stringent strict convexity conditions. To avoid this, and still
aﬀord a large class of mirror-stratiﬁable functions that are of utmost in applications,
we consider spectral lifting of polyhedral functions, in the same vein as [19] did it for
partial smoothness.

A matrix function R : RN =n×n → R is said to be a spectral lift of a polyhedral
function if there exists a polyhedral function Rsym : Rn → R, invariant under signed
permutation of its coordinates, such that R = Rsym ◦ σ where σ computes the singular
values of a matrix. Associated to M = {MI }I the (polyhedral) stratiﬁcation induced
by Rsym, we consider its symmetrized stratiﬁcation. We deﬁne the symmetrization of
M ∈ M, as the set M sym

M sym = (cid:8)x ∈ RN : ∃y ∈ M such that for all i there exists j with |xi| = |yj|(cid:9) .

The next result is a corollary of the main result of [19].

Proposition 3. A spectral function R = Rsym ◦ σ is mirror-stratiﬁable with

respect to the smooth stratiﬁcation {σ−1(M sym)} and its image by JR.

Remark 6 (Back to || · ||∗). The nuclear norm is a spectral lift of the (cid:96)1 norm.
Therefore, one can recover mirror-stratiﬁcation of the nuclear norm, with the strati-
ﬁcations given in Section 2.3.4, by putting together Lemma 1 and Proposition 3.

9

2.4. Activity Identiﬁcation for Mirror-Stratiﬁable Functions. To our
point of view, the notion of mirror-stratiﬁbility deserves a special study in view of
the following simple but powerful geometrical observation. We state it as a theorem
because of its utmost importance in the subsequent developments of this paper.

Theorem 1 (Enlarged activity identiﬁcation). Let R be a proper lsc convex
function which is mirror-stratiﬁable with respect to primal-dual stratiﬁcations M =
{M } and M∗ = {M ∗}. Consider a pair of points (¯x, ¯u) and the associated strata M¯x
and M ∗
¯u. If the sequence pair (xk, uk) → (¯x, ¯u) is such that uk ∈ ∂R(xk), then for k
large enough, xk is localized in a speciﬁc set of strata such that

M¯x (cid:54) Mxk

(cid:54) JR∗ (M ∗

¯u).

(2.6)

Proof. By assumption on R, ∂R is sequentially closed [50, Theorem 24.4] and

thus ¯u ∈ ∂R(¯x). Now, since xk is close to ¯x, upon invoking Proposition 1, we get

Mxk

(cid:62) M¯x

M ∗
uk

(cid:62) M ∗
¯u.

which shows the left-hand side of (2.6). Similarly, we have

(2.7)

Using the fact that ∂R(xk) is a closed convex set together with (2.3) leads to

uk ∈ ∂R(xk) = cl(ri(∂R(xk))) = cl(JR({xk})) ⊂ cl(JR(Mxk ))

which entails Muk
decreasing for the relation (cid:54), we get

(cid:54) JR(Mxk ). Using ﬁnally (2.7) and that by deﬁnition, JR is

M ∗
¯u

(cid:54) M ∗
uk

(cid:54) JR(Mxk ) =⇒ Mxk

(cid:54) JR∗ (M ∗

¯u),

whence we deduce the right-hand side of (2.6).

Mirror-stratiﬁcation thus allows us to prove the simple but powerful claim of
Theorem 1. As we will see in the rest of the paper, this result will be the backbone
to prove sensitivity and ﬁnite enlarged activity identiﬁcation results in absence of
non-degeneracy.

3. Sensitivity of Composite Optimization Problems. In this section, we

consider a parametric convex optimization problem of the form

E(x, p) def.= F (x, p) + R(x),

min
x∈RN

(P(p))

depending on the parameter vector p ∈ Π, where Π is the parameter set, an open
subset of a ﬁnite dimensional linear space. Sensitivity analysis studies the properties
of solutions x(cid:63)(p) of (P(p)) (assuming they exist) to perturbations of the parameters
vector p ∈ Π around some reference point ˚p. In Section 3.1, we brieﬂy review the
existing results on this topic and their limitations. We then introduce in Section 3.2
our new results obtained owing to the mirror-stratiﬁable structure.

3.1. Existing sensitivity results. Classical sensitivity results (see e.g. [7, 47,
21]) study the regularity of the set-valued map p (cid:55)→ x(cid:63)(p). A typical result proves
Lipschitz continuity of this map provided that E is smooth enough and E(·, ˚p) has
a local second-order (quadratic) growth at x(cid:63)(˚p), i.e. that there exists some c > 0
such that E(x, ˚p) (cid:62) E(x(cid:63)(˚p), ˚p) + c||x − x(cid:63)(˚p)||2 for x nearby x(cid:63)(˚p). For C 2-smooth
10

optimization, this growth condition is equivalent to positive deﬁniteness of the hessian
of E with respect to x evaluated at (x(cid:63)(˚p), ˚p) [29]. For classical smooth constrained
optimization problems, activity is captured by the subset of active inequality con-
straints. Under reasonable non-degeneracy conditions (see, for example, [29]), this
active set is stable under small perturbations to the objective.

A nice nonsmooth sensitivity theory is based on the notion of partial smooth-
ness [39]. Partial smoothness is an intrinsically geometrical assumption which, infor-
mally speaking, says that E behaves smoothly along an active manifold and sharply in
directions normal to the manifold. Furthermore, under a non-degeneracy assumption
at a minimizer (see (3.1)), it allows appealing statements of second-order optimality
conditions (including second-order generalized diﬀerentiation) and associated sensi-
tivity analysis around that minimizer [39, 40].

Let ∂E(x, p) be the subdiﬀerential of E according to x. Specializing the result of
[24, Proposition 8.4] to a proper lsc convex function E(·, p), one can show that C 2-
partial smoothness of E(·, p) at x(cid:63)(˚p) relative to some ﬁxed manifold (independent of
p) for 0, together with the non-degeneracy assumption

0 ∈ ri(∂E(x(cid:63)(˚p), ˚p))

(3.1)

is equivalent to the existence of an identiﬁable C 2-smooth manifold, i.e. for x(cid:63)(p) and
u(cid:63)(p) ∈ ∂E(x(cid:63)(p), p) close enough to x(cid:63)(˚p) and 0, x(cid:63)(p) lives on the active/partly
smooth manifold of x(cid:63)(˚p). If these assumptions are supplemented with a quadratic
growth condition of E (see above) along the active manifold, then one also has C 1
smoothness of the single-valued mapping p (cid:55)→ x(cid:63)(p) [39, Theorem 5.7].
It can be
deduced from [23, Corollary 4.3] that for almost all linear perturbations of lsc convex
semialgebraic functions, the non-degeneracy and quadratic growth conditions hold.
However, this genericity fails to hold for many cases of interest. As an example, con-
sider E of (P(λ, y)) with λ > 0 ﬁxed and p = y. If R is a proper lsc convex and
semialgebraic function, one has from [23] that for Lebesgue almost all Φ∗y, prob-
lem (P(λ, y)) has at most one minimizer at which furthermore non-degeneracy and
quadratic growth hold. Of course genericity in terms of Φ∗y does not imply that in
terms of y, which is our parameter of interest. Not to mention that we supposed λ
ﬁxed while it is not in many cases of interest.

3.2. Sensitivity analysis without non-degeneracy. For a ﬁxed p, (P(p)) is
a standard composite optimization problem. Here, we assume that the objective is the
sum of a C 1(RN ) convex function F (·, p) and a nonsmooth proper lsc convex function
R. We denote ∇F (x, p) the gradient of F (·, p) at x.

We are going to show that if the minimizer x(cid:63)(˚p) is unique, slight perturbations
p of ˚p generate solutions x(cid:63)(p) that are in a “controlled” stratum Mx(cid:63)(p) precisely
sandwiched to extreme strata deﬁned from a primal-dual pair associated to ˚p.

Theorem 2 (Sensitivity analysis with mirror-stratiﬁable functions). Let ˚p be a
given point in the parameter space Π. Assume that: (i) E(·, ˚p) has a unique mini-
mizer x(cid:63)(˚p), (ii) E is lsc on RN × Π, (iii) E(x(cid:63)(˚p), ·) is continuous at ˚p, (iv) ∇F
is continuous at (x(cid:63)(˚p), ˚p), and (v) E is level-bounded3 in x uniformly in p locally
around ˚p. If R is mirror-stratiﬁable according to (M,M∗), then for all p close to ˚p,

3 Recall from [51, Deﬁnition 1.16] that the function E : RN × Π is said to be level-bounded in x
locally uniformly in p around ˚p if for each c ∈ R, there exists a neighbourhood V of ˚p and a bounded
set Ω such that the sublevel set (cid:8)x ∈ RN : E(x, p) (cid:54) c(cid:9) ⊂ Ω for all p ∈ V.

11

(a)

(b)

12

Fig. 3.1. Graphical illustration of Theorem 2 for a projection problem on the (cid:96)∞ ball. (a)

corresponds to a degenerate situation while (b) to a non-degenerate one.

any minimizer x(cid:63)(p) of E(·, p) is localized as follows

Mx(cid:63)(˚p) (cid:54) Mx(cid:63)(p) (cid:54) JR∗ (M ∗

u(cid:63)(˚p)) where u(cid:63)(˚p) def.= −∇F (x(cid:63)(˚p), ˚p).

(3.2)

Proof. Le (pk)k∈N ⊂ Π be a sequence of parameters converging to ˚p. By assump-
tion on F and R, E(·, p) is proper for any p. Since it is also lsc and is level-bounded
in x uniformly in p locally around ˚p by conditions (ii) and (v). It follows from [51,
Theorem 1.17(a)] that Argmin E(·, pk) is non-empty and compact, and, in turn, any
sequence (x(cid:63)
k)k∈N of minimizers is bounded. We consider a subsequence, which for
simplicity, we denote again x(cid:63)

k → ¯x. We then have

E(¯x, ˚p)

(cid:54)
condition (ii)
(cid:54)
Optimality

lim inf
k

E(x(cid:63)

k, pk)

lim inf
k

E(x(cid:63)(˚p), pk)

=
condition (iii)

lim
k

E(x(cid:63)(˚p), pk) = E(x(cid:63)(˚p), ˚p).

By the uniqueness condition (i), we conclude that ¯x = x(cid:63)(˚p). Let u(cid:63)
k
Since ∇F is continuous at (˚x, ˚p) by assumption (iv), one has u(cid:63)
optimality condition of problem (P(p)) reads u(cid:63)
position to invoke Theorem 1 to conclude.

k ∈ ∂R(x(cid:63)

def.= −∇F (x(cid:63)
k, pk).
k → ˚u. The ﬁrst-order
k). Hence we are now in

Remark 7 (Single manifold identiﬁcation). In the special case where we have

the non-degeneracy condition

u(cid:63)(˚p) = −∇F (x(cid:63)(˚p), ˚p) ∈ ri(∂R(x(cid:63)(˚p))),

(3.3)

our result simpliﬁes and we recover the known exact identiﬁcation result. To see this,
we note that (3.3) also reads u(cid:63)(˚p) ∈ JR({x(cid:63)(˚p)}), and thus M ∗
u(cid:63)(˚p) = JR({x(cid:63)(˚p)}).
In this case, (3.2) becomes Mx(cid:63)(p) = M˚x for all p close to ˚p. Such a result was also
established in [35, 40] under condition (3.3), when F (·, ˚p) is also locally C 2 around
x(cid:63)(˚p) and R is partly smooth at x(cid:63)(˚p) relative to a C 2-smooth manifold Mx(cid:63)(˚p). An
example of this non-degenerate scenario is shown in Figure 3.1(b). Our result covers
the more delicate degenerate situation where u(cid:63)(˚p) might be on the relative boundary
of the subdiﬀerential, but requires the stronger mirror-stratiﬁability structure on the
non-smooth part. However, the active set at x(cid:63)(p) is in general not unique for all p
close to ˚p, hence the terminology enlarged activity.

Example 4. The result of Theorem 2 is illustrated in Figure 3.1, for the projec-
tion problem on a (cid:96)∞ ball, in both the degenerate and the non-degenerate situations.
More precisely, we take E(x, p) = 1
2 ||x−p||2 +ιB(cid:96)∞ (x), so that u(cid:63)(p) = p−x(cid:63)(p), where
x(cid:63)(p) = PB(cid:96)∞ (p). Figure 3.1(a) illustrates the degenerate case where u(cid:63)(˚p) belongs to
the relative boundary of the normal cone of B(cid:96)∞ at x(cid:63)(˚p). Taking a perturbation p
around ˚p entails that x(cid:63)(p) belongs either to M1 = Mx(cid:63)(˚p) or to the enlarged stratum
M2 = JR∗ (M ∗
2 ). In the non-degenerate case of Figure 3.1(b) presented
in Remark 7, the optimal solution x(cid:63)(p) is always on M1 for small perturbations.

u(cid:63)(˚p)) = JR∗ (M ∗

Remark 8 (Quadratic growth). We can also establish the Lipschitz continuity
of x(cid:63)(p) under the conditions of Theorem 2, under an additional second-order growth
condition, just as in classical sensitivity analysis (see [7, Section 4.2.1]). Let us
assume that there exists a neighbourhood V of x(cid:63)(˚p) and κ > 0 such that

E(x, ˚p) (cid:62) E(x(cid:63)(˚p), ˚p) + κ||x − x(cid:63)(˚p)||2

∀x ∈ V.

(3.4)

k → x(cid:63)(˚p) for pk → ˚p (see the proof of Theorem 2), we have x(cid:63)

Since x(cid:63)
k ∈ V for k
large enough. If, moreover, ∇F (x, ·) is Lipschitz-continuous in a neighbourhood of ˚p
with Lipschitz constant ν independent of x ∈ V, we get

κ||x(cid:63)

k − x(cid:63)(˚p)||2 (cid:54) E(x(cid:63)

k, ˚p) − E(x(cid:63)(˚p), ˚p)
= (cid:0)(E(x(cid:63)(˚p), pk) − E(x(cid:63)

k, pk)) − (E(x(cid:63)(˚p), ˚p) − E(x(cid:63)

k, ˚p))(cid:1)

− (cid:0)E(x(cid:63)(˚p), pk) − E(x(cid:63)

k, pk)(cid:1)

(x(cid:63)

k minimizes E(·, pk)) (cid:54) (E(x(cid:63)(˚p), pk) − E(x(cid:63)
(By (P(p))) = (F (x(cid:63)(˚p), pk) − F (x(cid:63)

k, pk)) − (E(x(cid:63)(˚p), ˚p) − E(x(cid:63)
k, pk)) − (F (x(cid:63)(˚p), ˚p) − F (x(cid:63)

k, ˚p)
k, ˚p))

(Mean-Value Theorem) (cid:54) (cid:0) sup

||∇F (x, pk) − ∇F (x, ˚p)||(cid:1)||x(cid:63)

k − x(cid:63)(˚p)||

x∈V

(cid:54) ν||pk − ˚p||||x(cid:63)

k − x(cid:63)(˚p)||.

whence we conclude that

d(x(cid:63)(˚p), Argmin E(·, pk)) (cid:54) ν/κ||pk − ˚p||.

4. Regularized Inverse Problems. A typical context where our framework of
mirror-stratiﬁability is of usefulness is that of studying stability to noise of regularized
linear inverse problems. We come back to the situation presented in introduction:

13

studying stability issues to perturbed observations of the form y = ˚y + w amounts
to analyzing sensitivity of the minimizers and the optimal value function in (P(λ, y))
when the parameter p = (λ, y) evolves around the reference point ˚p = (0, ˚y). Unlike
the usual sensitivity analysis theory setting recalled in the previous section (e.g. [7]),
here the objective E may not even be continuous at ˚p.

We provide hereafter some pointers to the relevant literature, then we develop
our sensitivity results for mirror-stratiﬁable functions. These results involve primal
and dual solutions to the noiseless problem

E(x, (0, y)) def.= R(x)

s.t. Φx = y,

(P(0, y))

min
x∈RN

4.1. Existing sensitivity results for regularized Inverse Problems.
Lipschitz stability. Assume there exists a dual multiplier η ∈ RP (sometimes
referred to as a “dual certiﬁcate”) for the noiseless constrained problem (P(0, y))
taken at y = ˚y such that Φ∗η ∈ ∂R(˚x). The latter condition is equivalent to ˚x being
a minimizer of (P(0, y)). This condition goes by the name of the “source” or “range”
condition in the inverse problems literature. It has been widely used to derive stability
results in terms of ||Φx(cid:63)(λ, y)−Φ˚x|| or R(x(cid:63)(λ, y))−R(˚x)−(cid:104)Φ∗η, x(cid:63)(λ, y) − ˚x(cid:105); see [52]
and references therein. To aﬀord stability in terms of ||x(cid:63)(λ, y) −˚x|| directly, the range
condition has to be strengthened to its non-degenerate version Φ∗η ∈ ri(∂R(˚x)). It
has been shown that this condition implies that the set-valued map (λ, y) (cid:55)→ x(cid:63)(λ, y)
is Lipschitz-continuous at (0, ˚y); see [32] and [58].

Active set stability. In the case where R is partly smooth, one can approach
an even more complete sensitivity theory by studying stability of the partly smooth
manifold of R at ˚x. In particular, it can be shown from that if an appropriate non-
degeneracy assumption holds, see [59], then problems (P(0, y)) and (P(λ, y)) have
unique minimizers (respectively ˚x and x(cid:63)(λ, y)), and x(cid:63)(λ, y) lies on the partly smooth
manifold of R at ˚x. Observe that compared to Lipschitz stability, active set stability
is more demanding as the non-degeneracy condition has to hold for a speciﬁc dual
multiplier, which is obviously more stringent. This type of results has appeared many
times in the literature for special cases, e.g.
for the (cid:96)1 norm [30, 61], the nuclear
norm [1]. The work in [57, 59] has uniﬁed all these results.

Non-degeneracy in practice for deconvolution and compressed sensing. The above
results require that some abstract non-degeneracy condition holds, which imposes
strict limitations on practical situations. In particular, when Φ is a convolution oper-
ator and ˚x is sparse, [10] studies Lipschitz stability and [25] support stability. In this
setting, the non-degeneracy condition holds whenever the non-zero entries are sepa-
rated enough, which is not often veriﬁed. Another setting where this stability theory
has been applied in when Φ is drawn from a random matrix ensemble, i.e. compressed
sensing. For a variety of partly smooth regularizers (including the (cid:96)1, nuclear and (cid:96)1,2
norms), the non-degeneracy condition holds with high probability if, roughly speak-
ing, the sample size P is suﬃciently larger than the “dimension” of the active set at
˚x (see e.g. [12, 22]). Again this is a clear limitation as illustrated in Section 1.2.

4.2. Primal and dual problems. Suppose we have observations of the form
(1.1), and we want to recover ˚x (or a provably good approximation of it). As advocated
in Section 1.1, a popular approach is to adopt a regularization framework which can
be cast as the optimization problem (P(λ, y)) (for λ > 0) and (P(0, y)) (when λ = 0).
In the sequel, we assume that

R∞(z) > 0,

∀z ∈ ker(Φ) \ {0},

(4.1)

14

where R∞ is the asymptotic (or recession) function of R, deﬁned as

R∞(z) def.= lim
t→+∞

R(x + tz) − R(x)
t

,

∀x ∈ dom(R).

Condition (4.1) is a necessary and suﬃcient condition for the set of minimizers of
(P(λ, y)) and (P(0, y)) to be non-empty and compact [56, Lemma 5.1]. It is satisﬁed
for example when R is coercive.

Remark 9 (Discontinuity of F ). Letting p def.= (λ, y) ∈ Π def.= R+ × RP , we see

that (P(λ, y)) and (P(0, y)) are instances of (P(p)), by setting

F (x, p) def.=

(cid:40) 1

2λ ||y − Φx||2
ιHy (x)

if λ > 0,
if λ = 0,

where Hy

def.= (cid:8)x ∈ RN : Φx = y(cid:9) .

(4.2)

The corresponding function F considered does not obey the assumptions of Theorem 2.
Indeed, the parameter set Π is not open, with ˚p = (0, ˚y) that lives on the boundary of
Π, and F is only lsc at such ˚p (because of the aﬃne constraint Φx = y). The main
consequence will be that one can no longer allow p to vary freely nearby ˚p.

In order to study the sensitivity of solutions, we look at the Fenchel-Rockafellar

dual problem, which reads (for all λ (cid:62) 0)

(cid:104)q, y(cid:105) −

||q||2 − R∗(Φ∗q).

max
q∈RP

λ
2

(D(λ, y))

We denote D(λ, y) the set of solutions of (D(λ, y)). Note that for λ > 0, thanks to
strong concavity, there is a unique dual solution q(cid:63)(λ, y), i.e. D(λ, y) = {q(cid:63)(λ, y)}. We
also have from the primal-dual extremality relationship that for any primal solution
x(cid:63)(λ, y) of (P(λ, y)),

q(cid:63)(p) =

y − Φx(cid:63)(p)
λ

and Φ∗q(cid:63)(p) ∈ ∂R(x(cid:63)(p)).

(4.3)

While (D(0, y)) is the dual of (P(0, y)), it is important to realize that it is not the limit
of (D(λ, y)) in the sense that its set of dual solutions is in general not a singleton.
Lemma 3 hereafter singles out a speciﬁc dual optimal solution (sometimes called
“minimum norm certiﬁcate”) deﬁned by

˚q(0, y) = argmin

{||q|| : Φ∗q ∈ ∂R(x(cid:63)(0, y))} = argmin

{||q|| : q ∈ D(0, y)} .

(4.4)

q∈RP

q∈RP

The following two important lemmas ensure the convergence of the solutions to the
primal and dual problems as λ → 0 and as ||y − ˚y|| → 0.

Lemma 2 (Primal solution convergence). Assume that ˚x is the unique solution

to (P(0, ˚y)). For any sequence of parameters pk = (λk, yk) with λk > 0 such that

(cid:18) ||yk − ˚y||2
λk

(cid:19)

, λk

−→ (0, 0),

and any solution x(cid:63)(pk) of (P(pk)), we have x(cid:63)(pk) → ˚x.

Proof. Denote x(cid:63)
k

can extract a converging subsequence, which for simplicity, we denote again x(cid:63)
Optimality of x(cid:63)

k)k∈N is a bounded sequence by (4.1), one
k → ¯x.

def.= x(cid:63)(pk). Since (x(cid:63)

k implies that
k) (cid:54) 1
2λk

R(x(cid:63)

||yk − Φx(cid:63)

k||2 + R(x(cid:63)

||yk − ˚y||2 + R(˚x).

(4.5)

k) (cid:54) 1
2λk

15

Passing to the limit and using the hypothesis that 1
λk

||yk − ˚y||2 → 0, we get

lim sup
k

R(x(cid:63)

k) (cid:54) R(˚x).

k) (cid:62) R(¯x). Com-
On the other hand, lower semi-continuity of R entails lim inf k R(x(cid:63)
bining these two inequalities, we deduce that R(¯x) (cid:54) R(˚x). Since R is proper and lsc,
it is bounded from below on bounded sets [51, Corollary 1.10]. Let r = inf k R(x(cid:63)
k)
which then satisﬁes −∞ < r < +∞. Substracting r from (4.5) and multiplying by
λk, one obtains

1
2

||yk − Φx(cid:63)

k||2 (cid:54) 1
2

||yk − Φx(cid:63)

k||2 + λk(R(x(cid:63)

||yk − ˚y||2 + λk(R(˚x) − r). (4.6)

k) − r) (cid:54) 1
2

Consequently, passing to the limit in (4.6) shows that ||˚y − Φ¯x|| = 0, i.e. ¯x is a feasible
point of (P(0, ˚y)). Altogether, this shows that ¯x is a solution of (P(0, ˚y)), and by
uniqueness of the minimizer, ¯x = ˚x.

Lemma 3 (Dual solution convergence). For any sequence of parameters pk =

(λk, yk) such that

(cid:18) ||yk − ˚y||
λk

(cid:19)

, λk

−→ (0, 0),

we have q(cid:63)(pk) → ˚q(0, ˚y).

Proof. By the triangle inequality, we have

||q(cid:63)(λk, yk) − ˚q(0, ˚y)|| (cid:54) ||q(cid:63)(λk, yk) − q(cid:63)(λk, ˚y)|| + ||q(cid:63)(λk, ˚y) − ˚q(0, ˚y)||.

For the ﬁrst term, we notice that q(cid:63)(λ, y) = proxR∗◦Φ∗/λ(y/λ) for λ > 0. Thus,
Lipschitz continuity of the proximal mapping entails that

||q(cid:63)(λk, yk) − q(cid:63)(λk, ˚y)|| (cid:54) ||yk − ˚y||

,

λk

λk
2
λk
2

which in turn shows that q(cid:63)(λk, yk) → q(cid:63)(λk, ˚y). Let us now turn to the second term.
k = q(cid:63)(λk, ˚y) and ˚q def.= ˚q(0, ˚y) ∈ D(0, ˚y), one
Using the respective optimality of q(cid:63)
obtains

− (cid:104)q(cid:63)

k, ˚y(cid:105) +

||q(cid:63)

k||2 + R∗(Φ∗q(cid:63)

k) (cid:54) − (cid:104)˚q, ˚y(cid:105) +

||˚q||2 + R∗(Φ∗˚q)

λk
2

(cid:54) − (cid:104)q(cid:63)

k, ˚y(cid:105) +

||˚q||2 + R∗(Φ∗q(cid:63)

k),

(4.7)

k|| (cid:54) ||˚q||, which shows in particular that (q(cid:63)

whence we get ||q(cid:63)
k)k∈N is a bounded se-
quence. We can thus extract any converging subsequence, which for simplicity, we
denote again q(cid:63)
k → ¯q. Passing to the limit in (4.7), using the fact that R∗ is lsc, one
obtains

− (cid:104)¯q, ˚y(cid:105) + R∗(Φ∗ ¯q) (cid:54) − (cid:104)¯q, ˚y(cid:105) + lim inf

R∗(Φ∗q(cid:63)

k) (cid:54) − (cid:104)˚q, ˚y(cid:105) + R∗(Φ∗˚q),

which shows that ¯q ∈ D(0, ˚y). This together with the fact that ||q(cid:63)
already saw, shows that ¯q = ˚q by uniqueness of ˚q in (4.4).

k|| (cid:54) ||˚q|| as we

k

16

4.3. Sensitivity. We are now in position to state the main result of this section,
which tracks the strata of x(cid:63)(p) in the regime where the perturbation ||w|| = ||y − ˚y||
is suﬃciently small.

Theorem 3. Suppose that ˚x is the unique solution to (P(0, ˚y)). Assume fur-
thermore that R is mirror-stratiﬁable with respect to the primal-dual stratiﬁcations
(M, M∗). If there are constants (C0, C1) depending only on ˚x such that for all p in

{p = (λ, y) : C0||y − ˚y|| (cid:54) λ (cid:54) C1} ,

then there exists a minimizer x(cid:63)(p) of E(·, p) localized as follows

M˚x (cid:54) Mx(cid:63)(p) (cid:54) JR∗ (M ∗

˚u) where ˚u def.= Φ∗˚q(0, ˚y).

(4.8)

(4.9)

Proof. Under (4.8) with for instance C1 small enough, one can easly check
that there exists k large enough such that the regime required for (yk, λk) to ap-
In turn, we have a converging primal-dual pair
ply Lemma 2 and 3 is attained.
(x(cid:63)(pk), Φ∗q(cid:63)(pk)) → (˚x, ˚u). One can then apply Theorem 1 to conclude.

5. Activity Localization with Proximal Splitting Algorithms. Proximal
splitting methods are algorithms designed to solve large-scale structured optimization
and monotone inclusion problems, by evaluating various ﬁrst-order quantities such as
gradients, proximity operators, linear operators, all separately at various points in the
course of an iteration. Though they can show slow convergence each iteration has a
cheap cost. We refer to e.g. [3, 5, 17, 49] for a comprehensive treatment and review.
Capitalizing on our enlarged activity identiﬁcation result for mirror-stratiﬁable
functions, we now instantiate its consequences on ﬁnite activity localization of prox-
imal splitting algorithms. While existing results on ﬁnite identiﬁcation (of a sin-
gle active set) strongly rely on partial smoothness around a non-degenerate cluster
point [34, 42, 43, 41], we examine here intricate situations where neither of these
assumptions holds.

5.1. Forward-Backward algorithm. The Forward–Backward (FB) splitting
method [44] is probably the most well-known proximal splitting algorithm. In our
context, it can be used to solve optimization problems with the additive “smooth +
non-smooth” structure of the form

min
x∈RN

f (x) + R(x),

where f ∈ C 1(RN ) is convex with L-Lipschitz gradient, and R is a proper lsc and
convex function. We assume that Argmin(f + R) (cid:54)= ∅. The FB iteration in relaxed
form reads [16]

xk+1 = (1 − τk)xk + τk proxγkR(xk − γk∇f (xk)),

with γk ∈]0, 2/L[ and τk ∈]0, 1], where proxγR, γ > 0, is the proximal mapping of R,

(5.1)

(5.2)

(5.3)

proxγR(x) def.= argmin
z∈RN

1
2

||z − x||2 + γR(z).

Diﬀerent variants of FB method were studied, and a popular trend is the inertial
schemes which aim at speeding up the convergence (see [48, 4], and the sequence-
convergence version as proved recently in [14]).

17

Under the non-degeneracy assumption −∇f (x(cid:63)) ∈ ri(∂R(x(cid:63))), it was shown in
[42] that FB and its inertial variants correctly identify the active manifold in ﬁnitely
many iterations, and then enter a local linear convergence regime. These results
encompass many special cases such as those studied in [33, 8, 54]. Beyond this non-
degenerate case, we establish now the general localization of active strata.

Theorem 4. Consider the FB iteration (5.2) to solve (5.1) with 0 < inf k γk (cid:54)
supk γk < 2/L and inf k τk > 0. Then xk converges to x(cid:63) ∈ Argmin(f + R). Assume
that R is also mirror-stratiﬁable with respect to (M, M∗), then for k large enough,

Mx(cid:63) (cid:54) Mxk

(cid:54) JR∗ (M ∗

u(cid:63) ) where u(cid:63) def.= −∇f (x(cid:63)).

Proof. Convergence of the sequence (xk)k∈N to x(cid:63) is obtained from [16, Corol-
lary 6.5]. Moreover, since the proximal mapping is the resolvent of the subdiﬀerential,
(5.2) is equivalent to the monotone inclusion

uk+1

def.=

xk − vk+1
γk

− ∇f (xk) ∈ ∂R(vk+1),

def.= xk+1−xk

+ xk. In turn, with the conditions inf k τk > 0 and inf k γk > 0,
where vk+1
and continuity of ∇f , we have vk → x(cid:63) and thus uk → −∇f (x(cid:63)) ∈ ∂R(x(cid:63)). It then
remains to apply Theorem 1 to (vk, uk) and R to conclude.

τk

It can be easily shown that Theorem 4 holds for several extensions of the iterate-

convergent version of FISTA [14]. We omit the details here for the sake of brevity.

We rather take a closer look to the case when the FB scheme (5.2) to solve (P(λ, y))
(see also (4.2)) for λ > 0. Putting together Theorem 3 and 4, we obtain the following
localization result depending only on the data to estimate ˚x assuming that the noise
level ||y − ˚y|| is small enough.

Proposition 4. Under the assumptions of Theorem 3, consider the FB iteration
(5.2) to solve (P(λ, y)) with 0 < inf k γk (cid:54) supk γk < 2λ/||Φ||2 and inf k τk > 0. Then,
for k large enough, the iterates xk satisfy

M˚x (cid:54) Mxk

(cid:54) JR∗ (M ∗

˚u) where ˚u def.= Φ∗q(0, ˚y),

with q(0, ˚y) from (4.4).

Proof. To lighten notation, denote p = (λ, y). As in Theorem 4, we have xk →

x(cid:63)(p) a minimizer of (P(λ, y)). Thus we get

M˚x (cid:54) Mx(cid:63)(p) (cid:54) Mxk ,

where the ﬁrst inequality comes from Theorem 3 and the second one from Theorem 4.
This gives the ﬁrst inequality in the localization result.

Moreover, in the special case at hand ∂R(x(cid:63)(p)) (cid:51) −∇f (x(cid:63)(p)) = Φ∗q(cid:63)(p) → ˚u
as p → (0, ˚y), where we invoked (4.3). It then follows from (2.7) and the fact that
JR∗ is decreasing for the relation (cid:54), that

M ∗
˚u

(cid:54) M ∗

−∇f (x(cid:63)(p)) ⇐⇒ JR∗ (M ∗

−∇f (x(cid:63)(p))) (cid:54) JR∗ (M ∗

˚u).

Using once again Theorem 4, we get

Mxk

(cid:54) JR∗ (M ∗

−∇f (x(cid:63)(p))) (cid:54) JR∗ (M ∗

˚u),

18

which yields the second desired inequality.

Though the iterates xk of FB do not converge to ˚x, this proposition tells us that
the iterates identify an enlarged stratum associated to ˚x. This is an appealing feature
from a practical perspective, since one can often make some prior assumption on the
sought after vector ˚x, such as for instance sparsity or low-rank properties, as we have
illustrated in the numerical experiments of Section 6.

5.2. Douglas-Rachford Splitting Algorithm. The Douglas-Rachford (DR)
method [44] is another popular splitting method designed to minimize convex objec-
tives having the additive “non-smooth + non-smooth” structure of the form

with f and g be proper lsc and convex functions such that ri(dom(f ))∩ri(dom(g)) (cid:54)= ∅
and Argmin(f + g) (cid:54)= ∅. The DR scheme reads

min
x∈RN

f (x) + g(x).






vk+1 = proxγf (2xk − zk) ,
zk+1 = zk + τk (vk+1 − xk) ,
xk+1 = proxγg(zk+1),

(5.4)

(5.5)

where γ > 0, τk ∈]0, 2[ is a relaxation parameter. By deﬁnition, the DR method is
not symmetric with respect to the order of the functions f and/or g. Nevertheless,
all of our statements throughout hold true, with obvious adaptations, when the order
of f and g is reversed in (5.5).

It has been shown in [43] that under appropriate non-degeneracy assumptions,
the DR identiﬁes the active manifolds in ﬁnite time, and then shows a local linear
regime. These results unify all those that were established in the literature for special
problems, see e.g.
[20] for linearly constrained (cid:96)1-minimization, [6] for quadratic or
linear programs, [2] for feasibility with two subspaces. Under mirror-stratiﬁability of
f or g, we get the following enlarged activity identiﬁcation result.

Theorem 5. Consider the DR iteration (5.5) to solve (5.4) with τk ∈]0, 2[
such that (cid:80)
k∈N τk (2 − τk) = +∞. Then zk converges to a ﬁxed point z(cid:63) with
x(cid:63) = proxγg(z(cid:63)) ∈ Argmin(f + g), and xk and vk both converge to x(cid:63). Introduc-
ing u(cid:63) = z(cid:63)−x(cid:63)

, we have furthermore:

γ

(i) If g is mirror-stratiﬁable with respect to the primal-dual stratiﬁcations (M g, M g∗

),

then for k large enough

(ii) If f is mirror-stratiﬁable with respect to the primal-dual stratiﬁcations (M f , M f ∗

),

then for k large enough

M g

x(cid:63) (cid:54) M g
xk

(cid:54) Jg∗ (M g∗

u(cid:63) ).

M f

x(cid:63) (cid:54) M f
vk

(cid:54) Jf ∗ (M f ∗

−u(cid:63) ).

Proof. Under the prescribed choice of τk, convergence of zk is ensured by virtue
of [16, Corollary 5.2]. By non-expansiveness of the proximal mapping, and as we are
in ﬁnite dimension, we also obtain convergence of xk and vk to x(cid:63). To prove (i), note
that the update of xk in (5.5) is equivalent to the monotone inclusion

uk

def.=

∈ ∂g(xk).

zk − xk
γ

19

(5.6)

Since (xk, uk) → (x(cid:63), u(cid:63)), we conclude about (i) by invoking Theorem 1. Similarly,
we note that the update of vk in (5.5) is equivalent to

wk

def.=

2xk − zk − vk+1
γ

∈ ∂f (vk+1).

(5.7)

Using that (vk, wk) → (x(cid:63), −u(cid:63)) and applying Theorem 1 we get (ii).

In the same vein as for FB in the previous section, we now turn to applying
the DR scheme (5.5) to solve (P(λ, y)) by setting in (5.4) f (x) = 1
2λ ||y − Φx||2 and
g(x) = R(x). Putting Theorem 3 and 5-(i) together, we obtain the following analogue
to Proposition 4.

Proposition 5. Under the assumptions of Theorem 3, consider the DR iteration
k∈N τk (2 − τk) = +∞.

(5.5) to solve (P(λ, y)) with γ > 0 and τk ∈]0, 2[ such that (cid:80)
Then, for k large enough, the DR iterates satisfy

M˚x (cid:54) Mxk

(cid:54) JR∗ (M ∗

˚u) where ˚u def.= Φ∗q(0, ˚y).

Proof. The proof follows the same reasoning as that of Proposition 4 by addition-
ally observing (recall the notation in the proof of Theorem 5) that from (5.6)-(5.7),
we have u(cid:63)(p) = −∇f (x(cid:63)(p)) = Φ∗q(cid:63)(p) ∈ ∂R(x(cid:63)(p)).

6. Numerical Illustrations. In this section, we numerically illustrate our the-
oretical results on sensitivity and enlarged activity identiﬁcation in the context of
regularized inverse problems. We adopt the same two “compressed sensing” scenarios
described in Section 1.2. The dimension dim(M˚x) = R0(˚x) of the strata associated
to ˚x is measured as R0 = || · ||0 (resp. R0 = rank) for the (cid:96)1 (resp. nuclear) norm
regularization.

Strata sensitivity. We ﬁrst illustrate the relevance of the strata sensitivity result in
Theorem 3 by studying the dimension of the largest possible active stratum JR∗ (M ∗
˚u)
(in fact its closure). The dual ˚u = Φ∗˚q(0, ˚y) is computed from ˚x by solving the convex
optimization problem (4.4) (using CVX to get a high precision). Thus we know the
maximum complexity index excess predicted by Theorem 3, i.e.

δ(cid:63)(˚x) def.= dim(JR∗ (M ∗

˚u)) − dim(M˚x).

For each given δ and R0(˚x), and among the 1000 randomly generated replications of
(˚x, Φ), we compute the proportion ρ(R0(˚x), δ) of ˚x such that it is the unique solution
of (P(0, ˚y)) and δ(cid:63)(˚x) (cid:54) δ. The proportions ρ(R0(˚x), δ) are displayed in Figure 6.1 as
a function of the input complexity index R0(˚x) = dim(M˚x). The colors from blue to
red correspond to increasing δ.

The proportion ρ(R0(˚x), δ) is an increasing function of δ and a decreasing function
of R0(˚x). Indeed, as anticipated from standard compressed sensing results [58], active
strata M˚x of vectors ˚x whose dimension is small enough compared to the number of
measurements P can be provably and stably recovered with overwhelming probability
(on the sampling of (˚x, Φ, w)). As R0(˚x) increases, the number of measurements P
becomes insuﬃcient to ensure non-degeneracy with high probability, hence preventing
stable recovery of M˚x. However, Theorem 3 predicts that the active stratum of
x(cid:63)(λ, y) for y nearby ˚y is localized between M˚x and JR∗ (M ∗

˚u).

The blue curve in each plot of Figure 6.1 corresponds to δ = 0, which is the
proportion ρ(R0(˚x), 0) of vectors ˚x whose active stratum M˚x can be recovered stably
under small noise perturbation by solving (P(λ, y)) for λ chosen according to (4.8).

20

R = || · ||1

R = || · ||∗

Fig. 6.1. Proportion of ˚x such that δ(cid:63)(˚x) (cid:54) δ as a function of R0(˚x) (increasing value of δ

from 0 to its maximal value is depicted by a color evolving from blue to red).

R = || · ||1

R = || · ||∗

Fig. 6.2. Plots of R0(xk) for the 2000 ﬁrst iterates generated by the Forward-Backward algo-

rithm. Plots in blue correspond to the cases when δ(cid:63)(˚x) = 0, and the red ones for δ(cid:63)(˚x) = δ.

This proportion shows a phase transition phenomenon between stable recovery and
unstable recovery. The location of the phase transition for δ = 0 can be predicted
accurately; see for instance [59].

The red curves in Figure 6.1 correspond to the extreme case where δ takes its
largest achievable value, i.e. where we can guarantee recovery of the largest stratum
JR∗ (M ∗
˚u) with high probability. The phase transition occurs for higher dimension
R0(˚x). The intermediate curves, i.e. from blue to red, correspond to the recovered
strata that are localized between M˚x and JR∗ (M ∗
˚u) (i.e. increasing δ). The phase
transition progressively increases with δ. These curves illustrate and quantify the
typical tradeoﬀ observed in practice: one can allow for more complex input vectors ˚x
(i.e. those with larger R0(˚x)) at the expense of recovering active strata larger than M˚x.
Forward-Backward ﬁnite activity localization. We now numerically illustrate the
ﬁnite enlarged activity identiﬁcation of the FB splitting scheme as predicted by The-
orem 4 and Proposition 4. We remain under the same compressed sensing setting
as before. The randomly generated replications of ˚x are such that R0(˚x) = 10 for
R = || · ||1 and to 4 for R = || · ||∗.

The evolution of the complexity index R0(xk) of the FB iterate xk is shown in
Figure 6.2. The blue lines correspond to several trajectories (the bold one is the
average trajectory), each for a randomly generated instance of ˚x such that δ(cid:63)(˚x) =
21

0, i.e. those vectors whose active strata M˚x can be exactly recovered under small
perturbations. Thus, the iterates identify M˚x in ﬁnite time. The red lines (the bold
one is the average trajectory) are those for which δ(cid:63)(˚x) = δ > 0. As anticipated by
our theoretical results, the iterates identify a stratum strictly larger that M˚x.

Acknowledgements. The work of Gabriel Peyr´e has been supported by the Euro-
pean Research Council (ERC project SIGMA-Vision). Jalal Fadili was partly supported by
Institut Universitaire de France.

Appendix A. Proofs of the results in Section 2.
Proof of Proposition 1. Let us ﬁrst prove the ﬁrst equivalence. Observe that
Deﬁnition 2 can be read as follows: M is active at x if and only if d(x, cl(M )) = 0.
Since there is a ﬁnite number of strata in the stratiﬁcation, let us consider δ the
minimum of the nonzero distances d(x, cl(M (cid:48))) for M (cid:48) not active. For all x(cid:48) in the
open ball of radius δ and of center x, we have

d(x, cl(Mx(cid:48))) (cid:54) ||x − x(cid:48)|| < δ

⇐⇒

d(x, cl(Mx(cid:48))) = 0.

This shows that the set of these strata Mx(cid:48) indeed coincides the set of active strata,
whence we get the ﬁrst equivalence.

Let us turn to the second equivalence. Let M be an active strata at x, that
is, x ∈ cl(M ). Since x ∈ Mx, the intersection cl(M ) ∩ Mx contains x and thus is
nonempty. We deduce from (2.1) that M (cid:62) Mx. Conversely, if M (cid:62) Mx, then
x ∈ Mx ⊂ cl(M ) and therefore M is active at x.

Proof of Proposition 2. From classical convex analysis calculus rules, we get for

all x ∈ dom R

∂R(x) = conv {ai

: i ∈ I max(x)} + cone (cid:8)ai

: i ∈ I feas(x)(cid:9) .

By deﬁnition of MI , the relative interior of ∂R(x) is constant over a stratum: for all
x ∈ MI (with MI (cid:54)= ∅)

ri(∂R(x)) = ri (cid:0) conv {ai
= ri(conv {ai

= ri(conv {ai

: i ∈ I max(x)} + cone (cid:8)ai
: i ∈ I max(x)}) + ri(cone (cid:8)ai
: i ∈ I max}) + ri(cone (cid:8)ai

: i ∈ I feas(x)(cid:9) (cid:1)

: i ∈ I feas(x)(cid:9))

: i ∈ I feas(cid:9)).

This yields JR(MI ) = M ∗
entails for all u ∈ M ∗
I

I . Conversely we have ∂R∗(u) = {x : u ∈ ∂R(x)}, which

∂R∗(u) = (cid:8)x : I max(x) ∩ I feas(x) ⊃ I(cid:9) ,

and therefore ri(∂R∗(u)) = MI . This gives JR∗ (M ∗
Deﬁnition 4.

I ) = MI , which proves item (i) of

To show (ii) of Deﬁnition 4, we make the following observation:

MI (cid:54) MI (cid:48) ⇐⇒ any x ∈ MI lies in cl(MI (cid:48))
max

⇐⇒ I max = I max(x) ⊃ (I (cid:48))
⇐⇒ I ⊃ I (cid:48).

On the other hand we have

and I feas = I feas(x) ⊃ (I (cid:48))

feas

cl(JR(MI )) ⊃ cl(ri(conv {ai

: i ∈ I max})) + cl(ri cone (cid:8)ai

: i ∈ I feas(cid:9)))

= conv {ai

: i ∈ I max} + cone (cid:8)ai

: i ∈ I feas(cid:9) .

22

Note that the ﬁrst ⊃ is in fact = because conv {ai
unique decomposition of a polyhedron, we can write,

: i ∈ I max} is compact. Using

JR(MI ) (cid:62) JR(MI (cid:48)) ⇐⇒ cl(JR(MI )) ⊃ JR(MI (cid:48)) ⇐⇒ I ⊃ I (cid:48),

which ends the proof.

Proof of Proposition 3. The proof builds upon a key result stated in [19]. Theo-
rem 4.6(i) in [19] asserts that the collection (cid:8)σ−1(M sym) : M ∈ M(cid:9) forms a smooth
stratiﬁcation of dom(R) with the desired properties. The fact that spectral func-
tions are mirror-stratiﬁable follows from the polyhedral case with the help of Theo-
rem 4.6(iv) in [19], which states that

JR(σ−1(M sym)) = σ−1(cid:0)JRsym (M sym)(cid:1)

together with continuity of the singular value mapping σ.

REFERENCES

[1] F. Bach. Consistency of trace norm minimization. The Journal of Machine Learning Research,

9(Jun):1019–1048, 2008.

[2] H. Bauschke, J.Y.B. Cruz, T.A. Nghia, H.M. Phan, and X. Wang. The rate of linear convergence
of the douglas-rachford algorithm for subspaces is the cosine of the friedrichs angle. J. of
Approx. Theo., 185(63–79), 2014.

[3] H. H. Bauschke and P. L. Combettes. Convex analysis and monotone operator theory in Hilbert

spaces. Springer, 2011.

[4] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.

[5] A. Beck and M. Teboulle. Gradient-based algorithms with applications to signal recovery.

Convex Optimization in Signal Processing and Communications, 2009.

[6] D. Boley. Local linear convergence of the alternating direction method of multipliers on

quadratic or linear programs. SIAM J. Optim., 23(4):2183–2207, 2013.

[7] J. F. Bonnans and A. Shapiro. Perturbation analysis of optimization problems. Springer Series

in Operations Research and Financial Engineering. Springer Verlag, 2000.

[8] K. Bredies and D. A. Lorenz. Linear convergence of iterative soft-thresholding. Journal of

Fourier Analysis and Applications, 14(5-6):813–837, 2008.

[9] P. B¨uhlmann and S. Van De Geer. Statistics for high-dimensional data: methods, theory and

applications. Springer, 2011.

[10] E. J. Cand`es and C. Fernandez-Granda. Towards a mathematical theory of super-resolution.

Communications on Pure and Applied Mathematics, 67(6):906–956, 2013.

[11] E. J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations of

Computational mathematics, 9(6):717–772, 2009.
[12] E. J. Cand`es and T. Tao. Decoding by linear programming.

Information Theory, IEEE

Transactions on, 51(12):4203–4215, 2005.

[13] E. J. Cand`es and T. Tao. The power of convex relaxation: Near-optimal matrix completion.

Information Theory, IEEE Transactions on, 56(5):2053–2080, 2010.

[14] A. Chambolle and C. Dossal. On the convergence of the iterates of the “fast iterative
shrinkage/thresholding algorithm”. Journal of Optimization Theory and Applications,
166(3):968–982, 2015.

[15] S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM

journal on scientiﬁc computing, 20(1):33–61, 1999.

[16] P. L. Combettes. Solving monotone inclusions via compositions of nonexpansive averaged

operators. Optimization, 53(5-6):475–504, 2004.

[17] P. L. Combettes and J.-C. Pesquet. Proximal splitting methods in signal processing. In H. H.
Bauschke, Burachik R. S., P. L. Combettes, Elser. V., D. R. Luke, and H. Wolkowicz,
editors, Fixed-Point Algorithms for Inverse Problems in Science and Engineering, pages
185–212. Springer, 2011.

[18] M. Coste. An introduction to o-minimal geometry. Technical report, Institut de Recherche

Mathematiques de Rennes, November 1999.

[19] A. Daniilidis, D. Drusvyatskiy, and A. S. Lewis. Orthogonal invariance and identiﬁability.

SIAM Journal on Matrix Analysis and Applications, 35(2):580–598, 2014.

23

[20] L. Demanet and X. Zhang. Eventual linear convergence of the douglas-rachford iteration for

basis pursuit. Mathematics of Computation, 2013. to appear.

[21] A. L. Dontchev. Perturbations, approximations, and sensitivity analysis of optimal control

systems, volume 52. Springer-Verlag, Berlin, 1983.

[22] C. Dossal, M.-L. Chabanol, G. Peyr´e, and J. M. Fadili. Sharp support recovery from noisy ran-
dom measurements by (cid:96)1-minimization. Applied and Computational Harmonic Analysis,
33(1):24–43, 2012.

[23] D. Drusvyatskiy, A. D. Ioﬀe, and A. S. Lewis. Generic minimizing behavior in semialgebraic

optimization. SIAM J. Optim., 26(1):513–534, 2016.

[24] D. Drusvyatskiy and A.S. Lewis. Optimality, identiﬁability, and sensitivity. Mathematical

Programming, to appear in, pages 1–32, 2013.

[25] V. Duval and G. Peyr´e. Exact support recovery for sparse spikes deconvolution. Foundations

of Computational Mathematics, 15(5):1315–1355, 2015.

[26] V. Duval and G. Peyr´e. Sparse spikes deconvolution on thin grids. Preprint 01135200, HAL,

2015.

[27] C. Ekanadham, D. Tranchina, and E. P. Simoncelli. A uniﬁed framework and method for
automatic neural spike identiﬁcation. Journal of Neuroscience Methods, 222:47 – 55, 2014.
[28] M. Fazel. Matrix Rank Minimization with Applications. PhD thesis, Stanford University, 2002.
[29] A. V. Fiacco and G. P. McCormick. Nonlinear Programming: Sequential Unconstrained Min-

imization Techniques. Wiley, New York, 1968. reprinted, SIAM, Philadelphia, 1990.

[30] J.-J. Fuchs. On sparse representations in arbitrary redundant bases. Information Theory, IEEE

Transactions on, 50(6):1341–1344, 2004.

[31] Michael Grant and Stephen Boyd. CVX: Matlab software for disciplined convex programming,

version 2.1. http://cvxr.com/cvx, March 2014.

[32] M. Grasmair, O. Scherzer, and M. Haltmeier. Necessary and suﬃcient conditions for lin-
ear convergence of l1-regularization. Communications on Pure and Applied Mathematics,
64(2):161–182, 2011.

[33] E. Hale, W. Yin, and Y. Zhang. Fixed-point continuation for (cid:96)1-minimization: methodology

and convergence. SIAM J. Optim., 19(3):1107–1130, 2008.

[34] W. L. Hare.

Identifying active manifolds in regularization problems.

In H. H. Bauschke,
R. S., Burachik, P. L. Combettes, V. Elser, D. R. Luke, and H. Wolkowicz, editors, Fixed-
Point Algorithms for Inverse Problems in Science and Engineering, volume 49 of Springer
Optimization and Its Applications, chapter 13. Springer, 2011.

[35] W. L. Hare and A. S. Lewis. Identifying active constraints via partial smoothness and prox-

regularity. J. Convex Anal., 11(2):251–266, 2004.

[36] J.-B. Hiriart-Urruty and H. Y. Le. Convexifying the set of matrices of bounded rank: appli-
cations to the quasiconvexiﬁcation and convexiﬁcation of the rank function. Optimization
Letters, 6(5):841–849, 2012.

[37] Hai Yen Le. Confexifying the Counting Function on Rp for Convexifying the Rank Function

on Mm,n(R). Journal of Convex Analysis, 19(2):519–524, 2012.

[38] C. Lemar´echal, F. Oustry, and C. Sagastiz´abal. The U -lagrangian of a convex function. Trans.

Amer. Math. Soc., 352(2):711–729, 2000.

[39] A. S. Lewis. Active sets, nonsmoothness, and sensitivity. SIAM J. Optim., 13(3):702–725,

[40] A. S. Lewis and S. Zhang. Partial smoothness, tilt stability, and generalized hessians. SIAM

[41] J. Liang. Convergence Rates of First-Order Operator Splitting Methods. Theses, Normandie

2002.

J. Optim., 23(1):74–94, 2013.

Universit´e, October 2016.

[42] J. Liang, J. Fadili, and G. Peyr´e. Activity identiﬁcation and local linear convergence of forward–

backward-type methods. SIAM J. Optim., 27(1):408–437, 2017.

[43] J. Liang, J. Fadili, and G. Peyr´e. Local convergence properties of douglas–rachford and alter-
nating direction method of multipliers. Journal of Optimization Theory and Applications,
172(3):874–913, 2017.

[44] P. L. Lions and B. Mercier. Splitting algorithms for the sum of two nonlinear operators. SIAM

Journal on Numerical Analysis, 16(6):964–979, 1979.

[45] S. G. Mallat. A wavelet tour of signal processing. Elsevier, third edition, 2009.
[46] S. A. Miller and J. Malick. Newton methods for nonsmooth convex minimization: connections
among-Lagrangian, Riemannian Newton and SQP methods. Mathematical programming,
104(2-3):609–633, 2005.

[47] B. S. Mordukhovich. Sensitivity analysis in nonsmooth optimization.

In D. A. Field and
V. Komkov, editors, Theoretical Aspects of Industrial Design, volume 58, pages 32–46.
SIAM Volumes in Applied Mathematics, 1992.

24

[48] Y. Nesterov. A method for solving the convex programming problem with convergence rate

O(1/k2). Dokl. Akad. Nauk SSSR, 269(3):543–547, 1983.

[49] N. Parikh and S. P. Boyd. Proximal algorithms. Foundations and Trends in Optimization,

1(3):123–231, 2013.

university press, 1970.

[50] R. T. Rockafellar. Convex analysis. Number 28 in Princeton Mathematical Series. Princeton

[51] R. T. Rockafellar and R. Wets. Variational analysis, volume 317. Springer, Berlin, 1998.
[52] O. Scherzer. Variational methods in imaging, volume 167. Springer, 2009.
[53] J.-L. Starck and F. Murtagh. Astronomical Image and Data Analysis. Springer, 2006.
[54] S. Tao, D. Boley, and S. Zhang. Local linear convergence of ISTA and FISTA on the LASSO

problem. arXiv preprint arXiv:1501.02888, 2015.

[55] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical

Society. Series B. Methodological, 58(1):267–288, 1996.

[56] S. Vaiter. Low Complexity Regularization of Inverse Problems. Theses, Universit´e Paris

Dauphine - Paris IX, July 2014.

[57] S. Vaiter, M. Golbabaee, J. Fadili, and G. Peyr´e. Model selection with low complexity priors.

Information and Inference: A Journal of the IMA, 4(3):230, 2015.

[58] S. Vaiter, G. Peyr´e, and J. Fadili. Low complexity regularization of linear inverse prob-
lems. In G¨otz Pfander, editor, Sampling Theory, a Renaissance, pages 103–153. Springer-
Birkh¨auser, 2015.

[59] S. Vaiter, G. Peyr´e, and J. Fadili. Model consistency of partly smooth regularizers. IEEE

Trans. Inf. Theory, 64(3):1725 – 1737, 2018.

[60] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables.

Journal of the Royal Statistical Society: Series B, 68(1):49–67, 2005.

[61] P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning

Research, 7:2541–2563, 2006.

25

8
1
0
2
 
n
u
J
 
5
 
 
]

C
O
.
h
t
a
m

[
 
 
3
v
4
9
1
3
0
.
7
0
7
1
:
v
i
X
r
a

SENSITIVITY ANALYSIS
FOR MIRROR-STRATIFIABLE CONVEX FUNCTIONS

JALAL FADILI∗, J´ER ˆOME MALICK† , AND GABRIEL PEYR´E‡

Abstract. This paper provides a set of sensitivity analysis and activity identiﬁcation results for
a class of convex functions with a strong geometric structure, that we coined “mirror-stratiﬁable”.
These functions are such that there is a bijection between a primal and a dual stratiﬁcation of the
space into partitioning sets, called strata. This pairing is crucial to track the strata that are identi-
ﬁable by solutions of parametrized optimization problems or by iterates of optimization algorithms.
This class of functions encompasses all regularizers routinely used in signal and image processing,
machine learning, and statistics. We show that this “mirror-stratiﬁable” structure enjoys a nice
sensitivity theory, allowing us to study stability of solutions of optimization problems to small per-
turbations, as well as activity identiﬁcation of ﬁrst-order proximal splitting-type algorithms. Existing
results in the literature typically assume that, under a non-degeneracy condition, the active set asso-
ciated to a minimizer is stable to small perturbations and is identiﬁed in ﬁnite time by optimization
schemes. In contrast, our results do not require any non-degeneracy assumption:
in consequence,
the optimal active set is not necessarily stable anymore, but we are able to track precisely the set
of identiﬁable strata.We show that these results have crucial implications when solving challenging
ill-posed inverse problems via regularization, a typical scenario where the non-degeneracy condition
is not fulﬁlled. Our theoretical results, illustrated by numerical simulations, allow us to characterize
the instability behaviour of the regularized solutions, by locating the set of all low-dimensional strata
that can be potentially identiﬁed by these solutions.

Key words.

convex analysis, inverse problems, sensitivity, active sets, ﬁrst-order splitting

algorithms, applications in imaging and machine learning

AMS subject classiﬁcations. 65K05, 65K10, 90C25, 90C31.

1. Introduction. Variational methods and non-smooth optimization algorithms
are ubiquitous to solve large-scale inverse problems in various ﬁelds of science and
engineering, and in particular in data science. The non-smooth structure of the op-
timization problems promotes solutions conforming to some notion of simplicity or
low-complexity (e.g. sparsity, low-rank, etc.). The low-complexity structure is often
manifested in the form of a low-dimensional “active set”.
It is thus of prominent
interest to be able to quantitatively characterize the stability of these active sets to
perturbations of the objective function. Of crucial importance is also the identiﬁ-
cation in ﬁnite time of these active sets by iterates of optimization algorithms that
numerically minimize the objective function. This type of problems and results is
referred to as “activity identiﬁcation”; a review of the relevant literature will be pro-
vided in the sequel (at the beginning of each section).
In a nutshell, the existing
identiﬁcation results guarantee a perfect stability of the active set to perturbations,
or a ﬁnite identiﬁcation of this active set via algorithmic schemes, under some non-
degeneracy conditions (see in particular [39, 24, 58]). The crucial non-degeneracy
assumption, that takes generally the form (3.1), can be viewed as a geometric gen-
eralization of strict complementary in non-linear programming. However, as we will
illustrate shortly through some preliminary numerics, such a condition is too strin-
gent and is often barely veriﬁed. The goal of this paper is to investigate the situation
where this non-degeneracy assumption is violated.

∗Normandie Univ, ENSICAEN, CNRS, GREYC, France.
†CNRS and LJK, Grenoble, France
‡CNRS and DMA, ENS Paris, France.

1

1.1. Motivating Examples. In order to better grasp the relevance of our anal-
ysis, let us ﬁrst start with the setting of inverse problems that pervades various ﬁelds
including signal processing and machine learning. We will come back to this setting
in Section 4 with further discussions and references.

Regularized Inverse Problems. Assume one observes

y = ˚y + w ∈ RP where ˚y def.= Φ˚x

(1.1)

where w is some perturbation (called noise) and Φ ∈ RP ×N (called forward operator,
or design matrix in statistics). Solving an inverse problem amounts to recovering
˚x, to a good approximation, knowing y and Φ according to (1.1). Unfortunately, in
general, P can be much smaller than the ambient dimension N , and when P = N ,
the mapping Φ is in general ill-conditioned or even singular.

A classical approach is then to assume that ˚x has some ”low-complexity”, and
to use a prior regularization R promoting solutions with such low-complexity. This
leads to the following optimization problem

E(x, (λ, y)) def.= R(x) +

||y − Φx||2,

(P(λ, y))

min
x∈RN

1
2λ

Let us discuss two popular examples of regularizing functions promoting a low-complexity
structure. These two functions will be used in our numerical experiments.

Example 1 ((cid:96)1 norm). For x ∈ RN , its (cid:96)1 norm reads

R(x) = ||x||1

def.=

|xi|,

N
(cid:88)

i=1

(1.2)

where xi is the i-th entry of x. As advocated for instance by [15, 55], the (cid:96)1 enforces
to have a small number of non-zero
the solutions of (P(λ, y)) to be sparse, i.e.
components. Indeed, the (cid:96)1 norm can be shown to be the tightest convex relaxation
(in the sense of bi-conjugation) of the (cid:96)0 pseudo-norm restricted to the unit Euclidian
ball [37]. Recall that (cid:96)0 pseudo-norm of x ∈ RN measures its sparsity

||x||0

def.= (cid:93) (cid:8)i ∈ {1, . . . , N } : xi (cid:54)= 0(cid:9) .

Sparsity has witnessed a huge surge of interest in the last decades. For instance, in
signal and imaging sciences, one can approximate most natural signals and images
In statistics,
using sparse expansions in an appropriate dictionary (see e.g. [45]).
sparsity is a key toward model selection and interpretability [9].

Example 2 (Nuclear norm). For a matrix x ∈ Rn1×n2 ∼ RN , where N = n1n2,

the nuclear norm is deﬁned as

||x||∗

def.= ||σ(x)||1 =

σi(x),

n
(cid:88)

i=1

where n def.= min(n1, n2) and σ(x) = (σ1(x), . . . , σn(x)) ∈ Rn
+ is the vector of singular
values of x. The nuclear norm is the tightest convex relaxation of the rank (in the sense
of bi-conjugation) restricted to the unit Frobenius ball [36]. This underlies its wide use
to promote solutions of (P(λ, y)) with low rank, where we recall rank(x) def.= ||σ(x)||0.
Low-rank regularization has proved useful for a variety of applications, including con-
trol theory and machine learning; see e.g. [28, 1, 11]

2

Active set(s) identiﬁcation. The above discussed regularizers R are non-smooth
convex functions. This non-smoothness arises in a highly structured fashion and is
usually associated, locally, with some low-dimensional active subset of RN (in many
cases, such a subset is an aﬃne or a smooth manifold). Thus R will favor solutions
of (P(λ, y)) that lie in a low-dimensional active set and would allow the inversion of
the system (1.1) in a stable way. More precisely, one would like that under small per-
turbations w, the solutions of (P(λ, y)) move stably along the active set. A byproduct
of this behaviour is that from an algorithmic perspective, if an optimization algorithm
is used to solve (P(λ, y)), one would hope that the iterates of the scheme identify the
active set in ﬁnite time.

Identifying the low-dimensional active set in a stable way is highly desirable for
several reasons. One reason is that it is a fundamental property most practitioners are
looking for. Typical examples include neurosciences [27] where the goal is to recover
a spike train from neural activity, or astrophysics [53] where it is desired to separate
stars from a background.
In both examples, sparsity can be used as a modeling
hypothesis and the recovery method should comply with it in a stable way. A second
reason is algorithmic since one can also take advantage of the low-dimensionality of
the identiﬁed active set to reduce computational burden and memory storage, hence
opening the door to higher-order acceleration of optimization algorithms (see [38, 46]).
Unfortunately, this desirable behaviour rarely occurs in practical applications.
Yet one still observes some form of partial stability (to be given a rigorous meaning
in Section 3 and 4), as conﬁrmed by the numerical experiment of the next paragraph.

1.2. Illustrative numerical experiment. We consider a simple “compressed
sensing” scenario, with the (cid:96)1 norm R = || · ||1 [12] and the nuclear norm R = || · ||∗ [13]
as regularizers. The operator Φ ∈ RN ×P is drawn uniformly at random from the
standard Gaussian ensemble, i.e. the entries of Φ are independent and identically
distributed Gaussian random variables with zero-mean and unit variance. In the case
R = || · ||1, we set (N, P ) = (100, 50) and the vector to recover ˚x is drawn uniformly
at random among sparse vectors with R0(˚x) def.= ||˚x||0 = 10 and unit non-zero entries.
In the case R = || · ||∗, we set (N, P ) = (400, 300) (n1 = n2 = n = 20) and the
matrix to recover ˚x is drawn uniformly at random among low-rank matrices with
R0(˚x) def.= rank(˚x) = 4 and unit non-zero singular values. For each problem suite
(R, N, P ), 1000 realizations of (˚x, Φ, w) are drawn, and y is then generated according
to (1.1). The entries of the noise vector w are drawn uniformly from a Gaussian with
standard deviation 0.1, we set λ = 0.28 for R = || · ||1 and λ = 10 for R = || · ||∗.

For each realization of (˚x, Φ, w), we solve the associated problem (P(λ, y)) using
CVX [31] to get a high precision; denote x(cid:63)(λ, y) the obtained solutions. We also
solve (P(λ, y)) by the Forward-Backward (FB) scheme which reads in this case1

xk+1 = proxλγR(xk + γΦ∗(y − Φxk)).
In our setting, with γ = 1.8/σmax(Φ∗Φ) and non-emptiness of Argmin(E(·, λ, y), it is
well-known that the sequence (xk)k∈N converges to a point in Argmin(E(·, λ, y)).

The top row of Figure 1.1 displays the histogram of the complexity index excess
δ def.= R0(x(cid:63)(λ, y)) − R0(˚x) which clearly shows that we do not have exact stability

1The deﬁnition of the proximal mapping proxτ R (for τ > 0) is given in (5.3). The proximal

mappings of the (cid:96)1 and nuclear norms are

proxµ||·||1

(x) = (cid:0)sign(xi) max(0, |xi| − µ)(cid:1)

i , proxµ||·||∗ (x) = U diag(proxµ||·||1

(σ(x)))V ∗,

where x = U diag(σ(x))V ∗ is a SVD decomposition of x.

3

δ

f
o

s

m
a
r
g
o
t
s
i
H

k

n
o
i
t
a
r
e
t
i

s
v

)
k
x
(
0
R

R = || · ||1

R = || · ||∗

Fig. 1.1. Top row: Histogram of the complexity index excess δ = R0(x(cid:63)(λ, y)) − R0(˚x) where
x(cid:63)(λ, y) is the solution of (P(λ, y)) with noisy observation y = ˚y + w for a small perturbation w and
a well-chosen parameter value λ = c0||w||. Bottom row: Evolution of the complexity index R0(xk)
where (xk)k∈N is the FB iterates sequence converging to some x(cid:63)(λ, y) ∈ Argmin(E(·, λ, y)).

Indeed, although R0(x(cid:63)(λ, y)) is still rather small, its
under the perturbations w.
value is in most cases larger than the complexity index R0(˚x). The bottom row of
Figure 1.1 depicts the evolution with k of the complexity index R0(xk) for two random
instances of ˚x (in blue and red) corresponding to two diﬀerent values of δ. One can
observe that the iterates identify in ﬁnite time some low-dimensional active set with
a complexity index strictly larger than the one associated to ˚x (the red curves). We
will provide a more detailed discussion of this phenomenon in Section 6.

As we will emphasize in the review of previous work, existing results on sensitivity
analysis and low-complexity regularization only focus the case where R0(x(cid:63)(λ, y)) =
R0(˚x) (i.e. δ = 0), whose underlying claim is that the low-complexity model is per-
fectly stable to perturbations, which typically requires some non-degeneracy assump-
tion to hold. In many situations, however, including the compressed sensing scenario
described above, but also in super-resolution imaging (see [26]) and other challenging
inverse problems, non-degeneracy-type hypotheses are too restrictive. It is the goal
of this paper to develop a more general sensitivity analysis, that goes beyond the
non-degenerate case, to improve our understanding of stability to perturbations of
low-complexity regularized inverse problems.

1.3. Contributions and Outline. Our ﬁrst contribution consists in introduc-
ing, in Section 2, a class of proper lower-semicontinuous (lsc) convex functions that
we coin “mirror-stratiﬁable”. The subdiﬀerential of such a function induces a primal-
dual pairing of stratiﬁcations, which is pivotal to track identiﬁable strata. We discuss
several examples of functions enjoying such a structure. With this structure at hand,
we then turn to the main result of this paper, formalized in Theorem 1, and on which

4

all the others rely. This result shows ﬁnite enlarged activity identiﬁcation for a mirror-
stratiﬁable function without the need of any non-degeneracy condition. In addition,
the identiﬁable strata are precisely characterized in terms primal-dual optimal solu-
tions. Sections 3, 4 and 5 instantiate this abstract result for a set of concrete problems,
respectively: sensitivity of composite ”smooth+non-smooth” optimization problems
(Theorem 2), enlarged activity identiﬁcation by proximal splitting schemes such as
Forward-Backward and Douglas-Rachford algorithms (Theorems 4 and 5), and ﬁnally,
enlarged identiﬁcation for regularized inverse problems (Theorem 3). Before stating
these results, we make a short review of the associated literature. Finally Section 6
illustrates these theoretical ﬁndings with numerical experiments, in particular in a
compressed sensing scenario involving the (cid:96)1 and nuclear norms as regularizers. Fol-
lowing the philosophy of reproducible research, all the code to reproduce the ﬁgures
of this article is available online2.

2. Mirror-Stratiﬁable Functions. In this section, we introduce the class of
mirror-stratiﬁable convex functions, and present the sensitivity property they provide
(Theorem 1). We also illustrate that many popular regularizers used in data science
are mirror-stratiﬁable. Most the results of this section (in particular all the examples)
are easy to obtain by basic calculus; we therefore do not give these proofs in the text
and we gather some of them in Appendix A.

2.1. Stratiﬁcations and deﬁnitions. We start with recalling the following

standard deﬁnition of stratiﬁcation.

Definition 1 (Stratiﬁcation). A stratiﬁcation of a set D ⊂ RN is a ﬁnite
partition M = {Mi}i∈I such that for any partitioning sets (called strata) M and M (cid:48)
we have

M ∩ cl(M (cid:48)) (cid:54)= ∅ =⇒ M ⊂ cl(M (cid:48)).

If the strata are open polyhedra, then M is a polyhedral stratiﬁcation, and if they are
C 2-smooth manifolds then M entails a C 2-stratiﬁcation.

A stratiﬁcation is naturally endowed with the partial ordering (cid:54) in the sense that

M (cid:54) M (cid:48) ⇐⇒ M ⊂ cl(M (cid:48)) ⇐⇒ M ∩ cl(M (cid:48)) (cid:54)= ∅.

(2.1)

The relation is clearly reﬂexive and transitive. Furthermore, we have

cl(M ) =

(cid:91)

M (cid:48).

M (cid:48)(cid:54)M

(2.2)

An immediate consequence of Deﬁnition 1 is that for each point x ∈ D, there
is a unique stratum containing x, denoted Mx. Indeed, suppose that there are two
non-empty open strata M1 and M2 such that M1 ∩ M2 = {x}, and thus M1 ∩ M2 (cid:54)= ∅.
This implies, using (2.1), that M1 (cid:54) M2 and M2 (cid:54) M1, and thus M1 = M2.

At this stage, it is worth emphasizing that the strata are not needed to be mani-
folds in the rest of the paper. It is however the case that in many practical cases that
we will discuss, strata are indeed manifolds, and sometimes aﬃne manifolds.

Example 3 (Polyhedral sets and functions). A partition of a polyhedral set into
its open faces induces a natural ﬁnite polyhedral stratiﬁcation of it. In turn, let R :

2Code available at https://github.com/gpeyre/2017-SIOPT-stratification

5

RN → R def.= R∪{+∞} be a polyhedral function, and consider a polyhedral stratiﬁcation
of its epigraph, which is a polyhedral set in RN +1. Projecting all polyhedral strata onto
the ﬁrst N -coordinates one obtains a ﬁnite polyhedral stratiﬁcation of dom(R).

Remark 1. The previous example extends to semialgebraic sets and functions,
which are known to induce stratiﬁcations into ﬁnite disjoint unions of manifolds. In
fact, this holds for any tame class of sets/functions; see, e.g., [18].

We now single out a speciﬁc set of strata, called active strata, that will play a

central role for ﬁnite enlarged activity identiﬁcation purposes.

Definition 2 (Active strata). Given a stratiﬁcation M of D ⊂ RN and a point

x ∈ D, a stratum M ∈ M is said active at x if x ∈ cl(M ).

Proposition 1. Given a stratiﬁcation M of D and a point x ∈ D. Then there
exists δ > 0 such that set of strata Mx(cid:48) for any x(cid:48) such that ||x(cid:48) − x|| < δ, coincides
with the set of strata M (cid:62) Mx and with the set of active strata at x.

2.2. Mirror-Stratiﬁable functions. Following [19, Section 4], we deﬁne the

key correspondence operator whose role will become apparent shortly.

Definition 3. Let R : RN → R be a proper lsc convex function. The associated

correspondence operator JR : 2RN

→ 2RN

JR(S) def.=

is deﬁned as
(cid:91)

ri(∂R(x)),

x∈S

where ∂R is the subdiﬀerential of R.

Observe that, by deﬁnition, JR is increasing for set inclusion

S ⊂ S(cid:48) =⇒ JR(S) ⊂ JR(S(cid:48)).

(2.3)

For this operator to be useful in sensitivity analysis, we will further impose that JR
is decreasing for the partial ordering (cid:54) in (2.1), as captured in our main deﬁnition.
This is a key requirement that captures the intuitive idea that the larger a primal
stratum, the smaller its image by JR in the dual space.

In the following, we will denote R∗ the Legendre-Fenchel conjugate of R.
Definition 4 (Mirror-stratiﬁable functions). Let R : RN → R be a proper lsc
convex function; we say that R is mirror-stratiﬁable with respect to a (primal) strat-
iﬁcation M = {Mi}i∈I of dom(∂R) and a (dual) stratiﬁcation M∗ = {M ∗
i }i∈I of
dom(∂R∗) if the following holds:

(i) Conjugation induces a duality pairing between M and M∗, and JR : M →

M∗ is invertible with inverse JR∗ , i.e. ∀M ∈ M, M ∗ ∈ M∗ we have

M ∗ = JR(M ) ⇐⇒ JR∗ (M ∗) = M.

(ii) JR is decreasing for the relation (cid:54): for any M and M (cid:48) in M

M (cid:54) M (cid:48) ⇐⇒ JR(M ) (cid:62) JR(M (cid:48)).

The primal-dual stratiﬁcations that make R mirror-stratiﬁable are not unique, as
will be exempliﬁed in Remark 3. Though the deﬁnition is formal and the assumptions
looks restrictive, this class include many useful examples, as illustrated in the next
section. We ﬁnish this section with a remark, used in the sequel.

Remark 2 (Separability). For each m = 1, . . . , L, suppose that the proper lsc
convex function Rm : RNm → R is mirror-stratiﬁable with respect to stratiﬁcations
Mm and M∗
m. Then it is easy to show, using standard subdiﬀerential and conjugacy
calculus, that the function R : (xm)1(cid:54)m(cid:54)L ∈ RN1 × · · · × RNL (cid:55)→ (cid:80)L
m=1 Rm(xm) is
mirror-stratiﬁable with stratiﬁcations M1 × · · · × ML and M∗

1 × · · · × M∗
L.

6

2.3. Examples. The notion of a mirror-stratiﬁable function looks quite rigid.
However, many of the regularization functions routinely used in data science are
mirror-stratiﬁable. Let us provide some relevant examples in this section. In partic-
ular, the (cid:96)1-norm and the nuclear norm will be used in the numerical experiments.

2.3.1. Legendre functions. A lsc convex function R : RN → R is said to be a
Legendre function (see [50, Chapter 26]) if (i) it is diﬀerentiable and strictly convex
on the interior of its domain int(dom(R)) (cid:54)= ∅, and (ii) ||∇R(xk)|| → +∞ for every
sequence (xk)k∈N ⊂ int(dom(R)) converging to a boundary point of dom(R). Many
functions in convex optimization are Legendre; most notably, quadratic functions
and the log barrier of interior point methods. It was shown in [50, Theorem 26.5]
that R is Legendre if and only if its conjugate R∗ is Legendre, and that in this case
∇R is a bijection from int(dom(R)) to int(dom(R∗)) with ∇R∗ = (∇R)−1. As a
consequence, a Legendre function R is mirror-stratiﬁable with M = {int(dom(R))}
and M∗ = {int(dom(R∗))}.

2.3.2. (cid:96)1-norm. Let R : x ∈ R (cid:55)→ |x|, whose conjugate R∗ = ι[−1,1]. It follows
that R is mirror-stratiﬁable with M = { ] − ∞, 0[, {0}, ]0, +∞[} and M∗ = {{−1}, ] −
1, 1[, {+1}}. Using Remark 2, the next result is clear.

Lemma 1. The (cid:96)1-norm and its conjugate ι[−1,+1]N are mirror-stratiﬁable with re-
spect to the stratiﬁcations M = { ] − ∞, 0[, {0}, ]0, +∞[}N of RN and M∗ = {{−1}, ]−
1, 1[, {+1}}N of [−1, +1]N .

A graphical illustration of this lemma in dimension 2 is displayed in Figure 2.1(a).
Remark 3 (Non-uniqueness of stratiﬁcations).
In general, we do not have
uniqueness of stratiﬁcations inducing mirror-stratiﬁable functions. To illustrate this,
let us consider the (cid:96)1 norm. Take the partitions M = { ] − ∞, 0[∪]0, +∞[, {0}}N
of RN and M∗ = {{−1, +1}, ] − 1, 1[}N . These are valid stratiﬁcations though
the strata are not connected. Other possible valid stratiﬁcations are given by strata
Mj = (cid:8)x ∈ RN : ||x||0 = j(cid:9) and M ∗
j = J||·||1 (Mj), for j = 0, 1, . . . , N . It is immediate
to check that the (cid:96)1-norm is also mirror-stratiﬁable with respect to these stratiﬁcations.
However, as devised above, it is in general not wise to take such large strata as they
lead to less sharp localization and sensitivity results.

Remark 4 (Instability under the sum rule). The family of mirror-stratiﬁable
functions is unfortunately not stable under the sum. As a simple counter-example,
consider the pair of conjugate functions on R: R(x) = |x| + x2/2 and R∗(u) =
(max{|u| − 1, 0})2/2. We obviously have dom(∂R) = dom(∂R∗) = R. However we
observe that JR(R) = R \ {−1, 1}. This yields that JR cannot be a pairing between
any two stratiﬁcations of R, and therefore R cannot be mirror-stratiﬁable.

2.3.3. (cid:96)1,2 norm. The (cid:96)1,2 norm, also known as the group Lasso regularization,
has been advocated to promote group/block sparsity [60], i.e.
it drives all the coef-
ﬁcients in one group to zero together. Let B a non-overlapping uniform partition of
{1, . . . , N } into K blocks. The (cid:96)1,2 norm induced by the partition B reads

||x||B =

||xB||2

(cid:88)

B∈B

where xB is the restriction of x to the entries indexed by the block B. This is again
a separable function of the xB’s. One can easily show that the (cid:96)2 norm on R|B| and
its conjugate, the indicator of the unit (cid:96)2-ball B
in R|B|, are mirror-stratiﬁable
with respect to the stratiﬁcations {{0}, R|B| \ {0}} of RN and {S|B|−1

, int(B

(cid:96)|B|
2

)}

(cid:96)2

(cid:96)|B|
2

7

(a)

(b)

8

Fig. 2.1. Graphical illustration of mirror-stratiﬁcation for two norms: (a) (cid:96)1 norm in dimen-

sion 2; (b) (cid:96)1,2 norm in dimension 3 (with two blocks, of respectively sizes 1 and 2).

(cid:96)2

(cid:96)|B|
2

), where S|B|−1

of B
is the corresponding unit sphere. In turn, the (cid:96)1,2 norm is
mirror-stratiﬁable with respect to the stratiﬁcations M = {{0}, R|B| \ {0}}K and
M∗ = {S|B|−1, int(B
)}K. An illustration of this result in dimension 3 is portrayed
in Figure 2.1(b).

(cid:96)|B|
2

2.3.4. Nuclear norm. Let us come back to the nuclear norm deﬁned in Exam-
ple 2. For simplicity, we assume n1 = n2 = n. Let M be a stratum of the (cid:96)1 norm
stratiﬁcation. In the following, we denote by M sym its symmetrization. We observe
that σ−1(M sym) = {X ∈ Rn×n : rank(X) = ||z||0, z ∈ M }. So many inverse images
σ−1(M sym) of strata M sym coincide. This suggests the following n + 1 stratiﬁcations:
for a given i ∈ {0, . . . , N }

Mi = (cid:8)X ∈ Rn×n : rank(X) = i(cid:9)
i = {U ∈ Rn : σ1(U ) = · · · = σi(U ) = 1, ∀j > i, |σj(U )| < 1} .
M ∗

This yields that || · ||∗ is mirror-stratiﬁable with respect to these stratiﬁcations.

2.3.5. Polyhedral Functions. We here establish mirror-stratiﬁability of poly-
hedral functions, including the (cid:96)1 norm, the (cid:96)∞ norm and anisotropic TV semi-norm.

A polyhedral function R : RN → R can be expressed as

R(x) = max

{(cid:104)ai, x(cid:105) − αi} + ι(cid:84)

i=k+1,...,m{x : (cid:104)ai, x(cid:105)−αi(cid:54)0}(x).

(2.4)

i=1,...,k

For any x ∈ dom(R), introduce the two sets of indices

I max(x) = {i = 1, . . . , m : (cid:104)ai, x(cid:105) − αi = R(x)} ,
I feas(x) = {i = 1, . . . , m : (cid:104)ai, x(cid:105) = αi} .

For a given index set I ⊂ {1, . . . , m}, we consider the aﬃne manifold

MI

def.= (cid:8)x ∈ dom(R) : I max(x) ∩ I feas(x) = I(cid:9) .

(2.5)

We see that some MI may be empty and that M = {MI }I is a stratiﬁcation of
dom(R). The stratum MI is characterized by the optimality part I max = I ∩{1, . . . , k}
and the feasibility part I feas = I ∩ {k + 1, . . . , m} of I. Similarly we deﬁne

M ∗

I = ri(conv {ai

: i ∈ I max}) + ri(cone (cid:8)ai

: i ∈ I feas(cid:9)).

Let us formalize in the next proposition a result alluded to in [19].

Proposition 2. A polyhedral function R is mirror-stratiﬁable with respect to its

naturally induced stratiﬁcations {MI }I and {M ∗

I }I .

Remark 5 (Back to || · ||1). As we anticipated, Proposition 2 subsumes Lemma 1

as a special case. To see this, observe that

||x||1 = max
||u||∞(cid:54)1

(cid:104)u, x(cid:105) =

max
u∈{−1,+1}N

(cid:104)u, x(cid:105) ,

which is of the form (2.4) with k = m = 2N . Thus there are 22N
aﬃne manifolds as
deﬁned by (2.5). But many of them are empty: there is only 3N (non-empty, distinct)
manifolds in the stratiﬁcation of RN , and they coincide with those in Lemma 1.

2.3.6. Spectral Lifting of Polyhedral Functions. As we discussed in Re-
mark 4, JR may fail to induce a duality pairing between stratiﬁcations of R and R∗,
in which case R cannot be mirror-stratiﬁable. Hence, for this type of duality to hold,
one needs to impose stringent strict convexity conditions. To avoid this, and still
aﬀord a large class of mirror-stratiﬁable functions that are of utmost in applications,
we consider spectral lifting of polyhedral functions, in the same vein as [19] did it for
partial smoothness.

A matrix function R : RN =n×n → R is said to be a spectral lift of a polyhedral
function if there exists a polyhedral function Rsym : Rn → R, invariant under signed
permutation of its coordinates, such that R = Rsym ◦ σ where σ computes the singular
values of a matrix. Associated to M = {MI }I the (polyhedral) stratiﬁcation induced
by Rsym, we consider its symmetrized stratiﬁcation. We deﬁne the symmetrization of
M ∈ M, as the set M sym

M sym = (cid:8)x ∈ RN : ∃y ∈ M such that for all i there exists j with |xi| = |yj|(cid:9) .

The next result is a corollary of the main result of [19].

Proposition 3. A spectral function R = Rsym ◦ σ is mirror-stratiﬁable with

respect to the smooth stratiﬁcation {σ−1(M sym)} and its image by JR.

Remark 6 (Back to || · ||∗). The nuclear norm is a spectral lift of the (cid:96)1 norm.
Therefore, one can recover mirror-stratiﬁcation of the nuclear norm, with the strati-
ﬁcations given in Section 2.3.4, by putting together Lemma 1 and Proposition 3.

9

2.4. Activity Identiﬁcation for Mirror-Stratiﬁable Functions. To our
point of view, the notion of mirror-stratiﬁbility deserves a special study in view of
the following simple but powerful geometrical observation. We state it as a theorem
because of its utmost importance in the subsequent developments of this paper.

Theorem 1 (Enlarged activity identiﬁcation). Let R be a proper lsc convex
function which is mirror-stratiﬁable with respect to primal-dual stratiﬁcations M =
{M } and M∗ = {M ∗}. Consider a pair of points (¯x, ¯u) and the associated strata M¯x
and M ∗
¯u. If the sequence pair (xk, uk) → (¯x, ¯u) is such that uk ∈ ∂R(xk), then for k
large enough, xk is localized in a speciﬁc set of strata such that

M¯x (cid:54) Mxk

(cid:54) JR∗ (M ∗

¯u).

(2.6)

Proof. By assumption on R, ∂R is sequentially closed [50, Theorem 24.4] and

thus ¯u ∈ ∂R(¯x). Now, since xk is close to ¯x, upon invoking Proposition 1, we get

Mxk

(cid:62) M¯x

M ∗
uk

(cid:62) M ∗
¯u.

which shows the left-hand side of (2.6). Similarly, we have

(2.7)

Using the fact that ∂R(xk) is a closed convex set together with (2.3) leads to

uk ∈ ∂R(xk) = cl(ri(∂R(xk))) = cl(JR({xk})) ⊂ cl(JR(Mxk ))

which entails Muk
decreasing for the relation (cid:54), we get

(cid:54) JR(Mxk ). Using ﬁnally (2.7) and that by deﬁnition, JR is

M ∗
¯u

(cid:54) M ∗
uk

(cid:54) JR(Mxk ) =⇒ Mxk

(cid:54) JR∗ (M ∗

¯u),

whence we deduce the right-hand side of (2.6).

Mirror-stratiﬁcation thus allows us to prove the simple but powerful claim of
Theorem 1. As we will see in the rest of the paper, this result will be the backbone
to prove sensitivity and ﬁnite enlarged activity identiﬁcation results in absence of
non-degeneracy.

3. Sensitivity of Composite Optimization Problems. In this section, we

consider a parametric convex optimization problem of the form

E(x, p) def.= F (x, p) + R(x),

min
x∈RN

(P(p))

depending on the parameter vector p ∈ Π, where Π is the parameter set, an open
subset of a ﬁnite dimensional linear space. Sensitivity analysis studies the properties
of solutions x(cid:63)(p) of (P(p)) (assuming they exist) to perturbations of the parameters
vector p ∈ Π around some reference point ˚p. In Section 3.1, we brieﬂy review the
existing results on this topic and their limitations. We then introduce in Section 3.2
our new results obtained owing to the mirror-stratiﬁable structure.

3.1. Existing sensitivity results. Classical sensitivity results (see e.g. [7, 47,
21]) study the regularity of the set-valued map p (cid:55)→ x(cid:63)(p). A typical result proves
Lipschitz continuity of this map provided that E is smooth enough and E(·, ˚p) has
a local second-order (quadratic) growth at x(cid:63)(˚p), i.e. that there exists some c > 0
such that E(x, ˚p) (cid:62) E(x(cid:63)(˚p), ˚p) + c||x − x(cid:63)(˚p)||2 for x nearby x(cid:63)(˚p). For C 2-smooth
10

optimization, this growth condition is equivalent to positive deﬁniteness of the hessian
of E with respect to x evaluated at (x(cid:63)(˚p), ˚p) [29]. For classical smooth constrained
optimization problems, activity is captured by the subset of active inequality con-
straints. Under reasonable non-degeneracy conditions (see, for example, [29]), this
active set is stable under small perturbations to the objective.

A nice nonsmooth sensitivity theory is based on the notion of partial smooth-
ness [39]. Partial smoothness is an intrinsically geometrical assumption which, infor-
mally speaking, says that E behaves smoothly along an active manifold and sharply in
directions normal to the manifold. Furthermore, under a non-degeneracy assumption
at a minimizer (see (3.1)), it allows appealing statements of second-order optimality
conditions (including second-order generalized diﬀerentiation) and associated sensi-
tivity analysis around that minimizer [39, 40].

Let ∂E(x, p) be the subdiﬀerential of E according to x. Specializing the result of
[24, Proposition 8.4] to a proper lsc convex function E(·, p), one can show that C 2-
partial smoothness of E(·, p) at x(cid:63)(˚p) relative to some ﬁxed manifold (independent of
p) for 0, together with the non-degeneracy assumption

0 ∈ ri(∂E(x(cid:63)(˚p), ˚p))

(3.1)

is equivalent to the existence of an identiﬁable C 2-smooth manifold, i.e. for x(cid:63)(p) and
u(cid:63)(p) ∈ ∂E(x(cid:63)(p), p) close enough to x(cid:63)(˚p) and 0, x(cid:63)(p) lives on the active/partly
smooth manifold of x(cid:63)(˚p). If these assumptions are supplemented with a quadratic
growth condition of E (see above) along the active manifold, then one also has C 1
smoothness of the single-valued mapping p (cid:55)→ x(cid:63)(p) [39, Theorem 5.7].
It can be
deduced from [23, Corollary 4.3] that for almost all linear perturbations of lsc convex
semialgebraic functions, the non-degeneracy and quadratic growth conditions hold.
However, this genericity fails to hold for many cases of interest. As an example, con-
sider E of (P(λ, y)) with λ > 0 ﬁxed and p = y. If R is a proper lsc convex and
semialgebraic function, one has from [23] that for Lebesgue almost all Φ∗y, prob-
lem (P(λ, y)) has at most one minimizer at which furthermore non-degeneracy and
quadratic growth hold. Of course genericity in terms of Φ∗y does not imply that in
terms of y, which is our parameter of interest. Not to mention that we supposed λ
ﬁxed while it is not in many cases of interest.

3.2. Sensitivity analysis without non-degeneracy. For a ﬁxed p, (P(p)) is
a standard composite optimization problem. Here, we assume that the objective is the
sum of a C 1(RN ) convex function F (·, p) and a nonsmooth proper lsc convex function
R. We denote ∇F (x, p) the gradient of F (·, p) at x.

We are going to show that if the minimizer x(cid:63)(˚p) is unique, slight perturbations
p of ˚p generate solutions x(cid:63)(p) that are in a “controlled” stratum Mx(cid:63)(p) precisely
sandwiched to extreme strata deﬁned from a primal-dual pair associated to ˚p.

Theorem 2 (Sensitivity analysis with mirror-stratiﬁable functions). Let ˚p be a
given point in the parameter space Π. Assume that: (i) E(·, ˚p) has a unique mini-
mizer x(cid:63)(˚p), (ii) E is lsc on RN × Π, (iii) E(x(cid:63)(˚p), ·) is continuous at ˚p, (iv) ∇F
is continuous at (x(cid:63)(˚p), ˚p), and (v) E is level-bounded3 in x uniformly in p locally
around ˚p. If R is mirror-stratiﬁable according to (M,M∗), then for all p close to ˚p,

3 Recall from [51, Deﬁnition 1.16] that the function E : RN × Π is said to be level-bounded in x
locally uniformly in p around ˚p if for each c ∈ R, there exists a neighbourhood V of ˚p and a bounded
set Ω such that the sublevel set (cid:8)x ∈ RN : E(x, p) (cid:54) c(cid:9) ⊂ Ω for all p ∈ V.

11

(a)

(b)

12

Fig. 3.1. Graphical illustration of Theorem 2 for a projection problem on the (cid:96)∞ ball. (a)

corresponds to a degenerate situation while (b) to a non-degenerate one.

any minimizer x(cid:63)(p) of E(·, p) is localized as follows

Mx(cid:63)(˚p) (cid:54) Mx(cid:63)(p) (cid:54) JR∗ (M ∗

u(cid:63)(˚p)) where u(cid:63)(˚p) def.= −∇F (x(cid:63)(˚p), ˚p).

(3.2)

Proof. Le (pk)k∈N ⊂ Π be a sequence of parameters converging to ˚p. By assump-
tion on F and R, E(·, p) is proper for any p. Since it is also lsc and is level-bounded
in x uniformly in p locally around ˚p by conditions (ii) and (v). It follows from [51,
Theorem 1.17(a)] that Argmin E(·, pk) is non-empty and compact, and, in turn, any
sequence (x(cid:63)
k)k∈N of minimizers is bounded. We consider a subsequence, which for
simplicity, we denote again x(cid:63)

k → ¯x. We then have

E(¯x, ˚p)

(cid:54)
condition (ii)
(cid:54)
Optimality

lim inf
k

E(x(cid:63)

k, pk)

lim inf
k

E(x(cid:63)(˚p), pk)

=
condition (iii)

lim
k

E(x(cid:63)(˚p), pk) = E(x(cid:63)(˚p), ˚p).

By the uniqueness condition (i), we conclude that ¯x = x(cid:63)(˚p). Let u(cid:63)
k
Since ∇F is continuous at (˚x, ˚p) by assumption (iv), one has u(cid:63)
optimality condition of problem (P(p)) reads u(cid:63)
position to invoke Theorem 1 to conclude.

k ∈ ∂R(x(cid:63)

def.= −∇F (x(cid:63)
k, pk).
k → ˚u. The ﬁrst-order
k). Hence we are now in

Remark 7 (Single manifold identiﬁcation). In the special case where we have

the non-degeneracy condition

u(cid:63)(˚p) = −∇F (x(cid:63)(˚p), ˚p) ∈ ri(∂R(x(cid:63)(˚p))),

(3.3)

our result simpliﬁes and we recover the known exact identiﬁcation result. To see this,
we note that (3.3) also reads u(cid:63)(˚p) ∈ JR({x(cid:63)(˚p)}), and thus M ∗
u(cid:63)(˚p) = JR({x(cid:63)(˚p)}).
In this case, (3.2) becomes Mx(cid:63)(p) = M˚x for all p close to ˚p. Such a result was also
established in [35, 40] under condition (3.3), when F (·, ˚p) is also locally C 2 around
x(cid:63)(˚p) and R is partly smooth at x(cid:63)(˚p) relative to a C 2-smooth manifold Mx(cid:63)(˚p). An
example of this non-degenerate scenario is shown in Figure 3.1(b). Our result covers
the more delicate degenerate situation where u(cid:63)(˚p) might be on the relative boundary
of the subdiﬀerential, but requires the stronger mirror-stratiﬁability structure on the
non-smooth part. However, the active set at x(cid:63)(p) is in general not unique for all p
close to ˚p, hence the terminology enlarged activity.

Example 4. The result of Theorem 2 is illustrated in Figure 3.1, for the projec-
tion problem on a (cid:96)∞ ball, in both the degenerate and the non-degenerate situations.
More precisely, we take E(x, p) = 1
2 ||x−p||2 +ιB(cid:96)∞ (x), so that u(cid:63)(p) = p−x(cid:63)(p), where
x(cid:63)(p) = PB(cid:96)∞ (p). Figure 3.1(a) illustrates the degenerate case where u(cid:63)(˚p) belongs to
the relative boundary of the normal cone of B(cid:96)∞ at x(cid:63)(˚p). Taking a perturbation p
around ˚p entails that x(cid:63)(p) belongs either to M1 = Mx(cid:63)(˚p) or to the enlarged stratum
M2 = JR∗ (M ∗
2 ). In the non-degenerate case of Figure 3.1(b) presented
in Remark 7, the optimal solution x(cid:63)(p) is always on M1 for small perturbations.

u(cid:63)(˚p)) = JR∗ (M ∗

Remark 8 (Quadratic growth). We can also establish the Lipschitz continuity
of x(cid:63)(p) under the conditions of Theorem 2, under an additional second-order growth
condition, just as in classical sensitivity analysis (see [7, Section 4.2.1]). Let us
assume that there exists a neighbourhood V of x(cid:63)(˚p) and κ > 0 such that

E(x, ˚p) (cid:62) E(x(cid:63)(˚p), ˚p) + κ||x − x(cid:63)(˚p)||2

∀x ∈ V.

(3.4)

k → x(cid:63)(˚p) for pk → ˚p (see the proof of Theorem 2), we have x(cid:63)

Since x(cid:63)
k ∈ V for k
large enough. If, moreover, ∇F (x, ·) is Lipschitz-continuous in a neighbourhood of ˚p
with Lipschitz constant ν independent of x ∈ V, we get

κ||x(cid:63)

k − x(cid:63)(˚p)||2 (cid:54) E(x(cid:63)

k, ˚p) − E(x(cid:63)(˚p), ˚p)
= (cid:0)(E(x(cid:63)(˚p), pk) − E(x(cid:63)

k, pk)) − (E(x(cid:63)(˚p), ˚p) − E(x(cid:63)

k, ˚p))(cid:1)

− (cid:0)E(x(cid:63)(˚p), pk) − E(x(cid:63)

k, pk)(cid:1)

(x(cid:63)

k minimizes E(·, pk)) (cid:54) (E(x(cid:63)(˚p), pk) − E(x(cid:63)
(By (P(p))) = (F (x(cid:63)(˚p), pk) − F (x(cid:63)

k, pk)) − (E(x(cid:63)(˚p), ˚p) − E(x(cid:63)
k, pk)) − (F (x(cid:63)(˚p), ˚p) − F (x(cid:63)

k, ˚p)
k, ˚p))

(Mean-Value Theorem) (cid:54) (cid:0) sup

||∇F (x, pk) − ∇F (x, ˚p)||(cid:1)||x(cid:63)

k − x(cid:63)(˚p)||

x∈V

(cid:54) ν||pk − ˚p||||x(cid:63)

k − x(cid:63)(˚p)||.

whence we conclude that

d(x(cid:63)(˚p), Argmin E(·, pk)) (cid:54) ν/κ||pk − ˚p||.

4. Regularized Inverse Problems. A typical context where our framework of
mirror-stratiﬁability is of usefulness is that of studying stability to noise of regularized
linear inverse problems. We come back to the situation presented in introduction:

13

studying stability issues to perturbed observations of the form y = ˚y + w amounts
to analyzing sensitivity of the minimizers and the optimal value function in (P(λ, y))
when the parameter p = (λ, y) evolves around the reference point ˚p = (0, ˚y). Unlike
the usual sensitivity analysis theory setting recalled in the previous section (e.g. [7]),
here the objective E may not even be continuous at ˚p.

We provide hereafter some pointers to the relevant literature, then we develop
our sensitivity results for mirror-stratiﬁable functions. These results involve primal
and dual solutions to the noiseless problem

E(x, (0, y)) def.= R(x)

s.t. Φx = y,

(P(0, y))

min
x∈RN

4.1. Existing sensitivity results for regularized Inverse Problems.
Lipschitz stability. Assume there exists a dual multiplier η ∈ RP (sometimes
referred to as a “dual certiﬁcate”) for the noiseless constrained problem (P(0, y))
taken at y = ˚y such that Φ∗η ∈ ∂R(˚x). The latter condition is equivalent to ˚x being
a minimizer of (P(0, y)). This condition goes by the name of the “source” or “range”
condition in the inverse problems literature. It has been widely used to derive stability
results in terms of ||Φx(cid:63)(λ, y)−Φ˚x|| or R(x(cid:63)(λ, y))−R(˚x)−(cid:104)Φ∗η, x(cid:63)(λ, y) − ˚x(cid:105); see [52]
and references therein. To aﬀord stability in terms of ||x(cid:63)(λ, y) −˚x|| directly, the range
condition has to be strengthened to its non-degenerate version Φ∗η ∈ ri(∂R(˚x)). It
has been shown that this condition implies that the set-valued map (λ, y) (cid:55)→ x(cid:63)(λ, y)
is Lipschitz-continuous at (0, ˚y); see [32] and [58].

Active set stability. In the case where R is partly smooth, one can approach
an even more complete sensitivity theory by studying stability of the partly smooth
manifold of R at ˚x. In particular, it can be shown from that if an appropriate non-
degeneracy assumption holds, see [59], then problems (P(0, y)) and (P(λ, y)) have
unique minimizers (respectively ˚x and x(cid:63)(λ, y)), and x(cid:63)(λ, y) lies on the partly smooth
manifold of R at ˚x. Observe that compared to Lipschitz stability, active set stability
is more demanding as the non-degeneracy condition has to hold for a speciﬁc dual
multiplier, which is obviously more stringent. This type of results has appeared many
times in the literature for special cases, e.g.
for the (cid:96)1 norm [30, 61], the nuclear
norm [1]. The work in [57, 59] has uniﬁed all these results.

Non-degeneracy in practice for deconvolution and compressed sensing. The above
results require that some abstract non-degeneracy condition holds, which imposes
strict limitations on practical situations. In particular, when Φ is a convolution oper-
ator and ˚x is sparse, [10] studies Lipschitz stability and [25] support stability. In this
setting, the non-degeneracy condition holds whenever the non-zero entries are sepa-
rated enough, which is not often veriﬁed. Another setting where this stability theory
has been applied in when Φ is drawn from a random matrix ensemble, i.e. compressed
sensing. For a variety of partly smooth regularizers (including the (cid:96)1, nuclear and (cid:96)1,2
norms), the non-degeneracy condition holds with high probability if, roughly speak-
ing, the sample size P is suﬃciently larger than the “dimension” of the active set at
˚x (see e.g. [12, 22]). Again this is a clear limitation as illustrated in Section 1.2.

4.2. Primal and dual problems. Suppose we have observations of the form
(1.1), and we want to recover ˚x (or a provably good approximation of it). As advocated
in Section 1.1, a popular approach is to adopt a regularization framework which can
be cast as the optimization problem (P(λ, y)) (for λ > 0) and (P(0, y)) (when λ = 0).
In the sequel, we assume that

R∞(z) > 0,

∀z ∈ ker(Φ) \ {0},

(4.1)

14

where R∞ is the asymptotic (or recession) function of R, deﬁned as

R∞(z) def.= lim
t→+∞

R(x + tz) − R(x)
t

,

∀x ∈ dom(R).

Condition (4.1) is a necessary and suﬃcient condition for the set of minimizers of
(P(λ, y)) and (P(0, y)) to be non-empty and compact [56, Lemma 5.1]. It is satisﬁed
for example when R is coercive.

Remark 9 (Discontinuity of F ). Letting p def.= (λ, y) ∈ Π def.= R+ × RP , we see

that (P(λ, y)) and (P(0, y)) are instances of (P(p)), by setting

F (x, p) def.=

(cid:40) 1

2λ ||y − Φx||2
ιHy (x)

if λ > 0,
if λ = 0,

where Hy

def.= (cid:8)x ∈ RN : Φx = y(cid:9) .

(4.2)

The corresponding function F considered does not obey the assumptions of Theorem 2.
Indeed, the parameter set Π is not open, with ˚p = (0, ˚y) that lives on the boundary of
Π, and F is only lsc at such ˚p (because of the aﬃne constraint Φx = y). The main
consequence will be that one can no longer allow p to vary freely nearby ˚p.

In order to study the sensitivity of solutions, we look at the Fenchel-Rockafellar

dual problem, which reads (for all λ (cid:62) 0)

(cid:104)q, y(cid:105) −

||q||2 − R∗(Φ∗q).

max
q∈RP

λ
2

(D(λ, y))

We denote D(λ, y) the set of solutions of (D(λ, y)). Note that for λ > 0, thanks to
strong concavity, there is a unique dual solution q(cid:63)(λ, y), i.e. D(λ, y) = {q(cid:63)(λ, y)}. We
also have from the primal-dual extremality relationship that for any primal solution
x(cid:63)(λ, y) of (P(λ, y)),

q(cid:63)(p) =

y − Φx(cid:63)(p)
λ

and Φ∗q(cid:63)(p) ∈ ∂R(x(cid:63)(p)).

(4.3)

While (D(0, y)) is the dual of (P(0, y)), it is important to realize that it is not the limit
of (D(λ, y)) in the sense that its set of dual solutions is in general not a singleton.
Lemma 3 hereafter singles out a speciﬁc dual optimal solution (sometimes called
“minimum norm certiﬁcate”) deﬁned by

˚q(0, y) = argmin

{||q|| : Φ∗q ∈ ∂R(x(cid:63)(0, y))} = argmin

{||q|| : q ∈ D(0, y)} .

(4.4)

q∈RP

q∈RP

The following two important lemmas ensure the convergence of the solutions to the
primal and dual problems as λ → 0 and as ||y − ˚y|| → 0.

Lemma 2 (Primal solution convergence). Assume that ˚x is the unique solution

to (P(0, ˚y)). For any sequence of parameters pk = (λk, yk) with λk > 0 such that

(cid:18) ||yk − ˚y||2
λk

(cid:19)

, λk

−→ (0, 0),

and any solution x(cid:63)(pk) of (P(pk)), we have x(cid:63)(pk) → ˚x.

Proof. Denote x(cid:63)
k

can extract a converging subsequence, which for simplicity, we denote again x(cid:63)
Optimality of x(cid:63)

k)k∈N is a bounded sequence by (4.1), one
k → ¯x.

def.= x(cid:63)(pk). Since (x(cid:63)

k implies that
k) (cid:54) 1
2λk

R(x(cid:63)

||yk − Φx(cid:63)

k||2 + R(x(cid:63)

||yk − ˚y||2 + R(˚x).

(4.5)

k) (cid:54) 1
2λk

15

Passing to the limit and using the hypothesis that 1
λk

||yk − ˚y||2 → 0, we get

lim sup
k

R(x(cid:63)

k) (cid:54) R(˚x).

k) (cid:62) R(¯x). Com-
On the other hand, lower semi-continuity of R entails lim inf k R(x(cid:63)
bining these two inequalities, we deduce that R(¯x) (cid:54) R(˚x). Since R is proper and lsc,
it is bounded from below on bounded sets [51, Corollary 1.10]. Let r = inf k R(x(cid:63)
k)
which then satisﬁes −∞ < r < +∞. Substracting r from (4.5) and multiplying by
λk, one obtains

1
2

||yk − Φx(cid:63)

k||2 (cid:54) 1
2

||yk − Φx(cid:63)

k||2 + λk(R(x(cid:63)

||yk − ˚y||2 + λk(R(˚x) − r). (4.6)

k) − r) (cid:54) 1
2

Consequently, passing to the limit in (4.6) shows that ||˚y − Φ¯x|| = 0, i.e. ¯x is a feasible
point of (P(0, ˚y)). Altogether, this shows that ¯x is a solution of (P(0, ˚y)), and by
uniqueness of the minimizer, ¯x = ˚x.

Lemma 3 (Dual solution convergence). For any sequence of parameters pk =

(λk, yk) such that

(cid:18) ||yk − ˚y||
λk

(cid:19)

, λk

−→ (0, 0),

we have q(cid:63)(pk) → ˚q(0, ˚y).

Proof. By the triangle inequality, we have

||q(cid:63)(λk, yk) − ˚q(0, ˚y)|| (cid:54) ||q(cid:63)(λk, yk) − q(cid:63)(λk, ˚y)|| + ||q(cid:63)(λk, ˚y) − ˚q(0, ˚y)||.

For the ﬁrst term, we notice that q(cid:63)(λ, y) = proxR∗◦Φ∗/λ(y/λ) for λ > 0. Thus,
Lipschitz continuity of the proximal mapping entails that

||q(cid:63)(λk, yk) − q(cid:63)(λk, ˚y)|| (cid:54) ||yk − ˚y||

,

λk

λk
2
λk
2

which in turn shows that q(cid:63)(λk, yk) → q(cid:63)(λk, ˚y). Let us now turn to the second term.
k = q(cid:63)(λk, ˚y) and ˚q def.= ˚q(0, ˚y) ∈ D(0, ˚y), one
Using the respective optimality of q(cid:63)
obtains

− (cid:104)q(cid:63)

k, ˚y(cid:105) +

||q(cid:63)

k||2 + R∗(Φ∗q(cid:63)

k) (cid:54) − (cid:104)˚q, ˚y(cid:105) +

||˚q||2 + R∗(Φ∗˚q)

λk
2

(cid:54) − (cid:104)q(cid:63)

k, ˚y(cid:105) +

||˚q||2 + R∗(Φ∗q(cid:63)

k),

(4.7)

k|| (cid:54) ||˚q||, which shows in particular that (q(cid:63)

whence we get ||q(cid:63)
k)k∈N is a bounded se-
quence. We can thus extract any converging subsequence, which for simplicity, we
denote again q(cid:63)
k → ¯q. Passing to the limit in (4.7), using the fact that R∗ is lsc, one
obtains

− (cid:104)¯q, ˚y(cid:105) + R∗(Φ∗ ¯q) (cid:54) − (cid:104)¯q, ˚y(cid:105) + lim inf

R∗(Φ∗q(cid:63)

k) (cid:54) − (cid:104)˚q, ˚y(cid:105) + R∗(Φ∗˚q),

which shows that ¯q ∈ D(0, ˚y). This together with the fact that ||q(cid:63)
already saw, shows that ¯q = ˚q by uniqueness of ˚q in (4.4).

k|| (cid:54) ||˚q|| as we

k

16

4.3. Sensitivity. We are now in position to state the main result of this section,
which tracks the strata of x(cid:63)(p) in the regime where the perturbation ||w|| = ||y − ˚y||
is suﬃciently small.

Theorem 3. Suppose that ˚x is the unique solution to (P(0, ˚y)). Assume fur-
thermore that R is mirror-stratiﬁable with respect to the primal-dual stratiﬁcations
(M, M∗). If there are constants (C0, C1) depending only on ˚x such that for all p in

{p = (λ, y) : C0||y − ˚y|| (cid:54) λ (cid:54) C1} ,

then there exists a minimizer x(cid:63)(p) of E(·, p) localized as follows

M˚x (cid:54) Mx(cid:63)(p) (cid:54) JR∗ (M ∗

˚u) where ˚u def.= Φ∗˚q(0, ˚y).

(4.8)

(4.9)

Proof. Under (4.8) with for instance C1 small enough, one can easly check
that there exists k large enough such that the regime required for (yk, λk) to ap-
In turn, we have a converging primal-dual pair
ply Lemma 2 and 3 is attained.
(x(cid:63)(pk), Φ∗q(cid:63)(pk)) → (˚x, ˚u). One can then apply Theorem 1 to conclude.

5. Activity Localization with Proximal Splitting Algorithms. Proximal
splitting methods are algorithms designed to solve large-scale structured optimization
and monotone inclusion problems, by evaluating various ﬁrst-order quantities such as
gradients, proximity operators, linear operators, all separately at various points in the
course of an iteration. Though they can show slow convergence each iteration has a
cheap cost. We refer to e.g. [3, 5, 17, 49] for a comprehensive treatment and review.
Capitalizing on our enlarged activity identiﬁcation result for mirror-stratiﬁable
functions, we now instantiate its consequences on ﬁnite activity localization of prox-
imal splitting algorithms. While existing results on ﬁnite identiﬁcation (of a sin-
gle active set) strongly rely on partial smoothness around a non-degenerate cluster
point [34, 42, 43, 41], we examine here intricate situations where neither of these
assumptions holds.

5.1. Forward-Backward algorithm. The Forward–Backward (FB) splitting
method [44] is probably the most well-known proximal splitting algorithm. In our
context, it can be used to solve optimization problems with the additive “smooth +
non-smooth” structure of the form

min
x∈RN

f (x) + R(x),

where f ∈ C 1(RN ) is convex with L-Lipschitz gradient, and R is a proper lsc and
convex function. We assume that Argmin(f + R) (cid:54)= ∅. The FB iteration in relaxed
form reads [16]

xk+1 = (1 − τk)xk + τk proxγkR(xk − γk∇f (xk)),

with γk ∈]0, 2/L[ and τk ∈]0, 1], where proxγR, γ > 0, is the proximal mapping of R,

(5.1)

(5.2)

(5.3)

proxγR(x) def.= argmin
z∈RN

1
2

||z − x||2 + γR(z).

Diﬀerent variants of FB method were studied, and a popular trend is the inertial
schemes which aim at speeding up the convergence (see [48, 4], and the sequence-
convergence version as proved recently in [14]).

17

Under the non-degeneracy assumption −∇f (x(cid:63)) ∈ ri(∂R(x(cid:63))), it was shown in
[42] that FB and its inertial variants correctly identify the active manifold in ﬁnitely
many iterations, and then enter a local linear convergence regime. These results
encompass many special cases such as those studied in [33, 8, 54]. Beyond this non-
degenerate case, we establish now the general localization of active strata.

Theorem 4. Consider the FB iteration (5.2) to solve (5.1) with 0 < inf k γk (cid:54)
supk γk < 2/L and inf k τk > 0. Then xk converges to x(cid:63) ∈ Argmin(f + R). Assume
that R is also mirror-stratiﬁable with respect to (M, M∗), then for k large enough,

Mx(cid:63) (cid:54) Mxk

(cid:54) JR∗ (M ∗

u(cid:63) ) where u(cid:63) def.= −∇f (x(cid:63)).

Proof. Convergence of the sequence (xk)k∈N to x(cid:63) is obtained from [16, Corol-
lary 6.5]. Moreover, since the proximal mapping is the resolvent of the subdiﬀerential,
(5.2) is equivalent to the monotone inclusion

uk+1

def.=

xk − vk+1
γk

− ∇f (xk) ∈ ∂R(vk+1),

def.= xk+1−xk

+ xk. In turn, with the conditions inf k τk > 0 and inf k γk > 0,
where vk+1
and continuity of ∇f , we have vk → x(cid:63) and thus uk → −∇f (x(cid:63)) ∈ ∂R(x(cid:63)). It then
remains to apply Theorem 1 to (vk, uk) and R to conclude.

τk

It can be easily shown that Theorem 4 holds for several extensions of the iterate-

convergent version of FISTA [14]. We omit the details here for the sake of brevity.

We rather take a closer look to the case when the FB scheme (5.2) to solve (P(λ, y))
(see also (4.2)) for λ > 0. Putting together Theorem 3 and 4, we obtain the following
localization result depending only on the data to estimate ˚x assuming that the noise
level ||y − ˚y|| is small enough.

Proposition 4. Under the assumptions of Theorem 3, consider the FB iteration
(5.2) to solve (P(λ, y)) with 0 < inf k γk (cid:54) supk γk < 2λ/||Φ||2 and inf k τk > 0. Then,
for k large enough, the iterates xk satisfy

M˚x (cid:54) Mxk

(cid:54) JR∗ (M ∗

˚u) where ˚u def.= Φ∗q(0, ˚y),

with q(0, ˚y) from (4.4).

Proof. To lighten notation, denote p = (λ, y). As in Theorem 4, we have xk →

x(cid:63)(p) a minimizer of (P(λ, y)). Thus we get

M˚x (cid:54) Mx(cid:63)(p) (cid:54) Mxk ,

where the ﬁrst inequality comes from Theorem 3 and the second one from Theorem 4.
This gives the ﬁrst inequality in the localization result.

Moreover, in the special case at hand ∂R(x(cid:63)(p)) (cid:51) −∇f (x(cid:63)(p)) = Φ∗q(cid:63)(p) → ˚u
as p → (0, ˚y), where we invoked (4.3). It then follows from (2.7) and the fact that
JR∗ is decreasing for the relation (cid:54), that

M ∗
˚u

(cid:54) M ∗

−∇f (x(cid:63)(p)) ⇐⇒ JR∗ (M ∗

−∇f (x(cid:63)(p))) (cid:54) JR∗ (M ∗

˚u).

Using once again Theorem 4, we get

Mxk

(cid:54) JR∗ (M ∗

−∇f (x(cid:63)(p))) (cid:54) JR∗ (M ∗

˚u),

18

which yields the second desired inequality.

Though the iterates xk of FB do not converge to ˚x, this proposition tells us that
the iterates identify an enlarged stratum associated to ˚x. This is an appealing feature
from a practical perspective, since one can often make some prior assumption on the
sought after vector ˚x, such as for instance sparsity or low-rank properties, as we have
illustrated in the numerical experiments of Section 6.

5.2. Douglas-Rachford Splitting Algorithm. The Douglas-Rachford (DR)
method [44] is another popular splitting method designed to minimize convex objec-
tives having the additive “non-smooth + non-smooth” structure of the form

with f and g be proper lsc and convex functions such that ri(dom(f ))∩ri(dom(g)) (cid:54)= ∅
and Argmin(f + g) (cid:54)= ∅. The DR scheme reads

min
x∈RN

f (x) + g(x).






vk+1 = proxγf (2xk − zk) ,
zk+1 = zk + τk (vk+1 − xk) ,
xk+1 = proxγg(zk+1),

(5.4)

(5.5)

where γ > 0, τk ∈]0, 2[ is a relaxation parameter. By deﬁnition, the DR method is
not symmetric with respect to the order of the functions f and/or g. Nevertheless,
all of our statements throughout hold true, with obvious adaptations, when the order
of f and g is reversed in (5.5).

It has been shown in [43] that under appropriate non-degeneracy assumptions,
the DR identiﬁes the active manifolds in ﬁnite time, and then shows a local linear
regime. These results unify all those that were established in the literature for special
problems, see e.g.
[20] for linearly constrained (cid:96)1-minimization, [6] for quadratic or
linear programs, [2] for feasibility with two subspaces. Under mirror-stratiﬁability of
f or g, we get the following enlarged activity identiﬁcation result.

Theorem 5. Consider the DR iteration (5.5) to solve (5.4) with τk ∈]0, 2[
such that (cid:80)
k∈N τk (2 − τk) = +∞. Then zk converges to a ﬁxed point z(cid:63) with
x(cid:63) = proxγg(z(cid:63)) ∈ Argmin(f + g), and xk and vk both converge to x(cid:63). Introduc-
ing u(cid:63) = z(cid:63)−x(cid:63)

, we have furthermore:

γ

(i) If g is mirror-stratiﬁable with respect to the primal-dual stratiﬁcations (M g, M g∗

),

then for k large enough

(ii) If f is mirror-stratiﬁable with respect to the primal-dual stratiﬁcations (M f , M f ∗

),

then for k large enough

M g

x(cid:63) (cid:54) M g
xk

(cid:54) Jg∗ (M g∗

u(cid:63) ).

M f

x(cid:63) (cid:54) M f
vk

(cid:54) Jf ∗ (M f ∗

−u(cid:63) ).

Proof. Under the prescribed choice of τk, convergence of zk is ensured by virtue
of [16, Corollary 5.2]. By non-expansiveness of the proximal mapping, and as we are
in ﬁnite dimension, we also obtain convergence of xk and vk to x(cid:63). To prove (i), note
that the update of xk in (5.5) is equivalent to the monotone inclusion

uk

def.=

∈ ∂g(xk).

zk − xk
γ

19

(5.6)

Since (xk, uk) → (x(cid:63), u(cid:63)), we conclude about (i) by invoking Theorem 1. Similarly,
we note that the update of vk in (5.5) is equivalent to

wk

def.=

2xk − zk − vk+1
γ

∈ ∂f (vk+1).

(5.7)

Using that (vk, wk) → (x(cid:63), −u(cid:63)) and applying Theorem 1 we get (ii).

In the same vein as for FB in the previous section, we now turn to applying
the DR scheme (5.5) to solve (P(λ, y)) by setting in (5.4) f (x) = 1
2λ ||y − Φx||2 and
g(x) = R(x). Putting Theorem 3 and 5-(i) together, we obtain the following analogue
to Proposition 4.

Proposition 5. Under the assumptions of Theorem 3, consider the DR iteration
k∈N τk (2 − τk) = +∞.

(5.5) to solve (P(λ, y)) with γ > 0 and τk ∈]0, 2[ such that (cid:80)
Then, for k large enough, the DR iterates satisfy

M˚x (cid:54) Mxk

(cid:54) JR∗ (M ∗

˚u) where ˚u def.= Φ∗q(0, ˚y).

Proof. The proof follows the same reasoning as that of Proposition 4 by addition-
ally observing (recall the notation in the proof of Theorem 5) that from (5.6)-(5.7),
we have u(cid:63)(p) = −∇f (x(cid:63)(p)) = Φ∗q(cid:63)(p) ∈ ∂R(x(cid:63)(p)).

6. Numerical Illustrations. In this section, we numerically illustrate our the-
oretical results on sensitivity and enlarged activity identiﬁcation in the context of
regularized inverse problems. We adopt the same two “compressed sensing” scenarios
described in Section 1.2. The dimension dim(M˚x) = R0(˚x) of the strata associated
to ˚x is measured as R0 = || · ||0 (resp. R0 = rank) for the (cid:96)1 (resp. nuclear) norm
regularization.

Strata sensitivity. We ﬁrst illustrate the relevance of the strata sensitivity result in
Theorem 3 by studying the dimension of the largest possible active stratum JR∗ (M ∗
˚u)
(in fact its closure). The dual ˚u = Φ∗˚q(0, ˚y) is computed from ˚x by solving the convex
optimization problem (4.4) (using CVX to get a high precision). Thus we know the
maximum complexity index excess predicted by Theorem 3, i.e.

δ(cid:63)(˚x) def.= dim(JR∗ (M ∗

˚u)) − dim(M˚x).

For each given δ and R0(˚x), and among the 1000 randomly generated replications of
(˚x, Φ), we compute the proportion ρ(R0(˚x), δ) of ˚x such that it is the unique solution
of (P(0, ˚y)) and δ(cid:63)(˚x) (cid:54) δ. The proportions ρ(R0(˚x), δ) are displayed in Figure 6.1 as
a function of the input complexity index R0(˚x) = dim(M˚x). The colors from blue to
red correspond to increasing δ.

The proportion ρ(R0(˚x), δ) is an increasing function of δ and a decreasing function
of R0(˚x). Indeed, as anticipated from standard compressed sensing results [58], active
strata M˚x of vectors ˚x whose dimension is small enough compared to the number of
measurements P can be provably and stably recovered with overwhelming probability
(on the sampling of (˚x, Φ, w)). As R0(˚x) increases, the number of measurements P
becomes insuﬃcient to ensure non-degeneracy with high probability, hence preventing
stable recovery of M˚x. However, Theorem 3 predicts that the active stratum of
x(cid:63)(λ, y) for y nearby ˚y is localized between M˚x and JR∗ (M ∗

˚u).

The blue curve in each plot of Figure 6.1 corresponds to δ = 0, which is the
proportion ρ(R0(˚x), 0) of vectors ˚x whose active stratum M˚x can be recovered stably
under small noise perturbation by solving (P(λ, y)) for λ chosen according to (4.8).

20

R = || · ||1

R = || · ||∗

Fig. 6.1. Proportion of ˚x such that δ(cid:63)(˚x) (cid:54) δ as a function of R0(˚x) (increasing value of δ

from 0 to its maximal value is depicted by a color evolving from blue to red).

R = || · ||1

R = || · ||∗

Fig. 6.2. Plots of R0(xk) for the 2000 ﬁrst iterates generated by the Forward-Backward algo-

rithm. Plots in blue correspond to the cases when δ(cid:63)(˚x) = 0, and the red ones for δ(cid:63)(˚x) = δ.

This proportion shows a phase transition phenomenon between stable recovery and
unstable recovery. The location of the phase transition for δ = 0 can be predicted
accurately; see for instance [59].

The red curves in Figure 6.1 correspond to the extreme case where δ takes its
largest achievable value, i.e. where we can guarantee recovery of the largest stratum
JR∗ (M ∗
˚u) with high probability. The phase transition occurs for higher dimension
R0(˚x). The intermediate curves, i.e. from blue to red, correspond to the recovered
strata that are localized between M˚x and JR∗ (M ∗
˚u) (i.e. increasing δ). The phase
transition progressively increases with δ. These curves illustrate and quantify the
typical tradeoﬀ observed in practice: one can allow for more complex input vectors ˚x
(i.e. those with larger R0(˚x)) at the expense of recovering active strata larger than M˚x.
Forward-Backward ﬁnite activity localization. We now numerically illustrate the
ﬁnite enlarged activity identiﬁcation of the FB splitting scheme as predicted by The-
orem 4 and Proposition 4. We remain under the same compressed sensing setting
as before. The randomly generated replications of ˚x are such that R0(˚x) = 10 for
R = || · ||1 and to 4 for R = || · ||∗.

The evolution of the complexity index R0(xk) of the FB iterate xk is shown in
Figure 6.2. The blue lines correspond to several trajectories (the bold one is the
average trajectory), each for a randomly generated instance of ˚x such that δ(cid:63)(˚x) =
21

0, i.e. those vectors whose active strata M˚x can be exactly recovered under small
perturbations. Thus, the iterates identify M˚x in ﬁnite time. The red lines (the bold
one is the average trajectory) are those for which δ(cid:63)(˚x) = δ > 0. As anticipated by
our theoretical results, the iterates identify a stratum strictly larger that M˚x.

Acknowledgements. The work of Gabriel Peyr´e has been supported by the Euro-
pean Research Council (ERC project SIGMA-Vision). Jalal Fadili was partly supported by
Institut Universitaire de France.

Appendix A. Proofs of the results in Section 2.
Proof of Proposition 1. Let us ﬁrst prove the ﬁrst equivalence. Observe that
Deﬁnition 2 can be read as follows: M is active at x if and only if d(x, cl(M )) = 0.
Since there is a ﬁnite number of strata in the stratiﬁcation, let us consider δ the
minimum of the nonzero distances d(x, cl(M (cid:48))) for M (cid:48) not active. For all x(cid:48) in the
open ball of radius δ and of center x, we have

d(x, cl(Mx(cid:48))) (cid:54) ||x − x(cid:48)|| < δ

⇐⇒

d(x, cl(Mx(cid:48))) = 0.

This shows that the set of these strata Mx(cid:48) indeed coincides the set of active strata,
whence we get the ﬁrst equivalence.

Let us turn to the second equivalence. Let M be an active strata at x, that
is, x ∈ cl(M ). Since x ∈ Mx, the intersection cl(M ) ∩ Mx contains x and thus is
nonempty. We deduce from (2.1) that M (cid:62) Mx. Conversely, if M (cid:62) Mx, then
x ∈ Mx ⊂ cl(M ) and therefore M is active at x.

Proof of Proposition 2. From classical convex analysis calculus rules, we get for

all x ∈ dom R

∂R(x) = conv {ai

: i ∈ I max(x)} + cone (cid:8)ai

: i ∈ I feas(x)(cid:9) .

By deﬁnition of MI , the relative interior of ∂R(x) is constant over a stratum: for all
x ∈ MI (with MI (cid:54)= ∅)

ri(∂R(x)) = ri (cid:0) conv {ai
= ri(conv {ai

= ri(conv {ai

: i ∈ I max(x)} + cone (cid:8)ai
: i ∈ I max(x)}) + ri(cone (cid:8)ai
: i ∈ I max}) + ri(cone (cid:8)ai

: i ∈ I feas(x)(cid:9) (cid:1)

: i ∈ I feas(x)(cid:9))

: i ∈ I feas(cid:9)).

This yields JR(MI ) = M ∗
entails for all u ∈ M ∗
I

I . Conversely we have ∂R∗(u) = {x : u ∈ ∂R(x)}, which

∂R∗(u) = (cid:8)x : I max(x) ∩ I feas(x) ⊃ I(cid:9) ,

and therefore ri(∂R∗(u)) = MI . This gives JR∗ (M ∗
Deﬁnition 4.

I ) = MI , which proves item (i) of

To show (ii) of Deﬁnition 4, we make the following observation:

MI (cid:54) MI (cid:48) ⇐⇒ any x ∈ MI lies in cl(MI (cid:48))
max

⇐⇒ I max = I max(x) ⊃ (I (cid:48))
⇐⇒ I ⊃ I (cid:48).

On the other hand we have

and I feas = I feas(x) ⊃ (I (cid:48))

feas

cl(JR(MI )) ⊃ cl(ri(conv {ai

: i ∈ I max})) + cl(ri cone (cid:8)ai

: i ∈ I feas(cid:9)))

= conv {ai

: i ∈ I max} + cone (cid:8)ai

: i ∈ I feas(cid:9) .

22

Note that the ﬁrst ⊃ is in fact = because conv {ai
unique decomposition of a polyhedron, we can write,

: i ∈ I max} is compact. Using

JR(MI ) (cid:62) JR(MI (cid:48)) ⇐⇒ cl(JR(MI )) ⊃ JR(MI (cid:48)) ⇐⇒ I ⊃ I (cid:48),

which ends the proof.

Proof of Proposition 3. The proof builds upon a key result stated in [19]. Theo-
rem 4.6(i) in [19] asserts that the collection (cid:8)σ−1(M sym) : M ∈ M(cid:9) forms a smooth
stratiﬁcation of dom(R) with the desired properties. The fact that spectral func-
tions are mirror-stratiﬁable follows from the polyhedral case with the help of Theo-
rem 4.6(iv) in [19], which states that

JR(σ−1(M sym)) = σ−1(cid:0)JRsym (M sym)(cid:1)

together with continuity of the singular value mapping σ.

REFERENCES

[1] F. Bach. Consistency of trace norm minimization. The Journal of Machine Learning Research,

9(Jun):1019–1048, 2008.

[2] H. Bauschke, J.Y.B. Cruz, T.A. Nghia, H.M. Phan, and X. Wang. The rate of linear convergence
of the douglas-rachford algorithm for subspaces is the cosine of the friedrichs angle. J. of
Approx. Theo., 185(63–79), 2014.

[3] H. H. Bauschke and P. L. Combettes. Convex analysis and monotone operator theory in Hilbert

spaces. Springer, 2011.

[4] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.

[5] A. Beck and M. Teboulle. Gradient-based algorithms with applications to signal recovery.

Convex Optimization in Signal Processing and Communications, 2009.

[6] D. Boley. Local linear convergence of the alternating direction method of multipliers on

quadratic or linear programs. SIAM J. Optim., 23(4):2183–2207, 2013.

[7] J. F. Bonnans and A. Shapiro. Perturbation analysis of optimization problems. Springer Series

in Operations Research and Financial Engineering. Springer Verlag, 2000.

[8] K. Bredies and D. A. Lorenz. Linear convergence of iterative soft-thresholding. Journal of

Fourier Analysis and Applications, 14(5-6):813–837, 2008.

[9] P. B¨uhlmann and S. Van De Geer. Statistics for high-dimensional data: methods, theory and

applications. Springer, 2011.

[10] E. J. Cand`es and C. Fernandez-Granda. Towards a mathematical theory of super-resolution.

Communications on Pure and Applied Mathematics, 67(6):906–956, 2013.

[11] E. J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations of

Computational mathematics, 9(6):717–772, 2009.
[12] E. J. Cand`es and T. Tao. Decoding by linear programming.

Information Theory, IEEE

Transactions on, 51(12):4203–4215, 2005.

[13] E. J. Cand`es and T. Tao. The power of convex relaxation: Near-optimal matrix completion.

Information Theory, IEEE Transactions on, 56(5):2053–2080, 2010.

[14] A. Chambolle and C. Dossal. On the convergence of the iterates of the “fast iterative
shrinkage/thresholding algorithm”. Journal of Optimization Theory and Applications,
166(3):968–982, 2015.

[15] S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM

journal on scientiﬁc computing, 20(1):33–61, 1999.

[16] P. L. Combettes. Solving monotone inclusions via compositions of nonexpansive averaged

operators. Optimization, 53(5-6):475–504, 2004.

[17] P. L. Combettes and J.-C. Pesquet. Proximal splitting methods in signal processing. In H. H.
Bauschke, Burachik R. S., P. L. Combettes, Elser. V., D. R. Luke, and H. Wolkowicz,
editors, Fixed-Point Algorithms for Inverse Problems in Science and Engineering, pages
185–212. Springer, 2011.

[18] M. Coste. An introduction to o-minimal geometry. Technical report, Institut de Recherche

Mathematiques de Rennes, November 1999.

[19] A. Daniilidis, D. Drusvyatskiy, and A. S. Lewis. Orthogonal invariance and identiﬁability.

SIAM Journal on Matrix Analysis and Applications, 35(2):580–598, 2014.

23

[20] L. Demanet and X. Zhang. Eventual linear convergence of the douglas-rachford iteration for

basis pursuit. Mathematics of Computation, 2013. to appear.

[21] A. L. Dontchev. Perturbations, approximations, and sensitivity analysis of optimal control

systems, volume 52. Springer-Verlag, Berlin, 1983.

[22] C. Dossal, M.-L. Chabanol, G. Peyr´e, and J. M. Fadili. Sharp support recovery from noisy ran-
dom measurements by (cid:96)1-minimization. Applied and Computational Harmonic Analysis,
33(1):24–43, 2012.

[23] D. Drusvyatskiy, A. D. Ioﬀe, and A. S. Lewis. Generic minimizing behavior in semialgebraic

optimization. SIAM J. Optim., 26(1):513–534, 2016.

[24] D. Drusvyatskiy and A.S. Lewis. Optimality, identiﬁability, and sensitivity. Mathematical

Programming, to appear in, pages 1–32, 2013.

[25] V. Duval and G. Peyr´e. Exact support recovery for sparse spikes deconvolution. Foundations

of Computational Mathematics, 15(5):1315–1355, 2015.

[26] V. Duval and G. Peyr´e. Sparse spikes deconvolution on thin grids. Preprint 01135200, HAL,

2015.

[27] C. Ekanadham, D. Tranchina, and E. P. Simoncelli. A uniﬁed framework and method for
automatic neural spike identiﬁcation. Journal of Neuroscience Methods, 222:47 – 55, 2014.
[28] M. Fazel. Matrix Rank Minimization with Applications. PhD thesis, Stanford University, 2002.
[29] A. V. Fiacco and G. P. McCormick. Nonlinear Programming: Sequential Unconstrained Min-

imization Techniques. Wiley, New York, 1968. reprinted, SIAM, Philadelphia, 1990.

[30] J.-J. Fuchs. On sparse representations in arbitrary redundant bases. Information Theory, IEEE

Transactions on, 50(6):1341–1344, 2004.

[31] Michael Grant and Stephen Boyd. CVX: Matlab software for disciplined convex programming,

version 2.1. http://cvxr.com/cvx, March 2014.

[32] M. Grasmair, O. Scherzer, and M. Haltmeier. Necessary and suﬃcient conditions for lin-
ear convergence of l1-regularization. Communications on Pure and Applied Mathematics,
64(2):161–182, 2011.

[33] E. Hale, W. Yin, and Y. Zhang. Fixed-point continuation for (cid:96)1-minimization: methodology

and convergence. SIAM J. Optim., 19(3):1107–1130, 2008.

[34] W. L. Hare.

Identifying active manifolds in regularization problems.

In H. H. Bauschke,
R. S., Burachik, P. L. Combettes, V. Elser, D. R. Luke, and H. Wolkowicz, editors, Fixed-
Point Algorithms for Inverse Problems in Science and Engineering, volume 49 of Springer
Optimization and Its Applications, chapter 13. Springer, 2011.

[35] W. L. Hare and A. S. Lewis. Identifying active constraints via partial smoothness and prox-

regularity. J. Convex Anal., 11(2):251–266, 2004.

[36] J.-B. Hiriart-Urruty and H. Y. Le. Convexifying the set of matrices of bounded rank: appli-
cations to the quasiconvexiﬁcation and convexiﬁcation of the rank function. Optimization
Letters, 6(5):841–849, 2012.

[37] Hai Yen Le. Confexifying the Counting Function on Rp for Convexifying the Rank Function

on Mm,n(R). Journal of Convex Analysis, 19(2):519–524, 2012.

[38] C. Lemar´echal, F. Oustry, and C. Sagastiz´abal. The U -lagrangian of a convex function. Trans.

Amer. Math. Soc., 352(2):711–729, 2000.

[39] A. S. Lewis. Active sets, nonsmoothness, and sensitivity. SIAM J. Optim., 13(3):702–725,

[40] A. S. Lewis and S. Zhang. Partial smoothness, tilt stability, and generalized hessians. SIAM

[41] J. Liang. Convergence Rates of First-Order Operator Splitting Methods. Theses, Normandie

2002.

J. Optim., 23(1):74–94, 2013.

Universit´e, October 2016.

[42] J. Liang, J. Fadili, and G. Peyr´e. Activity identiﬁcation and local linear convergence of forward–

backward-type methods. SIAM J. Optim., 27(1):408–437, 2017.

[43] J. Liang, J. Fadili, and G. Peyr´e. Local convergence properties of douglas–rachford and alter-
nating direction method of multipliers. Journal of Optimization Theory and Applications,
172(3):874–913, 2017.

[44] P. L. Lions and B. Mercier. Splitting algorithms for the sum of two nonlinear operators. SIAM

Journal on Numerical Analysis, 16(6):964–979, 1979.

[45] S. G. Mallat. A wavelet tour of signal processing. Elsevier, third edition, 2009.
[46] S. A. Miller and J. Malick. Newton methods for nonsmooth convex minimization: connections
among-Lagrangian, Riemannian Newton and SQP methods. Mathematical programming,
104(2-3):609–633, 2005.

[47] B. S. Mordukhovich. Sensitivity analysis in nonsmooth optimization.

In D. A. Field and
V. Komkov, editors, Theoretical Aspects of Industrial Design, volume 58, pages 32–46.
SIAM Volumes in Applied Mathematics, 1992.

24

[48] Y. Nesterov. A method for solving the convex programming problem with convergence rate

O(1/k2). Dokl. Akad. Nauk SSSR, 269(3):543–547, 1983.

[49] N. Parikh and S. P. Boyd. Proximal algorithms. Foundations and Trends in Optimization,

1(3):123–231, 2013.

university press, 1970.

[50] R. T. Rockafellar. Convex analysis. Number 28 in Princeton Mathematical Series. Princeton

[51] R. T. Rockafellar and R. Wets. Variational analysis, volume 317. Springer, Berlin, 1998.
[52] O. Scherzer. Variational methods in imaging, volume 167. Springer, 2009.
[53] J.-L. Starck and F. Murtagh. Astronomical Image and Data Analysis. Springer, 2006.
[54] S. Tao, D. Boley, and S. Zhang. Local linear convergence of ISTA and FISTA on the LASSO

problem. arXiv preprint arXiv:1501.02888, 2015.

[55] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical

Society. Series B. Methodological, 58(1):267–288, 1996.

[56] S. Vaiter. Low Complexity Regularization of Inverse Problems. Theses, Universit´e Paris

Dauphine - Paris IX, July 2014.

[57] S. Vaiter, M. Golbabaee, J. Fadili, and G. Peyr´e. Model selection with low complexity priors.

Information and Inference: A Journal of the IMA, 4(3):230, 2015.

[58] S. Vaiter, G. Peyr´e, and J. Fadili. Low complexity regularization of linear inverse prob-
lems. In G¨otz Pfander, editor, Sampling Theory, a Renaissance, pages 103–153. Springer-
Birkh¨auser, 2015.

[59] S. Vaiter, G. Peyr´e, and J. Fadili. Model consistency of partly smooth regularizers. IEEE

Trans. Inf. Theory, 64(3):1725 – 1737, 2018.

[60] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables.

Journal of the Royal Statistical Society: Series B, 68(1):49–67, 2005.

[61] P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning

Research, 7:2541–2563, 2006.

25

8
1
0
2
 
n
u
J
 
5
 
 
]

C
O
.
h
t
a
m

[
 
 
3
v
4
9
1
3
0
.
7
0
7
1
:
v
i
X
r
a

SENSITIVITY ANALYSIS
FOR MIRROR-STRATIFIABLE CONVEX FUNCTIONS

JALAL FADILI∗, J´ER ˆOME MALICK† , AND GABRIEL PEYR´E‡

Abstract. This paper provides a set of sensitivity analysis and activity identiﬁcation results for
a class of convex functions with a strong geometric structure, that we coined “mirror-stratiﬁable”.
These functions are such that there is a bijection between a primal and a dual stratiﬁcation of the
space into partitioning sets, called strata. This pairing is crucial to track the strata that are identi-
ﬁable by solutions of parametrized optimization problems or by iterates of optimization algorithms.
This class of functions encompasses all regularizers routinely used in signal and image processing,
machine learning, and statistics. We show that this “mirror-stratiﬁable” structure enjoys a nice
sensitivity theory, allowing us to study stability of solutions of optimization problems to small per-
turbations, as well as activity identiﬁcation of ﬁrst-order proximal splitting-type algorithms. Existing
results in the literature typically assume that, under a non-degeneracy condition, the active set asso-
ciated to a minimizer is stable to small perturbations and is identiﬁed in ﬁnite time by optimization
schemes. In contrast, our results do not require any non-degeneracy assumption:
in consequence,
the optimal active set is not necessarily stable anymore, but we are able to track precisely the set
of identiﬁable strata.We show that these results have crucial implications when solving challenging
ill-posed inverse problems via regularization, a typical scenario where the non-degeneracy condition
is not fulﬁlled. Our theoretical results, illustrated by numerical simulations, allow us to characterize
the instability behaviour of the regularized solutions, by locating the set of all low-dimensional strata
that can be potentially identiﬁed by these solutions.

Key words.

convex analysis, inverse problems, sensitivity, active sets, ﬁrst-order splitting

algorithms, applications in imaging and machine learning

AMS subject classiﬁcations. 65K05, 65K10, 90C25, 90C31.

1. Introduction. Variational methods and non-smooth optimization algorithms
are ubiquitous to solve large-scale inverse problems in various ﬁelds of science and
engineering, and in particular in data science. The non-smooth structure of the op-
timization problems promotes solutions conforming to some notion of simplicity or
low-complexity (e.g. sparsity, low-rank, etc.). The low-complexity structure is often
manifested in the form of a low-dimensional “active set”.
It is thus of prominent
interest to be able to quantitatively characterize the stability of these active sets to
perturbations of the objective function. Of crucial importance is also the identiﬁ-
cation in ﬁnite time of these active sets by iterates of optimization algorithms that
numerically minimize the objective function. This type of problems and results is
referred to as “activity identiﬁcation”; a review of the relevant literature will be pro-
vided in the sequel (at the beginning of each section).
In a nutshell, the existing
identiﬁcation results guarantee a perfect stability of the active set to perturbations,
or a ﬁnite identiﬁcation of this active set via algorithmic schemes, under some non-
degeneracy conditions (see in particular [39, 24, 58]). The crucial non-degeneracy
assumption, that takes generally the form (3.1), can be viewed as a geometric gen-
eralization of strict complementary in non-linear programming. However, as we will
illustrate shortly through some preliminary numerics, such a condition is too strin-
gent and is often barely veriﬁed. The goal of this paper is to investigate the situation
where this non-degeneracy assumption is violated.

∗Normandie Univ, ENSICAEN, CNRS, GREYC, France.
†CNRS and LJK, Grenoble, France
‡CNRS and DMA, ENS Paris, France.

1

1.1. Motivating Examples. In order to better grasp the relevance of our anal-
ysis, let us ﬁrst start with the setting of inverse problems that pervades various ﬁelds
including signal processing and machine learning. We will come back to this setting
in Section 4 with further discussions and references.

Regularized Inverse Problems. Assume one observes

y = ˚y + w ∈ RP where ˚y def.= Φ˚x

(1.1)

where w is some perturbation (called noise) and Φ ∈ RP ×N (called forward operator,
or design matrix in statistics). Solving an inverse problem amounts to recovering
˚x, to a good approximation, knowing y and Φ according to (1.1). Unfortunately, in
general, P can be much smaller than the ambient dimension N , and when P = N ,
the mapping Φ is in general ill-conditioned or even singular.

A classical approach is then to assume that ˚x has some ”low-complexity”, and
to use a prior regularization R promoting solutions with such low-complexity. This
leads to the following optimization problem

E(x, (λ, y)) def.= R(x) +

||y − Φx||2,

(P(λ, y))

min
x∈RN

1
2λ

Let us discuss two popular examples of regularizing functions promoting a low-complexity
structure. These two functions will be used in our numerical experiments.

Example 1 ((cid:96)1 norm). For x ∈ RN , its (cid:96)1 norm reads

R(x) = ||x||1

def.=

|xi|,

N
(cid:88)

i=1

(1.2)

where xi is the i-th entry of x. As advocated for instance by [15, 55], the (cid:96)1 enforces
to have a small number of non-zero
the solutions of (P(λ, y)) to be sparse, i.e.
components. Indeed, the (cid:96)1 norm can be shown to be the tightest convex relaxation
(in the sense of bi-conjugation) of the (cid:96)0 pseudo-norm restricted to the unit Euclidian
ball [37]. Recall that (cid:96)0 pseudo-norm of x ∈ RN measures its sparsity

||x||0

def.= (cid:93) (cid:8)i ∈ {1, . . . , N } : xi (cid:54)= 0(cid:9) .

Sparsity has witnessed a huge surge of interest in the last decades. For instance, in
signal and imaging sciences, one can approximate most natural signals and images
In statistics,
using sparse expansions in an appropriate dictionary (see e.g. [45]).
sparsity is a key toward model selection and interpretability [9].

Example 2 (Nuclear norm). For a matrix x ∈ Rn1×n2 ∼ RN , where N = n1n2,

the nuclear norm is deﬁned as

||x||∗

def.= ||σ(x)||1 =

σi(x),

n
(cid:88)

i=1

where n def.= min(n1, n2) and σ(x) = (σ1(x), . . . , σn(x)) ∈ Rn
+ is the vector of singular
values of x. The nuclear norm is the tightest convex relaxation of the rank (in the sense
of bi-conjugation) restricted to the unit Frobenius ball [36]. This underlies its wide use
to promote solutions of (P(λ, y)) with low rank, where we recall rank(x) def.= ||σ(x)||0.
Low-rank regularization has proved useful for a variety of applications, including con-
trol theory and machine learning; see e.g. [28, 1, 11]

2

Active set(s) identiﬁcation. The above discussed regularizers R are non-smooth
convex functions. This non-smoothness arises in a highly structured fashion and is
usually associated, locally, with some low-dimensional active subset of RN (in many
cases, such a subset is an aﬃne or a smooth manifold). Thus R will favor solutions
of (P(λ, y)) that lie in a low-dimensional active set and would allow the inversion of
the system (1.1) in a stable way. More precisely, one would like that under small per-
turbations w, the solutions of (P(λ, y)) move stably along the active set. A byproduct
of this behaviour is that from an algorithmic perspective, if an optimization algorithm
is used to solve (P(λ, y)), one would hope that the iterates of the scheme identify the
active set in ﬁnite time.

Identifying the low-dimensional active set in a stable way is highly desirable for
several reasons. One reason is that it is a fundamental property most practitioners are
looking for. Typical examples include neurosciences [27] where the goal is to recover
a spike train from neural activity, or astrophysics [53] where it is desired to separate
stars from a background.
In both examples, sparsity can be used as a modeling
hypothesis and the recovery method should comply with it in a stable way. A second
reason is algorithmic since one can also take advantage of the low-dimensionality of
the identiﬁed active set to reduce computational burden and memory storage, hence
opening the door to higher-order acceleration of optimization algorithms (see [38, 46]).
Unfortunately, this desirable behaviour rarely occurs in practical applications.
Yet one still observes some form of partial stability (to be given a rigorous meaning
in Section 3 and 4), as conﬁrmed by the numerical experiment of the next paragraph.

1.2. Illustrative numerical experiment. We consider a simple “compressed
sensing” scenario, with the (cid:96)1 norm R = || · ||1 [12] and the nuclear norm R = || · ||∗ [13]
as regularizers. The operator Φ ∈ RN ×P is drawn uniformly at random from the
standard Gaussian ensemble, i.e. the entries of Φ are independent and identically
distributed Gaussian random variables with zero-mean and unit variance. In the case
R = || · ||1, we set (N, P ) = (100, 50) and the vector to recover ˚x is drawn uniformly
at random among sparse vectors with R0(˚x) def.= ||˚x||0 = 10 and unit non-zero entries.
In the case R = || · ||∗, we set (N, P ) = (400, 300) (n1 = n2 = n = 20) and the
matrix to recover ˚x is drawn uniformly at random among low-rank matrices with
R0(˚x) def.= rank(˚x) = 4 and unit non-zero singular values. For each problem suite
(R, N, P ), 1000 realizations of (˚x, Φ, w) are drawn, and y is then generated according
to (1.1). The entries of the noise vector w are drawn uniformly from a Gaussian with
standard deviation 0.1, we set λ = 0.28 for R = || · ||1 and λ = 10 for R = || · ||∗.

For each realization of (˚x, Φ, w), we solve the associated problem (P(λ, y)) using
CVX [31] to get a high precision; denote x(cid:63)(λ, y) the obtained solutions. We also
solve (P(λ, y)) by the Forward-Backward (FB) scheme which reads in this case1

xk+1 = proxλγR(xk + γΦ∗(y − Φxk)).
In our setting, with γ = 1.8/σmax(Φ∗Φ) and non-emptiness of Argmin(E(·, λ, y), it is
well-known that the sequence (xk)k∈N converges to a point in Argmin(E(·, λ, y)).

The top row of Figure 1.1 displays the histogram of the complexity index excess
δ def.= R0(x(cid:63)(λ, y)) − R0(˚x) which clearly shows that we do not have exact stability

1The deﬁnition of the proximal mapping proxτ R (for τ > 0) is given in (5.3). The proximal

mappings of the (cid:96)1 and nuclear norms are

proxµ||·||1

(x) = (cid:0)sign(xi) max(0, |xi| − µ)(cid:1)

i , proxµ||·||∗ (x) = U diag(proxµ||·||1

(σ(x)))V ∗,

where x = U diag(σ(x))V ∗ is a SVD decomposition of x.

3

δ

f
o

s

m
a
r
g
o
t
s
i
H

k

n
o
i
t
a
r
e
t
i

s
v

)
k
x
(
0
R

R = || · ||1

R = || · ||∗

Fig. 1.1. Top row: Histogram of the complexity index excess δ = R0(x(cid:63)(λ, y)) − R0(˚x) where
x(cid:63)(λ, y) is the solution of (P(λ, y)) with noisy observation y = ˚y + w for a small perturbation w and
a well-chosen parameter value λ = c0||w||. Bottom row: Evolution of the complexity index R0(xk)
where (xk)k∈N is the FB iterates sequence converging to some x(cid:63)(λ, y) ∈ Argmin(E(·, λ, y)).

Indeed, although R0(x(cid:63)(λ, y)) is still rather small, its
under the perturbations w.
value is in most cases larger than the complexity index R0(˚x). The bottom row of
Figure 1.1 depicts the evolution with k of the complexity index R0(xk) for two random
instances of ˚x (in blue and red) corresponding to two diﬀerent values of δ. One can
observe that the iterates identify in ﬁnite time some low-dimensional active set with
a complexity index strictly larger than the one associated to ˚x (the red curves). We
will provide a more detailed discussion of this phenomenon in Section 6.

As we will emphasize in the review of previous work, existing results on sensitivity
analysis and low-complexity regularization only focus the case where R0(x(cid:63)(λ, y)) =
R0(˚x) (i.e. δ = 0), whose underlying claim is that the low-complexity model is per-
fectly stable to perturbations, which typically requires some non-degeneracy assump-
tion to hold. In many situations, however, including the compressed sensing scenario
described above, but also in super-resolution imaging (see [26]) and other challenging
inverse problems, non-degeneracy-type hypotheses are too restrictive. It is the goal
of this paper to develop a more general sensitivity analysis, that goes beyond the
non-degenerate case, to improve our understanding of stability to perturbations of
low-complexity regularized inverse problems.

1.3. Contributions and Outline. Our ﬁrst contribution consists in introduc-
ing, in Section 2, a class of proper lower-semicontinuous (lsc) convex functions that
we coin “mirror-stratiﬁable”. The subdiﬀerential of such a function induces a primal-
dual pairing of stratiﬁcations, which is pivotal to track identiﬁable strata. We discuss
several examples of functions enjoying such a structure. With this structure at hand,
we then turn to the main result of this paper, formalized in Theorem 1, and on which

4

all the others rely. This result shows ﬁnite enlarged activity identiﬁcation for a mirror-
stratiﬁable function without the need of any non-degeneracy condition. In addition,
the identiﬁable strata are precisely characterized in terms primal-dual optimal solu-
tions. Sections 3, 4 and 5 instantiate this abstract result for a set of concrete problems,
respectively: sensitivity of composite ”smooth+non-smooth” optimization problems
(Theorem 2), enlarged activity identiﬁcation by proximal splitting schemes such as
Forward-Backward and Douglas-Rachford algorithms (Theorems 4 and 5), and ﬁnally,
enlarged identiﬁcation for regularized inverse problems (Theorem 3). Before stating
these results, we make a short review of the associated literature. Finally Section 6
illustrates these theoretical ﬁndings with numerical experiments, in particular in a
compressed sensing scenario involving the (cid:96)1 and nuclear norms as regularizers. Fol-
lowing the philosophy of reproducible research, all the code to reproduce the ﬁgures
of this article is available online2.

2. Mirror-Stratiﬁable Functions. In this section, we introduce the class of
mirror-stratiﬁable convex functions, and present the sensitivity property they provide
(Theorem 1). We also illustrate that many popular regularizers used in data science
are mirror-stratiﬁable. Most the results of this section (in particular all the examples)
are easy to obtain by basic calculus; we therefore do not give these proofs in the text
and we gather some of them in Appendix A.

2.1. Stratiﬁcations and deﬁnitions. We start with recalling the following

standard deﬁnition of stratiﬁcation.

Definition 1 (Stratiﬁcation). A stratiﬁcation of a set D ⊂ RN is a ﬁnite
partition M = {Mi}i∈I such that for any partitioning sets (called strata) M and M (cid:48)
we have

M ∩ cl(M (cid:48)) (cid:54)= ∅ =⇒ M ⊂ cl(M (cid:48)).

If the strata are open polyhedra, then M is a polyhedral stratiﬁcation, and if they are
C 2-smooth manifolds then M entails a C 2-stratiﬁcation.

A stratiﬁcation is naturally endowed with the partial ordering (cid:54) in the sense that

M (cid:54) M (cid:48) ⇐⇒ M ⊂ cl(M (cid:48)) ⇐⇒ M ∩ cl(M (cid:48)) (cid:54)= ∅.

(2.1)

The relation is clearly reﬂexive and transitive. Furthermore, we have

cl(M ) =

(cid:91)

M (cid:48).

M (cid:48)(cid:54)M

(2.2)

An immediate consequence of Deﬁnition 1 is that for each point x ∈ D, there
is a unique stratum containing x, denoted Mx. Indeed, suppose that there are two
non-empty open strata M1 and M2 such that M1 ∩ M2 = {x}, and thus M1 ∩ M2 (cid:54)= ∅.
This implies, using (2.1), that M1 (cid:54) M2 and M2 (cid:54) M1, and thus M1 = M2.

At this stage, it is worth emphasizing that the strata are not needed to be mani-
folds in the rest of the paper. It is however the case that in many practical cases that
we will discuss, strata are indeed manifolds, and sometimes aﬃne manifolds.

Example 3 (Polyhedral sets and functions). A partition of a polyhedral set into
its open faces induces a natural ﬁnite polyhedral stratiﬁcation of it. In turn, let R :

2Code available at https://github.com/gpeyre/2017-SIOPT-stratification

5

RN → R def.= R∪{+∞} be a polyhedral function, and consider a polyhedral stratiﬁcation
of its epigraph, which is a polyhedral set in RN +1. Projecting all polyhedral strata onto
the ﬁrst N -coordinates one obtains a ﬁnite polyhedral stratiﬁcation of dom(R).

Remark 1. The previous example extends to semialgebraic sets and functions,
which are known to induce stratiﬁcations into ﬁnite disjoint unions of manifolds. In
fact, this holds for any tame class of sets/functions; see, e.g., [18].

We now single out a speciﬁc set of strata, called active strata, that will play a

central role for ﬁnite enlarged activity identiﬁcation purposes.

Definition 2 (Active strata). Given a stratiﬁcation M of D ⊂ RN and a point

x ∈ D, a stratum M ∈ M is said active at x if x ∈ cl(M ).

Proposition 1. Given a stratiﬁcation M of D and a point x ∈ D. Then there
exists δ > 0 such that set of strata Mx(cid:48) for any x(cid:48) such that ||x(cid:48) − x|| < δ, coincides
with the set of strata M (cid:62) Mx and with the set of active strata at x.

2.2. Mirror-Stratiﬁable functions. Following [19, Section 4], we deﬁne the

key correspondence operator whose role will become apparent shortly.

Definition 3. Let R : RN → R be a proper lsc convex function. The associated

correspondence operator JR : 2RN

→ 2RN

JR(S) def.=

is deﬁned as
(cid:91)

ri(∂R(x)),

x∈S

where ∂R is the subdiﬀerential of R.

Observe that, by deﬁnition, JR is increasing for set inclusion

S ⊂ S(cid:48) =⇒ JR(S) ⊂ JR(S(cid:48)).

(2.3)

For this operator to be useful in sensitivity analysis, we will further impose that JR
is decreasing for the partial ordering (cid:54) in (2.1), as captured in our main deﬁnition.
This is a key requirement that captures the intuitive idea that the larger a primal
stratum, the smaller its image by JR in the dual space.

In the following, we will denote R∗ the Legendre-Fenchel conjugate of R.
Definition 4 (Mirror-stratiﬁable functions). Let R : RN → R be a proper lsc
convex function; we say that R is mirror-stratiﬁable with respect to a (primal) strat-
iﬁcation M = {Mi}i∈I of dom(∂R) and a (dual) stratiﬁcation M∗ = {M ∗
i }i∈I of
dom(∂R∗) if the following holds:

(i) Conjugation induces a duality pairing between M and M∗, and JR : M →

M∗ is invertible with inverse JR∗ , i.e. ∀M ∈ M, M ∗ ∈ M∗ we have

M ∗ = JR(M ) ⇐⇒ JR∗ (M ∗) = M.

(ii) JR is decreasing for the relation (cid:54): for any M and M (cid:48) in M

M (cid:54) M (cid:48) ⇐⇒ JR(M ) (cid:62) JR(M (cid:48)).

The primal-dual stratiﬁcations that make R mirror-stratiﬁable are not unique, as
will be exempliﬁed in Remark 3. Though the deﬁnition is formal and the assumptions
looks restrictive, this class include many useful examples, as illustrated in the next
section. We ﬁnish this section with a remark, used in the sequel.

Remark 2 (Separability). For each m = 1, . . . , L, suppose that the proper lsc
convex function Rm : RNm → R is mirror-stratiﬁable with respect to stratiﬁcations
Mm and M∗
m. Then it is easy to show, using standard subdiﬀerential and conjugacy
calculus, that the function R : (xm)1(cid:54)m(cid:54)L ∈ RN1 × · · · × RNL (cid:55)→ (cid:80)L
m=1 Rm(xm) is
mirror-stratiﬁable with stratiﬁcations M1 × · · · × ML and M∗

1 × · · · × M∗
L.

6

2.3. Examples. The notion of a mirror-stratiﬁable function looks quite rigid.
However, many of the regularization functions routinely used in data science are
mirror-stratiﬁable. Let us provide some relevant examples in this section. In partic-
ular, the (cid:96)1-norm and the nuclear norm will be used in the numerical experiments.

2.3.1. Legendre functions. A lsc convex function R : RN → R is said to be a
Legendre function (see [50, Chapter 26]) if (i) it is diﬀerentiable and strictly convex
on the interior of its domain int(dom(R)) (cid:54)= ∅, and (ii) ||∇R(xk)|| → +∞ for every
sequence (xk)k∈N ⊂ int(dom(R)) converging to a boundary point of dom(R). Many
functions in convex optimization are Legendre; most notably, quadratic functions
and the log barrier of interior point methods. It was shown in [50, Theorem 26.5]
that R is Legendre if and only if its conjugate R∗ is Legendre, and that in this case
∇R is a bijection from int(dom(R)) to int(dom(R∗)) with ∇R∗ = (∇R)−1. As a
consequence, a Legendre function R is mirror-stratiﬁable with M = {int(dom(R))}
and M∗ = {int(dom(R∗))}.

2.3.2. (cid:96)1-norm. Let R : x ∈ R (cid:55)→ |x|, whose conjugate R∗ = ι[−1,1]. It follows
that R is mirror-stratiﬁable with M = { ] − ∞, 0[, {0}, ]0, +∞[} and M∗ = {{−1}, ] −
1, 1[, {+1}}. Using Remark 2, the next result is clear.

Lemma 1. The (cid:96)1-norm and its conjugate ι[−1,+1]N are mirror-stratiﬁable with re-
spect to the stratiﬁcations M = { ] − ∞, 0[, {0}, ]0, +∞[}N of RN and M∗ = {{−1}, ]−
1, 1[, {+1}}N of [−1, +1]N .

A graphical illustration of this lemma in dimension 2 is displayed in Figure 2.1(a).
Remark 3 (Non-uniqueness of stratiﬁcations).
In general, we do not have
uniqueness of stratiﬁcations inducing mirror-stratiﬁable functions. To illustrate this,
let us consider the (cid:96)1 norm. Take the partitions M = { ] − ∞, 0[∪]0, +∞[, {0}}N
of RN and M∗ = {{−1, +1}, ] − 1, 1[}N . These are valid stratiﬁcations though
the strata are not connected. Other possible valid stratiﬁcations are given by strata
Mj = (cid:8)x ∈ RN : ||x||0 = j(cid:9) and M ∗
j = J||·||1 (Mj), for j = 0, 1, . . . , N . It is immediate
to check that the (cid:96)1-norm is also mirror-stratiﬁable with respect to these stratiﬁcations.
However, as devised above, it is in general not wise to take such large strata as they
lead to less sharp localization and sensitivity results.

Remark 4 (Instability under the sum rule). The family of mirror-stratiﬁable
functions is unfortunately not stable under the sum. As a simple counter-example,
consider the pair of conjugate functions on R: R(x) = |x| + x2/2 and R∗(u) =
(max{|u| − 1, 0})2/2. We obviously have dom(∂R) = dom(∂R∗) = R. However we
observe that JR(R) = R \ {−1, 1}. This yields that JR cannot be a pairing between
any two stratiﬁcations of R, and therefore R cannot be mirror-stratiﬁable.

2.3.3. (cid:96)1,2 norm. The (cid:96)1,2 norm, also known as the group Lasso regularization,
has been advocated to promote group/block sparsity [60], i.e.
it drives all the coef-
ﬁcients in one group to zero together. Let B a non-overlapping uniform partition of
{1, . . . , N } into K blocks. The (cid:96)1,2 norm induced by the partition B reads

||x||B =

||xB||2

(cid:88)

B∈B

where xB is the restriction of x to the entries indexed by the block B. This is again
a separable function of the xB’s. One can easily show that the (cid:96)2 norm on R|B| and
its conjugate, the indicator of the unit (cid:96)2-ball B
in R|B|, are mirror-stratiﬁable
with respect to the stratiﬁcations {{0}, R|B| \ {0}} of RN and {S|B|−1

, int(B

(cid:96)|B|
2

)}

(cid:96)2

(cid:96)|B|
2

7

(a)

(b)

8

Fig. 2.1. Graphical illustration of mirror-stratiﬁcation for two norms: (a) (cid:96)1 norm in dimen-

sion 2; (b) (cid:96)1,2 norm in dimension 3 (with two blocks, of respectively sizes 1 and 2).

(cid:96)2

(cid:96)|B|
2

), where S|B|−1

of B
is the corresponding unit sphere. In turn, the (cid:96)1,2 norm is
mirror-stratiﬁable with respect to the stratiﬁcations M = {{0}, R|B| \ {0}}K and
M∗ = {S|B|−1, int(B
)}K. An illustration of this result in dimension 3 is portrayed
in Figure 2.1(b).

(cid:96)|B|
2

2.3.4. Nuclear norm. Let us come back to the nuclear norm deﬁned in Exam-
ple 2. For simplicity, we assume n1 = n2 = n. Let M be a stratum of the (cid:96)1 norm
stratiﬁcation. In the following, we denote by M sym its symmetrization. We observe
that σ−1(M sym) = {X ∈ Rn×n : rank(X) = ||z||0, z ∈ M }. So many inverse images
σ−1(M sym) of strata M sym coincide. This suggests the following n + 1 stratiﬁcations:
for a given i ∈ {0, . . . , N }

Mi = (cid:8)X ∈ Rn×n : rank(X) = i(cid:9)
i = {U ∈ Rn : σ1(U ) = · · · = σi(U ) = 1, ∀j > i, |σj(U )| < 1} .
M ∗

This yields that || · ||∗ is mirror-stratiﬁable with respect to these stratiﬁcations.

2.3.5. Polyhedral Functions. We here establish mirror-stratiﬁability of poly-
hedral functions, including the (cid:96)1 norm, the (cid:96)∞ norm and anisotropic TV semi-norm.

A polyhedral function R : RN → R can be expressed as

R(x) = max

{(cid:104)ai, x(cid:105) − αi} + ι(cid:84)

i=k+1,...,m{x : (cid:104)ai, x(cid:105)−αi(cid:54)0}(x).

(2.4)

i=1,...,k

For any x ∈ dom(R), introduce the two sets of indices

I max(x) = {i = 1, . . . , m : (cid:104)ai, x(cid:105) − αi = R(x)} ,
I feas(x) = {i = 1, . . . , m : (cid:104)ai, x(cid:105) = αi} .

For a given index set I ⊂ {1, . . . , m}, we consider the aﬃne manifold

MI

def.= (cid:8)x ∈ dom(R) : I max(x) ∩ I feas(x) = I(cid:9) .

(2.5)

We see that some MI may be empty and that M = {MI }I is a stratiﬁcation of
dom(R). The stratum MI is characterized by the optimality part I max = I ∩{1, . . . , k}
and the feasibility part I feas = I ∩ {k + 1, . . . , m} of I. Similarly we deﬁne

M ∗

I = ri(conv {ai

: i ∈ I max}) + ri(cone (cid:8)ai

: i ∈ I feas(cid:9)).

Let us formalize in the next proposition a result alluded to in [19].

Proposition 2. A polyhedral function R is mirror-stratiﬁable with respect to its

naturally induced stratiﬁcations {MI }I and {M ∗

I }I .

Remark 5 (Back to || · ||1). As we anticipated, Proposition 2 subsumes Lemma 1

as a special case. To see this, observe that

||x||1 = max
||u||∞(cid:54)1

(cid:104)u, x(cid:105) =

max
u∈{−1,+1}N

(cid:104)u, x(cid:105) ,

which is of the form (2.4) with k = m = 2N . Thus there are 22N
aﬃne manifolds as
deﬁned by (2.5). But many of them are empty: there is only 3N (non-empty, distinct)
manifolds in the stratiﬁcation of RN , and they coincide with those in Lemma 1.

2.3.6. Spectral Lifting of Polyhedral Functions. As we discussed in Re-
mark 4, JR may fail to induce a duality pairing between stratiﬁcations of R and R∗,
in which case R cannot be mirror-stratiﬁable. Hence, for this type of duality to hold,
one needs to impose stringent strict convexity conditions. To avoid this, and still
aﬀord a large class of mirror-stratiﬁable functions that are of utmost in applications,
we consider spectral lifting of polyhedral functions, in the same vein as [19] did it for
partial smoothness.

A matrix function R : RN =n×n → R is said to be a spectral lift of a polyhedral
function if there exists a polyhedral function Rsym : Rn → R, invariant under signed
permutation of its coordinates, such that R = Rsym ◦ σ where σ computes the singular
values of a matrix. Associated to M = {MI }I the (polyhedral) stratiﬁcation induced
by Rsym, we consider its symmetrized stratiﬁcation. We deﬁne the symmetrization of
M ∈ M, as the set M sym

M sym = (cid:8)x ∈ RN : ∃y ∈ M such that for all i there exists j with |xi| = |yj|(cid:9) .

The next result is a corollary of the main result of [19].

Proposition 3. A spectral function R = Rsym ◦ σ is mirror-stratiﬁable with

respect to the smooth stratiﬁcation {σ−1(M sym)} and its image by JR.

Remark 6 (Back to || · ||∗). The nuclear norm is a spectral lift of the (cid:96)1 norm.
Therefore, one can recover mirror-stratiﬁcation of the nuclear norm, with the strati-
ﬁcations given in Section 2.3.4, by putting together Lemma 1 and Proposition 3.

9

2.4. Activity Identiﬁcation for Mirror-Stratiﬁable Functions. To our
point of view, the notion of mirror-stratiﬁbility deserves a special study in view of
the following simple but powerful geometrical observation. We state it as a theorem
because of its utmost importance in the subsequent developments of this paper.

Theorem 1 (Enlarged activity identiﬁcation). Let R be a proper lsc convex
function which is mirror-stratiﬁable with respect to primal-dual stratiﬁcations M =
{M } and M∗ = {M ∗}. Consider a pair of points (¯x, ¯u) and the associated strata M¯x
and M ∗
¯u. If the sequence pair (xk, uk) → (¯x, ¯u) is such that uk ∈ ∂R(xk), then for k
large enough, xk is localized in a speciﬁc set of strata such that

M¯x (cid:54) Mxk

(cid:54) JR∗ (M ∗

¯u).

(2.6)

Proof. By assumption on R, ∂R is sequentially closed [50, Theorem 24.4] and

thus ¯u ∈ ∂R(¯x). Now, since xk is close to ¯x, upon invoking Proposition 1, we get

Mxk

(cid:62) M¯x

M ∗
uk

(cid:62) M ∗
¯u.

which shows the left-hand side of (2.6). Similarly, we have

(2.7)

Using the fact that ∂R(xk) is a closed convex set together with (2.3) leads to

uk ∈ ∂R(xk) = cl(ri(∂R(xk))) = cl(JR({xk})) ⊂ cl(JR(Mxk ))

which entails Muk
decreasing for the relation (cid:54), we get

(cid:54) JR(Mxk ). Using ﬁnally (2.7) and that by deﬁnition, JR is

M ∗
¯u

(cid:54) M ∗
uk

(cid:54) JR(Mxk ) =⇒ Mxk

(cid:54) JR∗ (M ∗

¯u),

whence we deduce the right-hand side of (2.6).

Mirror-stratiﬁcation thus allows us to prove the simple but powerful claim of
Theorem 1. As we will see in the rest of the paper, this result will be the backbone
to prove sensitivity and ﬁnite enlarged activity identiﬁcation results in absence of
non-degeneracy.

3. Sensitivity of Composite Optimization Problems. In this section, we

consider a parametric convex optimization problem of the form

E(x, p) def.= F (x, p) + R(x),

min
x∈RN

(P(p))

depending on the parameter vector p ∈ Π, where Π is the parameter set, an open
subset of a ﬁnite dimensional linear space. Sensitivity analysis studies the properties
of solutions x(cid:63)(p) of (P(p)) (assuming they exist) to perturbations of the parameters
vector p ∈ Π around some reference point ˚p. In Section 3.1, we brieﬂy review the
existing results on this topic and their limitations. We then introduce in Section 3.2
our new results obtained owing to the mirror-stratiﬁable structure.

3.1. Existing sensitivity results. Classical sensitivity results (see e.g. [7, 47,
21]) study the regularity of the set-valued map p (cid:55)→ x(cid:63)(p). A typical result proves
Lipschitz continuity of this map provided that E is smooth enough and E(·, ˚p) has
a local second-order (quadratic) growth at x(cid:63)(˚p), i.e. that there exists some c > 0
such that E(x, ˚p) (cid:62) E(x(cid:63)(˚p), ˚p) + c||x − x(cid:63)(˚p)||2 for x nearby x(cid:63)(˚p). For C 2-smooth
10

optimization, this growth condition is equivalent to positive deﬁniteness of the hessian
of E with respect to x evaluated at (x(cid:63)(˚p), ˚p) [29]. For classical smooth constrained
optimization problems, activity is captured by the subset of active inequality con-
straints. Under reasonable non-degeneracy conditions (see, for example, [29]), this
active set is stable under small perturbations to the objective.

A nice nonsmooth sensitivity theory is based on the notion of partial smooth-
ness [39]. Partial smoothness is an intrinsically geometrical assumption which, infor-
mally speaking, says that E behaves smoothly along an active manifold and sharply in
directions normal to the manifold. Furthermore, under a non-degeneracy assumption
at a minimizer (see (3.1)), it allows appealing statements of second-order optimality
conditions (including second-order generalized diﬀerentiation) and associated sensi-
tivity analysis around that minimizer [39, 40].

Let ∂E(x, p) be the subdiﬀerential of E according to x. Specializing the result of
[24, Proposition 8.4] to a proper lsc convex function E(·, p), one can show that C 2-
partial smoothness of E(·, p) at x(cid:63)(˚p) relative to some ﬁxed manifold (independent of
p) for 0, together with the non-degeneracy assumption

0 ∈ ri(∂E(x(cid:63)(˚p), ˚p))

(3.1)

is equivalent to the existence of an identiﬁable C 2-smooth manifold, i.e. for x(cid:63)(p) and
u(cid:63)(p) ∈ ∂E(x(cid:63)(p), p) close enough to x(cid:63)(˚p) and 0, x(cid:63)(p) lives on the active/partly
smooth manifold of x(cid:63)(˚p). If these assumptions are supplemented with a quadratic
growth condition of E (see above) along the active manifold, then one also has C 1
smoothness of the single-valued mapping p (cid:55)→ x(cid:63)(p) [39, Theorem 5.7].
It can be
deduced from [23, Corollary 4.3] that for almost all linear perturbations of lsc convex
semialgebraic functions, the non-degeneracy and quadratic growth conditions hold.
However, this genericity fails to hold for many cases of interest. As an example, con-
sider E of (P(λ, y)) with λ > 0 ﬁxed and p = y. If R is a proper lsc convex and
semialgebraic function, one has from [23] that for Lebesgue almost all Φ∗y, prob-
lem (P(λ, y)) has at most one minimizer at which furthermore non-degeneracy and
quadratic growth hold. Of course genericity in terms of Φ∗y does not imply that in
terms of y, which is our parameter of interest. Not to mention that we supposed λ
ﬁxed while it is not in many cases of interest.

3.2. Sensitivity analysis without non-degeneracy. For a ﬁxed p, (P(p)) is
a standard composite optimization problem. Here, we assume that the objective is the
sum of a C 1(RN ) convex function F (·, p) and a nonsmooth proper lsc convex function
R. We denote ∇F (x, p) the gradient of F (·, p) at x.

We are going to show that if the minimizer x(cid:63)(˚p) is unique, slight perturbations
p of ˚p generate solutions x(cid:63)(p) that are in a “controlled” stratum Mx(cid:63)(p) precisely
sandwiched to extreme strata deﬁned from a primal-dual pair associated to ˚p.

Theorem 2 (Sensitivity analysis with mirror-stratiﬁable functions). Let ˚p be a
given point in the parameter space Π. Assume that: (i) E(·, ˚p) has a unique mini-
mizer x(cid:63)(˚p), (ii) E is lsc on RN × Π, (iii) E(x(cid:63)(˚p), ·) is continuous at ˚p, (iv) ∇F
is continuous at (x(cid:63)(˚p), ˚p), and (v) E is level-bounded3 in x uniformly in p locally
around ˚p. If R is mirror-stratiﬁable according to (M,M∗), then for all p close to ˚p,

3 Recall from [51, Deﬁnition 1.16] that the function E : RN × Π is said to be level-bounded in x
locally uniformly in p around ˚p if for each c ∈ R, there exists a neighbourhood V of ˚p and a bounded
set Ω such that the sublevel set (cid:8)x ∈ RN : E(x, p) (cid:54) c(cid:9) ⊂ Ω for all p ∈ V.

11

(a)

(b)

12

Fig. 3.1. Graphical illustration of Theorem 2 for a projection problem on the (cid:96)∞ ball. (a)

corresponds to a degenerate situation while (b) to a non-degenerate one.

any minimizer x(cid:63)(p) of E(·, p) is localized as follows

Mx(cid:63)(˚p) (cid:54) Mx(cid:63)(p) (cid:54) JR∗ (M ∗

u(cid:63)(˚p)) where u(cid:63)(˚p) def.= −∇F (x(cid:63)(˚p), ˚p).

(3.2)

Proof. Le (pk)k∈N ⊂ Π be a sequence of parameters converging to ˚p. By assump-
tion on F and R, E(·, p) is proper for any p. Since it is also lsc and is level-bounded
in x uniformly in p locally around ˚p by conditions (ii) and (v). It follows from [51,
Theorem 1.17(a)] that Argmin E(·, pk) is non-empty and compact, and, in turn, any
sequence (x(cid:63)
k)k∈N of minimizers is bounded. We consider a subsequence, which for
simplicity, we denote again x(cid:63)

k → ¯x. We then have

E(¯x, ˚p)

(cid:54)
condition (ii)
(cid:54)
Optimality

lim inf
k

E(x(cid:63)

k, pk)

lim inf
k

E(x(cid:63)(˚p), pk)

=
condition (iii)

lim
k

E(x(cid:63)(˚p), pk) = E(x(cid:63)(˚p), ˚p).

By the uniqueness condition (i), we conclude that ¯x = x(cid:63)(˚p). Let u(cid:63)
k
Since ∇F is continuous at (˚x, ˚p) by assumption (iv), one has u(cid:63)
optimality condition of problem (P(p)) reads u(cid:63)
position to invoke Theorem 1 to conclude.

k ∈ ∂R(x(cid:63)

def.= −∇F (x(cid:63)
k, pk).
k → ˚u. The ﬁrst-order
k). Hence we are now in

Remark 7 (Single manifold identiﬁcation). In the special case where we have

the non-degeneracy condition

u(cid:63)(˚p) = −∇F (x(cid:63)(˚p), ˚p) ∈ ri(∂R(x(cid:63)(˚p))),

(3.3)

our result simpliﬁes and we recover the known exact identiﬁcation result. To see this,
we note that (3.3) also reads u(cid:63)(˚p) ∈ JR({x(cid:63)(˚p)}), and thus M ∗
u(cid:63)(˚p) = JR({x(cid:63)(˚p)}).
In this case, (3.2) becomes Mx(cid:63)(p) = M˚x for all p close to ˚p. Such a result was also
established in [35, 40] under condition (3.3), when F (·, ˚p) is also locally C 2 around
x(cid:63)(˚p) and R is partly smooth at x(cid:63)(˚p) relative to a C 2-smooth manifold Mx(cid:63)(˚p). An
example of this non-degenerate scenario is shown in Figure 3.1(b). Our result covers
the more delicate degenerate situation where u(cid:63)(˚p) might be on the relative boundary
of the subdiﬀerential, but requires the stronger mirror-stratiﬁability structure on the
non-smooth part. However, the active set at x(cid:63)(p) is in general not unique for all p
close to ˚p, hence the terminology enlarged activity.

Example 4. The result of Theorem 2 is illustrated in Figure 3.1, for the projec-
tion problem on a (cid:96)∞ ball, in both the degenerate and the non-degenerate situations.
More precisely, we take E(x, p) = 1
2 ||x−p||2 +ιB(cid:96)∞ (x), so that u(cid:63)(p) = p−x(cid:63)(p), where
x(cid:63)(p) = PB(cid:96)∞ (p). Figure 3.1(a) illustrates the degenerate case where u(cid:63)(˚p) belongs to
the relative boundary of the normal cone of B(cid:96)∞ at x(cid:63)(˚p). Taking a perturbation p
around ˚p entails that x(cid:63)(p) belongs either to M1 = Mx(cid:63)(˚p) or to the enlarged stratum
M2 = JR∗ (M ∗
2 ). In the non-degenerate case of Figure 3.1(b) presented
in Remark 7, the optimal solution x(cid:63)(p) is always on M1 for small perturbations.

u(cid:63)(˚p)) = JR∗ (M ∗

Remark 8 (Quadratic growth). We can also establish the Lipschitz continuity
of x(cid:63)(p) under the conditions of Theorem 2, under an additional second-order growth
condition, just as in classical sensitivity analysis (see [7, Section 4.2.1]). Let us
assume that there exists a neighbourhood V of x(cid:63)(˚p) and κ > 0 such that

E(x, ˚p) (cid:62) E(x(cid:63)(˚p), ˚p) + κ||x − x(cid:63)(˚p)||2

∀x ∈ V.

(3.4)

k → x(cid:63)(˚p) for pk → ˚p (see the proof of Theorem 2), we have x(cid:63)

Since x(cid:63)
k ∈ V for k
large enough. If, moreover, ∇F (x, ·) is Lipschitz-continuous in a neighbourhood of ˚p
with Lipschitz constant ν independent of x ∈ V, we get

κ||x(cid:63)

k − x(cid:63)(˚p)||2 (cid:54) E(x(cid:63)

k, ˚p) − E(x(cid:63)(˚p), ˚p)
= (cid:0)(E(x(cid:63)(˚p), pk) − E(x(cid:63)

k, pk)) − (E(x(cid:63)(˚p), ˚p) − E(x(cid:63)

k, ˚p))(cid:1)

− (cid:0)E(x(cid:63)(˚p), pk) − E(x(cid:63)

k, pk)(cid:1)

(x(cid:63)

k minimizes E(·, pk)) (cid:54) (E(x(cid:63)(˚p), pk) − E(x(cid:63)
(By (P(p))) = (F (x(cid:63)(˚p), pk) − F (x(cid:63)

k, pk)) − (E(x(cid:63)(˚p), ˚p) − E(x(cid:63)
k, pk)) − (F (x(cid:63)(˚p), ˚p) − F (x(cid:63)

k, ˚p)
k, ˚p))

(Mean-Value Theorem) (cid:54) (cid:0) sup

||∇F (x, pk) − ∇F (x, ˚p)||(cid:1)||x(cid:63)

k − x(cid:63)(˚p)||

x∈V

(cid:54) ν||pk − ˚p||||x(cid:63)

k − x(cid:63)(˚p)||.

whence we conclude that

d(x(cid:63)(˚p), Argmin E(·, pk)) (cid:54) ν/κ||pk − ˚p||.

4. Regularized Inverse Problems. A typical context where our framework of
mirror-stratiﬁability is of usefulness is that of studying stability to noise of regularized
linear inverse problems. We come back to the situation presented in introduction:

13

studying stability issues to perturbed observations of the form y = ˚y + w amounts
to analyzing sensitivity of the minimizers and the optimal value function in (P(λ, y))
when the parameter p = (λ, y) evolves around the reference point ˚p = (0, ˚y). Unlike
the usual sensitivity analysis theory setting recalled in the previous section (e.g. [7]),
here the objective E may not even be continuous at ˚p.

We provide hereafter some pointers to the relevant literature, then we develop
our sensitivity results for mirror-stratiﬁable functions. These results involve primal
and dual solutions to the noiseless problem

E(x, (0, y)) def.= R(x)

s.t. Φx = y,

(P(0, y))

min
x∈RN

4.1. Existing sensitivity results for regularized Inverse Problems.
Lipschitz stability. Assume there exists a dual multiplier η ∈ RP (sometimes
referred to as a “dual certiﬁcate”) for the noiseless constrained problem (P(0, y))
taken at y = ˚y such that Φ∗η ∈ ∂R(˚x). The latter condition is equivalent to ˚x being
a minimizer of (P(0, y)). This condition goes by the name of the “source” or “range”
condition in the inverse problems literature. It has been widely used to derive stability
results in terms of ||Φx(cid:63)(λ, y)−Φ˚x|| or R(x(cid:63)(λ, y))−R(˚x)−(cid:104)Φ∗η, x(cid:63)(λ, y) − ˚x(cid:105); see [52]
and references therein. To aﬀord stability in terms of ||x(cid:63)(λ, y) −˚x|| directly, the range
condition has to be strengthened to its non-degenerate version Φ∗η ∈ ri(∂R(˚x)). It
has been shown that this condition implies that the set-valued map (λ, y) (cid:55)→ x(cid:63)(λ, y)
is Lipschitz-continuous at (0, ˚y); see [32] and [58].

Active set stability. In the case where R is partly smooth, one can approach
an even more complete sensitivity theory by studying stability of the partly smooth
manifold of R at ˚x. In particular, it can be shown from that if an appropriate non-
degeneracy assumption holds, see [59], then problems (P(0, y)) and (P(λ, y)) have
unique minimizers (respectively ˚x and x(cid:63)(λ, y)), and x(cid:63)(λ, y) lies on the partly smooth
manifold of R at ˚x. Observe that compared to Lipschitz stability, active set stability
is more demanding as the non-degeneracy condition has to hold for a speciﬁc dual
multiplier, which is obviously more stringent. This type of results has appeared many
times in the literature for special cases, e.g.
for the (cid:96)1 norm [30, 61], the nuclear
norm [1]. The work in [57, 59] has uniﬁed all these results.

Non-degeneracy in practice for deconvolution and compressed sensing. The above
results require that some abstract non-degeneracy condition holds, which imposes
strict limitations on practical situations. In particular, when Φ is a convolution oper-
ator and ˚x is sparse, [10] studies Lipschitz stability and [25] support stability. In this
setting, the non-degeneracy condition holds whenever the non-zero entries are sepa-
rated enough, which is not often veriﬁed. Another setting where this stability theory
has been applied in when Φ is drawn from a random matrix ensemble, i.e. compressed
sensing. For a variety of partly smooth regularizers (including the (cid:96)1, nuclear and (cid:96)1,2
norms), the non-degeneracy condition holds with high probability if, roughly speak-
ing, the sample size P is suﬃciently larger than the “dimension” of the active set at
˚x (see e.g. [12, 22]). Again this is a clear limitation as illustrated in Section 1.2.

4.2. Primal and dual problems. Suppose we have observations of the form
(1.1), and we want to recover ˚x (or a provably good approximation of it). As advocated
in Section 1.1, a popular approach is to adopt a regularization framework which can
be cast as the optimization problem (P(λ, y)) (for λ > 0) and (P(0, y)) (when λ = 0).
In the sequel, we assume that

R∞(z) > 0,

∀z ∈ ker(Φ) \ {0},

(4.1)

14

where R∞ is the asymptotic (or recession) function of R, deﬁned as

R∞(z) def.= lim
t→+∞

R(x + tz) − R(x)
t

,

∀x ∈ dom(R).

Condition (4.1) is a necessary and suﬃcient condition for the set of minimizers of
(P(λ, y)) and (P(0, y)) to be non-empty and compact [56, Lemma 5.1]. It is satisﬁed
for example when R is coercive.

Remark 9 (Discontinuity of F ). Letting p def.= (λ, y) ∈ Π def.= R+ × RP , we see

that (P(λ, y)) and (P(0, y)) are instances of (P(p)), by setting

F (x, p) def.=

(cid:40) 1

2λ ||y − Φx||2
ιHy (x)

if λ > 0,
if λ = 0,

where Hy

def.= (cid:8)x ∈ RN : Φx = y(cid:9) .

(4.2)

The corresponding function F considered does not obey the assumptions of Theorem 2.
Indeed, the parameter set Π is not open, with ˚p = (0, ˚y) that lives on the boundary of
Π, and F is only lsc at such ˚p (because of the aﬃne constraint Φx = y). The main
consequence will be that one can no longer allow p to vary freely nearby ˚p.

In order to study the sensitivity of solutions, we look at the Fenchel-Rockafellar

dual problem, which reads (for all λ (cid:62) 0)

(cid:104)q, y(cid:105) −

||q||2 − R∗(Φ∗q).

max
q∈RP

λ
2

(D(λ, y))

We denote D(λ, y) the set of solutions of (D(λ, y)). Note that for λ > 0, thanks to
strong concavity, there is a unique dual solution q(cid:63)(λ, y), i.e. D(λ, y) = {q(cid:63)(λ, y)}. We
also have from the primal-dual extremality relationship that for any primal solution
x(cid:63)(λ, y) of (P(λ, y)),

q(cid:63)(p) =

y − Φx(cid:63)(p)
λ

and Φ∗q(cid:63)(p) ∈ ∂R(x(cid:63)(p)).

(4.3)

While (D(0, y)) is the dual of (P(0, y)), it is important to realize that it is not the limit
of (D(λ, y)) in the sense that its set of dual solutions is in general not a singleton.
Lemma 3 hereafter singles out a speciﬁc dual optimal solution (sometimes called
“minimum norm certiﬁcate”) deﬁned by

˚q(0, y) = argmin

{||q|| : Φ∗q ∈ ∂R(x(cid:63)(0, y))} = argmin

{||q|| : q ∈ D(0, y)} .

(4.4)

q∈RP

q∈RP

The following two important lemmas ensure the convergence of the solutions to the
primal and dual problems as λ → 0 and as ||y − ˚y|| → 0.

Lemma 2 (Primal solution convergence). Assume that ˚x is the unique solution

to (P(0, ˚y)). For any sequence of parameters pk = (λk, yk) with λk > 0 such that

(cid:18) ||yk − ˚y||2
λk

(cid:19)

, λk

−→ (0, 0),

and any solution x(cid:63)(pk) of (P(pk)), we have x(cid:63)(pk) → ˚x.

Proof. Denote x(cid:63)
k

can extract a converging subsequence, which for simplicity, we denote again x(cid:63)
Optimality of x(cid:63)

k)k∈N is a bounded sequence by (4.1), one
k → ¯x.

def.= x(cid:63)(pk). Since (x(cid:63)

k implies that
k) (cid:54) 1
2λk

R(x(cid:63)

||yk − Φx(cid:63)

k||2 + R(x(cid:63)

||yk − ˚y||2 + R(˚x).

(4.5)

k) (cid:54) 1
2λk

15

Passing to the limit and using the hypothesis that 1
λk

||yk − ˚y||2 → 0, we get

lim sup
k

R(x(cid:63)

k) (cid:54) R(˚x).

k) (cid:62) R(¯x). Com-
On the other hand, lower semi-continuity of R entails lim inf k R(x(cid:63)
bining these two inequalities, we deduce that R(¯x) (cid:54) R(˚x). Since R is proper and lsc,
it is bounded from below on bounded sets [51, Corollary 1.10]. Let r = inf k R(x(cid:63)
k)
which then satisﬁes −∞ < r < +∞. Substracting r from (4.5) and multiplying by
λk, one obtains

1
2

||yk − Φx(cid:63)

k||2 (cid:54) 1
2

||yk − Φx(cid:63)

k||2 + λk(R(x(cid:63)

||yk − ˚y||2 + λk(R(˚x) − r). (4.6)

k) − r) (cid:54) 1
2

Consequently, passing to the limit in (4.6) shows that ||˚y − Φ¯x|| = 0, i.e. ¯x is a feasible
point of (P(0, ˚y)). Altogether, this shows that ¯x is a solution of (P(0, ˚y)), and by
uniqueness of the minimizer, ¯x = ˚x.

Lemma 3 (Dual solution convergence). For any sequence of parameters pk =

(λk, yk) such that

(cid:18) ||yk − ˚y||
λk

(cid:19)

, λk

−→ (0, 0),

we have q(cid:63)(pk) → ˚q(0, ˚y).

Proof. By the triangle inequality, we have

||q(cid:63)(λk, yk) − ˚q(0, ˚y)|| (cid:54) ||q(cid:63)(λk, yk) − q(cid:63)(λk, ˚y)|| + ||q(cid:63)(λk, ˚y) − ˚q(0, ˚y)||.

For the ﬁrst term, we notice that q(cid:63)(λ, y) = proxR∗◦Φ∗/λ(y/λ) for λ > 0. Thus,
Lipschitz continuity of the proximal mapping entails that

||q(cid:63)(λk, yk) − q(cid:63)(λk, ˚y)|| (cid:54) ||yk − ˚y||

,

λk

λk
2
λk
2

which in turn shows that q(cid:63)(λk, yk) → q(cid:63)(λk, ˚y). Let us now turn to the second term.
k = q(cid:63)(λk, ˚y) and ˚q def.= ˚q(0, ˚y) ∈ D(0, ˚y), one
Using the respective optimality of q(cid:63)
obtains

− (cid:104)q(cid:63)

k, ˚y(cid:105) +

||q(cid:63)

k||2 + R∗(Φ∗q(cid:63)

k) (cid:54) − (cid:104)˚q, ˚y(cid:105) +

||˚q||2 + R∗(Φ∗˚q)

λk
2

(cid:54) − (cid:104)q(cid:63)

k, ˚y(cid:105) +

||˚q||2 + R∗(Φ∗q(cid:63)

k),

(4.7)

k|| (cid:54) ||˚q||, which shows in particular that (q(cid:63)

whence we get ||q(cid:63)
k)k∈N is a bounded se-
quence. We can thus extract any converging subsequence, which for simplicity, we
denote again q(cid:63)
k → ¯q. Passing to the limit in (4.7), using the fact that R∗ is lsc, one
obtains

− (cid:104)¯q, ˚y(cid:105) + R∗(Φ∗ ¯q) (cid:54) − (cid:104)¯q, ˚y(cid:105) + lim inf

R∗(Φ∗q(cid:63)

k) (cid:54) − (cid:104)˚q, ˚y(cid:105) + R∗(Φ∗˚q),

which shows that ¯q ∈ D(0, ˚y). This together with the fact that ||q(cid:63)
already saw, shows that ¯q = ˚q by uniqueness of ˚q in (4.4).

k|| (cid:54) ||˚q|| as we

k

16

4.3. Sensitivity. We are now in position to state the main result of this section,
which tracks the strata of x(cid:63)(p) in the regime where the perturbation ||w|| = ||y − ˚y||
is suﬃciently small.

Theorem 3. Suppose that ˚x is the unique solution to (P(0, ˚y)). Assume fur-
thermore that R is mirror-stratiﬁable with respect to the primal-dual stratiﬁcations
(M, M∗). If there are constants (C0, C1) depending only on ˚x such that for all p in

{p = (λ, y) : C0||y − ˚y|| (cid:54) λ (cid:54) C1} ,

then there exists a minimizer x(cid:63)(p) of E(·, p) localized as follows

M˚x (cid:54) Mx(cid:63)(p) (cid:54) JR∗ (M ∗

˚u) where ˚u def.= Φ∗˚q(0, ˚y).

(4.8)

(4.9)

Proof. Under (4.8) with for instance C1 small enough, one can easly check
that there exists k large enough such that the regime required for (yk, λk) to ap-
In turn, we have a converging primal-dual pair
ply Lemma 2 and 3 is attained.
(x(cid:63)(pk), Φ∗q(cid:63)(pk)) → (˚x, ˚u). One can then apply Theorem 1 to conclude.

5. Activity Localization with Proximal Splitting Algorithms. Proximal
splitting methods are algorithms designed to solve large-scale structured optimization
and monotone inclusion problems, by evaluating various ﬁrst-order quantities such as
gradients, proximity operators, linear operators, all separately at various points in the
course of an iteration. Though they can show slow convergence each iteration has a
cheap cost. We refer to e.g. [3, 5, 17, 49] for a comprehensive treatment and review.
Capitalizing on our enlarged activity identiﬁcation result for mirror-stratiﬁable
functions, we now instantiate its consequences on ﬁnite activity localization of prox-
imal splitting algorithms. While existing results on ﬁnite identiﬁcation (of a sin-
gle active set) strongly rely on partial smoothness around a non-degenerate cluster
point [34, 42, 43, 41], we examine here intricate situations where neither of these
assumptions holds.

5.1. Forward-Backward algorithm. The Forward–Backward (FB) splitting
method [44] is probably the most well-known proximal splitting algorithm. In our
context, it can be used to solve optimization problems with the additive “smooth +
non-smooth” structure of the form

min
x∈RN

f (x) + R(x),

where f ∈ C 1(RN ) is convex with L-Lipschitz gradient, and R is a proper lsc and
convex function. We assume that Argmin(f + R) (cid:54)= ∅. The FB iteration in relaxed
form reads [16]

xk+1 = (1 − τk)xk + τk proxγkR(xk − γk∇f (xk)),

with γk ∈]0, 2/L[ and τk ∈]0, 1], where proxγR, γ > 0, is the proximal mapping of R,

(5.1)

(5.2)

(5.3)

proxγR(x) def.= argmin
z∈RN

1
2

||z − x||2 + γR(z).

Diﬀerent variants of FB method were studied, and a popular trend is the inertial
schemes which aim at speeding up the convergence (see [48, 4], and the sequence-
convergence version as proved recently in [14]).

17

Under the non-degeneracy assumption −∇f (x(cid:63)) ∈ ri(∂R(x(cid:63))), it was shown in
[42] that FB and its inertial variants correctly identify the active manifold in ﬁnitely
many iterations, and then enter a local linear convergence regime. These results
encompass many special cases such as those studied in [33, 8, 54]. Beyond this non-
degenerate case, we establish now the general localization of active strata.

Theorem 4. Consider the FB iteration (5.2) to solve (5.1) with 0 < inf k γk (cid:54)
supk γk < 2/L and inf k τk > 0. Then xk converges to x(cid:63) ∈ Argmin(f + R). Assume
that R is also mirror-stratiﬁable with respect to (M, M∗), then for k large enough,

Mx(cid:63) (cid:54) Mxk

(cid:54) JR∗ (M ∗

u(cid:63) ) where u(cid:63) def.= −∇f (x(cid:63)).

Proof. Convergence of the sequence (xk)k∈N to x(cid:63) is obtained from [16, Corol-
lary 6.5]. Moreover, since the proximal mapping is the resolvent of the subdiﬀerential,
(5.2) is equivalent to the monotone inclusion

uk+1

def.=

xk − vk+1
γk

− ∇f (xk) ∈ ∂R(vk+1),

def.= xk+1−xk

+ xk. In turn, with the conditions inf k τk > 0 and inf k γk > 0,
where vk+1
and continuity of ∇f , we have vk → x(cid:63) and thus uk → −∇f (x(cid:63)) ∈ ∂R(x(cid:63)). It then
remains to apply Theorem 1 to (vk, uk) and R to conclude.

τk

It can be easily shown that Theorem 4 holds for several extensions of the iterate-

convergent version of FISTA [14]. We omit the details here for the sake of brevity.

We rather take a closer look to the case when the FB scheme (5.2) to solve (P(λ, y))
(see also (4.2)) for λ > 0. Putting together Theorem 3 and 4, we obtain the following
localization result depending only on the data to estimate ˚x assuming that the noise
level ||y − ˚y|| is small enough.

Proposition 4. Under the assumptions of Theorem 3, consider the FB iteration
(5.2) to solve (P(λ, y)) with 0 < inf k γk (cid:54) supk γk < 2λ/||Φ||2 and inf k τk > 0. Then,
for k large enough, the iterates xk satisfy

M˚x (cid:54) Mxk

(cid:54) JR∗ (M ∗

˚u) where ˚u def.= Φ∗q(0, ˚y),

with q(0, ˚y) from (4.4).

Proof. To lighten notation, denote p = (λ, y). As in Theorem 4, we have xk →

x(cid:63)(p) a minimizer of (P(λ, y)). Thus we get

M˚x (cid:54) Mx(cid:63)(p) (cid:54) Mxk ,

where the ﬁrst inequality comes from Theorem 3 and the second one from Theorem 4.
This gives the ﬁrst inequality in the localization result.

Moreover, in the special case at hand ∂R(x(cid:63)(p)) (cid:51) −∇f (x(cid:63)(p)) = Φ∗q(cid:63)(p) → ˚u
as p → (0, ˚y), where we invoked (4.3). It then follows from (2.7) and the fact that
JR∗ is decreasing for the relation (cid:54), that

M ∗
˚u

(cid:54) M ∗

−∇f (x(cid:63)(p)) ⇐⇒ JR∗ (M ∗

−∇f (x(cid:63)(p))) (cid:54) JR∗ (M ∗

˚u).

Using once again Theorem 4, we get

Mxk

(cid:54) JR∗ (M ∗

−∇f (x(cid:63)(p))) (cid:54) JR∗ (M ∗

˚u),

18

which yields the second desired inequality.

Though the iterates xk of FB do not converge to ˚x, this proposition tells us that
the iterates identify an enlarged stratum associated to ˚x. This is an appealing feature
from a practical perspective, since one can often make some prior assumption on the
sought after vector ˚x, such as for instance sparsity or low-rank properties, as we have
illustrated in the numerical experiments of Section 6.

5.2. Douglas-Rachford Splitting Algorithm. The Douglas-Rachford (DR)
method [44] is another popular splitting method designed to minimize convex objec-
tives having the additive “non-smooth + non-smooth” structure of the form

with f and g be proper lsc and convex functions such that ri(dom(f ))∩ri(dom(g)) (cid:54)= ∅
and Argmin(f + g) (cid:54)= ∅. The DR scheme reads

min
x∈RN

f (x) + g(x).






vk+1 = proxγf (2xk − zk) ,
zk+1 = zk + τk (vk+1 − xk) ,
xk+1 = proxγg(zk+1),

(5.4)

(5.5)

where γ > 0, τk ∈]0, 2[ is a relaxation parameter. By deﬁnition, the DR method is
not symmetric with respect to the order of the functions f and/or g. Nevertheless,
all of our statements throughout hold true, with obvious adaptations, when the order
of f and g is reversed in (5.5).

It has been shown in [43] that under appropriate non-degeneracy assumptions,
the DR identiﬁes the active manifolds in ﬁnite time, and then shows a local linear
regime. These results unify all those that were established in the literature for special
problems, see e.g.
[20] for linearly constrained (cid:96)1-minimization, [6] for quadratic or
linear programs, [2] for feasibility with two subspaces. Under mirror-stratiﬁability of
f or g, we get the following enlarged activity identiﬁcation result.

Theorem 5. Consider the DR iteration (5.5) to solve (5.4) with τk ∈]0, 2[
such that (cid:80)
k∈N τk (2 − τk) = +∞. Then zk converges to a ﬁxed point z(cid:63) with
x(cid:63) = proxγg(z(cid:63)) ∈ Argmin(f + g), and xk and vk both converge to x(cid:63). Introduc-
ing u(cid:63) = z(cid:63)−x(cid:63)

, we have furthermore:

γ

(i) If g is mirror-stratiﬁable with respect to the primal-dual stratiﬁcations (M g, M g∗

),

then for k large enough

(ii) If f is mirror-stratiﬁable with respect to the primal-dual stratiﬁcations (M f , M f ∗

),

then for k large enough

M g

x(cid:63) (cid:54) M g
xk

(cid:54) Jg∗ (M g∗

u(cid:63) ).

M f

x(cid:63) (cid:54) M f
vk

(cid:54) Jf ∗ (M f ∗

−u(cid:63) ).

Proof. Under the prescribed choice of τk, convergence of zk is ensured by virtue
of [16, Corollary 5.2]. By non-expansiveness of the proximal mapping, and as we are
in ﬁnite dimension, we also obtain convergence of xk and vk to x(cid:63). To prove (i), note
that the update of xk in (5.5) is equivalent to the monotone inclusion

uk

def.=

∈ ∂g(xk).

zk − xk
γ

19

(5.6)

Since (xk, uk) → (x(cid:63), u(cid:63)), we conclude about (i) by invoking Theorem 1. Similarly,
we note that the update of vk in (5.5) is equivalent to

wk

def.=

2xk − zk − vk+1
γ

∈ ∂f (vk+1).

(5.7)

Using that (vk, wk) → (x(cid:63), −u(cid:63)) and applying Theorem 1 we get (ii).

In the same vein as for FB in the previous section, we now turn to applying
the DR scheme (5.5) to solve (P(λ, y)) by setting in (5.4) f (x) = 1
2λ ||y − Φx||2 and
g(x) = R(x). Putting Theorem 3 and 5-(i) together, we obtain the following analogue
to Proposition 4.

Proposition 5. Under the assumptions of Theorem 3, consider the DR iteration
k∈N τk (2 − τk) = +∞.

(5.5) to solve (P(λ, y)) with γ > 0 and τk ∈]0, 2[ such that (cid:80)
Then, for k large enough, the DR iterates satisfy

M˚x (cid:54) Mxk

(cid:54) JR∗ (M ∗

˚u) where ˚u def.= Φ∗q(0, ˚y).

Proof. The proof follows the same reasoning as that of Proposition 4 by addition-
ally observing (recall the notation in the proof of Theorem 5) that from (5.6)-(5.7),
we have u(cid:63)(p) = −∇f (x(cid:63)(p)) = Φ∗q(cid:63)(p) ∈ ∂R(x(cid:63)(p)).

6. Numerical Illustrations. In this section, we numerically illustrate our the-
oretical results on sensitivity and enlarged activity identiﬁcation in the context of
regularized inverse problems. We adopt the same two “compressed sensing” scenarios
described in Section 1.2. The dimension dim(M˚x) = R0(˚x) of the strata associated
to ˚x is measured as R0 = || · ||0 (resp. R0 = rank) for the (cid:96)1 (resp. nuclear) norm
regularization.

Strata sensitivity. We ﬁrst illustrate the relevance of the strata sensitivity result in
Theorem 3 by studying the dimension of the largest possible active stratum JR∗ (M ∗
˚u)
(in fact its closure). The dual ˚u = Φ∗˚q(0, ˚y) is computed from ˚x by solving the convex
optimization problem (4.4) (using CVX to get a high precision). Thus we know the
maximum complexity index excess predicted by Theorem 3, i.e.

δ(cid:63)(˚x) def.= dim(JR∗ (M ∗

˚u)) − dim(M˚x).

For each given δ and R0(˚x), and among the 1000 randomly generated replications of
(˚x, Φ), we compute the proportion ρ(R0(˚x), δ) of ˚x such that it is the unique solution
of (P(0, ˚y)) and δ(cid:63)(˚x) (cid:54) δ. The proportions ρ(R0(˚x), δ) are displayed in Figure 6.1 as
a function of the input complexity index R0(˚x) = dim(M˚x). The colors from blue to
red correspond to increasing δ.

The proportion ρ(R0(˚x), δ) is an increasing function of δ and a decreasing function
of R0(˚x). Indeed, as anticipated from standard compressed sensing results [58], active
strata M˚x of vectors ˚x whose dimension is small enough compared to the number of
measurements P can be provably and stably recovered with overwhelming probability
(on the sampling of (˚x, Φ, w)). As R0(˚x) increases, the number of measurements P
becomes insuﬃcient to ensure non-degeneracy with high probability, hence preventing
stable recovery of M˚x. However, Theorem 3 predicts that the active stratum of
x(cid:63)(λ, y) for y nearby ˚y is localized between M˚x and JR∗ (M ∗

˚u).

The blue curve in each plot of Figure 6.1 corresponds to δ = 0, which is the
proportion ρ(R0(˚x), 0) of vectors ˚x whose active stratum M˚x can be recovered stably
under small noise perturbation by solving (P(λ, y)) for λ chosen according to (4.8).

20

R = || · ||1

R = || · ||∗

Fig. 6.1. Proportion of ˚x such that δ(cid:63)(˚x) (cid:54) δ as a function of R0(˚x) (increasing value of δ

from 0 to its maximal value is depicted by a color evolving from blue to red).

R = || · ||1

R = || · ||∗

Fig. 6.2. Plots of R0(xk) for the 2000 ﬁrst iterates generated by the Forward-Backward algo-

rithm. Plots in blue correspond to the cases when δ(cid:63)(˚x) = 0, and the red ones for δ(cid:63)(˚x) = δ.

This proportion shows a phase transition phenomenon between stable recovery and
unstable recovery. The location of the phase transition for δ = 0 can be predicted
accurately; see for instance [59].

The red curves in Figure 6.1 correspond to the extreme case where δ takes its
largest achievable value, i.e. where we can guarantee recovery of the largest stratum
JR∗ (M ∗
˚u) with high probability. The phase transition occurs for higher dimension
R0(˚x). The intermediate curves, i.e. from blue to red, correspond to the recovered
strata that are localized between M˚x and JR∗ (M ∗
˚u) (i.e. increasing δ). The phase
transition progressively increases with δ. These curves illustrate and quantify the
typical tradeoﬀ observed in practice: one can allow for more complex input vectors ˚x
(i.e. those with larger R0(˚x)) at the expense of recovering active strata larger than M˚x.
Forward-Backward ﬁnite activity localization. We now numerically illustrate the
ﬁnite enlarged activity identiﬁcation of the FB splitting scheme as predicted by The-
orem 4 and Proposition 4. We remain under the same compressed sensing setting
as before. The randomly generated replications of ˚x are such that R0(˚x) = 10 for
R = || · ||1 and to 4 for R = || · ||∗.

The evolution of the complexity index R0(xk) of the FB iterate xk is shown in
Figure 6.2. The blue lines correspond to several trajectories (the bold one is the
average trajectory), each for a randomly generated instance of ˚x such that δ(cid:63)(˚x) =
21

0, i.e. those vectors whose active strata M˚x can be exactly recovered under small
perturbations. Thus, the iterates identify M˚x in ﬁnite time. The red lines (the bold
one is the average trajectory) are those for which δ(cid:63)(˚x) = δ > 0. As anticipated by
our theoretical results, the iterates identify a stratum strictly larger that M˚x.

Acknowledgements. The work of Gabriel Peyr´e has been supported by the Euro-
pean Research Council (ERC project SIGMA-Vision). Jalal Fadili was partly supported by
Institut Universitaire de France.

Appendix A. Proofs of the results in Section 2.
Proof of Proposition 1. Let us ﬁrst prove the ﬁrst equivalence. Observe that
Deﬁnition 2 can be read as follows: M is active at x if and only if d(x, cl(M )) = 0.
Since there is a ﬁnite number of strata in the stratiﬁcation, let us consider δ the
minimum of the nonzero distances d(x, cl(M (cid:48))) for M (cid:48) not active. For all x(cid:48) in the
open ball of radius δ and of center x, we have

d(x, cl(Mx(cid:48))) (cid:54) ||x − x(cid:48)|| < δ

⇐⇒

d(x, cl(Mx(cid:48))) = 0.

This shows that the set of these strata Mx(cid:48) indeed coincides the set of active strata,
whence we get the ﬁrst equivalence.

Let us turn to the second equivalence. Let M be an active strata at x, that
is, x ∈ cl(M ). Since x ∈ Mx, the intersection cl(M ) ∩ Mx contains x and thus is
nonempty. We deduce from (2.1) that M (cid:62) Mx. Conversely, if M (cid:62) Mx, then
x ∈ Mx ⊂ cl(M ) and therefore M is active at x.

Proof of Proposition 2. From classical convex analysis calculus rules, we get for

all x ∈ dom R

∂R(x) = conv {ai

: i ∈ I max(x)} + cone (cid:8)ai

: i ∈ I feas(x)(cid:9) .

By deﬁnition of MI , the relative interior of ∂R(x) is constant over a stratum: for all
x ∈ MI (with MI (cid:54)= ∅)

ri(∂R(x)) = ri (cid:0) conv {ai
= ri(conv {ai

= ri(conv {ai

: i ∈ I max(x)} + cone (cid:8)ai
: i ∈ I max(x)}) + ri(cone (cid:8)ai
: i ∈ I max}) + ri(cone (cid:8)ai

: i ∈ I feas(x)(cid:9) (cid:1)

: i ∈ I feas(x)(cid:9))

: i ∈ I feas(cid:9)).

This yields JR(MI ) = M ∗
entails for all u ∈ M ∗
I

I . Conversely we have ∂R∗(u) = {x : u ∈ ∂R(x)}, which

∂R∗(u) = (cid:8)x : I max(x) ∩ I feas(x) ⊃ I(cid:9) ,

and therefore ri(∂R∗(u)) = MI . This gives JR∗ (M ∗
Deﬁnition 4.

I ) = MI , which proves item (i) of

To show (ii) of Deﬁnition 4, we make the following observation:

MI (cid:54) MI (cid:48) ⇐⇒ any x ∈ MI lies in cl(MI (cid:48))
max

⇐⇒ I max = I max(x) ⊃ (I (cid:48))
⇐⇒ I ⊃ I (cid:48).

On the other hand we have

and I feas = I feas(x) ⊃ (I (cid:48))

feas

cl(JR(MI )) ⊃ cl(ri(conv {ai

: i ∈ I max})) + cl(ri cone (cid:8)ai

: i ∈ I feas(cid:9)))

= conv {ai

: i ∈ I max} + cone (cid:8)ai

: i ∈ I feas(cid:9) .

22

Note that the ﬁrst ⊃ is in fact = because conv {ai
unique decomposition of a polyhedron, we can write,

: i ∈ I max} is compact. Using

JR(MI ) (cid:62) JR(MI (cid:48)) ⇐⇒ cl(JR(MI )) ⊃ JR(MI (cid:48)) ⇐⇒ I ⊃ I (cid:48),

which ends the proof.

Proof of Proposition 3. The proof builds upon a key result stated in [19]. Theo-
rem 4.6(i) in [19] asserts that the collection (cid:8)σ−1(M sym) : M ∈ M(cid:9) forms a smooth
stratiﬁcation of dom(R) with the desired properties. The fact that spectral func-
tions are mirror-stratiﬁable follows from the polyhedral case with the help of Theo-
rem 4.6(iv) in [19], which states that

JR(σ−1(M sym)) = σ−1(cid:0)JRsym (M sym)(cid:1)

together with continuity of the singular value mapping σ.

REFERENCES

[1] F. Bach. Consistency of trace norm minimization. The Journal of Machine Learning Research,

9(Jun):1019–1048, 2008.

[2] H. Bauschke, J.Y.B. Cruz, T.A. Nghia, H.M. Phan, and X. Wang. The rate of linear convergence
of the douglas-rachford algorithm for subspaces is the cosine of the friedrichs angle. J. of
Approx. Theo., 185(63–79), 2014.

[3] H. H. Bauschke and P. L. Combettes. Convex analysis and monotone operator theory in Hilbert

spaces. Springer, 2011.

[4] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.

[5] A. Beck and M. Teboulle. Gradient-based algorithms with applications to signal recovery.

Convex Optimization in Signal Processing and Communications, 2009.

[6] D. Boley. Local linear convergence of the alternating direction method of multipliers on

quadratic or linear programs. SIAM J. Optim., 23(4):2183–2207, 2013.

[7] J. F. Bonnans and A. Shapiro. Perturbation analysis of optimization problems. Springer Series

in Operations Research and Financial Engineering. Springer Verlag, 2000.

[8] K. Bredies and D. A. Lorenz. Linear convergence of iterative soft-thresholding. Journal of

Fourier Analysis and Applications, 14(5-6):813–837, 2008.

[9] P. B¨uhlmann and S. Van De Geer. Statistics for high-dimensional data: methods, theory and

applications. Springer, 2011.

[10] E. J. Cand`es and C. Fernandez-Granda. Towards a mathematical theory of super-resolution.

Communications on Pure and Applied Mathematics, 67(6):906–956, 2013.

[11] E. J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations of

Computational mathematics, 9(6):717–772, 2009.
[12] E. J. Cand`es and T. Tao. Decoding by linear programming.

Information Theory, IEEE

Transactions on, 51(12):4203–4215, 2005.

[13] E. J. Cand`es and T. Tao. The power of convex relaxation: Near-optimal matrix completion.

Information Theory, IEEE Transactions on, 56(5):2053–2080, 2010.

[14] A. Chambolle and C. Dossal. On the convergence of the iterates of the “fast iterative
shrinkage/thresholding algorithm”. Journal of Optimization Theory and Applications,
166(3):968–982, 2015.

[15] S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM

journal on scientiﬁc computing, 20(1):33–61, 1999.

[16] P. L. Combettes. Solving monotone inclusions via compositions of nonexpansive averaged

operators. Optimization, 53(5-6):475–504, 2004.

[17] P. L. Combettes and J.-C. Pesquet. Proximal splitting methods in signal processing. In H. H.
Bauschke, Burachik R. S., P. L. Combettes, Elser. V., D. R. Luke, and H. Wolkowicz,
editors, Fixed-Point Algorithms for Inverse Problems in Science and Engineering, pages
185–212. Springer, 2011.

[18] M. Coste. An introduction to o-minimal geometry. Technical report, Institut de Recherche

Mathematiques de Rennes, November 1999.

[19] A. Daniilidis, D. Drusvyatskiy, and A. S. Lewis. Orthogonal invariance and identiﬁability.

SIAM Journal on Matrix Analysis and Applications, 35(2):580–598, 2014.

23

[20] L. Demanet and X. Zhang. Eventual linear convergence of the douglas-rachford iteration for

basis pursuit. Mathematics of Computation, 2013. to appear.

[21] A. L. Dontchev. Perturbations, approximations, and sensitivity analysis of optimal control

systems, volume 52. Springer-Verlag, Berlin, 1983.

[22] C. Dossal, M.-L. Chabanol, G. Peyr´e, and J. M. Fadili. Sharp support recovery from noisy ran-
dom measurements by (cid:96)1-minimization. Applied and Computational Harmonic Analysis,
33(1):24–43, 2012.

[23] D. Drusvyatskiy, A. D. Ioﬀe, and A. S. Lewis. Generic minimizing behavior in semialgebraic

optimization. SIAM J. Optim., 26(1):513–534, 2016.

[24] D. Drusvyatskiy and A.S. Lewis. Optimality, identiﬁability, and sensitivity. Mathematical

Programming, to appear in, pages 1–32, 2013.

[25] V. Duval and G. Peyr´e. Exact support recovery for sparse spikes deconvolution. Foundations

of Computational Mathematics, 15(5):1315–1355, 2015.

[26] V. Duval and G. Peyr´e. Sparse spikes deconvolution on thin grids. Preprint 01135200, HAL,

2015.

[27] C. Ekanadham, D. Tranchina, and E. P. Simoncelli. A uniﬁed framework and method for
automatic neural spike identiﬁcation. Journal of Neuroscience Methods, 222:47 – 55, 2014.
[28] M. Fazel. Matrix Rank Minimization with Applications. PhD thesis, Stanford University, 2002.
[29] A. V. Fiacco and G. P. McCormick. Nonlinear Programming: Sequential Unconstrained Min-

imization Techniques. Wiley, New York, 1968. reprinted, SIAM, Philadelphia, 1990.

[30] J.-J. Fuchs. On sparse representations in arbitrary redundant bases. Information Theory, IEEE

Transactions on, 50(6):1341–1344, 2004.

[31] Michael Grant and Stephen Boyd. CVX: Matlab software for disciplined convex programming,

version 2.1. http://cvxr.com/cvx, March 2014.

[32] M. Grasmair, O. Scherzer, and M. Haltmeier. Necessary and suﬃcient conditions for lin-
ear convergence of l1-regularization. Communications on Pure and Applied Mathematics,
64(2):161–182, 2011.

[33] E. Hale, W. Yin, and Y. Zhang. Fixed-point continuation for (cid:96)1-minimization: methodology

and convergence. SIAM J. Optim., 19(3):1107–1130, 2008.

[34] W. L. Hare.

Identifying active manifolds in regularization problems.

In H. H. Bauschke,
R. S., Burachik, P. L. Combettes, V. Elser, D. R. Luke, and H. Wolkowicz, editors, Fixed-
Point Algorithms for Inverse Problems in Science and Engineering, volume 49 of Springer
Optimization and Its Applications, chapter 13. Springer, 2011.

[35] W. L. Hare and A. S. Lewis. Identifying active constraints via partial smoothness and prox-

regularity. J. Convex Anal., 11(2):251–266, 2004.

[36] J.-B. Hiriart-Urruty and H. Y. Le. Convexifying the set of matrices of bounded rank: appli-
cations to the quasiconvexiﬁcation and convexiﬁcation of the rank function. Optimization
Letters, 6(5):841–849, 2012.

[37] Hai Yen Le. Confexifying the Counting Function on Rp for Convexifying the Rank Function

on Mm,n(R). Journal of Convex Analysis, 19(2):519–524, 2012.

[38] C. Lemar´echal, F. Oustry, and C. Sagastiz´abal. The U -lagrangian of a convex function. Trans.

Amer. Math. Soc., 352(2):711–729, 2000.

[39] A. S. Lewis. Active sets, nonsmoothness, and sensitivity. SIAM J. Optim., 13(3):702–725,

[40] A. S. Lewis and S. Zhang. Partial smoothness, tilt stability, and generalized hessians. SIAM

[41] J. Liang. Convergence Rates of First-Order Operator Splitting Methods. Theses, Normandie

2002.

J. Optim., 23(1):74–94, 2013.

Universit´e, October 2016.

[42] J. Liang, J. Fadili, and G. Peyr´e. Activity identiﬁcation and local linear convergence of forward–

backward-type methods. SIAM J. Optim., 27(1):408–437, 2017.

[43] J. Liang, J. Fadili, and G. Peyr´e. Local convergence properties of douglas–rachford and alter-
nating direction method of multipliers. Journal of Optimization Theory and Applications,
172(3):874–913, 2017.

[44] P. L. Lions and B. Mercier. Splitting algorithms for the sum of two nonlinear operators. SIAM

Journal on Numerical Analysis, 16(6):964–979, 1979.

[45] S. G. Mallat. A wavelet tour of signal processing. Elsevier, third edition, 2009.
[46] S. A. Miller and J. Malick. Newton methods for nonsmooth convex minimization: connections
among-Lagrangian, Riemannian Newton and SQP methods. Mathematical programming,
104(2-3):609–633, 2005.

[47] B. S. Mordukhovich. Sensitivity analysis in nonsmooth optimization.

In D. A. Field and
V. Komkov, editors, Theoretical Aspects of Industrial Design, volume 58, pages 32–46.
SIAM Volumes in Applied Mathematics, 1992.

24

[48] Y. Nesterov. A method for solving the convex programming problem with convergence rate

O(1/k2). Dokl. Akad. Nauk SSSR, 269(3):543–547, 1983.

[49] N. Parikh and S. P. Boyd. Proximal algorithms. Foundations and Trends in Optimization,

1(3):123–231, 2013.

university press, 1970.

[50] R. T. Rockafellar. Convex analysis. Number 28 in Princeton Mathematical Series. Princeton

[51] R. T. Rockafellar and R. Wets. Variational analysis, volume 317. Springer, Berlin, 1998.
[52] O. Scherzer. Variational methods in imaging, volume 167. Springer, 2009.
[53] J.-L. Starck and F. Murtagh. Astronomical Image and Data Analysis. Springer, 2006.
[54] S. Tao, D. Boley, and S. Zhang. Local linear convergence of ISTA and FISTA on the LASSO

problem. arXiv preprint arXiv:1501.02888, 2015.

[55] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical

Society. Series B. Methodological, 58(1):267–288, 1996.

[56] S. Vaiter. Low Complexity Regularization of Inverse Problems. Theses, Universit´e Paris

Dauphine - Paris IX, July 2014.

[57] S. Vaiter, M. Golbabaee, J. Fadili, and G. Peyr´e. Model selection with low complexity priors.

Information and Inference: A Journal of the IMA, 4(3):230, 2015.

[58] S. Vaiter, G. Peyr´e, and J. Fadili. Low complexity regularization of linear inverse prob-
lems. In G¨otz Pfander, editor, Sampling Theory, a Renaissance, pages 103–153. Springer-
Birkh¨auser, 2015.

[59] S. Vaiter, G. Peyr´e, and J. Fadili. Model consistency of partly smooth regularizers. IEEE

Trans. Inf. Theory, 64(3):1725 – 1737, 2018.

[60] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables.

Journal of the Royal Statistical Society: Series B, 68(1):49–67, 2005.

[61] P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning

Research, 7:2541–2563, 2006.

25

8
1
0
2
 
n
u
J
 
5
 
 
]

C
O
.
h
t
a
m

[
 
 
3
v
4
9
1
3
0
.
7
0
7
1
:
v
i
X
r
a

SENSITIVITY ANALYSIS
FOR MIRROR-STRATIFIABLE CONVEX FUNCTIONS

JALAL FADILI∗, J´ER ˆOME MALICK† , AND GABRIEL PEYR´E‡

Abstract. This paper provides a set of sensitivity analysis and activity identiﬁcation results for
a class of convex functions with a strong geometric structure, that we coined “mirror-stratiﬁable”.
These functions are such that there is a bijection between a primal and a dual stratiﬁcation of the
space into partitioning sets, called strata. This pairing is crucial to track the strata that are identi-
ﬁable by solutions of parametrized optimization problems or by iterates of optimization algorithms.
This class of functions encompasses all regularizers routinely used in signal and image processing,
machine learning, and statistics. We show that this “mirror-stratiﬁable” structure enjoys a nice
sensitivity theory, allowing us to study stability of solutions of optimization problems to small per-
turbations, as well as activity identiﬁcation of ﬁrst-order proximal splitting-type algorithms. Existing
results in the literature typically assume that, under a non-degeneracy condition, the active set asso-
ciated to a minimizer is stable to small perturbations and is identiﬁed in ﬁnite time by optimization
schemes. In contrast, our results do not require any non-degeneracy assumption:
in consequence,
the optimal active set is not necessarily stable anymore, but we are able to track precisely the set
of identiﬁable strata.We show that these results have crucial implications when solving challenging
ill-posed inverse problems via regularization, a typical scenario where the non-degeneracy condition
is not fulﬁlled. Our theoretical results, illustrated by numerical simulations, allow us to characterize
the instability behaviour of the regularized solutions, by locating the set of all low-dimensional strata
that can be potentially identiﬁed by these solutions.

Key words.

convex analysis, inverse problems, sensitivity, active sets, ﬁrst-order splitting

algorithms, applications in imaging and machine learning

AMS subject classiﬁcations. 65K05, 65K10, 90C25, 90C31.

1. Introduction. Variational methods and non-smooth optimization algorithms
are ubiquitous to solve large-scale inverse problems in various ﬁelds of science and
engineering, and in particular in data science. The non-smooth structure of the op-
timization problems promotes solutions conforming to some notion of simplicity or
low-complexity (e.g. sparsity, low-rank, etc.). The low-complexity structure is often
manifested in the form of a low-dimensional “active set”.
It is thus of prominent
interest to be able to quantitatively characterize the stability of these active sets to
perturbations of the objective function. Of crucial importance is also the identiﬁ-
cation in ﬁnite time of these active sets by iterates of optimization algorithms that
numerically minimize the objective function. This type of problems and results is
referred to as “activity identiﬁcation”; a review of the relevant literature will be pro-
vided in the sequel (at the beginning of each section).
In a nutshell, the existing
identiﬁcation results guarantee a perfect stability of the active set to perturbations,
or a ﬁnite identiﬁcation of this active set via algorithmic schemes, under some non-
degeneracy conditions (see in particular [39, 24, 58]). The crucial non-degeneracy
assumption, that takes generally the form (3.1), can be viewed as a geometric gen-
eralization of strict complementary in non-linear programming. However, as we will
illustrate shortly through some preliminary numerics, such a condition is too strin-
gent and is often barely veriﬁed. The goal of this paper is to investigate the situation
where this non-degeneracy assumption is violated.

∗Normandie Univ, ENSICAEN, CNRS, GREYC, France.
†CNRS and LJK, Grenoble, France
‡CNRS and DMA, ENS Paris, France.

1

1.1. Motivating Examples. In order to better grasp the relevance of our anal-
ysis, let us ﬁrst start with the setting of inverse problems that pervades various ﬁelds
including signal processing and machine learning. We will come back to this setting
in Section 4 with further discussions and references.

Regularized Inverse Problems. Assume one observes

y = ˚y + w ∈ RP where ˚y def.= Φ˚x

(1.1)

where w is some perturbation (called noise) and Φ ∈ RP ×N (called forward operator,
or design matrix in statistics). Solving an inverse problem amounts to recovering
˚x, to a good approximation, knowing y and Φ according to (1.1). Unfortunately, in
general, P can be much smaller than the ambient dimension N , and when P = N ,
the mapping Φ is in general ill-conditioned or even singular.

A classical approach is then to assume that ˚x has some ”low-complexity”, and
to use a prior regularization R promoting solutions with such low-complexity. This
leads to the following optimization problem

E(x, (λ, y)) def.= R(x) +

||y − Φx||2,

(P(λ, y))

min
x∈RN

1
2λ

Let us discuss two popular examples of regularizing functions promoting a low-complexity
structure. These two functions will be used in our numerical experiments.

Example 1 ((cid:96)1 norm). For x ∈ RN , its (cid:96)1 norm reads

R(x) = ||x||1

def.=

|xi|,

N
(cid:88)

i=1

(1.2)

where xi is the i-th entry of x. As advocated for instance by [15, 55], the (cid:96)1 enforces
to have a small number of non-zero
the solutions of (P(λ, y)) to be sparse, i.e.
components. Indeed, the (cid:96)1 norm can be shown to be the tightest convex relaxation
(in the sense of bi-conjugation) of the (cid:96)0 pseudo-norm restricted to the unit Euclidian
ball [37]. Recall that (cid:96)0 pseudo-norm of x ∈ RN measures its sparsity

||x||0

def.= (cid:93) (cid:8)i ∈ {1, . . . , N } : xi (cid:54)= 0(cid:9) .

Sparsity has witnessed a huge surge of interest in the last decades. For instance, in
signal and imaging sciences, one can approximate most natural signals and images
In statistics,
using sparse expansions in an appropriate dictionary (see e.g. [45]).
sparsity is a key toward model selection and interpretability [9].

Example 2 (Nuclear norm). For a matrix x ∈ Rn1×n2 ∼ RN , where N = n1n2,

the nuclear norm is deﬁned as

||x||∗

def.= ||σ(x)||1 =

σi(x),

n
(cid:88)

i=1

where n def.= min(n1, n2) and σ(x) = (σ1(x), . . . , σn(x)) ∈ Rn
+ is the vector of singular
values of x. The nuclear norm is the tightest convex relaxation of the rank (in the sense
of bi-conjugation) restricted to the unit Frobenius ball [36]. This underlies its wide use
to promote solutions of (P(λ, y)) with low rank, where we recall rank(x) def.= ||σ(x)||0.
Low-rank regularization has proved useful for a variety of applications, including con-
trol theory and machine learning; see e.g. [28, 1, 11]

2

Active set(s) identiﬁcation. The above discussed regularizers R are non-smooth
convex functions. This non-smoothness arises in a highly structured fashion and is
usually associated, locally, with some low-dimensional active subset of RN (in many
cases, such a subset is an aﬃne or a smooth manifold). Thus R will favor solutions
of (P(λ, y)) that lie in a low-dimensional active set and would allow the inversion of
the system (1.1) in a stable way. More precisely, one would like that under small per-
turbations w, the solutions of (P(λ, y)) move stably along the active set. A byproduct
of this behaviour is that from an algorithmic perspective, if an optimization algorithm
is used to solve (P(λ, y)), one would hope that the iterates of the scheme identify the
active set in ﬁnite time.

Identifying the low-dimensional active set in a stable way is highly desirable for
several reasons. One reason is that it is a fundamental property most practitioners are
looking for. Typical examples include neurosciences [27] where the goal is to recover
a spike train from neural activity, or astrophysics [53] where it is desired to separate
stars from a background.
In both examples, sparsity can be used as a modeling
hypothesis and the recovery method should comply with it in a stable way. A second
reason is algorithmic since one can also take advantage of the low-dimensionality of
the identiﬁed active set to reduce computational burden and memory storage, hence
opening the door to higher-order acceleration of optimization algorithms (see [38, 46]).
Unfortunately, this desirable behaviour rarely occurs in practical applications.
Yet one still observes some form of partial stability (to be given a rigorous meaning
in Section 3 and 4), as conﬁrmed by the numerical experiment of the next paragraph.

1.2. Illustrative numerical experiment. We consider a simple “compressed
sensing” scenario, with the (cid:96)1 norm R = || · ||1 [12] and the nuclear norm R = || · ||∗ [13]
as regularizers. The operator Φ ∈ RN ×P is drawn uniformly at random from the
standard Gaussian ensemble, i.e. the entries of Φ are independent and identically
distributed Gaussian random variables with zero-mean and unit variance. In the case
R = || · ||1, we set (N, P ) = (100, 50) and the vector to recover ˚x is drawn uniformly
at random among sparse vectors with R0(˚x) def.= ||˚x||0 = 10 and unit non-zero entries.
In the case R = || · ||∗, we set (N, P ) = (400, 300) (n1 = n2 = n = 20) and the
matrix to recover ˚x is drawn uniformly at random among low-rank matrices with
R0(˚x) def.= rank(˚x) = 4 and unit non-zero singular values. For each problem suite
(R, N, P ), 1000 realizations of (˚x, Φ, w) are drawn, and y is then generated according
to (1.1). The entries of the noise vector w are drawn uniformly from a Gaussian with
standard deviation 0.1, we set λ = 0.28 for R = || · ||1 and λ = 10 for R = || · ||∗.

For each realization of (˚x, Φ, w), we solve the associated problem (P(λ, y)) using
CVX [31] to get a high precision; denote x(cid:63)(λ, y) the obtained solutions. We also
solve (P(λ, y)) by the Forward-Backward (FB) scheme which reads in this case1

xk+1 = proxλγR(xk + γΦ∗(y − Φxk)).
In our setting, with γ = 1.8/σmax(Φ∗Φ) and non-emptiness of Argmin(E(·, λ, y), it is
well-known that the sequence (xk)k∈N converges to a point in Argmin(E(·, λ, y)).

The top row of Figure 1.1 displays the histogram of the complexity index excess
δ def.= R0(x(cid:63)(λ, y)) − R0(˚x) which clearly shows that we do not have exact stability

1The deﬁnition of the proximal mapping proxτ R (for τ > 0) is given in (5.3). The proximal

mappings of the (cid:96)1 and nuclear norms are

proxµ||·||1

(x) = (cid:0)sign(xi) max(0, |xi| − µ)(cid:1)

i , proxµ||·||∗ (x) = U diag(proxµ||·||1

(σ(x)))V ∗,

where x = U diag(σ(x))V ∗ is a SVD decomposition of x.

3

δ

f
o

s

m
a
r
g
o
t
s
i
H

k

n
o
i
t
a
r
e
t
i

s
v

)
k
x
(
0
R

R = || · ||1

R = || · ||∗

Fig. 1.1. Top row: Histogram of the complexity index excess δ = R0(x(cid:63)(λ, y)) − R0(˚x) where
x(cid:63)(λ, y) is the solution of (P(λ, y)) with noisy observation y = ˚y + w for a small perturbation w and
a well-chosen parameter value λ = c0||w||. Bottom row: Evolution of the complexity index R0(xk)
where (xk)k∈N is the FB iterates sequence converging to some x(cid:63)(λ, y) ∈ Argmin(E(·, λ, y)).

Indeed, although R0(x(cid:63)(λ, y)) is still rather small, its
under the perturbations w.
value is in most cases larger than the complexity index R0(˚x). The bottom row of
Figure 1.1 depicts the evolution with k of the complexity index R0(xk) for two random
instances of ˚x (in blue and red) corresponding to two diﬀerent values of δ. One can
observe that the iterates identify in ﬁnite time some low-dimensional active set with
a complexity index strictly larger than the one associated to ˚x (the red curves). We
will provide a more detailed discussion of this phenomenon in Section 6.

As we will emphasize in the review of previous work, existing results on sensitivity
analysis and low-complexity regularization only focus the case where R0(x(cid:63)(λ, y)) =
R0(˚x) (i.e. δ = 0), whose underlying claim is that the low-complexity model is per-
fectly stable to perturbations, which typically requires some non-degeneracy assump-
tion to hold. In many situations, however, including the compressed sensing scenario
described above, but also in super-resolution imaging (see [26]) and other challenging
inverse problems, non-degeneracy-type hypotheses are too restrictive. It is the goal
of this paper to develop a more general sensitivity analysis, that goes beyond the
non-degenerate case, to improve our understanding of stability to perturbations of
low-complexity regularized inverse problems.

1.3. Contributions and Outline. Our ﬁrst contribution consists in introduc-
ing, in Section 2, a class of proper lower-semicontinuous (lsc) convex functions that
we coin “mirror-stratiﬁable”. The subdiﬀerential of such a function induces a primal-
dual pairing of stratiﬁcations, which is pivotal to track identiﬁable strata. We discuss
several examples of functions enjoying such a structure. With this structure at hand,
we then turn to the main result of this paper, formalized in Theorem 1, and on which

4

all the others rely. This result shows ﬁnite enlarged activity identiﬁcation for a mirror-
stratiﬁable function without the need of any non-degeneracy condition. In addition,
the identiﬁable strata are precisely characterized in terms primal-dual optimal solu-
tions. Sections 3, 4 and 5 instantiate this abstract result for a set of concrete problems,
respectively: sensitivity of composite ”smooth+non-smooth” optimization problems
(Theorem 2), enlarged activity identiﬁcation by proximal splitting schemes such as
Forward-Backward and Douglas-Rachford algorithms (Theorems 4 and 5), and ﬁnally,
enlarged identiﬁcation for regularized inverse problems (Theorem 3). Before stating
these results, we make a short review of the associated literature. Finally Section 6
illustrates these theoretical ﬁndings with numerical experiments, in particular in a
compressed sensing scenario involving the (cid:96)1 and nuclear norms as regularizers. Fol-
lowing the philosophy of reproducible research, all the code to reproduce the ﬁgures
of this article is available online2.

2. Mirror-Stratiﬁable Functions. In this section, we introduce the class of
mirror-stratiﬁable convex functions, and present the sensitivity property they provide
(Theorem 1). We also illustrate that many popular regularizers used in data science
are mirror-stratiﬁable. Most the results of this section (in particular all the examples)
are easy to obtain by basic calculus; we therefore do not give these proofs in the text
and we gather some of them in Appendix A.

2.1. Stratiﬁcations and deﬁnitions. We start with recalling the following

standard deﬁnition of stratiﬁcation.

Definition 1 (Stratiﬁcation). A stratiﬁcation of a set D ⊂ RN is a ﬁnite
partition M = {Mi}i∈I such that for any partitioning sets (called strata) M and M (cid:48)
we have

M ∩ cl(M (cid:48)) (cid:54)= ∅ =⇒ M ⊂ cl(M (cid:48)).

If the strata are open polyhedra, then M is a polyhedral stratiﬁcation, and if they are
C 2-smooth manifolds then M entails a C 2-stratiﬁcation.

A stratiﬁcation is naturally endowed with the partial ordering (cid:54) in the sense that

M (cid:54) M (cid:48) ⇐⇒ M ⊂ cl(M (cid:48)) ⇐⇒ M ∩ cl(M (cid:48)) (cid:54)= ∅.

(2.1)

The relation is clearly reﬂexive and transitive. Furthermore, we have

cl(M ) =

(cid:91)

M (cid:48).

M (cid:48)(cid:54)M

(2.2)

An immediate consequence of Deﬁnition 1 is that for each point x ∈ D, there
is a unique stratum containing x, denoted Mx. Indeed, suppose that there are two
non-empty open strata M1 and M2 such that M1 ∩ M2 = {x}, and thus M1 ∩ M2 (cid:54)= ∅.
This implies, using (2.1), that M1 (cid:54) M2 and M2 (cid:54) M1, and thus M1 = M2.

At this stage, it is worth emphasizing that the strata are not needed to be mani-
folds in the rest of the paper. It is however the case that in many practical cases that
we will discuss, strata are indeed manifolds, and sometimes aﬃne manifolds.

Example 3 (Polyhedral sets and functions). A partition of a polyhedral set into
its open faces induces a natural ﬁnite polyhedral stratiﬁcation of it. In turn, let R :

2Code available at https://github.com/gpeyre/2017-SIOPT-stratification

5

RN → R def.= R∪{+∞} be a polyhedral function, and consider a polyhedral stratiﬁcation
of its epigraph, which is a polyhedral set in RN +1. Projecting all polyhedral strata onto
the ﬁrst N -coordinates one obtains a ﬁnite polyhedral stratiﬁcation of dom(R).

Remark 1. The previous example extends to semialgebraic sets and functions,
which are known to induce stratiﬁcations into ﬁnite disjoint unions of manifolds. In
fact, this holds for any tame class of sets/functions; see, e.g., [18].

We now single out a speciﬁc set of strata, called active strata, that will play a

central role for ﬁnite enlarged activity identiﬁcation purposes.

Definition 2 (Active strata). Given a stratiﬁcation M of D ⊂ RN and a point

x ∈ D, a stratum M ∈ M is said active at x if x ∈ cl(M ).

Proposition 1. Given a stratiﬁcation M of D and a point x ∈ D. Then there
exists δ > 0 such that set of strata Mx(cid:48) for any x(cid:48) such that ||x(cid:48) − x|| < δ, coincides
with the set of strata M (cid:62) Mx and with the set of active strata at x.

2.2. Mirror-Stratiﬁable functions. Following [19, Section 4], we deﬁne the

key correspondence operator whose role will become apparent shortly.

Definition 3. Let R : RN → R be a proper lsc convex function. The associated

correspondence operator JR : 2RN

→ 2RN

JR(S) def.=

is deﬁned as
(cid:91)

ri(∂R(x)),

x∈S

where ∂R is the subdiﬀerential of R.

Observe that, by deﬁnition, JR is increasing for set inclusion

S ⊂ S(cid:48) =⇒ JR(S) ⊂ JR(S(cid:48)).

(2.3)

For this operator to be useful in sensitivity analysis, we will further impose that JR
is decreasing for the partial ordering (cid:54) in (2.1), as captured in our main deﬁnition.
This is a key requirement that captures the intuitive idea that the larger a primal
stratum, the smaller its image by JR in the dual space.

In the following, we will denote R∗ the Legendre-Fenchel conjugate of R.
Definition 4 (Mirror-stratiﬁable functions). Let R : RN → R be a proper lsc
convex function; we say that R is mirror-stratiﬁable with respect to a (primal) strat-
iﬁcation M = {Mi}i∈I of dom(∂R) and a (dual) stratiﬁcation M∗ = {M ∗
i }i∈I of
dom(∂R∗) if the following holds:

(i) Conjugation induces a duality pairing between M and M∗, and JR : M →

M∗ is invertible with inverse JR∗ , i.e. ∀M ∈ M, M ∗ ∈ M∗ we have

M ∗ = JR(M ) ⇐⇒ JR∗ (M ∗) = M.

(ii) JR is decreasing for the relation (cid:54): for any M and M (cid:48) in M

M (cid:54) M (cid:48) ⇐⇒ JR(M ) (cid:62) JR(M (cid:48)).

The primal-dual stratiﬁcations that make R mirror-stratiﬁable are not unique, as
will be exempliﬁed in Remark 3. Though the deﬁnition is formal and the assumptions
looks restrictive, this class include many useful examples, as illustrated in the next
section. We ﬁnish this section with a remark, used in the sequel.

Remark 2 (Separability). For each m = 1, . . . , L, suppose that the proper lsc
convex function Rm : RNm → R is mirror-stratiﬁable with respect to stratiﬁcations
Mm and M∗
m. Then it is easy to show, using standard subdiﬀerential and conjugacy
calculus, that the function R : (xm)1(cid:54)m(cid:54)L ∈ RN1 × · · · × RNL (cid:55)→ (cid:80)L
m=1 Rm(xm) is
mirror-stratiﬁable with stratiﬁcations M1 × · · · × ML and M∗

1 × · · · × M∗
L.

6

2.3. Examples. The notion of a mirror-stratiﬁable function looks quite rigid.
However, many of the regularization functions routinely used in data science are
mirror-stratiﬁable. Let us provide some relevant examples in this section. In partic-
ular, the (cid:96)1-norm and the nuclear norm will be used in the numerical experiments.

2.3.1. Legendre functions. A lsc convex function R : RN → R is said to be a
Legendre function (see [50, Chapter 26]) if (i) it is diﬀerentiable and strictly convex
on the interior of its domain int(dom(R)) (cid:54)= ∅, and (ii) ||∇R(xk)|| → +∞ for every
sequence (xk)k∈N ⊂ int(dom(R)) converging to a boundary point of dom(R). Many
functions in convex optimization are Legendre; most notably, quadratic functions
and the log barrier of interior point methods. It was shown in [50, Theorem 26.5]
that R is Legendre if and only if its conjugate R∗ is Legendre, and that in this case
∇R is a bijection from int(dom(R)) to int(dom(R∗)) with ∇R∗ = (∇R)−1. As a
consequence, a Legendre function R is mirror-stratiﬁable with M = {int(dom(R))}
and M∗ = {int(dom(R∗))}.

2.3.2. (cid:96)1-norm. Let R : x ∈ R (cid:55)→ |x|, whose conjugate R∗ = ι[−1,1]. It follows
that R is mirror-stratiﬁable with M = { ] − ∞, 0[, {0}, ]0, +∞[} and M∗ = {{−1}, ] −
1, 1[, {+1}}. Using Remark 2, the next result is clear.

Lemma 1. The (cid:96)1-norm and its conjugate ι[−1,+1]N are mirror-stratiﬁable with re-
spect to the stratiﬁcations M = { ] − ∞, 0[, {0}, ]0, +∞[}N of RN and M∗ = {{−1}, ]−
1, 1[, {+1}}N of [−1, +1]N .

A graphical illustration of this lemma in dimension 2 is displayed in Figure 2.1(a).
Remark 3 (Non-uniqueness of stratiﬁcations).
In general, we do not have
uniqueness of stratiﬁcations inducing mirror-stratiﬁable functions. To illustrate this,
let us consider the (cid:96)1 norm. Take the partitions M = { ] − ∞, 0[∪]0, +∞[, {0}}N
of RN and M∗ = {{−1, +1}, ] − 1, 1[}N . These are valid stratiﬁcations though
the strata are not connected. Other possible valid stratiﬁcations are given by strata
Mj = (cid:8)x ∈ RN : ||x||0 = j(cid:9) and M ∗
j = J||·||1 (Mj), for j = 0, 1, . . . , N . It is immediate
to check that the (cid:96)1-norm is also mirror-stratiﬁable with respect to these stratiﬁcations.
However, as devised above, it is in general not wise to take such large strata as they
lead to less sharp localization and sensitivity results.

Remark 4 (Instability under the sum rule). The family of mirror-stratiﬁable
functions is unfortunately not stable under the sum. As a simple counter-example,
consider the pair of conjugate functions on R: R(x) = |x| + x2/2 and R∗(u) =
(max{|u| − 1, 0})2/2. We obviously have dom(∂R) = dom(∂R∗) = R. However we
observe that JR(R) = R \ {−1, 1}. This yields that JR cannot be a pairing between
any two stratiﬁcations of R, and therefore R cannot be mirror-stratiﬁable.

2.3.3. (cid:96)1,2 norm. The (cid:96)1,2 norm, also known as the group Lasso regularization,
has been advocated to promote group/block sparsity [60], i.e.
it drives all the coef-
ﬁcients in one group to zero together. Let B a non-overlapping uniform partition of
{1, . . . , N } into K blocks. The (cid:96)1,2 norm induced by the partition B reads

||x||B =

||xB||2

(cid:88)

B∈B

where xB is the restriction of x to the entries indexed by the block B. This is again
a separable function of the xB’s. One can easily show that the (cid:96)2 norm on R|B| and
its conjugate, the indicator of the unit (cid:96)2-ball B
in R|B|, are mirror-stratiﬁable
with respect to the stratiﬁcations {{0}, R|B| \ {0}} of RN and {S|B|−1

, int(B

(cid:96)|B|
2

)}

(cid:96)2

(cid:96)|B|
2

7

(a)

(b)

8

Fig. 2.1. Graphical illustration of mirror-stratiﬁcation for two norms: (a) (cid:96)1 norm in dimen-

sion 2; (b) (cid:96)1,2 norm in dimension 3 (with two blocks, of respectively sizes 1 and 2).

(cid:96)2

(cid:96)|B|
2

), where S|B|−1

of B
is the corresponding unit sphere. In turn, the (cid:96)1,2 norm is
mirror-stratiﬁable with respect to the stratiﬁcations M = {{0}, R|B| \ {0}}K and
M∗ = {S|B|−1, int(B
)}K. An illustration of this result in dimension 3 is portrayed
in Figure 2.1(b).

(cid:96)|B|
2

2.3.4. Nuclear norm. Let us come back to the nuclear norm deﬁned in Exam-
ple 2. For simplicity, we assume n1 = n2 = n. Let M be a stratum of the (cid:96)1 norm
stratiﬁcation. In the following, we denote by M sym its symmetrization. We observe
that σ−1(M sym) = {X ∈ Rn×n : rank(X) = ||z||0, z ∈ M }. So many inverse images
σ−1(M sym) of strata M sym coincide. This suggests the following n + 1 stratiﬁcations:
for a given i ∈ {0, . . . , N }

Mi = (cid:8)X ∈ Rn×n : rank(X) = i(cid:9)
i = {U ∈ Rn : σ1(U ) = · · · = σi(U ) = 1, ∀j > i, |σj(U )| < 1} .
M ∗

This yields that || · ||∗ is mirror-stratiﬁable with respect to these stratiﬁcations.

2.3.5. Polyhedral Functions. We here establish mirror-stratiﬁability of poly-
hedral functions, including the (cid:96)1 norm, the (cid:96)∞ norm and anisotropic TV semi-norm.

A polyhedral function R : RN → R can be expressed as

R(x) = max

{(cid:104)ai, x(cid:105) − αi} + ι(cid:84)

i=k+1,...,m{x : (cid:104)ai, x(cid:105)−αi(cid:54)0}(x).

(2.4)

i=1,...,k

For any x ∈ dom(R), introduce the two sets of indices

I max(x) = {i = 1, . . . , m : (cid:104)ai, x(cid:105) − αi = R(x)} ,
I feas(x) = {i = 1, . . . , m : (cid:104)ai, x(cid:105) = αi} .

For a given index set I ⊂ {1, . . . , m}, we consider the aﬃne manifold

MI

def.= (cid:8)x ∈ dom(R) : I max(x) ∩ I feas(x) = I(cid:9) .

(2.5)

We see that some MI may be empty and that M = {MI }I is a stratiﬁcation of
dom(R). The stratum MI is characterized by the optimality part I max = I ∩{1, . . . , k}
and the feasibility part I feas = I ∩ {k + 1, . . . , m} of I. Similarly we deﬁne

M ∗

I = ri(conv {ai

: i ∈ I max}) + ri(cone (cid:8)ai

: i ∈ I feas(cid:9)).

Let us formalize in the next proposition a result alluded to in [19].

Proposition 2. A polyhedral function R is mirror-stratiﬁable with respect to its

naturally induced stratiﬁcations {MI }I and {M ∗

I }I .

Remark 5 (Back to || · ||1). As we anticipated, Proposition 2 subsumes Lemma 1

as a special case. To see this, observe that

||x||1 = max
||u||∞(cid:54)1

(cid:104)u, x(cid:105) =

max
u∈{−1,+1}N

(cid:104)u, x(cid:105) ,

which is of the form (2.4) with k = m = 2N . Thus there are 22N
aﬃne manifolds as
deﬁned by (2.5). But many of them are empty: there is only 3N (non-empty, distinct)
manifolds in the stratiﬁcation of RN , and they coincide with those in Lemma 1.

2.3.6. Spectral Lifting of Polyhedral Functions. As we discussed in Re-
mark 4, JR may fail to induce a duality pairing between stratiﬁcations of R and R∗,
in which case R cannot be mirror-stratiﬁable. Hence, for this type of duality to hold,
one needs to impose stringent strict convexity conditions. To avoid this, and still
aﬀord a large class of mirror-stratiﬁable functions that are of utmost in applications,
we consider spectral lifting of polyhedral functions, in the same vein as [19] did it for
partial smoothness.

A matrix function R : RN =n×n → R is said to be a spectral lift of a polyhedral
function if there exists a polyhedral function Rsym : Rn → R, invariant under signed
permutation of its coordinates, such that R = Rsym ◦ σ where σ computes the singular
values of a matrix. Associated to M = {MI }I the (polyhedral) stratiﬁcation induced
by Rsym, we consider its symmetrized stratiﬁcation. We deﬁne the symmetrization of
M ∈ M, as the set M sym

M sym = (cid:8)x ∈ RN : ∃y ∈ M such that for all i there exists j with |xi| = |yj|(cid:9) .

The next result is a corollary of the main result of [19].

Proposition 3. A spectral function R = Rsym ◦ σ is mirror-stratiﬁable with

respect to the smooth stratiﬁcation {σ−1(M sym)} and its image by JR.

Remark 6 (Back to || · ||∗). The nuclear norm is a spectral lift of the (cid:96)1 norm.
Therefore, one can recover mirror-stratiﬁcation of the nuclear norm, with the strati-
ﬁcations given in Section 2.3.4, by putting together Lemma 1 and Proposition 3.

9

2.4. Activity Identiﬁcation for Mirror-Stratiﬁable Functions. To our
point of view, the notion of mirror-stratiﬁbility deserves a special study in view of
the following simple but powerful geometrical observation. We state it as a theorem
because of its utmost importance in the subsequent developments of this paper.

Theorem 1 (Enlarged activity identiﬁcation). Let R be a proper lsc convex
function which is mirror-stratiﬁable with respect to primal-dual stratiﬁcations M =
{M } and M∗ = {M ∗}. Consider a pair of points (¯x, ¯u) and the associated strata M¯x
and M ∗
¯u. If the sequence pair (xk, uk) → (¯x, ¯u) is such that uk ∈ ∂R(xk), then for k
large enough, xk is localized in a speciﬁc set of strata such that

M¯x (cid:54) Mxk

(cid:54) JR∗ (M ∗

¯u).

(2.6)

Proof. By assumption on R, ∂R is sequentially closed [50, Theorem 24.4] and

thus ¯u ∈ ∂R(¯x). Now, since xk is close to ¯x, upon invoking Proposition 1, we get

Mxk

(cid:62) M¯x

M ∗
uk

(cid:62) M ∗
¯u.

which shows the left-hand side of (2.6). Similarly, we have

(2.7)

Using the fact that ∂R(xk) is a closed convex set together with (2.3) leads to

uk ∈ ∂R(xk) = cl(ri(∂R(xk))) = cl(JR({xk})) ⊂ cl(JR(Mxk ))

which entails Muk
decreasing for the relation (cid:54), we get

(cid:54) JR(Mxk ). Using ﬁnally (2.7) and that by deﬁnition, JR is

M ∗
¯u

(cid:54) M ∗
uk

(cid:54) JR(Mxk ) =⇒ Mxk

(cid:54) JR∗ (M ∗

¯u),

whence we deduce the right-hand side of (2.6).

Mirror-stratiﬁcation thus allows us to prove the simple but powerful claim of
Theorem 1. As we will see in the rest of the paper, this result will be the backbone
to prove sensitivity and ﬁnite enlarged activity identiﬁcation results in absence of
non-degeneracy.

3. Sensitivity of Composite Optimization Problems. In this section, we

consider a parametric convex optimization problem of the form

E(x, p) def.= F (x, p) + R(x),

min
x∈RN

(P(p))

depending on the parameter vector p ∈ Π, where Π is the parameter set, an open
subset of a ﬁnite dimensional linear space. Sensitivity analysis studies the properties
of solutions x(cid:63)(p) of (P(p)) (assuming they exist) to perturbations of the parameters
vector p ∈ Π around some reference point ˚p. In Section 3.1, we brieﬂy review the
existing results on this topic and their limitations. We then introduce in Section 3.2
our new results obtained owing to the mirror-stratiﬁable structure.

3.1. Existing sensitivity results. Classical sensitivity results (see e.g. [7, 47,
21]) study the regularity of the set-valued map p (cid:55)→ x(cid:63)(p). A typical result proves
Lipschitz continuity of this map provided that E is smooth enough and E(·, ˚p) has
a local second-order (quadratic) growth at x(cid:63)(˚p), i.e. that there exists some c > 0
such that E(x, ˚p) (cid:62) E(x(cid:63)(˚p), ˚p) + c||x − x(cid:63)(˚p)||2 for x nearby x(cid:63)(˚p). For C 2-smooth
10

optimization, this growth condition is equivalent to positive deﬁniteness of the hessian
of E with respect to x evaluated at (x(cid:63)(˚p), ˚p) [29]. For classical smooth constrained
optimization problems, activity is captured by the subset of active inequality con-
straints. Under reasonable non-degeneracy conditions (see, for example, [29]), this
active set is stable under small perturbations to the objective.

A nice nonsmooth sensitivity theory is based on the notion of partial smooth-
ness [39]. Partial smoothness is an intrinsically geometrical assumption which, infor-
mally speaking, says that E behaves smoothly along an active manifold and sharply in
directions normal to the manifold. Furthermore, under a non-degeneracy assumption
at a minimizer (see (3.1)), it allows appealing statements of second-order optimality
conditions (including second-order generalized diﬀerentiation) and associated sensi-
tivity analysis around that minimizer [39, 40].

Let ∂E(x, p) be the subdiﬀerential of E according to x. Specializing the result of
[24, Proposition 8.4] to a proper lsc convex function E(·, p), one can show that C 2-
partial smoothness of E(·, p) at x(cid:63)(˚p) relative to some ﬁxed manifold (independent of
p) for 0, together with the non-degeneracy assumption

0 ∈ ri(∂E(x(cid:63)(˚p), ˚p))

(3.1)

is equivalent to the existence of an identiﬁable C 2-smooth manifold, i.e. for x(cid:63)(p) and
u(cid:63)(p) ∈ ∂E(x(cid:63)(p), p) close enough to x(cid:63)(˚p) and 0, x(cid:63)(p) lives on the active/partly
smooth manifold of x(cid:63)(˚p). If these assumptions are supplemented with a quadratic
growth condition of E (see above) along the active manifold, then one also has C 1
smoothness of the single-valued mapping p (cid:55)→ x(cid:63)(p) [39, Theorem 5.7].
It can be
deduced from [23, Corollary 4.3] that for almost all linear perturbations of lsc convex
semialgebraic functions, the non-degeneracy and quadratic growth conditions hold.
However, this genericity fails to hold for many cases of interest. As an example, con-
sider E of (P(λ, y)) with λ > 0 ﬁxed and p = y. If R is a proper lsc convex and
semialgebraic function, one has from [23] that for Lebesgue almost all Φ∗y, prob-
lem (P(λ, y)) has at most one minimizer at which furthermore non-degeneracy and
quadratic growth hold. Of course genericity in terms of Φ∗y does not imply that in
terms of y, which is our parameter of interest. Not to mention that we supposed λ
ﬁxed while it is not in many cases of interest.

3.2. Sensitivity analysis without non-degeneracy. For a ﬁxed p, (P(p)) is
a standard composite optimization problem. Here, we assume that the objective is the
sum of a C 1(RN ) convex function F (·, p) and a nonsmooth proper lsc convex function
R. We denote ∇F (x, p) the gradient of F (·, p) at x.

We are going to show that if the minimizer x(cid:63)(˚p) is unique, slight perturbations
p of ˚p generate solutions x(cid:63)(p) that are in a “controlled” stratum Mx(cid:63)(p) precisely
sandwiched to extreme strata deﬁned from a primal-dual pair associated to ˚p.

Theorem 2 (Sensitivity analysis with mirror-stratiﬁable functions). Let ˚p be a
given point in the parameter space Π. Assume that: (i) E(·, ˚p) has a unique mini-
mizer x(cid:63)(˚p), (ii) E is lsc on RN × Π, (iii) E(x(cid:63)(˚p), ·) is continuous at ˚p, (iv) ∇F
is continuous at (x(cid:63)(˚p), ˚p), and (v) E is level-bounded3 in x uniformly in p locally
around ˚p. If R is mirror-stratiﬁable according to (M,M∗), then for all p close to ˚p,

3 Recall from [51, Deﬁnition 1.16] that the function E : RN × Π is said to be level-bounded in x
locally uniformly in p around ˚p if for each c ∈ R, there exists a neighbourhood V of ˚p and a bounded
set Ω such that the sublevel set (cid:8)x ∈ RN : E(x, p) (cid:54) c(cid:9) ⊂ Ω for all p ∈ V.

11

(a)

(b)

12

Fig. 3.1. Graphical illustration of Theorem 2 for a projection problem on the (cid:96)∞ ball. (a)

corresponds to a degenerate situation while (b) to a non-degenerate one.

any minimizer x(cid:63)(p) of E(·, p) is localized as follows

Mx(cid:63)(˚p) (cid:54) Mx(cid:63)(p) (cid:54) JR∗ (M ∗

u(cid:63)(˚p)) where u(cid:63)(˚p) def.= −∇F (x(cid:63)(˚p), ˚p).

(3.2)

Proof. Le (pk)k∈N ⊂ Π be a sequence of parameters converging to ˚p. By assump-
tion on F and R, E(·, p) is proper for any p. Since it is also lsc and is level-bounded
in x uniformly in p locally around ˚p by conditions (ii) and (v). It follows from [51,
Theorem 1.17(a)] that Argmin E(·, pk) is non-empty and compact, and, in turn, any
sequence (x(cid:63)
k)k∈N of minimizers is bounded. We consider a subsequence, which for
simplicity, we denote again x(cid:63)

k → ¯x. We then have

E(¯x, ˚p)

(cid:54)
condition (ii)
(cid:54)
Optimality

lim inf
k

E(x(cid:63)

k, pk)

lim inf
k

E(x(cid:63)(˚p), pk)

=
condition (iii)

lim
k

E(x(cid:63)(˚p), pk) = E(x(cid:63)(˚p), ˚p).

By the uniqueness condition (i), we conclude that ¯x = x(cid:63)(˚p). Let u(cid:63)
k
Since ∇F is continuous at (˚x, ˚p) by assumption (iv), one has u(cid:63)
optimality condition of problem (P(p)) reads u(cid:63)
position to invoke Theorem 1 to conclude.

k ∈ ∂R(x(cid:63)

def.= −∇F (x(cid:63)
k, pk).
k → ˚u. The ﬁrst-order
k). Hence we are now in

Remark 7 (Single manifold identiﬁcation). In the special case where we have

the non-degeneracy condition

u(cid:63)(˚p) = −∇F (x(cid:63)(˚p), ˚p) ∈ ri(∂R(x(cid:63)(˚p))),

(3.3)

our result simpliﬁes and we recover the known exact identiﬁcation result. To see this,
we note that (3.3) also reads u(cid:63)(˚p) ∈ JR({x(cid:63)(˚p)}), and thus M ∗
u(cid:63)(˚p) = JR({x(cid:63)(˚p)}).
In this case, (3.2) becomes Mx(cid:63)(p) = M˚x for all p close to ˚p. Such a result was also
established in [35, 40] under condition (3.3), when F (·, ˚p) is also locally C 2 around
x(cid:63)(˚p) and R is partly smooth at x(cid:63)(˚p) relative to a C 2-smooth manifold Mx(cid:63)(˚p). An
example of this non-degenerate scenario is shown in Figure 3.1(b). Our result covers
the more delicate degenerate situation where u(cid:63)(˚p) might be on the relative boundary
of the subdiﬀerential, but requires the stronger mirror-stratiﬁability structure on the
non-smooth part. However, the active set at x(cid:63)(p) is in general not unique for all p
close to ˚p, hence the terminology enlarged activity.

Example 4. The result of Theorem 2 is illustrated in Figure 3.1, for the projec-
tion problem on a (cid:96)∞ ball, in both the degenerate and the non-degenerate situations.
More precisely, we take E(x, p) = 1
2 ||x−p||2 +ιB(cid:96)∞ (x), so that u(cid:63)(p) = p−x(cid:63)(p), where
x(cid:63)(p) = PB(cid:96)∞ (p). Figure 3.1(a) illustrates the degenerate case where u(cid:63)(˚p) belongs to
the relative boundary of the normal cone of B(cid:96)∞ at x(cid:63)(˚p). Taking a perturbation p
around ˚p entails that x(cid:63)(p) belongs either to M1 = Mx(cid:63)(˚p) or to the enlarged stratum
M2 = JR∗ (M ∗
2 ). In the non-degenerate case of Figure 3.1(b) presented
in Remark 7, the optimal solution x(cid:63)(p) is always on M1 for small perturbations.

u(cid:63)(˚p)) = JR∗ (M ∗

Remark 8 (Quadratic growth). We can also establish the Lipschitz continuity
of x(cid:63)(p) under the conditions of Theorem 2, under an additional second-order growth
condition, just as in classical sensitivity analysis (see [7, Section 4.2.1]). Let us
assume that there exists a neighbourhood V of x(cid:63)(˚p) and κ > 0 such that

E(x, ˚p) (cid:62) E(x(cid:63)(˚p), ˚p) + κ||x − x(cid:63)(˚p)||2

∀x ∈ V.

(3.4)

k → x(cid:63)(˚p) for pk → ˚p (see the proof of Theorem 2), we have x(cid:63)

Since x(cid:63)
k ∈ V for k
large enough. If, moreover, ∇F (x, ·) is Lipschitz-continuous in a neighbourhood of ˚p
with Lipschitz constant ν independent of x ∈ V, we get

κ||x(cid:63)

k − x(cid:63)(˚p)||2 (cid:54) E(x(cid:63)

k, ˚p) − E(x(cid:63)(˚p), ˚p)
= (cid:0)(E(x(cid:63)(˚p), pk) − E(x(cid:63)

k, pk)) − (E(x(cid:63)(˚p), ˚p) − E(x(cid:63)

k, ˚p))(cid:1)

− (cid:0)E(x(cid:63)(˚p), pk) − E(x(cid:63)

k, pk)(cid:1)

(x(cid:63)

k minimizes E(·, pk)) (cid:54) (E(x(cid:63)(˚p), pk) − E(x(cid:63)
(By (P(p))) = (F (x(cid:63)(˚p), pk) − F (x(cid:63)

k, pk)) − (E(x(cid:63)(˚p), ˚p) − E(x(cid:63)
k, pk)) − (F (x(cid:63)(˚p), ˚p) − F (x(cid:63)

k, ˚p)
k, ˚p))

(Mean-Value Theorem) (cid:54) (cid:0) sup

||∇F (x, pk) − ∇F (x, ˚p)||(cid:1)||x(cid:63)

k − x(cid:63)(˚p)||

x∈V

(cid:54) ν||pk − ˚p||||x(cid:63)

k − x(cid:63)(˚p)||.

whence we conclude that

d(x(cid:63)(˚p), Argmin E(·, pk)) (cid:54) ν/κ||pk − ˚p||.

4. Regularized Inverse Problems. A typical context where our framework of
mirror-stratiﬁability is of usefulness is that of studying stability to noise of regularized
linear inverse problems. We come back to the situation presented in introduction:

13

studying stability issues to perturbed observations of the form y = ˚y + w amounts
to analyzing sensitivity of the minimizers and the optimal value function in (P(λ, y))
when the parameter p = (λ, y) evolves around the reference point ˚p = (0, ˚y). Unlike
the usual sensitivity analysis theory setting recalled in the previous section (e.g. [7]),
here the objective E may not even be continuous at ˚p.

We provide hereafter some pointers to the relevant literature, then we develop
our sensitivity results for mirror-stratiﬁable functions. These results involve primal
and dual solutions to the noiseless problem

E(x, (0, y)) def.= R(x)

s.t. Φx = y,

(P(0, y))

min
x∈RN

4.1. Existing sensitivity results for regularized Inverse Problems.
Lipschitz stability. Assume there exists a dual multiplier η ∈ RP (sometimes
referred to as a “dual certiﬁcate”) for the noiseless constrained problem (P(0, y))
taken at y = ˚y such that Φ∗η ∈ ∂R(˚x). The latter condition is equivalent to ˚x being
a minimizer of (P(0, y)). This condition goes by the name of the “source” or “range”
condition in the inverse problems literature. It has been widely used to derive stability
results in terms of ||Φx(cid:63)(λ, y)−Φ˚x|| or R(x(cid:63)(λ, y))−R(˚x)−(cid:104)Φ∗η, x(cid:63)(λ, y) − ˚x(cid:105); see [52]
and references therein. To aﬀord stability in terms of ||x(cid:63)(λ, y) −˚x|| directly, the range
condition has to be strengthened to its non-degenerate version Φ∗η ∈ ri(∂R(˚x)). It
has been shown that this condition implies that the set-valued map (λ, y) (cid:55)→ x(cid:63)(λ, y)
is Lipschitz-continuous at (0, ˚y); see [32] and [58].

Active set stability. In the case where R is partly smooth, one can approach
an even more complete sensitivity theory by studying stability of the partly smooth
manifold of R at ˚x. In particular, it can be shown from that if an appropriate non-
degeneracy assumption holds, see [59], then problems (P(0, y)) and (P(λ, y)) have
unique minimizers (respectively ˚x and x(cid:63)(λ, y)), and x(cid:63)(λ, y) lies on the partly smooth
manifold of R at ˚x. Observe that compared to Lipschitz stability, active set stability
is more demanding as the non-degeneracy condition has to hold for a speciﬁc dual
multiplier, which is obviously more stringent. This type of results has appeared many
times in the literature for special cases, e.g.
for the (cid:96)1 norm [30, 61], the nuclear
norm [1]. The work in [57, 59] has uniﬁed all these results.

Non-degeneracy in practice for deconvolution and compressed sensing. The above
results require that some abstract non-degeneracy condition holds, which imposes
strict limitations on practical situations. In particular, when Φ is a convolution oper-
ator and ˚x is sparse, [10] studies Lipschitz stability and [25] support stability. In this
setting, the non-degeneracy condition holds whenever the non-zero entries are sepa-
rated enough, which is not often veriﬁed. Another setting where this stability theory
has been applied in when Φ is drawn from a random matrix ensemble, i.e. compressed
sensing. For a variety of partly smooth regularizers (including the (cid:96)1, nuclear and (cid:96)1,2
norms), the non-degeneracy condition holds with high probability if, roughly speak-
ing, the sample size P is suﬃciently larger than the “dimension” of the active set at
˚x (see e.g. [12, 22]). Again this is a clear limitation as illustrated in Section 1.2.

4.2. Primal and dual problems. Suppose we have observations of the form
(1.1), and we want to recover ˚x (or a provably good approximation of it). As advocated
in Section 1.1, a popular approach is to adopt a regularization framework which can
be cast as the optimization problem (P(λ, y)) (for λ > 0) and (P(0, y)) (when λ = 0).
In the sequel, we assume that

R∞(z) > 0,

∀z ∈ ker(Φ) \ {0},

(4.1)

14

where R∞ is the asymptotic (or recession) function of R, deﬁned as

R∞(z) def.= lim
t→+∞

R(x + tz) − R(x)
t

,

∀x ∈ dom(R).

Condition (4.1) is a necessary and suﬃcient condition for the set of minimizers of
(P(λ, y)) and (P(0, y)) to be non-empty and compact [56, Lemma 5.1]. It is satisﬁed
for example when R is coercive.

Remark 9 (Discontinuity of F ). Letting p def.= (λ, y) ∈ Π def.= R+ × RP , we see

that (P(λ, y)) and (P(0, y)) are instances of (P(p)), by setting

F (x, p) def.=

(cid:40) 1

2λ ||y − Φx||2
ιHy (x)

if λ > 0,
if λ = 0,

where Hy

def.= (cid:8)x ∈ RN : Φx = y(cid:9) .

(4.2)

The corresponding function F considered does not obey the assumptions of Theorem 2.
Indeed, the parameter set Π is not open, with ˚p = (0, ˚y) that lives on the boundary of
Π, and F is only lsc at such ˚p (because of the aﬃne constraint Φx = y). The main
consequence will be that one can no longer allow p to vary freely nearby ˚p.

In order to study the sensitivity of solutions, we look at the Fenchel-Rockafellar

dual problem, which reads (for all λ (cid:62) 0)

(cid:104)q, y(cid:105) −

||q||2 − R∗(Φ∗q).

max
q∈RP

λ
2

(D(λ, y))

We denote D(λ, y) the set of solutions of (D(λ, y)). Note that for λ > 0, thanks to
strong concavity, there is a unique dual solution q(cid:63)(λ, y), i.e. D(λ, y) = {q(cid:63)(λ, y)}. We
also have from the primal-dual extremality relationship that for any primal solution
x(cid:63)(λ, y) of (P(λ, y)),

q(cid:63)(p) =

y − Φx(cid:63)(p)
λ

and Φ∗q(cid:63)(p) ∈ ∂R(x(cid:63)(p)).

(4.3)

While (D(0, y)) is the dual of (P(0, y)), it is important to realize that it is not the limit
of (D(λ, y)) in the sense that its set of dual solutions is in general not a singleton.
Lemma 3 hereafter singles out a speciﬁc dual optimal solution (sometimes called
“minimum norm certiﬁcate”) deﬁned by

˚q(0, y) = argmin

{||q|| : Φ∗q ∈ ∂R(x(cid:63)(0, y))} = argmin

{||q|| : q ∈ D(0, y)} .

(4.4)

q∈RP

q∈RP

The following two important lemmas ensure the convergence of the solutions to the
primal and dual problems as λ → 0 and as ||y − ˚y|| → 0.

Lemma 2 (Primal solution convergence). Assume that ˚x is the unique solution

to (P(0, ˚y)). For any sequence of parameters pk = (λk, yk) with λk > 0 such that

(cid:18) ||yk − ˚y||2
λk

(cid:19)

, λk

−→ (0, 0),

and any solution x(cid:63)(pk) of (P(pk)), we have x(cid:63)(pk) → ˚x.

Proof. Denote x(cid:63)
k

can extract a converging subsequence, which for simplicity, we denote again x(cid:63)
Optimality of x(cid:63)

k)k∈N is a bounded sequence by (4.1), one
k → ¯x.

def.= x(cid:63)(pk). Since (x(cid:63)

k implies that
k) (cid:54) 1
2λk

R(x(cid:63)

||yk − Φx(cid:63)

k||2 + R(x(cid:63)

||yk − ˚y||2 + R(˚x).

(4.5)

k) (cid:54) 1
2λk

15

Passing to the limit and using the hypothesis that 1
λk

||yk − ˚y||2 → 0, we get

lim sup
k

R(x(cid:63)

k) (cid:54) R(˚x).

k) (cid:62) R(¯x). Com-
On the other hand, lower semi-continuity of R entails lim inf k R(x(cid:63)
bining these two inequalities, we deduce that R(¯x) (cid:54) R(˚x). Since R is proper and lsc,
it is bounded from below on bounded sets [51, Corollary 1.10]. Let r = inf k R(x(cid:63)
k)
which then satisﬁes −∞ < r < +∞. Substracting r from (4.5) and multiplying by
λk, one obtains

1
2

||yk − Φx(cid:63)

k||2 (cid:54) 1
2

||yk − Φx(cid:63)

k||2 + λk(R(x(cid:63)

||yk − ˚y||2 + λk(R(˚x) − r). (4.6)

k) − r) (cid:54) 1
2

Consequently, passing to the limit in (4.6) shows that ||˚y − Φ¯x|| = 0, i.e. ¯x is a feasible
point of (P(0, ˚y)). Altogether, this shows that ¯x is a solution of (P(0, ˚y)), and by
uniqueness of the minimizer, ¯x = ˚x.

Lemma 3 (Dual solution convergence). For any sequence of parameters pk =

(λk, yk) such that

(cid:18) ||yk − ˚y||
λk

(cid:19)

, λk

−→ (0, 0),

we have q(cid:63)(pk) → ˚q(0, ˚y).

Proof. By the triangle inequality, we have

||q(cid:63)(λk, yk) − ˚q(0, ˚y)|| (cid:54) ||q(cid:63)(λk, yk) − q(cid:63)(λk, ˚y)|| + ||q(cid:63)(λk, ˚y) − ˚q(0, ˚y)||.

For the ﬁrst term, we notice that q(cid:63)(λ, y) = proxR∗◦Φ∗/λ(y/λ) for λ > 0. Thus,
Lipschitz continuity of the proximal mapping entails that

||q(cid:63)(λk, yk) − q(cid:63)(λk, ˚y)|| (cid:54) ||yk − ˚y||

,

λk

λk
2
λk
2

which in turn shows that q(cid:63)(λk, yk) → q(cid:63)(λk, ˚y). Let us now turn to the second term.
k = q(cid:63)(λk, ˚y) and ˚q def.= ˚q(0, ˚y) ∈ D(0, ˚y), one
Using the respective optimality of q(cid:63)
obtains

− (cid:104)q(cid:63)

k, ˚y(cid:105) +

||q(cid:63)

k||2 + R∗(Φ∗q(cid:63)

k) (cid:54) − (cid:104)˚q, ˚y(cid:105) +

||˚q||2 + R∗(Φ∗˚q)

λk
2

(cid:54) − (cid:104)q(cid:63)

k, ˚y(cid:105) +

||˚q||2 + R∗(Φ∗q(cid:63)

k),

(4.7)

k|| (cid:54) ||˚q||, which shows in particular that (q(cid:63)

whence we get ||q(cid:63)
k)k∈N is a bounded se-
quence. We can thus extract any converging subsequence, which for simplicity, we
denote again q(cid:63)
k → ¯q. Passing to the limit in (4.7), using the fact that R∗ is lsc, one
obtains

− (cid:104)¯q, ˚y(cid:105) + R∗(Φ∗ ¯q) (cid:54) − (cid:104)¯q, ˚y(cid:105) + lim inf

R∗(Φ∗q(cid:63)

k) (cid:54) − (cid:104)˚q, ˚y(cid:105) + R∗(Φ∗˚q),

which shows that ¯q ∈ D(0, ˚y). This together with the fact that ||q(cid:63)
already saw, shows that ¯q = ˚q by uniqueness of ˚q in (4.4).

k|| (cid:54) ||˚q|| as we

k

16

4.3. Sensitivity. We are now in position to state the main result of this section,
which tracks the strata of x(cid:63)(p) in the regime where the perturbation ||w|| = ||y − ˚y||
is suﬃciently small.

Theorem 3. Suppose that ˚x is the unique solution to (P(0, ˚y)). Assume fur-
thermore that R is mirror-stratiﬁable with respect to the primal-dual stratiﬁcations
(M, M∗). If there are constants (C0, C1) depending only on ˚x such that for all p in

{p = (λ, y) : C0||y − ˚y|| (cid:54) λ (cid:54) C1} ,

then there exists a minimizer x(cid:63)(p) of E(·, p) localized as follows

M˚x (cid:54) Mx(cid:63)(p) (cid:54) JR∗ (M ∗

˚u) where ˚u def.= Φ∗˚q(0, ˚y).

(4.8)

(4.9)

Proof. Under (4.8) with for instance C1 small enough, one can easly check
that there exists k large enough such that the regime required for (yk, λk) to ap-
In turn, we have a converging primal-dual pair
ply Lemma 2 and 3 is attained.
(x(cid:63)(pk), Φ∗q(cid:63)(pk)) → (˚x, ˚u). One can then apply Theorem 1 to conclude.

5. Activity Localization with Proximal Splitting Algorithms. Proximal
splitting methods are algorithms designed to solve large-scale structured optimization
and monotone inclusion problems, by evaluating various ﬁrst-order quantities such as
gradients, proximity operators, linear operators, all separately at various points in the
course of an iteration. Though they can show slow convergence each iteration has a
cheap cost. We refer to e.g. [3, 5, 17, 49] for a comprehensive treatment and review.
Capitalizing on our enlarged activity identiﬁcation result for mirror-stratiﬁable
functions, we now instantiate its consequences on ﬁnite activity localization of prox-
imal splitting algorithms. While existing results on ﬁnite identiﬁcation (of a sin-
gle active set) strongly rely on partial smoothness around a non-degenerate cluster
point [34, 42, 43, 41], we examine here intricate situations where neither of these
assumptions holds.

5.1. Forward-Backward algorithm. The Forward–Backward (FB) splitting
method [44] is probably the most well-known proximal splitting algorithm. In our
context, it can be used to solve optimization problems with the additive “smooth +
non-smooth” structure of the form

min
x∈RN

f (x) + R(x),

where f ∈ C 1(RN ) is convex with L-Lipschitz gradient, and R is a proper lsc and
convex function. We assume that Argmin(f + R) (cid:54)= ∅. The FB iteration in relaxed
form reads [16]

xk+1 = (1 − τk)xk + τk proxγkR(xk − γk∇f (xk)),

with γk ∈]0, 2/L[ and τk ∈]0, 1], where proxγR, γ > 0, is the proximal mapping of R,

(5.1)

(5.2)

(5.3)

proxγR(x) def.= argmin
z∈RN

1
2

||z − x||2 + γR(z).

Diﬀerent variants of FB method were studied, and a popular trend is the inertial
schemes which aim at speeding up the convergence (see [48, 4], and the sequence-
convergence version as proved recently in [14]).

17

Under the non-degeneracy assumption −∇f (x(cid:63)) ∈ ri(∂R(x(cid:63))), it was shown in
[42] that FB and its inertial variants correctly identify the active manifold in ﬁnitely
many iterations, and then enter a local linear convergence regime. These results
encompass many special cases such as those studied in [33, 8, 54]. Beyond this non-
degenerate case, we establish now the general localization of active strata.

Theorem 4. Consider the FB iteration (5.2) to solve (5.1) with 0 < inf k γk (cid:54)
supk γk < 2/L and inf k τk > 0. Then xk converges to x(cid:63) ∈ Argmin(f + R). Assume
that R is also mirror-stratiﬁable with respect to (M, M∗), then for k large enough,

Mx(cid:63) (cid:54) Mxk

(cid:54) JR∗ (M ∗

u(cid:63) ) where u(cid:63) def.= −∇f (x(cid:63)).

Proof. Convergence of the sequence (xk)k∈N to x(cid:63) is obtained from [16, Corol-
lary 6.5]. Moreover, since the proximal mapping is the resolvent of the subdiﬀerential,
(5.2) is equivalent to the monotone inclusion

uk+1

def.=

xk − vk+1
γk

− ∇f (xk) ∈ ∂R(vk+1),

def.= xk+1−xk

+ xk. In turn, with the conditions inf k τk > 0 and inf k γk > 0,
where vk+1
and continuity of ∇f , we have vk → x(cid:63) and thus uk → −∇f (x(cid:63)) ∈ ∂R(x(cid:63)). It then
remains to apply Theorem 1 to (vk, uk) and R to conclude.

τk

It can be easily shown that Theorem 4 holds for several extensions of the iterate-

convergent version of FISTA [14]. We omit the details here for the sake of brevity.

We rather take a closer look to the case when the FB scheme (5.2) to solve (P(λ, y))
(see also (4.2)) for λ > 0. Putting together Theorem 3 and 4, we obtain the following
localization result depending only on the data to estimate ˚x assuming that the noise
level ||y − ˚y|| is small enough.

Proposition 4. Under the assumptions of Theorem 3, consider the FB iteration
(5.2) to solve (P(λ, y)) with 0 < inf k γk (cid:54) supk γk < 2λ/||Φ||2 and inf k τk > 0. Then,
for k large enough, the iterates xk satisfy

M˚x (cid:54) Mxk

(cid:54) JR∗ (M ∗

˚u) where ˚u def.= Φ∗q(0, ˚y),

with q(0, ˚y) from (4.4).

Proof. To lighten notation, denote p = (λ, y). As in Theorem 4, we have xk →

x(cid:63)(p) a minimizer of (P(λ, y)). Thus we get

M˚x (cid:54) Mx(cid:63)(p) (cid:54) Mxk ,

where the ﬁrst inequality comes from Theorem 3 and the second one from Theorem 4.
This gives the ﬁrst inequality in the localization result.

Moreover, in the special case at hand ∂R(x(cid:63)(p)) (cid:51) −∇f (x(cid:63)(p)) = Φ∗q(cid:63)(p) → ˚u
as p → (0, ˚y), where we invoked (4.3). It then follows from (2.7) and the fact that
JR∗ is decreasing for the relation (cid:54), that

M ∗
˚u

(cid:54) M ∗

−∇f (x(cid:63)(p)) ⇐⇒ JR∗ (M ∗

−∇f (x(cid:63)(p))) (cid:54) JR∗ (M ∗

˚u).

Using once again Theorem 4, we get

Mxk

(cid:54) JR∗ (M ∗

−∇f (x(cid:63)(p))) (cid:54) JR∗ (M ∗

˚u),

18

which yields the second desired inequality.

Though the iterates xk of FB do not converge to ˚x, this proposition tells us that
the iterates identify an enlarged stratum associated to ˚x. This is an appealing feature
from a practical perspective, since one can often make some prior assumption on the
sought after vector ˚x, such as for instance sparsity or low-rank properties, as we have
illustrated in the numerical experiments of Section 6.

5.2. Douglas-Rachford Splitting Algorithm. The Douglas-Rachford (DR)
method [44] is another popular splitting method designed to minimize convex objec-
tives having the additive “non-smooth + non-smooth” structure of the form

with f and g be proper lsc and convex functions such that ri(dom(f ))∩ri(dom(g)) (cid:54)= ∅
and Argmin(f + g) (cid:54)= ∅. The DR scheme reads

min
x∈RN

f (x) + g(x).






vk+1 = proxγf (2xk − zk) ,
zk+1 = zk + τk (vk+1 − xk) ,
xk+1 = proxγg(zk+1),

(5.4)

(5.5)

where γ > 0, τk ∈]0, 2[ is a relaxation parameter. By deﬁnition, the DR method is
not symmetric with respect to the order of the functions f and/or g. Nevertheless,
all of our statements throughout hold true, with obvious adaptations, when the order
of f and g is reversed in (5.5).

It has been shown in [43] that under appropriate non-degeneracy assumptions,
the DR identiﬁes the active manifolds in ﬁnite time, and then shows a local linear
regime. These results unify all those that were established in the literature for special
problems, see e.g.
[20] for linearly constrained (cid:96)1-minimization, [6] for quadratic or
linear programs, [2] for feasibility with two subspaces. Under mirror-stratiﬁability of
f or g, we get the following enlarged activity identiﬁcation result.

Theorem 5. Consider the DR iteration (5.5) to solve (5.4) with τk ∈]0, 2[
such that (cid:80)
k∈N τk (2 − τk) = +∞. Then zk converges to a ﬁxed point z(cid:63) with
x(cid:63) = proxγg(z(cid:63)) ∈ Argmin(f + g), and xk and vk both converge to x(cid:63). Introduc-
ing u(cid:63) = z(cid:63)−x(cid:63)

, we have furthermore:

γ

(i) If g is mirror-stratiﬁable with respect to the primal-dual stratiﬁcations (M g, M g∗

),

then for k large enough

(ii) If f is mirror-stratiﬁable with respect to the primal-dual stratiﬁcations (M f , M f ∗

),

then for k large enough

M g

x(cid:63) (cid:54) M g
xk

(cid:54) Jg∗ (M g∗

u(cid:63) ).

M f

x(cid:63) (cid:54) M f
vk

(cid:54) Jf ∗ (M f ∗

−u(cid:63) ).

Proof. Under the prescribed choice of τk, convergence of zk is ensured by virtue
of [16, Corollary 5.2]. By non-expansiveness of the proximal mapping, and as we are
in ﬁnite dimension, we also obtain convergence of xk and vk to x(cid:63). To prove (i), note
that the update of xk in (5.5) is equivalent to the monotone inclusion

uk

def.=

∈ ∂g(xk).

zk − xk
γ

19

(5.6)

Since (xk, uk) → (x(cid:63), u(cid:63)), we conclude about (i) by invoking Theorem 1. Similarly,
we note that the update of vk in (5.5) is equivalent to

wk

def.=

2xk − zk − vk+1
γ

∈ ∂f (vk+1).

(5.7)

Using that (vk, wk) → (x(cid:63), −u(cid:63)) and applying Theorem 1 we get (ii).

In the same vein as for FB in the previous section, we now turn to applying
the DR scheme (5.5) to solve (P(λ, y)) by setting in (5.4) f (x) = 1
2λ ||y − Φx||2 and
g(x) = R(x). Putting Theorem 3 and 5-(i) together, we obtain the following analogue
to Proposition 4.

Proposition 5. Under the assumptions of Theorem 3, consider the DR iteration
k∈N τk (2 − τk) = +∞.

(5.5) to solve (P(λ, y)) with γ > 0 and τk ∈]0, 2[ such that (cid:80)
Then, for k large enough, the DR iterates satisfy

M˚x (cid:54) Mxk

(cid:54) JR∗ (M ∗

˚u) where ˚u def.= Φ∗q(0, ˚y).

Proof. The proof follows the same reasoning as that of Proposition 4 by addition-
ally observing (recall the notation in the proof of Theorem 5) that from (5.6)-(5.7),
we have u(cid:63)(p) = −∇f (x(cid:63)(p)) = Φ∗q(cid:63)(p) ∈ ∂R(x(cid:63)(p)).

6. Numerical Illustrations. In this section, we numerically illustrate our the-
oretical results on sensitivity and enlarged activity identiﬁcation in the context of
regularized inverse problems. We adopt the same two “compressed sensing” scenarios
described in Section 1.2. The dimension dim(M˚x) = R0(˚x) of the strata associated
to ˚x is measured as R0 = || · ||0 (resp. R0 = rank) for the (cid:96)1 (resp. nuclear) norm
regularization.

Strata sensitivity. We ﬁrst illustrate the relevance of the strata sensitivity result in
Theorem 3 by studying the dimension of the largest possible active stratum JR∗ (M ∗
˚u)
(in fact its closure). The dual ˚u = Φ∗˚q(0, ˚y) is computed from ˚x by solving the convex
optimization problem (4.4) (using CVX to get a high precision). Thus we know the
maximum complexity index excess predicted by Theorem 3, i.e.

δ(cid:63)(˚x) def.= dim(JR∗ (M ∗

˚u)) − dim(M˚x).

For each given δ and R0(˚x), and among the 1000 randomly generated replications of
(˚x, Φ), we compute the proportion ρ(R0(˚x), δ) of ˚x such that it is the unique solution
of (P(0, ˚y)) and δ(cid:63)(˚x) (cid:54) δ. The proportions ρ(R0(˚x), δ) are displayed in Figure 6.1 as
a function of the input complexity index R0(˚x) = dim(M˚x). The colors from blue to
red correspond to increasing δ.

The proportion ρ(R0(˚x), δ) is an increasing function of δ and a decreasing function
of R0(˚x). Indeed, as anticipated from standard compressed sensing results [58], active
strata M˚x of vectors ˚x whose dimension is small enough compared to the number of
measurements P can be provably and stably recovered with overwhelming probability
(on the sampling of (˚x, Φ, w)). As R0(˚x) increases, the number of measurements P
becomes insuﬃcient to ensure non-degeneracy with high probability, hence preventing
stable recovery of M˚x. However, Theorem 3 predicts that the active stratum of
x(cid:63)(λ, y) for y nearby ˚y is localized between M˚x and JR∗ (M ∗

˚u).

The blue curve in each plot of Figure 6.1 corresponds to δ = 0, which is the
proportion ρ(R0(˚x), 0) of vectors ˚x whose active stratum M˚x can be recovered stably
under small noise perturbation by solving (P(λ, y)) for λ chosen according to (4.8).

20

R = || · ||1

R = || · ||∗

Fig. 6.1. Proportion of ˚x such that δ(cid:63)(˚x) (cid:54) δ as a function of R0(˚x) (increasing value of δ

from 0 to its maximal value is depicted by a color evolving from blue to red).

R = || · ||1

R = || · ||∗

Fig. 6.2. Plots of R0(xk) for the 2000 ﬁrst iterates generated by the Forward-Backward algo-

rithm. Plots in blue correspond to the cases when δ(cid:63)(˚x) = 0, and the red ones for δ(cid:63)(˚x) = δ.

This proportion shows a phase transition phenomenon between stable recovery and
unstable recovery. The location of the phase transition for δ = 0 can be predicted
accurately; see for instance [59].

The red curves in Figure 6.1 correspond to the extreme case where δ takes its
largest achievable value, i.e. where we can guarantee recovery of the largest stratum
JR∗ (M ∗
˚u) with high probability. The phase transition occurs for higher dimension
R0(˚x). The intermediate curves, i.e. from blue to red, correspond to the recovered
strata that are localized between M˚x and JR∗ (M ∗
˚u) (i.e. increasing δ). The phase
transition progressively increases with δ. These curves illustrate and quantify the
typical tradeoﬀ observed in practice: one can allow for more complex input vectors ˚x
(i.e. those with larger R0(˚x)) at the expense of recovering active strata larger than M˚x.
Forward-Backward ﬁnite activity localization. We now numerically illustrate the
ﬁnite enlarged activity identiﬁcation of the FB splitting scheme as predicted by The-
orem 4 and Proposition 4. We remain under the same compressed sensing setting
as before. The randomly generated replications of ˚x are such that R0(˚x) = 10 for
R = || · ||1 and to 4 for R = || · ||∗.

The evolution of the complexity index R0(xk) of the FB iterate xk is shown in
Figure 6.2. The blue lines correspond to several trajectories (the bold one is the
average trajectory), each for a randomly generated instance of ˚x such that δ(cid:63)(˚x) =
21

0, i.e. those vectors whose active strata M˚x can be exactly recovered under small
perturbations. Thus, the iterates identify M˚x in ﬁnite time. The red lines (the bold
one is the average trajectory) are those for which δ(cid:63)(˚x) = δ > 0. As anticipated by
our theoretical results, the iterates identify a stratum strictly larger that M˚x.

Acknowledgements. The work of Gabriel Peyr´e has been supported by the Euro-
pean Research Council (ERC project SIGMA-Vision). Jalal Fadili was partly supported by
Institut Universitaire de France.

Appendix A. Proofs of the results in Section 2.
Proof of Proposition 1. Let us ﬁrst prove the ﬁrst equivalence. Observe that
Deﬁnition 2 can be read as follows: M is active at x if and only if d(x, cl(M )) = 0.
Since there is a ﬁnite number of strata in the stratiﬁcation, let us consider δ the
minimum of the nonzero distances d(x, cl(M (cid:48))) for M (cid:48) not active. For all x(cid:48) in the
open ball of radius δ and of center x, we have

d(x, cl(Mx(cid:48))) (cid:54) ||x − x(cid:48)|| < δ

⇐⇒

d(x, cl(Mx(cid:48))) = 0.

This shows that the set of these strata Mx(cid:48) indeed coincides the set of active strata,
whence we get the ﬁrst equivalence.

Let us turn to the second equivalence. Let M be an active strata at x, that
is, x ∈ cl(M ). Since x ∈ Mx, the intersection cl(M ) ∩ Mx contains x and thus is
nonempty. We deduce from (2.1) that M (cid:62) Mx. Conversely, if M (cid:62) Mx, then
x ∈ Mx ⊂ cl(M ) and therefore M is active at x.

Proof of Proposition 2. From classical convex analysis calculus rules, we get for

all x ∈ dom R

∂R(x) = conv {ai

: i ∈ I max(x)} + cone (cid:8)ai

: i ∈ I feas(x)(cid:9) .

By deﬁnition of MI , the relative interior of ∂R(x) is constant over a stratum: for all
x ∈ MI (with MI (cid:54)= ∅)

ri(∂R(x)) = ri (cid:0) conv {ai
= ri(conv {ai

= ri(conv {ai

: i ∈ I max(x)} + cone (cid:8)ai
: i ∈ I max(x)}) + ri(cone (cid:8)ai
: i ∈ I max}) + ri(cone (cid:8)ai

: i ∈ I feas(x)(cid:9) (cid:1)

: i ∈ I feas(x)(cid:9))

: i ∈ I feas(cid:9)).

This yields JR(MI ) = M ∗
entails for all u ∈ M ∗
I

I . Conversely we have ∂R∗(u) = {x : u ∈ ∂R(x)}, which

∂R∗(u) = (cid:8)x : I max(x) ∩ I feas(x) ⊃ I(cid:9) ,

and therefore ri(∂R∗(u)) = MI . This gives JR∗ (M ∗
Deﬁnition 4.

I ) = MI , which proves item (i) of

To show (ii) of Deﬁnition 4, we make the following observation:

MI (cid:54) MI (cid:48) ⇐⇒ any x ∈ MI lies in cl(MI (cid:48))
max

⇐⇒ I max = I max(x) ⊃ (I (cid:48))
⇐⇒ I ⊃ I (cid:48).

On the other hand we have

and I feas = I feas(x) ⊃ (I (cid:48))

feas

cl(JR(MI )) ⊃ cl(ri(conv {ai

: i ∈ I max})) + cl(ri cone (cid:8)ai

: i ∈ I feas(cid:9)))

= conv {ai

: i ∈ I max} + cone (cid:8)ai

: i ∈ I feas(cid:9) .

22

Note that the ﬁrst ⊃ is in fact = because conv {ai
unique decomposition of a polyhedron, we can write,

: i ∈ I max} is compact. Using

JR(MI ) (cid:62) JR(MI (cid:48)) ⇐⇒ cl(JR(MI )) ⊃ JR(MI (cid:48)) ⇐⇒ I ⊃ I (cid:48),

which ends the proof.

Proof of Proposition 3. The proof builds upon a key result stated in [19]. Theo-
rem 4.6(i) in [19] asserts that the collection (cid:8)σ−1(M sym) : M ∈ M(cid:9) forms a smooth
stratiﬁcation of dom(R) with the desired properties. The fact that spectral func-
tions are mirror-stratiﬁable follows from the polyhedral case with the help of Theo-
rem 4.6(iv) in [19], which states that

JR(σ−1(M sym)) = σ−1(cid:0)JRsym (M sym)(cid:1)

together with continuity of the singular value mapping σ.

REFERENCES

[1] F. Bach. Consistency of trace norm minimization. The Journal of Machine Learning Research,

9(Jun):1019–1048, 2008.

[2] H. Bauschke, J.Y.B. Cruz, T.A. Nghia, H.M. Phan, and X. Wang. The rate of linear convergence
of the douglas-rachford algorithm for subspaces is the cosine of the friedrichs angle. J. of
Approx. Theo., 185(63–79), 2014.

[3] H. H. Bauschke and P. L. Combettes. Convex analysis and monotone operator theory in Hilbert

spaces. Springer, 2011.

[4] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.

[5] A. Beck and M. Teboulle. Gradient-based algorithms with applications to signal recovery.

Convex Optimization in Signal Processing and Communications, 2009.

[6] D. Boley. Local linear convergence of the alternating direction method of multipliers on

quadratic or linear programs. SIAM J. Optim., 23(4):2183–2207, 2013.

[7] J. F. Bonnans and A. Shapiro. Perturbation analysis of optimization problems. Springer Series

in Operations Research and Financial Engineering. Springer Verlag, 2000.

[8] K. Bredies and D. A. Lorenz. Linear convergence of iterative soft-thresholding. Journal of

Fourier Analysis and Applications, 14(5-6):813–837, 2008.

[9] P. B¨uhlmann and S. Van De Geer. Statistics for high-dimensional data: methods, theory and

applications. Springer, 2011.

[10] E. J. Cand`es and C. Fernandez-Granda. Towards a mathematical theory of super-resolution.

Communications on Pure and Applied Mathematics, 67(6):906–956, 2013.

[11] E. J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations of

Computational mathematics, 9(6):717–772, 2009.
[12] E. J. Cand`es and T. Tao. Decoding by linear programming.

Information Theory, IEEE

Transactions on, 51(12):4203–4215, 2005.

[13] E. J. Cand`es and T. Tao. The power of convex relaxation: Near-optimal matrix completion.

Information Theory, IEEE Transactions on, 56(5):2053–2080, 2010.

[14] A. Chambolle and C. Dossal. On the convergence of the iterates of the “fast iterative
shrinkage/thresholding algorithm”. Journal of Optimization Theory and Applications,
166(3):968–982, 2015.

[15] S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM

journal on scientiﬁc computing, 20(1):33–61, 1999.

[16] P. L. Combettes. Solving monotone inclusions via compositions of nonexpansive averaged

operators. Optimization, 53(5-6):475–504, 2004.

[17] P. L. Combettes and J.-C. Pesquet. Proximal splitting methods in signal processing. In H. H.
Bauschke, Burachik R. S., P. L. Combettes, Elser. V., D. R. Luke, and H. Wolkowicz,
editors, Fixed-Point Algorithms for Inverse Problems in Science and Engineering, pages
185–212. Springer, 2011.

[18] M. Coste. An introduction to o-minimal geometry. Technical report, Institut de Recherche

Mathematiques de Rennes, November 1999.

[19] A. Daniilidis, D. Drusvyatskiy, and A. S. Lewis. Orthogonal invariance and identiﬁability.

SIAM Journal on Matrix Analysis and Applications, 35(2):580–598, 2014.

23

[20] L. Demanet and X. Zhang. Eventual linear convergence of the douglas-rachford iteration for

basis pursuit. Mathematics of Computation, 2013. to appear.

[21] A. L. Dontchev. Perturbations, approximations, and sensitivity analysis of optimal control

systems, volume 52. Springer-Verlag, Berlin, 1983.

[22] C. Dossal, M.-L. Chabanol, G. Peyr´e, and J. M. Fadili. Sharp support recovery from noisy ran-
dom measurements by (cid:96)1-minimization. Applied and Computational Harmonic Analysis,
33(1):24–43, 2012.

[23] D. Drusvyatskiy, A. D. Ioﬀe, and A. S. Lewis. Generic minimizing behavior in semialgebraic

optimization. SIAM J. Optim., 26(1):513–534, 2016.

[24] D. Drusvyatskiy and A.S. Lewis. Optimality, identiﬁability, and sensitivity. Mathematical

Programming, to appear in, pages 1–32, 2013.

[25] V. Duval and G. Peyr´e. Exact support recovery for sparse spikes deconvolution. Foundations

of Computational Mathematics, 15(5):1315–1355, 2015.

[26] V. Duval and G. Peyr´e. Sparse spikes deconvolution on thin grids. Preprint 01135200, HAL,

2015.

[27] C. Ekanadham, D. Tranchina, and E. P. Simoncelli. A uniﬁed framework and method for
automatic neural spike identiﬁcation. Journal of Neuroscience Methods, 222:47 – 55, 2014.
[28] M. Fazel. Matrix Rank Minimization with Applications. PhD thesis, Stanford University, 2002.
[29] A. V. Fiacco and G. P. McCormick. Nonlinear Programming: Sequential Unconstrained Min-

imization Techniques. Wiley, New York, 1968. reprinted, SIAM, Philadelphia, 1990.

[30] J.-J. Fuchs. On sparse representations in arbitrary redundant bases. Information Theory, IEEE

Transactions on, 50(6):1341–1344, 2004.

[31] Michael Grant and Stephen Boyd. CVX: Matlab software for disciplined convex programming,

version 2.1. http://cvxr.com/cvx, March 2014.

[32] M. Grasmair, O. Scherzer, and M. Haltmeier. Necessary and suﬃcient conditions for lin-
ear convergence of l1-regularization. Communications on Pure and Applied Mathematics,
64(2):161–182, 2011.

[33] E. Hale, W. Yin, and Y. Zhang. Fixed-point continuation for (cid:96)1-minimization: methodology

and convergence. SIAM J. Optim., 19(3):1107–1130, 2008.

[34] W. L. Hare.

Identifying active manifolds in regularization problems.

In H. H. Bauschke,
R. S., Burachik, P. L. Combettes, V. Elser, D. R. Luke, and H. Wolkowicz, editors, Fixed-
Point Algorithms for Inverse Problems in Science and Engineering, volume 49 of Springer
Optimization and Its Applications, chapter 13. Springer, 2011.

[35] W. L. Hare and A. S. Lewis. Identifying active constraints via partial smoothness and prox-

regularity. J. Convex Anal., 11(2):251–266, 2004.

[36] J.-B. Hiriart-Urruty and H. Y. Le. Convexifying the set of matrices of bounded rank: appli-
cations to the quasiconvexiﬁcation and convexiﬁcation of the rank function. Optimization
Letters, 6(5):841–849, 2012.

[37] Hai Yen Le. Confexifying the Counting Function on Rp for Convexifying the Rank Function

on Mm,n(R). Journal of Convex Analysis, 19(2):519–524, 2012.

[38] C. Lemar´echal, F. Oustry, and C. Sagastiz´abal. The U -lagrangian of a convex function. Trans.

Amer. Math. Soc., 352(2):711–729, 2000.

[39] A. S. Lewis. Active sets, nonsmoothness, and sensitivity. SIAM J. Optim., 13(3):702–725,

[40] A. S. Lewis and S. Zhang. Partial smoothness, tilt stability, and generalized hessians. SIAM

[41] J. Liang. Convergence Rates of First-Order Operator Splitting Methods. Theses, Normandie

2002.

J. Optim., 23(1):74–94, 2013.

Universit´e, October 2016.

[42] J. Liang, J. Fadili, and G. Peyr´e. Activity identiﬁcation and local linear convergence of forward–

backward-type methods. SIAM J. Optim., 27(1):408–437, 2017.

[43] J. Liang, J. Fadili, and G. Peyr´e. Local convergence properties of douglas–rachford and alter-
nating direction method of multipliers. Journal of Optimization Theory and Applications,
172(3):874–913, 2017.

[44] P. L. Lions and B. Mercier. Splitting algorithms for the sum of two nonlinear operators. SIAM

Journal on Numerical Analysis, 16(6):964–979, 1979.

[45] S. G. Mallat. A wavelet tour of signal processing. Elsevier, third edition, 2009.
[46] S. A. Miller and J. Malick. Newton methods for nonsmooth convex minimization: connections
among-Lagrangian, Riemannian Newton and SQP methods. Mathematical programming,
104(2-3):609–633, 2005.

[47] B. S. Mordukhovich. Sensitivity analysis in nonsmooth optimization.

In D. A. Field and
V. Komkov, editors, Theoretical Aspects of Industrial Design, volume 58, pages 32–46.
SIAM Volumes in Applied Mathematics, 1992.

24

[48] Y. Nesterov. A method for solving the convex programming problem with convergence rate

O(1/k2). Dokl. Akad. Nauk SSSR, 269(3):543–547, 1983.

[49] N. Parikh and S. P. Boyd. Proximal algorithms. Foundations and Trends in Optimization,

1(3):123–231, 2013.

university press, 1970.

[50] R. T. Rockafellar. Convex analysis. Number 28 in Princeton Mathematical Series. Princeton

[51] R. T. Rockafellar and R. Wets. Variational analysis, volume 317. Springer, Berlin, 1998.
[52] O. Scherzer. Variational methods in imaging, volume 167. Springer, 2009.
[53] J.-L. Starck and F. Murtagh. Astronomical Image and Data Analysis. Springer, 2006.
[54] S. Tao, D. Boley, and S. Zhang. Local linear convergence of ISTA and FISTA on the LASSO

problem. arXiv preprint arXiv:1501.02888, 2015.

[55] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical

Society. Series B. Methodological, 58(1):267–288, 1996.

[56] S. Vaiter. Low Complexity Regularization of Inverse Problems. Theses, Universit´e Paris

Dauphine - Paris IX, July 2014.

[57] S. Vaiter, M. Golbabaee, J. Fadili, and G. Peyr´e. Model selection with low complexity priors.

Information and Inference: A Journal of the IMA, 4(3):230, 2015.

[58] S. Vaiter, G. Peyr´e, and J. Fadili. Low complexity regularization of linear inverse prob-
lems. In G¨otz Pfander, editor, Sampling Theory, a Renaissance, pages 103–153. Springer-
Birkh¨auser, 2015.

[59] S. Vaiter, G. Peyr´e, and J. Fadili. Model consistency of partly smooth regularizers. IEEE

Trans. Inf. Theory, 64(3):1725 – 1737, 2018.

[60] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables.

Journal of the Royal Statistical Society: Series B, 68(1):49–67, 2005.

[61] P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning

Research, 7:2541–2563, 2006.

25

