9
1
0
2
 
c
e
D
 
4
2
 
 
]

V
C
.
s
c
[
 
 
2
v
4
9
6
0
1
.
2
1
9
1
:
v
i
X
r
a

Oriented Objects as pairs of Middle Lines

Haoran Wei1,2,3, Lin Zhou1,2,3, Yue Zhang1,3, Hao Li1,3, Rongxin Guo1,2,3 and
Hongqi Wang1,3

1Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing
2 School of Electronic, Electrical and Communication Engineering,

100190, China.

3 Key
University of Chinese Academy of Sciences, Beijing 100190, China.
Laboratory of Network Information System Technology (NIST), Aerospace
Information Research Institute, Chinese Academy of Sciences, Beijing 100190, China.
weihaoran18@mails.ucas.ac.cn

Abstract. The detection of oriented objects is frequently appeared in
the ﬁeld of natural scene text detection as well as object detection in
aerial images. Traditional detectors for oriented objects are common to
rotate anchors on the basis of the RCNN frameworks, which will mul-
tiple the number of anchors with a variety of angles, coupled with ro-
tating NMS algorithm, the computational complexities of these models
are greatly increased. In this paper, we propose a novel model named
Oriented Objects Detection Network (O2-DNet) to detect oriented ob-
jects by predicting a pair of middle lines inside each target. O2-DNet is
an one-stage, anchor-free and NMS-free model. The target line segments
of our model are deﬁned as two corresponding middle lines of original
rotating bounding box annotations which can be transformed directly in-
stead of additional manual tagging. Experiments show that our O2-DNet
achieves excellent performance on ICDAR 2015 and DOTA datasets. It
is noteworthy that the objects in COCO can be regard as a special form
of oriented objects with an angle of 90 degrees. O2-DNet can still achieve
competitive results in these general natural object detection datasets.

Keywords: Oriented Objects, Object Detection, Middle Lines, Anchor-
Free

1

Introduction

When the object such as text in natural scene and aerial target(e.g., airplane,
ship, vehicle) appear in an image with a certain degree, the output form of
horizontal bounding box which usually used in the detection of natural object
no longer meet the detection requirement generally for it may include many
redundant pixels which belong to background actually. Moreover, when detecting
some objects which have a large aspect ratio and park densely, the cooperation
between horizontal bounding box and NMS is easy to cause missed detection as
shown in Fig. 1. In order to solve problems aforementioned, an oriented bounding
box output form has been proposed, and the following detection of oriented
objects has attracted more and more attention in recent years.

2

Haoran Wei, Lin Zhou, Yue Zhang, Hao Li, Rongxin Guo. et al.

Fig. 1. Horizontal bounding box vs. Oriented bounding box. Compared with Figure
(c), the bounding box in Figure(a) carries too much redundant information. The trucks
in Figure (b) and (d) have a large aspect ratio and park densely. When detecting them
with horizontal bounding box, NMS algorithm will lead to missed detection due to the
intersection-over-union (IOU) of two objects is too large. This problem can be solved
via introducing oriented bounding box like Figure (d).

Many of recent research achievements of oriented objects detection rely on
RCNN frameworks heavily. In the ﬁeld of text detection in natural scene im-
ages, R2CNN[1] adds a new branch to regress two points and the height of
oriented bounding box based on Faster R-CNN[2]. However, during the training
stage, horizontal anchors are still be suppressed by NMS within the RPN net-
work when the objects need to be detected have large aspect ratios and park
densely. RRPN[3] proposes rotation anchors to replace the horizontal ones and
the corresponding NMS is also replaced by a new NMS algorithm calculated by
rotating IOU. However, in order to cover objects of any angles, more anchors
with diﬀerent angles, aspect ratios and scales need to be set which increases the
computational complexity of the model. Moreover, the introduction of rotation
anchors in RRPN adds additional regression of angle information via Smooth
L1[4], but the combination of rotating IOU and Smooth L1 is not perfect be-
cause the angle has boundary problem, so the oriented bounding box output by
this algorithm is not accurate and often accompanied by angle jitter.

In the ﬁeld of aerial images, the detection of oriented object is more diﬃcult
campare with text detection in natural scene for the complex background as well
as the variation of spatial resolution of images. SCRDet[5] proposes an IOU Loss
to address the boundary problem for oriented bounding box mentioned in RRPN.
In addition, a Multi-Dimensional Attention Network is added to deal with the
complex background of aerial images. However, SCRDet is still anchor-based and
NMS-based and also faces some problems bring by them. For instance, in the
testing stage, 300 proposals are taken from 10000 regression boxes by NMS in
[5] but according to our statistics, for DOTA dataset, a crop image of 800 × 800
size can reach up to 2000+ objects, which will cause missed detection.

Recently, in view of the disadvantages of the anchor-based models, a number
of anchor-free algorithms[6,7,8,9] have emerged. CornerNet[6] and ExtremeNet[7]
can be divided into the process of detection and grouping of keypoints. While it

Oriented Objects as pairs of Middle Lines

3

Fig. 2. Views of O2-DNet. Figure (a1) shows that if the intersection point drift in the
drift region, the ﬁnal bounding box do not drift. If the angle of oriented object is 90,
they will be predicted by branch 1, and objects with other angles will be detected by
branch 2. In ICDAR 2015, the shape of bounding box of text is not always a rectangular,
so we set the vertical loss in Line Loss to 0 when detecting text in natural scene.

is not suitable to apply them into aerial images which may obtain many objects
due to the high time complexity of their grouping algorithm. CenterNet[9] puts
forward a new method of regressing the height and weight of object at the center
point. In order to achieve NMS-free, they obtain the center point in heatmap
through a method of searching 8-connect of each center point. However, this
method still needs to choose K top scoring objects which can not be applied
to single image with numerous targets. In addition, it may lead to the drift of
the center point, and further cause the bounding box to drift. Diﬀerent from the
anchor-free models based on keypoints detection above, FCOS[8] belongs to the
category of dense sampling, which regress in numerous pixels for one objects.
Because the bounding box of an object is regressed by a large number of pixels,
NMS is still needed to ﬁlter the redundant boxes in FCOS.

In this paper, we propose a novel anchor-free and NMS-free model named
O2-DNet as shown in Fig. 2 to detect oriented objects by a pair of middle
lines. Our model is a new form of anchor-free which combines the mothods of
keypoints detection and dense sampling. In order to reach the aim of NMS-free,

4

Haoran Wei, Lin Zhou, Yue Zhang, Hao Li, Rongxin Guo. et al.

we choose the method of keypoint detection to locate the intersection point
of each pair of median lines. For the problem of intersection point drift, we
design a drift region inspired by the method of dense sampling to ensure that
the intersection point drift in the drift region will not aﬀect the position of the
ﬁnal bounding box. In order to successfully predict the middle line, we design
a speciﬁc Line Loss according to the characteristics of lines(e.g., length, slope,
position) to regress each median line. The Line Loss consists of three parts:
the position loss to control the location of the endpoints of each middle line, the
parallel loss to control the two endpoints of each middle line and the intersection
point of two middle lines are collinear. The last one is vertical loss to control
the geometric relationship between two median lines of one object. There is an
order for the regression of endpoints of middle lines, so we wil also face the
boundary problem. In order to solve it, we design O2-DNet as two branches,
one for predicting horizontal objects with 90 degrees and the other for oriented
objects of other angles. The design of two branches also enables us to apply
O2-DNet to COCO[10] without changing any network structure.

Our contributions and innovations are as follows:
(1) We propose a noval anchor-free and NMS-free model named O2-DNet to

detect oriented objects by a pair of middle lines.

(2) Our O2-DNet is a new form of anchor-free which combines keypoints
detection and dense sampling, and we design drift region to relax the requirement
for accurate extracting keypoints.

(3) Our O2-DNet can detecting oriented objects and horizontal objects un-
der a single network without increasing computational complexities via two
branches. For the regression of middle lines, we design a special Line Loss.

The rest of this paper is organized as follows: In Section 2, we introduce
the related works done by researchers before and basic principle in our method.
The details of our network and algorithms are shown in Section 3. We place our
experiments results and analysis in Section 4. At last, our work is summarized
and concluded in Section 5.

2 Related Works

2.1 Detectors based on Manually Engineered Features

Traditional object detectors mainly depend on manually engineered features,
they ﬁrst select features like Histogram of Oriented Gradient (HOG)[11] gener-
ally and then input them to classiﬁer such as Support Vector Machine (SVM)[12]
to identify the existence of object. The generalization capability of these detec-
tors is limited by features extraction and the robustness of this type of detector
needs to be further improved.

2.2 Detectors based on DCNNs

In recent years, the success and development of deep convolution neural net-
works (DCNNs)[13,14] bring great progress to the ﬁeld of object detection.

Oriented Objects as pairs of Middle Lines

5

Compared with tradition detectors aforementioned, detectors based on DC-
NNs [2,6,7,8,9,15,16,17] can automatically extract features through the backbone
networks[18,19], and the accuracy as well as robustness of models is greatly
improved. There are two branches which are anchor-based and anchor-free in
DCNNs based detectors at present.

2.3 Anchor-Based vs. Anchor-Free Detectors

The concept of anchor was proposed in Region Proposals Networks (RPN)
of Faster R-CNN[2], which acts as extracting proposals and guiding the re-
gression task of networks. Subsequently, the anchor mechanism within RPN is
widely used in two-stage detectors[1,3,5,20]. For one-stage detectors which detect
objects[15,16,17,21] without RPN, YOLOv1[15] not use the anchor mechanism
can’t provide accuracy comparable to that of two-stage detectors. Afterwards,
anchor methods are also extensively utilized in one-stage detectors[16,17,21] to
improve the performance of models. In the detection of oriented objects, most
algorithms rely on anchor mechanism heavily. In general, these models output
the oriented bounding boxes by rotating anchors to regress an additional angle
information, and then obtain the ﬁnal bounding boxes by the ﬁltering of rotated
NMS algorithm.

The anchor mechanism promotes the development of object detection, but
it is still not perfect and also has some problems like mentioned in [6,7,8,9].
Recently, the research of anchor-free has become a hot topic in the ﬁeld of
object detection . At present, the anchor-free detectors can be roughly divided
into two categories. One is to locate objects through keypoints such as corner
points in CornerNet[6], extreme points in ExtremeNet[7] and center points in
CenterNet[9], the other is via the regression of a lot of points like FCOS[8] to
get the location of objects. For oriented objects, both of these two anchor-free
categories have defects. In the inference stage, they all need to keep K of the
highest scoring objects and may cause missed detection in the case of many
targets in a single image like small cars in aerial image. O2-DNet is an one stage
and anchor-free detector, which locates objects through a pair of median lines
and their intersection point. In order to solve the top K problem, we combine the
two categories of existing anchor-free alogrithm to design O2-DNet. The details
of our model will be explained in the next section.

3 O2-DNet

3.1 Overview

Fig. 3 illustrates the architecture of our method. O2-DNet locates per object by
detecting a pair of corresponding middle lines. We use 104-Hourgalss[6] as the
backbone of our model following the CornerNet [6] for its excellent performance
of extracting features. For an image of size w × h, as input, our model outputs
2×(1×2×C × w
d ) heatmaps to predict the intersection points of target middle

d × h

6

Haoran Wei, Lin Zhou, Yue Zhang, Hao Li, Rongxin Guo. et al.

Fig. 3. Architecture of O2-DNet. O2-DNet locates each object via a pair of median lines
and their intersection point. Each middle line is represented by two corresponding end-
points. Two branches are added to detect horizontal and oriented objects respectively.
O2-DNet outputs diﬀerent classes of objects to diﬀerent channels.

d × h

lines, and 2 × (2 × 4 × C × w
d ) regression maps to predict the corresponding
two middle lines, where C represents the number of classes in this image, the d
is output stride of down sampling module, and the ﬁrst 2 means two branches
of O2-DNet. The design of two branches is to deal with the angle boundary
problems of oriented objects via the independent prediction of objects with angle
of 90 degree. For the prediction of middle lines, we obtain them by regressing
their corresponding two endpoints. The form of regressing middle lines is inspired
by CenterNet[9], it is the relative position of endpoint from intersection point.
Moreover, we design special loss functions to control the relationship of endpoints
to ensure the predicted median lines more accurate. In addition, in order to
reduce the dependence of middle lines extraction on the precision of intersection
point extraction, we propose the point drift region to make O2-DNet output

Oriented Objects as pairs of Middle Lines

7

high-quality oriented bounding boxes when the extraction of intersection points
is not accurate enough.

3.2 Hourglass Networks

Hourglass Networks[22] was ﬁrst proposed for human keypoints detection. In
CornerNet[6], Law et al. modiﬁed hourglass network and introduced it into
the ﬁeld of object detection. We choose 104-Hourglass Networks modiﬁed in
CornerNet[6] as our backbone. For one image as input, HourglassNet regress C
channels of heatmaps with each pixel value ˇy ∈ (0, 1) which means the conﬁdence
of being judged as positive. Compared with CornerNet, O2-DNet deﬁnes each
keypoint with value setting to 1, instead of the Gaussian Kernel. In the stage of
inference, in order to avoid missed detection, instead of NMS and top K method
to extract keypoints in heatmap used in [6,7,8,9], we take a simple and rough
way to extract keypoints, which is ﬁnding the connected domains in heatmap,
and then deﬁne the center of each connected domain as the target keypoint. It
is true that this method is not accurate enough, but it can achieve satisfactory
results in the experiments through the collocation with intersection drift region
proposed in our model.

3.3 Middle Lines and Their Intersection Point

As shown in Fig. 4, let L1[(x1, y1), (x2, y2)] denote middle line 1 of the ob-
ject, (x1, y1) and (x2, y2) are the endpoint 1 and 2 of L1 respectively. Similarly,
L2[(x3, y3), (x4, y4)] is deﬁned as the middle line 2. For the ﬁrst branch of O2-
DNet, we deﬁne the horizontal median line as L1, and the vertical one as L2.
We regard the right endpoint and the top one as the endpoint 1 of L1 and L2
respectively. For the second branch, the longer one of the two median lines is de-
ﬁned as L1, the other is L2. Endpoints 1 are also deﬁned as the right one and the
top one in L1 and L2 in this branch. The intersection point (x, y) of two middle
lines can be obtained through simple operators ( x1+x2+x3+x4
) in
both two branches.

, y1+y2+y3+y4
4

4

Intersection Point We follow the modiﬁed focal loss[21] in CornerNet[6] to
predict the heatmap of intersection point of target middle lines. Because the
ground truth of our model with value setting to 1 instead of the Gaussian Kernel
like mentioned in Section 3.2, the loss in O2-DNet is a little diﬀerent compared
with CornerNet in form. We name the loss of intersection point in our model as
Lip:

Lip = −

(cid:40)

1
N

(cid:88)

xy

(1 − ´Yxy)α log ´Yxy,
log(1 − ´Yxy),
´Yxy

α

if Yxy = 1
if Yxy = 0

(1)

where α is a hyper-parameter and the value ﬁxed to 2 in our experiment. ´Yxy rep-
resents the pixel value at the coordinate (x, y) in heatmap and Yxy corresponds
to the ground truth. N is the number of objects.

8

Haoran Wei, Lin Zhou, Yue Zhang, Hao Li, Rongxin Guo. et al.

Fig. 4. The deﬁnition of two middle lines and the order of two endpoints of per middle
line. The ﬁrst line of ﬁgures represent objects with angles of 90 degree, and the second
line of ﬁgures represent oriented objects with another angles.

Fig. 5. The method of regressing middle lines in our model. The relative distance
between endpoint and intersection point is represented as ∆ xi, ∆ yi, where i is 1,2,3,4
meaning the four corresponding endpoints of middle lines.

Oriented Objects as pairs of Middle Lines

9

Middle Lines The method of regressing middle lines is to regress the relative
distance between each endpoint of per middle line with the intersection point.
As shown in Fig. 6, for middle line 1, we need to regress 4×H ×W maps, and the
values of these 4 maps in the position of intersection point are ∆x1, ∆y1, ∆x2
and ∆y2 respectively. The form of regression of middle line 2 is the same as
middle line 1. The loss to regress each middle line is as follows:

L1 =

1
N

2
(cid:88)

(cid:88)

ep=1

ˆxy

[SmoothL1(∆x∗

ˆxy, ∆x ˆxy) + SmoothL1(∆y∗

ˆxy, ∆y ˆxy)]ep

(2)

where N is the number of objects. ep denotes the endpoint of the correspond-
ing middle line. ˆxy means the coordinate of the regression map. ∆x∗ and ∆y∗
represent the ground truth.

The way of regressing each endpoint of per middle line independently may
result in two endpoints and the intersection point being not collinear. In order
to address this problem, we introduce a loss function as follows:

L2 =

1
N

2
(cid:88)

(cid:88)

l=1

ˆxy

[SmoothL1(∆xep1

ˆxy × ∆yep2

ˆxy , ∆xep2

ˆxy × ∆yep1

ˆxy )]l

(3)

where l means two middle lines of per object. ep1 and ep2 denote the endpoint
1 and 2 of each middle line.

There are two middle lines of one object, and they are vertical in space
generally. In order to control the relationship of two target middle lines, we
design L3 as follows:

L3 =

1
N

(cid:88)

ˆxy

[SmoothL1(∆xep1l1

ˆxy × ∆xep1l2

ˆxy + ∆yep1l1

ˆxy × ∆yep1l2

ˆxy

, 0)]l

(4)

where ep1l1 means endpoint 1 of middle line 1, ep1l2 means endpoint 1 of middle
line 2.

The L1, L2 and L3 make up the Line Loss of our model:

And the total loss of our model can be expressed as:

Ll = L1 + αL2 + βL3

Loss = Lip + γLl

where α, β and γ are weights of losses.

(5)

(6)

Drift Region The extraction of intersection point in heatmap will aﬀect the
accuracy of middle lines extraction. Inspired by FCOS[8], we set up circular drift

10

Haoran Wei, Lin Zhou, Yue Zhang, Hao Li, Rongxin Guo. et al.

Fig. 6. Illustration of drift region. Per pixel in drift region will regress the relative
distances from 4 endpoints of corresponding two middle lines.

regions in the center of objects according to the size of them. All pixel points in
drift region will regress the diﬀerent values according to their relative distances
from endpoints of middle lines. The drift region guarantees that the extraction of
intersection point from heatmap will not inﬂuence the position of ﬁnal oriented
bounding box. The radius of the drift region is set as follows:

R = min[r/stride, min(L1, L2)/(2 × stride)]

(7)

where stride is output stride of our model, L1 and L2 are the middle line 1 and
2 respectively. Where r is 16 in our model.

Unlike remote sensing images, objects in natural scenes sometimes have over-
laps to form fuzzy samples, we also follow FCOS to address this problem which
is when a pixel belongs to two targets at the same time, we regress the small
one.

4 Experiments

4.1 Datasets

In the stage of experiments, we select three datasets to verify the performance
of our model. These datasets involve in diﬀerent research ﬁelds: oriented objects
detection of aerial images, text detection in natural scene, the detection of objects
in nature images. Their detailed introductions are as follows:

Oriented Objects as pairs of Middle Lines

11

DOTA DOTA[23] is a common benchmark for the detection of objects in aerial
images. It includes two detection tasks: horizontal bounding boxes and oriented
bounding boxes, and we only use the oriented one in our experiments. There are
2806 aerial images with size ranges from 800 × 800 to 4000 × 4000 pixels total
in DOTA. These images are annotated using 15 categories (e.g., aircraft, small
car, ship). In practice, we divide each large image to crop images in 800 × 800
with overlap of 0.25.

ICDAR 2015 ICDAR 2015[24] is a dataset used for the detection of text in
natural scene. The training set and test set include 1000 and 500 images with
the size of 720 × 1280, respectively.

COCO The challenging MS COCO[10] dataset contains 80k images for training,
40k for validation and 20k for testing and includes 80 categories. The annotations
in COCO are horizontal bounding boxes which we used to test the generality of
our model.

4.2 Training and Testing Details

All our experiments are implemented on PyTorch 1.0[25] by two NVIDIA Tesla
V100 GPUs with 2 × 32 GB memories. For DOTA, we set the input resolution
800 × 800 to 511 × 511 and the output stride to 4 following settings in Cor-
nerNet[6] during the training stage. Adam[26] is selected as the optimizer for
O2-DNet. We train our model from scratch to 300k iterations with the batch
size setting to 32. The learning rate starts from 0.001 and 10 times lower for
every third iterations. Simple random horizontal and vertical ﬂipping as well as
color dithering are used to enhance the data in our experiments. The weights
of loss α, β and γ (Section 3.3) are setting to 1, 1 and 0.5 respectively during
training. For ICDAR 2015 and COCO, O2-DNet is ﬁnetuned on two v100 GPUs
for 200k iterations with a batch size of 32 from a pre-trained CornerNet model
which trained on 10 GPUs for 500k iterations. Other settings are the same as
DOTA. It is worth noting that for the two branches of our model, we do not
strictly divide them by 90 degrees, but by an angle range of (88, 92) degrees.

During test stage, as mentioned in Section 3.2, we need to transform the
heatmap into a binary image to extract the intersection point of two target
median lines, where the threshold is set to 0.3. When the angle of an object is
critical in two branches, it may have output in both two. We take the one with
the highest intersection point score as the ﬁnal output of O2-DNet.

4.3 Comparisons with State-of-the-art Frameworks

In this part, we ﬁrst prove the advancement of O2-DNet on the oriented objects
datasets (DOTA, ICDAR 2015). Then we test the strong generality of our model
on the dataset of natural objects (COCO).

12

Haoran Wei, Lin Zhou, Yue Zhang, Hao Li, Rongxin Guo. et al.

St

Pl

Sv

Br

Sp

Sh

Bc

Lv

Tc

Ha

Bd

Gft

Sbf Ra

Method
Two-stage models
R2CNN [1]
80.94 65.67 35.34 67.44 59.92 50.91 55.81 90.67 66.92 72.39 55.06 52.23 55.14 53.35 48.22 60.67
88.52 71.20 31.66 59.30 51.85 56.19 57.25 90.81 72.84 67.38 56.69 52.84 53.08 51.94 53.58 61.01
RRPN [3]
80.92 65.82 33.77 58.94 55.77 50.94 54.78 90.33 66.34 68.66 48.73 51.76 55.10 51.32 35.88 57.94
R-DFPN [27]
ICN [28]
81.40 74.30 47.70 70.30 64.90 67.80 70.00 90.80 79.10 78.20 53.60 62.90 67.00 64.20 50.20 68.20
RoI-Transformer [29] 88.64 78.52 43.44 75.92 68.81 73.68 83.59 90.74 77.27 81.46 58.39 53.54 62.83 58.93 47.67 69.56
SCRDet [5]
89.98 80.65 52.09 68.36 68.36 60.32 72.41 90.85 87.94 86.86 65.02 66.68 66.25 68.24 65.21 72.61
One-stage models
SSD [17]
YOLOv2 [15]
RetinaNet [21]
R3Det [30]
O2-DNet

39.83 9.09
0.64 13.18 0.26 0.39 1.11 16.24 27.57 9.23 27.16 9.09 3.03 1.05 1.01 10.59
39.57 20.29 36.58 23.42 8.85 2.09 4.82 44.34 38.35 34.65 16.02 37.62 47.23 25.5 7.45 21.39
88.92 67.67 33.55 56.83 66.11 73.28 75.24 90.87 73.95 75.07 43.77 56.72 51.05 55.86 21.46 62.02
89.54 81.99 48.46 62.52 70.48 74.29 77.54 90.80 81.39 83.54 61.97 59.82 65.44 67.46 60.05 71.69
89.31 82.14 47.33 61.21 71.32 74.03 78.62 90.76 82.23 81.36 60.93 60.17 58.21 66.98 61.03 71.04

He mAP

Table 1. State-of-the-art comparisons on DOTA. The abbreviations of the names are
deﬁned as: Pl: Plane, Bd: Baseball diamond, Br: Bridge, Gft: Ground ﬁeld track, Sv:
Small vehicle, Lv: Large vehicle, Sh:Ship, Tc: Tennis court, Bc: Basketball court, St:
Storage tank, Sbf: Soccer-ball ﬁeld, Ra: Roundabout, Ha: Harbor, Sp: Swimming pool,
and He: Helicopter. The SSD, YOLOv2 and RetinaNet are modiﬁed by us to output
oriented bounding boxes.

DOTA As shown in Table 1, our O2DNet achieve 71.04% mAP on DOTA
dataset, better than most two-stage and one-stage models used in the detection
of aerial objects at present. For bridges with large aspect ratio and dense parked
small vehicles, our anchor-free model achieves the most advanced accuracy on AP
due to the better adaptive feature extraction ability than anchor-based models.

ICDAR 2015 For ICDAR 2015 dataset, most of the annotation of objects is
not in the form of rectangle, but in the form of irregular quadrilateral which is
close to parallelogram. We shield the L3 of the Line Loss, which is used to control
the two target middle line to remain vertical. As shown in Table 2, our O2-DNet
achieve 82.97% F1, better than other models we choose for comparison. The
experimental results show that our model can be used not only in the detection
of aerial images but also in natural scene text detection.

Recall Precision F1 Method
Method
51.56 74.22
CTPN [31]
R2CNN [1]
79.68 85.62
FOTS RT [34] 85.95 79.83
O2-DNet
80.52 85.58

60.85 SegLink [32] 76.80 73.10
78.33 83.27
82.54 EAST [33]
82.17 73.23
82.78 RRPN [3]
82.97

Recall Precision F1

75.00
80.72
77.44

Table 2. Comparison with diﬀerent methods on the ICDAR2015 dataset. Our model
achieve 82.97% F1, better than other models in this table.

Oriented Objects as pairs of Middle Lines

13

COCO In order to verify the general performance of our model, we also make
experiments on the COCO dataset of natural scene object detection. For COCO
with objects labeled in horizontal bounding boxes, our model will only have
output in the ﬁrst branch. As shown in Table 3, our O2-DNet achieve 41.3% AP
on COCO dataset, leading most one-stage detectors.

Method

Backbone

AP AP50 AP75 APS APM APL

Two-stage detectors

Faster R-CNN[35]
Deformable-CNN[36] Inception-ResNet 37.5 58.0
Mask R-CNN[37]
Cascade R-CNN[20]
D-RFCN + SNIP[38]
PANet[39]

ResNet-101-FPN 36.2 59.1 39.0 18.2 39.0 48.2
19.4 40.1 52.5
39.8 62.3 43.4 22.1 43.2 51.2
42.8 62.1 46.3 23.7 45.5 55.2
45.7 67.3 51.1 29.3 48.8 57.1
47.4 67.2 51.8 30.1 51.7 60.0

ResNeXt-101
ResNet-101
DPN-98
ResNeXt-101

-

One-stage detectors

YOLOv2[16]
SSD[17]
RetinaNet[21]
ReﬁneDet[40]
CornerNet[6]
ExtremeNet[7]
CenterNet[9]
FCOS[8]
O2-DNet

21.6 44.0 19.2 5.0 22.4 35.5
DarkNet-19
31.2 50.4 33.3 10.2 34.5 49.8
ResNet-101
39.1 59.1 42.3 21.8 42.7 50.2
ResNet-101
36.4 57.5 39.5 16.6 39.9 51.4
ResNet-101
40.5 56.5 43.1 19.4 42.7 53.9
104-Hourglass
104-Hourglass
40.2 55.5 43.2 20.4 43.2 53.1
104-Hourglass 42.1 61.1 45.9 24.1 45.5 52.8
ResNet-101-FPN 41.0 60.7 44.1 24.0 44.1 51.0
41.3 60.9 45.2 24.2 44.5 52.3

104-Hourglass

Table 3. State-of-the-art comparison on COCO test-dev. It shows that our O2-DNet
has a strong competitiveness in natural scene object detection. In this table, all data
are from single scale detection results.

4.4 Ablation Studies

In this part, we conduct three ablation experiments on the DOTA dataset, which
are the inﬂuence of diﬀerent backbones on the performance of our model, the
inﬂuence of Line Loss on the performance of our model, and the inﬂuence of
single branch on the performance of our model. Table 4 shows all experimental
data. The following is the speciﬁc analysis:

14

Haoran Wei, Lin Zhou, Yue Zhang, Hao Li, Rongxin Guo. et al.

Pl

Bd Br Gft

Method
Sp He mAP
ResNet101-FPN[35] 88.07 81.43 47.03 60.51 68.42 70.96 73.72 90.01 81.15 78.24 59.83 57.52 56.01 62.63 58.37 68.93
Without Line Loss 87.25 82.12 47.04 60.14 67.21 70.01 71.38 90.26 79.89 81.09 58.43 59.12 56.05 66.82 60.06 69.12
86.17 81.07 47.06 60.28 66.28 72.44 69.09 88.90 81.29 78.77 59.12 57.97 58.14 64.69 59.98 68.81
Single branch
O2-DNet
89.31 82.14 47.33 61.21 71.32 74.03 78.62 90.76 82.23 81.36 60.93 60.17 58.21 66.98 61.03 71.04

Sbf Ra Ha

Sh Tc

Bc

Lv

Sv

St

Table 4. Ablation Studies on DOTA. ResNet101-FPN means that the backbone of
our model is replaced by ResNet101-FPN. Without Line Loss denotes that we only
keep the L1 part of the total Line Loss. Single branch means that we only keep the
second branch of O2-DNet.

ResNet101-FPN We replace the backbone 104-Hourglass with ResNet101-
FPN[35] to verify the eﬀect of our model with diﬀerent backbones. As shown
in Table 4, our model also achieve satisfactory 68.93% mAP in matching with
ResNet101-FPN, which means that the performance of O2-DNet does not depend
entirely on 104-Hourglass.

Without Line Loss In order to prove the validity of Line Loss, we shield the
L2, L3 part of Line Loss, and keep the other settings of O2-DNet. Table 4 shows
that our model with Line Loss improves 1.92% mAP compared with the model
without Line Loss. The Line Loss eﬀectively controls the line segment property
of the regression target median lines in our model. The eﬀect of Line Loss is
shown in Fig. 7.

Fig. 7. Eﬀects of Line Loss. Figure (a) shows the output of our model with Line Loss
and Figure (b) shows the result of our model without Line Loss.

Single branch In order to verify that the two branches of O2-DNet can solve
the boundary problem better, we cut oﬀ the ﬁrst branch and input the 90 degree
object into the second branch in the form of the original ground truth deﬁned
in Section 3.3. The experimental results show that the mAP of two branches is
2.23% higher than that of single branch. The design of two branches is signiﬁcant
for our model.

Oriented Objects as pairs of Middle Lines

15

Fig. 8. Qualitative results output by O2-DNet.

5 Conclusion

We propose a novel one-stage and anchor-free model named O2-DNet to detect
oriented objects. O2-DNet locates each object by predicting a pair of middle
lines inside them. As a result, our model is competitive compared with state-

16

Haoran Wei, Lin Zhou, Yue Zhang, Hao Li, Rongxin Guo. et al.

of-the-art detectors in several ﬁelds: oriented objects detection of aerial images,
text detection in natural scene, the detection of objects in nature images.

References

1. Jiang, Y., Zhu, X., Wang, X., Yang, S., Li, W., Wang, H., Fu, P., Luo, Z.: R2cnn:
Rotational region cnn for orientation robust scene text detection. arXiv preprint
arXiv:1706.09579 (2017)

2. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detec-
tion with region proposal networks. In: Advances in neural information processing
systems. (2015) 91–99

3. Ma, J., Shao, W., Ye, H., Wang, L., Wang, H., Zheng, Y., Xue, X.: Arbitrary-
oriented scene text detection via rotation proposals. IEEE Transactions on Multi-
media 20(11) (2018) 3111–3122

4. Girshick, R.: Fast r-cnn. In: Proceedings of the IEEE international conference on

computer vision. (2015) 1440–1448

5. Yang, X., Yang, J., Yan, J., Zhang, Y., Zhang, T., Guo, Z., Sun, X., Fu, K.:
Scrdet: Towards more robust detection for small, cluttered and rotated objects.
In: Proceedings of the IEEE International Conference on Computer Vision. (2019)
8232–8241

6. Law, H., Deng, J.: Cornernet: Detecting objects as paired keypoints. In: Proceed-
ings of the European Conference on Computer Vision (ECCV). (2018) 734–750
7. Zhou, X., Zhuo, J., Krahenbuhl, P.: Bottom-up object detection by grouping

extreme and center points. (2019) 850–859

8. Tian, Z., Shen, C., Chen, H., He, T.: Fcos: Fully convolutional one-stage object

detection. arXiv preprint arXiv:1904.01355 (2019)

9. Zhou, X., Wang, D., Kr¨ahenb¨uhl, P.: Objects as points.

arXiv preprint

arXiv:1904.07850 (2019)

10. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: European conference
on computer vision, Springer (2014) 740–755

11. Dalal, N., Triggs, B.: Histograms of oriented gradients for human detection. In:
international Conference on computer vision & Pattern Recognition (CVPR’05).
Volume 1., IEEE Computer Society (2005) 886–893

12. Cortes, C., Vapnik, V.: Support-vector networks. Machine learning 20(3) (1995)

273–297

13. LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P., et al.: Gradient-based learning
applied to document recognition. Proceedings of the IEEE 86(11) (1998) 2278–
2324

14. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-
volutional neural networks. In: Advances in neural information processing systems.
(2012) 1097–1105

15. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Uniﬁed,
real-time object detection. In: Proceedings of the IEEE conference on computer
vision and pattern recognition. (2016) 779–788

16. Redmon, J., Farhadi, A.: Yolo9000: better, faster, stronger. In: Proceedings of the
IEEE conference on computer vision and pattern recognition. (2017) 7263–7271
17. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.:
Ssd: Single shot multibox detector. In: European conference on computer vision,
Springer (2016) 21–37

Oriented Objects as pairs of Middle Lines

17

18. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556 (2014)

19. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: Proceedings of the IEEE conference on computer vision and pattern recognition.
(2016) 770–778

20. Cai, Z., Vasconcelos, N.: Cascade r-cnn: Delving into high quality object detection.
In: Proceedings of the IEEE conference on computer vision and pattern recognition.
(2018) 6154–6162

21. Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll´ar, P.: Focal loss for dense object
detection. In: Proceedings of the IEEE international conference on computer vision.
(2017) 2980–2988

22. Newell, A., Yang, K., Deng, J.: Stacked hourglass networks for human pose esti-

mation. In: European Conference on Computer Vision, Springer (2016) 483–499

23. Xia, G.S., Bai, X., Ding, J., Zhu, Z., Belongie, S., Luo, J., Datcu, M., Pelillo,
M., Zhang, L.: Dota: A large-scale dataset for object detection in aerial images.
In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
(June 2018)

24. Karatzas, D., Gomez-Bigorda, L., Nicolaou, A., Ghosh, S., Bagdanov, A., Iwamura,
M., Matas, J., Neumann, L., Chandrasekhar, V.R., Lu, S., et al.: Icdar 2015 com-
petition on robust reading. In: 2015 13th International Conference on Document
Analysis and Recognition (ICDAR), IEEE (2015) 1156–1160

25. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
Desmaison, A., Antiga, L., Lerer, A.: Automatic diﬀerentiation in pytorch. (2017)
26. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980 (2014)

27. Yang, X., Sun, H., Fu, K., Yang, J., Sun, X., Yan, M., Guo, Z.: Automatic ship
detection in remote sensing images from google earth of complex scenes based on
multiscale rotation dense feature pyramid networks. Remote Sensing 10(1) (2018)
132

28. Azimi, S.M., Vig, E., Bahmanyar, R., K¨orner, M., Reinartz, P.: Towards multi-
class object detection in unconstrained remote sensing imagery. arXiv preprint
arXiv:1807.02700 (2018)

29. Jian Ding, Nan Xue, Y.L.G.S.X.Q.L.: Learning roi transformer for detecting ori-
ented objects in aerial images. In: The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR). (June 2019)

30. Yang, X., Liu, Q., Yan, J., Li, A.: R3det: Reﬁned single-stage detector with feature

reﬁnement for rotating object. arXiv preprint arXiv:1908.05612 (2019)

31. Tian, Z., Huang, W., He, T., He, P., Qiao, Y.: Detecting text in natural image
with connectionist text proposal network. In: European conference on computer
vision, Springer (2016) 56–72

32. Shi, B., Bai, X., Belongie, S.: Detecting oriented text in natural images by linking

segments. (2017)

33. Zhou, X., Yao, C., Wen, H., Wang, Y., Zhou, S., He, W., Liang, J.: East: An
eﬃcient and accurate scene text detector. In: Proceedings of the IEEE conference
on computer vision and pattern recognition. (2017)

34. Liu, X., Liang, D., Yan, S., Chen, D., Qiao, Y., Yan, J.: Fots: Fast oriented
text spotting with a uniﬁed network. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. (2018) 5676–5685

35. Lin, T.Y., Doll´ar, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature
pyramid networks for object detection. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. (2017) 2117–2125

18

Haoran Wei, Lin Zhou, Yue Zhang, Hao Li, Rongxin Guo. et al.

36. Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.: Deformable convolu-
tional networks. In: Proceedings of the IEEE international conference on computer
vision. (2017) 764–773

37. He, K., Gkioxari, G., Doll´ar, P., Girshick, R.: Mask r-cnn. In: Proceedings of the

IEEE international conference on computer vision. (2017) 2961–2969

38. Singh, B., Davis, L.S.: An analysis of scale invariance in object detection snip. In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
(2018) 3578–3587

39. Liu, S., Qi, L., Qin, H., Shi, J., Jia, J.: Path aggregation network for instance
segmentation. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. (2018) 8759–8768

40. Zhang, S., Wen, L., Bian, X., Lei, Z., Li, S.Z.: Single-shot reﬁnement neural network
for object detection. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. (2018) 4203–4212

