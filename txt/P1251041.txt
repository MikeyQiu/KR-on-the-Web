Robust Minutiae Extractor:
Integrating Deep Networks and Fingerprint Domain Knowledge

Dinh-Luan Nguyen, Kai Cao and Anil K. Jain
Michigan State University
East Lansing, Michigan, USA
nguye590@msu.edu, {kaicao,jain}@cse.msu.edu

7
1
0
2
 
c
e
D
 
6
2
 
 
]

V
C
.
s
c
[
 
 
1
v
1
0
4
9
0
.
2
1
7
1
:
v
i
X
r
a

Abstract

We propose a fully automatic minutiae extractor, called
MinutiaeNet, based on deep neural networks with com-
pact feature representation for fast comparison of minu-
tiae sets. Speciﬁcally, ﬁrst a network, called CoarseNet,
estimates the minutiae score map and minutiae orientation
based on convolutional neural network and ﬁngerprint do-
main knowledge (enhanced image, orientation ﬁeld, and
segmentation map). Subsequently, another network, called
FineNet, reﬁnes the candidate minutiae locations based on
score map. We demonstrate the effectiveness of using the
ﬁngerprint domain knowledge together with the deep net-
works. Experimental results on both latent (NIST SD27)
and plain (FVC 2004) public domain ﬁngerprint datasets
provide comprehensive empirical support for the merits of
our method. Further, our method ﬁnds minutiae sets that
are better in terms of precision and recall in comparison
with state-of-the-art on these two datasets. Given the lack of
annotated ﬁngerprint datasets with minutiae ground truth,
the proposed approach to robust minutiae detection will
be useful to train network-based ﬁngerprint matching al-
gorithms as well as for evaluating ﬁngerprint individual-
ity at scale. MinutiaeNet is implemented in Tensorﬂow:
https://github.com/luannd/MinutiaeNet

Figure 1. Minutiae detection by the proposed approach on two
latent ﬁngerprint images (#7 and #39) from the NIST SD27
dataset [6]. Left column: minutiae score maps obtained from the
latent images shown in the right column. Right column: minu-
tiae detected by the proposed framework (red) and ground truth
minutiae (blue) overlaid on the latent image.

1. Introduction

Automatic ﬁngerprint recognition is one of the most
the past 50
widely studied topic in biometrics over
years [12]. One of the main challenges in ﬁngerprint recog-
nition is to increase the recognition accuracy, especially
for latent ﬁngerprints. Fingerprint comparison is primar-
ily based on minutiae set comparison [28, 11]. A num-
ber of hand-crafted approaches [10, 28] have been used to
augment the minutiae with their attributes to improve the
recognition accuracy. However, robust automatic ﬁnger-
print minutiae extraction, particularly for noisy ﬁngerprint

images, continues to be a bottleneck in ﬁngerprint recogni-
tion systems.

With rapid developments and success of deep learning
techniques in a variety of applications in computer vision
and pattern recognition [8, 20], we are beginning to see
network-based approaches being proposed for ﬁngerprint
recognition. Still, the prevailing methods of minutiae ex-
traction primarily utilize ﬁngerprint domain knowledge and
handcrafted features. Typically, minutiae extraction and
matching involves pre-processing stages such as ridge ex-
traction and ridge thinning, followed by minutiae extrac-
tion [10, 4] and ﬁnally heuristics to deﬁne minutiae at-

Table 1. Published network-based approaches for automatic minutiae extraction.

Study

Sankaran et al.
[18]
Jiang et al.
[13]
Tang et al.
[21]
Darlow et al.
[3]
Tang et al.
[22]

Method

Training Data

Testing Data

Comments

Performance Evaluation

Sparse autoencoders
for classiﬁcation
A combination of
JudgeNet and LocateNet
Fully convolutional
neural network
Convolutional network
classiﬁer
Uniﬁed network with
domain knowledge

132, 560 plain ﬁngerprint images;
129 images from NIST SD27
200 live scan ﬁngerprints

4, 205 latent images from private
database and 129 latents from NIST SD27
6, 336 images from
FVC 2000, 2002, and 2004
8, 000 images from private
forensic latent database

129 remaining latents
from NIST SD27
100 images from
private database
129 latents
from NIST SD27
1, 584 images from
FVC 2000, 2002, and 2004
Set A of FVC 2004 and
NIST SD27

Sliding window; manual segmentation
of latent ﬁngerprints
Sliding window; hand-crafted dividing
regions; no minutiae orientation information
Hard thresholds to cut off candidate regions;
plain network
Sliding window; hard threshold for candidate regions
(minutiae); separately estimated minutiae orientation
Plain network; depends largely on the quality of
the enhancement and segmentation stages

Patch-based and minutia-based,
metric and matching performance(*)
Precision, recall,
and F1 score
Precision, recall, F1 score,
and matching performance
Equal error rate and
matching performance
Precision, recall, and
matching performance

Proposed
approach

Domain knowledge with Residual
learning based CoarseNet and
inception-resnet based FineNet

Precision, recall, and F1
score under different location
and orientation thresholds
(*) different matchers were used in different studies and none of them were state of the art, i.e. top performing latent or slap matchers identiﬁed in the

Residual network; automatic minutiae extractor
utilizing domain knowledge; robust patch based
minutiae classiﬁer

FVC 2002 with data augmentation
(8, 000 images in total)

FVC 2004 (3, 200 images) and
NIST SD27 (all 258 latents)

NIST evaluations. For this reason, we do not report matching performance because otherwise it would not be a fair comparison with previous studies.

tributes. While such an approach works well for good qual-
ity ﬁngerprint images, it provides inaccurate minutiae loca-
tion and orientation for poor quality rolled/plain prints and,
particularly for latent ﬁngerprints. To overcome the noise in
ﬁngerprint images, Yoon et al. [27] used Gabor ﬁltering to
calculate the reliability of extracted minutiae. Although this
approach can work better than [10], it also resulted in poor
results with highly noisy images. Because these prevailing
approaches are based on handcrafted methods or heuristics,
they are only able to extract basic (or low level) features1 of
images. We believe learning based approaches using deep
networks will have better ability to extract high level fea-
tures2 from low quality ﬁngerprint images.

In this paper, we present a novel framework that ex-
ploits useful domain knowledge coded in the deep neural
networks to overcome limitations of existing approaches to
minutiae extraction. Figure 1 visualizes results of the pro-
posed framework on two latent ﬁngerprints from the NIST
SD27 dataset.

Speciﬁcally, our proposed approach comprises of two

networks, called CoarseNet and FineNet:

- CoarseNet is a residual learning [8] based convolu-
tional neural network that takes a ﬁngerprint image as initial
input, and the corresponding enhanced image, segmentation
map, and orientation ﬁeld (computed by the early stages
of CoarseNet) as secondary input to generate the minutiae
score map. The minutiae orientation is also estimated by
comparing with the ﬁngerprint orientation.

- FineNet is a robust inception-resnet [20] based minu-
tiae classiﬁer. It processes each candidate patch, a square
region whose center is the candidate minutiae point, to re-
ﬁne the minutiae score map and approximate minutiae ori-
entation by regression. Final minutiae are the classiﬁcation
results.

Deep learning approach has been used by other re-
searchers for minutiae extraction (see Table 1). But,
our approach differs from published methods in the way
we encode ﬁngerprint domain knowledge in deep learn-

1Features such as edges, corners, etc.
2Abstract/semantic features retrieved from deep layers.

ing. Sankaran et al. [18] classiﬁed the minutiae and non-
minutiae patches by using sparse autoencoders. Jiang et
al. [13] introduced a combination of two networks: Jud-
geNet for classifying minutiae patches, and LocateNet for
locating precise minutiae location. While Jiang et al. use
neural networks, their approach is very time-consuming due
to use of sliding window to extract minutiae candidates. An-
other limitation of this approach is that it does not provide
minutiae orientation information.

Tang et al. [21] utilized the idea of object detection to
detect candidate minutiae patches, but it suffers from two
major weaknesses: (i) hard threshold to delete the candi-
date patches, and (ii) the same network is used for both
candidate generation and classiﬁcation. By using sliding
windows, Darlow et al. [3] fed each pixel of the input ﬁn-
gerprint to a convolutional neural network, called MENet,
to classify whether it corresponds to a minutia or not. It also
suffers from time-consuming sliding windows as in [13],
and separate modules for minutiae location and orientation
estimates. Tang et al. [22] proposed FingerNet that maps
traditional minutiae extraction pipeline including orienta-
tion estimation, segmentation, enhancement, and extraction
to a network with ﬁxed weights. Although this approach
is promising because it combines domain knowledge and
deep network, it still uses plain 3 network architecture and
hard threshold in non-maximum suppression 4. Finally, the
accuracy of FingerNet depends largely on the quality of the
enhanced and segmentation stage while ignoring texture in-
formation in the ridge pattern.

In summary, the published approaches suffer from using
sliding windows to process each pixel in input images, set-
ting hard threshold in post-processing step, and using plain
convolutional neural network to classify candidate regions.
Furthermore, the evaluation process in these studies is not
consistent in terms of deﬁning “correct” minutiae.

The contributions of our approach are as follows:
• A network-based automatic minutiae extractor utiliz-
ing domain knowledge is proposed to provide reli-

3A series of stacked layers.
4A post-processing algorithm that merges all detections belonging to

the same object.

Figure 2. Proposed automatic minutiae extraction architecture. While CoarseNet takes full ﬁngerprint image as input, FineNet processes
minutiae proposed patches output by CoarseNet.

able minutiae location and orientation without a hard
threshold or ﬁne tuning.

• A robust patch based minutiae classiﬁer that signif-
icantly boosts the precision and recall of candidate
patches. This can be used as a robust minutiae extrac-
tor with compact embedding of minutiae features.

• A non-maximum suppression is proposed to get precise
locations for candidate patches. Experimental evalu-
ations on FVC 2004 [15] and NIST SD27 [6] show
that the proposed approach is superior to published ap-
proaches in terms of precision, recall, and F1 score val-
ues.

2. Proposed framework

Our minutiae extraction framework has two modules:
(i) residual learning based convolutional neural network,
called CoarseNet that generates candidate patches contain-
ing minutiae from input ﬁngerprint image; (ii) inception-
resnet based network architecture, called FineNet which
is a strong minutiae classiﬁer that classiﬁes the candidate
patches output by CoarseNet. These two networks also pro-
vide minutiae location and orientation information as out-
puts. Figure 2 describes the complete network architecture
for automatic minutiae location and orientation for an in-
put ﬁngerprint image. Section 2.1 presents the architecture
of CoarseNet. In Section 2.2, we introduce FineNet with
details on training to make it a strong classiﬁer.

2.1. CoarseNet for minutiae extraction

We adopt the idea of combining domain knowledge and
deep representation of neural networks in [22] to boost the
minutiae detection accuracy. In essence, we utilize the au-
tomatically extracted segmentation map, enhanced image,
and orientation map as complementary information to the
input ﬁngerprint image. The goal of CoarseNet is not to

produce the segmentation map or enhanced image or orien-
tation map. They are just the byproducts of the network.
However, these byproducts as ﬁngerprint domain knowl-
edge must be reliable to get robust minutiae score map. Be-
cause Tang et al. [22] proposed an end-to-end uniﬁed net-
work that maps handcrafted features to network based ar-
chitecture, we use this as a baseline for our CoarseNet.

2.1.1 Segmentation and orientation feature sharing

Adding more layers in the deep network with the hope of
increasing accuracy might lead to the exploding or vanish-
ing gradients problem. From the success of residual learn-
ing [8], we use residual instead of just plain stacked con-
volutional layers in our network to make it more powerful.
Figure 3 shows the detailed architecture of the network.

Unlike existing works using plain convolutional neural
network [21, 22] or sliding window [18, 3] to process each
patch with ﬁxed size and stride, we use a deeper residual
learning based network with more pooling layers to scale
down the region patch. Speciﬁcally, we get the output after
the 2nd, 3rd, and 4th pooling layer to feed to an ASPP net-
work [2] with corresponding rates for multiscale segmen-
tation. This ensures the output has the same size as input
without a loss of information when upsampling the score
map.

By using four pooling layers, each pixel in the jth feature
map, called level j, corresponds to a region 2j × 2j in the
original input. Result layers at level 4 and 3 will be tested as
coarse estimates while the level 2 serves as ﬁne estimation.
Segmentation map. Image segmentation and ﬁngerprint
orientation estimation share the same convolutional layers.
Thus, by applying multi-level approach mentioned above,
we get probability maps of each level-corresponding region
in input image. For instance, to get ﬁner-detailed segmen-
tation for each region level jl, we continue to process prob-
ability map of region level jl/2.

Figure 3. CoarseNet architecture.

Orientation map. To get complete minutiae informa-
tion from context, we adopt the fusion idea of Cao et al. [1].
We fuse the results of Dictionary-based method [26] with
our orientation results from CoarseNet. Because [26] uses a
hand-crafted approach, we set the fusion weight ratio of its
output with our network-based approach as 1:3.

2.1.2 Candidate generation

The input ﬁngerprint image might contain large amounts of
noise. So, without using domain knowledge we may not be
able to identify prominent ﬁngerprint features. The domain
knowledge comprises of four things: raw input image, en-
hanced image, orientation map, and segmentation map. In
the Gabor image enhancement module, we take the average
of ﬁltered image and the orientation map for ridge ﬂow es-
timation. To emphasize texture information in ﬁngerprints,
we stack the original input image with the output of en-
hancement module to obtain the ﬁnal enhancement map. To
remove spurious noises, we apply segmentation map on the
enhancement map and use it as input to coarse minutiae ex-
tractor module.

To obtain the precise location of minutiae, each level of
residual net is fused to get the ﬁnal minutiae score map with
size h/16 × w/16, where h and w are the height and width
of the input image. Figure 4 shows the details of processing

score map. To reduce the processing time, we use score
map at level 4 as a coarse location. To get precise location,
lower level score maps are used.

2.1.3 Non-maximum suppression

Using non-maximum suppression to reduce the number of
candidates is common in object detection [7, 17]. Some of
the candidate regions are deleted to get a reliable minutiae
score map by setting a hard threshold [3] or using heuris-
tics [21, 22]. However, a hard threshold can also suppress
valid minutiae locations. A commonly used heuristics is to
sort the candidate scores in ascending order. The L2 dis-
tance between pairwise candidates is calculated with hard
thresholds for distance and orientation. By iteratively com-
paring each candidate with the rest in the candidate list, only
the candidate with higher score and score above the thresh-
olds is kept. However, this approach fails when two minu-
tiae are near each other and the inter-minutiae distance is
below the hard thresholds.

Since each score in the minutiae map corresponds to a
speciﬁc region in the input image, we propose to use the
intersection over union strategy. Speciﬁcally, after sorting
the scores of the candidate list, we keep high score candi-
dates while ignoring the lower scoring candidates with at
least 50% overlap with the candidates already selected.

2.1.4 Training data for CoarseNet

Given the lack of datasets with ground truth, we use the ap-
proach in Tang et al. [22] to generate weak labels for train-
ing the segmentation and orientation module. The coarse
minutiae extractor module uses minutiae location and minu-
tiae orientation ground truth provided in the two datasets.
We also use data augmentation techniques as mentioned in
Section 3.

2.2. FineNet

Figure 4. Candidate patch processing map. Lower the level of
score map, more detail it provides.

Extracting minutiae based on candidate patches is not
adequate. Although CoarseNet is reliable, it still fails to de-

tect true minutiae or detects spurious minutiae. This can
lead to poor performance in ﬁngerprint matching. This mo-
tivates our use of FineNet - a minutiae classiﬁer from gener-
ated candidate patches. FineNet takes candidates from the
output of CoarseNet as input to decide whether the region
10 × 10 in the center of the corresponding patch has a valid
minutia or not.

2.2.1 FineNet architecture

Figure 5 describes the architecture of FineNet. As men-
tioned in Section 2.1.1, we use the Inception-Resnet v1 ar-
chitecture as a core network in FineNet.

For FineNet training, we extract an equal number of
t1 × t1 sized minutiae and non-minutiae patches with t1 =
45. FineNet determines the whether the 10 × 10 pixel re-
gion in the center of each patch contains a valid minutia
or not. The candidate patches are resized into t2 × t2 pix-
els that feed to FineNet. Based on the observation that the
original input image size (without rescaling) is not large in
comparison with images for object classiﬁcation, too much
up scaling the image can cause blurring the tiny details, and
too small an input image size is not sufﬁcient for network
with complex architecture, we choose t2 = 160 pixels.

Training data for FineNet is extracted from the input gray
scale images where minutiae data are based on the ground
truth minutiae location and non-minutiae ones are from ran-

Figure 5. FineNet architecture. For details of the Inception-Resnet
v1 arichitecture block, we refer the readers to [20].

dom sampling with the center regions do not contain partial
or fully minutiae location. To make the network more ro-
bust, we use some small techniques like Batch Normaliza-
tion [9], rotation, ﬂipping, scaled augmentation [19], and
bounding blurring [13] as pre-processing stage.

2.2.2 Losses and implementation details

Intra-Inter class losses. Because input captured ﬁnger-
print image is not always in the ideal condition, it might
be easily affected by distortion, ﬁnger movement, or quality
(wet/dry) of ﬁnger. Thus, the variation of minutiae shapes
and surrounding ridges can affect the accuracy of the clas-
siﬁer. To handle this situation and make the FineNet more
robust with intra-class variations, we use Center Loss [25]
as a complementary of softmax loss and minutiae orienta-
tion loss. While softmax loss tends to push away features
of different classes, center loss tends to pull features in the
same class closer. Let L, LC, LS , LO be the total loss, cen-
ter loss, sofmax loss, and orientation loss, the total loss for
training is calculated as follows:

L = αLC + (1 − α)LS + βLO

(1)

where we set α = 0.5 to balance between intra-class (cen-
ter) and inter-class (sofmax) loss and β = 2 to emphasize
the importance of precision of minutiae orientation.

Parameter settings. As mentioned in Section 2.2.1, ﬁn-
gerprint patches are input to FineNet where the patch size
is 160 × 160. To ensure our network can handle distortion
in input image, we apply scaled augmentation [19], random
cropping, and brightness adjustment. Horizontal and verti-
cal ﬂip with pixel mean subtraction are also adopted. We
randomly initialize all variables by using a Gaussian distri-
bution N (0, 0.01). Batch size is set to 100. We use schedule
learning rate after particular iterations. Speciﬁcally, we set
it as 0.01 in the beginning and reduce 10 times after 50K
iterations. To prevent vanishing gradient problem, we set
the maximum epoch to 200K. We use 0.0004 as the value
for momentum and weight decay is 0.9.
3. Experimental results

We evaluate our method on two different datasets with
different characteristics under different settings of parame-
ters D and O (see Eq. (2)). We also visualize examples of
score maps with correct and incorrect minutiae extractions
in Figure 7. All experiments were implemented in Tensor-
ﬂow and ran on Nvidia GTX GeForce.
3.1. Datasets

We use FVC 2002 dataset [16] with data augmentation
consisting of 3, 200 plain ﬁngerprint images for training. To
compensate for the lack of a large scale dataset for training,
we distort the input images in x and y coordinates in the
spirit of hard-training with non-ideal input ﬁngerprint im-
ages. Furthermore, we also apply additive random noise to

Table 2. Comparison of different methods for minutiae extraction on FVC 2004 and NIST SD27 datasets. Note that [18, 21] reported their
results only on subsets of FVC 2004 and NIST SD27 as mentioned in Table 1. “ ” means the authors neither provided these results in their
paper nor made their code available. D and O are parameters deﬁned in Eq. (2).

Dataset

Methods

Setting 1 (D = 8, O = 10)

Setting 2 (D = 12, O = 20)

Setting 3 (D = 16, O = 30)

NIST SD27

FVC 2004

MINDTCT [24]
VeriFinger [23]
Gao et al. [5]
Sankaran et al. [18]
Tang et al. [21]
FingerNet [22]
Proposed method
MINDTCT [24]
VeriFinger [23]
Gao et al. [5]
FingerNet [22]
Proposed method

Precision Recall
14.7%
40.1%

8.3%
3.6%

F1 score
0.106
0.066

Precision Recall
16.4%
47.9%

10.0%
5.3%

F1 score
0.124
0.095

49.5%
53.2%
69.2% 67.7%
64.3%
30.8%
69.2%
39.8%

62.1%
68.7%
79.0% 80.1%

0.513
0.684
0.416
0.505

0.643
0.795

58.1%
58.0%
70.5% 72.3%
72.1%
37.7%
77.5%
45.6%

70.4%
72.9%
83.6% 83.9%

0.58
0.714
0.495
0.574

0.716
0.837

Precision Recall
18.9%
11.2%
58.3%
7.6%
8.7%
23.5%
63.1%
26.4%
53.4%
53.0%
63.2%
63.0%
71.2% 75.7%
79.8%
42.1%
81.9%
51.8%
82.7%
48.8%
80.0%
76.0%
85.9% 84.8%

F1 score
0.141
0.134
0.127
0.372
0.532
0.631
0.734
0.551
0.635
0.614
0.779
0.853

the input images. Thus, for training CoarseNet, we have
an augmented dataset of 8, 000 images. To obtain data for
training FineNet, we extract 45 × 45 pixel patches from
these 8K training images for CoarseNet whose center is a
ground truth minutia point. For non-minutiae patches, we
randomly extract patches with the criteria that the 10 × 10
center of each patch does not contain any minutia. Thus,
we collect around 100K minutia and non-minutia patches
for training FineNet.

As mentioned in Section 2.1.4, we use FingerNet [22]
to generate labels for domain knowledge groundtruth. Be-
sides, we manually correct segmentation grountruth results
from FingerNet to ensure better learning for CoarseNet.

3.2. Evaluation

To demonstrate the robustness of our framework, we
compare our results with published approaches on FVC
2004 [15]5 and NIST SD27 [6] datasets under different cri-
teria of distance and orientation thresholds. Let the tuples
(lp, op) and (lgt, ogt) be the location coordinates and ori-
entation values of predicted and ground truth minutia. The
predicted minutia is called true if it satisﬁes the following
constrains:

(cid:40)

(cid:107)lp − lgt(cid:107)2 ≤ D
(cid:107)op − ogt(cid:107)1 ≤ O

(2)

where D and O are the thresholds in pixels and degrees,
respectively. Speciﬁcally, we set the range of distances be-
tween detected and ground truth minutiaes from 8 to 16
pixels (in location) and 10 to 30 degree (in orientation)
with default threshold value (0.5). We choose these set-
tings to demonstrate the robust and precise results from the
proposed approach while published works degrade rather
quickly.

Table 2 shows

the precision and recall compar-
isons of different approaches to minutiae extraction.

5We obtained the groundtruth from [14]

Table 3. The importance of non-maximum suppression in our
framework. N M S and N M S∗ denote the non-maximum ap-
proaches of FingerNet and the proposed approach, respectively.

Conﬁguration
FingerNet + N M S
FingerNet + N M S∗
Proposed method + N M S
Proposed method + N M S∗

Precision Recall
63.2%
63.0%
65.4%
65.2%
69.4%
73.5%
71.2% 75.7%

F1 score
0.631
0.653
0.714
0.734

MINDTCT [24] is the open source NIST Biometric Image
Software. VeriFinger [23] is a commercial SDK for minu-
tiae extraction and matching. Since Gao et al. [5] did not
release their code in public domain, we report their results
on NIST SD27 and FVC 2004 database from [22]. Darlow
et al. [3] use only a subset of the FVC dataset for training
and the rest for testing, we do not include in our evaluation.
Table 2 shows that the proposed method outperforms
state-of-the-art techniques under all settings of parameters
(thresholds) D and O for both FVC 2004 and NIST SD27.
Our results also reveal that by using only rolled/plain ﬁn-
gerprint images for training, our framework can work pretty
well for detecting minutiae in latents.

Table 3 shows a comparison between using and not us-
ing our proposed non-maximum suppression method on the
NIST SD27 dataset with setting 3 in Table 2. Because non-
maximum suppression is a post processing step, it helps im-
prove precision, recall and F1 values.

To make a complete comparison (at all the operating
points) with published methods, we present the precision-
recall curves in Figure 6. The proposed approach surpasses
all published works on both FVC 2004 and NIST SD27
datasets.

Figure 7 shows the minutiae extraction results on both
FVC 2004 and NIST SD27 datasets with different qual-
ity images. Our framework works well in difﬁcult situa-
tions such as noisy background or dry ﬁngerprints. How-
ever, there are some cases where the proposed framework

Figure 6. Precision-Recall curves on FVC 2004 (left) and NIST SD27 (right) datasets with published approaches in Setting 3.

either misses the true minutiae or extracts spurious minu-
tiae. For the FVC 2004 dataset and rolled ﬁngerprints from
NIST SD27 dataset, we obtain results that are close to the
ground truth minutiae. However, some minutiae points are
wrongly detected (image a) because of the discontinuity of
ridges or missed detections (image c) because the location
of minutiae is near the ﬁngerprint edge. For the latent ﬁn-
gerprints from NIST SD27 dataset, besides the correctly ex-
tracted minutiae, the proposed method is sensitive to severe
background noise (image e) and poor latent ﬁngerprint qual-
ity (image g). The run time per image is around 1.5 seconds
for NIST SD27 and 1.2 seconds for FVC 2004 on Nvidia
GTX GeForce.

4. Conclusions

We have presented two network architectures for auto-
matic and robust ﬁngerprint minutiae extraction that fuse
ﬁngerprint domain knowledge and deep network represen-
tation:

- CoarseNet: an automatic robust minutiae extractor that
provides candidate minutiae location and orientation with-
out a hard threshold or ﬁne tuning.

- FineNet: a strong patch based classiﬁer that acceler-
ates the reliability of candidates from CoarseNet to get ﬁnal
results.

A non-maximum suppression is proposed as a post pro-
cessing step to boost the performance of the whole frame-
work. We also reveal the impact of residual learning on
minutiae extraction in the latent ﬁngerprint dataset despite
using only plain ﬁngerprint images for training. Our exper-
imental results show that the proposed framework is robust
and achieves superior performance in terms of precision,
recall and F1 values over published state-of-the-art on both
benchmark datasets, namely FVC 2004 and NIST SD27.

The proposed framework can be further improved by (i)
using larger training set for network training that includes
latent images, (ii) constructing context descriptor to exploit
the region surrounding minutiae, (iii) improving processing
time, and (iv) unifying minutiae extractor into an end-to-end

ﬁngerprint matching framework.

References

[1] K. Cao and A. K. Jain. Automated latent ﬁngerprint recognition.

arXiv preprint arXiv:1704.01925, 2017. 3

[2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille. Deeplab: Semantic image segmentation with deep convolu-
tional nets, atrous convolution, and fully connected crfs. IEEE Trans.
PAMI, 2017. 3

[3] L. Darlow and B. Rosman. Fingerprint minutiae extraction using

deep learning. In Proc. IEEE IJCB, 2017. 2, 3, 4, 6

[4] J. Feng. Combining minutiae descriptors for ﬁngerprint matching.

Pattern Recognition, 41(1):342–352, 2008. 1

[5] X. Gao, X. Chen, J. Cao, Z. Deng, C. Liu, and J. Feng. A novel
method of ﬁngerprint minutiae extraction based on Gabor phase. In
Proc. 17th IEEE ICIP, pages 3077–3080, 2010. 6

[6] M. D. Garris and R. M. McCabe. NIST special database 27: Fin-
gerprint minutiae from latent and matching tenprint images. NIST
Technical Report NISTIR, 6534, 2000. 1, 3, 6

[7] R. Girshick, F. Iandola, T. Darrell, and J. Malik. Deformable part
In Proc. IEEE CVPR,

models are convolutional neural networks.
pages 437–446, 2015. 4

[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for
image recognition. In Proc. IEEE CVPR, pages 770–778, 2016. 1,
2, 3

[9] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep
network training by reducing internal covariate shift. In Proc. ICML,
pages 448–456, 2015. 5

[10] A. Jain, L. Hong, and R. Bolle. On-line ﬁngerprint veriﬁcation. IEEE

Trans. PAMI, 19(4):302–314, 1997. 1, 2

[11] A. K. Jain, Y. Chen, and M. Demirkus. Pores and ridges: High-
resolution ﬁngerprint matching using level 3 features. IEEE Trans.
PAMI, 29(1):15–27, 2007. 1

[12] A. K. Jain, K. Nandakumar, and A. Ross. 50 years of biometric
research: Accomplishments, challenges, and opportunities. Pattern
Recognition Letters, 79:80–105, 2016. 1

[13] L. Jiang, T. Zhao, C. Bai, A. Yong, and M. Wu. A direct ﬁnger-
print minutiae extraction approach based on convolutional neural net-
works. In Proc. IEEE IJCNN, pages 571–578, 2016. 2, 5

[14] M. Kayaoglu, B. Topcu, and U. Uludag.

Standard ﬁngerprint
databases: Manual minutiae labeling and matcher performance anal-
yses. arXiv preprint arXiv:1305.1443, 2013. 6

[15] D. Maio, D. Maltoni, R. Cappelli, J. Wayman, and A. Jain.
FVC2004: Third ﬁngerprint veriﬁcation competition. In Biometric
Authentication, pages 31–35. Springer, 2004. 3, 6

Figure 7. Visualizing minutiae extraction results. From top to bottom in each column: score maps, and minutiae extraction results overlaid
on ﬁngerprint images. (a)-(b): two plain images from FVC 2004; (c)-(d): rolled (reference) ﬁngerprints from NIST SD27; (e)-(h): latent
ﬁngerprint images from NIST SD27.

[16] D. Maio, D. Maltoni, R. Cappelli, J. L. Wayman, and A. K. Jain.
FVC2002: Second ﬁngerprint veriﬁcation competition. In Proc. 16th
ICPR, volume 3, pages 811–814, 2002. 5

[17] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-
IEEE Trans.

time object detection with region proposal networks.
PAMI, 39(6):1137–1149, 2017. 4

[18] A. Sankaran, P. Pandey, M. Vatsa, and R. Singh. On latent ﬁngerprint
minutiae extraction using stacked denoising sparse autoencoders. In
Proc. IEEE IJCB, pages 1–7, 2014. 2, 3, 6

[19] K. Simonyan and A. Zisserman. Very deep convolutional networks
for large-scale image recognition. arXiv:1409.1556, 2014. 5

[20] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi.

Inception-v4,
inception-resnet and the impact of residual connections on learning.
In Proc. AAAI, pages 4278–4284, 2017. 1, 2, 5

[21] Y. Tang, F. Gao, and J. Feng. Latent ﬁngerprint minutia extraction
using fully convolutional network. In Proc. IEEE IJCB, 2017. 2, 3,
4, 6

[22] Y. Tang, F. Gao, J. Feng, and Y. Liu. Fingernet: An uniﬁed deep
In Proc. IEEE IJCB,

network for ﬁngerprint minutiae extraction.
2017. 2, 3, 4, 6

[23] Veriﬁnger. Neuro-technology, 2010. 6
[24] C. I. Watson, M. D. Garris, E. Tabassi, C. L. Wilson, R. M. McCabe,
S. Janet, and K. Ko. User’s guide to NIST biometric image software
(NBIS). NIST Interagency/Internal Report 7392, 2007. 6

[25] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative feature learn-
ing approach for deep face recognition. In Proc. ECCV, pages 499–
515. Springer, 2016. 5

[26] X. Yang, J. Feng, and J. Zhou. Localized dictionaries based ori-
entation ﬁeld estimation for latent ﬁngerprints. IEEE Trans. PAMI,
36(5):955–969, 2014. 4

[27] S. Yoon, J. Feng, and A. K. Jain. Latent ﬁngerprint enhancement via
robust orientation ﬁeld estimation. In Proc. IEEE IJCB, pages 1–8,
2011. 2

[28] F. Zhao and X. Tang.

Preprocessing and postprocessing for
skeleton-based ﬁngerprint minutiae extraction. Pattern Recognition,
40(4):1270–1281, 2007. 1

Robust Minutiae Extractor:
Integrating Deep Networks and Fingerprint Domain Knowledge

Dinh-Luan Nguyen, Kai Cao and Anil K. Jain
Michigan State University
East Lansing, Michigan, USA
nguye590@msu.edu, {kaicao,jain}@cse.msu.edu

7
1
0
2
 
c
e
D
 
6
2
 
 
]

V
C
.
s
c
[
 
 
1
v
1
0
4
9
0
.
2
1
7
1
:
v
i
X
r
a

Abstract

We propose a fully automatic minutiae extractor, called
MinutiaeNet, based on deep neural networks with com-
pact feature representation for fast comparison of minu-
tiae sets. Speciﬁcally, ﬁrst a network, called CoarseNet,
estimates the minutiae score map and minutiae orientation
based on convolutional neural network and ﬁngerprint do-
main knowledge (enhanced image, orientation ﬁeld, and
segmentation map). Subsequently, another network, called
FineNet, reﬁnes the candidate minutiae locations based on
score map. We demonstrate the effectiveness of using the
ﬁngerprint domain knowledge together with the deep net-
works. Experimental results on both latent (NIST SD27)
and plain (FVC 2004) public domain ﬁngerprint datasets
provide comprehensive empirical support for the merits of
our method. Further, our method ﬁnds minutiae sets that
are better in terms of precision and recall in comparison
with state-of-the-art on these two datasets. Given the lack of
annotated ﬁngerprint datasets with minutiae ground truth,
the proposed approach to robust minutiae detection will
be useful to train network-based ﬁngerprint matching al-
gorithms as well as for evaluating ﬁngerprint individual-
ity at scale. MinutiaeNet is implemented in Tensorﬂow:
https://github.com/luannd/MinutiaeNet

Figure 1. Minutiae detection by the proposed approach on two
latent ﬁngerprint images (#7 and #39) from the NIST SD27
dataset [6]. Left column: minutiae score maps obtained from the
latent images shown in the right column. Right column: minu-
tiae detected by the proposed framework (red) and ground truth
minutiae (blue) overlaid on the latent image.

1. Introduction

Automatic ﬁngerprint recognition is one of the most
the past 50
widely studied topic in biometrics over
years [12]. One of the main challenges in ﬁngerprint recog-
nition is to increase the recognition accuracy, especially
for latent ﬁngerprints. Fingerprint comparison is primar-
ily based on minutiae set comparison [28, 11]. A num-
ber of hand-crafted approaches [10, 28] have been used to
augment the minutiae with their attributes to improve the
recognition accuracy. However, robust automatic ﬁnger-
print minutiae extraction, particularly for noisy ﬁngerprint

images, continues to be a bottleneck in ﬁngerprint recogni-
tion systems.

With rapid developments and success of deep learning
techniques in a variety of applications in computer vision
and pattern recognition [8, 20], we are beginning to see
network-based approaches being proposed for ﬁngerprint
recognition. Still, the prevailing methods of minutiae ex-
traction primarily utilize ﬁngerprint domain knowledge and
handcrafted features. Typically, minutiae extraction and
matching involves pre-processing stages such as ridge ex-
traction and ridge thinning, followed by minutiae extrac-
tion [10, 4] and ﬁnally heuristics to deﬁne minutiae at-

Table 1. Published network-based approaches for automatic minutiae extraction.

Study

Sankaran et al.
[18]
Jiang et al.
[13]
Tang et al.
[21]
Darlow et al.
[3]
Tang et al.
[22]

Method

Training Data

Testing Data

Comments

Performance Evaluation

Sparse autoencoders
for classiﬁcation
A combination of
JudgeNet and LocateNet
Fully convolutional
neural network
Convolutional network
classiﬁer
Uniﬁed network with
domain knowledge

132, 560 plain ﬁngerprint images;
129 images from NIST SD27
200 live scan ﬁngerprints

4, 205 latent images from private
database and 129 latents from NIST SD27
6, 336 images from
FVC 2000, 2002, and 2004
8, 000 images from private
forensic latent database

129 remaining latents
from NIST SD27
100 images from
private database
129 latents
from NIST SD27
1, 584 images from
FVC 2000, 2002, and 2004
Set A of FVC 2004 and
NIST SD27

Sliding window; manual segmentation
of latent ﬁngerprints
Sliding window; hand-crafted dividing
regions; no minutiae orientation information
Hard thresholds to cut off candidate regions;
plain network
Sliding window; hard threshold for candidate regions
(minutiae); separately estimated minutiae orientation
Plain network; depends largely on the quality of
the enhancement and segmentation stages

Patch-based and minutia-based,
metric and matching performance(*)
Precision, recall,
and F1 score
Precision, recall, F1 score,
and matching performance
Equal error rate and
matching performance
Precision, recall, and
matching performance

Proposed
approach

Domain knowledge with Residual
learning based CoarseNet and
inception-resnet based FineNet

Precision, recall, and F1
score under different location
and orientation thresholds
(*) different matchers were used in different studies and none of them were state of the art, i.e. top performing latent or slap matchers identiﬁed in the

Residual network; automatic minutiae extractor
utilizing domain knowledge; robust patch based
minutiae classiﬁer

FVC 2002 with data augmentation
(8, 000 images in total)

FVC 2004 (3, 200 images) and
NIST SD27 (all 258 latents)

NIST evaluations. For this reason, we do not report matching performance because otherwise it would not be a fair comparison with previous studies.

tributes. While such an approach works well for good qual-
ity ﬁngerprint images, it provides inaccurate minutiae loca-
tion and orientation for poor quality rolled/plain prints and,
particularly for latent ﬁngerprints. To overcome the noise in
ﬁngerprint images, Yoon et al. [27] used Gabor ﬁltering to
calculate the reliability of extracted minutiae. Although this
approach can work better than [10], it also resulted in poor
results with highly noisy images. Because these prevailing
approaches are based on handcrafted methods or heuristics,
they are only able to extract basic (or low level) features1 of
images. We believe learning based approaches using deep
networks will have better ability to extract high level fea-
tures2 from low quality ﬁngerprint images.

In this paper, we present a novel framework that ex-
ploits useful domain knowledge coded in the deep neural
networks to overcome limitations of existing approaches to
minutiae extraction. Figure 1 visualizes results of the pro-
posed framework on two latent ﬁngerprints from the NIST
SD27 dataset.

Speciﬁcally, our proposed approach comprises of two

networks, called CoarseNet and FineNet:

- CoarseNet is a residual learning [8] based convolu-
tional neural network that takes a ﬁngerprint image as initial
input, and the corresponding enhanced image, segmentation
map, and orientation ﬁeld (computed by the early stages
of CoarseNet) as secondary input to generate the minutiae
score map. The minutiae orientation is also estimated by
comparing with the ﬁngerprint orientation.

- FineNet is a robust inception-resnet [20] based minu-
tiae classiﬁer. It processes each candidate patch, a square
region whose center is the candidate minutiae point, to re-
ﬁne the minutiae score map and approximate minutiae ori-
entation by regression. Final minutiae are the classiﬁcation
results.

Deep learning approach has been used by other re-
searchers for minutiae extraction (see Table 1). But,
our approach differs from published methods in the way
we encode ﬁngerprint domain knowledge in deep learn-

1Features such as edges, corners, etc.
2Abstract/semantic features retrieved from deep layers.

ing. Sankaran et al. [18] classiﬁed the minutiae and non-
minutiae patches by using sparse autoencoders. Jiang et
al. [13] introduced a combination of two networks: Jud-
geNet for classifying minutiae patches, and LocateNet for
locating precise minutiae location. While Jiang et al. use
neural networks, their approach is very time-consuming due
to use of sliding window to extract minutiae candidates. An-
other limitation of this approach is that it does not provide
minutiae orientation information.

Tang et al. [21] utilized the idea of object detection to
detect candidate minutiae patches, but it suffers from two
major weaknesses: (i) hard threshold to delete the candi-
date patches, and (ii) the same network is used for both
candidate generation and classiﬁcation. By using sliding
windows, Darlow et al. [3] fed each pixel of the input ﬁn-
gerprint to a convolutional neural network, called MENet,
to classify whether it corresponds to a minutia or not. It also
suffers from time-consuming sliding windows as in [13],
and separate modules for minutiae location and orientation
estimates. Tang et al. [22] proposed FingerNet that maps
traditional minutiae extraction pipeline including orienta-
tion estimation, segmentation, enhancement, and extraction
to a network with ﬁxed weights. Although this approach
is promising because it combines domain knowledge and
deep network, it still uses plain 3 network architecture and
hard threshold in non-maximum suppression 4. Finally, the
accuracy of FingerNet depends largely on the quality of the
enhanced and segmentation stage while ignoring texture in-
formation in the ridge pattern.

In summary, the published approaches suffer from using
sliding windows to process each pixel in input images, set-
ting hard threshold in post-processing step, and using plain
convolutional neural network to classify candidate regions.
Furthermore, the evaluation process in these studies is not
consistent in terms of deﬁning “correct” minutiae.

The contributions of our approach are as follows:
• A network-based automatic minutiae extractor utiliz-
ing domain knowledge is proposed to provide reli-

3A series of stacked layers.
4A post-processing algorithm that merges all detections belonging to

the same object.

Figure 2. Proposed automatic minutiae extraction architecture. While CoarseNet takes full ﬁngerprint image as input, FineNet processes
minutiae proposed patches output by CoarseNet.

able minutiae location and orientation without a hard
threshold or ﬁne tuning.

• A robust patch based minutiae classiﬁer that signif-
icantly boosts the precision and recall of candidate
patches. This can be used as a robust minutiae extrac-
tor with compact embedding of minutiae features.

• A non-maximum suppression is proposed to get precise
locations for candidate patches. Experimental evalu-
ations on FVC 2004 [15] and NIST SD27 [6] show
that the proposed approach is superior to published ap-
proaches in terms of precision, recall, and F1 score val-
ues.

2. Proposed framework

Our minutiae extraction framework has two modules:
(i) residual learning based convolutional neural network,
called CoarseNet that generates candidate patches contain-
ing minutiae from input ﬁngerprint image; (ii) inception-
resnet based network architecture, called FineNet which
is a strong minutiae classiﬁer that classiﬁes the candidate
patches output by CoarseNet. These two networks also pro-
vide minutiae location and orientation information as out-
puts. Figure 2 describes the complete network architecture
for automatic minutiae location and orientation for an in-
put ﬁngerprint image. Section 2.1 presents the architecture
of CoarseNet. In Section 2.2, we introduce FineNet with
details on training to make it a strong classiﬁer.

2.1. CoarseNet for minutiae extraction

We adopt the idea of combining domain knowledge and
deep representation of neural networks in [22] to boost the
minutiae detection accuracy. In essence, we utilize the au-
tomatically extracted segmentation map, enhanced image,
and orientation map as complementary information to the
input ﬁngerprint image. The goal of CoarseNet is not to

produce the segmentation map or enhanced image or orien-
tation map. They are just the byproducts of the network.
However, these byproducts as ﬁngerprint domain knowl-
edge must be reliable to get robust minutiae score map. Be-
cause Tang et al. [22] proposed an end-to-end uniﬁed net-
work that maps handcrafted features to network based ar-
chitecture, we use this as a baseline for our CoarseNet.

2.1.1 Segmentation and orientation feature sharing

Adding more layers in the deep network with the hope of
increasing accuracy might lead to the exploding or vanish-
ing gradients problem. From the success of residual learn-
ing [8], we use residual instead of just plain stacked con-
volutional layers in our network to make it more powerful.
Figure 3 shows the detailed architecture of the network.

Unlike existing works using plain convolutional neural
network [21, 22] or sliding window [18, 3] to process each
patch with ﬁxed size and stride, we use a deeper residual
learning based network with more pooling layers to scale
down the region patch. Speciﬁcally, we get the output after
the 2nd, 3rd, and 4th pooling layer to feed to an ASPP net-
work [2] with corresponding rates for multiscale segmen-
tation. This ensures the output has the same size as input
without a loss of information when upsampling the score
map.

By using four pooling layers, each pixel in the jth feature
map, called level j, corresponds to a region 2j × 2j in the
original input. Result layers at level 4 and 3 will be tested as
coarse estimates while the level 2 serves as ﬁne estimation.
Segmentation map. Image segmentation and ﬁngerprint
orientation estimation share the same convolutional layers.
Thus, by applying multi-level approach mentioned above,
we get probability maps of each level-corresponding region
in input image. For instance, to get ﬁner-detailed segmen-
tation for each region level jl, we continue to process prob-
ability map of region level jl/2.

Figure 3. CoarseNet architecture.

Orientation map. To get complete minutiae informa-
tion from context, we adopt the fusion idea of Cao et al. [1].
We fuse the results of Dictionary-based method [26] with
our orientation results from CoarseNet. Because [26] uses a
hand-crafted approach, we set the fusion weight ratio of its
output with our network-based approach as 1:3.

2.1.2 Candidate generation

The input ﬁngerprint image might contain large amounts of
noise. So, without using domain knowledge we may not be
able to identify prominent ﬁngerprint features. The domain
knowledge comprises of four things: raw input image, en-
hanced image, orientation map, and segmentation map. In
the Gabor image enhancement module, we take the average
of ﬁltered image and the orientation map for ridge ﬂow es-
timation. To emphasize texture information in ﬁngerprints,
we stack the original input image with the output of en-
hancement module to obtain the ﬁnal enhancement map. To
remove spurious noises, we apply segmentation map on the
enhancement map and use it as input to coarse minutiae ex-
tractor module.

To obtain the precise location of minutiae, each level of
residual net is fused to get the ﬁnal minutiae score map with
size h/16 × w/16, where h and w are the height and width
of the input image. Figure 4 shows the details of processing

score map. To reduce the processing time, we use score
map at level 4 as a coarse location. To get precise location,
lower level score maps are used.

2.1.3 Non-maximum suppression

Using non-maximum suppression to reduce the number of
candidates is common in object detection [7, 17]. Some of
the candidate regions are deleted to get a reliable minutiae
score map by setting a hard threshold [3] or using heuris-
tics [21, 22]. However, a hard threshold can also suppress
valid minutiae locations. A commonly used heuristics is to
sort the candidate scores in ascending order. The L2 dis-
tance between pairwise candidates is calculated with hard
thresholds for distance and orientation. By iteratively com-
paring each candidate with the rest in the candidate list, only
the candidate with higher score and score above the thresh-
olds is kept. However, this approach fails when two minu-
tiae are near each other and the inter-minutiae distance is
below the hard thresholds.

Since each score in the minutiae map corresponds to a
speciﬁc region in the input image, we propose to use the
intersection over union strategy. Speciﬁcally, after sorting
the scores of the candidate list, we keep high score candi-
dates while ignoring the lower scoring candidates with at
least 50% overlap with the candidates already selected.

2.1.4 Training data for CoarseNet

Given the lack of datasets with ground truth, we use the ap-
proach in Tang et al. [22] to generate weak labels for train-
ing the segmentation and orientation module. The coarse
minutiae extractor module uses minutiae location and minu-
tiae orientation ground truth provided in the two datasets.
We also use data augmentation techniques as mentioned in
Section 3.

2.2. FineNet

Figure 4. Candidate patch processing map. Lower the level of
score map, more detail it provides.

Extracting minutiae based on candidate patches is not
adequate. Although CoarseNet is reliable, it still fails to de-

tect true minutiae or detects spurious minutiae. This can
lead to poor performance in ﬁngerprint matching. This mo-
tivates our use of FineNet - a minutiae classiﬁer from gener-
ated candidate patches. FineNet takes candidates from the
output of CoarseNet as input to decide whether the region
10 × 10 in the center of the corresponding patch has a valid
minutia or not.

2.2.1 FineNet architecture

Figure 5 describes the architecture of FineNet. As men-
tioned in Section 2.1.1, we use the Inception-Resnet v1 ar-
chitecture as a core network in FineNet.

For FineNet training, we extract an equal number of
t1 × t1 sized minutiae and non-minutiae patches with t1 =
45. FineNet determines the whether the 10 × 10 pixel re-
gion in the center of each patch contains a valid minutia
or not. The candidate patches are resized into t2 × t2 pix-
els that feed to FineNet. Based on the observation that the
original input image size (without rescaling) is not large in
comparison with images for object classiﬁcation, too much
up scaling the image can cause blurring the tiny details, and
too small an input image size is not sufﬁcient for network
with complex architecture, we choose t2 = 160 pixels.

Training data for FineNet is extracted from the input gray
scale images where minutiae data are based on the ground
truth minutiae location and non-minutiae ones are from ran-

Figure 5. FineNet architecture. For details of the Inception-Resnet
v1 arichitecture block, we refer the readers to [20].

dom sampling with the center regions do not contain partial
or fully minutiae location. To make the network more ro-
bust, we use some small techniques like Batch Normaliza-
tion [9], rotation, ﬂipping, scaled augmentation [19], and
bounding blurring [13] as pre-processing stage.

2.2.2 Losses and implementation details

Intra-Inter class losses. Because input captured ﬁnger-
print image is not always in the ideal condition, it might
be easily affected by distortion, ﬁnger movement, or quality
(wet/dry) of ﬁnger. Thus, the variation of minutiae shapes
and surrounding ridges can affect the accuracy of the clas-
siﬁer. To handle this situation and make the FineNet more
robust with intra-class variations, we use Center Loss [25]
as a complementary of softmax loss and minutiae orienta-
tion loss. While softmax loss tends to push away features
of different classes, center loss tends to pull features in the
same class closer. Let L, LC, LS , LO be the total loss, cen-
ter loss, sofmax loss, and orientation loss, the total loss for
training is calculated as follows:

L = αLC + (1 − α)LS + βLO

(1)

where we set α = 0.5 to balance between intra-class (cen-
ter) and inter-class (sofmax) loss and β = 2 to emphasize
the importance of precision of minutiae orientation.

Parameter settings. As mentioned in Section 2.2.1, ﬁn-
gerprint patches are input to FineNet where the patch size
is 160 × 160. To ensure our network can handle distortion
in input image, we apply scaled augmentation [19], random
cropping, and brightness adjustment. Horizontal and verti-
cal ﬂip with pixel mean subtraction are also adopted. We
randomly initialize all variables by using a Gaussian distri-
bution N (0, 0.01). Batch size is set to 100. We use schedule
learning rate after particular iterations. Speciﬁcally, we set
it as 0.01 in the beginning and reduce 10 times after 50K
iterations. To prevent vanishing gradient problem, we set
the maximum epoch to 200K. We use 0.0004 as the value
for momentum and weight decay is 0.9.
3. Experimental results

We evaluate our method on two different datasets with
different characteristics under different settings of parame-
ters D and O (see Eq. (2)). We also visualize examples of
score maps with correct and incorrect minutiae extractions
in Figure 7. All experiments were implemented in Tensor-
ﬂow and ran on Nvidia GTX GeForce.
3.1. Datasets

We use FVC 2002 dataset [16] with data augmentation
consisting of 3, 200 plain ﬁngerprint images for training. To
compensate for the lack of a large scale dataset for training,
we distort the input images in x and y coordinates in the
spirit of hard-training with non-ideal input ﬁngerprint im-
ages. Furthermore, we also apply additive random noise to

Table 2. Comparison of different methods for minutiae extraction on FVC 2004 and NIST SD27 datasets. Note that [18, 21] reported their
results only on subsets of FVC 2004 and NIST SD27 as mentioned in Table 1. “ ” means the authors neither provided these results in their
paper nor made their code available. D and O are parameters deﬁned in Eq. (2).

Dataset

Methods

Setting 1 (D = 8, O = 10)

Setting 2 (D = 12, O = 20)

Setting 3 (D = 16, O = 30)

NIST SD27

FVC 2004

MINDTCT [24]
VeriFinger [23]
Gao et al. [5]
Sankaran et al. [18]
Tang et al. [21]
FingerNet [22]
Proposed method
MINDTCT [24]
VeriFinger [23]
Gao et al. [5]
FingerNet [22]
Proposed method

Precision Recall
14.7%
40.1%

8.3%
3.6%

F1 score
0.106
0.066

Precision Recall
16.4%
47.9%

10.0%
5.3%

F1 score
0.124
0.095

49.5%
53.2%
69.2% 67.7%
64.3%
30.8%
69.2%
39.8%

62.1%
68.7%
79.0% 80.1%

0.513
0.684
0.416
0.505

0.643
0.795

58.1%
58.0%
70.5% 72.3%
72.1%
37.7%
77.5%
45.6%

70.4%
72.9%
83.6% 83.9%

0.58
0.714
0.495
0.574

0.716
0.837

Precision Recall
18.9%
11.2%
58.3%
7.6%
8.7%
23.5%
63.1%
26.4%
53.4%
53.0%
63.2%
63.0%
71.2% 75.7%
79.8%
42.1%
81.9%
51.8%
82.7%
48.8%
80.0%
76.0%
85.9% 84.8%

F1 score
0.141
0.134
0.127
0.372
0.532
0.631
0.734
0.551
0.635
0.614
0.779
0.853

the input images. Thus, for training CoarseNet, we have
an augmented dataset of 8, 000 images. To obtain data for
training FineNet, we extract 45 × 45 pixel patches from
these 8K training images for CoarseNet whose center is a
ground truth minutia point. For non-minutiae patches, we
randomly extract patches with the criteria that the 10 × 10
center of each patch does not contain any minutia. Thus,
we collect around 100K minutia and non-minutia patches
for training FineNet.

As mentioned in Section 2.1.4, we use FingerNet [22]
to generate labels for domain knowledge groundtruth. Be-
sides, we manually correct segmentation grountruth results
from FingerNet to ensure better learning for CoarseNet.

3.2. Evaluation

To demonstrate the robustness of our framework, we
compare our results with published approaches on FVC
2004 [15]5 and NIST SD27 [6] datasets under different cri-
teria of distance and orientation thresholds. Let the tuples
(lp, op) and (lgt, ogt) be the location coordinates and ori-
entation values of predicted and ground truth minutia. The
predicted minutia is called true if it satisﬁes the following
constrains:

(cid:40)

(cid:107)lp − lgt(cid:107)2 ≤ D
(cid:107)op − ogt(cid:107)1 ≤ O

(2)

where D and O are the thresholds in pixels and degrees,
respectively. Speciﬁcally, we set the range of distances be-
tween detected and ground truth minutiaes from 8 to 16
pixels (in location) and 10 to 30 degree (in orientation)
with default threshold value (0.5). We choose these set-
tings to demonstrate the robust and precise results from the
proposed approach while published works degrade rather
quickly.

Table 2 shows

the precision and recall compar-
isons of different approaches to minutiae extraction.

5We obtained the groundtruth from [14]

Table 3. The importance of non-maximum suppression in our
framework. N M S and N M S∗ denote the non-maximum ap-
proaches of FingerNet and the proposed approach, respectively.

Conﬁguration
FingerNet + N M S
FingerNet + N M S∗
Proposed method + N M S
Proposed method + N M S∗

Precision Recall
63.2%
63.0%
65.4%
65.2%
69.4%
73.5%
71.2% 75.7%

F1 score
0.631
0.653
0.714
0.734

MINDTCT [24] is the open source NIST Biometric Image
Software. VeriFinger [23] is a commercial SDK for minu-
tiae extraction and matching. Since Gao et al. [5] did not
release their code in public domain, we report their results
on NIST SD27 and FVC 2004 database from [22]. Darlow
et al. [3] use only a subset of the FVC dataset for training
and the rest for testing, we do not include in our evaluation.
Table 2 shows that the proposed method outperforms
state-of-the-art techniques under all settings of parameters
(thresholds) D and O for both FVC 2004 and NIST SD27.
Our results also reveal that by using only rolled/plain ﬁn-
gerprint images for training, our framework can work pretty
well for detecting minutiae in latents.

Table 3 shows a comparison between using and not us-
ing our proposed non-maximum suppression method on the
NIST SD27 dataset with setting 3 in Table 2. Because non-
maximum suppression is a post processing step, it helps im-
prove precision, recall and F1 values.

To make a complete comparison (at all the operating
points) with published methods, we present the precision-
recall curves in Figure 6. The proposed approach surpasses
all published works on both FVC 2004 and NIST SD27
datasets.

Figure 7 shows the minutiae extraction results on both
FVC 2004 and NIST SD27 datasets with different qual-
ity images. Our framework works well in difﬁcult situa-
tions such as noisy background or dry ﬁngerprints. How-
ever, there are some cases where the proposed framework

Figure 6. Precision-Recall curves on FVC 2004 (left) and NIST SD27 (right) datasets with published approaches in Setting 3.

either misses the true minutiae or extracts spurious minu-
tiae. For the FVC 2004 dataset and rolled ﬁngerprints from
NIST SD27 dataset, we obtain results that are close to the
ground truth minutiae. However, some minutiae points are
wrongly detected (image a) because of the discontinuity of
ridges or missed detections (image c) because the location
of minutiae is near the ﬁngerprint edge. For the latent ﬁn-
gerprints from NIST SD27 dataset, besides the correctly ex-
tracted minutiae, the proposed method is sensitive to severe
background noise (image e) and poor latent ﬁngerprint qual-
ity (image g). The run time per image is around 1.5 seconds
for NIST SD27 and 1.2 seconds for FVC 2004 on Nvidia
GTX GeForce.

4. Conclusions

We have presented two network architectures for auto-
matic and robust ﬁngerprint minutiae extraction that fuse
ﬁngerprint domain knowledge and deep network represen-
tation:

- CoarseNet: an automatic robust minutiae extractor that
provides candidate minutiae location and orientation with-
out a hard threshold or ﬁne tuning.

- FineNet: a strong patch based classiﬁer that acceler-
ates the reliability of candidates from CoarseNet to get ﬁnal
results.

A non-maximum suppression is proposed as a post pro-
cessing step to boost the performance of the whole frame-
work. We also reveal the impact of residual learning on
minutiae extraction in the latent ﬁngerprint dataset despite
using only plain ﬁngerprint images for training. Our exper-
imental results show that the proposed framework is robust
and achieves superior performance in terms of precision,
recall and F1 values over published state-of-the-art on both
benchmark datasets, namely FVC 2004 and NIST SD27.

The proposed framework can be further improved by (i)
using larger training set for network training that includes
latent images, (ii) constructing context descriptor to exploit
the region surrounding minutiae, (iii) improving processing
time, and (iv) unifying minutiae extractor into an end-to-end

ﬁngerprint matching framework.

References

[1] K. Cao and A. K. Jain. Automated latent ﬁngerprint recognition.

arXiv preprint arXiv:1704.01925, 2017. 3

[2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille. Deeplab: Semantic image segmentation with deep convolu-
tional nets, atrous convolution, and fully connected crfs. IEEE Trans.
PAMI, 2017. 3

[3] L. Darlow and B. Rosman. Fingerprint minutiae extraction using

deep learning. In Proc. IEEE IJCB, 2017. 2, 3, 4, 6

[4] J. Feng. Combining minutiae descriptors for ﬁngerprint matching.

Pattern Recognition, 41(1):342–352, 2008. 1

[5] X. Gao, X. Chen, J. Cao, Z. Deng, C. Liu, and J. Feng. A novel
method of ﬁngerprint minutiae extraction based on Gabor phase. In
Proc. 17th IEEE ICIP, pages 3077–3080, 2010. 6

[6] M. D. Garris and R. M. McCabe. NIST special database 27: Fin-
gerprint minutiae from latent and matching tenprint images. NIST
Technical Report NISTIR, 6534, 2000. 1, 3, 6

[7] R. Girshick, F. Iandola, T. Darrell, and J. Malik. Deformable part
In Proc. IEEE CVPR,

models are convolutional neural networks.
pages 437–446, 2015. 4

[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for
image recognition. In Proc. IEEE CVPR, pages 770–778, 2016. 1,
2, 3

[9] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep
network training by reducing internal covariate shift. In Proc. ICML,
pages 448–456, 2015. 5

[10] A. Jain, L. Hong, and R. Bolle. On-line ﬁngerprint veriﬁcation. IEEE

Trans. PAMI, 19(4):302–314, 1997. 1, 2

[11] A. K. Jain, Y. Chen, and M. Demirkus. Pores and ridges: High-
resolution ﬁngerprint matching using level 3 features. IEEE Trans.
PAMI, 29(1):15–27, 2007. 1

[12] A. K. Jain, K. Nandakumar, and A. Ross. 50 years of biometric
research: Accomplishments, challenges, and opportunities. Pattern
Recognition Letters, 79:80–105, 2016. 1

[13] L. Jiang, T. Zhao, C. Bai, A. Yong, and M. Wu. A direct ﬁnger-
print minutiae extraction approach based on convolutional neural net-
works. In Proc. IEEE IJCNN, pages 571–578, 2016. 2, 5

[14] M. Kayaoglu, B. Topcu, and U. Uludag.

Standard ﬁngerprint
databases: Manual minutiae labeling and matcher performance anal-
yses. arXiv preprint arXiv:1305.1443, 2013. 6

[15] D. Maio, D. Maltoni, R. Cappelli, J. Wayman, and A. Jain.
FVC2004: Third ﬁngerprint veriﬁcation competition. In Biometric
Authentication, pages 31–35. Springer, 2004. 3, 6

Figure 7. Visualizing minutiae extraction results. From top to bottom in each column: score maps, and minutiae extraction results overlaid
on ﬁngerprint images. (a)-(b): two plain images from FVC 2004; (c)-(d): rolled (reference) ﬁngerprints from NIST SD27; (e)-(h): latent
ﬁngerprint images from NIST SD27.

[16] D. Maio, D. Maltoni, R. Cappelli, J. L. Wayman, and A. K. Jain.
FVC2002: Second ﬁngerprint veriﬁcation competition. In Proc. 16th
ICPR, volume 3, pages 811–814, 2002. 5

[17] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-
IEEE Trans.

time object detection with region proposal networks.
PAMI, 39(6):1137–1149, 2017. 4

[18] A. Sankaran, P. Pandey, M. Vatsa, and R. Singh. On latent ﬁngerprint
minutiae extraction using stacked denoising sparse autoencoders. In
Proc. IEEE IJCB, pages 1–7, 2014. 2, 3, 6

[19] K. Simonyan and A. Zisserman. Very deep convolutional networks
for large-scale image recognition. arXiv:1409.1556, 2014. 5

[20] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi.

Inception-v4,
inception-resnet and the impact of residual connections on learning.
In Proc. AAAI, pages 4278–4284, 2017. 1, 2, 5

[21] Y. Tang, F. Gao, and J. Feng. Latent ﬁngerprint minutia extraction
using fully convolutional network. In Proc. IEEE IJCB, 2017. 2, 3,
4, 6

[22] Y. Tang, F. Gao, J. Feng, and Y. Liu. Fingernet: An uniﬁed deep
In Proc. IEEE IJCB,

network for ﬁngerprint minutiae extraction.
2017. 2, 3, 4, 6

[23] Veriﬁnger. Neuro-technology, 2010. 6
[24] C. I. Watson, M. D. Garris, E. Tabassi, C. L. Wilson, R. M. McCabe,
S. Janet, and K. Ko. User’s guide to NIST biometric image software
(NBIS). NIST Interagency/Internal Report 7392, 2007. 6

[25] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative feature learn-
ing approach for deep face recognition. In Proc. ECCV, pages 499–
515. Springer, 2016. 5

[26] X. Yang, J. Feng, and J. Zhou. Localized dictionaries based ori-
entation ﬁeld estimation for latent ﬁngerprints. IEEE Trans. PAMI,
36(5):955–969, 2014. 4

[27] S. Yoon, J. Feng, and A. K. Jain. Latent ﬁngerprint enhancement via
robust orientation ﬁeld estimation. In Proc. IEEE IJCB, pages 1–8,
2011. 2

[28] F. Zhao and X. Tang.

Preprocessing and postprocessing for
skeleton-based ﬁngerprint minutiae extraction. Pattern Recognition,
40(4):1270–1281, 2007. 1

Robust Minutiae Extractor:
Integrating Deep Networks and Fingerprint Domain Knowledge

Dinh-Luan Nguyen, Kai Cao and Anil K. Jain
Michigan State University
East Lansing, Michigan, USA
nguye590@msu.edu, {kaicao,jain}@cse.msu.edu

7
1
0
2
 
c
e
D
 
6
2
 
 
]

V
C
.
s
c
[
 
 
1
v
1
0
4
9
0
.
2
1
7
1
:
v
i
X
r
a

Abstract

We propose a fully automatic minutiae extractor, called
MinutiaeNet, based on deep neural networks with com-
pact feature representation for fast comparison of minu-
tiae sets. Speciﬁcally, ﬁrst a network, called CoarseNet,
estimates the minutiae score map and minutiae orientation
based on convolutional neural network and ﬁngerprint do-
main knowledge (enhanced image, orientation ﬁeld, and
segmentation map). Subsequently, another network, called
FineNet, reﬁnes the candidate minutiae locations based on
score map. We demonstrate the effectiveness of using the
ﬁngerprint domain knowledge together with the deep net-
works. Experimental results on both latent (NIST SD27)
and plain (FVC 2004) public domain ﬁngerprint datasets
provide comprehensive empirical support for the merits of
our method. Further, our method ﬁnds minutiae sets that
are better in terms of precision and recall in comparison
with state-of-the-art on these two datasets. Given the lack of
annotated ﬁngerprint datasets with minutiae ground truth,
the proposed approach to robust minutiae detection will
be useful to train network-based ﬁngerprint matching al-
gorithms as well as for evaluating ﬁngerprint individual-
ity at scale. MinutiaeNet is implemented in Tensorﬂow:
https://github.com/luannd/MinutiaeNet

Figure 1. Minutiae detection by the proposed approach on two
latent ﬁngerprint images (#7 and #39) from the NIST SD27
dataset [6]. Left column: minutiae score maps obtained from the
latent images shown in the right column. Right column: minu-
tiae detected by the proposed framework (red) and ground truth
minutiae (blue) overlaid on the latent image.

1. Introduction

Automatic ﬁngerprint recognition is one of the most
the past 50
widely studied topic in biometrics over
years [12]. One of the main challenges in ﬁngerprint recog-
nition is to increase the recognition accuracy, especially
for latent ﬁngerprints. Fingerprint comparison is primar-
ily based on minutiae set comparison [28, 11]. A num-
ber of hand-crafted approaches [10, 28] have been used to
augment the minutiae with their attributes to improve the
recognition accuracy. However, robust automatic ﬁnger-
print minutiae extraction, particularly for noisy ﬁngerprint

images, continues to be a bottleneck in ﬁngerprint recogni-
tion systems.

With rapid developments and success of deep learning
techniques in a variety of applications in computer vision
and pattern recognition [8, 20], we are beginning to see
network-based approaches being proposed for ﬁngerprint
recognition. Still, the prevailing methods of minutiae ex-
traction primarily utilize ﬁngerprint domain knowledge and
handcrafted features. Typically, minutiae extraction and
matching involves pre-processing stages such as ridge ex-
traction and ridge thinning, followed by minutiae extrac-
tion [10, 4] and ﬁnally heuristics to deﬁne minutiae at-

Table 1. Published network-based approaches for automatic minutiae extraction.

Study

Sankaran et al.
[18]
Jiang et al.
[13]
Tang et al.
[21]
Darlow et al.
[3]
Tang et al.
[22]

Method

Training Data

Testing Data

Comments

Performance Evaluation

Sparse autoencoders
for classiﬁcation
A combination of
JudgeNet and LocateNet
Fully convolutional
neural network
Convolutional network
classiﬁer
Uniﬁed network with
domain knowledge

132, 560 plain ﬁngerprint images;
129 images from NIST SD27
200 live scan ﬁngerprints

4, 205 latent images from private
database and 129 latents from NIST SD27
6, 336 images from
FVC 2000, 2002, and 2004
8, 000 images from private
forensic latent database

129 remaining latents
from NIST SD27
100 images from
private database
129 latents
from NIST SD27
1, 584 images from
FVC 2000, 2002, and 2004
Set A of FVC 2004 and
NIST SD27

Sliding window; manual segmentation
of latent ﬁngerprints
Sliding window; hand-crafted dividing
regions; no minutiae orientation information
Hard thresholds to cut off candidate regions;
plain network
Sliding window; hard threshold for candidate regions
(minutiae); separately estimated minutiae orientation
Plain network; depends largely on the quality of
the enhancement and segmentation stages

Patch-based and minutia-based,
metric and matching performance(*)
Precision, recall,
and F1 score
Precision, recall, F1 score,
and matching performance
Equal error rate and
matching performance
Precision, recall, and
matching performance

Proposed
approach

Domain knowledge with Residual
learning based CoarseNet and
inception-resnet based FineNet

Precision, recall, and F1
score under different location
and orientation thresholds
(*) different matchers were used in different studies and none of them were state of the art, i.e. top performing latent or slap matchers identiﬁed in the

Residual network; automatic minutiae extractor
utilizing domain knowledge; robust patch based
minutiae classiﬁer

FVC 2002 with data augmentation
(8, 000 images in total)

FVC 2004 (3, 200 images) and
NIST SD27 (all 258 latents)

NIST evaluations. For this reason, we do not report matching performance because otherwise it would not be a fair comparison with previous studies.

tributes. While such an approach works well for good qual-
ity ﬁngerprint images, it provides inaccurate minutiae loca-
tion and orientation for poor quality rolled/plain prints and,
particularly for latent ﬁngerprints. To overcome the noise in
ﬁngerprint images, Yoon et al. [27] used Gabor ﬁltering to
calculate the reliability of extracted minutiae. Although this
approach can work better than [10], it also resulted in poor
results with highly noisy images. Because these prevailing
approaches are based on handcrafted methods or heuristics,
they are only able to extract basic (or low level) features1 of
images. We believe learning based approaches using deep
networks will have better ability to extract high level fea-
tures2 from low quality ﬁngerprint images.

In this paper, we present a novel framework that ex-
ploits useful domain knowledge coded in the deep neural
networks to overcome limitations of existing approaches to
minutiae extraction. Figure 1 visualizes results of the pro-
posed framework on two latent ﬁngerprints from the NIST
SD27 dataset.

Speciﬁcally, our proposed approach comprises of two

networks, called CoarseNet and FineNet:

- CoarseNet is a residual learning [8] based convolu-
tional neural network that takes a ﬁngerprint image as initial
input, and the corresponding enhanced image, segmentation
map, and orientation ﬁeld (computed by the early stages
of CoarseNet) as secondary input to generate the minutiae
score map. The minutiae orientation is also estimated by
comparing with the ﬁngerprint orientation.

- FineNet is a robust inception-resnet [20] based minu-
tiae classiﬁer. It processes each candidate patch, a square
region whose center is the candidate minutiae point, to re-
ﬁne the minutiae score map and approximate minutiae ori-
entation by regression. Final minutiae are the classiﬁcation
results.

Deep learning approach has been used by other re-
searchers for minutiae extraction (see Table 1). But,
our approach differs from published methods in the way
we encode ﬁngerprint domain knowledge in deep learn-

1Features such as edges, corners, etc.
2Abstract/semantic features retrieved from deep layers.

ing. Sankaran et al. [18] classiﬁed the minutiae and non-
minutiae patches by using sparse autoencoders. Jiang et
al. [13] introduced a combination of two networks: Jud-
geNet for classifying minutiae patches, and LocateNet for
locating precise minutiae location. While Jiang et al. use
neural networks, their approach is very time-consuming due
to use of sliding window to extract minutiae candidates. An-
other limitation of this approach is that it does not provide
minutiae orientation information.

Tang et al. [21] utilized the idea of object detection to
detect candidate minutiae patches, but it suffers from two
major weaknesses: (i) hard threshold to delete the candi-
date patches, and (ii) the same network is used for both
candidate generation and classiﬁcation. By using sliding
windows, Darlow et al. [3] fed each pixel of the input ﬁn-
gerprint to a convolutional neural network, called MENet,
to classify whether it corresponds to a minutia or not. It also
suffers from time-consuming sliding windows as in [13],
and separate modules for minutiae location and orientation
estimates. Tang et al. [22] proposed FingerNet that maps
traditional minutiae extraction pipeline including orienta-
tion estimation, segmentation, enhancement, and extraction
to a network with ﬁxed weights. Although this approach
is promising because it combines domain knowledge and
deep network, it still uses plain 3 network architecture and
hard threshold in non-maximum suppression 4. Finally, the
accuracy of FingerNet depends largely on the quality of the
enhanced and segmentation stage while ignoring texture in-
formation in the ridge pattern.

In summary, the published approaches suffer from using
sliding windows to process each pixel in input images, set-
ting hard threshold in post-processing step, and using plain
convolutional neural network to classify candidate regions.
Furthermore, the evaluation process in these studies is not
consistent in terms of deﬁning “correct” minutiae.

The contributions of our approach are as follows:
• A network-based automatic minutiae extractor utiliz-
ing domain knowledge is proposed to provide reli-

3A series of stacked layers.
4A post-processing algorithm that merges all detections belonging to

the same object.

Figure 2. Proposed automatic minutiae extraction architecture. While CoarseNet takes full ﬁngerprint image as input, FineNet processes
minutiae proposed patches output by CoarseNet.

able minutiae location and orientation without a hard
threshold or ﬁne tuning.

• A robust patch based minutiae classiﬁer that signif-
icantly boosts the precision and recall of candidate
patches. This can be used as a robust minutiae extrac-
tor with compact embedding of minutiae features.

• A non-maximum suppression is proposed to get precise
locations for candidate patches. Experimental evalu-
ations on FVC 2004 [15] and NIST SD27 [6] show
that the proposed approach is superior to published ap-
proaches in terms of precision, recall, and F1 score val-
ues.

2. Proposed framework

Our minutiae extraction framework has two modules:
(i) residual learning based convolutional neural network,
called CoarseNet that generates candidate patches contain-
ing minutiae from input ﬁngerprint image; (ii) inception-
resnet based network architecture, called FineNet which
is a strong minutiae classiﬁer that classiﬁes the candidate
patches output by CoarseNet. These two networks also pro-
vide minutiae location and orientation information as out-
puts. Figure 2 describes the complete network architecture
for automatic minutiae location and orientation for an in-
put ﬁngerprint image. Section 2.1 presents the architecture
of CoarseNet. In Section 2.2, we introduce FineNet with
details on training to make it a strong classiﬁer.

2.1. CoarseNet for minutiae extraction

We adopt the idea of combining domain knowledge and
deep representation of neural networks in [22] to boost the
minutiae detection accuracy. In essence, we utilize the au-
tomatically extracted segmentation map, enhanced image,
and orientation map as complementary information to the
input ﬁngerprint image. The goal of CoarseNet is not to

produce the segmentation map or enhanced image or orien-
tation map. They are just the byproducts of the network.
However, these byproducts as ﬁngerprint domain knowl-
edge must be reliable to get robust minutiae score map. Be-
cause Tang et al. [22] proposed an end-to-end uniﬁed net-
work that maps handcrafted features to network based ar-
chitecture, we use this as a baseline for our CoarseNet.

2.1.1 Segmentation and orientation feature sharing

Adding more layers in the deep network with the hope of
increasing accuracy might lead to the exploding or vanish-
ing gradients problem. From the success of residual learn-
ing [8], we use residual instead of just plain stacked con-
volutional layers in our network to make it more powerful.
Figure 3 shows the detailed architecture of the network.

Unlike existing works using plain convolutional neural
network [21, 22] or sliding window [18, 3] to process each
patch with ﬁxed size and stride, we use a deeper residual
learning based network with more pooling layers to scale
down the region patch. Speciﬁcally, we get the output after
the 2nd, 3rd, and 4th pooling layer to feed to an ASPP net-
work [2] with corresponding rates for multiscale segmen-
tation. This ensures the output has the same size as input
without a loss of information when upsampling the score
map.

By using four pooling layers, each pixel in the jth feature
map, called level j, corresponds to a region 2j × 2j in the
original input. Result layers at level 4 and 3 will be tested as
coarse estimates while the level 2 serves as ﬁne estimation.
Segmentation map. Image segmentation and ﬁngerprint
orientation estimation share the same convolutional layers.
Thus, by applying multi-level approach mentioned above,
we get probability maps of each level-corresponding region
in input image. For instance, to get ﬁner-detailed segmen-
tation for each region level jl, we continue to process prob-
ability map of region level jl/2.

Figure 3. CoarseNet architecture.

Orientation map. To get complete minutiae informa-
tion from context, we adopt the fusion idea of Cao et al. [1].
We fuse the results of Dictionary-based method [26] with
our orientation results from CoarseNet. Because [26] uses a
hand-crafted approach, we set the fusion weight ratio of its
output with our network-based approach as 1:3.

2.1.2 Candidate generation

The input ﬁngerprint image might contain large amounts of
noise. So, without using domain knowledge we may not be
able to identify prominent ﬁngerprint features. The domain
knowledge comprises of four things: raw input image, en-
hanced image, orientation map, and segmentation map. In
the Gabor image enhancement module, we take the average
of ﬁltered image and the orientation map for ridge ﬂow es-
timation. To emphasize texture information in ﬁngerprints,
we stack the original input image with the output of en-
hancement module to obtain the ﬁnal enhancement map. To
remove spurious noises, we apply segmentation map on the
enhancement map and use it as input to coarse minutiae ex-
tractor module.

To obtain the precise location of minutiae, each level of
residual net is fused to get the ﬁnal minutiae score map with
size h/16 × w/16, where h and w are the height and width
of the input image. Figure 4 shows the details of processing

score map. To reduce the processing time, we use score
map at level 4 as a coarse location. To get precise location,
lower level score maps are used.

2.1.3 Non-maximum suppression

Using non-maximum suppression to reduce the number of
candidates is common in object detection [7, 17]. Some of
the candidate regions are deleted to get a reliable minutiae
score map by setting a hard threshold [3] or using heuris-
tics [21, 22]. However, a hard threshold can also suppress
valid minutiae locations. A commonly used heuristics is to
sort the candidate scores in ascending order. The L2 dis-
tance between pairwise candidates is calculated with hard
thresholds for distance and orientation. By iteratively com-
paring each candidate with the rest in the candidate list, only
the candidate with higher score and score above the thresh-
olds is kept. However, this approach fails when two minu-
tiae are near each other and the inter-minutiae distance is
below the hard thresholds.

Since each score in the minutiae map corresponds to a
speciﬁc region in the input image, we propose to use the
intersection over union strategy. Speciﬁcally, after sorting
the scores of the candidate list, we keep high score candi-
dates while ignoring the lower scoring candidates with at
least 50% overlap with the candidates already selected.

2.1.4 Training data for CoarseNet

Given the lack of datasets with ground truth, we use the ap-
proach in Tang et al. [22] to generate weak labels for train-
ing the segmentation and orientation module. The coarse
minutiae extractor module uses minutiae location and minu-
tiae orientation ground truth provided in the two datasets.
We also use data augmentation techniques as mentioned in
Section 3.

2.2. FineNet

Figure 4. Candidate patch processing map. Lower the level of
score map, more detail it provides.

Extracting minutiae based on candidate patches is not
adequate. Although CoarseNet is reliable, it still fails to de-

tect true minutiae or detects spurious minutiae. This can
lead to poor performance in ﬁngerprint matching. This mo-
tivates our use of FineNet - a minutiae classiﬁer from gener-
ated candidate patches. FineNet takes candidates from the
output of CoarseNet as input to decide whether the region
10 × 10 in the center of the corresponding patch has a valid
minutia or not.

2.2.1 FineNet architecture

Figure 5 describes the architecture of FineNet. As men-
tioned in Section 2.1.1, we use the Inception-Resnet v1 ar-
chitecture as a core network in FineNet.

For FineNet training, we extract an equal number of
t1 × t1 sized minutiae and non-minutiae patches with t1 =
45. FineNet determines the whether the 10 × 10 pixel re-
gion in the center of each patch contains a valid minutia
or not. The candidate patches are resized into t2 × t2 pix-
els that feed to FineNet. Based on the observation that the
original input image size (without rescaling) is not large in
comparison with images for object classiﬁcation, too much
up scaling the image can cause blurring the tiny details, and
too small an input image size is not sufﬁcient for network
with complex architecture, we choose t2 = 160 pixels.

Training data for FineNet is extracted from the input gray
scale images where minutiae data are based on the ground
truth minutiae location and non-minutiae ones are from ran-

Figure 5. FineNet architecture. For details of the Inception-Resnet
v1 arichitecture block, we refer the readers to [20].

dom sampling with the center regions do not contain partial
or fully minutiae location. To make the network more ro-
bust, we use some small techniques like Batch Normaliza-
tion [9], rotation, ﬂipping, scaled augmentation [19], and
bounding blurring [13] as pre-processing stage.

2.2.2 Losses and implementation details

Intra-Inter class losses. Because input captured ﬁnger-
print image is not always in the ideal condition, it might
be easily affected by distortion, ﬁnger movement, or quality
(wet/dry) of ﬁnger. Thus, the variation of minutiae shapes
and surrounding ridges can affect the accuracy of the clas-
siﬁer. To handle this situation and make the FineNet more
robust with intra-class variations, we use Center Loss [25]
as a complementary of softmax loss and minutiae orienta-
tion loss. While softmax loss tends to push away features
of different classes, center loss tends to pull features in the
same class closer. Let L, LC, LS , LO be the total loss, cen-
ter loss, sofmax loss, and orientation loss, the total loss for
training is calculated as follows:

L = αLC + (1 − α)LS + βLO

(1)

where we set α = 0.5 to balance between intra-class (cen-
ter) and inter-class (sofmax) loss and β = 2 to emphasize
the importance of precision of minutiae orientation.

Parameter settings. As mentioned in Section 2.2.1, ﬁn-
gerprint patches are input to FineNet where the patch size
is 160 × 160. To ensure our network can handle distortion
in input image, we apply scaled augmentation [19], random
cropping, and brightness adjustment. Horizontal and verti-
cal ﬂip with pixel mean subtraction are also adopted. We
randomly initialize all variables by using a Gaussian distri-
bution N (0, 0.01). Batch size is set to 100. We use schedule
learning rate after particular iterations. Speciﬁcally, we set
it as 0.01 in the beginning and reduce 10 times after 50K
iterations. To prevent vanishing gradient problem, we set
the maximum epoch to 200K. We use 0.0004 as the value
for momentum and weight decay is 0.9.
3. Experimental results

We evaluate our method on two different datasets with
different characteristics under different settings of parame-
ters D and O (see Eq. (2)). We also visualize examples of
score maps with correct and incorrect minutiae extractions
in Figure 7. All experiments were implemented in Tensor-
ﬂow and ran on Nvidia GTX GeForce.
3.1. Datasets

We use FVC 2002 dataset [16] with data augmentation
consisting of 3, 200 plain ﬁngerprint images for training. To
compensate for the lack of a large scale dataset for training,
we distort the input images in x and y coordinates in the
spirit of hard-training with non-ideal input ﬁngerprint im-
ages. Furthermore, we also apply additive random noise to

Table 2. Comparison of different methods for minutiae extraction on FVC 2004 and NIST SD27 datasets. Note that [18, 21] reported their
results only on subsets of FVC 2004 and NIST SD27 as mentioned in Table 1. “ ” means the authors neither provided these results in their
paper nor made their code available. D and O are parameters deﬁned in Eq. (2).

Dataset

Methods

Setting 1 (D = 8, O = 10)

Setting 2 (D = 12, O = 20)

Setting 3 (D = 16, O = 30)

NIST SD27

FVC 2004

MINDTCT [24]
VeriFinger [23]
Gao et al. [5]
Sankaran et al. [18]
Tang et al. [21]
FingerNet [22]
Proposed method
MINDTCT [24]
VeriFinger [23]
Gao et al. [5]
FingerNet [22]
Proposed method

Precision Recall
14.7%
40.1%

8.3%
3.6%

F1 score
0.106
0.066

Precision Recall
16.4%
47.9%

10.0%
5.3%

F1 score
0.124
0.095

49.5%
53.2%
69.2% 67.7%
64.3%
30.8%
69.2%
39.8%

62.1%
68.7%
79.0% 80.1%

0.513
0.684
0.416
0.505

0.643
0.795

58.1%
58.0%
70.5% 72.3%
72.1%
37.7%
77.5%
45.6%

70.4%
72.9%
83.6% 83.9%

0.58
0.714
0.495
0.574

0.716
0.837

Precision Recall
18.9%
11.2%
58.3%
7.6%
8.7%
23.5%
63.1%
26.4%
53.4%
53.0%
63.2%
63.0%
71.2% 75.7%
79.8%
42.1%
81.9%
51.8%
82.7%
48.8%
80.0%
76.0%
85.9% 84.8%

F1 score
0.141
0.134
0.127
0.372
0.532
0.631
0.734
0.551
0.635
0.614
0.779
0.853

the input images. Thus, for training CoarseNet, we have
an augmented dataset of 8, 000 images. To obtain data for
training FineNet, we extract 45 × 45 pixel patches from
these 8K training images for CoarseNet whose center is a
ground truth minutia point. For non-minutiae patches, we
randomly extract patches with the criteria that the 10 × 10
center of each patch does not contain any minutia. Thus,
we collect around 100K minutia and non-minutia patches
for training FineNet.

As mentioned in Section 2.1.4, we use FingerNet [22]
to generate labels for domain knowledge groundtruth. Be-
sides, we manually correct segmentation grountruth results
from FingerNet to ensure better learning for CoarseNet.

3.2. Evaluation

To demonstrate the robustness of our framework, we
compare our results with published approaches on FVC
2004 [15]5 and NIST SD27 [6] datasets under different cri-
teria of distance and orientation thresholds. Let the tuples
(lp, op) and (lgt, ogt) be the location coordinates and ori-
entation values of predicted and ground truth minutia. The
predicted minutia is called true if it satisﬁes the following
constrains:

(cid:40)

(cid:107)lp − lgt(cid:107)2 ≤ D
(cid:107)op − ogt(cid:107)1 ≤ O

(2)

where D and O are the thresholds in pixels and degrees,
respectively. Speciﬁcally, we set the range of distances be-
tween detected and ground truth minutiaes from 8 to 16
pixels (in location) and 10 to 30 degree (in orientation)
with default threshold value (0.5). We choose these set-
tings to demonstrate the robust and precise results from the
proposed approach while published works degrade rather
quickly.

Table 2 shows

the precision and recall compar-
isons of different approaches to minutiae extraction.

5We obtained the groundtruth from [14]

Table 3. The importance of non-maximum suppression in our
framework. N M S and N M S∗ denote the non-maximum ap-
proaches of FingerNet and the proposed approach, respectively.

Conﬁguration
FingerNet + N M S
FingerNet + N M S∗
Proposed method + N M S
Proposed method + N M S∗

Precision Recall
63.2%
63.0%
65.4%
65.2%
69.4%
73.5%
71.2% 75.7%

F1 score
0.631
0.653
0.714
0.734

MINDTCT [24] is the open source NIST Biometric Image
Software. VeriFinger [23] is a commercial SDK for minu-
tiae extraction and matching. Since Gao et al. [5] did not
release their code in public domain, we report their results
on NIST SD27 and FVC 2004 database from [22]. Darlow
et al. [3] use only a subset of the FVC dataset for training
and the rest for testing, we do not include in our evaluation.
Table 2 shows that the proposed method outperforms
state-of-the-art techniques under all settings of parameters
(thresholds) D and O for both FVC 2004 and NIST SD27.
Our results also reveal that by using only rolled/plain ﬁn-
gerprint images for training, our framework can work pretty
well for detecting minutiae in latents.

Table 3 shows a comparison between using and not us-
ing our proposed non-maximum suppression method on the
NIST SD27 dataset with setting 3 in Table 2. Because non-
maximum suppression is a post processing step, it helps im-
prove precision, recall and F1 values.

To make a complete comparison (at all the operating
points) with published methods, we present the precision-
recall curves in Figure 6. The proposed approach surpasses
all published works on both FVC 2004 and NIST SD27
datasets.

Figure 7 shows the minutiae extraction results on both
FVC 2004 and NIST SD27 datasets with different qual-
ity images. Our framework works well in difﬁcult situa-
tions such as noisy background or dry ﬁngerprints. How-
ever, there are some cases where the proposed framework

Figure 6. Precision-Recall curves on FVC 2004 (left) and NIST SD27 (right) datasets with published approaches in Setting 3.

either misses the true minutiae or extracts spurious minu-
tiae. For the FVC 2004 dataset and rolled ﬁngerprints from
NIST SD27 dataset, we obtain results that are close to the
ground truth minutiae. However, some minutiae points are
wrongly detected (image a) because of the discontinuity of
ridges or missed detections (image c) because the location
of minutiae is near the ﬁngerprint edge. For the latent ﬁn-
gerprints from NIST SD27 dataset, besides the correctly ex-
tracted minutiae, the proposed method is sensitive to severe
background noise (image e) and poor latent ﬁngerprint qual-
ity (image g). The run time per image is around 1.5 seconds
for NIST SD27 and 1.2 seconds for FVC 2004 on Nvidia
GTX GeForce.

4. Conclusions

We have presented two network architectures for auto-
matic and robust ﬁngerprint minutiae extraction that fuse
ﬁngerprint domain knowledge and deep network represen-
tation:

- CoarseNet: an automatic robust minutiae extractor that
provides candidate minutiae location and orientation with-
out a hard threshold or ﬁne tuning.

- FineNet: a strong patch based classiﬁer that acceler-
ates the reliability of candidates from CoarseNet to get ﬁnal
results.

A non-maximum suppression is proposed as a post pro-
cessing step to boost the performance of the whole frame-
work. We also reveal the impact of residual learning on
minutiae extraction in the latent ﬁngerprint dataset despite
using only plain ﬁngerprint images for training. Our exper-
imental results show that the proposed framework is robust
and achieves superior performance in terms of precision,
recall and F1 values over published state-of-the-art on both
benchmark datasets, namely FVC 2004 and NIST SD27.

The proposed framework can be further improved by (i)
using larger training set for network training that includes
latent images, (ii) constructing context descriptor to exploit
the region surrounding minutiae, (iii) improving processing
time, and (iv) unifying minutiae extractor into an end-to-end

ﬁngerprint matching framework.

References

[1] K. Cao and A. K. Jain. Automated latent ﬁngerprint recognition.

arXiv preprint arXiv:1704.01925, 2017. 3

[2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille. Deeplab: Semantic image segmentation with deep convolu-
tional nets, atrous convolution, and fully connected crfs. IEEE Trans.
PAMI, 2017. 3

[3] L. Darlow and B. Rosman. Fingerprint minutiae extraction using

deep learning. In Proc. IEEE IJCB, 2017. 2, 3, 4, 6

[4] J. Feng. Combining minutiae descriptors for ﬁngerprint matching.

Pattern Recognition, 41(1):342–352, 2008. 1

[5] X. Gao, X. Chen, J. Cao, Z. Deng, C. Liu, and J. Feng. A novel
method of ﬁngerprint minutiae extraction based on Gabor phase. In
Proc. 17th IEEE ICIP, pages 3077–3080, 2010. 6

[6] M. D. Garris and R. M. McCabe. NIST special database 27: Fin-
gerprint minutiae from latent and matching tenprint images. NIST
Technical Report NISTIR, 6534, 2000. 1, 3, 6

[7] R. Girshick, F. Iandola, T. Darrell, and J. Malik. Deformable part
In Proc. IEEE CVPR,

models are convolutional neural networks.
pages 437–446, 2015. 4

[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for
image recognition. In Proc. IEEE CVPR, pages 770–778, 2016. 1,
2, 3

[9] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep
network training by reducing internal covariate shift. In Proc. ICML,
pages 448–456, 2015. 5

[10] A. Jain, L. Hong, and R. Bolle. On-line ﬁngerprint veriﬁcation. IEEE

Trans. PAMI, 19(4):302–314, 1997. 1, 2

[11] A. K. Jain, Y. Chen, and M. Demirkus. Pores and ridges: High-
resolution ﬁngerprint matching using level 3 features. IEEE Trans.
PAMI, 29(1):15–27, 2007. 1

[12] A. K. Jain, K. Nandakumar, and A. Ross. 50 years of biometric
research: Accomplishments, challenges, and opportunities. Pattern
Recognition Letters, 79:80–105, 2016. 1

[13] L. Jiang, T. Zhao, C. Bai, A. Yong, and M. Wu. A direct ﬁnger-
print minutiae extraction approach based on convolutional neural net-
works. In Proc. IEEE IJCNN, pages 571–578, 2016. 2, 5

[14] M. Kayaoglu, B. Topcu, and U. Uludag.

Standard ﬁngerprint
databases: Manual minutiae labeling and matcher performance anal-
yses. arXiv preprint arXiv:1305.1443, 2013. 6

[15] D. Maio, D. Maltoni, R. Cappelli, J. Wayman, and A. Jain.
FVC2004: Third ﬁngerprint veriﬁcation competition. In Biometric
Authentication, pages 31–35. Springer, 2004. 3, 6

Figure 7. Visualizing minutiae extraction results. From top to bottom in each column: score maps, and minutiae extraction results overlaid
on ﬁngerprint images. (a)-(b): two plain images from FVC 2004; (c)-(d): rolled (reference) ﬁngerprints from NIST SD27; (e)-(h): latent
ﬁngerprint images from NIST SD27.

[16] D. Maio, D. Maltoni, R. Cappelli, J. L. Wayman, and A. K. Jain.
FVC2002: Second ﬁngerprint veriﬁcation competition. In Proc. 16th
ICPR, volume 3, pages 811–814, 2002. 5

[17] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-
IEEE Trans.

time object detection with region proposal networks.
PAMI, 39(6):1137–1149, 2017. 4

[18] A. Sankaran, P. Pandey, M. Vatsa, and R. Singh. On latent ﬁngerprint
minutiae extraction using stacked denoising sparse autoencoders. In
Proc. IEEE IJCB, pages 1–7, 2014. 2, 3, 6

[19] K. Simonyan and A. Zisserman. Very deep convolutional networks
for large-scale image recognition. arXiv:1409.1556, 2014. 5

[20] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi.

Inception-v4,
inception-resnet and the impact of residual connections on learning.
In Proc. AAAI, pages 4278–4284, 2017. 1, 2, 5

[21] Y. Tang, F. Gao, and J. Feng. Latent ﬁngerprint minutia extraction
using fully convolutional network. In Proc. IEEE IJCB, 2017. 2, 3,
4, 6

[22] Y. Tang, F. Gao, J. Feng, and Y. Liu. Fingernet: An uniﬁed deep
In Proc. IEEE IJCB,

network for ﬁngerprint minutiae extraction.
2017. 2, 3, 4, 6

[23] Veriﬁnger. Neuro-technology, 2010. 6
[24] C. I. Watson, M. D. Garris, E. Tabassi, C. L. Wilson, R. M. McCabe,
S. Janet, and K. Ko. User’s guide to NIST biometric image software
(NBIS). NIST Interagency/Internal Report 7392, 2007. 6

[25] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative feature learn-
ing approach for deep face recognition. In Proc. ECCV, pages 499–
515. Springer, 2016. 5

[26] X. Yang, J. Feng, and J. Zhou. Localized dictionaries based ori-
entation ﬁeld estimation for latent ﬁngerprints. IEEE Trans. PAMI,
36(5):955–969, 2014. 4

[27] S. Yoon, J. Feng, and A. K. Jain. Latent ﬁngerprint enhancement via
robust orientation ﬁeld estimation. In Proc. IEEE IJCB, pages 1–8,
2011. 2

[28] F. Zhao and X. Tang.

Preprocessing and postprocessing for
skeleton-based ﬁngerprint minutiae extraction. Pattern Recognition,
40(4):1270–1281, 2007. 1

Robust Minutiae Extractor:
Integrating Deep Networks and Fingerprint Domain Knowledge

Dinh-Luan Nguyen, Kai Cao and Anil K. Jain
Michigan State University
East Lansing, Michigan, USA
nguye590@msu.edu, {kaicao,jain}@cse.msu.edu

7
1
0
2
 
c
e
D
 
6
2
 
 
]

V
C
.
s
c
[
 
 
1
v
1
0
4
9
0
.
2
1
7
1
:
v
i
X
r
a

Abstract

We propose a fully automatic minutiae extractor, called
MinutiaeNet, based on deep neural networks with com-
pact feature representation for fast comparison of minu-
tiae sets. Speciﬁcally, ﬁrst a network, called CoarseNet,
estimates the minutiae score map and minutiae orientation
based on convolutional neural network and ﬁngerprint do-
main knowledge (enhanced image, orientation ﬁeld, and
segmentation map). Subsequently, another network, called
FineNet, reﬁnes the candidate minutiae locations based on
score map. We demonstrate the effectiveness of using the
ﬁngerprint domain knowledge together with the deep net-
works. Experimental results on both latent (NIST SD27)
and plain (FVC 2004) public domain ﬁngerprint datasets
provide comprehensive empirical support for the merits of
our method. Further, our method ﬁnds minutiae sets that
are better in terms of precision and recall in comparison
with state-of-the-art on these two datasets. Given the lack of
annotated ﬁngerprint datasets with minutiae ground truth,
the proposed approach to robust minutiae detection will
be useful to train network-based ﬁngerprint matching al-
gorithms as well as for evaluating ﬁngerprint individual-
ity at scale. MinutiaeNet is implemented in Tensorﬂow:
https://github.com/luannd/MinutiaeNet

Figure 1. Minutiae detection by the proposed approach on two
latent ﬁngerprint images (#7 and #39) from the NIST SD27
dataset [6]. Left column: minutiae score maps obtained from the
latent images shown in the right column. Right column: minu-
tiae detected by the proposed framework (red) and ground truth
minutiae (blue) overlaid on the latent image.

1. Introduction

Automatic ﬁngerprint recognition is one of the most
the past 50
widely studied topic in biometrics over
years [12]. One of the main challenges in ﬁngerprint recog-
nition is to increase the recognition accuracy, especially
for latent ﬁngerprints. Fingerprint comparison is primar-
ily based on minutiae set comparison [28, 11]. A num-
ber of hand-crafted approaches [10, 28] have been used to
augment the minutiae with their attributes to improve the
recognition accuracy. However, robust automatic ﬁnger-
print minutiae extraction, particularly for noisy ﬁngerprint

images, continues to be a bottleneck in ﬁngerprint recogni-
tion systems.

With rapid developments and success of deep learning
techniques in a variety of applications in computer vision
and pattern recognition [8, 20], we are beginning to see
network-based approaches being proposed for ﬁngerprint
recognition. Still, the prevailing methods of minutiae ex-
traction primarily utilize ﬁngerprint domain knowledge and
handcrafted features. Typically, minutiae extraction and
matching involves pre-processing stages such as ridge ex-
traction and ridge thinning, followed by minutiae extrac-
tion [10, 4] and ﬁnally heuristics to deﬁne minutiae at-

Table 1. Published network-based approaches for automatic minutiae extraction.

Study

Sankaran et al.
[18]
Jiang et al.
[13]
Tang et al.
[21]
Darlow et al.
[3]
Tang et al.
[22]

Method

Training Data

Testing Data

Comments

Performance Evaluation

Sparse autoencoders
for classiﬁcation
A combination of
JudgeNet and LocateNet
Fully convolutional
neural network
Convolutional network
classiﬁer
Uniﬁed network with
domain knowledge

132, 560 plain ﬁngerprint images;
129 images from NIST SD27
200 live scan ﬁngerprints

4, 205 latent images from private
database and 129 latents from NIST SD27
6, 336 images from
FVC 2000, 2002, and 2004
8, 000 images from private
forensic latent database

129 remaining latents
from NIST SD27
100 images from
private database
129 latents
from NIST SD27
1, 584 images from
FVC 2000, 2002, and 2004
Set A of FVC 2004 and
NIST SD27

Sliding window; manual segmentation
of latent ﬁngerprints
Sliding window; hand-crafted dividing
regions; no minutiae orientation information
Hard thresholds to cut off candidate regions;
plain network
Sliding window; hard threshold for candidate regions
(minutiae); separately estimated minutiae orientation
Plain network; depends largely on the quality of
the enhancement and segmentation stages

Patch-based and minutia-based,
metric and matching performance(*)
Precision, recall,
and F1 score
Precision, recall, F1 score,
and matching performance
Equal error rate and
matching performance
Precision, recall, and
matching performance

Proposed
approach

Domain knowledge with Residual
learning based CoarseNet and
inception-resnet based FineNet

Precision, recall, and F1
score under different location
and orientation thresholds
(*) different matchers were used in different studies and none of them were state of the art, i.e. top performing latent or slap matchers identiﬁed in the

Residual network; automatic minutiae extractor
utilizing domain knowledge; robust patch based
minutiae classiﬁer

FVC 2002 with data augmentation
(8, 000 images in total)

FVC 2004 (3, 200 images) and
NIST SD27 (all 258 latents)

NIST evaluations. For this reason, we do not report matching performance because otherwise it would not be a fair comparison with previous studies.

tributes. While such an approach works well for good qual-
ity ﬁngerprint images, it provides inaccurate minutiae loca-
tion and orientation for poor quality rolled/plain prints and,
particularly for latent ﬁngerprints. To overcome the noise in
ﬁngerprint images, Yoon et al. [27] used Gabor ﬁltering to
calculate the reliability of extracted minutiae. Although this
approach can work better than [10], it also resulted in poor
results with highly noisy images. Because these prevailing
approaches are based on handcrafted methods or heuristics,
they are only able to extract basic (or low level) features1 of
images. We believe learning based approaches using deep
networks will have better ability to extract high level fea-
tures2 from low quality ﬁngerprint images.

In this paper, we present a novel framework that ex-
ploits useful domain knowledge coded in the deep neural
networks to overcome limitations of existing approaches to
minutiae extraction. Figure 1 visualizes results of the pro-
posed framework on two latent ﬁngerprints from the NIST
SD27 dataset.

Speciﬁcally, our proposed approach comprises of two

networks, called CoarseNet and FineNet:

- CoarseNet is a residual learning [8] based convolu-
tional neural network that takes a ﬁngerprint image as initial
input, and the corresponding enhanced image, segmentation
map, and orientation ﬁeld (computed by the early stages
of CoarseNet) as secondary input to generate the minutiae
score map. The minutiae orientation is also estimated by
comparing with the ﬁngerprint orientation.

- FineNet is a robust inception-resnet [20] based minu-
tiae classiﬁer. It processes each candidate patch, a square
region whose center is the candidate minutiae point, to re-
ﬁne the minutiae score map and approximate minutiae ori-
entation by regression. Final minutiae are the classiﬁcation
results.

Deep learning approach has been used by other re-
searchers for minutiae extraction (see Table 1). But,
our approach differs from published methods in the way
we encode ﬁngerprint domain knowledge in deep learn-

1Features such as edges, corners, etc.
2Abstract/semantic features retrieved from deep layers.

ing. Sankaran et al. [18] classiﬁed the minutiae and non-
minutiae patches by using sparse autoencoders. Jiang et
al. [13] introduced a combination of two networks: Jud-
geNet for classifying minutiae patches, and LocateNet for
locating precise minutiae location. While Jiang et al. use
neural networks, their approach is very time-consuming due
to use of sliding window to extract minutiae candidates. An-
other limitation of this approach is that it does not provide
minutiae orientation information.

Tang et al. [21] utilized the idea of object detection to
detect candidate minutiae patches, but it suffers from two
major weaknesses: (i) hard threshold to delete the candi-
date patches, and (ii) the same network is used for both
candidate generation and classiﬁcation. By using sliding
windows, Darlow et al. [3] fed each pixel of the input ﬁn-
gerprint to a convolutional neural network, called MENet,
to classify whether it corresponds to a minutia or not. It also
suffers from time-consuming sliding windows as in [13],
and separate modules for minutiae location and orientation
estimates. Tang et al. [22] proposed FingerNet that maps
traditional minutiae extraction pipeline including orienta-
tion estimation, segmentation, enhancement, and extraction
to a network with ﬁxed weights. Although this approach
is promising because it combines domain knowledge and
deep network, it still uses plain 3 network architecture and
hard threshold in non-maximum suppression 4. Finally, the
accuracy of FingerNet depends largely on the quality of the
enhanced and segmentation stage while ignoring texture in-
formation in the ridge pattern.

In summary, the published approaches suffer from using
sliding windows to process each pixel in input images, set-
ting hard threshold in post-processing step, and using plain
convolutional neural network to classify candidate regions.
Furthermore, the evaluation process in these studies is not
consistent in terms of deﬁning “correct” minutiae.

The contributions of our approach are as follows:
• A network-based automatic minutiae extractor utiliz-
ing domain knowledge is proposed to provide reli-

3A series of stacked layers.
4A post-processing algorithm that merges all detections belonging to

the same object.

Figure 2. Proposed automatic minutiae extraction architecture. While CoarseNet takes full ﬁngerprint image as input, FineNet processes
minutiae proposed patches output by CoarseNet.

able minutiae location and orientation without a hard
threshold or ﬁne tuning.

• A robust patch based minutiae classiﬁer that signif-
icantly boosts the precision and recall of candidate
patches. This can be used as a robust minutiae extrac-
tor with compact embedding of minutiae features.

• A non-maximum suppression is proposed to get precise
locations for candidate patches. Experimental evalu-
ations on FVC 2004 [15] and NIST SD27 [6] show
that the proposed approach is superior to published ap-
proaches in terms of precision, recall, and F1 score val-
ues.

2. Proposed framework

Our minutiae extraction framework has two modules:
(i) residual learning based convolutional neural network,
called CoarseNet that generates candidate patches contain-
ing minutiae from input ﬁngerprint image; (ii) inception-
resnet based network architecture, called FineNet which
is a strong minutiae classiﬁer that classiﬁes the candidate
patches output by CoarseNet. These two networks also pro-
vide minutiae location and orientation information as out-
puts. Figure 2 describes the complete network architecture
for automatic minutiae location and orientation for an in-
put ﬁngerprint image. Section 2.1 presents the architecture
of CoarseNet. In Section 2.2, we introduce FineNet with
details on training to make it a strong classiﬁer.

2.1. CoarseNet for minutiae extraction

We adopt the idea of combining domain knowledge and
deep representation of neural networks in [22] to boost the
minutiae detection accuracy. In essence, we utilize the au-
tomatically extracted segmentation map, enhanced image,
and orientation map as complementary information to the
input ﬁngerprint image. The goal of CoarseNet is not to

produce the segmentation map or enhanced image or orien-
tation map. They are just the byproducts of the network.
However, these byproducts as ﬁngerprint domain knowl-
edge must be reliable to get robust minutiae score map. Be-
cause Tang et al. [22] proposed an end-to-end uniﬁed net-
work that maps handcrafted features to network based ar-
chitecture, we use this as a baseline for our CoarseNet.

2.1.1 Segmentation and orientation feature sharing

Adding more layers in the deep network with the hope of
increasing accuracy might lead to the exploding or vanish-
ing gradients problem. From the success of residual learn-
ing [8], we use residual instead of just plain stacked con-
volutional layers in our network to make it more powerful.
Figure 3 shows the detailed architecture of the network.

Unlike existing works using plain convolutional neural
network [21, 22] or sliding window [18, 3] to process each
patch with ﬁxed size and stride, we use a deeper residual
learning based network with more pooling layers to scale
down the region patch. Speciﬁcally, we get the output after
the 2nd, 3rd, and 4th pooling layer to feed to an ASPP net-
work [2] with corresponding rates for multiscale segmen-
tation. This ensures the output has the same size as input
without a loss of information when upsampling the score
map.

By using four pooling layers, each pixel in the jth feature
map, called level j, corresponds to a region 2j × 2j in the
original input. Result layers at level 4 and 3 will be tested as
coarse estimates while the level 2 serves as ﬁne estimation.
Segmentation map. Image segmentation and ﬁngerprint
orientation estimation share the same convolutional layers.
Thus, by applying multi-level approach mentioned above,
we get probability maps of each level-corresponding region
in input image. For instance, to get ﬁner-detailed segmen-
tation for each region level jl, we continue to process prob-
ability map of region level jl/2.

Figure 3. CoarseNet architecture.

Orientation map. To get complete minutiae informa-
tion from context, we adopt the fusion idea of Cao et al. [1].
We fuse the results of Dictionary-based method [26] with
our orientation results from CoarseNet. Because [26] uses a
hand-crafted approach, we set the fusion weight ratio of its
output with our network-based approach as 1:3.

2.1.2 Candidate generation

The input ﬁngerprint image might contain large amounts of
noise. So, without using domain knowledge we may not be
able to identify prominent ﬁngerprint features. The domain
knowledge comprises of four things: raw input image, en-
hanced image, orientation map, and segmentation map. In
the Gabor image enhancement module, we take the average
of ﬁltered image and the orientation map for ridge ﬂow es-
timation. To emphasize texture information in ﬁngerprints,
we stack the original input image with the output of en-
hancement module to obtain the ﬁnal enhancement map. To
remove spurious noises, we apply segmentation map on the
enhancement map and use it as input to coarse minutiae ex-
tractor module.

To obtain the precise location of minutiae, each level of
residual net is fused to get the ﬁnal minutiae score map with
size h/16 × w/16, where h and w are the height and width
of the input image. Figure 4 shows the details of processing

score map. To reduce the processing time, we use score
map at level 4 as a coarse location. To get precise location,
lower level score maps are used.

2.1.3 Non-maximum suppression

Using non-maximum suppression to reduce the number of
candidates is common in object detection [7, 17]. Some of
the candidate regions are deleted to get a reliable minutiae
score map by setting a hard threshold [3] or using heuris-
tics [21, 22]. However, a hard threshold can also suppress
valid minutiae locations. A commonly used heuristics is to
sort the candidate scores in ascending order. The L2 dis-
tance between pairwise candidates is calculated with hard
thresholds for distance and orientation. By iteratively com-
paring each candidate with the rest in the candidate list, only
the candidate with higher score and score above the thresh-
olds is kept. However, this approach fails when two minu-
tiae are near each other and the inter-minutiae distance is
below the hard thresholds.

Since each score in the minutiae map corresponds to a
speciﬁc region in the input image, we propose to use the
intersection over union strategy. Speciﬁcally, after sorting
the scores of the candidate list, we keep high score candi-
dates while ignoring the lower scoring candidates with at
least 50% overlap with the candidates already selected.

2.1.4 Training data for CoarseNet

Given the lack of datasets with ground truth, we use the ap-
proach in Tang et al. [22] to generate weak labels for train-
ing the segmentation and orientation module. The coarse
minutiae extractor module uses minutiae location and minu-
tiae orientation ground truth provided in the two datasets.
We also use data augmentation techniques as mentioned in
Section 3.

2.2. FineNet

Figure 4. Candidate patch processing map. Lower the level of
score map, more detail it provides.

Extracting minutiae based on candidate patches is not
adequate. Although CoarseNet is reliable, it still fails to de-

tect true minutiae or detects spurious minutiae. This can
lead to poor performance in ﬁngerprint matching. This mo-
tivates our use of FineNet - a minutiae classiﬁer from gener-
ated candidate patches. FineNet takes candidates from the
output of CoarseNet as input to decide whether the region
10 × 10 in the center of the corresponding patch has a valid
minutia or not.

2.2.1 FineNet architecture

Figure 5 describes the architecture of FineNet. As men-
tioned in Section 2.1.1, we use the Inception-Resnet v1 ar-
chitecture as a core network in FineNet.

For FineNet training, we extract an equal number of
t1 × t1 sized minutiae and non-minutiae patches with t1 =
45. FineNet determines the whether the 10 × 10 pixel re-
gion in the center of each patch contains a valid minutia
or not. The candidate patches are resized into t2 × t2 pix-
els that feed to FineNet. Based on the observation that the
original input image size (without rescaling) is not large in
comparison with images for object classiﬁcation, too much
up scaling the image can cause blurring the tiny details, and
too small an input image size is not sufﬁcient for network
with complex architecture, we choose t2 = 160 pixels.

Training data for FineNet is extracted from the input gray
scale images where minutiae data are based on the ground
truth minutiae location and non-minutiae ones are from ran-

Figure 5. FineNet architecture. For details of the Inception-Resnet
v1 arichitecture block, we refer the readers to [20].

dom sampling with the center regions do not contain partial
or fully minutiae location. To make the network more ro-
bust, we use some small techniques like Batch Normaliza-
tion [9], rotation, ﬂipping, scaled augmentation [19], and
bounding blurring [13] as pre-processing stage.

2.2.2 Losses and implementation details

Intra-Inter class losses. Because input captured ﬁnger-
print image is not always in the ideal condition, it might
be easily affected by distortion, ﬁnger movement, or quality
(wet/dry) of ﬁnger. Thus, the variation of minutiae shapes
and surrounding ridges can affect the accuracy of the clas-
siﬁer. To handle this situation and make the FineNet more
robust with intra-class variations, we use Center Loss [25]
as a complementary of softmax loss and minutiae orienta-
tion loss. While softmax loss tends to push away features
of different classes, center loss tends to pull features in the
same class closer. Let L, LC, LS , LO be the total loss, cen-
ter loss, sofmax loss, and orientation loss, the total loss for
training is calculated as follows:

L = αLC + (1 − α)LS + βLO

(1)

where we set α = 0.5 to balance between intra-class (cen-
ter) and inter-class (sofmax) loss and β = 2 to emphasize
the importance of precision of minutiae orientation.

Parameter settings. As mentioned in Section 2.2.1, ﬁn-
gerprint patches are input to FineNet where the patch size
is 160 × 160. To ensure our network can handle distortion
in input image, we apply scaled augmentation [19], random
cropping, and brightness adjustment. Horizontal and verti-
cal ﬂip with pixel mean subtraction are also adopted. We
randomly initialize all variables by using a Gaussian distri-
bution N (0, 0.01). Batch size is set to 100. We use schedule
learning rate after particular iterations. Speciﬁcally, we set
it as 0.01 in the beginning and reduce 10 times after 50K
iterations. To prevent vanishing gradient problem, we set
the maximum epoch to 200K. We use 0.0004 as the value
for momentum and weight decay is 0.9.
3. Experimental results

We evaluate our method on two different datasets with
different characteristics under different settings of parame-
ters D and O (see Eq. (2)). We also visualize examples of
score maps with correct and incorrect minutiae extractions
in Figure 7. All experiments were implemented in Tensor-
ﬂow and ran on Nvidia GTX GeForce.
3.1. Datasets

We use FVC 2002 dataset [16] with data augmentation
consisting of 3, 200 plain ﬁngerprint images for training. To
compensate for the lack of a large scale dataset for training,
we distort the input images in x and y coordinates in the
spirit of hard-training with non-ideal input ﬁngerprint im-
ages. Furthermore, we also apply additive random noise to

Table 2. Comparison of different methods for minutiae extraction on FVC 2004 and NIST SD27 datasets. Note that [18, 21] reported their
results only on subsets of FVC 2004 and NIST SD27 as mentioned in Table 1. “ ” means the authors neither provided these results in their
paper nor made their code available. D and O are parameters deﬁned in Eq. (2).

Dataset

Methods

Setting 1 (D = 8, O = 10)

Setting 2 (D = 12, O = 20)

Setting 3 (D = 16, O = 30)

NIST SD27

FVC 2004

MINDTCT [24]
VeriFinger [23]
Gao et al. [5]
Sankaran et al. [18]
Tang et al. [21]
FingerNet [22]
Proposed method
MINDTCT [24]
VeriFinger [23]
Gao et al. [5]
FingerNet [22]
Proposed method

Precision Recall
14.7%
40.1%

8.3%
3.6%

F1 score
0.106
0.066

Precision Recall
16.4%
47.9%

10.0%
5.3%

F1 score
0.124
0.095

49.5%
53.2%
69.2% 67.7%
64.3%
30.8%
69.2%
39.8%

62.1%
68.7%
79.0% 80.1%

0.513
0.684
0.416
0.505

0.643
0.795

58.1%
58.0%
70.5% 72.3%
72.1%
37.7%
77.5%
45.6%

70.4%
72.9%
83.6% 83.9%

0.58
0.714
0.495
0.574

0.716
0.837

Precision Recall
18.9%
11.2%
58.3%
7.6%
8.7%
23.5%
63.1%
26.4%
53.4%
53.0%
63.2%
63.0%
71.2% 75.7%
79.8%
42.1%
81.9%
51.8%
82.7%
48.8%
80.0%
76.0%
85.9% 84.8%

F1 score
0.141
0.134
0.127
0.372
0.532
0.631
0.734
0.551
0.635
0.614
0.779
0.853

the input images. Thus, for training CoarseNet, we have
an augmented dataset of 8, 000 images. To obtain data for
training FineNet, we extract 45 × 45 pixel patches from
these 8K training images for CoarseNet whose center is a
ground truth minutia point. For non-minutiae patches, we
randomly extract patches with the criteria that the 10 × 10
center of each patch does not contain any minutia. Thus,
we collect around 100K minutia and non-minutia patches
for training FineNet.

As mentioned in Section 2.1.4, we use FingerNet [22]
to generate labels for domain knowledge groundtruth. Be-
sides, we manually correct segmentation grountruth results
from FingerNet to ensure better learning for CoarseNet.

3.2. Evaluation

To demonstrate the robustness of our framework, we
compare our results with published approaches on FVC
2004 [15]5 and NIST SD27 [6] datasets under different cri-
teria of distance and orientation thresholds. Let the tuples
(lp, op) and (lgt, ogt) be the location coordinates and ori-
entation values of predicted and ground truth minutia. The
predicted minutia is called true if it satisﬁes the following
constrains:

(cid:40)

(cid:107)lp − lgt(cid:107)2 ≤ D
(cid:107)op − ogt(cid:107)1 ≤ O

(2)

where D and O are the thresholds in pixels and degrees,
respectively. Speciﬁcally, we set the range of distances be-
tween detected and ground truth minutiaes from 8 to 16
pixels (in location) and 10 to 30 degree (in orientation)
with default threshold value (0.5). We choose these set-
tings to demonstrate the robust and precise results from the
proposed approach while published works degrade rather
quickly.

Table 2 shows

the precision and recall compar-
isons of different approaches to minutiae extraction.

5We obtained the groundtruth from [14]

Table 3. The importance of non-maximum suppression in our
framework. N M S and N M S∗ denote the non-maximum ap-
proaches of FingerNet and the proposed approach, respectively.

Conﬁguration
FingerNet + N M S
FingerNet + N M S∗
Proposed method + N M S
Proposed method + N M S∗

Precision Recall
63.2%
63.0%
65.4%
65.2%
69.4%
73.5%
71.2% 75.7%

F1 score
0.631
0.653
0.714
0.734

MINDTCT [24] is the open source NIST Biometric Image
Software. VeriFinger [23] is a commercial SDK for minu-
tiae extraction and matching. Since Gao et al. [5] did not
release their code in public domain, we report their results
on NIST SD27 and FVC 2004 database from [22]. Darlow
et al. [3] use only a subset of the FVC dataset for training
and the rest for testing, we do not include in our evaluation.
Table 2 shows that the proposed method outperforms
state-of-the-art techniques under all settings of parameters
(thresholds) D and O for both FVC 2004 and NIST SD27.
Our results also reveal that by using only rolled/plain ﬁn-
gerprint images for training, our framework can work pretty
well for detecting minutiae in latents.

Table 3 shows a comparison between using and not us-
ing our proposed non-maximum suppression method on the
NIST SD27 dataset with setting 3 in Table 2. Because non-
maximum suppression is a post processing step, it helps im-
prove precision, recall and F1 values.

To make a complete comparison (at all the operating
points) with published methods, we present the precision-
recall curves in Figure 6. The proposed approach surpasses
all published works on both FVC 2004 and NIST SD27
datasets.

Figure 7 shows the minutiae extraction results on both
FVC 2004 and NIST SD27 datasets with different qual-
ity images. Our framework works well in difﬁcult situa-
tions such as noisy background or dry ﬁngerprints. How-
ever, there are some cases where the proposed framework

Figure 6. Precision-Recall curves on FVC 2004 (left) and NIST SD27 (right) datasets with published approaches in Setting 3.

either misses the true minutiae or extracts spurious minu-
tiae. For the FVC 2004 dataset and rolled ﬁngerprints from
NIST SD27 dataset, we obtain results that are close to the
ground truth minutiae. However, some minutiae points are
wrongly detected (image a) because of the discontinuity of
ridges or missed detections (image c) because the location
of minutiae is near the ﬁngerprint edge. For the latent ﬁn-
gerprints from NIST SD27 dataset, besides the correctly ex-
tracted minutiae, the proposed method is sensitive to severe
background noise (image e) and poor latent ﬁngerprint qual-
ity (image g). The run time per image is around 1.5 seconds
for NIST SD27 and 1.2 seconds for FVC 2004 on Nvidia
GTX GeForce.

4. Conclusions

We have presented two network architectures for auto-
matic and robust ﬁngerprint minutiae extraction that fuse
ﬁngerprint domain knowledge and deep network represen-
tation:

- CoarseNet: an automatic robust minutiae extractor that
provides candidate minutiae location and orientation with-
out a hard threshold or ﬁne tuning.

- FineNet: a strong patch based classiﬁer that acceler-
ates the reliability of candidates from CoarseNet to get ﬁnal
results.

A non-maximum suppression is proposed as a post pro-
cessing step to boost the performance of the whole frame-
work. We also reveal the impact of residual learning on
minutiae extraction in the latent ﬁngerprint dataset despite
using only plain ﬁngerprint images for training. Our exper-
imental results show that the proposed framework is robust
and achieves superior performance in terms of precision,
recall and F1 values over published state-of-the-art on both
benchmark datasets, namely FVC 2004 and NIST SD27.

The proposed framework can be further improved by (i)
using larger training set for network training that includes
latent images, (ii) constructing context descriptor to exploit
the region surrounding minutiae, (iii) improving processing
time, and (iv) unifying minutiae extractor into an end-to-end

ﬁngerprint matching framework.

References

[1] K. Cao and A. K. Jain. Automated latent ﬁngerprint recognition.

arXiv preprint arXiv:1704.01925, 2017. 3

[2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille. Deeplab: Semantic image segmentation with deep convolu-
tional nets, atrous convolution, and fully connected crfs. IEEE Trans.
PAMI, 2017. 3

[3] L. Darlow and B. Rosman. Fingerprint minutiae extraction using

deep learning. In Proc. IEEE IJCB, 2017. 2, 3, 4, 6

[4] J. Feng. Combining minutiae descriptors for ﬁngerprint matching.

Pattern Recognition, 41(1):342–352, 2008. 1

[5] X. Gao, X. Chen, J. Cao, Z. Deng, C. Liu, and J. Feng. A novel
method of ﬁngerprint minutiae extraction based on Gabor phase. In
Proc. 17th IEEE ICIP, pages 3077–3080, 2010. 6

[6] M. D. Garris and R. M. McCabe. NIST special database 27: Fin-
gerprint minutiae from latent and matching tenprint images. NIST
Technical Report NISTIR, 6534, 2000. 1, 3, 6

[7] R. Girshick, F. Iandola, T. Darrell, and J. Malik. Deformable part
In Proc. IEEE CVPR,

models are convolutional neural networks.
pages 437–446, 2015. 4

[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for
image recognition. In Proc. IEEE CVPR, pages 770–778, 2016. 1,
2, 3

[9] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep
network training by reducing internal covariate shift. In Proc. ICML,
pages 448–456, 2015. 5

[10] A. Jain, L. Hong, and R. Bolle. On-line ﬁngerprint veriﬁcation. IEEE

Trans. PAMI, 19(4):302–314, 1997. 1, 2

[11] A. K. Jain, Y. Chen, and M. Demirkus. Pores and ridges: High-
resolution ﬁngerprint matching using level 3 features. IEEE Trans.
PAMI, 29(1):15–27, 2007. 1

[12] A. K. Jain, K. Nandakumar, and A. Ross. 50 years of biometric
research: Accomplishments, challenges, and opportunities. Pattern
Recognition Letters, 79:80–105, 2016. 1

[13] L. Jiang, T. Zhao, C. Bai, A. Yong, and M. Wu. A direct ﬁnger-
print minutiae extraction approach based on convolutional neural net-
works. In Proc. IEEE IJCNN, pages 571–578, 2016. 2, 5

[14] M. Kayaoglu, B. Topcu, and U. Uludag.

Standard ﬁngerprint
databases: Manual minutiae labeling and matcher performance anal-
yses. arXiv preprint arXiv:1305.1443, 2013. 6

[15] D. Maio, D. Maltoni, R. Cappelli, J. Wayman, and A. Jain.
FVC2004: Third ﬁngerprint veriﬁcation competition. In Biometric
Authentication, pages 31–35. Springer, 2004. 3, 6

Figure 7. Visualizing minutiae extraction results. From top to bottom in each column: score maps, and minutiae extraction results overlaid
on ﬁngerprint images. (a)-(b): two plain images from FVC 2004; (c)-(d): rolled (reference) ﬁngerprints from NIST SD27; (e)-(h): latent
ﬁngerprint images from NIST SD27.

[16] D. Maio, D. Maltoni, R. Cappelli, J. L. Wayman, and A. K. Jain.
FVC2002: Second ﬁngerprint veriﬁcation competition. In Proc. 16th
ICPR, volume 3, pages 811–814, 2002. 5

[17] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-
IEEE Trans.

time object detection with region proposal networks.
PAMI, 39(6):1137–1149, 2017. 4

[18] A. Sankaran, P. Pandey, M. Vatsa, and R. Singh. On latent ﬁngerprint
minutiae extraction using stacked denoising sparse autoencoders. In
Proc. IEEE IJCB, pages 1–7, 2014. 2, 3, 6

[19] K. Simonyan and A. Zisserman. Very deep convolutional networks
for large-scale image recognition. arXiv:1409.1556, 2014. 5

[20] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi.

Inception-v4,
inception-resnet and the impact of residual connections on learning.
In Proc. AAAI, pages 4278–4284, 2017. 1, 2, 5

[21] Y. Tang, F. Gao, and J. Feng. Latent ﬁngerprint minutia extraction
using fully convolutional network. In Proc. IEEE IJCB, 2017. 2, 3,
4, 6

[22] Y. Tang, F. Gao, J. Feng, and Y. Liu. Fingernet: An uniﬁed deep
In Proc. IEEE IJCB,

network for ﬁngerprint minutiae extraction.
2017. 2, 3, 4, 6

[23] Veriﬁnger. Neuro-technology, 2010. 6
[24] C. I. Watson, M. D. Garris, E. Tabassi, C. L. Wilson, R. M. McCabe,
S. Janet, and K. Ko. User’s guide to NIST biometric image software
(NBIS). NIST Interagency/Internal Report 7392, 2007. 6

[25] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative feature learn-
ing approach for deep face recognition. In Proc. ECCV, pages 499–
515. Springer, 2016. 5

[26] X. Yang, J. Feng, and J. Zhou. Localized dictionaries based ori-
entation ﬁeld estimation for latent ﬁngerprints. IEEE Trans. PAMI,
36(5):955–969, 2014. 4

[27] S. Yoon, J. Feng, and A. K. Jain. Latent ﬁngerprint enhancement via
robust orientation ﬁeld estimation. In Proc. IEEE IJCB, pages 1–8,
2011. 2

[28] F. Zhao and X. Tang.

Preprocessing and postprocessing for
skeleton-based ﬁngerprint minutiae extraction. Pattern Recognition,
40(4):1270–1281, 2007. 1

