7
1
0
2
 
t
c
O
 
5
2
 
 
]
E
N
.
s
c
[
 
 
2
v
2
3
4
7
0
.
9
0
7
1
:
v
i
X
r
a

DYNAMIC EVALUATION OF NEURAL SEQUENCE MODELS

Ben Krause, Emmanuel Kahembwe, Iain Murray, & Steve Renals
School of Informatics, University of Edinburgh
Edinburgh, Scotland, UK
ben.krause,e.kahembwe,i.murray,s.renals@ed.ac.uk

ABSTRACT

We present methodology for using dynamic evaluation to improve neural sequence
models. Models are adapted to recent history via a gradient descent based mecha-
nism, causing them to assign higher probabilities to re-occurring sequential patterns.
Dynamic evaluation outperforms existing adaptation approaches in our compar-
isons. Dynamic evaluation improves the state-of-the-art word-level perplexities
on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and
the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize
datasets to 1.19 bits/char and 1.08 bits/char respectively.

1

INTRODUCTION

Sequence generation and prediction tasks span many modes of data, ranging from audio and language
modelling, to more general timeseries prediction tasks. Applications of such models include speech
recognition, machine translation, dialogue generation, speech synthesis, forecasting, and music
generation, among others. Neural networks can be applied to these tasks by predicting sequence
elements one-by-one, conditioning on the history of sequence elements, forming an autoregressive
model. Convolutional neural networks (CNNs) and recurrent neural networks (RNNs), including
long-short term memory (LSTM) networks (Hochreiter & Schmidhuber, 1997) in particular, have
achieved many successes at these tasks. However, in their basic form, these models have a limited
ability to adapt to recently observed parts of a sequence.

Many sequences contain repetition; a pattern that occurs once is more likely to occur again. For
instance, a word that occurs once in a document is much more likely to occur again. A sequence of
handwriting will generally stay in the same handwriting style. A sequence of speech will generally
stay in the same voice. Although RNNs have a hidden state that can summarize the recent past, they
are often unable to exploit new patterns that occur repeatedly in a test sequence.

This paper concerns dynamic evaluation, which we investigate as a candidate solution to this problem.
Our approach adapts models to recent sequences using gradient descent based mechanisms. We show
several ways to improve on past dynamic evaluation approaches in Section 5, and use our improved
methodology to achieve state-of-the-art results in Section 7.1 and Section 7.2. In Section 6 we design
a method to dramatically to reduce the number of adaptation parameters in dynamic evaluation,
making it practical in a wider range of situations. In Section 7.3 we analyse dynamic evaluation’s
performance over varying time-scales and distribution shifts, and demonstrate that dynamically
evaluated models can generate conditional samples that repeat many patterns from the conditioning
data.

2 MOTIVATION

Generative models can assign probabilities to sequences by modelling each term in the factorization
given by the product rule. The probability of a sequence x1:T = {x1, . . . , xT } factorizes as

P (x1:T ) = P (x1)P (x2|x1)P (x3|x2, x1) · · · P (xT |x1 . . . xT −1).

(1)

Methods that apply this factorization either use a ﬁxed context when predicting P (xt|x1:t−1), for
instance as in N-grams or CNNs, or use a recurrent hidden state to summarize the context, as in an

1

1:T , x2

1:T , ..., xn

1:T }, each sequence xi

RNN. However, for longer sequences, the history x1:t−1 often contains re-occurring patterns that are
difﬁcult to capture using models with ﬁxed parameters (static models).
In many domains, in a dataset of sequences {x1
1:T is generated
1:T ). At any point in time t, the history of a sequence xi
from a slightly different distribution P (xi
1:t−1
contains useful information about the generating distribution for that speciﬁc sequence P (xi
1:T ).
Therefore adapting the model parameters learned during training θg is justiﬁed. We aim to infer a set
1:t−1 that will better approximate P (xi
of model parameters θl from xi
Many sequence modelling tasks are characterised by sequences generated from slightly different
distributions as in the scenario described above. The generating distribution may also change
continuously across a single sequence; for instance, a text excerpt may change topic. Furthermore,
many machine learning benchmarks do not distinguish between sequence boundaries, and concatenate
all sequences into one continuous sequence. Thus, many sequence modelling tasks could be seen as
having a local distribution Pl(x) as well as a global distribution Pg(x) := (cid:82) P (l)Pl(x) dl. During
training time, the goal is to ﬁnd the best ﬁxed model possible for Pg(x). However, during evaluation
time, a model that can infer the current Pl(x) from the recent history has an advantage.

1:t−1) within sequence i.

t|xi

3 DYNAMIC EVALUATION

Dynamic evaluation methods continuously adapt the model parameters θg, learned at training time,
to parts of a sequence during evaluation. The goal is to learn adapted parameters θl that provide a
better model of the local sequence distribution, Pl(x). When dynamic evaluation is applied in the
present work, a long test sequence x1:T is divided up into shorter sequences of length n. We deﬁne
s1:M to be a sequence of shorter sequence segments si

s1:M = {s1 = x1:n, s2 = xn+1:2n, s3 = x2n+1:3n, ..., sM }.

(2)

The initial adapted parameters θ0
l are set to θg, and used to compute the probability of the ﬁrst
segment, P (s1|θ0
l ). This probability gives a cross entropy loss L(s1), with gradient ∇L(s1), which
is computed using truncated back-propagation through time (Werbos, 1990). The gradient ∇L(s1) is
used to update the model, resulting in adapted parameters θ1
l ). The same
procedure is then repeated for s2, and for each si in the sequence as shown in Figure 1. Gradients
for each loss L(si) are only backpropagated to the beginning of si, so computation is linear in the
sequence length. Each update applies one maximum likelihood training step to approximate the
current local distribution Pl(x). The computational cost of dynamic evaluation is one forward pass
and one gradient computation through the data, with some slight overhead to apply the update rule
for every sequence segment.

l , before evaluating P (s2|θ1

As in all autoregressive models, dynamic evaluation only conditions on sequence elements that it has
already predicted, and so evaluates a valid log-probability for each sequence. Dynamic evaluation can
also be used while generating sequences. In this case, the model generates each sequence segment
si using ﬁxed weights, and performs a gradient descent based update step on L(si). Applying
dynamic evaluation for sequence generation could result in generated sequences with more consistent
regularities, meaning that patterns that occur in the generated sequence are more likely to occur again.

4 BACKGROUND

4.1 RELATED APPROACHES

Adaptive language modelling was ﬁrst considered for n-grams, adapting to recent history via caching
(Jelinek et al., 1991; Kuhn, 1988), and other methods Bellegarda (2004). More recently, the neural
cache approach (Grave et al., 2017) and the closely related pointer sentinel-LSTM (Merity et al.,
2017b) have been used to for adaptive neural language modelling. Neural caching has recently been
used to improve the state-of-the-art at word-level language modelling (Merity et al., 2017a).

The neural cache model learns a type of non-parametric output layer on the ﬂy at test time, which
allows the network to adapt to recent observations. Each past hidden state hi is paired with the next
input xi+1, and is stored as a tuple (hi, xi+1). When a new hidden state ht is observed, the output
probabilities are adjusted to give a higher weight to output words that coincided with past hidden

2

Figure 1: Illustration of dynamic evaluation. The model evaluates the probability of sequence
segments si. The gradient ∇L(si) with respect to the log probability of si is used to update the model
parameters θi−1
l before the model progresses to the next sequence segment. Dashed edges are
what distinguish dynamic evaluation from static (normal) evaluation.

to θi

l

states with a large inner product (hT

t hi).

Pcache(xt+1|x1:t, h1:t) ∝

e(xi+1) exp(ωhT

t hi),

(3)

t−1
(cid:88)

i=1

where e(xi+1) is a one hot encoding of xi+1, and ω is a scaling parameter. The cache probabilities are
interpolated with the base network probabilities to adapt the base network at test time.

The neural cache closely relates to dynamic evaluation, as both methods can be added on top of a base
model for adaptation at test time. The main difference is the mechanism used to ﬁt to recent history:
the neural cache approach uses a non-parametric, nearest neighbours-like method, whereas dynamic
evaluation uses a gradient descent based method to change model parameters dynamically. Both
methods rely on an autoregressive factorisation, as they depend on observing sequence elements after
they are predicted in order to perform adaptation. Dynamic evaluation and neural caching methods
are therefore both applicable to sequence prediction and generation tasks, but not directly to more
general supervised learning tasks.

One drawback of the neural cache method is that it cannot adjust the recurrent hidden state dynamics.
As a result, the neural cache’s ability to capture information that occurs jointly between successive
sequence elements is limited. This capability is critical for adapting to sequences where each element
has very little independent meaning, e.g. character level language modelling.

4.2 DYNAMIC EVALUATION IN NEURAL NETWORKS

Dynamic evaluation of neural language models was proposed by Mikolov et al. (2010). Their
approach simply used stochastic gradient descent (SGD) updates at every time step, computing the
gradient with fully truncated backpropagation through time, which is equivalent to setting n = 1

3

in equation (2). Dynamic evaluation has since been applied to character and word-level language
models (Graves, 2013; Krause et al., 2017; Ororbia II et al., 2017; Fortunato et al., 2017). Previous
work using dynamic evaluation considered it as an aside, and did not explore it in depth.

5 UPDATE RULE METHODOLOGY FOR DYNAMIC EVALUATION

We propose several changes to Mikolov et al. (2010)’s dynamic evaluation method with SGD and
fully truncated backpropagation, which we refer to as traditional dynamic evaluation. The ﬁrst
modiﬁcation reduces the update frequency, so that gradients are backpropagated over more timesteps.
This change provides more accurate gradient information, and also improves the computational
efﬁciency of dynamic evaluation, since the update rule is applied much less often. We use sequence
segments of length 5 for word-level tasks and 20 for character-level tasks.

Next, we add a global decay prior to bias the model towards the parameters θg learned during
training. Our motivation for dynamic evaluation assumes that the local generating distribution Pl(x)
is constantly changing, so it is potentially desirable to weight recent sequence history higher in
adaptation. Adding a global decay prior accomplishes this by causing previous adaptation updates to
decay exponentially over time. For SGD with a global prior, learning rate η and decay rate λ; we
form the update rule

θi ← θi−1 − η∇L(si) + λ(θg − θi−1

).

l

We then consider using an RMSprop (Tieleman & Hinton, 2012) derived update rule for the learning
rule in place of SGD. RMSprop uses a moving average of recent squared gradients to scale learning
rates for each weight. In dynamic evaluation, near the start of a test sequence, RMSprop has had very
few gradients to average, and therefore may not be able to leverage its updates as effectively. For this
reason, we collect mean squared gradients, MSg, on the training data rather than on recent test data
(which is what RMSprop would do). MSg is given by
Nb(cid:88)

MSg =

(∇Lk)2,

1
Nb

k=1

where Nb is the number of training batches and ∇Lk is the gradient on the kth training batch. The
mini-batch size for this computation becomes a hyper-parameter, as larger mini-batches will result
in smaller mean squared gradients. The update rule, which we call RMS with a global prior in our
experiments, is then

l − η

l ← θi−1
θi

∇L(si)
(cid:112)MSg + (cid:15)
where (cid:15) is a stabilization parameter. For the decay step of our update rule, we also consider scaling
the decay rate for each parameter proportionally to (cid:112)MSg. Parameters with a high RMS gradient
affect the dynamics of the network more, so it makes sense to decay them faster. RMSnorm is (cid:112)MSg
divided by its mean, resulting in a normalized version of (cid:112)MSg with a mean of 1:

+ λ(θg − θi−1

(6)

),

l

RMSnorm =

(cid:112)MSg
avg((cid:112)MSg)

.

We clip the values of RMSnorm to be no greater than 1/λ to be sure that the decay rate does not
exceed 1 for any parameter. Combining the learning component and the regularization component
results in the ﬁnal update equation, which we refer to as RMS with an RMS global prior

l ← θi−1
θi

l − η

∇L(si)
(cid:112)MSg + (cid:15)

+ λ(θg − θi−1

l

) (cid:12) RMSnorm.

6 SPARSE DYNAMIC EVALUATION

Mini-batching over sequences is desirable for some test-time sequence modelling applications because
it allows faster processing of multiple sequences in parallel. Dynamic evaluation has a high memory

(4)

(5)

(7)

(8)

4

cost for mini-batching because it is necessary to store a different set of parameters for each sequence
in the mini-batch. Therefore, we consider a sparse dynamic evaluation variant that updates a smaller
number of parameters. We introduce a new adaptation matrix M which is initialized to zeros. M
multiplies hidden state vector ht of an RNN at every time-step to get a new hidden state h(cid:48)

t, via

h(cid:48)
t = ht + Mht.
h(cid:48)
t then replaces ht and is propagated throughout the network via both recurrent and feed-forward
connections. Applying dynamic evaluation to M avoids the need to apply dynamic evaluation to
the original parameters of the network, reduces the number of adaptation parameters, and makes
mini-batching less memory intensive. We reduce the number of adaptation parameters further by
only using M to transform an arbitrary subset of H hidden units. This results in M being an H ×H
matrix with d = H 2 adaptation parameters. If H is chosen to be much less than the number of hidden
units, this reduces the number of adaptation parameters dramatically. In Section 7.2 we experiment
with sparse dynamic evaluation for character-level language models.

(9)

7 EXPERIMENTS

We applied dynamic evaluation to word-level and character-level language modelling. In all tasks,
we evaluate dynamic evaluation on top of a base model. After training the base model, we tune
hyper-parameters for dynamic evaluation on the validation set, and evaluate both the static and
dynamic versions of the model on the test set. We also consider follow up experiments that analyse
the sequence lengths for which dynamic evaluation is useful. Code for our dynamic evaluation
methodology is available1.

7.1 WORD-LEVEL LANGUAGE MODELLING

We train base models on the Penn Treebank (PTB, Marcus et al., 1993) and WikiText-2 (Merity et al.,
2017b) datasets, and compare the performance of static and dynamic evaluation. These experiments
compare dynamic evaluation against past approaches such as the neural cache and measure dynamic
evaluation’s general performance across different models and datasets.

PTB is derived from articles of the Wall Street Journal. It contains 929k training tokens and a vocab
size limited to 10k words. It is one of the most commonly used benchmarks in language modelling.
We consider two baseline models on PTB, a standard LSTM implementation with recurrent dropout
(Zaremba et al., 2014), and the recent state-of-the-art AWD-LSTM (Merity et al., 2017a).

Our standard LSTM was taken from the Chainer tutorial on language modelling2, and used two
LSTM layers with 650 units each, trained with SGD and regularized with recurrent dropout. On
our standard LSTM, we experiment with traditional dynamic evaluation as applied by Mikolov et al.
(2010), as well as each modiﬁcation we make building up to our ﬁnal update rule as described in
Section 5. As our ﬁnal update rule (RMS + RMS global prior) worked best, we use this for all other
experiments and use “dynamic eval” by default to refer to this update rule in tables.

We applied dynamic evaluation on a more powerful model, the ASGD weight-dropped LSTM
(AWD-LSTM, Merity et al., 2017a). The AWD-LSTM is a vanilla LSTM that combines the use of
drop-connect (Wan et al., 2013) on recurrent weights for regularization, and a variant of averaged
stochastic gradient descent (Polyak & Juditsky, 1992) for optimisation. Our model, which used 3
layers and tied input and output embeddings (Press & Wolf, 2017; Inan et al., 2017), was intended to
be a direct replication of AWD-LSTM, using code from their implementation3. Results are given in
Table 1.

Dynamic evaluation gives signiﬁcant overall improvements to both models on this dataset. Dynamic
evaluation also achieves better ﬁnal results than the neural cache on both a standard LSTM and the
AWD-LSTM reimplementation, and improves the state-of-the-art on PTB.

WikiText-2 is roughly twice the size of PTB, with 2 million training tokens and a vocab size of 33k.
It features articles in a non-shufﬂed order, with dependencies across articles that adaptive methods

1https://github.com/benkrause/dynamic-evaluation
2https://github.com/chainer/chainer/tree/master/examples/ptb
3https://github.com/salesforce/awd-lstm-lm

5

model

parameters

valid

test

RNN+LDA+kN-5+cache (Mikolov & Zweig, 2012)
CharCNN (Kim et al., 2016)
LSTM (Zaremba et al., 2014)
Variational LSTM (Gal & Ghahramani, 2016)
Pointer sentinel-LSTM (Merity et al., 2017b)
Variational LSTM + augmented loss (Inan et al., 2017)
Variational RHN (Zilly et al., 2017)
NAS cell (Zoph & Le, 2017)
Variational LSTM + gradual learning (Aharoni et al., 2017)
LSTM + BB tuning (Melis et al., 2017)

LSTM (Grave et al., 2017)
LSTM + neural cache (Grave et al., 2017)
LSTM (ours)
LSTM + traditional dynamic eval (sgd, bptt=1)
LSTM + dynamic eval (sgd, bptt=5)
LSTM + dynamic eval (sgd, bptt=5, global prior)
LSTM + dynamic eval (RMS, bptt=5, global prior)
LSTM + dynamic eval (RMS, bptt=5, RMS global prior)

AWD-LSTM (Merity et al., 2017a)
AWD-LSTM +neural cache (Merity et al., 2017a)
AWD-LSTM (ours)
AWD-LSTM + dynamic eval

19M
66M
66M
21M
51M
23M
54M
105M
24M

20M
20M
20M
20M
20M
20M

24M
24M
24M
24M

92.0
78.9
78.4
73.4
70.9
68.5
65.4
62.4
61.7
58.3

82.3
72.1
85.6
76.2
75.6
74.8
72.2
71.7

57.3
52.8
57.7
51.1

82.2

72.4
71.1
67.9

60.9

86.9
74.6
88.0
78.6
78.0
77.4
74.3
73.5

60.0
53.9
59.8
51.6

Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.

model

parameters

valid

Byte mLSTM (Krause et al., 2016)
Variational LSTM (Inan et al., 2017)
Pointer sentinel-LSTM (Merity et al., 2017b)
LSTM + BB tuning (Melis et al., 2017)

LSTM (Grave et al., 2017)
LSTM + neural cache (Grave et al., 2017)
LSTM (ours)
LSTM + dynamic eval

AWD-LSTM (Merity et al., 2017a)
AWD-LSTM + neural cache (Merity et al., 2017a)
AWD-LSTM (ours)
AWD-LSTM + dynamic eval

46M
28M

24M

50M
50M

33M
33M
33M
33M

Table 2: WikiText-2 perplexities.

test

88.8
87.0
80.8
65.9

99.3
68.9
103.4
59.8

65.8
52.0
66.1
44.3

92.8
91.5
84.8
69.1

104.2
72.1
109.1
63.7

68.6
53.8
68.9
46.4

should be able to exploit. For this dataset, we use the same baseline LSTM implementation and
AWD-LSTM re-implementation as on PTB. Results are given in Table 2.

Dynamic evaluation improves the state-of-the-art perplexity on WikiText-2, and provides a signiﬁ-
cantly greater improvement than neural caching to both base models. This suggests that dynamic
evaluation is effective at exploiting regularities that co-occur across non-shufﬂed documents.

6

model

parameters

test

Stacked LSTM (Graves, 2013)
Stacked LSTM + traditional dynamic eval (Graves, 2013)
Multiplicative integration LSTM (Wu et al., 2016)
HyperLSTM (Ha et al., 2017)
Hierarchical multiscale LSTM (Chung et al., 2017)
Bytenet decoder (Kalchbrenner et al., 2016)
LSTM + BB tuning (Melis et al., 2017)
Recurrent highway networks (Zilly et al., 2017)
Fast-slow LSTM (Mujika et al., 2017)

mLSTM (Krause et al., 2016)
mLSTM + sparse dynamic eval (d = 250k)
mLSTM + dynamic eval

21M
21M
17M
27M

46M
46M
47M

46M
46M
46M

1.67
1.33
1.44
1.34
1.32
1.31
1.30
1.27
1.25

1.24
1.13
1.08

Table 3: Hutter Prize test set error in bits/char.

model

parameters

test

Multiplicative RNN (Mikolov et al., 2012)
Multiplicative integration LSTM (Wu et al., 2016)
LSTM (Cooijmans et al., 2017)
Batch normalised LSTM (Cooijmans et al., 2017)
Hierarchical multiscale LSTM (Chung et al., 2017)
Recurrent highway networks (Zilly et al., 2017)

mLSTM (Krause et al., 2016)
mLSTM + dynamic eval

5M
4M

45M

45M
45M

1.54
1.44
1.43
1.36
1.29
1.27

1.27
1.19

Table 4: text8 test set error in bits/char.

7.2 CHARACTER-LEVEL LANGUAGE MODELLING

We consider dynamic evaluation on the text84, and Hutter Prize (Hutter, 2006) datasets. The Hutter
Prize dataset is comprised of Wikipedia text, and includes XML and characters from non-Latin
languages. It is 100 million UTF-8 bytes long and contains 205 unique bytes. Similarly to other
reported results, we use a 90-5-5 split for training, validation, and testing. The text8 dataset is also
derived from Wikipedia text, but has all XML removed, and is lower cased to only have 26 characters
of English text plus spaces. As with Hutter Prize, we use the standard 90-5-5 split for training,
validation, and testing for text8. We used a multiplicative LSTM (mLSTM) (Krause et al., 2016)5 as
our base model for both datasets. The mLSTMs for both tasks used 2800 hidden units, an embedding
layer of 400 units, weight normalization (Salimans & Kingma, 2016), variational dropout (Gal &
Ghahramani, 2016), and ADAM (Kingma & Ba, 2014) for training.

We also consider sparse dynamic evaluation, as described in Section 6, on the Hutter Prize dataset.
For sparse dynamic evaluation, we adapted a subset of 500 hidden units, resulting in a 500×500
adaptation matrix and 250k adaptation parameters. All of our dynamic evaluation results in this
section use the ﬁnal update rule given in Section 5. Results for Hutter Prize are given in Table 3, and
results for text8 are given in Table 4.

Dynamic evaluation achieves large improvements to our base models and state-of-the-art results on
both datasets. Sparse dynamic evaluation also achieves signiﬁcant improvements on Hutter Prize
using only 0.5% of the adaptation parameters of regular dynamic evaluation.

4http://mattmahoney.net/dc/textdata
5https://github.com/benkrause/mLSTM

7

(a) Hutter data

(b) Spanish data

Figure 2: Average losses in bits/char of dynamic evaluation and static evaluation plotted against
number of characters processed; on sequences from the Hutter Prize test set (left) and European
Parliament dataset in Spanish (right), averaged over 500 trials for each. Losses at each data point are
averaged over sequence segments of length 100, and are not cumulative. Note the different y-axis
scales in the two plots.

7.3 TIME-SCALES OF DYNAMIC EVALUATION

We measure time-scales at which dynamic evaluation gains an advantage over static evaluation.
Starting from the model trained on Hutter Prize, we plot the performance of static and dynamic
evaluation against the number of characters processed on sequences from the Hutter Prize test set,
and sequences in Spanish from the European Parliament dataset (Koehn, 2005).

The Hutter Prize data experiments show the timescales at which dynamic evaluation gained the
advantage observed in Table 3. We divided the Hutter Prize test set into 500 sequences of length
10000, and applied static and dynamic evaluation to these sequences using the same model and
methodology used to obtain results in Table 3. Losses were averaged across these 500 sequences to
obtain average losses at each time step. Plots of the average cross-entropy errors against the number
of Hutter characters sequenced are given in Figure 2a.

The Spanish experiments measure how dynamic evaluation handles large distribution shifts between
training and test time, as Hutter Prize contains very little Spanish. We used the ﬁrst 5 million
characters of the Spanish European Parliament data in place of the Hutter Prize test set. The Spanish
experiments used the same base model and dynamic evaluation settings as Hutter Prize. Plots of
the average cross-entropy errors against the number of Spanish characters sequenced are given in
Figure 2b.

On both datasets, dynamic evaluation gave a very noticeable advantage after a few hundred characters.
For Spanish this advantage continued to grow as more of the sequence was processed, whereas
for Hutter, this advantage was maximized after viewing around 2-3k characters. The advantage of
dynamic evaluation was also much greater on Spanish sequences than Hutter sequences.

We also drew 300 character conditional samples from the static and dynamic versions of our model
after viewing 10k characters of Spanish. For the dynamic model, we continued to apply dynamic
evaluation during sampling as well, by the process described in Section 3. The conditional samples
are given in the appendix. The static samples quickly switched to English that resembled Hutter Prize
data. The dynamic model generated data with some Spanish words and a number of made up words
with characteristics of Spanish words for the entirety of the sample. This is an example of the kinds
of features that dynamic evaluation was able to learn to model on the ﬂy.

8 CONCLUSION

This work explores and develops methodology for applying dynamic evaluation to sequence modelling
tasks. Experiments show that the proposed dynamic evaluation methodology gives large test time
improvements across character and word level language modelling. Our improvements to language

8

modelling have applications to speech recognition and machine translation over longer contexts,
including broadcast speech recognition and paragraph level machine translation. Overall, dynamic
evaluation is shown to be an effective method for exploiting pattern re-occurrence in sequences.

REFERENCES

arXiv:1708.08863, 2017.

93–108, 2004.

Z. Aharoni, G. Rattner, and H. Permuter. Gradual learning of deep recurrent neural networks. arXiv preprint

J. R. Bellegarda. Statistical language model adaptation: review and perspectives. Speech Communication, 42(1):

J. Chung, S. Ahn, and Y. Bengio. Hierarchical multiscale recurrent neural networks. ICLR, 2017.
T. Cooijmans, N. Ballas, C. Laurent, and A. Courville. Recurrent batch normalization. ICLR, 2017.
M. Fortunato, C. Blundell, and O. Vinyals. Bayesian recurrent neural networks. arXiv preprint arXiv:1704.02798,

2017.

Y. Gal and Z. Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In

Advances in neural information processing systems, pp. 1019–1027, 2016.

E. Grave, A. Joulin, and N. Usunier. Improving neural language models with a continuous cache. ICLR, 2017.
A. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
D. Ha, A. Dai, and Q. Lee. Hypernetworks. ICLR, 2017.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9:1735–1780, 1997.
M. Hutter. The human knowledge compression prize. URL http://prize.hutter1.net, 2006.
H. Inan, K. Khosravi, and R. Socher. Tying word vectors and word classiﬁers: A loss framework for language

F. Jelinek, B. Merialdo, S. Roukos, and M. Strauss. A dynamic language model for speech recognition. In HLT,

modeling. ICLR, 2017.

volume 91, pp. 293–295, 1991.

N. Kalchbrenner, L. Espeholt, K. Simonyan, A. Oord, A. Graves, and K. Kavukcuoglu. Neural machine

translation in linear time. arXiv preprint arXiv:1610.10099, 2016.

Y. Kim, Y. Jernite, D. Sontag, and A. M. Rush. Character-aware neural language models. In Thirtieth AAAI

Conference on Artiﬁcial Intelligence, 2016.

D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
P. Koehn. Europarl: A parallel corpus for statistical machine translation. In MT Summit, volume 5, pp. 79–86,

B. Krause, L. Lu, I. Murray, and S. Renals. Multiplicative LSTM for sequence modelling. arXiv preprint

2005.

arXiv:1609.07959, 2016.

B. Krause, I. Murray, S. Renals, and L. Lu. Multiplicative LSTM for sequence modelling. ICLR Workshop track,

2017. URL https://openreview.net/forum?id=SJCS5rXFl.

R. Kuhn. Speech recognition and the frequency of recently used words: A modiﬁed Markov model for natural
In Proceedings of the 12th conference on Computational linguistics-Volume 1, pp. 348–350.

language.
Association for Computational Linguistics, 1988.

M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini. Building a large annotated corpus of English: The Penn

Treebank. Computational linguistics, 19(2):313–330, 1993.

G. Melis, C. Dyer, and P. Blunsom. On the state of the art of evaluation in neural language models. arXiv

S. Merity, N. S. Keskar, and R. Socher. Regularizing and optimizing LSTM language models. arXiv preprint

preprint arXiv:1707.05589, 2017.

arXiv:1708.02182, 2017a.

S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. ICLR, 2017b.
T. Mikolov and G. Zweig. Context dependent recurrent neural network language model. SLT, 12:234–239, 2012.
T. Mikolov, M. Karaﬁát, L. Burget, J. Cernock`y, and S. Khudanpur. Recurrent neural network based language

model. In Interspeech, volume 2, pp. 3, 2010.

T. Mikolov, I. Sutskever, A. Deoras, H. Le, S. Kombrink, and J. Cernocky. Subword language modeling with

neural networks. preprint (http://www. ﬁt. vutbr. cz/imikolov/rnnlm/char. pdf), 2012.

A. Mujika, F. Meier, and A. Steger. Fast-slow recurrent neural networks. arXiv preprint arXiv:1705.08639,

2017.

A. G. Ororbia II, T. Mikolov, and D. Reitter. Learning simpler language models with the differential state

B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on

framework. Neural Computation, 2017.

Control and Optimization, 30(4):838–855, 1992.

O. Press and L. Wolf. Using the output embedding to improve language models. EACL 2017, pp. 157, 2017.
T. Salimans and D. P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep

neural networks. In Advances in Neural Information Processing Systems, pp. 901–909, 2016.

T. Tieleman and G. E. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent

magnitude. COURSERA: Neural Networks for Machine Learning, 4(2), 2012.

L. Wan, M. Zeiler, S. Zhang, Yann L. Cun, and R. Fergus. Regularization of neural networks using dropconnect.
In Proceedings of the 30th international conference on machine learning (ICML-13), pp. 1058–1066, 2013.

9

P. J. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78:

Y. Wu, S. Zhang, Y. Zhang, Y. Bengio, and R. Salakhutdinov. On multiplicative integration with recurrent neural

W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent neural network regularization.

arXiv preprint

1550–1560, 1990.

networks. In NIPS, 2016.

arXiv:1409.2329, 2014.

J. G. Zilly, R. K. Srivastava, J. Koutník, and J. Schmidhuber. Recurrent highway networks. ICLR, 2017.
B. Zoph and Quoc V Le. Neural architecture search with reinforcement learning. ICLR, 2017.

A APPENDIX

A.1 DYNAMIC SAMPLES CONDITIONED ON SPANISH

300 character samples generated from the dynamic version of the model trained on Hutter Prize,
conditioned on 10k of Spanish characters. The ﬁnal sentence fragment of the 10k conditioning
characters is given to the reader, with the generated text given in bold:

Tiene importancia este compromiso en la medida en que la Comisión es un organismo que
tiene el montembre tas procedíns la conscriptione se ha Tesalo del Pómienda que et hanemos
que Pe la Siemina.
De la Pedrera Orden es Señora Presidente civil, Orden de siemin presente relevante frónmida
que esculdad pludiore e formidad President de la Presidenta Antidorne Adamirmidad i
ciemano de el 200’. Fo

A.2 STATIC SAMPLES CONDITIONED ON SPANISH

300 character samples generated from the static version of the model trained on Hutter Prize,
conditioned on 10k of Spanish characters. The ﬁnal sentence fragment of the 10k conditioning
characters is given to the reader, with the generated text given in bold:

Tiene importancia este compromiso en la medida en que la Comisión es un organismo que
tiene el monde,
&lt;br&gt;There is a secret act in the world except Cape Town, seen in now ﬂat comalo
and ball market and has seen the closure of the eagle as imprints in a dallas within the
country.&quot; Is a topic for an increasingly small contract saying Allan Roth acquired the
government in [[1916]].

===

10

7
1
0
2
 
t
c
O
 
5
2
 
 
]
E
N
.
s
c
[
 
 
2
v
2
3
4
7
0
.
9
0
7
1
:
v
i
X
r
a

DYNAMIC EVALUATION OF NEURAL SEQUENCE MODELS

Ben Krause, Emmanuel Kahembwe, Iain Murray, & Steve Renals
School of Informatics, University of Edinburgh
Edinburgh, Scotland, UK
ben.krause,e.kahembwe,i.murray,s.renals@ed.ac.uk

ABSTRACT

We present methodology for using dynamic evaluation to improve neural sequence
models. Models are adapted to recent history via a gradient descent based mecha-
nism, causing them to assign higher probabilities to re-occurring sequential patterns.
Dynamic evaluation outperforms existing adaptation approaches in our compar-
isons. Dynamic evaluation improves the state-of-the-art word-level perplexities
on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and
the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize
datasets to 1.19 bits/char and 1.08 bits/char respectively.

1

INTRODUCTION

Sequence generation and prediction tasks span many modes of data, ranging from audio and language
modelling, to more general timeseries prediction tasks. Applications of such models include speech
recognition, machine translation, dialogue generation, speech synthesis, forecasting, and music
generation, among others. Neural networks can be applied to these tasks by predicting sequence
elements one-by-one, conditioning on the history of sequence elements, forming an autoregressive
model. Convolutional neural networks (CNNs) and recurrent neural networks (RNNs), including
long-short term memory (LSTM) networks (Hochreiter & Schmidhuber, 1997) in particular, have
achieved many successes at these tasks. However, in their basic form, these models have a limited
ability to adapt to recently observed parts of a sequence.

Many sequences contain repetition; a pattern that occurs once is more likely to occur again. For
instance, a word that occurs once in a document is much more likely to occur again. A sequence of
handwriting will generally stay in the same handwriting style. A sequence of speech will generally
stay in the same voice. Although RNNs have a hidden state that can summarize the recent past, they
are often unable to exploit new patterns that occur repeatedly in a test sequence.

This paper concerns dynamic evaluation, which we investigate as a candidate solution to this problem.
Our approach adapts models to recent sequences using gradient descent based mechanisms. We show
several ways to improve on past dynamic evaluation approaches in Section 5, and use our improved
methodology to achieve state-of-the-art results in Section 7.1 and Section 7.2. In Section 6 we design
a method to dramatically to reduce the number of adaptation parameters in dynamic evaluation,
making it practical in a wider range of situations. In Section 7.3 we analyse dynamic evaluation’s
performance over varying time-scales and distribution shifts, and demonstrate that dynamically
evaluated models can generate conditional samples that repeat many patterns from the conditioning
data.

2 MOTIVATION

Generative models can assign probabilities to sequences by modelling each term in the factorization
given by the product rule. The probability of a sequence x1:T = {x1, . . . , xT } factorizes as

P (x1:T ) = P (x1)P (x2|x1)P (x3|x2, x1) · · · P (xT |x1 . . . xT −1).

(1)

Methods that apply this factorization either use a ﬁxed context when predicting P (xt|x1:t−1), for
instance as in N-grams or CNNs, or use a recurrent hidden state to summarize the context, as in an

1

1:T , x2

1:T , ..., xn

1:T }, each sequence xi

RNN. However, for longer sequences, the history x1:t−1 often contains re-occurring patterns that are
difﬁcult to capture using models with ﬁxed parameters (static models).
In many domains, in a dataset of sequences {x1
1:T is generated
1:T ). At any point in time t, the history of a sequence xi
from a slightly different distribution P (xi
1:t−1
contains useful information about the generating distribution for that speciﬁc sequence P (xi
1:T ).
Therefore adapting the model parameters learned during training θg is justiﬁed. We aim to infer a set
1:t−1 that will better approximate P (xi
of model parameters θl from xi
Many sequence modelling tasks are characterised by sequences generated from slightly different
distributions as in the scenario described above. The generating distribution may also change
continuously across a single sequence; for instance, a text excerpt may change topic. Furthermore,
many machine learning benchmarks do not distinguish between sequence boundaries, and concatenate
all sequences into one continuous sequence. Thus, many sequence modelling tasks could be seen as
having a local distribution Pl(x) as well as a global distribution Pg(x) := (cid:82) P (l)Pl(x) dl. During
training time, the goal is to ﬁnd the best ﬁxed model possible for Pg(x). However, during evaluation
time, a model that can infer the current Pl(x) from the recent history has an advantage.

1:t−1) within sequence i.

t|xi

3 DYNAMIC EVALUATION

Dynamic evaluation methods continuously adapt the model parameters θg, learned at training time,
to parts of a sequence during evaluation. The goal is to learn adapted parameters θl that provide a
better model of the local sequence distribution, Pl(x). When dynamic evaluation is applied in the
present work, a long test sequence x1:T is divided up into shorter sequences of length n. We deﬁne
s1:M to be a sequence of shorter sequence segments si

s1:M = {s1 = x1:n, s2 = xn+1:2n, s3 = x2n+1:3n, ..., sM }.

(2)

The initial adapted parameters θ0
l are set to θg, and used to compute the probability of the ﬁrst
segment, P (s1|θ0
l ). This probability gives a cross entropy loss L(s1), with gradient ∇L(s1), which
is computed using truncated back-propagation through time (Werbos, 1990). The gradient ∇L(s1) is
used to update the model, resulting in adapted parameters θ1
l ). The same
procedure is then repeated for s2, and for each si in the sequence as shown in Figure 1. Gradients
for each loss L(si) are only backpropagated to the beginning of si, so computation is linear in the
sequence length. Each update applies one maximum likelihood training step to approximate the
current local distribution Pl(x). The computational cost of dynamic evaluation is one forward pass
and one gradient computation through the data, with some slight overhead to apply the update rule
for every sequence segment.

l , before evaluating P (s2|θ1

As in all autoregressive models, dynamic evaluation only conditions on sequence elements that it has
already predicted, and so evaluates a valid log-probability for each sequence. Dynamic evaluation can
also be used while generating sequences. In this case, the model generates each sequence segment
si using ﬁxed weights, and performs a gradient descent based update step on L(si). Applying
dynamic evaluation for sequence generation could result in generated sequences with more consistent
regularities, meaning that patterns that occur in the generated sequence are more likely to occur again.

4 BACKGROUND

4.1 RELATED APPROACHES

Adaptive language modelling was ﬁrst considered for n-grams, adapting to recent history via caching
(Jelinek et al., 1991; Kuhn, 1988), and other methods Bellegarda (2004). More recently, the neural
cache approach (Grave et al., 2017) and the closely related pointer sentinel-LSTM (Merity et al.,
2017b) have been used to for adaptive neural language modelling. Neural caching has recently been
used to improve the state-of-the-art at word-level language modelling (Merity et al., 2017a).

The neural cache model learns a type of non-parametric output layer on the ﬂy at test time, which
allows the network to adapt to recent observations. Each past hidden state hi is paired with the next
input xi+1, and is stored as a tuple (hi, xi+1). When a new hidden state ht is observed, the output
probabilities are adjusted to give a higher weight to output words that coincided with past hidden

2

Figure 1: Illustration of dynamic evaluation. The model evaluates the probability of sequence
segments si. The gradient ∇L(si) with respect to the log probability of si is used to update the model
parameters θi−1
l before the model progresses to the next sequence segment. Dashed edges are
what distinguish dynamic evaluation from static (normal) evaluation.

to θi

l

states with a large inner product (hT

t hi).

Pcache(xt+1|x1:t, h1:t) ∝

e(xi+1) exp(ωhT

t hi),

(3)

t−1
(cid:88)

i=1

where e(xi+1) is a one hot encoding of xi+1, and ω is a scaling parameter. The cache probabilities are
interpolated with the base network probabilities to adapt the base network at test time.

The neural cache closely relates to dynamic evaluation, as both methods can be added on top of a base
model for adaptation at test time. The main difference is the mechanism used to ﬁt to recent history:
the neural cache approach uses a non-parametric, nearest neighbours-like method, whereas dynamic
evaluation uses a gradient descent based method to change model parameters dynamically. Both
methods rely on an autoregressive factorisation, as they depend on observing sequence elements after
they are predicted in order to perform adaptation. Dynamic evaluation and neural caching methods
are therefore both applicable to sequence prediction and generation tasks, but not directly to more
general supervised learning tasks.

One drawback of the neural cache method is that it cannot adjust the recurrent hidden state dynamics.
As a result, the neural cache’s ability to capture information that occurs jointly between successive
sequence elements is limited. This capability is critical for adapting to sequences where each element
has very little independent meaning, e.g. character level language modelling.

4.2 DYNAMIC EVALUATION IN NEURAL NETWORKS

Dynamic evaluation of neural language models was proposed by Mikolov et al. (2010). Their
approach simply used stochastic gradient descent (SGD) updates at every time step, computing the
gradient with fully truncated backpropagation through time, which is equivalent to setting n = 1

3

in equation (2). Dynamic evaluation has since been applied to character and word-level language
models (Graves, 2013; Krause et al., 2017; Ororbia II et al., 2017; Fortunato et al., 2017). Previous
work using dynamic evaluation considered it as an aside, and did not explore it in depth.

5 UPDATE RULE METHODOLOGY FOR DYNAMIC EVALUATION

We propose several changes to Mikolov et al. (2010)’s dynamic evaluation method with SGD and
fully truncated backpropagation, which we refer to as traditional dynamic evaluation. The ﬁrst
modiﬁcation reduces the update frequency, so that gradients are backpropagated over more timesteps.
This change provides more accurate gradient information, and also improves the computational
efﬁciency of dynamic evaluation, since the update rule is applied much less often. We use sequence
segments of length 5 for word-level tasks and 20 for character-level tasks.

Next, we add a global decay prior to bias the model towards the parameters θg learned during
training. Our motivation for dynamic evaluation assumes that the local generating distribution Pl(x)
is constantly changing, so it is potentially desirable to weight recent sequence history higher in
adaptation. Adding a global decay prior accomplishes this by causing previous adaptation updates to
decay exponentially over time. For SGD with a global prior, learning rate η and decay rate λ; we
form the update rule

θi ← θi−1 − η∇L(si) + λ(θg − θi−1

).

l

We then consider using an RMSprop (Tieleman & Hinton, 2012) derived update rule for the learning
rule in place of SGD. RMSprop uses a moving average of recent squared gradients to scale learning
rates for each weight. In dynamic evaluation, near the start of a test sequence, RMSprop has had very
few gradients to average, and therefore may not be able to leverage its updates as effectively. For this
reason, we collect mean squared gradients, MSg, on the training data rather than on recent test data
(which is what RMSprop would do). MSg is given by
Nb(cid:88)

MSg =

(∇Lk)2,

1
Nb

k=1

where Nb is the number of training batches and ∇Lk is the gradient on the kth training batch. The
mini-batch size for this computation becomes a hyper-parameter, as larger mini-batches will result
in smaller mean squared gradients. The update rule, which we call RMS with a global prior in our
experiments, is then

l − η

l ← θi−1
θi

∇L(si)
(cid:112)MSg + (cid:15)
where (cid:15) is a stabilization parameter. For the decay step of our update rule, we also consider scaling
the decay rate for each parameter proportionally to (cid:112)MSg. Parameters with a high RMS gradient
affect the dynamics of the network more, so it makes sense to decay them faster. RMSnorm is (cid:112)MSg
divided by its mean, resulting in a normalized version of (cid:112)MSg with a mean of 1:

+ λ(θg − θi−1

(6)

),

l

RMSnorm =

(cid:112)MSg
avg((cid:112)MSg)

.

We clip the values of RMSnorm to be no greater than 1/λ to be sure that the decay rate does not
exceed 1 for any parameter. Combining the learning component and the regularization component
results in the ﬁnal update equation, which we refer to as RMS with an RMS global prior

l ← θi−1
θi

l − η

∇L(si)
(cid:112)MSg + (cid:15)

+ λ(θg − θi−1

l

) (cid:12) RMSnorm.

6 SPARSE DYNAMIC EVALUATION

Mini-batching over sequences is desirable for some test-time sequence modelling applications because
it allows faster processing of multiple sequences in parallel. Dynamic evaluation has a high memory

(4)

(5)

(7)

(8)

4

cost for mini-batching because it is necessary to store a different set of parameters for each sequence
in the mini-batch. Therefore, we consider a sparse dynamic evaluation variant that updates a smaller
number of parameters. We introduce a new adaptation matrix M which is initialized to zeros. M
multiplies hidden state vector ht of an RNN at every time-step to get a new hidden state h(cid:48)

t, via

h(cid:48)
t = ht + Mht.
h(cid:48)
t then replaces ht and is propagated throughout the network via both recurrent and feed-forward
connections. Applying dynamic evaluation to M avoids the need to apply dynamic evaluation to
the original parameters of the network, reduces the number of adaptation parameters, and makes
mini-batching less memory intensive. We reduce the number of adaptation parameters further by
only using M to transform an arbitrary subset of H hidden units. This results in M being an H ×H
matrix with d = H 2 adaptation parameters. If H is chosen to be much less than the number of hidden
units, this reduces the number of adaptation parameters dramatically. In Section 7.2 we experiment
with sparse dynamic evaluation for character-level language models.

(9)

7 EXPERIMENTS

We applied dynamic evaluation to word-level and character-level language modelling. In all tasks,
we evaluate dynamic evaluation on top of a base model. After training the base model, we tune
hyper-parameters for dynamic evaluation on the validation set, and evaluate both the static and
dynamic versions of the model on the test set. We also consider follow up experiments that analyse
the sequence lengths for which dynamic evaluation is useful. Code for our dynamic evaluation
methodology is available1.

7.1 WORD-LEVEL LANGUAGE MODELLING

We train base models on the Penn Treebank (PTB, Marcus et al., 1993) and WikiText-2 (Merity et al.,
2017b) datasets, and compare the performance of static and dynamic evaluation. These experiments
compare dynamic evaluation against past approaches such as the neural cache and measure dynamic
evaluation’s general performance across different models and datasets.

PTB is derived from articles of the Wall Street Journal. It contains 929k training tokens and a vocab
size limited to 10k words. It is one of the most commonly used benchmarks in language modelling.
We consider two baseline models on PTB, a standard LSTM implementation with recurrent dropout
(Zaremba et al., 2014), and the recent state-of-the-art AWD-LSTM (Merity et al., 2017a).

Our standard LSTM was taken from the Chainer tutorial on language modelling2, and used two
LSTM layers with 650 units each, trained with SGD and regularized with recurrent dropout. On
our standard LSTM, we experiment with traditional dynamic evaluation as applied by Mikolov et al.
(2010), as well as each modiﬁcation we make building up to our ﬁnal update rule as described in
Section 5. As our ﬁnal update rule (RMS + RMS global prior) worked best, we use this for all other
experiments and use “dynamic eval” by default to refer to this update rule in tables.

We applied dynamic evaluation on a more powerful model, the ASGD weight-dropped LSTM
(AWD-LSTM, Merity et al., 2017a). The AWD-LSTM is a vanilla LSTM that combines the use of
drop-connect (Wan et al., 2013) on recurrent weights for regularization, and a variant of averaged
stochastic gradient descent (Polyak & Juditsky, 1992) for optimisation. Our model, which used 3
layers and tied input and output embeddings (Press & Wolf, 2017; Inan et al., 2017), was intended to
be a direct replication of AWD-LSTM, using code from their implementation3. Results are given in
Table 1.

Dynamic evaluation gives signiﬁcant overall improvements to both models on this dataset. Dynamic
evaluation also achieves better ﬁnal results than the neural cache on both a standard LSTM and the
AWD-LSTM reimplementation, and improves the state-of-the-art on PTB.

WikiText-2 is roughly twice the size of PTB, with 2 million training tokens and a vocab size of 33k.
It features articles in a non-shufﬂed order, with dependencies across articles that adaptive methods

1https://github.com/benkrause/dynamic-evaluation
2https://github.com/chainer/chainer/tree/master/examples/ptb
3https://github.com/salesforce/awd-lstm-lm

5

model

parameters

valid

test

RNN+LDA+kN-5+cache (Mikolov & Zweig, 2012)
CharCNN (Kim et al., 2016)
LSTM (Zaremba et al., 2014)
Variational LSTM (Gal & Ghahramani, 2016)
Pointer sentinel-LSTM (Merity et al., 2017b)
Variational LSTM + augmented loss (Inan et al., 2017)
Variational RHN (Zilly et al., 2017)
NAS cell (Zoph & Le, 2017)
Variational LSTM + gradual learning (Aharoni et al., 2017)
LSTM + BB tuning (Melis et al., 2017)

LSTM (Grave et al., 2017)
LSTM + neural cache (Grave et al., 2017)
LSTM (ours)
LSTM + traditional dynamic eval (sgd, bptt=1)
LSTM + dynamic eval (sgd, bptt=5)
LSTM + dynamic eval (sgd, bptt=5, global prior)
LSTM + dynamic eval (RMS, bptt=5, global prior)
LSTM + dynamic eval (RMS, bptt=5, RMS global prior)

AWD-LSTM (Merity et al., 2017a)
AWD-LSTM +neural cache (Merity et al., 2017a)
AWD-LSTM (ours)
AWD-LSTM + dynamic eval

19M
66M
66M
21M
51M
23M
54M
105M
24M

20M
20M
20M
20M
20M
20M

24M
24M
24M
24M

92.0
78.9
78.4
73.4
70.9
68.5
65.4
62.4
61.7
58.3

82.3
72.1
85.6
76.2
75.6
74.8
72.2
71.7

57.3
52.8
57.7
51.1

82.2

72.4
71.1
67.9

60.9

86.9
74.6
88.0
78.6
78.0
77.4
74.3
73.5

60.0
53.9
59.8
51.6

Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.

model

parameters

valid

Byte mLSTM (Krause et al., 2016)
Variational LSTM (Inan et al., 2017)
Pointer sentinel-LSTM (Merity et al., 2017b)
LSTM + BB tuning (Melis et al., 2017)

LSTM (Grave et al., 2017)
LSTM + neural cache (Grave et al., 2017)
LSTM (ours)
LSTM + dynamic eval

AWD-LSTM (Merity et al., 2017a)
AWD-LSTM + neural cache (Merity et al., 2017a)
AWD-LSTM (ours)
AWD-LSTM + dynamic eval

46M
28M

24M

50M
50M

33M
33M
33M
33M

Table 2: WikiText-2 perplexities.

test

88.8
87.0
80.8
65.9

99.3
68.9
103.4
59.8

65.8
52.0
66.1
44.3

92.8
91.5
84.8
69.1

104.2
72.1
109.1
63.7

68.6
53.8
68.9
46.4

should be able to exploit. For this dataset, we use the same baseline LSTM implementation and
AWD-LSTM re-implementation as on PTB. Results are given in Table 2.

Dynamic evaluation improves the state-of-the-art perplexity on WikiText-2, and provides a signiﬁ-
cantly greater improvement than neural caching to both base models. This suggests that dynamic
evaluation is effective at exploiting regularities that co-occur across non-shufﬂed documents.

6

model

parameters

test

Stacked LSTM (Graves, 2013)
Stacked LSTM + traditional dynamic eval (Graves, 2013)
Multiplicative integration LSTM (Wu et al., 2016)
HyperLSTM (Ha et al., 2017)
Hierarchical multiscale LSTM (Chung et al., 2017)
Bytenet decoder (Kalchbrenner et al., 2016)
LSTM + BB tuning (Melis et al., 2017)
Recurrent highway networks (Zilly et al., 2017)
Fast-slow LSTM (Mujika et al., 2017)

mLSTM (Krause et al., 2016)
mLSTM + sparse dynamic eval (d = 250k)
mLSTM + dynamic eval

21M
21M
17M
27M

46M
46M
47M

46M
46M
46M

1.67
1.33
1.44
1.34
1.32
1.31
1.30
1.27
1.25

1.24
1.13
1.08

Table 3: Hutter Prize test set error in bits/char.

model

parameters

test

Multiplicative RNN (Mikolov et al., 2012)
Multiplicative integration LSTM (Wu et al., 2016)
LSTM (Cooijmans et al., 2017)
Batch normalised LSTM (Cooijmans et al., 2017)
Hierarchical multiscale LSTM (Chung et al., 2017)
Recurrent highway networks (Zilly et al., 2017)

mLSTM (Krause et al., 2016)
mLSTM + dynamic eval

5M
4M

45M

45M
45M

1.54
1.44
1.43
1.36
1.29
1.27

1.27
1.19

Table 4: text8 test set error in bits/char.

7.2 CHARACTER-LEVEL LANGUAGE MODELLING

We consider dynamic evaluation on the text84, and Hutter Prize (Hutter, 2006) datasets. The Hutter
Prize dataset is comprised of Wikipedia text, and includes XML and characters from non-Latin
languages. It is 100 million UTF-8 bytes long and contains 205 unique bytes. Similarly to other
reported results, we use a 90-5-5 split for training, validation, and testing. The text8 dataset is also
derived from Wikipedia text, but has all XML removed, and is lower cased to only have 26 characters
of English text plus spaces. As with Hutter Prize, we use the standard 90-5-5 split for training,
validation, and testing for text8. We used a multiplicative LSTM (mLSTM) (Krause et al., 2016)5 as
our base model for both datasets. The mLSTMs for both tasks used 2800 hidden units, an embedding
layer of 400 units, weight normalization (Salimans & Kingma, 2016), variational dropout (Gal &
Ghahramani, 2016), and ADAM (Kingma & Ba, 2014) for training.

We also consider sparse dynamic evaluation, as described in Section 6, on the Hutter Prize dataset.
For sparse dynamic evaluation, we adapted a subset of 500 hidden units, resulting in a 500×500
adaptation matrix and 250k adaptation parameters. All of our dynamic evaluation results in this
section use the ﬁnal update rule given in Section 5. Results for Hutter Prize are given in Table 3, and
results for text8 are given in Table 4.

Dynamic evaluation achieves large improvements to our base models and state-of-the-art results on
both datasets. Sparse dynamic evaluation also achieves signiﬁcant improvements on Hutter Prize
using only 0.5% of the adaptation parameters of regular dynamic evaluation.

4http://mattmahoney.net/dc/textdata
5https://github.com/benkrause/mLSTM

7

(a) Hutter data

(b) Spanish data

Figure 2: Average losses in bits/char of dynamic evaluation and static evaluation plotted against
number of characters processed; on sequences from the Hutter Prize test set (left) and European
Parliament dataset in Spanish (right), averaged over 500 trials for each. Losses at each data point are
averaged over sequence segments of length 100, and are not cumulative. Note the different y-axis
scales in the two plots.

7.3 TIME-SCALES OF DYNAMIC EVALUATION

We measure time-scales at which dynamic evaluation gains an advantage over static evaluation.
Starting from the model trained on Hutter Prize, we plot the performance of static and dynamic
evaluation against the number of characters processed on sequences from the Hutter Prize test set,
and sequences in Spanish from the European Parliament dataset (Koehn, 2005).

The Hutter Prize data experiments show the timescales at which dynamic evaluation gained the
advantage observed in Table 3. We divided the Hutter Prize test set into 500 sequences of length
10000, and applied static and dynamic evaluation to these sequences using the same model and
methodology used to obtain results in Table 3. Losses were averaged across these 500 sequences to
obtain average losses at each time step. Plots of the average cross-entropy errors against the number
of Hutter characters sequenced are given in Figure 2a.

The Spanish experiments measure how dynamic evaluation handles large distribution shifts between
training and test time, as Hutter Prize contains very little Spanish. We used the ﬁrst 5 million
characters of the Spanish European Parliament data in place of the Hutter Prize test set. The Spanish
experiments used the same base model and dynamic evaluation settings as Hutter Prize. Plots of
the average cross-entropy errors against the number of Spanish characters sequenced are given in
Figure 2b.

On both datasets, dynamic evaluation gave a very noticeable advantage after a few hundred characters.
For Spanish this advantage continued to grow as more of the sequence was processed, whereas
for Hutter, this advantage was maximized after viewing around 2-3k characters. The advantage of
dynamic evaluation was also much greater on Spanish sequences than Hutter sequences.

We also drew 300 character conditional samples from the static and dynamic versions of our model
after viewing 10k characters of Spanish. For the dynamic model, we continued to apply dynamic
evaluation during sampling as well, by the process described in Section 3. The conditional samples
are given in the appendix. The static samples quickly switched to English that resembled Hutter Prize
data. The dynamic model generated data with some Spanish words and a number of made up words
with characteristics of Spanish words for the entirety of the sample. This is an example of the kinds
of features that dynamic evaluation was able to learn to model on the ﬂy.

8 CONCLUSION

This work explores and develops methodology for applying dynamic evaluation to sequence modelling
tasks. Experiments show that the proposed dynamic evaluation methodology gives large test time
improvements across character and word level language modelling. Our improvements to language

8

modelling have applications to speech recognition and machine translation over longer contexts,
including broadcast speech recognition and paragraph level machine translation. Overall, dynamic
evaluation is shown to be an effective method for exploiting pattern re-occurrence in sequences.

REFERENCES

arXiv:1708.08863, 2017.

93–108, 2004.

Z. Aharoni, G. Rattner, and H. Permuter. Gradual learning of deep recurrent neural networks. arXiv preprint

J. R. Bellegarda. Statistical language model adaptation: review and perspectives. Speech Communication, 42(1):

J. Chung, S. Ahn, and Y. Bengio. Hierarchical multiscale recurrent neural networks. ICLR, 2017.
T. Cooijmans, N. Ballas, C. Laurent, and A. Courville. Recurrent batch normalization. ICLR, 2017.
M. Fortunato, C. Blundell, and O. Vinyals. Bayesian recurrent neural networks. arXiv preprint arXiv:1704.02798,

2017.

Y. Gal and Z. Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In

Advances in neural information processing systems, pp. 1019–1027, 2016.

E. Grave, A. Joulin, and N. Usunier. Improving neural language models with a continuous cache. ICLR, 2017.
A. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
D. Ha, A. Dai, and Q. Lee. Hypernetworks. ICLR, 2017.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9:1735–1780, 1997.
M. Hutter. The human knowledge compression prize. URL http://prize.hutter1.net, 2006.
H. Inan, K. Khosravi, and R. Socher. Tying word vectors and word classiﬁers: A loss framework for language

F. Jelinek, B. Merialdo, S. Roukos, and M. Strauss. A dynamic language model for speech recognition. In HLT,

modeling. ICLR, 2017.

volume 91, pp. 293–295, 1991.

N. Kalchbrenner, L. Espeholt, K. Simonyan, A. Oord, A. Graves, and K. Kavukcuoglu. Neural machine

translation in linear time. arXiv preprint arXiv:1610.10099, 2016.

Y. Kim, Y. Jernite, D. Sontag, and A. M. Rush. Character-aware neural language models. In Thirtieth AAAI

Conference on Artiﬁcial Intelligence, 2016.

D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
P. Koehn. Europarl: A parallel corpus for statistical machine translation. In MT Summit, volume 5, pp. 79–86,

B. Krause, L. Lu, I. Murray, and S. Renals. Multiplicative LSTM for sequence modelling. arXiv preprint

2005.

arXiv:1609.07959, 2016.

B. Krause, I. Murray, S. Renals, and L. Lu. Multiplicative LSTM for sequence modelling. ICLR Workshop track,

2017. URL https://openreview.net/forum?id=SJCS5rXFl.

R. Kuhn. Speech recognition and the frequency of recently used words: A modiﬁed Markov model for natural
In Proceedings of the 12th conference on Computational linguistics-Volume 1, pp. 348–350.

language.
Association for Computational Linguistics, 1988.

M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini. Building a large annotated corpus of English: The Penn

Treebank. Computational linguistics, 19(2):313–330, 1993.

G. Melis, C. Dyer, and P. Blunsom. On the state of the art of evaluation in neural language models. arXiv

S. Merity, N. S. Keskar, and R. Socher. Regularizing and optimizing LSTM language models. arXiv preprint

preprint arXiv:1707.05589, 2017.

arXiv:1708.02182, 2017a.

S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. ICLR, 2017b.
T. Mikolov and G. Zweig. Context dependent recurrent neural network language model. SLT, 12:234–239, 2012.
T. Mikolov, M. Karaﬁát, L. Burget, J. Cernock`y, and S. Khudanpur. Recurrent neural network based language

model. In Interspeech, volume 2, pp. 3, 2010.

T. Mikolov, I. Sutskever, A. Deoras, H. Le, S. Kombrink, and J. Cernocky. Subword language modeling with

neural networks. preprint (http://www. ﬁt. vutbr. cz/imikolov/rnnlm/char. pdf), 2012.

A. Mujika, F. Meier, and A. Steger. Fast-slow recurrent neural networks. arXiv preprint arXiv:1705.08639,

2017.

A. G. Ororbia II, T. Mikolov, and D. Reitter. Learning simpler language models with the differential state

B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on

framework. Neural Computation, 2017.

Control and Optimization, 30(4):838–855, 1992.

O. Press and L. Wolf. Using the output embedding to improve language models. EACL 2017, pp. 157, 2017.
T. Salimans and D. P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep

neural networks. In Advances in Neural Information Processing Systems, pp. 901–909, 2016.

T. Tieleman and G. E. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent

magnitude. COURSERA: Neural Networks for Machine Learning, 4(2), 2012.

L. Wan, M. Zeiler, S. Zhang, Yann L. Cun, and R. Fergus. Regularization of neural networks using dropconnect.
In Proceedings of the 30th international conference on machine learning (ICML-13), pp. 1058–1066, 2013.

9

P. J. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78:

Y. Wu, S. Zhang, Y. Zhang, Y. Bengio, and R. Salakhutdinov. On multiplicative integration with recurrent neural

W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent neural network regularization.

arXiv preprint

1550–1560, 1990.

networks. In NIPS, 2016.

arXiv:1409.2329, 2014.

J. G. Zilly, R. K. Srivastava, J. Koutník, and J. Schmidhuber. Recurrent highway networks. ICLR, 2017.
B. Zoph and Quoc V Le. Neural architecture search with reinforcement learning. ICLR, 2017.

A APPENDIX

A.1 DYNAMIC SAMPLES CONDITIONED ON SPANISH

300 character samples generated from the dynamic version of the model trained on Hutter Prize,
conditioned on 10k of Spanish characters. The ﬁnal sentence fragment of the 10k conditioning
characters is given to the reader, with the generated text given in bold:

Tiene importancia este compromiso en la medida en que la Comisión es un organismo que
tiene el montembre tas procedíns la conscriptione se ha Tesalo del Pómienda que et hanemos
que Pe la Siemina.
De la Pedrera Orden es Señora Presidente civil, Orden de siemin presente relevante frónmida
que esculdad pludiore e formidad President de la Presidenta Antidorne Adamirmidad i
ciemano de el 200’. Fo

A.2 STATIC SAMPLES CONDITIONED ON SPANISH

300 character samples generated from the static version of the model trained on Hutter Prize,
conditioned on 10k of Spanish characters. The ﬁnal sentence fragment of the 10k conditioning
characters is given to the reader, with the generated text given in bold:

Tiene importancia este compromiso en la medida en que la Comisión es un organismo que
tiene el monde,
&lt;br&gt;There is a secret act in the world except Cape Town, seen in now ﬂat comalo
and ball market and has seen the closure of the eagle as imprints in a dallas within the
country.&quot; Is a topic for an increasingly small contract saying Allan Roth acquired the
government in [[1916]].

===

10

