4
1
0
2
 
p
e
S
 
1
1
 
 
]

G
L
.
s
c
[
 
 
2
v
9
6
1
6
.
1
0
4
1
:
v
i
X
r
a

Parsimonious Topic Models
with Salient Word Discovery

Hossein Soleimani, and David J. Miller, Senior Member,IEEE

1

Abstract—We propose a parsimonious topic model for text corpora. In related models such as Latent Dirichlet Allocation (LDA), all
words are modeled topic-speciﬁcally, even though many words occur with similar frequencies across different topics. Our modeling
determines salient words for each topic, which have topic-speciﬁc probabilities, with the rest explained by a universal shared model.
Further, in LDA all topics are in principle present in every document. By contrast our model gives sparse topic representation,
determining the (small) subset of relevant topics for each document. We derive a Bayesian Information Criterion (BIC), balancing
model complexity and goodness of ﬁt. Here, interestingly, we identify an effective sample size and corresponding penalty speciﬁc to
each parameter type in our model. We minimize BIC to jointly determine our entire model – the topic-speciﬁc words, document-speciﬁc
topics, all model parameter values, and the total number of topics – in a wholly unsupervised fashion. Results on three text corpora
and an image dataset show that our model achieves higher test set likelihood and better agreement with ground-truth class labels,
compared to LDA and to a model designed to incorporate sparsity.

Index Terms—Bayesian Information Criterion (BIC), Model selection, Parsimonious models, Sparse Models, Topic models, Unsuper-
vised feature selection

1 INTRODUCTION

P ARSIMONIOUS models are economical in the number

of free parameters they require to model data. Such
models have been proposed e.g. for model-based clus-
tering, reducing the number of free (covariance matrix)
parameters needed to specify a cluster’s shape (which,
without parsimony, grows quadratically with the num-
ber of feature dimensions) [1], [2]. For clustering of text
documents, great reduction in the number of free param-
eters has also been achieved by tying parameter values
across many clusters, e.g. [3]. Parsimonious models are
less prone to overﬁtting than parameter-rich ones in
various modeling problems [4],[5]. Document modeling
is a good “target” here, as the “bag-of-words” model
[6] introduces one parameter per word, per topic, for
each word in a given lexicon. This can amount to tens
of thousands of word probability parameters for every
topic in the model. It is expected that economization on
this model representation should be possible.

In topics models such as Latent Dirichlet Allocation
(LDA) [7], each topic is a probability mass function
deﬁned over the given vocabulary. LDA is an extension
of mixtures of unigrams, with the latter assuming each
document is generated by a single topic randomly se-
lected based on corpus-level topic proportions [8]. By
contrast, in LDA, each word in a given document is in-
dependently modeled by a (document-speciﬁc) mixture
over the topics.

In LDA, each word has a freely chosen probability
parameter under every topic. This entails a huge set
of parameters to estimate and hence makes the model

• H. Soleimani

(email: hsoleimani@psu.edu) and D.

J. Miller (email:
djmiller@engr.psu.edu) are with the Department of Electrical Engineering,
Pennsylvania State University, University Park, PA, 16802.

prone to over-ﬁtting. Moreover, intuitively, many words
are not context-speciﬁc,
i.e. they may be used with
roughly the same frequency under different topics. Thus,
these (many) words may be reasonably modeled using
a universal shared model across all topics. Our approach
will exploit such parsimony.

Similarly, in LDA every topic is in principle present in
every document, with a non-zero proportion. This seems
implausible since each document is expected to have a
main theme covered by a modest subset of related topics.
Allowing all topics to have nonzero proportions in every
document again complicates the model’s representation
of the data. By contrast, our proposed method identiﬁes
a sparse set of topics present in each document.

Our model is in fact sparse in both abovementioned
senses. First, under a given topic, each word may be a
topic-speciﬁc feature with its own probability parameter
or a shared feature, using a parameter shared with
other topics. Second, for each document a sparse set of
“occurring” topics – those with non-zero proportions – is
identiﬁed. These two sparsities can be understood from
Fig. 1, with βjn the probability of topic j generating word
n and αjd the proportion of topic j in document d. For
example, here only topics 1 and 3 are present in docu-
ment 1, and the ﬁrst word’s probability of generation is
shared by all the topics.

Our model learning consists of two main parts, al-
ternately, iteratively applied: structure learning and pa-
rameter learning. Assuming the number of topics is
known and given current parameter estimates, structure
learning determines the topic-speciﬁc words for each
topic and the topics with non-zero proportions in each
document. Likewise, given a ﬁxed structure, in parame-
ter learning the document-speciﬁc topic proportions and
topic-speciﬁc word probabilities are re-estimated. In the

0.1
0.1
0.1

β = 


0.2
0.25
0.05

0.2
0.2
0.3

0.25
0.3
0.1

0.1
0.05
0.3

0.15
0.1
0.15





0.2
0
0.8

α = 


0
0.4
0.6





Fig. 1: A simple example to illustrate sparsity on word
probabilities (β) (left) and topic proportions (α) (right)
for a corpus with 2 documents, 6 words, and 3 topics.
(Shared words are in bold.)

following sections we introduce an objective function
and a computationally efﬁcient procedure, jointly per-
forming structure and parameter learning to minimize
this objective.

Our objective function is derived from the model
posterior probability [9],[10]. One such criterion is the
Bayesian Information Criterion (BIC) [11]. BIC, which is
a widely accepted criterion for model comparison, is the
negative logarithm of the Bayes marginal likelihood and
is composed of two main terms: the data log-likelihood
and the model complexity cost. BIC thus achieves a
balance between goodness of ﬁt and model complexity.
However, BIC has some deﬁciencies which limit
applicability. The parameter penalty in BIC
its
( 1
2 log(sample size))
is the same for all parameters
in the model [11], while we expect different parameter
types in general contribute unequally both to model
complexity and to goodness of ﬁt. Moreover,
the
Laplace’s approximation used in deriving BIC is only
valid when the ratio of sample size to feature space
dimensionality is large [9]. However,
in document
modeling this ratio is very small because the feature
dimensionality is the (huge) dictionary size.

In this paper, we derive an approximation of the
model posterior which improves on the na¨ıve form
of BIC in two aspects: 1) Our proposed form of BIC
has differentiated cost terms, based on different effective
sample sizes for the different parameter types in our
model. 2) Making use of a shared feature representation
essentially increases the sample size to feature dimension
ratio, thus giving a better approximation of the model
posterior.

Our framework also gives, in a wholly unsupervised
fashion, a direct estimate of the number of topics present
in the corpus. The number of topics (i.e. model order) is
a hyper-parameter in topic models, usually determined
based on validation set performance for a secondary
task such as classiﬁcation. Here, solely using the (unla-
beled) document corpus, we select the model order with
highest approximate model posterior (minimum BIC).
Thus, the model in its entirety – its structure, parameter
values, and the number of topics – are all jointly chosen
to minimize BIC.

1.1 Related Work

“Common” and “speciﬁc” words, sparsity in topic pro-
portions and word probabilities, as well as estimation
of the number of topics have all been the subject of
previous studies. [12] introduced asymmetric Dirichlet

2

priors over topic distributions and word probabilities
to control skewness in word and topic frequency dis-
tributions. Asymmetric priors were shown to prevent
common words from dominating all topics and also help
achieve sparser topic presence in documents. However,
similar to LDA, this approach is not parsimonious. All
topics have nonzero proportion in every document and
all words are modeled in a topic-speciﬁc fashion. [13]
introduced a spike and slab model to control sparsity in
word probabilities. Unlike our approach, [13] does not
use a shared distribution. Moreover, it does not provide
the subset of relevant topics for each document. A sim-
ilar approach, based on the Indian Buffet Process, was
used in [14] to address sparsity only in topic proportions
in a non-parametric topic model.

Global background models have been used in infor-
mation retrieval [15], [16], [17]. In these models, the
probability of each word under every topic is a mix-
ture of the background model and topic-speciﬁc word
probabilities. A similar idea has been used in [18], with
words well-modeled by the background model having
small (near zero) topic-speciﬁc probabilities. The mixing
proportions in these models are hyperparameters that
should be estimated by cross-validation. [19] proposed
a combination of background, general, and document-
speciﬁc topics to improve information retrieval. The
authors argued that LDA “over-generalizes” and is thus
not effective for matching queries that contain both high-
level semantics and keywords. [19] introduced a huge set
of new free parameters by adding a document-speciﬁc
topic for every document. In these models, similar to
LDA, each word, under every topic, has a free parameter.
By contrast, in our model, the shared model is heavily
used, with each topic possessing relatively few topic-
speciﬁc words. Also unlike these approaches, our model
is sparse in topic proportions.

[20] presented Sparse Topical Coding (STC), a non-
probabilistic topic model which gives parsimony in
topic-proportions but models all words topic-speciﬁcally.
Moreover, this method has three hyper-parameters that
must be determined by cross-validation.

Non-parametric topic models have been proposed that
relax the requirement of specifying the number of topics
[21]. However similar to LDA, these methods do not
exhibit parsimony in their modeling.

Our approach can also be viewed from the stand-
point of unsupervised feature selection. For each topic
we select salient features in an unsupervised fashion,
modeling the rest using a universal shared model. Unsu-
pervised feature selection and shared feature represen-
tations have been considered in some prior works. [22]
used a minimum message length criterion to ﬁnd salient
features in a mixture model. Features were tied across all
components; i.e. each feature is either salient or shared
in all components. A related Bayesian framework was
presented in [23].

The concept of shared feature space for mixtures was
further improved in [3] by proposing a component-

speciﬁc feature space; i.e. a feature can be salient in some
components but represented by the shared model in
others. [3] used the Minimum Description Length (MDL)
[24] and standard mixture of unigrams for modeling doc-
uments. [25] performed unsupervised feature selection
by minimizing the message length of the data, consider-
ing mixtures of generalized Dirichlet distributions. This
model was then optimized in a Bayesian framework via
variational inference in [26].

Contributions of

this paper: Compared to previous

works, our main contributions are:

1) We extend the concept of shared feature space from
standard mixtures to more general topic models,
allowing presence of multiple topics in documents.
In doing so, we achieve sparsity in topic propor-
tions and in topic-speciﬁc words. Prior works at
best achieve sparsity in one of these two senses.
2) Unlike most works, our model allows the subset
of salient words to be topic-speciﬁc. This follows
the premise that some words may have common
frequency of occurrence under some subset of the
topics. For example, the word “component” has
different meanings under “statistics” and “machine
learning” than under other topics, and could have
higher frequencies of occurrence for these special-
ized topics.

3) We derive a novel BIC objective function, used for
learning our model. Unlike the na¨ıve form, satis-
fyingly, our derived objective has distinct penalty
terms for the different parameter types in our
model, interpretable vis a vis the effective sample
size for each of the parameter types.

The rest of the paper is organized as follows: Section
2 introduces the notation we use throughout. In section
3, we brieﬂy review LDA and introduce a criterion to
measure sparsity of topics for LDA. Next, in section 4, we
present our parsimonious model. Section 5 derives our
BIC objective function. Then, in section 6, we develop
the joint structure and parameter learning algorithm for
our model, which locally minimizes our BIC objective.
Experimental results on three text corpora and an image
dataset are reported in section 7. Concluding remarks
are in section 8.

2 NOTATION AND TERMINOLOGY

D

is a collection of D documents and a
A corpus
dictionary is a set of N unique words. We index unique
documents in the corpus and unique words in the dic-
tionary by d
and n
1, ..., N
, respectively.
∈ {
}
Also, j
is a model’s topic index, M the total
1, ..., M
∈ {
number of topics.

1, ..., D

∈ {

}

}

We deﬁne the following terms speciﬁc to our modeling

of each document d:

• Ld is the number of words in document d.
• wid ∈ {

, i = 1, ..., Ld is the i-th word in

1, ..., N

}

document d.

3

• vjd ∈ {

}

0, 1

indicates whether (vjd = 1) or not (vjd =

0) topic j is present in document d.

M

Pj=1

• Md ≡

vjd ∈ {

}

1, ..., M

is the number of topics

present in document d.

• αjd is the proportion for topic j in document d.
The following quantities are speciﬁc to each topic:
• βjn is the topic-speciﬁc probability of word n under

topic j.
• ujn ∈ {

}

N

0, 1

indicates whether (ujn = 1) or not

(ujn = 0) word n is topic-speciﬁc under topic j.

ujn is the number of topic-speciﬁc words

Pn=1
in topic j.
D
d=1 Ldvjd is the sum of the lengths of the

• Nj ≡
• ¯Lj ≡
Finally, β0n is the probability of word n under the

documents for which topic j is present.

P

shared model.

3 LATENT DIRICHLET ALLOCATION

LDA is a generative model originally proposed for ex-
tracting topics and organizing text documents [7]. Its
generative process for a document d is as follows:

1) Choose topic proportions αd from a Dirichlet dis-

tribution with parameter η, i.e. αd ∼

Dir(η).

2) For each of the Ld words wid:
a) Choose a topic zid ∼
b) Choose word wid

M ultinomial(αd).
to
according

multinomial distribution for topic zid,
βzidn, n = 1, ..., N
.
}

{

the
i.e.

3.1 Parameter Estimation in LDA

The main inference in LDA is computing the posterior
, given a docu-
probability of the hidden variables
ment. Exact inference is intractable. Thus, approximate
algorithms such as variational inference [7] and Markov
Chain Monte Carlo [27] have been used. Here we brieﬂy
review mean-ﬁeld variational inference.

zid}

{

Ld

),

φ(d)
i

γ(d))

γ(d), φ(d)) = q(αd|

Approximate inference in this method is achieved
by obtaining a lower bound on the log-likelihood. A
new family of variational distributions is deﬁned by
changing some of the statistical dependencies in the
original model:
q(zid|
q(αd, zd|
Here, zd is the set of hidden variables

∀
z1d, ..., zLdd}
{
γ(d)) is a Dirichlet distribution with parameter
and q(αd|
γ(d). Also, q(zid|
) is a multinomial distribution on
i1 , ..., φ(d)
the M topics, with variational parameters φ(d)
iM .
The values for γ and φ for each document are de-
termined by minimizing the Kullback-Leibler (KL) di-
vergence between q and the posterior distribution of
hidden variables, which gives a lower bound on the
single-document log-likelihood. Update equations for
the variational parameters are:

φ(d)
i

Yi=1

(1)

d.

βjwid exp (Ψ(γ(d)

)

j

φ(d)
ij ∝
γ(d)
j = ηj +

Ld

Xi=1

φ(d)
ij

M

Ψ(

−

Xj′=1

γ(d)
j′ ))

where Ψ(
) is the ﬁrst derivative of the log-gamma
·
function. Also the φ parameters are constrained so that

M

ij = 1 i = 1, ..., Ld, d = 1, ..., D.

j=1 φ(d)
In the next step, after optimizing the variational pa-
P
rameters, the lower bound is optimized with respect
to parameters of the model (i.e. the word probabilities
under each topic and η). For the word probabilities, this
minimization is achieved via the closed form updates:

βjn =

D
d=1

Ld

i=1:wid=n φ(d)
i=1 φ(d)

Ld

ij

ij

P
D
d=1

P

,

n, j.

∀

(4)

P
There is no closed-form update for η. It is updated
using Newton-Raphson [7]. This process is iteratively
repeated until a termination criterion is met.

P

3.2 Sparsity in LDA

Sparsity of topic proportions is controlled by η, a corpus-
level parameter optimized along with all other param-
eters of the model. Values of η smaller than 1 lead to
sparser topic proportions in a given document.

In order to compare sparsity in LDA with our model,
we estimate the actual number of LDA topics present
in each document. The variational parameter φ(d)
is
ij
essentially the probability that word i in document d is
generated by topic j. To estimate the number of topics
present, we hard-assign each word in a document to the
topic that has maximum φ(d)
ij , j = 1, ..., M . In so doing,
we determine the set of topics used to model at least
one word in a given document. This is how we measure
topic sparsity for LDA.

In the next section, we develop our new topic model,
which fundamentally differs from LDA in: 1) possessing
two types of sparsities; 2) treating parameters as deter-
ministic unknowns, rather than random variables, i.e. we
take a maximum likelihood rather than Bayesian learn-
ing approach [28]; thus, our method does not require
variational inference; 3) requiring no hyperparameters.
In LDA, M is the sole hyperparameter. However, in our
approach, M is automatically estimated, along with the
rest of our model.
4 PARSIMONIOUS TOPIC MODEL
In this section we develop our parsimonious model
and its associated parameter estimation. For clarity’s
sake, in this section, we will focus only on parameter
estimation given the model structure assumed known, i.e.
the subset of topics present in each document (speciﬁed
by v =
vjd}
), the topic-speciﬁc words under each topic
{
(speciﬁed by u =
), and the number of topics,
M . Structure learning and model order selection will be
developed in section 6.

ujn}

{

We treat model parameters as deterministic un-
to be estimated by maximum likelihood.

knowns,

4

(2)

(3)

While the Bayesian setting is a natural alternative,
our approach (developed in section 5) approximates
the Bayesian model posterior. Moreover, unlike a fully
Bayesian approach, our approach gives a computation-
ally tractable way for learning parsimonious models, and
thus for avoiding overﬁtting.
Stochastic Data Generation:
The document corpus is generated under our model as
follows.

1) For each document d = 1, . . . , D
2) For each word i = 1, . . . , Ld

.

{

{

}

{

0n

, n = 1, . . . , N

Note that v =

αjvjd, j = 1, . . . , M

ity mass function (pmf)

a) Randomly select a topic based on the probabil-
.
}
b) Given the selected topic l, randomly generate
the i-th word based on the topic’s pmf over
ln β1−uln
βuln
the word space
vjd}

indicate the topics present in each
document – topic j’s probability (αjvjd) of generating
a word in document d is non-zero only if vjd = 1.
Likewise, (ujn = 1) if topic j possesses a topic-speciﬁc
probability βjn of generating word n; otherwise, this
probability is β0n. As aforementioned, the u and v
switches together with the number of topics, M , specify
(M, v, u). Given a ﬁxed structure,
the model structure
the full complement of model parameters is given by Θ =
(M, v, u)

{{
and parameters Θ constitute our model.

β0n}
Given the above data generation mechanism, the like-

. Together, the structure

H
αjd}}

βjn}

H

{

{

,

,

lihood of the corpus

D

D
Ld

under our model is:
M

p(

D|H

, ˆΘ) =

Yd=1

Yi=1

Xj=1

[αjdvjd ·

β

ujwid
jwid

β

1−ujwid
0wid

].

(5)

Here, the double product appears because documents
and words are generated i.i.d. Also, we emphasize for
clarity that ujwid is topic j’s binary switch variable on
word wid, the i-th word from document d.

P

N
n=1 (ujnβjn + (1

The set of word probability parameters (both topic-
speciﬁc and shared) must be jointly constrained so
that they give a valid pmf, conditioned on each topic,
j. Likewise,
i.e. (
the topic proportions must satisfy the pmf constraints
d. In the parameter learning step of
αjd}}
our algorithm we estimate Θ =
P
consistent with these constraints, assuming the model
structure
4.1 Parameter Estimation

(M, v, u) is known.

M
j=1 αjdvjd = 1,

ujn)β0n) = 1),

β0n}

βjn}

{{

H

−

∀

∀

{

{

,

,

Based on the derivation given in the sequel in section
5, we seek the model which minimizes an objective
function of the BIC form: (penalty term)
(data log-
likelihood). Moreover, as will be seen, the penalty term
in fact has no dependence on the model parameters, Θ.
Thus, minimizing BIC with respect to Θ is equivalent
to maximizing the log-likelihood with respect to Θ.
Accordingly, in our framework, the parameters Θ are
chosen to (locally) maximize the log-likelihood of the
via the Expectation Maximization (EM) algorithm
data
[29], as we next formulate.

−

D

{

Z (j)
id }

Let Zid be an M-dimensional binary random vector
that has only a single element equal to one and all other
elements equal to zero. Here, the non-zero element in
Zid (i.e. j s.t. Z (j)
id = 1) is the topic of origin for word
wid. We treat Z =
as the hidden data within our
EM framework [29]. At each iteration, EM maximizes
a lower bound on the (incomplete) data log-likelihood,
with guaranteed monotonic increase in the incomplete
data log-likelihood and locally optimal convergence [29].
Each such iteration consists of i) an E-step, comput-
ing the expected value of the hidden data, given the
observed data and current parameter estimates, which
is used to determine the expected complete data log-
likelihood; ii) An M-step, in which we update the model
parameters by maximizing the expected complete data
log-likelihood. For our likelihood model (5) and choice
of hidden data, these two steps are speciﬁed as follows:
E-step: Given the model structure and the current esti-
mate of the parameters Θ(t), we compute

=

P

E[Z (j)

id |D

; Θ(t),

H

] = P (Z (j)

id = 1
ujwid
αjdvjdβ
β
jwid
ulwid
M
l=1 αldvldβ
lwid

wid; Θ(t),
|
1−ujwid
0wid

β

1−ulwid
0wid

)

H

,

i, d,

∀

(6)

i.e., the expected hidden data in this case are the poste-
rior probabilities on topic origin

id.

Adding normalization constraints to the expected
value of the complete data log-likelihood measured with
respect to (6), we construct the Lagrangian at the current
parameter set estimate Θ(t):

∀

E[ log(p(

, Z

Θ,

D

|

))
|

H

Θ(t),

] =

H

D

Ld

M

vjd

Xd=1

Xi=1

Xj=1 h

P (Z (j)

id = 1

wid; Θ(t),

|

·

)
(cid:16)

H

log(αjd) + ujwid log(βjwid )

D

M

+ (1

ujwid ) log(β0wid )

−

λd(

(cid:17)i −

Xd=1

Xj=1

αjdvjd −

1)

µj

(ujnβjn + (1

ujn)β0n)

(7)

−

1

.
(cid:17)

−

M

N

−

Xj=1

(cid:16)

Xn=1

M-step:

Maximization with respect to topic proportions is
achieved by setting the partial derivative of (7) with
respect to αjd equal to zero and satisfying the normal-
ization constraint:

αjd =

Ld

i=1 P (Z (j)

id = 1
i=1 P (Z (l)

|
id = 1

Ld

M
P
l=1

P

P

wid; Θ(t),

)vjd

H
wid; Θ(t),

,

)vld

H
j, d, : vjd = 1.

|

∀

Similarly, maximization with respect to βjn is achieved

by:

βjn =

xjnujn
µj

,

(8)

(9)

where

xjn ,

D

Ld

Xd=1

Xi=1:wid=n

P (Z (j)

id = 1

wid; Θ(t),

|

)vjd,

j, n.

H

∀

5

Here, µj is the Lagrange multiplier, which we solve
for by multiplying both sides of (9) by ujn, summing
over all n, and applying the distribution constraint on
topic j’s word model to obtain:

µj =

¯xj
N
n=1 (1

1

−

P

ujn)β0n

−

, ¯xj ,

N

Xn=1

xjnujn,

j.

(10)

∀

The E and M-steps are alternately applied, starting
from initial parameter estimates, until a convergence
condition (assessing diminishing improvement in log-
likelihood from one iteration to the next) is met.

Estimating the Shared Model Parameters:

β0n}
{

In principle, we can take the derivative of (7) with
and optimize it along with all other
respect to
parameters. Alternatively, we can treat the shared model
as a “universal” model that is estimated once, at initial-
ization, and then held ﬁxed while performing EM. Our
experiments show that this latter approach is quite effec-
tive. Thus, in this work the shared model is estimated
only once, via the global frequency counts:

β0n =

P

D
d=1

Ld
i=1:wid=n 1
P
D
d=1 Ld

,

∀

P

n = 1, ..., N.

(11)

H

5 BIC DERIVATION
In this section we derive our BIC objective function,
with respect to which we jointly optimize the entire
(M, v, u) and Θ. For ﬁxed M and Θ, we will
model –
minimize BIC with respect to (v, u). Alternately, given
ﬁxed structure, minimizing BIC with respect to Θ is
achieved by EM as described in section 4.1. Using such
alternating minimization, locally BIC-optimal models are
learned at each M . The BIC-minimizing model, at order
M ∗, is then chosen.

The posterior probability of the model structure
)p(

H
is proportional to p(
) the marginal
D|H
likelihood and p(
), the structure prior. The marginal
likelihood is computed by taking expectation of the data
likelihood with respect to the prior distribution on the
parameters:

), p(

D|H

H

H

I = p(

) =

p(

D|H

Z

D|H

, Θ)p(Θ

)dΘ,

(12)

|H

|H

|H

D|H

, Θ)p(Θ

We approximate log (p(

) the prior on parameters given structure

with p(Θ
.
H
We use Laplace’s method to approximate (12). This is
based on the assumption that p(
) is peaked
around its maximum (the posterior mode ˜Θ), which in
fact is valid for large sample sizes [9].
, Θ)p(Θ

)) around the
D|H
posterior mode using a Taylor series expansion. Note
that the ﬁrst Taylor term is constant with respect to
the variables of integration and the term corresponding
to ﬁrst derivatives is zero. Thus, exponentiating the
Taylor’s series expansion at second order and plugging
into (12), we obtain an approximation of the integral:

|H

ˆI = p(

, ˜Θ)p( ˜Θ

D|H

)

|H

Z

e(− 1

2 (Θ− ˜Θ)T ˜Σ(Θ− ˜Θ))dΘ,

(13)

where ˜Σ is the negative of the Hessian matrix.

|

|

˜Σ

−1/2

The integrand in (13), the exponentiated second order
Taylor series term, is a scaled normal distribution with
mean ˜Θ and covariance ˜Σ, which, when integrated,
evaluates to (2π)k/2
. Here k is the number of
D
M
j=1 Nj,
d=1 Md +
parameters in Θ, which is equal to
M
P
j=1 vjd is the number of active topics
where Md =
N
n=1 ujn is the number of
in document d and Nj =
topic-speciﬁc words under topic j. Also, since the shared
P
model parameters are estimated in a universal fashion,
once, and then ﬁxed (11), their descriptive complexity is
(M, v, u) and Θ.
assumed to be ﬁxed, irrespective of
Thus, this (assumed constant) term need not be included
in our objective function.

P

P

H

We thus have the following approximation for the

marginal likelihood [9],[30]:

ˆI = (2π)k/2

˜Σ

−1/2

p(

|

|

, ˜Θ)p( ˜Θ

).

|H

D|H

(14)

Based on (14), an approximate negative log-model

posterior (BIC cost) is:

BIC =

log( ˆI) =

−
log(p(

, ˜Θ))

log (2π) +

k
2
log(p( ˜Θ

−

))

1
2

−

˜Σ

)

|

log (
|
log(p(

)).

H

−

D|H

|H
As usually done, we take p( ˜Θ

−

) to be an uninfor-
mative prior, which can be neglected. In this way, we
transition from a random description of the parameters
Θ to treating them as deterministic unknowns.

|H

By minimizing BIC, a tradeoff is achieved between the
, ˜Θ)) and the other terms,

data log-likelihood, log(p(
interpreted as the model “complexity” cost.

D|H
The negative of the Hessian matrix ˜Σ, neglecting

p(Θ

), is:

|H
[ ˜Σ]ij =

−

∂2 log (p(

, Θ))

D|H
∂Θi∂Θj

The diagonal elements corresponding to topic propor-

, i, j = 1, 2, ...k.

(16)

(cid:12)
Θ= ˜Θ
(cid:12)
(cid:12)
(cid:12)

tions are:

where

−

∂2 log(p(D|H, Θ))
∂α2
jd

= −vjd
Θ= ˜Θ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ld
X
i=1

∂2 log(p(wid|H, Θ))
∂α2
jd

(cid:12)
, (17)
(cid:12)
(cid:12)
(cid:12)
Θ= ˜Θ
(cid:12)

p(wid|H

, Θ) =

M

Xl=1

αldvldβ

ulwid
lwid

β

1−ulwid
0wid

.

(18)

Invoking the weak law of large numbers, we have with

probability 1 that

∂2 log(p(

, Θ))

−

D|H
∂α2
jd

Ld

Θ= ˜Θ

(cid:12)
(cid:12)
(cid:12)
∂2 log(p(wid|H
(cid:12)
∂α2
Xi=1
jd
∂2E[ log(p(Wid|H
, Θ))]

Ld

∂α2
jd

, Θ))

=

vjd

−

1
Ld

vjdLd

→ −

(19)

(cid:12)
Θ= ˜Θ
(cid:12)
(cid:12)
(cid:12)

Θ= ˜Θ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= vjdLdI (α)
jd ,

6

(20)

1, . . . , N

where Wid ∈ {
is
the corresponding element in the Fisher information ma-
trix. Similarly, diagonal elements of the Hessian matrix
corresponding to βjn are

is a random index and I (α)
jd

}

∂2 log(p(

, Θ))

=

D|H
∂β2
jn
D

Ld

−

ujn

−

1
¯Lj

¯Lj

∂2 log(p(wid|
∂β2
jn

Xi=1

Xd=1
D

ujn(

→ −

Xd=1

∂2E[ log(p(Wid|

¯Lj)

∂β2
jn

H, Θ))

(cid:12)
(cid:12)
Θ= ˜Θ
(cid:12)
(cid:12)
H, θ))]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= ujn ¯LjI (β)
jn
Θ= ˜Θ

Here I (β)
is the Fisher information matrix element
jd
corresponding to βjn; moreover, we identify ¯Lj ≡
D
d=1 Ldvjd.
P
Following the common approach in deriving BIC [30],
we neglect the off-diagonal elements and write ˜Σ as a
block-diagonal matrix,
˜Σα
0

˜Σα , Diag[LdI (α)
jd ],
˜Σβ , Diag[ ¯LjI (β)
jn ].

0
˜Σβ (cid:21)

, where

˜Σ =

(21)

(cid:20)

As the sample size grows, the Fisher matrix terms
jd ) become negligible [30]. Thus, we

jn and I (α)

(15)

in (21) (I (β)
have:

log (
|

˜Σ

)

|

≈

(Md −

D

Xd=1

M

Xj=1

1) log(Ld) +

Nj log( ¯Lj).

(22)

This expression is reminiscent of the penalty term
in the na¨ıve BIC form where, for each parameter, one
“pays” 1
2 log(sample size). But here, in (22), we identify
different penalties for each of the two different parame-
ter types in our model, with the argument of the log() the
effective sample size, consistent with our BIC derivation.
That is, this effective size is Ld for an αjd parameter
and ¯Lj
for a βjn parameter. However, we can also
sanity-check this interpretation against our parameter
re-estimation equations. Note in particular that the αjd
parameters are indeed re-estimated in (8) based on a sum
over Ld “word samples” and the βjn parameters are re-
estimated in (9) based on a sum over ¯Lj “total word
samples over all documents in which topic j is present”.
Thus, indeed, our derivation leads to a generalization of the
standard BIC cost, wherein the penalty on a parameter of given
type is 1

2 log(effective sample size).

The other important term in (15) is the prior proba-
bility of the model structure p(
), deﬁned based on the
conﬁgurations of u and v switches for a given number
of topics M . Here, we assume that priors on u and v
switches are independent.

H

Our general principle in deﬁning these prior dis-
tributions is to invoke uninformativeness (uniformity).
We parameterize the prior distribution on u switches
as a function of
topic-speciﬁc
words across all topics, N0, and (uninformatively) as-
sume all conﬁgurations with at most N0 “on” switches
(
) are equally likely. However, under the
(cid:1)

the total number of

N0
n=1

MN
n

(cid:0)

P

assumption that, generally, a small number of words
N M ),
across all topics will be topic-speciﬁc (N0 ≪
the total number of these conﬁgurations can be well-
approximated by [31]:

N0

Xn=1

M N

2MN H(

N0
M N )

,

(cid:18)

n (cid:19) ≈

√M N

(23)

−

1
2

) is Shannon’s entropy for a Bernoulli random
MN [32]. Accordingly, the cor-

where H(
·
variable with parameter N0
responding term in BIC is:
¯N
N
M
j=1 Nj is the average number

where ¯N , 1
of topic-speciﬁc words across all topics.

log(p(u)) = M N H(

M N0 = 1

log(M N ),

) log(2)

(24)

To deﬁne the prior on v conﬁgurations, we consider
a two stage process for each document, d. First, the
number of topics (Md) is selected from a uniform distri-
bution ( 1
M ); then a switch conﬁguration is selected from
M
a uniform distribution over all
conﬁgurations with
Md
Md “on” switches. Therefore,
(cid:0)

P

−

M

log(p(v)) = D log(M ) +

−

log

M
Md(cid:19)

.

(cid:18)

(25)

(cid:1)
D

Xd=1

By substituting (22), (24), and (25) into (15), our overall

BIC expression becomes:

BIC = D log(M ) +

log

+ M N H(

) log(2)

M
Md(cid:19)

(cid:18)

¯N
N

(Md −

1) log(

Ld
2π

)

D

Xd=1
D

1
2

Xd=1

¯Lj
2π

)

−

1
2

1
2

−

+

log(M N ) +

M

Xj=1

Nj log(

log(p(

, ˜Θ)) .1

D|H

(26)

6 INTEGRATED MODEL SELECTION AND PA-
RAMETER ESTIMATION

H

In this section we develop our algorithm for jointly
(M, v, u) and Θ to minimize the BIC
determining
objective (26). Supposing, for the moment, that M is
ﬁxed, there are 2MD+N M different possible (u, v) switch
conﬁgurations – global minimization of BIC over this
search space is a formidable task. However, using a
generalized Expectation Maximization (GEM) algorithm
[29],[33], we can break up this problem into a series of
simple update steps, each decreasing in BIC, with the
overall algorithm thus guaranteed to converge to a local
minimum.

To formulate this approach, we note that the EM
framework is applicable to monotonically descend in the
BIC cost (26), just as it is applied to monotonically ascend
, ˜Θ)).
in the incomplete data log-likelihood log(p(
Toward this end, we deﬁne the expected complete data

D|H

1. We remind the reader that, though this explicit dependence is

omitted, Nj is a function of u and Md is a function of v.

7

D

, Z

D|H

BIC cost. This is formed by: 1) substituting in the com-
, ˜Θ)), to replace the
plete data log-likelihood log(p(
|H
, ˜Θ)), in (26),
incomplete data log-likelihood log(p(
yielding the complete data BIC; 2) Taking the expected
value of the complete data BIC. The only term in the
complete data BIC that is a function of the (random)
hidden variables is the complete data log-likelihood.
Thus, the expected value of the complete data BIC is
formed simply by replacing the incomplete data log-
likelihood term in (26) by the expected complete data
log-likelihood term from (7).

This expected complete data BIC is the keystone quan-
tity for deﬁning our GEM algorithm, which descends in
, Θ). Given
(26) while iteratively re-estimating the pair (
a ﬁxed model order M , our GEM algorithm consists of
the following steps, iterated until convergence:

H

1) E-step: Following section 4.1, expected values of
the hidden variables are given in (6). Based on the
discussion above, the expected complete data BIC
is formed by replacing the incomplete data log-
likelihood term in (26) by the expected complete
data log-likelihood term from (7).

2) Generalized M-step:

a) Estimation of Θ given ﬁxed structure: Since
the model complexity terms in BIC have no
dependence on Θ, minimization of the ex-
pected complete data BIC with respect to Θ
given ﬁxed structure is equivalent to maximiz-
ing the expected complete data log-likelihood,
with closed form updates as described in sec-
tion 4.1.

b) Optimization of (u, v) given ﬁxed Θ.

Updating the (u, v) switches given ﬁxed parameters
(step 2(b)) is done via an iterative loop in which switches
are cyclically visited one by one, with all parameters and
other switches ﬁxed. If a change in the current switch
reduces BIC, that change is accepted.2 This process is
repeated over all switches until no further decrease in
BIC occurs or a predeﬁned maximum number of cycles is
reached. We update the u and v switches separately, ﬁrst
performing cycles over all u switches until convergence
and then over the v switches until convergence. We then
go back to the E-step.

A change in one switch affects both the model cost
(∆Cost) and log-likelihood (∆L) terms in BIC. In the
following sections, we determine (∆BIC = ∆Cost
∆L)
associated with switch updates.

−

6.1 Updating
Here, we trial-ﬂip each switch uj′n′, n′ = 1, ..., N in every
topic j′ = 1, ..., M , one by one. In performing these

ujn}

{

2. For computational efﬁciency, optimization over u is with respect
to the expected complete data BIC. However for v, we directly min-
imize BIC since there is no computational advantage in minimizing
with respect to expected BIC in this case. Since minimizing expected
complete data BIC ensures descent in BIC, both u and v update steps
descend in the BIC objective (26).

µ+
j′
µ−
j′

),

(28)

.

(29)

updates, each topic is constrained to have at least one
topic-speciﬁc word.

In the model cost terms of BIC, a change in the value
of uj′n′ only affects the number of topic-speciﬁc words
in that topic, N ′

j. Thus,

∆Cost(uj′n′) =

−

1)u

j′ n′ log(

1
2

(
−

+ N M log(2)(H(

¯Lj′
)
2π
¯N +
N

)

H(

)),

¯N −
N

−

(27)

where − and + superscripts denote the current and new
values of functions of the switch, respectively.

In computing the change in the log-likelihood, we
need the corresponding word probability under the new
and old values of the switch. Introducing the variables
xjn and ¯xj from (9) and (10) allows us to compute µ+
j′
efﬁciently (without performing the E-step and recomput-
ing the required statistics). The effect of changing uj′n′
on the expected value of the complete data log-likelihood
is:

∆L(uj

′

′ ) = (
n

−

−

1)u

j′ n′ xj′n′ log(

xj′n′
µ+
j′ β0n′

)

−

¯x−
j′ log(

where:

µ+
j′ =

−

1)u

j′ n′ xj′n′

¯x−
j′ + (
−
u−
j′n)β0n + (

N
n=1 (1

−

−

1)u

j′ n′ β0n′

−

1

−

{

vjd}

P
6.2 Updating
We update the v switches by sequentially visiting each
topic vj′d′ j′ = 1, ..., M in every document d′ = 1, ..., D.
Obviously, each document should be constrained to
have at least one active topic. Moreover, every topic is
constrained to be used by at least one document.

Trial-ﬂipping switch vj′d′ changes Md′ and ¯Lj′ . There-

fore,

∆Cost(vj′d′ ) = log( (cid:0)

1
2 · (cid:20)

(
−

−

1)v

j′ d′ log(

Ld′
2π

)

(cid:1)

) +

M
M +
d′
M
−
M
d′
(cid:0)
(cid:1)
¯L−
j′ + (

+ Nj′ log

(cid:16)

−

j′ d′ Ld′

1)v
−
¯L−
j′

.

(cid:17)(cid:21)

(30)

Unlike the u updates, since expected values of hidden
variables are zero for topics j′ such that vj′d′ = 0, we
need to recompute (6) for document d′ and trial-update
j = 1, .., M using (8). Then, we compute ∆L(vj′d′)
αjd′
based on the incomplete data log-likelihood:

∀

Ld′

∆L(vj′d′) =

log

jd′ β

M

j=1 α+
j=1 α−

jd′ v+
jd′ v−

M

ujw
id′
jwid′ β
ujw
id′
jwid′ β

1−ujw
0wid′
1−ujw
0wid′

id′

id′

.

(cid:16) P
P

Xi=1

jd′ β

(cid:17)
(31)
Note that since all topic proportions in document
d′ change when ﬂipping one switch, unlike computing
∆L(uj
′ ), there is no computational beneﬁt in using
n
the expected value of the complete data log-likelihood.
This is why we are evaluating the incomplete data log-
likelihood for these updates.

′

8

6.3 Initialization

It is important to initialize the model sensibly to avoid
poor local minima. Here, we use a simple but prag-
matic initialization process: 1) to initialize each topic,
we randomly select Dinit documents. Only the words
that occur in these documents are initially chosen as
topic-speciﬁc, and their probabilities are initialized via
frequency counts; 2) Based on this initial model, we use
a maximum likelihood decision rule to hard-assign each
document to a single topic; 3) Using this now more
“reﬁned” set of documents for each topic, we re-estimate
topic-speciﬁc word probabilities via frequency counts.

6.4 Computational Complexity

For ﬁxed M , the computational complexity of our pa-
rameter learning is O(MDDLD) where MD and LD
are the number of topics present in a document and
the length of a document, respectively. Since in LDA
all topics are potentially present in each document, its
computational complexity is O(M DLD), higher than for
our model. However, structure learning in our model
imposes further complexity.

Updates of word switches involve an iterative loop
over all words in all topics. But the trial-ﬂip of a single
switch only requires scalar computation. Thus, complex-
ity of this part of structure learning is O(M N ).

Experimentally, we ﬁnd that the heaviest part of our
algorithm is updating the v switches, where at each
step we trial-estimate topic proportions and evaluate the
incomplete data log-likelihood for the current document
under consideration. We need computations of order
O(MDLD) for trial-update of each switch. Thus the total
complexity for this part, including the loop over all v
switches, is O(M MDDLD).

Overall, computational complexity of our model,
O(M MDDLD + M N ), is higher than for LDA. Complex-
ity may be an issue for a corpus with a large number of
active topics across documents. However, we have also
investigated complexity experimentally, via recorded ex-
ecution times, and have found that our method and LDA
require comparable execution. (cf. Table 4).

6.5 Model Order Selection

We compare estimated models at different orders and
choose the one with minimum BIC. Different strategies
are conceivable for learning models over a range of
orders. A sensible approach is to learn a model at some
order and use it for initializing new models at other
orders. In this way, we can sweep a range of model
orders in either a bottom-up or top-down fashion. In
the top-down approach, we initialize our model with a
speciﬁed “ceiling” number of topics Mmax and reduce
the number of topics by a predeﬁned step. We remove
the least “plausible” topics from the existing model
and apply the learning algorithm to minimize BIC at
the now reduced model order, using the current set

of model parameters as initialization. Here, as “least
plausible”, we simply remove the topics with the small-
est aggregate mass across the document corpus. This is
applied repeatedly until a minimum number of topics
is reached. Experimentally, this method has been found
to be superior to an alternative “bottom-up” approach.
Thus, we have applied the top-down method in our
experiments.

7 EXPERIMENTAL RESULTS

In this section we report results of our model on
the Ohsumed, Reuters-21578, and 20-Newsgroup cor-
pora as well as a subset of the LabelMe [34] image
dataset. For each dataset, we compared against LDA3
with respect to model ﬁt (training and held-out log-
likelihood), sparsity, and a class label consistency mea-
sure. We also compared against an extension of LDA
(LDAbackground) which includes one additional topic
(a background topic) whose probabilities are ﬁxed at
the global frequency estimates. Furthermore, we com-
pared against STC4[20] with respect to class label con-
sistency5. C implementation of our model is available
from https://github.com/hsoleimani/PTM.

[35] suggests that comparing log-likelihoods com-
puted by different topic models may be unfair, since
each may compute a different approximation of the log-
likelihood. Here, we use the method described in [35],
[36] to compare model ﬁtness on a held-out test set. In
this approach, we divide each document in the test set
into two sets, observed and held-out parts, keeping the
set of unique words in these two parts disjoint. We use
the observed part of the test set to compute the expected
topic proportions and then compute the log-likelihood
on the words in the held-out set:

Dtest

Lheldout

d

M

log

Eq[αjd]Eq[βjwid ]

.

(32)

Xd=1

Xi=1

(cid:16)

Xj=1

(cid:17)

Note that for LDA, Eq[αd] are the variational param-
eters γd normalized to sum to one, while for our model
they are directly the topic proportions αd. Eq[βjn] is
ujnβjn + (1
ujn)β0n in our model. In LDA, however,
Eq[βjn] is simply equal to the corresponding estimated
word probability.

−

Topic models can also be compared with respect to
“quality” of extracted topics. From a computational
linguistics standpoint, topics are expected to exhibit a
coherent semantic theme rather than covering loosely
related concepts. In this paper, we use the criterion
proposed in [37] to evaluate coherence of learned topics.
This measure has been shown to be in agreement with
experts’ coherence judgments [37]. The topic coherence of

3. http://www.cs.princeton.edu/∼blei/lda-c/
4. http://www.ml-thu.net/ jun/stc.shtml
5. Since STC is not a generative modelling approach, it is not suitable

9

topic j, which is based on word co-occurrence frequen-
cies, is computed as:

C(j; N (j)) =

T

k−1

Xk=2

Xl=1

log

S(n(j)

k , n(j)
l
S(n(j)
)
l

) + 1

,

(33)

1 , ..., n(j)

where N (j) = (n(j)
T ) are the T most probable
topic-speciﬁc words under topic j and S(n, n′) is the
number of documents in the corpus containing both
words n and n′. Similarly, S(n) represents the number of
documents with word n. Here, we use the top 2 percent
of topic-speciﬁc words in each topic for our model to
compute coherence. We use the same number of top
words to compute coherence for LDA topics. In our
experiments, we report the average coherence over all
topics.

Results reported here are averaged based on 10 differ-
ent initializations. For STC, we determined the hyper-
parameters by a validation set approach, working on
the model with highest number of topics, and then kept
them ﬁxed for all other model orders. This makes STC
complexity manageable and is also reasonable because
we observe that the best STC accuracies are anyway
achieved at the highest model order. The validation set
was created by randomly choosing 20% of the docu-
ments in the training set. For inference on test docu-
ments, for our model, we allow all topics to be active
in each document and only optimize topic proportions
using (8), given a ﬁxed number of topics. We have taken
this approach rather than using a transductive inference
approach.

7.1 Ohsumed Corpus

Ohsumed6 consists of 34389 documents, each assigned
to one or multiple labels of the 23 MeSH diseases
categories. Documents have on average 2.26 (std. dev.
= 0.84) labels. The dataset was randomly divided into
24218 training and 10171 test documents. There are 12072
unique words in the corpus after applying standard
stopword removal.

For all four methods, models were initialized with 150
topics and at each step the ﬁve least massive topics were
removed. Fig. 2 shows the BIC curve and the training
and held-out log-likelihood of our model compared to
LDA and LDAbackground. The minimum of the BIC
curve (i.e. estimated number of topics) is on average
M ∗ = 105 (std. dev. = 5.98). The ﬁgure shows that LDA
achieves higher log-likelihood on the training set for
models with more than 120 topics; but by controlling
the likelihood-complexity trade-off, our model achieves
higher log-likelihood on the held-out set at all orders.

We also used the class labels provided with the dataset
to evaluate a class label consistency measure. We ﬁrst
associated to each topic a multinomial distribution on
the class labels. We learned these label distributions

to evaluate it with respect to data likelihood.

6. http://disi.unitn.it/moschitti/corpora.htm

BIC

Training Log−
Likelihood

Held−out Log−
Likelihood

107

1.65
1.64
107
−1.57

−1.67
106
−3.4
−3.5
−3.6

10

AUC

0.70

0.65

0.60

0.55

−500

−2500

Coherence

−1500

(a)

(b)

20

40

60

80

100
Number of Topics

120

140

20

40

60

80

100
Number of Topics

120

140

LDA

LDAbackground

Our Model

LDA

LDAbackground

Our Model

STC

Fig. 2: Average performance, based on 10 random ini-
tialization, for the Ohsumed corpus.

Fig. 3: (a) Class label consistency and (b) coherence
versus number of topics on Ohsumed.

for each topic by frequency counting over the ground-
truth class labels of all documents, weighted by topic
proportions:

pj(c) =

D
d=1
P
D
d=1

|Cd|
i=1:lid=c αjdvjd
|Cd|
i=1 αjdvjd

P

,

j, c,

∀

(34)

|

|

C

P

P
where lid is the i-th class label in document d and
Cd|
and
are, respectively, the number of labels for docu-
ment d and the total number of class labels. For labeling
a test document, we then compute the probability of
each class label based on the topic proportions in that
M
j=1 αjdpj(c) c = 1, ...,
document; i.e.
, and assign
the labels that have probability higher than a threshold
value ν:

P

C

M

|

|

|

ˆCd =

c :

(cid:26)

Xj=1

αjdpj(c) > ν

(cid:27)

(35)

As label consistency criteria, we measure precision and
recall on the test set. Precision is the number of true
discovered labels divided by the total number of ground-
truth labels. Recall is the number of correctly classiﬁed
labels divided by the total number of labels assigned to
documents by our classiﬁer. We measure these criteria
for different threshold values ν and report the area under
the precision/recall curve (AUC) as the ﬁnal measure of
performance. For LDA and LDAbackground we used the
normalized Dirichlet variational parameters γ(d)
in (3) as
topic j’s proportion for document d. Similarly, we used
STC’s normalized document codes for its unsupervised
j, d.
classiﬁcation. For both LDA and STC we set vjd = 1
Fig. 3a shows the AUC for our model, LDA, LD-
Abackground and STC. We see that adding a back-
ground model to LDA improves class label consistency.
However, the best performance of our model is better
than other methods’ over the entire range of model
orders. Also, the highest AUC for our model occurs near
M ∗ = 105, which is the minimum of the BIC curve.

∀

j

Average coherence over all topics for our model, LD-
Abackground and LDA are plotted in Fig. 3b. This ﬁgure
shows that the average coherence in our model is higher
than LDA and LDAbackground for a wide range of
model orders.

Tables 1 and 2 compares sparsity in our model against
LDA, LDAbackground, and STC at M = 105. As the
measure of topic proportion sparsity, we report the

TABLE 2: Sparsity measure on topic-speciﬁc words for
our model; N : Average number of topic-speciﬁc words
per topic; Nunique : Average number of unique topic-
speciﬁc words over all topics; (std. dev.)

Ohsumed
20-Newsgroup
Reuters
LabelMe

N
12072
54520
16000
158

N
863.04 (3.20)
1640 (20.00)
506.4 (4.51)
95.73 (2.31)

Nunique
11182 (11.96)
22070 (352)
7149.8 (55.24)
158 (0)

average number of occurring topics in documents (M ).
In STC and LDA, we considered those topics which
dominantly contributed in generating at least one word
in a document as “occurring topics”. We also report
average number of topic-speciﬁc words per topic (N ),
and the total number of topic-speciﬁc words over all
topics (Nunique) for our model.

Average number of occurring topics in our model is
1.85 which compared to LDA and STC is much closer
to the average number of labels per document in this
corpus (2.26). This suggests our topics better resemble
the ground-truth classes than those of LDA and STC.
Also, the shared model is widely used in our model
and only a relatively small number of words are speciﬁc
for each topic. Average number of unique topic-speciﬁc
words over all topics is much larger than N , which
indicates that there is not great overlap between the
set of topic-speciﬁc words across topics. In fact, topic-
speciﬁc words, on average, are salient in 8.10 topics (out
of 105) and modeled by the shared representation in
other topics.

7.2 20-Newsgroups Corpus

In this section we report the results of our comparison on
20-Newsgroups7. This dataset consists of, respectively,
11293 and 7527 documents in the training and test
sets, with 20 designated newsgroups. Each document is
labeled to a single newsgroup. After standard stopword
removal and stemming there are 54520 unique words in
the corpus.

Models were initialized with 100 topics and two topics
were removed at each elimination step. Fig. 4 shows

7. http://people.csail.mit.edu/jrennie/20Newsgroups/

TABLE 1: Sparsity on topic proportions; M : Average number of topics present in a document;(std. dev.)

Dataset
Our Model
LDA
LDAbackground
STC

Ohsumed (M =105)
1.85 (0.01)
10.55 (0.37)
12.49 (0.31)
10.22 (0.50)

20-Newsgroup (M =36)
1.14 (0.01)
6.42 (0.28)
5.27 (0.12)
2.67 (0.06)

Reuters (M =40)
1.17 (0.01)
4.97 (0.13)
4.18 (0.74)
3.01 (0.20)

LabelMe (M =46)
7.4 (0.14)
37.15 (1.03)
36.17 (0.71)
1.08 (0.04)

11

BIC

Training Log−
Likelihood

Held−out Log−
Likelihood

106
2.81

2.79

106
−2.55

−2.75

105
−4.55

−4.65

−4.75

BIC

Training Log−
Likelihood

Held−out Log−
Likelihood

107
1.14

1.13
107
−1.03

−1.13
106
−3.45

−3.55

Class Label
 Consistency

Coherence

0.7

0.6

0.5

−500
−1000
−1500
−2000

(a)

(b)

20

40
Number of Topics

60

80

100

20

40
Number of Topics

60

80

100

LDA

LDAbackground

Our Model

LDA

LDAbackground

Our Model

Fig. 4: Average performance, based on 10 random ini-
tializations, versus number of topics on 20-Newsgroups.

Fig. 6: Average performance, based on 10 random ini-
tializations, versus number of topics on Reuters.

20

40

60
Number of Topics

80

100

LDA

LDAbackground

Our Model

STC

Fig. 5: (a) Class label consistency and (b) coherence
versus number of topics on 20-Newsgroups.

the performance of our model relative to LDA and
LDAbackground. The minimum of the BIC curve is on
average M ∗ = 36 (std. dev. = 3.79). Although LDA
achieves higher likelihood on the training data, our
parsimonious model has better performance on the held-
out set for models with less than 60 topics.

P

M
j=1 αjdpj(c) c = 1, ...,

Learning the label distributions of topics is done simi-
lar to the procedure explained for the Ohsumed corpus.
But since documents in this corpus have single labels,
we compute the probability over the class labels for each
, and assign only
document,
the label with highest probability. Class consistency on
the test set is reported in Fig. 5a. We can see that the
BIC-chosen model order M ∗ = 36 achieves good class
consistency. Also, our model achieves better consistency
compared to all other methods for models with less than
70 topics. Coherence of the discovered topics in LDA,
LDAbackground and our model is plotted in Fig. 5b.
The curves are very similar.

C

|

|

Sparsity measures of our model on this dataset are
compared with LDA, LDAbackground and STC in Tables
1 and 2. Only a small fraction of the unique words are
used as topic-speciﬁc words in our model. We can see
that although adding a background model reduces the
average number of occurring topics in LDA, our model

still has the smallest M (1.14) among all methods. Note
again that documents in this corpus are single-labeled.
On average, the 22070 unique topic-speciﬁc words in
this dataset are context-speciﬁc in only 2.67 topics, and
shared in the others.

The 10 words with highest probabilities from some
of the topics discovered by LDA and by our model
are reported in Table 3. For our model, we separately
report top 10 topic-speciﬁc and shared words. Note that
some high probability words in LDA topics (“write” and
“articl” in the ﬁrst, and “system” in the third topic), are
shared words under our model.

7.3 Reuters Corpus

We compared our model against LDA, LDAbackground
and STC on a subset of the Reuters dataset-215788 con-
sisting of documents from 35 classes. There are respec-
tively 6454 and 2513 documents in the training and test
sets, each labeled with a single class. The documents
include 16000 unique words after applying standard
stopword removal and stemming.

We initialized the models with 100 topics and removed
two topics at each elimination step. Average BIC for our
model as well as the training and held-out data log-
likelihood of our model, LDA and LDAbackground are
shown in Fig. 6. The minimum of the BIC curve is on
average at M ∗ = 40 (std. dev. = 5.75). Fig. 6 shows that
our model at M = 40 achieves higher log-likelihood
on the held-out set compared to the LDA models at
all orders. Moreover, this in spite of the fact that the
held-out likelihood for our model increases modestly for
orders above M = 40.

Class label consistency on this dataset is evaluated
based on the procedure described for the 20-Newsgroup
corpus. Average test set classiﬁcation accuracy of our

8. http://www.daviddlewis.com/resources/testcollections/reuters21578

12

TABLE 3: Top 10 words for sample topics extracted from the 20-Newsgroup and the Reuters corpora by our method
and LDA. “Speciﬁc” and “shared” denote, respectively, the top topic-speciﬁc and shared words of the topics in our
model.

Model
LDA
Shared
Speciﬁc
LDA
Shared
Speciﬁc
LDA
Shared
Speciﬁc

20-Newsgroup Topics
medic diseas articl write patient pitt bank health gordon doctor
write articl don time work year good even problem thing
doctor patient diseas medic pain peopl pitt treatment gordon bank
god christian jesu israel church peopl bibl christ come law
write articl time good even apr come ﬁnd sinc gener
god christian exist peopl don atheist question thing mean reason
drive problem disk window system work hard run driver printer
write work good system want apr two question post program
printer font print window problem driver deskjet laser ﬁle articl

Reuters Topics
gold mine ounc dixon miner ton dome south year feet
compani oper corp thi two unit includ expect report plan
gold mine ounc ton year feet pct reserv mln dlr
oil opec ga price energi dlr barrel mln petroleum crude
stock last two march industri expect quarter per month report
oil opec price bpd mln saudi barrel crude dlr product
dividend april record split stock march declar payabl set share
corp offer unit quarter plan exchang invest acquir propos pai
dividend stock share split april record compani common declar
sharehold

Class Label
 Consistency

Coherence

−75

0.85
0.80
0.75
0.70

−25

−125

(a)

(b)

BIC

Training Log−
Likelihood

Held−out Log−
Likelihood

Class Label
 Consistency

106
9.08
9.06
106
−9.05

−9.12
106
−4.8

−5

0.65
0.50
0.35

20

40

60
Number of Topics

80

100

LDA

LDAbackground

Our Model

STC

Fig. 7: (a) Class label consistency and (b) coherence
versus number of topics on Reuters.

model, LDA, LDAbackground, and STC for this dataset
are shown in Fig. 7a. Our model has best performance
across all model orders. Also, the minimum of the BIC
curve, M ∗ = 40, is consistent with high class label
consistency.

We plotted the average coherence of topics for our
model and LDA in Fig. 7b. This ﬁgure shows that for
model orders less than 50, topics discovered by our
model have on average higher coherence than LDA.

Tables 1 and 2 report the sparsity measures for our
model, LDA, LDAbackground, and STC. The topic-
speciﬁc words in our model are a small subset of the
dictionary size. Also, our model achieves sparser topic
presence compared to LDA, LDAbackground, and STC.
Table 3 shows the top 10 words from three sample
topics extracted by our model and LDA. For our model,
we separately report the top 10 topic-speciﬁc and shared
words.

7.4 LabelMe Image Datatset

In this section we report the results of our comparison
on a subset of the LabelMe image dataset. The dataset
which we downloaded9 consists of 1600 images from
8 classes. Unique codewords were extracted from the
images using the method described in [38], [39]. First,
128-dimensional SIFT vector descriptors [40] were gen-
erated by 5
5 sliding grids for each image in the
training set. Then, K-means clustering was performed
on the collection of SIFT descriptors, giving learned
cluster centers. Finally, each SIFT descriptor in every

×

9. http://www.cs.cmu.edu/∼chongw/slda/

20

40

60

80

Number of Topics

LDA

LDAbackground

Our Model

STC

Fig. 8: Average performance versus number of topics on
LabelMe image domain.

image was assigned to its nearest cluster [41]. There are
158 clusters in this dataset after performing K-means
clustering, merging close clusters, and pruning clusters
with small number of members. The cluster index set
1, 2, ..., 158
is effectively the dictionary of words. Each
{
image is represented by a sequence of these cluster
indices, of length 2401.

}

Since the number of codewords in this dataset is very
small relative to the text corpora, the approximation
used in (23) is not valid anymore. Accordingly, we
considered any conﬁguration with at most (N0 = N M )
“on” switches, equally likely and used the exact form
of (23). Thus, the cost of topic-speciﬁc words in BIC is
N M log(2) in this case.

We initialized the models with 80 topics and reduced
the number of topics by one at each step. Fig. 8 shows
performance of our model compared with LDA and
LDAbackground. The minimum of BIC is on average at
M ∗ = 46 (std. dev. = 3.4). Held-out set log-likelihood is
higher in our model compared to LDA and LDAback-
ground, across all model orders.

We also performed single-label classiﬁcation, similar
to the procedure described for the single-labeled text
corpora. Fig. 8 shows the class label consistency of our
model compared to LDAbackground, LDA, and STC. We
can see that our model achieves better label consistency
than LDA, LDAbackground, and STC.

Tables 1 and 2 compare the sparsity measure in our
model with LDA, LDAbackground, and STC for this
dataset at M ∗ = 46. At this model order, the average
number of occurring topics for LDA is 37.15, which

seems implausible, but our model and STC provide
more reasonable sparsity in topic proportions. However,
unlike STC, our model exhibits parsimony in word
probabilities, with on average only 95.73 words modeled
in a topic-speciﬁc fashion.

Discussion: On all four data sets, our models achieve
higher held-out likelihoods than LDA, at nearly all model
orders, except for high orders on 20-Newsgroups. Even
stronger, our model evaluated at the single (BIC min-
imum) order achieves higher held-out likelihood than
the LDA models evaluated at nearly all orders. With
respect to class label consistency, our BIC-minimum
model gives accuracy better than LDA evaluated at
nearly all orders. However, it is seen on all the data sets
that LDA’s class consistency improves with increasing
order, even exceeding our model’s on 20-Newsgroups
for M > 70. In general, our model has much greater
topic sparsity than LDA, and with only modest increases
in sparsity achieved for LDA by using a background
model. Moreover, our model’s average number of topics
per document has good agreement with the average
number of class labels per document. In this sense, our
learned topics better correspond to individual classes
than LDA’s. From the curves in Figs. 3, 5, 7, and 8, it
may be possible for LDA to achieve greater class label
consistency than our model on some of the data sets
by using a very large number of topics. However, one
must recognize that topic models perform unsupervised
clustering, aiming to capture the most salient content and
to provide human-interpretable data groupings. Choos-
ing a huge number of topics may improve LDA’s class
label consistency, but such solutions would defy human
interpretation. Our BIC-minimizing solutions are more
interpretable due to their topic sparsity, their word spar-
sity and, as noted, due to topics better corresponding
to ground-truth classes. Moreover, this parsimony and
interpretability are achieved without giving up perfor-
mance (held-out set likelihood and class consistency).

Execution Time: We report the execution times for the
complete model learning process, from M = Mmax down
to M = Mmin, for our model, LDA, and STC on the four
datasets in Table 4. All models were run on machines
with Quad-Core 3.0 GHz processors. For the datasets
with small M or modest document length, execution
time of our model is smaller or comparable to LDA.
But on the Ohsumed corpus with M = 1.85 and larger
typical document lengths, running time in our model
is three times that of LDA. Also, STC has the smallest
overall running time across these datasets.

TABLE 4: Average execution times in minutes (std. dev.)

Dataset
Ohsumed
20-Newsgroup
Reuters
LabelMe

Our Model
2529 (97)
757 (46)
395 (31)
209 (7)

LDA
818 (251)
1510 (94)
746 (52)
124 (6)

STC
243 (11)
527 (10)
140 (9)
280 (30)

The E-steps in both variational

inference for LDA
and EM for our model can be readily parallelized and

13

performed separately for each document. This will im-
prove scalability to larger data sets. Nevertheless, we ran
the standard implementation of our model and LDA,
learning a single model with M = 100 topics on the
NSF Research Awards corpus10 which consists of 129000
abstracts of NSF awards from 1990-2003. Training time
for our model and LDA were respectively 30 and 10
hours on this (much larger) data set.

8 CONCLUSION

We have proposed a parsimonious model for estimat-
ing topics and their salient words, given a database
of unstructured documents. Unlike LDA, our model
gives sparse representation both in topic presence in
documents and in word probabilities under different
topics. We have derived a BIC objective function speciﬁc
to our model, with complexity penalization consistent
with the effective sample size for each parameter type.
We minimize this objective to jointly determine our
model, including the total number of topics. Experiments
show that our model outperforms LDA and a sparsity-
based topic model [20] with respect to several clustering
performance measures, including test set log-likelihood
and agreement with ground-truth class labels.

REFERENCES

[1] C. Fraley and A. Raftery, “How many clusters? Which clustering
method? Answers via model-based cluster analysis,” The Comput.
J., vol. 41, no. 8, pp. 578–588, 1998.

[2] Z. Ghahramani and M. Beal, “Variational Inference for Bayesian

Mixtures of Factor Analysers.” in NIPS, 1999, pp. 449–455.

[3] M. W. Graham and D. J. Miller, “Unsupervised learning of par-
simonious mixtures on large spaces with integrated feature and
component selection,” IEEE Trans. Signal Process., vol. 54, no. 4,
pp. 1289–1303, 2006.

[4] W. H. Jefferys and J. O. Berger, “Ockham’s razor and bayesian

analysis,” Amer. Scientist, vol. 80, no. 1, pp. 64–72, 1992.

[5] D. J. C. MacKay, Information Theory, Inference and Learning Algo-

rithms. Cambridge University Press, 2003.

[6] C. D. Manning and H. Sch ¨utze, Foundations of Statistical Natural

Language Processing. MIT Press, 1999.

[7] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent Dirichlet Allocation,”

J. of Machine Learning Research, vol. 3, pp. 993–1022, 2003.

[8] K. Nigam, A. K. McCallum, S. Thrun, and T. Mitchell, “Text
classiﬁcation from labeled and unlabeled documents using EM,”
Machine learning, vol. 39, no. 2-3, pp. 103–134, 2000.

[9] R. E. Kass and A. E. Raftery, “Bayes Factors,” Journal of the American

Statistical Association, vol. 90, no. 430, pp. 773–795, 1995.

[10] C. M. Bishop, Pattern Recognition and Machine Learning. Springer,

2006.

[11] G. Schwarz, “Estimating the dimension of a model,” Annals of

Stat., vol. 6, no. 2, pp. 461–464, 1978.

[12] H. M. Wallach, D. M. Mimno, and A. McCallum, “Rethinking

LDA: Why Priors Matter.” NIPS, pp. 973–1981, 2009.

[13] C. Wang and D. M. Blei, “Decoupling sparsity and smoothness
in the discrete hierarchical dirichlet process,” in NIPS, 2009, pp.
1982–1989.

[14] S. Williamson, C. Wang, K. Heller, and D. M. Blei, “The IBP
compound Dirichlet process and its application to focused topic
modeling,” in ICML, 2010.

[15] H. Jin, R. Schwartz, S. Sista, and F. Walls, “Topic Tracking for
Radio, TV Broadcast, and Newswire,” in Proc. DARPA Broadcast
News Workshop., 1999.

10. http://kdd.ics.uci.edu/databases/nsfabs/nsfawards.data.html

[16] Y. Zhang, J. Callan, and T. Minka, “Novelty and redundancy
detection in adaptive ﬁltering,” Proc. 25th Annu. Int. ACM SIGIR
Conf. Research and Develop. in Inform. Retrieval, p. 81, 2002.

[17] C. Zhai and J. Lafferty, “Model-based feedback in the language
modeling approach to information retrieval,” Proc. 10th Int. Conf.
Inform. and knowledge Manage., p. 403, 2001.

[18] D. Hiemstra, S. Robertson, and H. Zaragoza, “Parsimonious
language models for information retrieval,” Proc. 27th Annu. Int.
Conf. Research and Develop. in Inform. Retrieval, p. 178, 2004.

[19] C. Chemudugunta and S. M. S. Steyvers, “Modeling general and
speciﬁc aspects of documents with a probabilistic topic model,” in
NIPS, 2007, pp. 241–248.

[20] J. Zhu and E. P. Xing, “Sparse topical coding,” in 27th Conf.

Uncertainty in Artiﬁcial Intelligence (UAI), 2011.

[21] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei, “Hierarchical
Dirichlet Processes,” J. of the Amer. Stat. Assoc., vol. 101, no. 476,
pp. 1566–1581, Dec. 2006.

[22] M. H. C. Law, M. A. T. Figueiredo, and A. K. Jain, “Simultaneous
feature selection and clustering using mixture models.” IEEE Trans.
Pattern Anal. Mach. Intell., vol. 26, no. 9, pp. 1154–66, Sep. 2004.
[23] C. Constantinopoulos, M. K. Titsias, and A. Likas, “Bayesian
feature and model selection for Gaussian mixture models.” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 28, no. 6, pp. 1013–8, Jun.
2006.

[24] P. D. Gr ¨unwald, The minimum description length principle. MIT

Press, 2007.

[25] S. Boutemedjet, N. Bouguila, and D. Ziou, “A hybrid feature
extraction selection approach for high-dimensional non-Gaussian
data clustering.” IEEE Trans. Pattern Anal. Mach. Intell., vol. 31,
no. 8, pp. 1429–43, Aug. 2009.

[26] W. Fan, N. Bouguila, and D. Ziou, “Unsupervised hybrid feature
extraction selection for high-dimensional non-Gaussian data clus-
tering with variational inference,” IEEE Trans. Knowl. Data Eng.,
vol. 25, no. 7, pp. 1670–1685, 2013.

[27] T. L. Grifﬁths and M. Steyvers, “Finding scientiﬁc topics,” Proc.
Nat. Academy of Sciences of the United States of America, vol. 101
Suppl, pp. 5228–35, Apr. 2004.

14

[28] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern classiﬁcation,

2nd ed.

John Wiley & Sons, 2012.

[29] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum
likelihood from incomplete data via the EM algorithm,” J. of the
Roy. Stat. Soc., vol. 39, no. 1, pp. 1–38, 1977.

[30] J. K. Ghosh, M. Delampady, and T. Samanta, An Introduction to
Bayesian Analysis: Theory and Methods. Springer New York, 2006.
[31] R. L. Graham, D. E. Knuth, and O. Patashnik, Concrete Mathemat-
ics: A Foundation for Computer Science, 2nd ed. Boston, MA, USA:
Addison-Wesley Longman Publishing, Jan. 1994.

[32] T. M. Cover and J. A. Thomas, Elements of Information Theory,

2nd ed. Wiley, 2006.

[33] X.-L. Meng and D. Van Dyk, “The EM algorithm–an old folk-
song sung to a fast new tune,” J. of the Roy. Stat. Soc.: Series B (Stat.
Methodology), vol. 59, no. 3, pp. 511–567, 1997.

[34] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman,
“LabelMe: a database and web-based tool for image annotation,”
Int. J. of Computer Vision, vol. 77, no. 1-3, pp. 157–173, 2008.

[35] M. Hoffman, D. Blei, C. Wang, and J. Paisley, “Stochastic varia-
tional inference,” Journal of Machine Learning Research, vol. 14, pp.
1303–1347, 2012.

[36] Y. Teh, K. Kurihara, and M. Welling, “Collapsed Variational

Inference for HDP.” NIPS, 2007.

[37] D. Mimno, H. M. Wallach, and E. Talley, “Optimizing semantic
coherence in topic models,” in Proc. Conf. Empirical Methods in
Natural Language Process., no. 2, 2011, pp. 262–272.

[38] L. Fei-Fei and P. Perona, “A Bayesian hierarchical model for
learning natural scene categories,” in IEEE Computer Society Conf.
Computer Vision and Pattern Recognition, vol. 2, 2005, pp. 524–531.
[39] C. Wang, D. M. Blei, and F.-F. Li, “Simultaneous image classiﬁ-
cation and annotation,” in IEEE Conf. Computer Vision and Pattern
Recognition, Jun. 2009, pp. 1903–1910.

[40] D. G. Lowe, “Object recognition from local scale-invariant fea-
tures,” in IEEE Int. Conf. Computer Vision, 1999, pp. 1150–1157.
[41] T. Leung and J. Malik, “Representing and recognizing the visual
appearance of materials using three-dimensional textons,” Int. J. of
Computer Vision, vol. 43, no. 1, pp. 29–44, 2001.

