7
1
0
2
 
c
e
D
 
5
 
 
]

G
L
.
s
c
[
 
 
1
v
7
9
8
1
0
.
2
1
7
1
:
v
i
X
r
a

Online Learning with Gated Linear Networks

Online Learning with Gated Linear Networks

Joel Veness†
Tor Lattimore†
Avishkar Bhoopchand†
Agnieszka Grabska-Barwinska
Christopher Mattern
Peter Toth
Google, DeepMind

aixi@google.com
lattimore@google.com
avishkar@google.com
agnigb@google.com
cmattern@google.com
petertoth@google.com

Abstract

This paper describes a family of probabilistic architectures designed for online learning un-
der the logarithmic loss. Rather than relying on non-linear transfer functions, our method
gains representational power by the use of data conditioning. We state under general con-
ditions a learnable capacity theorem that shows this approach can in principle learn any
bounded Borel-measurable function on a compact subset of euclidean space; the result is
stronger than many universality results for connectionist architectures because we provide
both the model and the learning procedure for which convergence is guaranteed.

Keywords: Compression, online learning, geometric mixing, logarithmic loss.

1. Introduction

This paper explores the use of techniques from the online learning and data compression
communities for the purpose of high dimensional density modeling, with a particular empha-
sis on image density modeling. The current state of the art is almost exclusively dominated
by various deep learning based approaches from the machine learning community. These
methods are trained oﬄine, can generate plausible looking samples, and generally oﬀer ex-
cellent empirical performance. But they also have some drawbacks. Notably, they require
multiple passes through the data (not online) and do not come with any kind of meaningful
theoretical guarantees. Of course density modeling is not just a machine learning problem;
in particular, it has received considerable study from the statistical data compression com-
munity. Here the emphasis is typically on online density estimation, as this in conjunction
with an adaptive arithmetic encoder (Witten et al., 1987) obviates the need to encode the
model parameters along with the encoded data when performing compression. Our main
contribution is to show that a certain family of neural networks, composed of techniques
originating from the data compression and online learning communities, can in some cir-
cumstances match the performance of deep learning based approaches in just a single pass
through the data, while also enjoying universal source coding guarantees.

. † denotes joint ﬁrst authorship.

1

Veness, Lattimore, Bhoopchand, Grabska-Barwinska, Mattern, Toth

1.1 Related Work

1.1.1 Online, neural, density estimation

Although neural networks have been around for a long time, their application to data
compression has been restricted until relatively recently due to hardware limitations. To the
best of our knowledge, the ﬁrst promising empirical results were presented by Schmidhuber
and Heil (1996), who showed that an oﬄine trained 3-layer neural network could outperform
Lempel-Ziv based approaches on text prediction; interestingly online prediction was noted
as a promising future direction if the computational performance issues could be addressed.
Building on the work of Schmidhuber and Heil (1996), Mahoney (2000) introduced a
number of key algorithmic and architectural changes tailored towards computationally eﬃ-
cient online density estimation. Rather than use a standard fully connected MLP, neurons
within a layer were partitioned into disjoint sets; given a single data point, hashing was
used to select only a single neuron from each set in each layer, implementing a kind of hard
gating mechanism. Since only a small subset of the total weights were used for any given
data point, a speed up of multiple orders of magnitude was achieved. Also, rather than
using backpropagation, the weights in each neuron were adjusted using a local learning rule,
which was found to work better in an online setting.

The next major enhancement was the introduction of context-mixing, culminating in the
PAQ family of algorithms (Mahoney, 2013). At a high level, the main idea is to combine
the predictions of multiple history-based models using a network architecture similar in
spirit to the one described above. Common choices of models to combine include n-grams,
skip-grams (Guthrie et al., 2006), match models (Mahoney, 2013), Dynamic Markov Coding
(Cormack and Horspool, 1987), as well as specialized models for commonly occurring data
sources. Many variants along this theme were developed, including improvements to the
network architecture and weight update mechanism, with a full history given in the book
by Mahoney (2013). At the time of writing, context mixing variants achieve the best
compression ratios on widely used benchmarks such as the Calgary Corpus (Bell et al.,
1990), Canterbury Corpus (Bell and Arnold, 1997), and Wikipedia (Hutter, 2017).

The excellent empirical performance of these methods has recently motivated further
investigation into understanding why these techniques seem to perform so well and whether
they have applicability beyond data compression. Knoll and de Freitas (2012) investigated
the PAQ8 variant (Mahoney, 2013) from a machine learning perspective, providing a useful
interpretation of the as a kind of mixture of experts (Jacobs et al., 1991) architecture, as well
as providing an up-to-date summary of the algorithmic developments since the tech report
of Mahoney (2005). They also explored the performance of PAQ8 in a variety of standard
machine learning settings including text categorization, shape recognition and classiﬁca-
tion, showing that competitive performance was possible. One of the key enhancements in
PAQ7 was the introduction of logistic mixing, which applied a logit based non-linearity to
a neurons input; Mattern (2012, 2013) generalized this technique to non-binary alphabets
and justiﬁed this particular form of input combination via a KL-divergence minimization
argument. Furthermore he showed that due to the convexity of the instantaneous loss (in
the single neuron case), the local gradient based weight update used within PAQ7 and
later versions is justiﬁed in a regret-minimizing sense with respect to piece-wise stationary
sources. Subsequent work (Mattern, 2016) further analyzed the regret properties of many

2

Online Learning with Gated Linear Networks

of the core building blocks used to construct many of the base models used within PAQ
models, but a theoretical understanding of the mixing network with multiple interacting
geometric mixing neurons remained open.

1.1.2 Batch Neural Density Estimation

Generative image modeling is a core topic in unsupervised learning and has been studied
extensively over the last decade from a deep learning perspective. The topic is suﬃciently
developed that there are now practical techniques for generating realistic looking natural
images (Larochelle and Murray, 2011b; Gregor et al., 2015; Van Den Oord et al., 2016;
Chen et al., 2016; Gulrajani et al., 2016, and others). Modeling images is necessarily a
high dimensional problem, which precludes the use of many traditional density estimation
techniques on both computational and statistical grounds. Although image data is high
dimensional, it is also quite structured, and is well suited to deep learning approaches
which can easily incorporate appropriate notions of locality and translation invariance into
the architecture.

The aforementioned methods all require multiple passes over the dataset to work well,
which makes it signiﬁcantly more challenging to incorporate them into naturally online set-
tings such as reinforcement learning, where density modeling is just starting to see promising
applications (Veness et al., 2015; Ostrovski et al., 2017).

1.1.3 Miscellaneous

Our work shares some similarity to that of Balduzzi (2016), who used techniques from
game theory and online convex programming to analyze some convergence properties of
deep convolutional ReLU neural networks by introducing a notion of gated regret. While
extremely general, this notion of gated regret is deﬁned with respect to an oracle that
knows which ReLU units will ﬁre for any given input; while this is suﬃcient to guarantee
convergence, it does not provide any guarantees on the quality of the obtained solution.
Our work fundamentally diﬀers in that we explicitly construct a family of gated neural
architectures where it is easy to locally bound the usual notion of regret, allowing us to
provide far more meaningful theoretical guarantees in our restricted setting.

Foerster et al. (2017) recently introduced a novel recurrent neural architecture whose
modeling power was derived from using a data-dependent aﬃne transformation as opposed
to a non-linear activation function. As we shall see later, this is similar in spirit to our
approach; we use a product of data-dependent weight matrices to provide representation
power instead of a non-linear activation function. Our work diﬀers in that we consider
an online setting, and use a local learning rule instead of backpropagation to adjust the
weights.

1.2 Contribution

Our main contributions are:

• to introduce a family of neural models, Gated Linear Networks, which consist of a
sequence of data dependent linear networks coupled with an appropriate choice of

3

Veness, Lattimore, Bhoopchand, Grabska-Barwinska, Mattern, Toth

gating function; we show that the high performance PAQ models are a special case of
this framework, opening up many potential avenues for improvement;

• to theoretically justify the local weight learning mechanism in these architectures,
and highlight how the structure of the loss allows many other techniques to give
similar guarantees. Interestingly, while the original motivation for introducing gating
was computational (Mahoney, 2000; Knoll and de Freitas, 2012), we show that this
technique also adds meaningful representational power as well;

• introduce an adaptive regularization technique which allows a Gated Linear Network
to have competitive loss guarantees with respect to all possible sub-networks obtained
via pruning the original network;

• to provide an eﬀective capacity theorem showing that with an appropriate choice of
gating function, a suﬃciently large network size and a Hannan consistent/no-regret
(Cesa-Bianchi and Lugosi, 2006) local learning method, Gated Linear Networks will
learn any continuous density function to an arbitrary level of accuracy.

1.3 Paper Outline

After introducing the required notation, we proceed by describing a notion of “neuron”
suitable for our purposes, which we will call a gated geometric mixer ; this will form the
elementary building block for all of our subsequent work. We then proceed to describe
gated linear networks, which are feed-forward architectures composed of multiple layers
of such gated geometric mixing neurons. Our theoretical results are then presented, which
include a capacity analysis of this family of models. A technique for adaptive regularization,
sub-network switching, is then introduced, along with its associated theoretical properties.
Next we describe an architecture built from these principles, reporting empirical results on
mnist, both classiﬁcation and density modelling.

2. Gated Geometric Mixing

This section introduces the basic building block of our work, the gated geometric mixer.
We start with some necessary notation, before describing (ungated) geometric mixing and
some of its properties. We then generalize this technique to the gated case, giving rise to
the gated geometric mixer.

2.1 Notation
Let ∆d = {x ∈ [0, 1]d : (cid:107)x(cid:107)1 = 1} be the d dimensional probability simplex embedded in
Rd+1 and B = {0, 1} the set of binary elements. The indicator function for set A is 1A
and satisﬁes 1A(x) = 1 if x ∈ A and 1A(x) = 0 otherwise. For predicate P we also write
1[P ], which evaluates to 1 if P is true and 0 otherwise. The scalar element located at
position (i, j) of a matrix A is Aij, with the ith row and j column denoted by Ai∗ and A∗j
respectively. For functions f : R → R and vectors x ∈ Rd we adopt the convention of writing
f (x) ∈ Rd for the coordinate-wise image of x under f so that f (x) = (f (x1), . . . , f (xd)). If
p, q ∈ [0, 1], then D(p, q) = p log p/q + (1 − p) log(1 − p)/(1 − q) is the Kullback-Leibler (KL)

4

Online Learning with Gated Linear Networks

Figure 1: Role of w in a one-dimensional geometric mixer with input probability p.

divergence between Bernoulli distributions with parameters p and q respectively. Let X be
a ﬁnite, non-empty set of symbols, which we call the alphabet. A string of length n over X
is a ﬁnite sequence x1:n = x1x2 . . . xn ∈ X n with xt ∈ X for all t. For t ≤ n we introduce
the shorthands x<t = x1:t−1 and x≤t = x1:t. The string of length zero is (cid:15) and the set of all
ﬁnite strings is X ∗ = {(cid:15)} ∪ (cid:83)∞
i=1 X i. The concatenation of two strings s, r ∈ X ∗ is denoted
by sr. A sequential, probabilistic model ρ is a probability mass function ρ : X ∗ → [0, 1],
satisfying the constraint that ρ(x1:n) = (cid:80)
y∈X ρ(x1:ny) for all n ∈ N, x1:n ∈ X n, with
ρ((cid:15)) = 1. Under this deﬁnition, the conditional probability of a symbol xn given previous
data x<n is deﬁned as ρ(xn | x<n) = ρ(x1:n)/ρ(x<n) provided ρ(x<n) > 0, with the familiar
chain rules ρ(x1:n) = (cid:81)n
k=i ρ(xk | x<k) applying as usual.

i=1 ρ(xi | x<i) and ρ(xi:j | x<i) = (cid:81)j

2.2 Geometric Mixing

We now describe geometric mixing, an adaptive online ensemble technique for obtaining a
single conditional probability estimate from the output of multiple models. We provide a
brief overview here, restricting our attention to the case of binary sequence prediction for
simplicity of exposition; for more information see the work of Mattern (2012, 2013, 2016).

2.2.1 Model

Given m sequential, probabilistic, binary models ρ1, . . . , ρm, Geometric Mixing provides a
principled way of combining the m associated conditional probability distributions into a
single conditional probability distribution, giving rise to a probability measure on binary
sequences that has a number of desirable properties. Let xt ∈ {0, 1} denote the Boolean
target at time t. Furthermore, let pt = ( ρ1(xt = 1|x<t), . . . , ρm(xt = 1|x<t) ). Given a
convex set W ⊂ Rm and parameter vector w ∈ W, the Geometric Mixture is deﬁned by

geow(xt = 1; pt) =

i=1 pwi

(cid:81)m
t,i + (cid:81)m

t,i

(cid:81)m

i=1 pwi

i=1(1 − pt,i)wi

,

(1)

with geow(xt = 0; pt) = 1 − geow(xt = 1; pt).

Properties. A few properties of this formulation are worth discussing. Setting wi = 1/m
for i ∈ [1, m] is equivalent to taking the geometric mean of the m input probabilities. As

5

Veness, Lattimore, Bhoopchand, Grabska-Barwinska, Mattern, Toth

illustrated in Figure 1, higher absolute values of wi translate into an increased belief into
model i’s prediction; for negative values of wi, the prediction needs to be reversed (see blue
If w = 0 then geow(xt = 1; pt) = 1/2; and in the case where wi = 0
lines in Fig. 1).
for i ∈ S where S is a proper subset of [1, m], the contributions of the models in S are
essentially ignored (taking 00 to be 1). Due to the product formulation, every model also
has “the right of veto”, in the sense that a single pt,i close to 0 coupled with a wi > 0 drives
geow(xt = 1; pt) close to zero. These properties are graphically depicted in Figure 1.

Alternate form. Via simple algebraic manipulation, one can also express Equation 1 as

geow(xt = 1; pt) = σ (w · logit(pt)) ,

(2)

where σ(x) = 1/(1 + e−x) denotes the sigmoid function, and logit(x) = log(x/(1 − x)) is its
inverse. This form is best suited for numerical implementation. Furthermore, the property
of having an input non-linearity that is the inverse of the output non-linearity is the reason
why a linear network is obtained when layers of geometric mixers are stacked on top of each
other.

Remarks This form of combining probabilities is certainly not new; for example it is
discussed by Genest and Zidek (1986) under the name logarithmic opinion pooling and is
closely related to the Product of Experts model of Hinton (2002). While one can attempt to
justify this choice of probability combination via appealing to notions such as its externally
Bayesian properties (Genest and Zidek, 1986), or as the solution to various kinds of weighted
redundancy minimization problems (Mattern, 2013, 2016), the strongest case for this form of
model mixing is simply its superior empirical performance when w is adapted over time via
online gradient descent (Zinkevich, 2003) compared with other similarly complex alternates
such as linear mixing (Mattern, 2012; Mahoney, 2013).

2.2.2 Properties under the Logarithmic Loss

We assume a standard online learning setting, whereby at each round t ∈ N a predictor
outputs a binary distribution qt
: B → [0, 1], with the environment responding with an
observation xt ∈ B. The predictor then suﬀers the logarithmic loss

(cid:96)t(qt, xt) = − log qt(xt),

before moving onto round t + 1. The loss will be close to 0 when the predictor assigns high
probability to xt, and large when low probability is assigned to xt; in the extreme cases, a
zero loss is obtained when qt(xt) = 1, and an inﬁnite loss is suﬀered when qt(xt) = 0. In
the case of geometric mixing, which depends on both the m dimensional input predictions
pt and the parameter vector w ∈ W, we abbreviate the loss by deﬁning

(cid:96)geo
t

(w) = (cid:96)t(geow(· ; pt), xt).

(3)

The following proposition, proven in Appendix A, establishes some useful properties about
the logarithmic loss when applied to Geometric Mixing.

Proposition 1 For all t ∈ N, xt ∈ B, pt ∈ (0, 1)m and w ∈ W we have:

6

Online Learning with Gated Linear Networks

1. ∇(cid:96)geo

t
2. (cid:107)∇(cid:96)geo

t
3. (cid:96)geo
t

(w) = (geow(1; pt) − xt) logit(pt).

(w)(cid:107)2 ≤ (cid:107)logit(pt)(cid:107)2.

(w) is a convex function of w.

4. If pt ∈ [ε, 1 − ε]m for some ε ∈ (0, 1/2), then:

(a) (cid:96)geo
t

: W → R is α-exp-concave with α = σ

log

(cid:18)

(cid:16) ε
1−ε

(cid:17)maxw∈W (cid:107)w(cid:107)1

(cid:19)

;

(b) (cid:107)∇(cid:96)geo

t

(w)(cid:107)2 ≤

√

m log (cid:0) 1
ε

(cid:1).

Note also that Part 4.(a) of Proposition 1 implies that α = Θ

(cid:16)

(ε/(1 − ε))maxw∈W (cid:107)w(cid:107)1

(cid:17)

.

t

√

(w) allows one to derive an O(

Remarks The above properties of the sequence of loss functions make it straightforward
to apply one of the many diﬀerent online convex programming techniques to adapt w at
the end of each round. The simplest to implement and most computationally eﬃcient is
Online Gradient Descent (Zinkevich, 2003) with W equal to some choice of hypercube. The
convexity of (cid:96)geo
T ) regret bound using standard techniques
with respect to the best w∗ ∈ W chosen in hindsight provided an appropriate schedule of
decaying learning rates is used, where T denotes the total number of rounds. Furthermore,
when a ﬁxed learning rate is used one can show O(s
T ) regret guarantees with respect
to data sequences composed of s pieces (Mattern, 2013). More reﬁned algorithms based
on coin betting allow the s to be moved inside the square root as shown by Jun et al.
(2017). The α-exp-concavity also allows second order techniques such as Online Newton
Step (Hazan et al., 2007) and its sketched variants (Luo et al., 2016) to be applied. These
techniques allow for O(log T ) regret guarantees with respect to the best w∗ ∈ W chosen in
hindsight at the price of a more computationally demanding weight update procedure and
further dependence on ε as given in Part 4.(a) of Proposition 1.

√

2.3 Gated Geometric Mixing

We are now in a position to deﬁne our notion of a single neuron, a gated geometric mixer,
which we obtain by adding a contextual gating procedure to geometric mixing. Here,
contextual gating has the intuitive meaning of mapping particular examples to particular
sets of weights. More precisely, associated with each neuron is a context function c : Z → C,
where Z is the set of possible side information and C = {0, . . . , k − 1} for some k ∈ N is
the context space. Given a convex set W ⊂ Rd, each neuron is parametrized by a matrix




with each row vector wi ∈ W for 0 ≤ i < k. The context function c is responsible for
mapping a given piece of side information zt ∈ Z to a particular row wc(zt) of W , which
we then use with standard geometric mixing. More formally, we deﬁne the gated geometric
mixture prediction as

geoc

W (xt = 1 ; pt, zt) = geowc(zt)(xt = 1 ; pt),

(4)

W =

w0
...
wk−1







7

Veness, Lattimore, Bhoopchand, Grabska-Barwinska, Mattern, Toth

with geoc
equivalent form

W (xt = 0 ; pt, zt) := 1 − geoc

W (xt = 1 ; pt, zt). Once again we have the following

geoc

W (xt = 1; pt, zt) = σ (cid:0)wc(zt) · logit(pt)(cid:1) .

(5)

The key idea is that our neuron can now specialize its weighting of the input predictions
based on some property of the side information zt. The side information can be arbitrary,
for example it could be some additional input features, or even functions of pt. Ideally the
choice of context function should be informative in the sense that it simpliﬁes the probability
combination task.

2.3.1 Context Functions

Here we introduce several classes of general purpose context functions that have proven
useful empirically, theoretically, or both. All of these context functions take the form of
an indicator function 1S(z) : Z → B on a particular choice of set S ⊆ Z, with 1S(z) := 1
if z ∈ S and 0 otherwise. This list, while suﬃcient for understanding the content in this
paper, is by no means exhaustive; practitioners can and should choose context functions
that make sense for the given domain of interest.

Half-space contexts This choice of context function is useful for real-valued side infor-
mation. Given a normal v ∈ Rd and oﬀset b ∈ R, consider the associated aﬃne hyperplane
{x ∈ Rd : x · v = b}. This divides Rd in two, giving rise to two half-spaces, one of which
we denote Hv,b = {x ∈ Rd : x · v ≥ b}. The associated half-space context is then given by
1Hv,b(z).

Skip-gram contexts The following type of context function is useful when we have
multi-dimensional binary side information and can expect single components of Z to be
If Z = Bd, given an index i ∈ [1, d], a skip-gram context is given by the
informative.
function 1Si(z) where Si := {z ∈ Z : zi = 1}. One can also naturally extend this notion
to categorical multi-dimensional side information or real valued side information by using
thresholding.

2.3.2 Context Function Composition

In particular, any ﬁnite
Richer notions of context can be created from composition.
set of d context functions {ci
i=1 with associated context spaces C1, . . . , Cd
can be composed into a single higher order context function c : Z → C, where C =
(cid:110)
0, 1, . . . , −1 + (cid:81)d

: Z → Ci}d

by deﬁning

(cid:111)

i=1 |Ci|

c(z) =

ci(z)



|Cj|

 .

d
(cid:88)

i=1





d
(cid:89)

j=i+1

For example, we can combine four diﬀerent skip-gram contexts into a single context
function with a context space containing 16 elements. The combined context function
partitions the side information based on the values of the four diﬀerent binary components
of the side information.

8

Online Learning with Gated Linear Networks

Figure 2: Behaviour of the kth neuron in layer i > 0 of a Gated Linear Network.

3. Gated Linear Networks

We now introduce gated linear networks (GLNs), which are feed-forward networks composed
of many layers of gated geometric mixing neurons. Each neuron in a given layer outputs a
gated geometric mixture over the predictions from the previous layer, with the ﬁnal layer
consisting of just a single neuron that determines the output of the entire network.

3.1 Model

Once again let Z denote the set of possible side information and C ⊂ N be a ﬁnite set called
the context space. A GLN is a network of sequential, probabilistic models organized in
L + 1 layers indexed by i ∈ {0, . . . , L}, with Ki models in each layer. Models are indexed
by their position in the network when laid out on a grid; for example, ρik will refer to the
kth model in the ith layer. The zeroth layer of the network is called the base layer and is
constructed from K0 probabilistic base models {ρ0k}K0−1
k=0 of the form given in Section 2.1.
Since each of their predictions is assumed to be a function of the given side information and
all previously seen examples, these base models can essentially be arbitrary. The nonzero
layers are composed of gated geometric mixing neurons. Associated to each of these will be
a ﬁxed context function cik : Z → C that determines the behavior of the gating. In addition

9

Veness, Lattimore, Bhoopchand, Grabska-Barwinska, Mattern, Toth

Figure 3: A graphical illustration of a Gated Linear Network.

to the context function, for each context c ∈ C and each neuron (i, k) there is an associated
weight vector wikc ∈ RKi−1 which is used to geometrically mix the inputs. We also enforce
the constraint of having a non-adaptive bias model on every layer, which will be denoted
by ρi0 for each layer i. Each of these bias models will correspond to a Bernoulli Process
with parameter β. These bias models play a similar role to the bias inputs in MLPs.

Network Output Given a z ∈ Z, a weight vector for each neuron is determined by
evaluating its associated context function. The output of each neuron can now be described
inductively in terms of the outputs of the previous layer. To simplify the notation, we
assume an implicit dependence on x<t and let pij(z) = ρij(xt = 1 | x<t; z) denote the
output of the jth neuron in the ith layer, and pi(z) = (cid:0)pi0(z), pi1(z), . . . , piKi−1(z)(cid:1) the
output of the ith layer. The bias output for each layer is deﬁned to be pi0(z) = β for all
z ∈ Z, for all 0 ≤ i ≤ L + 1, where β ∈ (0, 1) \ {1/2}.1 From here onwards we adopt the
convention of setting β = e/(e + 1) so that logit(β) = 1. For layers i ≥ 1, the kth node in
the ith layer receives as input the vector of dimension Ki−1 of predictions of the preceding
layer (see Figure 3). The output of a single neuron is the geometric mixture of the inputs
with respect to a set of weights that depend on its context, namely

pik(z) = σ (cid:0)wikcik(z) · logit (pi−1 (z))(cid:1) ,

as illustrated by Figure 2. The output of layer i can be re-written in matrix form as

pi(z) = σ(Wi(z) logit(pi−1(z))) ,

(6)

1. The seemingly odd technical constraint β (cid:54)= 0.5 is required to stop the partial derivatives of the loss with

respect to the bias weight being 0 under geometric mixing.

10

Online Learning with Gated Linear Networks

where Wi(z) ∈ RKi×Ki−1 is the matrix with kth row equal to wik(z) = wikcik(z). Iterating
Equation 6 once gives

pi(z) = σ (Wi (z) logit (σ (Wi−1 logit(pi−2 (z)))) .

Since logit is the inverse of σ, the ith iteration of Equation 6 simpliﬁes to

pi(z) = σ

Wi(z)Wi−1(z) . . . W1(z) logit(p0(z))

(7)

(cid:17)

.

(cid:16)

Equation (7) shows the network behaves like a linear network (Baldi and Hornik, 1989;
Saxe et al., 2013), but with weight matrices that are data-dependent. Without the data
dependent gating, the product of matrices would collapse to single linear mapping, giving
the network no additional modeling power over a single neuron (Minsky and Papert, 1969).

3.2 Learning in Gated Linear Networks

We now describe how the weights are learnt in a Gated Linear Network. While architec-
turally a GLN appears superﬁcially similar to the well-known multilayer perception (MLP),
what and how it learns is very diﬀerent. The key diﬀerence is that every neuron in a GLN
probabilistically predicts the target. This allows us to associate a loss function to each
neuron. This loss function will be deﬁned in terms of just the parameters of the neuron
itself; thus, unlike backpropagation, learning will be local. Furthermore, this loss function
will be convex, which will allow us to avoid many of the diﬃculties associated with training
typical deep architectures. For example, we can get away with simple deterministic weight
initializations, which aids the reproducibility of empirical results. The convexity allows us
to learn from correlated inputs in an online fashion without suﬀering signiﬁcant degrada-
tions in performance. And as we shall see later, GLNs are extremely data eﬃcient, and
can produce state of the art results in a single pass through the data. One should think of
each layer as being responsible for trying to directly improve the predictions of the previous
layer, rather than a form of implicit non-linear feature/ﬁlter construction as is the case with
MLPs trained oﬄine with back-propagation (Rumelhart et al., 1988).

Weight space and initialization For our subsequent theoretical analysis, we will require
that the weights satisfy the following mild technical constraints:

1. wijc0 ∈ [a, b] ⊂ R for some real a < 0 and b > 0;

2. wijc ∈ S ⊂ RKi−1 where S is a compact, convex set such that ∆Ki−1−1 ⊂ S.

One natural way to simultaneously meet these constraints is to restrict each neurons con-
textual weight vectors to lie within some (scaled) hypercube: wijc ∈ [−b, b]Ki−1, where
b ≥ 1. For what follows we will need notation to talk about weight vectors at speciﬁc times.
From here onwards we will use w(t)
ijc to denote the weight vector wijc at time t. As each
neuron will be solving an online convex programming problem, initialization of the weights
is straightforward and is non-essential to the theoretical analysis. Choices found to work
well in practice are:

• zero initialization: w(1)

ikc = 0

for all i, k, c.

11

Veness, Lattimore, Bhoopchand, Grabska-Barwinska, Mattern, Toth

for all i, k, c.

• geometric average initialization: w(1)

ikc = 1/Ki−1
The zero initialization can be seen as a kind of sparsity prior, where each input model is
considered a-priori to be unimportant, which has the eﬀect of making the geometric mixture
rapidly adapt to incorporate the predictions of the best performing models. The geometric
average initialization forces the geometric mixer to (unsurprisingly) initially behave like a
geometric average of its inputs, which makes sense if one believes that the predictions of each
input model are reasonable. One could also use small random weights, as is typically done
in MLPs, but we recommend against this choice because it makes little practical diﬀerence
and has a negative impact on reproducibility.

Weight update Learning in GLNs is straightforward in principle; as each neuron prob-
abilistically predicts the target, one can treat the current input to any neuron as a set of
expert predictions and apply a single step of local online learning using one of the no-regret
methods discussed in Section 2.2.2. The particular choice of optimization method will not
in any way aﬀect the capacity results we present later. For practical reasons however we
focus our attention on Online Gradient Descent (Zinkevich, 2003) with Wik a hypercube.
This allows the weight update for any neuron at layer i to be done in time complexity
O(Ki−1), which permits the construction of large networks.

t (wijc) denote the loss of the jth neuron in layer i. Using Equa-

More precisely, let (cid:96)ij

tion (3) we have

(cid:96)ij
t (wijc) = (cid:96)t(geow(· ; pi−1(zt), xt).

Now, for all i ∈ [1, L], j ∈ Ki, and for all c = cij(zt), we set
t (w(t)
ijc)

w(t)
ijc − ηt∇(cid:96)ij

ijc = Πi

w(t+1)

(cid:16)

(cid:17)

,

where Πi is the projection operation onto hypercube [−b, b]Ki−1:

Πi(x) = arg min

(cid:107)y − x(cid:107)2 .

y∈[−b,b]Ki−1

The projection is eﬃciently implemented by clipping every component of w(t)
ijc to the in-
terval [−b, b]. The learning rate ηt ∈ R+ can depend on time; we will revisit the topic of
appropriately setting the learning rate in subsequent sections.

Performance guarantees for individual neurons Let (i, k) refer to a non-bias neuron.
We now state a performance guarantee for a single neuron when Wik = [−b, b]Ki−1. Given
a ∈ C consider the index set Nika(n) = {t ≤ n : cik(zt) = a}, the set of rounds when context
a is observed by neuron (i, k). The regret experienced by the neuron on Nika(n) is deﬁned
as the diﬀerence between the losses suﬀered by the neuron and the losses it would have
suﬀered using the best choice of weights in hindsight.

Rika(n) =

(cid:88)

t (w(t)
(cid:96)ik

ika) − min
w∈Wik

(cid:88)

(cid:96)ik
t (w) .

t∈Nika(n)

t∈Nika(n)

Zinkevich (2003) showed that if the learning rates are set to ηt = D
√
G
gradient descent is at most

t

, then the regret of

(8)

(9)

(10)

(11)

Rika(n) ≤

(cid:112)|Nika(n)| ,

3DG
2

12

Online Learning with Gated Linear Networks

√

where D = maxx,y∈W (cid:107)x − y(cid:107)2 is the diameter of Wik and G is an upper bound on the
2-norm of the gradient of the loss.
Ki−1, with a bound
In our case we have D = 2b
G ≤
ε ) following from Proposition 1 Part 4.(b) by ensuring the input to each
neuron is within [ε, 1 − ε]. The regret deﬁnition given in Eq. (10) is on the subsequence of
rounds when cik(zt) = a. The total regret for neuron (i, k) is

Ki−1 log( 1

√

Rik(n) =

Rika(n) .

(cid:88)

a∈C

(12)

Combining the above and simplifying, we see that

Rik(n) ≤

3DG
2

(cid:88)

a∈C

(cid:112)|Nika(n)| =

3DG|C|
2

1
|C|

(cid:88)

a∈C

(cid:112)|Nika(n)|

≤

3DG|C|
2

(cid:115)

(cid:88)

a∈C

1
|C|

|Nika(n)| =

|C|

|Nika(n)|,

(cid:115)

3DG
2

(cid:88)

a∈C

where the ﬁrst and second inequalities follow from Eq. (11) and Jensen’s inequality re-
spectively. Noting that (cid:83)
a∈C Nika(n) = {1, . . . , n} allows us to further simplify the bound
to

Rik(n) ≤

3DG
2

(cid:112)|C|n .

In the particular case where each input probability is bounded between [ε, 1 − ε] and the
weights reside in the hypercube [−b, b]Ki−1, the above bound becomes

Rik(n) ≤ 3bKi−1

(cid:112)|C|n log

(cid:19)

.

(cid:18) 1
ε

(13)

The cumulative loss suﬀered by the omniscient algorithm is:

(cid:88)

a∈C

min
w∈Wik

(cid:88)

(cid:96)ik
t (w) .

t∈Nika(n)

The regret is the diﬀerence between the losses suﬀered by the learning procedure and the
n) additional penalty
above quantity. Since the latter usually grows like Ω(n), the O(
suﬀered by the algorithm is asymptotically negligible.

√

How should we interpret Equation 13? First of all, note the linear dependence on the
number of inputs; this is the price a single neuron pays in the worst case for increasing the
width of the preceding layer. There is also a linear dependence on b, which is expected as
when the competitor set gets larger it should be more diﬃcult to compete with the optimal
solution in hindsight. Clipping inputs is common implementation trick for many deep
learning systems; here this has the eﬀect of stopping the 2-norm of the gradient exploding,
which would adversely aﬀect the worst case regret. Furthermore, by assuming the unit basis
vector ej ∈ Wik we guarantee that the cumulative loss of neuron (i, k) is never much larger
than the cumulative loss of neuron (i − 1, j); to see this, note that if the weights were set to
ej then geometric mixing simply replicates the output of the jth neuron in the preceding

13

Veness, Lattimore, Bhoopchand, Grabska-Barwinska, Mattern, Toth

layer. This is the bare minimum we should expect, but the guarantee actually provides
much more than this by ensuring that the cumulative loss of neuron (i, k) is never much
worse than the best geometric mixture of its inputs. As we will observe in Section 4, if the
context functions are suﬃciently rich, then the layering of geometric mixtures combined
with Eq. (11) leads to a network that learns to approximate arbitrary functions.

3.3 Computational properties

Some computational properties of Gated Linear Networks are now discussed.

Complexity of a single online learning step Generating a prediction requires com-
puting the contexts from the given side information for each neuron, and then performing
L matrix-vector products. Under the assumption that multiplying a m × n by n × 1 pair
of matrices takes O(mn) work, the total time complexity to generate a single prediction is
for the matrix-vector products, which in typical cases will dominate the
O
overall runtime. Using online gradient descent just requires updating the rows of the weight
matrices using Equation 9; this again takes time O

i=1 KiKi−1

(cid:16)(cid:80)L

(cid:16)(cid:80)L

(cid:17)

(cid:17)

.

i=1 KiKi−1

Parallelism When generating a prediction, parallelism can occur within a layer, similar
to an MLP. The local training rule however enables all the neurons to be updated simul-
taneously, as they have no need to communicate information to each other. This compares
favorably to back-propagation and signiﬁcantly simpliﬁes any possible distributed imple-
mentation. Furthermore, as the bulk of the computation is primarily matrix multiplication,
large speedups can be obtained straightforwardly using GPUs.

In the case where no online updating is
Static prediction with pre-trained models
desired, prediction can be implemented potentially more eﬃciently depending on the exact
shape of the network architecture. Here one should implement Equation 7 by ﬁrst solving
a Matrix Chain Ordering problem (Hu and Shing, 1982) to determine the optimal way to
group the matrix multiplications.

4. Eﬀective Capacity of Gated Linear Networks

Neural networks have long been known to be capable of approximating arbitrary continuous
functions with almost any reasonable activation function (Hornik, 1991, and others). We
will show that provided the contexts are chosen suﬃciently richly, then GLNs also have the
capacity to approximate large classes of functions. More than this, the capacity is eﬀective
in the sense that gradient descent will eventually ﬁnd the approximation.
In contrast,
similar results for neural networks show the existence of a choice of weights for which the
neural network will approximate some function, but do not show that gradient descent (or
any other single algorithm) will converge to these weights. Of course we do not claim that
gated linear networks are the only model with an eﬀective capacity result. For example,
k-nearest-neighbour with appropriately chosen k is also universal for large function classes.
We view universality as a good ﬁrst step towards a more practical theoretical understanding
of a model, but the ultimate goal is to provide meaningful ﬁnite-time theoretical guarantees
for interesting and useful function classes. Gated linear networks have some advantages

14

Online Learning with Gated Linear Networks

over other architectures in the sense that they are constructed from small pieces that are
well-understood in isolation and the nature of the training rule eases the analysis relative
to neural networks.

Our main theorem is the following abstract result that connects the richness of the
context functions used in each layer to the ability of a network with no base predictors to
make constant improvements. After the theorem statement we provide intuition on simple
synthetic examples and analyse the richness of certain classes of context functions.

Theorem 1 (Capacity) Let µ be a measure on (Rd, F) with F the Lebesgue σ-algebra and
let zt ∈ Rd be a sequence of independent random variables sampled from µ. Furthermore, let
x1, x2, . . . be a sequence of independent Bernoulli random variables with P (xt = 1|zt) = f (zt)
for some F-measurable function f : Rd → [0, 1]. Consider a gated linear network and for
each layer i let Gi = {cik : 1 ≤ k ≤ Ki} be the set of context functions in that layer.
Assume there exists a δ > 0 such that for each non-bias neuron (i, k) the weight-space Wik
is compact and (−δ, δ) × ∆Ki−1−2 ⊆ Wik. Provided the weights are learned using a no-regret
algorithm, then the following hold with probability one:

1. For each non-bias neuron (i, k) there exists a non-random function p∗

ik : Rd → (0, 1)

such that

lim
n→∞

1
n

n
(cid:88)

t=1

sup
z∈Supp(µ)

(cid:12)
(cid:12)pik(z; w(t)) − p∗
(cid:12)

(cid:12)
(cid:12)
(cid:12) = 0 .
ik(z)

2. The average of p∗
the contexts:

ik converges to the average of f on the partitions of Rd induced by

∞
(cid:88)

i=1

max
k

max
c∈Gi+1

(cid:32)(cid:90)

(cid:88)

a∈C

c−1(a)

(f (z) − p∗

ik(z)) dµ(z)

(cid:33)2

≤ max

2,

log

(cid:26)

(cid:27)

4
δ

(cid:19)

(cid:18) 1
β

,

where β = e/(1 + e) is the output of bias neurons pi0(z).

3. There exists a non-random F-measurable function p∗

∞ : Rd → (0, 1) such that

(cid:90)

lim
i→∞

max
k

Rd

(p∗

ik(z) − p∗

∞(z))2dµ(z) = 0 .

The assumption that the sequence of side information is independent and identically
distributed may be relaxed signiﬁcantly. A suﬃcient criteria is that z1, z2, . . . follows a
Markov processes that mixes to stationary measure µ. Unfortunately this requires a benign
assumption on the stability of the learning algorithm, which is satisﬁed by most online
learning algorithms including online gradient descent with appropriately decreasing learning
rates.

Deﬁnition 2 A learning procedure for GLNs is stable if there exist constants r < 0 and
C > 0 such that for all data sequences (xt)t and (zt)t it holds that:

(cid:13)
(cid:13)w(t)
(cid:13)

ika − w(t+1)

ika

(cid:13)
(cid:13)
(cid:13)∞

≤ 1

ik (a)(zt)Ctr
c−1

for all t .

15

Veness, Lattimore, Bhoopchand, Grabska-Barwinska, Mattern, Toth

The proofs of Theorems 1 and 3 are given in Appendices B and C respectively.

Theorem 3 Suppose the learning procedure is stable and that all conditions of Theorem 1
hold except that (zt)t is sampled from an aperiodic and φ-irreducible Markov chain with
stationary distribution µ. Then (1), (2) and (3) from Theorem 1 hold.

Theorem 3 suggests that GLNs could be well suited reinforcement learning applications.
For example, one could combine a GLN-based density model with the policy evaluation
approach of Veness et al. (2015).

Interpretation of Theorems 1 and 3 The theorem has three parts. The ﬁrst says
the output of each neuron converges almost surely in Cesaro average to some nonrandom
function on the support of µ. Outside the support of µ the algorithm will never observe
data, so provided there is no covariate shift we should not be concerned about this. The
second part gives a partial characterisation of what this function is. Let

(cid:32)(cid:90)

(cid:33)2

εi = max

k

max
c∈Gi+1

c−1(a)

(f (z) − p∗

ik(z))dµ(z)

.

The theorem says that (cid:80)∞
i=1 εi = O(1), which since εi is clearly positive implies that
limi→∞ εi → 0. The convergence is fast in the sense that εi is approximately subharmonic.
Suppose that Gi = G is the same for all layers and εi is small, then for all c ∈ G and a ∈ C
we have

(cid:90)

(cid:90)

f (z)dµ(z) ≈

c−1(a)

c−1(a)

p∗
ik(z)dµ(z) ,

ik is approximately equal to f on average on all sets c−1(a) ⊂ Rd. We
which means that p∗
will shortly see that if {c−1(a) : c ∈ G, a ∈ C} is suﬃciently rich, then f (z) ≈ p∗
ik(z) also
holds. Finally, the last part of the theorem shows that as the number of layers tends to
inﬁnity, the output of neurons in later layers converges to some single function. Note that
all results are about what happens in the limit of inﬁnite data. In principle one could apply
the regret guarantees from gradient descent to calculate a rate of convergence, a task which
we leave for the future.

Intuition on synthetic data We now illustrate the behaviour of a small network with
various contexts in the simple case of d = 1. Figure 5 shows the chosen architecture
and the problem setup. Each box relates to a non-bias neuron of the network, comparing
the ground truth, f (z) (black line) to the output of each neuron, pik(x|z) (coloured line).
Axes are identical for all boxes, and labelled on bottom left: The single-dimensional side
information z ∈ [−3, 3] is used to derive the Bernoulli parameters according to a Gaussian
transformation f (z) = e−z2/2. Thus, the training data consist of samples zt drawn uniformly
over the range [−3, 3], and their labels drawn from Bernoulli distributions parametrised by
f (zt).

The GLN has six layers, with 3/2/2/2/2/1 non-bias neurons in each layer reading from
bottom to top (Fig. 5), and a single base predictor p00(z) = α for all z (not shown). All
bias neurons are also set to p0k = β (not shown). The half-space contexts of the non-bias

16

Online Learning with Gated Linear Networks

neurons are parametrized by an oﬀset, chosen randomly (coloured dashed lines in Fig. 5).
The bottom row depicts the output of the network in the ﬁrst layer after training. Each
neuron learns to predict the average of f on the partitions of [−3, 3] induced by c−1
1k (a)
for each a ∈ {0, 1}. Already in the second layer, the neurons’ outputs gain in complexity,
combining information not only from the neuron’s own contexts, but also from the estimates
of the neurons below. In eﬀect, every output has 4 discontinuities (albeit two of them are
very close, due to two contexts in the ﬁrst layer having similar oﬀsets). The top panel shows
the output of the non-bias neuron in the sixth layer, which even for this small network is
predicting with reasonable accuracy.

The performance of the network is easily improved by increasing the number of neurons
in the ﬁrst layer, as shown in Figure 4. If K1 = 100, then the output of the non-bias neuron
in the second layer is a near-perfect predictor. As it happens, a two-layer network with half-
space contexts and suﬃciently many neurons in the ﬁrst layer can learn to approximate any
continuous function, which explains the performance of such a shallow network on this
problem. Note that the base predictor is completely uninformative in this case and the
half-space contexts are not tuned to the data.

Applications of Theorem 1 We now see several applications of Theorem 1 for diﬀerent
contexts.
If we assume that Gi = G for all i, then the result shows that in the limit of
inﬁnite data a suﬃciently deep network predicts like the average of f on c−1(a) for all
context functions c ∈ G and all a ∈ C. When the space of context functions is suﬃciently
rich this implies that the network indeed predicts like f . Let G be a (possibly inﬁnite) set
of context functions such that

(cid:107)h(cid:107)G = sup
c∈G

(cid:12)
(cid:90)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

a∈C

c−1(a)

h(z)dµ(z)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

is a norm on the space of bounded measurable functions. Note that homogeneity, non-
negativity and the triangle inequality are trivial, which leaves one only to check that (cid:107)h(cid:107)G =
0 implies that h = 0 µ-almost-everywhere. Now suppose that G1 ⊂ G2 ⊂ G3 ⊂ · · ·
is a
sequence of ﬁnite subsets of G such that for any bounded measurable h

lim
i→∞

(cid:107)h(cid:107)Gi

= (cid:107)h(cid:107)G ,

(14)

Figure 4: Output of two-layer network with evenly spaced half-space contexts and K1 = 100

17

Veness, Lattimore, Bhoopchand, Grabska-Barwinska, Mattern, Toth

Figure 5: Output of a six-layer network with half-space contexts. Each box represents a non-bias
neuron in the network, the function to ﬁt is shown in black, and the output distribution learnt by
each neuron is shown in colour (for example, red for the ﬁrst layer and purple for the top-most
neuron). All axes are identical, as labeled on bottom left. The dashed coloured lines represent the
choice of hyperplane for each neuron.

which is true in the special case that G is countable and Gi ↑ G. Then by Parts (2) and (3)
of Theorem 1 we have

0 = lim
i→∞

max
c∈Gi

(cid:12)
(cid:90)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

a∈C

c−1(a)

(f (z) − p∗

(cid:12)
(cid:12)
(cid:12)
∞)dµ(z)
(cid:12)
(cid:12)

= lim
i→∞

(cid:107)f − p∗

∞(cid:107)Gi

= (cid:107)f − p∗

∞(cid:107)G .

Therefore µ(z : f (z) − p∞(z) = 0) = 1, which means that almost surely the network
asymptotically predicts like f µ-almost-everywhere. We now give some example countable
classes of contexts, which by Eq. (14) can be used to build a universal network. All the
claims in the following enumeration are proven in Appendix D.

• (Covers) Let Br(x) = {y : (cid:107)x − y(cid:107)2 < r} be the open set of radius r > 0.

If
G = {1Br(x) : x ∈ Qd, r ∈ Q ∩ (0, ∞)}, then (cid:107)·(cid:107)G is a norm on the space of bounded
measurable functions.

• (Covers cont.) The above result can easily be generalised to the situation where G is

the set of indicator functions on any countable base of Rd.

• (Hyperplanes) Now assume that µ is absolutely continuous with respect to the Lebesgue

If G is the space of context functions that are indicators on half-spaces
measure.
G = {1Hv,c : v ∈ Qd, (cid:107)v(cid:107) = 1, c ∈ Q}, then (cid:107)·(cid:107)G is a norm on the spaces of bounded
F-measurable functions.

18

Online Learning with Gated Linear Networks

We conclude this section with three remarks.

Remark 4 (Asymptotic non-diversity) Part (3) of Theorem 1 shows that asymptoti-
cally in the depth of the network there is vanishingly little diversity in the predictors in the
bottom layer. The intuition is quite simple. If the contexts are suﬃcient that the network
learn the true function, then eventually all neurons will predict perfectly (and hence be the
same). On the other hand, if the class of contexts is not rich enough for perfect learning,
then there comes a time when all neurons in a given layer cannot do better than copy-
ing the output of their best parent. Once this happens, all further neurons make the same
predictions.

Remark 5 (Universality of shallow half-spaces) If µ is supported on a compact set
and f is continuous, then a two-layer network is suﬃcient to approximate f to arbitrary
precision with half-space contexts. A discussion of this curiosity is deferred to Appendix E.
In practice we observe that except when d = 1 it is beneﬁcial to use more layers. And note
that arbitrary precision is only possible by using arbitrarily large weights and wide networks.

Remark 6 (Eﬀective capacity (cid:54)= capacity) Theorem 1 shows that if the context func-
tions are suﬃciently rich and the network is deep, then the network can learn any well-
behaved function using online gradient descent. This is an eﬀective capacity result because
we have provided a model (the network) and a learning procedure (OGD) that work together
to learn any well-behaved function. For a ﬁxed architecture, the capacity of a GLN can be
much larger than the eﬀective capacity. In particular, there exist functions that can be mod-
elled by a particular choice of weights that online gradient descent will never ﬁnd. (This does
not contradict Theorem 1; one can always construct a larger GLN whose eﬀective capacity
is suﬃciently rich to model such functions.)

To construct an example demonstrating this failure we exploit the
fact that training each neuron against the target means that neurons do
not coordinate to solve the following ‘exclusive-or’ problem. Let d = 2
and µ be uniform on [−1, 1]2 and f : R2 → [0, 1] be given by f (z) =
1{z1z2 ≥ 0}. Next suppose G = {c1, c2} where ci(z) = 1{zi ≥ 0}.
Notice that z1 or z2 alone is insuﬃcient to predict f (z).
If a gated
linear network with Ki = 2 for all i ≥ 2 and contexts from G is trained
in the iid setting of Theorem 1, then it is easy to check that limi→∞ p∗
ik(z) = 1/2 almost
surely. And yet there exists a choice of weights such that pik(z; w) = f (z) for all z. The
problem is the optimal weights have neurons in the same layer coordinating so that a single
neuron in the next layer has access to z1 and z2 as inputs. But online gradient descent has
neurons selﬁshly optimizing their own performance, which does not lead to coordination in
this case.

f (z) = 1 f (z) = 0

f (z) = 0

f (z) = 1

5. Adaptive Regularization via Sub-network Switching

While GLNs can have almost arbitrary capacity in principle, large networks are susceptible
to a form of the catch-up phenomenon (van Erven et al., 2007). During the initial stages
of learning neurons in the lower layers typically have better predictive performance than
neurons in the higher layers. This section presents a solution to this problem based on

19

Veness, Lattimore, Bhoopchand, Grabska-Barwinska, Mattern, Toth

Figure 6: A visual depiction of the run-length encoding prior.

switching (Veness et al., 2012), a ﬁxed share (Herbster and Warmuth, 1998) variant tailored
to the logarithmic loss. The main idea is simple: as each neuron predicts the target, one
can construct a switching ensemble across all neurons predictions. This guarantees that the
predictions made by the ensemble are not much worse than the predictions made by the
best sparsely changing sequence of neurons. We now describe this process in detail.

5.1 Model

Let M = {ρij : i ∈ [1, L], j ∈ [0, Ki − 1]} denote the model class consisting of all neurons
that make up a particular GLN with L layers and Ki neurons in each layer. Now for all
n ∈ N, for all x1:n ∈ X n, consider a Bayesian (non-parametric) mixture that puts a prior
wτ (·) over all sequences of neurons inside the index set In(M) = Mn, namely

τ (x1:n) =

wτ (ν1:n) ν1:n(x1:n)

(15)

(cid:88)

ν1:n∈In(M)

where ν1:n(x1:n) = (cid:81)n
satisfy (cid:80)
ν1:n
it immediately follows that for any ν∗

k=1 νk(xk | x<k). As wτ (·) is a prior, it needs to be non-negative and
wτ (ν1:n) = 1 for all n ∈ N. From the dominance property of Bayesian mixtures

1:n ∈ In(M) we have

− log τ (x1:n) ≤ − log (wτ (ν∗

1:n) − log ν∗

1:n(x1:n).

(16)

1:n (x1:n)) ≤ − log wτ (ν∗
1:n) ν∗
(cid:17)

(cid:16) τ (x1:n)
ν∗
1:n(x1:n)

Thus the regret Rn = − log
1:n is upper
bounded by − log wτ (ν∗
1:n). Putting a uniform prior over all neuron sequences would lead to
a vacuous regret of n log(|M|), so we are forced to concentrate our prior mass on a smaller
set of neuron sequences we think a-priori are likely to predict well.

with respect to a sequence of models ν∗

Empirically we found that when the number of training examples is small neurons in
the lower layers usually predict better than those in higher layers, but this reverses as more
data becomes available. Viewing the sequence of best-predicting neurons over time as a

20

Online Learning with Gated Linear Networks

string, we see that a run-length encoding gives rise to a highly compressed representation
with a length linear in the number of times the best-predicting neuron changes. Run-length
encoding can be implemented probabilistically by using an arithmetic encoder with the
following recursively deﬁned prior:

wτ (ν1:n) =






1

if
if

1
|M|
wτ (ν<n) ×

n = 0
n = 1
(cid:16) n−1
n

1[νn = νn−1] +

1
n(|M|−1)

(cid:17)
1[νn (cid:54)= νn−1]

otherwise .

(17)

A graphical depiction of this prior is shown in Figure 6. Multiple sequences of models are
depicted by rows of light blue and dark blue circles, with each colour depicting a diﬀerent
choice of model. A run length encoding of the sequence is given by a string made up of
pairs of symbols, the ﬁrst being an integer representing the length of the run and the second
a character (B for dark blue, b for light blue) depicting the choice of model. One can see
from the ﬁgure that high prior weight is assigned to model sequences which have shorter
run length encodings. More formally, one can show (see Appendix F) that

− log wτ (ν1:n) ≤ (s(ν1:n) + 1) (log |M| + log n) ,

(18)

for all ν1:n ∈ In(M), where s(ν1:n) := (cid:80)n
I[νt (cid:54)= νt−1] denotes the number of switches
from one neuron to another in ν1:n. Combining Equations 16 and 18 allows us to upper
bound the regret Rn with respect to an arbitrary ν∗

1:n ∈ In(M) by

t=2

(s(ν∗

1:n) + 1) (log |M| + log n) .

Therefore, when there exists a sequence of neurons with a small number s(ν∗
1:n) (cid:28) n of
switches that performs well, only logarithmic regret is suﬀered, and one can expect the
switching ensemble to predict almost as well as if we knew what the best performing sparsely
changing sequence of neurons was in advance. Note too that the bound holds in the case
where we just predict using a single choice of neuron; here only log |M| + log n is suﬀered,
so there is essentially no downside to using this method if such architecture adaptivity is
not needed.

5.2 Algorithm

A direct computation of Equation 15 would require |M|n additions, which is clearly in-
tractable. The switching algorithm of Veness et al. (2012) runs in time O(n|M|), but was
originally presented in terms of log-marginal probabilities, which requires a log-space im-
plementation to avoid numerical diﬃculties. An equivalent numerically robust formulation
is described here, which incrementally maintains a weight vector that is used to compute a
convex combination of model predictions at each time step.

Let u(t)

ik ∈ (0, 1] denote the switching weight associated with the neuron (i, k) at time t.
k=0 u(t)
ik = 1 for all t. At each time step t,

The weights will satisfy the invariant (cid:80)|L|
i=1
the switching mixture outputs the conditional probability

(cid:80)Ki−1

τ (xt|x<t) =

u(t)
ik ρik(xt|x<t),

|L|
(cid:88)

Ki−1
(cid:88)

i=1

k=0

21

Veness, Lattimore, Bhoopchand, Grabska-Barwinska, Mattern, Toth

Figure 7: Evolution of model ﬁt over time

with the weights deﬁned, for all 1 ≤ i ≤ L and 0 ≤ k < Ki, by u(1)

ik = 1/|M| and

u(t+1)
ik =

1
(t + 1)(|M| − 1)

+

(cid:18) t|M| − t − 1

(cid:19) u(t)

(t + 1)(|M| − 1)

ik ρik(xt|x<t)
τ (xt|x<t)

.

This weight update can be straightforwardly implemented in O(|M|) time per step. To avoid
numerical issues, we recommend enforcing the weights to sum to 1 by explicitly dividing by
(cid:80)|L|
i=1

after each weight update.

k=0 u(t+1)

(cid:80)Ki−1

ik

5.3 Illustration

Figure 7 shows an example of a small 6 layer GLN ﬁtting a family of Bernoulli distributions
parametrized by a Gaussian transformation g(z) = e−z2/2 of a single dimensional side
information value z ∈ [−3, 3], which is the same setup depicted in Fig. 5 without switching.
The GLN has 6 layers, with 3/2/2/2/2/1 neurons on each layer reading from bottom to
top. The output distribution learnt by each neuron is shown in colour (for example, red

22

Online Learning with Gated Linear Networks

Figure 8: The spiral dataset. The horizontal and vertical axes describe the values of the two
dimensional side information, with the class label being indicated by either a red, grey or blue point.

for the bottom layer and purple for the top neuron). The dashed vertical line in each
distribution denotes the choice of half-space which controls the gating of input examples.
Each example was generated by ﬁrst sampling a z uniformly distribution between [−3, 3]
and then sampling a label from a Bernoulli distribution parameterized by g(z). The topmost
box in each of the four subﬁgures shows the output distribution (in black) obtained using
subnetwork switching. The top-left subﬁgure shows the model ﬁt at t = 1000; the top-
middle at t = 2000; and the top-right the ﬁt at t = 4000. One can clearly see the model ﬁt
improving over time, and that with suﬃcient training the higher level neurons better model
the target family of distributions. The bottom graph in the ﬁgure shows the evolution of the
switching weights over time; here we see that initially a neuron on the 3rd layer dominates
the predictions, then one in the 4th layer, then the 5th and subsequently settling down on
the predictions made by the top-most neuron.

6. Empirical Results

We now present a short series of case studies to shed some insight into the capabilities of
GLNs. When describing our particular choice of architecture, we adopt the convention of
describing the number of neurons on each layer by a dashed list of natural numbers going
from the most shallow layer to the deepest layer.

6.1 Non-linear decision boundaries with half-spaces

Our ﬁrst experiment is on a synthetic ‘spiral’ ternary classiﬁcation task, which we use to
demonstrate the ability of GLNs to model non-linear decision boundaries. A visualization
of the dataset is shown in Figure 8. The horizontal and vertical axes describe the values of
the two dimensional side information, with the class label being indicated by either a red,
grey or blue point. To address this task with a GLN, we constructed an ensemble of 3 GLNs
to form a standard one-vs-all classiﬁer. Each member of the ensemble was a 3 layer network

23

Veness, Lattimore, Bhoopchand, Grabska-Barwinska, Mattern, Toth

Figure 9: From left to right, the decision boundary determined by a neuron on the bottom layer,
the middle layer, and the ﬁnal layer, and the top-level switching mixture. The ﬁt of the bottommost
neuron is quite poor. The middle neuron improves signiﬁcantly, making mistakes only at the center
of the spiral. The topmost neuron has learnt to ﬁt the data almost perfectly. The ﬁnal decision
boundary of the switching ensemble resembles a smoothed version of the deepest neuron.

consisting of 50-25-1 neurons which each used a half-space context for gating. The half-
space contexts for each neuron were determined by sampling a 2 dimensional normal vector
whose components were distributed according to N (0, 36), and a bias weight distributed
according to N (0, 9). Each component of all weight vectors were constrained to lie within
[−200, 200]. The side information was the 2-dimensional x/y values, and the input to the
network was the component-wise sigmoid of these x/y values (as GLNs require the input to
be within [0,1]). The learning rate was set to 0.01. Figure 9 shows a plot of the resultant
decision boundaries for neurons of diﬀering depths after training on a single pass through
all the examples.

6.2 Online Half-space Classiﬁcation: MNIST

Next we explore the use of GLNs for classiﬁcation on the well known MNIST dataset
(Lecun et al., 1998). Once again we used an ensemble of 10 GLNs to construct a one-vs-all
classiﬁer. Each member of the ensemble was a 3 layer network consisting of 1500-1500-1
neurons, each of which used 6 half-space context functions (using the context composition
technnique of Section 2.3.2) as the gating procedure, meaning that each neuron contained 64
diﬀerent possible weight vectors. The half-space contexts for each neuron were determined
by sampling a 784 dimensional normal vector whose components were distributed according
to N (0, 0.01), and a bias weight of 0. The learning rate for an example at time t was set to
min{8000/t, 0.3}. These parameters were determined by a grid search across the training
data. Each component of all weight vectors were constrained to lie within [−200, 200].
The input features were pre-processed by ﬁrst applying mean-subtraction and a de-skewing
operation (Ghosh and Wan, 2017).

Running the method purely online across a single pass of the data gives an accuracy
on the test set of 98.3%. If weight updating during the testing set is disabled, the test set
accuracy drops slightly to 98.1%. Without the de-skewing operation, the accuracy drops to
96.9%. It is worth nothing that our classiﬁer contains no image speciﬁc domain knowledge;
in fact, one could even apply a (ﬁxed across all images) permutation to each image input
and get exactly the same result.

24

Online Learning with Gated Linear Networks

6.3 Online Density Modeling: MNIST

Our ﬁnal result is to use GLNs and image speciﬁc gating to construct an online image
density model for the binarized MNIST dataset (Larochelle and Murray, 2011a), a standard
benchmark for image density modeling. By exploiting the chain rule

P(X1, X2, . . . , Xd) =

P(Xi | X<i)

d
(cid:89)

i=1

of probability, we constructed a so-called autoregressive density model over the 28 × 28
dimensional binary space by using 784 GLNs to model the conditional distribution for each
pixel; a row-major ordering was used to linearize the two dimensional pixel locations.

Base Layer The base layer for each GLN was determined by taking the outputs of a
number of skip-gram models (Guthrie et al., 2006). The context functions used by these
skip-gram models included a number of speciﬁc geometric patterns that are known to be
helpful for lossless image compression (Witten et al., 1999), plus a large number of randomly
sampled pixel locations. Some example skip-grams are shown in Figure 10; the red box

Figure 10: Example skip-gram context functions.

indicates the location of the pixel in the image, and the blue boxes indicate the pixels which
are used to determine the skip-gram context. For each pixel model, the base layer would
consist of up to 600 skip-gram predictions; the exact number of predictions depends on the
location of the pixel in the linear ordering; the set of permitted skip-gram models for pixel
i are those whose context functions do not depend on pixels greater than or equal i within
the linear ordering. The Zero-Redundancy estimator (Willems et al., 1997) was used to
estimate the per-context skip-gram probability estimates online.

Image speciﬁc context functions Empirically we found that we could signiﬁcantly
boost the performance of our method by introducing two simple kinds of image speciﬁc
context functions.

The ﬁrst were max-pool context functions, some examples of which are depicted in Figure
11. Once again the red square denotes the location of the current pixel. For each shaded
region of a particular colour, the maximum of all binary pixel values was computed. The
max-pool context function returns the number represented by the binary representation
obtained by concatenating these max-pooled values together (in some ﬁxed order).

The next type of context we introduced were distance context functions, some examples
of which are depicted in Figure 12. Once again the red square denotes the location of the
current pixel. Each non-white/non-red location is labeled with an index between 1 and

25

Veness, Lattimore, Bhoopchand, Grabska-Barwinska, Mattern, Toth

Figure 11: Example max-pool context functions.

the number of non-white/non-red locations; darker colours indicate smaller index values.
The distance context function returns the smallest index value whose pixel value is equal
to 1. This class of context allows one to measure distance to the nearest active pixel under
various orderings of local pixel locations.

Figure 12: Example distance context functions.

Network construction Each GLN used 4-layers, with the gating function for each neu-
ron determined by a choice of skip-gram/max-pool/distance context function. A set of 200
diﬀerent context functions were deﬁned, and these were randomly distributed across a net-
work architecture whose shape was 35-60-35-70. The learning rate for an example seen at
time t was set to min{25/t, 0.005}.

Results Running the method purely online across a single pass of the data (we concate-
nated the training, validation and test sets into one contiguous stream of data) gave an
average loss of 79.0 nats per image across the test data. To the best of our knowledge,
this result matches the state of the art (Van Den Oord et al., 2016) of any batch trained
density model which outputs exact probabilities, and is close to the estimated upper bounds
reported for state of the art variational approaches (Bachman, 2016).

7. Relationship to the PAQ family compressors

One of the main contributions of this paper is to justify and explain the empirical success
of the PAQ family of compression programs. At the time of writing, the best performing
PAQ implementation in terms of general purpose compression performance is cmix, an
open-source project whose details are described by Knoll (2017) on his personal website.
cmix uses a mixing network comprised of many gated geometric mixers, each of which
use diﬀerent skip-gram context functions to deﬁne the gating operations. While many
of the core building blocks have been analyzed previously by Mattern (2016), the reason

26

Online Learning with Gated Linear Networks

for the empirical success of such locally trained mixing networks had until now remained
somewhat of a mystery. Our work shows that such architectures are a special cases of
Gated Linear Networks, and that future improvements may result by exploring alternate
no-regret learning methods, by using a broader class of context functions or by using our
gated linear network formulation to enable eﬃcient implementation on GPUs via vectorized
matrix operations. In principle the universality of GLNs suggests that the performance of
PAQ-like approaches will continue to scale as hardware improves. Furthermore, our MNIST
density modeling results suggest that such methods can be competitive with state of the
art deep learning approaches on domains beyond general purpose ﬁle compression.

8. Conclusion

We have introduced Gated Linear Networks, a family of architectures designed for online
learning under the logarithmic loss. Under appropriate conditions, such architectures are
universal in the sense that they can can model any bounded Borel-measurable function;
more signiﬁcantly, they are guaranteed to ﬁnd this solution using any appropriate choice of
no-regret online optimization algorithm to locally adapt the weights of each neuron. Initial
experimental results suggest that the method warrants further investigation.

Thanks to A¨aron van den Oord, Byron Knoll, Malcolm Reynolds, Kieran Milan, Simon
Schmitt, Nando de Freitas, Chrisantha Fernando, Charles Blundell, Ian Osband, David
Budden, Michael Bowling, Shane Legg and Demis Hassabis.

Acknowledgments

References

Philip Bachman. An architecture for deep, hierarchical generative models. In D. D. Lee,
M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural
Information Processing Systems 29, pages 4826–4834. Curran Associates, Inc., 2016.

P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning
from examples without local minima. Neural Networks, 2(1):53–58, January 1989. ISSN
0893-6080. doi: 10.1016/0893-6080(89)90014-2.

David Balduzzi. Deep online convex optimization with gated games. CoRR, abs/1604.01952,

2016. URL http://arxiv.org/abs/1604.01952.

T. C. Bell, J. G. Cleary, and I. H. Witten. Text Compression. Prentice Hall, Englewood

Cliﬀs, NJ, 1990.

Tim Bell and Ross Arnold. A corpus for the evaluation of lossless compression algo-
doi: doi.

ISSN 1068-0314.

rithms. Data Compression Conference, 00:201, 1997.
ieeecomputersociety.org/10.1109/DCC.1997.582019.

Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge

University Press, New York, NY, USA, 2006. ISBN 0521841089.

27

Veness, Lattimore, Bhoopchand, Grabska-Barwinska, Mattern, Toth

Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman,
Ilya Sutskever, and Pieter Abbeel. Variational lossy autoencoder. CoRR, abs/1611.02731,
2016.

G. V. Cormack and R. N. S. Horspool. Data compression using dynamic markov modelling.

The Computer Journal, 30(6), 1987.

Jakob N. Foerster, Justin Gilmer, Jascha Sohl-Dickstein, Jan Chorowski, and David Sus-
sillo. Input switched aﬃne networks: An RNN architecture designed for interpretability.
In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Con-
ference on Machine Learning, volume 70 of Proceedings of Machine Learning Research,
pages 1136–1145, International Convention Centre, Sydney, Australia, 06–11 Aug 2017.
PMLR.

Christian Genest and James V. Zidek. Combining probability distributions: A critique and

an annotated bibliography. Statistical Science, 1(1):114–135, 1986.

Dibya Ghosh and Alvin Wan, 2017. URL https://fsix.github.io/mnist/Deskewing.

html.

Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Rezende, and Daan Wierstra. Draw: A
recurrent neural network for image generation. In David Blei and Francis Bach, editors,
Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages
1462–1471. JMLR Workshop and Conference Proceedings, 2015.

Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David
V´azquez, and Aaron C. Courville. Pixelvae: A latent variable model for natural images.
CoRR, abs/1611.05013, 2016.

David Guthrie, Ben Allison, W. Liu, Louise Guthrie, and Yorick Wilks. A closer look at
skip-gram modelling. In Proceedings of the Fifth international Conference on Language
Resources and Evaluation (LREC-2006), Genoa, Italy, 2006.

Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Opti-

mization, 2(3-4):157–325, 2016. doi: 10.1561/2400000013.

Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online

convex optimization. Machine Learning, 69(2-3):169–192, 2007.

Sigurdur Helgason. The radon transform on r n. In Integral Geometry and Radon Trans-

forms, pages 1–62. Springer, 2011.

Mark Herbster and Manfred K. Warmuth. Tracking the best expert. Machine Learning, 32

(2):151–178, August 1998. ISSN 0885-6125.

Geoﬀrey E. Hinton. Training products of experts by minimizing contrastive divergence.

Neural Computation, 14(8):1771–1800, August 2002. ISSN 0899-7667.

Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural net-

works, 4(2):251–257, 1991.

28

Online Learning with Gated Linear Networks

T. C. Hu and M. T. Shing. Computation of matrix chain products. part i. SIAM Journal

on Computing, 11(2):362–373, 1982. doi: 10.1137/0211028.

Marcus Hutter. Hutter prize, 2017. URL http://prize.hutter1.net/.

Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoﬀrey E. Hinton. Adaptive
ISSN 0899-7667. doi:

mixtures of local experts. Neural Comput., 3(1):79–87, 1991.
10.1162/neco.1991.3.1.79.

Kwang-Sung Jun, Francesco Orabona, Stephen Wright, and Rebecca Willett.

Improved
strongly adaptive online learning using coin betting. In Artiﬁcial Intelligence and Statis-
tics, pages 943–951, 2017.

B. Knoll and N. de Freitas. A machine learning perspective on predictive coding with paq8.

In Data Compression Conference (DCC), pages 377–386, April 2012.

Byron Knoll, 2017. URL http://www.byronknoll.com/cmix.html.

Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Ge-
oﬀrey Gordon, David Dunson, and Miroslav Dudk, editors, Proceedings of the Fourteenth
International Conference on Artiﬁcial Intelligence and Statistics, volume 15 of Proceed-
ings of Machine Learning Research, pages 29–37, Fort Lauderdale, FL, USA, 11–13 Apr
2011a. PMLR.

Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. Journal

of Machine Learning Research (JMLR), 15:29–37, 2011b.

Yann Lecun, Lon Bottou, Yoshua Bengio, and Patrick Haﬀner. Gradient-based learning
applied to document recognition. In Proceedings of the IEEE, pages 2278–2324, 1998.

Haipeng Luo, Alekh Agarwal, Nicol`o Cesa-Bianchi, and John Langford. Eﬃcient second
order online learning by sketching. In Advances in Neural Information Processing Systems
29: Annual Conference on Neural Information Processing Systems 2016, December 5-10,
2016, Barcelona, Spain, pages 902–910, 2016.

Matthew Mahoney. Fast text compression with neural networks. AAAI, 2000.

Matthew Mahoney. Adaptive weighing of context models for lossless data compression.

Technical Report, Florida Institute of Technology CS, 2005.

Matthew Mahoney. Data Compression Explained. 2013.

Christopher Mattern. Mixing strategies in data compression. In 2012 Data Compression

Conference, Snowbird, UT, USA, April 10-12, pages 337–346, 2012.

Christopher Mattern. Linear and geometric mixtures - analysis. In 2013 Data Compression
Conference, DCC 2013, Snowbird, UT, USA, March 20-22, 2013, pages 301–310, 2013.

Christopher Mattern. On Statistical Data Compression. PhD thesis, Technische Universit¨at

Ilmenau, Germany, 2016.

29

Veness, Lattimore, Bhoopchand, Grabska-Barwinska, Mattern, Toth

Sean P Meyn and Richard L Tweedie. Markov chains and stochastic stability. Springer

Science & Business Media, 2012.

Marvin Minsky and Seymour Papert. Perceptrons: An Introduction to Computational Ge-

ometry. MIT Press, Cambridge, MA, USA, 1969.

Georg Ostrovski, Marc G. Bellemare, A¨aron van den Oord, and R´emi Munos. Count-
based exploration with neural density models. In Proceedings of the 34th International
Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August
2017, pages 2721–2730, 2017.

David E. Rumelhart, Geoﬀrey E. Hinton, and Ronald J. Williams. Neurocomputing: Foun-
dations of research. chapter Learning Representations by Back-propagating Errors, pages
696–699. MIT Press, Cambridge, MA, USA, 1988. ISBN 0-262-01097-6.

Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear

dynamics of learning in deep linear neural networks. CoRR, abs/1312.6120, 2013.

J. Schmidhuber and S. Heil. Sequential neural text compression. IEEE Transactions on
Neural Networks, 7(1):142–146, Jan 1996. ISSN 1045-9227. doi: 10.1109/72.478398.

A¨aron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural
networks. In Proceedings of the 33rd International Conference on International Confer-
ence on Machine Learning - Volume 48, ICML’16, pages 1747–1756. JMLR.org, 2016.

Tim van Erven, Peter Grunwald, and Steven de Rooij. Catching up faster in bayesian
model selection and model averaging.
In Advances in Neural Information Processing
Systems 20, Proceedings of the Twenty-First Annual Conference on Neural Information
Processing Systems, Vancouver, British Columbia, Canada, December 3-6, 2007, pages
417–424, 2007.

Joel Veness, Kee Siong Ng, Marcus Hutter, and Michael H. Bowling. Context tree switching.
In 2012 Data Compression Conference, Snowbird, UT, USA, April 10-12, 2012, pages
327–336, 2012.

Joel Veness, Marc G. Bellemare, Marcus Hutter, Alvin Chua, and Guillaume Desjardins.
Compress and control. In Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial
Intelligence, January 25-30, 2015, Austin, Texas, USA., pages 3016–3023, 2015.

Frans Willems, Yuri Shtarkov, and Tjalling Tjalkens. Reﬂections on ”the context-tree
weighting method: Basic properties”. In IEEE Information Theory Society Newsletter,
volume 47, 1997.

Ian H. Witten, Radford M. Neal, and John G. Cleary. Arithmetic coding for data com-
pression. Communications of the ACM, 30(6):520–540, June 1987. ISSN 0001-0782. doi:
10.1145/214762.214771.

Ian H. Witten, Alistair Moﬀat, and Timothy C. Bell. Managing Gigabytes (2nd Ed.):
Compressing and Indexing Documents and Images. Morgan Kaufmann Publishers Inc.,
San Francisco, CA, USA, 1999. ISBN 1-55860-570-3.

30

Online Learning with Gated Linear Networks

Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient as-
cent. In Machine Learning, Proceedings of the Twentieth International Conference (ICML
2003), August 21-24, 2003, Washington, DC, USA, pages 928–936, 2003.

Appendix A. Proof of Proposition 1
Part (1): If xt = 1 then (cid:96)geo
t
σ(cid:48)(x) = σ(x)[1 − σ(x)] and Equation 2 once more gives

(w) = − log σ(logit(pt) · w) from Equation 2, hence using

∂(cid:96)geo
t
∂wi

∂(cid:96)geo
t
∂wi

= [σ(logit(pt) · w) − 1] logit(pt,i) = (geow(1 ; pt) − xt) logit(pt,i).

(19)

Similarly, if xt = 0 then (cid:96)geo

t

(w) = − log (1 − σ (logit(pt) · w)), and so

= σ(logit(pt) · w) logit(pt,i) = (geow(1 ; pt) − xt) logit(pt,i).

(20)

Hence ∇(cid:96)geo

t

(w) = (geow(1 ; pt) − xt) logit(pt).

Part (2): As |(geow(1 ; pt) − xt)| ≤ 1 it holds that (cid:107)∇(cid:96)geo

t

(w)(cid:107)2 ≤ (cid:107)logit(p)(cid:107)2 for all w ∈ C.

Part (3): the convexity of (cid:96)geo

t

has been established already by Mattern (2013).

Part (4a): We make use of Lemma 4.1 in Hazan (2016), which states that a twice diﬀeren-
tiable function f : Rm → R is α-exp-concave at x ∈ Rm if and only if there exists a scalar
α > 0 such that ∇2f (x) − α∇f (x)∇f (x)(cid:62) is positive semi-deﬁnite.

We can calculate the Hessian of the loss directly from Equations 19 and 20 by observing

that

and so

∂2(cid:96)geo
t
∂wj ∂wi

∇2(cid:96)geo
t

= logit(pt,i) logit(pt,j) σ(logit(pt) · w)[1 − σ(logit(pt) · w)],

(w) = σ(logit(pt) · w)[1 − σ(logit(pt) · w)] logit(pt) logit(pt)(cid:62)
= geow(1 ; pt)[1 − geow(1 ; pt)] logit(pt) logit(pt)(cid:62).

(21)

Furthermore, from Part (1), we have

∇(cid:96)geo
t

(w) ∇(cid:96)geo

t

(w)(cid:62) = (geow(1 ; pt) − xt)2 logit(pt) logit(pt)(cid:62).

Therefore, letting q = geow(1 ; pt), we have that

A : = ∇2(cid:96)geo

(w) − α∇(cid:96)geo

(w) ∇(cid:96)geo

t

t
= logit(pt) logit(pt)(cid:62)[q(1 − q) − α(q − xt)2]

t

(w)(cid:62)

Now if we could show k := q(1 − q) − α(q − xt)2 ≥ 0, then we would have that A is positive-
semideﬁnite as x(cid:62)Ax = k x(cid:62) logit(pt) logit(pt)(cid:62)x = k (x · logit(pt))2 ≥ 0 for any non-zero
real vector x. Considering the two cases xt = 0 and xt = 1 separately, for k ≥ 0 to hold, we

31

Veness, Lattimore, Bhoopchand, Grabska-Barwinska, Mattern, Toth

must have either α ≤ (1−q)/q and α ≤ q/(1−q); these conditions can be met independently
of xt by choosing α = ε0/(1 − ε0), where ε0 is the smallest possible value of geow(1 ; pt) for
any pt or w. Given the assumption that pt ∈ [ε, 1 − ε]d for some ε ∈ (0, 1/2), we conclude
that (cid:96)geo

(w) is α-exp-concave with

t

α = min
w∈W

σ(w · logit(ε)) = min
w∈W

σ

(cid:32) m
(cid:89)

i=1

log

(cid:18) ε

1 − ε

(cid:19)wi(cid:33)

(cid:32)

= σ

log

(cid:18) ε

1 − ε

(cid:19)maxw∈W (cid:107)w(cid:107)1

(cid:33)

.

Also, notice that Eq. (21) is clearly positive-semideﬁnite, conﬁrming Part (3).

Part (4b): Applying Part (2), and using the assumption that pt ∈ [ε, 1 − ε]m we have

(cid:107)∇(cid:96)(w)(cid:107)2 ≤ (cid:107)logit(p)(cid:107)2 =

logit2(pt,i) ≤

m log2

(cid:115)

(cid:19)

(cid:18) 1 − ε
ε

(cid:118)
(cid:117)
(cid:117)
(cid:116)

m
(cid:88)

i=1

√

=

m (log (1 − ε) − log (ε)) ≤

m log

√

(cid:19)

.

(cid:18) 1
ε

Appendix B. Proof of Theorem 1

The proof of Theorem 1 relies on several simple technical lemmas. Recall for p, q ∈ (0, 1)
that D(p, q) = p log(p/q) + (1 − p) log((1 − p)/(1 − q)) is the relative entropy (or Kullback-
Leibler divergence) between Bernoulli distributions with biases p and q respectively.

Lemma 7 Let p ∈ [0, 1] and gp(y) = D(p, σ(y)), then:

1. g(cid:48)

p(logit(q)) = q − p.

2. g(cid:48)(cid:48)

p (y) = exp(y)/(1 + exp(y))2 ∈ (0, 1/4].

3. gp(∆ + logit(q)) ≤ D(p, q) + ∆(q − p) + ∆2/8.

Proof The ﬁrst and second parts are trivial. The third follows from the fact that gp is
everywhere twice diﬀerentiable for all p ∈ [0, 1], which implies that

gp(x + ∆) ≤ gp(x) + ∆g(cid:48)

p(x) + max

g(cid:48)(cid:48)
p (y)

≤ gp(x) + ∆g(cid:48)

p(x) +

∆2
2

y

∆2
8

.

Then choose x = logit(q) and note that gp(logit(q)) = D(p, q).

Lemma 8 Given f, p, q ∈ (0, 1),

D(f, q) − D(f, p) = D(p, q) +

D(f, σ((1 − α) logit(p) + α logit(q)))

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)α=0

∂
∂α

32

Online Learning with Gated Linear Networks

Proof Note that

∂
∂α

(cid:12)
(cid:12)
(cid:12)
D(f, σ((1 − α) logit(p) + α logit(q)))
(cid:12)
(cid:12)α=0

= (f − p)(logit(q) − logit(p)) .

The proof follows from the deﬁnition of D(·, ·).

Proof [of Theorem 1]

Part (1) Recall that for all layers i (including i = 0) the output of the bias neuron in the
ith layer is p∗

i0(z) = β. Given non-bias neuron (i, k) and context a ∈ C let

Nika(n) = {t ≤ n : cik(zt) = a}

be the set of rounds when the context for neuron (i, k) is a. Now deﬁne

(cid:90)

w∗

ika = arg min
w∈Wik

ik(z) = σ(w∗
p∗

c−1
ik (a)
ika · logit(p∗

i−1(z))) .

D(f (z), σ(w · logit(p∗

i−1(z))))dµ(z)

Note that w∗
that p∗
almost surely for all non-bias neurons (i, k)

ika need not be unique, but the strict convexity of D(f (z), σ(·)) for all z ensures
ik does not depend on this choice. We prove by induction that the following holds

lim
n→∞

1
n

n
(cid:88)

t=1

sup
z∈Supp(µ)

(cid:12)
(cid:12)pik(z; w(t)) − p∗
(cid:12)

ik(z)

(cid:12)
(cid:12)
(cid:12) = 0 .

(22)

Let (i, k) be a non-bias neuron and c = cik be its context function and assume the
above holds for all preceding layers. Fix a ∈ C and abbreviate N (n) = Nika(n) and
qt = logit(pi−1(zt; w(t))) be the input to the ith layer in the tth round q∗(z) = logit(p∗
i−1(z))
and

(cid:96)t(w) = xt log

+ (1 − xt) log

(cid:18)

(cid:18)

(cid:19)

1
σ(w · qt))

1
σ(w · q∗(zt))

(cid:19)

(cid:18)

(cid:19)

.

1
1 − σ(w · qt)
(cid:18)
1
1 − σ(w · q∗(zt))

(cid:19)

.

(cid:96)∗
t (w) = xt log

+ (1 − xt) log

Let (cid:96)∗(w) = E[(cid:96)∗
is independent and identically distributed. Furthermore,

t (w)1N (n)(t)], which does not depend on t by the assumption that (zt, xt)

D(f (z), σ(w · q∗(z)))dµ(z) + C ,

(23)

where C is a constant depending only on µ and f and given by:

C = −

(f (z) log f (z) + (1 − f (z)) log(1 − f (z))) dµ(z) .

(cid:90)

(cid:96)∗(w) =

c−1(a)

(cid:90)

c−1(a)

33

Veness, Lattimore, Bhoopchand, Grabska-Barwinska, Mattern, Toth

Since Wik is bounded it follows that qt is bounded. Therefore (cid:96)t(u) is bounded and contin-
uous in u and qt, which by the induction hypothesis means that

lim
n→∞

1
n

(cid:88)

t∈N (n)

sup
u∈Wik

|(cid:96)t(u) − (cid:96)∗

t (u)| = 0 .

Since the weights are learned using an algorithm with sublinear regret it follows that

0 ≥ lim
n→∞

min
u∈Wik

1
n

(cid:88)

(cid:16)

(cid:96)t(w(t)) − (cid:96)t(u)

(cid:17)

= lim
n→∞

min
u∈Wik

1
n

(cid:88)

(cid:16)

t (w(t)) − (cid:96)∗
(cid:96)∗

(cid:17)
t (u)

.

t∈N (n)

t∈N (n)

For each u ∈ Wik deﬁne martingale

Mn(u) =

(cid:88)

(cid:16)

t (w(t)) − (cid:96)∗
(cid:96)∗

(cid:17)
t (u) − ((cid:96)∗(w(t)) − (cid:96)∗(u))

.

(24)

t∈N (n)

By the martingale law of large numbers P (limn→∞ Mn(u)/n = 0) = 1. Since (cid:96)∗
t (u) is
uniformly Lipschitz in u for all t, a union bound on a dense subset of Wik is enough to
ensure that with probability one it holds for all u ∈ Wik that

0 ≥ lim
n→∞

1
n

(cid:88)

t∈N (n)

t (w(t)) − (cid:96)∗
(cid:96)∗

t (u) = lim
n→∞

(cid:88)

(cid:16)

(cid:17)
(cid:96)∗(w(t)) − (cid:96)∗(u)

.

1
n

t∈N (n)

The conclusion follows from the deﬁnition of (cid:96)∗ in Eq. (23).

Part (2) We begin by noting that for any non-bias neuron (i, k) it holds that

Dik =

D(f (z), p∗

ik(z))dµ(z)

D(f (z), p∗

ik(z))dµ(z)

(cid:90)

Rd
(cid:88)

(cid:90)

=

=

a∈C
(cid:88)

a∈C

c−1
ik (a)
(cid:90)

min
w∈Wik
(cid:90)

Rd

≤ min
w∈Wik
(cid:90)

≤ min

j

Rd

D(f (z), σ(w · logit(p∗

i−1(z))))dµ(z)

c−1
ik (a)

D(f (z), σ(w · logit(p∗

i−1(z))))dµ(z)

D(f (z), p∗

i−1,j(z))dµ(z) = D∗

i−1 ,

where the ﬁrst and last equalities serve as deﬁnitions and in the last line we made use of
the assumption that ej ∈ Wik for each neuron (i − 1, j). Therefore the expected loss of any
neuron in the ith layer is smaller than that of the best neuron in the (i − 1)th layer. Since
i0(z) = β for all layers i and z ∈ Rd we have
p∗

(cid:90)

Rd

D(f (z), p∗

i0(z))dµ(z) =

D(f (z), β)dµ(z) ≤ D(0, β) = − log(β) .

(25)

(cid:90)

Rd

34

Online Learning with Gated Linear Networks

Next ﬁx a layer i and let (i, k) and (i − 1, j) be neurons maximising

(cid:32)(cid:90)

Bi =

(cid:88)

a∈C

c−1
ik (a)

(cid:0)f (z) − p∗

i−1,j(z)(cid:1) dµ(z)

∈ [0, 1] .

(cid:33)2

Let ∆a = (cid:82)
Lemma 7 we have

ik (a)(f (z) − p∗
c−1

i−1,j(z))dµ(z) and ˜∆a = sign(∆a) min{|∆a|, δ/4}. Then by

D∗

i ≤ Dik =

D(f (z), σ(w · p∗

i−1(z)))dµ(z)

(cid:90)

(cid:88)

a∈C

min
w∈Wik

c−1
ik (a)
(cid:16)

(cid:16)

D

f (z), σ

c−1
ik (a)

(cid:16)

c−1
ik (a)

4 ˜∆a + logit(p∗

i−1,j(z))

(cid:17)(cid:17)

dµ(z)

D(f (z), p∗

i−1,ki−1

(z)) + 4 ˜∆a(p∗

i−1,j(z) − f (z)) + 2 ˜∆2
a

dµ(z)

(cid:17)

≤

≤

≤

≤

≤

(cid:90)

(cid:88)

a∈C
(cid:88)

(cid:90)

a∈C
(cid:90)

Rd

(cid:90)

(cid:90)

Rd

Rd

D(f (z), p∗

i−1,j(z))dµ(z) − 4

∆a ˜∆a + 2

˜∆2
a

(cid:88)

a∈C

D(f (z), p∗

i−1,j(z))dµ(z) − 2

∆a ˜∆a

(cid:88)

a∈C
(cid:88)

a∈C

D(f (z), p∗

i−1,j(z))dµ(z) − 2 min

1,

∆2
a

= Di−1,j − min

2,

Bi = D∗

i−2 − min

2,

Bi .

(cid:26)

(cid:27)

δ
2

(cid:26)

(cid:27) (cid:88)

δ
4
(cid:26)

a∈C
(cid:27)
δ
2

Therefore by telescoping the sum over odd and even natural numbers and Eq. (25) we have

∞
(cid:88)

i=1

Bi ≤

2
min (cid:8)2, δ
2

(cid:9) log

(cid:19)

(cid:18) 1
β

= max

1,

log

(cid:26)

(cid:27)

4
δ

(cid:19)

(cid:18) 1
β

,

which implies the result.

Part (3) Let (i, k) and (i−1, j) be two non-bias neurons and abbreviate p(z) = p∗
and q(z) = p∗

i−1,j(z) ∈ RKi−1. Let

ik(z) ∈ R

ε =

(D(f (z), q(z)) − D(f (z), p(z))) dµ(z) .

(cid:90)

Rd

Then by Lemma 8 and Pinsker’s inequality

ε =

D(p(z), q(z)) +

∂
∂α

(cid:12)
(cid:12)
(cid:12)
D(f (z), σ((1 − α) logit(p(z)) + α logit(q(z))))
(cid:12)
(cid:12)α=0

(cid:33)

dµ(z)

≥

D(p(z), q(z))dµ(z) ≥ 2

(p(z) − q(z))2dµ(z) ,

(cid:32)

(cid:90)

(cid:90)

Rd

Rd

(cid:90)

Rd

35

Veness, Lattimore, Bhoopchand, Grabska-Barwinska, Mattern, Toth

where the ﬁrst inequality follows from the deﬁnition of p = p∗
ik, which ensures the derivative
is nonnegative and the second follows from Pinsker’s inequality. Therefore for all layers i
and non-bias neurons (i, k) and (i − 1, j) it holds that

(cid:90)

Rd

(cid:0)p∗

ik(z) − p∗

i−1,j(z)(cid:1)2 dµ(z) ≤

(cid:0)D(f (z), p∗

i−1,j(z)) − D(f (z), p∗

i,k(z))(cid:1) dµ(z)

By telescoping the sum over odd and even i there exists a p∗

∞ such that

lim
i→∞

max
k>1

Rd

(p∗

ik(z) − p∗

∞(z))2dµ(z) = 0 .

Since p∗
surability it follows that p∗

∞ is also F-measurable.

ik is F-measurable for all neurons (i, k) and pointwise convergence preserves mea-

(cid:90)

1
2
1
2

Rd
i−2 − D∗

(D∗

i ) .

≤

(cid:90)

Appendix C. Proof of Theorem 3

We use the same notation as the proof of Theorem 1 in the previous section. Only the
ﬁrst part is diﬀerent. Since the side information is no longer independent and identically
distributed we cannot claim anymore that E[(cid:96)∗
t (u)] = (cid:96)∗(u). The idea is to use the stability
of the learning procedure to partition the data into chunks of increasing size on which the
weights are slowly changing, but where the mixing of the Markov chain will eventually ensure
that the empirical distribution converges to stationary. Let (Si)i be a random contiguous
partition of N (∞) = {t ∈ N : cik(zt) = a}, which means that max Si < min Si+1 for
all i and (cid:83)∞
i=1 Si = N (∞). By the stability assumption we may choose (Si)i such that
(cid:13)∞ ≤ εi with limi→∞ εi = 0 for all possible data
limi→∞ |Si| = ∞ and maxs,t∈Si
sequences. Then abusing notation by letting w(i) = w(min Si) and letting nI = (cid:80)I
i=1 |Si| we
have the following holding almost surely:

(cid:13)w(s) − w(t)(cid:13)
(cid:13)

0 ≥ lim
n→∞

n
(cid:88)

(cid:16)

t=1

t (w(t)) − (cid:96)∗
(cid:96)∗

(cid:17)
t (u)

= lim
I→∞

1
nI

I
(cid:88)

(cid:88)

(cid:16)

i=1

t∈Si

t (w(t)) − (cid:96)∗
(cid:96)∗

t (u)

(cid:17)

1
n

1
nI

1
nI

1
nI

1
nI

= lim
I→∞

≥ lim
I→∞

= lim
I→∞

= lim
I→∞

I
(cid:88)

(cid:88)

(cid:16)

i=1

t∈Si

I
(cid:88)

(cid:88)

(cid:16)

i=1

t∈Si

I
(cid:88)

(cid:88)

(cid:16)

i=1

t∈Si

I
(cid:88)

(cid:88)

i=1

t∈Si

t (w(i)) − (cid:96)∗
(cid:96)∗

t (u) + (cid:96)∗

t (w(t)) − (cid:96)∗

t (w(i))

(cid:17)

t (w(i)) − (cid:96)∗
(cid:96)∗

t (u) − εi

(cid:17)

t (w(i)) − (cid:96)∗
(cid:96)∗

(cid:17)
t (u)

= lim
I→∞

(cid:16)

|Si|

(cid:96)∗(w(i)) − (cid:96)∗(u)

(cid:17)

(cid:16)

(cid:17)
(cid:96)∗(w(t)) − (cid:96)∗(u)

= lim
n→∞

(cid:96)∗(w(t)) − (cid:96)∗(u)

,

(cid:17)

1
nI

I
(cid:88)

i=1

1
n

n
(cid:88)

(cid:16)

t=1

36

Online Learning with Gated Linear Networks

where we used the convergence theorem for aperiodic φ-irreducible Markov chains (Meyn
and Tweedie, 2012, Chap. 10). The proof is concluded in the same way as the proof of
Theorem 1.

Appendix D. Norms and topological arguments

Recall that (Rd, F, µ) is a probability space with F the Lebesgue σ-algebra and for G a set
of F-measurable functions from Rd to C we deﬁne

(cid:107)h(cid:107)G = sup
c∈G

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

a∈C

c−1(a)

(cid:12)
(cid:12)
(cid:12)
h(x)dµ(x)
(cid:12)
(cid:12)

.

Of course (cid:107)·(cid:107)G is not a norm for all G. In the following we establish that it is a norm for a
few natural choices. First we note that (cid:107)·(cid:107)G is ‘almost’ a norm for all G as summarised in
the following trivial lemma.

Lemma 9 Let g, h : Rd → R be µ-integrable and G ⊂ C → Rd. Then

1. (cid:107)g + h(cid:107)G ≤ (cid:107)g(cid:107)G + (cid:107)h(cid:107)G

2. (cid:107)ag(cid:107)G = a (cid:107)g(cid:107)G

3. (cid:107)0(cid:107)G = 0.

Thus in order to show that (cid:107)·(cid:107)G is a norm it is only necessary to prove that (cid:107)h(cid:107)G = 0
implies that h = 0 µ-almost-everywhere. We are especially interested in countable G,
since we can construct (inﬁnite) networks that eventually use all contexts in such sets.
The following lemma allows us to connect Theorem 1 to a claim about the asymptotic
universality of a network.

Lemma 10 If G = {c1, c2, . . .} is countable and (cid:107)·(cid:107)G is a norm on the space of µ-integrable
h, then for Gi = {c1, c2, . . . , ci} it holds that limi→∞ (cid:107)h(cid:107)Gi

= (cid:107)h(cid:107)G for all h.

For the remainder we assume that µ is absolutely continuous with respect to the

Lebesgue measure.

Lemma 11 Let G = {1Br(x) : x ∈ Qd, r ∈ (0, ∞) ∩ Q} and suppose that h is bounded and
measurable with (cid:107)h(cid:107)G = 0. Then h = 0 µ-almost-everywhere.

Proof Assume without loss of generality that supx∈Rd |h(x)| ≤ 1. Suppose that |h| does not
vanish µ-almost everywhere. Then there exists an ε > 0 and δ > 0 such that µ(|h| > ε) > δ.
Let A = {x : |h(x)| > ε}, which is measurable because |h| is measurable. Therefore by
outer-regularity there exists an open U ⊆ Rd with A ⊂ U and µ(U ) ≤ µ(A) + εδ/2. There-
fore (cid:82)
A h(x)dµ(x) + (cid:82)
U h(x)dµ(x) = (cid:82)
i Bi where
(Bi) are disjoint open balls with rational-valued centers and radii. Then by assumption
(cid:82)
(cid:82)
U h(x)dµ(x) = (cid:80)∞
h(x)dµ(x) = 0, which is a contradiction.

U −A h(x)dµ(x) ≥ εδ/2. Next write U = (cid:83)

i=1

Bi

37

Veness, Lattimore, Bhoopchand, Grabska-Barwinska, Mattern, Toth

Lemma 12 Let B be a countable base for Rd and G = {1U : U ∈ B} and (cid:107)h(cid:107)G = 0 for
some bounded and F-measurable h. Then h = 0 µ-almost-everywhere.

Proof As above.

Lemma 13 Let G = {1Hν,c : ν ∈ Qd, (cid:107)ν(cid:107) = 1, c ∈ Q}. If h : Rd → R is measurable and
(cid:107)h(cid:107)G = 0, then h = 0 almost-everywhere.

Proof Since Q is dense in R it follows from absolutely continuity of µ that if ν ∈ Rd and
c ∈ R and (cid:107)h(cid:107)G = 0, then (cid:82)
H hdµ = 0 for all half-spaces H. Therefore if λ is the Lebesgue
measure, then

ˆh(∂H) =

(cid:90)

h

dµ
dλ

∂H

dλ = 0

for all hyperplanes ∂H. But ˆh is the Radon transform of hdµ/dλ, which is zero everywhere
only if hdµ/dλ is zero almost everywhere (Helgason, 2011). Therefore h dµ
dλ ≡ 0 and so h ≡ 0
almost everywhere.

Appendix E. Capacity for Half-Space Contexts with Two Layers

We now argue that isolation networks can approximate continuous functions to arbitrary
precision with just two layers where the ﬁrst layer is suﬃciently wide and consists of half-
space contexts and the second layer (output) has just one neuron and no context. For
dimension one we give an explicit construction, while for higher dimensions we sketch the
argument via the Radon transform inversion formula. The interestingness of these results is
tempered by the fact that the precision of the approximation can only be made arbitrarily
small by having an arbitrarily wide network and that the weights must also be permitted to
be arbitrarily large. In practice we did not ﬁnd two-layer half-space contexts to be eﬀective
beyond the one-dimensional case.

Theorem 14 Let f : R → [0, 1] be continuous and µ have compact support and be absolutely
continuous with respect to the Lebesgue measure. Then for any ε > 0 there exists a two-
layer network with half-space contexts in the ﬁrst layer and a single non-bias neuron in the
second layer with a trivial context function such that (cid:107)p∗
21(z) is given
in Theorem 1.

21 − f (cid:107)∞ < ε, where p∗

Proof By applying a molliﬁer, any continuous f may be approximated to arbitrary preci-
sion by a diﬀerentiable function with range a subset of (ε, 1 − ε) for small ε > 0. From now
on we simply assume that f is diﬀerentiable. For b ∈ (0, 1) let

(cid:96)(b) = logit

(cid:18)

1
µ([b, 1])

(cid:90) 1

b

(cid:19)

f (z)dµ(z)

¯(cid:96)(b) = logit

(cid:18)

1
µ([0, b])

(cid:90) b

0

(cid:19)

f (z)dµ(z)

38

Online Learning with Gated Linear Networks

and (cid:96) = (cid:82) 1
0 f (z)dµ(z). Because f (z) ∈ (ε, 1−ε) it follows that (cid:96)(b) and ¯(cid:96)(b) are also bounded.
By the ﬁrst part of Theorem 1, if neuron (1, k) uses context function c1k(z) = 1z≥bk (z),
then

logit(p∗

1k(z)) =

(cid:40)

(cid:96)(bk)
¯(cid:96)(bk)

if z ≥ bk
otherwise .

Since neuron (2, 1) has a trivial context function there exists an a ∈ C such that c21(z) = a
for all z ∈ R. Abbreviating wk = w21ak we can write the output of neuron (2, 1) as

p∗
21(z) = σ

wk logit(p∗

1k(z))

(cid:33)

= σ

w0 +

wk(cid:96)(bk) +

(cid:32) K1(cid:88)

k=0





(cid:88)

k:bk≤z

(cid:88)

k:bk≤z



wk

¯(cid:96)(bk)



(cid:88)

k:bk>z



(cid:88)

k

= σ

w0 +

wk((cid:96)(bk) − ¯(cid:96)(bk)) +

wk

¯(cid:96)(bk)

 .

(26)

The main idea is to write an approximation of f as an integral that in turn may be approx-
imated by the sum above for suﬃciently large K1 and well chosen wk. Deﬁne

v(x) =

f (cid:48)(x)
f (x)(1 − f (x))

(cid:18)

1
(cid:96)(x) − ¯(cid:96)(x)

(cid:19)

.

By diﬀerentiating the logit function and the fundamental theorem of calculus and the as-
sumption that f is diﬀerentiable and f (z) ∈ (ε, 1 − ε) we have

f (z) = σ

logit(f (0)) +

(cid:18)

(cid:18)

= σ

logit(f (0)) +

v(x)((cid:96)(x) − ¯(cid:96)(x))dx

.

f (cid:48)(x)
f (x)(1 − f (x))

(cid:19)

dx

(cid:19)

(cid:90) z

0
(cid:90) z

0

The result follows by approximating the integral above with the sum in Eq. (26).

The situation for higher dimensions is more complicated, but the same theorem may be
proven for d > 1 by appealing to the Radon transform inversion formula (Helgason, 2011,
Chap 1), which says that any inﬁnitely diﬀerentiable and compactly supported function
g : Rd → R may be represented by

where S = {x ∈ Rd : (cid:107)x(cid:107)2 = 1} is the sphere and the integral is with respect to the uniform
measure and ˜g : S × R → R is carefully chosen. The precise form of ˜g is quite complicated,
but only depends on the Radon transform of g, which is the operator (Rg) : S × R → R

g(z) =

˜g(η, (cid:104)z, η(cid:105))dη ,

(cid:90)

S

39

Veness, Lattimore, Bhoopchand, Grabska-Barwinska, Mattern, Toth

∂Hη,x

given by (Rg)(η, x) = (cid:82)
f (z)dz. This result means that suﬃciently regular functions are
entirely determined by their integrals on hyperplanes. It remains to note that for wide two-
layer networks with arbitrarily large weights and half-space contexts, the output of neuron
(2, 1) can approximate the above integral to arbitrary precision. We leave the details for
the interested reader.

Appendix F. Proof of Equation 18

This result is the same as that of Lemma 1 in (Veness et al., 2012). We provide the proof
here for the sake of completeness.

Lemma 15 If s(ν1:n) = (cid:80)n

k=2

1[νk (cid:54)= νk−1], then for all ν1:n ∈ In(M), we have

− log wτ (ν1:n) ≤ (s(ν1:n) + 1) (log |M| + log2 n) .

Proof Consider an arbitrary ν1:n ∈ In(M). Letting m = s(ν1:n), from Equation 17 we
have that

− log wτ (ν1:n) = log |M| − log

1[νt = νt−1] +

1
t(|M|−1)

(cid:17)
1[νt (cid:54)= νt−1]

(cid:16) t−1
t

(cid:16) t−1
t

n
(cid:89)

t=2
n
(cid:89)

t=2
(cid:32)

≤ log |M| − log

1[νt = νt−1] +

1
n(|M|−1)

(cid:17)
1[νt (cid:54)= νt−1]

≤ log |M| − log

n−m(|M| − 1)−m

(cid:33)

t−1
t

n−m
(cid:89)

t=2

= log |M| + m log n + m log(|M| − 1) + log(n − m)

≤ (m + 1)[log |M| + log n].

40

