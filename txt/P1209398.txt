7
1
0
2
 
l
u
J
 
5
2
 
 
]

V
C
.
s
c
[
 
 
3
v
6
4
7
9
0
.
3
0
7
1
:
v
i
X
r
a

Coordinating Filters for Faster Deep Neural Networks

Wei Wen
University of Pittsburgh
wew57@pitt.edu

Cong Xu
Hewlett Packard Labs
cong.xu@hpe.com

Chunpeng Wu
University of Pittsburgh
chw127@pitt.edu

Yandan Wang
University of Pittsburgh
yaw46@pitt.edu

Yiran Chen
Duke University
yiran.chen@duke.edu

Hai Li
Duke University
hai.li@duke.edu

Abstract

Very large-scale Deep Neural Networks (DNNs) have
achieved remarkable successes in a large variety of com-
puter vision tasks. However, the high computation intensity
of DNNs makes it challenging to deploy these models on
resource-limited systems. Some studies used low-rank ap-
proaches that approximate the ﬁlters by low-rank basis to
accelerate the testing. Those works directly decomposed
the pre-trained DNNs by Low-Rank Approximations (LRA).
How to train DNNs toward lower-rank space for more ef-
ﬁcient DNNs, however, remains as an open area. To solve
the issue, in this work, we propose Force Regularization,
which uses attractive forces to enforce ﬁlters so as to coor-
dinate more weight information into lower-rank space1. We
mathematically and empirically verify that after applying
our technique, standard LRA methods can reconstruct ﬁlters
using much lower basis and thus result in faster DNNs. The
effectiveness of our approach is comprehensively evaluated
in ResNets, AlexNet, and GoogLeNet. In AlexNet, for ex-
ample, Force Regularization gains 2× speedup on modern
GPU without accuracy loss and 4.05× speedup on CPU by
paying small accuracy degradation. Moreover, Force Reg-
ularization better initializes the low-rank DNNs such that
the ﬁne-tuning can converge faster toward higher accuracy.
The obtained lower-rank DNNs can be further sparsiﬁed,
proving that Force Regularization can be integrated with
state-of-the-art sparsity-based acceleration methods.

1. Introduction

Deep Neural Networks (DNNs) have achieved record-
breaking accuracy in many image classiﬁcation tasks [16]
[24][25][10]. With the advances of algorithms, availabil-
ity of database, and improvement in hardware performance,

1The

source

code

is

available

in https://github.com/

wenwei202/caffe

Figure 1. The low-rank basis of ﬁlters in the ﬁrst layer of the con-
volutional neural network [16] on CIFAR-10. The low-rank basis
is formed by the most signiﬁcant principal ﬁlters that are obtained
by PCA. Top: the low-rank basis of the original network. Bottom:
the low-rank basis of the same network after applying Force Reg-
ularization. The number of red boxes indicates the required rank
to reconstruct the original ﬁlters with ≤ 20% error.

the depth of DNNs grows dramatically from a few to hun-
dreds or even thousands of layers, enabling human-level
performance [9]. However, deploying these large models on
resource-limited platforms, e.g., mobiles and autonomous
cars, is very challenging due to the high demand in the com-
putation resource and hence energy consumption.

Recently, many techniques to accelerate the testing pro-
cess of deployed DNNs have been studied, such as weight
sparsifying or connection pruning [8][7][28][23][22][6]
[19]. These approaches require delicate hardware cus-
tomization and/or software design to transfer sparsity into
practical speedup. Unlike sparsity-based methods, Low-
Rank Approximation (LRA) methods [22][4][5][12][11]
[26][27][18][30][14] directly decompose an original large
model to a compact model with more lightweight layers.
Thanks to the redundancy (correlation) among ﬁlters in
DNNs, original weight tensors can be approximated by very
low-rank basis. From the viewpoint of matrix computation,
LRA approximates a large weight matrix by the product of
two or more small ones to reduce computation complexity.
Previous LRA methods mostly focus on how to decom-
pose the pre-trained weight tensors for maximizing the re-
duction of computation complexity, meanwhile retaining
the classiﬁcation accuracy. Instead, we propose to nudge
the weights by additional gradients (attractive forces) to co-
ordinate the ﬁlters to a more correlated state. Our approach

aims to improve the correlation among ﬁlters and therefore
obtain more lightweight DNNs through LRA. To the best of
our knowledge, this is the ﬁrst work to train DNNs toward
lower-rank space such that LRA can achieve faster DNNs.
The motivation of this work is fundamental. It has been
proven that trained ﬁlters are highly clustered and corre-
lated [5][4][12]. Suppose each ﬁlter is reshaped as a vector.
A cluster of highly-correlated vectors then will have small
included angles. If we are able to coordinate these vectors
toward a state with smaller included angles, the correlation
of the ﬁlters within that cluster improves. Consequently,
LRA can produce a DNN with lower ranks and higher com-
putation efﬁciency.

We propose a Force Regularization to coordinate ﬁl-
ters in DNNs. As demonstrated in Fig. 1, when using
the same LRA method, say, cross-ﬁlter Principal Compo-
nent Analysis (PCA) [30], applying Force Regularization
can greatly reduce the required ranks from the original de-
sign (i.e., 5 vs. 11), while keeping the same approximation
errors (≤ 20%). As we shall show in Section 5, apply-
ing Force Regularization in the training of state-of-the-art
DNNs will successfully obtain lower-rank DNNs and thus
improve computation efﬁciency, e.g., 4.05× speedup for
AlexNet with small accuracy loss.

The contributions of our work include:

(1) We pro-
pose an effective and easy-to-implement Force Regulariza-
tion to train DNNs for lower-rank approximation. To the
best of our knowledge, this is the ﬁrst work to manipulate
the correlation among ﬁlters during training such that LRA
can achieve faster DNNs; (2) DNNs manipulated by Force
Regularization can have better initialization for the retrain-
ing of LRA-decomposed DNNs, resulting in faster conver-
gence to better accuracy; (3) Those lightweight DNNs that
have been aggressively compressed by our method can be
further sparsiﬁed. That is, our method can be integrated
with state-of-the-art sparsity-based methods to potentially
achieve faster computation; (4) Force Regularization can
be easily generalized to Discrimination Regularization that
can learn more discriminative ﬁlters to improve classiﬁca-
tion accuracy; (5) Our implementation is open-source on
both CPUs and GPUs.

2. Related work

Low-rank approximation. LRA method decomposes a
large model to a compact one with more lightweight lay-
ers by weight/tensor factorization. Denil et al. [4] studied
different dictionaries to remove the redundancy between ﬁl-
ters and channels in DNNs. Jaderberg et al. [12] explored
ﬁlter and data reconstruction optimizations to attain opti-
mal separable basis. Denton et al. [5] clustered ﬁlters, ex-
tended LRA (e.g., Singular Value Decomposition, SVD ) to
larger-scale DNNs, and achieved 2× speedup for the ﬁrst
two layers with 1% accuracy loss. Many new decomposi-

tion methods were proposed [11][26][18][30] and the ef-
fectiveness of LRA in state-of-the-art DNNs were evalu-
ated [24][25]. Similar evaluations on mobile devices were
also reported [14][27]. Unlike them, we propose Force Reg-
ularization to coordinate DNN ﬁlters to more correlated
states, in which lower-rank or more compact DNNs are
achievable for faster computation.

Sparse deep neural networks.

The studies on
sparse DNNs can be categorized into two types: non-
structured [20][23][22][8][6] and structured [28][21][19][1]
sparsity methods. The ﬁrst category prunes each connec-
tion independently. Consequently, sparse weights are ran-
domly distributed. The level of non-structured sparsity is
usually insufﬁcient to achieve good practical speedup in
modern hardware [28][19]. Software optimization [23][22]
and hardware customization [7] are proposed to overcome
this issue. Conversely, the structured approaches prune con-
nections group by group, such that the sparsiﬁed DNNs
have regular distribution of sparse weights. The regular-
ity is friendly to modern hardware for acceleration. Our
work is orthogonal to sparsity-based methods. More impor-
tantly, we ﬁnd that DNNs accelerated by our method can
be further sparsiﬁed by both non-structured and structured
sparsity methods, potentially achieving faster computation.

3. Correlated Filters and Their Approximation

The prior knowledge is that correlation exists among
trained ﬁlters in DNNs and those ﬁlters lie in a low-rank
space. For example, the color-agnostic ﬁlters [16] learned
in the ﬁrst layer of AlexNet lie in a hyper-plane, where RGB
channels at each pixel have the same value. Fig. 2 presents
the results of Linear Discriminant Analysis (LDA) of the
ﬁrst convolutional ﬁlters in AlexNet and GoogLeNet. The
ﬁlters are normalized to unit vectors and colored to four
clusters by k-means clustering, and then projected to 2D
space by LDA to maximize cluster separation. The ﬁgure
indicates high correlation among ﬁlters within a cluster. A
na¨ıve approach of ﬁlter approximation is to use the centroid
of a cluster to approximate ﬁlters within that cluster, thus,
the number of clusters is the rank of the space. Essentially,
k-means clustering is a LRA [2] method, although we will

Figure 2. Linear Discriminant Analysis (LDA) of ﬁlters in the ﬁrst
convolutional layer of AlexNet (left) and GoogLeNet (right).

Figure 3. Cross-ﬁlter LRA of a convolutional layer.

Figure 4. Force Regularization to coordinate ﬁlters.

later show that other LRA methods can give better approxi-
mation. The motivation of this work is that if we are able to
nudge ﬁlters during the training such that the ﬁlters within
a cluster are coordinated closer and some adjacent clusters
are even merged into one cluster, then more accurate ﬁlter
approximation using lower rank can be achieved. We pro-
pose Force Regularization to realize it.

Before introducing Force Regularization, we ﬁrst mathe-
matically formulate LRA of DNN ﬁlters. Theoretically, al-
most all LRA methods can gain lower-rank approximation
upon our method because ﬁlters are coordinated to more
correlated state. Instead of onerously replicating all of these
LRA methods, we choose cross-ﬁlter approximation [4][30]
and a state-of-the-art work in [26] as our baselines.

Fig. 3 illustrates the cross-ﬁlter approximation of a con-
volutional layer. We assume all weights in a convolutional
layer is a tensor W ∈ RN ×C×H×W , where N and C are
the numbers of ﬁlters and input channels, and H and W
are the spatial height and width of the ﬁlters, respectively.
With input feature map I, the n-th output feature map
On = Wn ∗ I, where Wn ∈ R1×C×H×W is the n-th ﬁlter.
Because of the redundancy (or correlation) across the ﬁl-
ters [4], tensor Wn(∀n ∈ [1...N ]) can be approximated by
a linear combination of the basis Bm ∈ R1×C×H×W (m ∈
[1...M ], M (cid:28) N ) of a low-rank space B ∈ RM ×C×H×W ,
such as

On ≈

b(n)
m Bm

∗ I =

(cid:33)

(cid:32) M
(cid:88)

m=1

M
(cid:88)

(cid:16)

b(n)
m Fm

(cid:17)

.

(1)

m=1

Where b(n)
m is a scalar, and Fm = Bm ∗ I is the feature
map generated by basis ﬁlter Bm. Therefore, the output fea-
ture map On is a linear combination of Fm(m ∈ [1...M ])
which can be interpreted as the feature map basis. Since the
linear combination essentially is a 1 × 1 convolution, the
convolutional layer can be decomposed to two sequential
lightweight convolutional layers as shown in Fig. 3. The
original computation complexity is O(N CHW H
),
where H
is the height and width of output fea-
ture maps, respectively. After applying cross-ﬁlter LRA, the
computation complexity is reduced to O(M CHW H
+
). The computation complexity decreases when
N M H

and W

W

W

W

(cid:48)

(cid:48)

(cid:48)

(cid:48)

(cid:48)

(cid:48)

(cid:48)

(cid:48)

the rank M < N CHW

CHW +N .

4. Force Regularization

4.1. Regularization by Attractive Forces

This section proposes Force Regularization from the per-
spective of physics.
It is a gradient-based approach that
adds extra gradients to data loss gradients. The data loss
gradients aim to minimize classiﬁcation error as traditional
DNNs do. The extra gradients introduced by Force Regular-
ization gently adjust the lengths and directions of data loss
gradients so as to nudge ﬁlters to a more correlated state.
With a good setup of hyper-parameter, our method can co-
ordinate more useful information of ﬁlters to a lower-rank
space meanwhile maintain accuracy. Inspired by Newton’s
Laws, we propose an intuitive, computation-efﬁcient and
effective Force Regularization that uses attractive forces to
coordinate ﬁlters.

Force Regularization: As illustrated in Fig. 4, suppose
the ﬁlter Wn ∈ W is reshaped as a vector Wn ∈ R1×CHW
and normalized as wn ∈ R1×CHW (∀n ∈ [1...N ]), with
their origin at O. We introduce the pair-wise attractive force
fji = f (wj − wi) (∀i, j ∈ [1...N ]) on wi generated by wj.
The gradient of Force Regularization to update ﬁlter Wi is
deﬁned as

∆Wi =

∆Wij = ||Wi||

(cid:0)fji − fjiwT

i wi

(cid:1) ,

(2)

N
(cid:88)

j=1

N
(cid:88)

j=1

·

where ||
|| is the Euclidean norm. The regularization
gradient in Eq. (2) is perpendicular to ﬁlter vector and can
be efﬁciently computed by addition and multiplication. The
ﬁnal updating of weights by gradient descent is

Wi ← Wi − η ·

− λs · ∆Wi

,

(3)

(cid:18) ∂E(W)
∂Wi

(cid:19)

where E(W) is data loss, η is learning rate and λs > 0 is
the coefﬁcient of Force Regularization to trade off the rank
and accuracy. We select λs by cross-validation in this work.
The gradient of common weight-wise regularization (e.g.,
(cid:96)2-norm) is omitted in Eq. (3) for simplicity.

Fig. 4 intuitively explains our method. Suppose each
vector wi is a rigid stick and there is a particle ﬁxed at
the endpoint. The particle has unit mass, and the stick is
massless and can freely spin around the origin. Given the
pair-wise attractive forces (e.g., universal gravitation) fji,
Eq. (2) is the acceleration of particle i. As the forces are at-
tractive, neighbor particles tend to spin around the origin to
assemble together. Although our regularizer seems to col-
lapse all particles to one point which is the rank-one space
for most lightweight DNNs, there exist gradients of data
loss to avoid this. More speciﬁc, pre-trained ﬁlters orient
to discriminative directions wn (n ∈ [1...N ]). In each di-
rection wn, there are some correlated ﬁlters as observed in
Fig. 2. During the subsequent retraining with our regular-
izer, regularization gradients coordinate a cluster of ﬁlters
closer to a typical direction dm (m ∈ [1...M ], M (cid:28) N ),
but data loss gradients avoid collapsing dm together so as to
maintain the ﬁlters’ capability of extracting discriminative
features. If all ﬁlters could be extremely collapsed toward
one point meanwhile maintain classiﬁcation accuracy, it im-
plies the ﬁlters are over-redundant and we can attain a very
efﬁcient DNN by decomposing it to a rank-one space.

We derive the Force Regularization gradient from the
normalized ﬁlters based on the following facts: (1) A nor-
malized ﬁlter is on the unit hypersphere, and its orientation
is the only free parameter we need to optimize; (2) The gra-
dient of Wi can be easily scaled by the vector length ||Wi||
without changing the angular velocity.

In Eq. (2), fji = f (wj − wi) is the force function related

to distance. We study (cid:96)2-norm Force

f(cid:96)2(wj − wi) = wj − wi

and (cid:96)1-norm Force

f(cid:96)1 (wj − wi) =

wj − wi
||wj − wi||

(4)

(5)

in this work. We deﬁne the force of Eq. (4) as (cid:96)2-norm
Force because the strength linearly decreases with the dis-
tance ||wj − wi||, just as the gradient of regularization (cid:96)2-
norm does. We name the force of Eq. (5) as (cid:96)1-norm Force
because the gradient is a constant unit vector regardless of
the distance, just as the gradient of sparsity regularization
(cid:96)1-norm is.

Table 1. Ranks vs. scalers of step sizes of regularization gradients.

Scaler

Error

conv1*

conv2

conv3

0 (baseline)
||Wi||
1/||Wi||
* The ﬁrst convolutional layer.

18.0% 17/32
17.9% 15/32
18.0% 16/32

27/32
22/32
27/32

55/64
30/64
32/64

[1...N ]). For each ﬁlter, Force Regularization under (cid:96)2-
norm force has the same gradient direction of regulariza-
tion R(W), but differs by adapting the step size to the ﬁler’s
length, where

R(W) =

1
2

N
(cid:88)

N
(cid:88)

j=1

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Wj
||Wj||

−

Wi
||Wi||

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(6)

Proof : Because wj = Wj

||Wj || ,

∂R(W)
∂Wi

=

=

1
2

1
2

= −

N
(cid:88)

j=1

N
(cid:88)

j=1

N
(cid:88)

j=1

∂ (wj − wi) (wj − wi)T
∂Wi
∂ (cid:0)1 − 2wjwT

i + 1(cid:1)

∂Wi

(cid:1)

∂ (cid:0)wjwT
∂Wi

i

= −

wj

N
(cid:88)

j=1

∂wT
i
∂Wi

,

where ∂wT
i
∂Wi

:= Gi is a derivative matrix with element

G(pq)

i =

∂ W (p)
i
||Wi||
∂W (q)
i

=

(cid:32)

∂w(p)
i
∂W (q)
i

1
||Wi||

=

δ(p, q) −

i W (q)
W (p)
i
||Wi||2

(cid:33)

.

Superscripts p, q ∈ [1 . . . CHW ] index the elements in

vectors wi and Wi. δ(p, q) is the unit impulse function:
(cid:40)

δ(p, q) =

1 p = q
0 p (cid:54)= q

.

Therefore,

Gi =

1
||Wi||

(cid:0)I − wT

i wi

(cid:1) .

4.2. Mathematical Implications

Replacing Eq. (10) to Eq. (7), we have

This section explains the mathematical implications be-
hind: Force Regularization is related to but different from
minimizing the sum of pair-wise distances between normal-
ized ﬁlters.

Theorem 1 Suppose ﬁlter Wn ∈ W is reshaped as a vector
Wn ∈ R1×CHW and normalized as wn ∈ R1×CHW (∀n ∈

−

∂R(W)
∂Wi

=

1
||Wi||

(cid:0)(wj − wi) − (wj − wi) wT

(cid:1)

i wi

=

1
||Wi||





fji

 −






 wT

fji

i wi

 ,

N
(cid:88)

j=1

N
(cid:88)

j=1

N
(cid:88)

j=1







(7)

(8)

(9)

(10)

(11)

where fji = f(cid:96)2(wj − wi) = wj − wi. Therefore, Eq. (11)
and Eq. (2) have the same direction.

Theorem 1 states that our proposed Force Regularization
in Eq. (2) is related to Eq. (11). However, the step size of
the gradient in Eq. (2) is scaled by the length ||Wi|| of the
ﬁlter instead of its reciprocal in Eq. (11). This ensures that
the ﬁlter spins the same angle regardless of its length and
avoids the issue of being divided by zero. Table 1 summa-
rizes the ranks vs. step sizes for the ConvNet [16], which is
trained by CIFAR-10 database without data augmentation.
The original ConvNet has 32, 32, and 64 ﬁlters in each con-
volutional layer, respectively. The rank is the smallest num-
ber of basis ﬁlters (in Fig. 3) obtained by PCA with ≤ 5%
reconstruction error. Therefore, ||Wi|| works better than its
reciprocal when coordinating ﬁlters to a lower-rank space.
Following the same proof procedure, we can easily ﬁnd
that Force Regularization under (cid:96)1-norm Force has the same
conclusion when

R(W) =

N
(cid:88)

N
(cid:88)

j=1

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Wj
||Wj||

−

Wi
||Wi||

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(12)

5. Experiments

5.1. Implementation

Our experiments are performed in Caffe [13] using
CIFAR-10 [15] and ILSVRC-2012 ImageNet [3]. Pub-
lished models are adopted as the baselines: In CIFAR-10,
we choose ConvNet without data augmentation [16] and
ResNets-20 with data augmentation [10]. We adopt the
same shortcut connections in [28] for ResNets-20. For Im-
ageNet, we use AlexNet and GoogLeNet models trained by
Caffe, and report accuracy using only center crop of images.
Our experiments of Force Regularization show that, with
the same maximum iterations, the training from the baseline
can achieve a better tradeoff between accuracy and speedup
comparing with the training from scratch, because the base-
line offers a good initial point for both accuracy and ﬁlter
correlation. During the training with Force Regularization
on CIFAR-10, we use the same base learning rate as the
baseline; while in ImageNet, 0.1× base learning rate of the
baseline is adopted.

Figure 5. The rank M in each convolutional layer of ResNets-20
and GoogLeNet. Red bar overlaps blue bar. The accuracy loss is
0.75% for ResNets-20 and 2.46% (top-5) for GoogLeNet.

5.2. Rank Analysis of Coordinated DNNs

In light of various low-rank approximation methods,
without losing the generalization, we ﬁrst adopt Principal
Component Analysis (PCA) [30][22] to evaluate the effec-
tiveness of Force Regularization. Speciﬁcally, the ﬁlter
tensor W can be reshaped to a matrix W ∈ RN ×CHW ,
the rows of which are the reshaped ﬁlters Wn (∀n ∈
[1...N ]). PCA minimizes the least square reconstruction
error when projecting a column (RN ) of W to a low-
rank space RM (M (cid:28) N ). The reconstruction error is
eM = (cid:80)N
i=M +1 λi, where λi is the i-th largest eigenvalue
of covariance matrix WWT
CHW −1 . Under the constraint of error
(e.g., eM
percentage eM
≤ 5%), lower-rank approximation
e0
e0
can be obtained if the minimal rank M can be smaller. In
this section, without explicit explanation, we deﬁne rank M
of a convolutional layer as the minimal M which has ≤ 5%
reconstruction error by PCA.

Table 2 summarizes the rank M in each layer of Con-
vNet and AlexNet without accuracy loss after Force Regu-
larization. In the baselines, the learned ﬁlters in the front
layers are intrinsically in a very low-rank space but the rank
M in deeper layers is high. This could explain why only
speedups of the ﬁrst two convolutional layers were reported
in [5]. Fortunately, by using either (cid:96)2-norm or (cid:96)1-norm
force, our method can efﬁciently maintain the low rank M
in the ﬁrst two layers (e.g., conv1-conv2 in AlexNet), mean-
while signiﬁcantly reduce the rank M of deeper layers (e.g.,
conv3-conv5 in AlexNet). On average, our method can re-
duce the layer-wise rank ratio by ∼ 50%. The effective-
ness of our method on deep layers is very important as the

Net

Table 2. The rank M in each convolutional layer after Force Regularization.
conv2

Top-1 error

conv5

conv3

conv4

Force

Average rank ratio ‡

ConvNet None (baseline)†
ConvNet
ConvNet

(cid:96)2-norm
(cid:96)1-norm

18.0%
17.9%
18.0%

27/32
22/32
25/32

55/64
30/64
20/64

–
–
–

–
–
–

74.48%
54.17%
54.17%

AlexNet
72.29%
AlexNet
46.98%
AlexNet
50.03%
†The baseline without Force Regularization. ‡M /N : Low rank M over full rank N , which is deﬁned as rank ratio.

None (baseline)
(cid:96)2-norm
(cid:96)1-norm

164/256
143/256
155/256

306/384
128/384
157/384

318/384
122/384
108/384

220/256
161/256
178/256

42.63%
42.70%
42.45%

47/96
49/96
49/96

conv1
17/32‡
15/32
17/32

Figure 6. Th rank ratio (having ≤ 5% PCA reconstruction error)
in each layer vs. top-1 error for AlexNet. Horizontal dotted lines
represent the rank ratios of the baseline, and vertical dotted line
is the error of baseline. Solid (dashed) curves depict rank ratios
of the AlexNet after Force Regularization by (cid:96)2-norm ((cid:96)1-norm)
force. Each layer is denoted by a typical color. The sensitivity of
hyper-parameter λs: along the direction from left to right, λs of
(cid:96)2-norm force changes from 1.2e-5, to 1.8e-5, 2.0e-5, 3.0e-5, and
3.5e-5; and for (cid:96)1-norm force, it changes from 1.5e-5, to 1.8e-5,
2.0e-5, and 2.5e-5.

depth of modern DNNs grows dramatically [25][10]. Fig. 5
shows the rank M of ResNets-20 [10] and GoogLeNet [25]
after Force Regularization, representing the scalability of
our method on deeper DNNs. With an acceptable accuracy
loss, 5 layers in ResNets-20 and 6 layers in GoogLeNet are
even coordinated to rank M = 1, which indicates those In-
ception blocks in GoogLeNet or Residual blocks in ResNets
have been over-parameterized and can be greatly simpliﬁed.
To study the trade-off between rank, accuracy, and the
pros and cons of (cid:96)2-norm and (cid:96)1-norm force, we conducted
comprehensive experiments on AlexNet. As shown in Fig. 6,
with mere 1.71% (1.80%) accuracy loss, the average rank
ratio can be reduced to 28.59% (28.72%) using (cid:96)2-norm ((cid:96)1-
norm) force. Very impressively, the rank M of each group
in conv4 can be reduced to one by (cid:96)1-norm force. The re-
sults also show that (cid:96)2-norm force is more effective than
(cid:96)1-norm force when the rank ratio is high (e.g., conv2 and
conv5), while (cid:96)1-norm force works better for layers whose
potential rank ratios are low (e.g., conv3 and conv4).
In
general, (cid:96)2-norm force can better balance the ranks across
all the layers.

Because Force Regularization coordinates more useful
weight information in a low-rank space, it essentially can
provide a better training initialization for the DNNs that are
decomposed by LRA. Fig. 7 plots the training data loss and
top-1 validation error of AlexNet, which is decomposed to
the same ranks by PCA. The baseline is the original AlexNet
and the other AlexNet is coordinated by Force Regulariza-

Figure 7. Training data loss and top-1 validation error vs. iteration
when ﬁne-tuning AlexNet which is decomposed to the same ranks.

tion. The ﬁgure shows that the error sharply converges to
a low level after a few iterations, indicating LRA provides
a very good initialization for the low-rank DNNs. Train-
ing it from scratch has signiﬁcant accuracy loss. More im-
portantly, DNNs coordinated by Force Regularization can
converge faster to a lower error.

Besides PCA [22][30], we also evaluated the effec-
tiveness of Force Regularization when integrating it with
SVD [5][26] or k-means clustering [5][2]. Table 3 com-
pares the accuracies of AlexNet decomposed by different
LRA methods. All LRAs preserve the same ranks in all lay-
ers, which means the decomposed AlexNet have the same
network structure. In summary, PCA and SVD obtain sim-
ilar accuracy and surpass k-means clustering. Due to the
limited pages, we adopt PCA as the representative in our
study.

5.3. Acceleration of DNN Testing

In our experiments, we ﬁrst train DNNs with Force Regu-
larization, then decompose DNNs using LRA methods and
ﬁne-tune them to recover accuracy. In evaluation of speed,
we omit small CIFAR-10 database and focus on large-scale
DNNs on ImageNet, whose speed is a real concern. To
prove the effective acceleration of Force Regularization, we
adopt the speedup of state-of-the-art LRAs [30][4][26] as
our baseline. Our speedup is achieved in the case that the
DNN ﬁlters are ﬁrst coordinated by Force Regularization
and then decomposed using the same LRAs. The practical
GPU speed is proﬁled by the advanced hardware (NVIDIA

Table 3. The accuracy of different LRA under the same ranks.

Force

LRA

Top-1 error

None

(cid:96)2-norm

PCA
SVD†
k-means†

PCA
SVD†
k-means†

43.21%
43.27%
44.34%

43.25%
43.20%
44.80%

† SVD and k-means preserve the same ranks with PCA

Table 4. The higher speedups of AlexNet by Force Regularization.

Table 5. The higher speedup factors by force regularization.

Force

Top-1 error

conv3

conv4

conv5

LRA

Force

Top-5 err.

conv3

conv4

conv5

None
(cid:96)2-norm

None
(cid:96)2-norm

None
(cid:96)2-norm

None
(cid:96)2-norm

43.21%
43.25%

43.21%
43.25%

43.21%
43.25%

43.21%
43.25%

rank
rank

184
124

201
106

146
129

GPU 1.58× 1.21× 1.15×
GPU 2.16× 2.03× 1.33×

CPU 1.78× 1.60× 1.47×
CPU 2.45× 2.76× 1.64×

theoretical
theoretical

1.79× 1.72× 1.63×
2.65× 3.26× 1.85×

GTX 1080) and software (cuDNN 5.0). The CPU speed is
measured in Intel Xeon E5-2630 and ATLAS library. The
batch size is 256.

Cross-ﬁlter LRA: We ﬁrst evaluate the speedup of
cross-ﬁlter LRA shown in Fig. 3. In previous works [5][26],
the optimal rank in each layer can be selected layer-by-
layer using cross validation. However, the number of hyper-
parameters increases linearly with the depth of DNNs. To
save development time, we utilize an identical error per-
centage eM
across all layers as the single hyper-parameter
e0
although layer-wise rank selection may give better tradeoff.
The rank in a layer is the minimal M which has error ≤ eM
.
e0
As aforementioned in Section 5.2 and Table 2,
the
learned conv1 and conv2 of AlexNet are already in a very
low-rank space and achieve good speedups using LRAs [5].
Thus we mainly focus on conv3-conv5 here. Table 4 sum-
marizes the speedups of PCA approximation of AlexNet
with and without (cid:96)2-norm Force Regularization. With igno-
ble accuracy difference, Force Regularization successfully
coordinates ﬁlters to a lower-rank space and accelerates the
testing by a higher factor, comparing with the state-of-the-
art LRA. Similar results are observed when applying (cid:96)1-
norm force.

Results in Table 4 also show that practical speedup is dif-
ferent from theoretical speedup. Generally, the difference is
smaller in lower-performance processors. In CPU mode of
Table 4, Force Regularization achieves 2× speedup of total
convolutional time.

Speeding up state-of-the-art LRA: We also duplicate
the state-of-the-art work [26] as the baseline2 (lra1). After
LRA, AlexNet is ﬁne-tuned with learning rate starting from
0.001 and divided by 10 at iteration 70,000 and 140,000.
Fine-tuning terminates after 150,000 iterations.

The ﬁrst row in Table 5 contains the results of the base-
line [26], which don’t scale well to the advanced “TITAN
1080 + cuDNN 5.0” in conv3–5. This is because 3 × 3
convolution is highly optimized in cuDNN 5.0, e.g., us-
ing Winograd’s minimal ﬁltering algorithms [17]. However,
the baseline decomposes the 3 × 3 convolution to a pair of

2Code is provided by the authors in https://github.com/

chengtaipu/lowrankcnn/

lra1 [26]

20.65% GPU 0.86× 0.57× 0.40×

None

None

(cid:96)2-norm

(cid:96)2-norm

lra2

lra2

lra2

19.93% GPU 1.89× 1.57× 1.57×

20.14% GPU 2.25× 2.03× 1.60×
GPU 3.56× 3.01× 2.40×
CPU 4.81× 4.00× 2.92×

21.68%

3 × 1 and 1 × 3 convolution so that the optimized cuDNN
is not fully exploited. This will be a common issue in the
baseline, considering Winograd’s algorithm is universally
used and 3 × 3 convolution is one of the most common
structures. We ﬁnd that LRA in Fig. 3 can be utilized for
conv 3–5 to solve this issue, because it can maintain the
3 × 3 shape. We name this LRA as lra2, which decomposes
conv1–conv2 using LRA in [26] and conv 3–5 using LRA of
Fig. 3. The second row in Table 5 shows that our lra2 can
scale well to the hardware and software advances of “TI-
TAN 1080 + cuDNN 5.0”. More importantly, Force Regu-
larization on conv3–5 can enforce them to more lightweight
layers and attain higher speedup factors than lra2 without
using it. The result is shown in the third row, which in total
achieves 2.03× speedup for the whole convolution in GPU.
With small accuracy loss in row 4 of Table 5, Force Regu-
larization achieves 2.50× speedup of total convolution on
GPU and 4.05× on CPU.

Table 6 compares our method with state-of-the-art DNN
acceleration methods, in CPU mode. When the speedup of
total time was not reported by the authors, we estimate it
by the weighted average speedups over all layers, where
the weighting coefﬁcients are derived from the percent-
age of running time of each layer.
In our hardware plat-
form, conv1–conv5 respectively consume 15.89%, 28.25%,
24.32%, 18.70% and 12.84% testing time. The estimation
is accurate, for example, we estimate 2.58× of total time
in one-shot [14], which is very close to 2.52× reported by
the authors. Comparing with both cp-decomposition and
one-shot methods, our method can achieve higher accuracy
and higher speedup. Comparing with SSL, with almost the
same top-5 error (21.68% vs. 21.63%), we can attain higher
speedup of 4.05× vs. 3.13×.

deep-compression [7] reported 3× to 4× speedups in
fully-connected layers when batch size was 1. However,
convolution is the bottleneck of DNNs, e.g., the convolution
time in AlexNet is 5× of the time in fully-connected layers
when proﬁled in our CPU platform. Moreover, no speedup
was observed in the batching scenario as reported by the au-
thors [7]. More importantly, as we will show in Section 5.4,
our work can work together with sparsity-based methods
(e.g., SSL or deep-compression) to obtain lower-rank and
sparse DNNs and potentially further accelerate the testing
of DNNs.

Table 6. Comparison of speedup factor on AlexNet by state-of-the-art DNN acceleration methods.

Method

Top-5 err.

conv1

conv2

conv3

conv4

conv5

total

AlexNet in Caffe

19.97%

1.00× 1.00× 1.00× 1.00× 1.00× 1.00×

cp-decomposition [18]

20.97% (+1.00%)

–

4.00×

–

–

–

1.27×

one-shot [14]

21.67% (+1.70%)

1.48× 2.30× 3.84× 3.53× 3.13× 2.52×

SSL [28]

our lra2

19.58% (-0.39%)
21.63% (+1.66%)

1.00× 1.27× 1.64× 1.68× 1.32× 1.35×
1.05× 3.37× 6.27× 9.73× 4.93× 3.13×

20.14% (+0.17%)
21.68% (+1.71%)

2.61× 6.06× 2.48× 2.20× 1.58× 2.69×
2.65× 6.22× 4.81× 4.00× 2.92× 4.05×

5.4. Lower-rank and Sparse DNNs

We sparsify the lightweight deep neural network (i.e.,
the ﬁrst one of lra2 in Table 6), using Structured Spar-
sity Learning SSL [28] or non-structured connection-
pruning [23]. Note that Guided Sparsity Learning (GSL) is
not adopted in our connection-pruning though better spar-
sity is achievable when applying it. Figure 8 summarizes
the results.

Experiments prove that our method can work together
with both structured and non-structured sparsity methods to
further compress and accelerate models. Comparing with
deep-compression in Figure 8(a), our model has compara-
ble compression rates but 2.69× faster testing time. Typ-
ically, our model has higher compression rates in convo-
lutional layers, which provides more space for computation
reduction and generalizes better to modern DNNs (ResNets-
152 [10], for example, whose parameters in fc layers are
only 4%).
In Figure 8(b), our accelerated model can be
further accelerated using SSL. The shape-wise sparsity in

Figure 8. The results of sparsifying lightweight DNNs whose ﬁl-
ters are coordinated to a lower-rank space by Force Regularization.
In terms of deep-compression in (a), we only count the compres-
sion rate obtained from connection pruning for a fair comparison,
but quantization and Huffman coding can also be utilized to im-
prove the compression rate for our model. Based on SSL in (b), we
enforce shape-wise sparsity on conv3 s, conv4 s and conv5 s to
learn the shapes of basis ﬁlters meanwhile enforce ﬁlter-wise spar-
sity on conv3 f and conv4 f to learn the number of ﬁlters [28]. As
each convolutional layer in the lra2 is decomposed to two small
layers, we respectively denote the ﬁrst and second small layer by
sufﬁxing “ s” and “ f”. The baseline and our model have the same
accuracy.

conv3–5 of our model is slightly lower because our model is
already aggressively compressed by LRA. The higher ﬁlter-
wise sparsity, however, implies the orthogonality of our ap-
proach to SSL.

5.5. Generalization of Force Regularization

In convolutional layers, each ﬁlter basically extracts a
discriminative feature, e.g., an orientation-selective pattern
or a color blob in the ﬁrst layer [16] or a high-level fea-
ture (e.g., textures, faces, etc.) in deeper layers [29]. The
discrimination among ﬁlters is important for classiﬁcation
performance. Our method can coordinate ﬁlters for more
lightweight DNNs meanwhile maintain the discrimination.
It can also be generalized to learn more discriminative ﬁl-
ters to improve the accuracy. The extension to Discrimi-
nation Regularization is straightforward but effective: the
opposite gradient of Force Regularization (i.e., λs < 0) is
utilized to update the ﬁlter. In this scenario, it works as the
repulsive force to repel surrounding ﬁlters and enhance the
discrimination. Table 7 summarizes the improved accuracy
of state-of-the-art DNNs.

Acknowledgments

This work was supported in part by NSF CCF-1744082.
Any opinions, ﬁndings and conclusions or recommenda-
tions expressed in this material are those of the authors and
do not necessarily reﬂect the views of NSF or their contrac-
tors.

Table 7. Improved accuracy with Discrimination Regularization.

Net

Regularization

Top-1 error

AlexNet
AlexNet
AlexNet

None (baseline)
(cid:96)2-norm force
(cid:96)1-norm force

ResNets-20 None (baseline)
ResNets-20
(cid:96)2-norm force
ResNets-20
(cid:96)1-norm force

42.63%
41.71%
41.53%

8.82%
7.97%
8.02%

References

[1] J. M. Alvarez and M. Salzmann. Learning the number of
neurons in deep networks. In Advances in Neural Informa-
tion Processing Systems (NIPS), pages 2262–2270, 2016. 2
[2] C. Bauckhage. k-means clustering is matrix factorization.

arXiv:1512.07548, 2015. 2, 6

[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2009. 5

[4] M. Denil, B. Shakibi, L. Dinh, M. A. Ranzato, and N. de Fre-
itas. Predicting parameters in deep learning. In Advances in
Neural Information Processing Systems (NIPS). 2013. 1, 2,
3, 6

[5] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fer-
gus. Exploiting linear structure within convolutional net-
In Advances in Neural In-
works for efﬁcient evaluation.
formation Processing Systems (NIPS). 2014. 1, 2, 5, 6, 7
[6] Y. Guo, A. Yao, and Y. Chen. Dynamic network surgery for
efﬁcient dnns. In Advances in Neural Information Process-
ing Systems (NIPS). 2016. 1, 2

[7] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural network with pruning, trained quanti-
zation and huffman coding. arXiv:1510.00149, 2015. 1, 2,
7

[8] S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights
and connections for efﬁcient neural network. In Advances in
Neural Information Processing Systems (NIPS). 2015. 1, 2

[9] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In International Conference on Computer Vi-
sion (ICCV), 2015. 1

[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
In IEEE Conference on Computer

for image recognition.
Vision and Pattern Recognition (CVPR), 2016. 1, 5, 6, 8
[11] Y. Ioannou, D. P. Robertson, J. Shotton, R. Cipolla, and
A. Criminisi. Training cnns with low-rank ﬁlters for efﬁcient
image classiﬁcation. arXiv:1511.06744, 2015. 1, 2

[12] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up
convolutional neural networks with low rank expansions.
In Proceedings of the British Machine Vision Conference
(BMVC), 2014. 1, 2

[13] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. arXiv:1408.5093,
2014. 5

[14] Y.-D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin.
Compression of deep convolutional neural networks for fast
arXiv:1511.06530,
and low power mobile applications.
2015. 1, 2, 7, 8

[15] A. Krizhevsky and G. Hinton. Learning multiple layers of

features from tiny images. 2009. 5

[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
In
classiﬁcation with deep convolutional neural networks.
Advances in Neural Information Processing Systems (NIPS).
2012. 1, 2, 5, 8

[17] A. Lavin. Fast algorithms for convolutional neural networks.

arXiv:1509.09308, 2015. 7

[18] V. Lebedev, Y. Ganin, M. Rakhuba, I. Oseledets, and V. Lem-
pitsky. Speeding-up convolutional neural networks using
ﬁne-tuned cp-decomposition. arXiv:1412.6553, 2014. 1, 2,
8

[19] V. Lebedev and V. Lempitsky. Fast convnets using group-
wise brain damage. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2016. 1, 2

[20] Y. LeCun, J. S. Denker, S. A. Solla, R. E. Howard, and L. D.
Jackel. Optimal brain damage. In Advances in Neural In-
formation Processing Systems (NIPS), volume 2, pages 598–
605, 1989. 2

[21] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf.
Pruning ﬁlters for efﬁcient convnets. In International Con-
ference on Learning Representations (ICLR), 2017. 2
[22] B. Liu, M. Wang, H. Foroosh, M. Tappen, and M. Pensky.
Sparse convolutional neural networks. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2015.
1, 2, 5, 6

[23] J. Park, S. Li, W. Wen, P. T. P. Tang, H. Li, Y. Chen, and
P. Dubey. Faster cnns with direct sparse convolutions and
In International Conference on Learning
guided pruning.
Representations (ICLR), 2017. 1, 2, 8
[24] K. Simonyan and A. Zisserman.

Very deep con-
large-scale image recognition.

volutional networks for
arXiv:1409.1556, 2014. 1, 2

[25] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
In IEEE Conference on
Going deeper with convolutions.
Computer Vision and Pattern Recognition (CVPR), 2015. 1,
2, 6

[26] C. Tai, T. Xiao, X. Wang, and W. E. Convolutional neu-
ral networks with low-rank regularization. In International
Conference on Learning Representations (ICLR), 2016. 1, 2,
3, 6, 7

[27] P. Wang and J. Cheng. Accelerating convolutional neural
networks for mobile applications. In Proceedings of the 2016
ACM on Multimedia Conference, 2016. 1, 2

[28] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning
structured sparsity in deep neural networks. In Advances in
Neural Information Processing Systems (NIPS). 2016. 1, 2,
5, 8

[29] M. D. Zeiler and R. Fergus. Visualizing and understanding
convolutional networks. In European Conference on Com-
puter Vision (ECCV), 2014. 8

[30] X. Zhang, J. Zou, K. He, and J. Sun. Accelerating very
deep convolutional networks for classiﬁcation and detection.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 38(10):1943–1955, Oct 2016. 1, 2, 3, 5, 6

7
1
0
2
 
l
u
J
 
5
2
 
 
]

V
C
.
s
c
[
 
 
3
v
6
4
7
9
0
.
3
0
7
1
:
v
i
X
r
a

Coordinating Filters for Faster Deep Neural Networks

Wei Wen
University of Pittsburgh
wew57@pitt.edu

Cong Xu
Hewlett Packard Labs
cong.xu@hpe.com

Chunpeng Wu
University of Pittsburgh
chw127@pitt.edu

Yandan Wang
University of Pittsburgh
yaw46@pitt.edu

Yiran Chen
Duke University
yiran.chen@duke.edu

Hai Li
Duke University
hai.li@duke.edu

Abstract

Very large-scale Deep Neural Networks (DNNs) have
achieved remarkable successes in a large variety of com-
puter vision tasks. However, the high computation intensity
of DNNs makes it challenging to deploy these models on
resource-limited systems. Some studies used low-rank ap-
proaches that approximate the ﬁlters by low-rank basis to
accelerate the testing. Those works directly decomposed
the pre-trained DNNs by Low-Rank Approximations (LRA).
How to train DNNs toward lower-rank space for more ef-
ﬁcient DNNs, however, remains as an open area. To solve
the issue, in this work, we propose Force Regularization,
which uses attractive forces to enforce ﬁlters so as to coor-
dinate more weight information into lower-rank space1. We
mathematically and empirically verify that after applying
our technique, standard LRA methods can reconstruct ﬁlters
using much lower basis and thus result in faster DNNs. The
effectiveness of our approach is comprehensively evaluated
in ResNets, AlexNet, and GoogLeNet. In AlexNet, for ex-
ample, Force Regularization gains 2× speedup on modern
GPU without accuracy loss and 4.05× speedup on CPU by
paying small accuracy degradation. Moreover, Force Reg-
ularization better initializes the low-rank DNNs such that
the ﬁne-tuning can converge faster toward higher accuracy.
The obtained lower-rank DNNs can be further sparsiﬁed,
proving that Force Regularization can be integrated with
state-of-the-art sparsity-based acceleration methods.

1. Introduction

Deep Neural Networks (DNNs) have achieved record-
breaking accuracy in many image classiﬁcation tasks [16]
[24][25][10]. With the advances of algorithms, availabil-
ity of database, and improvement in hardware performance,

1The

source

code

is

available

in https://github.com/

wenwei202/caffe

Figure 1. The low-rank basis of ﬁlters in the ﬁrst layer of the con-
volutional neural network [16] on CIFAR-10. The low-rank basis
is formed by the most signiﬁcant principal ﬁlters that are obtained
by PCA. Top: the low-rank basis of the original network. Bottom:
the low-rank basis of the same network after applying Force Reg-
ularization. The number of red boxes indicates the required rank
to reconstruct the original ﬁlters with ≤ 20% error.

the depth of DNNs grows dramatically from a few to hun-
dreds or even thousands of layers, enabling human-level
performance [9]. However, deploying these large models on
resource-limited platforms, e.g., mobiles and autonomous
cars, is very challenging due to the high demand in the com-
putation resource and hence energy consumption.

Recently, many techniques to accelerate the testing pro-
cess of deployed DNNs have been studied, such as weight
sparsifying or connection pruning [8][7][28][23][22][6]
[19]. These approaches require delicate hardware cus-
tomization and/or software design to transfer sparsity into
practical speedup. Unlike sparsity-based methods, Low-
Rank Approximation (LRA) methods [22][4][5][12][11]
[26][27][18][30][14] directly decompose an original large
model to a compact model with more lightweight layers.
Thanks to the redundancy (correlation) among ﬁlters in
DNNs, original weight tensors can be approximated by very
low-rank basis. From the viewpoint of matrix computation,
LRA approximates a large weight matrix by the product of
two or more small ones to reduce computation complexity.
Previous LRA methods mostly focus on how to decom-
pose the pre-trained weight tensors for maximizing the re-
duction of computation complexity, meanwhile retaining
the classiﬁcation accuracy. Instead, we propose to nudge
the weights by additional gradients (attractive forces) to co-
ordinate the ﬁlters to a more correlated state. Our approach

aims to improve the correlation among ﬁlters and therefore
obtain more lightweight DNNs through LRA. To the best of
our knowledge, this is the ﬁrst work to train DNNs toward
lower-rank space such that LRA can achieve faster DNNs.
The motivation of this work is fundamental. It has been
proven that trained ﬁlters are highly clustered and corre-
lated [5][4][12]. Suppose each ﬁlter is reshaped as a vector.
A cluster of highly-correlated vectors then will have small
included angles. If we are able to coordinate these vectors
toward a state with smaller included angles, the correlation
of the ﬁlters within that cluster improves. Consequently,
LRA can produce a DNN with lower ranks and higher com-
putation efﬁciency.

We propose a Force Regularization to coordinate ﬁl-
ters in DNNs. As demonstrated in Fig. 1, when using
the same LRA method, say, cross-ﬁlter Principal Compo-
nent Analysis (PCA) [30], applying Force Regularization
can greatly reduce the required ranks from the original de-
sign (i.e., 5 vs. 11), while keeping the same approximation
errors (≤ 20%). As we shall show in Section 5, apply-
ing Force Regularization in the training of state-of-the-art
DNNs will successfully obtain lower-rank DNNs and thus
improve computation efﬁciency, e.g., 4.05× speedup for
AlexNet with small accuracy loss.

The contributions of our work include:

(1) We pro-
pose an effective and easy-to-implement Force Regulariza-
tion to train DNNs for lower-rank approximation. To the
best of our knowledge, this is the ﬁrst work to manipulate
the correlation among ﬁlters during training such that LRA
can achieve faster DNNs; (2) DNNs manipulated by Force
Regularization can have better initialization for the retrain-
ing of LRA-decomposed DNNs, resulting in faster conver-
gence to better accuracy; (3) Those lightweight DNNs that
have been aggressively compressed by our method can be
further sparsiﬁed. That is, our method can be integrated
with state-of-the-art sparsity-based methods to potentially
achieve faster computation; (4) Force Regularization can
be easily generalized to Discrimination Regularization that
can learn more discriminative ﬁlters to improve classiﬁca-
tion accuracy; (5) Our implementation is open-source on
both CPUs and GPUs.

2. Related work

Low-rank approximation. LRA method decomposes a
large model to a compact one with more lightweight lay-
ers by weight/tensor factorization. Denil et al. [4] studied
different dictionaries to remove the redundancy between ﬁl-
ters and channels in DNNs. Jaderberg et al. [12] explored
ﬁlter and data reconstruction optimizations to attain opti-
mal separable basis. Denton et al. [5] clustered ﬁlters, ex-
tended LRA (e.g., Singular Value Decomposition, SVD ) to
larger-scale DNNs, and achieved 2× speedup for the ﬁrst
two layers with 1% accuracy loss. Many new decomposi-

tion methods were proposed [11][26][18][30] and the ef-
fectiveness of LRA in state-of-the-art DNNs were evalu-
ated [24][25]. Similar evaluations on mobile devices were
also reported [14][27]. Unlike them, we propose Force Reg-
ularization to coordinate DNN ﬁlters to more correlated
states, in which lower-rank or more compact DNNs are
achievable for faster computation.

Sparse deep neural networks.

The studies on
sparse DNNs can be categorized into two types: non-
structured [20][23][22][8][6] and structured [28][21][19][1]
sparsity methods. The ﬁrst category prunes each connec-
tion independently. Consequently, sparse weights are ran-
domly distributed. The level of non-structured sparsity is
usually insufﬁcient to achieve good practical speedup in
modern hardware [28][19]. Software optimization [23][22]
and hardware customization [7] are proposed to overcome
this issue. Conversely, the structured approaches prune con-
nections group by group, such that the sparsiﬁed DNNs
have regular distribution of sparse weights. The regular-
ity is friendly to modern hardware for acceleration. Our
work is orthogonal to sparsity-based methods. More impor-
tantly, we ﬁnd that DNNs accelerated by our method can
be further sparsiﬁed by both non-structured and structured
sparsity methods, potentially achieving faster computation.

3. Correlated Filters and Their Approximation

The prior knowledge is that correlation exists among
trained ﬁlters in DNNs and those ﬁlters lie in a low-rank
space. For example, the color-agnostic ﬁlters [16] learned
in the ﬁrst layer of AlexNet lie in a hyper-plane, where RGB
channels at each pixel have the same value. Fig. 2 presents
the results of Linear Discriminant Analysis (LDA) of the
ﬁrst convolutional ﬁlters in AlexNet and GoogLeNet. The
ﬁlters are normalized to unit vectors and colored to four
clusters by k-means clustering, and then projected to 2D
space by LDA to maximize cluster separation. The ﬁgure
indicates high correlation among ﬁlters within a cluster. A
na¨ıve approach of ﬁlter approximation is to use the centroid
of a cluster to approximate ﬁlters within that cluster, thus,
the number of clusters is the rank of the space. Essentially,
k-means clustering is a LRA [2] method, although we will

Figure 2. Linear Discriminant Analysis (LDA) of ﬁlters in the ﬁrst
convolutional layer of AlexNet (left) and GoogLeNet (right).

Figure 3. Cross-ﬁlter LRA of a convolutional layer.

Figure 4. Force Regularization to coordinate ﬁlters.

later show that other LRA methods can give better approxi-
mation. The motivation of this work is that if we are able to
nudge ﬁlters during the training such that the ﬁlters within
a cluster are coordinated closer and some adjacent clusters
are even merged into one cluster, then more accurate ﬁlter
approximation using lower rank can be achieved. We pro-
pose Force Regularization to realize it.

Before introducing Force Regularization, we ﬁrst mathe-
matically formulate LRA of DNN ﬁlters. Theoretically, al-
most all LRA methods can gain lower-rank approximation
upon our method because ﬁlters are coordinated to more
correlated state. Instead of onerously replicating all of these
LRA methods, we choose cross-ﬁlter approximation [4][30]
and a state-of-the-art work in [26] as our baselines.

Fig. 3 illustrates the cross-ﬁlter approximation of a con-
volutional layer. We assume all weights in a convolutional
layer is a tensor W ∈ RN ×C×H×W , where N and C are
the numbers of ﬁlters and input channels, and H and W
are the spatial height and width of the ﬁlters, respectively.
With input feature map I, the n-th output feature map
On = Wn ∗ I, where Wn ∈ R1×C×H×W is the n-th ﬁlter.
Because of the redundancy (or correlation) across the ﬁl-
ters [4], tensor Wn(∀n ∈ [1...N ]) can be approximated by
a linear combination of the basis Bm ∈ R1×C×H×W (m ∈
[1...M ], M (cid:28) N ) of a low-rank space B ∈ RM ×C×H×W ,
such as

On ≈

b(n)
m Bm

∗ I =

(cid:33)

(cid:32) M
(cid:88)

m=1

M
(cid:88)

(cid:16)

b(n)
m Fm

(cid:17)

.

(1)

m=1

Where b(n)
m is a scalar, and Fm = Bm ∗ I is the feature
map generated by basis ﬁlter Bm. Therefore, the output fea-
ture map On is a linear combination of Fm(m ∈ [1...M ])
which can be interpreted as the feature map basis. Since the
linear combination essentially is a 1 × 1 convolution, the
convolutional layer can be decomposed to two sequential
lightweight convolutional layers as shown in Fig. 3. The
original computation complexity is O(N CHW H
),
where H
is the height and width of output fea-
ture maps, respectively. After applying cross-ﬁlter LRA, the
computation complexity is reduced to O(M CHW H
+
). The computation complexity decreases when
N M H

and W

W

W

W

(cid:48)

(cid:48)

(cid:48)

(cid:48)

(cid:48)

(cid:48)

(cid:48)

(cid:48)

the rank M < N CHW

CHW +N .

4. Force Regularization

4.1. Regularization by Attractive Forces

This section proposes Force Regularization from the per-
spective of physics.
It is a gradient-based approach that
adds extra gradients to data loss gradients. The data loss
gradients aim to minimize classiﬁcation error as traditional
DNNs do. The extra gradients introduced by Force Regular-
ization gently adjust the lengths and directions of data loss
gradients so as to nudge ﬁlters to a more correlated state.
With a good setup of hyper-parameter, our method can co-
ordinate more useful information of ﬁlters to a lower-rank
space meanwhile maintain accuracy. Inspired by Newton’s
Laws, we propose an intuitive, computation-efﬁcient and
effective Force Regularization that uses attractive forces to
coordinate ﬁlters.

Force Regularization: As illustrated in Fig. 4, suppose
the ﬁlter Wn ∈ W is reshaped as a vector Wn ∈ R1×CHW
and normalized as wn ∈ R1×CHW (∀n ∈ [1...N ]), with
their origin at O. We introduce the pair-wise attractive force
fji = f (wj − wi) (∀i, j ∈ [1...N ]) on wi generated by wj.
The gradient of Force Regularization to update ﬁlter Wi is
deﬁned as

∆Wi =

∆Wij = ||Wi||

(cid:0)fji − fjiwT

i wi

(cid:1) ,

(2)

N
(cid:88)

j=1

N
(cid:88)

j=1

·

where ||
|| is the Euclidean norm. The regularization
gradient in Eq. (2) is perpendicular to ﬁlter vector and can
be efﬁciently computed by addition and multiplication. The
ﬁnal updating of weights by gradient descent is

Wi ← Wi − η ·

− λs · ∆Wi

,

(3)

(cid:18) ∂E(W)
∂Wi

(cid:19)

where E(W) is data loss, η is learning rate and λs > 0 is
the coefﬁcient of Force Regularization to trade off the rank
and accuracy. We select λs by cross-validation in this work.
The gradient of common weight-wise regularization (e.g.,
(cid:96)2-norm) is omitted in Eq. (3) for simplicity.

Fig. 4 intuitively explains our method. Suppose each
vector wi is a rigid stick and there is a particle ﬁxed at
the endpoint. The particle has unit mass, and the stick is
massless and can freely spin around the origin. Given the
pair-wise attractive forces (e.g., universal gravitation) fji,
Eq. (2) is the acceleration of particle i. As the forces are at-
tractive, neighbor particles tend to spin around the origin to
assemble together. Although our regularizer seems to col-
lapse all particles to one point which is the rank-one space
for most lightweight DNNs, there exist gradients of data
loss to avoid this. More speciﬁc, pre-trained ﬁlters orient
to discriminative directions wn (n ∈ [1...N ]). In each di-
rection wn, there are some correlated ﬁlters as observed in
Fig. 2. During the subsequent retraining with our regular-
izer, regularization gradients coordinate a cluster of ﬁlters
closer to a typical direction dm (m ∈ [1...M ], M (cid:28) N ),
but data loss gradients avoid collapsing dm together so as to
maintain the ﬁlters’ capability of extracting discriminative
features. If all ﬁlters could be extremely collapsed toward
one point meanwhile maintain classiﬁcation accuracy, it im-
plies the ﬁlters are over-redundant and we can attain a very
efﬁcient DNN by decomposing it to a rank-one space.

We derive the Force Regularization gradient from the
normalized ﬁlters based on the following facts: (1) A nor-
malized ﬁlter is on the unit hypersphere, and its orientation
is the only free parameter we need to optimize; (2) The gra-
dient of Wi can be easily scaled by the vector length ||Wi||
without changing the angular velocity.

In Eq. (2), fji = f (wj − wi) is the force function related

to distance. We study (cid:96)2-norm Force

f(cid:96)2(wj − wi) = wj − wi

and (cid:96)1-norm Force

f(cid:96)1 (wj − wi) =

wj − wi
||wj − wi||

(4)

(5)

in this work. We deﬁne the force of Eq. (4) as (cid:96)2-norm
Force because the strength linearly decreases with the dis-
tance ||wj − wi||, just as the gradient of regularization (cid:96)2-
norm does. We name the force of Eq. (5) as (cid:96)1-norm Force
because the gradient is a constant unit vector regardless of
the distance, just as the gradient of sparsity regularization
(cid:96)1-norm is.

Table 1. Ranks vs. scalers of step sizes of regularization gradients.

Scaler

Error

conv1*

conv2

conv3

0 (baseline)
||Wi||
1/||Wi||
* The ﬁrst convolutional layer.

18.0% 17/32
17.9% 15/32
18.0% 16/32

27/32
22/32
27/32

55/64
30/64
32/64

[1...N ]). For each ﬁlter, Force Regularization under (cid:96)2-
norm force has the same gradient direction of regulariza-
tion R(W), but differs by adapting the step size to the ﬁler’s
length, where

R(W) =

1
2

N
(cid:88)

N
(cid:88)

j=1

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Wj
||Wj||

−

Wi
||Wi||

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(6)

Proof : Because wj = Wj

||Wj || ,

∂R(W)
∂Wi

=

=

1
2

1
2

= −

N
(cid:88)

j=1

N
(cid:88)

j=1

N
(cid:88)

j=1

∂ (wj − wi) (wj − wi)T
∂Wi
∂ (cid:0)1 − 2wjwT

i + 1(cid:1)

∂Wi

(cid:1)

∂ (cid:0)wjwT
∂Wi

i

= −

wj

N
(cid:88)

j=1

∂wT
i
∂Wi

,

where ∂wT
i
∂Wi

:= Gi is a derivative matrix with element

G(pq)

i =

∂ W (p)
i
||Wi||
∂W (q)
i

=

(cid:32)

∂w(p)
i
∂W (q)
i

1
||Wi||

=

δ(p, q) −

i W (q)
W (p)
i
||Wi||2

(cid:33)

.

Superscripts p, q ∈ [1 . . . CHW ] index the elements in

vectors wi and Wi. δ(p, q) is the unit impulse function:
(cid:40)

δ(p, q) =

1 p = q
0 p (cid:54)= q

.

Therefore,

Gi =

1
||Wi||

(cid:0)I − wT

i wi

(cid:1) .

4.2. Mathematical Implications

Replacing Eq. (10) to Eq. (7), we have

This section explains the mathematical implications be-
hind: Force Regularization is related to but different from
minimizing the sum of pair-wise distances between normal-
ized ﬁlters.

Theorem 1 Suppose ﬁlter Wn ∈ W is reshaped as a vector
Wn ∈ R1×CHW and normalized as wn ∈ R1×CHW (∀n ∈

−

∂R(W)
∂Wi

=

1
||Wi||

(cid:0)(wj − wi) − (wj − wi) wT

(cid:1)

i wi

=

1
||Wi||





fji

 −






 wT

fji

i wi

 ,

N
(cid:88)

j=1

N
(cid:88)

j=1

N
(cid:88)

j=1







(7)

(8)

(9)

(10)

(11)

where fji = f(cid:96)2(wj − wi) = wj − wi. Therefore, Eq. (11)
and Eq. (2) have the same direction.

Theorem 1 states that our proposed Force Regularization
in Eq. (2) is related to Eq. (11). However, the step size of
the gradient in Eq. (2) is scaled by the length ||Wi|| of the
ﬁlter instead of its reciprocal in Eq. (11). This ensures that
the ﬁlter spins the same angle regardless of its length and
avoids the issue of being divided by zero. Table 1 summa-
rizes the ranks vs. step sizes for the ConvNet [16], which is
trained by CIFAR-10 database without data augmentation.
The original ConvNet has 32, 32, and 64 ﬁlters in each con-
volutional layer, respectively. The rank is the smallest num-
ber of basis ﬁlters (in Fig. 3) obtained by PCA with ≤ 5%
reconstruction error. Therefore, ||Wi|| works better than its
reciprocal when coordinating ﬁlters to a lower-rank space.
Following the same proof procedure, we can easily ﬁnd
that Force Regularization under (cid:96)1-norm Force has the same
conclusion when

R(W) =

N
(cid:88)

N
(cid:88)

j=1

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Wj
||Wj||

−

Wi
||Wi||

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(12)

5. Experiments

5.1. Implementation

Our experiments are performed in Caffe [13] using
CIFAR-10 [15] and ILSVRC-2012 ImageNet [3]. Pub-
lished models are adopted as the baselines: In CIFAR-10,
we choose ConvNet without data augmentation [16] and
ResNets-20 with data augmentation [10]. We adopt the
same shortcut connections in [28] for ResNets-20. For Im-
ageNet, we use AlexNet and GoogLeNet models trained by
Caffe, and report accuracy using only center crop of images.
Our experiments of Force Regularization show that, with
the same maximum iterations, the training from the baseline
can achieve a better tradeoff between accuracy and speedup
comparing with the training from scratch, because the base-
line offers a good initial point for both accuracy and ﬁlter
correlation. During the training with Force Regularization
on CIFAR-10, we use the same base learning rate as the
baseline; while in ImageNet, 0.1× base learning rate of the
baseline is adopted.

Figure 5. The rank M in each convolutional layer of ResNets-20
and GoogLeNet. Red bar overlaps blue bar. The accuracy loss is
0.75% for ResNets-20 and 2.46% (top-5) for GoogLeNet.

5.2. Rank Analysis of Coordinated DNNs

In light of various low-rank approximation methods,
without losing the generalization, we ﬁrst adopt Principal
Component Analysis (PCA) [30][22] to evaluate the effec-
tiveness of Force Regularization. Speciﬁcally, the ﬁlter
tensor W can be reshaped to a matrix W ∈ RN ×CHW ,
the rows of which are the reshaped ﬁlters Wn (∀n ∈
[1...N ]). PCA minimizes the least square reconstruction
error when projecting a column (RN ) of W to a low-
rank space RM (M (cid:28) N ). The reconstruction error is
eM = (cid:80)N
i=M +1 λi, where λi is the i-th largest eigenvalue
of covariance matrix WWT
CHW −1 . Under the constraint of error
(e.g., eM
percentage eM
≤ 5%), lower-rank approximation
e0
e0
can be obtained if the minimal rank M can be smaller. In
this section, without explicit explanation, we deﬁne rank M
of a convolutional layer as the minimal M which has ≤ 5%
reconstruction error by PCA.

Table 2 summarizes the rank M in each layer of Con-
vNet and AlexNet without accuracy loss after Force Regu-
larization. In the baselines, the learned ﬁlters in the front
layers are intrinsically in a very low-rank space but the rank
M in deeper layers is high. This could explain why only
speedups of the ﬁrst two convolutional layers were reported
in [5]. Fortunately, by using either (cid:96)2-norm or (cid:96)1-norm
force, our method can efﬁciently maintain the low rank M
in the ﬁrst two layers (e.g., conv1-conv2 in AlexNet), mean-
while signiﬁcantly reduce the rank M of deeper layers (e.g.,
conv3-conv5 in AlexNet). On average, our method can re-
duce the layer-wise rank ratio by ∼ 50%. The effective-
ness of our method on deep layers is very important as the

Net

Table 2. The rank M in each convolutional layer after Force Regularization.
conv2

Top-1 error

conv5

conv3

conv4

Force

Average rank ratio ‡

ConvNet None (baseline)†
ConvNet
ConvNet

(cid:96)2-norm
(cid:96)1-norm

18.0%
17.9%
18.0%

27/32
22/32
25/32

55/64
30/64
20/64

–
–
–

–
–
–

74.48%
54.17%
54.17%

AlexNet
72.29%
AlexNet
46.98%
AlexNet
50.03%
†The baseline without Force Regularization. ‡M /N : Low rank M over full rank N , which is deﬁned as rank ratio.

None (baseline)
(cid:96)2-norm
(cid:96)1-norm

164/256
143/256
155/256

306/384
128/384
157/384

318/384
122/384
108/384

220/256
161/256
178/256

42.63%
42.70%
42.45%

47/96
49/96
49/96

conv1
17/32‡
15/32
17/32

Figure 6. Th rank ratio (having ≤ 5% PCA reconstruction error)
in each layer vs. top-1 error for AlexNet. Horizontal dotted lines
represent the rank ratios of the baseline, and vertical dotted line
is the error of baseline. Solid (dashed) curves depict rank ratios
of the AlexNet after Force Regularization by (cid:96)2-norm ((cid:96)1-norm)
force. Each layer is denoted by a typical color. The sensitivity of
hyper-parameter λs: along the direction from left to right, λs of
(cid:96)2-norm force changes from 1.2e-5, to 1.8e-5, 2.0e-5, 3.0e-5, and
3.5e-5; and for (cid:96)1-norm force, it changes from 1.5e-5, to 1.8e-5,
2.0e-5, and 2.5e-5.

depth of modern DNNs grows dramatically [25][10]. Fig. 5
shows the rank M of ResNets-20 [10] and GoogLeNet [25]
after Force Regularization, representing the scalability of
our method on deeper DNNs. With an acceptable accuracy
loss, 5 layers in ResNets-20 and 6 layers in GoogLeNet are
even coordinated to rank M = 1, which indicates those In-
ception blocks in GoogLeNet or Residual blocks in ResNets
have been over-parameterized and can be greatly simpliﬁed.
To study the trade-off between rank, accuracy, and the
pros and cons of (cid:96)2-norm and (cid:96)1-norm force, we conducted
comprehensive experiments on AlexNet. As shown in Fig. 6,
with mere 1.71% (1.80%) accuracy loss, the average rank
ratio can be reduced to 28.59% (28.72%) using (cid:96)2-norm ((cid:96)1-
norm) force. Very impressively, the rank M of each group
in conv4 can be reduced to one by (cid:96)1-norm force. The re-
sults also show that (cid:96)2-norm force is more effective than
(cid:96)1-norm force when the rank ratio is high (e.g., conv2 and
conv5), while (cid:96)1-norm force works better for layers whose
potential rank ratios are low (e.g., conv3 and conv4).
In
general, (cid:96)2-norm force can better balance the ranks across
all the layers.

Because Force Regularization coordinates more useful
weight information in a low-rank space, it essentially can
provide a better training initialization for the DNNs that are
decomposed by LRA. Fig. 7 plots the training data loss and
top-1 validation error of AlexNet, which is decomposed to
the same ranks by PCA. The baseline is the original AlexNet
and the other AlexNet is coordinated by Force Regulariza-

Figure 7. Training data loss and top-1 validation error vs. iteration
when ﬁne-tuning AlexNet which is decomposed to the same ranks.

tion. The ﬁgure shows that the error sharply converges to
a low level after a few iterations, indicating LRA provides
a very good initialization for the low-rank DNNs. Train-
ing it from scratch has signiﬁcant accuracy loss. More im-
portantly, DNNs coordinated by Force Regularization can
converge faster to a lower error.

Besides PCA [22][30], we also evaluated the effec-
tiveness of Force Regularization when integrating it with
SVD [5][26] or k-means clustering [5][2]. Table 3 com-
pares the accuracies of AlexNet decomposed by different
LRA methods. All LRAs preserve the same ranks in all lay-
ers, which means the decomposed AlexNet have the same
network structure. In summary, PCA and SVD obtain sim-
ilar accuracy and surpass k-means clustering. Due to the
limited pages, we adopt PCA as the representative in our
study.

5.3. Acceleration of DNN Testing

In our experiments, we ﬁrst train DNNs with Force Regu-
larization, then decompose DNNs using LRA methods and
ﬁne-tune them to recover accuracy. In evaluation of speed,
we omit small CIFAR-10 database and focus on large-scale
DNNs on ImageNet, whose speed is a real concern. To
prove the effective acceleration of Force Regularization, we
adopt the speedup of state-of-the-art LRAs [30][4][26] as
our baseline. Our speedup is achieved in the case that the
DNN ﬁlters are ﬁrst coordinated by Force Regularization
and then decomposed using the same LRAs. The practical
GPU speed is proﬁled by the advanced hardware (NVIDIA

Table 3. The accuracy of different LRA under the same ranks.

Force

LRA

Top-1 error

None

(cid:96)2-norm

PCA
SVD†
k-means†

PCA
SVD†
k-means†

43.21%
43.27%
44.34%

43.25%
43.20%
44.80%

† SVD and k-means preserve the same ranks with PCA

Table 4. The higher speedups of AlexNet by Force Regularization.

Table 5. The higher speedup factors by force regularization.

Force

Top-1 error

conv3

conv4

conv5

LRA

Force

Top-5 err.

conv3

conv4

conv5

None
(cid:96)2-norm

None
(cid:96)2-norm

None
(cid:96)2-norm

None
(cid:96)2-norm

43.21%
43.25%

43.21%
43.25%

43.21%
43.25%

43.21%
43.25%

rank
rank

184
124

201
106

146
129

GPU 1.58× 1.21× 1.15×
GPU 2.16× 2.03× 1.33×

CPU 1.78× 1.60× 1.47×
CPU 2.45× 2.76× 1.64×

theoretical
theoretical

1.79× 1.72× 1.63×
2.65× 3.26× 1.85×

GTX 1080) and software (cuDNN 5.0). The CPU speed is
measured in Intel Xeon E5-2630 and ATLAS library. The
batch size is 256.

Cross-ﬁlter LRA: We ﬁrst evaluate the speedup of
cross-ﬁlter LRA shown in Fig. 3. In previous works [5][26],
the optimal rank in each layer can be selected layer-by-
layer using cross validation. However, the number of hyper-
parameters increases linearly with the depth of DNNs. To
save development time, we utilize an identical error per-
centage eM
across all layers as the single hyper-parameter
e0
although layer-wise rank selection may give better tradeoff.
The rank in a layer is the minimal M which has error ≤ eM
.
e0
As aforementioned in Section 5.2 and Table 2,
the
learned conv1 and conv2 of AlexNet are already in a very
low-rank space and achieve good speedups using LRAs [5].
Thus we mainly focus on conv3-conv5 here. Table 4 sum-
marizes the speedups of PCA approximation of AlexNet
with and without (cid:96)2-norm Force Regularization. With igno-
ble accuracy difference, Force Regularization successfully
coordinates ﬁlters to a lower-rank space and accelerates the
testing by a higher factor, comparing with the state-of-the-
art LRA. Similar results are observed when applying (cid:96)1-
norm force.

Results in Table 4 also show that practical speedup is dif-
ferent from theoretical speedup. Generally, the difference is
smaller in lower-performance processors. In CPU mode of
Table 4, Force Regularization achieves 2× speedup of total
convolutional time.

Speeding up state-of-the-art LRA: We also duplicate
the state-of-the-art work [26] as the baseline2 (lra1). After
LRA, AlexNet is ﬁne-tuned with learning rate starting from
0.001 and divided by 10 at iteration 70,000 and 140,000.
Fine-tuning terminates after 150,000 iterations.

The ﬁrst row in Table 5 contains the results of the base-
line [26], which don’t scale well to the advanced “TITAN
1080 + cuDNN 5.0” in conv3–5. This is because 3 × 3
convolution is highly optimized in cuDNN 5.0, e.g., us-
ing Winograd’s minimal ﬁltering algorithms [17]. However,
the baseline decomposes the 3 × 3 convolution to a pair of

2Code is provided by the authors in https://github.com/

chengtaipu/lowrankcnn/

lra1 [26]

20.65% GPU 0.86× 0.57× 0.40×

None

None

(cid:96)2-norm

(cid:96)2-norm

lra2

lra2

lra2

19.93% GPU 1.89× 1.57× 1.57×

20.14% GPU 2.25× 2.03× 1.60×
GPU 3.56× 3.01× 2.40×
CPU 4.81× 4.00× 2.92×

21.68%

3 × 1 and 1 × 3 convolution so that the optimized cuDNN
is not fully exploited. This will be a common issue in the
baseline, considering Winograd’s algorithm is universally
used and 3 × 3 convolution is one of the most common
structures. We ﬁnd that LRA in Fig. 3 can be utilized for
conv 3–5 to solve this issue, because it can maintain the
3 × 3 shape. We name this LRA as lra2, which decomposes
conv1–conv2 using LRA in [26] and conv 3–5 using LRA of
Fig. 3. The second row in Table 5 shows that our lra2 can
scale well to the hardware and software advances of “TI-
TAN 1080 + cuDNN 5.0”. More importantly, Force Regu-
larization on conv3–5 can enforce them to more lightweight
layers and attain higher speedup factors than lra2 without
using it. The result is shown in the third row, which in total
achieves 2.03× speedup for the whole convolution in GPU.
With small accuracy loss in row 4 of Table 5, Force Regu-
larization achieves 2.50× speedup of total convolution on
GPU and 4.05× on CPU.

Table 6 compares our method with state-of-the-art DNN
acceleration methods, in CPU mode. When the speedup of
total time was not reported by the authors, we estimate it
by the weighted average speedups over all layers, where
the weighting coefﬁcients are derived from the percent-
age of running time of each layer.
In our hardware plat-
form, conv1–conv5 respectively consume 15.89%, 28.25%,
24.32%, 18.70% and 12.84% testing time. The estimation
is accurate, for example, we estimate 2.58× of total time
in one-shot [14], which is very close to 2.52× reported by
the authors. Comparing with both cp-decomposition and
one-shot methods, our method can achieve higher accuracy
and higher speedup. Comparing with SSL, with almost the
same top-5 error (21.68% vs. 21.63%), we can attain higher
speedup of 4.05× vs. 3.13×.

deep-compression [7] reported 3× to 4× speedups in
fully-connected layers when batch size was 1. However,
convolution is the bottleneck of DNNs, e.g., the convolution
time in AlexNet is 5× of the time in fully-connected layers
when proﬁled in our CPU platform. Moreover, no speedup
was observed in the batching scenario as reported by the au-
thors [7]. More importantly, as we will show in Section 5.4,
our work can work together with sparsity-based methods
(e.g., SSL or deep-compression) to obtain lower-rank and
sparse DNNs and potentially further accelerate the testing
of DNNs.

Table 6. Comparison of speedup factor on AlexNet by state-of-the-art DNN acceleration methods.

Method

Top-5 err.

conv1

conv2

conv3

conv4

conv5

total

AlexNet in Caffe

19.97%

1.00× 1.00× 1.00× 1.00× 1.00× 1.00×

cp-decomposition [18]

20.97% (+1.00%)

–

4.00×

–

–

–

1.27×

one-shot [14]

21.67% (+1.70%)

1.48× 2.30× 3.84× 3.53× 3.13× 2.52×

SSL [28]

our lra2

19.58% (-0.39%)
21.63% (+1.66%)

1.00× 1.27× 1.64× 1.68× 1.32× 1.35×
1.05× 3.37× 6.27× 9.73× 4.93× 3.13×

20.14% (+0.17%)
21.68% (+1.71%)

2.61× 6.06× 2.48× 2.20× 1.58× 2.69×
2.65× 6.22× 4.81× 4.00× 2.92× 4.05×

5.4. Lower-rank and Sparse DNNs

We sparsify the lightweight deep neural network (i.e.,
the ﬁrst one of lra2 in Table 6), using Structured Spar-
sity Learning SSL [28] or non-structured connection-
pruning [23]. Note that Guided Sparsity Learning (GSL) is
not adopted in our connection-pruning though better spar-
sity is achievable when applying it. Figure 8 summarizes
the results.

Experiments prove that our method can work together
with both structured and non-structured sparsity methods to
further compress and accelerate models. Comparing with
deep-compression in Figure 8(a), our model has compara-
ble compression rates but 2.69× faster testing time. Typ-
ically, our model has higher compression rates in convo-
lutional layers, which provides more space for computation
reduction and generalizes better to modern DNNs (ResNets-
152 [10], for example, whose parameters in fc layers are
only 4%).
In Figure 8(b), our accelerated model can be
further accelerated using SSL. The shape-wise sparsity in

Figure 8. The results of sparsifying lightweight DNNs whose ﬁl-
ters are coordinated to a lower-rank space by Force Regularization.
In terms of deep-compression in (a), we only count the compres-
sion rate obtained from connection pruning for a fair comparison,
but quantization and Huffman coding can also be utilized to im-
prove the compression rate for our model. Based on SSL in (b), we
enforce shape-wise sparsity on conv3 s, conv4 s and conv5 s to
learn the shapes of basis ﬁlters meanwhile enforce ﬁlter-wise spar-
sity on conv3 f and conv4 f to learn the number of ﬁlters [28]. As
each convolutional layer in the lra2 is decomposed to two small
layers, we respectively denote the ﬁrst and second small layer by
sufﬁxing “ s” and “ f”. The baseline and our model have the same
accuracy.

conv3–5 of our model is slightly lower because our model is
already aggressively compressed by LRA. The higher ﬁlter-
wise sparsity, however, implies the orthogonality of our ap-
proach to SSL.

5.5. Generalization of Force Regularization

In convolutional layers, each ﬁlter basically extracts a
discriminative feature, e.g., an orientation-selective pattern
or a color blob in the ﬁrst layer [16] or a high-level fea-
ture (e.g., textures, faces, etc.) in deeper layers [29]. The
discrimination among ﬁlters is important for classiﬁcation
performance. Our method can coordinate ﬁlters for more
lightweight DNNs meanwhile maintain the discrimination.
It can also be generalized to learn more discriminative ﬁl-
ters to improve the accuracy. The extension to Discrimi-
nation Regularization is straightforward but effective: the
opposite gradient of Force Regularization (i.e., λs < 0) is
utilized to update the ﬁlter. In this scenario, it works as the
repulsive force to repel surrounding ﬁlters and enhance the
discrimination. Table 7 summarizes the improved accuracy
of state-of-the-art DNNs.

Acknowledgments

This work was supported in part by NSF CCF-1744082.
Any opinions, ﬁndings and conclusions or recommenda-
tions expressed in this material are those of the authors and
do not necessarily reﬂect the views of NSF or their contrac-
tors.

Table 7. Improved accuracy with Discrimination Regularization.

Net

Regularization

Top-1 error

AlexNet
AlexNet
AlexNet

None (baseline)
(cid:96)2-norm force
(cid:96)1-norm force

ResNets-20 None (baseline)
ResNets-20
(cid:96)2-norm force
ResNets-20
(cid:96)1-norm force

42.63%
41.71%
41.53%

8.82%
7.97%
8.02%

References

[1] J. M. Alvarez and M. Salzmann. Learning the number of
neurons in deep networks. In Advances in Neural Informa-
tion Processing Systems (NIPS), pages 2262–2270, 2016. 2
[2] C. Bauckhage. k-means clustering is matrix factorization.

arXiv:1512.07548, 2015. 2, 6

[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2009. 5

[4] M. Denil, B. Shakibi, L. Dinh, M. A. Ranzato, and N. de Fre-
itas. Predicting parameters in deep learning. In Advances in
Neural Information Processing Systems (NIPS). 2013. 1, 2,
3, 6

[5] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fer-
gus. Exploiting linear structure within convolutional net-
In Advances in Neural In-
works for efﬁcient evaluation.
formation Processing Systems (NIPS). 2014. 1, 2, 5, 6, 7
[6] Y. Guo, A. Yao, and Y. Chen. Dynamic network surgery for
efﬁcient dnns. In Advances in Neural Information Process-
ing Systems (NIPS). 2016. 1, 2

[7] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural network with pruning, trained quanti-
zation and huffman coding. arXiv:1510.00149, 2015. 1, 2,
7

[8] S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights
and connections for efﬁcient neural network. In Advances in
Neural Information Processing Systems (NIPS). 2015. 1, 2

[9] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In International Conference on Computer Vi-
sion (ICCV), 2015. 1

[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
In IEEE Conference on Computer

for image recognition.
Vision and Pattern Recognition (CVPR), 2016. 1, 5, 6, 8
[11] Y. Ioannou, D. P. Robertson, J. Shotton, R. Cipolla, and
A. Criminisi. Training cnns with low-rank ﬁlters for efﬁcient
image classiﬁcation. arXiv:1511.06744, 2015. 1, 2

[12] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up
convolutional neural networks with low rank expansions.
In Proceedings of the British Machine Vision Conference
(BMVC), 2014. 1, 2

[13] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. arXiv:1408.5093,
2014. 5

[14] Y.-D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin.
Compression of deep convolutional neural networks for fast
arXiv:1511.06530,
and low power mobile applications.
2015. 1, 2, 7, 8

[15] A. Krizhevsky and G. Hinton. Learning multiple layers of

features from tiny images. 2009. 5

[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
In
classiﬁcation with deep convolutional neural networks.
Advances in Neural Information Processing Systems (NIPS).
2012. 1, 2, 5, 8

[17] A. Lavin. Fast algorithms for convolutional neural networks.

arXiv:1509.09308, 2015. 7

[18] V. Lebedev, Y. Ganin, M. Rakhuba, I. Oseledets, and V. Lem-
pitsky. Speeding-up convolutional neural networks using
ﬁne-tuned cp-decomposition. arXiv:1412.6553, 2014. 1, 2,
8

[19] V. Lebedev and V. Lempitsky. Fast convnets using group-
wise brain damage. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2016. 1, 2

[20] Y. LeCun, J. S. Denker, S. A. Solla, R. E. Howard, and L. D.
Jackel. Optimal brain damage. In Advances in Neural In-
formation Processing Systems (NIPS), volume 2, pages 598–
605, 1989. 2

[21] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf.
Pruning ﬁlters for efﬁcient convnets. In International Con-
ference on Learning Representations (ICLR), 2017. 2
[22] B. Liu, M. Wang, H. Foroosh, M. Tappen, and M. Pensky.
Sparse convolutional neural networks. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2015.
1, 2, 5, 6

[23] J. Park, S. Li, W. Wen, P. T. P. Tang, H. Li, Y. Chen, and
P. Dubey. Faster cnns with direct sparse convolutions and
In International Conference on Learning
guided pruning.
Representations (ICLR), 2017. 1, 2, 8
[24] K. Simonyan and A. Zisserman.

Very deep con-
large-scale image recognition.

volutional networks for
arXiv:1409.1556, 2014. 1, 2

[25] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
In IEEE Conference on
Going deeper with convolutions.
Computer Vision and Pattern Recognition (CVPR), 2015. 1,
2, 6

[26] C. Tai, T. Xiao, X. Wang, and W. E. Convolutional neu-
ral networks with low-rank regularization. In International
Conference on Learning Representations (ICLR), 2016. 1, 2,
3, 6, 7

[27] P. Wang and J. Cheng. Accelerating convolutional neural
networks for mobile applications. In Proceedings of the 2016
ACM on Multimedia Conference, 2016. 1, 2

[28] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning
structured sparsity in deep neural networks. In Advances in
Neural Information Processing Systems (NIPS). 2016. 1, 2,
5, 8

[29] M. D. Zeiler and R. Fergus. Visualizing and understanding
convolutional networks. In European Conference on Com-
puter Vision (ECCV), 2014. 8

[30] X. Zhang, J. Zou, K. He, and J. Sun. Accelerating very
deep convolutional networks for classiﬁcation and detection.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 38(10):1943–1955, Oct 2016. 1, 2, 3, 5, 6

