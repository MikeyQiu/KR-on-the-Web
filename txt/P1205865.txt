7
1
0
2
 
t
c
O
 
1
3
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
9
9
4
4
0
.
1
1
6
1
:
v
i
X
r
a

Post Training in Deep Learning

Thomas Moreau

Julien Audiﬀren

CMLA, ENS Paris-Saclay, CNRS,
Universit´e Paris-Saclay,
94235 Cachan, France
{thomas.moreau, julien.audiffren}@cmla.ens-cachan.fr

November 1, 2017

Abstract

One of the main challenges of deep learning methods is the choice of an appropriate
training strategy. In particular, additional steps, such as unsupervised pre-training, have
been shown to greatly improve the performances of deep structures.
In this article, we
propose an extra training step, called post-training, which only optimizes the last layer of
the network. We show that this procedure can be analyzed in the context of kernel theory,
with the ﬁrst layers computing an embedding of the data and the last layer a statistical
model to solve the task based on this embedding. This step makes sure that the embedding,
or representation, of the data is used in the best possible way for the considered task.
This idea is then tested on multiple architectures with various data sets, showing that it
consistently provides a boost in performance.

1 Training Neural Networks

One of the main challenges of the deep learning methods is to eﬃciently solve the highly complex
and non-convex optimization problem involved in the training step. Many parameters inﬂuence
the performances of trained networks, and small mistakes can drive the algorithm into a sub-
optimal local minimum, resulting into poor performances [1]. Consequently, the choice of an
appropriate training strategy is critical to the usage of deep learning models.

The most common approach to train deep networks is to use the stochastic gradient descent
(SGD) algorithm. This method selects a few points in the training set, called a batch, and
compute the gradient of a cost function relatively to all the layers parameter. The gradient is
then used to update the weights of all layers. Empirically, this method converges most of the
time to a local minimum of the cost function which have good generalization properties. The
stochastic updates estimate the gradient of the error on the input distribution, and several works
proposed to use variance reduction technique such as Adagrap [3], RMSprop [9] or Adam [11],
to achieve faster convergence.

While these algorithms converge to a local minima, this minima is often inﬂuenced by the proper-
ties of the initialization used for the network weights. A frequently used approach to ﬁnd a good
starting point is to use pre-training [13, 8, 6]. This method iteratively constructs each layer using
unsupervised learning to capture the information from the data. The network is then ﬁne-tuned

1

using SGD to solve the task at hand. Pre-training strategies have been applied successfully to
many applications, such as classiﬁcation tasks [1, 18], regression [7], robotics [5] or information
retrieval [20]. The inﬂuence of diﬀerent pre-training strategies over the diﬀerent layers has been
thoroughly studied in Larochelle et al. [14]. In addition to improving the training strategies,
these works also shed light onto the role of the diﬀerent layers [4, 16]. The ﬁrst layers of a deep
neural network, qualiﬁed as general, tend to learn feature extractors which can be reused in other
architectures, independently of the solved task. Meanwhile, the last layers of the network are
much more dependent of the task and data set, and are said to be speciﬁc.

Deep Learning generally achieves better results than shallow structures, but the later are gener-
ally easier to train and more stable. For convex models such as logistic regression, the training
problem is also convex when the data representation is ﬁxed. The separation between the repre-
sentation and the model learning is a key ingredient for the model stability. When the represen-
tation is learned simultaneously, for instance with dictionary learning or with EM algorithms, the
problem often become non-convex. But this coupling between the representation and the model
is critical for end-to-end models. For instance, Hinton et al. [8] showed that for networks trained
using pre-training, the ﬁne-tuning step – where all the layers are trained together – improves the
performances of the network. This shows the importance of the adaptation of the representation
to the task in end-to-end models.

Our contribution in this chapter is an additional training step which improves the use of the
representation learned by the network to solve the considered task. This new step is called post-
training. It is based on the idea of separating representation learning and statistical analysis
and it should be used after the training of the network. In this step, only the speciﬁc layers
are trained. Since the general layers – which encode the data representation – are ﬁxed, this
step focuses on ﬁnding the best usage of the learned representation to solve the desired task.
In particular, we chose to study the case where only the last layer is trained during the post-
training, as this layer is the most speciﬁc one [23]. In this setting, learning the weights of the last
layer corresponds to learning the weights for the kernel associated to the feature map given by
the previous layers. The post-training scheme can thus be interpreted in light of diﬀerent results
from kernel theory. To summarize our contributions:

• We introduce a post-training step, where all layers except the last one are frozen. This
method can be applied after any traditional training scheme for deep networks. Note
that this step does not replace the end-to-end training, which co-adapts the last layer
representation with the solver weights, but it makes sure that this representation is used
in the most eﬃcient way for the given task.

• We show that this post-training step is easy to use, that it can be eﬀortlessly added to

most learning strategies, and that it is computationally inexpensive.

• We highlight the link existing between this method and the kernel techniques. We also
show numerically that the previous layers can be used as a kernel map when the problem
is small enough.

• We experimentally show that the post-training does not overﬁt and often produces im-

provement for various architectures and data sets.

The rest of this article is organized as follows: Section 2 introduces the post-training step and
discusses its relation with kernel methods. Section 4 presents our numerical experiments with
multiple neural network architectures and data sets and Section 5 discusses these results.

2

x

y

Features ΦL−1(x)

Weights WL

Figure 1: Illustration of post-training applied to a neural network. During the post-training,
only the weights of the blue edges are updated. The blue nodes can be seen as the embedding
of x in the feature space XL .

2 Post-training

In this section, we consider a feedforward neural network with L layers, where X1, . . . , XL denote
the input space of the diﬀerent layers, typically Rdl with dl > 0 and Y = XL+1 the output
space of our network. Let φl : Xl (cid:55)→ Xl+1 be the applications which respectively compute the
output of the l-th layer of the network, for 1 ≤ l ≤ L, using the output of the l−1-th layer and
ΦL = φL ◦ · · · ◦ φ1 be the mapping of the full network from X1 to Y . Also, for each layer l, we
denote WWW l its weights matrix and ψl its activation function.
The training of our network is done using a convex and continuous loss function (cid:96) : Y × Y (cid:55)→ R+.
The objective of the neural network training is to ﬁnd weights parametrizing ΦL that solves the
following problem:

E(x,y)∼P

min
ΦL

(cid:16)

(cid:20)
(cid:96)

(cid:17)(cid:21)

ΦL(x), y

.

for a certain input distribution P in (X1, Y). The training set is D = (cid:0)xi, yi
this input distribution.

(cid:1)N
i=1, drawn from

Using these notations, the training objective (1) can then be rewritten
(cid:19)(cid:35)

(cid:18)

(cid:34)
(cid:96)

(cid:16)
WWW LΦL−1(x)

(cid:17)

ψL

, y

.

min
ΦL−1,WWW L

E(x,y)∼P

(1)

(2)

This reformulation highlights the special role of the last layer in our network compared to the
others. When ΦL−1 is ﬁxed, the problem of ﬁnding WWW L is simple for several popular choices of
activation function ψL and loss (cid:96) . For instance, when the activation function ψL is the softmax
function and the loss (cid:96) is the cross entropy, (2) is a multinomial logistic regression. In this case,
training the last layer is equivalent to a regression of the labels y using the embedding of the data
x in XL by the mapping ΦL−1 . Since the problem is convex in WWW L (see Appendix A), classical
optimization techniques can eﬃciently produce an accurate approximation of the optimal weights
WWW L – and this optimization given the mapping ΦL−1 is the idea behind post-training.

Indeed, during the regular training, the network tries to simultaneously learn suitable repre-
sentation for the data in the space XL through its L − 1 ﬁrst layer and the best use of this
representation with WWW L. This joint minimization is a strongly non-convex problem, therefore
resulting in a potentially sub-optimal usage of the learned data representation.

The post-training is an additional step of learning which takes place after the regular training
and proceeds as follows :

3

1. Regular training: This step aims to obtain interesting features to solve the initial prob-
lem, as in any usual deep learning training. Any training strategy can be applied to the
network, optimizing the empirical loss

argmin
ΦL

1
N

N
(cid:88)

(cid:16)

i=1

(cid:96)

ΦL(xi), yi

.

(cid:17)

(3)

(4)

The stochastic gradient descent explores the parameter space and provides a solution for
ΦL−1 and WWW L. This step is non restrictive: any type of training strategy can be used
here, including gradient bias reduction techniques, such as Adagrad [3], or regularization
strategies, for instance using Dropout [2]. Similarly, any type of stopping criterion can be
used here. The training might last for a ﬁxed number of epochs, or can stop after using
early stopping [17]. Diﬀerent combinations of training strategies and stopping criterion are
tested in Section 4.

2. Post-training: During this step, the ﬁrst L − 1 layers are ﬁxed and only the last layer of

the network, φL, is trained by minimizing over WWW L the following problem

argmin
WWW L

1
N

N
(cid:88)

(cid:16)

˜(cid:96)

i=1

ΦL−1(xi)WWW T

L, yi

+ λ(cid:107)WWW L(cid:107)2

2 ,

(cid:17)

where ˜(cid:96)(x, y) := (cid:96)(ψL(x), y) . This extra learning step uses the mapping ΦL−1 as an embed-
ding of the data in XL and learn the best linear predictor in this space. This optimization
problem takes place in a signiﬁcantly lower dimensional space and since there is no need
for back propagation, this step is computationally faster. To reduce the risk of overﬁtting
with this step, a (cid:96)2-regularization is added. Figure 1 illustrates the post-training step.

We would like to emphasize the importance of the (cid:96)2-regularization used during the post-training
(4). This regularization is added regardless of the one used in the regular training, and for all
the network architectures. The extra term improves the strong convexity of the minimization
problem, making post-training more eﬃcient, and promotes the generalization of the model.
The choice of the (cid:96)2-regularization is motivated from the comparison with the kernel framework
discussed in Section 3 and from our experimental results.

Remark 1 (Dropout.). It is important to note that Dropout should not be applied on the previous
layers of the network during the post-training, as it would lead to changes in the feature function
ΦL−1.

3 Link with Kernels

In this section, we show that for the case where XL = RdL for some dL > 0 and XL+1 = R, WWW ∗
L
can be approximated using kernel methods. We deﬁne the kernel k as follows,

k : X1 × X1 (cid:55)→ R
(cid:68)

(x1, x2) →

ΦL−1(x1), ΦL−1(x2)

.

(cid:69)

4

(5)

(6)

(7)

Then k is the kernel associated with the feature function ΦL−1. It is easy to see that this kernel
is continuous positive deﬁnite and that for WWW ∈ RdL , the function

gWWW : X1 (cid:55)→ XL+1

(cid:68)

x →

ΦL−1(x), WWW

(cid:69)

belongs by construction to the Reproducing Kernel Hilbert Space (RKHS) Hk generated by k.
The post-training problem (4) is therefore related to the problem posed in the RKHS space Hk,
deﬁned by

g∗ = argmin
g∈Hk

1
N

N
(cid:88)

(cid:16)

˜(cid:96)

i=1

(cid:17)

g(xi), yi

+ λ(cid:107)g(cid:107)2

Hk

,

This problem is classic for the kernel methods. With mild hypothesis on ˜(cid:96), the generalized
representer theorem can be applied [21]. As a consequence, there exists α∗ ∈ RN such that

g∗ := argmin
g∈Hk

1
N

N
(cid:88)

(cid:16)

˜(cid:96)

i=1

(cid:17)

g(xi), yi

+ λ(cid:107)g(cid:107)2

Hk

=

N
(cid:88)

i=1

α∗

i k(Xi, ·) =

α∗

i ΦL−1

(cid:0)xi

(cid:69)
(cid:1) , ΦL−1(·)

.

N
(cid:88)

(cid:68)

i=1

Rewriting (6) with g∗ of the form (5), we have that g∗ = gWWW ∗ , with

WWW ∗ =

α∗

i ΦL−1

(cid:0)xi

(cid:1) ‘.

N
(cid:88)

i=1

We emphasize that WWW ∗ gives the optimal solution for the problem (6) and should not be confused
with WWW ∗
L , the optimum of (4). However, the two problems diﬀer only in their regularization,
which are closely related (see the next paragraph). Thus WWW ∗ can thus be seen as an approxima-
L. It is worth noting that in our experiments, WWW ∗ appears to be a
tion of the optimal value WWW ∗
nearly optimal estimator of WWW ∗

L (see Subsection 4.3).

Relation between (cid:107) · (cid:107)H and (cid:107) · (cid:107)2. The problems (6) and (4) only diﬀer in the choice of the
regularization norm. By deﬁnition of the RKHS norm, we have
(cid:110)

(cid:46)

(cid:111)

(cid:107)gW (cid:107)H = inf

(cid:107)v(cid:107)2

∀x ∈ X1,

(cid:104)v, ΦL−1(x)(cid:105) = gW (x)

.

Consequently, we have that (cid:107)gW (cid:107)H ≤ (cid:107)W (cid:107)2 , with equality when Vect(ΦL−1(X1)) spans the
entire space XL. In this case, the norm induced by the RKHS is equal to the (cid:96)2-norm. This
is generally the case, as the input space is usually in a far higher dimensional space than the
embedding space, and since the neural network structure generally enforces the independence of
the features. Therefore, while both norms can be used in (4), we chose to use the (cid:96)2-norm for
all our experiments as it is easier to compute than the RKHS norm.

Close-form Solution.
can be reduced to a classical Kernel Ridge Regression problem.
computed by combining (7) and

In the particular case where (cid:96)(y1, y2) = (cid:107)y1 − y2(cid:107)2 and f (x) = x, (6)
In this setting, W ∗ can be

(cid:16)

α∗ =

ΦL−1(D)TΦL−1(D) + λIII N

(cid:17)−1

Y ,

(8)

5

3

32

64

...

64

...

16

8

16

8

32

5x5 conv
max
pool
lrn

5x5 conv
lrn
max
pool

10

192

384

fc

fc

fc

Figure 2: Illustration of the neural network structure used for CIFAR-10. The last layer, repre-
sented in blue, is the one trained during the post-training. The layers are composed with classical
layers: 5x5 convolutional layers (5x5 conv), max pooling activation (max pool), local response
normalization (lrn) and fully connected linear layers (fc).

(cid:104)

(cid:105)
represents the matrix of the input data (cid:8)x1, . . . xN
(cid:9)
ΦL−1(x1), . . . ΦL−1(xN )
where ΦL−1(D) =
(cid:9) and III N is the identity matrix in
embedded in XL, Y is the matrix of the output data (cid:8)y1, . . . , yN
RN . This result is experimentally illustrated in Subsection 4.3. Although data sets are generally
too large for (8) to be computed in practice, it is worth noting that some kernel methods, such as
Random Features [19], can be applied to compute approximations of the optimal weights during
the post-training.

Multidimensional Output. Most of the previously discussed results related to kernel theory
hold for multidimensional output spaces, i.e. dim(XL+1) = d > 1, using multitask or operator
valued kernels [10]. Hence the previous remarks can be easily extended to multidimensional
outputs, encouraging the use of post-training in most settings.

4 Experimental Results

This section provides numerical arguments to study post-training and its inﬂuence on perfor-
mances, over diﬀerent data sets and network architectures. All the experiments were run using
python and Tensorflow. The code to reproduce the ﬁgures is available online1. The results of
all the experiments are discussed in depth in Section 5.

4.1 Convolutional Neural Networks

The post-training method can be applied easily to feedforward convolutional neural network,
used to solve a wide class of real world problems. To assert its performance, we apply it to three
classic benchmark datsets: CIFAR10 [12], MNIST and FACES [6].

1The code is available at https://github.com/tomMoral/post_training

6

Figure 3: Evolution of the performances of the neural network on the CIFAR10 data set, (dashed)
with the usual training and (solid) with the post-training phase. For the post-training, the value
of the curve at iteration q is the error for a network trained for q − 100 iterations with the regular
training strategy and then trained for 100 iterations with post-training. The top ﬁgure presents
the classiﬁcation error on the training set and the bottom ﬁgure displays the loss cost on the test
set. The curves have been smoothed to increase readability.

CIFAR10. This data set is composed of 60, 000 images 32 × 32, representing objects from
10 classes. We use the default architecture proposed by Tensorflow for CIFAR10 in our ex-
periments, based on the original architecture proposed by Krizhevsky [12]. It is composed of 5
layers described in Figure 2. The ﬁrst layers use various common tools such as local response
normalization (lrn), max pooling and RELU activation. The last layer have a softmax activation
function and the chosen training loss was the cross entropy function. The network is trained for
90k iterations, with batches of size 128, using stochastic gradient descent (SGD), dropout and an
exponential weight decay for the learning rate. Figure 3 presents the performance of the network
on the training and test sets for 2 diﬀerent training strategies. The dashed line present the classic
training with SGD, with performance evaluated every 100 iterations and the solid line present
the performance of the same network where the last 100 iterations are done using post-training
instead of regular training. To be clearer, the value of this curve at iteration q is the error of
the network, trained for q − 100 iterations with the regular training strategy, and then trained
for 100 iterations with post-training. The regularization parameter λ for post-training is set to
1 × 10−3.

The results show that while the training cost of the network mildly increases due to the use of
post-training, this extra step improves the generalization of the solution. The gain is smaller at
the end of the training as the network converges to a local minimum, but it is consistent. Also,
it is interesting to note that the post-training iterations are 4× faster than the classic iterations,

7

Table 1: Comparison of the performances (classiﬁcation error) of diﬀerent networks on diﬀerent
data sets, at diﬀerent epochs, with or without post-training.

Data set

Network

Iterations Mean (Std) Error in % Mean (Std) Error with

Small

Small

Large

FACES

Large

MNIST

5000
10000
20000
5000
10000
20000
1000
2000
5000
1000
2000
5000

due to their inexpensiveness.

21,5 (10)
20 (4)
18 (0,9)
25 (15)
15 (5)
11 (0,5)
10.7 (1)
7,5 (0,7)
4,1 (0,2)
9,1 (1,3)
4,1 (0,2)
1,1 (0,01)

post-training in %
19,1 (12)
19 (3,5)
16,5 (0,8)
24 (15)
12 (5)
10 (0,5)
9.2 (1,1)
6,7 (0,6)
3,9 (0,2)
8,5 (1,4)
3,5 (0,2)
0,9 (0,01)

Additional Data Sets. We also evaluate post-training on the MNIST data set (65000 images
27×27, with 55000 for train and 10000 for test; 10 classes) and the pre-processed FACES data set
(400 images 64×64, from which 102400 sub-images, 32×32, are extracted, with 92160 for training
and 10240 for testing; 40 classes). For each data set, we train two diﬀerent convolutional neural
networks – to assert the inﬂuence of the complexity of the network over post-training:

• a small network, with one convolutional layer (5 × 5 patches, 32 channels), one 2 × 2 max

pooling layer, and one fully connected hidden layer with 512 neurons,

• a large network, with one convolutional layer (5 × 5 patches, 32 channels), one 2 × 2 max
pooling layer, one convolutional layer (5 × 5 patches, 64 channels), one 2 × 2 max pooling
layer and one fully connected hidden layer with 1024 neurons.

We use dropout for the regularization, and set λ = 1 × 10−2. We compare the performance gain
resulting of the application of post-training (100 iterations) at diﬀerent epochs of each of these
networks. The results are reported in Table 1.

As seen in Table 1, post-training improves the test performance of the networks with as little
as 100 iterations – which is negligible compared to the time required to train the network.
While the improvement varies depending on the complexity of the network, of the data set, and
of the time spent training the network, it is important to remark that it always provides an
improvement.

8

Figure 4: Evolution of the performances of the Recurrent network on the PTB data set. The
top ﬁgure presents the train perplexity and the bottom ﬁgure displays the test perplexity. For
the post-training, the value of the curve at iteration q is the error for a network trained for
q − 100 iterations with the regular training strategy and then trained for 100 iterations with
post-training.

4.2 Recurrent Neural Network

While the kernel framework developed in Section 2 does not apply directly to Recurrent Neural
Network, the idea of post-training can still be applied. In this experiment, we test the perfor-
mances of post-training on Long Short-Term Memory-based networks (LSTM), using PTB data
set [15].

Penn Tree Bank (PTB). This data set is composed of 929k training words and 82k test
word, with a 10000 words vocabulary. We train a recurrent neural network to predict the next
word given the word history. We use the architecture proposed by Zaremba et al. [24], composed
of 2 layers of 1500 LSTM units with tanh activation, followed by a fully connected softmax layer.
The network is trained to minimize the average per-word perplexity for 100 epochs, with batches
of size 20, using gradient descent, an exponential weight decay for the learning rate, and dropout
for regularization. The performances of the network after each epoch are compared to the results
obtained if the 100 last steps (i.e. 100 batches) are done using post-training. The regularization
parameter for post-training, λ, is set to 1 × 10−3. The results are reported in Figure 4, which
presents the evolution of the training and testing perplexity.

Similarly to the previous experiments, post-training improves the test performance of the net-
works, even after the network has converged.

9

4.3 Optimal Last Layer for Deep Ridge Regression

In this subsection we aim to empirically evaluate the close-form solution discussed in Section 2
for regression tasks. We set the activation function of the last layer to be the identity fL(x) = x,
and consider the loss function to be the least-squared error (cid:96)(x, y) = (cid:107)x − y(cid:107)2
In in
each experiment, (8) and (7) are used to compute W ∗ for the kernel learned after the regular
training of the neural network, which learn the embedding ΦL−1 and an estimate WL . In order
to illustrate this result, and to compare the performances of the weights W ∗ with respect to the
weights WL, learned either with usual learning strategies or with post-training, we train a neural
network on two regression problems using a real and a synthetic data set. 70% of the data are
used for training, and 30% for testing.

2 in (1).

Real Data Set Regression. For this experiment, we use the Parkinson Telemonitoring data
set [22]. The input consists in 5, 875 instances of 17 dimensional data, and the output are
one dimensional real number. For this data set, a neural network made of two fully connected
hidden layers of size 17 and 10 with respectively tanh and RELU activation, is trained for 250,
500 and 750 iterations, with batches of size 50. The layer weights are all regularized with the
(cid:96)2-norm and a ﬁxed regularization parameter λ = 10−3 . Then, starting from each of the trained
networks, 200 iterations of post-training are used with the same regularization parameter λ and
the performances are compared to the closed-form solutions computed using (8) for each saved
network. The results are presented in Table 2.

Simulated Data Set Regression. For this experiment, we use a synthetic data set. The
inputs were generated using a uniform distribution on (cid:2)0, 1(cid:3)10
. The outputs are computed as
follows:

Y = tanh(XW1)W2

and W2 ∈ (cid:2)−1, 1(cid:3)5

where W1 ∈ (cid:2)−1, 1(cid:3)10×5
are randomly generated using a uniform law. In total,
the data set is composed of 10, 000 pairs (xi, yj). For this data set, a neural network with two
fully connected hidden layers of size 10 with activation tanh for the ﬁrst layer and RELU for the
second layer is trained for 250, 500 and 750 iterations, with batches of size 50. We use the same
protocol with 200 extra post-training iterations. The results are presented in Table 2.

For these two experiments, the post-training improves the performances toward these of the
optimal solution, for several choices of stopping times. It is worth noting that the performance
of the optimal solution is better when the ﬁrst layers are not fully optimized with Parkinson
Telemonitoring data set. This eﬀect denotes an overﬁtting tendency with the full training, where
the ﬁrst layers become overly speciﬁed for the training set.

5 Discussion

The experiments presented in Section 4 show that post-training improves the performances of
all the networks considered – including recurrent, convolutional and fully connected networks.
The gain is signiﬁcant, regardless of the time at which the regular training is stopped and the
post-training is done. In both the CIFAR10 and the PTB experiment, the gap between the losses

10

Table 2: Comparison of the performances (RMSE) of fully connected networks on diﬀerent data
sets, at diﬀerent epochs, with or without post-training.

Data set

Iterations

Parkinson

Simulated

250
500
750
250
500
750

Error with
classic training
0.832
0.147
0.134
1.185
0.533
0.322

Error with
post-training
0.434
0.147
0.132
1.117
0.450
0.300

Error with
optimal last layer
0.119
0.140
0.131
1.075
0.447
0.296

with and without post-training is more pronounced if the training is stopped early, and tends
to be smaller as the network converges to a better solution (see Figure 4 and Figure 3). The
reduction of the gap between the test performances with and without post-training is made clear
in Table 1. For the MNIST data set, with a small-size convolutional neural network, while the
error rate drops by 1.5% when post-training is applied after 5000 iterations, this same error rate
only drops by 0.2% when it is applied after 20000 iterations. This same observation can be done
for the other results reported in Table 1. However, while the improvement is larger when the
network did not fully converge prior to the post-training, it is still signiﬁcant when the network
has reached its minimum: for example in PTB the ﬁnal test perplexity is 81.7 with post-training
and 82.4 without; in CIFAR10 the errors are respectively 0.147 and 0.154.

If the networks are allowed to moderately overﬁt, for instance by training them with regular
algorithm for a very large number of iterations, the advantage provided by post-training vanishes:
for example in PTB the test perplexity after 2000 iterations (instead of 400) is 83.2 regardless of
post-training. This is coherent with the intuition behind the post-training: after overﬁtting, the
features learned by the network become less appropriate to the general problem, therefore their
optimal usage obtained by post-training no longer provide an advantage.

It is important to note that the post-training computational cost is very low compared to the full
training computations. For instance, in the CIFAR10 experiment, each iteration for post-training
is 4× faster on the same GPU than an iteration using the full gradient. Also, in the diﬀerent
experiments, post-training produces a performance gap after using as little as 100 batches. There
are multiple reasons behind this eﬃciency: ﬁrst, the system reaches a local minimum relatively
rapidly for post-training as the problem (4) has a small number of parameters compared to
the dimensionality of the original optimization problem. Second, the iterations used for the
resolution of (4) are computationally cheaper, as there is no need to chain high dimensional
linear operations, contrarily to regular backpropagation used during the training phase. Finally,
since the post-training optimization problem is generally convex, the optimization is guaranteed
to converge rapidly to the optimal weights for the last layer.

Another interesting point is that there is no evidence that the post-training step leads to overﬁt-
ting. In CIFAR10, the test error is improved by the use of post-training, although the training
loss is similar. The other experiments do not show signs of overﬁtting either as the test error is
mostly improved by our additional step. This stems from the fact that the post-training opti-
mization is much simpler than the original problem as it lies in a small-dimensional space – which,
combined with the added (cid:96)2-regularization, eﬃciently prevents overﬁtting. The regularization
parameter λ plays an important role in post-training. Setting λ to be very large reduces the ex-

11

planatory capacity of the networks whereas if λ is too small, the capacity can become too large
and lead to overﬁtting. Overall, our experiments highlighted that the post-training produces
signiﬁcant results for any choice of λ reasonably small (i.e 10−5 ≤ λ ≤ 10−2 ). This parameter
is linked to the regularization parameter of the kernel methods, as stated in Section 3.

Overall, these results show that the post-training step can be applied to most trained networks,
without prerequisites about how optimized they are since post-training does not degrade their
performances, providing a consistent gain in performances for a very low additional computa-
tional cost.

In Subsection 4.3, numerical experiments highlight the link between post-training and kernel
methods. As illustrated in Table 2, using the optimal weights derived from kernel theory im-
mediately a performance boost for the considered network. The post-training step estimate
numerically this optimal layer with the gradient descent optimizer. However, computing the
optimal weights for the last layer is only achievable for small data set due to the required ma-
trix inversion. Moreover, the closed form solution is known only for speciﬁc problems, such as
kernelized least square regression. But post-training approaches the same performance in these
cases solving (4) with gradient-based methods.

The post-training can be linked very naturally to the idea of pre-training, developed notably
by Larochelle et al. [13], Hinton et al. [8] and Hinton & Salakhutdinov [6]. The unsupervised
pre-training of a layer is designed to ﬁnd a representation that captures enough information
from the data to be able to reconstruct it from its embedding. The goal is thus to ﬁnd suitable
parametrization of the general layers to extract good features, summarizing the data. Conversely,
the goal of the post-training is, given a representation, to ﬁnd the best parametrization of the
last layer to discriminate the data. These two steps, in contrast with the usual training, focus
on respectively the general or speciﬁc layers.

6 Conclusion

In this work, we studied the concept of post-training, an additional step performed after the
regular training, where only the last layer is trained. This step is intended to take fully advantage
of the data representation learned by the network. We empirically shown that post-training is
computationally inexpensive and provide a non negligible increase of performance on most neural
network structures. While we chose to focus on post-training solely the last layer – as it is the
most speciﬁc layer in the network and the resulting problem is strongly convex under reasonable
prerequisites – the relationship between the number of layers frozen in the post-training and the
resulting improvements might be an interesting direction for future works.

12

References

[1] Yoshua Bengio and Yann LeCun. Scaling learning algorithms towards AI. Large-scale kernel

machines, 34(5):1–41, 2007.

[2] George E Dahl, Tara N Sainath, and Geoﬀrey E Hinton. Improving deep neural networks
for LVCSR using rectiﬁed linear units and dropout. In IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 8609–8613, Vancouver, Canada,
2013. IEEE.

[3] John Duchi, Elad Hazan, and Yoram Singer. Adaptive Subgradient Methods for Online
Learning and Stochastic Optimization. Journal of Machine Learning Research (JMLR), 12:
2121–2159, 2011. ISSN 15324435. URL http://jmlr.org/papers/v12/duchi11a.html.

[4] Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent,
and Samy Bengio. Why does unsupervised pre-training help deep learning? Journal of
Machine Learning Research (JMLR), 11(Feb):625–660, 2010.

[5] Raia Hadsell, Ayse Erkan, Pierre Sermanet, Marco Scoﬃer, Urs Muller, and Yann LeCun.
Deep belief net learning in a long-range vision system for autonomous oﬀ-road driving. In
IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 628–633. IEEE,
2008.

[6] Geoﬀrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with

neural networks. Science, 313(5786):504–507, 2006.

[7] Geoﬀrey E Hinton and Ruslan R Salakhutdinov. Using deep belief nets to learn covariance
In Advances in Neural Information Processing Systems

kernels for Gaussian processes.
(NIPS), pp. 1249–1256, Vancouver, Canada, 2008.

[8] Geoﬀrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep

belief nets. Neural computation, 18(7):1527–1554, 2006.

[9] Geoﬀrey E Hinton, Nitish Srivastava, and Kevin Swersky. Lecture 6a- overview of mini-batch
gradient descent. Slide for online class COURSERA: Neural Networks for Machine Learn-
ing, 2012. URL http://www.cs.toronto.edu/$\sim$tijmen/csc321/slides/lecture_
slides_lec6.pdf.

[10] Hachem Kadri, Emmanuel DUFLOS, Philippe Preux, Stephane canu, Alain Rakotoma-
monjy, and Julien Audiﬀren. Operator-valued Kernels for Learning from Functional Re-
sponse Data. Journal of Machine Learning Research (JMLR), 2015.

[11] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization.

In
International Conference on Learning Representation (ICLR), pp. 1–10, San Diego, CA,
USA, 2015.
ISBN 9781450300728. doi: http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/
1830483.1830503. URL http://arxiv.org/abs/1412.6980.

[12] Alex Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis,

University of Toronto, 2009.

[13] Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio.
An empirical evaluation of deep architectures on problems with many factors of variation.
In International Conference on Machine Learning (ICML), pp. 473–480, Corvallis, United
States, 2007. ACM.

13

[14] Hugo Larochelle, Yoshua Bengio, J´erˆome Louradour, and Pascal Lamblin. Exploring strate-
gies for training deep neural networks. Journal of Machine Learning Research (JMLR), 10
(Jan):1–40, 2009.

[15] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large
annotated corpus of English: The Penn Treebank. Computational linguistics, 19(2):313–
330, 1993.

[16] Gr´egoire Montavon, Mikio L Braun, and Klaus-Robert M¨uller. Kernel analysis of deep
networks. Journal of Machine Learning Research (JMLR), 12(Sep):2563–2581, 2011. ISSN
1532-4435.

[17] Nelson Morgan and Herv´e Bourlard. Generalization and parameter estimation in feedforward
nets: Some experiments. International Computer Science Institute, Denver, United States,
1990.

[18] Christopher Poultney, Sumit Chopra, Yann L Cun, and Others. Eﬃcient learning of sparse
representations with an energy-based model. In Advances in Neural Information Processing
Systems (NIPS), pp. 1137–1144, Vancouver, Canada, 2006.

[19] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines.

In
Advances in Neural Information Processing Systems (NIPS), pp. 1177–1184, Vancouver,
Canada, 2007.

[20] Ruslan Salakhutdinov and Geoﬀrey Hinton. Semantic hashing. International Journal of

Approximate Reasoning, 50(7):969–978, 2009.

[21] Bernhard Sch¨olkopf, Ralf Herbrich, and Alex J Smola. A generalized representer theo-
rem. In International Conference on Computational Learning Theory (COLT), pp. 416–426.
Springer, 2001.

[22] Athanasios Tsanas, Max A Little, Patrick E McSharry, and Lorraine O Ramig. Accurate
telemonitoring of Parkinson’s disease progression by noninvasive speech tests. IEEE Trans-
actions on Biomedical Engineering, 57(4):884–893, 2010.

[23] Jason Yosinski, Jeﬀ Clune, Yoshua Bengio, and Hod Lipson. How transferable are features
in deep neural networks? In Advances in Neural Information Processing Systems (NIPS),
pp. 3320–3328, Montreal, Canada, 2014. URL http://arxiv.org/abs/1411.1792.

[24] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regulariza-

tion. arXiv preprint, arXiv:1409(2329), 2014.

14

Post Training in Deep Learning
Supplementary materials

A Convex loss

We show here, for the sake of completeness, that the post-training problem is convex for the
softmax activation in the last layer and the cross entropy loss. This result is proved showing that
the hessian of the function is positive semideﬁnite, as it is a diagonally dominant matrix.
Proposition 2 (convexity). ∀N, M ∈ N, ∀X ∈ RN , ∀j ∈ (cid:2)1, M (cid:3), the following function F is
convex:

F :RN ×M (cid:55)→ R


W → log



exp(XWi)

 −

δij log

exp(XWi)

.

(cid:16)

(cid:17)

M
(cid:88)

i=1



M
(cid:88)

i=1

where δ is the Dirac function, and Wi denotes the i-th row of a W .

Proof 1. Let

then

Noting that

we have

hence

Pi(W ) =

exp(XWi)
j=1 exp(XWj)

.

(cid:80)M

∂Pi
∂Wm,n

=

− xnPi(W )Pm(W )
− xnP 2

m(W ) + xnPm(W )

if i (cid:54)= m

otherwise






F (W ) = −

δij log

Pi(W )

,

(cid:16)

(cid:17)

M
(cid:88)

i=1

∂F (W )
∂Wm,n


= −

M
(cid:88)

i=1

δij

1
Pi(W )

∂Pi
∂Wm,n


= xn



δijPm(W ) − δmj



M
(cid:88)

i=1

(cid:16)

= xn

Pm(W ) − δmj

(cid:17)

,

∂2F (W )
∂Wm,n∂Wp,q

(cid:32)

(cid:33)

,

∂Pm
∂Wp,q

= xn

(cid:16)

= xnxqPm(W )

δm,p − Pp(W )

.

(cid:17)

15

Hence the following identity

H(F ) = P(W ) ⊗ (XX T)

where ⊗ is the Kronecker product, and the matrix P(W ) is deﬁned by Pm,p = Pm(W )
Now since ∀1 ≤ m ≤ M ,

(cid:16)

δm,p − Pp(W )

(cid:17)

.

M
(cid:88)

p=1,p(cid:54)=m

(cid:12)
(cid:12)
(cid:12)Pm,p

(cid:12)
(cid:12)
(cid:12) = Pm(W )

M
(cid:88)

Pp(W )

(cid:16)

= Pm(W )

1 − Pm(W )

p=1,p(cid:54)=m
(cid:17)

= Pm,m

P(W ) is thus a diagonally dominant matrix. Its diagonal elements are positive

Pm,m = Pm(W )

1 − Pm(W )

≥ 0,

as Pm(W ) ∈ [0, 1]

(cid:16)

(cid:17)

and thus P(W ) is positive semideﬁnite. Since XX T is positive semideﬁnite too, their Kronecker
product is also positive semideﬁnite, hence the conclusion.

16

