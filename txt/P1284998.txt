Towards a quality metric for dense light ﬁelds

Vamsi Kiran Adhikarla1

Marek Vinkler1

Denis Sumin1

Rafał K. Mantiuk3

Karol Myszkowski1

Hans-Peter Seidel1

Piotr Didyk1,2

1MPI Informatik

2Saarland University, MMCI

3The Computer Laboratory, University of Cambridge

7
1
0
2
 
r
p
A
 
5
2
 
 
]

V
C
.
s
c
[
 
 
1
v
6
7
5
7
0
.
4
0
7
1
:
v
i
X
r
a

Abstract

Light ﬁelds become a popular representation of three-
dimensional scenes, and there is interest in their processing,
resampling, and compression. As those operations often
result in loss of quality, there is a need to quantify it. In
this work, we collect a new dataset of dense reference and
distorted light ﬁelds as well as the corresponding quality
scores which are scaled in perceptual units. The scores
were acquired in a subjective experiment using an interac-
tive light-ﬁeld viewing setup. The dataset contains typical
artifacts that occur in light-ﬁeld processing chain due to
light-ﬁeld reconstruction, multi-view compression, and lim-
itations of automultiscopic displays. We test a number of
existing objective quality metrics to determine how well they
can predict the quality of light ﬁelds. We ﬁnd that the existing
image quality metrics provide good measures of light-ﬁeld
quality, but require dense reference light- ﬁelds for optimal
performance. For more complex tasks of comparing two
distorted light ﬁelds, their performance drops signiﬁcantly,
which reveals the need for new, light-ﬁeld-speciﬁc metrics.

1. Introduction

A light ﬁeld can be seen as a generalization of a 2D image,
which encodes most of the depth cues and allows to render
a scene simulating arbitrary optics (e.g., defocus blur) [16].
It is a convenient representation for multiscotopic and light-
ﬁeld displays [43], but also attractive format for capturing
high-quality cinematographic content, offering new editing
possibilities in post-production [19]. Due the enormous stor-
age requirements, light ﬁelds are usually sparsely sampled in
spatial and angular dimensions, stored using lossy compres-
sion, and reconstructed later. It is unclear how the distortions
introduced on the way affect the perceived quality.

Similar problems have been addressed for 2D images, videos,
and sparse multiview content. Many quality metrics have
been designed to predict perceived differences between vari-

ous versions of the same content [1]. However, measuring
quality for dense light ﬁelds still remains a complex task.
While several works applied the existing metrics to such con-
tent [12, 8], their performance has never been systematically
evaluated in this context. One of the challenges is acquiring
dense light-ﬁeld data to validate a metric. Wide baselines
as in multi-camera rigs [44] need to be considered, and the
reference light ﬁelds should be sufﬁciently dense to avoid
uncontrolled visual artifacts. Obtaining human responses for
light-ﬁeld distortions is also difﬁcult due to current display
limitations. This work is an attempt to overcome these prob-
lems by ﬁrst building a new dense light-ﬁeld dataset which
is suitable for testing quality metrics, and second, using a
custom light-ﬁeld viewing setup to obtain the quality judg-
ments for this dataset. The collected subjective scores are
used to evaluate the performance of existing metrics in the
context of dense light ﬁelds.

We focus on light-ﬁeld-speciﬁc angular effects akin to mo-
tion parallax, complex surface appearance, and binocular
vision that arise in free viewing experience. To capture a
rich variability over these effects and make quality scaling
in our perceptual experiments tractable, we design fourteen
real and synthetic scenes and introduce light-ﬁeld distortions
that are speciﬁc to light-ﬁeld reconstruction, compression,
and display. We then run a pair-wise comparison experiment
over light-ﬁeld pairs, and derive perceptual scaling of differ-
ences between original and distorted stimuli. This allows us
to investigate the suitability of a broad spectrum of existing
image, video, and multiview quality metrics to predict such
perceptual scaling. We also propose simple extensions of
selected metrics to capture the angular aspects of light-ﬁeld
perception. While the original metrics are not meant for
light ﬁelds, our results show that they can be used in this
context, given a dense light ﬁeld as the reference. We also
demonstrate that the robustness of such metrics predictions
drops when evaluating the quality between two distorted
light ﬁelds. The main contributions of this work are:

• a publicly available dense light-ﬁelds dataset that is
designed for training and evaluating quality metrics;
• a perceptual experiment that provides human quality

judgments for several typical light-ﬁeld distortions;
• an evaluation, analysis, and extensions of existing qual-

ity metrics in the context of light ﬁelds;

• identiﬁed challenges of quality assessment in light
ﬁelds, such as the need for a high quality reference.

2. Previous works

In this section, we provide an overview of existing datasets
for light ﬁelds as well as the experiments that measure the
perceived distortions in various types of content.

Light-ﬁeld datasets: There are several publicly available
light-ﬁeld datasets. The most popular ones are: 4D light-
ﬁeld dataset
[42] containing seven synthetic scenes and
ﬁve real-world scenes, Stanford archive [7] with twenty 4D
light ﬁelds, and Disney 3D light-ﬁeld dataset [14] contain-
ing ﬁve scenes. Although the ﬁrst two datasets provide a
good quality and reasonable number of light ﬁelds, they are
captured over very narrow baselines that are insufﬁcient for
the new generation autostereoscopic displays. The Disney
dataset provides high spatio-angular resolution light ﬁelds;
however, they are few and do not have consistent spatial
and angular resolution, which makes it difﬁcult to use in
quality evaluations. In the context of quality evaluation of
3D light ﬁelds, three real-world light ﬁelds are provided in
the IRCCyN/IVC DIBR Images database [4]. These contain
several scenes captured along a wide baseline at the cost of
reduced angular resolution. Tamboli et al. [35] provided
360◦ round table shots of three scenes that are used for qual-
ity evaluation on a 3D light-ﬁeld display. These are rather
simple scenes with single objects and the images contain a
lot of noise. In our work, we provide ﬁrst consistent dataset
of dense, complex-scene light ﬁelds with large appearance
variation. We use the dataset for training and evaluating
quality metrics. The database cane also serve as a ground
truth for automultiscopic displays.

Metrics and experiments: Because of their proven efﬁ-
ciency on 2D images, 2D objective metrics are viable candi-
dates for evaluating light-ﬁeld quality. Yasakethu et al. [46]
tested the suitability of objective measures – Structural SIM-
ilarity (SSIM) [40], Peak Signal-to-Noise Ratio (PSNR) and
Video Quality Metric (VQM) [25] for quality assessment for
stereoscopic and 2D+Depth videos that are compressed at
different bitrates. They carried out subjective experiments on
an autostereoscopic display and showed that 2D metrics can
be used separately on each view to assess 3D video quality.
They used few sequences and studied only compression arti-
facts. Several metrics have been proposed to determine the
quality of synthesized views from multiview images. Bosc
et al. [4] advocated two measures for assessing the quality
of synthesized views. However, they did not conduct thor-
ough subjective studies. Solh et al. [34] presented a metric

for quantifying the geometric and photometric distortions in
multiview acquisition. Bosc et al. [3] suggested a method to
asses the quality of virtual synthesized views in the context
of multiview video. Battisti et al. [2] proposed more sophis-
ticated framework for evaluating the quality of depth image
based rendering techniques by comparing the statistical fea-
tures of wavelet subbands and used image registration and
skin detection steps for additional optimization. Sandic et
al. [30] exploited multi-scale pyramid decompositions with
morphological ﬁlters for obtaining the quality of intermedi-
ate views and showed that they achieve signiﬁcantly higher
correlation with subjective scores. These methods form a
class of metrics speciﬁc to view-interpolation artifacts, and
2D stimuli containing the interpolated views are used for
subjective experiments.

Vangorp et al. [38] ran a psychophysical study to account
for the plausibility of visual artifacts associated with view
interpolation methods. They considered such artifacts as a
function of different number of input images; however, they
limited their study to monocular viewing and Lambertian
surfaces. An experiment was also performed for precom-
puted videos, so that the impact of user’s interaction and
dynamic aspects of free viewing could be judged. More re-
cently, this work was extended to transitions between videos
[37]. Similar studies were also performed in the context of
panoramas [23]. Tamboli et al. [35] conducted subjective
studies on a 3D light-ﬁeld display. Users were asked to judge
the quality as perceived from different viewing locations in
front of the display and the scores were averaged over all
locations. The user could rate the quality only from a cer-
tain viewing position. Moreover, they only considered three
distinct scenes. We believe that, for inferring a light-ﬁeld
quality, all the views should be taken into account at the
same time.

Light-ﬁeld displays: Our work focuses on wide-baseline
3D light ﬁelds which enable perfect simulation of stereo-
scopic viewing and continuous horizontal motion parallax
crucial for new light-ﬁeld displays. Although many light-
ﬁeld display designs exist [22], including more advanced
ones that provide focus cues [20], they suffer from several
drawbacks such as limited ﬁeld of view, discontinous mo-
tion parallax, visible crosstalk, and limited depth budget.
Several strategies have been proposed to minimize these
artifacts by ﬁltering the content [47, 10] and manipulating
depth [15, 9, 22]. However, display designs that enable dis-
playing reference light ﬁelds for quality measurements are
still unavailable.

3. Data collection

Our dataset consist of light ﬁelds which are parameterized
using two parallel planes [16]. We consider only horizontal

Figure 1: Representative images of all light ﬁelds in our collection. Below each image representative EPIs are presented.

motion parallax that can be described using one plane and a
line that is parallel to it.

total disparity range during the presentation was limited to
0.2 visual degree [31].

More formally, we denote
our light ﬁelds as L(ω, x) ∈
(R × R2) → R3, where ω is
a position on the line, and
x is a position on the plane.
We refer to them as angu-
lar and spatial coordinates,
respectively. In practice, ω
describes a position of the viewer, and x is a coordinate of
the observed image. Below, we describe the acquisition of
our light ﬁelds.

3.1. Scenes

We designed and rendered nine synthetic and captured ﬁve
real-world scenes (Figure 1). They span a large variety of
different conditions, e.g. outdoor/indoor, daylight/night etc.
They also contain objects with large range of different ap-
pearance properties. The scene objects distribution in depth
is widely varied to study the artifacts resulting from disoc-
clusions and depth discontinuities. For capturing real-world
scenes, we used a one-meter long motorized linear stage
with Canon EOS 5D Mark II camera and 50 mm and 28 mm
lenses. After capturing all views, we performed lens dis-
tortion correction using PTLens [26], estimated the camera
poses using Voodoo camera tracker [39], and rectiﬁed all the
images using the baseline drawn from the ﬁrst to last camera
using the approach in [11]. For rendered images, we used
cameras with off-axis asymmetric frustums. For real-world
scenes, the same effect was achieved by applying horizontal
shift to the individual views. All the light ﬁelds are of iden-
tical spatial and angular resolution (960×720×101). The
angular resolution was chosen high enough to avoid visible
angular aliasing. This was achieved by assuring that the
maximum on-screen disparities between consecutive views
are around 1 pixel. To guarantee a comfortable viewing, the

3.2. Distortions

We considered typical light-ﬁeld distortions that are speciﬁc
to transmission, reconstruction, and display. For each dis-
tortion, we generated multiple light ﬁelds by varying the
distortion severity level. The exact levels were chosen to
keep the differences between two consecutive levels small
and similar. To this end, we conducted a small pilot study
with 10 distortion levels, and then, selected the ﬁnal levels
manually.

Transmission: To transmit the light-ﬁeld data, an efﬁcient
data compression algorithm is highly required. We consider
well-known 3D extension of HEVC encoder [36]. The light-
ﬁeld views are encoded into a bit stream at various quantiza-
tion steps, and then, decoded back from the bit stream using
the 3D-HEVC coder. We chose the following quantization
steps: {25, 29, 33, 37, 41, 45}.

Reconstruction: Light-ﬁeld reconstruction techniques are
used to recover a dense light ﬁeld from sparse view samples.
They interpolate the missing views using several techniques
which alter the nature and appearance of the distortion. We
chose the distortions resulting from linear (LINEAR) and
nearest neighbor (NN) interpolation, as well as image warp-
ing using optical ﬂow estimation (OPT). We also investigated
the impact of using quantized depth maps (DQ). All the dis-
tortions are parametrized by the angular subsampling factor
k (the distortion severity) that deﬁnes the angular resolution
of the light ﬁeld prior to applying the reconstruction tech-
nique. We considered k ∈ {2, 5, 8, 11, 18, 25}. The linear
ﬁlter reconstructs dense light ﬁeld by blending the refer-
ence views, and the NN method clones the closest reference
view. For OPT method, we used the TV-L1 optical ﬂow [29],
and apply an image-warping technique [5] to synthesize in-
between views. For DQ, we considered ground-truth depth
map which is quantized using 8 discrete levels. Then, we

used the same image-warping technique [5] to reconstruct
the light ﬁeld. As this distortion requires ground-truth depth
information, it is only applied to the synthetic scenes.

Display: As an example of multiview autostereoscopic dis-
play artifacts, we chose a crosstalk between adjacent views,
which can be modeled using a Gaussian blur in angular do-
main [18]. Consequently, we include such artifacts into our
dataset (GAUSS). In particular, we considered the same an-
gular subsampling parameters used in light-ﬁeld reconstruc-
tion distortions and created hypothetical displays with corre-
sponding number of views. The upsampling to higher reso-
lution light ﬁeld was achieved by using the display crosstalk
model.

Four different distortions with all severity levels were ap-
plied to every scene. To all synthetic scenes we apply NN,
LINEAR, OPT, and DQ. For all real-world scenes, we used
NN, OPT, GAUSS, and HEVC. Including original light
ﬁelds, our database consists of 350 different light ﬁelds and
it is available online [24]. The examples of resulting arti-
facts are presented in Figure 2. Please refer to supplemental
materials for the whole light-ﬁeld dataset.

Figure 2: Examples of distortions introduced to our light
ﬁeld for one of our scenes (BARCELONA). The images visu-
alize central EPI of each of the distorted light ﬁelds and the
enlarged portion of it is shown in the bottom row.

4. Experiment

To acquire subjective quality scores that enable both training
and testing different quality metrics, we performed a large-
scale subjective experiment.

pants to view light ﬁelds in an unconstrained manner. The
viewing distance was approximately 60 cm, and users could
move their heads along a baseline of 20 cm in the direction
parallel to the screen plane. The eye accommodation was
ﬁxed to the screen and did not change with eye vergence.
The display was operated at the full brightness to minimize
the effect of luminance on depth perception [9].

Figure 3: Experiment session: viewer’s position is tracked
using a head lamp and a webcam, a pair of NVIDIA 3D
Vision 2 Kit active glasses provides stereoscopic viewing.

Stimuli: Each stimuli was a pair of light ﬁelds. As our scal-
ing procedure used for obtaining quality scores (Section 5)
can handle an incomplete set of comparisons and prefers
when more comparisons are made for pairs of similar quality
[33], each pair consisted of light ﬁelds with neighboring
severity levels of the same distortion type. This results in
336 different stimuli which were presented stereoscopically.

Task: We experimented with direct rating methods, such as
ACR [13], in order to measure Mean-Opinion-Scores of the
distorted images. However, we found these methods to be in-
sensitive to subtle but noticeable degradation of quality. Also
participants found the direct rating task difﬁcult. Therefore,
we decided to use a more sensitive pair-wise comparison
method with a two-alternative-forced-choice. In each trial,
the participants were shown a pair of light ﬁelds side-by-side,
and the task was to indicate the light ﬁeld that a user “would
prefer to see on a 3-dimensional display”. Participants were
given unlimited time to investigate the light ﬁelds, but they
were allowed to give their response only after 80% of all
perspective images were seen. The order of the light-ﬁelds
pairs as well as their placement on the screen were random-
ized. Before each session, the participants were provided
with a form summarizing the task, and a training session was
conducted to familiarize participants with the experiment.

Equipment: To simulate stereoscopic viewing with high-
quality motion parallax, we used on our own setup (Figure 3)
that consists of ASUS VG278 27 (cid:48)(cid:48) Full HD 120 Hz LCD
desktop monitor and NVIDIA 3D Vision 2 Kit for displaying
stereoscopic images. Motion parallax was reproduced using
a custom head tracking in which a small LED headlamp was
tracked using a Logitech HD C920 Pro webcam (refer to the
supplemental video). The head tracking allowed the partici-

Participants: Forty participants took part in the test, includ-
ing both male (20) and female (20) aged 24–40 with normal
or corrected-to-normal vision. Each subject performed the
test in three sessions within one week. In one session, the
participants saw 120–180 light-ﬁeld pairs consisting of all
the test conditions, but for a subset of the scenes. For a given
subject, two test sessions were allowed during a single day,
and these were separated by at least an hour of break.

5. Analysis of subjective data

The results of pair-wise comparison experiment are usually
scaled in just-noticeable-differences (JNDs). We observed
that considering measured differences as “noticeable” leads
to incorrect interpretation of the experimental results. Two
stimuli are 1 JND apart if 75% of observers can see the dif-
ference between them. However, our experimental question
was not whether observers can tell if the light ﬁelds are dif-
ferent, but rather which one has higher quality. As shown
in Figure 4, a pair of stimuli could be noticeably different
from each other (JND>1), but they could appear to have the
same quality. For that reason, we denote measured values as
just-objectionable-differences (JODS). These units quantify
the quality difference in relation to the perfect reference im-
age. Note that the measure of JOD is more similar to visual
equivalence [28] or to the quality expressed as a difference-
mean-opinion-score (DMOS) rather than to JNDs.

Illustration of

the difference between just-
Figure 4:
objectionable-differences
(JODs) and just-noticeable-
differences (JNDs). The image affected by blur and noise
may appear to be similarly degraded in comparison to the
reference image (the same JOD), but they are noticeably
different and therefore several JNDs apart. The mapping be-
tween JODs and JNDs can be very complex and the relation
shown in this plot using Cartesian and polar coordinates is
just for illustration purposes.

To scale the results in JOD units we used a Bayesian method
based on the method of Silverstein and Farrell [33]. It em-
ploys a maximum-likelihood-estimator to maximize the prob-
ability that the collected data explains JOD-scaled quality
scores under the Thurstone Case V assumptions [27]. The
optimization procedure ﬁnds a quality value for each pair
of light ﬁelds that maximizes the likelihood modeled by the
binomial distribution. Unlike standard scaling procedures,
the Bayesian approach robustly scales pairs of conditions
for which there is unanimous agreement. Such pairs are
common when a large number of conditions are compared.
It can also scale the result of an incomplete and imbalanced
pair-wise design, when not all the pairs are compared and
some are compared more often. As the pair-wise compar-
isons provide relative quality information, the JOD values
are relative. To maintain consistency across the scenes, we
ﬁx the starting point of the JOD scale at 0 for different dis-

tortions and thus the quality degradation results in negative
JOD values.

The results of the subjective quality assessment experiment
are shown in Figure 5. The error bars represent 95% conﬁ-
dence intervals, relative to the reference light ﬁeld, computed
by bootstrapping by sampling with replacement. The results
show interesting patterns in the objectionability of different
distortions. OPT offers a consistent performance improve-
ment over NN. The only exception is the Furniture scene
featuring thin and irregularly shaped foreground objects, in
which case all types of view interpolation are more objec-
tionable than the selection of the nearest single view. The
optical ﬂow interpolation works better for real-world scenes
as there are more features that can be detected. The LIN-
EAR interpolation in most of the cases results in the worst
performance, except for small distortion levels, which may
indicate that visible blur due to this distortion is strongly
objectionable. Similar ﬁndings have been reported by Van-
gorp et al. [38] in their study on the visual performance of
view interpolation methods in monocular vision. HEVC
and GAUSS distortions are usually the easiest to detect as
they induce signiﬁcant amount of spatial distortion when
compared to others. Overall the results show clearly that
light-ﬁeld quality is scene-dependent and successful qual-
ity metric must predict the effect of scene content on the
visibility of light-ﬁeld distortions.

6. Evaluation of quality metrics

We considered several popular image, video, stereo, and
multiview quality metrics. We brieﬂy describe the metrics
and then show their individual performance on our dataset.
For obtaining the quality of a light ﬁeld using image quality
metrics, we apply the metrics on individual light-ﬁeld images
and then average the scores over all images.

Quality metrics: Although studies show that perceptual
metrics perform better than an absolute difference (AD) [17],
because of its signiﬁcant usage in image quality assessment,
we considered peak signal-to-noise ratio (PSNR). We also in-
vestigated SSIM2D [40], which is widely used on 2D images,
and its extensions to angular domains – SSIM2D×1D and
SSIM3D. SSIM3D computes the same statistics as standard
SSIM2D but on 3D patches extracted from the light-ﬁeld
volume. SSIM2D×1D uses 2D×1D patch which contains a
2D window extracted from a particular view and a 1D row
of pixels that extends from the center of the 2D window in
the angular domain (see Figure 6). We applied the metrics
to all light-ﬁeld images without resampling and averaged
the scores over all images. Although we experimented with
various pooling strategies, we found that the average value
performs best. Due to better performance, we chose the an-
gular window sizes of 32 and 64 pixels for SSIM2D×1D and

Figure 5: The results of the subjective quality assessment experiment. The distortion level indicates the distortion severity:
0–reference 6–severest distortion level. JOD is the scaled subjective quality value. The error bars denote 95% conﬁdence
interval. The bars are horizontally displaced to avoid overlapping. The scene names are indicated in the corner of each plot.
The character in parenthesis after the scene name indicates whether the scene is synthetic (S) or real-world (R).

SSIM3D respectively. We also considered a multi-scale ver-
sion of SSIM2D– MS-SSIM [41] which extends SSIM2D
to compute differences on multiple levels. We also used
GMSD [45] which provides good performance over a rich
collection of image datasets. The most advanced 2D metric
considered in our experiments was HDR-VDP-2 [21] which
stands out among perception-based quality metrics.

Figure 6: Patches used in our extensions of SSIM2D.

We further considered the NTIA General Model – VQM [25]
which was standardized for video-signals evaluation (ANSI
T1.801.03-2003). For this metric, light-ﬁeld images are input
in a form of video panning from the leftmost view to the
rightmost view and back. We also chose the stereoscopic
image quality metric – SIQM [6] that is based on the concept
of cyclopean image where, we averaged scores obtained
from all stereo pairs shown in our experiment. To capture
the full range of stereo quality metrics, we also included a
stereoscopic video quality metric STSDLC [32].

Finally, we chose metrics that address multiview data and
account for interpolation artifacts. 3DSWIM proposed by

Battisti et al. [2] ﬁrst shift-compensates blocks from the ref-
erence and distorted (interpolated) images. These matched
blocks undergo the ﬁrst level of Haar wavelet transform
and histogram of the sub-band corresponding to horizontal
details in the block is computed. Finally, the Kolmogorov-
Smirnov distance of these histograms is taken as the metric
prediction. Another metric for the multiview video is MP-
PSNR [30]. It computes the multi-resolution morphological
pyramid decomposition on the reference and test images.
Detail images of the top levels of these pyramids are then
compared through the mean squared error. The resulting per
pixel errors maps are then pooled and converted to a peak
signal-to-noise ratio measure.

6.1. Metric performance comparison

The quality values predicted by each metric are expected to
be related to JOD values, but this relation can be complex
and non-linear. To account for this relation, we follow a
common practice and ﬁt a logistic function:

q(o) = a1

(cid:26) 1
2

−

1
1 + exp [a2 (o − a3)]

(cid:27)

+ a4o + a5

(1)

where o is the output of a metric. The parameters a1..5 are
optimized to minimize a given goodness-of-ﬁt measure. We
computed several such measures, such as Spearman rank-
order correlation, or MSE, which can be found in the supple-
mentary materials. Here we report the reduced chi-squared

red) after cross-validation. The results for each cross-validation fold are shown. χ 2

Figure 7: The goodness-of-ﬁt scores for the metrics expressed as Pearson Correlation Coefﬁcient (ρ) and reduced chi-square
(χ 2
red = 1 indicates that the goodness of ﬁt
between the metric predictions and the subjective data is in perfect agreement with the measured subjective variance and
ρ = 1 indicates perfect positive linear relation between objective scores and JODs. The error bars represent standard error.

red) and Pearson correlation coefﬁcient (ρ). χ 2

statistic (χ 2
red is
computed as a weighted average of the squared differences,
in which weights are the inverse of sample variance. This ac-
counts for the fact that larger JOD values are more uncertain
(refer to Figure 5), and therefore, the accuracy of their pre-
diction can be lower. For a fair comparison, we employed a
seven fold cross-validation across different scenes. We mea-
sured the goodness-of-ﬁt on two randomly chosen scenes in
a cross-validation fold and averaged the results over all folds.
The resulting Pearson correlation and χ 2
red values are shown
in Figure 7. The performance of the metrics on individual
distortions are shown in Figure 8. A more elaborate analysis
including the evaluation on real-world and synthetic scenes
separately is presented in the supplementary materials.

The results show good performance of 2D image and video
quality metrics. This is unexpected as our dataset was meant
to emphasize visibility of angular artefacts, which are not
directly considered by these metrics. We observed, how-
ever, that angular distortions indirectly translate into the
differences in spatial patterns, which could explain the good
performance. We hypothesize that relatively better perfor-
mance of HDR-VDP-2 and GMSD is achieved by detecting
changes in contrast across multiple scales, which in case of
HDR-VDP-2 is additionally backed by perceptual scaling
of distortions and discarding of those that are invisible. A
comparable performance of video (VQM) and stereoscopic
(STSDLC) metrics can be explained by their emphasis on
the relation between neighboring views, which in some way
captures angular aspects of light-ﬁeld viewing. Figure 8
shows that some metrics are better at predicting some dis-
tortion types than the others. For example, HDR-VDP-
2 consistently under-predicts quality for HEVC. Training
such metrics for a particular distortion type could substan-
tially boost their performance. Unexpectedly, our ad hoc
attempts to extend the SSIM2D metric by adding the angular
dimension (SSIM3D) or right away considering 3D patches
(SSIM2D×1D) that should account for angular changes has

led to signiﬁcantly worse results. Clearly, there is a room
for improvements and a suitable dataset, such as the one
provided in this work, should help to develop a better metric
in future.

6.2. Sparse light-ﬁeld reference case

In all our tests, we provided a high quality, 101-view light
ﬁeld as a reference for the quality metrics. In practice, in
most applications only sparsely sampled light ﬁeld is un-
available. When a sparse light ﬁeld is used as a reference, a
full-reference metric is given to compare two distorted light
ﬁelds without a perfect reference. This is a task that such
metrics were not designed for as they are intended to pre-
dict JODs relative to the perfect reference image, not JNDs
relative to any other image (refer to Figure 4). This issue is
potentially shared with other quality assessment tasks, for
example when a metric is trained on 4K images, but it is used
on much lower resolution images. However, this problem
is exacerbated in case of light ﬁelds, where the reduction of
angular resolution is often substantial.

To test whether the metrics can predict the quality of dis-
torted light ﬁelds using sparse light ﬁelds as a reference, we
measured the performance of the metrics on a subset of our
dataset. As a reference, we chose light ﬁelds with distortion
NN and severity level two, which correspond to original light
ﬁelds subsampled to 21 angular views. For the testing light
ﬁeld, we considered all light ﬁelds with a higher distortion
levels. For a fair comparison, we also ran the metrics on the
same subset using full 101-view light ﬁelds as a reference.
The results of these tests are shown as cyan and blue bars in
Figure 9. The signiﬁcant difference in goodness-of-ﬁt scores
(marked with dots) show that metrics predictions get worse
if imperfect (sparse) reference is used. This suggests that
the existing metrics must be provided with a high-quality
reference light ﬁeld to predict reliably the quality.

Figure 8: Left: The prediction accuracy per-distortion reported as reduced chi-squared goodness of ﬁt score. Middle and
right: χ 2
red–ﬁt for the metrics HDR-VDP and GMSD over all scenes. The prediction accuracy for individual distortions are
shown inside the plots and the overall accuracy is indicated on the top of the plots.

image ﬂickering and jumping for NN. Our experiments re-
veal how these different artifacts affect perceived quality.
Our subjective scores are derived from an interactive 3D
light-ﬁeld viewing setup and correspond precisely to overall
quality of light ﬁelds rather than individual views. We have
evaluated the potential of existing image, video, stereo, and
multiview quality metrics in predicting the subjective scores.
Our observations show that the metrics – HDR-VDP-2,
GMSD, STSDLC and VQM perform reasonably well when
comparing a distorted light ﬁeld to a dense reference, and can
be used in applications requiring such comparisons. When
dense light ﬁeld is not available, which is the case in some ap-
plications, the usage of these metrics for quality assessment
is not justiﬁed. The perceptually scaled data that we pro-
vide can be used for training and validating new light-ﬁeld
quality metrics. Of practical interest for such development
is the problem identiﬁed in this work, where incomplete,
sparse light ﬁelds must serve as the reference. Our results
also reveal the quality of different light-ﬁeld reconstruction
method, which can directly guide the choice of the light-ﬁeld
reconstruction technique. In the current work, we did not
consider aspects such as masking properties of the human
visual system. It could be interesting to investigate how
much the metrics gain by considering this effect rather than
simple averaging of scores over all views. When creating
our dataset, we did not consider focus cue. We are, however,
not aware of any display setup that could be used to evaluate
both motion parallax and focus cue quality. We also believe
that the problems revealed in this work should be addressed
before including additional cues.

Figure 9: The goodness-of-ﬁt scores for the subset of the
dataset when a dense LF is used as a reference (blue), when
nearest-neighbour at the 2nd distortion level is a reference
(cyan), or when optical ﬂow is used to up-sample the ref-
erence LFs. The dots at cyan bars mean that the value is
statistically different from the dense LF case and the dots on
the yellow bars that the values are statistically different from
the NN case. The signiﬁcance is computed by bootstrapping
and running one-tailed test (p = 0.05).

But if such high quality reference is not available, can it
be approximated? Our subjective data shows that optical-
ﬂow interpolation (OPT) produces the highest quality results.
Therefore, we used OPT to produce reference 101-view light
ﬁelds from sparse 21-view light ﬁelds and reran the metrics
on the subset. The results indicate that the predictions im-
proved as compared to using sparse light ﬁeld (yellow vs.
cyan bars in Figure 9). This suggests that a potential solution
to the problem of imperfect reference is to use high-quality
interpolation method in order to generate reference.

7. Conclusions and future work

We have established a new 3D dense light-ﬁeld dataset to-
gether with the subjective quality scaling for various distor-
tions that occur in light-ﬁeld applications. Different methods
in light-ﬁeld processing lead to visual artifacts with quite dif-
ferent appearance, e.g., blur for LINEAR, ghosting for OPT,

Acknowledgements: This project was supported by the
Fraunhofer and Max Planck cooperation program within
the German pact for research and innovation (PFI). De-
nis Sumin was supported by the European Union’s Horizon
2020 research and innovation programme under the Marie
Sklodowska-Curie grant agreement No 642841. The authors
would like to thank Tobias Ritschel for the initial discussions
and providing synthetic scenes.

References

[1] T. O. Aydin, M. ˇCadík, K. Myszkowski, and H.-P. Seidel.
Video quality assessment for computer graphics applications.
In ACM Trans. Graph., volume 29, page 161, 2010. 1

[2] F. Battisti, E. Bosc, M. Carli, P. L. Callet, and S. Perugia. Ob-
jective image quality assessment of 3D synthesized views. Sig-
nal Processing: Image Communication, 30(C):78–88, 2015.
2, 6

[3] E. Bosc, F. Battisti, M. Carli, and P. Le Callet. A wavelet-
based image quality metric for the assessment of 3D synthe-
sized views. In Proc. SPIE, volume 8648, pages 86481Z–
86481Z–9, 2013. 2

[4] E. Bosc, R. Pépion, P. Le Callet, M. Köppel, P. Ndjiki-Nya,
M. Pressigout, and L. Morin. Towards a new quality metric for
3-D synthesized view assessment. IEEE Journal on Selected
Topics in Signal Processing, pages J–STSP–ETVC–00048–
2011, Nov. 2011. 2

[5] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High ac-
curacy optical ﬂow estimation based on a theory for warping.
In Comput. Vis.-ECCV 2004, pages 25–36. Springer, 2004. 3
[6] M.-J. Chen, C.-C. Su, D.-K. Kwon, L. K. Cormack, and
A. C. Bovik. Full-reference quality assessment of stereopairs
accounting for rivalry. Signal Processing: Image Communi-
cation, 28(9):1143–1155, 2013. 6

[7] Computer Graphics Laboratory, Stanford University. The
(new) Stanford light ﬁeld archive. http://lightfield.
stanford.edu/acq.html, 2008. Accessed: 2016-04-23. 2
[8] D. G. Dansereau, D. L. Bongiorno, O. Pizarro, and S. B.
Williams. Light ﬁeld image denoising using a linear 4d
frequency-hyperfan all-in-focus ﬁlter. In Proceedings of the
SPIE Conference on Computational Imaging (SPIE’13), vol-
ume 8657, 2013. 1

[9] P. Didyk, T. Ritschel, E. Eisemann, K. Myszkowski, H.-P.
Seidel, and W. Matusik. A luminance-contrast-aware disparity
model and applications. ACM Trans. Graph., 31(6):184, 2012.
2, 4

[10] S.-P. Du, P. Didyk, F. Durand, S.-M. Hu, and W. Matusik. Im-
proving visual quality of view transitions in automultiscopic
displays. ACM Trans. Graph., 33(6):192, 2014. 2

[11] A. Fusiello, E. Trucco, and A. Verri. A compact algorithm for
rectiﬁcation of stereo pairs. Mach. Vision Appl., 12(1):16–22,
July 2000. 3

[12] R. S. Higa, R. F. L. Chavez, R. B. Leite, R. Arthur, and
Y. Iano. Plenoptic image compression comparison between
jpeg, jpeg2000 and spith. Cyber Journals: JSAT, 3(6), 2013.
1

[13] ITU-T-P.910. Subjective audiovisual quality assessment meth-

ods for multimedia applications. Technical report, 2008. 4

[14] C. Kim, H. Zimmer, Y. Pritch, A. Sorkine-Hornung, and
M. Gross. Scene reconstruction from high spatio-angular
resolution light ﬁelds. ACM Trans. Graph., 32(4):73:1–73:12,
July 2013. 2

[15] M. Lang, A. Hornung, O. Wang, S. Poulakos, A. Smolic, and
M. Gross. Nonlinear disparity mapping for stereoscopic 3D.
ACM Trans. Graph., 29(4):75:1–75:10, July 2010. 2

and Interactive Techniques, pages 31–42, New York, NY,
USA, 1996. ACM. 1, 2

[17] W. Lin and C.-C. J. Kuo. Perceptual visual quality metrics:
A survey. J. Vis. Commun. Image Represent., 22(4):297–312,
2011. 5

[18] J. Liu, T. Malzbender, S. Qin, B. Zhang, C.-A. Wu, and
J. Davis. Dynamic mapping for multiview autostereoscopic
displays. In Proc. SPIE, vol. 9391, pages 1I:1–1I:8, 2015. 4
[19] Lytro. Lytro cinema. https://www.lytro.com/cinema,

2016. Accessed: 2016-15-11. 1

[20] A. Maimone, G. Wetzstein, D. Lanman, M. Hirsch, R. Raskar,
and H. Fuchs. Focus 3D: Compressive accommodation dis-
play. ACM Trans. Graph., 32(5):1–13, 2013. 2

[21] R. Mantiuk, K. J. Kim, A. G. Rempel, and W. Heidrich. HDR-
VDP-2: a calibrated visual metric for visibility and quality
predictions in all luminance conditions. ACM Trans. Graph.,
30(4):40:1–40:12, 2011. 6

[22] B. Masia, G. Wetzstein, P. Didyk, and D. Gutierrez. A survey
on computational displays: Pushing the boundaries of optics,
computation, and perception. Comput. Graph., 37(8):1012–
1038, 2013. 2

[23] Y. Morvan and C. O’Sullivan. Handling occluders in tran-
sitions from panoramic images: A perceptual study. ACM
Trans. Appl. Percept., 6(4):1–15, 2009. 2
Light-ﬁeld archive.

http://lightfields.
mpi-inf.mpg.de/Dataset.html, 2017. Accessed: 2017-
07-04. 4

[24] MPI.

[25] M. H. Pinson and S. Wolf. A new standardized method for
objectively measuring video quality. IEEE Transactions on
Broadcasting, 50(3):312–322, 2004. 2, 6

[26] PTLens. Lens distortion correction software. http://www.
epaperpress.com/ptlens/, 2016. Accessed: 2016-15-11.
3

[27] Rafał K. Mantiuk. Thurstonian scaling for pair-wise compari-
son experiments. https://github.com/mantiuk/pwcmp,
2016. Accessed: 2016-15-11. 5

[28] G. Ramanarayanan, J. Ferwerda, and B. Walter. Visual equiv-
alence: towards a new standard for image ﬁdelity. ACM
Transactions on Graphics (TOG), 26(3):76, 2007. 5

[29] J. Sánchez Pérez, E. Meinhardt-Llopis, and G. Facciolo. TV-
L1 optical ﬂow estimation. Image Processing On Line, 3:137–
150, 2013. 3

[30] D. Sandic-Stankovic, D. Kukolj, and P. Le Callet. Multi-
scale synthesized view assessment based on morphological
pyramids. Journal of Electrical Engineering, 67(1):3–11,
2016. 2, 6

[31] T. Shibata, J. Kim, D. M. Hoffman, and M. S. Banks. The
zone of comfort: Predicting visual discomfort with stereo
displays. J. Vis., 11(8):11:1–11:29, 2011. 3

[32] V. D. Silva, H. K. Arachchi, E. Ekmekcioglu, and A. Kon-
doz. Toward an impairment metric for stereoscopic video:
A full-reference video quality metric to assess compressed
stereoscopic video. IEEE Transactions on Image Processing,
22(9):3392–3404, Sept 2013. 6

[16] M. Levoy and P. Hanrahan. Light ﬁeld rendering. In Proceed-
ings of the 23rd Annual Conference on Computer Graphics

[33] D. Silverstein and J. Farrell. Efﬁcient method for paired
comparison. J. Electron. Imaging, 10(2):394–398, 2001. 4, 5

[34] M. Solh and G. AlRegib. MIQM: A novel multi-view images
quality measure. In Quality of Multimedia Experience, 2009.
QoMEx 2009. International Workshop on, pages 186–191,
July 2009. 2

[35] R. R. Tamboli, B. Appina, S. Channappayya, and S. Jana.
Super-multiview content with high angular resolution: 3D
quality assessment on horizontal-parallax lightﬁeld display.
Signal Processing: Image Communication, 47:42 – 55, 2016.
2

[36] G. Tech, Y. Chen, K. MÜller, J. R. Ohm, A. Vetro, and Y. K.
Wang. Overview of the multiview and 3D extensions of high
efﬁciency video coding. IEEE Transactions on Circuits and
Systems for Video Technology, 26(1):35–49, Jan 2016. 3
[37] J. Tompkin, M. H. Kim, K. I. Kim, J. Kautz, and C. Theobalt.
Preference and artifact analysis for video transitions of places.
ACM Trans. Appl. Percept., 10(3):13:1–19, 2013. 2

[38] P. Vangorp, G. Chaurasia, P.-Y. Laffont, R. W. Fleming, and
G. Drettakis. Perception of visual artifacts in image-based
rendering of façades. Comput. Graph. Forum, 30(4):1241–
1250, 2011. 2, 5

[39] Viscoda. Voodoo Camera Tracker. http://www.viscoda.
com/en/voodoo-download, 2016. Accessed: 2016-15-11.
3

[40] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.
Image quality assessment: from error visibility to structural
similarity. IEEE Trans. Image Process., 13(4):600–612, 2004.
2, 5

[41] Z. Wang, E. P. Simoncelli, and A. C. Bovik. Multiscale
structural similarity for image quality assessment. In Signals,
Systems and Computers, 2004. Conference Record of the
Thirty-Seventh Asilomar Conference on, volume 2, pages
1398–1402 Vol.2, Nov 2003. 6

[42] S. Wanner, S. Meister, and B. Goldluecke. Datasets and
benchmarks for densely sampled 4d light ﬁelds. 2013. 2
[43] G. Wetzstein, D. Lanman, M. Hirsch, and R. Raskar. Tensor
displays: Compressive light ﬁeld synthesis using multilayer
displays with directional backlighting. ACM Trans. Graph.,
31(4):1–11, 2012. 1

[44] B. Wilburn, N. Joshi, V. Vaish, E.-V. Talvala, E. Antunez,
A. Barth, A. Adams, M. Horowitz, and M. Levoy. High
performance imaging using large camera arrays. ACM Trans.
Graph., 24(3):765–776, 2005. 1

[45] W. Xue, L. Zhang, X. Mou, and A. C. Bovik. Gradient
magnitude similarity deviation: A highly efﬁcient perceptual
image quality index. IEEE Transactions on Image Processing,
23(2):684–695, Feb 2014. 6

[46] S. L. P. Yasakethu, C. T. E. R. Hewage, W. A. C. Fernando,
and A. M. Kondoz. Quality analysis for 3D video using
2d video quality models. IEEE Transactions on Consumer
Electronics, 54(4):1969–1976, November 2008. 2

[47] M. Zwicker, W. Matusik, F. Durand, and H. Pﬁster. Antialias-
ing for automultiscopic 3D displays. In Proc. of EGSR, pages
73–82, 2006. 2

