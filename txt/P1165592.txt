7
1
0
2
 
t
c
O
 
0
2
 
 
]
L
C
.
s
c
[
 
 
1
v
9
2
7
7
0
.
0
1
7
1
:
v
i
X
r
a

Is space is a word, too?

Jake Ryland Williams1, ∗ and Giovanni C. Santia1, †
1 Department of Information Science, College of Computing and Informatics,
Drexel University, 30 N. 33rd St. Philadelphia, PA 19104
(Dated: October 24, 2017)

For words, rank-frequency distributions have long been heralded for adherence to a potentially-
universal phenomenon known as Zipf’s law. The hypothetical form of this empirical phenomenon
was reﬁned by Benˆıot Mandelbrot to that which is presently referred to as the Zipf-Mandelbrot
law. Parallel to this, Herbert Simon proposed a selection model potentially explaining Zipf’s law.
However, a signiﬁcant dispute between Simon and Mandelbrot, notable empirical exceptions, and the
lack of a strong empirical connection between Simon’s model and the Zipf-Mandelbrot law have left
the questions of universality and mechanistic generation open. We oﬀer a resolution to these issues
by exhibiting how the dark matter of word segmentation, i.e., space, punctuation, etc., connect the
Zipf-Mandelbrot law to Simon’s mechanistic process. This explains Mandelbrot’s reﬁnement as no
more than a fudge factor, accommodating the eﬀects of the exclusion of the rank-frequency dark
matter. Thus, integrating these non-word objects resolves a more-generalized rank-frequency law.
Since this relies upon the integration of space, etc., we ﬁnd support for the hypothesis that all are
generated by common processes, indicating from a physical perspective that space is a word, too.

PACS numbers: 89.75.-Da, 02.50.Ey, 89.70.-a, 89.75.-Fb

Where does Zipf’s law come from? This potentially
universal linguistic phenomenon is simple to express, but
its origins have a large diversity of explanations [1–6].
Its applicability and the methods for its determination
remain subjects of discussion [7–10], which have coincid-
ed with observations of its variation [11] and gross devi-
ations [12]. We have devoted signiﬁcant eﬀort to under-
standing these breaches of universality; the granularity
of linguistic units (e.g., words vs. phrases) [13] and the
scales of corpora (e.g, documents vs. compilations) [14]
impact the measurable properties of Zipf’s law. Zipf’s
law has even been observed to describe the frequencies of
punctuation along with words [15]; surprisingly, it may
describe whitespace, too.

One possible generation mechanism also underpins
the preferential attachment concept from complex net-
works [16, 17], and traces its roots to the study of evolu-
tion [18]. This linguistic selection model was proposed by
Herbert Simon [3] and has been the subject of our direct
focus [19], exhibiting its prediction of a scaling outlier
at the ﬁrst rank of Zipf’s law. This property presents
an unforeseen opportunity to explain the phenomenon.
If the scaling outlier is observed, Simon’s model has a
smoking gun behind Zipf’s law as the only mechanism
known to generate the ﬁrst mover. In the proceedings of
this work, we thus show how this preferential selection
mechanism is at the heart of Zipf’s phenomenon.

For a text of R distinct words: w1, · · · , wR; ranked,
r = 1, · · · , R, descending, according to their frequencies
of occurrence: f (w1) ≥ · · · f (wR), Zipf’s law is a power-
law distribution:

ˆf Z(wr) ∝ r−γ;

γ ≥ 0,

(1)

which, in canonical form, has scaling exponent: γ = 1.
Empirical studies of rank-frequency distributions have

Typeset by REVTEX

exhibited variation in γ [11, 20]; it is not uncommon for
γ to be greater or less than 1, but γ is always greater
than 0 as a result of the rank ordering. Following Zipf,
Benoˆıt Mandelbrot observed a common tendency of rank-
frequency distributions to roll back, falling beneath Zipf’s
law at the highest ranks. To suit, Mandelbrot proposed a
horizontal translation, k, as a reﬁnement of Zipf’s law [2],
rooting it in the optimization of communication against
the cost of transmission. This reﬁnement by k is com-
monly accepted in the modern incarnation of Zipf’s law
and is now empirically investigated along with γ [9]. The
form of this Zipf-Mandelbrot (ZM) law is:
ˆf ZM(wr) ∝ (r + k)−γ;

γ ≥ 0, k > −1.

(2)

Seeking to understand the ZM law, we are confront-
ed with the question: why would a translation of ranks
result in a more-accurate model? For Zipf’s or the ZM
law, ranks more or less describe the number of words of
similar and more-severe frequency. So, accepting a Man-
delbrot translation of 2.75 [9], should we conclude that
there generally exist between two and three mysterious
words of greater frequency than the largest observed? If
so, what are these words? To approach these questions
we must ﬁrst deﬁne what words are.

Approaches to deﬁning words for a rank-frequency
study include criteria like things separated by space and
accepted dictionary entries. Studies handle case sensi-
tivity diﬀerently, and have even focused on the eﬀects
of lemmatization [21].
In this work, we choose not to
modify case, do not lemmatize, and move forward with
the criterion that a word has at least one word charac-
ter to encompass a large number of word-like objects.
We deﬁne the set of word characters as alphanumerics,
their diacritic-modiﬁed forms,
ligatures, and any oth-
er characters not used as punctuation from across all

2

electronically-encoded natural languages (although our
focus will be on English). Thus, we allow whitespace and
all known punctuation characters to act as word delim-
iters, with the exception of contractions, whose compo-
nents’ words are separated in accordance to known rules,
such as: {don’tcha} (cid:55)→ {do, n’t, cha}.

Returning to candidates for Mandelbrot’s two to three
mysteriously-missing objects, the process of deﬁning
words has left a glaring possibility. While semantic rich-
ness has made words the historical focus of study, this
bias has hidden non-words—delimiters—away as a lin-
guistic dark matter. Why is this conventional? Space and
punctuation, etc., may act as word delimiters, but these
objects have measurable frequencies, too. Mechanistical-
ly, hand writing relegates the placement of space to a
non-action, but typewriting, and now electronic encod-
ing, have made the insertion of space into an active com-
ponent of the language-recording process.

So, how do our non-words integrate as components of
rank-frequency analyses? Does the number of non-word
objects ranking higher than the ﬁrst word explain Man-
delbrot’s shift? Speciﬁc quantities actually vary from
text to text, but there is generally one non-word that
takes rank 1 for English. Perhaps unsurprisingly, this
object is space, which is often followed at rank 2 by com-
ma. An intuition for rank, frequency, and the word/non-
word dichotomy may be gained by observing how ranks
map between our integrated and the conventional rank-
frequency representation. Fig. 1 exhibits how rank-
frequency distributions roll back when non-word objects
are excluded. Is this transition related to Mandelbrot’s
k? While the removal of non-words results in a trans-
lation in the correct direction, Mandelbrot’s k is deﬁned
constant. This exists in contrast to the shift by non-word
deletion, which is variable and increasing. However, we
may still experiment to determine if the two are relat-
ed. Provided a relationship is uncovered, Mandelbrot’s
k may be no more than a constant, fudge factor that
partially accounts for the missing non-words.

To understand any relationship between the shift by
non-word deletion and Mandelbrot’s k, we need a method
for k’s measurement. While the ZM law has a simple
functional form, power-law regression is notably diﬃcult.
Accepted methodology for the determination of expo-
nents centers around maximum likelihood estimation [8],
although a number of other methodologies exist [21–23].
However, we are not immediately interested in exponents
or frequencies, but instead, ranks. Furthermore, opti-
mization of γ can obfuscate variation in k; γ is well-
known to be non-constant across ranks [10, 12, 14, 24],
and is diﬃcult to measure with precision [8]. For these
reasons, we ﬁx γ = 1 and switch to the rank domain
for regression—the natural domain for k. While ﬁxing
γ imposes bias in the measurement of k, it does so in a
consistent way, allowing us to precisely understand k’s
variation, which is all that is necessary for our study.

FIG. 1: The translation of ranks for A Tale of Two Cities,
by Charles Dickens. When both words and non-words are
ranked by frequency (upper curve) the highest-frequency
words emerge at rank 3—here, after space, and comma. When
non-words are removed from the frequency ranking, all words
are translated left (lower curve). This translation is highlight-
ed in the inset zoom, where kept words move back along the
red-dashed lines. Given the upper curve has a rank-frequency
power law like Zipf’s (Eq. 1), the deletion of non-words (open
circles) most signiﬁcantly aﬀects the lowest-ranked/highest-
frequency words (solid circles), rolling back to the solid trian-
gles, potentially related to Mandelbrot’s rank translation.

With γ = 1, a quotient of Zipf’s law takes a convenient
form: ˆyZ = ˆfZ(w1)/ ˆfZ(wr) = r. For the ZM law, the
quotient becomes: ˆyZM(wr) = (r + k)/(1 + k). Deﬁning
y(wr) = f (w1)/f (wr) for all r, the sum of squared errors:
(cid:80)R

r=1(y(wr) − ˆyZM(wr))2 has analytic minimizer:

ˆk =

(cid:80)R

r=1(r − 1)2
r=1(r − 1)(y(wr) − 1)

.

(cid:80)R

(3)

This regression is convenient to our study for its analytic
optimization. Not only does this avoid the possibility
of multiple optimization minima, but eﬃciencies in its
computation allow observation of ˆk as a function of R.

To distinguish traditional word-frequency distributions
from our space-integrated distributions we express the
latter with n-ranks: n = 1, · · · , N ≥ R; subscript models
and parameters, e.g., ˆfS ∝ (n + kS)−γS for space’s inte-
gration; and write nr to indicate the space-integrated
rank of the rth conventionally-ranked word. For a text
that perfectly conforms to Zipf’s law and our dark-matter
hypothesis, its space shift should be ˆkS = 0, making its
ZM shift related to n1, potentially as ˆkZM ≈ n1 − 1.

3

of secondary scalings at high ranks, r (cid:29) 1, we explore an
upper limit, rcut ≤ R, on the number of terms in Eq. 3.
The results of our experiment are non-trivial. We
Log-space values of rcut and measure the linear, Pear-
son correlations between ˆkZM and n1 − 1. While large
cutoﬀs (rcut → R) exhibit little-to-no association, we
ﬁnd a signal that is maximized near rcut = 100, for
which ˆρ ≈ 0.67. This is a strong correlation over near-
ly 20, 000 points, and even though we have performed
multiple comparisons (676, for the Log-spaced values of
rcut), the low p-values observed (all p-values fell below
2 × 10−5) still test signiﬁcant by the most stringent, Bon-
ferroni correction [25]. This scan can be observed in
Fig. 2 (top left), showing how strong correlations coin-
cide with small cutoﬀs (rcut < 1, 000). For the optimal
cutoﬀ (rcut = 134), we exhibit a scatter plot (Fig. 2,
top right), highlighting how ˆkZM increases with n1 − 1,
despite coarse, integer values of n1 − 1. While these two
quantities increase together, values of n1 −1 appear oﬀset
from ˆkZM by a regular quantity near 1. This deviation
does not appear to be random, but is in fact in align-
ment with another quantity.
In the lower-left panel of
Fig. 2, variation (middle 10th percentile, gray envelope)
in the deviations, ˆkZM − n1 + 1, is presented alongside
variation (middle 50th percentile, blue envelope) in the
shift regressed from the space-inclusive frequency distri-
bution, ˆkS. Appearing much more stable than ˆkZM, val-
ues of ˆkS nearly align with the deviations at roughly the
optimal cutoﬀ. Thus, ˆkZM is embedded with information
that resolves the parameterization of the space-inclusive
distribution, despite having no direct information on the
presence of the excluded non-word objects! This result
can be seen in the lower-right panel of Fig. 2, showing
histograms of ˆkS and ˆkZM − n1 + 1, where the variation
in ˆkS is quite constrained, appearing like a vertical line.
While we have uncovered a strong signal relating the
inclusion of non-words to Mandelbrot’s shift, this sig-
nal has appeared in a surprising manner. The space-
inclusive distributions did not exhibit shift values close
to zero, but instead extraordinarily regularly near a dif-
ferent value, ˆkS ≈ −0.85. The variation in this param-
eter is in fact far more narrow than that found for
Zipf’s exponent [11, 13, 20]. Zipf’s scaling exponent,
γ, is prone to mixing eﬀects, which we have seen here
to impact the space-exclusive shift (ˆkZM) when rcut is
roughly larger than 1000. What makes ˆkS so much more
robust than ˆkZM? At rank n = 1, space is extraordi-
narily dominant in the frequency distributions, which is
no surprise, since it acts as the primary word delim-
iter for English. More interestingly, though,
is the
range in which the very-regular, negative value that ˆkS
assumes. Instead of describing a roll-back, ˆkS describes
the opposite behavior—a disproportionately large quan-
tity of high-frequency non-words. Unlike Mandelbrot’s

FIG. 2: Top left: linear correlations between ˆkZM and n1 − 1
as a function of increasing cutoﬀs, rcut. The maximum value,
ˆρmax ≈ 0.67, is observed over the ranks: r = 1, · · · , rcut =
134, indicated by vertical red line. Top right: scatter plot
of ˆkZM against n1 − 1 for the maximizing cutoﬀ with density
indicated by shade:
low to high/black to copper. A per-
fect relationship is indicated by the diagonal red line, which
appears oﬀset from the most dense region by a value near
1. Bottom left: median oﬀset (black line), ˆkZM − n1 + 1,
as a function of increasing cutoﬀs, rcut, centered in the oﬀ-
set distribution’s middle decile (gray envelope). Oﬀsets are
compared to the median value for space-inclusive shifts (blue
line), centered in the same distribution’s interquartile range.
The same correlation-optimal cutoﬀ is indicated by a vertical
red line. Bottom right: histograms of ˆkZM − n1 + 1 (gray) and
ˆkS (blue) for the correlation-optimal cutoﬀ, in addition to the
average value of ˆkZM − n1 + 1 (red line).

To investigate if ˆkZM and n1 − 1 covary we apply our
rank regression to nearly 20, 000 English texts from the
Project Gutenberg eBooks collection. We have used this
data set in the past to explain tail behaviors of rank-
frequency distributions. In this past study on text mix-
ing [14] we showed how the compilation of documents into
large, combined corpora leads to modulation of Zipf’s
exponent, with more severe values of γ > 1 at high ranks.
The course of that study also led to our observation that
many of the eBooks are themselves mixed. A notable
example of our past study was the Complete Histori-
cal Romances of Georg Ebers, which is a compilation of
over 100 short stories. The existence of these texts has
a capacity to throw oﬀ regression of ˆkZM in accommo-
dation of the severe, mixing-derived exponents, γ > 1.
However, any true values of k should be most apparent
at smaller ranks [2], i.e., r → 1. To separate the eﬀects

4

roll-back, disproportionately high frequencies are actual-
ly predicted at these ranks by a generative mechanism.
The rank-frequency empiricism of the 20th century was
complimented by a stochastic model of language gener-
ation, deﬁned by Herbert Simon [3]. The goal of this
endeavor was to go beyond observation of linguistic reg-
ularity to a mechanistic understanding of its production.
Simon’s model relied on a ﬁxed word-innovation rate: α.
With each time step, either a new word is drawn with
probability α, or an old word is drawn with probability
θ = 1 − α. The choice of old words is based on an inde-
pendence assumption: old words are drawn in propor-
tion to their current frequencies. While Simon’s original
analysis of this model identiﬁed a power-law frequency
distribution of scaling exponent θ, our recent work with
this model [19] identiﬁed how as θ → 1, the frequency of
the rank-1 word becomes disproportionately large. How-
ever, the disproportionate growth of this rank-1 outlier is
not encoded by a piecewise functional form for frequency,
but as we will show, a negative translation that is most
impactive at small ranks. Thus, we now show how ˆkS is
analytically predicted by Simon’s model, making space
the run away, ﬁrst mover of English language.

To exhibit Simon’s negative translation, we must deﬁne
Mj : j = 1, · · · , N , as the step at which the jth word is
innovated. Intuitively, Mj is the number of words pro-
duced by the time the jth unique word appears. Previ-
ously [19], we determined the analytic form for the result-
ing frequency distribution:

ˆf (wj) =

B(Mj, θ)
B(MN , θ)

≈

(cid:18) Mj
MN

(cid:19)−θ

.

(4)

Using this, our piecewise ansatz for Mj: M1 = 1; with
(cid:104)Mj(cid:105) = (j − 1)/α, for all j > 1 exhibited a ﬁrst mover’s
disproportion, since the rank-1 word’s frequency was not
damped by α. In truth, this is not the real reason for
the ﬁrst mover’s dominance in Simon’s model. While
near truth, our ansatz misses a key analytic feature that
explains the ﬁrst mover via a negative translation.

For any j ≥ 1, (cid:104)Mj+1 − Mj(cid:105) is the expectation of a
geometric distribution with success probability α, i.e.,
(cid:104)Mj+1 − Mj(cid:105) = α−1. Since one can show separation:
(cid:104)Mj+1 − Mj(cid:105) = (cid:104)Mj+1(cid:105) − (cid:104)Mj(cid:105), the resulting recursion
equation: (cid:104)Mj+1(cid:105) = (cid:104)Mj(cid:105) + α−1 provides:

(cid:104)Mj(cid:105) =

α + j − 1
α

=

j − θ
α

.

(5)

While the ﬁrst form in Eq. 5 exhibits the accuracy of our
old ansatz, the latter will provide the most succinct and
illustrative expression of frequency.

Approximation of Mj by (cid:104)Mj(cid:105) = (j − θ)/α into both
the numerator and denominator of Eq. 4 renders our
reﬁnement of the Simon model’s analytic frequencies:

ˆf (wj) ≈

(cid:18) j − θ
N − θ

(cid:19)−θ

.

This uniﬁes the Simon model’s frequencies into a com-
mon expression. For j = 1, the negative shift by θ leaves
j − θ = α < 1 in the numerator, whose reciprocal ( ˆf is
hyperbolic) can be large. This only occurs for the j = 1,
ﬁrst mover, and since estimates [19] place θ near 1, the
entailed scaling outlier aligns with our empirical observa-
tions presented in Fig. 2. Thus, our empirical connection
between Mandelbrot’s k and the exclusion of non-words is
simultaneously a smoking gun for Simon’s model as a pri-
mary mechanism for the generation of language (English,
at least), i.e., our measurement of ˆkS tracks θ, though not
exactly, since we have forced γ = 1.

So, what of languages other than English? Many uti-
lize non-word delimiters similarly, and perhaps exhib-
it the space/ﬁrst mover phenomenon. However, word
segmentation is less trivial for a number of east-Asian
languages, with space infrequent, in a diminished role.
If Simon’s model applies to these, space’s sparsity as
a delimiter and the absence of a dominant ﬁrst mover
should be apparent in values of ˆkS closer to zero (though
still negative). This is an important avenue for study, and
requires adequate word-segmentation tools, but is not the
only question left open. Zipf studied other domains; e.g.,
do analogs for segmentation and dark matter exist and
what would they mean for Zipﬁan city sizes [1]?

Perhaps more immediately important is how our
results may impact the language processing communi-
ty. Excluding space from frequency analysis may be
equivalent to ignoring the single most massive celestial
object from a model of planetary motion. Could space’s
inclusion have positive eﬀects on the engineering side?
Recently, we explored this possibility [26]. Going beyond
the incorporation of space, etc., we centered non-words
as features in an algorithm. This algorithm is now state
of the art for multiword expressions segmentation, thus
an example for the practical integration of non-words.
Furthermore, while many machine learning algorithms
exclude stop words in preprocessing, Pennebaker [27]
exhibited how variation of pronouns and other high-
frequency words is predictive of a variety of social and
psychological characteristics. Regardless, this study pro-
vides a novel understanding of language, both in terms
of empirical laws, and the nature of its formation.

The authors thank Sharon Williams for her thoughtful
discussions, and gratefully acknowledge research support
from Drexel University’s Department of Information Sci-
ence and College of Computing and Informatics.

∗ Electronic address: jw3477@drexel.edu
† Electronic address: gs495@drexel.edu

[1] G. K. Zipf, Human Behaviour and the Principle of Least-

Eﬀort (Addison-Wesley, 1949).

(6)

[2] B. B. Mandelbrot, in Communication Theory, edited by
B. W. Jackson (Butterworth, Woburn, MA, 1953), pp.

[3] H. A. Simon, Biometrika 42, 425 (1955).
[4] G. A. Miller, American Journal of Psychology 70, 311

(2001).

486–502.

(1957).

[5] W. Li, IEEE Trans. Inf. Theor. 38, 1842 (1992).
[6] B. Corominas-Murtra, R. Handel, and S. Thurner, Pro-
ceedings of the National Academy of Sciences 112, 5348
(2015).

[7] C. M. Urzua, Economics Letters 66, 257 (2000).
[8] A. Clauset, C. R. Shalizi, and M. E. J. Newman, SIAM

Review 51, 661 (2009).

[9] S. T. Piantadosi, Psychonomic Bulletin & Review 21,
1112 (2014), ISSN 1069-9384, URL http://dx.doi.org/
10.3758/s13423-014-0585-6.

5

[16] A. L. Barab´asi and R. Albert, Science 286, 509 (1999).
[17] P. L. Krapivsky and S. Redner, Phys. Rev. E 63, 066123

[18] G. U. Yule, Phil. Trans. B 213, 21 (1924).
[19] P. S. Dodds, D. R. Dewhurst, F. F. Hazlehurst,
C. M. Van Oort, L. Mitchell, A. J. Reagan, J. R.
Williams, and C. M. Danforth, Phys. Rev. E 95,
052301 (2017), URL https://link.aps.org/doi/10.
1103/PhysRevE.95.052301.

[20] A. Mehri and M. Jamaati, Physical Letters A 381:31,

[21] A. Corral, G. Boleda, and R. Ferrer-i Cancho, PLOS

2470 (2017).

ONE 10, 1 (2015).

[22] X. Gabaix and R. Ibragimov, Journal of Business & Eco-

[10] M. Gerlach and E. G. Altmann, Phys. Rev. X 3, 021006

nomic Statistics 29, 24 (2012).

(2013).

[23] Y. Virkar and A. Clauset, The Annals of Applied Statis-

[11] R. Ferrer-i Cancho, The European Physical Journal B -
Condensed Matter and Complex Systems 44, 249 (2005).
[12] R. Ferrer-i-Cancho and R. V. Sol´e, Journal of Quantita-

tics 8, 89 (2014).

[24] M. A. Montemurro, Physica A: Statistical Mechanics and

Its Applications 300, 567 (2001).

tive Linguistics 8, 165 (2001).

[25] O. G. Dunn, Journal of the American Statistical Associ-

[13] J. R. Williams, P. R. Lessard, S. Desu, E. M. Clark,
J. P. Bagrow, C. M. Danforth, and P. S. Dodds, Nature
Scientiﬁc Reports 5, 12209 (2015).

[14] J. R. Williams, J. P. Bagrow, C. M. Danforth, and P. S.

Dodds, Physical Review E 91, 052811 (2015).

ation 56, 52 (1961).

[26] J. R. Williams, in In Proceedings of the 3rd Workshop on
Noisy User Generated Text at the Conference on Empir-
ical Methods in Natural Language Processing (2017).
[27] J. W. Pennebaker, The secret life of pronouns: what our

[15] A. Kulig, J. Kwapie´n, T. Stanisz, and S. Dro˙zd˙z, Infor-

words say about us (Bloomsbury Press, 2011).

mation Sciences 375, 98 (2017).

