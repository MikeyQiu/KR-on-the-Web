Cross-Lingual Cross-Platform Rumor Veriﬁcation Pivoting on
Multimedia Content

Weiming Wen1, Songwen Su1, Zhou Yu1

1University of California, Davis
wmwen@ucdavis.edu

8
1
0
2
 
g
u
A
 
8
2
 
 
]
L
C
.
s
c
[
 
 
2
v
1
1
9
4
0
.
8
0
8
1
:
v
i
X
r
a

Abstract

With the increasing popularity of smart de-
vices, rumors with multimedia content become
more and more common on social networks.
The multimedia information usually makes ru-
mors look more convincing. Therefore, ﬁnd-
ing an automatic approach to verify rumors
with multimedia content is a pressing task.
Previous rumor veriﬁcation research only uti-
lizes multimedia as input features. We propose
not to use the multimedia content but to ﬁnd
external information in other news platforms
pivoting on it. We introduce a new features
set, cross-lingual cross-platform features that
leverage the semantic similarity between the
rumors and the external information. When
implemented, machine learning methods uti-
lizing such features achieved the state-of-the-
art rumor veriﬁcation results.

1

Introduction

Social network’s unmoderated nature leads to the
spread and emergence of information with ques-
tionable sources. With the increasing popularity
of the social media, we are exposed to a plethora
of rumors. Here we borrow the rumor deﬁnition
from DiFonzo and Bordia (2007) as unveriﬁed in-
formation. Unmoderated rumors have not only
caused ﬁnancial losses to trading companies but
also panic for the public (Matthews, 2013). Es-
pecially if rumors contain multimedia content, the
public generally accepts the multimedia informa-
tion as a “proof of occurrence” of the event (Sen-
car and Memon, 2009). Readers usually don’t
have time to look through similar events across
different platforms to make an informed judgment.
Therefore, even if a credible platform, such as
CNN, has debunked a rumor, it can still go viral
on other social media platforms.

Intuitively, people believe fake rumors would
contain fabricated multimedia content. Boididou

et al. (2015b) used forensics features for detecting
multimedia fabrication to verify rumors. However,
these features did not lead to noticeable improve-
ment. We suspect that this is because un-tampered
multimedia content can still convey false informa-
tion when paired with fake news from a separate
event. For example, Figure 1 shows one fake post
on MH 370 that used a real video about US Air-
ways Flight 1549. Inspired by the fact that readers

Figure 1: A video of US Airways Flight 1549 was bor-
rowed by news on Malaysia Airlines Flight 370.

tend to search related information covered by dif-
ferent media outlets to garner an objective view,
we propose to verify rumors pivoting on multime-
dia content to tackle such problems. Compared to
keywords, searching information pivoting on the
visual content is more effective and accurate.

In order to access information from different
platforms easily, we created a new rumor veriﬁca-
tion dataset by expanding a Twitter rumor dataset
to include webpages from different social me-
dia platforms using search engines. Previous ru-
mor veriﬁcation datasets are mainly monolingual,
such as English (Derczynski et al., 2017) or Chi-
nese (Wu et al., 2015). However, textual informa-
tion in the native language where the rumor hap-
pened can be more helpful when it comes to ver-

ifying worldwide rumors. Therefore, we not only
indexed English webpages by searching Google
with images but also included Chinese webpages
via Baidu.
We next

introduced our cross-lingual cross-
platform features which capture the similarity
and agreement among rumors with posts from
different social media. We built an automatic
veriﬁcation model using the proposed features
and achieved the state-of-the-art performance on
the MediaEval 2015’s Verifying Multimedia Use
(VMU 2015) dataset (Boididou et al., 2015a) uti-
lizing information from Google.

Collecting and annotating rumors in foreign
languages is difﬁcult and time-consuming, espe-
cially for languages with low rumor veriﬁcation
labeling. Finding out an automatic way to verify
those rumors in an unsupervised way is also mean-
ingful. Since our cross-lingual cross-platform fea-
tures are adaptable to rumors in different lan-
guages, we demonstrated that these features could
transfer learned knowledge by training on one lan-
guage and testing on another. Such cross-lingual
adaptation ability is especially useful for predict-
ing rumors that have low annotation resource with
available annotated rumors in languages such as
English.

We published our code and dataset on GitHub1.

2 Related work

Previous research has utilized multimedia infor-
mation for rumor veriﬁcation in various ways.
Zampoglou et al. (2015); Jin et al. (2015); Boi-
didou et al. (2015b) veriﬁed rumors by leverag-
ing forensic features which are extracted to en-
sure the digital images are not tempered (Sencar
and Memon, 2009). However, none of these stud-
ies found such information useful for rumor ver-
iﬁcation on a Twitter-based multimedia dataset.
Jin et al. (2017) incorporated image features us-
ing a pre-trained deep convolutional neural net-
work (Krizhevsky et al., 2012) on the extended
Twitter-based multimedia dataset. Although the
image features improve their results, their frame-
work cannot outperform methods other than mul-
timodal fusing networks. One possible reason is
that the multimedia content in fake rumors is bor-
rowed from another real event and usually their
content corresponds to the text of the rumors. In
this case, the image itself is real but not real in

1https://github.com/WeimingWen/CCRV

the context of the fake news. We thus propose
to leverage the multimedia information by ﬁnding
the agreement and disagreement among posts that
are from different social media platforms but share
similar visual contents.

The agreement between rumors and their com-
ments is used heavily in automatic veriﬁcation.
Mendoza et al. (2010) declared that fake rumors
tended to have more people question their validity.
Later, Qazvinian et al. (2011) ﬁrst annotated com-
ments on tweets as supporting, denying or query-
ing, and then used such stance information in the
classiﬁcation to leverage the “wisdom of crowds”.
Recently, the best performing system (Enayet and
El-Beltagy, 2017) in RumourEval shared task at
SemEval 2017 (Derczynski et al., 2017) also used
such information. However, the crowd is not al-
ways wise. For example, Starbird et al. (2014)
suspected the correctness of public opinions in
rumors, pointing out some certain fake news re-
ceived more support than questions. In our work,
instead of using the “wisdom of crowds”, we used
the knowledge from different news platforms to
assist rumor veriﬁcation.

Computational journalism (Cohen et al., 2011)
exploits external knowledge widely. Diakopoulos
et al. (2012) ﬁrst leveraged information from reli-
able sources in the context of journalism. They de-
veloped a tool for journalists to search for and as-
sess sources in social media around breaking news
events. Ciampaglia et al. (2015) utilized factual
knowledge bases, such as Wikipedia, to assess the
truth of simple statements. Shao et al. (2016) de-
signed a system for tracking rumors on different
platforms, which is probably the closest work to
ours. However, they did not utilize cross-platform
information for rumor veriﬁcation. Our proposed
method is able to leverage information on any plat-
form to verify rumors as long as it has both textual
and multimedia information.

3 CCMR dataset

We created a cross-lingual cross-platform multi-
media rumor veriﬁcation dataset (CCMR) to study
how to leverage information from different me-
dia platforms and different languages to verify
rumor automatically. CCMR consists of three
sub-datasets: CCMR Twitter, CCMR Google, and
CCMR Baidu.

CCMR Twitter is borrowed from VMU 2015
dataset (Boididou et al., 2015a). There are 17

ID Event

Hurricane Sandy
Boston Marathon bombing
Sochi Olympics

01
02
03
04 MA ﬂight 370
05
06
07
08
09
10
11
12
13
14
15
16
17

Bring Back Our Girls
Columbian Chemicals
Passport hoax
Rock Elephant
Underwater bedroom
Livr mobile app
Pig ﬁsh
Solar Eclipse
Girl with Samurai boots
Nepal Earthquake
Garissa Attack
Syrian boy
Varoufakis and zdf
Total

Twitter

Real
4,664
344
0
0
0
0
0
0
0
0
0
140
0
1,004
73
0
0
6,225

Fake
5,558
189
274
310
131
185
44
13
113
9
14
137
218
356
6
1,786
61
9,404

Real
1,836
619
139
143
29
35
24
3
1
0
3
40
2
257
60
4
2
3,197

Google
Fake Others Real
693
165
317
54
64
132
80
65
2
42
19
2
16
0
4
17
0
58
0
4
1
13
0
64
2
52
159
60
36
0
0
1
0
0
1,393
729

203
49
76
115
37
26
2
0
0
11
4
39
6
107
3
3
18
699

Baidu
Fake Others
134
55
124
59
6
1
0
2
37
0
12
10
48
19
1
0
0
508

291
16
53
31
4
0
4
14
13
0
7
91
0
81
0
0
0
605

Table 1: CCMR dataset statistics.

events containing fake and real posts with images
or videos shared on Twitter. We created CCMR
Google and CCMR Baidu by searching Google
and Baidu indexed webpages that share similar
multimodal content with CCMR Twitter. The up-
per part of Figure 2 shows the collection pro-
cess. We searched Google with every image (URL
for video) in CCMR Twitter to get English web-
pages. Then we indexed those webpages to form
CCMR Google. Similarly, we searched Baidu to
get Chinese webpages and created CCMR Baidu.
Two human annotators manually annotated both
datasets. The annotation is for better analysis and
dataset quality control. None of it is utilized dur-
ing our feature extraction process. Annotators
were asked to label collected webpages based on
their title and multimedia content. If they are not
enough to tell fake news from real news, the web-
page is labeled as “others”. The Cohen’s kappa
coefﬁcient for two annotators is 0.8891 in CCMR
Google and 0.7907 in CCMR Baidu. CCMR has
15,629 tweets indexed by CCMR Twitter (Twit-
ter), 4,625 webpages indexed by CCMR Google
(Google) and 2,506 webages indexed by CCMR
Baidu (Baidu) related to 17 events. The webpages
from Google and Baidu are in English and Chinese
respectively. The statistics of the CCMR dataset
with respect to each event is listed in Table 1.

3.1 Observation in Annotation

We observe that 15.7% of webpages are fake in
CCMR Google while 20.3% in CCMR Baidu. We
speculate that this is because all events in CCMR
dataset took place outside China. Chinese web-

pages searched via Baidu are thus more likely to
mistake the information.
In the manual annota-
tion process, we found that many images are actu-
ally borrowed from other events, which conﬁrms
our assumption. Another interesting observation
is that webpages indexed by Baidu tend to have
more exaggerating or misleading titles to attract
click rates. We labeled such webpages as fake if
they also convey false information through their
multimedia content.

For example,

We also found that news in different

lan-
guages has different distributions concerning the
subtopics of the event.
in the
Boston marathon bombing, Baidu indexed Chi-
nese reports generally put more emphasis on a
Chinese student who is one of the victims, while
Google indexed English reports cover a wider
range of subtopics of the event, such as the pos-
sible bomber. This phenomenon is understandable
as social media from a speciﬁc country usually fo-
cus more on information related to their readers.

4 Framework Overview

Figure 2 describes the overview of our framework.
After collecting CCMR dataset in Section 3, we
ﬁrst performed Twitter rumor veriﬁcation leverag-
ing Google in Section 6 as shown in the bottom
left of the ﬁgure. We extracted cross-lingual cross-
platform features for tweets in CCMR Twitter
leveraging webpages from CCMR Google (TFG).
Section 5 discusses the automatic construction of
this feature set. We then use the features to verify
rumors automatically.

We then performed Twitter rumor veriﬁcation

Figure 2: The information ﬂow of our proposed pipeline. TFG represents the cross-lingual cross-platform features
for tweets leveraging Google information, while TFB is similar but leverages Baidu information instead. BFG
means cross-lingual cross-platform features for Baidu leveraging Google information.

leveraging Baidu in Section 7 to test if our method
can verify rumors by borrowing information from
different languages and platforms. This experi-
ment is meant to demonstrate that our method is
language and platform agnostic. Such an advan-
tage also enables our method to use one language
information to predict in another language (see
the experiment in Section 8). We extracted cross-
lingual cross-platform features for tweets leverag-
ing webpages from CCMR Baidu instead (TFB)
and used it to verify tweets in Section 6.

the classiﬁer pre-trained with TFG on CCMR
Twitter to verify webpages in CCMR Baidu using
BFG, under the assumption that tweets and web-
pages follow a similar distribution.

Although we labeled webpages in CCMR
Google and CCMR Baidu, we did not leverage
the annotation here because annotation is time-
consuming and using annotation information is
not generalizable to other datasets.

5 Cross-lingual Cross-platform Features

In Section 8, we performed Baidu rumor ver-
iﬁcation via transfer learning to test the cross-
lingual adaptation ability of the cross-lingual
cross-platform features. We treated Chinese web-
pages in CCMR Baidu as rumors and empirically
veriﬁed them via transfer learning. We extracted
cross-lingual cross-platform features for Baidu
webpages leveraging Google (BFG) in Section 6.
Since BFG and TFG are both cross-lingual cross-
platform features leveraging Google, we adopted

We propose a set of cross-lingual cross-platform
features to leverage information across different
social media platforms. We ﬁrst embed both the
rumor and the titles of the retrieved webpages into
300-dimension vectors with a pre-trained multi-
lingual sentence embedding.
It is trained using
English-Chinese parallel news and micro-blogs
in UM-Corpus (Tian et al., 2014). We encode
English-Chinese parallel sentences with the same
word dictionary, as they share some tokens such as

URLs and punctuation. We then use a two-layer
bidirectional gated recurrent unit (GRU) (Cho
et al., 2014) to generate hidden states. We obtain
the embedding by averaging the hidden states of
the GRU. A pairwise ranking loss is used to force
the cosine distance between embeddings of paired
sentences to be small and unpaired sentences to be
large. We train our multilingual sentence embed-
ding on 453,000 pairs of English-Chinese parallel
sentences and evaluated it on another 2000 sen-
tence pairs. Our published code includes the im-
plementation details of the multilingual sentence
embedding.

After obtaining the embeddings of the rumor
and the titles of the retrieved webpages, we fur-
ther calculate the distance and agreement features
between these embeddings to create a set of cross-
lingual cross-platform features. In total, there are
10 features, two for distance features and eight for
agreement features.

5.1 Distance Features

We compute the cosine distances between the em-
beddings of the target rumors and the titles of the
retrieved webpages. The distance indicates if the
rumor has similar meaning with the retrieved web-
pages that have similar multimedia content. We
calculate the mean and variance of the distance.

The mean of the distance indicates the aver-
age similarity between a rumor and its correspond-
ing webpages from other platforms. A high value
in mean suggests that the rumor is very differ-
ent from the retrieved information from other plat-
forms. We suspect that rumors with this property
have a higher probability of being fake. Because
the rumor might have borrowed the image from
another event that was covered in the retrieved in-
formation. Meanwhile, the variance indicates how
much these retrieved webpages are different from
each other. A high variance indicates that the mul-
timedia information is used by different events or
is described in different statements. So the event
or the statement the rumor covers could be fake.

5.2 Agreement Features

We ﬁrst pre-train an agreement classiﬁer on a
stance detection dataset provided by the Fake
News Challenge2. This dataset provides pairs of
English sentences with their agreement annota-
tions in “agree”, “disagree”, “discuss” and “unre-

2http://www.fakenewschallenge.org/

lated”. Figure 3 shows four example body texts of
a headline corresponding to each type of annota-
tion in the dataset. During the training process, we
embed the sentences in the Fake News Challenge
dataset using our pre-trained multilingual sentence
embedding. We then concatenate the embeddings
of the sentence pair as the input. We use a multi-
layer perceptron to pre-train our agreement classi-
ﬁer. We randomly select a balanced development
set containing 250 pairs for each label (1000 in to-
tal) and train our agreement classiﬁer on the rest
of 74,385 pairs. The agreement classiﬁer achieves
0.652 in the macro-averaged F1-score on the de-
velopment set. Our published code also includes
the details.

Figure 3: An example headline and its body texts of the
Fake News Challenge dataset.

We calculate the agreement features using the
mean and variance of the prediction probability
between the rumor and all the retrieved webpages.
There are in total four agreement labels. There-
fore, we have eight agreement features in total.
Agreement features capture information about if
the rumor’s statement agrees with the correspond-
ing information in other platforms. Besides being
able to gain similar beneﬁts as distance features,
our agreement features also capture the case that
the information stance is portrayed differently by
different resource rumors. Conﬂicting information
will also be an indicator of fake news.

6 Rumor Veriﬁcation Leveraging
Cross-platform Information

We extracted cross-lingual cross-platform features
for tweets in CCMR Twitter leveraging Google
(TFG) and evaluated the effectiveness of TFG on
rumor veriﬁcation tasks.

We proposed a simple multi-layer perceptron
classiﬁer (MLP) to leverage the extracted features.
MLP has two fully-connected hidden layers of
20 neurons with ReLU as the activation function.
Each layer is followed by a dropout of 0.5. We
task and event.
evaluated TFG in two settings:
1) The task setting used event 1-11 for training
and event 12-17 for testing according to (Boidi-
dou et al., 2015a). 2) The event setting evaluated
the model performance on a leave-one-event-out
cross-validation fashion. F1-score is used for eval-
uation metric. Since both collecting source rumors
from Google and doing feature extraction for a
given tweet can be done automatically, it is fair to
compare the performance of our model with base-
lines described below.

6.1 Baselines

We adopted three best performing models in the
VMU 2015 task as our baselines:

UoS-ITI (UoS) (Middleton, 2015) uses a natural
language processing pipeline to verify tweets. It is
a rule-based regular expression pattern matching
method. It ranks evidence from Twitter according
to the most trusted and credible sources.

MCG-ICT (MCG) (Jin et al., 2015) is an ap-
proach including two levels of classiﬁcation.
It
treats each image or video in the dataset as a topic
and uses the credibility of these topics as a new
feature for the tweets. They used the tweet-based
and user-based features (Base), such as the num-
ber of hashtags in the tweet or the number of
friends of the user who posted the tweet.

CERTH-UNITN (CER) (Boididou et al., 2015b)
uses an agreement-based retraining scheme.
It
takes advantage of its own predictions to combine
two classiﬁers built from tweet-based and user-
based features (Base). Besides features provided
by the task, it included some additional features
such as the number of nouns in tweets and trust
scores of URLs in tweets obtained from third-
party APIs.

F1-Task F1-Event
Method
0.830
UoS-ITI
0.942
MCG-ICT
CERTH-UNITN 0.911
0.908
TFG
0.810
BFG
0.899
Combo

0.224
0.756
0.693
0.822
0.739
0.816

Table 2: The task and event settings performance.

ID
01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
16
17
Avg

UoS MCG CER TFG TFB
0.718
0.704
0.658
0.745
0.448
0.007
0.822
0.595
0.057
0.678
0.717
0.538
0.956
0.947
0.000
1.000
0.916
0.555
0.989
0.475
0.000
1.000
1.000
0.000
0.996
0.996
0.000
-
0.821
0.000
0.667
0.000
0.000
0.754
0.656
0.000
0.850
0.795
0.000
0.430
0.419
0.000
0.156
0.154
0.000
1.000
-
0.999
1.000
1.000
-
0.739
0.693
0.224

0.594
0.494
0.882
0.826
0.988
0.949
1.000
0.870
0.772
0.615
0.963
0.655
0.954
0.330
0.130
0.990
0.827
0.756

0.715
0.557
0.956
0.856
0.969
0.912
0.989
0.960
1.000
0.875
0.963
0.677
0.998
0.409
0.145
0.996
0.992
0.822

Table 3: F1-scores for each event.

6.2 Results

We describe the task setting results in Table 2,
and detailed per-event results in Table 3. Al-
though TFG does not achieve the highest F1-score
in the task setting, it is mainly due to the split of
the dataset. More than half of the tweets in the
test set do not have images. Thus we can only
leverage cross-platform information by searching
videos’ URLs, which results in less accurate cross-
lingual cross-platform features. In the event set-
ting, which has a more fair comparison, TFG
outperformed other methods with a big margin
(p<0.001). It is surprising to see that only 10 fea-
tures extracted from external resources indexed by
search engines leveraged by a simple classiﬁer can
bring such a big performance boost.

To further explore the quality of the cross-
lingual cross-platform features, we calculated the
Pearson correlation coefﬁcient (PCC) between

each feature with respect to the tweet’s label (fake
or real). We evaluated both TFG and features used
by baseline models. Table 4 lists the top six fea-
tures with the highest absolute PCC values. A pos-
itive value indicates this feature positively corre-
lates with fake news. We can see four out of the
top six features are cross-lingual cross-platform
features. The variance of the unrelated probability
(unrelated variance) has the highest score, which
further validates our design intuition that tweets
might convey false information when they have
different agreement with all other webpages that
shared similar multimedia content. The second
feature, “distance var” is also highly correlated
with fake news. This result supports our hypothe-
sis that if there is a large information dissimilarity
across different platforms, there is a high probabil-
ity of fake information involved. The only feature
from baselines (56 features in total) in the top six
features is whether a tweet contains an exclama-
tion mark or not (containsExclaimationMark).

Feature
unrelated variance
distance variance
agree variance
discuss mean
unrelated mean
containsExclamationMark

PCC
0.306
0.286
0.280
-0.231
0.210
0.192

Table 4: Top six features correlated with fake news.

6.3 Analysis

We found that Google webpages usually cover a
complete set of different information. There are
usually both fake news and real news that debunk
these fake ones. As a result, there is a big in-
formation variance among all those webpages’ ti-
tles, which is captured by the cross-lingual cross-
platform features. Therefore, TFG performs much
better than baselines in a number of events, such
as Event 03 (Sochi Olympics). The statistics of the
CCMR dataset, described in Table 1, also supports
our observation that the labels of posts in CCMR
Google are distributed more evenly compared to
other media sources.

However, the F1-score of TFG is very low in
Event 15 (Garissa Attack). Gunmen stormed the
Garissa University College in this event. We an-
alyzed the Google webpages’ titles that share the
same image in this event. Although some titles

are related to the event, more of them are talk-
ing about completely unrelated information such
as “Daily Graphic News Sun 20th Oct, 2013 —
GhHeadlines Total News ...”. This webpage’s ti-
tle only shows its published date and the name
of the website. Such noise hurt the performance
of the cross-lingual cross-platform features. Since
we did not perform any manual labeling or ﬁlter-
ing, sometimes the crawled webpages can be mis-
leading. To analyze the prevalence of such noise,
we randomly picked 100 Google webpages and
100 Baidu webpages from CCMR and counted the
number of noisy posts. The ratio of noise is 22%
in Google and 18% in Baidu. However, even with
such noise, our proposed methods can still outper-
form current state-of-the-art methods.

7 Rumor Veriﬁcation Leveraging
Cross-lingual Information

We tested if our cross-lingual cross-platform fea-
tures are able to leverage external information
from another language for rumor veriﬁcation. We
simply replaced the Google webpages with Baidu
webpages to extract features for tweets (TFB), be-
cause we have a pre-trained multilingual sentence
embedding that can project Chinese and English to
a shared embedding space. We used the same clas-
siﬁer, MLP to evaluate the performance of TFB
with both baselines and TFG.

7.1 Results

Experiment results using the task setting are
shown in Table 2 and the detailed per-event results
are listed in Table 3. Similar to the problem in
TFG, we can not obtain any Chinese webpages re-
lated to events such as Syrian boy, Varoufakis and
zdf, which cover most tweets in the test set. Those
missing features make TFB perform poorly in the
task setting. However, TFB performs better than
two of the baselines in the event setting. If we ex-
clude events without Baidu webpages (event 10,
16 and 17), the average F1-score of UoS, MCG
and CER are 0.130, 0.732 and 0.660, which are
all lower than TFB’s. The performance of TFB
proves that our method can be generalized across
languages or platforms.

To further test the robustness of our cross-
lingual cross-platform features, we also examined
if it would still work when leveraging external in-
formation that contains different languages. We
extracted the cross-lingual cross-platform features

for tweets leveraging Google and Baidu webpages
together (Combo) and accessed the performance
of Combo using MLP similarly. The performance
of Combo is also listed in Table 2. Since Combo
would contain noise introduced from combining
webpages indexed by different search engines, it is
not surprising that Combo performs slightly worse
than TFG extracted from Google webpages which
already cover a wide range of information solely.
However, Combo performs much better than TFB
which only leverages Baidu webpages. It proves
that our cross-lingual cross-platform features are
robust enough to utilize combined external infor-
mation from different languages and platforms.

7.2 Analysis

We checked the actual titles of webpages from
CCMR in certain events to analyze the reason for
TFB’s worse performance exhaustively. We found
that those Baidu webpages’ titles often talk about
subtopics different from the target rumor’s on
Twitter, while Google webpages are more related.
For example, the F1-score of TFB is much lower
than TFG’s in event 02 (Boston Marathon bomb-
ing). This performance corresponds to our obser-
vation in Section 3 that Baidu webpages mainly
focus on a subtopic related to the Chinese student
instead of other things discussed on Twitter.

8 Low-resource Rumor Veriﬁcation via

Transfer Learning

We extracted cross-lingual cross-platform features
of webpages in CCMR Baidu leveraging informa-
tion from Google (BFG). Then we applied Trans-
fer, MLP in Section 6 trained on the whole CCMR
Twitter using TFG, to verify those webpages. Be-
cause this pre-trained model is for binary classi-
ﬁcation, only webpages labeled as real or fake in
CCMR Baidu are involved.

Since webpages in CCMR Baidu do not share
the same features with tweets, such as the number
of likes and retweets, we adopted a random selec-
tion model as our baseline. It would predict a ru-
mor as real or fake with the same probability. We
compared the performance of the Transfer model
with this baseline on each event. F1-score is also
used for evaluation metric.

8.1 Results

Table 5 lists the detailed results of our transfer
learning experiment. We achieved much better

performance compared to the baseline with statis-
tical signiﬁcance (p<0.001), which indicates that
our cross-lingual cross-platform feature set can be
generalized to rumors in different languages.
It
enables the trained classiﬁer to leverage the infor-
mation learned from one language to another.

Hurricane Sandy
Boston Marathon bombing
Sochi Olympics

ID Event
01
02
03
04 MH ﬂight 370
05
06
07
08
09
10
11
12
13
14
15
16
17

Bring Back Our Girls
Columbian Chemicals
Passport hoax
Rock Elephant
Underwater bedroom
Livr mobile app
Pig ﬁsh
Solar Eclipse
Girl with Samurai boots
Nepal Earthquake
Garissa Attack
Syrian boy
Varoufakis and zdf
Avg

Random Transfer
0.247
0.230
0.555
0.407
0.500
0.000
0.000
0.000
0.577
-
0.375
0.571
0.559
0.227
0.125
-
-
0.312

0.287
0.284
0.752
0.536
0.923
0.100
0.000
0.500
0.972
-
1.00
0.889
0.925
0.211
0.059
-
-
0.531

Table 5:
Rumor veriﬁcation performance on the
CCMR Baidu, where - indicates there is no webpage
in that event.

8.2 Analysis

In event 11 (Pig ﬁsh), Transfer achieves much
higher performance than the random baseline.
Generally, Baidu webpages’ titles are semantically
different from tweets. However, in this particular
event, the textual information of those titles and
tweets are semantically close. As a result, mod-
els learned from English rumors can easily work
on Chinese rumors, which is helpful for our trans-
fer learning. Figure 4 shows three Twitter-Baidu
rumor pairs with similar meaning in this event.

Transfer obtains pretty low F1-scores in event
07 (Passport hoax). The annotation conﬂict caused
its weak performance. This event is about a Child
drew all over his dads passport and made his dad
stuck in South Korea. During the manual annota-
tion process, we found out that it is a real event
conﬁrmed by ofﬁcial accounts according to one
news article from Chinese social media3, while
CCMR Twitter labeled such tweets as fake. Since
Transfer is pre-trained using Twitter dataset, it is
not surprising that Transfer achieves 0 in F1-score
on this event. The annotation conﬂict also brings
out that rumor veriﬁcation will beneﬁt from utiliz-
ing cross-lingual and cross-platform information.

3http://new.qq.com/cmsn/20140605/20140605002796

Alessandro Flammini. 2015. Computational fact
PloS one,
checking from knowledge networks.
10(6):e0128193.

Sarah Cohen, James T Hamilton, and Fred Turner.
2011. Computational journalism. Communications
of the ACM, 54(10):66–71.

Leon Derczynski, Kalina Bontcheva, Maria Liakata,
Rob Procter, Geraldine Wong Sak Hoi, and Arkaitz
Zubiaga. 2017. Semeval-2017 task 8: Rumoureval:
Determining rumour veracity and support for ru-
mours. arXiv preprint arXiv:1704.05972.

Nicholas Diakopoulos, Munmun De Choudhury, and
Mor Naaman. 2012. Finding and assessing social
media information sources in the context of journal-
ism. In Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems, pages 2451–
2460. ACM.

Nicholas DiFonzo and Prashant Bordia. 2007. Rumor,
gossip and urban legends. Diogenes, 54(1):19–35.

Omar Enayet and Samhaa R El-Beltagy. 2017.
Niletmrg at semeval-2017 task 8: Determining ru-
mour and veracity support for rumours on twitter. In
Proceedings of the 11th International Workshop on
Semantic Evaluation (SemEval-2017), pages 470–
474.

Zhiwei Jin, Juan Cao, Han Guo, Yongdong Zhang,
and Jiebo Luo. 2017. Multimodal fusion with re-
current neural networks for rumor detection on mi-
croblogs. In Proceedings of the 2017 ACM on Mul-
timedia Conference, pages 795–816. ACM.

Zhiwei Jin, Juan Cao, Yazi Zhang, and Yongdong
Zhang. 2015. Mcg-ict at mediaeval 2015: Verify-
ing multimedia use with a two-level classiﬁcation
model. In MediaEval.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012.
Imagenet classiﬁcation with deep con-
volutional neural networks. In Advances in neural
information processing systems, pages 1097–1105.

Christopher Matthews. 2013. How does one fake tweet
cause a stock market crash. Wall Street & Markets:
Time.

Marcelo Mendoza, Barbara Poblete,

and Carlos
Castillo. 2010. Twitter under crisis: Can we trust
what we rt? In Proceedings of the ﬁrst workshop on
social media analytics, pages 71–79. ACM.

Stuart Middleton. 2015. Extracting attributed veri-
ﬁcation and debunking reports from social media:
Mediaeval-2015 trust and credibility analysis of im-
age and video.

Vahed Qazvinian, Emily Rosengren, Dragomir R
Radev, and Qiaozhu Mei. 2011. Rumor has it: Iden-
tifying misinformation in microblogs. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1589–1599. Asso-
ciation for Computational Linguistics.

Figure 4: Example parallel rumors in the Pig ﬁsh event.

9 Conclusion

We created a new multimedia rumor veriﬁcation
dataset by extending a multimedia Twitter dataset
with external webpages from Google and Baidu
that share similar image content. We designed
a set of cross-lingual cross-platform features that
leverage the similarity and agreement between
information across different platforms and lan-
guages to verify rumors. The proposed features
are compact and generalizable across languages.
We also designed a neural network based model
that utilizes the cross-lingual cross-platform fea-
tures and achieved state-of-the-art results in auto-
matic rumor veriﬁcation.

References

Christina Boididou, Katerina Andreadou, Symeon Pa-
padopoulos, Duc-Tien Dang-Nguyen, Giulia Boato,
Michael Riegler, and Yiannis Kompatsiaris. 2015a.
Verifying multimedia use at mediaeval 2015. In Me-
diaEval.

Christina Boididou, Symeon Papadopoulos, Duc-Tien
Dang-Nguyen, Giulia Boato, and Yiannis Kompat-
siaris. 2015b. The certh-unitn participation@ veri-
fying multimedia use 2015. In MediaEval.

Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014.
Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

Giovanni Luca Ciampaglia, Prashant Shiralkar, Luis M
and

Johan Bollen, Filippo Menczer,

Rocha,

Husrev T Sencar and Nasir Memon. 2009. Overview of
state-of-the-art in digital image forensics. In Algo-
rithms, Architectures and Information Systems Secu-
rity, pages 325–347. World Scientiﬁc.

Chengcheng Shao, Giovanni Luca Ciampaglia,
Alessandro Flammini, and Filippo Menczer. 2016.
Hoaxy: A platform for tracking online misinfor-
In Proceedings of the 25th International
mation.
Conference Companion on World Wide Web,
pages 745–750. International World Wide Web
Conferences Steering Committee.

Kate Starbird, Jim Maddock, Mania Orand, Peg
Achterman, and Robert M Mason. 2014. Rumors,
false ﬂags, and digital vigilantes: Misinformation
on twitter after the 2013 boston marathon bombing.
iConference 2014 Proceedings.

Liang Tian, Derek F Wong, Lidia S Chao, Paulo
Quaresma, Francisco Oliveira, and Lu Yi. 2014.
Um-corpus: A large english-chinese parallel corpus
for statistical machine translation. In LREC, pages
1837–1842.

Ke Wu, Song Yang, and Kenny Q Zhu. 2015. False ru-
mors detection on sina weibo by propagation struc-
tures. In Data Engineering (ICDE), 2015 IEEE 31st
International Conference on, pages 651–662. IEEE.

Markos Zampoglou, Symeon Papadopoulos, and Yian-
nis Kompatsiaris. 2015. Detecting image splicing in
the wild (web). In Multimedia & Expo Workshops
(ICMEW), 2015 IEEE International Conference on,
pages 1–6. IEEE.

Cross-Lingual Cross-Platform Rumor Veriﬁcation Pivoting on
Multimedia Content

Weiming Wen1, Songwen Su1, Zhou Yu1

1University of California, Davis
wmwen@ucdavis.edu

8
1
0
2
 
g
u
A
 
8
2
 
 
]
L
C
.
s
c
[
 
 
2
v
1
1
9
4
0
.
8
0
8
1
:
v
i
X
r
a

Abstract

With the increasing popularity of smart de-
vices, rumors with multimedia content become
more and more common on social networks.
The multimedia information usually makes ru-
mors look more convincing. Therefore, ﬁnd-
ing an automatic approach to verify rumors
with multimedia content is a pressing task.
Previous rumor veriﬁcation research only uti-
lizes multimedia as input features. We propose
not to use the multimedia content but to ﬁnd
external information in other news platforms
pivoting on it. We introduce a new features
set, cross-lingual cross-platform features that
leverage the semantic similarity between the
rumors and the external information. When
implemented, machine learning methods uti-
lizing such features achieved the state-of-the-
art rumor veriﬁcation results.

1

Introduction

Social network’s unmoderated nature leads to the
spread and emergence of information with ques-
tionable sources. With the increasing popularity
of the social media, we are exposed to a plethora
of rumors. Here we borrow the rumor deﬁnition
from DiFonzo and Bordia (2007) as unveriﬁed in-
formation. Unmoderated rumors have not only
caused ﬁnancial losses to trading companies but
also panic for the public (Matthews, 2013). Es-
pecially if rumors contain multimedia content, the
public generally accepts the multimedia informa-
tion as a “proof of occurrence” of the event (Sen-
car and Memon, 2009). Readers usually don’t
have time to look through similar events across
different platforms to make an informed judgment.
Therefore, even if a credible platform, such as
CNN, has debunked a rumor, it can still go viral
on other social media platforms.

Intuitively, people believe fake rumors would
contain fabricated multimedia content. Boididou

et al. (2015b) used forensics features for detecting
multimedia fabrication to verify rumors. However,
these features did not lead to noticeable improve-
ment. We suspect that this is because un-tampered
multimedia content can still convey false informa-
tion when paired with fake news from a separate
event. For example, Figure 1 shows one fake post
on MH 370 that used a real video about US Air-
ways Flight 1549. Inspired by the fact that readers

Figure 1: A video of US Airways Flight 1549 was bor-
rowed by news on Malaysia Airlines Flight 370.

tend to search related information covered by dif-
ferent media outlets to garner an objective view,
we propose to verify rumors pivoting on multime-
dia content to tackle such problems. Compared to
keywords, searching information pivoting on the
visual content is more effective and accurate.

In order to access information from different
platforms easily, we created a new rumor veriﬁca-
tion dataset by expanding a Twitter rumor dataset
to include webpages from different social me-
dia platforms using search engines. Previous ru-
mor veriﬁcation datasets are mainly monolingual,
such as English (Derczynski et al., 2017) or Chi-
nese (Wu et al., 2015). However, textual informa-
tion in the native language where the rumor hap-
pened can be more helpful when it comes to ver-

ifying worldwide rumors. Therefore, we not only
indexed English webpages by searching Google
with images but also included Chinese webpages
via Baidu.
We next

introduced our cross-lingual cross-
platform features which capture the similarity
and agreement among rumors with posts from
different social media. We built an automatic
veriﬁcation model using the proposed features
and achieved the state-of-the-art performance on
the MediaEval 2015’s Verifying Multimedia Use
(VMU 2015) dataset (Boididou et al., 2015a) uti-
lizing information from Google.

Collecting and annotating rumors in foreign
languages is difﬁcult and time-consuming, espe-
cially for languages with low rumor veriﬁcation
labeling. Finding out an automatic way to verify
those rumors in an unsupervised way is also mean-
ingful. Since our cross-lingual cross-platform fea-
tures are adaptable to rumors in different lan-
guages, we demonstrated that these features could
transfer learned knowledge by training on one lan-
guage and testing on another. Such cross-lingual
adaptation ability is especially useful for predict-
ing rumors that have low annotation resource with
available annotated rumors in languages such as
English.

We published our code and dataset on GitHub1.

2 Related work

Previous research has utilized multimedia infor-
mation for rumor veriﬁcation in various ways.
Zampoglou et al. (2015); Jin et al. (2015); Boi-
didou et al. (2015b) veriﬁed rumors by leverag-
ing forensic features which are extracted to en-
sure the digital images are not tempered (Sencar
and Memon, 2009). However, none of these stud-
ies found such information useful for rumor ver-
iﬁcation on a Twitter-based multimedia dataset.
Jin et al. (2017) incorporated image features us-
ing a pre-trained deep convolutional neural net-
work (Krizhevsky et al., 2012) on the extended
Twitter-based multimedia dataset. Although the
image features improve their results, their frame-
work cannot outperform methods other than mul-
timodal fusing networks. One possible reason is
that the multimedia content in fake rumors is bor-
rowed from another real event and usually their
content corresponds to the text of the rumors. In
this case, the image itself is real but not real in

1https://github.com/WeimingWen/CCRV

the context of the fake news. We thus propose
to leverage the multimedia information by ﬁnding
the agreement and disagreement among posts that
are from different social media platforms but share
similar visual contents.

The agreement between rumors and their com-
ments is used heavily in automatic veriﬁcation.
Mendoza et al. (2010) declared that fake rumors
tended to have more people question their validity.
Later, Qazvinian et al. (2011) ﬁrst annotated com-
ments on tweets as supporting, denying or query-
ing, and then used such stance information in the
classiﬁcation to leverage the “wisdom of crowds”.
Recently, the best performing system (Enayet and
El-Beltagy, 2017) in RumourEval shared task at
SemEval 2017 (Derczynski et al., 2017) also used
such information. However, the crowd is not al-
ways wise. For example, Starbird et al. (2014)
suspected the correctness of public opinions in
rumors, pointing out some certain fake news re-
ceived more support than questions. In our work,
instead of using the “wisdom of crowds”, we used
the knowledge from different news platforms to
assist rumor veriﬁcation.

Computational journalism (Cohen et al., 2011)
exploits external knowledge widely. Diakopoulos
et al. (2012) ﬁrst leveraged information from reli-
able sources in the context of journalism. They de-
veloped a tool for journalists to search for and as-
sess sources in social media around breaking news
events. Ciampaglia et al. (2015) utilized factual
knowledge bases, such as Wikipedia, to assess the
truth of simple statements. Shao et al. (2016) de-
signed a system for tracking rumors on different
platforms, which is probably the closest work to
ours. However, they did not utilize cross-platform
information for rumor veriﬁcation. Our proposed
method is able to leverage information on any plat-
form to verify rumors as long as it has both textual
and multimedia information.

3 CCMR dataset

We created a cross-lingual cross-platform multi-
media rumor veriﬁcation dataset (CCMR) to study
how to leverage information from different me-
dia platforms and different languages to verify
rumor automatically. CCMR consists of three
sub-datasets: CCMR Twitter, CCMR Google, and
CCMR Baidu.

CCMR Twitter is borrowed from VMU 2015
dataset (Boididou et al., 2015a). There are 17

ID Event

Hurricane Sandy
Boston Marathon bombing
Sochi Olympics

01
02
03
04 MA ﬂight 370
05
06
07
08
09
10
11
12
13
14
15
16
17

Bring Back Our Girls
Columbian Chemicals
Passport hoax
Rock Elephant
Underwater bedroom
Livr mobile app
Pig ﬁsh
Solar Eclipse
Girl with Samurai boots
Nepal Earthquake
Garissa Attack
Syrian boy
Varoufakis and zdf
Total

Twitter

Real
4,664
344
0
0
0
0
0
0
0
0
0
140
0
1,004
73
0
0
6,225

Fake
5,558
189
274
310
131
185
44
13
113
9
14
137
218
356
6
1,786
61
9,404

Real
1,836
619
139
143
29
35
24
3
1
0
3
40
2
257
60
4
2
3,197

Google
Fake Others Real
693
165
317
54
64
132
80
65
2
42
19
2
16
0
4
17
0
58
0
4
1
13
0
64
2
52
159
60
36
0
0
1
0
0
1,393
729

203
49
76
115
37
26
2
0
0
11
4
39
6
107
3
3
18
699

Baidu
Fake Others
134
55
124
59
6
1
0
2
37
0
12
10
48
19
1
0
0
508

291
16
53
31
4
0
4
14
13
0
7
91
0
81
0
0
0
605

Table 1: CCMR dataset statistics.

events containing fake and real posts with images
or videos shared on Twitter. We created CCMR
Google and CCMR Baidu by searching Google
and Baidu indexed webpages that share similar
multimodal content with CCMR Twitter. The up-
per part of Figure 2 shows the collection pro-
cess. We searched Google with every image (URL
for video) in CCMR Twitter to get English web-
pages. Then we indexed those webpages to form
CCMR Google. Similarly, we searched Baidu to
get Chinese webpages and created CCMR Baidu.
Two human annotators manually annotated both
datasets. The annotation is for better analysis and
dataset quality control. None of it is utilized dur-
ing our feature extraction process. Annotators
were asked to label collected webpages based on
their title and multimedia content. If they are not
enough to tell fake news from real news, the web-
page is labeled as “others”. The Cohen’s kappa
coefﬁcient for two annotators is 0.8891 in CCMR
Google and 0.7907 in CCMR Baidu. CCMR has
15,629 tweets indexed by CCMR Twitter (Twit-
ter), 4,625 webpages indexed by CCMR Google
(Google) and 2,506 webages indexed by CCMR
Baidu (Baidu) related to 17 events. The webpages
from Google and Baidu are in English and Chinese
respectively. The statistics of the CCMR dataset
with respect to each event is listed in Table 1.

3.1 Observation in Annotation

We observe that 15.7% of webpages are fake in
CCMR Google while 20.3% in CCMR Baidu. We
speculate that this is because all events in CCMR
dataset took place outside China. Chinese web-

pages searched via Baidu are thus more likely to
mistake the information.
In the manual annota-
tion process, we found that many images are actu-
ally borrowed from other events, which conﬁrms
our assumption. Another interesting observation
is that webpages indexed by Baidu tend to have
more exaggerating or misleading titles to attract
click rates. We labeled such webpages as fake if
they also convey false information through their
multimedia content.

For example,

We also found that news in different

lan-
guages has different distributions concerning the
subtopics of the event.
in the
Boston marathon bombing, Baidu indexed Chi-
nese reports generally put more emphasis on a
Chinese student who is one of the victims, while
Google indexed English reports cover a wider
range of subtopics of the event, such as the pos-
sible bomber. This phenomenon is understandable
as social media from a speciﬁc country usually fo-
cus more on information related to their readers.

4 Framework Overview

Figure 2 describes the overview of our framework.
After collecting CCMR dataset in Section 3, we
ﬁrst performed Twitter rumor veriﬁcation leverag-
ing Google in Section 6 as shown in the bottom
left of the ﬁgure. We extracted cross-lingual cross-
platform features for tweets in CCMR Twitter
leveraging webpages from CCMR Google (TFG).
Section 5 discusses the automatic construction of
this feature set. We then use the features to verify
rumors automatically.

We then performed Twitter rumor veriﬁcation

Figure 2: The information ﬂow of our proposed pipeline. TFG represents the cross-lingual cross-platform features
for tweets leveraging Google information, while TFB is similar but leverages Baidu information instead. BFG
means cross-lingual cross-platform features for Baidu leveraging Google information.

leveraging Baidu in Section 7 to test if our method
can verify rumors by borrowing information from
different languages and platforms. This experi-
ment is meant to demonstrate that our method is
language and platform agnostic. Such an advan-
tage also enables our method to use one language
information to predict in another language (see
the experiment in Section 8). We extracted cross-
lingual cross-platform features for tweets leverag-
ing webpages from CCMR Baidu instead (TFB)
and used it to verify tweets in Section 6.

the classiﬁer pre-trained with TFG on CCMR
Twitter to verify webpages in CCMR Baidu using
BFG, under the assumption that tweets and web-
pages follow a similar distribution.

Although we labeled webpages in CCMR
Google and CCMR Baidu, we did not leverage
the annotation here because annotation is time-
consuming and using annotation information is
not generalizable to other datasets.

5 Cross-lingual Cross-platform Features

In Section 8, we performed Baidu rumor ver-
iﬁcation via transfer learning to test the cross-
lingual adaptation ability of the cross-lingual
cross-platform features. We treated Chinese web-
pages in CCMR Baidu as rumors and empirically
veriﬁed them via transfer learning. We extracted
cross-lingual cross-platform features for Baidu
webpages leveraging Google (BFG) in Section 6.
Since BFG and TFG are both cross-lingual cross-
platform features leveraging Google, we adopted

We propose a set of cross-lingual cross-platform
features to leverage information across different
social media platforms. We ﬁrst embed both the
rumor and the titles of the retrieved webpages into
300-dimension vectors with a pre-trained multi-
lingual sentence embedding.
It is trained using
English-Chinese parallel news and micro-blogs
in UM-Corpus (Tian et al., 2014). We encode
English-Chinese parallel sentences with the same
word dictionary, as they share some tokens such as

URLs and punctuation. We then use a two-layer
bidirectional gated recurrent unit (GRU) (Cho
et al., 2014) to generate hidden states. We obtain
the embedding by averaging the hidden states of
the GRU. A pairwise ranking loss is used to force
the cosine distance between embeddings of paired
sentences to be small and unpaired sentences to be
large. We train our multilingual sentence embed-
ding on 453,000 pairs of English-Chinese parallel
sentences and evaluated it on another 2000 sen-
tence pairs. Our published code includes the im-
plementation details of the multilingual sentence
embedding.

After obtaining the embeddings of the rumor
and the titles of the retrieved webpages, we fur-
ther calculate the distance and agreement features
between these embeddings to create a set of cross-
lingual cross-platform features. In total, there are
10 features, two for distance features and eight for
agreement features.

5.1 Distance Features

We compute the cosine distances between the em-
beddings of the target rumors and the titles of the
retrieved webpages. The distance indicates if the
rumor has similar meaning with the retrieved web-
pages that have similar multimedia content. We
calculate the mean and variance of the distance.

The mean of the distance indicates the aver-
age similarity between a rumor and its correspond-
ing webpages from other platforms. A high value
in mean suggests that the rumor is very differ-
ent from the retrieved information from other plat-
forms. We suspect that rumors with this property
have a higher probability of being fake. Because
the rumor might have borrowed the image from
another event that was covered in the retrieved in-
formation. Meanwhile, the variance indicates how
much these retrieved webpages are different from
each other. A high variance indicates that the mul-
timedia information is used by different events or
is described in different statements. So the event
or the statement the rumor covers could be fake.

5.2 Agreement Features

We ﬁrst pre-train an agreement classiﬁer on a
stance detection dataset provided by the Fake
News Challenge2. This dataset provides pairs of
English sentences with their agreement annota-
tions in “agree”, “disagree”, “discuss” and “unre-

2http://www.fakenewschallenge.org/

lated”. Figure 3 shows four example body texts of
a headline corresponding to each type of annota-
tion in the dataset. During the training process, we
embed the sentences in the Fake News Challenge
dataset using our pre-trained multilingual sentence
embedding. We then concatenate the embeddings
of the sentence pair as the input. We use a multi-
layer perceptron to pre-train our agreement classi-
ﬁer. We randomly select a balanced development
set containing 250 pairs for each label (1000 in to-
tal) and train our agreement classiﬁer on the rest
of 74,385 pairs. The agreement classiﬁer achieves
0.652 in the macro-averaged F1-score on the de-
velopment set. Our published code also includes
the details.

Figure 3: An example headline and its body texts of the
Fake News Challenge dataset.

We calculate the agreement features using the
mean and variance of the prediction probability
between the rumor and all the retrieved webpages.
There are in total four agreement labels. There-
fore, we have eight agreement features in total.
Agreement features capture information about if
the rumor’s statement agrees with the correspond-
ing information in other platforms. Besides being
able to gain similar beneﬁts as distance features,
our agreement features also capture the case that
the information stance is portrayed differently by
different resource rumors. Conﬂicting information
will also be an indicator of fake news.

6 Rumor Veriﬁcation Leveraging
Cross-platform Information

We extracted cross-lingual cross-platform features
for tweets in CCMR Twitter leveraging Google
(TFG) and evaluated the effectiveness of TFG on
rumor veriﬁcation tasks.

We proposed a simple multi-layer perceptron
classiﬁer (MLP) to leverage the extracted features.
MLP has two fully-connected hidden layers of
20 neurons with ReLU as the activation function.
Each layer is followed by a dropout of 0.5. We
task and event.
evaluated TFG in two settings:
1) The task setting used event 1-11 for training
and event 12-17 for testing according to (Boidi-
dou et al., 2015a). 2) The event setting evaluated
the model performance on a leave-one-event-out
cross-validation fashion. F1-score is used for eval-
uation metric. Since both collecting source rumors
from Google and doing feature extraction for a
given tweet can be done automatically, it is fair to
compare the performance of our model with base-
lines described below.

6.1 Baselines

We adopted three best performing models in the
VMU 2015 task as our baselines:

UoS-ITI (UoS) (Middleton, 2015) uses a natural
language processing pipeline to verify tweets. It is
a rule-based regular expression pattern matching
method. It ranks evidence from Twitter according
to the most trusted and credible sources.

MCG-ICT (MCG) (Jin et al., 2015) is an ap-
proach including two levels of classiﬁcation.
It
treats each image or video in the dataset as a topic
and uses the credibility of these topics as a new
feature for the tweets. They used the tweet-based
and user-based features (Base), such as the num-
ber of hashtags in the tweet or the number of
friends of the user who posted the tweet.

CERTH-UNITN (CER) (Boididou et al., 2015b)
uses an agreement-based retraining scheme.
It
takes advantage of its own predictions to combine
two classiﬁers built from tweet-based and user-
based features (Base). Besides features provided
by the task, it included some additional features
such as the number of nouns in tweets and trust
scores of URLs in tweets obtained from third-
party APIs.

F1-Task F1-Event
Method
0.830
UoS-ITI
0.942
MCG-ICT
CERTH-UNITN 0.911
0.908
TFG
0.810
BFG
0.899
Combo

0.224
0.756
0.693
0.822
0.739
0.816

Table 2: The task and event settings performance.

ID
01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
16
17
Avg

UoS MCG CER TFG TFB
0.718
0.704
0.658
0.745
0.448
0.007
0.822
0.595
0.057
0.678
0.717
0.538
0.956
0.947
0.000
1.000
0.916
0.555
0.989
0.475
0.000
1.000
1.000
0.000
0.996
0.996
0.000
-
0.821
0.000
0.667
0.000
0.000
0.754
0.656
0.000
0.850
0.795
0.000
0.430
0.419
0.000
0.156
0.154
0.000
1.000
-
0.999
1.000
1.000
-
0.739
0.693
0.224

0.594
0.494
0.882
0.826
0.988
0.949
1.000
0.870
0.772
0.615
0.963
0.655
0.954
0.330
0.130
0.990
0.827
0.756

0.715
0.557
0.956
0.856
0.969
0.912
0.989
0.960
1.000
0.875
0.963
0.677
0.998
0.409
0.145
0.996
0.992
0.822

Table 3: F1-scores for each event.

6.2 Results

We describe the task setting results in Table 2,
and detailed per-event results in Table 3. Al-
though TFG does not achieve the highest F1-score
in the task setting, it is mainly due to the split of
the dataset. More than half of the tweets in the
test set do not have images. Thus we can only
leverage cross-platform information by searching
videos’ URLs, which results in less accurate cross-
lingual cross-platform features. In the event set-
ting, which has a more fair comparison, TFG
outperformed other methods with a big margin
(p<0.001). It is surprising to see that only 10 fea-
tures extracted from external resources indexed by
search engines leveraged by a simple classiﬁer can
bring such a big performance boost.

To further explore the quality of the cross-
lingual cross-platform features, we calculated the
Pearson correlation coefﬁcient (PCC) between

each feature with respect to the tweet’s label (fake
or real). We evaluated both TFG and features used
by baseline models. Table 4 lists the top six fea-
tures with the highest absolute PCC values. A pos-
itive value indicates this feature positively corre-
lates with fake news. We can see four out of the
top six features are cross-lingual cross-platform
features. The variance of the unrelated probability
(unrelated variance) has the highest score, which
further validates our design intuition that tweets
might convey false information when they have
different agreement with all other webpages that
shared similar multimedia content. The second
feature, “distance var” is also highly correlated
with fake news. This result supports our hypothe-
sis that if there is a large information dissimilarity
across different platforms, there is a high probabil-
ity of fake information involved. The only feature
from baselines (56 features in total) in the top six
features is whether a tweet contains an exclama-
tion mark or not (containsExclaimationMark).

Feature
unrelated variance
distance variance
agree variance
discuss mean
unrelated mean
containsExclamationMark

PCC
0.306
0.286
0.280
-0.231
0.210
0.192

Table 4: Top six features correlated with fake news.

6.3 Analysis

We found that Google webpages usually cover a
complete set of different information. There are
usually both fake news and real news that debunk
these fake ones. As a result, there is a big in-
formation variance among all those webpages’ ti-
tles, which is captured by the cross-lingual cross-
platform features. Therefore, TFG performs much
better than baselines in a number of events, such
as Event 03 (Sochi Olympics). The statistics of the
CCMR dataset, described in Table 1, also supports
our observation that the labels of posts in CCMR
Google are distributed more evenly compared to
other media sources.

However, the F1-score of TFG is very low in
Event 15 (Garissa Attack). Gunmen stormed the
Garissa University College in this event. We an-
alyzed the Google webpages’ titles that share the
same image in this event. Although some titles

are related to the event, more of them are talk-
ing about completely unrelated information such
as “Daily Graphic News Sun 20th Oct, 2013 —
GhHeadlines Total News ...”. This webpage’s ti-
tle only shows its published date and the name
of the website. Such noise hurt the performance
of the cross-lingual cross-platform features. Since
we did not perform any manual labeling or ﬁlter-
ing, sometimes the crawled webpages can be mis-
leading. To analyze the prevalence of such noise,
we randomly picked 100 Google webpages and
100 Baidu webpages from CCMR and counted the
number of noisy posts. The ratio of noise is 22%
in Google and 18% in Baidu. However, even with
such noise, our proposed methods can still outper-
form current state-of-the-art methods.

7 Rumor Veriﬁcation Leveraging
Cross-lingual Information

We tested if our cross-lingual cross-platform fea-
tures are able to leverage external information
from another language for rumor veriﬁcation. We
simply replaced the Google webpages with Baidu
webpages to extract features for tweets (TFB), be-
cause we have a pre-trained multilingual sentence
embedding that can project Chinese and English to
a shared embedding space. We used the same clas-
siﬁer, MLP to evaluate the performance of TFB
with both baselines and TFG.

7.1 Results

Experiment results using the task setting are
shown in Table 2 and the detailed per-event results
are listed in Table 3. Similar to the problem in
TFG, we can not obtain any Chinese webpages re-
lated to events such as Syrian boy, Varoufakis and
zdf, which cover most tweets in the test set. Those
missing features make TFB perform poorly in the
task setting. However, TFB performs better than
two of the baselines in the event setting. If we ex-
clude events without Baidu webpages (event 10,
16 and 17), the average F1-score of UoS, MCG
and CER are 0.130, 0.732 and 0.660, which are
all lower than TFB’s. The performance of TFB
proves that our method can be generalized across
languages or platforms.

To further test the robustness of our cross-
lingual cross-platform features, we also examined
if it would still work when leveraging external in-
formation that contains different languages. We
extracted the cross-lingual cross-platform features

for tweets leveraging Google and Baidu webpages
together (Combo) and accessed the performance
of Combo using MLP similarly. The performance
of Combo is also listed in Table 2. Since Combo
would contain noise introduced from combining
webpages indexed by different search engines, it is
not surprising that Combo performs slightly worse
than TFG extracted from Google webpages which
already cover a wide range of information solely.
However, Combo performs much better than TFB
which only leverages Baidu webpages. It proves
that our cross-lingual cross-platform features are
robust enough to utilize combined external infor-
mation from different languages and platforms.

7.2 Analysis

We checked the actual titles of webpages from
CCMR in certain events to analyze the reason for
TFB’s worse performance exhaustively. We found
that those Baidu webpages’ titles often talk about
subtopics different from the target rumor’s on
Twitter, while Google webpages are more related.
For example, the F1-score of TFB is much lower
than TFG’s in event 02 (Boston Marathon bomb-
ing). This performance corresponds to our obser-
vation in Section 3 that Baidu webpages mainly
focus on a subtopic related to the Chinese student
instead of other things discussed on Twitter.

8 Low-resource Rumor Veriﬁcation via

Transfer Learning

We extracted cross-lingual cross-platform features
of webpages in CCMR Baidu leveraging informa-
tion from Google (BFG). Then we applied Trans-
fer, MLP in Section 6 trained on the whole CCMR
Twitter using TFG, to verify those webpages. Be-
cause this pre-trained model is for binary classi-
ﬁcation, only webpages labeled as real or fake in
CCMR Baidu are involved.

Since webpages in CCMR Baidu do not share
the same features with tweets, such as the number
of likes and retweets, we adopted a random selec-
tion model as our baseline. It would predict a ru-
mor as real or fake with the same probability. We
compared the performance of the Transfer model
with this baseline on each event. F1-score is also
used for evaluation metric.

8.1 Results

Table 5 lists the detailed results of our transfer
learning experiment. We achieved much better

performance compared to the baseline with statis-
tical signiﬁcance (p<0.001), which indicates that
our cross-lingual cross-platform feature set can be
generalized to rumors in different languages.
It
enables the trained classiﬁer to leverage the infor-
mation learned from one language to another.

Hurricane Sandy
Boston Marathon bombing
Sochi Olympics

ID Event
01
02
03
04 MH ﬂight 370
05
06
07
08
09
10
11
12
13
14
15
16
17

Bring Back Our Girls
Columbian Chemicals
Passport hoax
Rock Elephant
Underwater bedroom
Livr mobile app
Pig ﬁsh
Solar Eclipse
Girl with Samurai boots
Nepal Earthquake
Garissa Attack
Syrian boy
Varoufakis and zdf
Avg

Random Transfer
0.247
0.230
0.555
0.407
0.500
0.000
0.000
0.000
0.577
-
0.375
0.571
0.559
0.227
0.125
-
-
0.312

0.287
0.284
0.752
0.536
0.923
0.100
0.000
0.500
0.972
-
1.00
0.889
0.925
0.211
0.059
-
-
0.531

Table 5:
Rumor veriﬁcation performance on the
CCMR Baidu, where - indicates there is no webpage
in that event.

8.2 Analysis

In event 11 (Pig ﬁsh), Transfer achieves much
higher performance than the random baseline.
Generally, Baidu webpages’ titles are semantically
different from tweets. However, in this particular
event, the textual information of those titles and
tweets are semantically close. As a result, mod-
els learned from English rumors can easily work
on Chinese rumors, which is helpful for our trans-
fer learning. Figure 4 shows three Twitter-Baidu
rumor pairs with similar meaning in this event.

Transfer obtains pretty low F1-scores in event
07 (Passport hoax). The annotation conﬂict caused
its weak performance. This event is about a Child
drew all over his dads passport and made his dad
stuck in South Korea. During the manual annota-
tion process, we found out that it is a real event
conﬁrmed by ofﬁcial accounts according to one
news article from Chinese social media3, while
CCMR Twitter labeled such tweets as fake. Since
Transfer is pre-trained using Twitter dataset, it is
not surprising that Transfer achieves 0 in F1-score
on this event. The annotation conﬂict also brings
out that rumor veriﬁcation will beneﬁt from utiliz-
ing cross-lingual and cross-platform information.

3http://new.qq.com/cmsn/20140605/20140605002796

Alessandro Flammini. 2015. Computational fact
PloS one,
checking from knowledge networks.
10(6):e0128193.

Sarah Cohen, James T Hamilton, and Fred Turner.
2011. Computational journalism. Communications
of the ACM, 54(10):66–71.

Leon Derczynski, Kalina Bontcheva, Maria Liakata,
Rob Procter, Geraldine Wong Sak Hoi, and Arkaitz
Zubiaga. 2017. Semeval-2017 task 8: Rumoureval:
Determining rumour veracity and support for ru-
mours. arXiv preprint arXiv:1704.05972.

Nicholas Diakopoulos, Munmun De Choudhury, and
Mor Naaman. 2012. Finding and assessing social
media information sources in the context of journal-
ism. In Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems, pages 2451–
2460. ACM.

Nicholas DiFonzo and Prashant Bordia. 2007. Rumor,
gossip and urban legends. Diogenes, 54(1):19–35.

Omar Enayet and Samhaa R El-Beltagy. 2017.
Niletmrg at semeval-2017 task 8: Determining ru-
mour and veracity support for rumours on twitter. In
Proceedings of the 11th International Workshop on
Semantic Evaluation (SemEval-2017), pages 470–
474.

Zhiwei Jin, Juan Cao, Han Guo, Yongdong Zhang,
and Jiebo Luo. 2017. Multimodal fusion with re-
current neural networks for rumor detection on mi-
croblogs. In Proceedings of the 2017 ACM on Mul-
timedia Conference, pages 795–816. ACM.

Zhiwei Jin, Juan Cao, Yazi Zhang, and Yongdong
Zhang. 2015. Mcg-ict at mediaeval 2015: Verify-
ing multimedia use with a two-level classiﬁcation
model. In MediaEval.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012.
Imagenet classiﬁcation with deep con-
volutional neural networks. In Advances in neural
information processing systems, pages 1097–1105.

Christopher Matthews. 2013. How does one fake tweet
cause a stock market crash. Wall Street & Markets:
Time.

Marcelo Mendoza, Barbara Poblete,

and Carlos
Castillo. 2010. Twitter under crisis: Can we trust
what we rt? In Proceedings of the ﬁrst workshop on
social media analytics, pages 71–79. ACM.

Stuart Middleton. 2015. Extracting attributed veri-
ﬁcation and debunking reports from social media:
Mediaeval-2015 trust and credibility analysis of im-
age and video.

Vahed Qazvinian, Emily Rosengren, Dragomir R
Radev, and Qiaozhu Mei. 2011. Rumor has it: Iden-
tifying misinformation in microblogs. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1589–1599. Asso-
ciation for Computational Linguistics.

Figure 4: Example parallel rumors in the Pig ﬁsh event.

9 Conclusion

We created a new multimedia rumor veriﬁcation
dataset by extending a multimedia Twitter dataset
with external webpages from Google and Baidu
that share similar image content. We designed
a set of cross-lingual cross-platform features that
leverage the similarity and agreement between
information across different platforms and lan-
guages to verify rumors. The proposed features
are compact and generalizable across languages.
We also designed a neural network based model
that utilizes the cross-lingual cross-platform fea-
tures and achieved state-of-the-art results in auto-
matic rumor veriﬁcation.

References

Christina Boididou, Katerina Andreadou, Symeon Pa-
padopoulos, Duc-Tien Dang-Nguyen, Giulia Boato,
Michael Riegler, and Yiannis Kompatsiaris. 2015a.
Verifying multimedia use at mediaeval 2015. In Me-
diaEval.

Christina Boididou, Symeon Papadopoulos, Duc-Tien
Dang-Nguyen, Giulia Boato, and Yiannis Kompat-
siaris. 2015b. The certh-unitn participation@ veri-
fying multimedia use 2015. In MediaEval.

Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014.
Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

Giovanni Luca Ciampaglia, Prashant Shiralkar, Luis M
and

Johan Bollen, Filippo Menczer,

Rocha,

Husrev T Sencar and Nasir Memon. 2009. Overview of
state-of-the-art in digital image forensics. In Algo-
rithms, Architectures and Information Systems Secu-
rity, pages 325–347. World Scientiﬁc.

Chengcheng Shao, Giovanni Luca Ciampaglia,
Alessandro Flammini, and Filippo Menczer. 2016.
Hoaxy: A platform for tracking online misinfor-
In Proceedings of the 25th International
mation.
Conference Companion on World Wide Web,
pages 745–750. International World Wide Web
Conferences Steering Committee.

Kate Starbird, Jim Maddock, Mania Orand, Peg
Achterman, and Robert M Mason. 2014. Rumors,
false ﬂags, and digital vigilantes: Misinformation
on twitter after the 2013 boston marathon bombing.
iConference 2014 Proceedings.

Liang Tian, Derek F Wong, Lidia S Chao, Paulo
Quaresma, Francisco Oliveira, and Lu Yi. 2014.
Um-corpus: A large english-chinese parallel corpus
for statistical machine translation. In LREC, pages
1837–1842.

Ke Wu, Song Yang, and Kenny Q Zhu. 2015. False ru-
mors detection on sina weibo by propagation struc-
tures. In Data Engineering (ICDE), 2015 IEEE 31st
International Conference on, pages 651–662. IEEE.

Markos Zampoglou, Symeon Papadopoulos, and Yian-
nis Kompatsiaris. 2015. Detecting image splicing in
the wild (web). In Multimedia & Expo Workshops
(ICMEW), 2015 IEEE International Conference on,
pages 1–6. IEEE.

