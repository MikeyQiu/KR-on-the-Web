8
1
0
2
 
v
o
N
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
7
7
5
0
0
.
9
0
6
1
:
v
i
X
r
a

Generic Inference in Latent Gaussian Process Models

Edwin V. Bonilla∗†

Karl Krauth∗‡

Amir Dezfouli†

November 6, 2018

Abstract

We develop an automated variational method for inference in models with Gaussian
process (gp) priors and general likelihoods. The method supports multiple outputs
and multiple latent functions and does not require detailed knowledge of the condi-
tional likelihood, only needing its evaluation as a black-box function. Using a mixture
of Gaussians as the variational distribution, we show that the evidence lower bound
and its gradients can be estimated eﬃciently using samples from univariate Gaussian
distributions. Furthermore, the method is scalable to large datasets which is achieved
by using an augmented prior via the inducing-variable approach underpinning most
sparse gp approximations, along with parallel computation and stochastic optimiza-
tion. We evaluate our approach quantitatively and qualitatively with experiments on
small datasets, medium-scale datasets and large datasets, showing its competitiveness
under diﬀerent likelihood models and sparsity levels. On the large-scale experiments
involving prediction of airline delays and classiﬁcation of handwritten digits, we show
that our method is on par with the state-of-the-art hard-coded approaches for scalable
gp regression and classiﬁcation.

Keywords: Gaussian processes, black-box likelihoods, nonlinear likelihoods, scalable
inference, variational inference

1 Introduction

Developing fully automated methods for inference in complex probabilistic models has be-
come arguably one of the most exciting areas of research in machine learning, with notable
examples in the probabilistic programming community (see e.g. Hoﬀman and Gelman, 2014;
Wood et al., 2014; Goodman et al., 2008; Domingos et al., 2006). Indeed, while these prob-
abilistic programming systems allow for the formulation of very expressive and ﬂexible
probabilistic models, building eﬃcient inference methods for reasoning in such systems is a
signiﬁcant challenge.

One particularly interesting setting is when the prior over the latent parameters of the
model can be well described with a Gaussian process (gp, Rasmussen and Williams, 2006).
gp priors are a popular choice in practical non-parametric Bayesian modeling with perhaps

∗Joint ﬁrst author.
†Machine Learning Research Group, Data61, Sydney NSW 2015, Australia.
‡Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA

94720-1776, USA.

1

the most straightforward and well-known application being the standard regression model
with Gaussian likelihood, for which the posterior can be computed in closed form (see e.g.
Rasmussen and Williams, 2006, §2.2).

Interest in gp models stems from their functional non-parametric Bayesian nature and
there are at least three critical beneﬁts from adopting such a modeling approach. Firstly, by
being a prior over functions, they represent a much more elegant way to address problems
such as regression, where it is less natural to think of a prior over parameters such as the
weights in a linear model. Secondly, by being Bayesian, they provide us with a principled
way to combine our prior beliefs with the observed data in order to predict a full posterior
distribution over unknown quantities, which of course is much more informative than a single
point prediction. Finally, by being non-parametric, they address the common concern of
parametric approaches about how well we can ﬁt the data, since the model space is not
constrained to have a parametric form.

1.1 Key Inference Challenges in Gaussian Process Models

Nevertheless, such a principled and ﬂexible approach to machine learning comes at the
cost of facing two fundamental probabilistic inference challenges, (i) scalability to large
datasets and (ii) dealing with nonlinear likelihoods. With regards to the ﬁrst challenge,
scalability, gp models have been notorious for their poor scalability as a function of the
number of training points. Despite ground-breaking work in understanding scalable gps
through the so-called sparse approximations (Qui˜nonero-Candela and Rasmussen, 2005) and
in developing practical inference methods for these models (Titsias, 2009), recent literature
on addressing this challenge gives a clear indication that the scalability problem is still a
very active area of research, see e.g. Das et al. (2015); Deisenroth and Ng (2015); Hoang
et al. (2015); Dong et al. (2017); Hensman et al. (2017); Salimbeni et al. (2018); John and
Hensman (2018).

Concerning the second challenge, dealing with nonlinear likelihoods, the main diﬃculty
is that of estimating a posterior over latent functions distributed according to a gp prior,
given observations assumed to be drawn from a possibly nonlinear conditional likelihood
model. In general, this posterior is analytically intractable and one must resort to approx-
imate methods. These methods can be roughly classiﬁed into stochastic approaches and
deterministic approaches. The former, stochastic approaches, are based on sampling algo-
rithms such as Markov Chain Monte Carlo (mcmc, see e.g. Neal, 1993, for an overview)
and the latter, deterministic approaches, are based on optimization techniques and include
variational inference (vi, Jordan et al., 1998), the Laplace approximation (see e.g. MacKay,
2003, Chapter 27) and expectation propagation (ep, Minka, 2001).

On the one hand, although stochastic techniques such as mcmc provide a ﬂexible frame-
work for sampling from complex posterior distributions of probabilistic models, their gen-
erality comes at the expense of a very high computational cost as well as cumbersome con-
vergence analysis. On the other hand, deterministic methods such as variational inference
build upon the main insight that optimization is generally easier than integration. Conse-
quently, they estimate a posterior by maximizing a lower bound of the marginal likelihood,
the so-called evidence lower bound (elbo). Variational methods can be considerably faster

2

than mcmc but they lack mcmc’s broader applicability, usually requiring mathematical
derivations on a model-by-model basis.1

1.2 Contributions

In this paper we address the above challenges by developing a scalable automated variational
method for inference in models with Gaussian process priors and general likelihoods. This
method reduces the overhead of the tedious mathematical derivations traditionally inherent
to variational algorithms, allowing their application to a wide range of problems. In par-
ticular, we consider models with multiple latent functions, multiple outputs and non-linear
likelihoods that satisfy the following properties: (i) factorization across latent functions and
(ii) factorization across observations. The former assumes that, when there are more than
one latent function, they are generated from independent gps. The latter assumes that,
given the latent functions, the observations are conditionally independent.

Existing gp models, such as regression (Rasmussen and Williams, 2006), binary and
multi-class classiﬁcation (Nickisch and Rasmussen, 2008; Williams and Barber, 1998), warped
gps (Snelson et al., 2003), log Gaussian Cox process (Møller et al., 1998), and multi-output
regression (Wilson et al., 2012), all fall into this class of models. Furthermore, our ap-
proach goes well beyond standard settings for which elaborate learning machinery has been
developed, as we only require access to the likelihood function in a black-box manner. As
we shall see below, our inference method can scale up to very large datasets, hence we will
refer to it as savigp, which stands for scalable automated variational inference for Gaussian
process models. The key characteristics of our method and contributions of this work are
summarized below.

• Black-box likelihoods: As mentioned above, the main contribution of this work is to
be able to carry out posterior inference with gp priors and general likelihood models,
without knowing the details of the conditional likelihood (or its gradients) and only
requiring its evaluation as a black-box function.

• Scalable inference and stochastic optimization: By building upon the inducing-variable
approach underpinning most sparse approximations to gp models (Qui˜nonero-Candela
and Rasmussen, 2005; Titsias, 2009), the computational complexity of our method is
dominated by O(M 3) operations in time, where M (cid:28) N is the number of inducing
variables and N the number of observations. This is in contrast to naive inference in
gp models which has a time complexity of O(N 3). As the resulting elbo decomposes
over the training datapoints, our model is amenable to parallel computation and
stochastic optimization. In fact, we provide an implementation that can scale up to
a very large number of observations, exploiting stochastic optimization, multi-core
architectures and gpu computation.

• Joint learning of model parameters: As our approach is underpinned by variational
inference principles, savigp allows for learning of all model parameters, including

1However, we note recent developments such as variational auto-encoders (Kingma and Welling, 2014)

and refer the reader to the related work and the discussion (Sections 2.4 and 10, respectively).

3

posterior parameters, inducing inputs, covariance hyperparameters and likelihood pa-
rameters, within the same framework via maximization of the evidence lower bound
(elbo).

• Multiple outputs and multiple latent functions: savigp is designed to support models
with multiple outputs and multiple latent functions, such as in multi-class classiﬁca-
tion (Williams and Barber, 1998) and non-stationary multi-output regression (Wilson
et al., 2012). It does so in a very ﬂexible way, allowing the diﬀerent gp priors on the
latent functions to have diﬀerent covariance functions and inducing inputs.

• Flexible posterior: savigp uses a mixture of Gaussians as the approximating posterior
distribution. This is a very general approach as it is well known that, with a suﬃcient
number of components, almost any continuous density can be approximated with
arbitrary accuracy (Maz’ya and Schmidt, 1996).

• Statistical eﬃciency: By using knowledge of the approximate posterior and the struc-
ture of the gp prior, we exploit the decomposition of the elbo, into a KL-divergence
term and an expected log likelihood term, to provide statistically eﬃcient parameter
estimates. In particular, we derive an analytical lower bound for the KL-divergence
term and we show that, for general black-box likelihood models, the expected log likeli-
hood term and its gradients can be computed eﬃciently using samples from univariate
Gaussian distributions.

• Eﬃcient re-parametrization: For the case of a single full Gaussian variational pos-
terior, we show that it is possible to re-parametrize the model so that the optimal
posterior can be represented using a parametrization that is linear in the number of
observations. This parametrization becomes useful for denser models, i.e. for models
that have a larger number of inducing variables.

• Extensive experimentation: We evaluate savigp with experiments on small datasets,
medium-scale datasets and two large datasets. The experiments on small datasets
(N < 1, 300) evaluate the method under diﬀerent likelihood models and sparsity
levels (as determined by the number of inducing variables), including problems such as
regression, classiﬁcation, Log Gaussian Cox processes, and warped gps (Snelson et al.,
2003). We show that savigp can perform as well as hard-coded inference methods
under high levels of sparsity. The medium-scale experiments consider binary and
multi-class classiﬁcation using the mnist dataset (N = 60, 000) and non-stationary
regression under the gprn model (Wilson et al., 2012) using the sarcos dataset
(N ≈ 45, 000). Besides showing the competitiveness of our model for problems at this
scale, we analyze the eﬀect of learning the inducing inputs, i.e. the location of inducing
In our ﬁrst large-scale experiment, we study the problem of predicting
variables.
airline delays (using N = 700, 000), and show that our method is on par with the
state-of-the-art approach for scalable gp regression (Hensman et al., 2013), which
uses full knowledge of the likelihood model. In our second large-scale experiment, we
consider the mnist8m dataset, which is an augmented version of mnist (containing
N = 8, 100, 000 observations). We show that by using this augmented dataset we

4

can improve performance signiﬁcantly. Finally, in an experiment concerning a non-
linear seismic inversion problem, we show that our approach can be applied easily
(without any changes to the inference algorithm) to non-standard machine learning
tasks and that our inference method can match closely the solution found by more
computationally demanding approaches such as mcmc.

Before describing the family of gp models that we focus on, we start by relating our
work to the previous literature concerning the key inference challenges mentioned above,
i.e. scalability and dealing with non-linear likelihoods.

2 Related Work

As pointed out by Rasmussen and Williams (2006), there has been a long-standing inter-
est in Gaussian processes with early work dating back at least to the 1880s when Danish
astronomer and mathematician T. N. Thiel, concerned with determining the distance be-
tween Copenhagen and Lund from astronomical observations, essentially proposed the ﬁrst
In
mathematical formulation of Brownian motion (see e.g. Lauritzen, 1981, for details).
geostatistics, gp regression is known as Kriging (see e.g. Cressie, 1993) where, naturally, it
has focused on 2-dimensional and 3-dimensional problems.

While the work of O’Hagan and Kingman (1978) has been quite inﬂuential in applying
gps to general regression problems, the introduction of gp regression to main-stream ma-
chine machine learning by Williams and Rasmussen (1996) sparked signiﬁcant interest in the
machine learning community. Indeed, henceforth, the community has taken gp models well
beyond the standard regression setting, addressing other problems such as non-stationary
and heteroscedastic regression (Paciorek and Schervish, 2004; Kersting et al., 2007; Wilson
et al., 2012); nonlinear dimensionality reduction (Lawrence, 2005); classiﬁcation (Williams
and Barber, 1998; Nickisch and Rasmussen, 2008); multi-task learning (Bonilla et al., 2008;
Yu and Chu, 2008; Alvarez and Lawrence, 2009; Wilson et al., 2012); preference learn-
ing (Bonilla et al., 2010); and ordinal regression (Chu and Ghahramani, 2005).
In fact,
their book (Rasmussen and Williams, 2006) is the de facto reference in any work related
to Gaussian process models in machine learning. As we shall see below, despite all these
signiﬁcant advances, most previous work in the gp community has focused on addressing
the scalability and the non-linear likelihood challenges in isolation.

2.1 Scalable Models

The cubic scaling on the number of observations of Gaussian process models, until very
recently, has hindered the use of these models in a wider variety of applications. Work on
approaching this problem has ranged from selecting informative (inducing) datapoints from
the training data so as to facilitate sparse approximations to the posterior (Lawrence et al.,
2002) to considering these inducing points as continuous parameters and optimizing them
within a coherent probabilistic framework (Snelson and Ghahramani, 2006).

Although none of these methods actually scale to very large datasets as their time
complexity is O(M 2N ), where N is the number of observations and M is the number

5

of inducing points, unifying such approaches from a probabilistic perspective (Qui˜nonero-
Candela and Rasmussen, 2005) has been extremely valuable to the community, not only to
understand what those methods are actually doing but also to develop new approaches to
sparse gp models. In particular, the framework of Titsias (2009) which has been placed
within a solid theoretical grounding by Matthews et al. (2016), has become the underpinning
machinery of modern scalable approaches to gp regression and classiﬁcation. This has been
taken one step further by allowing optimization of variational parameters within a stochastic
optimization framework (Hensman et al., 2013), hence enabling the applicability of these
inference methods to very large datasets.

Besides variational approaches based on reverse-KL divergence minimization, other
methods have adopted diﬀerent inference engines, based e.g. on the minimization of the for-
ward KL divergence, such as expectation propagation (Hern´andez-Lobato and Hern´andez-
Lobato, 2016). It turns out that all these methods can be seen from a more general per-
spective as minimization of α-divergences, see Bui et al. (2017) and references therein.

In addition to inducing-variable approaches to scalability in gp-models, other approaches
have exploited the relationship between inﬁnite-feature linear-in-the-parameters models and
gps. In particular, early work investigated truncated linear-in-the-parameters models as
approximations to gp regression (see e.g. Tipping, 2001). This idea has been developed in
an alternative direction that exploits the relationship between a covariance function of a
stationary process and its spectral density, hence providing random-feature approximations
to the covariance function of a gp, similarly to the work of Rahimi and Recht (2008,
2009), who focused on non-probabilistic kernel machines. For example, L´azaro-Gredilla
et al. (2010), Gal and Turner (2015) and Yang et al. (2015) have followed these types of
approaches, with the latter mostly concerned about having fast kernel evaluations.

Unlike our work, none of these approaches deals with the harder task of developing

scalable inference methods for multi-output problems and general likelihood models.

2.2 Multi-output and Multi-task Gaussian Processes

Developing multi-task or multi-output learning approaches using Gaussian processes has
also proved an intensive area of research. Most gp-based models for these problems as-
sume that correlations between the outputs are induced via linear combinations of a set of
independent latent processes. Such linear combinations can use ﬁxed coeﬃcients (see e.g.
Teh et al., 2005) or input-dependent coeﬃcients (Wilson et al., 2012; Nguyen and Bonilla,
2013). Tasks dependencies can also be deﬁned explicitly through the covariance function as
done by Bonilla et al. (2008), who assume a covariance decomposition between tasks and
inputs. More complex dependencies can be modeled by using the convolution formalism as
done in earlier work by Boyle and Frean (2005) and generalized by Alvarez and Lawrence
(2009); ´Alvarez et al. (2010), whose later work also provides eﬃcient inference algorithms
for such models ( ´Alvarez and Lawrence, 2011). Finally, the work of Nguyen and Bonilla
(2014b) also assumes a linear combination of independent latent processes but it exploits
the developments of Hensman et al. (2013) to scale up to very large datasets.

Besides the work of Nguyen and Bonilla (2014b), these approaches do not scale to a
very large number of observations and all of them are mainly concerned with regression
problems.

6

2.3 General Nonlinear Likelihoods

The problem of inference in models with gp priors and nonlinear likelihoods has been
tackled from a sampling perspective with algorithms such as elliptical slice sampling (ess,
Murray et al., 2010), which are more eﬀective at drawing samples from strongly correlated
Gaussians than generic mcmc algorithms. Nevertheless, as we shall see in Section 9.3.5, the
sampling cost of ess remains a major challenge for practical usages.

From a variational inference perspective, the work by Opper and Archambeau (2009)
has been slightly under-recognized by the community even though it proposes an eﬃcient
full Gaussian posterior approximation for gp models with i.i.d. observations. Our work
pushes this breakthrough further by allowing multiple latent functions, multiple outputs,
and more importantly, scalability to large datasets.

Another approach to deterministic approximate inference is the integrated nested Laplace
approximation (inla, Rue et al., 2009). inla uses numerical integration to approximate the
marginal likelihood, which makes it unsuitable for gp models that contain a large number
of hyperparameters.

2.4 More Recent Developments

As mentioned in §1, the scalability problem continues to attract the interest of researchers
working with gp models, with recently developed distributed inference frameworks (Gal
et al., 2014; Deisenroth and Ng, 2015), and the variational inference frameworks for scal-
able gp regression and classiﬁcation by Hensman et al. (2013) and Hensman et al. (2015),
respectively. As with the previous work described in §2.1, these approaches have been lim-
ited to classiﬁcation or regression problems or speciﬁc to a particular class of likelihood
models such as gp-modulated Cox processes (John and Hensman, 2018).

Contemporarily to the work of Nguyen and Bonilla (2014a), which underpins our ap-
proach, Ranganath et al. (2014) developed black-box variational inference (bbvi) for gen-
eral latent variable models. Due to this generality, it under-utilizes the rich amount of
information available in Gaussian process models. For example, bbvi approximates the
KL-divergence term of the evidence lower bound but this is computed analytically in our
method. Additionally, for practical reasons, bbvi imposes the variational distribution to
fully factorize over the latent variables, while we make no such a restriction. A clear dis-
advantage of bbvi is that it does not provide a practical way of learning the covariance
hyperparameters of gps — in fact, these are set to ﬁxed values. In principle, these values
can be learned in bbvi using stochastic optimization, but experimentally, we have found
this to be problematic, ineﬀectual, and time-consuming.

Very recently, Bonilla et al. (2016) have used the random-feature approach mentioned in
Section 2.1 along with linearization techniques to provide scalable methods for inference in
gp models with general likelihoods. Unlike our approach for estimating expectations of the
conditional likelihood, such linearizations of the conditional likelihood are approximations
and generally do not converge to the exact expectations even in the limit of a large number
of observations.

Following the recent advances in making automatic diﬀerentiation widely available and
easy to use in practical systems (see e.g. Baydin et al., 2015), developments on stochastic
variational inference for fairly complex probabilistic models (Rezende et al., 2014; Kingma

7

and Welling, 2014) can be used when the likelihood can be implemented in such systems
and can be expressed using the so-called re-parametrization trick (see e.g. Kingma and
Welling, 2014, for details). Nevertheless, the advantage of our method over these generic
approaches is twofold. Firstly, as with bbvi, in practice such a generality comes at the cost
of making assumptions about the posterior (such as factorization), which is not suitable for
gp models. Secondly, and more importantly, such methods are not truly black-box as they
require explicit access to the implementation of the conditional likelihood. An interesting
example where one cannot apply the re-parametrization trick is given by Challis and Barber
(2013), who describe a large class of functions (that include the Laplace log likelihood) that
are neither diﬀerentiable or continuous but their expectation over a Gaussian posterior is
smooth. A more general setting where a truly black-box approach is required concerns
inversion problems (Tarantola, 2004) where latent functions are passed through domain-
speciﬁc forward models followed by a known noise model (Bonilla et al., 2016; Reid et al.,
2013). These forward models may be non-diﬀerentiable, given as an executable, or too
complex to re-implement quickly in an automatic diﬀerentiation framework. To illustrate
this case, we present results on a seismic inversion application in §9.8.

Nevertheless, we acknowledge that some of these developments mentioned above have
been extended to the gp literature, where the re-parametrization trick has been used
(Krauth et al., 2017; Matthews et al., 2017; Hensman et al., 2017; Cutajar et al., 2017).
These contemporary works show that it is worthwhile building more tailored methods that
may require a lower number of samples to estimate the expectations in our framework.

Finally, a related area of research is that of modeling complex data with deep belief net-
works based on Gaussian process mappings (Damianou and Lawrence, 2013), which have
been proposed primarily as hierarchical extensions of the Gaussian process latent variable
model (Lawrence, 2005). Unlike our approach, these models target the unsupervised prob-
lem of discovering structure in high-dimensional data and have focused mainly on small-data
applications. However, much like the recent work on “shallow”-gp architectures, inference
in these models have also been made scalable for supervised and unsupervised learning, ex-
ploiting the reparameterization trick, e.g. using random-feature expansions (Cutajar et al.,
2017) or inducing-variable approximations (Salimbeni and Deisenroth, 2017).

3 Latent Gaussian Process Models

Before starting our description of the types of models we are addressing in this paper, we
refer the reader to Appendix L for a summary of the notation used henceforth.

We consider supervised learning problems where we are given a dataset D = {xn, yn}N
n=1,
where xn is a D-dimensional input vector and yn is a P -dimensional output. Our goal is to
learn the mapping from inputs to outputs, which can be established via Q underlying latent
functions {fj}Q
j=1. In many problems these latent functions have a physical interpretation,
while in others they are simply nuisance parameters and we just want to integrate them
out in order to make probabilistic predictions.

A sensible modeling approach to the above problem is to assume that the Q latent
functions {fj} are uncorrelated a priori and that they are drawn from Q zero-mean Gaussian

8

processes (Rasmussen and Williams, 2006):

p(fj|θj) ∼ GP (0, κj(·, ·; θj)) ,

j = 1, . . . Q,

then

p(f |θ) =

p(f·j|θj) =

N (f·j; 0, Kj

xx),

(1)

Q
(cid:89)

j=1

Q
(cid:89)

j=1

where f is the set of all latent function values; f·j = {fj(xn)}N
n=1 denotes the values of
latent function j; Kj
xx is the covariance matrix induced by the covariance function κj(·, ·; θj)
evaluated at every pair of inputs; and θ = {θj} are the parameters of the corresponding
covariance functions. Along with the prior in Equation (1), we can also assume that our
multi-dimensional observations {yn} are i.i.d. given the corresponding set of latent functions
{fn}:

p(y|f , φ) =

p(yn|fn·, φ),

(2)

N
(cid:89)

n=1

where y is the set of all output observations; yn is the nth output observation; fn· =
{fj(xn)}Q
j=1 is the set of latent function values which yn depends upon; and φ are the
conditional likelihood parameters. In the sequel, we will refer to the covariance parameters
(θ) as the model hyperparameters.

In other words, we are interested in models for which the following criteria are satisﬁed:

(i) factorization of the prior over the latent functions, as speciﬁed by Equation (1), and

(ii) factorization of the conditional likelihood over the observations given the latent func-

tions, as speciﬁed by Equation (2).

We refer to the models satisfying the above assumptions (when using gp priors) as la-
tent Gaussian process models (lgpms). Interestingly, a large class of problems can be well
modeled with the above assumptions. For example binary classiﬁcation (Nickisch and Ras-
mussen, 2008; Williams and Barber, 1998), warped gps (Snelson et al., 2003), log Gaussian
Cox processes (Møller et al., 1998), multi-class classiﬁcation (Williams and Barber, 1998),
and multi-output regression (Wilson et al., 2012) all belong to this family of models.

More importantly, besides the i.i.d. assumption, there are not additional constraints
on the conditional likelihood which can be any linear or nonlinear model. Furthermore,
as we shall see in §4, our proposed inference algorithm only requires evaluations of this
likelihood model in a black-box manner, i.e. without requiring detailed knowledge of its
implementation or its gradients.

3.1 Inference Challenges

As mentioned in §1, for general lgpms, the inference problem of estimating the posterior
distribution over the latent functions given the observed data p(f |D) and, ultimately, for
a new observation x(cid:63), estimating the predictive posterior distribution p(f(cid:63)·|x(cid:63), D), poses
two important challenges (even in the case of a single latent function Q = 1) from the

9

computational and statistical perspectives. These challenges are (i) scalability to large
datasets and (ii) dealing with nonlinear likelihoods.

We address the scalability challenge inherent to gp models (given their cubic time com-
plexity on the number of observations) by augmenting our priors using an inducing-variable
approach (see e.g. Qui˜nonero-Candela and Rasmussen, 2005) and embedding our model into
a variational inference framework (Titsias, 2009). In short, inducing variables in sparse gp
models act as latent summary statistics, avoiding the computation of large inverse covari-
ances. Crucially, unlike other approaches (Qui˜nonero-Candela and Rasmussen, 2005), we
keep an explicit representation of these variables (and integrate them out variationally),
which facilitates the decomposition of the variational objective into a sum on the individual
datapoints. This allows us to devise stochastic optimization strategies and parallel imple-
mentations in cloud computing services such as Amazon EC2. Such a strategy, also allows us
to learn the location of the inducing variables (i.e. the inducing inputs) in conjunction with
variational parameters and hyperparameters, which in general provides better performance,
especially in high-dimensional problems.

To address the nonlinear likelihood challenge, which from a variational perspective boils
down to estimating expectations of a nonlinear function over the approximate posterior,
we follow a stochastic estimation approach, in which we develop low-variance Monte Carlo
estimates of the expected log-likelihood term in the variational objective. Crucially, we
will show that the expected log-likelihood term can be estimated eﬃciently by using only
samples from univariate Gaussian distributions.

4 Scalable Inference

Here we describe our scalable inference method for the model class speciﬁed in §3. We
build upon the inducing-variable formalism underlying most sparse gp approximations
(Qui˜nonero-Candela and Rasmussen, 2005; Titsias, 2009) and obtain an algorithm with
time complexity O(M 3), where M is the number of inducing variables per latent process.
We show that, under this sparse approximation and the variational inference framework,
the expected log likelihood term and its gradient can be estimated using only samples from
univariate Gaussian distributions.

4.1 Augmented Prior

In order to make inference scalable we redeﬁne our prior in terms of some auxiliary variables
{u·j}Q
j=1, which we will refer to as the inducing variables. These inducing variables lie in
the same space as {f·j} and are drawn from the same zero-mean gp priors. As before,
we assume factorization of the prior across the Q latent functions. Hence the resulting
augmented prior is given by:

Q
(cid:89)

p(u) =

N (u·j; 0, Kj

zz),

p(f |u) =

N (f·j; ˜µj, (cid:101)Kj), where

Q
(cid:89)

j=1

j=1
˜µj = Kj
(cid:101)Kj = Kj

zz)−1u·j, and

xz(Kj
xx − AjKj

zx with Aj = Kj

xz(Kj

zz)−1,

(3)

(4)

10

where u·j are the inducing variables for latent process j; u is the set of all the inducing
variables; Zj are all the inducing inputs for latent process j; X is the matrix of all input
locations {xn}; and Kj
uv is the covariance matrix induced by evaluating the covariance
function κj(·, ·) at all pairwise columns of matrices U and V. We note that while each of
the inducing variables in u·j lies in the same space as the elements in f·j, each of the M
inducing inputs in Zj lies in the same space as each input data point xn. Therefore, while
u·j is a M -dimensional vector, Zj is a M × D matrix where each of the rows corresponds
to a D-dimensional inducing input. We refer the reader to Appendix L for a summary of
the notation and dimensionality of the above kernel matrices.

As thoroughly investigated by Qui˜nonero-Candela and Rasmussen (2005), most gp ap-
proximations can be formulated using the augmented prior above and additional assump-
tions on the training and test conditional distributions p(f |u) and p(f(cid:63)·|u), respectively.
Such approaches have been traditionally referred to as sparse approximations and we will
use this terminology as well. Analogously, we will refer to models with a larger number of
inducing inputs as denser models.

It is important to emphasize that the joint prior p(f , u) deﬁned in Equation (3) is an
equivalent prior to that in the original model, as if we integrate out the inducing variables
u from this joint prior we will obtain the prior p(f ) in Equation (1) exactly. Nevertheless,
as we shall see later, following a variational inference approach and having an explicit rep-
resentation of the (approximate) posterior over the inducing variables will be fundamental
to scaling up inference in these types of models without making the assumptions on the
training or test conditionals described in Qui˜nonero-Candela and Rasmussen (2005).

Along with the joint prior deﬁned above, we maintain the factorization assumption of

the conditional likelihood given in Equation (2).

4.2 Variational Inference and the Evidence Lower Bound

Given the prior in Equation (3) and the likelihood in Equation (2), posterior inference
for general (non-Gaussian) likelihoods is analytically intractable. Therefore, we resort to
approximate methods such as variational inference (Jordan et al., 1998). Variational in-
ference methods entail positing a tractable family of distributions and ﬁnding the member
of the family that is “closest” to the true posterior in terms of their the Kullback-Leibler
divergence. In our case, we are seeking to approximate the joint posterior p(f , u|y) with a
variational distribution q(f , u|λ).

4.3 Approximate Posterior

Motivated by the fact that the true joint posterior is given by p(f , u|y) = p(f |u, y)p(u|y),
our approximate posterior has the form:

q(f , u|λ) = p(f |u)q(u|λ),

(5)

where p(f |u) is the conditional prior given in Equation (3) and q(u|λ) is our approximate
(variational) posterior. This decomposition has proved eﬀective in regression problems with
a single latent process and a single output (see e.g. Titsias, 2009).

11

Hence, we can deﬁne our variational distribution using a mixture of Gaussians (mog):

q(u|λ) =

πkqk(u|mk, Sk) =

πk

N (u·j; mkj, Skj),

(6)

K
(cid:88)

k=1

K
(cid:88)

Q
(cid:89)

k=1

j=1

where λ = {πk, mkj, Skj} are the variational parameters: the mixture proportions {πk},
the posterior means {mkj} and posterior covariances {Skj} of the inducing variables cor-
responding to mixture component k and latent function j. We also note that each of the
mixture components qk(u|mk, Sk) is a Gaussian with mean mk and block-diagonal covari-
ance Sk.

An early reference for using a mixture of Gaussians (mog) within variational inference
is given by Bishop et al. (1998) in the context of Bayesian networks. Similarly, Gershman
et al. (2012) have used mog for non-gp models and, unlike our approach, used a second-order
Taylor series approximation of the variational lower bound.

4.4 Variational Lower Bound

It is easy to show that minimizing the Kullback-Leibler divergence between our approximate
posterior and the true posterior, KL(q(f , u|λ)(cid:107)p(f , u|y)) is equivalent to maximizing the
log-evidence lower bound (Lelbo), which is composed of a KL-term (Lkl) and an expected
log-likelihood term (Lell). In other words:

log p(y) ≥ Lelbo(λ) def= Lkl(λ) + Lell(λ), where

Lkl(λ) = −KL(q(f , u|λ)(cid:107)p(f , u)), and
Lell(λ) = Eq(f ,u|λ)[log p(y|f )],

(7)

(8)

(9)

where Eq(x)[g(x)] denotes the expectation of function g(x) over distribution q(x). Here
we note that Lkl is a negative KL divergence between the joint approximate posterior
q(f , u|λ) and the joint prior p(f , u). Therefore, maximization of the Lelbo in Equation (7)
entails minimization of an expected loss (given by the negative expected log-likelihood Lell)
regularized by Lkl, which imposes the constraint of ﬁnding solutions to our approximate
posterior that are close to the prior in the KL-sense.

An interesting observation of the decomposition of the Lelbo objective is that, unlike
Lell, Lkl in Equation (8) does not depend on the conditional likelihood p(y|f ), for which we
do not assume any speciﬁc parametric form (i.e. black-box likelihood). We can thus address
the technical diﬃculties regarding each component and their gradients separately using
diﬀerent approaches. In particular, for Lkl we will exploit the structure of the variational
posterior in order to avoid computing KL-divergences over distributions involving all the
data. Furthermore, we will obtain a lower bound for Lkl in the general case of q(u) being a
mixture-of-Gaussians (mog) as given in Equation (6). For the expected log-likelihood term
(Lell) we will use a Monte Carlo approach in order to estimate the required expectation and
its gradients.

12

4.5 Computation of the KL-divergence term (Lkl)

In order to have an explicit form for Lkl and its gradients, we start by expanding Equation
(8):

Lkl(λ) = −KL(q(f , u|λ)(cid:107)p(f , u)) = −Eq(f ,u|λ)
(cid:20)

(cid:21)

= −Ep(f |u)q(u|λ)

log

q(u|λ)
p(u)

,

= −KL(q(u|λ)(cid:107)p(u)),

(cid:20)

log

q(f , u|λ)
p(f , u)

(cid:21)

,

(10)

(11)

(12)

where we have applied the deﬁnition of the KL-divergence in Equation (10); used the
variational joint posterior q(f , u|λ) given in Equation (5) to go from Equation (10) to
Equation (11); and integrated out f to obtain Equation (12). We note that the deﬁnition of
the joint posterior q(f , u|λ) in Equation (5) has been crucial to transform a KL-divergence
between the joint approximate posterior and the joint prior into a KL-divergence between
the variational posterior q(u|λ) and the prior p(u) over the inducing variables. In doing that,
we have avoided computing a KL-divergence between distributions over the N -dimensional
variables f·j. As we shall see later, this implies a reduction in time complexity from O(N 3)
to O(M 3), where N is the number of datapoints and M is the number of inducing variables.
We now decompose the resulting KL-divergence term in Equation (12) as follows,

Lkl(λ) = −KL(q(u|λ)(cid:107)p(u)) = Lent(λ) + Lcross(λ), where:
Lent(λ) = −Eq(u|λ)[log q(u|λ)], and
Lcross(λ) = Eq(u|λ)[log p(u)],

(13)

where Lent(λ) denotes the diﬀerential entropy of the approximating distribution q(u|λ) and
Lcross(λ) denotes the negative cross-entropy between the approximating distribution q(u|λ)
and the prior p(u).

Computing the entropy of the variational distribution in Equation (6), which is a
mixture-of-Gaussians (mog), is analytically intractable. However, a lower bound of this
entropy can be obtained using Jensen’s inequality (see e.g. Huber et al., 2008) giving:

Lent(λ) ≥ −

πk log

π(cid:96)N (mk; m(cid:96), Sk + S(cid:96)) def= (cid:98)Lent.

(14)

K
(cid:88)

k=1

K
(cid:88)

(cid:96)=1

The negative cross-entropy in Equation (13) between a Gaussian mixture q(u|λ) and a
Gaussian p(u), can be obtained analytically,

Lcross(λ) = −

[M log 2π + log (cid:12)

(cid:12)Kj
zz

(cid:12)
(cid:12) + mT

kj(Kj

zz)−1mkj + tr (Kj

zz)−1Skj]. (15)

1
2

K
(cid:88)

Q
(cid:88)

πk

k=1

j=1

The derivation of the entropy term and the cross-entropy term in Equations (14) and (15)
is given in Appendix A.

13

4.6 Estimation of the expected log likelihood term (Lell)

We now address the computation of the expected log likelihood term in Equation (9). The
main diﬃculty of computing this term is that, unlike the Lkl where we have full knowledge
of the prior and the approximate posterior, here we do not assume a speciﬁc form for the
conditional likelihood p(y|f , φ). Furthermore, we only require evaluations of log p(yn|fn·, φ)
for each datapoint n, hence yielding a truly black-box likelihood method.

We will show one of our main results, that of statistical eﬃciency of our Lell estimator.
This means that, despite having a full Gaussian approximate posterior, estimation of the
Lell and its gradients only requires samples from univariate Gaussian distributions. We
start by expanding Equation (9) using our deﬁnitions of the approximate posterior and the
factorization of the conditional likelihood:

Lell(λ) = Eq(f ,u|λ)[log p(y|f , φ)] = Eq(f |λ)[log p(y|f , φ)],

(16)

where, given the deﬁnition of the approximate joint posterior q(f , u|λ) in Equation (5), the
distribution q(f |λ) resulting from marginalizing u from this joint posterior can be obtained
analytically,

q(f |λ) =

πkqk(f |λk) =

πk

N (f·j; bkj, Σkj), with

K
(cid:88)

K
(cid:88)

Q
(cid:89)

k=1

j=1

k=1
bkj = Ajmkj, and
Σkj = (cid:101)Kj + AjSkjAT
j ,

where (cid:101)Kj and Aj are given in Equation (4) and, as deﬁned before, {mkj} and {Skj} are
the posterior means and posterior covariances of the inducing variables corresponding to
mixture component k and latent function j. We are now ready to state our result of a
statistically eﬃcient estimator for the Lell:

Theorem 1 For the gp model with prior deﬁned in Equations (3) to (4), and conditional
likelihood deﬁned in Equation (2), the expected log likelihood over the variational distribution
in Equation (5) and its gradients can be estimated using samples from univariate Gaussian
distributions.

The proof is constructive and can be found in Appendix B. We note that a less general
result, for the case of one latent function and a single variational Gaussian posterior, was
obtained in Opper and Archambeau (2009) using a diﬀerent derivation. Here we state our
ﬁnal result on how to compute these estimates:

Lell(λ) =

πkEqk(n)(fn·|λk)[log p(yn|fn·, φ)],

N
(cid:88)

K
(cid:88)

n=1

k=1
N
(cid:88)

n=1

∇λk Lell(λ) = πk

Eqk(n)(fn·|λk)

(cid:2)∇λk log qk(n)(fn·|λk) log p(yn|fn·, φ)(cid:3), for λk ∈ {mk, Sk},

(17)

(18)

(19)

(20)

(21)

14

∇πk Lell(λ) =

Eqk(n)(fn·|λk)[log p(yn|fn·, φ)],

(22)

N
(cid:88)

n=1

where qk(n)(fn·|λk) is a Q-dimensional Gaussian with:

qk(n)(fn·|λk) = N (fn·; bk(n), Σk(n)),

where Σk(n) is a diagonal matrix. The jth element of the mean and the (j, j)th entry of
the covariance of the above distribution are given by:

[bk(n)]j = aT

jnmkj,

[Σk(n)]j,j = [ (cid:101)Kj]n,n + aT

jnSkjajn,

(23)

def= [Aj]:,n denotes the M -dimensional vector corresponding to the nth column of
where ajn
matrix Aj; (cid:101)Kj and Aj are given in Equation (4); and, as before, {mkj, Skj} are the varia-
tional parameters corresponding to the mean and covariance of the approximate posterior
over the inducing variables for mixture component k and latent process j.

We emphasize that when Q > 1, qk(n)(fn·|λk) is not a univariate marginal but a Q-
dimensional marginal posterior with diagonal covariance. Therefore, only samples from
univariate Gaussians are required to estimate the expressions in Equations (20) to (22).

4.6.1 Practical consequences and unbiased estimates

There are two immediate practical consequences of the result in Theorem 1. The ﬁrst
consequence is that we can use unbiased empirical estimates of the expected log likelihood
term and its gradients. In our experiments we use Monte Carlo (mc) estimates, hence we
can compute Lell as:

(cid:110)

f (k,i)
n·

(cid:111)S

i=1

∼ N (fn·; bk(n), Σk(n)), k = 1, . . . , K,

(cid:98)Lell =

1
S

N
(cid:88)

K
(cid:88)

S
(cid:88)

πk

n=1

k=1

i=1

log p(yn|f (k,i)

n·

, φ),

(24)

where bk(n) and Σk(n) are the vector and matrix forms of the mean and covariance of the
posterior over the latent functions as given in Equation (23). Analogous mc estimates of
the gradients are given in Appendix C.2.

The second practical consequence is that in order to estimate the gradients of the Lell,
using Equations (21) and (22), we only require evaluations of the conditional likelihood in a
black-box manner, without resorting to numerical approximations or analytical derivations
of its gradients.

5 Parameter Optimization

In order to learn the parameters of our model we seek to maximize our (estimated) log-
evidence lower bound ( (cid:98)Lelbo) using gradient-based optimization. Let η = {λ, θ, φ} be all
the model parameters for which point estimates are required. We have that:

(cid:98)Lelbo(η) def= (cid:98)Lent(η) + Lcross(η) + (cid:98)Lell(η),

15

∇η (cid:98)Lelbo = ∇η (cid:98)Lent + ∇ηLcross + ∇η (cid:98)Lell,

where we have made explicit the dependency of the log-evidence lower bound on any pa-
rameter of the model and (cid:98)Lent, Lcross, and (cid:98)Lell are given in Equations (14), (15), (24)
respectively. The gradients of (cid:98)Lelbo wrt variational parameters λ, covariance hyperparam-
eters θ and likelihood parameters φ are given in Appendices C, D.1, and D.2, respectively.
As shown in these appendices, not all constituents of (cid:98)Lelbo contribute to learning all pa-
rameters, for example ∇θ (cid:98)Lent = ∇φ (cid:98)Lent = ∇φLcross = 0.

Using the above objective function ( (cid:98)Lelbo) and its gradients we can consider batch opti-
mization with limited-memory bfgs for small datasets and medium-size datasets. However,
under this batch setting, the computational cost can be too large to be aﬀorded in practice
even for medium-size datasets on single-core architectures.

5.1 Stochastic optimization

To deal with the above problem, we ﬁrst note that the terms corresponding to the KL-
divergence (cid:98)Lent and Lcross in Equations (14) and (15) do not depend on the observed data,
hence their computational complexity is independent of N . More importantly, we note that
(cid:98)Lell in Equation (24) decomposes as a sum of expectations over individual datapoints. This
makes our inference framework amenable to parallel computation and stochastic optimiza-
tion (Robbins and Monro, 1951). More precisely, we can rewrite (cid:98)Lelbo as:

(cid:98)Lelbo =

(cid:98)Lent + Lcross

(cid:16)

N
(cid:88)

n=1

(cid:20) 1
N

(cid:17)

+ (cid:98)L(n)

ell

(cid:21)

,

(cid:80)S

(cid:80)K

k=1 πk

ell = 1
S

i=1 log p(yn|f (k,i)

where (cid:98)L(n)
, φ), which enables us to apply stochastic opti-
mization techniques such as stochastic gradients descend (sgd, Robbins and Monro, 1951)
or adadelta (Zeiler, 2012). The complexity of the resulting algorithm is independent of
N and dominated by algebraic operations that are O(M 3) in time, where M is the num-
ber of inducing points per latent process. This makes our automated variational inference
framework practical for very large datasets.

n·

5.2 Reducing the variance of the gradients with control variates

Theorem 1 is fundamental to having a statistical eﬃcient algorithm that only requires sam-
pling from univariate Gaussian distributions (instead of sampling from very high-dimensional
Gaussians) for the estimation of the expected log likelihood term and its gradients.

However, the variance of the gradient estimates may be too large for the algorithm to
work in practice, and variance reduction techniques become necessary. Here we use the well-
known technique of control variates (see e.g. Ross, 2006, §8.2), where a new gradient estimate
is constructed so as to have the same expectation but lower variance than the original
estimate. Our control variate is the so-called score function h(fn·) = ∇λk log qk(n)(fn·|λk)
and full details are given in Appendix F.

16

5.3 Optimization of the inducing inputs

So far we have discussed optimization of variational parameters (λ), i.e. the parameters of
the approximate posterior ({πk, mk, Sk}); covariance hyperparameters (θ); and likelihood
parameters (φ). However, as discussed by Titsias (2009), the inducing inputs {Zj} can
be seen as additional variational parameters and, therefore, their optimization should be
somehow robust to overﬁtting. As described in the Experiments (Section 9.5), learning of
the inducing inputs can improve performance, requiring a lower number of inducing variables
than when these are ﬁxed. This, of course, comes at an additional computational cost which
can be signiﬁcant when considering high-dimensional input spaces. As with the variational
parameters, we study learning of the inducing inputs via gradient-based optimization, for
which we use the gradients provided in Appendix E.

Early references in the machine learning community where a single variational objective
is used for parameter inference (in addition to posterior estimation over latent variables)
can be found in Hinton and van Camp (1993); MacKay (1995); Lappalainen and Miskin
(2000). These methods are now known under the umbrella term of variational Bayes2 and
consider a prior and an approximate posterior for these parameters within the variational
framework. As mentioned above, rather than a full variational Bayes approach, we provide
point estimates of {θ, φ} and {Zj}, and our experiments in §9 conﬁrm the eﬃcacy of our
approach. More speciﬁcally, we show in §9.5 that point-estimation of the inducing inputs
{Zj} using the variational objective can be signiﬁcantly better than using heuristics such
as k-means clustering.

6 Dense Posterior and Practical Distributions

In this section we consider the case when the inducing inputs are placed at the training
points, i.e. Zj = X and consequently M = N . As mentioned in Section 4.1, we refer to
this setting as dense to distinguish it from the case when M < N , for which the resulting
models are usually called sparse approximations. It is important to realize that not all real
datasets are very large and that in many cases the resulting time and memory complexity
O(N 3) and O(N 2) can be aﬀorded. Besides the dense posterior case, we also study some
particular variational distributions that make our framework more practical, especially in
large-scale applications.

6.1 Dense Approximate Posterior

When our posterior is dense the only approximation made is the assumed variational dis-
tribution in Equation (6). We will show that, in this case, we recover the objective function
in Nguyen and Bonilla (2014a) and that hyper-parameter learning is easier as the terms
in the resulting objective function that depend on the hyperparameters do not involve mc
estimates. Therefore, their analytical gradients can be used. Furthermore, in the following

2We note that these methods were then referred to as “Ensemble Learning”, since a full distribution (i.e. an
ensemble of parameters) was used instead of a single point estimate. Fortunately, nowadays, variational
Bayes is a preferred term since Ensemble Learning is more commonly associated with methods for combining
multiple models.

17

section, we will provide an eﬃcient exact parametrization of the posterior covariance that
reduces the O(N 2) memory complexity to a linear complexity O(N ).

zz = Kj

We start by looking at the components of Lelbo when we make Zj = X, which we
can simply obtain by making Kj
xx and M = N , and realizing that the posterior
parameters {mkj, Skj} are now N -dimensional objects. Therefore, we leave the entropy
term (cid:98)Lent in Equation (14) unchanged and we replace all the appearances of Kj
zz with Kj
xx
and all the appearances of M with N for the Lcross. We refer the reader to Appendix G for
details of the equations but it is easy to see that the resulting (cid:98)Lent and Lcross are identical
to those obtained by Nguyen and Bonilla (2014a, Equations 5 and 6).

For the expected log likelihood term, the generic expression in Equation (16) still applies
but we need to ﬁgure out the resulting expressions for the approximate posterior parame-
ters in Equations (18) and (19). It is easy to show that the resulting posterior means and
covariances are in fact bkj = mkj and Σkj = Skj (see Appendix G for details). This means
that in the dense case we simply estimate the (cid:98)Lell by using empirical expectations over the
unconstrained variational posterior q(f |λ), with ‘free’ mean and covariance parameters. In
contrast, in the sparse case, although these expectations are still computed over q(f |λ), the
parameters of the variational posterior q(f |λ) are constrained by Equations (18) and (19)
which are functions of the prior covariance and the parameters of the variational distribution
over the inducing variables q(u|λ). As we shall see in the following section, this distinc-
tion between the dense case and sparse case has critical consequences on hyperparameter
learning.

6.1.1 Exact Hyperparameter Optimization

The above insight reveals a remarkable property of the model in the dense case. Unlike
the sparse case, the expected log likelihood term does not depend on the covariance hyper-
parameters, as the expectation of the conditional likelihood is taken over the variational
distribution q(f |λ) with ‘free’ parameters. Therefore, only the cross-entropy term Lcross
depends on the hyperparameters (as we also know that ∇θ (cid:98)Lent = 0). For this term, as seen
in Appendix G.1 and corresponding gradients in Equation (37), we have derived the exact
(analytical) expressions for the objective function and its gradients, avoiding empirical mc
estimates altogether. This has a signiﬁcant practical implication: despite using black-box
inference, the hyperparameters are optimized wrt the true evidence lower bound (given
ﬁxed variational parameters). This is an additional and crucial advantage of our automated
inference method over other generic inference techniques (see e.g. Ranganath et al., 2014),
which do not exploit knowledge of the prior.

6.1.2 Exact Solution with Gaussian Likelihoods

Another interesting property of our approach arises from the fact that, as we are using mc
estimates, (cid:98)Lell is an unbiased estimator of Lell. This means that, as the number of samples
S increases, (cid:98)Lelbo will converge to the true value Lelbo. In the case of a Gaussian likelihood,
the posterior over the latent functions is also Gaussian and a variational approach with a
full Gaussian posterior will converge to the true parameters of the posterior, see e.g. Opper
and Archambeau (2009) and Appendix G.2 for details.

18

6.2 Practical Variational Distributions

As we have seen in Section 5, learning of all parameters in the model can be done in a
scalable way through stochastic optimization for general likelihood models, providing au-
tomated variational inference for models with Gaussian process priors. However, the gen-
eral mog approximate posterior in Equation (6) requires O(M 2) variational parameters for
each covariance matrix of the corresponding latent process, yielding a total requirement of
O(QKM 2) parameters. This may cause diﬃculties for learning when these parameters are
optimized simultaneously. In this section we introduce two special members of the assumed
variational posterior family that improve the practical tractability of our inference frame-
work. These members are a full Gaussian posterior and a mixture of diagonal Gaussians
posterior.

6.2.1 Full Gaussian Posterior

This instance considers the case of only one component in the mixture (K = 1) in Equation
(6),which has a Gaussian distribution with a full covariance matrix for each latent process.
Therefore, following the factorization assumption in the posterior across latent processes,
the posterior distribution over the inducing variables u, and consequently over the latent
functions f , is a Gaussian with block diagonal covariance, where each block is a full covari-
ance corresponding to that of a single latent function. We thus refer to this approximate
posterior as the full Gaussian posterior (fg).

6.2.2 Mixture of Diagonal Gaussians Posterior

Our second practical variational posterior considers a mixture distribution as in Equation
(6), constraining each of the mixture components for each latent process to be a Gaus-
sian distribution with diagonal covariance matrix. Therefore, following the factorization
assumption in the posterior across latent processes, the posterior distribution over the in-
ducing variables u is a mixture of diagonal Gaussians. However, we note that, as seen
in Equations (18) and (19), the posterior over the latent functions f is not a mixture of
diagonal Gaussians in the general sparse case. Obviously, in the dense case (where Zj = X)
the posterior covariance over f of each component does have a diagonal structure. Hence-
forth, we will refer to this approximation simply as mog, to distinguish it from the fg case
above, while avoiding the use of additional notation. One immediate beneﬁt of using this
approximate posterior is computational, as we avoid the inversion of a full covariance for
each component in the mixture.

As we shall see in the following sections, there are additional beneﬁts from the assumed
practical distributions and they concern the eﬃcient parametrization of the covariance for
both distributions and the lower variance of the gradient estimates for the mog posterior.

6.3 Eﬃcient Re-parametrization

As mentioned above, one of the main motivations for having speciﬁc practical distribu-
tions is to reduce the computational overhead due to the large number of parameters to
optimize. For the mog approximation, it is obvious that only O(M ) parameters for each

19

latent process and mixture component are required to represent the posterior covariance,
hence one obtains an eﬃcient parametrization by deﬁnition. However, for the full Gaussian
(fg) approximation, naively, one would require O(M 2) parameters. The following theorem
states that for settings that require a large number of inducing variables, the fg variational
distribution can be represented using a signiﬁcantly lower number of parameters.

Theorem 2 The optimal full Gaussian variational posterior can be represented using a
parametrization that is linear in the number of observations (N ).

Before proceeding with the analysis of this theorem, we remind the reader that the general
form of our variational distribution in Equation (6) requires O(KQM 2) parameters for the
covariance, for a model with K mixture components, Q latent processes and M inducing
variables. Nevertheless, for simplicity and because usually K (cid:28) M and Q (cid:28) M , we will
omit K and Q in in the following discussion.

The proof the theorem can be found in Appendix H, where it is shown that in the fg

case the optimal solution for the posterior covariance is given by:

(cid:99)Sj = Kj
zz

(cid:0)Kj

zz + Kj

zxΛjKj
xz

(cid:1)−1

Kj

zz,

(25)

where Λj is a N -dimensional diagonal matrix. Since the optimal covariance can be expressed
in terms of ﬁxed kernel computations and a free set of parameters given by Λj, only N
parameters are necessary to represent the posterior covariance. As we shall see below, this
parametrization becomes useful for denser models, i.e. for models that have a large number
of inducing variables.

6.3.1 Sparse Posterior

In the sparse case the inducing inputs Zj are at arbitrary locations and, more importantly,
M (cid:28) N , the number of inducing variables is considerably smaller than the number of
training points. The result in Equation (25) implies that if we parameterize Sj in that way,
we will need O(N ) parameters instead of O(M 2). Of course this is useful when roughly
N < M 2
2 . A natural question arises when we deﬁne the number of inducing points as a
fraction of the number of training points, i.e. M = (cid:15)N with 0 ≤ (cid:15) ≤ 1, when is such a
parameterization useful? In this case, using the alternative parametrization will become
N . To illustrate this, consider for example the mnist dataset used
beneﬁcial when (cid:15) >
in our medium-scale experiments in Section 9.4 where N = 60, 000. This yields a beneﬁcial
regime around roughly (cid:15) > 0.006, which is a realistic setting. In fact, in our experiments on
this dataset we did consider sparsity factors of this magnitude. For example, our biggest
experiment used (cid:15) = 0.04. With a naive parametrization of the posterior covariance, we
would need roughly 2 × 106 parameters. In contrast, by using the eﬃcient parametrization
we only need 50 × 103 parameters. As shown below, these gains are greater as the model
becomes denser, yielding a dramatic reduction in the number of parameters when having a
fully dense Gaussian posterior.

(cid:113) 2

20

6.3.2 Dense Posterior

In the dense case we have that Zj = X, ∀j = 1, . . . , Q and consequently M = N . Therefore
we have that the optimal posterior covariance is given by:

(cid:99)Sj = (cid:0)(Kj

xx)−1 + Λj

(cid:1)−1

.

In principle, the parametrization of the posterior covariance would require O(N 2) parame-
ters for each latent process. However, the above result shows that we can parametrize these
covariances eﬃciently using only O(N ) parameters.

We note that, for models with Q = 1, this eﬃcient re-parametrization has been used by
Sheth et al. (2015) in the sparse case and Opper and Archambeau (2009) in the dense case,
while adopting an inference algorithm diﬀerent to ours.

6.4 Automatic Variance Reduction with a Mixture-of-Diagonals Poste-

rior

An additional beneﬁt of having a mixture-of-diagonals (mog) posterior in the dense case is
that optimization of the variational parameters will typically converge faster when using a
mixture of diagonal Gaussians. This is an immediate consequence of the following theorem.

Theorem 3 When having a dense posterior, the estimator of the gradients wrt the vari-
ational parameters using the mixture of diagonal Gaussians has a lower variance than the
full Gaussian posterior’s.

The proof is in Appendix I and is simply a manifestation of the Rao-Blackwellization tech-
nique (Casella and Robert, 1996). The theorem is only made possible due to the analytical
tractability of the KL-divergence term (Lkl) in the variational objective (Lelbo). The practi-
cal consequence of this theorem is that optimization will typically converge faster when using
a mixture-of-diagonals Gaussians than when using a full Gaussian posterior approximation.

7 Predictions

Given the general posterior over the inducing variables q(u|λ) in equation (6), the predictive
distribution for a new test point x(cid:63) is given by:

p(y(cid:63)|x(cid:63)) =

πk

p(y(cid:63)|f(cid:63)·)

p(f(cid:63)·|u)qk(u|λk)dudf(cid:63)·.

(cid:90)

=

πk

p(y(cid:63)|f(cid:63)·)qk(f(cid:63)·|λk)df(cid:63)·,

(26)

where qk(f(cid:63)·|λk) is the predictive distribution over the Q latent functions corresponding to
mixture component k given the learned variational parameters λ = {mk, Sk}:

(cid:90)

(cid:90)

K
(cid:88)

k=1
K
(cid:88)

k=1

qk(f(cid:63)·|λk) =

N (f(cid:63)j; µ(cid:63)

kj, σ(cid:63)2

kj ), with

Q
(cid:89)

j=1

21

µ(cid:63)
zz)−1mkj, and
kj = κj(x(cid:63), Zj)(Kj
kj = κj(x(cid:63), x(cid:63)) − κj(x(cid:63), Zj) (cid:0)(Kj
σ(cid:63)2

zz)−1 − (Kj

zz)−1Skj(Kj

zz)−1(cid:1) κj(Zj, x(cid:63)).

Thus, the probability of the test points taking values y(cid:63) (e.g. in classiﬁcation) in Equation
(26) can be readily estimated via Monte Carlo sampling.

8 Complexity analysis

Throughout this paper we have only considered computational complexity with respect to
the number of inducing points and training points for simplicity. Although this has also
been common practice in previous work (see e.g. Dezfouli and Bonilla, 2015; Nguyen and
Bonilla, 2014a; Hensman et al., 2013; Titsias, 2009), we believe it is necessary to provide
an in-depth complexity analysis of the computational cost of our model. Here we analyze
the computational cost of evaluating the Lelbo and its gradients once.

We begin by reminding the reader of the dimensionality notation used so far and by
introducing additional notation speciﬁc to this section. We analyze the more general case of
stochastic optimization using mini-batches. Let K be the number of mixture components
in our variational posterior; Q the number of latent functions; D the dimensionality of the
input data; S the number of samples used to estimate the required expectations via Monte
Carlo; B the number of inputs used per mini-batch; M the number of inducing inputs; and
N the number of observations. We note that in the case of batch optimization B = N , hence
our analysis applies to both the stochastic and batch setting. We also let T (e) represent
the computational costs of expression e. Furthermore, we assume that the kernel function
is simple enough such that evaluating its output between two points is O(D) and we denote
with T (log p(yn|fn·, φ)) ∈ O(L) the cost of a single evaluation of the conditional likelihood.

8.1 Overall Complexity

While the detailed derivations of the computational complexity are in Appendix J, here we
state the main results. Assuming that K (cid:28) M , the total computational cost is given by:

T (Lelbo) ∈ O(Q(M 2D + BM D + K2M 3 + KBM 2 + KBSL)),
and for diagonal posterior covariances we have:

T (Lelbo) ∈ O(Q(M 2D + BM D + M 3 + KBM + KBSL)).

We note that it takes (cid:100)N/B(cid:101) gradient updates to go over an entire pass of the data. If we
assume that K is a small constant, the asymptotic complexity of a single pass over the data
does not improve by increasing B beyond B = M . Hence if we assume that B ∈ O(M ) the
computational cost of a single gradient update is given by

T (Lelbo) ∈ O(QM (M D + M 2 + SL)),

for both diagonal and full covariances. If we assume that it takes a constant amount of time
to compute the likelihood function between a sample and an output, we see that setting
S ∈ O(M D + M 2) does not increase the computational complexity of our algorithm. As we
shall see in section 9.4, even a complex likelihood function only requires 10, 000 samples to

22

approximate it. Since we expect to have more than 100 inducing points in most large scale
settings, it is reasonable to assume that the overhead from generated samples will not be
signiﬁcant in most cases.

9 Experiments

In this section we analyze the behavior of our model on a wide range of experiments involving
small-scale to large-scale datasets. The main aim of the experiments is to evaluate the
performance of the model when considering diﬀerent likelihoods and diﬀerent dataset sizes.
We analyze how our algorithm’s performance is aﬀected by the density and location of the
inducing variables, and how the performance of batch and stochastic optimization compare.
We start by evaluating our algorithm using ﬁve small-scale datasets (N < 1, 300) under
diﬀerent likelihood models and number of inducing variables (§9.3). Then, using medium-
scale experiments (N < 70, 000), we compare stochastic and batch optimization settings,
and determine the eﬀect of learning the inducing inputs on the performance (§9.4). Sub-
sequently, we use savigp on two large-scale datasets. The ﬁrst one involves the prediction
of airline delays, where we compare the convergence properties of our algorithm to models
that leverage full knowledge of the likelihood (§9.7.1). The second large dataset considers
an augmented version of the popular mnist dataset for handwritten digit recognition, in-
volving more than 8 million observations (§9.7.3). Finally, we showcase our algorithm on a
non-standard inference problem concerning a seismic inversion task, where we show that our
variational inference algorithm can yield solutions that closely match (non-scalable) sam-
pling approaches (§9.8). Before proceeding with the experimental set-up, we give details of
our implementation which uses gpus.

9.1 Implementation

We have implemented our savigp method in Python and all the code is publicly available
at https://github.com/Karl-Krauth/Sparse-GP. Most current mainstream implemen-
tations of Gaussian process models do not support gpu computation, and instead opt to
oﬄoad most of the work to the cpu, with the notable exception of Matthews et al. (2017).
For example, neither of the popular packages gpml3 or gpy4 provide support for gpus.
This is despite the fact that matrix manipulation operations, which are easily paralleliz-
able, are at the core of any Gaussian process model. In fact, the rate of progress between
subsequent gpu models has been much larger than for cpus, thus ensuring that any gpu
implementation would run at an accelerated rate as faster hardware gets released.

With these advantages in mind, we provide an implementation of savigp that uses
Theano (Al-Rfou et al., 2016), a library that allows users to deﬁne symbolic mathematical
expressions that get compiled to highly optimized gpu cuda code. Any operation that
involved the manipulation of large matrices or vectors was done in Theano. Most of our
experiments were either run on g2.2 aws instances, or on a desktop machine with an Intel
core i5-4460 cpu, 8GB of ram, and a gtx760 gpu.

3Available at http://www.gaussianprocess.org/gpml/code/matlab/doc/.
4Available at https://github.com/SheffieldML/GPy.

23

Despite using a low-end outdated gpu, we found a time speed-up of 5x on average when
we oﬄoaded work to the gpu. For example, in the case of the mnist-b dataset (used in
section 9.4), we averaged the time it took to compute ten gradient evaluations of the Lelbo
with respect to the posterior parameters over the entire training set, where we expressed
the posterior as a full Gaussian and used a sparsity factor of 0.04. While it took 42.35
seconds, on average, per gradient computation when making use of the cpu only, it took
a mere 8.52 seconds when work was oﬄoaded to the gpu. We expect the diﬀerence to be
even greater given a high-end current-generation gpu.

9.2 Details of the Experiments

Performance measures. We evaluated the performance of the algorithm using non-
probabilistic and probabilistic measures according to the type of learning problem we are
addressing. The standardized squared error (sse) and the negative log predictive density
(nlpd) were used in the case of continuous-output problems. The error rate (er) and
the negative log probability (nlp) were used in the case of discrete-output problems. In
the experiments using the airline dataset, we used the root mean squared error (rmse)
instead of the sse to be able to compare our method with previous work that used rmse
for performance evaluation.

In small-scale experiments, inducing inputs were placed on a
Experimental settings.
subset of the training data in a nested fashion, so that experiments on less sparse models
contained the inducing points of the sparser models. In medium-scale and large-scale ex-
periments the location of the inducing points was initialized using the k-means clustering
method. In all experiments the squared exponential covariance function was used.

Optimization methods. Learning the model involves optimizing variational parame-
ters, hyperparameters, likelihood parameters, and inducing inputs. These were optimized
iteratively in a global loop. In every iteration, each set of parameters was optimized sepa-
rately while keeping the other sets of parameters ﬁxed, and this process was continued until
the change in the objective function between two successive iterations was less than 10−6.
For optimization in the batch settings, each set of parameters was optimized using l-bfgs,
with the maximum number of global iterations limited to 200. In the case of stochastic
optimization, we used the adadelta method (Zeiler, 2012) with parameters (cid:15) = 10−6 and
a decay rate of 0.95. The choice of this optimization algorithm was motivated by (and to
be consistent with) the work of Hensman et al. (2013), who found this speciﬁc algorithm
successful in the context of Gaussian process regression. We compare with the method of
Hensman et al. (2013) in this context in §9.7.

Reading the graphs. Results are presented using boxplots and bar/line charts. In the
case of boxplots, the lower, middle, and upper hinges correspond to the 25th, 50th, and
75th percentile of the data. The upper whisker extends to the highest value that is within
1.5×IQR of the top of the box, and the lower whisker extends to the lowest value within
1.5×IQR of the bottom of the box (IQR is the distance between the ﬁrst and third quantile).

24

Table 1: Details of the datasets used in the small-scale experiments and their corresponding
likelihood models. Ntrain, Ntest, D are the number of training points, test points and input
dimensions respectively; ‘likelihood’ is the conditional likelihood used on each problem; and
‘model’ is the name of the model associated with that likelihood. For mining Ntest = 0 as
only posterior inference is done (predictions on a test set are not made for this problem).

Dataset Ntrain Ntest D
mining 811
boston 300
creep 800
abalone 1000
cancer 300
usps
1233

0
206
1266
3177
383
1232

Likelihood p(y|f )

Model

λy exp(−λ)/y!

Log Gaussian Cox process
Standard regression

1
13 N (y; f, σ2)
30 ∇yt(y)N (t(y); f, σ2) Warped Gaussian processes
∇yt(y)N (t(y); f, σ2) Warped Gaussian processes
8
1/(1 + exp(−f ))
9
exp(fc)/ (cid:80)
256

i=1 exp(fi) Multi-class classiﬁcation

Binary classiﬁcation

In the case of bar charts, the error bars represent 95% conﬁdence intervals computed over
multiple partitions of the dataset.

Model conﬁgurations. We refer to the ratio of inducing points to training points as the
sparsity factor (SF = M/N ). For each sparsity factor, three diﬀerent variations of savigp
corresponding to (i) a full Gaussian posterior, (ii) a diagonal Gaussian posterior, and (iii)
a mixture of two diagonal Gaussian posteriors were tested, which are denoted respectively
by fg, mog1, and mog2 in the graphs.

9.3 Small-scale Experiments

We tested savigp on six small-scale datasets with diﬀerent likelihood models. The datasets
are summarized in Table 1, and are the same as those used by Nguyen and Bonilla (2014a).
For each dataset, the model was tested ﬁve times across diﬀerent subsets of the data; except
for the mining dataset where only training data were used for evaluation of the posterior
distribution.

9.3.1 Standard regression

The model was evaluated on the boston dataset (Bache and Lichman, 2013), which involves
a standard regression problem with a univariate Gaussian likelihood model, i.e., p(yn|fn) =
N (yn|fn, σ2). Figure 1 shows the performance of savigp for diﬀerent sparsity factors, as
well as the performance of exact Gaussian process inference (gp). As we can see, sse
increases slightly on sparser models. However, the sse of all the models (fg, mog1, mog2)
across all sparsity factors are comparable to the performance of exact inference (gp). In
terms of nlpd, as expected, the dense (SF = 1) fg model performs exactly like the exact
inference method (gp). In the sparse models, nlpd shows less variation in lower sparsity
factors (especially for mog1 and mog2), which can be attributed to the tendency of such
models to make less conﬁdent predictions under high sparsity settings.

25

Figure 1: The distributions of sse and nlpd for a regression problem with a univariate
Gaussian likelihood model on the boston housing dataset. Three approximate posteriors
in savigp are used: fg (full Gaussian), mog1 (diagonal Gaussian), and mog2 (mixture of
two diagonal Gaussians), along with various sparsity factors (SF = M/N ). The smaller the
sf the sparser the model, with SF = 1 corresponding to the dense model. gp corresponds
to the performance of exact inference using standard Gaussian process regression.

9.3.2 Warped Gaussian process

In warped Gaussian processes, the likelihood function is p(yn|fn) = ∇yt(yn)N (t(yn); fn, σ2),
for some transformation t. We used the same neural-net style transformation as Snelson
et al. (2003), and evaluated the performance of our model on two datasets: creep (Cole
et al., 2000), and abalone (Bache and Lichman, 2013). The results are compared with the
performance of exact inference for warped Gaussian processes (wgp) described by Snelson
et al. (2003), and also with the performance of exact Gaussian process inference with a
univariate Gaussian likelihood model (gp). As shown in Figure 2, in the case of the abalone
dataset, the performance is similar across all the models (fg, mog1 and mog2) and sparsity
factors, and is comparable to the performance of the exact inference method for warped
Gaussian processes (wgp). The results on creep are given in Appendix K.1.

9.3.3 Binary classiﬁcation

For binary classiﬁcation we used the logistic likelihood p(yn = 1|fn) = 1/(1 + e−fn) on the
breast cancer dataset (Bache and Lichman, 2013) and compared our model against the
expectation propagation (ep) and variational bounds (vbo) methods described by Nickisch

26

Figure 2: The distributions of sse and nlpd for a warped Gaussian process likelihood
model on the abalone dataset. Three approximate posteriors in savigp are used: fg
(full Gaussian), mog1 (diagonal Gaussian), and mog2 (mixture of two diagonal Gaussians),
along with various sparsity factors (SF = M/N ). The smaller the sf the sparser the model,
with SF = 1 corresponding to the non-sparse model. wgp corresponds to the performance
of the exact inference method for warped Gaussian process models (Snelson et al., 2003),
and gp is the performance of exact inference on a univariate Gaussian likelihood model.

and Rasmussen (2008). Results depicted in Figure 3 indicate that the error rate remains
comparable across all models (fg, mog1, mog2) and sparsity factors, and is almost identical
to the error rates obtained by inference using ep and vbo. Interestingly, the nlp shows more
variation and generally degrades as the number of inducing points is increased, especially
for mog1 and mog2 models, which can be attributed to the fact that these denser models
are overly conﬁdent in their predictions.

9.3.4 Multi-class classiﬁcation
For multi-class classiﬁcation, we used the softmax likelihood p(yn = c) = e−fc/ (cid:80)
i e−fi,
and trained the model to classify the digits 4, 7, and 9 from the usps dataset (Rasmussen
and Williams, 2006). We compared our model against a variational inference method (vq)
which represents the elbo using a quadratic lower bound on the likelihood terms (Khan
et al., 2012). As we see in Figure 4, the error rates are slightly lower in denser fg models.
We also note that all versions of savigp achieve comparable error rates to vq’s. Similarly
to the binary classiﬁcation case, nlp shows higher variation with higher sparsity factor,
especially in mog1 and mog2 models.

27

Figure 3: Error rates and nlp for binary classiﬁcation using a logistic likelihood model on
the Wisconsin breast cancer dataset. Three approximate posteriors are used: fg (full
Gaussian), mog1 (diagonal Gaussian), and mog2 (mixture of two diagonal Gaussians),
along with various sparsity factors (SF = M/N ). The smaller the sf the sparser the model,
with SF = 1 corresponding to the original model without sparsity. The performance of
inference using expectation propagation and variational bounds are denoted by ep and vbo
respectively.

9.3.5 Log Gaussian Cox Process

The log Gaussian Cox process (lgcp) is an inhomogeneous Poisson process in which the
log-intensity function is a shifted draw from a Gaussian process. Following Murray et al.
(2010), we used the likelihood p(yn|fn) = λyn
, where λn = exp fn + m is the mean
of a Poisson distribution and m is the oﬀset of the log mean. We applied savigp with
the lgcp likelihood on a coal-mining disaster dataset (Jarrett, 1979), which can be seen in
Figure 5 (top).

n exp (−λn)
yn!

As baseline comparisons, we use hybrid Monte Carlo (hmc) and elliptical slice sampling
(ess) described by Duane et al. (1987) and Murray et al. (2010) respectively. We collected
every 100th sample for a total of 10k samples after a burn-in period of 5k samples and used
the Gelman-Rubin potential scale reduction factors (Gelman and Rubin, 1992) to check for
convergence.

The bottom plot of Figure 5 shows the mean and variance of the predictions made by
savigp, hmc and ess. We see that the fg model provides similar results across all sparsity
factors, which is also comparable to the results provided by hmc and ess. mog1 and mog2
models provide the same mean as the fg models, but tend to underestimate the posterior

28

Figure 4: Classiﬁcation error rates and nlp for the multi-class classiﬁcation problem using a
softmax likelihood model on the usps dataset. Three approximate posteriors in savigp are
used: fg (full Gaussian), mog1 (diagonal Gaussian), and mog2 (mixture of two diagonal
Gaussians), along with various sparsity factors (SF = M/N ). The smaller the sf the
sparser the model, with SF = 1 corresponding to the original model without sparsity. vq
corresponds to a variational inference method, which represents the elbo as a quadratic
lower bound to the likelihood terms.

variance. This under-estimation of posterior variance is well known for variational methods,
especially under factorized posteriors. This results are more signiﬁcant when comparing the
running times across models. When using a slower Matlab implementation of our model,
for a fair comparison across all methods, savigp was at least two orders of magnitude faster
than hmc and one order of magnitude faster than ess.

9.4 Medium-scale Experiments

In this section we investigate the performance of savigp on four medium-scale problems on
the datasets summarized in Table 2. Our goal here is to evaluate the model on medium-
scale datasets, to study the eﬀect of optimizing the inducing inputs, and to compare the
performance of batch and stochastic optimization of the parameters. The ﬁrst problem that
we consider is multi-class classiﬁcation of handwriting digits on the mnist dataset using a
softmax likelihood model. The second problem uses the same dataset, but the task involves
binary classiﬁcation of odd and even digits using the logistic likelihood model. Therefore
we refer to this dataset as the mnist binary dataset (mnist-b). The third problem uses
the sarcos dataset (Vijayakumar and Schaal, 2000) and concerns an inverse-dynamics

29

Figure 5: Top: the coal-mining disasters data. Bottom: the posteriors for a Log Gaussian
Cox process on the data when using a fg (full Gaussian), mog1 (diagonal Gaussian), and
mog2 (mixture of two diagonal Gaussians), along with various sparsity factors (SF = M/N ).
The smaller the sf the sparser the model, with SF = 1 corresponding to non-sparse model.
The solid line is the posterior mean and the shaded area represents 95% conﬁdence intervals.
hmc and ess correspond to hybrid Monte Carlo and elliptical slice sampling inference
methods and are represented by orange dots and dashed green lines respectively.

problem for a seven degrees-of-freedom sarcos anthropomorphic robot arm. The task
is to map from a 21-dimensional input space (7 joint positions, 7 joint velocities, 7 joint
accelerations) to the corresponding 7 joint torques. For this multi-output regression problem
we use the Gaussian process regression network (gprn) likelihood model of Wilson et al.
(2012), which allows for nonlinear models where the correlation between the outputs can
be spatially adaptive. This is achieved by taking a linear combination of latent Gaussian
processes, where the weights are also drawn from Gaussian processes. Finally, sarcos-2

30

Table 2: Datasets used on the medium-scale experiments. gprn stands for Gaussian process
regression networks; Ntrain, Ntest, D are the number of training points, test points and input
dimensions respectively; Q is the number of latent processes; P is the dimensionality of the
output data; and ‘model’ is the model associated with the conditional likelihood used.

Dataset

Ntrain Ntest

D

Q P Model

mnist-b
60,000
mnist
60,000
sarcos-2 44,484
sarcos
44,484

10,000
10,000
4,449
4,449

784
784
21
21

1
10
3
8

Binary classiﬁcation
1
10 Multi-class classiﬁcation
2
7

gprn (Wilson et al., 2012)
gprn (Wilson et al., 2012)

is the same as sarcos, but the model is learned using only data from joints 4 and 7. For
both sarcos and sarcos-2, the mean of the sse and nlpd across joints 4 and 7 are used
for performance evaluation. We only consider joints 4 and 7 for sarcos, despite the fact
that predictions are made across all 7 joints to provide a direct comparison with sarcos-2
and with previous literature (Nguyen and Bonilla, 2014b). We also made use of automatic
relevance determination (ard) for both the sarcos and sarcos-2 datasets.

9.4.1 Batch optimization

Classiﬁcation. Here we evaluate the performance of batch optimization of model param-
eters on multi-class classiﬁcation using mnist and refer the reader to Appendix K.2 for the
results on binary classiﬁcation using mnist-b. We optimized the kernel hyperparameters
and the variational parameters, but ﬁxed the inducing point locations using k-means clus-
tering. Unlike most previous approaches (Ranzato et al., 2006; Jarrett et al., 2009), we did
not tune model parameters using the validation set. Instead, we considered the validation
set to be part of the training set and used our variational framework to learn all model
parameters. As such, the current setting likely provides a lower performance on test accu-
racy compared to the approaches that use a validation dataset, however our goal is simply
to show that we were able to achieve competitive performance in a sparse setting when no
knowledge of the likelihood model is used.

Figure 6 shows the result on the mnist dataset. we see that the performance improves
with denser models. Overall, savigp achieves an error rate of 2.77% at SF = 0.04. This
is a signiﬁcant improvement over the results reported by Gal et al. (2014), in which a
separate model was trained for each digit, which achieved an error rate of 5.95%. As a
reference, previous literature reports about 12% error rate by linear classiﬁers and less
than 1% error rate by state-of-the-art deep convolutional nets. Our results show that our
method reduces the gap between gps and deep nets while solving the harder problem of full
posterior estimation. In Appendix K.2 we show that our model can achieve slightly better
performance than that reported by Hensman et al. (2015) on mnist-b.

Gaussian process regression networks. Figure 7 shows that sarcos-2 gets a signif-
icant beneﬁt from a higher number of inducing points, which is consistent with previous

31

Figure 6: Error rates and nlp for multi-class classiﬁcation on the mnist dataset. We used
a full Gaussian (fg) posterior approximation across various sparsity factors (SF = M/N ).
The smaller the SF the sparser the model.

Figure 7: Mean sse and nlpd for multi-output regression on the sarcos-2 dataset. We used
a full Gaussian (fg) posterior approximation across various sparsity factors (SF = M/N ).
The smaller the SF the sparser the model.

work that found that the performance on this dataset improves as more data is being used
to train the model (Vijayakumar and Schaal, 2000). The performance is signiﬁcantly better
than the results reported by Nguyen and Bonilla (2014b), who achieved a mean standard-
ized squared eror (msse) of 0.2631 and 0.0127 across joints 4 and 7, against our values of
0.0033 and 0.0065 for SF = 0.04. However, we note that their setting was much sparser
than ours on joint 4. The results on sarcos (predicting on all joints) are given in Appendix
K.3.

9.5 Inducing-input learning

We now compare the eﬀect of adaptively learning the inducing inputs, versus initializing
them using k-means and leaving them ﬁxed. We look at the performance of our model
under two settings: (i) a low number of inducing variables (SF= 0.001, 0.004) where the
inducing inputs are learned, and (ii) a large number of inducing variables (SF = 0.02, 0.04)
without learning of their locations.

Figure 8 shows the performance of the model under the two settings on mnist-b. We
see that learning the location of the inducing variables yields a large gain in performance.

32

Figure 8: Comparison of error rate and nlp obtained by savigp without learning (fg) and
with learning (ind) of inducing inputs for binary classiﬁcation on the mnist-b dataset.

Table 3: Error rate (er) and negative log probability (nlp) obtained with savigp optimized
using batch optimization (with l-bfgs) and stochastic optimization (with adadelta) on
the mnist-b dataset. The inducing inputs are optimized in both cases.

Method Batch

SF = 0.001

Batch
SF = 0.004

Stochastic
SF = 0.001

Stochastic
SF = 0.004

er
nlp

3.17%
0.097

2.12%
0.068

3.11%
0.099

2.68%
0.083

In fact, the sparser models with inducing point learning performed similarly to the denser
models, despite the fact that the two models diﬀered by an order of magnitude when it came
to the number of inducing variables. Additional results on mnist, sarcos and sarcos-2
are shown in Appendix K.4. Our analyses indicate that there is a trade-oﬀ between the
reduction in computational complexity gained by reducing the number of inducing variables
and the increased computational cost of calculating inducing-input gradients. As such, the
advantage of learning the inducing inputs is dataset dependent and it is aﬀected mainly by
the input dimensionality (D).

9.6 Batch Optimization vs Stochastic Optimization

In this section we compare the performance of our model after it has been trained in a
batch setting versus a stochastic setting. We used adadelta as our stochastic optimization
algorithm as it requires less hand-tuning than other algorithms such as vanilla stochastic
gradient descend (sgd).

Table 3 shows only a slight deterioration in predictive performance on mnist-b when
using stochastic optimization instead of batch optimization. In fact, our exploratory exper-
iments showed that the error metrics could have signiﬁcantly been reduced by meticulously
hand-tuning momentum stochastic gradient descent (sgd). However, our goal was simply
to show that our model does not suﬀer from a large loss in performance when going from a
batch to a stochastic setting in medium-scale experiments, without requiring extensive hand

33

tuning of the optimization algorithm. As we shall see in the next section, when batch opti-
mization is not feasible, stochastic optimization in our model performs well when compared
to state-of-the-art approaches for inference in gp models on very large datasets.

9.7 Large-scale Experiments

In this section we evaluate savigp on two large-scale problems involving prediction of airline
delays (regression, N = 700, 000) and handwritten digit recognition (classiﬁcation, N =
8, 100, 000).

9.7.1 Airline delays

Here we consider the problem of predicting airline delays using a univariate Gaussian like-
lihood model (Hensman et al., 2013). We note that this dataset has outliers that can
signiﬁcantly aﬀect the performance of regression algorithms when using metrics such as the
rmse. This has also been pointed out by Das et al. (2015). Additionally, in order to match
the application of the algorithms to a realistic setting, evaluation of learning algorithms
on this dataset should always consider making predictions in the future. Therefore, unlike
Hensman et al. (2013), we did not randomly select the training and test sets, instead we
selected the ﬁrst 700, 000 data points starting at a given oﬀset as the training set and the
next 100, 000 data points as the test set. We generated ﬁve training/test sets by setting the
initial oﬀset to 0 and increasing it by 200, 000 each time.

We used the squared exponential covariance function with automatic relevance deter-
mination (ard), and optimized all the parameters using adadelta (Zeiler, 2012). We
compare the performance of our model with the stochastic variational inference on gps
method (svigp) described by Hensman et al. (2013), which assumes full knowledge of the
likelihood function. Our method (savigp) and svigp were optimized using adadelta with
identical settings. We also report the results of Bayesian linear regression with a zero-mean
unit-variance prior over the weights (linear) and Gaussian process regression using subsets
of the training data (gp1000 and gp2000 using 1, 000 and 2, 000 datapoints respectively).
For each run we ﬁt gp1000 and gp2000 ten times using a randomly selected subset of
the training data. Kernel hyper-parameters of gp1000 and gp2000 were optimized using
l-bfgs.

Figure 9 shows the performance of savigp and the four baselines. We see that savigp
converges at a very similar rate to svigp, despite making no assumption about the likelihood
model. Furthermore, savigp performs better than all three simple baselines after less than
5 epochs. We note that our results are not directly comparable with those reported by
Hensman et al. (2013), since we deal with the harder problem of predicting future events
due to the way our dataset is selected.

9.7.2 Training Loss vs Test Performance

An additional interesting question about the behavior of our model relates to how well
the training objective function correlates to the test error metrics. Figure 10 shows how
the performance metrics (rmse and nlpd) on the test data vary along with the negative
evidence lower bound (nelbo). As we can see, changes in the nelbo closely mirror changes

34

Figure 9: The rmse (left) and nlpd (right) of svigp (Hensman et al., 2013), and savigp
(our method) averaged across all 5 airline-delay experiment runs. The x-axis represents the
number of passes over the data (epochs). The ﬁgure also shows the performance of gp1000,
gp2000 and Bayesian linear regression (linear) after training is completed.

Figure 10: The rmse (left) and nlpd (right) of savigp on the test data alongside the
negative evidence lower bound (nelbo) on the training data, averaged over ﬁve runs of the
airline-delay experiment.

in rmse and nlpd in our model, which is an indication that the variational objective
generalizes well to test performance when using these metrics.

9.7.3 Large-scale mnist

In this section we evaluate the performance of savigp on the mnist8m (Loosli et al., 2007)
dataset, which artiﬁcially extends the mnist dataset to 8.1 million training points by pseudo-
randomly transforming existing mnist images.

We train savigp on the mnist8m dataset by optimizing only variational parameters
stochastically, with a batch size of 1000 and 2000 inducing points. We also use a squared
exponential covariance function without automatic relevance determination.

After optimizing the model for 19 hours, we observe an error rate of 1.54% on the test set
and lower, middle and upper nlp quartiles of 8.61e−4, 3.81e−3, and 2.16e−2 respectively.
We see that this outperforms signiﬁcantly our previous result in §9.4 on standard mnist,
where we reported an error rate of 2.77%. As a point of comparison with hard-coded ap-

35

Figure 11: The negative evidence lower bound (nelbo) on the current batch of the mnist8m
training set over time on savigp. The x-axis represents the number of training steps taken.

proaches, Henao and Winther (2012) reported 0.86% error rate on mnist when considering
an augmented active set (which is analogous to the concept of inducing inputs/variables)
and a 9th-degree polynomial covariance function.

Finally, Figure 11 shows the training loss (the nelbo) of savigp on mnist8m as a
function of the number of training steps, where we note that the loss decreases rapidly in
the ﬁrst 2,000 iterations and stabilizes at around 4,000 iterations.

9.8 Seismic inversion

In this experiment we aim to evaluate our method qualitatively on a non-standard inference
problem motivating the need for black-box likelihoods. This application considers a one-
dimensional seismic inversion and was investigated by Bonilla et al. (2016). Given noisy
surface observations of sound-reﬂection times, the goal is to infer the geometry (layer depths)
of subsurface geological layers and seismic propagation velocities within each layer. For this,
we consider a real dataset from a seismic survey carried out in the Otway basin region in
the state of Victoria, Australia.

Setting There are N = 113 site locations with P = 4 interface reﬂections (layers) per site.
The inputs are given by the surface locations of the seismic sensors (X), the outputs are
the observed times at the diﬀerent locations for each of the layers (Y) and we aim to infer
Q = 2 ∗ P latent functions, i.e. P functions corresponding to layer depths and P functions
corresponding to seismic propagation velocities. For clarity, in this section we refer to the
latent functions corresponding to depth and velocity as f d and f v, respectively.

Likelihood The likelihood of the observed times ynp for location xn and interface p is
given by:

ynp =






(cid:17)

(cid:16) f d
np
f v
np
(cid:18) f d
np−f d
f v
np

2

2

+ (cid:15)np,
(cid:19)

np−1

for p = 1,

+ ynp−1 + (cid:15)np,

for 1 < p ≤ P ,

where (cid:15)np ∼ N (0, σ2
(2016), we set the corresponding standard deviations to 0.025s, 0.05s, 0.075s and 0.1s.

p is the output-dependent noise variance. As in Bonilla et al.

p) and σ2

36

Figure 12: Results for the seismic inversion experiment using our algorithm (savigp) and
the mcmc algorithm developed by Bonilla et al. (2016). The mean and standard deviations
envelopes are shown for savigp and mcmc in solid and dashed lines, respectively. Left:
inferred layer boundaries. Right: inferred seismic velocities.

Prior setting: We used the same prior as in Bonilla et al. (2016), with prior mean depths
of 200m, 500m, 1600m and 2200m and prior mean velocities of 1950m/s, 2300m/s, 2750m/s
and 3650m/s. The corresponding standard deviations for the depths were set to 15% of
the layer mean, and for the velocities they were set to 10% of the layer mean. A squared
exponential covariance function with unit length-scale was used.

Posterior estimation We ran our algorithm for the dense case (Z = X) using a full
Gaussian posterior and batch optimization, initializing the posterior means to the prior
means and the posterior covariance to a diagonal matrix with entries corresponding to 0.1%
of the prior variances. The results are given in Figure 12, where we see that savigp’s
posterior closely match the “true” posterior obtained by the mcmc algorithm developed
in Bonilla et al. (2016), although the variances are overestimated, which can be see as a
consequence of our variational approach using a full Gaussian approximate posterior.

10 Conclusions and Discussion

We have developed scalable automated variational inference for Gaussian process models
(savigp), an inference method for models with Gaussian process (gp) priors, multiple out-
puts, and nonlinear likelihoods. The method is generally applicable to black-box likelihoods,
i.e. it does not need to know the details of the conditional likelihood (or its gradients), only
requiring its evaluation as a black-box function.

One of the key properties of this method is that, despite using a ﬂexible variational
posterior such as a mixture-of-Gaussians distribution, it is statistically eﬃcient in that

37

it requires samples from univariate Gaussian distributions to estimate the evidence lower
bound (elbo) and its gradients.

In order to provide scalability to very large datasets, we have used an augmented prior
via the so-called inducing variables, which are prevalent in most sparse gp approximations.
This has allowed us to decompose the elbo as a sum of terms over the datapoints, hence
giving way for the application of stochastic optimization and parallel computation.

Our small-scale experiments have shown that savigp can perform as accurately as so-
lutions that have been hard-coded speciﬁcally for the conditional likelihood of the problem
at hand, even under high sparsity levels.

Our medium-scale experiments also have shown the eﬀectiveness of savigp, when consid-
ering problems such as multi-class classiﬁcation on the mnist dataset and highly nonlinear
likelihoods such as that used in gprn. On these experiments, we also have analyzed the
eﬀect of learning the inducing inputs, i.e. the locations corresponding to the inducing vari-
ables, and concluded that doing this can yield signiﬁcant improvements in performance,
reducing the numbers of inducing variables required to achieve similar accuracy by an or-
der of magnitude. Nevertheless, there is generally a trade-oﬀ between reducing the time
cost gained by having a lower number of inducing inputs and the overhead of carrying out
optimization over these locations.

Our ﬁrst large-scale experiment, on the problem of predicting airline delays, showed
that savigp is on par with the state-of-the-art approach for Gaussian process regression on
big data (Hensman et al., 2013). In fact, our results show that savigp is slightly better but
we attribute those diﬀerences to implementation speciﬁcs. The important message is that
savigp can attain state-of-the-art performance even without exploiting speciﬁc knowledge
of the likelihood model. Our second large-scale experiment shows that having an inference
algorithm for gp models with non-Gaussian likelihood for large datasets is worthwhile, as
the performance obtained on mnist8m (using 8.1M observations) was signiﬁcantly better
than on the standard mnist (using 60K observations).

Our ﬁnal experiment considered a non-standard inference problem concerning a seismic
In this problem savigp yielded a solution for the posterior over latent
inversion task.
functions that closely matched the solution obtained by a (non-scalable) sampling algorithm.
Overall, we believe savigp has the potential to be a powerful tool for practitioners and
researchers when devising models for new or existing problems with Gaussian process priors
for which variational inference is not yet available. As mentioned in Section 2, we are very
much aware of recent developments in the areas of probabilistic programming, stochastic
variational inference and Bayesian deep learning. Advances such as those in black-box
variational inference (bbvi, Ranganath et al., 2014) and variational auto-encoders (Rezende
et al., 2014; Kingma and Welling, 2014) are incredibly exciting for the machine learning
community. While the former, bbvi, is somewhat too general to be useful in practice for gp
models, the latter (variational auto-encoders) requires speciﬁc knowledge of the likelihood
and is not a truly black-box method.

Finally, we are working on extending our models to more complex settings such as
structured prediction problems, i.e. where the conditional likelihood is a non-iid model such
as a chain or a tree (see e.g. Galliani et al., 2017, for a recent reference). Such settings provide
incredible challenges from the computational and modeling perspectives. For example, how
to deal with the exponential increase in the number of parameters of the model, and how

38

to reduce the number of calls to an expensive conditional likelihood model. We believe that
the beneﬁts of being Bayesian well outweigh the eﬀort in exploring such challenges.

Acknowledgments

We acknowledge the contribution by Trung V. Nguyen to the original conference paper
(Nguyen and Bonilla, 2014a). evb started this work at the University of New South Wales
(unsw sydney) and was partially supported by unsw’s Faculty of Engineering Research
Grant Program project # PS37866; unsw’s Academic Start-Up Funding Scheme project #
PS41327; and an aws in Education Research Grant award. AD started this work at unsw
sydney, and was supported by a Research Fellowship from unsw sydney.

A Derivation of the KL-divergence Term

Here we derive the expressions for the terms composing the KL-divergence (Lkl = Lent +
Lcross) part of the log-evidence lower bound (Lelbo).

The entropy term is given by:

Lent(λ) = −Eq(u|λ)[log q(u|λ)]
(cid:90) K
(cid:88)

= −

πkqk(u|mk, Sk) log

π(cid:96)q(cid:96)(u|m(cid:96), S(cid:96))du

K
(cid:88)

(cid:96)=1
K
(cid:88)

(cid:96)=1
K
(cid:88)

(cid:96)=1

= −

πk

N (u; mk, Sk) log

π(cid:96)N (u; m(cid:96), S(cid:96))du

k=1

(cid:90)

K
(cid:88)

k=1
K
(cid:88)

k=1
K
(cid:88)

k=1
K
(cid:88)

k=1

(cid:90)

(cid:90)

K
(cid:88)

(cid:96)=1
K
(cid:88)

(cid:96)=1

≥ −

πk log

N (u; mk, Sk)

π(cid:96)N (u|m(cid:96), S(cid:96))du

= −

πk log

π(cid:96)

N (u; mk, Sk)N (u|m(cid:96), S(cid:96))du

= −

πk log

π(cid:96)N (mk; m(cid:96), Sk + S(cid:96)) def= (cid:98)Lent,

(27)

(28)

where we have used Jensen’s inequality to bring the logarithm out of the integral from
Equations (27) to (28).

The negative cross-entropy term can be computed as:

Lcross(λ) = Eq(u|λ)[log p(u)] =

πkqk(u|λk) log p(u)du

K
(cid:88)

(cid:90)

k=1

K
(cid:88)

Q
(cid:88)

(cid:90)

πk

=

k=1

j=1

N (uj; mkj, Skj) log N (uj; 0, Kj

zz)duj

39

πk

log N (mkj; 0, Kj

zz) −

tr (Kj

zz)−1Skj

(cid:21)

1
2

K
(cid:88)

Q
(cid:88)

(cid:20)

=

k=1

j=1

= −

1
2

K
(cid:88)

Q
(cid:88)

πk

k=1

j=1

(cid:2)M log 2π + log (cid:12)

(cid:12)Kj
zz

(cid:12)
(cid:12) + mT

kj(Kj

zz)−1mkj + tr (Kj

zz)−1Skj

(cid:3) .

B Proof of Theorem 1

In this section we prove Theorem 1 concerning the statistical eﬃciency of the estimator of the
expected log likelihood and its gradients, i.e. that both can be estimated using expectations
over Univariate Gaussian distributions.

B.1 Estimation of Lell

Taking the original expression in Equation (16) we have that:

Lell(λ) = Eq(f |λ)[log p(y|f , φ)],
(cid:90)

q(f |λ)log p(yn|fn·, φ)df ,

f

(cid:90)

(cid:90)

fn·

f¬n·

N
(cid:88)

n=1
N
(cid:88)

n=1
N
(cid:88)

n=1
N
(cid:88)

=

=

=

=

K
(cid:88)

n=1

k=1

q(f¬n·|fn·)q(fn·) log p(yn|fn·, φ)df¬n·dfn·,

Eq(n)(fn·)[log p(yn|fn·, φ)],

πkEqk(n)(fn·|λk)[log p(yn|fn·, φ)],

(29)

(30)

(31)

where we have applied the linear property of the expectation operation in Equation (29);
used f¬n· to denote all the latent functions except those corresponding to the nth observa-
tion, and integrated these out to obtain Equation (30); and used the form of the marginal
posterior (mog in Equation (17)) to get Equation (31).

As described in Section 4.6, qk(n)(fn·|λk) is a Q-dimensional Gaussian with diagonal
covariance, hence computation of Equation (31) only requires expectations over univariate
(cid:4)
Gaussian distributions.

B.2 Estimation of the Gradients of Lell
Denoting the kth term for the nth observation of the expected log likelihood with L(k,n)
have that:

ell we

L(k,n)
ell = Eqk(n)(fn·|λk)[log p(yn|fn·, φ)]

(cid:90)

=

fn·

qk(n)(fn·|λk) log p(yn|fn·, φ)dfn·

40

(cid:90)

∇λk L(k,n)

ell =

qk(n)(fn·|λk)∇λk log qk(n)(fn·|λk) log p(yn|fn·, φ)dfn·
(cid:2)∇λk log qk(n)(fn·|λk) log p(yn|fn·, φ)(cid:3),

= Eqk(n)(fn·|λk)

fn·

(32)

(33)

for λk ∈ {mk, Sk}, and for the mixture proportions the gradients can be estimated straight-
forwardly using Equation (22). We have used in Equation (32) the fact that ∇xf (x) =
f (x)∇x log f (x) for any nonnegative function f (x). We see that our resulting gradient esti-
mate has an analogous form to that obtained in Equation (31), hence its computation only
(cid:4)
requires expectations over univariate Gaussian distributions.

C Gradients of the Evidence Lower Bound wrt Variational

Parameters

Here we specify the gradients of the log-evidence lower bound (Lelbo) wrt variational pa-
rameters. For the covariance, we consider Skj of general structure but also give the updates
when Skj is a diagonal matrix, denoted with ˜Skj.

C.1 KL-divergence Term
Let Kzz be the block-diagonal covariance with Q blocks Kj
us assume the following deﬁnitions:

zz, j = 1, . . . Q. Additionally, let

(34)

Ckl

Nk(cid:96)

def= Sk + S(cid:96),
def= N (mk; m(cid:96), Ckl),

zk

def=

π(cid:96)Nk(cid:96).

K
(cid:88)

(cid:96)=1

The gradients of Lkl wrt the posterior mean and posterior covariance for component k are:

∇mk Lcross = −πkK−1

zz mk,

∇Sk Lcross = −

πkK−1

zz , and for diagonal covariance we have:

∇˜Sk

Lcross = −

πk diag(K−1

zz ),

1
2
1
2

1
2

Q
(cid:88)

j=1

∇πk Lcross = −

[M log 2π + log (cid:12)

(cid:12)Kj
zz

(cid:12)
(cid:12) + mT

kj(Kj

zz)−1mkj + tr (Kj

zz)−1Skj],

where we note that we compute K−1
dently. The gradients of the entropy term wrt the variational parameters are:

zz by inverting the corresponding blocks Kj

zz indepen-

∇mk (cid:98)Lent = πk

K
(cid:88)

(cid:96)=1

π(cid:96)

(cid:18) Nk(cid:96)
zk

+

Nk(cid:96)
z(cid:96)

(cid:19)

C−1

kl (mk − m(cid:96)),

∇Sk (cid:98)Lent =

πk

1
2

K
(cid:88)

(cid:96)=1

π(cid:96)

(cid:18) Nk(cid:96)
zk

+

Nk(cid:96)
z(cid:96)

(cid:19)

(cid:2)C−1

kl − C−1

kl (mk − m(cid:96))(mk − m(cid:96))T C−1

kl

(cid:3) ,

41

and for diagonal covariance we have:

∇˜Sk (cid:98)Lent =

1
2

πk

K
(cid:88)

(cid:96)=1

π(cid:96)

(cid:18) Nk(cid:96)
zk

+

Nk(cid:96)
z(cid:96)

(cid:19) (cid:104) ˜C−1

∇πk (cid:98)Lent = − log zk −

K
(cid:88)

(cid:96)=1

π(cid:96)

Nk(cid:96)
z(cid:96)

,

kl − ˜C−1

kl diag ((mk − m(cid:96)) (cid:12) (mk − m(cid:96))) ˜C−1

kl

(cid:105)

,

where ˜Ckl is the diagonal matrix deﬁned analogously to Ckl in Equation (34) and (cid:12) is the
Hadamard product.

C.2 Expected Log Likelihood Term

Monte Carlo estimates of the gradients of the expected log likelihood term are:

∇mkj (cid:98)Lell =

(Kj

zz)−1

zn[Σk(n)]−1
kj
j,j

f (k,i)
nj − [bk(n)]j

log p(yn|f (k,i)

n·

),

N
(cid:88)

n=1

S
(cid:88)

(cid:16)

i=1

∇Skj (cid:98)Lell =

N
(cid:88)

n=1

(cid:0)ajnaT

jn

(cid:1)

(cid:20)
[Σk(n)]−2
j,j

(cid:16)

S
(cid:88)

i=1

f (k,i)
nj − [bk(n)]j

− [Σk(n)]−1
j,j

log p(yn|f (k,i)

n·

),

(cid:21)

πk
S

πk
2S

(cid:17)

(cid:17)2

(35)

∇πk (cid:98)Lell =

log p(yn|f (k,i)

n·

),

1
S

N
(cid:88)

S
(cid:88)

n=1

i=1

(cid:16)

(cid:17)

and for diagonal covariance ˜Skj we replace
with diag (ajn (cid:12) ajn) in Equation
(35), where (cid:12) is the Hadamard product and diag(v) takes the input vector v and outputs
a matrix with v on its diagonal. We have also deﬁned above kj
def= κj(Zj, xn), i.e. the
zn
vector obtained from evaluating the covariance function j between the inducing points Zj
and datapoint xn.

ajnaT
jn

D Gradients of the Evidence Lower Bound wrt Covariance

Hyperparameters and Likelihood Parameters

Here we give the gradients of the variational objective (Lelbo) wrt the covariance hyper-
parameters and, when required, the conditional likelihood parameters. We note here that
our method does not require gradients of the conditional likelihood p(y|f , φ) wrt the latent
functions f . However, if the conditional likelihood is parametrized by φ and point-estimates
of these parameters are needed, these can also be learned in our variational framework.

D.1 Covariance Hyperparameters

The gradients of the terms in the KL-divergence part of Lelbo wrt a covariance hyperpa-
rameter θj are:

∇θj (cid:98)Lent = 0,

42

∇θj Lcross = −

πk tr (cid:2)(Kj

zz)−1∇θj Kj
zz

1
2

K
(cid:88)

k=1

− (Kj

zz)−1∇θj Kj

zz(Kj

zz)−1 (cid:0)mkjmT

kj + Sj

(cid:1) (cid:3).

For the (cid:98)Lell we have that:

∇θj (cid:98)Lell =

πkEqk(n)(fn·|λk)

(cid:2)∇θj log qk(n)(fn·|λk) log p(yn|fn·)(cid:3),

N
(cid:88)

K
(cid:88)

n=1

k=1

and computing the corresponding gradient we obtain:

∇θj (cid:98)Lell = −

1
2

N
(cid:88)

K
(cid:88)

n=1

k=1

πkEqk(n)(fn·|λk)

[Σk(n)]−1

j,j ∇θj [Σk(n)]j,j

(cid:104)(cid:16)

(36)

− 2(fnj − [bk(n)]j)[Σk(n)]−1
− (fnj − [bk(n)]j)2[Σk(n)]−2

j,j ∇θj [bk(n)]j

j,j ∇θj [Σk(n)]j,j

(cid:17)

(cid:105)
log p(yn|fn·)

,

for which we need:

jn

(cid:1) mkj,

∇θj [bk(n)]j = (cid:0)∇θj aT
∇θj [Σk(n)]j,j = ∇θj [ (cid:101)Kj]n,n + 2 (cid:0)∇θj aT
= ∇θj κ(xn, xn) − (cid:0)∇θj aT
(cid:1) Skjajn,

+ 2 (cid:0)∇θj aT

jn

jn

jn

(cid:1) Skjajn
(cid:1) kj

zn − aT

jn∇θj kj
zn

where

∇θj aT

jn =

∇θj (kj

zn)T − aT

jn∇θj Kj
zz

(Kj

zz)−1,

(cid:16)

(cid:17)

and as in the main text we have deﬁned ajn
nth column of Aj. Furthermore, as in the previous section, kj
zn

def= κj(Zj, xn).

def= [Aj]:,n, i.e. the vector corresponding to the

D.2 Likelihood Parameters

Since the terms in the KL-divergence do not depend on the likelihood parameters we have
that ∇φ (cid:98)Lent = ∇φLcross = 0. For the gradients of the expected log likelihood term we have
that:

∇φ (cid:98)Lell =

1
S

N
(cid:88)

K
(cid:88)

S
(cid:88)

πk

n=1

k=1

i=1

∇φ log p(yn|f (k,i)

n·

, φ),

where {f (k,i)

n· } ∼ N (fn·; bk(n), Σk(n)), for k = 1, . . . , K and i = 1, . . . , S.

E Gradients of the Log-Evidence Lower Bound wrt Inducing

Inputs

These gradients can be obtained by using the same expression as the gradients wrt co-
variance hyperparameters in Appendix D.1, considering the inducing inputs as additional

43

hyperparameters of the covariances. Therefore, we rewrite the gradients above keeping in
mind that θj is an element of inducing input Zj, hence dropping those gradient terms that
do not depend on Zj. As before we have that ∇θj (cid:98)Lent = 0.

∇θj Lcross = −

πk tr (cid:8)(cid:2)(Kj

zz)−1 − (Kj

zz)−1 (cid:0)mkjmT

kj + Sj

(cid:1) (Kj

zz)−1(cid:3)∇θj Kj

zz

(cid:9). (37)

1
2

K
(cid:88)

k=1

Similarly, for the gradients of (cid:98)Lell in Equation (36) we have that:

∇θj [bk(n)]j = ∇θj (kj

zn)T (Kj

zz)−1mkj − aT

jn∇θj Kj

zz(Kj

zz)−1mkj,

and

∇θj [Σk(n)]j,j = ∇θj (kj
+ aT

zn)T (cid:16)
jn∇θj Kj

− (Kj

zz)−1kj
zz)−1kj

zn − ajn + 2(Kj
zn − 2aT
jn∇θj Kj

zz)−1Skjajn
zz(Kj

zz)−1Skjajn.

zz(Kj

(cid:17)

From the equations above, we see that in the computation of the gradients of (cid:98)Lell there

are two types of terms. The ﬁrst term is of the form:
n ∇θKj

∇θt(1)
n

def= vT

zzwn,

where we have dropped the index j on the LHS of the equation for simplicity in the notation
and vn, wn are M -dimensional vectors. Let V and W be the M ×N matrices corresponding
to the N vectors {vn} and {wn}, respectively. Furthermore, let us assume ˜Z(d)
is the M ×M
j
matrix of all pairwise diﬀerences on dimension d of all inducing points divided by the squared
length-scale of the dimension (cid:96)2
d,

[ ˜Z(d)
j

]o,p =

[Zj]o,d − [Zj]p,d
(cid:96)2
d

.

Hence, in the case of the squared exponential covariance function the above gradient can
be calculated as follows (for all data points):

∇[Zj ]:,dt(1) = −(( ˜Z(d)

j (cid:12) Kj

zz)W) (cid:12) V − (( ˜Z(d)

j (cid:12) Kj

zz)V) (cid:12) W,

where ∇[Zj ]:,dt(1) is the M × N matrix of gradients corresponding to dimension d for all
m = 1, . . . , M and n = 1, . . . N .

Similarly, the second type of term is of the form:
n ∇θkj

∇θt(2)
n

def= vT

zn,

where vn and V are deﬁned as before. The gradients in the above expression wrt to the
inducing points (for all datapoints or a mini-batch) can be calculated as follows:

∇[Zj ]:,dt(2) = −(Kj

xz (cid:12) VT (cid:12) ˜X(d))T ,

where in the above equation ˜X(d) is the N ×M matrix of all pairwise diﬀerences on dimension
d between all datapoints and inducing points divided by the squared length-scale of the
dimension (cid:96)2
d:

[ ˜X(d)]o,p =

[X]o,d − [Zj]p,d
(cid:96)2
d

.

44

F Control Variates

We use control variates (see e.g. Ross, 2006, §8.2) to reduce the variance of the gradient
estimates. In particular, we are interested in estimating gradients of the form:

∇λk

Eqk(n)(fn·|λk)[log p(yn|fn·)] = Eqk(n)(fn·|λk)[g(fn·)], with

g(fn·) = ∇λk log qk(n)(fn·|λk) log p(yn|fn·),

where the expectations are computed using samples from qk(n)(fn·|λk), which depends on
the variational parameter λk. As suggested by Ranganath et al. (2014), a sensible control
variate is the so-called score function

whose expectation is zero. Hence, the function:

h(fn·) = ∇λk log qk(n)(fn·|λk),

has the same expectation as g(fn·) but lower variance when ˆa is given by:

˜g(fn·) = g(fn·) − ˆah(fn·),

ˆa =

Cov[g(fn·), h(fn·)]
V[h(fn·)]

,

where Cov[g(fn·), h(fn·)] is the covariance between g(fn·) and h(fn·); V[h(fn·)] is the variance
of h(fn·); and both are estimated using samples from qk(n)(fn·|λk). Therefore, our corrected
gradient is given by:

˜∇λk

Eqk(n)(fn·|λk)[log p(yn|fn·)] def= Eqk(n)(fn·|λk)[˜g(fn·)]

= Eqk(n)(fn·|λk)

(cid:2)∇λk log qk(n)(fn·|λk)(log p(yn|fn·) − ˆa)(cid:3).

G Derivations for the Dense Posterior Case

In this section we consider the case of having a dense variational posterior, i.e. not using the
so-called sparse approximations. We derive the expressions for the components of the log-
evidence lower bound and show that the posterior parameters of the variational distribution
q(f |λ) are, in fact, of ‘free’-form. Furthermore, we analyze the case of a Gaussian likelihood
showing that, in the limit of a large number of samples, our estimates converge to the exact
analytical solution.

G.1 Evidence Lower Bound

Here we show the derivations of all the terms in Lelbo when considering the dense case,
i.e. M = N and Zj = X. As described in the main text, the resulting changes to these
terms can be obtained by replacing M with N and Kj
xx. Hence, for the terms in
the KL-divergence part of Lelbo we have that:

zz with Kj

(cid:98)Lent(λ) = −

πk log

π(cid:96)N (mk; m(cid:96), Sk + S(cid:96)),

(38)

K
(cid:88)

k=1

K
(cid:88)

(cid:96)=1

45

Lcross(λ) = −

[N log 2π + log (cid:12)

(cid:12)Kj
xx

(cid:12)
(cid:12) + mT

kj(Kj

xx)−1mkj + tr (Kj

xx)−1Skj],(39)

1
2

K
(cid:88)

Q
(cid:88)

πk

k=1

j=1

where we recall that now mkj and Skj are N -dimensional objects. For the (cid:98)Lell we still
need to compute empirical expectations over the variational distribution q(f |λ), where the
corresponding parameters are given by:

bkj = Ajmkj = Kj
Σkj = (cid:101)Kj − AjKj
xx − Kj

xx(Kj
xx + AjSkjAj
xx)−1Kj
xx(Kj

xx)−1mkj = mkj,

= Kj
= Skj.

xx + Kj

xx(Kj

xx)−1SkjKj

xx(Kj

xx)−1

G.2 Gaussian Likelihoods

Consider the case of a single output and a single latent function (Q = P = 1) with a
Gaussian conditional likelihood and a single full Gaussian variational posterior (K = 1).

The entropy and the cross entropy terms are given in Equations (38) and (39) with Q =
K = 1. The expect log likelihood term can be determined analytically,

p(y|f ) = N (y; f , σ2I), and
q(f |λ) = N (f ; m, S).

Lell(λ) =

Eq(f |λ)[log p(yn|fn, φ)],

N
(cid:88)

n=1

= log N (y; m, σ2I) −

1
2σ2 tr S.

(cid:18)

K−1

xx +

(cid:19)

1
σ2

∇mLelbo = K−1

xxm +

1
σ2 (y − m) = 0

m =

1
σ2 y
(cid:99)m = (cid:0)σ2K−1
= Kxx

xx + I(cid:1)−1
(cid:0)Kxx + σ2I(cid:1)−1

y

y.

Hence the gradients of Lelbo can also be determined analytically, yielding the optimal mean
posterior as follows:

Similarly for the posterior covariances we have that:

∇SLelbo =

S−1 −

1
2
(cid:18)

(cid:98)S =

K−1

xx +

1
2σ2 I = 0

1
2

K−1

xx −
(cid:19)−1

1
σ2 I

46

(40)

= Kxx − Kxx

= Kxx − Kxx

(cid:18)

I + Kxx

1
1
σ2
σ2
(cid:0)Kxx + σ2I(cid:1)−1

(cid:19)−1

Kxx

Kxx,

(41)

where we have used Woodbury’s formula to go from Equation (40) to Equation (41). The
Equations above for the optimal posterior mean and posterior covariance, (cid:99)m and (cid:98)S, are
the exact analytical expressions for regressions with Gaussian process priors and isotropic
noise likelihoods, see Rasmussen and Williams (2006, §2.2).

H Eﬃcient Parametrization

In this appendix we prove Theorem 2, showing that it is possible to obtain an eﬃcient
parameterization of the posterior covariances when using a full Gaussian approximation. In
this case we have that:

∇Sj Lelbo = ∇Sj Lent + ∇Sj Lcross + ∇Sj Lell

= −

(Kj

zz)−1 +

S−1

j +

∇Sj

Eq(n)(fn·)[log p(yn|fn·)]

1
2

Setting the gradients to zero we have that

1
2

1
2

S−1

j =

(Kj

zz)−1 +

(Kj

zz)−1

znλjn(kj
kj

zn)T (Kj

zz)−1,

where

1
2

1
2

N
(cid:88)

n=1

N
(cid:88)

n=1

λjn = −2

d¯(cid:96)n
d[Σj]n,n
¯(cid:96)n = Eq(n)(fn·)[log p(yn|fn·)].

Therefore, the optimal solution for the posterior covariance is given by:

(cid:99)Sj = (cid:0)(Kj
= Kj
zz

zz)−1 + (Kj
(cid:0)Kj
zz + Kj

zz)−1Kj
zxΛjKj
xz

zxΛjKj
(cid:1)−1
Kj

xz(Kj
zz,

zz)−1(cid:1)−1

,

where Λj is a N × N diagonal matrix with {λjn}N

n=1 on the diagonal.

(cid:4)

I Lower Variance of Mixture-of-Diagonals Posterior

In this section we prove Theorem 3. First we review Rao-Blackwellization (Casella and
Robert, 1996), which is also known as partial averaging or conditional Monte Carlo. Sup-
pose we want to estimate V = Ep(x,y)[h(X, Y)] where (X, Y) is a random variable with

47

probability density p(x, y) and h(X, Y) is a random variable that is a function of X and
Y. It is easy to see that

Ep(x,y)[h(X, Y)] =

p(x, y)h(x, y)dxdy

(cid:90)

(cid:90)

=

p(y)p(x|y)h(x, y)dxdy

=Ep(y)

(cid:105)
(cid:104)ˆh(Y)

, with ˆh(Y) = Ep(x|y)[h(X, Y)|Y],

and, from the conditional variance formula,

V[ˆh(Y)] < V[h(X, Y)].

Therefore when ˆh(Y) is easy to compute, it can be used to estimate V with a lower variance
than the original estimator. When p(x, y) = p(x)p(y), then ˆh(Y) is simpliﬁed to

ˆh(Y = y) =

p(x)h(x, y)dx = Ep(x)[h(X, Y)|Y].

(cid:90)

We apply Rao-Blackwellization to our problem with fn· playing the role of the conditioning
variable Y and f¬n· playing the role of X, where f¬n· denotes f excluding fn·. We note that
this Rao-Blackwellization analysis is only applicable to the dense case with a mixture of
diagonals posterior, as it satisﬁes the independence condition, i.e. p(x, y) = p(x)p(y).

First, we express the gradient of λk(n) as an expectation by interchanging the integral

and gradient operators giving

∇λk(n)

Eqk(f |λk)[log p(y|f , φ)] =Eqk(f |λk)

(cid:105)
(cid:104)
∇λk(n) log qk(f |λk) log p(y|f , φ)

.

The Rao-Blackwellized estimator is thus

ˆh(fn·) =

q(f¬n·)∇λk(n) log qk(f |λk) log p(y|f )df¬n·

(cid:90)

(cid:90)

=

q(f¬n·)∇λk(n) log qk(n)(fn·|λk(n)) log p(y|f )df¬n·
(cid:90)

= ∇λk(n) log qk(n)(fn·|λk(n))

q(f¬n·) [log p(yn|fn·) + log p(y¬n|f¬n·)] df¬n·

(42)

= ∇λk(n) log qk(n)(fn·|λk(n)) [log p(yn|fn·) + C] ,

where we have used the factorization of the conditional likelihood in Equation (42), and
where C is a constant w.r.t fn·. This gives the Rao-Blackwellized gradient,

∇λk(n)

Eqk(f |λk)[log p(y|f )] = Eqk(n)(fn·|λk(n))
= Eqk(n)(fn·|λk(n))

(cid:105)

(cid:104)ˆh(fn·)
(cid:104)

∇λk(n) log qk(n)(fn·|λk(n)) log p(yn|fn·)

(cid:105)
,

where we have used the fact that Eq[∇ log q] = 0 for any q. We see then that the expression
(cid:4)
above is exactly the gradient obtained in Equation (33).

48

J Details of Complexity analysis

Here we give the details of the computational complexity of a single evaluation of the Lelbo
and its gradients. Let A be some matrix of size p × q, B be some matrix of size q × r, C
be some matrix of size q × p, and D be a square matrix of size s × s. Then we assume
that T (AB) ∈ O(pqr), T (D−1) ∈ O(s3), T (|D|) ∈ O(s3), T (diag(AC)) ∈ O(pq), and
T ( tr (AC)) ∈ O(pq).

J.1 Interim values

We ﬁrst provide an analysis of the cost of various interim expressions that get re-used
throughout the computation of the Lelbo. The interim expressions include: various values
computed from the kernel functions (Kj
zn), values used to determine
the shape of the Gaussian distributions from which we sample ([bk(n)]j, [Σk(n)]j,j), and
various intermediate values ([ (cid:101)Kj]n,n, ajn). We note that expressions with a subscript or
superscript of j, n, or k are dependent on Q, B, and K respectively. For example, in the
case of kj

zn, we actually need to consider the cost of Q × B vectors of size M .

zz, (Kj

(cid:12)
(cid:12), kj
(cid:12)

(cid:12)
(cid:12)Kj
(cid:12)

zz)−1,

zz

As stated in the main text, we assume that the kernel function is simple enough such
that evaluating its output between two points is in O(D). Hence the cost of evaluating
interim values is given by:

T ((Kj
T ((cid:12)

zz) ∈ O(QM 2D),

T (Kj
zz)−1) ∈ O(QM 3),
(cid:12)
(cid:12)Kj
(cid:12)) ∈ O(QM 3),
zz
T (kj
zn) ∈ O(QBM D),
T ([bk(n)]j) ∈ O(KQBM ),
T ([Σk(n)]j,j) ∈ O(KQBM 2),
T ([ (cid:101)Kj]n,n) ∈ O(QBM ),
T (ajn) ∈ O(QBM 2),

T ([Σk(n)]j,j) ∈ O(KQBM ).

and for diagonal covariance we have:

The computational cost of evaluating all interim values is thus in O(QM (M D +M 2 +BD +
KBM )) for the case of full covariance and in O(QM (M D + M 2, BD, KB)) for diagonal
covariances.

J.2 Analysis of the Lkl

The computational complexity of terms in the KL-divergence and its gradients is given by:

T (Lcross) ∈ O(KQM 2),
T (Lent) ∈ O(K2QM 3),

49

T (∇mLcross) ∈ O(KQM 2),
T (∇SLcross) ∈ O(KQM 2),
T (∇πk Lcross) ∈ O(KQM 2),
T (∇mLent) ∈ O(K2QM 3),
T (∇SLent) ∈ O(K2QM 3),
T (∇πk Lent) ∈ O(K2QM 3),

T (∇˜SLcross) ∈ O(KQM ),
T (∇mLent) ∈ O(K2QM ),
T (∇˜SLent) ∈ O(K2QM ),
T (∇πk Lent) ∈ O(K2QM ).

and for diagonal covariance we have:

Hence the computational cost of evaluating the KL-term and its gradients is in O(K2QM 3)
for full covariances, and O(KQM (K + M )) for diagonal covariances.

J.3 Analysis of the Lell

Let T (log p(yn|fn·, φ)) ∈ O(L), then the cost of evaluating the expected log likelihood and
its gradients is given by:

T (Lell) ∈ O(KBS(L + Q)),

T (∇mLell) ∈ O(KQ(M 2 + BM + BSL)),
T (∇SLell) ∈ O(KQB(M 2 + SL)),
T (∇πk Lell) ∈ O(KBS(L + Q)),

and for diagonal covariance we have:

T (∇˜SLell) ∈ O(KQB(M + SL)).

Hence the computational cost of evaluating the ell term and its gradients is in O(KQB(M 2+
SL)) for full covariances, and O(KQB(M + SL)) for diagonal covariances.

K Additional Results

K.1 Small-scale Experiments — Warped GPs

In the case of the creep dataset (Figure 13), sse and nlpd are better in denser models. As
expected, the performance of the dense (SF = 1) fg model is identical to the performance
of wgp. We also note that nlpd in gp has less variation compared to the other models,
which can be attributed to its assumption of Gaussian noise.

K.2 Medium-scale Experiments — Binary Classiﬁcation

We see in Figure 14 that the performance improves signiﬁcantly with denser models,
i.e. when we use a larger number of inducing variables. Overall, our model achieves an

50

Figure 13: The distributions of sse and nlpd for a warped Gaussian process likelihood
model on the creep dataset. Three approximate posteriors in savigp are used: fg (full
Gaussian), mog1 (diagonal Gaussian), and mog2 (mixture of two diagonal Gaussians),
along with various sparsity factors (SF = M/N ). The smaller the sf the sparser the model,
with SF = 1 corresponding to the original model without sparsity. wgp corresponds to the
performance of the exact inference method for warped Gaussian process models (Snelson
et al., 2003), and gp is the performance of the exact inference on a univariate Gaussian
likelihood model.

Figure 14: Error rates and nlp for binary classiﬁcation on the mnist-b dataset. We used
a full Gaussian (fg) posterior approximation across various sparsity factors (SF = M/N ).
The smaller the SF the sparser the model.

accuracy of 98.1% and a mean nlp of 0.062 on mnist-b with a sparsity factor of 0.04.
This is slightly better than the performance obtained by Hensman et al. (2015), who report

51

Figure 15: Mean sse and nlpd for multi-output regression on the sarcos dataset. We used
a full Gaussian (fg) posterior approximation across various sparsity factors (SF = M/N ).
The smaller the SF the sparser the model.

an accuracy of 97.8% and nlp of 0.069, although they used a smaller number of inducing
variables. As we have seen in §9.5, our model also achieves similar performance when using
the same sparsity factor as that used by Hensman et al. (2015) when the inducing inputs
(i.e. the locations of the inducing variables) are learned.

K.3 Medium-scale Experiments — Gaussian Process Regression Net-

works

Figure 15 shows the performance on the sarcos dataset. Compared to sarcos-2, learning
all joints yields a slight increase in msse for joints 4 and 7, but also resulted in a signiﬁcant
decrease in nlpd. This shows that the transition from learning only 2 joints to all 7 joints
(as in sarcos-2), while unhelpful for point estimates, led to a signiﬁcantly better posterior
density estimate. We also note that we had to make use of 10, 000 samples for this dataset
to get stable results over diﬀerent sparsity factors. This is a much larger number than
the 100 samples used in mnist. This is likely because the gprn likelihood model involves
multiplication between latent processes, which increases the variance of sampling results by
a squared factor, and therefore, it is reasonable to require more samples in this complex
likelihood model.

K.4 Medium-scale Experiments — Inducing-input Learning

Figure 16 shows the performance of the model under the two settings (with and without
learning inducing inputs) on mnist. As with mnist-b, we see that learning the location
of the inducing variables yields a large gain in performance. In fact, the sparser models
with inducing point learning performed similarly to the denser models, despite the fact that
the two models diﬀered by an order of magnitude when it came to the number of inducing
variables. Figures 17, 18 show the performance on sarcos and sarcos-2. We observe
an improvement in performance from learning inducing inputs. However, the improvement
is less signiﬁcant than for mnist, which shows that diﬀerent datasets have signiﬁcantly
diﬀerent sensitivity to the location of the inducing variables.

52

Figure 16: Comparison of error rate and nlp obtained by savigp without learning (fg) and
with learning (ind) of inducing inputs for multi-class classiﬁcation on the mnist dataset.

Figure 17: Comparison of mean sse and nlpd obtained by savigp without learning (fg)
and with learning (ind) of inducing points for multi-output regression on the sarcos-2
dataset.

Figure 18: Comparison of mean sse and nlpd obtained by savigp without learning (fg) and
with learning (ind) of inducing inputs for multi-output regression on the sarcos dataset.

L Notation

A summary of the notation used in this paper is shown in Table 4.

53

Table 4: Summary of notation used in the paper.

Symbol Description

Bold lower case denotes a vector
Bold upper case denotes a matrix
Determinant of matrix C
Trace of matrix C
Input dimensionality
Number of training datapoints
Number of inducing variables per latent process
Number of outputs
Number of latent functions
Number of mixture components in variational posterior
Number of samples in Monte Carlo estimates
nth input datapoint
N × D matrix of input data
P -dimensional vector of outputs (label) for nth observation
Q-dimensional vector of latent function values for nth observation
N -dimensional vector of latent function values for jth process
Inducing variables of latent process j
Vector of all inducing variables
M × D Matrix of inducing inputs for latent process j

v
C
|C|
tr C
D
N
M
P
Q
K
S
xn
X
yn
fn·
f·j
u·j
u
Zj
κj(x, x(cid:48)) Covariance function of latent process j evaluated at x, x(cid:48)
Kj
xx
Kj
zz
Kzz
Kj
xz
kj
zn
q(u|λ)
q(f |λ)
m
S
b
Σ

N × N covariance matrix obtained by evaluating κj(X, X)
M × M covariance matrix obtained by evaluating κj(Zj, Zj)
Block-diagonal covariance with blocks Kj
zz
N × M covariance between X and Zj
M × 1 covariance vector between Zj and xn
Variational distribution over inducing variables
Variational distribution over latent functions
Posterior mean over inducing variables
Posterior covariance over inducing variables
Posterior mean over latent functions
Posterior covariance over latent functions

54

References

Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, and Dzmitry
Bahdanau. Theano: A python framework for fast computation of mathematical expres-
sions. arXiv preprint arXiv:1605.02688, 2016.

Mauricio Alvarez and Neil D Lawrence. Sparse convolved Gaussian processes for multi-

output regression. In Neural Information Processing Systems, pages 57–64. 2009.

Mauricio A ´Alvarez and Neil D Lawrence. Computationally eﬃcient convolved multiple
output Gaussian processes. Journal of Machine Learning Research, 12(5):1459–1500,
2011.

Mauricio A. ´Alvarez, David Luengo, Michalis K. Titsias, and Neil D. Lawrence. Eﬃcient
multioutput Gaussian processes through variational inducing kernels. In Artiﬁcial Intel-
ligence and Statistics, 2010.

K. Bache and M. Lichman. UCI machine learning repository, 2013. URL http://archive.

ics.uci.edu/ml.

Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeﬀrey Mark
arXiv preprint

Siskind. Automatic diﬀerentiation in machine learning: a survey.
arXiv:1502.05767, 2015.

Christopher M. Bishop, Neil D. Lawrence, Tommi Jaakkola, and Michael I. Jordan. Ap-
proximating posterior distributions in belief networks using mixtures. In M. I. Jordan,
M. J. Kearns, and S. A. Solla, editors, Neural Information Processing Systems, pages
416–422. MIT Press, 1998.

Edwin V. Bonilla, Kian Ming A. Chai, and Christopher K. I. Williams. Multi-task Gaussian

process prediction. In Neural Information Processing Systems. 2008.

Edwin V. Bonilla, Shengbo Guo, and Scott Sanner. Gaussian process preference elicitation.

In Neural Information Processing Systems. 2010.

Edwin V Bonilla, Daniel Steinberg, and Alistair Reid. Extended and unscented kitchen

sinks. In International Conference on Machine Learning, 2016.

Phillip Boyle and Marcus Frean. Dependent Gaussian processes. In Neural Information

Processing Systems. 2005.

Thang D Bui, Josiah Yan, and Richard E Turner. A unifying framework for Gaussian
process pseudo-point approximations using power expectation propagation. The Journal
of Machine Learning Research, 18(1):3649–3720, 2017.

George Casella and Christian P. Robert. Rao-Blackwellisation of sampling schemes.

Biometrika, 1996.

Edward Challis and David Barber. Gaussian Kullback-Leibler approximate inference. Jour-

nal of Machine Learning Research, 14:2239–2286, 2013.

55

Wei Chu and Zoubin Ghahramani. Gaussian processes for ordinal regression. Journal of

Machine Learning Research, 6(Jul):1019–1041, 2005.

D. Cole, C. Martin-Moran, A.G. Sheard, H.K.D.H. Bhadeshia, and D.J.C. MacKay. Mod-
elling creep rupture strength of ferritic steel welds. Science and Technology of Welding
& Joining, 5(2):81–89, 2000.

Noel Cressie. Statistics for spatial data. John Wiley & Sons, 1993.

Kurt Cutajar, Edwin V. Bonilla, Pietro Michiardi, and Maurizio Filippone. Random fea-
ture expansions for deep Gaussian processes. In International Conference on Machine
Learning, volume 70, pages 884–893. PMLR, 06–11 Aug 2017.

Andreas Damianou and Neil Lawrence. Deep Gaussian processes. In Artiﬁcial Intelligence

and Statistics, 2013.

Sourish Das, Sasanka Roy, and Rajiv Sambasivan. Fast Gaussian process regression for big

data. arXiv preprint arXiv:1509.05142, 2015.

Marc Peter Deisenroth and Jun Wei Ng. Distributed Gaussian processes. In International

Conference on Machine Learning, 2015.

Amir Dezfouli and Edwin V. Bonilla. Scalable inference for Gaussian process models with

black-box likelihoods. In Neural Information Processing Systems. 2015.

Pedro Domingos, Stanley Kok, Hoifung Poon, Matthew Richardson, and Parag Singla.
Unifying logical and statistical AI. In AAAI Conference on Artiﬁcial Intelligence, 2006.

Kun Dong, David Eriksson, Hannes Nickisch, David Bindel, and Andrew G Wilson. Scalable
log determinants for Gaussian process kernel learning. In Neural Information Processing
Systems, pages 6327–6337, 2017.

Simon Duane, Anthony D. Kennedy, Brian J. Pendleton, and Duncan Roweth. Hybrid

Monte Carlo. Physics letters B, 195(2):216–222, 1987.

Yarin Gal and Richard Turner. Improving the Gaussian process sparse spectrum approxi-
mation by representing uncertainty in frequency inputs. In International Conference on
Machine Learning, pages 655–664, 2015.

Yarin Gal, Mark van der Wilk, and Carl Rasmussen. Distributed variational inference in
In Neural Information

sparse Gaussian process regression and latent variable models.
Processing Systems. 2014.

Pietro Galliani, Amir Dezfouli, Edwin V. Bonilla, and Novi Quadrianto. Gray-box inference
for structured Gaussian process models. In Aarti Singh and Jerry Zhu, editors, Artiﬁcial
Intelligence and Statistics, pages 353–361, Fort Lauderdale, FL, USA, Apr 2017. PMLR.

Andrew Gelman and Donald B Rubin. Inference from iterative simulation using multiple

sequences. Statistical science, pages 457–472, 1992.

56

Samuel J. Gershman, Matthew D. Hoﬀman, and David M. Blei. Nonparametric variational

inference. In International Conference on Machine Learning, 2012.

Noah D. Goodman, Vikash K. Mansinghka, Daniel M. Roy, Keith Bonawitz, and Joshua B.
In Uncertainty in Artiﬁcial

Tenenbaum. Church: A language for generative models.
Intelligence, 2008.

Ricardo Henao and Ole Winther. Predictive active set selection methods for Gaussian

processes. Neurocomputing, 80:10–18, 2012.

James Hensman, Nicolo Fusi, and Neil D Lawrence. Gaussian processes for big data. In

Uncertainty in Artiﬁcial Intelligence, 2013.

James Hensman, Alexander Matthews, and Zoubin Ghahramani. Scalable variational Gaus-

sian process classiﬁcation. In Artiﬁcial Intelligence and Statistics, 2015.

James Hensman, Nicolas Durrande, and Arno Solin. Variational Fourier features for Gaus-

sian processes. Journal of Machine Learning Research, 18:151:1–151:52, 2017.

Daniel Hern´andez-Lobato and Jos´e Miguel Hern´andez-Lobato. Scalable Gaussian process
classiﬁcation via expectation propagation. In Artiﬁcial Intelligence and Statistics, pages
168–176, 2016.

Geoﬀrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing
In Sixth Annual Conference on Computational

the description length of the weights.
Learning Theory, pages 5–13. ACM, 1993.

Trong Nghia Hoang, Quang Minh Hoang, and Kian Hsiang Low. A unifying framework of
anytime sparse Gaussian process regression models with stochastic variational inference
for big data. In International Conference on Machine Learning, 2015.

Matthew D. Hoﬀman and Andrew Gelman. The no-u-turn sampler: adaptively setting
path lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15(1):
1593–1623, 2014.

Marco F. Huber, Tim Bailey, Hugh Durrant-Whyte, and Uwe D. Hanebeck. On entropy
approximation for Gaussian mixture random vectors. In IEEE International Conference
on Multisensor Fusion and Integration for Intelligent Systems, 2008.

Kevin Jarrett, Koray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann Lecun. What is the
In International Conference on

best multi-stage architecture for object recognition?
Computer Vision, 2009.

R.G. Jarrett. A note on the intervals between coal-mining disasters. Biometrika, 66(1):

191–193, 1979.

ST John and James Hensman. Large-scale Cox process inference using variational Fourier

features. In International Conference on Machine Learning, 2018.

Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An

introduction to variational methods for graphical models. Springer, 1998.

57

Kristian Kersting, Christian Plagemann, Patrick Pfaﬀ, and Wolfram Burgard. Most likely
In International Conference on Machine

heteroscedastic Gaussian process regression.
Learning, 2007.

Mohammad E. Khan, Shakir Mohamed, Benjamin M. Marlin, and Kevin P. Murphy. A
stick-breaking likelihood for categorical data analysis with latent Gaussian models. In
Artiﬁcial Intelligence and Statistics, pages 610–618, 2012.

Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In International

Conference on Learning Representations, 2014.

Karl Krauth, Edwin V. Bonilla, Kurt Cutajar, and Maurizio Filippone. AutoGP: Exploring
the capabilities and limitations of gaussian process models. In Uncertainty in Artiﬁcial
Intelligence, 2017.

Harri Lappalainen and James W. Miskin. Ensemble Learning, pages 75–92. Springer Lon-

don, London, 2000.

Steﬀen L Lauritzen. Time series analysis in 1880: A discussion of contributions made by

tn thiele. International Statistical Review, pages 319–331, 1981.

Neil Lawrence. Probabilistic non-linear principal component analysis with Gaussian process

latent variable models. Journal of Machine Learning Research, 6:1783–1816, 2005.

Neil D Lawrence, Matthias Seeger, and Ralf Herbrich. Fast sparse Gaussian process meth-
ods: The informative vector machine. In Neural Information Processing Systems, 2002.

Miguel L´azaro-Gredilla, Joaquin Qui˜nonero-Candela, Carl Edward Rasmussen, and
An´ıbal R Figueiras-Vidal. Sparse spectrum Gaussian process regression. The Journal
of Machine Learning Research, 99:1865–1881, 2010.

Ga¨elle Loosli, St´ephane Canu, and L´eon Bottou. Training invariant support vector machines
using selective sampling. In Large Scale Kernel Machines, pages 301–320. MIT Press,
Cambridge, MA., 2007.

David J. C. MacKay. Developments in probabilistic modelling with neural networks —
ensemble learning. In Bert Kappen and Stan Gielen, editors, Third Annual Symposium
on Neural Networks, pages 191–198, London, 1995. Springer London.

David J. C. MacKay. Information theory, inference and learning algorithms. Cambridge

University Press, 2003.

Alexander G de G Matthews, James Hensman, Richard Turner, and Zoubin Ghahramani.
On sparse variational methods and the Kullback-Leibler divergence between stochastic
processes. Journal of Machine Learning Research, 51:231–239, 2016.

Alexander G. de G. Matthews, Mark van der Wilk, Tom Nickson, Keisuke. Fujii, Alexis
Boukouvalas, Pablo Le’on-Villagr’a, Zoubin Ghahramani, and James Hensman. GPﬂow:
A Gaussian process library using TensorFlow. Journal of Machine Learning Research, 18
(40):1–6, apr 2017.

58

Vladimir Maz’ya and Gunther Schmidt. On approximate approximations using Gaussian

kernels. IMA Journal of Numerical Analysis, 16(1):13–29, 1996.

Thomas P Minka. Expectation propagation for approximate Bayesian inference. In Uncer-

tainty in Artiﬁcial Intelligence, 2001.

Jesper Møller, Anne Randi Syversveen, and Rasmus Plenge Waagepetersen. Log Gaussian

Cox processes. Scandinavian journal of statistics, 25(3):451–482, 1998.

Iain Murray, Ryan Prescott Adams, and David J.C. MacKay. Elliptical slice sampling. In

Artiﬁcial Intelligence and Statistics, 2010.

Radford M. Neal. Probabilistic inference using Markov chain Monte Carlo methods. Tech-

nical report, Department of Computer Science, University of Toronto, 1993.

Trung V. Nguyen and Edwin Bonilla. Eﬃcient variational inference for Gaussian process

regression networks. In Artiﬁcial Intelligence and Statistics, pages 472–480, 2013.

Trung V. Nguyen and Edwin V. Bonilla. Automated variational inference for Gaussian

process models. In Neural Information Processing Systems. 2014a.

Trung V. Nguyen and Edwin V. Bonilla. Collaborative multi-output Gaussian processes.

In Uncertainty in Artiﬁcial Intelligence, 2014b.

Hannes Nickisch and Carl Edward Rasmussen. Approximations for binary Gaussian process

classiﬁcation. Journal of Machine Learning Research, 9(10), 2008.

Anthony O’Hagan and JFC Kingman. Curve ﬁtting and optimal design for prediction.
Journal of the Royal Statistical Society. Series B (Methodological), pages 1–42, 1978.

Manfred Opper and C´edric Archambeau. The variational Gaussian approximation revisited.

Neural Computation, 21(3):786–792, 2009.

C Paciorek and M Schervish. Nonstationary covariance functions for Gaussian process

regression. Neural Information Processing Systems, 2004.

Joaquin Qui˜nonero-Candela and Carl Edward Rasmussen. A unifying view of sparse approx-
imate Gaussian process regression. Journal of Machine Learning Research, 6:1939–1959,
2005.

Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Neural

Information Processing Systems. 2008.

Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing min-
In Neural Information Processing Systems.

imization with randomization in learning.
2009.

Rajesh Ranganath, Sean Gerrish, and David M. Blei. Black box variational inference. In

Artiﬁcial Intelligence and Statistics, 2014.

59

Marc’Aurelio Ranzato, Christopher Poultney, Sumit Chopra, and Yann Lecun. Eﬃcient
learning of sparse representations with an energy-based model. In Neural Information
Processing Systems, 2006.

Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine

learning. The MIT Press, 2006.

Alistair Reid, Simon O’Callaghan, Edwin V. Bonilla, Lachlan McCalman, Tim Rawling,
and Fabio Ramos. Bayesian joint inversions for the exploration of Earth resources. In
International Joint Conference on Artiﬁcial Intelligence, 2013.

Danilo J Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In International Conference on Machine
Learning, 2014.

Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of

mathematical statistics, pages 400–407, 1951.

Sheldon M Ross. Simulation. Burlington, MA: Elsevier, 2006.

H˚avard Rue, Sara Martino, and Nicolas Chopin. Approximate Bayesian inference for latent
Gaussian models by using integrated nested Laplace approximations. Journal of the royal
statistical society: Series b (statistical methodology), 71(2):319–392, 2009.

Hugh Salimbeni and Marc Deisenroth. Doubly stochastic variational inference for deep
Gaussian processes. In Advances in Neural Information Processing Systems, pages 4588–
4599, 2017.

Hugh Salimbeni, Stefanos Eleftheriadis, and James Hensman. Natural gradients in practice:
Non-conjugate variational inference in Gaussian process models. In Artiﬁcial Intelligence
and Statistics, 2018.

Rishit Sheth, Yuyang Wang, and Roni Khardon. Sparse variational inference for generalized

GP models. In International Conference on Machine Learning, 2015.

Ed Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. In

Neural Information Processing Systems, 2006.

Edward Snelson, Carl Edward Rasmussen, and Zoubin Ghahramani. Warped Gaussian

processes. In Neural Information Processing Systems, 2003.

Albert Tarantola. Inverse Problem Theory and Methods for Model Parameter Estimation.

Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 2004.

Yee Whye Teh, Matthias Seeger, and Michael I. Jordan. Semiparametric latent factor

models. In Artiﬁcial Intelligence and Statistics, 2005.

Michael E Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of

machine learning research, 1(Jun):211–244, 2001.

60

Michalis Titsias. Variational learning of inducing variables in sparse Gaussian processes. In

Artiﬁcial Intelligence and Statistics, 2009.

Sethu Vijayakumar and Stefan Schaal. Locally weighted projection regression: An O(n)
algorithm for incremental real time learning in high dimensional space. In International
Conference on Machine Learning, 2000.

Christopher K.I. Williams and David Barber. Bayesian classiﬁcation with Gaussian pro-
cesses. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 20(12):1342–
1351, 1998.

Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for regression.

In Neural Information Processing Systems, pages 514–520, 1996.

Andrew G. Wilson, David A. Knowles, and Zoubin Ghahramani. Gaussian process regres-

sion networks. In International Conference on Machine Learning, 2012.

Frank Wood, Jan-Willem van de Meent, and Vikash Mansinghka. A new approach to
probabilistic programming inference. In Artiﬁcial Intelligence and Statistics, pages 1024–
1032, 2014.

Zichao Yang, Andrew Gordon Wilson, Alexander J. Smola, and Le Song. `A la carte —

learning fast kernels. In Artiﬁcial Intelligence and Statistics, 2015.

Kai Yu and Wei Chu. Gaussian process models for link analysis and transfer learning. In

Neural Information Processing Systems, 2008.

Matthew D Zeiler. ADADELTA: an adaptive learning rate method.

arXiv preprint

arXiv:1212.5701, 2012.

61

8
1
0
2
 
v
o
N
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
7
7
5
0
0
.
9
0
6
1
:
v
i
X
r
a

Generic Inference in Latent Gaussian Process Models

Edwin V. Bonilla∗†

Karl Krauth∗‡

Amir Dezfouli†

November 6, 2018

Abstract

We develop an automated variational method for inference in models with Gaussian
process (gp) priors and general likelihoods. The method supports multiple outputs
and multiple latent functions and does not require detailed knowledge of the condi-
tional likelihood, only needing its evaluation as a black-box function. Using a mixture
of Gaussians as the variational distribution, we show that the evidence lower bound
and its gradients can be estimated eﬃciently using samples from univariate Gaussian
distributions. Furthermore, the method is scalable to large datasets which is achieved
by using an augmented prior via the inducing-variable approach underpinning most
sparse gp approximations, along with parallel computation and stochastic optimiza-
tion. We evaluate our approach quantitatively and qualitatively with experiments on
small datasets, medium-scale datasets and large datasets, showing its competitiveness
under diﬀerent likelihood models and sparsity levels. On the large-scale experiments
involving prediction of airline delays and classiﬁcation of handwritten digits, we show
that our method is on par with the state-of-the-art hard-coded approaches for scalable
gp regression and classiﬁcation.

Keywords: Gaussian processes, black-box likelihoods, nonlinear likelihoods, scalable
inference, variational inference

1 Introduction

Developing fully automated methods for inference in complex probabilistic models has be-
come arguably one of the most exciting areas of research in machine learning, with notable
examples in the probabilistic programming community (see e.g. Hoﬀman and Gelman, 2014;
Wood et al., 2014; Goodman et al., 2008; Domingos et al., 2006). Indeed, while these prob-
abilistic programming systems allow for the formulation of very expressive and ﬂexible
probabilistic models, building eﬃcient inference methods for reasoning in such systems is a
signiﬁcant challenge.

One particularly interesting setting is when the prior over the latent parameters of the
model can be well described with a Gaussian process (gp, Rasmussen and Williams, 2006).
gp priors are a popular choice in practical non-parametric Bayesian modeling with perhaps

∗Joint ﬁrst author.
†Machine Learning Research Group, Data61, Sydney NSW 2015, Australia.
‡Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA

94720-1776, USA.

1

the most straightforward and well-known application being the standard regression model
with Gaussian likelihood, for which the posterior can be computed in closed form (see e.g.
Rasmussen and Williams, 2006, §2.2).

Interest in gp models stems from their functional non-parametric Bayesian nature and
there are at least three critical beneﬁts from adopting such a modeling approach. Firstly, by
being a prior over functions, they represent a much more elegant way to address problems
such as regression, where it is less natural to think of a prior over parameters such as the
weights in a linear model. Secondly, by being Bayesian, they provide us with a principled
way to combine our prior beliefs with the observed data in order to predict a full posterior
distribution over unknown quantities, which of course is much more informative than a single
point prediction. Finally, by being non-parametric, they address the common concern of
parametric approaches about how well we can ﬁt the data, since the model space is not
constrained to have a parametric form.

1.1 Key Inference Challenges in Gaussian Process Models

Nevertheless, such a principled and ﬂexible approach to machine learning comes at the
cost of facing two fundamental probabilistic inference challenges, (i) scalability to large
datasets and (ii) dealing with nonlinear likelihoods. With regards to the ﬁrst challenge,
scalability, gp models have been notorious for their poor scalability as a function of the
number of training points. Despite ground-breaking work in understanding scalable gps
through the so-called sparse approximations (Qui˜nonero-Candela and Rasmussen, 2005) and
in developing practical inference methods for these models (Titsias, 2009), recent literature
on addressing this challenge gives a clear indication that the scalability problem is still a
very active area of research, see e.g. Das et al. (2015); Deisenroth and Ng (2015); Hoang
et al. (2015); Dong et al. (2017); Hensman et al. (2017); Salimbeni et al. (2018); John and
Hensman (2018).

Concerning the second challenge, dealing with nonlinear likelihoods, the main diﬃculty
is that of estimating a posterior over latent functions distributed according to a gp prior,
given observations assumed to be drawn from a possibly nonlinear conditional likelihood
model. In general, this posterior is analytically intractable and one must resort to approx-
imate methods. These methods can be roughly classiﬁed into stochastic approaches and
deterministic approaches. The former, stochastic approaches, are based on sampling algo-
rithms such as Markov Chain Monte Carlo (mcmc, see e.g. Neal, 1993, for an overview)
and the latter, deterministic approaches, are based on optimization techniques and include
variational inference (vi, Jordan et al., 1998), the Laplace approximation (see e.g. MacKay,
2003, Chapter 27) and expectation propagation (ep, Minka, 2001).

On the one hand, although stochastic techniques such as mcmc provide a ﬂexible frame-
work for sampling from complex posterior distributions of probabilistic models, their gen-
erality comes at the expense of a very high computational cost as well as cumbersome con-
vergence analysis. On the other hand, deterministic methods such as variational inference
build upon the main insight that optimization is generally easier than integration. Conse-
quently, they estimate a posterior by maximizing a lower bound of the marginal likelihood,
the so-called evidence lower bound (elbo). Variational methods can be considerably faster

2

than mcmc but they lack mcmc’s broader applicability, usually requiring mathematical
derivations on a model-by-model basis.1

1.2 Contributions

In this paper we address the above challenges by developing a scalable automated variational
method for inference in models with Gaussian process priors and general likelihoods. This
method reduces the overhead of the tedious mathematical derivations traditionally inherent
to variational algorithms, allowing their application to a wide range of problems. In par-
ticular, we consider models with multiple latent functions, multiple outputs and non-linear
likelihoods that satisfy the following properties: (i) factorization across latent functions and
(ii) factorization across observations. The former assumes that, when there are more than
one latent function, they are generated from independent gps. The latter assumes that,
given the latent functions, the observations are conditionally independent.

Existing gp models, such as regression (Rasmussen and Williams, 2006), binary and
multi-class classiﬁcation (Nickisch and Rasmussen, 2008; Williams and Barber, 1998), warped
gps (Snelson et al., 2003), log Gaussian Cox process (Møller et al., 1998), and multi-output
regression (Wilson et al., 2012), all fall into this class of models. Furthermore, our ap-
proach goes well beyond standard settings for which elaborate learning machinery has been
developed, as we only require access to the likelihood function in a black-box manner. As
we shall see below, our inference method can scale up to very large datasets, hence we will
refer to it as savigp, which stands for scalable automated variational inference for Gaussian
process models. The key characteristics of our method and contributions of this work are
summarized below.

• Black-box likelihoods: As mentioned above, the main contribution of this work is to
be able to carry out posterior inference with gp priors and general likelihood models,
without knowing the details of the conditional likelihood (or its gradients) and only
requiring its evaluation as a black-box function.

• Scalable inference and stochastic optimization: By building upon the inducing-variable
approach underpinning most sparse approximations to gp models (Qui˜nonero-Candela
and Rasmussen, 2005; Titsias, 2009), the computational complexity of our method is
dominated by O(M 3) operations in time, where M (cid:28) N is the number of inducing
variables and N the number of observations. This is in contrast to naive inference in
gp models which has a time complexity of O(N 3). As the resulting elbo decomposes
over the training datapoints, our model is amenable to parallel computation and
stochastic optimization. In fact, we provide an implementation that can scale up to
a very large number of observations, exploiting stochastic optimization, multi-core
architectures and gpu computation.

• Joint learning of model parameters: As our approach is underpinned by variational
inference principles, savigp allows for learning of all model parameters, including

1However, we note recent developments such as variational auto-encoders (Kingma and Welling, 2014)

and refer the reader to the related work and the discussion (Sections 2.4 and 10, respectively).

3

posterior parameters, inducing inputs, covariance hyperparameters and likelihood pa-
rameters, within the same framework via maximization of the evidence lower bound
(elbo).

• Multiple outputs and multiple latent functions: savigp is designed to support models
with multiple outputs and multiple latent functions, such as in multi-class classiﬁca-
tion (Williams and Barber, 1998) and non-stationary multi-output regression (Wilson
et al., 2012). It does so in a very ﬂexible way, allowing the diﬀerent gp priors on the
latent functions to have diﬀerent covariance functions and inducing inputs.

• Flexible posterior: savigp uses a mixture of Gaussians as the approximating posterior
distribution. This is a very general approach as it is well known that, with a suﬃcient
number of components, almost any continuous density can be approximated with
arbitrary accuracy (Maz’ya and Schmidt, 1996).

• Statistical eﬃciency: By using knowledge of the approximate posterior and the struc-
ture of the gp prior, we exploit the decomposition of the elbo, into a KL-divergence
term and an expected log likelihood term, to provide statistically eﬃcient parameter
estimates. In particular, we derive an analytical lower bound for the KL-divergence
term and we show that, for general black-box likelihood models, the expected log likeli-
hood term and its gradients can be computed eﬃciently using samples from univariate
Gaussian distributions.

• Eﬃcient re-parametrization: For the case of a single full Gaussian variational pos-
terior, we show that it is possible to re-parametrize the model so that the optimal
posterior can be represented using a parametrization that is linear in the number of
observations. This parametrization becomes useful for denser models, i.e. for models
that have a larger number of inducing variables.

• Extensive experimentation: We evaluate savigp with experiments on small datasets,
medium-scale datasets and two large datasets. The experiments on small datasets
(N < 1, 300) evaluate the method under diﬀerent likelihood models and sparsity
levels (as determined by the number of inducing variables), including problems such as
regression, classiﬁcation, Log Gaussian Cox processes, and warped gps (Snelson et al.,
2003). We show that savigp can perform as well as hard-coded inference methods
under high levels of sparsity. The medium-scale experiments consider binary and
multi-class classiﬁcation using the mnist dataset (N = 60, 000) and non-stationary
regression under the gprn model (Wilson et al., 2012) using the sarcos dataset
(N ≈ 45, 000). Besides showing the competitiveness of our model for problems at this
scale, we analyze the eﬀect of learning the inducing inputs, i.e. the location of inducing
In our ﬁrst large-scale experiment, we study the problem of predicting
variables.
airline delays (using N = 700, 000), and show that our method is on par with the
state-of-the-art approach for scalable gp regression (Hensman et al., 2013), which
uses full knowledge of the likelihood model. In our second large-scale experiment, we
consider the mnist8m dataset, which is an augmented version of mnist (containing
N = 8, 100, 000 observations). We show that by using this augmented dataset we

4

can improve performance signiﬁcantly. Finally, in an experiment concerning a non-
linear seismic inversion problem, we show that our approach can be applied easily
(without any changes to the inference algorithm) to non-standard machine learning
tasks and that our inference method can match closely the solution found by more
computationally demanding approaches such as mcmc.

Before describing the family of gp models that we focus on, we start by relating our
work to the previous literature concerning the key inference challenges mentioned above,
i.e. scalability and dealing with non-linear likelihoods.

2 Related Work

As pointed out by Rasmussen and Williams (2006), there has been a long-standing inter-
est in Gaussian processes with early work dating back at least to the 1880s when Danish
astronomer and mathematician T. N. Thiel, concerned with determining the distance be-
tween Copenhagen and Lund from astronomical observations, essentially proposed the ﬁrst
In
mathematical formulation of Brownian motion (see e.g. Lauritzen, 1981, for details).
geostatistics, gp regression is known as Kriging (see e.g. Cressie, 1993) where, naturally, it
has focused on 2-dimensional and 3-dimensional problems.

While the work of O’Hagan and Kingman (1978) has been quite inﬂuential in applying
gps to general regression problems, the introduction of gp regression to main-stream ma-
chine machine learning by Williams and Rasmussen (1996) sparked signiﬁcant interest in the
machine learning community. Indeed, henceforth, the community has taken gp models well
beyond the standard regression setting, addressing other problems such as non-stationary
and heteroscedastic regression (Paciorek and Schervish, 2004; Kersting et al., 2007; Wilson
et al., 2012); nonlinear dimensionality reduction (Lawrence, 2005); classiﬁcation (Williams
and Barber, 1998; Nickisch and Rasmussen, 2008); multi-task learning (Bonilla et al., 2008;
Yu and Chu, 2008; Alvarez and Lawrence, 2009; Wilson et al., 2012); preference learn-
ing (Bonilla et al., 2010); and ordinal regression (Chu and Ghahramani, 2005).
In fact,
their book (Rasmussen and Williams, 2006) is the de facto reference in any work related
to Gaussian process models in machine learning. As we shall see below, despite all these
signiﬁcant advances, most previous work in the gp community has focused on addressing
the scalability and the non-linear likelihood challenges in isolation.

2.1 Scalable Models

The cubic scaling on the number of observations of Gaussian process models, until very
recently, has hindered the use of these models in a wider variety of applications. Work on
approaching this problem has ranged from selecting informative (inducing) datapoints from
the training data so as to facilitate sparse approximations to the posterior (Lawrence et al.,
2002) to considering these inducing points as continuous parameters and optimizing them
within a coherent probabilistic framework (Snelson and Ghahramani, 2006).

Although none of these methods actually scale to very large datasets as their time
complexity is O(M 2N ), where N is the number of observations and M is the number

5

of inducing points, unifying such approaches from a probabilistic perspective (Qui˜nonero-
Candela and Rasmussen, 2005) has been extremely valuable to the community, not only to
understand what those methods are actually doing but also to develop new approaches to
sparse gp models. In particular, the framework of Titsias (2009) which has been placed
within a solid theoretical grounding by Matthews et al. (2016), has become the underpinning
machinery of modern scalable approaches to gp regression and classiﬁcation. This has been
taken one step further by allowing optimization of variational parameters within a stochastic
optimization framework (Hensman et al., 2013), hence enabling the applicability of these
inference methods to very large datasets.

Besides variational approaches based on reverse-KL divergence minimization, other
methods have adopted diﬀerent inference engines, based e.g. on the minimization of the for-
ward KL divergence, such as expectation propagation (Hern´andez-Lobato and Hern´andez-
Lobato, 2016). It turns out that all these methods can be seen from a more general per-
spective as minimization of α-divergences, see Bui et al. (2017) and references therein.

In addition to inducing-variable approaches to scalability in gp-models, other approaches
have exploited the relationship between inﬁnite-feature linear-in-the-parameters models and
gps. In particular, early work investigated truncated linear-in-the-parameters models as
approximations to gp regression (see e.g. Tipping, 2001). This idea has been developed in
an alternative direction that exploits the relationship between a covariance function of a
stationary process and its spectral density, hence providing random-feature approximations
to the covariance function of a gp, similarly to the work of Rahimi and Recht (2008,
2009), who focused on non-probabilistic kernel machines. For example, L´azaro-Gredilla
et al. (2010), Gal and Turner (2015) and Yang et al. (2015) have followed these types of
approaches, with the latter mostly concerned about having fast kernel evaluations.

Unlike our work, none of these approaches deals with the harder task of developing

scalable inference methods for multi-output problems and general likelihood models.

2.2 Multi-output and Multi-task Gaussian Processes

Developing multi-task or multi-output learning approaches using Gaussian processes has
also proved an intensive area of research. Most gp-based models for these problems as-
sume that correlations between the outputs are induced via linear combinations of a set of
independent latent processes. Such linear combinations can use ﬁxed coeﬃcients (see e.g.
Teh et al., 2005) or input-dependent coeﬃcients (Wilson et al., 2012; Nguyen and Bonilla,
2013). Tasks dependencies can also be deﬁned explicitly through the covariance function as
done by Bonilla et al. (2008), who assume a covariance decomposition between tasks and
inputs. More complex dependencies can be modeled by using the convolution formalism as
done in earlier work by Boyle and Frean (2005) and generalized by Alvarez and Lawrence
(2009); ´Alvarez et al. (2010), whose later work also provides eﬃcient inference algorithms
for such models ( ´Alvarez and Lawrence, 2011). Finally, the work of Nguyen and Bonilla
(2014b) also assumes a linear combination of independent latent processes but it exploits
the developments of Hensman et al. (2013) to scale up to very large datasets.

Besides the work of Nguyen and Bonilla (2014b), these approaches do not scale to a
very large number of observations and all of them are mainly concerned with regression
problems.

6

2.3 General Nonlinear Likelihoods

The problem of inference in models with gp priors and nonlinear likelihoods has been
tackled from a sampling perspective with algorithms such as elliptical slice sampling (ess,
Murray et al., 2010), which are more eﬀective at drawing samples from strongly correlated
Gaussians than generic mcmc algorithms. Nevertheless, as we shall see in Section 9.3.5, the
sampling cost of ess remains a major challenge for practical usages.

From a variational inference perspective, the work by Opper and Archambeau (2009)
has been slightly under-recognized by the community even though it proposes an eﬃcient
full Gaussian posterior approximation for gp models with i.i.d. observations. Our work
pushes this breakthrough further by allowing multiple latent functions, multiple outputs,
and more importantly, scalability to large datasets.

Another approach to deterministic approximate inference is the integrated nested Laplace
approximation (inla, Rue et al., 2009). inla uses numerical integration to approximate the
marginal likelihood, which makes it unsuitable for gp models that contain a large number
of hyperparameters.

2.4 More Recent Developments

As mentioned in §1, the scalability problem continues to attract the interest of researchers
working with gp models, with recently developed distributed inference frameworks (Gal
et al., 2014; Deisenroth and Ng, 2015), and the variational inference frameworks for scal-
able gp regression and classiﬁcation by Hensman et al. (2013) and Hensman et al. (2015),
respectively. As with the previous work described in §2.1, these approaches have been lim-
ited to classiﬁcation or regression problems or speciﬁc to a particular class of likelihood
models such as gp-modulated Cox processes (John and Hensman, 2018).

Contemporarily to the work of Nguyen and Bonilla (2014a), which underpins our ap-
proach, Ranganath et al. (2014) developed black-box variational inference (bbvi) for gen-
eral latent variable models. Due to this generality, it under-utilizes the rich amount of
information available in Gaussian process models. For example, bbvi approximates the
KL-divergence term of the evidence lower bound but this is computed analytically in our
method. Additionally, for practical reasons, bbvi imposes the variational distribution to
fully factorize over the latent variables, while we make no such a restriction. A clear dis-
advantage of bbvi is that it does not provide a practical way of learning the covariance
hyperparameters of gps — in fact, these are set to ﬁxed values. In principle, these values
can be learned in bbvi using stochastic optimization, but experimentally, we have found
this to be problematic, ineﬀectual, and time-consuming.

Very recently, Bonilla et al. (2016) have used the random-feature approach mentioned in
Section 2.1 along with linearization techniques to provide scalable methods for inference in
gp models with general likelihoods. Unlike our approach for estimating expectations of the
conditional likelihood, such linearizations of the conditional likelihood are approximations
and generally do not converge to the exact expectations even in the limit of a large number
of observations.

Following the recent advances in making automatic diﬀerentiation widely available and
easy to use in practical systems (see e.g. Baydin et al., 2015), developments on stochastic
variational inference for fairly complex probabilistic models (Rezende et al., 2014; Kingma

7

and Welling, 2014) can be used when the likelihood can be implemented in such systems
and can be expressed using the so-called re-parametrization trick (see e.g. Kingma and
Welling, 2014, for details). Nevertheless, the advantage of our method over these generic
approaches is twofold. Firstly, as with bbvi, in practice such a generality comes at the cost
of making assumptions about the posterior (such as factorization), which is not suitable for
gp models. Secondly, and more importantly, such methods are not truly black-box as they
require explicit access to the implementation of the conditional likelihood. An interesting
example where one cannot apply the re-parametrization trick is given by Challis and Barber
(2013), who describe a large class of functions (that include the Laplace log likelihood) that
are neither diﬀerentiable or continuous but their expectation over a Gaussian posterior is
smooth. A more general setting where a truly black-box approach is required concerns
inversion problems (Tarantola, 2004) where latent functions are passed through domain-
speciﬁc forward models followed by a known noise model (Bonilla et al., 2016; Reid et al.,
2013). These forward models may be non-diﬀerentiable, given as an executable, or too
complex to re-implement quickly in an automatic diﬀerentiation framework. To illustrate
this case, we present results on a seismic inversion application in §9.8.

Nevertheless, we acknowledge that some of these developments mentioned above have
been extended to the gp literature, where the re-parametrization trick has been used
(Krauth et al., 2017; Matthews et al., 2017; Hensman et al., 2017; Cutajar et al., 2017).
These contemporary works show that it is worthwhile building more tailored methods that
may require a lower number of samples to estimate the expectations in our framework.

Finally, a related area of research is that of modeling complex data with deep belief net-
works based on Gaussian process mappings (Damianou and Lawrence, 2013), which have
been proposed primarily as hierarchical extensions of the Gaussian process latent variable
model (Lawrence, 2005). Unlike our approach, these models target the unsupervised prob-
lem of discovering structure in high-dimensional data and have focused mainly on small-data
applications. However, much like the recent work on “shallow”-gp architectures, inference
in these models have also been made scalable for supervised and unsupervised learning, ex-
ploiting the reparameterization trick, e.g. using random-feature expansions (Cutajar et al.,
2017) or inducing-variable approximations (Salimbeni and Deisenroth, 2017).

3 Latent Gaussian Process Models

Before starting our description of the types of models we are addressing in this paper, we
refer the reader to Appendix L for a summary of the notation used henceforth.

We consider supervised learning problems where we are given a dataset D = {xn, yn}N
n=1,
where xn is a D-dimensional input vector and yn is a P -dimensional output. Our goal is to
learn the mapping from inputs to outputs, which can be established via Q underlying latent
functions {fj}Q
j=1. In many problems these latent functions have a physical interpretation,
while in others they are simply nuisance parameters and we just want to integrate them
out in order to make probabilistic predictions.

A sensible modeling approach to the above problem is to assume that the Q latent
functions {fj} are uncorrelated a priori and that they are drawn from Q zero-mean Gaussian

8

processes (Rasmussen and Williams, 2006):

p(fj|θj) ∼ GP (0, κj(·, ·; θj)) ,

j = 1, . . . Q,

then

p(f |θ) =

p(f·j|θj) =

N (f·j; 0, Kj

xx),

(1)

Q
(cid:89)

j=1

Q
(cid:89)

j=1

where f is the set of all latent function values; f·j = {fj(xn)}N
n=1 denotes the values of
latent function j; Kj
xx is the covariance matrix induced by the covariance function κj(·, ·; θj)
evaluated at every pair of inputs; and θ = {θj} are the parameters of the corresponding
covariance functions. Along with the prior in Equation (1), we can also assume that our
multi-dimensional observations {yn} are i.i.d. given the corresponding set of latent functions
{fn}:

p(y|f , φ) =

p(yn|fn·, φ),

(2)

N
(cid:89)

n=1

where y is the set of all output observations; yn is the nth output observation; fn· =
{fj(xn)}Q
j=1 is the set of latent function values which yn depends upon; and φ are the
conditional likelihood parameters. In the sequel, we will refer to the covariance parameters
(θ) as the model hyperparameters.

In other words, we are interested in models for which the following criteria are satisﬁed:

(i) factorization of the prior over the latent functions, as speciﬁed by Equation (1), and

(ii) factorization of the conditional likelihood over the observations given the latent func-

tions, as speciﬁed by Equation (2).

We refer to the models satisfying the above assumptions (when using gp priors) as la-
tent Gaussian process models (lgpms). Interestingly, a large class of problems can be well
modeled with the above assumptions. For example binary classiﬁcation (Nickisch and Ras-
mussen, 2008; Williams and Barber, 1998), warped gps (Snelson et al., 2003), log Gaussian
Cox processes (Møller et al., 1998), multi-class classiﬁcation (Williams and Barber, 1998),
and multi-output regression (Wilson et al., 2012) all belong to this family of models.

More importantly, besides the i.i.d. assumption, there are not additional constraints
on the conditional likelihood which can be any linear or nonlinear model. Furthermore,
as we shall see in §4, our proposed inference algorithm only requires evaluations of this
likelihood model in a black-box manner, i.e. without requiring detailed knowledge of its
implementation or its gradients.

3.1 Inference Challenges

As mentioned in §1, for general lgpms, the inference problem of estimating the posterior
distribution over the latent functions given the observed data p(f |D) and, ultimately, for
a new observation x(cid:63), estimating the predictive posterior distribution p(f(cid:63)·|x(cid:63), D), poses
two important challenges (even in the case of a single latent function Q = 1) from the

9

computational and statistical perspectives. These challenges are (i) scalability to large
datasets and (ii) dealing with nonlinear likelihoods.

We address the scalability challenge inherent to gp models (given their cubic time com-
plexity on the number of observations) by augmenting our priors using an inducing-variable
approach (see e.g. Qui˜nonero-Candela and Rasmussen, 2005) and embedding our model into
a variational inference framework (Titsias, 2009). In short, inducing variables in sparse gp
models act as latent summary statistics, avoiding the computation of large inverse covari-
ances. Crucially, unlike other approaches (Qui˜nonero-Candela and Rasmussen, 2005), we
keep an explicit representation of these variables (and integrate them out variationally),
which facilitates the decomposition of the variational objective into a sum on the individual
datapoints. This allows us to devise stochastic optimization strategies and parallel imple-
mentations in cloud computing services such as Amazon EC2. Such a strategy, also allows us
to learn the location of the inducing variables (i.e. the inducing inputs) in conjunction with
variational parameters and hyperparameters, which in general provides better performance,
especially in high-dimensional problems.

To address the nonlinear likelihood challenge, which from a variational perspective boils
down to estimating expectations of a nonlinear function over the approximate posterior,
we follow a stochastic estimation approach, in which we develop low-variance Monte Carlo
estimates of the expected log-likelihood term in the variational objective. Crucially, we
will show that the expected log-likelihood term can be estimated eﬃciently by using only
samples from univariate Gaussian distributions.

4 Scalable Inference

Here we describe our scalable inference method for the model class speciﬁed in §3. We
build upon the inducing-variable formalism underlying most sparse gp approximations
(Qui˜nonero-Candela and Rasmussen, 2005; Titsias, 2009) and obtain an algorithm with
time complexity O(M 3), where M is the number of inducing variables per latent process.
We show that, under this sparse approximation and the variational inference framework,
the expected log likelihood term and its gradient can be estimated using only samples from
univariate Gaussian distributions.

4.1 Augmented Prior

In order to make inference scalable we redeﬁne our prior in terms of some auxiliary variables
{u·j}Q
j=1, which we will refer to as the inducing variables. These inducing variables lie in
the same space as {f·j} and are drawn from the same zero-mean gp priors. As before,
we assume factorization of the prior across the Q latent functions. Hence the resulting
augmented prior is given by:

Q
(cid:89)

p(u) =

N (u·j; 0, Kj

zz),

p(f |u) =

N (f·j; ˜µj, (cid:101)Kj), where

Q
(cid:89)

j=1

j=1
˜µj = Kj
(cid:101)Kj = Kj

zz)−1u·j, and

xz(Kj
xx − AjKj

zx with Aj = Kj

xz(Kj

zz)−1,

(3)

(4)

10

where u·j are the inducing variables for latent process j; u is the set of all the inducing
variables; Zj are all the inducing inputs for latent process j; X is the matrix of all input
locations {xn}; and Kj
uv is the covariance matrix induced by evaluating the covariance
function κj(·, ·) at all pairwise columns of matrices U and V. We note that while each of
the inducing variables in u·j lies in the same space as the elements in f·j, each of the M
inducing inputs in Zj lies in the same space as each input data point xn. Therefore, while
u·j is a M -dimensional vector, Zj is a M × D matrix where each of the rows corresponds
to a D-dimensional inducing input. We refer the reader to Appendix L for a summary of
the notation and dimensionality of the above kernel matrices.

As thoroughly investigated by Qui˜nonero-Candela and Rasmussen (2005), most gp ap-
proximations can be formulated using the augmented prior above and additional assump-
tions on the training and test conditional distributions p(f |u) and p(f(cid:63)·|u), respectively.
Such approaches have been traditionally referred to as sparse approximations and we will
use this terminology as well. Analogously, we will refer to models with a larger number of
inducing inputs as denser models.

It is important to emphasize that the joint prior p(f , u) deﬁned in Equation (3) is an
equivalent prior to that in the original model, as if we integrate out the inducing variables
u from this joint prior we will obtain the prior p(f ) in Equation (1) exactly. Nevertheless,
as we shall see later, following a variational inference approach and having an explicit rep-
resentation of the (approximate) posterior over the inducing variables will be fundamental
to scaling up inference in these types of models without making the assumptions on the
training or test conditionals described in Qui˜nonero-Candela and Rasmussen (2005).

Along with the joint prior deﬁned above, we maintain the factorization assumption of

the conditional likelihood given in Equation (2).

4.2 Variational Inference and the Evidence Lower Bound

Given the prior in Equation (3) and the likelihood in Equation (2), posterior inference
for general (non-Gaussian) likelihoods is analytically intractable. Therefore, we resort to
approximate methods such as variational inference (Jordan et al., 1998). Variational in-
ference methods entail positing a tractable family of distributions and ﬁnding the member
of the family that is “closest” to the true posterior in terms of their the Kullback-Leibler
divergence. In our case, we are seeking to approximate the joint posterior p(f , u|y) with a
variational distribution q(f , u|λ).

4.3 Approximate Posterior

Motivated by the fact that the true joint posterior is given by p(f , u|y) = p(f |u, y)p(u|y),
our approximate posterior has the form:

q(f , u|λ) = p(f |u)q(u|λ),

(5)

where p(f |u) is the conditional prior given in Equation (3) and q(u|λ) is our approximate
(variational) posterior. This decomposition has proved eﬀective in regression problems with
a single latent process and a single output (see e.g. Titsias, 2009).

11

Hence, we can deﬁne our variational distribution using a mixture of Gaussians (mog):

q(u|λ) =

πkqk(u|mk, Sk) =

πk

N (u·j; mkj, Skj),

(6)

K
(cid:88)

k=1

K
(cid:88)

Q
(cid:89)

k=1

j=1

where λ = {πk, mkj, Skj} are the variational parameters: the mixture proportions {πk},
the posterior means {mkj} and posterior covariances {Skj} of the inducing variables cor-
responding to mixture component k and latent function j. We also note that each of the
mixture components qk(u|mk, Sk) is a Gaussian with mean mk and block-diagonal covari-
ance Sk.

An early reference for using a mixture of Gaussians (mog) within variational inference
is given by Bishop et al. (1998) in the context of Bayesian networks. Similarly, Gershman
et al. (2012) have used mog for non-gp models and, unlike our approach, used a second-order
Taylor series approximation of the variational lower bound.

4.4 Variational Lower Bound

It is easy to show that minimizing the Kullback-Leibler divergence between our approximate
posterior and the true posterior, KL(q(f , u|λ)(cid:107)p(f , u|y)) is equivalent to maximizing the
log-evidence lower bound (Lelbo), which is composed of a KL-term (Lkl) and an expected
log-likelihood term (Lell). In other words:

log p(y) ≥ Lelbo(λ) def= Lkl(λ) + Lell(λ), where

Lkl(λ) = −KL(q(f , u|λ)(cid:107)p(f , u)), and
Lell(λ) = Eq(f ,u|λ)[log p(y|f )],

(7)

(8)

(9)

where Eq(x)[g(x)] denotes the expectation of function g(x) over distribution q(x). Here
we note that Lkl is a negative KL divergence between the joint approximate posterior
q(f , u|λ) and the joint prior p(f , u). Therefore, maximization of the Lelbo in Equation (7)
entails minimization of an expected loss (given by the negative expected log-likelihood Lell)
regularized by Lkl, which imposes the constraint of ﬁnding solutions to our approximate
posterior that are close to the prior in the KL-sense.

An interesting observation of the decomposition of the Lelbo objective is that, unlike
Lell, Lkl in Equation (8) does not depend on the conditional likelihood p(y|f ), for which we
do not assume any speciﬁc parametric form (i.e. black-box likelihood). We can thus address
the technical diﬃculties regarding each component and their gradients separately using
diﬀerent approaches. In particular, for Lkl we will exploit the structure of the variational
posterior in order to avoid computing KL-divergences over distributions involving all the
data. Furthermore, we will obtain a lower bound for Lkl in the general case of q(u) being a
mixture-of-Gaussians (mog) as given in Equation (6). For the expected log-likelihood term
(Lell) we will use a Monte Carlo approach in order to estimate the required expectation and
its gradients.

12

4.5 Computation of the KL-divergence term (Lkl)

In order to have an explicit form for Lkl and its gradients, we start by expanding Equation
(8):

Lkl(λ) = −KL(q(f , u|λ)(cid:107)p(f , u)) = −Eq(f ,u|λ)
(cid:20)

(cid:21)

= −Ep(f |u)q(u|λ)

log

q(u|λ)
p(u)

,

= −KL(q(u|λ)(cid:107)p(u)),

(cid:20)

log

q(f , u|λ)
p(f , u)

(cid:21)

,

(10)

(11)

(12)

where we have applied the deﬁnition of the KL-divergence in Equation (10); used the
variational joint posterior q(f , u|λ) given in Equation (5) to go from Equation (10) to
Equation (11); and integrated out f to obtain Equation (12). We note that the deﬁnition of
the joint posterior q(f , u|λ) in Equation (5) has been crucial to transform a KL-divergence
between the joint approximate posterior and the joint prior into a KL-divergence between
the variational posterior q(u|λ) and the prior p(u) over the inducing variables. In doing that,
we have avoided computing a KL-divergence between distributions over the N -dimensional
variables f·j. As we shall see later, this implies a reduction in time complexity from O(N 3)
to O(M 3), where N is the number of datapoints and M is the number of inducing variables.
We now decompose the resulting KL-divergence term in Equation (12) as follows,

Lkl(λ) = −KL(q(u|λ)(cid:107)p(u)) = Lent(λ) + Lcross(λ), where:
Lent(λ) = −Eq(u|λ)[log q(u|λ)], and
Lcross(λ) = Eq(u|λ)[log p(u)],

(13)

where Lent(λ) denotes the diﬀerential entropy of the approximating distribution q(u|λ) and
Lcross(λ) denotes the negative cross-entropy between the approximating distribution q(u|λ)
and the prior p(u).

Computing the entropy of the variational distribution in Equation (6), which is a
mixture-of-Gaussians (mog), is analytically intractable. However, a lower bound of this
entropy can be obtained using Jensen’s inequality (see e.g. Huber et al., 2008) giving:

Lent(λ) ≥ −

πk log

π(cid:96)N (mk; m(cid:96), Sk + S(cid:96)) def= (cid:98)Lent.

(14)

K
(cid:88)

k=1

K
(cid:88)

(cid:96)=1

The negative cross-entropy in Equation (13) between a Gaussian mixture q(u|λ) and a
Gaussian p(u), can be obtained analytically,

Lcross(λ) = −

[M log 2π + log (cid:12)

(cid:12)Kj
zz

(cid:12)
(cid:12) + mT

kj(Kj

zz)−1mkj + tr (Kj

zz)−1Skj]. (15)

1
2

K
(cid:88)

Q
(cid:88)

πk

k=1

j=1

The derivation of the entropy term and the cross-entropy term in Equations (14) and (15)
is given in Appendix A.

13

4.6 Estimation of the expected log likelihood term (Lell)

We now address the computation of the expected log likelihood term in Equation (9). The
main diﬃculty of computing this term is that, unlike the Lkl where we have full knowledge
of the prior and the approximate posterior, here we do not assume a speciﬁc form for the
conditional likelihood p(y|f , φ). Furthermore, we only require evaluations of log p(yn|fn·, φ)
for each datapoint n, hence yielding a truly black-box likelihood method.

We will show one of our main results, that of statistical eﬃciency of our Lell estimator.
This means that, despite having a full Gaussian approximate posterior, estimation of the
Lell and its gradients only requires samples from univariate Gaussian distributions. We
start by expanding Equation (9) using our deﬁnitions of the approximate posterior and the
factorization of the conditional likelihood:

Lell(λ) = Eq(f ,u|λ)[log p(y|f , φ)] = Eq(f |λ)[log p(y|f , φ)],

(16)

where, given the deﬁnition of the approximate joint posterior q(f , u|λ) in Equation (5), the
distribution q(f |λ) resulting from marginalizing u from this joint posterior can be obtained
analytically,

q(f |λ) =

πkqk(f |λk) =

πk

N (f·j; bkj, Σkj), with

K
(cid:88)

K
(cid:88)

Q
(cid:89)

k=1

j=1

k=1
bkj = Ajmkj, and
Σkj = (cid:101)Kj + AjSkjAT
j ,

where (cid:101)Kj and Aj are given in Equation (4) and, as deﬁned before, {mkj} and {Skj} are
the posterior means and posterior covariances of the inducing variables corresponding to
mixture component k and latent function j. We are now ready to state our result of a
statistically eﬃcient estimator for the Lell:

Theorem 1 For the gp model with prior deﬁned in Equations (3) to (4), and conditional
likelihood deﬁned in Equation (2), the expected log likelihood over the variational distribution
in Equation (5) and its gradients can be estimated using samples from univariate Gaussian
distributions.

The proof is constructive and can be found in Appendix B. We note that a less general
result, for the case of one latent function and a single variational Gaussian posterior, was
obtained in Opper and Archambeau (2009) using a diﬀerent derivation. Here we state our
ﬁnal result on how to compute these estimates:

Lell(λ) =

πkEqk(n)(fn·|λk)[log p(yn|fn·, φ)],

N
(cid:88)

K
(cid:88)

n=1

k=1
N
(cid:88)

n=1

∇λk Lell(λ) = πk

Eqk(n)(fn·|λk)

(cid:2)∇λk log qk(n)(fn·|λk) log p(yn|fn·, φ)(cid:3), for λk ∈ {mk, Sk},

(17)

(18)

(19)

(20)

(21)

14

∇πk Lell(λ) =

Eqk(n)(fn·|λk)[log p(yn|fn·, φ)],

(22)

N
(cid:88)

n=1

where qk(n)(fn·|λk) is a Q-dimensional Gaussian with:

qk(n)(fn·|λk) = N (fn·; bk(n), Σk(n)),

where Σk(n) is a diagonal matrix. The jth element of the mean and the (j, j)th entry of
the covariance of the above distribution are given by:

[bk(n)]j = aT

jnmkj,

[Σk(n)]j,j = [ (cid:101)Kj]n,n + aT

jnSkjajn,

(23)

def= [Aj]:,n denotes the M -dimensional vector corresponding to the nth column of
where ajn
matrix Aj; (cid:101)Kj and Aj are given in Equation (4); and, as before, {mkj, Skj} are the varia-
tional parameters corresponding to the mean and covariance of the approximate posterior
over the inducing variables for mixture component k and latent process j.

We emphasize that when Q > 1, qk(n)(fn·|λk) is not a univariate marginal but a Q-
dimensional marginal posterior with diagonal covariance. Therefore, only samples from
univariate Gaussians are required to estimate the expressions in Equations (20) to (22).

4.6.1 Practical consequences and unbiased estimates

There are two immediate practical consequences of the result in Theorem 1. The ﬁrst
consequence is that we can use unbiased empirical estimates of the expected log likelihood
term and its gradients. In our experiments we use Monte Carlo (mc) estimates, hence we
can compute Lell as:

(cid:110)

f (k,i)
n·

(cid:111)S

i=1

∼ N (fn·; bk(n), Σk(n)), k = 1, . . . , K,

(cid:98)Lell =

1
S

N
(cid:88)

K
(cid:88)

S
(cid:88)

πk

n=1

k=1

i=1

log p(yn|f (k,i)

n·

, φ),

(24)

where bk(n) and Σk(n) are the vector and matrix forms of the mean and covariance of the
posterior over the latent functions as given in Equation (23). Analogous mc estimates of
the gradients are given in Appendix C.2.

The second practical consequence is that in order to estimate the gradients of the Lell,
using Equations (21) and (22), we only require evaluations of the conditional likelihood in a
black-box manner, without resorting to numerical approximations or analytical derivations
of its gradients.

5 Parameter Optimization

In order to learn the parameters of our model we seek to maximize our (estimated) log-
evidence lower bound ( (cid:98)Lelbo) using gradient-based optimization. Let η = {λ, θ, φ} be all
the model parameters for which point estimates are required. We have that:

(cid:98)Lelbo(η) def= (cid:98)Lent(η) + Lcross(η) + (cid:98)Lell(η),

15

∇η (cid:98)Lelbo = ∇η (cid:98)Lent + ∇ηLcross + ∇η (cid:98)Lell,

where we have made explicit the dependency of the log-evidence lower bound on any pa-
rameter of the model and (cid:98)Lent, Lcross, and (cid:98)Lell are given in Equations (14), (15), (24)
respectively. The gradients of (cid:98)Lelbo wrt variational parameters λ, covariance hyperparam-
eters θ and likelihood parameters φ are given in Appendices C, D.1, and D.2, respectively.
As shown in these appendices, not all constituents of (cid:98)Lelbo contribute to learning all pa-
rameters, for example ∇θ (cid:98)Lent = ∇φ (cid:98)Lent = ∇φLcross = 0.

Using the above objective function ( (cid:98)Lelbo) and its gradients we can consider batch opti-
mization with limited-memory bfgs for small datasets and medium-size datasets. However,
under this batch setting, the computational cost can be too large to be aﬀorded in practice
even for medium-size datasets on single-core architectures.

5.1 Stochastic optimization

To deal with the above problem, we ﬁrst note that the terms corresponding to the KL-
divergence (cid:98)Lent and Lcross in Equations (14) and (15) do not depend on the observed data,
hence their computational complexity is independent of N . More importantly, we note that
(cid:98)Lell in Equation (24) decomposes as a sum of expectations over individual datapoints. This
makes our inference framework amenable to parallel computation and stochastic optimiza-
tion (Robbins and Monro, 1951). More precisely, we can rewrite (cid:98)Lelbo as:

(cid:98)Lelbo =

(cid:98)Lent + Lcross

(cid:16)

N
(cid:88)

n=1

(cid:20) 1
N

(cid:17)

+ (cid:98)L(n)

ell

(cid:21)

,

(cid:80)S

(cid:80)K

k=1 πk

ell = 1
S

i=1 log p(yn|f (k,i)

where (cid:98)L(n)
, φ), which enables us to apply stochastic opti-
mization techniques such as stochastic gradients descend (sgd, Robbins and Monro, 1951)
or adadelta (Zeiler, 2012). The complexity of the resulting algorithm is independent of
N and dominated by algebraic operations that are O(M 3) in time, where M is the num-
ber of inducing points per latent process. This makes our automated variational inference
framework practical for very large datasets.

n·

5.2 Reducing the variance of the gradients with control variates

Theorem 1 is fundamental to having a statistical eﬃcient algorithm that only requires sam-
pling from univariate Gaussian distributions (instead of sampling from very high-dimensional
Gaussians) for the estimation of the expected log likelihood term and its gradients.

However, the variance of the gradient estimates may be too large for the algorithm to
work in practice, and variance reduction techniques become necessary. Here we use the well-
known technique of control variates (see e.g. Ross, 2006, §8.2), where a new gradient estimate
is constructed so as to have the same expectation but lower variance than the original
estimate. Our control variate is the so-called score function h(fn·) = ∇λk log qk(n)(fn·|λk)
and full details are given in Appendix F.

16

5.3 Optimization of the inducing inputs

So far we have discussed optimization of variational parameters (λ), i.e. the parameters of
the approximate posterior ({πk, mk, Sk}); covariance hyperparameters (θ); and likelihood
parameters (φ). However, as discussed by Titsias (2009), the inducing inputs {Zj} can
be seen as additional variational parameters and, therefore, their optimization should be
somehow robust to overﬁtting. As described in the Experiments (Section 9.5), learning of
the inducing inputs can improve performance, requiring a lower number of inducing variables
than when these are ﬁxed. This, of course, comes at an additional computational cost which
can be signiﬁcant when considering high-dimensional input spaces. As with the variational
parameters, we study learning of the inducing inputs via gradient-based optimization, for
which we use the gradients provided in Appendix E.

Early references in the machine learning community where a single variational objective
is used for parameter inference (in addition to posterior estimation over latent variables)
can be found in Hinton and van Camp (1993); MacKay (1995); Lappalainen and Miskin
(2000). These methods are now known under the umbrella term of variational Bayes2 and
consider a prior and an approximate posterior for these parameters within the variational
framework. As mentioned above, rather than a full variational Bayes approach, we provide
point estimates of {θ, φ} and {Zj}, and our experiments in §9 conﬁrm the eﬃcacy of our
approach. More speciﬁcally, we show in §9.5 that point-estimation of the inducing inputs
{Zj} using the variational objective can be signiﬁcantly better than using heuristics such
as k-means clustering.

6 Dense Posterior and Practical Distributions

In this section we consider the case when the inducing inputs are placed at the training
points, i.e. Zj = X and consequently M = N . As mentioned in Section 4.1, we refer to
this setting as dense to distinguish it from the case when M < N , for which the resulting
models are usually called sparse approximations. It is important to realize that not all real
datasets are very large and that in many cases the resulting time and memory complexity
O(N 3) and O(N 2) can be aﬀorded. Besides the dense posterior case, we also study some
particular variational distributions that make our framework more practical, especially in
large-scale applications.

6.1 Dense Approximate Posterior

When our posterior is dense the only approximation made is the assumed variational dis-
tribution in Equation (6). We will show that, in this case, we recover the objective function
in Nguyen and Bonilla (2014a) and that hyper-parameter learning is easier as the terms
in the resulting objective function that depend on the hyperparameters do not involve mc
estimates. Therefore, their analytical gradients can be used. Furthermore, in the following

2We note that these methods were then referred to as “Ensemble Learning”, since a full distribution (i.e. an
ensemble of parameters) was used instead of a single point estimate. Fortunately, nowadays, variational
Bayes is a preferred term since Ensemble Learning is more commonly associated with methods for combining
multiple models.

17

section, we will provide an eﬃcient exact parametrization of the posterior covariance that
reduces the O(N 2) memory complexity to a linear complexity O(N ).

zz = Kj

We start by looking at the components of Lelbo when we make Zj = X, which we
can simply obtain by making Kj
xx and M = N , and realizing that the posterior
parameters {mkj, Skj} are now N -dimensional objects. Therefore, we leave the entropy
term (cid:98)Lent in Equation (14) unchanged and we replace all the appearances of Kj
zz with Kj
xx
and all the appearances of M with N for the Lcross. We refer the reader to Appendix G for
details of the equations but it is easy to see that the resulting (cid:98)Lent and Lcross are identical
to those obtained by Nguyen and Bonilla (2014a, Equations 5 and 6).

For the expected log likelihood term, the generic expression in Equation (16) still applies
but we need to ﬁgure out the resulting expressions for the approximate posterior parame-
ters in Equations (18) and (19). It is easy to show that the resulting posterior means and
covariances are in fact bkj = mkj and Σkj = Skj (see Appendix G for details). This means
that in the dense case we simply estimate the (cid:98)Lell by using empirical expectations over the
unconstrained variational posterior q(f |λ), with ‘free’ mean and covariance parameters. In
contrast, in the sparse case, although these expectations are still computed over q(f |λ), the
parameters of the variational posterior q(f |λ) are constrained by Equations (18) and (19)
which are functions of the prior covariance and the parameters of the variational distribution
over the inducing variables q(u|λ). As we shall see in the following section, this distinc-
tion between the dense case and sparse case has critical consequences on hyperparameter
learning.

6.1.1 Exact Hyperparameter Optimization

The above insight reveals a remarkable property of the model in the dense case. Unlike
the sparse case, the expected log likelihood term does not depend on the covariance hyper-
parameters, as the expectation of the conditional likelihood is taken over the variational
distribution q(f |λ) with ‘free’ parameters. Therefore, only the cross-entropy term Lcross
depends on the hyperparameters (as we also know that ∇θ (cid:98)Lent = 0). For this term, as seen
in Appendix G.1 and corresponding gradients in Equation (37), we have derived the exact
(analytical) expressions for the objective function and its gradients, avoiding empirical mc
estimates altogether. This has a signiﬁcant practical implication: despite using black-box
inference, the hyperparameters are optimized wrt the true evidence lower bound (given
ﬁxed variational parameters). This is an additional and crucial advantage of our automated
inference method over other generic inference techniques (see e.g. Ranganath et al., 2014),
which do not exploit knowledge of the prior.

6.1.2 Exact Solution with Gaussian Likelihoods

Another interesting property of our approach arises from the fact that, as we are using mc
estimates, (cid:98)Lell is an unbiased estimator of Lell. This means that, as the number of samples
S increases, (cid:98)Lelbo will converge to the true value Lelbo. In the case of a Gaussian likelihood,
the posterior over the latent functions is also Gaussian and a variational approach with a
full Gaussian posterior will converge to the true parameters of the posterior, see e.g. Opper
and Archambeau (2009) and Appendix G.2 for details.

18

6.2 Practical Variational Distributions

As we have seen in Section 5, learning of all parameters in the model can be done in a
scalable way through stochastic optimization for general likelihood models, providing au-
tomated variational inference for models with Gaussian process priors. However, the gen-
eral mog approximate posterior in Equation (6) requires O(M 2) variational parameters for
each covariance matrix of the corresponding latent process, yielding a total requirement of
O(QKM 2) parameters. This may cause diﬃculties for learning when these parameters are
optimized simultaneously. In this section we introduce two special members of the assumed
variational posterior family that improve the practical tractability of our inference frame-
work. These members are a full Gaussian posterior and a mixture of diagonal Gaussians
posterior.

6.2.1 Full Gaussian Posterior

This instance considers the case of only one component in the mixture (K = 1) in Equation
(6),which has a Gaussian distribution with a full covariance matrix for each latent process.
Therefore, following the factorization assumption in the posterior across latent processes,
the posterior distribution over the inducing variables u, and consequently over the latent
functions f , is a Gaussian with block diagonal covariance, where each block is a full covari-
ance corresponding to that of a single latent function. We thus refer to this approximate
posterior as the full Gaussian posterior (fg).

6.2.2 Mixture of Diagonal Gaussians Posterior

Our second practical variational posterior considers a mixture distribution as in Equation
(6), constraining each of the mixture components for each latent process to be a Gaus-
sian distribution with diagonal covariance matrix. Therefore, following the factorization
assumption in the posterior across latent processes, the posterior distribution over the in-
ducing variables u is a mixture of diagonal Gaussians. However, we note that, as seen
in Equations (18) and (19), the posterior over the latent functions f is not a mixture of
diagonal Gaussians in the general sparse case. Obviously, in the dense case (where Zj = X)
the posterior covariance over f of each component does have a diagonal structure. Hence-
forth, we will refer to this approximation simply as mog, to distinguish it from the fg case
above, while avoiding the use of additional notation. One immediate beneﬁt of using this
approximate posterior is computational, as we avoid the inversion of a full covariance for
each component in the mixture.

As we shall see in the following sections, there are additional beneﬁts from the assumed
practical distributions and they concern the eﬃcient parametrization of the covariance for
both distributions and the lower variance of the gradient estimates for the mog posterior.

6.3 Eﬃcient Re-parametrization

As mentioned above, one of the main motivations for having speciﬁc practical distribu-
tions is to reduce the computational overhead due to the large number of parameters to
optimize. For the mog approximation, it is obvious that only O(M ) parameters for each

19

latent process and mixture component are required to represent the posterior covariance,
hence one obtains an eﬃcient parametrization by deﬁnition. However, for the full Gaussian
(fg) approximation, naively, one would require O(M 2) parameters. The following theorem
states that for settings that require a large number of inducing variables, the fg variational
distribution can be represented using a signiﬁcantly lower number of parameters.

Theorem 2 The optimal full Gaussian variational posterior can be represented using a
parametrization that is linear in the number of observations (N ).

Before proceeding with the analysis of this theorem, we remind the reader that the general
form of our variational distribution in Equation (6) requires O(KQM 2) parameters for the
covariance, for a model with K mixture components, Q latent processes and M inducing
variables. Nevertheless, for simplicity and because usually K (cid:28) M and Q (cid:28) M , we will
omit K and Q in in the following discussion.

The proof the theorem can be found in Appendix H, where it is shown that in the fg

case the optimal solution for the posterior covariance is given by:

(cid:99)Sj = Kj
zz

(cid:0)Kj

zz + Kj

zxΛjKj
xz

(cid:1)−1

Kj

zz,

(25)

where Λj is a N -dimensional diagonal matrix. Since the optimal covariance can be expressed
in terms of ﬁxed kernel computations and a free set of parameters given by Λj, only N
parameters are necessary to represent the posterior covariance. As we shall see below, this
parametrization becomes useful for denser models, i.e. for models that have a large number
of inducing variables.

6.3.1 Sparse Posterior

In the sparse case the inducing inputs Zj are at arbitrary locations and, more importantly,
M (cid:28) N , the number of inducing variables is considerably smaller than the number of
training points. The result in Equation (25) implies that if we parameterize Sj in that way,
we will need O(N ) parameters instead of O(M 2). Of course this is useful when roughly
N < M 2
2 . A natural question arises when we deﬁne the number of inducing points as a
fraction of the number of training points, i.e. M = (cid:15)N with 0 ≤ (cid:15) ≤ 1, when is such a
parameterization useful? In this case, using the alternative parametrization will become
N . To illustrate this, consider for example the mnist dataset used
beneﬁcial when (cid:15) >
in our medium-scale experiments in Section 9.4 where N = 60, 000. This yields a beneﬁcial
regime around roughly (cid:15) > 0.006, which is a realistic setting. In fact, in our experiments on
this dataset we did consider sparsity factors of this magnitude. For example, our biggest
experiment used (cid:15) = 0.04. With a naive parametrization of the posterior covariance, we
would need roughly 2 × 106 parameters. In contrast, by using the eﬃcient parametrization
we only need 50 × 103 parameters. As shown below, these gains are greater as the model
becomes denser, yielding a dramatic reduction in the number of parameters when having a
fully dense Gaussian posterior.

(cid:113) 2

20

6.3.2 Dense Posterior

In the dense case we have that Zj = X, ∀j = 1, . . . , Q and consequently M = N . Therefore
we have that the optimal posterior covariance is given by:

(cid:99)Sj = (cid:0)(Kj

xx)−1 + Λj

(cid:1)−1

.

In principle, the parametrization of the posterior covariance would require O(N 2) parame-
ters for each latent process. However, the above result shows that we can parametrize these
covariances eﬃciently using only O(N ) parameters.

We note that, for models with Q = 1, this eﬃcient re-parametrization has been used by
Sheth et al. (2015) in the sparse case and Opper and Archambeau (2009) in the dense case,
while adopting an inference algorithm diﬀerent to ours.

6.4 Automatic Variance Reduction with a Mixture-of-Diagonals Poste-

rior

An additional beneﬁt of having a mixture-of-diagonals (mog) posterior in the dense case is
that optimization of the variational parameters will typically converge faster when using a
mixture of diagonal Gaussians. This is an immediate consequence of the following theorem.

Theorem 3 When having a dense posterior, the estimator of the gradients wrt the vari-
ational parameters using the mixture of diagonal Gaussians has a lower variance than the
full Gaussian posterior’s.

The proof is in Appendix I and is simply a manifestation of the Rao-Blackwellization tech-
nique (Casella and Robert, 1996). The theorem is only made possible due to the analytical
tractability of the KL-divergence term (Lkl) in the variational objective (Lelbo). The practi-
cal consequence of this theorem is that optimization will typically converge faster when using
a mixture-of-diagonals Gaussians than when using a full Gaussian posterior approximation.

7 Predictions

Given the general posterior over the inducing variables q(u|λ) in equation (6), the predictive
distribution for a new test point x(cid:63) is given by:

p(y(cid:63)|x(cid:63)) =

πk

p(y(cid:63)|f(cid:63)·)

p(f(cid:63)·|u)qk(u|λk)dudf(cid:63)·.

(cid:90)

=

πk

p(y(cid:63)|f(cid:63)·)qk(f(cid:63)·|λk)df(cid:63)·,

(26)

where qk(f(cid:63)·|λk) is the predictive distribution over the Q latent functions corresponding to
mixture component k given the learned variational parameters λ = {mk, Sk}:

(cid:90)

(cid:90)

K
(cid:88)

k=1
K
(cid:88)

k=1

qk(f(cid:63)·|λk) =

N (f(cid:63)j; µ(cid:63)

kj, σ(cid:63)2

kj ), with

Q
(cid:89)

j=1

21

µ(cid:63)
zz)−1mkj, and
kj = κj(x(cid:63), Zj)(Kj
kj = κj(x(cid:63), x(cid:63)) − κj(x(cid:63), Zj) (cid:0)(Kj
σ(cid:63)2

zz)−1 − (Kj

zz)−1Skj(Kj

zz)−1(cid:1) κj(Zj, x(cid:63)).

Thus, the probability of the test points taking values y(cid:63) (e.g. in classiﬁcation) in Equation
(26) can be readily estimated via Monte Carlo sampling.

8 Complexity analysis

Throughout this paper we have only considered computational complexity with respect to
the number of inducing points and training points for simplicity. Although this has also
been common practice in previous work (see e.g. Dezfouli and Bonilla, 2015; Nguyen and
Bonilla, 2014a; Hensman et al., 2013; Titsias, 2009), we believe it is necessary to provide
an in-depth complexity analysis of the computational cost of our model. Here we analyze
the computational cost of evaluating the Lelbo and its gradients once.

We begin by reminding the reader of the dimensionality notation used so far and by
introducing additional notation speciﬁc to this section. We analyze the more general case of
stochastic optimization using mini-batches. Let K be the number of mixture components
in our variational posterior; Q the number of latent functions; D the dimensionality of the
input data; S the number of samples used to estimate the required expectations via Monte
Carlo; B the number of inputs used per mini-batch; M the number of inducing inputs; and
N the number of observations. We note that in the case of batch optimization B = N , hence
our analysis applies to both the stochastic and batch setting. We also let T (e) represent
the computational costs of expression e. Furthermore, we assume that the kernel function
is simple enough such that evaluating its output between two points is O(D) and we denote
with T (log p(yn|fn·, φ)) ∈ O(L) the cost of a single evaluation of the conditional likelihood.

8.1 Overall Complexity

While the detailed derivations of the computational complexity are in Appendix J, here we
state the main results. Assuming that K (cid:28) M , the total computational cost is given by:

T (Lelbo) ∈ O(Q(M 2D + BM D + K2M 3 + KBM 2 + KBSL)),
and for diagonal posterior covariances we have:

T (Lelbo) ∈ O(Q(M 2D + BM D + M 3 + KBM + KBSL)).

We note that it takes (cid:100)N/B(cid:101) gradient updates to go over an entire pass of the data. If we
assume that K is a small constant, the asymptotic complexity of a single pass over the data
does not improve by increasing B beyond B = M . Hence if we assume that B ∈ O(M ) the
computational cost of a single gradient update is given by

T (Lelbo) ∈ O(QM (M D + M 2 + SL)),

for both diagonal and full covariances. If we assume that it takes a constant amount of time
to compute the likelihood function between a sample and an output, we see that setting
S ∈ O(M D + M 2) does not increase the computational complexity of our algorithm. As we
shall see in section 9.4, even a complex likelihood function only requires 10, 000 samples to

22

approximate it. Since we expect to have more than 100 inducing points in most large scale
settings, it is reasonable to assume that the overhead from generated samples will not be
signiﬁcant in most cases.

9 Experiments

In this section we analyze the behavior of our model on a wide range of experiments involving
small-scale to large-scale datasets. The main aim of the experiments is to evaluate the
performance of the model when considering diﬀerent likelihoods and diﬀerent dataset sizes.
We analyze how our algorithm’s performance is aﬀected by the density and location of the
inducing variables, and how the performance of batch and stochastic optimization compare.
We start by evaluating our algorithm using ﬁve small-scale datasets (N < 1, 300) under
diﬀerent likelihood models and number of inducing variables (§9.3). Then, using medium-
scale experiments (N < 70, 000), we compare stochastic and batch optimization settings,
and determine the eﬀect of learning the inducing inputs on the performance (§9.4). Sub-
sequently, we use savigp on two large-scale datasets. The ﬁrst one involves the prediction
of airline delays, where we compare the convergence properties of our algorithm to models
that leverage full knowledge of the likelihood (§9.7.1). The second large dataset considers
an augmented version of the popular mnist dataset for handwritten digit recognition, in-
volving more than 8 million observations (§9.7.3). Finally, we showcase our algorithm on a
non-standard inference problem concerning a seismic inversion task, where we show that our
variational inference algorithm can yield solutions that closely match (non-scalable) sam-
pling approaches (§9.8). Before proceeding with the experimental set-up, we give details of
our implementation which uses gpus.

9.1 Implementation

We have implemented our savigp method in Python and all the code is publicly available
at https://github.com/Karl-Krauth/Sparse-GP. Most current mainstream implemen-
tations of Gaussian process models do not support gpu computation, and instead opt to
oﬄoad most of the work to the cpu, with the notable exception of Matthews et al. (2017).
For example, neither of the popular packages gpml3 or gpy4 provide support for gpus.
This is despite the fact that matrix manipulation operations, which are easily paralleliz-
able, are at the core of any Gaussian process model. In fact, the rate of progress between
subsequent gpu models has been much larger than for cpus, thus ensuring that any gpu
implementation would run at an accelerated rate as faster hardware gets released.

With these advantages in mind, we provide an implementation of savigp that uses
Theano (Al-Rfou et al., 2016), a library that allows users to deﬁne symbolic mathematical
expressions that get compiled to highly optimized gpu cuda code. Any operation that
involved the manipulation of large matrices or vectors was done in Theano. Most of our
experiments were either run on g2.2 aws instances, or on a desktop machine with an Intel
core i5-4460 cpu, 8GB of ram, and a gtx760 gpu.

3Available at http://www.gaussianprocess.org/gpml/code/matlab/doc/.
4Available at https://github.com/SheffieldML/GPy.

23

Despite using a low-end outdated gpu, we found a time speed-up of 5x on average when
we oﬄoaded work to the gpu. For example, in the case of the mnist-b dataset (used in
section 9.4), we averaged the time it took to compute ten gradient evaluations of the Lelbo
with respect to the posterior parameters over the entire training set, where we expressed
the posterior as a full Gaussian and used a sparsity factor of 0.04. While it took 42.35
seconds, on average, per gradient computation when making use of the cpu only, it took
a mere 8.52 seconds when work was oﬄoaded to the gpu. We expect the diﬀerence to be
even greater given a high-end current-generation gpu.

9.2 Details of the Experiments

Performance measures. We evaluated the performance of the algorithm using non-
probabilistic and probabilistic measures according to the type of learning problem we are
addressing. The standardized squared error (sse) and the negative log predictive density
(nlpd) were used in the case of continuous-output problems. The error rate (er) and
the negative log probability (nlp) were used in the case of discrete-output problems. In
the experiments using the airline dataset, we used the root mean squared error (rmse)
instead of the sse to be able to compare our method with previous work that used rmse
for performance evaluation.

In small-scale experiments, inducing inputs were placed on a
Experimental settings.
subset of the training data in a nested fashion, so that experiments on less sparse models
contained the inducing points of the sparser models. In medium-scale and large-scale ex-
periments the location of the inducing points was initialized using the k-means clustering
method. In all experiments the squared exponential covariance function was used.

Optimization methods. Learning the model involves optimizing variational parame-
ters, hyperparameters, likelihood parameters, and inducing inputs. These were optimized
iteratively in a global loop. In every iteration, each set of parameters was optimized sepa-
rately while keeping the other sets of parameters ﬁxed, and this process was continued until
the change in the objective function between two successive iterations was less than 10−6.
For optimization in the batch settings, each set of parameters was optimized using l-bfgs,
with the maximum number of global iterations limited to 200. In the case of stochastic
optimization, we used the adadelta method (Zeiler, 2012) with parameters (cid:15) = 10−6 and
a decay rate of 0.95. The choice of this optimization algorithm was motivated by (and to
be consistent with) the work of Hensman et al. (2013), who found this speciﬁc algorithm
successful in the context of Gaussian process regression. We compare with the method of
Hensman et al. (2013) in this context in §9.7.

Reading the graphs. Results are presented using boxplots and bar/line charts. In the
case of boxplots, the lower, middle, and upper hinges correspond to the 25th, 50th, and
75th percentile of the data. The upper whisker extends to the highest value that is within
1.5×IQR of the top of the box, and the lower whisker extends to the lowest value within
1.5×IQR of the bottom of the box (IQR is the distance between the ﬁrst and third quantile).

24

Table 1: Details of the datasets used in the small-scale experiments and their corresponding
likelihood models. Ntrain, Ntest, D are the number of training points, test points and input
dimensions respectively; ‘likelihood’ is the conditional likelihood used on each problem; and
‘model’ is the name of the model associated with that likelihood. For mining Ntest = 0 as
only posterior inference is done (predictions on a test set are not made for this problem).

Dataset Ntrain Ntest D
mining 811
boston 300
creep 800
abalone 1000
cancer 300
usps
1233

0
206
1266
3177
383
1232

Likelihood p(y|f )

Model

λy exp(−λ)/y!

Log Gaussian Cox process
Standard regression

1
13 N (y; f, σ2)
30 ∇yt(y)N (t(y); f, σ2) Warped Gaussian processes
∇yt(y)N (t(y); f, σ2) Warped Gaussian processes
8
1/(1 + exp(−f ))
9
exp(fc)/ (cid:80)
256

i=1 exp(fi) Multi-class classiﬁcation

Binary classiﬁcation

In the case of bar charts, the error bars represent 95% conﬁdence intervals computed over
multiple partitions of the dataset.

Model conﬁgurations. We refer to the ratio of inducing points to training points as the
sparsity factor (SF = M/N ). For each sparsity factor, three diﬀerent variations of savigp
corresponding to (i) a full Gaussian posterior, (ii) a diagonal Gaussian posterior, and (iii)
a mixture of two diagonal Gaussian posteriors were tested, which are denoted respectively
by fg, mog1, and mog2 in the graphs.

9.3 Small-scale Experiments

We tested savigp on six small-scale datasets with diﬀerent likelihood models. The datasets
are summarized in Table 1, and are the same as those used by Nguyen and Bonilla (2014a).
For each dataset, the model was tested ﬁve times across diﬀerent subsets of the data; except
for the mining dataset where only training data were used for evaluation of the posterior
distribution.

9.3.1 Standard regression

The model was evaluated on the boston dataset (Bache and Lichman, 2013), which involves
a standard regression problem with a univariate Gaussian likelihood model, i.e., p(yn|fn) =
N (yn|fn, σ2). Figure 1 shows the performance of savigp for diﬀerent sparsity factors, as
well as the performance of exact Gaussian process inference (gp). As we can see, sse
increases slightly on sparser models. However, the sse of all the models (fg, mog1, mog2)
across all sparsity factors are comparable to the performance of exact inference (gp). In
terms of nlpd, as expected, the dense (SF = 1) fg model performs exactly like the exact
inference method (gp). In the sparse models, nlpd shows less variation in lower sparsity
factors (especially for mog1 and mog2), which can be attributed to the tendency of such
models to make less conﬁdent predictions under high sparsity settings.

25

Figure 1: The distributions of sse and nlpd for a regression problem with a univariate
Gaussian likelihood model on the boston housing dataset. Three approximate posteriors
in savigp are used: fg (full Gaussian), mog1 (diagonal Gaussian), and mog2 (mixture of
two diagonal Gaussians), along with various sparsity factors (SF = M/N ). The smaller the
sf the sparser the model, with SF = 1 corresponding to the dense model. gp corresponds
to the performance of exact inference using standard Gaussian process regression.

9.3.2 Warped Gaussian process

In warped Gaussian processes, the likelihood function is p(yn|fn) = ∇yt(yn)N (t(yn); fn, σ2),
for some transformation t. We used the same neural-net style transformation as Snelson
et al. (2003), and evaluated the performance of our model on two datasets: creep (Cole
et al., 2000), and abalone (Bache and Lichman, 2013). The results are compared with the
performance of exact inference for warped Gaussian processes (wgp) described by Snelson
et al. (2003), and also with the performance of exact Gaussian process inference with a
univariate Gaussian likelihood model (gp). As shown in Figure 2, in the case of the abalone
dataset, the performance is similar across all the models (fg, mog1 and mog2) and sparsity
factors, and is comparable to the performance of the exact inference method for warped
Gaussian processes (wgp). The results on creep are given in Appendix K.1.

9.3.3 Binary classiﬁcation

For binary classiﬁcation we used the logistic likelihood p(yn = 1|fn) = 1/(1 + e−fn) on the
breast cancer dataset (Bache and Lichman, 2013) and compared our model against the
expectation propagation (ep) and variational bounds (vbo) methods described by Nickisch

26

Figure 2: The distributions of sse and nlpd for a warped Gaussian process likelihood
model on the abalone dataset. Three approximate posteriors in savigp are used: fg
(full Gaussian), mog1 (diagonal Gaussian), and mog2 (mixture of two diagonal Gaussians),
along with various sparsity factors (SF = M/N ). The smaller the sf the sparser the model,
with SF = 1 corresponding to the non-sparse model. wgp corresponds to the performance
of the exact inference method for warped Gaussian process models (Snelson et al., 2003),
and gp is the performance of exact inference on a univariate Gaussian likelihood model.

and Rasmussen (2008). Results depicted in Figure 3 indicate that the error rate remains
comparable across all models (fg, mog1, mog2) and sparsity factors, and is almost identical
to the error rates obtained by inference using ep and vbo. Interestingly, the nlp shows more
variation and generally degrades as the number of inducing points is increased, especially
for mog1 and mog2 models, which can be attributed to the fact that these denser models
are overly conﬁdent in their predictions.

9.3.4 Multi-class classiﬁcation
For multi-class classiﬁcation, we used the softmax likelihood p(yn = c) = e−fc/ (cid:80)
i e−fi,
and trained the model to classify the digits 4, 7, and 9 from the usps dataset (Rasmussen
and Williams, 2006). We compared our model against a variational inference method (vq)
which represents the elbo using a quadratic lower bound on the likelihood terms (Khan
et al., 2012). As we see in Figure 4, the error rates are slightly lower in denser fg models.
We also note that all versions of savigp achieve comparable error rates to vq’s. Similarly
to the binary classiﬁcation case, nlp shows higher variation with higher sparsity factor,
especially in mog1 and mog2 models.

27

Figure 3: Error rates and nlp for binary classiﬁcation using a logistic likelihood model on
the Wisconsin breast cancer dataset. Three approximate posteriors are used: fg (full
Gaussian), mog1 (diagonal Gaussian), and mog2 (mixture of two diagonal Gaussians),
along with various sparsity factors (SF = M/N ). The smaller the sf the sparser the model,
with SF = 1 corresponding to the original model without sparsity. The performance of
inference using expectation propagation and variational bounds are denoted by ep and vbo
respectively.

9.3.5 Log Gaussian Cox Process

The log Gaussian Cox process (lgcp) is an inhomogeneous Poisson process in which the
log-intensity function is a shifted draw from a Gaussian process. Following Murray et al.
(2010), we used the likelihood p(yn|fn) = λyn
, where λn = exp fn + m is the mean
of a Poisson distribution and m is the oﬀset of the log mean. We applied savigp with
the lgcp likelihood on a coal-mining disaster dataset (Jarrett, 1979), which can be seen in
Figure 5 (top).

n exp (−λn)
yn!

As baseline comparisons, we use hybrid Monte Carlo (hmc) and elliptical slice sampling
(ess) described by Duane et al. (1987) and Murray et al. (2010) respectively. We collected
every 100th sample for a total of 10k samples after a burn-in period of 5k samples and used
the Gelman-Rubin potential scale reduction factors (Gelman and Rubin, 1992) to check for
convergence.

The bottom plot of Figure 5 shows the mean and variance of the predictions made by
savigp, hmc and ess. We see that the fg model provides similar results across all sparsity
factors, which is also comparable to the results provided by hmc and ess. mog1 and mog2
models provide the same mean as the fg models, but tend to underestimate the posterior

28

Figure 4: Classiﬁcation error rates and nlp for the multi-class classiﬁcation problem using a
softmax likelihood model on the usps dataset. Three approximate posteriors in savigp are
used: fg (full Gaussian), mog1 (diagonal Gaussian), and mog2 (mixture of two diagonal
Gaussians), along with various sparsity factors (SF = M/N ). The smaller the sf the
sparser the model, with SF = 1 corresponding to the original model without sparsity. vq
corresponds to a variational inference method, which represents the elbo as a quadratic
lower bound to the likelihood terms.

variance. This under-estimation of posterior variance is well known for variational methods,
especially under factorized posteriors. This results are more signiﬁcant when comparing the
running times across models. When using a slower Matlab implementation of our model,
for a fair comparison across all methods, savigp was at least two orders of magnitude faster
than hmc and one order of magnitude faster than ess.

9.4 Medium-scale Experiments

In this section we investigate the performance of savigp on four medium-scale problems on
the datasets summarized in Table 2. Our goal here is to evaluate the model on medium-
scale datasets, to study the eﬀect of optimizing the inducing inputs, and to compare the
performance of batch and stochastic optimization of the parameters. The ﬁrst problem that
we consider is multi-class classiﬁcation of handwriting digits on the mnist dataset using a
softmax likelihood model. The second problem uses the same dataset, but the task involves
binary classiﬁcation of odd and even digits using the logistic likelihood model. Therefore
we refer to this dataset as the mnist binary dataset (mnist-b). The third problem uses
the sarcos dataset (Vijayakumar and Schaal, 2000) and concerns an inverse-dynamics

29

Figure 5: Top: the coal-mining disasters data. Bottom: the posteriors for a Log Gaussian
Cox process on the data when using a fg (full Gaussian), mog1 (diagonal Gaussian), and
mog2 (mixture of two diagonal Gaussians), along with various sparsity factors (SF = M/N ).
The smaller the sf the sparser the model, with SF = 1 corresponding to non-sparse model.
The solid line is the posterior mean and the shaded area represents 95% conﬁdence intervals.
hmc and ess correspond to hybrid Monte Carlo and elliptical slice sampling inference
methods and are represented by orange dots and dashed green lines respectively.

problem for a seven degrees-of-freedom sarcos anthropomorphic robot arm. The task
is to map from a 21-dimensional input space (7 joint positions, 7 joint velocities, 7 joint
accelerations) to the corresponding 7 joint torques. For this multi-output regression problem
we use the Gaussian process regression network (gprn) likelihood model of Wilson et al.
(2012), which allows for nonlinear models where the correlation between the outputs can
be spatially adaptive. This is achieved by taking a linear combination of latent Gaussian
processes, where the weights are also drawn from Gaussian processes. Finally, sarcos-2

30

Table 2: Datasets used on the medium-scale experiments. gprn stands for Gaussian process
regression networks; Ntrain, Ntest, D are the number of training points, test points and input
dimensions respectively; Q is the number of latent processes; P is the dimensionality of the
output data; and ‘model’ is the model associated with the conditional likelihood used.

Dataset

Ntrain Ntest

D

Q P Model

mnist-b
60,000
mnist
60,000
sarcos-2 44,484
sarcos
44,484

10,000
10,000
4,449
4,449

784
784
21
21

1
10
3
8

Binary classiﬁcation
1
10 Multi-class classiﬁcation
2
7

gprn (Wilson et al., 2012)
gprn (Wilson et al., 2012)

is the same as sarcos, but the model is learned using only data from joints 4 and 7. For
both sarcos and sarcos-2, the mean of the sse and nlpd across joints 4 and 7 are used
for performance evaluation. We only consider joints 4 and 7 for sarcos, despite the fact
that predictions are made across all 7 joints to provide a direct comparison with sarcos-2
and with previous literature (Nguyen and Bonilla, 2014b). We also made use of automatic
relevance determination (ard) for both the sarcos and sarcos-2 datasets.

9.4.1 Batch optimization

Classiﬁcation. Here we evaluate the performance of batch optimization of model param-
eters on multi-class classiﬁcation using mnist and refer the reader to Appendix K.2 for the
results on binary classiﬁcation using mnist-b. We optimized the kernel hyperparameters
and the variational parameters, but ﬁxed the inducing point locations using k-means clus-
tering. Unlike most previous approaches (Ranzato et al., 2006; Jarrett et al., 2009), we did
not tune model parameters using the validation set. Instead, we considered the validation
set to be part of the training set and used our variational framework to learn all model
parameters. As such, the current setting likely provides a lower performance on test accu-
racy compared to the approaches that use a validation dataset, however our goal is simply
to show that we were able to achieve competitive performance in a sparse setting when no
knowledge of the likelihood model is used.

Figure 6 shows the result on the mnist dataset. we see that the performance improves
with denser models. Overall, savigp achieves an error rate of 2.77% at SF = 0.04. This
is a signiﬁcant improvement over the results reported by Gal et al. (2014), in which a
separate model was trained for each digit, which achieved an error rate of 5.95%. As a
reference, previous literature reports about 12% error rate by linear classiﬁers and less
than 1% error rate by state-of-the-art deep convolutional nets. Our results show that our
method reduces the gap between gps and deep nets while solving the harder problem of full
posterior estimation. In Appendix K.2 we show that our model can achieve slightly better
performance than that reported by Hensman et al. (2015) on mnist-b.

Gaussian process regression networks. Figure 7 shows that sarcos-2 gets a signif-
icant beneﬁt from a higher number of inducing points, which is consistent with previous

31

Figure 6: Error rates and nlp for multi-class classiﬁcation on the mnist dataset. We used
a full Gaussian (fg) posterior approximation across various sparsity factors (SF = M/N ).
The smaller the SF the sparser the model.

Figure 7: Mean sse and nlpd for multi-output regression on the sarcos-2 dataset. We used
a full Gaussian (fg) posterior approximation across various sparsity factors (SF = M/N ).
The smaller the SF the sparser the model.

work that found that the performance on this dataset improves as more data is being used
to train the model (Vijayakumar and Schaal, 2000). The performance is signiﬁcantly better
than the results reported by Nguyen and Bonilla (2014b), who achieved a mean standard-
ized squared eror (msse) of 0.2631 and 0.0127 across joints 4 and 7, against our values of
0.0033 and 0.0065 for SF = 0.04. However, we note that their setting was much sparser
than ours on joint 4. The results on sarcos (predicting on all joints) are given in Appendix
K.3.

9.5 Inducing-input learning

We now compare the eﬀect of adaptively learning the inducing inputs, versus initializing
them using k-means and leaving them ﬁxed. We look at the performance of our model
under two settings: (i) a low number of inducing variables (SF= 0.001, 0.004) where the
inducing inputs are learned, and (ii) a large number of inducing variables (SF = 0.02, 0.04)
without learning of their locations.

Figure 8 shows the performance of the model under the two settings on mnist-b. We
see that learning the location of the inducing variables yields a large gain in performance.

32

Figure 8: Comparison of error rate and nlp obtained by savigp without learning (fg) and
with learning (ind) of inducing inputs for binary classiﬁcation on the mnist-b dataset.

Table 3: Error rate (er) and negative log probability (nlp) obtained with savigp optimized
using batch optimization (with l-bfgs) and stochastic optimization (with adadelta) on
the mnist-b dataset. The inducing inputs are optimized in both cases.

Method Batch

SF = 0.001

Batch
SF = 0.004

Stochastic
SF = 0.001

Stochastic
SF = 0.004

er
nlp

3.17%
0.097

2.12%
0.068

3.11%
0.099

2.68%
0.083

In fact, the sparser models with inducing point learning performed similarly to the denser
models, despite the fact that the two models diﬀered by an order of magnitude when it came
to the number of inducing variables. Additional results on mnist, sarcos and sarcos-2
are shown in Appendix K.4. Our analyses indicate that there is a trade-oﬀ between the
reduction in computational complexity gained by reducing the number of inducing variables
and the increased computational cost of calculating inducing-input gradients. As such, the
advantage of learning the inducing inputs is dataset dependent and it is aﬀected mainly by
the input dimensionality (D).

9.6 Batch Optimization vs Stochastic Optimization

In this section we compare the performance of our model after it has been trained in a
batch setting versus a stochastic setting. We used adadelta as our stochastic optimization
algorithm as it requires less hand-tuning than other algorithms such as vanilla stochastic
gradient descend (sgd).

Table 3 shows only a slight deterioration in predictive performance on mnist-b when
using stochastic optimization instead of batch optimization. In fact, our exploratory exper-
iments showed that the error metrics could have signiﬁcantly been reduced by meticulously
hand-tuning momentum stochastic gradient descent (sgd). However, our goal was simply
to show that our model does not suﬀer from a large loss in performance when going from a
batch to a stochastic setting in medium-scale experiments, without requiring extensive hand

33

tuning of the optimization algorithm. As we shall see in the next section, when batch opti-
mization is not feasible, stochastic optimization in our model performs well when compared
to state-of-the-art approaches for inference in gp models on very large datasets.

9.7 Large-scale Experiments

In this section we evaluate savigp on two large-scale problems involving prediction of airline
delays (regression, N = 700, 000) and handwritten digit recognition (classiﬁcation, N =
8, 100, 000).

9.7.1 Airline delays

Here we consider the problem of predicting airline delays using a univariate Gaussian like-
lihood model (Hensman et al., 2013). We note that this dataset has outliers that can
signiﬁcantly aﬀect the performance of regression algorithms when using metrics such as the
rmse. This has also been pointed out by Das et al. (2015). Additionally, in order to match
the application of the algorithms to a realistic setting, evaluation of learning algorithms
on this dataset should always consider making predictions in the future. Therefore, unlike
Hensman et al. (2013), we did not randomly select the training and test sets, instead we
selected the ﬁrst 700, 000 data points starting at a given oﬀset as the training set and the
next 100, 000 data points as the test set. We generated ﬁve training/test sets by setting the
initial oﬀset to 0 and increasing it by 200, 000 each time.

We used the squared exponential covariance function with automatic relevance deter-
mination (ard), and optimized all the parameters using adadelta (Zeiler, 2012). We
compare the performance of our model with the stochastic variational inference on gps
method (svigp) described by Hensman et al. (2013), which assumes full knowledge of the
likelihood function. Our method (savigp) and svigp were optimized using adadelta with
identical settings. We also report the results of Bayesian linear regression with a zero-mean
unit-variance prior over the weights (linear) and Gaussian process regression using subsets
of the training data (gp1000 and gp2000 using 1, 000 and 2, 000 datapoints respectively).
For each run we ﬁt gp1000 and gp2000 ten times using a randomly selected subset of
the training data. Kernel hyper-parameters of gp1000 and gp2000 were optimized using
l-bfgs.

Figure 9 shows the performance of savigp and the four baselines. We see that savigp
converges at a very similar rate to svigp, despite making no assumption about the likelihood
model. Furthermore, savigp performs better than all three simple baselines after less than
5 epochs. We note that our results are not directly comparable with those reported by
Hensman et al. (2013), since we deal with the harder problem of predicting future events
due to the way our dataset is selected.

9.7.2 Training Loss vs Test Performance

An additional interesting question about the behavior of our model relates to how well
the training objective function correlates to the test error metrics. Figure 10 shows how
the performance metrics (rmse and nlpd) on the test data vary along with the negative
evidence lower bound (nelbo). As we can see, changes in the nelbo closely mirror changes

34

Figure 9: The rmse (left) and nlpd (right) of svigp (Hensman et al., 2013), and savigp
(our method) averaged across all 5 airline-delay experiment runs. The x-axis represents the
number of passes over the data (epochs). The ﬁgure also shows the performance of gp1000,
gp2000 and Bayesian linear regression (linear) after training is completed.

Figure 10: The rmse (left) and nlpd (right) of savigp on the test data alongside the
negative evidence lower bound (nelbo) on the training data, averaged over ﬁve runs of the
airline-delay experiment.

in rmse and nlpd in our model, which is an indication that the variational objective
generalizes well to test performance when using these metrics.

9.7.3 Large-scale mnist

In this section we evaluate the performance of savigp on the mnist8m (Loosli et al., 2007)
dataset, which artiﬁcially extends the mnist dataset to 8.1 million training points by pseudo-
randomly transforming existing mnist images.

We train savigp on the mnist8m dataset by optimizing only variational parameters
stochastically, with a batch size of 1000 and 2000 inducing points. We also use a squared
exponential covariance function without automatic relevance determination.

After optimizing the model for 19 hours, we observe an error rate of 1.54% on the test set
and lower, middle and upper nlp quartiles of 8.61e−4, 3.81e−3, and 2.16e−2 respectively.
We see that this outperforms signiﬁcantly our previous result in §9.4 on standard mnist,
where we reported an error rate of 2.77%. As a point of comparison with hard-coded ap-

35

Figure 11: The negative evidence lower bound (nelbo) on the current batch of the mnist8m
training set over time on savigp. The x-axis represents the number of training steps taken.

proaches, Henao and Winther (2012) reported 0.86% error rate on mnist when considering
an augmented active set (which is analogous to the concept of inducing inputs/variables)
and a 9th-degree polynomial covariance function.

Finally, Figure 11 shows the training loss (the nelbo) of savigp on mnist8m as a
function of the number of training steps, where we note that the loss decreases rapidly in
the ﬁrst 2,000 iterations and stabilizes at around 4,000 iterations.

9.8 Seismic inversion

In this experiment we aim to evaluate our method qualitatively on a non-standard inference
problem motivating the need for black-box likelihoods. This application considers a one-
dimensional seismic inversion and was investigated by Bonilla et al. (2016). Given noisy
surface observations of sound-reﬂection times, the goal is to infer the geometry (layer depths)
of subsurface geological layers and seismic propagation velocities within each layer. For this,
we consider a real dataset from a seismic survey carried out in the Otway basin region in
the state of Victoria, Australia.

Setting There are N = 113 site locations with P = 4 interface reﬂections (layers) per site.
The inputs are given by the surface locations of the seismic sensors (X), the outputs are
the observed times at the diﬀerent locations for each of the layers (Y) and we aim to infer
Q = 2 ∗ P latent functions, i.e. P functions corresponding to layer depths and P functions
corresponding to seismic propagation velocities. For clarity, in this section we refer to the
latent functions corresponding to depth and velocity as f d and f v, respectively.

Likelihood The likelihood of the observed times ynp for location xn and interface p is
given by:

ynp =






(cid:17)

(cid:16) f d
np
f v
np
(cid:18) f d
np−f d
f v
np

2

2

+ (cid:15)np,
(cid:19)

np−1

for p = 1,

+ ynp−1 + (cid:15)np,

for 1 < p ≤ P ,

where (cid:15)np ∼ N (0, σ2
(2016), we set the corresponding standard deviations to 0.025s, 0.05s, 0.075s and 0.1s.

p is the output-dependent noise variance. As in Bonilla et al.

p) and σ2

36

Figure 12: Results for the seismic inversion experiment using our algorithm (savigp) and
the mcmc algorithm developed by Bonilla et al. (2016). The mean and standard deviations
envelopes are shown for savigp and mcmc in solid and dashed lines, respectively. Left:
inferred layer boundaries. Right: inferred seismic velocities.

Prior setting: We used the same prior as in Bonilla et al. (2016), with prior mean depths
of 200m, 500m, 1600m and 2200m and prior mean velocities of 1950m/s, 2300m/s, 2750m/s
and 3650m/s. The corresponding standard deviations for the depths were set to 15% of
the layer mean, and for the velocities they were set to 10% of the layer mean. A squared
exponential covariance function with unit length-scale was used.

Posterior estimation We ran our algorithm for the dense case (Z = X) using a full
Gaussian posterior and batch optimization, initializing the posterior means to the prior
means and the posterior covariance to a diagonal matrix with entries corresponding to 0.1%
of the prior variances. The results are given in Figure 12, where we see that savigp’s
posterior closely match the “true” posterior obtained by the mcmc algorithm developed
in Bonilla et al. (2016), although the variances are overestimated, which can be see as a
consequence of our variational approach using a full Gaussian approximate posterior.

10 Conclusions and Discussion

We have developed scalable automated variational inference for Gaussian process models
(savigp), an inference method for models with Gaussian process (gp) priors, multiple out-
puts, and nonlinear likelihoods. The method is generally applicable to black-box likelihoods,
i.e. it does not need to know the details of the conditional likelihood (or its gradients), only
requiring its evaluation as a black-box function.

One of the key properties of this method is that, despite using a ﬂexible variational
posterior such as a mixture-of-Gaussians distribution, it is statistically eﬃcient in that

37

it requires samples from univariate Gaussian distributions to estimate the evidence lower
bound (elbo) and its gradients.

In order to provide scalability to very large datasets, we have used an augmented prior
via the so-called inducing variables, which are prevalent in most sparse gp approximations.
This has allowed us to decompose the elbo as a sum of terms over the datapoints, hence
giving way for the application of stochastic optimization and parallel computation.

Our small-scale experiments have shown that savigp can perform as accurately as so-
lutions that have been hard-coded speciﬁcally for the conditional likelihood of the problem
at hand, even under high sparsity levels.

Our medium-scale experiments also have shown the eﬀectiveness of savigp, when consid-
ering problems such as multi-class classiﬁcation on the mnist dataset and highly nonlinear
likelihoods such as that used in gprn. On these experiments, we also have analyzed the
eﬀect of learning the inducing inputs, i.e. the locations corresponding to the inducing vari-
ables, and concluded that doing this can yield signiﬁcant improvements in performance,
reducing the numbers of inducing variables required to achieve similar accuracy by an or-
der of magnitude. Nevertheless, there is generally a trade-oﬀ between reducing the time
cost gained by having a lower number of inducing inputs and the overhead of carrying out
optimization over these locations.

Our ﬁrst large-scale experiment, on the problem of predicting airline delays, showed
that savigp is on par with the state-of-the-art approach for Gaussian process regression on
big data (Hensman et al., 2013). In fact, our results show that savigp is slightly better but
we attribute those diﬀerences to implementation speciﬁcs. The important message is that
savigp can attain state-of-the-art performance even without exploiting speciﬁc knowledge
of the likelihood model. Our second large-scale experiment shows that having an inference
algorithm for gp models with non-Gaussian likelihood for large datasets is worthwhile, as
the performance obtained on mnist8m (using 8.1M observations) was signiﬁcantly better
than on the standard mnist (using 60K observations).

Our ﬁnal experiment considered a non-standard inference problem concerning a seismic
In this problem savigp yielded a solution for the posterior over latent
inversion task.
functions that closely matched the solution obtained by a (non-scalable) sampling algorithm.
Overall, we believe savigp has the potential to be a powerful tool for practitioners and
researchers when devising models for new or existing problems with Gaussian process priors
for which variational inference is not yet available. As mentioned in Section 2, we are very
much aware of recent developments in the areas of probabilistic programming, stochastic
variational inference and Bayesian deep learning. Advances such as those in black-box
variational inference (bbvi, Ranganath et al., 2014) and variational auto-encoders (Rezende
et al., 2014; Kingma and Welling, 2014) are incredibly exciting for the machine learning
community. While the former, bbvi, is somewhat too general to be useful in practice for gp
models, the latter (variational auto-encoders) requires speciﬁc knowledge of the likelihood
and is not a truly black-box method.

Finally, we are working on extending our models to more complex settings such as
structured prediction problems, i.e. where the conditional likelihood is a non-iid model such
as a chain or a tree (see e.g. Galliani et al., 2017, for a recent reference). Such settings provide
incredible challenges from the computational and modeling perspectives. For example, how
to deal with the exponential increase in the number of parameters of the model, and how

38

to reduce the number of calls to an expensive conditional likelihood model. We believe that
the beneﬁts of being Bayesian well outweigh the eﬀort in exploring such challenges.

Acknowledgments

We acknowledge the contribution by Trung V. Nguyen to the original conference paper
(Nguyen and Bonilla, 2014a). evb started this work at the University of New South Wales
(unsw sydney) and was partially supported by unsw’s Faculty of Engineering Research
Grant Program project # PS37866; unsw’s Academic Start-Up Funding Scheme project #
PS41327; and an aws in Education Research Grant award. AD started this work at unsw
sydney, and was supported by a Research Fellowship from unsw sydney.

A Derivation of the KL-divergence Term

Here we derive the expressions for the terms composing the KL-divergence (Lkl = Lent +
Lcross) part of the log-evidence lower bound (Lelbo).

The entropy term is given by:

Lent(λ) = −Eq(u|λ)[log q(u|λ)]
(cid:90) K
(cid:88)

= −

πkqk(u|mk, Sk) log

π(cid:96)q(cid:96)(u|m(cid:96), S(cid:96))du

K
(cid:88)

(cid:96)=1
K
(cid:88)

(cid:96)=1
K
(cid:88)

(cid:96)=1

= −

πk

N (u; mk, Sk) log

π(cid:96)N (u; m(cid:96), S(cid:96))du

k=1

(cid:90)

K
(cid:88)

k=1
K
(cid:88)

k=1
K
(cid:88)

k=1
K
(cid:88)

k=1

(cid:90)

(cid:90)

K
(cid:88)

(cid:96)=1
K
(cid:88)

(cid:96)=1

≥ −

πk log

N (u; mk, Sk)

π(cid:96)N (u|m(cid:96), S(cid:96))du

= −

πk log

π(cid:96)

N (u; mk, Sk)N (u|m(cid:96), S(cid:96))du

= −

πk log

π(cid:96)N (mk; m(cid:96), Sk + S(cid:96)) def= (cid:98)Lent,

(27)

(28)

where we have used Jensen’s inequality to bring the logarithm out of the integral from
Equations (27) to (28).

The negative cross-entropy term can be computed as:

Lcross(λ) = Eq(u|λ)[log p(u)] =

πkqk(u|λk) log p(u)du

K
(cid:88)

(cid:90)

k=1

K
(cid:88)

Q
(cid:88)

(cid:90)

πk

=

k=1

j=1

N (uj; mkj, Skj) log N (uj; 0, Kj

zz)duj

39

πk

log N (mkj; 0, Kj

zz) −

tr (Kj

zz)−1Skj

(cid:21)

1
2

K
(cid:88)

Q
(cid:88)

(cid:20)

=

k=1

j=1

= −

1
2

K
(cid:88)

Q
(cid:88)

πk

k=1

j=1

(cid:2)M log 2π + log (cid:12)

(cid:12)Kj
zz

(cid:12)
(cid:12) + mT

kj(Kj

zz)−1mkj + tr (Kj

zz)−1Skj

(cid:3) .

B Proof of Theorem 1

In this section we prove Theorem 1 concerning the statistical eﬃciency of the estimator of the
expected log likelihood and its gradients, i.e. that both can be estimated using expectations
over Univariate Gaussian distributions.

B.1 Estimation of Lell

Taking the original expression in Equation (16) we have that:

Lell(λ) = Eq(f |λ)[log p(y|f , φ)],
(cid:90)

q(f |λ)log p(yn|fn·, φ)df ,

f

(cid:90)

(cid:90)

fn·

f¬n·

N
(cid:88)

n=1
N
(cid:88)

n=1
N
(cid:88)

n=1
N
(cid:88)

=

=

=

=

K
(cid:88)

n=1

k=1

q(f¬n·|fn·)q(fn·) log p(yn|fn·, φ)df¬n·dfn·,

Eq(n)(fn·)[log p(yn|fn·, φ)],

πkEqk(n)(fn·|λk)[log p(yn|fn·, φ)],

(29)

(30)

(31)

where we have applied the linear property of the expectation operation in Equation (29);
used f¬n· to denote all the latent functions except those corresponding to the nth observa-
tion, and integrated these out to obtain Equation (30); and used the form of the marginal
posterior (mog in Equation (17)) to get Equation (31).

As described in Section 4.6, qk(n)(fn·|λk) is a Q-dimensional Gaussian with diagonal
covariance, hence computation of Equation (31) only requires expectations over univariate
(cid:4)
Gaussian distributions.

B.2 Estimation of the Gradients of Lell
Denoting the kth term for the nth observation of the expected log likelihood with L(k,n)
have that:

ell we

L(k,n)
ell = Eqk(n)(fn·|λk)[log p(yn|fn·, φ)]

(cid:90)

=

fn·

qk(n)(fn·|λk) log p(yn|fn·, φ)dfn·

40

(cid:90)

∇λk L(k,n)

ell =

qk(n)(fn·|λk)∇λk log qk(n)(fn·|λk) log p(yn|fn·, φ)dfn·
(cid:2)∇λk log qk(n)(fn·|λk) log p(yn|fn·, φ)(cid:3),

= Eqk(n)(fn·|λk)

fn·

(32)

(33)

for λk ∈ {mk, Sk}, and for the mixture proportions the gradients can be estimated straight-
forwardly using Equation (22). We have used in Equation (32) the fact that ∇xf (x) =
f (x)∇x log f (x) for any nonnegative function f (x). We see that our resulting gradient esti-
mate has an analogous form to that obtained in Equation (31), hence its computation only
(cid:4)
requires expectations over univariate Gaussian distributions.

C Gradients of the Evidence Lower Bound wrt Variational

Parameters

Here we specify the gradients of the log-evidence lower bound (Lelbo) wrt variational pa-
rameters. For the covariance, we consider Skj of general structure but also give the updates
when Skj is a diagonal matrix, denoted with ˜Skj.

C.1 KL-divergence Term
Let Kzz be the block-diagonal covariance with Q blocks Kj
us assume the following deﬁnitions:

zz, j = 1, . . . Q. Additionally, let

(34)

Ckl

Nk(cid:96)

def= Sk + S(cid:96),
def= N (mk; m(cid:96), Ckl),

zk

def=

π(cid:96)Nk(cid:96).

K
(cid:88)

(cid:96)=1

The gradients of Lkl wrt the posterior mean and posterior covariance for component k are:

∇mk Lcross = −πkK−1

zz mk,

∇Sk Lcross = −

πkK−1

zz , and for diagonal covariance we have:

∇˜Sk

Lcross = −

πk diag(K−1

zz ),

1
2
1
2

1
2

Q
(cid:88)

j=1

∇πk Lcross = −

[M log 2π + log (cid:12)

(cid:12)Kj
zz

(cid:12)
(cid:12) + mT

kj(Kj

zz)−1mkj + tr (Kj

zz)−1Skj],

where we note that we compute K−1
dently. The gradients of the entropy term wrt the variational parameters are:

zz by inverting the corresponding blocks Kj

zz indepen-

∇mk (cid:98)Lent = πk

K
(cid:88)

(cid:96)=1

π(cid:96)

(cid:18) Nk(cid:96)
zk

+

Nk(cid:96)
z(cid:96)

(cid:19)

C−1

kl (mk − m(cid:96)),

∇Sk (cid:98)Lent =

πk

1
2

K
(cid:88)

(cid:96)=1

π(cid:96)

(cid:18) Nk(cid:96)
zk

+

Nk(cid:96)
z(cid:96)

(cid:19)

(cid:2)C−1

kl − C−1

kl (mk − m(cid:96))(mk − m(cid:96))T C−1

kl

(cid:3) ,

41

and for diagonal covariance we have:

∇˜Sk (cid:98)Lent =

1
2

πk

K
(cid:88)

(cid:96)=1

π(cid:96)

(cid:18) Nk(cid:96)
zk

+

Nk(cid:96)
z(cid:96)

(cid:19) (cid:104) ˜C−1

∇πk (cid:98)Lent = − log zk −

K
(cid:88)

(cid:96)=1

π(cid:96)

Nk(cid:96)
z(cid:96)

,

kl − ˜C−1

kl diag ((mk − m(cid:96)) (cid:12) (mk − m(cid:96))) ˜C−1

kl

(cid:105)

,

where ˜Ckl is the diagonal matrix deﬁned analogously to Ckl in Equation (34) and (cid:12) is the
Hadamard product.

C.2 Expected Log Likelihood Term

Monte Carlo estimates of the gradients of the expected log likelihood term are:

∇mkj (cid:98)Lell =

(Kj

zz)−1

zn[Σk(n)]−1
kj
j,j

f (k,i)
nj − [bk(n)]j

log p(yn|f (k,i)

n·

),

N
(cid:88)

n=1

S
(cid:88)

(cid:16)

i=1

∇Skj (cid:98)Lell =

N
(cid:88)

n=1

(cid:0)ajnaT

jn

(cid:1)

(cid:20)
[Σk(n)]−2
j,j

(cid:16)

S
(cid:88)

i=1

f (k,i)
nj − [bk(n)]j

− [Σk(n)]−1
j,j

log p(yn|f (k,i)

n·

),

(cid:21)

πk
S

πk
2S

(cid:17)

(cid:17)2

(35)

∇πk (cid:98)Lell =

log p(yn|f (k,i)

n·

),

1
S

N
(cid:88)

S
(cid:88)

n=1

i=1

(cid:16)

(cid:17)

and for diagonal covariance ˜Skj we replace
with diag (ajn (cid:12) ajn) in Equation
(35), where (cid:12) is the Hadamard product and diag(v) takes the input vector v and outputs
a matrix with v on its diagonal. We have also deﬁned above kj
def= κj(Zj, xn), i.e. the
zn
vector obtained from evaluating the covariance function j between the inducing points Zj
and datapoint xn.

ajnaT
jn

D Gradients of the Evidence Lower Bound wrt Covariance

Hyperparameters and Likelihood Parameters

Here we give the gradients of the variational objective (Lelbo) wrt the covariance hyper-
parameters and, when required, the conditional likelihood parameters. We note here that
our method does not require gradients of the conditional likelihood p(y|f , φ) wrt the latent
functions f . However, if the conditional likelihood is parametrized by φ and point-estimates
of these parameters are needed, these can also be learned in our variational framework.

D.1 Covariance Hyperparameters

The gradients of the terms in the KL-divergence part of Lelbo wrt a covariance hyperpa-
rameter θj are:

∇θj (cid:98)Lent = 0,

42

∇θj Lcross = −

πk tr (cid:2)(Kj

zz)−1∇θj Kj
zz

1
2

K
(cid:88)

k=1

− (Kj

zz)−1∇θj Kj

zz(Kj

zz)−1 (cid:0)mkjmT

kj + Sj

(cid:1) (cid:3).

For the (cid:98)Lell we have that:

∇θj (cid:98)Lell =

πkEqk(n)(fn·|λk)

(cid:2)∇θj log qk(n)(fn·|λk) log p(yn|fn·)(cid:3),

N
(cid:88)

K
(cid:88)

n=1

k=1

and computing the corresponding gradient we obtain:

∇θj (cid:98)Lell = −

1
2

N
(cid:88)

K
(cid:88)

n=1

k=1

πkEqk(n)(fn·|λk)

[Σk(n)]−1

j,j ∇θj [Σk(n)]j,j

(cid:104)(cid:16)

(36)

− 2(fnj − [bk(n)]j)[Σk(n)]−1
− (fnj − [bk(n)]j)2[Σk(n)]−2

j,j ∇θj [bk(n)]j

j,j ∇θj [Σk(n)]j,j

(cid:17)

(cid:105)
log p(yn|fn·)

,

for which we need:

jn

(cid:1) mkj,

∇θj [bk(n)]j = (cid:0)∇θj aT
∇θj [Σk(n)]j,j = ∇θj [ (cid:101)Kj]n,n + 2 (cid:0)∇θj aT
= ∇θj κ(xn, xn) − (cid:0)∇θj aT
(cid:1) Skjajn,

+ 2 (cid:0)∇θj aT

jn

jn

jn

(cid:1) Skjajn
(cid:1) kj

zn − aT

jn∇θj kj
zn

where

∇θj aT

jn =

∇θj (kj

zn)T − aT

jn∇θj Kj
zz

(Kj

zz)−1,

(cid:16)

(cid:17)

and as in the main text we have deﬁned ajn
nth column of Aj. Furthermore, as in the previous section, kj
zn

def= κj(Zj, xn).

def= [Aj]:,n, i.e. the vector corresponding to the

D.2 Likelihood Parameters

Since the terms in the KL-divergence do not depend on the likelihood parameters we have
that ∇φ (cid:98)Lent = ∇φLcross = 0. For the gradients of the expected log likelihood term we have
that:

∇φ (cid:98)Lell =

1
S

N
(cid:88)

K
(cid:88)

S
(cid:88)

πk

n=1

k=1

i=1

∇φ log p(yn|f (k,i)

n·

, φ),

where {f (k,i)

n· } ∼ N (fn·; bk(n), Σk(n)), for k = 1, . . . , K and i = 1, . . . , S.

E Gradients of the Log-Evidence Lower Bound wrt Inducing

Inputs

These gradients can be obtained by using the same expression as the gradients wrt co-
variance hyperparameters in Appendix D.1, considering the inducing inputs as additional

43

hyperparameters of the covariances. Therefore, we rewrite the gradients above keeping in
mind that θj is an element of inducing input Zj, hence dropping those gradient terms that
do not depend on Zj. As before we have that ∇θj (cid:98)Lent = 0.

∇θj Lcross = −

πk tr (cid:8)(cid:2)(Kj

zz)−1 − (Kj

zz)−1 (cid:0)mkjmT

kj + Sj

(cid:1) (Kj

zz)−1(cid:3)∇θj Kj

zz

(cid:9). (37)

1
2

K
(cid:88)

k=1

Similarly, for the gradients of (cid:98)Lell in Equation (36) we have that:

∇θj [bk(n)]j = ∇θj (kj

zn)T (Kj

zz)−1mkj − aT

jn∇θj Kj

zz(Kj

zz)−1mkj,

and

∇θj [Σk(n)]j,j = ∇θj (kj
+ aT

zn)T (cid:16)
jn∇θj Kj

− (Kj

zz)−1kj
zz)−1kj

zn − ajn + 2(Kj
zn − 2aT
jn∇θj Kj

zz)−1Skjajn
zz(Kj

zz)−1Skjajn.

zz(Kj

(cid:17)

From the equations above, we see that in the computation of the gradients of (cid:98)Lell there

are two types of terms. The ﬁrst term is of the form:
n ∇θKj

∇θt(1)
n

def= vT

zzwn,

where we have dropped the index j on the LHS of the equation for simplicity in the notation
and vn, wn are M -dimensional vectors. Let V and W be the M ×N matrices corresponding
to the N vectors {vn} and {wn}, respectively. Furthermore, let us assume ˜Z(d)
is the M ×M
j
matrix of all pairwise diﬀerences on dimension d of all inducing points divided by the squared
length-scale of the dimension (cid:96)2
d,

[ ˜Z(d)
j

]o,p =

[Zj]o,d − [Zj]p,d
(cid:96)2
d

.

Hence, in the case of the squared exponential covariance function the above gradient can
be calculated as follows (for all data points):

∇[Zj ]:,dt(1) = −(( ˜Z(d)

j (cid:12) Kj

zz)W) (cid:12) V − (( ˜Z(d)

j (cid:12) Kj

zz)V) (cid:12) W,

where ∇[Zj ]:,dt(1) is the M × N matrix of gradients corresponding to dimension d for all
m = 1, . . . , M and n = 1, . . . N .

Similarly, the second type of term is of the form:
n ∇θkj

∇θt(2)
n

def= vT

zn,

where vn and V are deﬁned as before. The gradients in the above expression wrt to the
inducing points (for all datapoints or a mini-batch) can be calculated as follows:

∇[Zj ]:,dt(2) = −(Kj

xz (cid:12) VT (cid:12) ˜X(d))T ,

where in the above equation ˜X(d) is the N ×M matrix of all pairwise diﬀerences on dimension
d between all datapoints and inducing points divided by the squared length-scale of the
dimension (cid:96)2
d:

[ ˜X(d)]o,p =

[X]o,d − [Zj]p,d
(cid:96)2
d

.

44

F Control Variates

We use control variates (see e.g. Ross, 2006, §8.2) to reduce the variance of the gradient
estimates. In particular, we are interested in estimating gradients of the form:

∇λk

Eqk(n)(fn·|λk)[log p(yn|fn·)] = Eqk(n)(fn·|λk)[g(fn·)], with

g(fn·) = ∇λk log qk(n)(fn·|λk) log p(yn|fn·),

where the expectations are computed using samples from qk(n)(fn·|λk), which depends on
the variational parameter λk. As suggested by Ranganath et al. (2014), a sensible control
variate is the so-called score function

whose expectation is zero. Hence, the function:

h(fn·) = ∇λk log qk(n)(fn·|λk),

has the same expectation as g(fn·) but lower variance when ˆa is given by:

˜g(fn·) = g(fn·) − ˆah(fn·),

ˆa =

Cov[g(fn·), h(fn·)]
V[h(fn·)]

,

where Cov[g(fn·), h(fn·)] is the covariance between g(fn·) and h(fn·); V[h(fn·)] is the variance
of h(fn·); and both are estimated using samples from qk(n)(fn·|λk). Therefore, our corrected
gradient is given by:

˜∇λk

Eqk(n)(fn·|λk)[log p(yn|fn·)] def= Eqk(n)(fn·|λk)[˜g(fn·)]

= Eqk(n)(fn·|λk)

(cid:2)∇λk log qk(n)(fn·|λk)(log p(yn|fn·) − ˆa)(cid:3).

G Derivations for the Dense Posterior Case

In this section we consider the case of having a dense variational posterior, i.e. not using the
so-called sparse approximations. We derive the expressions for the components of the log-
evidence lower bound and show that the posterior parameters of the variational distribution
q(f |λ) are, in fact, of ‘free’-form. Furthermore, we analyze the case of a Gaussian likelihood
showing that, in the limit of a large number of samples, our estimates converge to the exact
analytical solution.

G.1 Evidence Lower Bound

Here we show the derivations of all the terms in Lelbo when considering the dense case,
i.e. M = N and Zj = X. As described in the main text, the resulting changes to these
terms can be obtained by replacing M with N and Kj
xx. Hence, for the terms in
the KL-divergence part of Lelbo we have that:

zz with Kj

(cid:98)Lent(λ) = −

πk log

π(cid:96)N (mk; m(cid:96), Sk + S(cid:96)),

(38)

K
(cid:88)

k=1

K
(cid:88)

(cid:96)=1

45

Lcross(λ) = −

[N log 2π + log (cid:12)

(cid:12)Kj
xx

(cid:12)
(cid:12) + mT

kj(Kj

xx)−1mkj + tr (Kj

xx)−1Skj],(39)

1
2

K
(cid:88)

Q
(cid:88)

πk

k=1

j=1

where we recall that now mkj and Skj are N -dimensional objects. For the (cid:98)Lell we still
need to compute empirical expectations over the variational distribution q(f |λ), where the
corresponding parameters are given by:

bkj = Ajmkj = Kj
Σkj = (cid:101)Kj − AjKj
xx − Kj

xx(Kj
xx + AjSkjAj
xx)−1Kj
xx(Kj

xx)−1mkj = mkj,

= Kj
= Skj.

xx + Kj

xx(Kj

xx)−1SkjKj

xx(Kj

xx)−1

G.2 Gaussian Likelihoods

Consider the case of a single output and a single latent function (Q = P = 1) with a
Gaussian conditional likelihood and a single full Gaussian variational posterior (K = 1).

The entropy and the cross entropy terms are given in Equations (38) and (39) with Q =
K = 1. The expect log likelihood term can be determined analytically,

p(y|f ) = N (y; f , σ2I), and
q(f |λ) = N (f ; m, S).

Lell(λ) =

Eq(f |λ)[log p(yn|fn, φ)],

N
(cid:88)

n=1

= log N (y; m, σ2I) −

1
2σ2 tr S.

(cid:18)

K−1

xx +

(cid:19)

1
σ2

∇mLelbo = K−1

xxm +

1
σ2 (y − m) = 0

m =

1
σ2 y
(cid:99)m = (cid:0)σ2K−1
= Kxx

xx + I(cid:1)−1
(cid:0)Kxx + σ2I(cid:1)−1

y

y.

Hence the gradients of Lelbo can also be determined analytically, yielding the optimal mean
posterior as follows:

Similarly for the posterior covariances we have that:

∇SLelbo =

S−1 −

1
2
(cid:18)

(cid:98)S =

K−1

xx +

1
2σ2 I = 0

1
2

K−1

xx −
(cid:19)−1

1
σ2 I

46

(40)

= Kxx − Kxx

= Kxx − Kxx

(cid:18)

I + Kxx

1
1
σ2
σ2
(cid:0)Kxx + σ2I(cid:1)−1

(cid:19)−1

Kxx

Kxx,

(41)

where we have used Woodbury’s formula to go from Equation (40) to Equation (41). The
Equations above for the optimal posterior mean and posterior covariance, (cid:99)m and (cid:98)S, are
the exact analytical expressions for regressions with Gaussian process priors and isotropic
noise likelihoods, see Rasmussen and Williams (2006, §2.2).

H Eﬃcient Parametrization

In this appendix we prove Theorem 2, showing that it is possible to obtain an eﬃcient
parameterization of the posterior covariances when using a full Gaussian approximation. In
this case we have that:

∇Sj Lelbo = ∇Sj Lent + ∇Sj Lcross + ∇Sj Lell

= −

(Kj

zz)−1 +

S−1

j +

∇Sj

Eq(n)(fn·)[log p(yn|fn·)]

1
2

Setting the gradients to zero we have that

1
2

1
2

S−1

j =

(Kj

zz)−1 +

(Kj

zz)−1

znλjn(kj
kj

zn)T (Kj

zz)−1,

where

1
2

1
2

N
(cid:88)

n=1

N
(cid:88)

n=1

λjn = −2

d¯(cid:96)n
d[Σj]n,n
¯(cid:96)n = Eq(n)(fn·)[log p(yn|fn·)].

Therefore, the optimal solution for the posterior covariance is given by:

(cid:99)Sj = (cid:0)(Kj
= Kj
zz

zz)−1 + (Kj
(cid:0)Kj
zz + Kj

zz)−1Kj
zxΛjKj
xz

zxΛjKj
(cid:1)−1
Kj

xz(Kj
zz,

zz)−1(cid:1)−1

,

where Λj is a N × N diagonal matrix with {λjn}N

n=1 on the diagonal.

(cid:4)

I Lower Variance of Mixture-of-Diagonals Posterior

In this section we prove Theorem 3. First we review Rao-Blackwellization (Casella and
Robert, 1996), which is also known as partial averaging or conditional Monte Carlo. Sup-
pose we want to estimate V = Ep(x,y)[h(X, Y)] where (X, Y) is a random variable with

47

probability density p(x, y) and h(X, Y) is a random variable that is a function of X and
Y. It is easy to see that

Ep(x,y)[h(X, Y)] =

p(x, y)h(x, y)dxdy

(cid:90)

(cid:90)

=

p(y)p(x|y)h(x, y)dxdy

=Ep(y)

(cid:105)
(cid:104)ˆh(Y)

, with ˆh(Y) = Ep(x|y)[h(X, Y)|Y],

and, from the conditional variance formula,

V[ˆh(Y)] < V[h(X, Y)].

Therefore when ˆh(Y) is easy to compute, it can be used to estimate V with a lower variance
than the original estimator. When p(x, y) = p(x)p(y), then ˆh(Y) is simpliﬁed to

ˆh(Y = y) =

p(x)h(x, y)dx = Ep(x)[h(X, Y)|Y].

(cid:90)

We apply Rao-Blackwellization to our problem with fn· playing the role of the conditioning
variable Y and f¬n· playing the role of X, where f¬n· denotes f excluding fn·. We note that
this Rao-Blackwellization analysis is only applicable to the dense case with a mixture of
diagonals posterior, as it satisﬁes the independence condition, i.e. p(x, y) = p(x)p(y).

First, we express the gradient of λk(n) as an expectation by interchanging the integral

and gradient operators giving

∇λk(n)

Eqk(f |λk)[log p(y|f , φ)] =Eqk(f |λk)

(cid:105)
(cid:104)
∇λk(n) log qk(f |λk) log p(y|f , φ)

.

The Rao-Blackwellized estimator is thus

ˆh(fn·) =

q(f¬n·)∇λk(n) log qk(f |λk) log p(y|f )df¬n·

(cid:90)

(cid:90)

=

q(f¬n·)∇λk(n) log qk(n)(fn·|λk(n)) log p(y|f )df¬n·
(cid:90)

= ∇λk(n) log qk(n)(fn·|λk(n))

q(f¬n·) [log p(yn|fn·) + log p(y¬n|f¬n·)] df¬n·

(42)

= ∇λk(n) log qk(n)(fn·|λk(n)) [log p(yn|fn·) + C] ,

where we have used the factorization of the conditional likelihood in Equation (42), and
where C is a constant w.r.t fn·. This gives the Rao-Blackwellized gradient,

∇λk(n)

Eqk(f |λk)[log p(y|f )] = Eqk(n)(fn·|λk(n))
= Eqk(n)(fn·|λk(n))

(cid:105)

(cid:104)ˆh(fn·)
(cid:104)

∇λk(n) log qk(n)(fn·|λk(n)) log p(yn|fn·)

(cid:105)
,

where we have used the fact that Eq[∇ log q] = 0 for any q. We see then that the expression
(cid:4)
above is exactly the gradient obtained in Equation (33).

48

J Details of Complexity analysis

Here we give the details of the computational complexity of a single evaluation of the Lelbo
and its gradients. Let A be some matrix of size p × q, B be some matrix of size q × r, C
be some matrix of size q × p, and D be a square matrix of size s × s. Then we assume
that T (AB) ∈ O(pqr), T (D−1) ∈ O(s3), T (|D|) ∈ O(s3), T (diag(AC)) ∈ O(pq), and
T ( tr (AC)) ∈ O(pq).

J.1 Interim values

We ﬁrst provide an analysis of the cost of various interim expressions that get re-used
throughout the computation of the Lelbo. The interim expressions include: various values
computed from the kernel functions (Kj
zn), values used to determine
the shape of the Gaussian distributions from which we sample ([bk(n)]j, [Σk(n)]j,j), and
various intermediate values ([ (cid:101)Kj]n,n, ajn). We note that expressions with a subscript or
superscript of j, n, or k are dependent on Q, B, and K respectively. For example, in the
case of kj

zn, we actually need to consider the cost of Q × B vectors of size M .

zz, (Kj

(cid:12)
(cid:12), kj
(cid:12)

(cid:12)
(cid:12)Kj
(cid:12)

zz)−1,

zz

As stated in the main text, we assume that the kernel function is simple enough such
that evaluating its output between two points is in O(D). Hence the cost of evaluating
interim values is given by:

T ((Kj
T ((cid:12)

zz) ∈ O(QM 2D),

T (Kj
zz)−1) ∈ O(QM 3),
(cid:12)
(cid:12)Kj
(cid:12)) ∈ O(QM 3),
zz
T (kj
zn) ∈ O(QBM D),
T ([bk(n)]j) ∈ O(KQBM ),
T ([Σk(n)]j,j) ∈ O(KQBM 2),
T ([ (cid:101)Kj]n,n) ∈ O(QBM ),
T (ajn) ∈ O(QBM 2),

T ([Σk(n)]j,j) ∈ O(KQBM ).

and for diagonal covariance we have:

The computational cost of evaluating all interim values is thus in O(QM (M D +M 2 +BD +
KBM )) for the case of full covariance and in O(QM (M D + M 2, BD, KB)) for diagonal
covariances.

J.2 Analysis of the Lkl

The computational complexity of terms in the KL-divergence and its gradients is given by:

T (Lcross) ∈ O(KQM 2),
T (Lent) ∈ O(K2QM 3),

49

T (∇mLcross) ∈ O(KQM 2),
T (∇SLcross) ∈ O(KQM 2),
T (∇πk Lcross) ∈ O(KQM 2),
T (∇mLent) ∈ O(K2QM 3),
T (∇SLent) ∈ O(K2QM 3),
T (∇πk Lent) ∈ O(K2QM 3),

T (∇˜SLcross) ∈ O(KQM ),
T (∇mLent) ∈ O(K2QM ),
T (∇˜SLent) ∈ O(K2QM ),
T (∇πk Lent) ∈ O(K2QM ).

and for diagonal covariance we have:

Hence the computational cost of evaluating the KL-term and its gradients is in O(K2QM 3)
for full covariances, and O(KQM (K + M )) for diagonal covariances.

J.3 Analysis of the Lell

Let T (log p(yn|fn·, φ)) ∈ O(L), then the cost of evaluating the expected log likelihood and
its gradients is given by:

T (Lell) ∈ O(KBS(L + Q)),

T (∇mLell) ∈ O(KQ(M 2 + BM + BSL)),
T (∇SLell) ∈ O(KQB(M 2 + SL)),
T (∇πk Lell) ∈ O(KBS(L + Q)),

and for diagonal covariance we have:

T (∇˜SLell) ∈ O(KQB(M + SL)).

Hence the computational cost of evaluating the ell term and its gradients is in O(KQB(M 2+
SL)) for full covariances, and O(KQB(M + SL)) for diagonal covariances.

K Additional Results

K.1 Small-scale Experiments — Warped GPs

In the case of the creep dataset (Figure 13), sse and nlpd are better in denser models. As
expected, the performance of the dense (SF = 1) fg model is identical to the performance
of wgp. We also note that nlpd in gp has less variation compared to the other models,
which can be attributed to its assumption of Gaussian noise.

K.2 Medium-scale Experiments — Binary Classiﬁcation

We see in Figure 14 that the performance improves signiﬁcantly with denser models,
i.e. when we use a larger number of inducing variables. Overall, our model achieves an

50

Figure 13: The distributions of sse and nlpd for a warped Gaussian process likelihood
model on the creep dataset. Three approximate posteriors in savigp are used: fg (full
Gaussian), mog1 (diagonal Gaussian), and mog2 (mixture of two diagonal Gaussians),
along with various sparsity factors (SF = M/N ). The smaller the sf the sparser the model,
with SF = 1 corresponding to the original model without sparsity. wgp corresponds to the
performance of the exact inference method for warped Gaussian process models (Snelson
et al., 2003), and gp is the performance of the exact inference on a univariate Gaussian
likelihood model.

Figure 14: Error rates and nlp for binary classiﬁcation on the mnist-b dataset. We used
a full Gaussian (fg) posterior approximation across various sparsity factors (SF = M/N ).
The smaller the SF the sparser the model.

accuracy of 98.1% and a mean nlp of 0.062 on mnist-b with a sparsity factor of 0.04.
This is slightly better than the performance obtained by Hensman et al. (2015), who report

51

Figure 15: Mean sse and nlpd for multi-output regression on the sarcos dataset. We used
a full Gaussian (fg) posterior approximation across various sparsity factors (SF = M/N ).
The smaller the SF the sparser the model.

an accuracy of 97.8% and nlp of 0.069, although they used a smaller number of inducing
variables. As we have seen in §9.5, our model also achieves similar performance when using
the same sparsity factor as that used by Hensman et al. (2015) when the inducing inputs
(i.e. the locations of the inducing variables) are learned.

K.3 Medium-scale Experiments — Gaussian Process Regression Net-

works

Figure 15 shows the performance on the sarcos dataset. Compared to sarcos-2, learning
all joints yields a slight increase in msse for joints 4 and 7, but also resulted in a signiﬁcant
decrease in nlpd. This shows that the transition from learning only 2 joints to all 7 joints
(as in sarcos-2), while unhelpful for point estimates, led to a signiﬁcantly better posterior
density estimate. We also note that we had to make use of 10, 000 samples for this dataset
to get stable results over diﬀerent sparsity factors. This is a much larger number than
the 100 samples used in mnist. This is likely because the gprn likelihood model involves
multiplication between latent processes, which increases the variance of sampling results by
a squared factor, and therefore, it is reasonable to require more samples in this complex
likelihood model.

K.4 Medium-scale Experiments — Inducing-input Learning

Figure 16 shows the performance of the model under the two settings (with and without
learning inducing inputs) on mnist. As with mnist-b, we see that learning the location
of the inducing variables yields a large gain in performance. In fact, the sparser models
with inducing point learning performed similarly to the denser models, despite the fact that
the two models diﬀered by an order of magnitude when it came to the number of inducing
variables. Figures 17, 18 show the performance on sarcos and sarcos-2. We observe
an improvement in performance from learning inducing inputs. However, the improvement
is less signiﬁcant than for mnist, which shows that diﬀerent datasets have signiﬁcantly
diﬀerent sensitivity to the location of the inducing variables.

52

Figure 16: Comparison of error rate and nlp obtained by savigp without learning (fg) and
with learning (ind) of inducing inputs for multi-class classiﬁcation on the mnist dataset.

Figure 17: Comparison of mean sse and nlpd obtained by savigp without learning (fg)
and with learning (ind) of inducing points for multi-output regression on the sarcos-2
dataset.

Figure 18: Comparison of mean sse and nlpd obtained by savigp without learning (fg) and
with learning (ind) of inducing inputs for multi-output regression on the sarcos dataset.

L Notation

A summary of the notation used in this paper is shown in Table 4.

53

Table 4: Summary of notation used in the paper.

Symbol Description

Bold lower case denotes a vector
Bold upper case denotes a matrix
Determinant of matrix C
Trace of matrix C
Input dimensionality
Number of training datapoints
Number of inducing variables per latent process
Number of outputs
Number of latent functions
Number of mixture components in variational posterior
Number of samples in Monte Carlo estimates
nth input datapoint
N × D matrix of input data
P -dimensional vector of outputs (label) for nth observation
Q-dimensional vector of latent function values for nth observation
N -dimensional vector of latent function values for jth process
Inducing variables of latent process j
Vector of all inducing variables
M × D Matrix of inducing inputs for latent process j

v
C
|C|
tr C
D
N
M
P
Q
K
S
xn
X
yn
fn·
f·j
u·j
u
Zj
κj(x, x(cid:48)) Covariance function of latent process j evaluated at x, x(cid:48)
Kj
xx
Kj
zz
Kzz
Kj
xz
kj
zn
q(u|λ)
q(f |λ)
m
S
b
Σ

N × N covariance matrix obtained by evaluating κj(X, X)
M × M covariance matrix obtained by evaluating κj(Zj, Zj)
Block-diagonal covariance with blocks Kj
zz
N × M covariance between X and Zj
M × 1 covariance vector between Zj and xn
Variational distribution over inducing variables
Variational distribution over latent functions
Posterior mean over inducing variables
Posterior covariance over inducing variables
Posterior mean over latent functions
Posterior covariance over latent functions

54

References

Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, and Dzmitry
Bahdanau. Theano: A python framework for fast computation of mathematical expres-
sions. arXiv preprint arXiv:1605.02688, 2016.

Mauricio Alvarez and Neil D Lawrence. Sparse convolved Gaussian processes for multi-

output regression. In Neural Information Processing Systems, pages 57–64. 2009.

Mauricio A ´Alvarez and Neil D Lawrence. Computationally eﬃcient convolved multiple
output Gaussian processes. Journal of Machine Learning Research, 12(5):1459–1500,
2011.

Mauricio A. ´Alvarez, David Luengo, Michalis K. Titsias, and Neil D. Lawrence. Eﬃcient
multioutput Gaussian processes through variational inducing kernels. In Artiﬁcial Intel-
ligence and Statistics, 2010.

K. Bache and M. Lichman. UCI machine learning repository, 2013. URL http://archive.

ics.uci.edu/ml.

Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeﬀrey Mark
arXiv preprint

Siskind. Automatic diﬀerentiation in machine learning: a survey.
arXiv:1502.05767, 2015.

Christopher M. Bishop, Neil D. Lawrence, Tommi Jaakkola, and Michael I. Jordan. Ap-
proximating posterior distributions in belief networks using mixtures. In M. I. Jordan,
M. J. Kearns, and S. A. Solla, editors, Neural Information Processing Systems, pages
416–422. MIT Press, 1998.

Edwin V. Bonilla, Kian Ming A. Chai, and Christopher K. I. Williams. Multi-task Gaussian

process prediction. In Neural Information Processing Systems. 2008.

Edwin V. Bonilla, Shengbo Guo, and Scott Sanner. Gaussian process preference elicitation.

In Neural Information Processing Systems. 2010.

Edwin V Bonilla, Daniel Steinberg, and Alistair Reid. Extended and unscented kitchen

sinks. In International Conference on Machine Learning, 2016.

Phillip Boyle and Marcus Frean. Dependent Gaussian processes. In Neural Information

Processing Systems. 2005.

Thang D Bui, Josiah Yan, and Richard E Turner. A unifying framework for Gaussian
process pseudo-point approximations using power expectation propagation. The Journal
of Machine Learning Research, 18(1):3649–3720, 2017.

George Casella and Christian P. Robert. Rao-Blackwellisation of sampling schemes.

Biometrika, 1996.

Edward Challis and David Barber. Gaussian Kullback-Leibler approximate inference. Jour-

nal of Machine Learning Research, 14:2239–2286, 2013.

55

Wei Chu and Zoubin Ghahramani. Gaussian processes for ordinal regression. Journal of

Machine Learning Research, 6(Jul):1019–1041, 2005.

D. Cole, C. Martin-Moran, A.G. Sheard, H.K.D.H. Bhadeshia, and D.J.C. MacKay. Mod-
elling creep rupture strength of ferritic steel welds. Science and Technology of Welding
& Joining, 5(2):81–89, 2000.

Noel Cressie. Statistics for spatial data. John Wiley & Sons, 1993.

Kurt Cutajar, Edwin V. Bonilla, Pietro Michiardi, and Maurizio Filippone. Random fea-
ture expansions for deep Gaussian processes. In International Conference on Machine
Learning, volume 70, pages 884–893. PMLR, 06–11 Aug 2017.

Andreas Damianou and Neil Lawrence. Deep Gaussian processes. In Artiﬁcial Intelligence

and Statistics, 2013.

Sourish Das, Sasanka Roy, and Rajiv Sambasivan. Fast Gaussian process regression for big

data. arXiv preprint arXiv:1509.05142, 2015.

Marc Peter Deisenroth and Jun Wei Ng. Distributed Gaussian processes. In International

Conference on Machine Learning, 2015.

Amir Dezfouli and Edwin V. Bonilla. Scalable inference for Gaussian process models with

black-box likelihoods. In Neural Information Processing Systems. 2015.

Pedro Domingos, Stanley Kok, Hoifung Poon, Matthew Richardson, and Parag Singla.
Unifying logical and statistical AI. In AAAI Conference on Artiﬁcial Intelligence, 2006.

Kun Dong, David Eriksson, Hannes Nickisch, David Bindel, and Andrew G Wilson. Scalable
log determinants for Gaussian process kernel learning. In Neural Information Processing
Systems, pages 6327–6337, 2017.

Simon Duane, Anthony D. Kennedy, Brian J. Pendleton, and Duncan Roweth. Hybrid

Monte Carlo. Physics letters B, 195(2):216–222, 1987.

Yarin Gal and Richard Turner. Improving the Gaussian process sparse spectrum approxi-
mation by representing uncertainty in frequency inputs. In International Conference on
Machine Learning, pages 655–664, 2015.

Yarin Gal, Mark van der Wilk, and Carl Rasmussen. Distributed variational inference in
In Neural Information

sparse Gaussian process regression and latent variable models.
Processing Systems. 2014.

Pietro Galliani, Amir Dezfouli, Edwin V. Bonilla, and Novi Quadrianto. Gray-box inference
for structured Gaussian process models. In Aarti Singh and Jerry Zhu, editors, Artiﬁcial
Intelligence and Statistics, pages 353–361, Fort Lauderdale, FL, USA, Apr 2017. PMLR.

Andrew Gelman and Donald B Rubin. Inference from iterative simulation using multiple

sequences. Statistical science, pages 457–472, 1992.

56

Samuel J. Gershman, Matthew D. Hoﬀman, and David M. Blei. Nonparametric variational

inference. In International Conference on Machine Learning, 2012.

Noah D. Goodman, Vikash K. Mansinghka, Daniel M. Roy, Keith Bonawitz, and Joshua B.
In Uncertainty in Artiﬁcial

Tenenbaum. Church: A language for generative models.
Intelligence, 2008.

Ricardo Henao and Ole Winther. Predictive active set selection methods for Gaussian

processes. Neurocomputing, 80:10–18, 2012.

James Hensman, Nicolo Fusi, and Neil D Lawrence. Gaussian processes for big data. In

Uncertainty in Artiﬁcial Intelligence, 2013.

James Hensman, Alexander Matthews, and Zoubin Ghahramani. Scalable variational Gaus-

sian process classiﬁcation. In Artiﬁcial Intelligence and Statistics, 2015.

James Hensman, Nicolas Durrande, and Arno Solin. Variational Fourier features for Gaus-

sian processes. Journal of Machine Learning Research, 18:151:1–151:52, 2017.

Daniel Hern´andez-Lobato and Jos´e Miguel Hern´andez-Lobato. Scalable Gaussian process
classiﬁcation via expectation propagation. In Artiﬁcial Intelligence and Statistics, pages
168–176, 2016.

Geoﬀrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing
In Sixth Annual Conference on Computational

the description length of the weights.
Learning Theory, pages 5–13. ACM, 1993.

Trong Nghia Hoang, Quang Minh Hoang, and Kian Hsiang Low. A unifying framework of
anytime sparse Gaussian process regression models with stochastic variational inference
for big data. In International Conference on Machine Learning, 2015.

Matthew D. Hoﬀman and Andrew Gelman. The no-u-turn sampler: adaptively setting
path lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15(1):
1593–1623, 2014.

Marco F. Huber, Tim Bailey, Hugh Durrant-Whyte, and Uwe D. Hanebeck. On entropy
approximation for Gaussian mixture random vectors. In IEEE International Conference
on Multisensor Fusion and Integration for Intelligent Systems, 2008.

Kevin Jarrett, Koray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann Lecun. What is the
In International Conference on

best multi-stage architecture for object recognition?
Computer Vision, 2009.

R.G. Jarrett. A note on the intervals between coal-mining disasters. Biometrika, 66(1):

191–193, 1979.

ST John and James Hensman. Large-scale Cox process inference using variational Fourier

features. In International Conference on Machine Learning, 2018.

Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An

introduction to variational methods for graphical models. Springer, 1998.

57

Kristian Kersting, Christian Plagemann, Patrick Pfaﬀ, and Wolfram Burgard. Most likely
In International Conference on Machine

heteroscedastic Gaussian process regression.
Learning, 2007.

Mohammad E. Khan, Shakir Mohamed, Benjamin M. Marlin, and Kevin P. Murphy. A
stick-breaking likelihood for categorical data analysis with latent Gaussian models. In
Artiﬁcial Intelligence and Statistics, pages 610–618, 2012.

Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In International

Conference on Learning Representations, 2014.

Karl Krauth, Edwin V. Bonilla, Kurt Cutajar, and Maurizio Filippone. AutoGP: Exploring
the capabilities and limitations of gaussian process models. In Uncertainty in Artiﬁcial
Intelligence, 2017.

Harri Lappalainen and James W. Miskin. Ensemble Learning, pages 75–92. Springer Lon-

don, London, 2000.

Steﬀen L Lauritzen. Time series analysis in 1880: A discussion of contributions made by

tn thiele. International Statistical Review, pages 319–331, 1981.

Neil Lawrence. Probabilistic non-linear principal component analysis with Gaussian process

latent variable models. Journal of Machine Learning Research, 6:1783–1816, 2005.

Neil D Lawrence, Matthias Seeger, and Ralf Herbrich. Fast sparse Gaussian process meth-
ods: The informative vector machine. In Neural Information Processing Systems, 2002.

Miguel L´azaro-Gredilla, Joaquin Qui˜nonero-Candela, Carl Edward Rasmussen, and
An´ıbal R Figueiras-Vidal. Sparse spectrum Gaussian process regression. The Journal
of Machine Learning Research, 99:1865–1881, 2010.

Ga¨elle Loosli, St´ephane Canu, and L´eon Bottou. Training invariant support vector machines
using selective sampling. In Large Scale Kernel Machines, pages 301–320. MIT Press,
Cambridge, MA., 2007.

David J. C. MacKay. Developments in probabilistic modelling with neural networks —
ensemble learning. In Bert Kappen and Stan Gielen, editors, Third Annual Symposium
on Neural Networks, pages 191–198, London, 1995. Springer London.

David J. C. MacKay. Information theory, inference and learning algorithms. Cambridge

University Press, 2003.

Alexander G de G Matthews, James Hensman, Richard Turner, and Zoubin Ghahramani.
On sparse variational methods and the Kullback-Leibler divergence between stochastic
processes. Journal of Machine Learning Research, 51:231–239, 2016.

Alexander G. de G. Matthews, Mark van der Wilk, Tom Nickson, Keisuke. Fujii, Alexis
Boukouvalas, Pablo Le’on-Villagr’a, Zoubin Ghahramani, and James Hensman. GPﬂow:
A Gaussian process library using TensorFlow. Journal of Machine Learning Research, 18
(40):1–6, apr 2017.

58

Vladimir Maz’ya and Gunther Schmidt. On approximate approximations using Gaussian

kernels. IMA Journal of Numerical Analysis, 16(1):13–29, 1996.

Thomas P Minka. Expectation propagation for approximate Bayesian inference. In Uncer-

tainty in Artiﬁcial Intelligence, 2001.

Jesper Møller, Anne Randi Syversveen, and Rasmus Plenge Waagepetersen. Log Gaussian

Cox processes. Scandinavian journal of statistics, 25(3):451–482, 1998.

Iain Murray, Ryan Prescott Adams, and David J.C. MacKay. Elliptical slice sampling. In

Artiﬁcial Intelligence and Statistics, 2010.

Radford M. Neal. Probabilistic inference using Markov chain Monte Carlo methods. Tech-

nical report, Department of Computer Science, University of Toronto, 1993.

Trung V. Nguyen and Edwin Bonilla. Eﬃcient variational inference for Gaussian process

regression networks. In Artiﬁcial Intelligence and Statistics, pages 472–480, 2013.

Trung V. Nguyen and Edwin V. Bonilla. Automated variational inference for Gaussian

process models. In Neural Information Processing Systems. 2014a.

Trung V. Nguyen and Edwin V. Bonilla. Collaborative multi-output Gaussian processes.

In Uncertainty in Artiﬁcial Intelligence, 2014b.

Hannes Nickisch and Carl Edward Rasmussen. Approximations for binary Gaussian process

classiﬁcation. Journal of Machine Learning Research, 9(10), 2008.

Anthony O’Hagan and JFC Kingman. Curve ﬁtting and optimal design for prediction.
Journal of the Royal Statistical Society. Series B (Methodological), pages 1–42, 1978.

Manfred Opper and C´edric Archambeau. The variational Gaussian approximation revisited.

Neural Computation, 21(3):786–792, 2009.

C Paciorek and M Schervish. Nonstationary covariance functions for Gaussian process

regression. Neural Information Processing Systems, 2004.

Joaquin Qui˜nonero-Candela and Carl Edward Rasmussen. A unifying view of sparse approx-
imate Gaussian process regression. Journal of Machine Learning Research, 6:1939–1959,
2005.

Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Neural

Information Processing Systems. 2008.

Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing min-
In Neural Information Processing Systems.

imization with randomization in learning.
2009.

Rajesh Ranganath, Sean Gerrish, and David M. Blei. Black box variational inference. In

Artiﬁcial Intelligence and Statistics, 2014.

59

Marc’Aurelio Ranzato, Christopher Poultney, Sumit Chopra, and Yann Lecun. Eﬃcient
learning of sparse representations with an energy-based model. In Neural Information
Processing Systems, 2006.

Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine

learning. The MIT Press, 2006.

Alistair Reid, Simon O’Callaghan, Edwin V. Bonilla, Lachlan McCalman, Tim Rawling,
and Fabio Ramos. Bayesian joint inversions for the exploration of Earth resources. In
International Joint Conference on Artiﬁcial Intelligence, 2013.

Danilo J Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In International Conference on Machine
Learning, 2014.

Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of

mathematical statistics, pages 400–407, 1951.

Sheldon M Ross. Simulation. Burlington, MA: Elsevier, 2006.

H˚avard Rue, Sara Martino, and Nicolas Chopin. Approximate Bayesian inference for latent
Gaussian models by using integrated nested Laplace approximations. Journal of the royal
statistical society: Series b (statistical methodology), 71(2):319–392, 2009.

Hugh Salimbeni and Marc Deisenroth. Doubly stochastic variational inference for deep
Gaussian processes. In Advances in Neural Information Processing Systems, pages 4588–
4599, 2017.

Hugh Salimbeni, Stefanos Eleftheriadis, and James Hensman. Natural gradients in practice:
Non-conjugate variational inference in Gaussian process models. In Artiﬁcial Intelligence
and Statistics, 2018.

Rishit Sheth, Yuyang Wang, and Roni Khardon. Sparse variational inference for generalized

GP models. In International Conference on Machine Learning, 2015.

Ed Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. In

Neural Information Processing Systems, 2006.

Edward Snelson, Carl Edward Rasmussen, and Zoubin Ghahramani. Warped Gaussian

processes. In Neural Information Processing Systems, 2003.

Albert Tarantola. Inverse Problem Theory and Methods for Model Parameter Estimation.

Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 2004.

Yee Whye Teh, Matthias Seeger, and Michael I. Jordan. Semiparametric latent factor

models. In Artiﬁcial Intelligence and Statistics, 2005.

Michael E Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of

machine learning research, 1(Jun):211–244, 2001.

60

Michalis Titsias. Variational learning of inducing variables in sparse Gaussian processes. In

Artiﬁcial Intelligence and Statistics, 2009.

Sethu Vijayakumar and Stefan Schaal. Locally weighted projection regression: An O(n)
algorithm for incremental real time learning in high dimensional space. In International
Conference on Machine Learning, 2000.

Christopher K.I. Williams and David Barber. Bayesian classiﬁcation with Gaussian pro-
cesses. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 20(12):1342–
1351, 1998.

Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for regression.

In Neural Information Processing Systems, pages 514–520, 1996.

Andrew G. Wilson, David A. Knowles, and Zoubin Ghahramani. Gaussian process regres-

sion networks. In International Conference on Machine Learning, 2012.

Frank Wood, Jan-Willem van de Meent, and Vikash Mansinghka. A new approach to
probabilistic programming inference. In Artiﬁcial Intelligence and Statistics, pages 1024–
1032, 2014.

Zichao Yang, Andrew Gordon Wilson, Alexander J. Smola, and Le Song. `A la carte —

learning fast kernels. In Artiﬁcial Intelligence and Statistics, 2015.

Kai Yu and Wei Chu. Gaussian process models for link analysis and transfer learning. In

Neural Information Processing Systems, 2008.

Matthew D Zeiler. ADADELTA: an adaptive learning rate method.

arXiv preprint

arXiv:1212.5701, 2012.

61

