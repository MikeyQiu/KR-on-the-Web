7
1
0
2
 
y
a
M
 
6
1
 
 
]

G
L
.
s
c
[
 
 
2
v
2
6
8
4
0
.
5
0
7
1
:
v
i
X
r
a

EFFICIENT PARALLEL METHODS FOR DEEP REIN-
FORCEMENT LEARNING

Alfredo V. Clemente
Department of Computer and Information Science
Norwegian University of Science and Technology
Trondheim, Norway
alfredvc@stud.ntnu.no

Humberto N. Castej´on
Telenor Research
Trondheim, Norway
humberto.castejon@telenor.com

Arjun Chandra
Telenor Research
Trondheim, Norway
arjun.chandra@telenor.com

ABSTRACT

We propose a novel framework for efﬁcient parallelization of deep reinforce-
ment learning algorithms, enabling these algorithms to learn from multiple ac-
tors on a single machine. The framework is algorithm agnostic and can be ap-
plied to on-policy, off-policy, value based and policy gradient based algorithms.
Given its inherent parallelism, the framework can be efﬁciently implemented
on a GPU, allowing the usage of powerful models while signiﬁcantly reduc-
ing training time. We demonstrate the effectiveness of our framework by im-
plementing an advantage actor-critic algorithm on a GPU, using on-policy ex-
periences and employing synchronous updates. Our algorithm achieves state-
of-the-art performance on the Atari domain after only a few hours of training.
Our framework thus opens the door for much faster experimentation on demand-
ing problem domains. Our implementation is open-source and is made public at
https://github.com/alfredvc/paac.

1

INTRODUCTION AND RELATED WORK

Incorporating deep learning models within reinforcement learning (RL) presents some challenges.
Standard stochastic gradient descent algorithms for training deep learning models assume all train-
ing examples to be independent and identically distributed (i.i.d.). This constraint is often violated
by RL agents, given the high correlation between encountered states. Additionally, when learning
on-policy, the policy affects the distribution of encountered states, which in turn affects the policy,
creating a feedback loop that may lead to divergence (Mnih et al., 2013). Two main attempts have
been made to solve these issues. One is to store experiences in a large replay memory and em-
ploy off-policy RL methods (Mnih et al., 2013). Sampling the replay memory can break the state
correlations, thus reducing the effect of the feedback loop. Another is to execute multiple asyn-
chronous agents in parallel, each interacting with an instance of the environment independently of
each other (Mnih et al., 2016). Both of these approaches suffer from different drawbacks; experi-
ence replay can only be employed with off-policy methods, while asynchronous agents can perform
inconsistent parameter updates due to stale gradients1 and simultaneous parameter updates from
different threads.

Parallel and distributed compute architectures have motivated innovative modiﬁcations to existing
RL algorithms to efﬁciently make use of parallel execution. In the General Reinforcement Learning
Architecture (Gorila) (Nair et al., 2015), the DQN (Mnih et al., 2015) algorithm is distributed across
multiple machines. Multiple learners learn off-policy using experiences collected into a common
replay memory by multiple actors. Gorila is shown to outperform standard DQN in a variety of

1Gradients may be computed w.r.t. stale parameters while updates applied to a new parameter set.

1

Atari games, while only training for 4 days. The distribution of the learning process is further ex-
plored in (Mnih et al., 2016), where multiple actor-learners are executed asynchronously on a single
machine. Each actor-learner holds its own copy of the policy/value function and interacts with its
own instance of the environment. This allows for both off-policy, as well as on-policy learning.
The actor-learners compute gradients in parallel and update shared parameters asynchronously in
a HOGWILD! (Recht et al., 2011) fashion. The authors suggest that multiple agents collecting in-
dependent experiences from their own environment instances reduces correlation between samples,
thereby improving learning. The asynchronous advantage actor-critic (A3C) algorithm (Mnih et al.,
2016) was able to surpass the state of the art on the Atari domain at the time of publication, while
training for 4 days on a single machine with 16 CPU cores. GA3C (Babaeizadeh et al., 2016) is a
GPU implementation of A3C. It batches action selection and learning using queues. Actors sample
from a shared policy by submitting a task to a predictor, which executes the policy and returns an
action once it has accumulated enough tasks. Similarly, learning is performed by submitting expe-
riences to a trainer, which computes gradients and applies updates once enough experiences have
been gathered. If the training queue is not empty when the model is updated, the learning will no
longer be on-policy, since the remaining experiences were generated by an old policy. This leads
to instabilities during training, which the authors address with a slight modiﬁcation to the weight
updates.

We propose a novel framework for efﬁcient parallelization of deep reinforcement learning algo-
rithms, which keeps the strengths of the aforementioned approaches, while alleviating their weak-
nesses. Algorithms based on this framework can learn from hundreds of actors in parallel, similar
to Gorila, while running on a single machine like A3C and GA3C. Having multiple actors help
decorrelate encountered states and attenuate feedback loops, while allowing us to leverage the par-
allel architecture of modern CPUs and GPUs. Unlike A3C and Gorila, there is only one copy of
the parameters, hence parameter updates are performed synchronously, thus avoiding the possible
drawbacks related to asynchronous updates. Our framework has many similarities to GA3C. How-
ever, the absence of queues allows for a much more simpler and computationally efﬁcient solution,
while allowing for true on-policy learning and faster convergence to optimal policies. We demon-
strate our framework with a Parallel Advantage Actor-Critic algorithm, that achieves state of the art
performance in the Atari 2600 domain after only a few hours of training. This opens the door for
much faster experimentation.

2 BACKGROUND

Reinforcement learning algorithms attempt to learn a policy π that maps states to actions, in order
(cid:3) for some discount
to maximize the expected sum of cumulative rewards Rt = Eπ
factor 0 < γ < 1, where rt is the reward observed at timestep t. Current reinforcement learning
algorithms represent the learned policy π as a neural network, either implicitly with a value function
or explicitly as a policy function.

k=0 γkrt+k

(cid:2) (cid:80)∞

2.1 BATCHING WITH STOCHASTIC GRADIENT DESCENT

Current reinforcement learning algorithms make heavy use of deep neural networks, both to extract
high level features from the observations it receives, and as function approximators to represent its
policy or value functions.
Consider the set of input-target pairs S =
(x0, y0), (x1, y1), ...(xn, yn)
generated by some func-
{
}
tion f ∗(x). The goal of supervised learning with neural networks is to learn a parametrized function
. The performance of f is evaluated with the empirical
f (x; θ) that best approximates function f
∗
loss

L(θ) =

l(f (xs; θ), ys)

(1)

(cid:88)

1
S

|

|

s∈S

where l(f (xs; θ), ys) is referred to as a loss function, and gives a quantitative measure of how good
f is at modelling f ∗. The model parameters θ are learned with stochastic gradient descent (SGD)
by iteratively applying the update

θi+1

θi

α

θiL(θi)

←

−

∇

2

for a learning rate α. In SGD, L(θ) is usually approximated with

¯L(θ) =

(cid:88)

1
S(cid:48)

l(f (xs(cid:48); θ), ys(cid:48)),

s(cid:48)∈S(cid:48)

|

|
S is a mini-batch sampled from S. The choice of α and nS(cid:48) =

where S(cid:48)
presents a trade-off
⊂
between computational efﬁciency and sample efﬁciency. Increasing nS(cid:48) by a factor of k increases the
k, and reduces its variance proportionally to
time needed to calculate
1
k (Bottou et al., 2016). In order to mitigate the increased time per parameter update we can increase
the learning rate to α(cid:48) for some α(cid:48)
α. However there are some limits on the size of the learning
≥
rate, so that in general α(cid:48)
kα (Bottou et al., 2016). The hyper-parameters α(cid:48) and k are chosen to
≤
simultaneously maximize k
k(cid:48) and minimize L.

θ ¯L by a factor of k(cid:48), for k(cid:48)

S(cid:48)
|

∇

≤

|

2.2 MODEL-FREE VALUE BASED METHODS

value

k=0 γkrt+k+1

based methods
(cid:12)
(cid:12)s = st, a = at

=
attempt
Model-free
(cid:3) that gives the expected return achieved by being in
(cid:2) (cid:80)∞
Eπ
state st taking action at and then following the policy π. A policy can be extracted from
the Q-function with π(st) = arg maxa(cid:48) q(st, a(cid:48)). DQN (Mnih et al., 2013) learns a function
Q(s, a; θ)
q(s, a) represented as a convolutional neural network with parameters θ. Model
parameters are updated based on model targets provided by the Bellman equation

the Q-funciton

to model

q(st, at)

≈

q(s, a) = Es(cid:48)

(cid:104)
r + γ max

a(cid:48)

(cid:105)
q(s(cid:48), a(cid:48))
s, a
|

to create the loss function

¯L(θi) = (rt + γ max
a(cid:48)

Q(st+1, a(cid:48); θi)

Q(st, at; θi))2

−

The parameters θ are improved by SGD with the gradient

θiL(θi)

∇

≈ ∇

¯L(θi) =

θi

1
2

−

(cid:0)r + γ max
a(cid:48)

Q(s(cid:48), a(cid:48); θi)

Q(s, a; θi)(cid:1)

−

∇

θiQ(s, a; θi)

(4)

2.3 POLICY GRADIENT METHODS

s; θ). This is
Policy gradient methods (Williams, 1992) directly learn a parametrized policy π(a
|
possible due to the policy gradient theorem (Sutton et al., 1999)

θL(θ) = Es,a

(cid:2)q(s, a)

s; θ)(cid:3),
θ log π(a
|

∇

∇

which provides an unbiased estimate of the gradient of the return with respect to the policy pa-
rameters. Sutton et al. (1999) propose an improvement upon the basic policy gradient update by
replacing the Q function with the advantage function Aπ(s, a) = (q(s, a)
v(s)) where v(s) is the
−
(cid:3). When π is continuously differentiable, θ can
value function given by Eπ
k=0 γkrt+k+1
be optimized via gradient ascent following

(cid:12)
(cid:12)s = st

(cid:2) (cid:80)∞

θL(θ) = Es,a

(cid:2)(cid:0)q(s, a)

v(s)(cid:1)

s; θ)(cid:3)
θ log π(a
|

∇

−

∇

Mnih et al. (2016) learn an estimate V (s; θv)
v(s) of the value function, with both V (s; θv)
≈
and π(a
s; θ) being represented as convolutional neural networks. Additionally, they estimate the
|
Q-function with the n-step return estimate given by

Q(n)(st, at; θ, θv) = rt+1 + ... + γn−1rt+n−1 + γnV (st+n; θv)

with 0 < n

tmax for some ﬁxed tmax. The ﬁnal gradient for the policy network is given by

≤

θL(θ)

∇

≈

(cid:0)Q(n)(st, at; θ, θv)

V (st; θv)(cid:1)

θ log π(at

st; θ)

∇

|

−

3

(2)

(3)

(5)

(6)

(7)

(8)

The gradient for the value network V is given by

θv L(θv)

∇

θv

≈ ∇

(cid:104)(cid:0)Q(n)(st, at; θ, θv)

V (st; θv)(cid:1)2(cid:105)

−

(9)

Samples are generated by having an actor interact with an environment by following the policy π(st)
and observing the next state st+1 and reward rt+1.

3 PARALLEL FRAMEWORK FOR DEEP REINFORCEMENT LEARNING

We propose a general framework for deep reinforcement learning, where multiple actors can be
trained synchronously on a single machine. A set of ne environment instances are maintained,
where actions for all environment instances are generated from the policy. The architecture can be
seen in Figure 1. The policy function can be represented implicitly, as in value based methods, or

Figure 1: Architecture of the Parallel Framework for Deep RL

explicitly as in policy gradient methods. As suggested in Mnih et al. (2016), by having multiple
environments instances in parallel it is likely that they will be exploring different locations of the
state space at any given time, which reduces the correlation of encountered states and helps stabilize
training. This approach can be motivated as an on-line experience memory, where experiences are
sampled from the distribution currently being observed from the environment, instead of sampling
uniformly from previous experience.

At each timestep the master generates actions for all environment instances by sampling the current
policy. Note, that the policy may be sampled differently for each environment. A set of nw workers
then apply all the actions to the their respective environments in parallel, and store the observed
experiences. The master then updates the policy with the observed experiences. This allows the
evaluation and training of the policy to be batched, which can be efﬁciently parallelized, leading to
signiﬁcant speed improvements on modern compute architectures.

4 PARALLEL ADVANTAGE ACTOR CRITIC

We used the proposed framework to implement a version of the n-step advantage actor-critic algo-
rithm proposed by Mnih et al. (2016). This algorithm maintains a policy π(at
st; θ) and an estimate
|
V (st; θv) of the value function, both approximated by deep neural networks. The parameters θ of
the policy network (the actor) are optimized via gradient ascent following

θ log π(at

st; θ)A(st, at; θ, θv) + β
|

∇

∇

θH(π(se,t; θ))

(Sutton et al., 1999), where A(st, at; θ, θv) = Q(n)(st, at; θ, θv)
advantage function, Q(n)(st, at; θ, θv) = (cid:80)n−1

−
k=0 γkrt+k + γnV (st+n; θv), with 0 < n

V (st; θv) is an estimate of the
tmax, is

≤

4

the n-step return estimation and H(π(se,t; θ)) is the entropy of the policy π, which as suggested
by Mnih et al. (2016) is added to improve exploration by discouraging premature convergence to
suboptimal deterministic policies. The parameters θv of value network (the critic) are in turn updated
via gradient descent in the direction of

(cid:104)(cid:0)Q(n)(st, at; θ, θv)

V (st; θv)(cid:1)2(cid:105)

−

θv

∇

In the context of our framework, the above gradients are calculated using mini batches of experi-
ences. For each of the ne environments, tmax experiences are generated, resulting in batches of size
V
ne
tmax. The gradients
θ for the value function thus
take the following form:

π
θ for the policy network and the gradients

∇

∇

·

π
θ ≈

∇

ne

·

1
tmax

ne(cid:88)

tmax(cid:88)

e=1

t=1

(cid:0)Q(tmax−t+1)(se,t, ae,t; θ, θv)

V (se,t; θv)(cid:1)

θ log π(ae,t

se,t; θ)+β

θH(π(se,t; θ))

−

∇

|

∇

(10)

V
θv ≈ ∇

∇

θv

1
tmax

ne(cid:88)

tmax(cid:88)

e=1

t=1

ne

·

(cid:0)Q(tmax−t+1)(se,t, ae,t; θ, θv)

V (se,t; θv)(cid:1)2

(11)

−

Pseudocode for our parallel advantage actor-critic algorithm (PAAC) is given in Algorithm 1. As
shown in the next section, PAAC achieves state of the art performance on the Atari 2600 domain in
half of the time required by GA3C and in only one eigth of the time required by A3C. Note that,
although we implement an actor-critic algorithm, this framework can be used to implement any other
reinforcement learning algorithm.

5 EXPERIMENTS

We evaluated the performance of PAAC in 12 games from Atari 2600 using the Atari Learning
Environment (Bellemare et al., 2013). The agent was developed in Python using TensorFlow (Abadi
et al., 2015) and all performance experiments were run on a computer with a 4 core Intel i7-4790K
processor and an Nvidia GTX 980 Ti GPU.

5.1 EXPERIMENTAL SETUP

To compare results with other algorithms for the Atari domain we follow the same pre-processing
and training procedures as Mnih et al. (2016). Each action is repeated 4 times, and the per-pixel
160
maximum value from the two latest frames is kept. The frame is then scaled down from 210
pixels and 3 color channels to 84
84 pixels and a single color channel for pixel intensity. Whenever
an environment is restarted, the state is reset to the starting state and between 1 and 30 no-op actions
are performed before giving control to the agent. The environment is restarted whenever the ﬁnal
state of the environment is reached.

×

×

5

for t = 1 to tmax do

Algorithm 1 Parallel advantage actor-critic
1: Initialize timestep counter N = 0 and network weights θ, θv
2: Instantiate set e of ne environments
3: repeat
4:
5:
6:
7:
8:
9:
10:
11:

Sample at from π(at|st; θ)
Calculate vt from V (st; θv)
parallel for i = 1 to ne do

Perform action at,i in environment ei
Observe new state st+1,i and reward rt+1,i

end parallel for

end for
Rtmax+1 =

(cid:26) 0

12:

V (stmax+1; θ)

for terminal st
for non-terminal st

for t = tmax down to 1 do
Rt = rt + γRt+1

13:
14:
15:
16:
17:
18:
19:
20: until N ≥ Nmax

1
ne·tmax

end for
dθ =
dθv =
Update θ using dθ and θv using dθv.
N ← N + ne · tmax

(cid:80)ne
i=1
(cid:80)ne
i=1

1
ne·tmax

(cid:80)tmax
t=1 (Rt,i − vt,i)∇θ log π(at,i|st,i; θ) + β∇θH(π(se,t; θ))
(cid:80)tmax
t=1 ∇θv (Rt,i − V (st,i; θv))2

As in (Mnih et al., 2016), a single convolutional network with two separate output layers was used to
jointly model the policy and the value functions. For the policy function, the output is a softmax with
one node per action, while for the value function a single linear output node is used. Moreover, to
compare the efﬁciency of PAAC for different model sizes, we implemented two variants of the policy
and value convolutional network. The ﬁrst variant, referred to as archnips, is the same architecture
used by A3C FF (Mnih et al., 2016), which is a modiﬁed version of the architecture used in Mnih
et al. (2013), adapted to an actor-critic algorithm. The second variant, archnature, is an adaptation
of the architecture presented in Mnih et al. (2015). The networks were trained with RMSProp.
The hyperparameters used to generate the results in Table 1 were nw = 8, ne = 32, tmax = 5,
108, γ = 0.99, α = 0.0224, (cid:15) = 0.1, β = 0.01, and a discount factor of 0.99 for
Nmax = 1.15
RMSProp. Additionally gradient clipping (Pascanu et al., 2012) with a threshold of 40 was used.

×

5.2 RESULTS

The performance of PAAC with archnips and archnature was evaluated on twelve different Atari 2600
games, where agents were trained for 115 million skipped frames (460 million actual frames).
The results and their comparison to Gorila (Nair et al., 2015), A3C (Mnih et al., 2016) and
GA3C (Babaeizadeh et al., 2016) are presented in Table 1. After a few hours of training on a
single computer, PAAC is able to outperform Gorila in 8 games, and A3C FF in 8 games. Of the
9 games used to test GA3C, PAAC matches its performance in 2 of them and surpasses it in the
remaining 7.

Figure 2: Time usage in the game of Pong for different ne.

To better understand the effect of the number of actors (and batch size) on the score, tests were
run with ne
. The learning rate was not tuned for each batch size, and was
}

16, 32, 64, 128, 256

∈ {

6

Game

Gorila

A3C FF

GA3C

PAAC archnips

PAAC archnature

Amidar
Centipede
Beam Rider
Boxing
Breakout
Ms. Pacman
Name This Game
Pong
Qbert
Seaquest
Space Invaders
Up n Down

1189.70
8432.30
3302.9
94.9
402.2
3233.50
6182.16
18.3
10815.6
13169.06
1883.4
12561.58

263.9
3755.8
22707.9
59.8
681.9
653.7
10476.1
5.6
15148.8
2355.4
15730.5
74705.7

218
7386
N/A
92
N/A
1978
5643
18
14966.0
1706
N/A
8623

701.8
5747.32
4062.0
99.6
470.1
2194.7
9743.7
20.6
16561.7
1754.0
1077.3
88105.3

Training

4d CPU cluster

4d CPU

1d GPU

12h GPU

1348.3
7368.1
6844.0
99.8
565.3
1976.0
14068.0
20.9
17249.2
1755.3
1427.8
100523.3

15h GPU

Table 1: Scores are measured from the best per- forming actor out of three, and averaged over 30
runs with upto 30 no-op actions start condition. Results for A3C FF use human start condition are
therefore not directly comparable. Gorila scores taken from Nair et al. (2015), A3C FF scores taken
from Mnih et al. (2016) and GA3C scores take from Babaeizadeh et al. (2016). Unavailable results
are shown as N/A.

·

ne for all runs across all games. Increasing ne decreases the frequency of pa-
chosen to be 0.0007
rameter updates, given that parameter updates are performed every ne
tmax timesteps. As the theory
suggests, the decreased frequency in parameter updates can be offset by increasing the learning rate.
As can be seen in Figure 3 most choices of ne result in similar scores at a given timestep, however
Figure 4 shows that higher values of ne reach those timesteps signiﬁcantly faster. The choice of
ne = 256 results in divergence in three out of the four games, which shows that the learning rate
can be increased proportional to the batch size, until a certain limit is reached. A limiting factor

·

Figure 3: Score comparison for PAAC on six Atari 2600 games for different ne, where one training
epoch is equivalent to 1 million timesteps (4 million skipped frames).

in the speed of training is the time spent in agent-environment interaction. When using archnips for
ne = 32 approximately 50% of the time is spent interacting with the environment, while only 37%
is used for learning and action selection, as is shown in Figure 2. This has strong implications for the
models and environments that can be used. Using a model-environment combination that doubles
the time needed for learning and action calculation would lead to a mere 37% increase in training
time. This can be seen in Figure 2 where using archnature on the GPU leads to a drop in timesteps per
second 22% for ne = 32 when compared to archnips. When running on the CPU however this leads
to a 41% drop in timesteps per second.

7

Figure 4: Training time and score comparison for PAAC on six Atari 2600 games for different ne.

6 CONCLUSION

In this work, we have introduced a parallel framework for deep reinforcement learning that can be
efﬁciently parallelized on a GPU. The framework is ﬂexible, and can be used for on-policy and off-
policy, as well as value based and policy gradient based algorithms. The presented implementation
of the framework is able to reduce training time for the Atari 2600 domain to a few hours, while
maintaining state-of-the-art performance. Improvements in training time, will allow the application
of these algorithms to more demanding environments, and the use of more powerful models.

REFERENCES

Abadi, Martın, Agarwal, Ashish, Barham, Paul, Brevdo, Eugene, Chen, Zhifeng, Citro, Craig, Cor-
rado, Greg S, Davis, Andy, Dean, Jeffrey, Devin, Matthieu, et al. Tensorﬂow: Large-scale machine
learning on heterogeneous systems, 2015. Software available from tensorﬂow. org, 1, 2015.

Babaeizadeh, M., Frosio, I., Tyree, S., Clemons, J., and Kautz, J. GA3C: GPU-based A3C for Deep

Reinforcement Learning. ArXiv e-prints, November 2016.

Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An
evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279,
06 2013.

Bottou, L., Curtis, F. E., and Nocedal, J. Optimization Methods for Large-Scale Machine Learning.

ArXiv e-prints, June 2016.

Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M.

Playing Atari with Deep Reinforcement Learning. ArXiv e-prints, December 2013.

Mnih, V., Puigdom`enech Badia, A., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., Silver, D.,
and Kavukcuoglu, K. Asynchronous Methods for Deep Reinforcement Learning. ArXiv e-prints,
February 2016.

Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A, Veness, Joel, Bellemare,
Marc G, Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K, Ostrovski, Georg, et al. Human-
level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.

Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De Maria, A., Panneershelvam, V.,
Suleyman, M., Beattie, C., Petersen, S., Legg, S., Mnih, V., Kavukcuoglu, K., and Silver, D.
Massively Parallel Methods for Deep Reinforcement Learning. ArXiv e-prints, July 2015.

8

Pascanu, R., Mikolov, T., and Bengio, Y. On the difﬁculty of training Recurrent Neural Networks.

ArXiv e-prints, November 2012.

Recht, Benjamin, Re, Christopher, Wright, Stephen, and Niu, Feng. Hogwild: A lock-free approach
to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Sys-
tems, pp. 693–701, 2011.

Sutton, Richard S, McAllester, David A, Singh, Satinder P, Mansour, Yishay, et al. Policy gradient
methods for reinforcement learning with function approximation. In NIPS, volume 99, pp. 1057–
1063, 1999.

Williams, Ronald J. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine learning, 8(3-4):229–256, 1992.

9

7
1
0
2
 
y
a
M
 
6
1
 
 
]

G
L
.
s
c
[
 
 
2
v
2
6
8
4
0
.
5
0
7
1
:
v
i
X
r
a

EFFICIENT PARALLEL METHODS FOR DEEP REIN-
FORCEMENT LEARNING

Alfredo V. Clemente
Department of Computer and Information Science
Norwegian University of Science and Technology
Trondheim, Norway
alfredvc@stud.ntnu.no

Humberto N. Castej´on
Telenor Research
Trondheim, Norway
humberto.castejon@telenor.com

Arjun Chandra
Telenor Research
Trondheim, Norway
arjun.chandra@telenor.com

ABSTRACT

We propose a novel framework for efﬁcient parallelization of deep reinforce-
ment learning algorithms, enabling these algorithms to learn from multiple ac-
tors on a single machine. The framework is algorithm agnostic and can be ap-
plied to on-policy, off-policy, value based and policy gradient based algorithms.
Given its inherent parallelism, the framework can be efﬁciently implemented
on a GPU, allowing the usage of powerful models while signiﬁcantly reduc-
ing training time. We demonstrate the effectiveness of our framework by im-
plementing an advantage actor-critic algorithm on a GPU, using on-policy ex-
periences and employing synchronous updates. Our algorithm achieves state-
of-the-art performance on the Atari domain after only a few hours of training.
Our framework thus opens the door for much faster experimentation on demand-
ing problem domains. Our implementation is open-source and is made public at
https://github.com/alfredvc/paac.

1

INTRODUCTION AND RELATED WORK

Incorporating deep learning models within reinforcement learning (RL) presents some challenges.
Standard stochastic gradient descent algorithms for training deep learning models assume all train-
ing examples to be independent and identically distributed (i.i.d.). This constraint is often violated
by RL agents, given the high correlation between encountered states. Additionally, when learning
on-policy, the policy affects the distribution of encountered states, which in turn affects the policy,
creating a feedback loop that may lead to divergence (Mnih et al., 2013). Two main attempts have
been made to solve these issues. One is to store experiences in a large replay memory and em-
ploy off-policy RL methods (Mnih et al., 2013). Sampling the replay memory can break the state
correlations, thus reducing the effect of the feedback loop. Another is to execute multiple asyn-
chronous agents in parallel, each interacting with an instance of the environment independently of
each other (Mnih et al., 2016). Both of these approaches suffer from different drawbacks; experi-
ence replay can only be employed with off-policy methods, while asynchronous agents can perform
inconsistent parameter updates due to stale gradients1 and simultaneous parameter updates from
different threads.

Parallel and distributed compute architectures have motivated innovative modiﬁcations to existing
RL algorithms to efﬁciently make use of parallel execution. In the General Reinforcement Learning
Architecture (Gorila) (Nair et al., 2015), the DQN (Mnih et al., 2015) algorithm is distributed across
multiple machines. Multiple learners learn off-policy using experiences collected into a common
replay memory by multiple actors. Gorila is shown to outperform standard DQN in a variety of

1Gradients may be computed w.r.t. stale parameters while updates applied to a new parameter set.

1

Atari games, while only training for 4 days. The distribution of the learning process is further ex-
plored in (Mnih et al., 2016), where multiple actor-learners are executed asynchronously on a single
machine. Each actor-learner holds its own copy of the policy/value function and interacts with its
own instance of the environment. This allows for both off-policy, as well as on-policy learning.
The actor-learners compute gradients in parallel and update shared parameters asynchronously in
a HOGWILD! (Recht et al., 2011) fashion. The authors suggest that multiple agents collecting in-
dependent experiences from their own environment instances reduces correlation between samples,
thereby improving learning. The asynchronous advantage actor-critic (A3C) algorithm (Mnih et al.,
2016) was able to surpass the state of the art on the Atari domain at the time of publication, while
training for 4 days on a single machine with 16 CPU cores. GA3C (Babaeizadeh et al., 2016) is a
GPU implementation of A3C. It batches action selection and learning using queues. Actors sample
from a shared policy by submitting a task to a predictor, which executes the policy and returns an
action once it has accumulated enough tasks. Similarly, learning is performed by submitting expe-
riences to a trainer, which computes gradients and applies updates once enough experiences have
been gathered. If the training queue is not empty when the model is updated, the learning will no
longer be on-policy, since the remaining experiences were generated by an old policy. This leads
to instabilities during training, which the authors address with a slight modiﬁcation to the weight
updates.

We propose a novel framework for efﬁcient parallelization of deep reinforcement learning algo-
rithms, which keeps the strengths of the aforementioned approaches, while alleviating their weak-
nesses. Algorithms based on this framework can learn from hundreds of actors in parallel, similar
to Gorila, while running on a single machine like A3C and GA3C. Having multiple actors help
decorrelate encountered states and attenuate feedback loops, while allowing us to leverage the par-
allel architecture of modern CPUs and GPUs. Unlike A3C and Gorila, there is only one copy of
the parameters, hence parameter updates are performed synchronously, thus avoiding the possible
drawbacks related to asynchronous updates. Our framework has many similarities to GA3C. How-
ever, the absence of queues allows for a much more simpler and computationally efﬁcient solution,
while allowing for true on-policy learning and faster convergence to optimal policies. We demon-
strate our framework with a Parallel Advantage Actor-Critic algorithm, that achieves state of the art
performance in the Atari 2600 domain after only a few hours of training. This opens the door for
much faster experimentation.

2 BACKGROUND

Reinforcement learning algorithms attempt to learn a policy π that maps states to actions, in order
(cid:3) for some discount
to maximize the expected sum of cumulative rewards Rt = Eπ
factor 0 < γ < 1, where rt is the reward observed at timestep t. Current reinforcement learning
algorithms represent the learned policy π as a neural network, either implicitly with a value function
or explicitly as a policy function.

k=0 γkrt+k

(cid:2) (cid:80)∞

2.1 BATCHING WITH STOCHASTIC GRADIENT DESCENT

Current reinforcement learning algorithms make heavy use of deep neural networks, both to extract
high level features from the observations it receives, and as function approximators to represent its
policy or value functions.
Consider the set of input-target pairs S =
(x0, y0), (x1, y1), ...(xn, yn)
generated by some func-
{
}
tion f ∗(x). The goal of supervised learning with neural networks is to learn a parametrized function
. The performance of f is evaluated with the empirical
f (x; θ) that best approximates function f
∗
loss

L(θ) =

l(f (xs; θ), ys)

(1)

(cid:88)

1
S

|

|

s∈S

where l(f (xs; θ), ys) is referred to as a loss function, and gives a quantitative measure of how good
f is at modelling f ∗. The model parameters θ are learned with stochastic gradient descent (SGD)
by iteratively applying the update

θi+1

θi

α

θiL(θi)

←

−

∇

2

for a learning rate α. In SGD, L(θ) is usually approximated with

¯L(θ) =

(cid:88)

1
S(cid:48)

l(f (xs(cid:48); θ), ys(cid:48)),

s(cid:48)∈S(cid:48)

|

|
S is a mini-batch sampled from S. The choice of α and nS(cid:48) =

where S(cid:48)
presents a trade-off
⊂
between computational efﬁciency and sample efﬁciency. Increasing nS(cid:48) by a factor of k increases the
k, and reduces its variance proportionally to
time needed to calculate
1
k (Bottou et al., 2016). In order to mitigate the increased time per parameter update we can increase
the learning rate to α(cid:48) for some α(cid:48)
α. However there are some limits on the size of the learning
≥
rate, so that in general α(cid:48)
kα (Bottou et al., 2016). The hyper-parameters α(cid:48) and k are chosen to
≤
simultaneously maximize k
k(cid:48) and minimize L.

θ ¯L by a factor of k(cid:48), for k(cid:48)

S(cid:48)
|

∇

≤

|

2.2 MODEL-FREE VALUE BASED METHODS

value

k=0 γkrt+k+1

based methods
(cid:12)
(cid:12)s = st, a = at

=
attempt
Model-free
(cid:3) that gives the expected return achieved by being in
(cid:2) (cid:80)∞
Eπ
state st taking action at and then following the policy π. A policy can be extracted from
the Q-function with π(st) = arg maxa(cid:48) q(st, a(cid:48)). DQN (Mnih et al., 2013) learns a function
Q(s, a; θ)
q(s, a) represented as a convolutional neural network with parameters θ. Model
parameters are updated based on model targets provided by the Bellman equation

the Q-funciton

to model

q(st, at)

≈

q(s, a) = Es(cid:48)

(cid:104)
r + γ max

a(cid:48)

(cid:105)
q(s(cid:48), a(cid:48))
s, a
|

to create the loss function

¯L(θi) = (rt + γ max
a(cid:48)

Q(st+1, a(cid:48); θi)

Q(st, at; θi))2

−

The parameters θ are improved by SGD with the gradient

θiL(θi)

∇

≈ ∇

¯L(θi) =

θi

1
2

−

(cid:0)r + γ max
a(cid:48)

Q(s(cid:48), a(cid:48); θi)

Q(s, a; θi)(cid:1)

−

∇

θiQ(s, a; θi)

(4)

2.3 POLICY GRADIENT METHODS

s; θ). This is
Policy gradient methods (Williams, 1992) directly learn a parametrized policy π(a
|
possible due to the policy gradient theorem (Sutton et al., 1999)

θL(θ) = Es,a

(cid:2)q(s, a)

s; θ)(cid:3),
θ log π(a
|

∇

∇

which provides an unbiased estimate of the gradient of the return with respect to the policy pa-
rameters. Sutton et al. (1999) propose an improvement upon the basic policy gradient update by
replacing the Q function with the advantage function Aπ(s, a) = (q(s, a)
v(s)) where v(s) is the
−
(cid:3). When π is continuously differentiable, θ can
value function given by Eπ
k=0 γkrt+k+1
be optimized via gradient ascent following

(cid:12)
(cid:12)s = st

(cid:2) (cid:80)∞

θL(θ) = Es,a

(cid:2)(cid:0)q(s, a)

v(s)(cid:1)

s; θ)(cid:3)
θ log π(a
|

∇

−

∇

Mnih et al. (2016) learn an estimate V (s; θv)
v(s) of the value function, with both V (s; θv)
≈
and π(a
s; θ) being represented as convolutional neural networks. Additionally, they estimate the
|
Q-function with the n-step return estimate given by

Q(n)(st, at; θ, θv) = rt+1 + ... + γn−1rt+n−1 + γnV (st+n; θv)

with 0 < n

tmax for some ﬁxed tmax. The ﬁnal gradient for the policy network is given by

≤

θL(θ)

∇

≈

(cid:0)Q(n)(st, at; θ, θv)

V (st; θv)(cid:1)

θ log π(at

st; θ)

∇

|

−

3

(2)

(3)

(5)

(6)

(7)

(8)

The gradient for the value network V is given by

θv L(θv)

∇

θv

≈ ∇

(cid:104)(cid:0)Q(n)(st, at; θ, θv)

V (st; θv)(cid:1)2(cid:105)

−

(9)

Samples are generated by having an actor interact with an environment by following the policy π(st)
and observing the next state st+1 and reward rt+1.

3 PARALLEL FRAMEWORK FOR DEEP REINFORCEMENT LEARNING

We propose a general framework for deep reinforcement learning, where multiple actors can be
trained synchronously on a single machine. A set of ne environment instances are maintained,
where actions for all environment instances are generated from the policy. The architecture can be
seen in Figure 1. The policy function can be represented implicitly, as in value based methods, or

Figure 1: Architecture of the Parallel Framework for Deep RL

explicitly as in policy gradient methods. As suggested in Mnih et al. (2016), by having multiple
environments instances in parallel it is likely that they will be exploring different locations of the
state space at any given time, which reduces the correlation of encountered states and helps stabilize
training. This approach can be motivated as an on-line experience memory, where experiences are
sampled from the distribution currently being observed from the environment, instead of sampling
uniformly from previous experience.

At each timestep the master generates actions for all environment instances by sampling the current
policy. Note, that the policy may be sampled differently for each environment. A set of nw workers
then apply all the actions to the their respective environments in parallel, and store the observed
experiences. The master then updates the policy with the observed experiences. This allows the
evaluation and training of the policy to be batched, which can be efﬁciently parallelized, leading to
signiﬁcant speed improvements on modern compute architectures.

4 PARALLEL ADVANTAGE ACTOR CRITIC

We used the proposed framework to implement a version of the n-step advantage actor-critic algo-
rithm proposed by Mnih et al. (2016). This algorithm maintains a policy π(at
st; θ) and an estimate
|
V (st; θv) of the value function, both approximated by deep neural networks. The parameters θ of
the policy network (the actor) are optimized via gradient ascent following

θ log π(at

st; θ)A(st, at; θ, θv) + β
|

∇

∇

θH(π(se,t; θ))

(Sutton et al., 1999), where A(st, at; θ, θv) = Q(n)(st, at; θ, θv)
advantage function, Q(n)(st, at; θ, θv) = (cid:80)n−1

−
k=0 γkrt+k + γnV (st+n; θv), with 0 < n

V (st; θv) is an estimate of the
tmax, is

≤

4

the n-step return estimation and H(π(se,t; θ)) is the entropy of the policy π, which as suggested
by Mnih et al. (2016) is added to improve exploration by discouraging premature convergence to
suboptimal deterministic policies. The parameters θv of value network (the critic) are in turn updated
via gradient descent in the direction of

(cid:104)(cid:0)Q(n)(st, at; θ, θv)

V (st; θv)(cid:1)2(cid:105)

−

θv

∇

In the context of our framework, the above gradients are calculated using mini batches of experi-
ences. For each of the ne environments, tmax experiences are generated, resulting in batches of size
V
ne
tmax. The gradients
θ for the value function thus
take the following form:

π
θ for the policy network and the gradients

∇

∇

·

π
θ ≈

∇

ne

·

1
tmax

ne(cid:88)

tmax(cid:88)

e=1

t=1

(cid:0)Q(tmax−t+1)(se,t, ae,t; θ, θv)

V (se,t; θv)(cid:1)

θ log π(ae,t

se,t; θ)+β

θH(π(se,t; θ))

−

∇

|

∇

(10)

V
θv ≈ ∇

∇

θv

1
tmax

ne(cid:88)

tmax(cid:88)

e=1

t=1

ne

·

(cid:0)Q(tmax−t+1)(se,t, ae,t; θ, θv)

V (se,t; θv)(cid:1)2

(11)

−

Pseudocode for our parallel advantage actor-critic algorithm (PAAC) is given in Algorithm 1. As
shown in the next section, PAAC achieves state of the art performance on the Atari 2600 domain in
half of the time required by GA3C and in only one eigth of the time required by A3C. Note that,
although we implement an actor-critic algorithm, this framework can be used to implement any other
reinforcement learning algorithm.

5 EXPERIMENTS

We evaluated the performance of PAAC in 12 games from Atari 2600 using the Atari Learning
Environment (Bellemare et al., 2013). The agent was developed in Python using TensorFlow (Abadi
et al., 2015) and all performance experiments were run on a computer with a 4 core Intel i7-4790K
processor and an Nvidia GTX 980 Ti GPU.

5.1 EXPERIMENTAL SETUP

To compare results with other algorithms for the Atari domain we follow the same pre-processing
and training procedures as Mnih et al. (2016). Each action is repeated 4 times, and the per-pixel
160
maximum value from the two latest frames is kept. The frame is then scaled down from 210
pixels and 3 color channels to 84
84 pixels and a single color channel for pixel intensity. Whenever
an environment is restarted, the state is reset to the starting state and between 1 and 30 no-op actions
are performed before giving control to the agent. The environment is restarted whenever the ﬁnal
state of the environment is reached.

×

×

5

for t = 1 to tmax do

Algorithm 1 Parallel advantage actor-critic
1: Initialize timestep counter N = 0 and network weights θ, θv
2: Instantiate set e of ne environments
3: repeat
4:
5:
6:
7:
8:
9:
10:
11:

Sample at from π(at|st; θ)
Calculate vt from V (st; θv)
parallel for i = 1 to ne do

Perform action at,i in environment ei
Observe new state st+1,i and reward rt+1,i

end parallel for

end for
Rtmax+1 =

(cid:26) 0

12:

V (stmax+1; θ)

for terminal st
for non-terminal st

for t = tmax down to 1 do
Rt = rt + γRt+1

13:
14:
15:
16:
17:
18:
19:
20: until N ≥ Nmax

1
ne·tmax

end for
dθ =
dθv =
Update θ using dθ and θv using dθv.
N ← N + ne · tmax

(cid:80)ne
i=1
(cid:80)ne
i=1

1
ne·tmax

(cid:80)tmax
t=1 (Rt,i − vt,i)∇θ log π(at,i|st,i; θ) + β∇θH(π(se,t; θ))
(cid:80)tmax
t=1 ∇θv (Rt,i − V (st,i; θv))2

As in (Mnih et al., 2016), a single convolutional network with two separate output layers was used to
jointly model the policy and the value functions. For the policy function, the output is a softmax with
one node per action, while for the value function a single linear output node is used. Moreover, to
compare the efﬁciency of PAAC for different model sizes, we implemented two variants of the policy
and value convolutional network. The ﬁrst variant, referred to as archnips, is the same architecture
used by A3C FF (Mnih et al., 2016), which is a modiﬁed version of the architecture used in Mnih
et al. (2013), adapted to an actor-critic algorithm. The second variant, archnature, is an adaptation
of the architecture presented in Mnih et al. (2015). The networks were trained with RMSProp.
The hyperparameters used to generate the results in Table 1 were nw = 8, ne = 32, tmax = 5,
108, γ = 0.99, α = 0.0224, (cid:15) = 0.1, β = 0.01, and a discount factor of 0.99 for
Nmax = 1.15
RMSProp. Additionally gradient clipping (Pascanu et al., 2012) with a threshold of 40 was used.

×

5.2 RESULTS

The performance of PAAC with archnips and archnature was evaluated on twelve different Atari 2600
games, where agents were trained for 115 million skipped frames (460 million actual frames).
The results and their comparison to Gorila (Nair et al., 2015), A3C (Mnih et al., 2016) and
GA3C (Babaeizadeh et al., 2016) are presented in Table 1. After a few hours of training on a
single computer, PAAC is able to outperform Gorila in 8 games, and A3C FF in 8 games. Of the
9 games used to test GA3C, PAAC matches its performance in 2 of them and surpasses it in the
remaining 7.

Figure 2: Time usage in the game of Pong for different ne.

To better understand the effect of the number of actors (and batch size) on the score, tests were
run with ne
. The learning rate was not tuned for each batch size, and was
}

16, 32, 64, 128, 256

∈ {

6

Game

Gorila

A3C FF

GA3C

PAAC archnips

PAAC archnature

Amidar
Centipede
Beam Rider
Boxing
Breakout
Ms. Pacman
Name This Game
Pong
Qbert
Seaquest
Space Invaders
Up n Down

1189.70
8432.30
3302.9
94.9
402.2
3233.50
6182.16
18.3
10815.6
13169.06
1883.4
12561.58

263.9
3755.8
22707.9
59.8
681.9
653.7
10476.1
5.6
15148.8
2355.4
15730.5
74705.7

218
7386
N/A
92
N/A
1978
5643
18
14966.0
1706
N/A
8623

701.8
5747.32
4062.0
99.6
470.1
2194.7
9743.7
20.6
16561.7
1754.0
1077.3
88105.3

Training

4d CPU cluster

4d CPU

1d GPU

12h GPU

1348.3
7368.1
6844.0
99.8
565.3
1976.0
14068.0
20.9
17249.2
1755.3
1427.8
100523.3

15h GPU

Table 1: Scores are measured from the best per- forming actor out of three, and averaged over 30
runs with upto 30 no-op actions start condition. Results for A3C FF use human start condition are
therefore not directly comparable. Gorila scores taken from Nair et al. (2015), A3C FF scores taken
from Mnih et al. (2016) and GA3C scores take from Babaeizadeh et al. (2016). Unavailable results
are shown as N/A.

·

ne for all runs across all games. Increasing ne decreases the frequency of pa-
chosen to be 0.0007
rameter updates, given that parameter updates are performed every ne
tmax timesteps. As the theory
suggests, the decreased frequency in parameter updates can be offset by increasing the learning rate.
As can be seen in Figure 3 most choices of ne result in similar scores at a given timestep, however
Figure 4 shows that higher values of ne reach those timesteps signiﬁcantly faster. The choice of
ne = 256 results in divergence in three out of the four games, which shows that the learning rate
can be increased proportional to the batch size, until a certain limit is reached. A limiting factor

·

Figure 3: Score comparison for PAAC on six Atari 2600 games for different ne, where one training
epoch is equivalent to 1 million timesteps (4 million skipped frames).

in the speed of training is the time spent in agent-environment interaction. When using archnips for
ne = 32 approximately 50% of the time is spent interacting with the environment, while only 37%
is used for learning and action selection, as is shown in Figure 2. This has strong implications for the
models and environments that can be used. Using a model-environment combination that doubles
the time needed for learning and action calculation would lead to a mere 37% increase in training
time. This can be seen in Figure 2 where using archnature on the GPU leads to a drop in timesteps per
second 22% for ne = 32 when compared to archnips. When running on the CPU however this leads
to a 41% drop in timesteps per second.

7

Figure 4: Training time and score comparison for PAAC on six Atari 2600 games for different ne.

6 CONCLUSION

In this work, we have introduced a parallel framework for deep reinforcement learning that can be
efﬁciently parallelized on a GPU. The framework is ﬂexible, and can be used for on-policy and off-
policy, as well as value based and policy gradient based algorithms. The presented implementation
of the framework is able to reduce training time for the Atari 2600 domain to a few hours, while
maintaining state-of-the-art performance. Improvements in training time, will allow the application
of these algorithms to more demanding environments, and the use of more powerful models.

REFERENCES

Abadi, Martın, Agarwal, Ashish, Barham, Paul, Brevdo, Eugene, Chen, Zhifeng, Citro, Craig, Cor-
rado, Greg S, Davis, Andy, Dean, Jeffrey, Devin, Matthieu, et al. Tensorﬂow: Large-scale machine
learning on heterogeneous systems, 2015. Software available from tensorﬂow. org, 1, 2015.

Babaeizadeh, M., Frosio, I., Tyree, S., Clemons, J., and Kautz, J. GA3C: GPU-based A3C for Deep

Reinforcement Learning. ArXiv e-prints, November 2016.

Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An
evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279,
06 2013.

Bottou, L., Curtis, F. E., and Nocedal, J. Optimization Methods for Large-Scale Machine Learning.

ArXiv e-prints, June 2016.

Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M.

Playing Atari with Deep Reinforcement Learning. ArXiv e-prints, December 2013.

Mnih, V., Puigdom`enech Badia, A., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., Silver, D.,
and Kavukcuoglu, K. Asynchronous Methods for Deep Reinforcement Learning. ArXiv e-prints,
February 2016.

Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A, Veness, Joel, Bellemare,
Marc G, Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K, Ostrovski, Georg, et al. Human-
level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.

Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De Maria, A., Panneershelvam, V.,
Suleyman, M., Beattie, C., Petersen, S., Legg, S., Mnih, V., Kavukcuoglu, K., and Silver, D.
Massively Parallel Methods for Deep Reinforcement Learning. ArXiv e-prints, July 2015.

8

Pascanu, R., Mikolov, T., and Bengio, Y. On the difﬁculty of training Recurrent Neural Networks.

ArXiv e-prints, November 2012.

Recht, Benjamin, Re, Christopher, Wright, Stephen, and Niu, Feng. Hogwild: A lock-free approach
to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Sys-
tems, pp. 693–701, 2011.

Sutton, Richard S, McAllester, David A, Singh, Satinder P, Mansour, Yishay, et al. Policy gradient
methods for reinforcement learning with function approximation. In NIPS, volume 99, pp. 1057–
1063, 1999.

Williams, Ronald J. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine learning, 8(3-4):229–256, 1992.

9

7
1
0
2
 
y
a
M
 
6
1
 
 
]

G
L
.
s
c
[
 
 
2
v
2
6
8
4
0
.
5
0
7
1
:
v
i
X
r
a

EFFICIENT PARALLEL METHODS FOR DEEP REIN-
FORCEMENT LEARNING

Alfredo V. Clemente
Department of Computer and Information Science
Norwegian University of Science and Technology
Trondheim, Norway
alfredvc@stud.ntnu.no

Humberto N. Castej´on
Telenor Research
Trondheim, Norway
humberto.castejon@telenor.com

Arjun Chandra
Telenor Research
Trondheim, Norway
arjun.chandra@telenor.com

ABSTRACT

We propose a novel framework for efﬁcient parallelization of deep reinforce-
ment learning algorithms, enabling these algorithms to learn from multiple ac-
tors on a single machine. The framework is algorithm agnostic and can be ap-
plied to on-policy, off-policy, value based and policy gradient based algorithms.
Given its inherent parallelism, the framework can be efﬁciently implemented
on a GPU, allowing the usage of powerful models while signiﬁcantly reduc-
ing training time. We demonstrate the effectiveness of our framework by im-
plementing an advantage actor-critic algorithm on a GPU, using on-policy ex-
periences and employing synchronous updates. Our algorithm achieves state-
of-the-art performance on the Atari domain after only a few hours of training.
Our framework thus opens the door for much faster experimentation on demand-
ing problem domains. Our implementation is open-source and is made public at
https://github.com/alfredvc/paac.

1

INTRODUCTION AND RELATED WORK

Incorporating deep learning models within reinforcement learning (RL) presents some challenges.
Standard stochastic gradient descent algorithms for training deep learning models assume all train-
ing examples to be independent and identically distributed (i.i.d.). This constraint is often violated
by RL agents, given the high correlation between encountered states. Additionally, when learning
on-policy, the policy affects the distribution of encountered states, which in turn affects the policy,
creating a feedback loop that may lead to divergence (Mnih et al., 2013). Two main attempts have
been made to solve these issues. One is to store experiences in a large replay memory and em-
ploy off-policy RL methods (Mnih et al., 2013). Sampling the replay memory can break the state
correlations, thus reducing the effect of the feedback loop. Another is to execute multiple asyn-
chronous agents in parallel, each interacting with an instance of the environment independently of
each other (Mnih et al., 2016). Both of these approaches suffer from different drawbacks; experi-
ence replay can only be employed with off-policy methods, while asynchronous agents can perform
inconsistent parameter updates due to stale gradients1 and simultaneous parameter updates from
different threads.

Parallel and distributed compute architectures have motivated innovative modiﬁcations to existing
RL algorithms to efﬁciently make use of parallel execution. In the General Reinforcement Learning
Architecture (Gorila) (Nair et al., 2015), the DQN (Mnih et al., 2015) algorithm is distributed across
multiple machines. Multiple learners learn off-policy using experiences collected into a common
replay memory by multiple actors. Gorila is shown to outperform standard DQN in a variety of

1Gradients may be computed w.r.t. stale parameters while updates applied to a new parameter set.

1

Atari games, while only training for 4 days. The distribution of the learning process is further ex-
plored in (Mnih et al., 2016), where multiple actor-learners are executed asynchronously on a single
machine. Each actor-learner holds its own copy of the policy/value function and interacts with its
own instance of the environment. This allows for both off-policy, as well as on-policy learning.
The actor-learners compute gradients in parallel and update shared parameters asynchronously in
a HOGWILD! (Recht et al., 2011) fashion. The authors suggest that multiple agents collecting in-
dependent experiences from their own environment instances reduces correlation between samples,
thereby improving learning. The asynchronous advantage actor-critic (A3C) algorithm (Mnih et al.,
2016) was able to surpass the state of the art on the Atari domain at the time of publication, while
training for 4 days on a single machine with 16 CPU cores. GA3C (Babaeizadeh et al., 2016) is a
GPU implementation of A3C. It batches action selection and learning using queues. Actors sample
from a shared policy by submitting a task to a predictor, which executes the policy and returns an
action once it has accumulated enough tasks. Similarly, learning is performed by submitting expe-
riences to a trainer, which computes gradients and applies updates once enough experiences have
been gathered. If the training queue is not empty when the model is updated, the learning will no
longer be on-policy, since the remaining experiences were generated by an old policy. This leads
to instabilities during training, which the authors address with a slight modiﬁcation to the weight
updates.

We propose a novel framework for efﬁcient parallelization of deep reinforcement learning algo-
rithms, which keeps the strengths of the aforementioned approaches, while alleviating their weak-
nesses. Algorithms based on this framework can learn from hundreds of actors in parallel, similar
to Gorila, while running on a single machine like A3C and GA3C. Having multiple actors help
decorrelate encountered states and attenuate feedback loops, while allowing us to leverage the par-
allel architecture of modern CPUs and GPUs. Unlike A3C and Gorila, there is only one copy of
the parameters, hence parameter updates are performed synchronously, thus avoiding the possible
drawbacks related to asynchronous updates. Our framework has many similarities to GA3C. How-
ever, the absence of queues allows for a much more simpler and computationally efﬁcient solution,
while allowing for true on-policy learning and faster convergence to optimal policies. We demon-
strate our framework with a Parallel Advantage Actor-Critic algorithm, that achieves state of the art
performance in the Atari 2600 domain after only a few hours of training. This opens the door for
much faster experimentation.

2 BACKGROUND

Reinforcement learning algorithms attempt to learn a policy π that maps states to actions, in order
(cid:3) for some discount
to maximize the expected sum of cumulative rewards Rt = Eπ
factor 0 < γ < 1, where rt is the reward observed at timestep t. Current reinforcement learning
algorithms represent the learned policy π as a neural network, either implicitly with a value function
or explicitly as a policy function.

k=0 γkrt+k

(cid:2) (cid:80)∞

2.1 BATCHING WITH STOCHASTIC GRADIENT DESCENT

Current reinforcement learning algorithms make heavy use of deep neural networks, both to extract
high level features from the observations it receives, and as function approximators to represent its
policy or value functions.
Consider the set of input-target pairs S =
(x0, y0), (x1, y1), ...(xn, yn)
generated by some func-
{
}
tion f ∗(x). The goal of supervised learning with neural networks is to learn a parametrized function
. The performance of f is evaluated with the empirical
f (x; θ) that best approximates function f
∗
loss

L(θ) =

l(f (xs; θ), ys)

(1)

(cid:88)

1
S

|

|

s∈S

where l(f (xs; θ), ys) is referred to as a loss function, and gives a quantitative measure of how good
f is at modelling f ∗. The model parameters θ are learned with stochastic gradient descent (SGD)
by iteratively applying the update

θi+1

θi

α

θiL(θi)

←

−

∇

2

for a learning rate α. In SGD, L(θ) is usually approximated with

¯L(θ) =

(cid:88)

1
S(cid:48)

l(f (xs(cid:48); θ), ys(cid:48)),

s(cid:48)∈S(cid:48)

|

|
S is a mini-batch sampled from S. The choice of α and nS(cid:48) =

where S(cid:48)
presents a trade-off
⊂
between computational efﬁciency and sample efﬁciency. Increasing nS(cid:48) by a factor of k increases the
k, and reduces its variance proportionally to
time needed to calculate
1
k (Bottou et al., 2016). In order to mitigate the increased time per parameter update we can increase
the learning rate to α(cid:48) for some α(cid:48)
α. However there are some limits on the size of the learning
≥
rate, so that in general α(cid:48)
kα (Bottou et al., 2016). The hyper-parameters α(cid:48) and k are chosen to
≤
simultaneously maximize k
k(cid:48) and minimize L.

θ ¯L by a factor of k(cid:48), for k(cid:48)

S(cid:48)
|

∇

≤

|

2.2 MODEL-FREE VALUE BASED METHODS

value

k=0 γkrt+k+1

based methods
(cid:12)
(cid:12)s = st, a = at

=
attempt
Model-free
(cid:3) that gives the expected return achieved by being in
(cid:2) (cid:80)∞
Eπ
state st taking action at and then following the policy π. A policy can be extracted from
the Q-function with π(st) = arg maxa(cid:48) q(st, a(cid:48)). DQN (Mnih et al., 2013) learns a function
Q(s, a; θ)
q(s, a) represented as a convolutional neural network with parameters θ. Model
parameters are updated based on model targets provided by the Bellman equation

the Q-funciton

to model

q(st, at)

≈

q(s, a) = Es(cid:48)

(cid:104)
r + γ max

a(cid:48)

(cid:105)
q(s(cid:48), a(cid:48))
s, a
|

to create the loss function

¯L(θi) = (rt + γ max
a(cid:48)

Q(st+1, a(cid:48); θi)

Q(st, at; θi))2

−

The parameters θ are improved by SGD with the gradient

θiL(θi)

∇

≈ ∇

¯L(θi) =

θi

1
2

−

(cid:0)r + γ max
a(cid:48)

Q(s(cid:48), a(cid:48); θi)

Q(s, a; θi)(cid:1)

−

∇

θiQ(s, a; θi)

(4)

2.3 POLICY GRADIENT METHODS

s; θ). This is
Policy gradient methods (Williams, 1992) directly learn a parametrized policy π(a
|
possible due to the policy gradient theorem (Sutton et al., 1999)

θL(θ) = Es,a

(cid:2)q(s, a)

s; θ)(cid:3),
θ log π(a
|

∇

∇

which provides an unbiased estimate of the gradient of the return with respect to the policy pa-
rameters. Sutton et al. (1999) propose an improvement upon the basic policy gradient update by
replacing the Q function with the advantage function Aπ(s, a) = (q(s, a)
v(s)) where v(s) is the
−
(cid:3). When π is continuously differentiable, θ can
value function given by Eπ
k=0 γkrt+k+1
be optimized via gradient ascent following

(cid:12)
(cid:12)s = st

(cid:2) (cid:80)∞

θL(θ) = Es,a

(cid:2)(cid:0)q(s, a)

v(s)(cid:1)

s; θ)(cid:3)
θ log π(a
|

∇

−

∇

Mnih et al. (2016) learn an estimate V (s; θv)
v(s) of the value function, with both V (s; θv)
≈
and π(a
s; θ) being represented as convolutional neural networks. Additionally, they estimate the
|
Q-function with the n-step return estimate given by

Q(n)(st, at; θ, θv) = rt+1 + ... + γn−1rt+n−1 + γnV (st+n; θv)

with 0 < n

tmax for some ﬁxed tmax. The ﬁnal gradient for the policy network is given by

≤

θL(θ)

∇

≈

(cid:0)Q(n)(st, at; θ, θv)

V (st; θv)(cid:1)

θ log π(at

st; θ)

∇

|

−

3

(2)

(3)

(5)

(6)

(7)

(8)

The gradient for the value network V is given by

θv L(θv)

∇

θv

≈ ∇

(cid:104)(cid:0)Q(n)(st, at; θ, θv)

V (st; θv)(cid:1)2(cid:105)

−

(9)

Samples are generated by having an actor interact with an environment by following the policy π(st)
and observing the next state st+1 and reward rt+1.

3 PARALLEL FRAMEWORK FOR DEEP REINFORCEMENT LEARNING

We propose a general framework for deep reinforcement learning, where multiple actors can be
trained synchronously on a single machine. A set of ne environment instances are maintained,
where actions for all environment instances are generated from the policy. The architecture can be
seen in Figure 1. The policy function can be represented implicitly, as in value based methods, or

Figure 1: Architecture of the Parallel Framework for Deep RL

explicitly as in policy gradient methods. As suggested in Mnih et al. (2016), by having multiple
environments instances in parallel it is likely that they will be exploring different locations of the
state space at any given time, which reduces the correlation of encountered states and helps stabilize
training. This approach can be motivated as an on-line experience memory, where experiences are
sampled from the distribution currently being observed from the environment, instead of sampling
uniformly from previous experience.

At each timestep the master generates actions for all environment instances by sampling the current
policy. Note, that the policy may be sampled differently for each environment. A set of nw workers
then apply all the actions to the their respective environments in parallel, and store the observed
experiences. The master then updates the policy with the observed experiences. This allows the
evaluation and training of the policy to be batched, which can be efﬁciently parallelized, leading to
signiﬁcant speed improvements on modern compute architectures.

4 PARALLEL ADVANTAGE ACTOR CRITIC

We used the proposed framework to implement a version of the n-step advantage actor-critic algo-
rithm proposed by Mnih et al. (2016). This algorithm maintains a policy π(at
st; θ) and an estimate
|
V (st; θv) of the value function, both approximated by deep neural networks. The parameters θ of
the policy network (the actor) are optimized via gradient ascent following

θ log π(at

st; θ)A(st, at; θ, θv) + β
|

∇

∇

θH(π(se,t; θ))

(Sutton et al., 1999), where A(st, at; θ, θv) = Q(n)(st, at; θ, θv)
advantage function, Q(n)(st, at; θ, θv) = (cid:80)n−1

−
k=0 γkrt+k + γnV (st+n; θv), with 0 < n

V (st; θv) is an estimate of the
tmax, is

≤

4

the n-step return estimation and H(π(se,t; θ)) is the entropy of the policy π, which as suggested
by Mnih et al. (2016) is added to improve exploration by discouraging premature convergence to
suboptimal deterministic policies. The parameters θv of value network (the critic) are in turn updated
via gradient descent in the direction of

(cid:104)(cid:0)Q(n)(st, at; θ, θv)

V (st; θv)(cid:1)2(cid:105)

−

θv

∇

In the context of our framework, the above gradients are calculated using mini batches of experi-
ences. For each of the ne environments, tmax experiences are generated, resulting in batches of size
V
ne
tmax. The gradients
θ for the value function thus
take the following form:

π
θ for the policy network and the gradients

∇

∇

·

π
θ ≈

∇

ne

·

1
tmax

ne(cid:88)

tmax(cid:88)

e=1

t=1

(cid:0)Q(tmax−t+1)(se,t, ae,t; θ, θv)

V (se,t; θv)(cid:1)

θ log π(ae,t

se,t; θ)+β

θH(π(se,t; θ))

−

∇

|

∇

(10)

V
θv ≈ ∇

∇

θv

1
tmax

ne(cid:88)

tmax(cid:88)

e=1

t=1

ne

·

(cid:0)Q(tmax−t+1)(se,t, ae,t; θ, θv)

V (se,t; θv)(cid:1)2

(11)

−

Pseudocode for our parallel advantage actor-critic algorithm (PAAC) is given in Algorithm 1. As
shown in the next section, PAAC achieves state of the art performance on the Atari 2600 domain in
half of the time required by GA3C and in only one eigth of the time required by A3C. Note that,
although we implement an actor-critic algorithm, this framework can be used to implement any other
reinforcement learning algorithm.

5 EXPERIMENTS

We evaluated the performance of PAAC in 12 games from Atari 2600 using the Atari Learning
Environment (Bellemare et al., 2013). The agent was developed in Python using TensorFlow (Abadi
et al., 2015) and all performance experiments were run on a computer with a 4 core Intel i7-4790K
processor and an Nvidia GTX 980 Ti GPU.

5.1 EXPERIMENTAL SETUP

To compare results with other algorithms for the Atari domain we follow the same pre-processing
and training procedures as Mnih et al. (2016). Each action is repeated 4 times, and the per-pixel
160
maximum value from the two latest frames is kept. The frame is then scaled down from 210
pixels and 3 color channels to 84
84 pixels and a single color channel for pixel intensity. Whenever
an environment is restarted, the state is reset to the starting state and between 1 and 30 no-op actions
are performed before giving control to the agent. The environment is restarted whenever the ﬁnal
state of the environment is reached.

×

×

5

for t = 1 to tmax do

Algorithm 1 Parallel advantage actor-critic
1: Initialize timestep counter N = 0 and network weights θ, θv
2: Instantiate set e of ne environments
3: repeat
4:
5:
6:
7:
8:
9:
10:
11:

Sample at from π(at|st; θ)
Calculate vt from V (st; θv)
parallel for i = 1 to ne do

Perform action at,i in environment ei
Observe new state st+1,i and reward rt+1,i

end parallel for

end for
Rtmax+1 =

(cid:26) 0

12:

V (stmax+1; θ)

for terminal st
for non-terminal st

for t = tmax down to 1 do
Rt = rt + γRt+1

13:
14:
15:
16:
17:
18:
19:
20: until N ≥ Nmax

1
ne·tmax

end for
dθ =
dθv =
Update θ using dθ and θv using dθv.
N ← N + ne · tmax

(cid:80)ne
i=1
(cid:80)ne
i=1

1
ne·tmax

(cid:80)tmax
t=1 (Rt,i − vt,i)∇θ log π(at,i|st,i; θ) + β∇θH(π(se,t; θ))
(cid:80)tmax
t=1 ∇θv (Rt,i − V (st,i; θv))2

As in (Mnih et al., 2016), a single convolutional network with two separate output layers was used to
jointly model the policy and the value functions. For the policy function, the output is a softmax with
one node per action, while for the value function a single linear output node is used. Moreover, to
compare the efﬁciency of PAAC for different model sizes, we implemented two variants of the policy
and value convolutional network. The ﬁrst variant, referred to as archnips, is the same architecture
used by A3C FF (Mnih et al., 2016), which is a modiﬁed version of the architecture used in Mnih
et al. (2013), adapted to an actor-critic algorithm. The second variant, archnature, is an adaptation
of the architecture presented in Mnih et al. (2015). The networks were trained with RMSProp.
The hyperparameters used to generate the results in Table 1 were nw = 8, ne = 32, tmax = 5,
108, γ = 0.99, α = 0.0224, (cid:15) = 0.1, β = 0.01, and a discount factor of 0.99 for
Nmax = 1.15
RMSProp. Additionally gradient clipping (Pascanu et al., 2012) with a threshold of 40 was used.

×

5.2 RESULTS

The performance of PAAC with archnips and archnature was evaluated on twelve different Atari 2600
games, where agents were trained for 115 million skipped frames (460 million actual frames).
The results and their comparison to Gorila (Nair et al., 2015), A3C (Mnih et al., 2016) and
GA3C (Babaeizadeh et al., 2016) are presented in Table 1. After a few hours of training on a
single computer, PAAC is able to outperform Gorila in 8 games, and A3C FF in 8 games. Of the
9 games used to test GA3C, PAAC matches its performance in 2 of them and surpasses it in the
remaining 7.

Figure 2: Time usage in the game of Pong for different ne.

To better understand the effect of the number of actors (and batch size) on the score, tests were
run with ne
. The learning rate was not tuned for each batch size, and was
}

16, 32, 64, 128, 256

∈ {

6

Game

Gorila

A3C FF

GA3C

PAAC archnips

PAAC archnature

Amidar
Centipede
Beam Rider
Boxing
Breakout
Ms. Pacman
Name This Game
Pong
Qbert
Seaquest
Space Invaders
Up n Down

1189.70
8432.30
3302.9
94.9
402.2
3233.50
6182.16
18.3
10815.6
13169.06
1883.4
12561.58

263.9
3755.8
22707.9
59.8
681.9
653.7
10476.1
5.6
15148.8
2355.4
15730.5
74705.7

218
7386
N/A
92
N/A
1978
5643
18
14966.0
1706
N/A
8623

701.8
5747.32
4062.0
99.6
470.1
2194.7
9743.7
20.6
16561.7
1754.0
1077.3
88105.3

Training

4d CPU cluster

4d CPU

1d GPU

12h GPU

1348.3
7368.1
6844.0
99.8
565.3
1976.0
14068.0
20.9
17249.2
1755.3
1427.8
100523.3

15h GPU

Table 1: Scores are measured from the best per- forming actor out of three, and averaged over 30
runs with upto 30 no-op actions start condition. Results for A3C FF use human start condition are
therefore not directly comparable. Gorila scores taken from Nair et al. (2015), A3C FF scores taken
from Mnih et al. (2016) and GA3C scores take from Babaeizadeh et al. (2016). Unavailable results
are shown as N/A.

·

ne for all runs across all games. Increasing ne decreases the frequency of pa-
chosen to be 0.0007
rameter updates, given that parameter updates are performed every ne
tmax timesteps. As the theory
suggests, the decreased frequency in parameter updates can be offset by increasing the learning rate.
As can be seen in Figure 3 most choices of ne result in similar scores at a given timestep, however
Figure 4 shows that higher values of ne reach those timesteps signiﬁcantly faster. The choice of
ne = 256 results in divergence in three out of the four games, which shows that the learning rate
can be increased proportional to the batch size, until a certain limit is reached. A limiting factor

·

Figure 3: Score comparison for PAAC on six Atari 2600 games for different ne, where one training
epoch is equivalent to 1 million timesteps (4 million skipped frames).

in the speed of training is the time spent in agent-environment interaction. When using archnips for
ne = 32 approximately 50% of the time is spent interacting with the environment, while only 37%
is used for learning and action selection, as is shown in Figure 2. This has strong implications for the
models and environments that can be used. Using a model-environment combination that doubles
the time needed for learning and action calculation would lead to a mere 37% increase in training
time. This can be seen in Figure 2 where using archnature on the GPU leads to a drop in timesteps per
second 22% for ne = 32 when compared to archnips. When running on the CPU however this leads
to a 41% drop in timesteps per second.

7

Figure 4: Training time and score comparison for PAAC on six Atari 2600 games for different ne.

6 CONCLUSION

In this work, we have introduced a parallel framework for deep reinforcement learning that can be
efﬁciently parallelized on a GPU. The framework is ﬂexible, and can be used for on-policy and off-
policy, as well as value based and policy gradient based algorithms. The presented implementation
of the framework is able to reduce training time for the Atari 2600 domain to a few hours, while
maintaining state-of-the-art performance. Improvements in training time, will allow the application
of these algorithms to more demanding environments, and the use of more powerful models.

REFERENCES

Abadi, Martın, Agarwal, Ashish, Barham, Paul, Brevdo, Eugene, Chen, Zhifeng, Citro, Craig, Cor-
rado, Greg S, Davis, Andy, Dean, Jeffrey, Devin, Matthieu, et al. Tensorﬂow: Large-scale machine
learning on heterogeneous systems, 2015. Software available from tensorﬂow. org, 1, 2015.

Babaeizadeh, M., Frosio, I., Tyree, S., Clemons, J., and Kautz, J. GA3C: GPU-based A3C for Deep

Reinforcement Learning. ArXiv e-prints, November 2016.

Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An
evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279,
06 2013.

Bottou, L., Curtis, F. E., and Nocedal, J. Optimization Methods for Large-Scale Machine Learning.

ArXiv e-prints, June 2016.

Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M.

Playing Atari with Deep Reinforcement Learning. ArXiv e-prints, December 2013.

Mnih, V., Puigdom`enech Badia, A., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., Silver, D.,
and Kavukcuoglu, K. Asynchronous Methods for Deep Reinforcement Learning. ArXiv e-prints,
February 2016.

Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A, Veness, Joel, Bellemare,
Marc G, Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K, Ostrovski, Georg, et al. Human-
level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.

Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De Maria, A., Panneershelvam, V.,
Suleyman, M., Beattie, C., Petersen, S., Legg, S., Mnih, V., Kavukcuoglu, K., and Silver, D.
Massively Parallel Methods for Deep Reinforcement Learning. ArXiv e-prints, July 2015.

8

Pascanu, R., Mikolov, T., and Bengio, Y. On the difﬁculty of training Recurrent Neural Networks.

ArXiv e-prints, November 2012.

Recht, Benjamin, Re, Christopher, Wright, Stephen, and Niu, Feng. Hogwild: A lock-free approach
to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Sys-
tems, pp. 693–701, 2011.

Sutton, Richard S, McAllester, David A, Singh, Satinder P, Mansour, Yishay, et al. Policy gradient
methods for reinforcement learning with function approximation. In NIPS, volume 99, pp. 1057–
1063, 1999.

Williams, Ronald J. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine learning, 8(3-4):229–256, 1992.

9

7
1
0
2
 
y
a
M
 
6
1
 
 
]

G
L
.
s
c
[
 
 
2
v
2
6
8
4
0
.
5
0
7
1
:
v
i
X
r
a

EFFICIENT PARALLEL METHODS FOR DEEP REIN-
FORCEMENT LEARNING

Alfredo V. Clemente
Department of Computer and Information Science
Norwegian University of Science and Technology
Trondheim, Norway
alfredvc@stud.ntnu.no

Humberto N. Castej´on
Telenor Research
Trondheim, Norway
humberto.castejon@telenor.com

Arjun Chandra
Telenor Research
Trondheim, Norway
arjun.chandra@telenor.com

ABSTRACT

We propose a novel framework for efﬁcient parallelization of deep reinforce-
ment learning algorithms, enabling these algorithms to learn from multiple ac-
tors on a single machine. The framework is algorithm agnostic and can be ap-
plied to on-policy, off-policy, value based and policy gradient based algorithms.
Given its inherent parallelism, the framework can be efﬁciently implemented
on a GPU, allowing the usage of powerful models while signiﬁcantly reduc-
ing training time. We demonstrate the effectiveness of our framework by im-
plementing an advantage actor-critic algorithm on a GPU, using on-policy ex-
periences and employing synchronous updates. Our algorithm achieves state-
of-the-art performance on the Atari domain after only a few hours of training.
Our framework thus opens the door for much faster experimentation on demand-
ing problem domains. Our implementation is open-source and is made public at
https://github.com/alfredvc/paac.

1

INTRODUCTION AND RELATED WORK

Incorporating deep learning models within reinforcement learning (RL) presents some challenges.
Standard stochastic gradient descent algorithms for training deep learning models assume all train-
ing examples to be independent and identically distributed (i.i.d.). This constraint is often violated
by RL agents, given the high correlation between encountered states. Additionally, when learning
on-policy, the policy affects the distribution of encountered states, which in turn affects the policy,
creating a feedback loop that may lead to divergence (Mnih et al., 2013). Two main attempts have
been made to solve these issues. One is to store experiences in a large replay memory and em-
ploy off-policy RL methods (Mnih et al., 2013). Sampling the replay memory can break the state
correlations, thus reducing the effect of the feedback loop. Another is to execute multiple asyn-
chronous agents in parallel, each interacting with an instance of the environment independently of
each other (Mnih et al., 2016). Both of these approaches suffer from different drawbacks; experi-
ence replay can only be employed with off-policy methods, while asynchronous agents can perform
inconsistent parameter updates due to stale gradients1 and simultaneous parameter updates from
different threads.

Parallel and distributed compute architectures have motivated innovative modiﬁcations to existing
RL algorithms to efﬁciently make use of parallel execution. In the General Reinforcement Learning
Architecture (Gorila) (Nair et al., 2015), the DQN (Mnih et al., 2015) algorithm is distributed across
multiple machines. Multiple learners learn off-policy using experiences collected into a common
replay memory by multiple actors. Gorila is shown to outperform standard DQN in a variety of

1Gradients may be computed w.r.t. stale parameters while updates applied to a new parameter set.

1

Atari games, while only training for 4 days. The distribution of the learning process is further ex-
plored in (Mnih et al., 2016), where multiple actor-learners are executed asynchronously on a single
machine. Each actor-learner holds its own copy of the policy/value function and interacts with its
own instance of the environment. This allows for both off-policy, as well as on-policy learning.
The actor-learners compute gradients in parallel and update shared parameters asynchronously in
a HOGWILD! (Recht et al., 2011) fashion. The authors suggest that multiple agents collecting in-
dependent experiences from their own environment instances reduces correlation between samples,
thereby improving learning. The asynchronous advantage actor-critic (A3C) algorithm (Mnih et al.,
2016) was able to surpass the state of the art on the Atari domain at the time of publication, while
training for 4 days on a single machine with 16 CPU cores. GA3C (Babaeizadeh et al., 2016) is a
GPU implementation of A3C. It batches action selection and learning using queues. Actors sample
from a shared policy by submitting a task to a predictor, which executes the policy and returns an
action once it has accumulated enough tasks. Similarly, learning is performed by submitting expe-
riences to a trainer, which computes gradients and applies updates once enough experiences have
been gathered. If the training queue is not empty when the model is updated, the learning will no
longer be on-policy, since the remaining experiences were generated by an old policy. This leads
to instabilities during training, which the authors address with a slight modiﬁcation to the weight
updates.

We propose a novel framework for efﬁcient parallelization of deep reinforcement learning algo-
rithms, which keeps the strengths of the aforementioned approaches, while alleviating their weak-
nesses. Algorithms based on this framework can learn from hundreds of actors in parallel, similar
to Gorila, while running on a single machine like A3C and GA3C. Having multiple actors help
decorrelate encountered states and attenuate feedback loops, while allowing us to leverage the par-
allel architecture of modern CPUs and GPUs. Unlike A3C and Gorila, there is only one copy of
the parameters, hence parameter updates are performed synchronously, thus avoiding the possible
drawbacks related to asynchronous updates. Our framework has many similarities to GA3C. How-
ever, the absence of queues allows for a much more simpler and computationally efﬁcient solution,
while allowing for true on-policy learning and faster convergence to optimal policies. We demon-
strate our framework with a Parallel Advantage Actor-Critic algorithm, that achieves state of the art
performance in the Atari 2600 domain after only a few hours of training. This opens the door for
much faster experimentation.

2 BACKGROUND

Reinforcement learning algorithms attempt to learn a policy π that maps states to actions, in order
(cid:3) for some discount
to maximize the expected sum of cumulative rewards Rt = Eπ
factor 0 < γ < 1, where rt is the reward observed at timestep t. Current reinforcement learning
algorithms represent the learned policy π as a neural network, either implicitly with a value function
or explicitly as a policy function.

k=0 γkrt+k

(cid:2) (cid:80)∞

2.1 BATCHING WITH STOCHASTIC GRADIENT DESCENT

Current reinforcement learning algorithms make heavy use of deep neural networks, both to extract
high level features from the observations it receives, and as function approximators to represent its
policy or value functions.
Consider the set of input-target pairs S =
(x0, y0), (x1, y1), ...(xn, yn)
generated by some func-
{
}
tion f ∗(x). The goal of supervised learning with neural networks is to learn a parametrized function
. The performance of f is evaluated with the empirical
f (x; θ) that best approximates function f
∗
loss

L(θ) =

l(f (xs; θ), ys)

(1)

(cid:88)

1
S

|

|

s∈S

where l(f (xs; θ), ys) is referred to as a loss function, and gives a quantitative measure of how good
f is at modelling f ∗. The model parameters θ are learned with stochastic gradient descent (SGD)
by iteratively applying the update

θi+1

θi

α

θiL(θi)

←

−

∇

2

for a learning rate α. In SGD, L(θ) is usually approximated with

¯L(θ) =

(cid:88)

1
S(cid:48)

l(f (xs(cid:48); θ), ys(cid:48)),

s(cid:48)∈S(cid:48)

|

|
S is a mini-batch sampled from S. The choice of α and nS(cid:48) =

where S(cid:48)
presents a trade-off
⊂
between computational efﬁciency and sample efﬁciency. Increasing nS(cid:48) by a factor of k increases the
k, and reduces its variance proportionally to
time needed to calculate
1
k (Bottou et al., 2016). In order to mitigate the increased time per parameter update we can increase
the learning rate to α(cid:48) for some α(cid:48)
α. However there are some limits on the size of the learning
≥
rate, so that in general α(cid:48)
kα (Bottou et al., 2016). The hyper-parameters α(cid:48) and k are chosen to
≤
simultaneously maximize k
k(cid:48) and minimize L.

θ ¯L by a factor of k(cid:48), for k(cid:48)

S(cid:48)
|

∇

≤

|

2.2 MODEL-FREE VALUE BASED METHODS

value

k=0 γkrt+k+1

based methods
(cid:12)
(cid:12)s = st, a = at

=
attempt
Model-free
(cid:3) that gives the expected return achieved by being in
(cid:2) (cid:80)∞
Eπ
state st taking action at and then following the policy π. A policy can be extracted from
the Q-function with π(st) = arg maxa(cid:48) q(st, a(cid:48)). DQN (Mnih et al., 2013) learns a function
Q(s, a; θ)
q(s, a) represented as a convolutional neural network with parameters θ. Model
parameters are updated based on model targets provided by the Bellman equation

the Q-funciton

to model

q(st, at)

≈

q(s, a) = Es(cid:48)

(cid:104)
r + γ max

a(cid:48)

(cid:105)
q(s(cid:48), a(cid:48))
s, a
|

to create the loss function

¯L(θi) = (rt + γ max
a(cid:48)

Q(st+1, a(cid:48); θi)

Q(st, at; θi))2

−

The parameters θ are improved by SGD with the gradient

θiL(θi)

∇

≈ ∇

¯L(θi) =

θi

1
2

−

(cid:0)r + γ max
a(cid:48)

Q(s(cid:48), a(cid:48); θi)

Q(s, a; θi)(cid:1)

−

∇

θiQ(s, a; θi)

(4)

2.3 POLICY GRADIENT METHODS

s; θ). This is
Policy gradient methods (Williams, 1992) directly learn a parametrized policy π(a
|
possible due to the policy gradient theorem (Sutton et al., 1999)

θL(θ) = Es,a

(cid:2)q(s, a)

s; θ)(cid:3),
θ log π(a
|

∇

∇

which provides an unbiased estimate of the gradient of the return with respect to the policy pa-
rameters. Sutton et al. (1999) propose an improvement upon the basic policy gradient update by
replacing the Q function with the advantage function Aπ(s, a) = (q(s, a)
v(s)) where v(s) is the
−
(cid:3). When π is continuously differentiable, θ can
value function given by Eπ
k=0 γkrt+k+1
be optimized via gradient ascent following

(cid:12)
(cid:12)s = st

(cid:2) (cid:80)∞

θL(θ) = Es,a

(cid:2)(cid:0)q(s, a)

v(s)(cid:1)

s; θ)(cid:3)
θ log π(a
|

∇

−

∇

Mnih et al. (2016) learn an estimate V (s; θv)
v(s) of the value function, with both V (s; θv)
≈
and π(a
s; θ) being represented as convolutional neural networks. Additionally, they estimate the
|
Q-function with the n-step return estimate given by

Q(n)(st, at; θ, θv) = rt+1 + ... + γn−1rt+n−1 + γnV (st+n; θv)

with 0 < n

tmax for some ﬁxed tmax. The ﬁnal gradient for the policy network is given by

≤

θL(θ)

∇

≈

(cid:0)Q(n)(st, at; θ, θv)

V (st; θv)(cid:1)

θ log π(at

st; θ)

∇

|

−

3

(2)

(3)

(5)

(6)

(7)

(8)

The gradient for the value network V is given by

θv L(θv)

∇

θv

≈ ∇

(cid:104)(cid:0)Q(n)(st, at; θ, θv)

V (st; θv)(cid:1)2(cid:105)

−

(9)

Samples are generated by having an actor interact with an environment by following the policy π(st)
and observing the next state st+1 and reward rt+1.

3 PARALLEL FRAMEWORK FOR DEEP REINFORCEMENT LEARNING

We propose a general framework for deep reinforcement learning, where multiple actors can be
trained synchronously on a single machine. A set of ne environment instances are maintained,
where actions for all environment instances are generated from the policy. The architecture can be
seen in Figure 1. The policy function can be represented implicitly, as in value based methods, or

Figure 1: Architecture of the Parallel Framework for Deep RL

explicitly as in policy gradient methods. As suggested in Mnih et al. (2016), by having multiple
environments instances in parallel it is likely that they will be exploring different locations of the
state space at any given time, which reduces the correlation of encountered states and helps stabilize
training. This approach can be motivated as an on-line experience memory, where experiences are
sampled from the distribution currently being observed from the environment, instead of sampling
uniformly from previous experience.

At each timestep the master generates actions for all environment instances by sampling the current
policy. Note, that the policy may be sampled differently for each environment. A set of nw workers
then apply all the actions to the their respective environments in parallel, and store the observed
experiences. The master then updates the policy with the observed experiences. This allows the
evaluation and training of the policy to be batched, which can be efﬁciently parallelized, leading to
signiﬁcant speed improvements on modern compute architectures.

4 PARALLEL ADVANTAGE ACTOR CRITIC

We used the proposed framework to implement a version of the n-step advantage actor-critic algo-
rithm proposed by Mnih et al. (2016). This algorithm maintains a policy π(at
st; θ) and an estimate
|
V (st; θv) of the value function, both approximated by deep neural networks. The parameters θ of
the policy network (the actor) are optimized via gradient ascent following

θ log π(at

st; θ)A(st, at; θ, θv) + β
|

∇

∇

θH(π(se,t; θ))

(Sutton et al., 1999), where A(st, at; θ, θv) = Q(n)(st, at; θ, θv)
advantage function, Q(n)(st, at; θ, θv) = (cid:80)n−1

−
k=0 γkrt+k + γnV (st+n; θv), with 0 < n

V (st; θv) is an estimate of the
tmax, is

≤

4

the n-step return estimation and H(π(se,t; θ)) is the entropy of the policy π, which as suggested
by Mnih et al. (2016) is added to improve exploration by discouraging premature convergence to
suboptimal deterministic policies. The parameters θv of value network (the critic) are in turn updated
via gradient descent in the direction of

(cid:104)(cid:0)Q(n)(st, at; θ, θv)

V (st; θv)(cid:1)2(cid:105)

−

θv

∇

In the context of our framework, the above gradients are calculated using mini batches of experi-
ences. For each of the ne environments, tmax experiences are generated, resulting in batches of size
V
ne
tmax. The gradients
θ for the value function thus
take the following form:

π
θ for the policy network and the gradients

∇

∇

·

π
θ ≈

∇

ne

·

1
tmax

ne(cid:88)

tmax(cid:88)

e=1

t=1

(cid:0)Q(tmax−t+1)(se,t, ae,t; θ, θv)

V (se,t; θv)(cid:1)

θ log π(ae,t

se,t; θ)+β

θH(π(se,t; θ))

−

∇

|

∇

(10)

V
θv ≈ ∇

∇

θv

1
tmax

ne(cid:88)

tmax(cid:88)

e=1

t=1

ne

·

(cid:0)Q(tmax−t+1)(se,t, ae,t; θ, θv)

V (se,t; θv)(cid:1)2

(11)

−

Pseudocode for our parallel advantage actor-critic algorithm (PAAC) is given in Algorithm 1. As
shown in the next section, PAAC achieves state of the art performance on the Atari 2600 domain in
half of the time required by GA3C and in only one eigth of the time required by A3C. Note that,
although we implement an actor-critic algorithm, this framework can be used to implement any other
reinforcement learning algorithm.

5 EXPERIMENTS

We evaluated the performance of PAAC in 12 games from Atari 2600 using the Atari Learning
Environment (Bellemare et al., 2013). The agent was developed in Python using TensorFlow (Abadi
et al., 2015) and all performance experiments were run on a computer with a 4 core Intel i7-4790K
processor and an Nvidia GTX 980 Ti GPU.

5.1 EXPERIMENTAL SETUP

To compare results with other algorithms for the Atari domain we follow the same pre-processing
and training procedures as Mnih et al. (2016). Each action is repeated 4 times, and the per-pixel
160
maximum value from the two latest frames is kept. The frame is then scaled down from 210
pixels and 3 color channels to 84
84 pixels and a single color channel for pixel intensity. Whenever
an environment is restarted, the state is reset to the starting state and between 1 and 30 no-op actions
are performed before giving control to the agent. The environment is restarted whenever the ﬁnal
state of the environment is reached.

×

×

5

for t = 1 to tmax do

Algorithm 1 Parallel advantage actor-critic
1: Initialize timestep counter N = 0 and network weights θ, θv
2: Instantiate set e of ne environments
3: repeat
4:
5:
6:
7:
8:
9:
10:
11:

Sample at from π(at|st; θ)
Calculate vt from V (st; θv)
parallel for i = 1 to ne do

Perform action at,i in environment ei
Observe new state st+1,i and reward rt+1,i

end parallel for

end for
Rtmax+1 =

(cid:26) 0

12:

V (stmax+1; θ)

for terminal st
for non-terminal st

for t = tmax down to 1 do
Rt = rt + γRt+1

13:
14:
15:
16:
17:
18:
19:
20: until N ≥ Nmax

1
ne·tmax

end for
dθ =
dθv =
Update θ using dθ and θv using dθv.
N ← N + ne · tmax

(cid:80)ne
i=1
(cid:80)ne
i=1

1
ne·tmax

(cid:80)tmax
t=1 (Rt,i − vt,i)∇θ log π(at,i|st,i; θ) + β∇θH(π(se,t; θ))
(cid:80)tmax
t=1 ∇θv (Rt,i − V (st,i; θv))2

As in (Mnih et al., 2016), a single convolutional network with two separate output layers was used to
jointly model the policy and the value functions. For the policy function, the output is a softmax with
one node per action, while for the value function a single linear output node is used. Moreover, to
compare the efﬁciency of PAAC for different model sizes, we implemented two variants of the policy
and value convolutional network. The ﬁrst variant, referred to as archnips, is the same architecture
used by A3C FF (Mnih et al., 2016), which is a modiﬁed version of the architecture used in Mnih
et al. (2013), adapted to an actor-critic algorithm. The second variant, archnature, is an adaptation
of the architecture presented in Mnih et al. (2015). The networks were trained with RMSProp.
The hyperparameters used to generate the results in Table 1 were nw = 8, ne = 32, tmax = 5,
108, γ = 0.99, α = 0.0224, (cid:15) = 0.1, β = 0.01, and a discount factor of 0.99 for
Nmax = 1.15
RMSProp. Additionally gradient clipping (Pascanu et al., 2012) with a threshold of 40 was used.

×

5.2 RESULTS

The performance of PAAC with archnips and archnature was evaluated on twelve different Atari 2600
games, where agents were trained for 115 million skipped frames (460 million actual frames).
The results and their comparison to Gorila (Nair et al., 2015), A3C (Mnih et al., 2016) and
GA3C (Babaeizadeh et al., 2016) are presented in Table 1. After a few hours of training on a
single computer, PAAC is able to outperform Gorila in 8 games, and A3C FF in 8 games. Of the
9 games used to test GA3C, PAAC matches its performance in 2 of them and surpasses it in the
remaining 7.

Figure 2: Time usage in the game of Pong for different ne.

To better understand the effect of the number of actors (and batch size) on the score, tests were
run with ne
. The learning rate was not tuned for each batch size, and was
}

16, 32, 64, 128, 256

∈ {

6

Game

Gorila

A3C FF

GA3C

PAAC archnips

PAAC archnature

Amidar
Centipede
Beam Rider
Boxing
Breakout
Ms. Pacman
Name This Game
Pong
Qbert
Seaquest
Space Invaders
Up n Down

1189.70
8432.30
3302.9
94.9
402.2
3233.50
6182.16
18.3
10815.6
13169.06
1883.4
12561.58

263.9
3755.8
22707.9
59.8
681.9
653.7
10476.1
5.6
15148.8
2355.4
15730.5
74705.7

218
7386
N/A
92
N/A
1978
5643
18
14966.0
1706
N/A
8623

701.8
5747.32
4062.0
99.6
470.1
2194.7
9743.7
20.6
16561.7
1754.0
1077.3
88105.3

Training

4d CPU cluster

4d CPU

1d GPU

12h GPU

1348.3
7368.1
6844.0
99.8
565.3
1976.0
14068.0
20.9
17249.2
1755.3
1427.8
100523.3

15h GPU

Table 1: Scores are measured from the best per- forming actor out of three, and averaged over 30
runs with upto 30 no-op actions start condition. Results for A3C FF use human start condition are
therefore not directly comparable. Gorila scores taken from Nair et al. (2015), A3C FF scores taken
from Mnih et al. (2016) and GA3C scores take from Babaeizadeh et al. (2016). Unavailable results
are shown as N/A.

·

ne for all runs across all games. Increasing ne decreases the frequency of pa-
chosen to be 0.0007
rameter updates, given that parameter updates are performed every ne
tmax timesteps. As the theory
suggests, the decreased frequency in parameter updates can be offset by increasing the learning rate.
As can be seen in Figure 3 most choices of ne result in similar scores at a given timestep, however
Figure 4 shows that higher values of ne reach those timesteps signiﬁcantly faster. The choice of
ne = 256 results in divergence in three out of the four games, which shows that the learning rate
can be increased proportional to the batch size, until a certain limit is reached. A limiting factor

·

Figure 3: Score comparison for PAAC on six Atari 2600 games for different ne, where one training
epoch is equivalent to 1 million timesteps (4 million skipped frames).

in the speed of training is the time spent in agent-environment interaction. When using archnips for
ne = 32 approximately 50% of the time is spent interacting with the environment, while only 37%
is used for learning and action selection, as is shown in Figure 2. This has strong implications for the
models and environments that can be used. Using a model-environment combination that doubles
the time needed for learning and action calculation would lead to a mere 37% increase in training
time. This can be seen in Figure 2 where using archnature on the GPU leads to a drop in timesteps per
second 22% for ne = 32 when compared to archnips. When running on the CPU however this leads
to a 41% drop in timesteps per second.

7

Figure 4: Training time and score comparison for PAAC on six Atari 2600 games for different ne.

6 CONCLUSION

In this work, we have introduced a parallel framework for deep reinforcement learning that can be
efﬁciently parallelized on a GPU. The framework is ﬂexible, and can be used for on-policy and off-
policy, as well as value based and policy gradient based algorithms. The presented implementation
of the framework is able to reduce training time for the Atari 2600 domain to a few hours, while
maintaining state-of-the-art performance. Improvements in training time, will allow the application
of these algorithms to more demanding environments, and the use of more powerful models.

REFERENCES

Abadi, Martın, Agarwal, Ashish, Barham, Paul, Brevdo, Eugene, Chen, Zhifeng, Citro, Craig, Cor-
rado, Greg S, Davis, Andy, Dean, Jeffrey, Devin, Matthieu, et al. Tensorﬂow: Large-scale machine
learning on heterogeneous systems, 2015. Software available from tensorﬂow. org, 1, 2015.

Babaeizadeh, M., Frosio, I., Tyree, S., Clemons, J., and Kautz, J. GA3C: GPU-based A3C for Deep

Reinforcement Learning. ArXiv e-prints, November 2016.

Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An
evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279,
06 2013.

Bottou, L., Curtis, F. E., and Nocedal, J. Optimization Methods for Large-Scale Machine Learning.

ArXiv e-prints, June 2016.

Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M.

Playing Atari with Deep Reinforcement Learning. ArXiv e-prints, December 2013.

Mnih, V., Puigdom`enech Badia, A., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., Silver, D.,
and Kavukcuoglu, K. Asynchronous Methods for Deep Reinforcement Learning. ArXiv e-prints,
February 2016.

Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A, Veness, Joel, Bellemare,
Marc G, Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K, Ostrovski, Georg, et al. Human-
level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.

Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De Maria, A., Panneershelvam, V.,
Suleyman, M., Beattie, C., Petersen, S., Legg, S., Mnih, V., Kavukcuoglu, K., and Silver, D.
Massively Parallel Methods for Deep Reinforcement Learning. ArXiv e-prints, July 2015.

8

Pascanu, R., Mikolov, T., and Bengio, Y. On the difﬁculty of training Recurrent Neural Networks.

ArXiv e-prints, November 2012.

Recht, Benjamin, Re, Christopher, Wright, Stephen, and Niu, Feng. Hogwild: A lock-free approach
to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Sys-
tems, pp. 693–701, 2011.

Sutton, Richard S, McAllester, David A, Singh, Satinder P, Mansour, Yishay, et al. Policy gradient
methods for reinforcement learning with function approximation. In NIPS, volume 99, pp. 1057–
1063, 1999.

Williams, Ronald J. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine learning, 8(3-4):229–256, 1992.

9

