8
1
0
2
 
r
a

M
 
8
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
9
7
7
3
0
.
6
0
7
1
:
v
i
X
r
a

General Latent Feature Models for Heterogeneous Datasets

Isabel Valera∗1, Melanie F. Pradier2, Maria Lomeli1, and Zoubin Ghahramani1,3

1University of Cambridge, Cambridge, UK
2Universidad Carlos III de Madrid, Madrid, Spain
3Uber AI Labs, San Francisco, US

Abstract

Latent feature modeling allows capturing the latent structure responsible for generating the observed properties
of a set of objects. It is often used to make predictions either for new observations or for missing information in
the original data, as well as to perform exploratory data analysis. However, although there is an extensive literature
on latent feature models for homogeneous datasets, where all the attributes that describe each object are of the same
(continuous or discrete) nature, there is a lack of work on latent feature modeling for heterogeneous databases. In
this paper, we introduce a general Bayesian nonparametric latent feature model suitable for heterogeneous datasets,
where the attributes describing each object can be either discrete, continuous or mixed variables. The proposed model
presents several important properties. Firstly, it accounts for heterogeneous data while keeping the properties of
conjugate models, which allows to infer the model in linear time with respect to the number of objects and attributes.
Secondly, its Bayesian nonparametric nature allows us to automatically infer the model complexity from the data, i.e.,
the number of features necessary to capture the latent structure in the data. Thirdly, the latent features in the model are
binary-valued, this facilitates the interpretability of the obtained latent features in exploratory data analysis. Finally, a
software package called GLFM toolbox is made publicly available for other researchers to use and extend. We show
the ﬂexibility of the proposed model by solving both prediction and data analysis tasks on several real-world datasets.

1 Introduction

One of the aims of unsupervised learning is recovering the latent structure responsible for generating the observed
properties or attributes of a set of objects. In particular, in latent feature modeling, one or more attributes of each object
can be represented by an unobserved vector of latent features. It is assumed that the observations are generated from
a distribution determined by those latent feature values. In other words, latent feature models allow us to represent,
with only a few features, the immense redundant information present in the observed data, by capturing the statistical
dependencies among the different objects and attributes. As a consequence, they have been broadly used to make
predictions either for new values of interest or missing information in the original data [25, 11], as well as to perform
exploratory data analysis, i.e, to better understand the data [2, 30].

There is an extensive literature in latent feature modeling for homogeneous data, where all the attributes describing
each object in the database are of the same type, that is continuous or discrete. In particular, these works assume that
databases contain only either continuous data, usually modeled as Gaussian variables [12, 28], or discrete, that can
be either modeled by discrete likelihoods [18, 24, 11] or simply treated as Gaussian variables [25, 2, 28]. However,
there still exists a lack of work dealing with heterogeneous databases, which in fact are common in real applications.
For instance, the Electronic Health Records of hospitals might contain lab measurements (often real-valued or positive
real-valued data), diagnoses (categorical data) and genomic information (ordinal, count data and categorical data).
Another example are surveys, which often contain diverse information about the participants such as age (count data),
gender (categorical data), salary (positive real data), etc. Despite the diversity of data types, the standard approach

∗Corresponding author: miv24@cam.ac.uk

1

when dealing with heterogeneous datasets is to treat all the attributes, either continuous or discrete, as Gaussian
variables.

This paper presents a general latent feature model (GLFM) suitable for heterogeneous datasets, where the attributes
describing each object can be either discrete, continuous or mixed variables. Speciﬁcally, we account for real-valued
and positive real-valued as examples of continuous variables, and categorical, ordinal and count data as examples of
discrete variables. The proposed model extends the essential building block of Bayesian nonparametric latent feature
models, the Indian Buffet Process (IBP) by [12], to account for heterogeneous data while maintaining the model com-
plexity of conjugate models. The IBP places a prior distribution over binary matrices where the number of columns,
corresponding to latent features, is potentially inﬁnite and can be inferred from the data. Among all the available latent
feature models in the literature, we opt for the IBP due to two main reasons. First, the nonparametric nature of the
IBP allows us to automatically infer the appropriate model complexity, i.e., the number of necessary features, from
the data. Second, the IBP considers binary-valued latent features which has been shown to provide more interpretable
results in data exploration than standard real-valued latent feature models [23, 24]. The standard IBP assumes real-
valued observations combined with conjugate likelihood models that allow for fast inference algorithms [8]. However,
we handle heterogeneous databases, such that conjugacy is not preserved.

In order to propose a general observation model for the IBP that accounts for heterogeneous data while keeping the
properties of conjugate models, we exploit two key ideas. First, we introduce an auxiliary real-valued variable (also
called pseudo-observation), such that, conditioned on it, the model behaves as the standard linear-Gaussian IBP in
[12]. Second, we assume that there exists a function that transforms the pseudo-observation into an actual observation,
mapping the real line into the (discrete or continuous) observation space of each attribute in the data. These two key
ideas allow us to derive an efﬁcient inference algorithm based on collapsed Gibbs sampling, which presents linear
complexity with the number of objects and attributes in the data.

We show the ﬂexibility and applicability of the proposed model by solving both prediction and data exploration
tasks in several real-world datasets. In particular, we ﬁrst use the proposed model to estimate and replace missing data
in heterogeneous databases (where the data is missing completely at random), showing that our approach for missing
data estimation outperforms, in terms of accuracy, the Bayesian probabilistic matrix factorization (BPMF) [25] and
the standard IBP which assumes Gaussian observations. These results have been previously discussed in [29], where
the main focus of the paper was missing data estimation (a.k.a. table completion). In contrast, this extended version of
the paper focuses on the model itself, providing the necessary details on the GLFM and its inference to perform latent
feature modeling in heterogeneous datasets, which is a powerful tool not only for missing data estimation but also for
exploratory data analysis tasks. This motivates the second part of the experiments, where we present several examples
of how to use the proposed model for data exploration in real-world datasets gathered from diverse application domains
related to medicine, psychiatry, sociology and politics.

The rest of the paper is organized as follows. In Section 2, we provide the details on the general Bayesian nonpara-
metric latent feature model for heterogeneous datasets. In Section 3, we develop an inference algorithm based on the
Gibbs sampler, where we make use of the introduced pseudo-observation to collapse the sampler. In Section 4, we ap-
ply our model to two real-world applications, missing data estimation and data analysis. Finally, Section 5 is devoted
to the discussion on the proposed latent feature model for heterogeneous datasets and its potential applications.

2 Latent Feature Model for Heterogeneous Data

n denote each entry of the observation matrix X, which might be of the following types:

D, each of the N objects is deﬁned by

We assume that the data can be stored in an observation matrix X of size N
a set of D attributes. Let xd
Continuous variables:
1. Real-valued, xd
2. Positive real-valued, xd

n ∈ (cid:60)

•

×

n ∈ (cid:60)+.

Discrete variables:

•

1. Categorical data, xd
2. Ordinal data, xd

n takes a value in a ﬁnite unordered set, e.g., xd
n ∈ {

n takes values in a ﬁnite ordered set, e.g., xd

‘blue’, ‘red’, ‘black’

n ∈ {
‘never’, ‘sometimes’, ‘often’, ‘usually’,

.
}

‘always’
}

.

3. Count data, xd

n ∈ {

0, . . . ,

.

∞}

2

Figure 1: Graphical Model for the Generalized Latent Feature Model. Grey nodes represent observed variables,
white nodes correspond to latent variables. Introducing the pseudo-observations Yd allow us to deal with heteroge-
neous data.

As in standard latent feature models, we assume that xd
associated to the n-th data point, zn = [zn1, . . . , znK], and a weight vector1 Bd = [bd
of latent variables), whose elements bd
Under this assumption, the likelihood can be factorized as

n can be explained by a K-length vector of latent features
K] (K being the number
k weight the contribution of the k-th latent feature to the d-th dimension of X.

1, . . . , bd

Z,

p(X
|

{

Bd, Ψd

D
d=1) =
}

p(xd
n|

zn, Bd, Ψd),

D

N

n=1
(cid:89)

(cid:89)d=1
where Ψd denotes the set of random variables necessary to deﬁne the distribution of the d-th attribute. Here, we
K matrix Z that follows an IBP
assume binary-valued latent binary feature vectors zn, which are gathered in an N
IBP(α) [12]. Additionally, we place a Gaussian distribution
prior with concentration parameter α, denoted by Z
with zero mean and covariance matrix σ2
is assumed to be
Gaussian with mean znBd for the d = 1, . . . , D attributes, the above model is equivalent to the standard IBP with
Gaussian observations [12], therefore can be efﬁciently learnt using the properties of the Gaussian distribution [8].
However, under heterogeneous (or non-Gaussian) observation matrices, developing an efﬁcient inference algorithm is
not straightforward, since the advantages of conjugate priors do not hold in the general case.

BIK over the weight vectors Bd. Note that if xd

n ∈ (cid:60)

×

∼

To solve this limitation, we introduce an auxiliary Gaussian variable yd

n in the observation matrix, and assume that there exists a transformation function fd(
·

n (which we might refer to as pseudo-
) over
into the observation space of the d-th attribute

n to obtain the observations xd

n, mapping the real line

observation) per entry xd
the variables yd
in the observation matrix Ωd, i.e.,

(1)

fd :

(cid:60) (cid:55)→
yd
n →

(cid:60)

.

Ωd
xd
n

Here, we assume that yd

n is Gaussian distributed with mean znBd and variance σ2

y, i.e.,

p(yd
n|

zn, Bd) =

(yd
n|

znBd, σ2

y),

N
such that, when conditioned on the pseudo-observations, the latent variable model behaves as a standard IBP with
Gaussian observations. Additionally, Section 2.1 details the mapping functions from the real line
into each of the
discrete and continuous spaces.Using auxiliary Gaussian variables to link a latent model with the observations has
been previously used in Gaussian processes for multi-class classiﬁcation [10] and for ordinal regression [5]. However,

(cid:60)

1For convenience, we capitalized here the notation for the weight vectors Bd.

3

to our knowledge, this simple approach has not been used to account for mixed continuous and discrete data. The
existent approaches for the IBP with discrete observations propose non-conjugate likelihood models and approximate
inference algorithms [23, 24, 30].

The resulting generative model is shown in Figure 11, where Z is the IBP latent matrix, and Yd and Bd contain,
k for the d-th dimension of the data. Additionally, Ψd
n and the weight factors bd
respectively, the pseudo-observations yd
denotes the set of auxiliary random variables needed to obtain the observation vector xd given Yd, and
d contains
the hyper-parameters associated to the random variables in Ψd. In order to obtain more interpretable results while
performing data exploration, we also assume that the latent feature matrix Z might be extended to contain an extra
latent feature that is active for every object in the data, playing the role of a bias term similar to [23, 24, 30].

H

2.1 Mapping Functions

n, i.e., that maps from the real line

In this section, we deﬁne the set of functions that transforms the pseudo-observations yd
n into the corresponding
observations xd
to the (continuous or discrete) observation space of the d-th
attribute describing the data. Since each attribute (dimension) in X may contain any discrete or continuous data types,
we provide a mapping function for each kind of data and the corresponding likelihood function for heterogeneous
data.

(cid:60)

2.1.1 Continuous Variables

In the case of continuous variables, we assume that the mapping functions are of the form x = f (y + u), where f (
)
·
is a continuous invertible and differentiable function and u corresponds to additive Gaussian noise with variance σ2
u.
In such case, we can obtain the corresponding likelihood function (after integrating out the pseudo-observation yd
n) as

p(xd
n|

zn, Bd) =

1

2π(σ2

y + σ2
u)

exp

−

2(σ2

(cid:26)

1
y + σ2
u)

(f −

1(xd
n)

znBd)2

−

f −

1(xd
n)

,

(2)

d
dxd
n

(cid:27) (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:113)

1(
where f −
), i.e., f −
) is the inverse function of the transformation f (
·
·
mapping functions that account for real-valued and positive real-valued data.

1(f (v)) = v. Next, we provide examples of

Real-valued Data. In order to obtain real-valued observations, i.e., xd
maps from the real numbers to the real numbers, i.e., fd :
y + u. Therefore, each observation is distributed as xd
can be used, e.g., one might opt for the transformation

(znbd
(cid:60)

n ∼ N

(cid:60) → (cid:60)

, σ2

, we need a transformation over yd

n that
. The simplest case is to assume that x = fd(y +u) =
u). Nevertheless, other mapping functions

n ∈ (cid:60)
y + σ2

x = fd(y + u) = w(y + u) + µ,

where w and µ are parameters which allow the user to scale or shift the attribute. A common choice would be taking
w = 1/V(cid:97)(cid:114)[xd] and µ = E[xd], which normalise the data. The corresponding the auxiliary variables and hyper-
σ2
parameters are Ψd =
u, w, µ
Positive Real-valued Data. In order to obtain positive real-valued observations, i.e., xd
transformation over yd
invertible and differentiable function. An example of this kind of functions is:

n ∈ (cid:60)+, we can apply any
(cid:60) → (cid:60)+, as long as fd is an

n that maps the real numbers to the positive real numbers, i.e., fd :

un
d }
{

d =

and

.
}

H

{

fd(y) = log(exp(wy + µ) + 1),

where w and µ are hyper-parameters. Similarly to the case of real-valued attributes, here we also make use of the
Gaussian variable ud

d =

n, therefore, Ψd =

n to obtain xd

n from yd

and

un
d }

{

H

σ2
u, w, µ
{

.
}

2.1.2 Discrete Variables

In the case of discrete variables, there is not a general way to map the real line into any type of discrete variable,
therefore, we need to derive a different transformation for each of the considered types of discrete variables, i.e.,
categorical, ordinal and count data.

4

Categorical Data. Let xd
Hence, assuming a multinomial probit model, we can then write:

n be a categorical observation, namely, it can take values in the index set given by

.
1, . . . , Rd}

{

nr|

(yd

r, σ2

znbd

nr ∼ N

y) where bd

with yd
ﬂuence of the k-th feature for the observation xd
pseudo-observations yd
r
1, . . . , Rd}
factors in a K
×

nr and weight vectors bd

Under this observation model, we can write yd

∈ {

argmax

r
n) =

n = fd(yd
xd
r denotes the K-length weight vector, in which each bd

1, . . . , Rd}

yd
nr,

∈ {

kr measures the in-
n taking value r. Under this likelihood model, we have as many
r per observation as number of categories in the d-th attribute, i.e.,
Rd matrix Yd and the weight

(3)

. In this case, the pseudo-observations can be gathered in the N
Rd matrix Bd.

×

y, therefore, we can obtain, as described in [10], the probability of each element xd

n taking value r

nr = znbd

r + ud

nr, where ud

nr is a Gaussian noise variable with

variance σ2
1, . . . , Rd}
{

as

p(xd

n = r

zn, Bd) = E
|

p(u)

Φ

u + zn(bd

bd
j )

r −

,

(cid:35)

(cid:17)

Rd

(cid:34)

j=1
(cid:89)
=r
j

(cid:16)

where the subscript r in bd
function of the standard normal distribution and E

r refers to the column in Bd (r

p(u)[

(0, σ2

y). Thus, the auxiliary variables and hyper-parameters are deﬁned as Ψd =

N
Ordinal Data. Consider ordinal data, in which each element xd
Then, assuming an ordered probit model, we can write

∈ {

), Φ(

1, . . . , Rd}

) denotes the cumulative distribution
·
] denotes expectation with respect to the distribution p(u) =
·
σ2
un
.
u}
d }
{
{
n takes values in the ordered index set

d =

and

H

.
1, . . . , Rd}

{

1
2

Rd

...

if yd
if θd

θd
n ≤
1
1 < yd
n ≤

θd
2

if θd
Rd

1 < yd
n

−

n = fd(yd
xd

n) = 



n is Gaussian distributed with mean znBd and variance σ2

where again yd
thresholds that divide the real line into Rd regions. We assume that the thresholds θd
the truncated Gaussian distribution θd
the value of xd
unique weight vector Bd and a unique Gaussian variable yd

are the
1, . . . , Rd −
r are sequentially generated from
. In this case,
Rd = +
n falls and, as opposed to the categorical case, now we have a

n is determined by the region in which yd

1), where θd

r > θd
r
−

y, and θd

θ )I(θd

r ∝ N

r for r

(θd
r |

and θd

0, σ2

0 =

−∞

∈ {

∞

1

}

n for each observation xd
n.
n taking value r

1, . . . , Rd}

∈ {

can be

Under the ordered probit model [5], the probability of each element xd

written as

p(xd

n = r

zn, Bd) = Φ
|

θd
r −

znBd
σy

(cid:32)

Φ

(cid:33) −

(cid:32)

znBd

θd
r

−

1 −
σy

.
(cid:33)

Let us ﬁnally remark that, if the d-th dimension of the observation matrix contains ordinal data, the set of auxiliary
variables reduces to the thresholds Ψd =

, and thus,

1, . . . , θd
θd
Rd
{

1}

−

d =

σ2
.
θ }
{
n takes non-negative integer values, , xd

H

Count Data. In the case of count data, each observation xd
Then, we assume

v

where
function that maps the real numbers to the positive real numbers. We can therefore write the likelihood function as

returns the ﬂoor of v, that is the largest integer that does not exceed v, and f

(cid:60)+ :

(cid:98)

(cid:99)

n = fd(yd
xd

n) =

f

(cid:60)+(yd
n)

,
(cid:99)

(cid:98)

n ∈ {

0, . . . ,

.

∞}

(7)
(cid:60) → (cid:60)+ is an invertible

p(xd
n|

zn, Bd) = Φ

(cid:32)

f −

1(xd

n + 1)
σy

−

znBd

Φ

(cid:33) −

(cid:32)

f −

1(xd
n)
σy

−

znBd

,
(cid:33)

1
where f −
(cid:60)+ → (cid:60)
(cid:60)+
variables Ψd or hyper-parameters

is the inverse function of the transformation f

(cid:60)+(
d which is speciﬁed by empty sets.

:

). In this case, there are no auxiliary random
·

H

5

∈

(4)

(5)

(6)

(8)

3 Inference

In this section, we describe our algorithm for learning the latent variables given the observation matrix. In order to
learn jointly the latent vectors zn, the weight factors Bd, and the auxiliary variables Ψd, we use a Markov Chain
Monte Carlo (MCMC) inference scheme. Such methods have been broadly applied to infer the IBP matrix, e.g., in
[12, 33, 27]. The proposed inference algorithm, summarized in Algorithm 1, exploits the information in the available
data to learn similarities among objects (captured in our model by the latent feature matrix Z), and identiﬁes how these
latent features show up in the attributes that describe the objects (captured in our model by Bd).

Bd
{

Yd
{

In Algorithm 1, we ﬁrst update the latent matrix Z. Note that conditioned on

D
d=1, both the latent matrix Z
D
Bd
D
d=1 are independent of the observation mthereforeatrix X. Additionally, since
and the weight matrices
d=1
}
{
}
D
Yd
D
D
Z).
and
d=1 to obtain p(
d=1 are Gaussian distributed, we can marginalize out the weight matrices
d=1|
}
{
}
}
In order to learn the matrix Z, we apply the collapsed Gibbs sampler which presents better mixing properties than the
uncollapsed version. As a consequence, is the standard method of choice in the context of the standard linear-Gaussian
IBP [12]. However, this algorithm suffers from a high computational cost, its is cubic in the number of data points
N at every iteration, which is a prohibitive cost when dealing with large databases. We use of the accelerated Gibbs
sampler [8] as an alternative, fast albeit approximate, scheme for inference. This algorithm presents linear complexity
with the number of objects N in the observation matrix.

Bd
{

Yd

{

}

Second, we sample the weight factors in Bd, which is a K

otherwise, it is a K-length column vector. We denote each column vector in Bd by bd
vectors is given by

×

Rd matrix in the case of categorical attributes,
r. The posterior over the weight

yd

p(bd
r , Z) =
r|
r , with yd
r = Z(cid:62)yd

(bd
r|

N

P−

1λd

r, P−

1),

BIk and λd

r the r-th column of Yd. Here, r takes values in

where P = Z(cid:62)Z + 1/σ2
in
1, . . . , Rd}
{
1
the case of categorical observations, while r = 1 for the rest of types of variables. Note that the covariance matrix P−
depend neither on the dimension d nor on r, so we only need to invert the K
K matrix P once at each iteration. In
Section 3.1, we describe how to efﬁciently sample Z, as well as how to efﬁciently compute P after the corresponding
changes are made in the Z matrix by rank one updates. Because of this, we manage to bypass the computation of
the matrix product Z(cid:62)Z. Once we have updated Z and Bd, we sample each element in Yd from the distribution
n, zn, Bd) speciﬁed in Section 3.2,
xd
n is missing, and from the posterior p(yd
N
otherwise. Finally, we sample the auxiliary variables in Ψd from their posterior distributions (also detailed in the
Section 3.2) if necessary2 This two latter steps involve, in the worst case, sampling from a doubly truncated univariate
normal distribution, for which we make use of the algorithm in [22].

y) if the observation xd

znbd

r, σ2

(yd

nr|

nr|

×

(9)

Algorithm 1 Inference Algorithm.

Input: X
Initialize: Z and

Yd
{

D
d=1
}

1: for each iteration do
Yd
Update Z given
2:
{
for d = 1, . . . , D do

D
d=1 as detailed in Section 3.1.
}

3:
4:
5:
6:
Output: Z,

Sample Bd given Z and Yd according to (9).
Sample Yd given X, Z and Bd as shown in Section 3.2.
Sample Ψd (if needed) as shown in Section 3.2.
D
d=1

Bd
{

D
d=1 and
}

Ψd
{

}

2The set of auxiliary variables for the d-dimension, Ψd, can be augmented to contain the variance of the pseudo-observations Yd associated
d and for which we assume an inverse-gamma prior with parameters β1 and β2. Under this prior
r )/2, where Sd is equal

to the d-th attribute, which we denote by σ2
d is an inverse-gamma with parameters β1 + N Sd/2 and β2 + (cid:80)N
distribution, the posterior of σ2
to the number of categories Rd for those dimensions d that contain categorical attributes, and it is equal to Sd = 1, otherwise.

nr − znbd

r=1(yd

(cid:80)Sd

n=1

6

3.1 Details on the Accelerated Gibbs Sampler

In this section we we review and adapt the sampler in [8], where the authors presented a linear-time accelerated Gibbs
sampler for conjugate IBP models that effectively marginalize out the weight factors. The per-iteration complexity
(N (K 2 + KD)), which is comparable to the uncollapsed linear-Gaussian IBP sampler that has
of this algorithm is
(N DK 2) but does not marginalize out the weight factors, and as a result, presents slower
per-iteration complexity
convergence rate. Next, we explain how to adapt this algorithm for the proposed IBP model for heterogeneous data.

O

O

The accelerated Gibbs sampling algorithm exploits the Bayes rule to avoid the cubic complexity with N due to
the computation of the marginal likelihood in the collapsed Gibbs sampler. In particular, it applies the Bayes rule to
obtain the probability of each element in the latent feature matrix Z of being active as

p(znk = 1

Yd

D
d=1, Z
}

¬

|{

nk)

∝

m

n,k

¬
N

p(yd

zn, bd

r)p(bd
r|

yd
¬

nr|

nrZ

n)dbd
r,

¬

(10)

D

Sd

(cid:89)d=1

r=1 (cid:90)bd
(cid:89)

r

where Sd is the number of columns in matrices Yd and Bd (Sd is the number of categories Rd for those dimensions
n corresponds to matrix Z after removing the n-th
d that contain categorical attributes, and Sd = 1 otherwise), Z
¬
row, vector yd
n) is the posterior of bd
n, Z
r
¬
computed without taking the n-th datapoint into account, i.e.,

nr is the r-th column of matrix Yd without the element yd

nr, and p(bd
r|

xd
¬

¬

n = Z(cid:62)
¬

where P
In this case, we condition on the Gaussian pseudo-observations
compute the marginal distribution p(znk = 1

nZ
¬

BIK and λd

n + 1/σ2

nk).

Yd

¬

¬

D
d=1, Z
}
¬

|{

p(bd
r|

n) =

¬

nr, Z

yd
¬
ny = Z(cid:62)
¬

(bd
r|

P−
¬

nλd
1
¬

nr, P−
¬

1
n),

N
nyd
nr are the natural parameters of the Gaussian distribution.
¬
D
d=1, instead of the actual observations X, to
}

Yd

{

(11)

In contrast to [8], we use the natural parameterisation for the Gaussian distribution over the posterior of bd

r instead
of the mean and covariance matrix. This formulation allows us to compute the full posterior over the weight factors as

p(bd
r|

yd

r , Z) =

(bd
r|

N

P−

1λd

r, P−

1).

In the accelerated Gibbs sampling scheme, we iteratively sample the value of each element znk, after marginalizing

nr are the natural parameters of the Gaussian distribution.

P = P

n + z(cid:62)n zn and λd

¬

r = λd
nr + z(cid:62)n yd
¬

out the weight factors Bd, according to

p(znk = 1

Yd

D
d=1, Z
}
¬

|{

nk)

∝

m

n,k

¬
N

D

Sd

(yd

nr|

znλd
¬

N

nr, znP

nz(cid:62)n + σ2

y).

¬

(cid:89)d=1
For each object n we ﬁrst sample the existing latent features znk for k = 1, . . . , K+ (where K+ is the number of
non-zero columns in Z, or number of active features up to this iteration). Succesively, we sample the number of new
features necessary to explain that data point from a Poisson distribution with mean α/N , as proposed in [12].

r=1
(cid:89)

(12)

(13)

3.2 Posterior Distribution over the Pseudo-observations

In Algorithm 1, we need to sample the pseudo-observations yd
sponding posterior distributions. The posterior distributions for yd
of data are given by:

nr and the auxiliary variables in Ψd from their corre-
nr, and for Ψd, if needed, for all the considered types

p(yd

n1|

n, zn, Bd) =
xd

yd
n1

N (cid:32)

znbd
1
σ2
y

+

1
d (xd
n)
f −
σ2

u (cid:19) (cid:18)

1
σ2
y

+

1
σ2

u (cid:19)

1

−

,

1
σ2
y

+

1
σ2

u (cid:19)

(cid:18)

1

−

,

(cid:33)

(14)

1. For real-valued observation:

1
where f −
d

:

.
(cid:60) → (cid:60)

(cid:18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

7

2. For positive real-valued observations:

1
where f −
d

:

.
(cid:60)+ → (cid:60)

3. For categorical observations:

(cid:18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

p(yd

n1|

n, zn, Bd) =
xd

yd
n1

N (cid:32)

znbd
1
σ2
y

+

1
d (xd
f −
n)
σ2

u (cid:19) (cid:18)

1
σ2
y

+

1
σ2

u (cid:19)

1

−

,

1
σ2
y

+

1
σ2

u (cid:19)

(cid:18)

1

−

,

(cid:33)

p(yd

nr|

n = T, zn, Bd) =
xd

(yd
(yd

nr|
nr|

N
N

(cid:26)

znbd
znbd

r, σ2
r, σ2

y)I(yd
y)I(yd

nr > maxj
nr < yd
nT )

=r(yd

nj))

If
If

r = T
= T
r

In Equation (16), if xd
n = T = r, we sample yd
nj). Otherwise, we
sample from a Gaussian right-truncated by yd
nr corresponds to solving
a multinomial probit regression problem. In order for the model to be identiﬁable, without loss of generality, we
assume that the regression function fRd (zn) is identically zero. Therefore, we ﬁx bd

nr from a Gaussian left-truncated by maxj
n. Sampling the variables yd
nr with r = xd

kRd = 0 for all k.

=r(yd

4. For ordinal observations:

In this case, we sample yd
also need to sample the threshold values θd

p(yd

n = r, zn, Bd)
xd

(yd

znbd

1, σ2

y)I(θd

r

1 < yd

θd
r ).

−

n1|

∼ N

n1|
n1 ≤
n1 form a Gaussian left-truncated by θd
1 and right-truncated by θd
r
−
r with r = 1, . . . , Rd −
1 as
0, σ2
r > max(θd
r
n1|
−
xd
(yd
n = r + 1)).

θ )I(θd
(θd
r |
I(θd
r < min(θd

xd
n = r))

1, max
n

yd
n1)

∼N

(yd

p(θd
r |

r , min
n

n1|

×

r . In this case, we

In this case, sampling the variables yd
thresholds
θr}
{
our case, to zero.

n1 corresponds to solving an ordered probit regression problem, where the
Rd
r=1 are unknown. In order for the model to be identiﬁable we set one of the thresholds, θ1 in

(15)

(16)

(17)

(18)

p(yd

n1|

xd
n, zn, Bd) =

(yd

n1|

N

znbd

1, σ2

y)I(f −
1
(cid:60)+

(xd
n)

≤

yd
n1 < f −

1(xd

n + 1)),

(19)

(cid:60)+ → (cid:60)

. We sample yd

1
n1 from a Gaussian left-truncated by f −
(cid:60)+

(xd

n) and right-truncated by

5. For count observations:

1
where f −
:
(cid:60)+
1
(xd
n + 1).
f −
(cid:60)+

4 Applications

In this section, we apply the proposed model to solve two different tasks on several real-world datasets. In Section 4.1,
we focus on a prediction task in which we aim to estimate and replace the missing data, which is assumed to be missing
completely at random. These results have been previously introduced in [29]. Then, in Section 4.2, we focus on a data
analysis task on several real-world datasets from different application domains including psychiatry, clinical trials and
politics. Weexploratory data analysis show how to use the proposed model to perform exploratory data analysis, i.e.,
to ﬁnd the latent structure in the data and capture the statistical dependencies among the objects and their attributes in
the data.

8

4.1 Missing Data Estimation

In this section, we use the proposed model to estimate missing data in heterogeneous databases, where we assume that
the data is missing completely at random (MCAR). Missing data may occur in diverse applications due to different
reasons. For example, participants of a survey may decide not to respond (or skip) some questions of the survey;
participants in a clinical study may drop out during the course of the study; or users of a recommendation system can
only rate a small fraction of the available books, movies, or songs, due to time constraints. The presence of missing
values might be challenging when the data is used for reporting, information sharing and decision support, and as
a consequence, handling missing data has captured the attention in diverse areas of data science such as machine
learning, data mining, and data warehousing and management [26, 21].

The extensive literature in probabilistic missing data estimation and imputation focus on homogeneous databases
which contain only either continuous data, usually modeled as Gaussian variables [28], or discrete data, that can be
either modeled by discrete likelihoods [18] or simply treated as Gaussian variables [25, 28]. However, up to our
knowledge, there is a lack of approaches for missing data estimation in heterogeneous databases, where the standard
approach is to treat all the attributes, either continuous or discrete, as Gaussian variables.

Experimental Setup. We evaluate the predictive power of the proposed model at estimating missing data on ﬁve
real databases, which are summarized in Table 1. The datasets contain different numbers of instances and attributes,
which cover all the discrete and continuous variables described in Section 2. According to our proposed model, the
D
probability distribution over the observation matrix is fully characterized by the latent matrices Z and
d=1 (as
}
well as the auxiliary variables Ψd). Hence, if we assume the latent vector zn for the n-th datapoint and the weight
factors Bd (and the auxiliary variables Ψd) to be known, we have a probability distribution over missing observations
n by sampling from this distribution3, or by simply taking either its mean,
n from which we can obtain estimates for xd
xd
mode or median value, once the latent matrix Z and the latent weight factors Bd (and Ψd) have been learnt.

Bd
{

Here, we compare in terms of predictive log-likelihood the following methods for missing data estimation:

•
•
•

The proposed general table completion approach denoted by GLFM (see Section 2).
The standard linear-Gaussian IBP [12] denoted by SIBP, treating all the attributes as Gaussian.
The Bayesian probabilistic matrix factorization approach [25] denoted by BPMF, that also treats all the attributes
in X as Gaussian distributed.

For the GLFM, we consider for the real positive and the count data the following transformation that maps from the
real numbers to the real positive numbers, f (x) = log(exp(wx) + 1), where w is a user-deﬁned hyper-parameter
that scales the data. Before running the SIBP and the BPMF methods, we normalized each column in the matrix X
to have zero-mean and unit-variance. This normalization ensures that the Gaussian likelihood evaluations of all the
attributes describing the objects in each dataset are comparable, regardless of their discrete or continuous nature. As
a consequence, ittherefore provides more accurate and fair results than applying the SIBP and BPMF directly on the
data without prior normalization. Additionally, since both the SIBP and the BPMF assume continuous observations,
when dealing with discrete data, we estimate each missing value as the closest integer value to the “de-normalized”
Gaussian variable.

Results. Figure 2 plots the average predictive log-likelihood per missing value as a function of the percentage of
missing data. Each value in Figure 2 was obtained by averaging the results across 20 independent split sets where the
missing values were randomly chosen. In Figures 2c and 2b, we cut the plot at a missing percentage of 50% because,
in these two databases, the discrete attributes present a mode value that appears for more than 80% of the instances.
As a consequence, the SIBP and the BPMF algorithms assign probability close to one to the mode, which results in an
artiﬁcial increase in the average test log-likelihood for larger percentages of missing data. For BPMF, we have used
different numbers of latent features (in particular, 10, 20 and 50), although we only show the best results for each
database, speciﬁcally, K = 10 for the NESARC and the wine databases, and K = 50 for the remainder. Both the
GLFM and the SIBP have not learnt a number of binary latent features above 25 in any case. In Figure 2e, we only
plot the test log-likelihood for the GLFM and SIBP because BPMF provides much lower values. As expected, we
observe in Figure 2 that the average test log-likelihood decreases for the three models as the number of missing values

3Note that sampling from this distribution might be computationally expensive. In this case, we can easily obtain samples of xd

the structure of our model. In particular, we can simply sample the auxiliary Gaussian variables yd
for xd

n by applying the corresponding transformation, detailed in Section 2.1.

n by exploiting
n given zn and Bd, and then obtain an estimate

9

Dataset
Statlog German credit dataset
[9]
QSAR biodegradation dataset
[20]
Internet usage survey dataset
[4]
Wine quality dataset [6]

N
1,000

1,055

1,006

6,497

D
20 (10 C + 4 O + 6
N)
41 (2 R + 17 P + 4
C + 18 N)
32 (23 C + 8 O + 1
N)
12 (11 P + 1 N)

NESARC dataset [24]

43,000

55 C

Description
Information about the credit risks of the applicants.

Molecular descriptors of biodegradable and non-
biodegradable chemicals.
Responses of the participants to a survey related to the
usage of internet.
Results of physicochemical tests realized to different
wines.
Responses of the participants to a survey related to per-
sonality disorders.

Table 1: Description of datasets. ‘R’ stands for real-valued variables, ‘P’ for positive real-valued variables, ‘C’ for
categorical variables, ‘O’ for ordinal variables and ‘N’ for count variables

(a) Statlog.

(b) QSAR biodegradation.

(c) Internet usage survey.

(d) Wine quality.

(e) Nesarc database.

Figure 2: Average test log-likelihood per missing datum versus percentage of missing data. The ‘whiskers’ show
one standard deviation away from the average test log-likelihood.

increases (ﬂat shape of the curves are due to the logarithmic scale of the y-axis). In this ﬁgure, we also observe that
the proposed GLFM outperforms SIBP and BPMF for four of the databases, being SIBP slightly better for the Internet
database. The BPMF model presents the worst test log-likelihood in all databases.

Successively, we analyzed the performance of the three models for each kind of discrete and continuous variables.
Figure 3 shows the average predictive likelihood per missing value for each attribute in the table, which corresponds to
each dimension in X. In this ﬁgure, we have grouped the dimensions according to the kind of data that they contain,
the x-axis shows the number of considered categories for the case of categorical and ordinal data (in the case of the
NESARC database, all the attributes are binary). The ﬁgure shows that the GLFM presents similar performance for all
the attributes in the ﬁve databases, while for the SIBP and the BPMF models, the test log-likelihood falls drastically
for some of the attributes, with this effect being more dramatic in the case of BPMF (it explains the low log-likelihood
in Figure 2). This effect is even more evident in Figures 2b and 2d. In Figures 2 and 3, we observe that both IBP-based
approaches (GLFM and SIBP) outperform BPMF, with the proposed GLFM being the one that best performs across
all databases. We can conclude that, unlike BPMF and SIBP, GLFM provides accurate estimates for the missing data
regardless of their discrete or continuous nature.

10

(a) Statlog.

(b) QSAR biodegradation.

(c) Internet usage survey.

(d) Wine quality.

(e) Nesarc database.
Figure 3: Average test log-likelihood per missing datum in each dimension. Here, we consider 50% of missing
data.
In the x-axis ‘R’ stands for real-valued variables, ‘P’ for positive real-valued variables, ‘C’ for categorical
variables, ‘O’ for ordinal variables and ‘N’ for count variables. The number that accompanies ‘C’ or ‘O’ corresponds
to the number of categories. In the Nesarc dataset, all the variables are binary, i.e., ‘C2’.

11

Attribute description
Stage of the cancer
DES treatment level
Tumor size in cm2
Serum Prostatic Acid Phosphatase (PAP)
Prognosis Status (outcome of the disease)

Type of variable
Categorical with 2 categories
Ordinal with 3 categories
Count data
Positive real-valued
Categorical with 4 categories

Table 2: List of considered attributes for the Prostate Cancer dataset.

4.2 Data Exploration

In this section, we describe how to use the GLFM for a data exploration task: the latent structure that underlies the
data is discovered and analized in three different application domains. Speciﬁcally, we use the proposed GLFM in the
context of i) clinical trials to discover the effects of a new drug for prostate cancer; ii) psychiatry to capture the impact
of social background in the development of mental disorders; and iii) politics to identify meaningful demographic
proﬁles, together with their geographic location, and voting tendencies in the United States.

The main goal of this section is to show how to include the speciﬁc domain knowledge into the proposed GLFM to
ease the data exploration process. In particular, we here show examples of how to select the input data for the GLFM,
as well as how to input these data into the model, in order to obtain interpretable results that can be used to get a better
understanding of the corresponding knowledge domain.

4.2.1 Drug effect in a clinical trial for prostate cancer

Clinical trials are conducted to collect data in order to determine the safety and efﬁcacy of a new drug before it can be
sold in the consumer market. Concretely, the main goal of clinical trials is to prove the efﬁcacy of a new treatment for
a disease while ensuring its safety, i.e., check whether its adverse effects remain low enough for any dosage level of
the drug. As an example, the publicly available Prostate Cancer dataset4 collects data of a clinical trial that analized
the effects of the drug diethylstilbestrol (DES) as a treatment against prostate cancer. The dataset contains information
about 502 patients with prostate cancer in stages5 3 and 4, who entered a clinical trial during 1967-1969 and were
randomly allocated to different levels of treatment with DES. The prostate cancer dataset have been used by several
studies [3, 14, 19] to analyze the survival times of the patients in the clinical trial and the causes behind their death.
These studies have pointed out that a large dose of the treatment tend to reduce the risk of a cancer death, but it might
also result in an increased risk of cardiovascular death. In this section, we apply the proposed GLFM to the Prostate
Cancer dataset to show that the proposed model can be efﬁciently used to discover the statistical dependencies in the
data, which in this example corresponds to the effect of the different levels of treatment with DES in the suffering of
prostate cancer and cardiovascular diseases.

Experimental Setup. The prostate cancer dataset consists of 502 patients and 16 attributes, from which we make
use of the ﬁve attributes listed in Table 2. The selection of these ﬁve attributes allows us to focus onlytherefore on
capturing the statistical dependencies between the target attributes, i.e, the relationship between the different levels of
treatment with DES and the suffering of prostate cancer and cardiovascular diseases. In our experiments, we sample
the variance of the pseudo-observations in each dimension and choose α = 5, σ2
θ = 1. We also consider
for the positive real and count data the following transformation that maps from the real numbers to the positive real
µ) + 1), where µ = min(xd) and w = 2/std(xd) are data-driven parameters whose
numbers: f (x) = log(w
objective is to shift and scale the data. In order to obtain more interpretable results, we also activate the bias term, i.e.,
a latent feature forced to be active for all patients.

B = 1, and σ2

(x

−

·

Results. After running our model, we obtain four latent features, whose empirical probabilities of activation and main

4Database available at: http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets
5The stage of a cancer describes the size of a cancer and how far it has grown. Stage 3 means that the cancer is already quite large and may have
started to spread into surrounding tissues or local lymph nodes. Stage 4 is more severe, and refers to a cancer that has already spread from where
it started to another body organ. This is also called secondary or metastatic cancer. Find more details in http://www.cancerresearchuk.
org/about-cancer/what-is-cancer/stages-of-cancer

12

Feature
Feature F1
Feature F2
Feature F3
Feature F4

Empirical Prob. Main implications

0.1952
0.2689
0.1594
0.1155

Favors stage 3, low DES levels and prostatic death
Favors stage 3, highest DES levels, and cardiovascular death
Favors stage 4, low DES levels, and mid-level prostatic death
Favors stage 4, low DES levels, and most severe prostatic cancer

Table 3: Empirical feature activation probabilities in the Prostate Cancer dataset. We show the empirical proba-
bility of possessing at least one latent feature. These probabilities are directly computed from the inferred IBP matrix
Z. Additionally, the table summarizes the main implications of the activation of each latent feature.

Patterns
Empirical Prob.

(0000)
0.4641

(0100)
0.1394

(1000)
0.0936

(0010)
0.0757

(0001)
0.0518

(1100)
0.0438

(0110)
0.0359

(0101)
0.0259

(1010)
0.0219

Table 4: Empirical probability of pattern activation for the top-nine most popular patterns. These probabilities
are computed directly from the inferred IBP matrix Z.

implications (which are discussed further below) are shown in Table 3. Additionally, Table 4 shows the nine most
common latent feature vectors, referred here as feature patterns, which capture over 95% of the observations. In order
to study the effect of the latent features on each attribute of the dataset, Figure 4 shows the inferred distribution of each
attribute for the ﬁve most common patterns, which happen to only have one active latent feature (plus the bias term).
We remark that GLFM assumes that the contribution of each latent feature is additive, therefore, patterns with more
than one active latent feature can be seen as combinations of the depicted patterns.6

In Figure 4, we can distinguish two groups of features. The ﬁrst group accounts for patients in stage 3 and includes
the bias term and the two ﬁrst latent features. Within this group, the bias term – or equivalently pattern (0000) – and
the ﬁrst feature –or equivalently pattern (1000) – account for patients in stage 3 with a low average level of treatment
with DES, as shown in Figure 4b. However, while the bias term models patients with low probability (
15%) of
prostate cancer death, the ﬁrst feature accounts for patients with higher probability (
40%) of prostate cancer death,
which can be explained by a larger tumor size, as shown in Figure 4c. The second feature – or equivalently pattern
(0100) –captures patients who exclusively received a high dosage (5 mg) of the drug, as shown in Figure 4b. These
patients present a small tumor size and the lowest probability of prostatic cancer death, suggesting a positive effect of
the drug as treatment for the cancer. However, they also present a signiﬁcant increase in the probability of dying from
50%), indicating a potential adverse-effect of the drug that increases the risk of suffering from
a vascular disease (
cardio-vascular diseases. Such observation is in agreement with previous studies [3, 14, 19].

∼

∼

∼

The second group of features corresponds to the activation patterns (0010) and (0001), and accounts for patients
in stage 4 with mild and severe conditions, respectively. In particular, the third feature corresponds to patients with
small tumor size, but intermediate values for the PAP biomarker, suggesting a certain spread in the degree of the tumor
compared to the features in the ﬁrst group, but not as severe as for patients with pattern (0001). Indeed, pattern (0001)
models those patients in stage 4 with relatively high tumor size and the highest PAP values – it is thus not surprising
that those patients present the highest probability (above 50%) of prostatic death.

4.2.2 Impact of Social Background on Mental Disorders

In this section, we aim at extending the analysis in [24] to account for the inﬂuence of the social background of
subjects (such as age, gender, etc.) in the probability of a subject suffering from a comorbid disorder. To this end, in
addition to the diagnoses of the the 20 most common psychiatric disorders detailed below, we also make use of the
information provided by the NESARC database7, which includes information both on the mental condition and on the
social background of participants.

Several studies have analyzed the impact of social background in the development of mental disorders. These stud-
ies usually focus on the relation between a mental disorder and a speciﬁc aspect of the social background of subjects.
Some examples in this area study the relation between depression and gender [32, 15], or the link between common
mental disorders and poverty or social class [31, 7, 13]. Other studies [2, 24] have focused on ﬁnding and analyzing

6In the case of patterns with multiple active latent features, the bias term should be counted only once.
7Database available at: http://aspe.hhs.gov/hsp/06/catalog-ai-an-na/nesarc.htm

13

Attribute description
Gender
Age
Census region
Race/ethnicity
Marital status
Highest grade or years of school completed

Type of variable
Categorical with 2 categories
Count data
Categorical with 4 categories
Categorical with 5 categories
Categorical with 6 categories
Ordinal with 14 categories

Table 5: List of considered social background attributes. We look for correlations between each of these attributes
with the twenty most common psychiatric disorders among the subjects in the NESARC database.

the co-occurring (comorbidity) pattern among the 20 most common psychiatric illnessesto our knowledge. These
studies found that the 20 most common disorders can be divided into three meta-groups of disorders: i) externaliz-
ing disorders, which include substance use disorders (alcohol abuse and dependence, drug abuse and dependence and
nicotine dependence); ii) internalizing disorders, which include mood and anxiety disorders (major depressive disorder
(MDD), bipolar disorder and dysthymia, panic disorder, social anxiety disorder (SAD), speciﬁc phobia and general-
ized anxiety disorder (GAD), and pathological gambling (PG)); and iii) personality disorders (avoidant, dependent,
obsessive-compulsive (OC), paranoid, schizoid, histrionic and antisocial personality disorders (PDs)). Additionally,
they also found that comorbid or co-occurring disorders tend to belong to the same group of disorders [30]. However,
to our knowledge, there is a lack of work in the study of the impact of social background in the suffering of comorbid
disorders.

Experimental Setup. The NESARC database contains the responses of a representative sample of the U.S. population
to a survey with questions related to the social background of participants, alcohol and other drug consumption, and
behaviors related to mental disorders. The ﬁrst wave of NESARC sampled the adult U.S. population with over 43,000
respondents who answered almost 3,000 questions. The dataset also include the diagnoses for each of the participants
of the survey. In this experiment, in addition to the diagnoses of the 20 most common psychiatric disorders described
above, we include one by one each of the social background questions as input data to the proposed model. Table 5
summarizes the considered questions and how we introduce them into our model as input variables. Here, we introduce
each attribute related to the social background of the participants independently to ensure that the model captures the
dependencies between latent disorders and social background, instead of correlations among the different aspect of
the social background. Note also that the diagnoses of the 20 psychiatric disorders correspond to categorical variables
with two possible categories, e.g., a patient suffering or not from a disorder.

y = 1, σ2

B = 1, σ2

For the following experimental results, we run the inference algorithm in Section 3 for each question independently
with α = 5, σ2
θ = 1, and consider for the positive real and count data the following transformation
that maps from the real numbers to the positive real numbers: f (x) = x2, which we chose to show that the proposed
model works with any differentiable and invertible function. Similarly to [24], we activate the bias term, i.e., an
additional latent feature which is active to all the subjects in the data set, so that we do not sample the rows of Z
corresponding to those subjects who do not suffer from any of the 20 disorders, but instead ﬁx their latent features
to zero. The idea is that the bias captures the population that does not suffer from any disorder, while the rest of
active features in matrix Z characterize the disorders. As we will see next, the use of the bias term aims to ease the
interpretability of the inferred latent features.

Results. After running our model, we ﬁnd that the census region, race/ethnicity, marital status and educational level
(i.e., highest grade or years of school completed) do not appear to have any inﬂuence in the comorbidity patterns of
the 20 most common psychiatric disorders. In contrast, as detailed below, gender and age of the participants inﬂuence
the probability of suffering from a set of comorbid (co-occurring) disorders.

‘male’, ‘female’
{

Gender. We model the gender information of the participants in the NESARC as a categorical variable with two
categories:
. The percentage of males in the NESARC is approximately 43%. In this case, the
GLFM found three latent features, whose empirical probabilities can be found in Table 6. Moreover, Table 7 shows
empirical probability of each feature pattern in the dataset. Here, we observe that the three latent feature activate
mostly in isolation, being the combination of two features rare (below 1%). Figure 5a shows the probability of

}

14

meeting each diagnostic criteria for the latent feature vectors zn listed in the legend and in the database (baseline).
Note that the obtained latent features are similar to the ones in [24], i.e., Feature F1 – pattern (100) – mainly models
the seven personality disorders (PDs), Feature F2, which corresponds to pattern (010), models the alcohol and drug
abuse disorders and the antisocial PD, while Feature F3 – pattern (001) – models the anxiety and mood disorders.
Additionally, in Figure 5b, we show the probability of being male and female for the latent feature vectors zn shown
in the legend and the empirical probability of being male and female in the database (baseline).

In Figure 5b, we observe that having no active features (pattern (000), which captures people that do not suffer from
any disorder), increases the probability of being male with respect to the baseline probability, therefore, it indicates
that females tend to suffer in a higher extent from psychiatric disorders. Additionally, we observe that Feature F1 –
pattern (100) – increases the probability of being male, while Feature F3 – pattern (001) – increases the probability of
being female. Hence, from the analysis of Figure 5b, we can conclude that, while women suffer more frequently from
mood and anxiety disorders than men, PDs are more common in men.

Feature
Feature F1
Feature F2
Feature F3

Empirical Prob. Main implications

0.0341
0.0470
0.0460

Increases the probability of personality disorders and male gender
Increases the probability of alcohol and drug abuse disorders
Increases the probability of anxiety and mood disorders and female gender

Table 6: Gender: empirical probabilities of possessing at least one latent feature. These probabilities are directly
computed from the inferred IBP matrix Z.

Patterns
Empirical Prob.

(000)
0.8615

(010)
0.0427

(001)
0.0414

(100)
0.0298

(111)
0.0023

(011)
0.0022

(110)
0.0020

Table 7: Gender: Empirical probability of feature pattern activations. These probabilities are computed directly
from the inferred IBP matrix Z.

Age. Now, we focus on the age of the participants, which we model as count data. After running our inference
algorithm with the diagnoses of the 20 disorders and the age of the subjects as input data, we again obtain three latent
features, whose empirical probabilities are listed in Table 8. Moreover, Table 9 shows empirical probability of each
feature pattern in the dataset. Here, we observe that the three latent features activate mostly in isolation, being the
combination of two features rare (below 3%).

Figure 6a shows the probability of meeting each diagnostic criteria for the latent feature vectors zn listed in
the legend and in the database (baseline). In addition to the baseline probability distribution, we plot in Figure 6b
the inferred probability distributions over the age when none or only one of the latent variables is active (which
correspond to the most common feature patterns). The empirical probability distribution over the age based on the
data is shown (and denoted by ‘baseline’) in Figure 6b. Here, we observe that introducing the age of the participants
as an input variable has changed the inferred latent features (with respect to the features in [24] depicted in Figure 5a).
In particular, we observe that the obtained latent features mainly differ in the probability of suffering from personality
disorders (i.e., disorders from 14 to 20), being the probability of suffering from disorders 1 to 13 similar for the three
plotted latent feature patterns. In this ﬁgure, we observe that the vector zn with no active latent features, e.g., pattern
(000), is trying to capture the mean of the age in the database (which coincides with middle-aged subjects, i.e., 30
50
years old). Moreover, we observe that the subjects with the highest probability of suffering from personality disorders
– pattern (100) – are likely to be middle-aged, followed in a decreasing order by young adults – pattern (010) – and
elderly people – pattern (001). Additionally, if we focus on the differences among the three features in disorders from
1 to 13, we also observe that, while young and elderly people tend to suffer from depression, middle-aged people tend
to suffer from the bipolar disorder. Hence, based on Figure 6, we can conclude that the bipolar disorder and the seven
personality disorders tend to show up in a higher extent in the mature age, while young and elderly people tend to
suffer more often from depression.

−

15

Feature
Feature F1
Feature F2
Feature F3

Empirical Prob.
0.0332
0.0550
0.0569

Main implications
Captures severe personality disorders and middle-age subjects
Captures mid-severe personality disorders and young subjects
Captures mid-severe personality disorders and older subjects

Table 8: Age: empirical probabilities of possessing at least one latent feature. These probabilities are directly
computed from the inferred IBP matrix Z.
(000)
0.8615

Patterns
Empirical Prob.

(101)
0.0019

(100)
0.0294

(010)
0.0522

(001)
0.0503

(011)
0.0028

(110)
0.0019

Table 9: Age: Empirical probability of feature pattern activations. These probabilities are computed directly from
the inferred IBP matrix Z.

4.2.3 Voters proﬁle in presidential election

Finally, we apply the proposed model to understand the correlations between demographic proﬁles and political vote
tendencies. In particular, we focus on the United States presidential election of 1992, in which three major candidates
ran for the race: the incumbent Republican president George H. W. Bush, the Democratic Arkansas governor Bill
Clinton, and the independent Texas businessman Ross Perot. In 1992, the public’s concern about the federal budget
deﬁcit and fears of professional politicians allowed the independent candidacy of billionaire Texan Ross Perot to
appear on the scene dramatically [1], to the point of even leading against the major party candidates in the polls during
the electoral race8. The race ended up with the victory of Bill Clinton by a wide margin in the Electoral College,
receiving 43% of the popular vote against Bush’s 37.5% and Perot’s 18.9% [16]. These results are noted for being the
highest vote share of a third-party candidate since 1912, even if Perot did not obtain any electoral votes [16].

Our primary objective in this section is to ﬁnd and analyze the different types of voters’ proﬁles, as well as
which candidate each proﬁle tends to favor. To this aim, we used the publicly available Counties database gathering
diverse information about voting results, demographics and sociological factors per counties9. This dataset contains
information for 3141 counties. Table 10 lists the per-county attributes that we used as input for our model.

Attribute description
State in which the county is located
Population density in 1992 per squared miles
% of white population in 1990
% of people with age above 65 in 1990
% of people above 25 years old with bachelor’s degree or higher
Median family income in 1989 (in dollars)
% of farm population in 1990
% of votes cast for Democratic president
% of votes cast for Republican president
% of votes cast for Ross Perot

Type of data
Categorical with 51 categories
Positive real data
Positive real data
Positive real data
Positive real data
Count data
Positive real data
Positive real data
Positive real data
Positive real data

Table 10: List of considered attributes regarding the United States presidential election of 1992. Attributes 1 to
7 include demographic information and sociological factors, while the last three attributes summarize the percentage
voting outcome in each county.

Experimental Setup. We run our inference algorithm with α = 5, σ2
θ = 1 and the mapping transformation
µ) + 1), with µ = min(xd) and w =
from the real numbers to the positive real numbers: f (x) = log(w
2/std(xd). In this experiment, we activate the bias term and sample the variance of the pseudo-observations for each
dimension/attribute. A challenging aspect of this database is that the distributions of some of its attributes are heavy-
tailed, leading to a large number of latent features as output of the GLFM, whose purpose is to capture the tails of the
distributions. This is not an issue for estimation and imputation of missing data, but it renders data exploration more

B = 1, σ2
(x

−

·

8New York Times: http://www.nytimes.com/1992/06/11/us/

the-1992-campaign-on-the-trail-poll-gives-perot-a-clear-lead.html

9Database available at: http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets

16

Feature
Feature F1

0.4874

Empirical Prob. Main implications

Feature F2

0.2703

Feature F3

0.2700

Feature F4
Feature F5

0.0411
0.0372

Favors Perot, increases the probability of white population, and decreases
average income.
Favors the Democrat candidate, increases population density, and decreases
family income, percentages of white population, farming and college de-
grees.
Favors the Republican candidate and Perot, increases the percentage of
farming, and decreases population density.

Capture the tails of the distributions of different attributes.

Table 11: Empirical feature activation probabilities for the Counties dataset. We show the empirical probability
of possessing at least one latent feature, as well as the main implications of the activation of each feature. These are
directly computed from the inferred IBP matrix Z.

Patterns

(110)
(100)
Empirical Prob. 0.2636 0.2407 0.1063 0.1060 0.0748

(000)

(101)

(010)

Table 12: Empirical probability of pattern activation for the top-ﬁve most popular patterns. These probabilities
are computed directly from the inferred IBP matrix Z. Features F4 and F5 are always switched off, and are thus
omitted from the labels.

−

tedious. To solve this limitation, we here perform an additional data preprocessing step by applying a logarithmic
transformation to heavy-tailed attributes. In more detail, we apply the function g1(x) = log(x + 1) for population
density, median family income, and percentage of farm population. For the percentage of white population, we used
the function g2(x) = log((100

x) + 1) since the distribution has most of its density close to 100%.

Results. Running the GLFM on this data results in 5 latent features, whose empirical activation probabilities are
shown in Table 11. Here, we observe that while the ﬁrst three features are active for at least 27% of the counties, the
last two features are active only for around 4% of the counties. Moreover, we ﬁnd that the different combinations of
the three ﬁrst latent features represent more than 92% of the counties in USA. In the following, we will thus focus
on the analysis of the three ﬁrst features and, in particular, on the top-ﬁve most popular feature patterns. We show in
Table 12 the empirical probabilities of these ﬁve patterns, which represent around 80% of the U.S. counties. Figure 7
shows the distribution of vote percentage per candidate associated to each of these top-ﬁve patterns, while Figure 8
shows the corresponding geographic distribution (i.e., the empirical activation probability) across states for each of
these patterns. In these ﬁgures, we observe that:

(i) pattern (000), corresponding to the bias term, tends to model middle values for the percentage of votes for the
48% for the
27% for Perot), and activates mainly in the east and west coasts of the country, as

three candidates (with an average percentage of votes of
Republican candidate and
well as Florida;

50% for the Democrat candidate,

∼

∼

∼

(ii) pattern (100) provides similar percentage of votes for the Democrat and Republican candidates as in pattern
(000), but it favors the independent candidate Perot (with an average percentage of votes above 30%), and
activates mostly in the north central-east region of the country and Maine (the state where Perot’s party managed
to beat the Republican party);

(iii) pattern (101) activates in the north central-west region of the USA (not including the coast) and represents
55%) while also

a proﬁle inclined towards the Republican party (with an average percentage of votes of
favoring in a lower extent the independent candidate; and

∼

(iv) patterns (010) and (001) clearly capture Democrat-oriented proﬁles, and activate mainly in the south east region

of the USA, including the state from which Bill Clinton comes from, Arkansas.

Note that the demographic results above are in agreement with the outcome of the election per counties10, as shown in
Figure 9. Next, we analyze the demographic information associated to each of the feature patterns above. To this end,
we show in Figure 10 the distribution of each attribute/dimension of the data for each of the considered patterns. First,

10https://en.wikipedia.org/wiki/United_States_presidential_election,_1992

17

we observe that pattern (000), which activates mostly in the coasts and Florida, corresponds to the highest population
density, average income, and percentage of college degrees, as well as an important race diversity and low farming
activity. These observations align with the typical proﬁle characterizing “big-cities”. As stated before, this pattern
is the most balanced in terms of voting tendency, with an equilibrated support for both Democrat and Republican, as
well as intermediate values for the percentage of votes cast for Perot.

Second, patterns (100) and (101) represent the largest share of Perot’s votes, both with an average percentage of
votes above 30% for Perot. Figure 10 shows that Perot’s main supporters, characterized mainly by pattern (101),
also correspond to Republican main supporters, who tend to live in low populated areas in the north central part of
the country where farming activity is considerable, and the percentages of white population and over-65 years old
population are also high. The second voting force backing Perot, captured by pattern (100) and located in the north
east-central part of USA, corresponds mostly to white population with an intermediate-high average income and an
average percentage of college degrees around 18% (the red curve in Figure 10e overlaps the green line). These results
back the analysis in [17], which showed that the majority of Perot’s voters (57%) were middle class, earning between
$15,000 and $49,000 annually, with the bulk of the remainder drawing from the upper middle class (29% earning more
than $50,000 annually). Perot’s campaign ended up taking 18.9% of the votes, ﬁnishing second in Maine and Utah, as
captured by pattern (100) and (101) respectively.

Finally, Democrat’s patterns (010) and (110) are mainly active in the Southeastern United States, and capture a
diverse range of voters in terms of their demographic properties. On the one hand, pattern (010) captures highly
populated counties, with low values of family income, percentage of college degrees, percentage of white population
and percentage of farming population. On the other hand, pattern (110) captures low populated counties with a
large percentage of population above 65 year old, as well as a larger presence of farming activity and lower average
income. These results might be explained by the broad appeal across all socio-ethno-economic demographics that the
Democratic party has historically targeted.

5 Conclusions

In this paper, we developed an efﬁcient general latent feature model, named GLFM, suitable for modelling and infer-
ence with real-world heterogeneous datasets. The proposed model presents attractive properties in terms of ﬂexibility
and interpretability. Its nonparametric nature allows it to automatically infer the appropriate model complexity (i.e.,
number of latent features) from data. Secondly, the fact that the latent features are binary-valued makes it easier to
identify and interpret meaningful patterns in the data exploration process. Thirdly, the model shares the properties
of conjugate models, which allow us to propose an efﬁcient inference scheme that scales linearly with the number of
observations (objects) and dimensions (attributes) in the data.

We showed the ﬂexibility and applicability of the proposed GLFM by solving both prediction and data exploration
tasks in several real-world datasets. In particular, we used the proposed model to estimate and replace missing data
in heterogeneous databases. Also, we used the GLFM for data exploratory analysis of real-world datasets related to
diverse application domains: clinical trials, psychiatry, sociology and politics. The software library provides the ﬁrst
available software for latent feature modeling in heterogeneous data, and includes user-friendly functions for missing
data estimation and data exploratory analysis. The software package is publicly available at: https://github.
com/ivaleraM/GLFM and reviewed in Appendix 6. We hope to bring a powerful tool for researchers from diverse
knowledge domains to help them analyze a wide range of datasets in an automatic manner.

6 Acknowledgements

Isabel Valera acknowledges her Humboldt Research Fellowship for Postdoctoral Researchers. Maria Lomeli and
Zoubin Ghahramani acknowledge support from the Alan Turing Institute (EPSRC Grant EP/N510129/1) and EPSRC
Grant EP/N014162/1, and donations from Google and Microsoft Research. Melanie F. Pradier is grateful to the
European Union 7th Framework Programme through the Marie Curie Initial Training Network “Machine Learning
for Personalized Medicine” MLPM2012, Grant No. 316861 (http://mlpm.eu). This work has been partly supported

18

by MINECO/FEDER (’ADVENTURE’, id. TEC2015-69868-C2-1-R), and Comunidad de Madrid (project ’CASI-
CAM-CM’, id. S2013/ICE-2845).

References

[1] R Michael Alvarez and Jonathan Nagler. Economics, issues and the perot candidacy: voter choice in the 1992

presidential election. American Journal of Political Science, pages 714–744, 1995.

[2] C. Blanco, R. F. Krueger, D. S. Hasin, S. M. Liu, S. Wang, B. T. Kerridge, T. Saha, and M. Olfson. Mapping com-
mon psychiatric disorders: Structure and predictive validity in the National Epidemiologic Survey on Alcohol
and Related Conditions. Journal of the American Medical Association Psychiatry, 70(2):199–208, 2013.

[3] D. P. Byar and S. B. Green. The choice of treatment for cancer patients based on covariate information: applica-

tion to prostate cancer. Bulletin du Cancer, 67:477–490, 1980.

[4] Pew Research Centre. 25th anniversary of the web. Available on: http://www.pewinternet.org/datasets/january-

2014-25th-anniversary-of-the-web-omnibus/, 2014.

[5] W. Chu and Z. Ghahramani. Gaussian processes for ordinal regression. J. Mach. Learn. Res., 6:1019–1041,

December 2005.

[6] P. Cortez, A. Cerdeira, F. Almeida, T. Matos,
data mining from physicochemical properties.
http://archive.ics.uci.edu/ml/datasets.html, 47(4):547–553, 2009.

and J. Reis.

Modeling wine preferences by
Decision Support Systems. Dataset available on:

[7] B. P. Dohrenwend. Sociocultural and social-psychological factors in the genesis of mental disorders. Journal of

Health and Social Behavior, 16(4):365–392, 1975.

[8] F. Doshi-Velez and Z. Ghahramani. Accelerated sampling for the indian buffet process. In Proceedings of the
26th Annual International Conference on Machine Learning, ICML ’09, pages 273–280, New York, NY, USA,
2009. ACM.

[9] J. Eggermont, J. N. Kok, and W. A. Kosters. Genetic programming for data classiﬁcation: Partitioning the search
space. In In Proceedings of the 2004 Symposium on applied computing (ACM SAC04). Dataset available on:
http://archive.ics.uci.edu/ml/datasets.html, pages 1001–1005. ACM, 2004.

[10] M. Girolami and S. Rogers. Variational Bayesian multinomial probit regression with Gaussian process priors.

Neural Computation, 18:2006, 2005.

[11] P. Gopalan, F. J. R. Ruiz, R. Ranganath, and D. M. Blei. Bayesian Nonparametric Poisson Factorization for

Recommendation Systems. International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2014.

[12] T. L. Grifﬁths and Z. Ghahramani. The Indian buffet process: an introduction and review. Journal of Machine

Learning Research, 12:1185–1224, 2011.

[13] A. B. Hollingshead and F. C. Redlich. Social stratiﬁcation and psychiatric disorders. American Sociological

Review, 18(2):163–169, 1953.

[14] R. Kay. Treatment effects in competing-risks analysis of prostate cancer data. Biometrics, 42(1):203–211, 1986.

[15] R. C. Kessler, K. A. McGonagle, M. Swartz, D. G. Blazer, and C. B. Nelson. Sex and depression in the national
comorbidity survey i: Lifetime prevalence, chronicity and recurrence. Journal of Affective Disorders, 29(2):85–
96, 1993.

[16] D. Lacy and B. C Burden. The vote-stealing and turnout effects of ross perot in the 1992 us presidential election.

American Journal of Political Science, pages 233–255, 1999.

19

[17] P. Lewis, C. McCracken, and R. Hunt. Politics: Who cares. American Demographics, pages 16–23, 1994.

[18] X.-B. Li. A Bayesian approach for estimating and replacing missing categorical data. J. Data and Information

Quality, 1(1):3:1–3:11, June 2009.

[19] M. Lunn and D. McNeil. Applying cox regression to competing risks. Biometrics, 51(2):524–532, 1995.

[20] K. Mansouri, T. Ringsted, D. Ballabio, R. Todeschini, and V. Consonni. Quantitative structure activity relation-
ship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling. Dataset
available on: http://archive.ics.uci.edu/ml/datasets.html, 2013.

[21] R. Mazumder, T. Hastie, and R. Tibshirani. Spectral regularization algorithms for learning large incomplete

matrices. Journal of machine learning research, 11(Aug):2287–2322, 2010.

[22] C. P. Robert. Simulation of truncated normal variables. Statistics and computing, 5(2):121–125, 1995.

[23] F. J. R. Ruiz, I. Valera, C. Blanco, and F. Perez-Cruz. Bayesian nonparametric modeling of suicide attempts.

Advances in Neural Information Processing Systems, 25:1862–1870, 2012.

[24] F. J. R. Ruiz, I. Valera, C. Blanco, and F. Perez-Cruz. Bayesian nonparametric comorbidity analysis of psychiatric

disorders. Journal of Machine Learning Research., 2013.

[25] R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markov Chain Monte Carlo.
Proceedings of the 25th International Conference on Machine Learning (ICML’08), pages 880–887, 2008.

[26] J. L. Schafer and J. W Graham. Missing data: our view of the state of the art. Psychological methods, 7(2):147,

[27] M. Titsias. The inﬁnite gamma-Poisson feature model. Advances in Neural Information Processing Systems, 19,

2002.

2007.

[28] A. Todeschini, F. Caron, and M. Chavent. Probabilistic low-rank matrix completion with adaptive spectral
regularization algorithms.
In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger,
editors, Advances in Neural Information Processing Systems 26, pages 845–853. Curran Associates, Inc., Dec.
2013.

[29] I. Valera and Z. Ghahramani. General table completion using a bayesian nonparametric model. Advances in

Neural Information Processing Systems 27 (NIPS 2014), 27:981–989, 2014.

[30] I. Valera, F. J. R. Ruiz, P. M. Olmos, C. Blanco, and F. Perez-Cruz.

Inﬁnite continuous feature model for

psychiatric comorbidity analysis. Neural computation, 2016.

[31] S. Weich and G. Lewis. Poverty, unemployment, and common mental disorders: population based cohort study.

BMJ, 317(7151):115–119, 1998.

[32] M. M. Weissman, Bland R., P. R. Joyce, S. Newman, J.E. Wells, and H.-U. Wittchen. Sex differences in rates of
depression: cross-national perspectives. Journal of Affective Disorders, 29:77 – 84, 1993. Special Issue Toward
a New Psychobiology of Depression in Women.

[33] S. Williamson, C. Wang, K. Heller, and D. Blei. The IBP compound Dirichlet process and its application to
focused topic modeling. Proceedings of the 27th Annual International Conference on Machine Learning, 2010.

20

GLFM: General Latent Feature Modeling Toolbox

A Introduction
The GLFM is a general Bayesian nonparametric latent feature model suitable for data exploration or missing values
imputation in heterogeneous datasets, where the attributes describing each object can be either discrete, continuous
or mixed variables. Speciﬁcally, the GLFM accounts for real-valued and positive real-valued data as examples of
continuous variables, and categorical, ordinal and count data as examples of discrete variables.

The GLFM builds on the Indian Buffet Process [12], and therefore, it assumes that each observation xd

n can be
explained by a potentially inﬁnite-length binary vector zn whose elements indicate whether a latent feature is active
or not for the n-th object; and a (real-valued) weighting vector Bd, whose elements weight the inﬂuence of each
latent feature in the d-th attribute11. Since the product of the latent feature vector and the weighting vector leads to
a real-valued variable, it is necessary to map this variable to the desirable output (continuous or discrete) space, for
. Thus, the GLFM assumes the existence
example, the positive real line or the ﬁnite ordered set
of intermediate real-valued auxiliary variables yd
d), called pseudo-observations, and a transformation
n ∼ N
) that maps this variable into the actual observation xd
function fd(
u) is
·
an auxiliary noise with zero mean and small variance σ2
u. Figure 11 illustrates the GLFM for a ordinal attribute taking
values in the ordered set

low, medium, high
}
{
(znBd, σ2
n = fd(yd
n, i.e., xd

n + u) where u

(0, σ2

∼ N

low, medium, high
.
}

{

B Implementation
The GLFM package contains an efﬁcient C++ implementation, together with an user interface in Python, Matlab
and R, of the collapsed Gibbs sampling algorithm described in Algorithm 1. The main function of the package,
hidden = GLFM.infer(data),12 runs the inference algorithm given the input structure data and returns the
learned latent variables in the output structure hidden. This function receives as input an observation matrix X and a
vector indicating the type of data for each dimension (optionally, model hyper-parameters and simulation settings can
be customized by the user). The latent variables are learned by making use of the mapping transformations listed in
Table 13 to account for the continuous and discrete data types mentioned above. Here, the parameters µ and w are used
to shift and scale the raw input data, and are respectively set to the empirical mean and the standard deviation for real-
valued attributes, and to the minimum value and the standard deviation for positive real-valued and count attributes.
This guarantees that the prior distributions on the latent variables are equally good for all the attributes in the dataset,
regardless their support. The output structure hidden contains the latest MCMC sample of the latent feature vectors
zn for n = 1, . . . , N , the weighting vectors Bd, as well as auxiliary variables (including the pseudo-observation
variances σ2

d and the thresholds θr) necessary for the corresponding transformation fd(

Moreover, since the Bayesian nonparametric nature of the GLFM allows the model complexity (i.e., the length of
the vectors zn and Bd) to grow with the number of observations, we bound the model complexity to a ﬁnite value,
which is an additional input to the GLFM package. This bound allows us not only to keep the model complexity,
and therefore the running time, under control, but also to efﬁciently manage the memory allocation. Finally, our
implementation of the GLFM makes use of the GNU Scientiﬁc Library (GSL),13 to efﬁciently perform a large variety
of mathematical routines such as random number generation, and matrix or vector operations.

), for d = 1, . . . , D.
·

C Usage
C.1 Data preprocessing and initialization
A convenient property of the GLFM package is that it can be used blindly on raw data without requiring any pre-
processing step on the dataset, nor special tuning of hyperparameters. The only requirement for the user to use the
package is to format the data as a numerical matrix of size N
D and indicate in an additional vector the type of
data for each of the D attributes. As mentioned above, the parameters of the transformations in Table 13 are internally

×

11For agreement with the notation in [? ], we here capitalize the vector Bd.
12This call corresponds to a python call. The equivalent call in Matlab is hidden = GLFM infer(data) and in R output ← GLFM infer(data).
13https://www.gnu.org/software/gsl/

21

Type of Variable
Real-valued

Domain
x ∈ (cid:60)

Positive real-valued

x ∈ (cid:60)+

Transformation x = fd(y)
x = w(y + u) + µ

x = log(exp(w(y + u) + µ) + 1)

Categorical

x ∈ {1, 2, . . . , R} (unordered set)

x = arg maxr∈{1,...,R} yr

Ordinal

x ∈ {1, 2, . . . , R} (ordered set)

x =

if
if

y ≤ θ1
θ1 < y ≤ θ2

1
2





...

R

if

θR−1 < y

Count

x ∈ {1, 2, 3, . . .}

x = (cid:98)log(exp(w(y + u) + µ) + 1)(cid:99)

Table 13: Mapping functions implemented in the toolbox.

ﬁxed to ensure that the pseudo-observation for all attributes fall in a similar interval of the real line, so that the prior
distribution on the latent variables is equally good for all attributes.

However, we incorporate an additional functionality that allows the user to specify external preprocessing functions
to further improve the performance of the algorithm. For instance, in cases in which the distribution of an attribute
presents a clearly non-Gaussian behavior, e.g., it is concentrated around a single value or heavy-tailed, it might be
suitable to preprocess this variable by applying a logarithmic transformation, as shown Figure 12.

C.2 Missing Data Estimation
GLFM can be used for estimation and imputation of missing data in heterogeneous datasets, where the missing values
can be encoded with any (numerical) value that the user speciﬁes. The Bayesian nature of the GLFM allows to
efﬁciently infer the latent feature representation of the data using the available information (i.e., the non-missing
values), and using it to compute the posterior distribution of each missing value in the data. Note that given the
posterior distribution of each missing value, one might opt for different approaches to impute missing values, e.g.,
one might opt for imputing a sample of the posterior distribution or simply the maximum a posteriori (MAP) value.
The GLFM package provides the function [Xmap, hidden]=GLFM.complete(data) which infers the latent
feature representation, given the (incomplete) observation matrix, and returns a complete matrix where the missing
values have been imputed to their MAP value; and the hidden structure containing all the inferred latent variables. This
function therefore runs the inference function GLFM.infer(), as well as the function GLFM.computeMAP(),
which computes the MAP of a single missing element xd

n given zn and Bd.

C.3 Data Exploration Analysis
GLFM can also be used as a tool for data exploratory analysis, since it is able to ﬁnd the latent structure in the
data and capture the statistical dependencies among the objects and their attributes in the data. GLFM provides
(weighted) binary latent features, easing their interpretation and making it possible to cluster the objects according
to their activation patterns of latent features. Moreover, it also allows to activate a latent feature that is active for
all the objects (a.k.a. bias term), which might be useful to capture the mode of the distribution of each attribute
in the dataset. In order to ease data exploration, GLFM provides the function GLFM.plotPatterns(), which
plots the posterior distribution for of each attribute under the given latent feature patterns, and therefore, allows us to
ﬁnd patterns and dependencies across both objects and attributes. This function, in turn, makes use of the function
GLFM.computePDF(), which evaluates the posterior distribution of an attribute under a given latent feature vector.

C.4 Examples
The package manual contains simple examples demonstrating package usage. Additionally, we provide the following
demonstrations (with the scripts in Python, Matlab and R):

•

•
•

demo toy example: Simple illustration of GLFM pipeline, replicating the example of the IBP linear-Gaussian
model in [12].
demo completion: Illustration of missing data estimation on the MNIST image dataset.
demo data exploration (counties & prostate): Replication of results on data exploration in the main paper. This
demo requires data download, which is instructed.

22

D Availability and Documentation
GLFM code is publicly available in https://github.com/ivaleraM/GLFM, where we provide a technical
document introducing the model and a user manual describing the usage details of the toolkit, including software
requirements. The Python and Matlab implementations are under MIT license. The R implementation extends the
RcppGSLExample14, and therefore, is under GPL (>= 2) license.

14https://github.com/eddelbuettel/rcppgsl/tree/master/inst/examples/RcppGSLExample

23

(a) Type of Cancer

(b) Drug Level

(c) Size of Primary Tumor (cm2)

(d) Serum Prostatic Acid Phosphatase

(e) Prognosis Status

Figure 4: Data exploration of a prostate cancer clinical trial. We depict the effect of each latent feature on each
attribute. Panels (a)-(d) shows different indicators of the prostate cancer, as well as the dose level of DES. Panel (d)
corresponds to Prognosis Status, which indicates whether the patient either is alive or dies from one of the following
three causes: vascular disease, prostatic cancer, or other reason. The baseline refers to the empirical distribution of
each attribute in the whole dataset.

24

(a) Probability of suffering from each disorder

Figure 5: Feature effects including gender in the analysis. (a) Probabilities of suffering from the 20 considered
disorders and (b) probability of being male and female for the latent feature vectors zn shown in the legend and for
the baseline.

(b) Gender

25

(a) Probability of suffering from each disorder

(b) Age

26

Figure 6: Feature effects including age in the analysis. (a) Probabilities of suffering from the 20 considered disorders
and (b) distribution of the age for the latent feature vectors zn shown in the legend and baseline probability distribution.

(a) % of votes cast for Ross Perot

(b) % of votes cast for Republican candidate

27

(c) % of votes cast for Democrat candidate

Figure 7: Inferred probability distribution for the ﬁve most popular patterns. The patterns are sorted in the legend
according to their degree of popularity, as described in Table 12. The baseline refers to the empirical distribution of
each attribute in the entire dataset.

(a) Pattern (000)

(b) Pattern (100)

(c) Pattern (101)

(d) Pattern (010)

(e) Pattern (110)

Figure 8: Empirical probability of pattern activation per state. We focus on the top-ﬁve most popular combinations
of features. The label for each pattern indicates whether Features F1, F2, and F3 are active (value ‘1’) or not (value
‘0’). Features F4 are F5 are always inactive in the ﬁve most common patterns, and thus are omitted in the labels.

Figure 9: Outcome of the 1992 presidential election per counties. Blue color corresponds to a majority of votes
for the Democrat party, red corresponds to a victory for the Republican party, green corresponds to a victory of the
independent party of Ross Perot.

28

(a) Population density (inhabitants/miles2)

(b) Percentage of white population

(c) Percentage of people ≥ 65 years old

(d) Average income (in dollars)

(e) Percentage of college degrees

(f) Percentage of farming population

Figure 10: Inferred probability distribution for the most occuring patterns. The baseline refers to the empirical
distribution of each attribute in the whole dataset.

Figure 11: Illustration of the GLFM.

29

(a) Xd without preprocessing

(b) Xd with log preprocessing

(c) Pseudo-observations yd

Figure 12: Illustration of optional data preprocessing. Panel (a) and (b) show the histograms of, respectively, a heavy-
tailed attribute and the attribute after a logarithmic transformation, as well the distribution of the inferred latent feature
patterns. Panel (c) shows the histogram of the pseudo-observations inferred for the original and the preprocessed
attributes, as well the distribution of the inferred latent feature patterns. Here, we observe that the distribution of
the attribute is better captured by the latent model when a preprocessing step is performed to correct/minimize the
non-Gaussian behavior of the attribute.

30

8
1
0
2
 
r
a

M
 
8
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
9
7
7
3
0
.
6
0
7
1
:
v
i
X
r
a

General Latent Feature Models for Heterogeneous Datasets

Isabel Valera∗1, Melanie F. Pradier2, Maria Lomeli1, and Zoubin Ghahramani1,3

1University of Cambridge, Cambridge, UK
2Universidad Carlos III de Madrid, Madrid, Spain
3Uber AI Labs, San Francisco, US

Abstract

Latent feature modeling allows capturing the latent structure responsible for generating the observed properties
of a set of objects. It is often used to make predictions either for new observations or for missing information in
the original data, as well as to perform exploratory data analysis. However, although there is an extensive literature
on latent feature models for homogeneous datasets, where all the attributes that describe each object are of the same
(continuous or discrete) nature, there is a lack of work on latent feature modeling for heterogeneous databases. In
this paper, we introduce a general Bayesian nonparametric latent feature model suitable for heterogeneous datasets,
where the attributes describing each object can be either discrete, continuous or mixed variables. The proposed model
presents several important properties. Firstly, it accounts for heterogeneous data while keeping the properties of
conjugate models, which allows to infer the model in linear time with respect to the number of objects and attributes.
Secondly, its Bayesian nonparametric nature allows us to automatically infer the model complexity from the data, i.e.,
the number of features necessary to capture the latent structure in the data. Thirdly, the latent features in the model are
binary-valued, this facilitates the interpretability of the obtained latent features in exploratory data analysis. Finally, a
software package called GLFM toolbox is made publicly available for other researchers to use and extend. We show
the ﬂexibility of the proposed model by solving both prediction and data analysis tasks on several real-world datasets.

1 Introduction

One of the aims of unsupervised learning is recovering the latent structure responsible for generating the observed
properties or attributes of a set of objects. In particular, in latent feature modeling, one or more attributes of each object
can be represented by an unobserved vector of latent features. It is assumed that the observations are generated from
a distribution determined by those latent feature values. In other words, latent feature models allow us to represent,
with only a few features, the immense redundant information present in the observed data, by capturing the statistical
dependencies among the different objects and attributes. As a consequence, they have been broadly used to make
predictions either for new values of interest or missing information in the original data [25, 11], as well as to perform
exploratory data analysis, i.e, to better understand the data [2, 30].

There is an extensive literature in latent feature modeling for homogeneous data, where all the attributes describing
each object in the database are of the same type, that is continuous or discrete. In particular, these works assume that
databases contain only either continuous data, usually modeled as Gaussian variables [12, 28], or discrete, that can
be either modeled by discrete likelihoods [18, 24, 11] or simply treated as Gaussian variables [25, 2, 28]. However,
there still exists a lack of work dealing with heterogeneous databases, which in fact are common in real applications.
For instance, the Electronic Health Records of hospitals might contain lab measurements (often real-valued or positive
real-valued data), diagnoses (categorical data) and genomic information (ordinal, count data and categorical data).
Another example are surveys, which often contain diverse information about the participants such as age (count data),
gender (categorical data), salary (positive real data), etc. Despite the diversity of data types, the standard approach

∗Corresponding author: miv24@cam.ac.uk

1

when dealing with heterogeneous datasets is to treat all the attributes, either continuous or discrete, as Gaussian
variables.

This paper presents a general latent feature model (GLFM) suitable for heterogeneous datasets, where the attributes
describing each object can be either discrete, continuous or mixed variables. Speciﬁcally, we account for real-valued
and positive real-valued as examples of continuous variables, and categorical, ordinal and count data as examples of
discrete variables. The proposed model extends the essential building block of Bayesian nonparametric latent feature
models, the Indian Buffet Process (IBP) by [12], to account for heterogeneous data while maintaining the model com-
plexity of conjugate models. The IBP places a prior distribution over binary matrices where the number of columns,
corresponding to latent features, is potentially inﬁnite and can be inferred from the data. Among all the available latent
feature models in the literature, we opt for the IBP due to two main reasons. First, the nonparametric nature of the
IBP allows us to automatically infer the appropriate model complexity, i.e., the number of necessary features, from
the data. Second, the IBP considers binary-valued latent features which has been shown to provide more interpretable
results in data exploration than standard real-valued latent feature models [23, 24]. The standard IBP assumes real-
valued observations combined with conjugate likelihood models that allow for fast inference algorithms [8]. However,
we handle heterogeneous databases, such that conjugacy is not preserved.

In order to propose a general observation model for the IBP that accounts for heterogeneous data while keeping the
properties of conjugate models, we exploit two key ideas. First, we introduce an auxiliary real-valued variable (also
called pseudo-observation), such that, conditioned on it, the model behaves as the standard linear-Gaussian IBP in
[12]. Second, we assume that there exists a function that transforms the pseudo-observation into an actual observation,
mapping the real line into the (discrete or continuous) observation space of each attribute in the data. These two key
ideas allow us to derive an efﬁcient inference algorithm based on collapsed Gibbs sampling, which presents linear
complexity with the number of objects and attributes in the data.

We show the ﬂexibility and applicability of the proposed model by solving both prediction and data exploration
tasks in several real-world datasets. In particular, we ﬁrst use the proposed model to estimate and replace missing data
in heterogeneous databases (where the data is missing completely at random), showing that our approach for missing
data estimation outperforms, in terms of accuracy, the Bayesian probabilistic matrix factorization (BPMF) [25] and
the standard IBP which assumes Gaussian observations. These results have been previously discussed in [29], where
the main focus of the paper was missing data estimation (a.k.a. table completion). In contrast, this extended version of
the paper focuses on the model itself, providing the necessary details on the GLFM and its inference to perform latent
feature modeling in heterogeneous datasets, which is a powerful tool not only for missing data estimation but also for
exploratory data analysis tasks. This motivates the second part of the experiments, where we present several examples
of how to use the proposed model for data exploration in real-world datasets gathered from diverse application domains
related to medicine, psychiatry, sociology and politics.

The rest of the paper is organized as follows. In Section 2, we provide the details on the general Bayesian nonpara-
metric latent feature model for heterogeneous datasets. In Section 3, we develop an inference algorithm based on the
Gibbs sampler, where we make use of the introduced pseudo-observation to collapse the sampler. In Section 4, we ap-
ply our model to two real-world applications, missing data estimation and data analysis. Finally, Section 5 is devoted
to the discussion on the proposed latent feature model for heterogeneous datasets and its potential applications.

2 Latent Feature Model for Heterogeneous Data

n denote each entry of the observation matrix X, which might be of the following types:

D, each of the N objects is deﬁned by

We assume that the data can be stored in an observation matrix X of size N
a set of D attributes. Let xd
Continuous variables:
1. Real-valued, xd
2. Positive real-valued, xd

n ∈ (cid:60)

•

×

n ∈ (cid:60)+.

Discrete variables:

•

1. Categorical data, xd
2. Ordinal data, xd

n takes a value in a ﬁnite unordered set, e.g., xd
n ∈ {

n takes values in a ﬁnite ordered set, e.g., xd

‘blue’, ‘red’, ‘black’

n ∈ {
‘never’, ‘sometimes’, ‘often’, ‘usually’,

.
}

‘always’
}

.

3. Count data, xd

n ∈ {

0, . . . ,

.

∞}

2

Figure 1: Graphical Model for the Generalized Latent Feature Model. Grey nodes represent observed variables,
white nodes correspond to latent variables. Introducing the pseudo-observations Yd allow us to deal with heteroge-
neous data.

As in standard latent feature models, we assume that xd
associated to the n-th data point, zn = [zn1, . . . , znK], and a weight vector1 Bd = [bd
of latent variables), whose elements bd
Under this assumption, the likelihood can be factorized as

n can be explained by a K-length vector of latent features
K] (K being the number
k weight the contribution of the k-th latent feature to the d-th dimension of X.

1, . . . , bd

Z,

p(X
|

{

Bd, Ψd

D
d=1) =
}

p(xd
n|

zn, Bd, Ψd),

D

N

n=1
(cid:89)

(cid:89)d=1
where Ψd denotes the set of random variables necessary to deﬁne the distribution of the d-th attribute. Here, we
K matrix Z that follows an IBP
assume binary-valued latent binary feature vectors zn, which are gathered in an N
IBP(α) [12]. Additionally, we place a Gaussian distribution
prior with concentration parameter α, denoted by Z
with zero mean and covariance matrix σ2
is assumed to be
Gaussian with mean znBd for the d = 1, . . . , D attributes, the above model is equivalent to the standard IBP with
Gaussian observations [12], therefore can be efﬁciently learnt using the properties of the Gaussian distribution [8].
However, under heterogeneous (or non-Gaussian) observation matrices, developing an efﬁcient inference algorithm is
not straightforward, since the advantages of conjugate priors do not hold in the general case.

BIK over the weight vectors Bd. Note that if xd

n ∈ (cid:60)

×

∼

To solve this limitation, we introduce an auxiliary Gaussian variable yd

n in the observation matrix, and assume that there exists a transformation function fd(
·

n (which we might refer to as pseudo-
) over
into the observation space of the d-th attribute

n to obtain the observations xd

n, mapping the real line

observation) per entry xd
the variables yd
in the observation matrix Ωd, i.e.,

(1)

fd :

(cid:60) (cid:55)→
yd
n →

(cid:60)

.

Ωd
xd
n

Here, we assume that yd

n is Gaussian distributed with mean znBd and variance σ2

y, i.e.,

p(yd
n|

zn, Bd) =

(yd
n|

znBd, σ2

y),

N
such that, when conditioned on the pseudo-observations, the latent variable model behaves as a standard IBP with
Gaussian observations. Additionally, Section 2.1 details the mapping functions from the real line
into each of the
discrete and continuous spaces.Using auxiliary Gaussian variables to link a latent model with the observations has
been previously used in Gaussian processes for multi-class classiﬁcation [10] and for ordinal regression [5]. However,

(cid:60)

1For convenience, we capitalized here the notation for the weight vectors Bd.

3

to our knowledge, this simple approach has not been used to account for mixed continuous and discrete data. The
existent approaches for the IBP with discrete observations propose non-conjugate likelihood models and approximate
inference algorithms [23, 24, 30].

The resulting generative model is shown in Figure 11, where Z is the IBP latent matrix, and Yd and Bd contain,
k for the d-th dimension of the data. Additionally, Ψd
n and the weight factors bd
respectively, the pseudo-observations yd
denotes the set of auxiliary random variables needed to obtain the observation vector xd given Yd, and
d contains
the hyper-parameters associated to the random variables in Ψd. In order to obtain more interpretable results while
performing data exploration, we also assume that the latent feature matrix Z might be extended to contain an extra
latent feature that is active for every object in the data, playing the role of a bias term similar to [23, 24, 30].

H

2.1 Mapping Functions

n, i.e., that maps from the real line

In this section, we deﬁne the set of functions that transforms the pseudo-observations yd
n into the corresponding
observations xd
to the (continuous or discrete) observation space of the d-th
attribute describing the data. Since each attribute (dimension) in X may contain any discrete or continuous data types,
we provide a mapping function for each kind of data and the corresponding likelihood function for heterogeneous
data.

(cid:60)

2.1.1 Continuous Variables

In the case of continuous variables, we assume that the mapping functions are of the form x = f (y + u), where f (
)
·
is a continuous invertible and differentiable function and u corresponds to additive Gaussian noise with variance σ2
u.
In such case, we can obtain the corresponding likelihood function (after integrating out the pseudo-observation yd
n) as

p(xd
n|

zn, Bd) =

1

2π(σ2

y + σ2
u)

exp

−

2(σ2

(cid:26)

1
y + σ2
u)

(f −

1(xd
n)

znBd)2

−

f −

1(xd
n)

,

(2)

d
dxd
n

(cid:27) (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:113)

1(
where f −
), i.e., f −
) is the inverse function of the transformation f (
·
·
mapping functions that account for real-valued and positive real-valued data.

1(f (v)) = v. Next, we provide examples of

Real-valued Data. In order to obtain real-valued observations, i.e., xd
maps from the real numbers to the real numbers, i.e., fd :
y + u. Therefore, each observation is distributed as xd
can be used, e.g., one might opt for the transformation

(znbd
(cid:60)

n ∼ N

(cid:60) → (cid:60)

, we need a transformation over yd

n that
. The simplest case is to assume that x = fd(y +u) =
u). Nevertheless, other mapping functions

n ∈ (cid:60)
y + σ2

, σ2

x = fd(y + u) = w(y + u) + µ,

where w and µ are parameters which allow the user to scale or shift the attribute. A common choice would be taking
w = 1/V(cid:97)(cid:114)[xd] and µ = E[xd], which normalise the data. The corresponding the auxiliary variables and hyper-
σ2
parameters are Ψd =
u, w, µ
Positive Real-valued Data. In order to obtain positive real-valued observations, i.e., xd
transformation over yd
invertible and differentiable function. An example of this kind of functions is:

n ∈ (cid:60)+, we can apply any
(cid:60) → (cid:60)+, as long as fd is an

n that maps the real numbers to the positive real numbers, i.e., fd :

un
d }
{

d =

and

.
}

H

{

fd(y) = log(exp(wy + µ) + 1),

where w and µ are hyper-parameters. Similarly to the case of real-valued attributes, here we also make use of the
Gaussian variable ud

d =

n, therefore, Ψd =

n to obtain xd

n from yd

and

un
d }

{

H

σ2
u, w, µ
{

.
}

2.1.2 Discrete Variables

In the case of discrete variables, there is not a general way to map the real line into any type of discrete variable,
therefore, we need to derive a different transformation for each of the considered types of discrete variables, i.e.,
categorical, ordinal and count data.

4

Categorical Data. Let xd
Hence, assuming a multinomial probit model, we can then write:

n be a categorical observation, namely, it can take values in the index set given by

.
1, . . . , Rd}

{

nr|

(yd

r, σ2

znbd

nr ∼ N

y) where bd

with yd
ﬂuence of the k-th feature for the observation xd
pseudo-observations yd
r
1, . . . , Rd}
factors in a K
×

nr and weight vectors bd

Under this observation model, we can write yd

∈ {

argmax

r
n) =

n = fd(yd
xd
r denotes the K-length weight vector, in which each bd

1, . . . , Rd}

yd
nr,

∈ {

kr measures the in-
n taking value r. Under this likelihood model, we have as many
r per observation as number of categories in the d-th attribute, i.e.,
Rd matrix Yd and the weight

(3)

. In this case, the pseudo-observations can be gathered in the N
Rd matrix Bd.

×

y, therefore, we can obtain, as described in [10], the probability of each element xd

n taking value r

nr = znbd

r + ud

nr, where ud

nr is a Gaussian noise variable with

variance σ2
1, . . . , Rd}
{

as

p(xd

n = r

zn, Bd) = E
|

p(u)

Φ

u + zn(bd

bd
j )

r −

,

(cid:35)

(cid:17)

Rd

(cid:34)

j=1
(cid:89)
=r
j

(cid:16)

where the subscript r in bd
function of the standard normal distribution and E

r refers to the column in Bd (r

p(u)[

(0, σ2

y). Thus, the auxiliary variables and hyper-parameters are deﬁned as Ψd =

N
Ordinal Data. Consider ordinal data, in which each element xd
Then, assuming an ordered probit model, we can write

∈ {

), Φ(

1, . . . , Rd}

) denotes the cumulative distribution
·
] denotes expectation with respect to the distribution p(u) =
·
σ2
un
.
u}
d }
{
{
n takes values in the ordered index set

d =

and

H

.
1, . . . , Rd}

{

1
2

Rd

...

if yd
if θd

θd
n ≤
1
1 < yd
n ≤

θd
2

if θd
Rd

1 < yd
n

−

n = fd(yd
xd

n) = 



n is Gaussian distributed with mean znBd and variance σ2

where again yd
thresholds that divide the real line into Rd regions. We assume that the thresholds θd
the truncated Gaussian distribution θd
the value of xd
unique weight vector Bd and a unique Gaussian variable yd

are the
1, . . . , Rd −
r are sequentially generated from
. In this case,
Rd = +
n falls and, as opposed to the categorical case, now we have a

n is determined by the region in which yd

1), where θd

r > θd
r
−

y, and θd

θ )I(θd

r ∝ N

r for r

(θd
r |

and θd

0, σ2

0 =

−∞

∈ {

∞

1

}

n for each observation xd
n.
n taking value r

1, . . . , Rd}

∈ {

can be

Under the ordered probit model [5], the probability of each element xd

written as

p(xd

n = r

zn, Bd) = Φ
|

θd
r −

znBd
σy

(cid:32)

Φ

(cid:33) −

(cid:32)

znBd

θd
r

−

1 −
σy

.
(cid:33)

Let us ﬁnally remark that, if the d-th dimension of the observation matrix contains ordinal data, the set of auxiliary
variables reduces to the thresholds Ψd =

, and thus,

1, . . . , θd
θd
Rd
{

1}

−

d =

σ2
.
θ }
{
n takes non-negative integer values, , xd

H

Count Data. In the case of count data, each observation xd
Then, we assume

v

where
function that maps the real numbers to the positive real numbers. We can therefore write the likelihood function as

returns the ﬂoor of v, that is the largest integer that does not exceed v, and f

(cid:60)+ :

(cid:98)

(cid:99)

n = fd(yd
xd

n) =

f

(cid:60)+(yd
n)

,
(cid:99)

(cid:98)

n ∈ {

0, . . . ,

.

∞}

(7)
(cid:60) → (cid:60)+ is an invertible

p(xd
n|

zn, Bd) = Φ

(cid:32)

f −

1(xd

n + 1)
σy

−

znBd

Φ

(cid:33) −

(cid:32)

f −

1(xd
n)
σy

−

znBd

,
(cid:33)

1
where f −
(cid:60)+ → (cid:60)
(cid:60)+
variables Ψd or hyper-parameters

is the inverse function of the transformation f

(cid:60)+(
d which is speciﬁed by empty sets.

:

). In this case, there are no auxiliary random
·

H

5

∈

(4)

(5)

(6)

(8)

3 Inference

In this section, we describe our algorithm for learning the latent variables given the observation matrix. In order to
learn jointly the latent vectors zn, the weight factors Bd, and the auxiliary variables Ψd, we use a Markov Chain
Monte Carlo (MCMC) inference scheme. Such methods have been broadly applied to infer the IBP matrix, e.g., in
[12, 33, 27]. The proposed inference algorithm, summarized in Algorithm 1, exploits the information in the available
data to learn similarities among objects (captured in our model by the latent feature matrix Z), and identiﬁes how these
latent features show up in the attributes that describe the objects (captured in our model by Bd).

Bd
{

Yd
{

In Algorithm 1, we ﬁrst update the latent matrix Z. Note that conditioned on

D
d=1, both the latent matrix Z
D
Bd
D
d=1 are independent of the observation mthereforeatrix X. Additionally, since
and the weight matrices
d=1
}
{
}
D
Yd
D
D
Z).
and
d=1 to obtain p(
d=1 are Gaussian distributed, we can marginalize out the weight matrices
d=1|
}
{
}
}
In order to learn the matrix Z, we apply the collapsed Gibbs sampler which presents better mixing properties than the
uncollapsed version. As a consequence, is the standard method of choice in the context of the standard linear-Gaussian
IBP [12]. However, this algorithm suffers from a high computational cost, its is cubic in the number of data points
N at every iteration, which is a prohibitive cost when dealing with large databases. We use of the accelerated Gibbs
sampler [8] as an alternative, fast albeit approximate, scheme for inference. This algorithm presents linear complexity
with the number of objects N in the observation matrix.

Bd
{

Yd

{

}

Second, we sample the weight factors in Bd, which is a K

otherwise, it is a K-length column vector. We denote each column vector in Bd by bd
vectors is given by

×

Rd matrix in the case of categorical attributes,
r. The posterior over the weight

yd

p(bd
r , Z) =
r|
r , with yd
r = Z(cid:62)yd

(bd
r|

N

P−

1λd

r, P−

1),

BIk and λd

r the r-th column of Yd. Here, r takes values in

where P = Z(cid:62)Z + 1/σ2
in
1, . . . , Rd}
{
1
the case of categorical observations, while r = 1 for the rest of types of variables. Note that the covariance matrix P−
depend neither on the dimension d nor on r, so we only need to invert the K
K matrix P once at each iteration. In
Section 3.1, we describe how to efﬁciently sample Z, as well as how to efﬁciently compute P after the corresponding
changes are made in the Z matrix by rank one updates. Because of this, we manage to bypass the computation of
the matrix product Z(cid:62)Z. Once we have updated Z and Bd, we sample each element in Yd from the distribution
n, zn, Bd) speciﬁed in Section 3.2,
xd
n is missing, and from the posterior p(yd
N
otherwise. Finally, we sample the auxiliary variables in Ψd from their posterior distributions (also detailed in the
Section 3.2) if necessary2 This two latter steps involve, in the worst case, sampling from a doubly truncated univariate
normal distribution, for which we make use of the algorithm in [22].

y) if the observation xd

znbd

r, σ2

(yd

nr|

nr|

×

(9)

Algorithm 1 Inference Algorithm.

Input: X
Initialize: Z and

Yd
{

D
d=1
}

1: for each iteration do
Yd
Update Z given
2:
{
for d = 1, . . . , D do

D
d=1 as detailed in Section 3.1.
}

3:
4:
5:
6:
Output: Z,

Sample Bd given Z and Yd according to (9).
Sample Yd given X, Z and Bd as shown in Section 3.2.
Sample Ψd (if needed) as shown in Section 3.2.
D
d=1

Bd
{

D
d=1 and
}

Ψd
{

}

2The set of auxiliary variables for the d-dimension, Ψd, can be augmented to contain the variance of the pseudo-observations Yd associated
d and for which we assume an inverse-gamma prior with parameters β1 and β2. Under this prior
r )/2, where Sd is equal

to the d-th attribute, which we denote by σ2
d is an inverse-gamma with parameters β1 + N Sd/2 and β2 + (cid:80)N
distribution, the posterior of σ2
to the number of categories Rd for those dimensions d that contain categorical attributes, and it is equal to Sd = 1, otherwise.

nr − znbd

r=1(yd

(cid:80)Sd

n=1

6

3.1 Details on the Accelerated Gibbs Sampler

In this section we we review and adapt the sampler in [8], where the authors presented a linear-time accelerated Gibbs
sampler for conjugate IBP models that effectively marginalize out the weight factors. The per-iteration complexity
(N (K 2 + KD)), which is comparable to the uncollapsed linear-Gaussian IBP sampler that has
of this algorithm is
(N DK 2) but does not marginalize out the weight factors, and as a result, presents slower
per-iteration complexity
convergence rate. Next, we explain how to adapt this algorithm for the proposed IBP model for heterogeneous data.

O

O

The accelerated Gibbs sampling algorithm exploits the Bayes rule to avoid the cubic complexity with N due to
the computation of the marginal likelihood in the collapsed Gibbs sampler. In particular, it applies the Bayes rule to
obtain the probability of each element in the latent feature matrix Z of being active as

p(znk = 1

Yd

D
d=1, Z
}

¬

|{

nk)

∝

m

n,k

¬
N

p(yd

zn, bd

r)p(bd
r|

yd
¬

nr|

nrZ

n)dbd
r,

¬

(10)

D

Sd

(cid:89)d=1

r=1 (cid:90)bd
(cid:89)

r

where Sd is the number of columns in matrices Yd and Bd (Sd is the number of categories Rd for those dimensions
n corresponds to matrix Z after removing the n-th
d that contain categorical attributes, and Sd = 1 otherwise), Z
¬
row, vector yd
n) is the posterior of bd
n, Z
r
¬
computed without taking the n-th datapoint into account, i.e.,

nr is the r-th column of matrix Yd without the element yd

nr, and p(bd
r|

xd
¬

¬

n = Z(cid:62)
¬

where P
In this case, we condition on the Gaussian pseudo-observations
compute the marginal distribution p(znk = 1

nZ
¬

BIK and λd

n + 1/σ2

nk).

Yd

¬

¬

D
d=1, Z
}
¬

|{

p(bd
r|

n) =

¬

nr, Z

yd
¬
ny = Z(cid:62)
¬

(bd
r|

P−
¬

nλd
1
¬

nr, P−
¬

1
n),

N
nyd
nr are the natural parameters of the Gaussian distribution.
¬
D
d=1, instead of the actual observations X, to
}

Yd

{

(11)

In contrast to [8], we use the natural parameterisation for the Gaussian distribution over the posterior of bd

r instead
of the mean and covariance matrix. This formulation allows us to compute the full posterior over the weight factors as

p(bd
r|

yd

r , Z) =

(bd
r|

N

P−

1λd

r, P−

1).

In the accelerated Gibbs sampling scheme, we iteratively sample the value of each element znk, after marginalizing

nr are the natural parameters of the Gaussian distribution.

P = P

n + z(cid:62)n zn and λd

¬

r = λd
nr + z(cid:62)n yd
¬

out the weight factors Bd, according to

p(znk = 1

Yd

D
d=1, Z
}
¬

|{

nk)

∝

m

n,k

¬
N

D

Sd

(yd

nr|

znλd
¬

N

nr, znP

nz(cid:62)n + σ2

y).

¬

(cid:89)d=1
For each object n we ﬁrst sample the existing latent features znk for k = 1, . . . , K+ (where K+ is the number of
non-zero columns in Z, or number of active features up to this iteration). Succesively, we sample the number of new
features necessary to explain that data point from a Poisson distribution with mean α/N , as proposed in [12].

r=1
(cid:89)

(12)

(13)

3.2 Posterior Distribution over the Pseudo-observations

In Algorithm 1, we need to sample the pseudo-observations yd
sponding posterior distributions. The posterior distributions for yd
of data are given by:

nr and the auxiliary variables in Ψd from their corre-
nr, and for Ψd, if needed, for all the considered types

p(yd

n1|

n, zn, Bd) =
xd

yd
n1

N (cid:32)

znbd
1
σ2
y

+

1
d (xd
n)
f −
σ2

u (cid:19) (cid:18)

1
σ2
y

+

1
σ2

u (cid:19)

1

−

,

1
σ2
y

+

1
σ2

u (cid:19)

(cid:18)

1

−

,

(cid:33)

(14)

1. For real-valued observation:

1
where f −
d

:

.
(cid:60) → (cid:60)

(cid:18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

7

2. For positive real-valued observations:

1
where f −
d

:

.
(cid:60)+ → (cid:60)

3. For categorical observations:

(cid:18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

p(yd

n1|

n, zn, Bd) =
xd

yd
n1

N (cid:32)

znbd
1
σ2
y

+

1
d (xd
f −
n)
σ2

u (cid:19) (cid:18)

1
σ2
y

+

1
σ2

u (cid:19)

1

−

,

1
σ2
y

+

1
σ2

u (cid:19)

(cid:18)

1

−

,

(cid:33)

p(yd

nr|

n = T, zn, Bd) =
xd

(yd
(yd

nr|
nr|

N
N

(cid:26)

znbd
znbd

r, σ2
r, σ2

y)I(yd
y)I(yd

nr > maxj
nr < yd
nT )

=r(yd

nj))

If
If

r = T
= T
r

In Equation (16), if xd
n = T = r, we sample yd
nj). Otherwise, we
sample from a Gaussian right-truncated by yd
nr corresponds to solving
a multinomial probit regression problem. In order for the model to be identiﬁable, without loss of generality, we
assume that the regression function fRd (zn) is identically zero. Therefore, we ﬁx bd

nr from a Gaussian left-truncated by maxj
n. Sampling the variables yd
nr with r = xd

kRd = 0 for all k.

=r(yd

4. For ordinal observations:

In this case, we sample yd
also need to sample the threshold values θd

p(yd

n = r, zn, Bd)
xd

(yd

znbd

1, σ2

y)I(θd

r

1 < yd

θd
r ).

−

n1|

∼ N

n1|
n1 ≤
n1 form a Gaussian left-truncated by θd
1 and right-truncated by θd
r
−
r with r = 1, . . . , Rd −
1 as
0, σ2
r > max(θd
r
n1|
−
xd
(yd
n = r + 1)).

θ )I(θd
(θd
r |
I(θd
r < min(θd

xd
n = r))

1, max
n

yd
n1)

∼N

(yd

p(θd
r |

r , min
n

n1|

×

r . In this case, we

In this case, sampling the variables yd
thresholds
θr}
{
our case, to zero.

n1 corresponds to solving an ordered probit regression problem, where the
Rd
r=1 are unknown. In order for the model to be identiﬁable we set one of the thresholds, θ1 in

(15)

(16)

(17)

(18)

p(yd

n1|

xd
n, zn, Bd) =

(yd

n1|

N

znbd

1, σ2

y)I(f −
1
(cid:60)+

(xd
n)

≤

yd
n1 < f −

1(xd

n + 1)),

(19)

(cid:60)+ → (cid:60)

. We sample yd

1
n1 from a Gaussian left-truncated by f −
(cid:60)+

(xd

n) and right-truncated by

5. For count observations:

1
where f −
:
(cid:60)+
1
(xd
n + 1).
f −
(cid:60)+

4 Applications

In this section, we apply the proposed model to solve two different tasks on several real-world datasets. In Section 4.1,
we focus on a prediction task in which we aim to estimate and replace the missing data, which is assumed to be missing
completely at random. These results have been previously introduced in [29]. Then, in Section 4.2, we focus on a data
analysis task on several real-world datasets from different application domains including psychiatry, clinical trials and
politics. Weexploratory data analysis show how to use the proposed model to perform exploratory data analysis, i.e.,
to ﬁnd the latent structure in the data and capture the statistical dependencies among the objects and their attributes in
the data.

8

4.1 Missing Data Estimation

In this section, we use the proposed model to estimate missing data in heterogeneous databases, where we assume that
the data is missing completely at random (MCAR). Missing data may occur in diverse applications due to different
reasons. For example, participants of a survey may decide not to respond (or skip) some questions of the survey;
participants in a clinical study may drop out during the course of the study; or users of a recommendation system can
only rate a small fraction of the available books, movies, or songs, due to time constraints. The presence of missing
values might be challenging when the data is used for reporting, information sharing and decision support, and as
a consequence, handling missing data has captured the attention in diverse areas of data science such as machine
learning, data mining, and data warehousing and management [26, 21].

The extensive literature in probabilistic missing data estimation and imputation focus on homogeneous databases
which contain only either continuous data, usually modeled as Gaussian variables [28], or discrete data, that can be
either modeled by discrete likelihoods [18] or simply treated as Gaussian variables [25, 28]. However, up to our
knowledge, there is a lack of approaches for missing data estimation in heterogeneous databases, where the standard
approach is to treat all the attributes, either continuous or discrete, as Gaussian variables.

Experimental Setup. We evaluate the predictive power of the proposed model at estimating missing data on ﬁve
real databases, which are summarized in Table 1. The datasets contain different numbers of instances and attributes,
which cover all the discrete and continuous variables described in Section 2. According to our proposed model, the
D
probability distribution over the observation matrix is fully characterized by the latent matrices Z and
d=1 (as
}
well as the auxiliary variables Ψd). Hence, if we assume the latent vector zn for the n-th datapoint and the weight
factors Bd (and the auxiliary variables Ψd) to be known, we have a probability distribution over missing observations
n by sampling from this distribution3, or by simply taking either its mean,
n from which we can obtain estimates for xd
xd
mode or median value, once the latent matrix Z and the latent weight factors Bd (and Ψd) have been learnt.

Bd
{

Here, we compare in terms of predictive log-likelihood the following methods for missing data estimation:

•
•
•

The proposed general table completion approach denoted by GLFM (see Section 2).
The standard linear-Gaussian IBP [12] denoted by SIBP, treating all the attributes as Gaussian.
The Bayesian probabilistic matrix factorization approach [25] denoted by BPMF, that also treats all the attributes
in X as Gaussian distributed.

For the GLFM, we consider for the real positive and the count data the following transformation that maps from the
real numbers to the real positive numbers, f (x) = log(exp(wx) + 1), where w is a user-deﬁned hyper-parameter
that scales the data. Before running the SIBP and the BPMF methods, we normalized each column in the matrix X
to have zero-mean and unit-variance. This normalization ensures that the Gaussian likelihood evaluations of all the
attributes describing the objects in each dataset are comparable, regardless of their discrete or continuous nature. As
a consequence, ittherefore provides more accurate and fair results than applying the SIBP and BPMF directly on the
data without prior normalization. Additionally, since both the SIBP and the BPMF assume continuous observations,
when dealing with discrete data, we estimate each missing value as the closest integer value to the “de-normalized”
Gaussian variable.

Results. Figure 2 plots the average predictive log-likelihood per missing value as a function of the percentage of
missing data. Each value in Figure 2 was obtained by averaging the results across 20 independent split sets where the
missing values were randomly chosen. In Figures 2c and 2b, we cut the plot at a missing percentage of 50% because,
in these two databases, the discrete attributes present a mode value that appears for more than 80% of the instances.
As a consequence, the SIBP and the BPMF algorithms assign probability close to one to the mode, which results in an
artiﬁcial increase in the average test log-likelihood for larger percentages of missing data. For BPMF, we have used
different numbers of latent features (in particular, 10, 20 and 50), although we only show the best results for each
database, speciﬁcally, K = 10 for the NESARC and the wine databases, and K = 50 for the remainder. Both the
GLFM and the SIBP have not learnt a number of binary latent features above 25 in any case. In Figure 2e, we only
plot the test log-likelihood for the GLFM and SIBP because BPMF provides much lower values. As expected, we
observe in Figure 2 that the average test log-likelihood decreases for the three models as the number of missing values

3Note that sampling from this distribution might be computationally expensive. In this case, we can easily obtain samples of xd

the structure of our model. In particular, we can simply sample the auxiliary Gaussian variables yd
for xd

n by applying the corresponding transformation, detailed in Section 2.1.

n by exploiting
n given zn and Bd, and then obtain an estimate

9

Dataset
Statlog German credit dataset
[9]
QSAR biodegradation dataset
[20]
Internet usage survey dataset
[4]
Wine quality dataset [6]

N
1,000

1,055

1,006

6,497

D
20 (10 C + 4 O + 6
N)
41 (2 R + 17 P + 4
C + 18 N)
32 (23 C + 8 O + 1
N)
12 (11 P + 1 N)

NESARC dataset [24]

43,000

55 C

Description
Information about the credit risks of the applicants.

Molecular descriptors of biodegradable and non-
biodegradable chemicals.
Responses of the participants to a survey related to the
usage of internet.
Results of physicochemical tests realized to different
wines.
Responses of the participants to a survey related to per-
sonality disorders.

Table 1: Description of datasets. ‘R’ stands for real-valued variables, ‘P’ for positive real-valued variables, ‘C’ for
categorical variables, ‘O’ for ordinal variables and ‘N’ for count variables

(a) Statlog.

(b) QSAR biodegradation.

(c) Internet usage survey.

(d) Wine quality.

(e) Nesarc database.

Figure 2: Average test log-likelihood per missing datum versus percentage of missing data. The ‘whiskers’ show
one standard deviation away from the average test log-likelihood.

increases (ﬂat shape of the curves are due to the logarithmic scale of the y-axis). In this ﬁgure, we also observe that
the proposed GLFM outperforms SIBP and BPMF for four of the databases, being SIBP slightly better for the Internet
database. The BPMF model presents the worst test log-likelihood in all databases.

Successively, we analyzed the performance of the three models for each kind of discrete and continuous variables.
Figure 3 shows the average predictive likelihood per missing value for each attribute in the table, which corresponds to
each dimension in X. In this ﬁgure, we have grouped the dimensions according to the kind of data that they contain,
the x-axis shows the number of considered categories for the case of categorical and ordinal data (in the case of the
NESARC database, all the attributes are binary). The ﬁgure shows that the GLFM presents similar performance for all
the attributes in the ﬁve databases, while for the SIBP and the BPMF models, the test log-likelihood falls drastically
for some of the attributes, with this effect being more dramatic in the case of BPMF (it explains the low log-likelihood
in Figure 2). This effect is even more evident in Figures 2b and 2d. In Figures 2 and 3, we observe that both IBP-based
approaches (GLFM and SIBP) outperform BPMF, with the proposed GLFM being the one that best performs across
all databases. We can conclude that, unlike BPMF and SIBP, GLFM provides accurate estimates for the missing data
regardless of their discrete or continuous nature.

10

(a) Statlog.

(b) QSAR biodegradation.

(c) Internet usage survey.

(d) Wine quality.

(e) Nesarc database.
Figure 3: Average test log-likelihood per missing datum in each dimension. Here, we consider 50% of missing
data.
In the x-axis ‘R’ stands for real-valued variables, ‘P’ for positive real-valued variables, ‘C’ for categorical
variables, ‘O’ for ordinal variables and ‘N’ for count variables. The number that accompanies ‘C’ or ‘O’ corresponds
to the number of categories. In the Nesarc dataset, all the variables are binary, i.e., ‘C2’.

11

Attribute description
Stage of the cancer
DES treatment level
Tumor size in cm2
Serum Prostatic Acid Phosphatase (PAP)
Prognosis Status (outcome of the disease)

Type of variable
Categorical with 2 categories
Ordinal with 3 categories
Count data
Positive real-valued
Categorical with 4 categories

Table 2: List of considered attributes for the Prostate Cancer dataset.

4.2 Data Exploration

In this section, we describe how to use the GLFM for a data exploration task: the latent structure that underlies the
data is discovered and analized in three different application domains. Speciﬁcally, we use the proposed GLFM in the
context of i) clinical trials to discover the effects of a new drug for prostate cancer; ii) psychiatry to capture the impact
of social background in the development of mental disorders; and iii) politics to identify meaningful demographic
proﬁles, together with their geographic location, and voting tendencies in the United States.

The main goal of this section is to show how to include the speciﬁc domain knowledge into the proposed GLFM to
ease the data exploration process. In particular, we here show examples of how to select the input data for the GLFM,
as well as how to input these data into the model, in order to obtain interpretable results that can be used to get a better
understanding of the corresponding knowledge domain.

4.2.1 Drug effect in a clinical trial for prostate cancer

Clinical trials are conducted to collect data in order to determine the safety and efﬁcacy of a new drug before it can be
sold in the consumer market. Concretely, the main goal of clinical trials is to prove the efﬁcacy of a new treatment for
a disease while ensuring its safety, i.e., check whether its adverse effects remain low enough for any dosage level of
the drug. As an example, the publicly available Prostate Cancer dataset4 collects data of a clinical trial that analized
the effects of the drug diethylstilbestrol (DES) as a treatment against prostate cancer. The dataset contains information
about 502 patients with prostate cancer in stages5 3 and 4, who entered a clinical trial during 1967-1969 and were
randomly allocated to different levels of treatment with DES. The prostate cancer dataset have been used by several
studies [3, 14, 19] to analyze the survival times of the patients in the clinical trial and the causes behind their death.
These studies have pointed out that a large dose of the treatment tend to reduce the risk of a cancer death, but it might
also result in an increased risk of cardiovascular death. In this section, we apply the proposed GLFM to the Prostate
Cancer dataset to show that the proposed model can be efﬁciently used to discover the statistical dependencies in the
data, which in this example corresponds to the effect of the different levels of treatment with DES in the suffering of
prostate cancer and cardiovascular diseases.

Experimental Setup. The prostate cancer dataset consists of 502 patients and 16 attributes, from which we make
use of the ﬁve attributes listed in Table 2. The selection of these ﬁve attributes allows us to focus onlytherefore on
capturing the statistical dependencies between the target attributes, i.e, the relationship between the different levels of
treatment with DES and the suffering of prostate cancer and cardiovascular diseases. In our experiments, we sample
the variance of the pseudo-observations in each dimension and choose α = 5, σ2
θ = 1. We also consider
for the positive real and count data the following transformation that maps from the real numbers to the positive real
µ) + 1), where µ = min(xd) and w = 2/std(xd) are data-driven parameters whose
numbers: f (x) = log(w
objective is to shift and scale the data. In order to obtain more interpretable results, we also activate the bias term, i.e.,
a latent feature forced to be active for all patients.

B = 1, and σ2

(x

−

·

Results. After running our model, we obtain four latent features, whose empirical probabilities of activation and main

4Database available at: http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets
5The stage of a cancer describes the size of a cancer and how far it has grown. Stage 3 means that the cancer is already quite large and may have
started to spread into surrounding tissues or local lymph nodes. Stage 4 is more severe, and refers to a cancer that has already spread from where
it started to another body organ. This is also called secondary or metastatic cancer. Find more details in http://www.cancerresearchuk.
org/about-cancer/what-is-cancer/stages-of-cancer

12

Feature
Feature F1
Feature F2
Feature F3
Feature F4

Empirical Prob. Main implications

0.1952
0.2689
0.1594
0.1155

Favors stage 3, low DES levels and prostatic death
Favors stage 3, highest DES levels, and cardiovascular death
Favors stage 4, low DES levels, and mid-level prostatic death
Favors stage 4, low DES levels, and most severe prostatic cancer

Table 3: Empirical feature activation probabilities in the Prostate Cancer dataset. We show the empirical proba-
bility of possessing at least one latent feature. These probabilities are directly computed from the inferred IBP matrix
Z. Additionally, the table summarizes the main implications of the activation of each latent feature.

Patterns
Empirical Prob.

(0000)
0.4641

(0100)
0.1394

(1000)
0.0936

(0010)
0.0757

(0001)
0.0518

(1100)
0.0438

(0110)
0.0359

(0101)
0.0259

(1010)
0.0219

Table 4: Empirical probability of pattern activation for the top-nine most popular patterns. These probabilities
are computed directly from the inferred IBP matrix Z.

implications (which are discussed further below) are shown in Table 3. Additionally, Table 4 shows the nine most
common latent feature vectors, referred here as feature patterns, which capture over 95% of the observations. In order
to study the effect of the latent features on each attribute of the dataset, Figure 4 shows the inferred distribution of each
attribute for the ﬁve most common patterns, which happen to only have one active latent feature (plus the bias term).
We remark that GLFM assumes that the contribution of each latent feature is additive, therefore, patterns with more
than one active latent feature can be seen as combinations of the depicted patterns.6

In Figure 4, we can distinguish two groups of features. The ﬁrst group accounts for patients in stage 3 and includes
the bias term and the two ﬁrst latent features. Within this group, the bias term – or equivalently pattern (0000) – and
the ﬁrst feature –or equivalently pattern (1000) – account for patients in stage 3 with a low average level of treatment
with DES, as shown in Figure 4b. However, while the bias term models patients with low probability (
15%) of
prostate cancer death, the ﬁrst feature accounts for patients with higher probability (
40%) of prostate cancer death,
which can be explained by a larger tumor size, as shown in Figure 4c. The second feature – or equivalently pattern
(0100) –captures patients who exclusively received a high dosage (5 mg) of the drug, as shown in Figure 4b. These
patients present a small tumor size and the lowest probability of prostatic cancer death, suggesting a positive effect of
the drug as treatment for the cancer. However, they also present a signiﬁcant increase in the probability of dying from
50%), indicating a potential adverse-effect of the drug that increases the risk of suffering from
a vascular disease (
cardio-vascular diseases. Such observation is in agreement with previous studies [3, 14, 19].

∼

∼

∼

The second group of features corresponds to the activation patterns (0010) and (0001), and accounts for patients
in stage 4 with mild and severe conditions, respectively. In particular, the third feature corresponds to patients with
small tumor size, but intermediate values for the PAP biomarker, suggesting a certain spread in the degree of the tumor
compared to the features in the ﬁrst group, but not as severe as for patients with pattern (0001). Indeed, pattern (0001)
models those patients in stage 4 with relatively high tumor size and the highest PAP values – it is thus not surprising
that those patients present the highest probability (above 50%) of prostatic death.

4.2.2 Impact of Social Background on Mental Disorders

In this section, we aim at extending the analysis in [24] to account for the inﬂuence of the social background of
subjects (such as age, gender, etc.) in the probability of a subject suffering from a comorbid disorder. To this end, in
addition to the diagnoses of the the 20 most common psychiatric disorders detailed below, we also make use of the
information provided by the NESARC database7, which includes information both on the mental condition and on the
social background of participants.

Several studies have analyzed the impact of social background in the development of mental disorders. These stud-
ies usually focus on the relation between a mental disorder and a speciﬁc aspect of the social background of subjects.
Some examples in this area study the relation between depression and gender [32, 15], or the link between common
mental disorders and poverty or social class [31, 7, 13]. Other studies [2, 24] have focused on ﬁnding and analyzing

6In the case of patterns with multiple active latent features, the bias term should be counted only once.
7Database available at: http://aspe.hhs.gov/hsp/06/catalog-ai-an-na/nesarc.htm

13

Attribute description
Gender
Age
Census region
Race/ethnicity
Marital status
Highest grade or years of school completed

Type of variable
Categorical with 2 categories
Count data
Categorical with 4 categories
Categorical with 5 categories
Categorical with 6 categories
Ordinal with 14 categories

Table 5: List of considered social background attributes. We look for correlations between each of these attributes
with the twenty most common psychiatric disorders among the subjects in the NESARC database.

the co-occurring (comorbidity) pattern among the 20 most common psychiatric illnessesto our knowledge. These
studies found that the 20 most common disorders can be divided into three meta-groups of disorders: i) externaliz-
ing disorders, which include substance use disorders (alcohol abuse and dependence, drug abuse and dependence and
nicotine dependence); ii) internalizing disorders, which include mood and anxiety disorders (major depressive disorder
(MDD), bipolar disorder and dysthymia, panic disorder, social anxiety disorder (SAD), speciﬁc phobia and general-
ized anxiety disorder (GAD), and pathological gambling (PG)); and iii) personality disorders (avoidant, dependent,
obsessive-compulsive (OC), paranoid, schizoid, histrionic and antisocial personality disorders (PDs)). Additionally,
they also found that comorbid or co-occurring disorders tend to belong to the same group of disorders [30]. However,
to our knowledge, there is a lack of work in the study of the impact of social background in the suffering of comorbid
disorders.

Experimental Setup. The NESARC database contains the responses of a representative sample of the U.S. population
to a survey with questions related to the social background of participants, alcohol and other drug consumption, and
behaviors related to mental disorders. The ﬁrst wave of NESARC sampled the adult U.S. population with over 43,000
respondents who answered almost 3,000 questions. The dataset also include the diagnoses for each of the participants
of the survey. In this experiment, in addition to the diagnoses of the 20 most common psychiatric disorders described
above, we include one by one each of the social background questions as input data to the proposed model. Table 5
summarizes the considered questions and how we introduce them into our model as input variables. Here, we introduce
each attribute related to the social background of the participants independently to ensure that the model captures the
dependencies between latent disorders and social background, instead of correlations among the different aspect of
the social background. Note also that the diagnoses of the 20 psychiatric disorders correspond to categorical variables
with two possible categories, e.g., a patient suffering or not from a disorder.

y = 1, σ2

B = 1, σ2

For the following experimental results, we run the inference algorithm in Section 3 for each question independently
with α = 5, σ2
θ = 1, and consider for the positive real and count data the following transformation
that maps from the real numbers to the positive real numbers: f (x) = x2, which we chose to show that the proposed
model works with any differentiable and invertible function. Similarly to [24], we activate the bias term, i.e., an
additional latent feature which is active to all the subjects in the data set, so that we do not sample the rows of Z
corresponding to those subjects who do not suffer from any of the 20 disorders, but instead ﬁx their latent features
to zero. The idea is that the bias captures the population that does not suffer from any disorder, while the rest of
active features in matrix Z characterize the disorders. As we will see next, the use of the bias term aims to ease the
interpretability of the inferred latent features.

Results. After running our model, we ﬁnd that the census region, race/ethnicity, marital status and educational level
(i.e., highest grade or years of school completed) do not appear to have any inﬂuence in the comorbidity patterns of
the 20 most common psychiatric disorders. In contrast, as detailed below, gender and age of the participants inﬂuence
the probability of suffering from a set of comorbid (co-occurring) disorders.

‘male’, ‘female’
{

Gender. We model the gender information of the participants in the NESARC as a categorical variable with two
categories:
. The percentage of males in the NESARC is approximately 43%. In this case, the
GLFM found three latent features, whose empirical probabilities can be found in Table 6. Moreover, Table 7 shows
empirical probability of each feature pattern in the dataset. Here, we observe that the three latent feature activate
mostly in isolation, being the combination of two features rare (below 1%). Figure 5a shows the probability of

}

14

meeting each diagnostic criteria for the latent feature vectors zn listed in the legend and in the database (baseline).
Note that the obtained latent features are similar to the ones in [24], i.e., Feature F1 – pattern (100) – mainly models
the seven personality disorders (PDs), Feature F2, which corresponds to pattern (010), models the alcohol and drug
abuse disorders and the antisocial PD, while Feature F3 – pattern (001) – models the anxiety and mood disorders.
Additionally, in Figure 5b, we show the probability of being male and female for the latent feature vectors zn shown
in the legend and the empirical probability of being male and female in the database (baseline).

In Figure 5b, we observe that having no active features (pattern (000), which captures people that do not suffer from
any disorder), increases the probability of being male with respect to the baseline probability, therefore, it indicates
that females tend to suffer in a higher extent from psychiatric disorders. Additionally, we observe that Feature F1 –
pattern (100) – increases the probability of being male, while Feature F3 – pattern (001) – increases the probability of
being female. Hence, from the analysis of Figure 5b, we can conclude that, while women suffer more frequently from
mood and anxiety disorders than men, PDs are more common in men.

Feature
Feature F1
Feature F2
Feature F3

Empirical Prob. Main implications

0.0341
0.0470
0.0460

Increases the probability of personality disorders and male gender
Increases the probability of alcohol and drug abuse disorders
Increases the probability of anxiety and mood disorders and female gender

Table 6: Gender: empirical probabilities of possessing at least one latent feature. These probabilities are directly
computed from the inferred IBP matrix Z.

Patterns
Empirical Prob.

(000)
0.8615

(010)
0.0427

(001)
0.0414

(100)
0.0298

(111)
0.0023

(011)
0.0022

(110)
0.0020

Table 7: Gender: Empirical probability of feature pattern activations. These probabilities are computed directly
from the inferred IBP matrix Z.

Age. Now, we focus on the age of the participants, which we model as count data. After running our inference
algorithm with the diagnoses of the 20 disorders and the age of the subjects as input data, we again obtain three latent
features, whose empirical probabilities are listed in Table 8. Moreover, Table 9 shows empirical probability of each
feature pattern in the dataset. Here, we observe that the three latent features activate mostly in isolation, being the
combination of two features rare (below 3%).

Figure 6a shows the probability of meeting each diagnostic criteria for the latent feature vectors zn listed in
the legend and in the database (baseline). In addition to the baseline probability distribution, we plot in Figure 6b
the inferred probability distributions over the age when none or only one of the latent variables is active (which
correspond to the most common feature patterns). The empirical probability distribution over the age based on the
data is shown (and denoted by ‘baseline’) in Figure 6b. Here, we observe that introducing the age of the participants
as an input variable has changed the inferred latent features (with respect to the features in [24] depicted in Figure 5a).
In particular, we observe that the obtained latent features mainly differ in the probability of suffering from personality
disorders (i.e., disorders from 14 to 20), being the probability of suffering from disorders 1 to 13 similar for the three
plotted latent feature patterns. In this ﬁgure, we observe that the vector zn with no active latent features, e.g., pattern
(000), is trying to capture the mean of the age in the database (which coincides with middle-aged subjects, i.e., 30
50
years old). Moreover, we observe that the subjects with the highest probability of suffering from personality disorders
– pattern (100) – are likely to be middle-aged, followed in a decreasing order by young adults – pattern (010) – and
elderly people – pattern (001). Additionally, if we focus on the differences among the three features in disorders from
1 to 13, we also observe that, while young and elderly people tend to suffer from depression, middle-aged people tend
to suffer from the bipolar disorder. Hence, based on Figure 6, we can conclude that the bipolar disorder and the seven
personality disorders tend to show up in a higher extent in the mature age, while young and elderly people tend to
suffer more often from depression.

−

15

Feature
Feature F1
Feature F2
Feature F3

Empirical Prob.
0.0332
0.0550
0.0569

Main implications
Captures severe personality disorders and middle-age subjects
Captures mid-severe personality disorders and young subjects
Captures mid-severe personality disorders and older subjects

Table 8: Age: empirical probabilities of possessing at least one latent feature. These probabilities are directly
computed from the inferred IBP matrix Z.
(000)
0.8615

Patterns
Empirical Prob.

(101)
0.0019

(010)
0.0522

(100)
0.0294

(001)
0.0503

(011)
0.0028

(110)
0.0019

Table 9: Age: Empirical probability of feature pattern activations. These probabilities are computed directly from
the inferred IBP matrix Z.

4.2.3 Voters proﬁle in presidential election

Finally, we apply the proposed model to understand the correlations between demographic proﬁles and political vote
tendencies. In particular, we focus on the United States presidential election of 1992, in which three major candidates
ran for the race: the incumbent Republican president George H. W. Bush, the Democratic Arkansas governor Bill
Clinton, and the independent Texas businessman Ross Perot. In 1992, the public’s concern about the federal budget
deﬁcit and fears of professional politicians allowed the independent candidacy of billionaire Texan Ross Perot to
appear on the scene dramatically [1], to the point of even leading against the major party candidates in the polls during
the electoral race8. The race ended up with the victory of Bill Clinton by a wide margin in the Electoral College,
receiving 43% of the popular vote against Bush’s 37.5% and Perot’s 18.9% [16]. These results are noted for being the
highest vote share of a third-party candidate since 1912, even if Perot did not obtain any electoral votes [16].

Our primary objective in this section is to ﬁnd and analyze the different types of voters’ proﬁles, as well as
which candidate each proﬁle tends to favor. To this aim, we used the publicly available Counties database gathering
diverse information about voting results, demographics and sociological factors per counties9. This dataset contains
information for 3141 counties. Table 10 lists the per-county attributes that we used as input for our model.

Attribute description
State in which the county is located
Population density in 1992 per squared miles
% of white population in 1990
% of people with age above 65 in 1990
% of people above 25 years old with bachelor’s degree or higher
Median family income in 1989 (in dollars)
% of farm population in 1990
% of votes cast for Democratic president
% of votes cast for Republican president
% of votes cast for Ross Perot

Type of data
Categorical with 51 categories
Positive real data
Positive real data
Positive real data
Positive real data
Count data
Positive real data
Positive real data
Positive real data
Positive real data

Table 10: List of considered attributes regarding the United States presidential election of 1992. Attributes 1 to
7 include demographic information and sociological factors, while the last three attributes summarize the percentage
voting outcome in each county.

Experimental Setup. We run our inference algorithm with α = 5, σ2
θ = 1 and the mapping transformation
µ) + 1), with µ = min(xd) and w =
from the real numbers to the positive real numbers: f (x) = log(w
2/std(xd). In this experiment, we activate the bias term and sample the variance of the pseudo-observations for each
dimension/attribute. A challenging aspect of this database is that the distributions of some of its attributes are heavy-
tailed, leading to a large number of latent features as output of the GLFM, whose purpose is to capture the tails of the
distributions. This is not an issue for estimation and imputation of missing data, but it renders data exploration more

B = 1, σ2
(x

−

·

8New York Times: http://www.nytimes.com/1992/06/11/us/

the-1992-campaign-on-the-trail-poll-gives-perot-a-clear-lead.html

9Database available at: http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets

16

Feature
Feature F1

0.4874

Empirical Prob. Main implications

Feature F2

0.2703

Feature F3

0.2700

Feature F4
Feature F5

0.0411
0.0372

Favors Perot, increases the probability of white population, and decreases
average income.
Favors the Democrat candidate, increases population density, and decreases
family income, percentages of white population, farming and college de-
grees.
Favors the Republican candidate and Perot, increases the percentage of
farming, and decreases population density.

Capture the tails of the distributions of different attributes.

Table 11: Empirical feature activation probabilities for the Counties dataset. We show the empirical probability
of possessing at least one latent feature, as well as the main implications of the activation of each feature. These are
directly computed from the inferred IBP matrix Z.

Patterns

(110)
(100)
Empirical Prob. 0.2636 0.2407 0.1063 0.1060 0.0748

(000)

(101)

(010)

Table 12: Empirical probability of pattern activation for the top-ﬁve most popular patterns. These probabilities
are computed directly from the inferred IBP matrix Z. Features F4 and F5 are always switched off, and are thus
omitted from the labels.

−

tedious. To solve this limitation, we here perform an additional data preprocessing step by applying a logarithmic
transformation to heavy-tailed attributes. In more detail, we apply the function g1(x) = log(x + 1) for population
density, median family income, and percentage of farm population. For the percentage of white population, we used
the function g2(x) = log((100

x) + 1) since the distribution has most of its density close to 100%.

Results. Running the GLFM on this data results in 5 latent features, whose empirical activation probabilities are
shown in Table 11. Here, we observe that while the ﬁrst three features are active for at least 27% of the counties, the
last two features are active only for around 4% of the counties. Moreover, we ﬁnd that the different combinations of
the three ﬁrst latent features represent more than 92% of the counties in USA. In the following, we will thus focus
on the analysis of the three ﬁrst features and, in particular, on the top-ﬁve most popular feature patterns. We show in
Table 12 the empirical probabilities of these ﬁve patterns, which represent around 80% of the U.S. counties. Figure 7
shows the distribution of vote percentage per candidate associated to each of these top-ﬁve patterns, while Figure 8
shows the corresponding geographic distribution (i.e., the empirical activation probability) across states for each of
these patterns. In these ﬁgures, we observe that:

(i) pattern (000), corresponding to the bias term, tends to model middle values for the percentage of votes for the
48% for the
27% for Perot), and activates mainly in the east and west coasts of the country, as

three candidates (with an average percentage of votes of
Republican candidate and
well as Florida;

50% for the Democrat candidate,

∼

∼

∼

(ii) pattern (100) provides similar percentage of votes for the Democrat and Republican candidates as in pattern
(000), but it favors the independent candidate Perot (with an average percentage of votes above 30%), and
activates mostly in the north central-east region of the country and Maine (the state where Perot’s party managed
to beat the Republican party);

(iii) pattern (101) activates in the north central-west region of the USA (not including the coast) and represents
55%) while also

a proﬁle inclined towards the Republican party (with an average percentage of votes of
favoring in a lower extent the independent candidate; and

∼

(iv) patterns (010) and (001) clearly capture Democrat-oriented proﬁles, and activate mainly in the south east region

of the USA, including the state from which Bill Clinton comes from, Arkansas.

Note that the demographic results above are in agreement with the outcome of the election per counties10, as shown in
Figure 9. Next, we analyze the demographic information associated to each of the feature patterns above. To this end,
we show in Figure 10 the distribution of each attribute/dimension of the data for each of the considered patterns. First,

10https://en.wikipedia.org/wiki/United_States_presidential_election,_1992

17

we observe that pattern (000), which activates mostly in the coasts and Florida, corresponds to the highest population
density, average income, and percentage of college degrees, as well as an important race diversity and low farming
activity. These observations align with the typical proﬁle characterizing “big-cities”. As stated before, this pattern
is the most balanced in terms of voting tendency, with an equilibrated support for both Democrat and Republican, as
well as intermediate values for the percentage of votes cast for Perot.

Second, patterns (100) and (101) represent the largest share of Perot’s votes, both with an average percentage of
votes above 30% for Perot. Figure 10 shows that Perot’s main supporters, characterized mainly by pattern (101),
also correspond to Republican main supporters, who tend to live in low populated areas in the north central part of
the country where farming activity is considerable, and the percentages of white population and over-65 years old
population are also high. The second voting force backing Perot, captured by pattern (100) and located in the north
east-central part of USA, corresponds mostly to white population with an intermediate-high average income and an
average percentage of college degrees around 18% (the red curve in Figure 10e overlaps the green line). These results
back the analysis in [17], which showed that the majority of Perot’s voters (57%) were middle class, earning between
$15,000 and $49,000 annually, with the bulk of the remainder drawing from the upper middle class (29% earning more
than $50,000 annually). Perot’s campaign ended up taking 18.9% of the votes, ﬁnishing second in Maine and Utah, as
captured by pattern (100) and (101) respectively.

Finally, Democrat’s patterns (010) and (110) are mainly active in the Southeastern United States, and capture a
diverse range of voters in terms of their demographic properties. On the one hand, pattern (010) captures highly
populated counties, with low values of family income, percentage of college degrees, percentage of white population
and percentage of farming population. On the other hand, pattern (110) captures low populated counties with a
large percentage of population above 65 year old, as well as a larger presence of farming activity and lower average
income. These results might be explained by the broad appeal across all socio-ethno-economic demographics that the
Democratic party has historically targeted.

5 Conclusions

In this paper, we developed an efﬁcient general latent feature model, named GLFM, suitable for modelling and infer-
ence with real-world heterogeneous datasets. The proposed model presents attractive properties in terms of ﬂexibility
and interpretability. Its nonparametric nature allows it to automatically infer the appropriate model complexity (i.e.,
number of latent features) from data. Secondly, the fact that the latent features are binary-valued makes it easier to
identify and interpret meaningful patterns in the data exploration process. Thirdly, the model shares the properties
of conjugate models, which allow us to propose an efﬁcient inference scheme that scales linearly with the number of
observations (objects) and dimensions (attributes) in the data.

We showed the ﬂexibility and applicability of the proposed GLFM by solving both prediction and data exploration
tasks in several real-world datasets. In particular, we used the proposed model to estimate and replace missing data
in heterogeneous databases. Also, we used the GLFM for data exploratory analysis of real-world datasets related to
diverse application domains: clinical trials, psychiatry, sociology and politics. The software library provides the ﬁrst
available software for latent feature modeling in heterogeneous data, and includes user-friendly functions for missing
data estimation and data exploratory analysis. The software package is publicly available at: https://github.
com/ivaleraM/GLFM and reviewed in Appendix 6. We hope to bring a powerful tool for researchers from diverse
knowledge domains to help them analyze a wide range of datasets in an automatic manner.

6 Acknowledgements

Isabel Valera acknowledges her Humboldt Research Fellowship for Postdoctoral Researchers. Maria Lomeli and
Zoubin Ghahramani acknowledge support from the Alan Turing Institute (EPSRC Grant EP/N510129/1) and EPSRC
Grant EP/N014162/1, and donations from Google and Microsoft Research. Melanie F. Pradier is grateful to the
European Union 7th Framework Programme through the Marie Curie Initial Training Network “Machine Learning
for Personalized Medicine” MLPM2012, Grant No. 316861 (http://mlpm.eu). This work has been partly supported

18

by MINECO/FEDER (’ADVENTURE’, id. TEC2015-69868-C2-1-R), and Comunidad de Madrid (project ’CASI-
CAM-CM’, id. S2013/ICE-2845).

References

[1] R Michael Alvarez and Jonathan Nagler. Economics, issues and the perot candidacy: voter choice in the 1992

presidential election. American Journal of Political Science, pages 714–744, 1995.

[2] C. Blanco, R. F. Krueger, D. S. Hasin, S. M. Liu, S. Wang, B. T. Kerridge, T. Saha, and M. Olfson. Mapping com-
mon psychiatric disorders: Structure and predictive validity in the National Epidemiologic Survey on Alcohol
and Related Conditions. Journal of the American Medical Association Psychiatry, 70(2):199–208, 2013.

[3] D. P. Byar and S. B. Green. The choice of treatment for cancer patients based on covariate information: applica-

tion to prostate cancer. Bulletin du Cancer, 67:477–490, 1980.

[4] Pew Research Centre. 25th anniversary of the web. Available on: http://www.pewinternet.org/datasets/january-

2014-25th-anniversary-of-the-web-omnibus/, 2014.

[5] W. Chu and Z. Ghahramani. Gaussian processes for ordinal regression. J. Mach. Learn. Res., 6:1019–1041,

December 2005.

[6] P. Cortez, A. Cerdeira, F. Almeida, T. Matos,
data mining from physicochemical properties.
http://archive.ics.uci.edu/ml/datasets.html, 47(4):547–553, 2009.

and J. Reis.

Modeling wine preferences by
Decision Support Systems. Dataset available on:

[7] B. P. Dohrenwend. Sociocultural and social-psychological factors in the genesis of mental disorders. Journal of

Health and Social Behavior, 16(4):365–392, 1975.

[8] F. Doshi-Velez and Z. Ghahramani. Accelerated sampling for the indian buffet process. In Proceedings of the
26th Annual International Conference on Machine Learning, ICML ’09, pages 273–280, New York, NY, USA,
2009. ACM.

[9] J. Eggermont, J. N. Kok, and W. A. Kosters. Genetic programming for data classiﬁcation: Partitioning the search
space. In In Proceedings of the 2004 Symposium on applied computing (ACM SAC04). Dataset available on:
http://archive.ics.uci.edu/ml/datasets.html, pages 1001–1005. ACM, 2004.

[10] M. Girolami and S. Rogers. Variational Bayesian multinomial probit regression with Gaussian process priors.

Neural Computation, 18:2006, 2005.

[11] P. Gopalan, F. J. R. Ruiz, R. Ranganath, and D. M. Blei. Bayesian Nonparametric Poisson Factorization for

Recommendation Systems. International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2014.

[12] T. L. Grifﬁths and Z. Ghahramani. The Indian buffet process: an introduction and review. Journal of Machine

Learning Research, 12:1185–1224, 2011.

[13] A. B. Hollingshead and F. C. Redlich. Social stratiﬁcation and psychiatric disorders. American Sociological

Review, 18(2):163–169, 1953.

[14] R. Kay. Treatment effects in competing-risks analysis of prostate cancer data. Biometrics, 42(1):203–211, 1986.

[15] R. C. Kessler, K. A. McGonagle, M. Swartz, D. G. Blazer, and C. B. Nelson. Sex and depression in the national
comorbidity survey i: Lifetime prevalence, chronicity and recurrence. Journal of Affective Disorders, 29(2):85–
96, 1993.

[16] D. Lacy and B. C Burden. The vote-stealing and turnout effects of ross perot in the 1992 us presidential election.

American Journal of Political Science, pages 233–255, 1999.

19

[17] P. Lewis, C. McCracken, and R. Hunt. Politics: Who cares. American Demographics, pages 16–23, 1994.

[18] X.-B. Li. A Bayesian approach for estimating and replacing missing categorical data. J. Data and Information

Quality, 1(1):3:1–3:11, June 2009.

[19] M. Lunn and D. McNeil. Applying cox regression to competing risks. Biometrics, 51(2):524–532, 1995.

[20] K. Mansouri, T. Ringsted, D. Ballabio, R. Todeschini, and V. Consonni. Quantitative structure activity relation-
ship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling. Dataset
available on: http://archive.ics.uci.edu/ml/datasets.html, 2013.

[21] R. Mazumder, T. Hastie, and R. Tibshirani. Spectral regularization algorithms for learning large incomplete

matrices. Journal of machine learning research, 11(Aug):2287–2322, 2010.

[22] C. P. Robert. Simulation of truncated normal variables. Statistics and computing, 5(2):121–125, 1995.

[23] F. J. R. Ruiz, I. Valera, C. Blanco, and F. Perez-Cruz. Bayesian nonparametric modeling of suicide attempts.

Advances in Neural Information Processing Systems, 25:1862–1870, 2012.

[24] F. J. R. Ruiz, I. Valera, C. Blanco, and F. Perez-Cruz. Bayesian nonparametric comorbidity analysis of psychiatric

disorders. Journal of Machine Learning Research., 2013.

[25] R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markov Chain Monte Carlo.
Proceedings of the 25th International Conference on Machine Learning (ICML’08), pages 880–887, 2008.

[26] J. L. Schafer and J. W Graham. Missing data: our view of the state of the art. Psychological methods, 7(2):147,

[27] M. Titsias. The inﬁnite gamma-Poisson feature model. Advances in Neural Information Processing Systems, 19,

2002.

2007.

[28] A. Todeschini, F. Caron, and M. Chavent. Probabilistic low-rank matrix completion with adaptive spectral
regularization algorithms.
In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger,
editors, Advances in Neural Information Processing Systems 26, pages 845–853. Curran Associates, Inc., Dec.
2013.

[29] I. Valera and Z. Ghahramani. General table completion using a bayesian nonparametric model. Advances in

Neural Information Processing Systems 27 (NIPS 2014), 27:981–989, 2014.

[30] I. Valera, F. J. R. Ruiz, P. M. Olmos, C. Blanco, and F. Perez-Cruz.

Inﬁnite continuous feature model for

psychiatric comorbidity analysis. Neural computation, 2016.

[31] S. Weich and G. Lewis. Poverty, unemployment, and common mental disorders: population based cohort study.

BMJ, 317(7151):115–119, 1998.

[32] M. M. Weissman, Bland R., P. R. Joyce, S. Newman, J.E. Wells, and H.-U. Wittchen. Sex differences in rates of
depression: cross-national perspectives. Journal of Affective Disorders, 29:77 – 84, 1993. Special Issue Toward
a New Psychobiology of Depression in Women.

[33] S. Williamson, C. Wang, K. Heller, and D. Blei. The IBP compound Dirichlet process and its application to
focused topic modeling. Proceedings of the 27th Annual International Conference on Machine Learning, 2010.

20

GLFM: General Latent Feature Modeling Toolbox

A Introduction
The GLFM is a general Bayesian nonparametric latent feature model suitable for data exploration or missing values
imputation in heterogeneous datasets, where the attributes describing each object can be either discrete, continuous
or mixed variables. Speciﬁcally, the GLFM accounts for real-valued and positive real-valued data as examples of
continuous variables, and categorical, ordinal and count data as examples of discrete variables.

The GLFM builds on the Indian Buffet Process [12], and therefore, it assumes that each observation xd

n can be
explained by a potentially inﬁnite-length binary vector zn whose elements indicate whether a latent feature is active
or not for the n-th object; and a (real-valued) weighting vector Bd, whose elements weight the inﬂuence of each
latent feature in the d-th attribute11. Since the product of the latent feature vector and the weighting vector leads to
a real-valued variable, it is necessary to map this variable to the desirable output (continuous or discrete) space, for
. Thus, the GLFM assumes the existence
example, the positive real line or the ﬁnite ordered set
of intermediate real-valued auxiliary variables yd
d), called pseudo-observations, and a transformation
n ∼ N
) that maps this variable into the actual observation xd
function fd(
u) is
·
an auxiliary noise with zero mean and small variance σ2
u. Figure 11 illustrates the GLFM for a ordinal attribute taking
values in the ordered set

low, medium, high
}
{
(znBd, σ2
n = fd(yd
n, i.e., xd

n + u) where u

(0, σ2

∼ N

low, medium, high
.
}

{

B Implementation
The GLFM package contains an efﬁcient C++ implementation, together with an user interface in Python, Matlab
and R, of the collapsed Gibbs sampling algorithm described in Algorithm 1. The main function of the package,
hidden = GLFM.infer(data),12 runs the inference algorithm given the input structure data and returns the
learned latent variables in the output structure hidden. This function receives as input an observation matrix X and a
vector indicating the type of data for each dimension (optionally, model hyper-parameters and simulation settings can
be customized by the user). The latent variables are learned by making use of the mapping transformations listed in
Table 13 to account for the continuous and discrete data types mentioned above. Here, the parameters µ and w are used
to shift and scale the raw input data, and are respectively set to the empirical mean and the standard deviation for real-
valued attributes, and to the minimum value and the standard deviation for positive real-valued and count attributes.
This guarantees that the prior distributions on the latent variables are equally good for all the attributes in the dataset,
regardless their support. The output structure hidden contains the latest MCMC sample of the latent feature vectors
zn for n = 1, . . . , N , the weighting vectors Bd, as well as auxiliary variables (including the pseudo-observation
variances σ2

d and the thresholds θr) necessary for the corresponding transformation fd(

Moreover, since the Bayesian nonparametric nature of the GLFM allows the model complexity (i.e., the length of
the vectors zn and Bd) to grow with the number of observations, we bound the model complexity to a ﬁnite value,
which is an additional input to the GLFM package. This bound allows us not only to keep the model complexity,
and therefore the running time, under control, but also to efﬁciently manage the memory allocation. Finally, our
implementation of the GLFM makes use of the GNU Scientiﬁc Library (GSL),13 to efﬁciently perform a large variety
of mathematical routines such as random number generation, and matrix or vector operations.

), for d = 1, . . . , D.
·

C Usage
C.1 Data preprocessing and initialization
A convenient property of the GLFM package is that it can be used blindly on raw data without requiring any pre-
processing step on the dataset, nor special tuning of hyperparameters. The only requirement for the user to use the
package is to format the data as a numerical matrix of size N
D and indicate in an additional vector the type of
data for each of the D attributes. As mentioned above, the parameters of the transformations in Table 13 are internally

×

11For agreement with the notation in [? ], we here capitalize the vector Bd.
12This call corresponds to a python call. The equivalent call in Matlab is hidden = GLFM infer(data) and in R output ← GLFM infer(data).
13https://www.gnu.org/software/gsl/

21

Type of Variable
Real-valued

Domain
x ∈ (cid:60)

Positive real-valued

x ∈ (cid:60)+

Transformation x = fd(y)
x = w(y + u) + µ

x = log(exp(w(y + u) + µ) + 1)

Categorical

x ∈ {1, 2, . . . , R} (unordered set)

x = arg maxr∈{1,...,R} yr

Ordinal

x ∈ {1, 2, . . . , R} (ordered set)

x =

if
if

y ≤ θ1
θ1 < y ≤ θ2

1
2





...

R

if

θR−1 < y

Count

x ∈ {1, 2, 3, . . .}

x = (cid:98)log(exp(w(y + u) + µ) + 1)(cid:99)

Table 13: Mapping functions implemented in the toolbox.

ﬁxed to ensure that the pseudo-observation for all attributes fall in a similar interval of the real line, so that the prior
distribution on the latent variables is equally good for all attributes.

However, we incorporate an additional functionality that allows the user to specify external preprocessing functions
to further improve the performance of the algorithm. For instance, in cases in which the distribution of an attribute
presents a clearly non-Gaussian behavior, e.g., it is concentrated around a single value or heavy-tailed, it might be
suitable to preprocess this variable by applying a logarithmic transformation, as shown Figure 12.

C.2 Missing Data Estimation
GLFM can be used for estimation and imputation of missing data in heterogeneous datasets, where the missing values
can be encoded with any (numerical) value that the user speciﬁes. The Bayesian nature of the GLFM allows to
efﬁciently infer the latent feature representation of the data using the available information (i.e., the non-missing
values), and using it to compute the posterior distribution of each missing value in the data. Note that given the
posterior distribution of each missing value, one might opt for different approaches to impute missing values, e.g.,
one might opt for imputing a sample of the posterior distribution or simply the maximum a posteriori (MAP) value.
The GLFM package provides the function [Xmap, hidden]=GLFM.complete(data) which infers the latent
feature representation, given the (incomplete) observation matrix, and returns a complete matrix where the missing
values have been imputed to their MAP value; and the hidden structure containing all the inferred latent variables. This
function therefore runs the inference function GLFM.infer(), as well as the function GLFM.computeMAP(),
which computes the MAP of a single missing element xd

n given zn and Bd.

C.3 Data Exploration Analysis
GLFM can also be used as a tool for data exploratory analysis, since it is able to ﬁnd the latent structure in the
data and capture the statistical dependencies among the objects and their attributes in the data. GLFM provides
(weighted) binary latent features, easing their interpretation and making it possible to cluster the objects according
to their activation patterns of latent features. Moreover, it also allows to activate a latent feature that is active for
all the objects (a.k.a. bias term), which might be useful to capture the mode of the distribution of each attribute
in the dataset. In order to ease data exploration, GLFM provides the function GLFM.plotPatterns(), which
plots the posterior distribution for of each attribute under the given latent feature patterns, and therefore, allows us to
ﬁnd patterns and dependencies across both objects and attributes. This function, in turn, makes use of the function
GLFM.computePDF(), which evaluates the posterior distribution of an attribute under a given latent feature vector.

C.4 Examples
The package manual contains simple examples demonstrating package usage. Additionally, we provide the following
demonstrations (with the scripts in Python, Matlab and R):

•

•
•

demo toy example: Simple illustration of GLFM pipeline, replicating the example of the IBP linear-Gaussian
model in [12].
demo completion: Illustration of missing data estimation on the MNIST image dataset.
demo data exploration (counties & prostate): Replication of results on data exploration in the main paper. This
demo requires data download, which is instructed.

22

D Availability and Documentation
GLFM code is publicly available in https://github.com/ivaleraM/GLFM, where we provide a technical
document introducing the model and a user manual describing the usage details of the toolkit, including software
requirements. The Python and Matlab implementations are under MIT license. The R implementation extends the
RcppGSLExample14, and therefore, is under GPL (>= 2) license.

14https://github.com/eddelbuettel/rcppgsl/tree/master/inst/examples/RcppGSLExample

23

(a) Type of Cancer

(b) Drug Level

(c) Size of Primary Tumor (cm2)

(d) Serum Prostatic Acid Phosphatase

(e) Prognosis Status

Figure 4: Data exploration of a prostate cancer clinical trial. We depict the effect of each latent feature on each
attribute. Panels (a)-(d) shows different indicators of the prostate cancer, as well as the dose level of DES. Panel (d)
corresponds to Prognosis Status, which indicates whether the patient either is alive or dies from one of the following
three causes: vascular disease, prostatic cancer, or other reason. The baseline refers to the empirical distribution of
each attribute in the whole dataset.

24

(a) Probability of suffering from each disorder

Figure 5: Feature effects including gender in the analysis. (a) Probabilities of suffering from the 20 considered
disorders and (b) probability of being male and female for the latent feature vectors zn shown in the legend and for
the baseline.

(b) Gender

25

(a) Probability of suffering from each disorder

(b) Age

26

Figure 6: Feature effects including age in the analysis. (a) Probabilities of suffering from the 20 considered disorders
and (b) distribution of the age for the latent feature vectors zn shown in the legend and baseline probability distribution.

(a) % of votes cast for Ross Perot

(b) % of votes cast for Republican candidate

27

(c) % of votes cast for Democrat candidate

Figure 7: Inferred probability distribution for the ﬁve most popular patterns. The patterns are sorted in the legend
according to their degree of popularity, as described in Table 12. The baseline refers to the empirical distribution of
each attribute in the entire dataset.

(a) Pattern (000)

(b) Pattern (100)

(c) Pattern (101)

(d) Pattern (010)

(e) Pattern (110)

Figure 8: Empirical probability of pattern activation per state. We focus on the top-ﬁve most popular combinations
of features. The label for each pattern indicates whether Features F1, F2, and F3 are active (value ‘1’) or not (value
‘0’). Features F4 are F5 are always inactive in the ﬁve most common patterns, and thus are omitted in the labels.

Figure 9: Outcome of the 1992 presidential election per counties. Blue color corresponds to a majority of votes
for the Democrat party, red corresponds to a victory for the Republican party, green corresponds to a victory of the
independent party of Ross Perot.

28

(a) Population density (inhabitants/miles2)

(b) Percentage of white population

(c) Percentage of people ≥ 65 years old

(d) Average income (in dollars)

(e) Percentage of college degrees

(f) Percentage of farming population

Figure 10: Inferred probability distribution for the most occuring patterns. The baseline refers to the empirical
distribution of each attribute in the whole dataset.

Figure 11: Illustration of the GLFM.

29

(a) Xd without preprocessing

(b) Xd with log preprocessing

(c) Pseudo-observations yd

Figure 12: Illustration of optional data preprocessing. Panel (a) and (b) show the histograms of, respectively, a heavy-
tailed attribute and the attribute after a logarithmic transformation, as well the distribution of the inferred latent feature
patterns. Panel (c) shows the histogram of the pseudo-observations inferred for the original and the preprocessed
attributes, as well the distribution of the inferred latent feature patterns. Here, we observe that the distribution of
the attribute is better captured by the latent model when a preprocessing step is performed to correct/minimize the
non-Gaussian behavior of the attribute.

30

