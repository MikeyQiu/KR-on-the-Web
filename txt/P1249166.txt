Entity Linking within a Social Media Platform: A Case Study on Yelp

Hongliang Dai1, Yangqiu Song1, Liwei Qiu2 and Rijia Liu2
1Department of CSE, HKUST
2Tencent Technology (SZ) Co., Ltd.
1{hdai,yqsong}@cse.ust.hk
2{drolcaqiu,rijialiu}@tencent.com

Abstract

In this paper, we study a new entity linking
problem where both the entity mentions and
the target entities are within a same social me-
dia platform. Compared with traditional en-
tity linking problems that link mentions to a
knowledge base, this new problem have less
information about the target entities. How-
ever, if we can successfully link mentions to
entities within a social media platform, we can
improve a lot of applications such as compara-
tive study in business intelligence and opinion
leader ﬁnding. To study this problem, we con-
structed a dataset called Yelp-EL, where the
business mentions in Yelp reviews are linked
to their corresponding businesses on the plat-
form. We conducted comprehensive experi-
ments and analysis on this dataset with a learn-
ing to rank model that takes different types of
features as input, as well as a few state-of-the-
art entity linking approaches. Our experimen-
tal results show that two types of features that
are not available in traditional entity linking:
social features and location features, can be
very helpful for this task.

1

Introduction

Entity linking is the task of determining the iden-
tities of entities mentioned in texts. Most exist-
ing studies on entity linking have focused on link-
ing entity mentions to their referred entities in a
knowledge base (Cucerzan, 2007; Liu et al., 2013;
Ling et al., 2015). However, on social media plat-
forms such as Twitter, Instagram, Yelp, Facebook,
etc., the texts produced on them may often men-
tion entities that cannot be found in a knowledge
base, but can be found on the platform itself. For
example, consider Yelp, a platform where users
can write reviews about businesses such as restau-
rants, hotels, etc., a restaurant review on Yelp may
mention another restaurant to compare, which is
also likely to be on Yelp but cannot be found in

a knowledge base such as Wikipedia. As another
example, when people post a photo on a social me-
dia platform, their friends may be mentioned in
this post if they are also in the photo. Usually,
their friends are not included in a knowledge base
but may also have accounts on the same platform.
Thus for such entity mentions, linking them to an
account that is also on the platform is more practi-
cal than linking them to a knowledge base.

Performing this kind of entity linking can ben-
eﬁt many applications. For example, on Yelp, we
can perform analysis on the comparative sentences
in reviews after linking the business mentions in
them. The results can be directly used to either
provide recommendations for users or suggestions
for business owners.

Thus, in this paper, we focus on a new en-
tity linking problem where both the entity men-
tions and the target entities are within a social me-
dia platform. Speciﬁcally, the entity mentions are
from the texts (which we will refer to as men-
tion texts) produced by the users on a social me-
dia platform; and these mentions are linked to the
accounts on this platform.

It is not straightforward to apply existing entity
linking systems that link to a knowledge base to
this problem, because they usually take advantage
of the rich information knowledge bases provide
for the entities. For example, they can use detailed
text descriptions, varies kinds of attributes, etc., as
features (Francis-Landau et al., 2016; Gupta et al.,
2017; Tan et al., 2017), or even additional signals
such as the anchor texts in Wikipedia articles (Guo
and Barbosa, 2014; Globerson et al., 2016; Ganea
et al., 2016). However, on social media platforms,
most of these resources or information are either
unavailable or of poor quality.

On the other hand, social media platforms also
have some unique resources that can be exploited.
One that commonly exists on all of them is social

Figure 1: An example of entity linking within the Yelp social media platform. On Yelp, users can have friends
which makes it a social network. Users can also write reviews about a business and compare with other businesses.

information, which can be intuitively used in our
problem where mention texts and target entities
may be directly connected by users and their social
activities. Other than this, for location-based so-
cial media platforms such as Yelp and Foursquare,
location information can also be helpful since peo-
ple are more likely to mention and compare places
close to each other.

To study this problem, we construct a dataset
based on Yelp, which we name as Yelp-EL. As
shown in Figure 1, on Yelp, users can write re-
views for businesses and friend other users, and
the reviews they write may mention businesses
other than the reviewed ones. Thus, reviews,
users, and businesses are connected and form a
network through users’ activities on the platform.
In Yelp-EL, we link the business mentions in re-
views to their corresponding businesses on the
platform. We choose Yelp because other social
media platforms such as Facebook and Instagram
do not provide open dataset and there can be pri-
vacy issues related.

We then study the roles of three types of fea-
tures in our entity linking problem: social fea-
tures, location features, as well as conventional
features that are also frequently used in traditional
entity linking problems. We implemented a learn-
ing to rank model that takes the above features as
input. We conducted comprehensive experiments
and analysis on Yelp-EL with this model and also a
few state-of-the-art entity linking approaches that
we tailored to meet the requirements of Yelp-EL.
Experimental results show that both social and lo-
cation features can improve performance signiﬁ-
cantly.

Our contributions are summarized as follows.

• We are the ﬁrst attempt to study the new en-
tity linking problem where both entity men-
tions and target entities are within a same so-

cial media platform.

• We created a dataset based on Yelp to illus-
trate the usefulness of this problem and use
it as a benchmark to compare different ap-
proaches.

• We studied both traditional entity linking fea-
tures and social/location based features that
are available from the social media platform,
and show that they are indeed helpful for im-
proving the entity linking performance.

The code and data are available at https://
github.com/HKUST-KnowComp/ELWSMP.

2 Yelp-EL Dataset Construction

In this section we introduce how we create the
dataset Yelp-EL based on the Yelp social media
platform. We used the Round 9 version of the
Yelp challenge dataset1 to build Yelp-EL. There
are 4,153,150 reviews, 144,072 businesses, and
1,029,432 users in this dataset. In order to build
Yelp-EL, we ﬁrst ﬁnd possible entity mentions in
Yelp reviews, and then ask people to manually link
these mentions to Yelp businesses if possible.

Ideally, the mentions we need to extract from
the reviews should be only those that refer the
businesses in Yelp. Unfortunately, there is no
existing method or tool that can accomplish this
task. In fact, this problem itself is worth studying.
Nonetheless, since we focus on entity linking in
this paper, we only try to ﬁnd as many mentions
that may refer to Yelp businesses as we can, and
then let the annotators decide whether to link this
mention to a business. Thus, we use the following
two ways to ﬁnd mentions and then merge their
results.

1https://www.yelp.com/dataset/challenge

#Mentions
7,731

#Linked
1,749

#NIL #Disagreement1
5,117

842

#Disagreement2 Agreement%

23

88.8%

Table 1: Annotation statistics. “Linked” means the mentions that both annotators link to a same business. “NIL”
means the mentions that both annotators think are “unlinkable.” “Disagreement1” means the mentions that are
labeled by one annotator as “unlinkable,” but are linked to a business by the other annotator. “Disagreement2”
means the mentions that are linked by two annotators to two different businesses.

(1) We use the Standford NER tool (Finkel
et al., 2005) to ﬁnd ordinary entity mentions and
ﬁlter those that are unlikely to refer to businesses.
To do the ﬁltering, we ﬁrst construct a dictionary
which contains entity names that may occur in
Yelp reviews frequently but are unlikely to refer to
businesses, e.g., city names, country names, etc.
Then we run through the mentions found with the
NER tool and remove those whose mention strings
matches one of the names in the dictionary.

(2) We ﬁnd all the words/multi-word expres-
sions in reviews that match the name of a business,
and output them as mentions.

After extracting the mentions, we obtain the
ground-truth by asking annotators to label them.
Each time, we show the annotator one review with
the mentions in this review highlighted, the anno-
tator then needs to label each of the highlighted
mentions. For each mention, we show several can-
didate businesses whose names match the mention
string well. The annotator can also search the busi-
ness by querying its name and/or location, in case
the referred business is not included in the given
candidates. We also ask the annotators to label the
mention as “unlinkable” when its referred entity is
not a Yelp business or it is not an entity mention.
An important issue to note is franchises. There
are some mentions that refer to a franchise as a
whole, e.g., the mention “Panda Express” in the
sentence “If you want something different than the
usual Panda Express this is the place to come.”
There are also some mentions that refer to a spe-
ciﬁc location of a franchise. For example, the
mention “Best Buy” in “Every store you could
possibly need is no further than 3 miles from here,
which at that distance is Best Buy” refers to a spe-
ciﬁc “Best Buy” shop. As a location based social
network platform, Yelp only contains businesses
for different locations of franchises, not franchises
themselves. Thus in these cases, we ask the an-
notators to link the mentions when they refer to a
speciﬁc location of a franchise, but label them as
“unlinkable” when they refer to a franchise as a
whole.

We asked 14 annotators who are all undergradu-
ate or graduate students in an English environment
university to perform the annotation. They were
given a tutorial before starting to annotate, and
the annotation supervisor answered questions dur-
ing the procedure to ensure the annotation quality.
Each review is assigned to two annotators.

The statistics of the annotation results are
shown in Table 1. The total agree rate, calculated
as (#Linked + #NIL)/#Mentions, is 88.8%. Most
disagreements are on whether to link a mention
or not. We checked the data and ﬁnd that this
happens mostly when: they disagree on whether
the mention refers to a franchise as a whole or
just one speciﬁc location; one of the annotators
fails to ﬁnd the referred business. However, when
both annotators think the mention should be linked
to a business,
the disagree rate, calculated as
#Disagreement2/(#Linked + #Disagreement2), is
very low (only 1.3%).

We only use the mentions that both annotators
give the same labeling results to build the dataset.
As a result, we obtain 1,749 mentions that are
linked to a business. These mentions refer to 1,134
different businesses (mentioned businesses) and
are from 1,110 reviews. The reviews that contain
these mentions are for 967 different businesses (re-
viewed businesses).

The reviewed businesses are located in 96 dif-
ferent cities and belong to 419 different categories.
Note that a business can only locate in one city but
may have several different categories. The men-
tioned businesses are located in 98 different cities
and belong to 425 different categories. Figure 2
shows the numbers of reviewed businesses and
mentioned businesses in the most popular cities
and categories, from where we can see that these
mentions have an acceptable level of diversity.

The mentions that can be linked are our focus,
but we also include the 5,117 unlinkable mentions
in our dataset since they can be helpful for building
a complete entity discovery and linking system (Ji
et al., 2016).

(a) Top Cities

(b) Top Categories

Figure 2: Statistics of the related businesses in Yelp-EL. (a) The number of reviewed businesses in the six most
popular cities. (b) The number of mentioned businesses in the ﬁve most popular categories. Here, “popular” means
having the largest number of businesses in the dataset.

3 Entity Linking Algorithm

In this section, we introduce LinkYelp, an entity
linking approach we design for Yelp-EL to inves-
tigate the new proposed problem. LinkYelp con-
tains two main steps: candidate generation and
candidate ranking. The candidate generation step
ﬁnds a set of businesses that are plausible to be the
target of a mention based on the mention string.
Afterwards, the candidate ranking step ranks all
the candidates and chooses the top ranked one as
the target business.

3.1 Candidate Generation

For the ﬁrst step, candidate generation, we score
each business b with g(m, b) = gc(m, b) ·
gn(sm, sb) for a mention m, where sm is the men-
tion string of m, sb is the name of b. gc(m, b)
equals to a constant value that is larger than 1 (it
is set to 1.3 in practice) when the review that con-
tains m is for a business that is located in the same
city with b; Otherwise, it equals to 0. gn is deﬁned
as

gn(sm, sb) =

(cid:40)
1
sim(sm, sb)

if sm ∈ A(sb)
Otherwise,

(1)
where A(sb) is the set of possible acronyms for
sb, sim(sm, sb) is the cosine similarity between
the TF-IDF representations of sm and sb. In prac-
tice, A(sb) is empty when sb contains less than
two words; Otherwise, it contains one string: the
concatenation of the ﬁrst letter of each word in sb.
Then, we ﬁnd the top 30 highest scored businesses

as candidates. This approach has a recall of 0.955
on Yelp-EL.

3.2 Candidate Ranking

Let m be a mention and b be a candidate business
of m. We use the following function to score how
likely b is the correct business that m refers to:

f (m, b) = w · φ(m, b),

(2)

where φ(m, b) is the feature vector for mention-
candidate pair m and b, Section 4 describes how
to obtain it in detail; w is a parameter vector.

We use a max-margin based loss function to

train w:

J =

1
|T |

(cid:88)

max[0, 1 − f (m, bt)

(3)

<m,bt,bc>∈T

+ f (m, bc)] + λ(cid:107)w(cid:107)2,

where bt is the true business mention m refers to;
bc (cid:54)= bt is a corrupted business sample randomly
picked from the candidates of m; T is the set of
training samples; (cid:107) · (cid:107) is the l2-norm; λ is a hyper-
parameter that controls the regularization strength.
We use stochastic gradient descent to train this
model.

4 Feature Engineering

We study the effectiveness of three types of fea-
tures: conventional features, social features, and
location features. Among them, conventional fea-
tures are those that can also be use in traditional
entity linking tasks; social features and location
features are unique in our problem.

4.1 Conventional Features

Lots of information used in traditional entity link-
ing cannot be found for Yelp businesses, but we
try our best to include all such features that can be
used in our problem.

For Yelp-EL, we use the following conventional
features for a mention m and its candidate busi-
ness b:

u1 : The cosine similarity between the TF-IDF
representations of the mention string of m
and the name of b.

u2 : Whether the mention string of m is a pos-
sible acronym of b’s name (i.e., whether it is
an element of the set A(sb) in Equation 1).

u3 : The popularity of b. Let the number of re-
views received by b be n. Then this feature
value equals to n/C if n is smaller than a pa-
rameter C that’s used for normalization, oth-
erwise it equals to 1.

u4 : The cosine similarity between the TF-IDF
representations of the review that contains m
and combination of all reviews of b. This fea-
ture evaluates how well b ﬁts m semantically.

u5 : Whether b is the same as the reviewed busi-
ness. This feature is actually not available in
traditional EL, and it is usually not available
on other social media platforms either. But
it is obviously useful on Yelp-EL. Including
it here helps us to see how beneﬁcial social
features and location features truly are.

4.2 Social Features

Through the activities of the users on the plat-
form, the users, mentions, reviews and businesses
in Yelp-EL form a network where there are differ-
ent types of nodes and edges. Thus we use Hetero-
geneous Information Networks (HIN) to model it,
and then design meta-path based features to cap-
ture the relations between mentions and their can-
didate businesses. We skip the formal deﬁnitions
of HIN and meta-path here, readers can refer to
(Sun et al., 2011) for detailed introduction. The
HIN schema for Yelp-EL is shown in Figure 3.

The following meta-paths are used:

Figure 3: HIN schema of Yelp-EL. M: mention; R: Re-
view; U: user; B: business.

where we denote M for mention, R for Review, U
for user, and B for business.

Different meta-paths above capture different
kinds of relations between a mention and its can-
didate entities that are induced by users’ social ac-
tivities. For example, if an instance of P 1 exists
between a mention m and a business b, then m is
contained in a review that is written by a user who
also reviewed business b. If many such instances
of P 1 exist, then we may assume that m and b are
related, which makes it more possible for m to be
referring to b.

With the meta-paths above, we use the Path
Count feature deﬁned in (Sun et al., 2011) to feed
into the entity linking model described in Section
3. Given a meta-path P , for mention m and busi-
ness b, Path Count is the number of path instances
of P that start from m and end with b. In practice,
we normalize this value based on global statistics
before feeding it to a model.

4.3 Location Features

commonly

information

in
Location
location-based social media platforms such as
Yelp and Foursquare. Users on platforms such
as Twitter and Instagram may also be willing to
provide their locations.

exists

Here, we use the following two features for a

mention m and its candidate business b:

v1 : Whether the reviewed business is in the

same city as b.

v2 : The geographical distance between the re-
viewed business and b. This value is calcu-
lated based on the longitude and latitude co-
ordinates of the businesses.

P 1 : M − R − U − R − B
P 2 : M − R − U − U − R − B
P 3 : M − R − U − R − B − R − U − R − B

There are still some other location features that
can be designed. For example, we can also con-
sider the locations of the other businesses that are

reviewed by the user. We only use the above two
since we ﬁnd in our experiments that including
them already provides high performance boost.

5 Experiments

5.1 Compared Methods

We compare with a baseline method we name as
DirectLink, as well as two existing entity linking
methods including the method proposed by (Liu
et al., 2013) (which we refer to as ELT) and SS-
Regu proposed by (Huang et al., 2014).

DirectLink simply links each mention to the
corresponding reviewed business. Many business
mentions in Yelp reviews actually refer to the busi-
ness that is being reviewed. This baseline method
tells us how many of these mentions there are in
Yelp-EL.

ELT collectively links a set of mentions with
an objective to maximize local compatibility and
global consistence. It achieves this by integrating
three types of similarities: mention-entity similar-
ity, entity-entity similarity, and mention-mention
similarity. To apply ELT to Yelp-EL, we use the
conventional features introduced in Section 4.1 for
mention-entity similarities. The path count fea-
ture of meta-path B–R–U–R–B is used as entity-
entity similarity. For mention-mention similarity,
we use two features that are both TF-IDF based
cosine similarities, with one between the two men-
tion strings and the other between the reviews that
the two mentions belong to.

SSRegu is also a collective approach.

It is a
graph regularization model that incorporates both
local and global evidence through three principals:
local compatibility, coreference, and semantic re-
latedness. SSRegu computes a weight matrix for
each of these three principals, and then forms a
graph based on the weight matrices and performs
graph regularization to rank candidate entities. To
apply SSRegu, we need to compute three weight
matrices. The weight matrix for local compatibil-
ity is based on features extracted from the men-
tion and the candidate entity.
In our case, the
conventional features are used for computing this
matrix. Computing the coreference weight matrix
requires to determine whether two corresponding
mentions are coreferential. Huang et al. (2014)
assume two mentions to be coreferential if their
mention strings are the same and there exists at
least one meta-path instance of speciﬁc patterns
between them. In our case, the meta-paths used

Method

Accuracy (mean±std)

DirectLink
ELT
SSRegu
LinkYelp

0.6684±0.008
0.8451±0.012
0.7970±0.013
0.9034±0.014

Table 2: Entity linking performance of different meth-
ods on Yelp-EL.

are M–R–M and M–R–U–R–M. To compute the
semantic relatedness weight matrix, we apply the
entity-entity similarity used for ELT. Note that SS-
Regu is a semi-supervised approach and is capable
of using unsupervised data, but for fair comparison
we do not use this feature here.

ELT and SSRegu are originally proposed to
tackle the problem of entity linking for tweets,
but their linking target is Wikipedia. Evaluating
the performance of these two methods on Yelp-EL
shows the difference between their problem and
ours.

5.2 Experimental Settings

Throughout our experiments, the hyperparameter
λ in Equation (3) is set to 0.001. For each men-
tion, three corrupted samples are random selected
for the training with Equation (3). For ELT and
SSRegu, the hyperparameters are tuned on the val-
idation set with grid search. The candidate busi-
nesses for ELT and SSRegu are also obtained with
the method describe in Section 3.1. We run ﬁve
trials of random split of the linked mentions in the
dataset, where each trail uses 60% of the linked
mentions as training data and 40% as test data. In
each of the training set, we further select 20% as
validation set.

Note that we only use linked mentions to eval-
uate different methods since NIL detection is not
our focus, but NIL mentions are utilized in Section
5.6 to build a complete entity linking system.

5.3 Comparison Results

Table 2 shows the entity linking performance of
different methods on Yelp-EL. Here, all three
types of features described in Section 4 are fed
into LinkYelp. Within the compared methods,
LinkYelp performs substantially better.
This
shows that methods carefully designed for tradi-
tional entity linking problems may not work so
well when applied to entity linking within a so-
cial media platform, and this new problem we pro-

Features
C
S
L
C+S
C+L
S+L
C+S+L

All
84.05
79.65
80.05
86.45
89.42
85.19
90.34

Restaurants Nightlife Shopping
79.18
75.09
72.70
84.98
84.98
79.86
85.67

83.15
82.05
81.32
85.71
90.48
87.91
91.94

86.80
82.92
85.71
89.29
93.32
89.29
93.79

Food A & E Bars
83.49
71.67
81.15
80.66
66.95
77.69
82.55
50.21
85.77
87.26
78.54
85.00
88.21
76.39
88.46
85.38
72.10
87.31
88.68
78.97
90.00

E & S H & T
63.70
68.79
67.41
72.25
37.78
49.13
71.85
77.46
66.67
72.83
67.41
72.25
71.11
76.30

Table 3: Entity linking accuracy (%) on different categories of businesses with different types of features as input.
On the “Features” column, “C,” “S,” and “L” means conventional, social, and location features respectively. “All”
means all the categories combined, i.e., the whole test set; “A & E” means Arts & Entertainment; “E & S” means
Event Planning & Services; “H & T” means Hotels & Travel.

pose is worth studying differently from the tradi-
tional entity linking problem. The accuracy of Di-
rectLink means that many mentions (about 67%)
in Yelp-EL simply refer to the corresponding re-
viewed businesses. However, this does not mean
that our problem is less challenging than tradi-
tional entity linking, since simply using the popu-
larity measure of entities can achieve an accuracy
of about 82% in the latter task (Pan et al., 2015).

5.4 Ablation Study

We further investigate how the three different
types of features described in Section 4 contribute
to the ﬁnal performance of LinkYelp, and how
they perform differently in linking mentions that
refer to a speciﬁc category of Yelp businesses. The
results are listed in Table 3. The categories in Ta-
ble 3 are those that include the largest numbers
of businesses in the dataset. Entries in the “All”
column in Table 3 are the accuracies on all the cat-
egories combined. We can see from this column
that both social features and location features are
able to improve the performance when combined
with conventional features. Location features are
relatively more effective than social features, this
is because people’s activities are mainly restricted
to a certain area, so they are more likely to mention
businesses that are within this area. But social fea-
tures are still helpful even when both conventional
features and location features are already used, as
the best performance is achieved with all the three
types of features combined. Moreover, social fea-
tures can become more important for other social
media platforms that do not have location infor-
mation available.

There are also some interesting ﬁndings if we
consider the performance on different categories.
For example, compared with only using conven-

tional features (row C), incorporating social fea-
tures (row C+S) provides the largest improvement
for Event Planing & Services (e.g., wedding plan-
ning, party planning). This matches our intuition
because for these kinds of businesses, people tend
to be inﬂuenced more by their friends and make
choices that are socially related. Table 3 also
shows that on the categories Event Planning &
Services and Hotels & Travel, incorporating loca-
tion features is not that helpful as it does on other
categories. We manually checked the mentions
under these two categories that are linked correctly
by C+S but incorrectly by C+L. We ﬁnd that the
reasons why incorporating location features fails
on these mentions vary from case to case. Two
possible reasons are: location information is not
helpful to disambiguate a hotel and the shops in
this hotel; it also does not work well in disam-
biguating different locations of a hotel chain that
are all not far away from the reviewed business.

5.5 Error Examples

We also manually checked some of the errors
made by LinkYelp with all the three types of fea-
tures as input. A few examples are shown in Table
4.
In the ﬁrst case, since the reviewed business
“Jean Philippe Patisserie” is a restaurant, our sys-
tem tends to ﬁnd a similar business instead of a
hotel. Location features do not help here because
Cafe Bellagio has the same location as Bellagio
Hotel. The system is also incapable of identify-
ing that “stay at” should be probably followed by
a hotel instead of a Cafe. In the second case, the
algorithm outputs the reviewed business because
it is unable to understand what “the other Second
Sole in Rock River” means. The above two exam-
ples show that there are still some errors caused
by the failure of natural language understanding.

Name: Jean Philippe Patisserie
Reviewed Biz:
Review:
... Even if you are not staying at the Bellagio, you have to stop by anyway to ...
Name: Bellagio Hotel
True Referent:
System Prediction: Name: Cafe Bellagio

Addr.: 3600 S Las Vegas Blvd, Las Vegas
Addr.: 3600 S Las Vegas Blvd, Las Vegas

Addr.: 3600 S Las Vegas Blvd, Las Vegas

Reviewed Biz:
Review:
True Referent:
System Prediction: Name: Second Sole Athletic Footwear

Name: Second Sole Athletic Footwear
I did a review of the other Second Sole in Rocky River. This one is in Lyndhurst...
Addr.: 19341 Detroit Rd, Rocky River
Name: Second Sole

Addr.: 5114 Mayﬁeld Rd, Cleveland

Addr.: 5114 Mayﬁeld Rd, Cleveland

Reviewed Biz:
Review:
True Referent:
System Prediction: Name: Frys

Name: Hoot Owl
This place is a really fun neighborhood bar ... Its tucked away in the Frys parking lot...
Name: Fry’s Food and Drug

Addr.: 4315 W Bell Road, Phoenix

Addr.: 4361 W Bell Rd, Phoenix

Addr.: 2626 S 83rd Ave, Phoenix

Table 4: Examples of errors made by LinkYelp. Business mentions are underlined.

Reviewed Biz
Name: Burger King
Addr.: 1194 King St W, Toronto
Name: The Turf Public House
Addr.: 705 N 1st St, Phoenix

Sentence
When you compare it
across the street, the service is way better.
I have to say I like the atmosphere and sur-
roundings of the Turf better than Seamus.

to the McDonald’s

Mentioned Biz
Name: McDonald’s
Addr.: 1221 King Street W, Toronto
Name: Seamus McCaffrey’s
Addr.: 18 W Monroe St, Phoenix

Table 5: Examples of comparative sentences and linked mentions.

Name

City

Bacchanal Buffet Las Vegas
Las Vegas
Phoenix
Phoenix
Las Vegas
Las Vegas

Wicked Spoon
Cibo
Pizzeria Bianco
XS Nightclub
Marquee

Stars
4.0
3.5
4.5
4.0
4.0
3.5

#Better
33
24
9
2
8
1

Table 6: Comparative study using texts and average
ratings. Each row is a pair of two frequently compared
businesses. #Better means the number of sentences that
claim the corresponding business to be better than the
other one.

In the third case, “Fry’s Food and Drug” is lo-
cated at “4315 W Bell Road, Phoenix” which is
nearer to the reviewed business “Hoot Owl” lo-
cated at “4361 W Bell Rd, Phoenix.” However,
although location information favors the correct
business, the others features may contribute more
for the system output “Frys” since “Frys” has an
exact match of the candidate mention name.

5.6 Comparative Study

In this study, we provide some insight on the pos-
sible applications of our task by checking the com-
parative sentences in Yelp reviews.

First, we ﬁnd comparative sentences from the
whole Yelp review dataset with a simple pat-
tern matching method: we retrieve the sentences
that contain one of eight predeﬁned comparison
phrases such as “is better,” “not as good as,” etc.

Then we extract the named entity mentions within
these sentences and link them to Yelp businesses.
A threshold based approach is used to detect NIL
mentions (Dalton and Dietz, 2013).

As a result, we get 12,149 comparative sen-
tences from the total 4,153,150 reviews that con-
tains at least one linked mention. Some of the re-
sults are shown in Table 5. We can successfully
identify both the entity names and their locations
on Yelp. We also selected the top three frequently
compared pairs and compare with the stars pro-
vided by Yelp dataset. From Table 6 we can see
that the text comparison is consistent with star rat-
ings.

6 Related Work

The traditional entity linking task of mapping
mentions in articles to their corresponding enti-
ties in a knowledge base has been studied exten-
sively (Shen et al., 2015; Ling et al., 2015). Vari-
ous kinds of methods have been studied, e.g., neu-
ral network models (Sun et al., 2015; He et al.,
2013), generative models (Li et al., 2013), etc.
A large group of the existing entity linking ap-
proaches are called collective approaches, which
are based on the observation that the entities men-
tioned in a same context are usually related with
each other. Thus they usually form entity linking
as an optimization problem that tries to maximizes
both local mention-entity compatibility and global
entity-entity coherence (Han et al., 2011; Nguyen
et al., 2016). LinkYelp does not consider global

entity-entity coherence as it is not the focus of this
paper, but it can be applied to our problem too.

The prevalence of on-line social networks has
also motivated researchers to study entity linking
in such environments. (Huang et al., 2014; Shen
et al., 2013; Liu et al., 2013) proposed methods
that are specially designed for linking named en-
tities in tweets. They mainly address the problem
that tweets are usually short and informal, while
taking advantage of some of the extra information
that tweets may provide. For example, (Shen et al.,
2013) assumed that each user’s tweets have an un-
derlying interest distribution and proposed a graph
based interest propagation algorithm to rank the
entities. (Huang et al., 2014) also used meta-path
on HIN in their entity linking approach, but they
only used it to get an indication of whether two
mentions are related. Finally, although these stud-
ies focused on entity linking for tweets, they still
use entities in knowledge bases as the target.

There are a few entity linking studies that do not
link mentions to knowledge bases.
(Shen et al.,
2017) proposed to link entity mentions to an HIN
such as DBLP and IMDB. However, their articles
are collected from the Internet through searching
and thus are not related to the target entities. They
also used an HIN based method, but their use is
restricted to get the relatedness between different
entities. (Lin et al., 2017) studied the entity linking
problem where the entities are included in differ-
ent lists and entities of the same type belong to the
same list. They only used this information along
with the name of each entity to perform entity link-
ing. Thus their focus is very different from ours.

7 Conclusions

In this paper, we propose a new entity linking
problem where both entity mentions and target en-
tities are in a same social media platform. To
study this problem, we ﬁrst create a dataset called
Yelp-EL, and then conduct extensive experiments
and analysis on it with a learning to rank model
that takes three different types of features as input.
Through the experimental results, we ﬁnd that tra-
ditional entity linking approaches may not work
so well on our problem. The two types of features
that are usually not available for traditional entity
linking tasks – social features and location features
– can both improve the performance signiﬁcantly
on Yelp-EL. Our work can also motivate and en-
able a lot of downstream applications such as com-

parative analysis of location based businesses. In
the future, we plan to extract more patterns to ob-
tain more comparative sentences, so that we may
more accurately demonstrate how useful perform-
ing comparative analysis after linking the business
mentions can be.

Acknowledgments

This paper was supported by the Early Career
Scheme (ECS, No.
26206717) from Research
Grants Council in Hong Kong. The experiments
and conclusions contained herein are those of the
authors and should not be interpreted as necessar-
ily representing the afﬁliated company.

References

Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. Proceedings
of EMNLP-CoNLL, page 708.

Jeffrey Dalton and Laura Dietz. 2013. A neighborhood
relevance model for entity linking. In Proceedings
of the 10th Conference on Open Research Areas in
Information Retrieval, pages 149–156.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005.
Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of ACL, pages 363–370.

Matthew Francis-Landau, Greg Durrett, and Dan
Klein. 2016. Capturing semantic similarity for en-
tity linking with convolutional neural networks. In
Proceedings of NAACL, pages 1256–1261.

Octavian-Eugen Ganea, Marina Ganea, Aurelien Luc-
chi, Carsten Eickhoff, and Thomas Hofmann. 2016.
Probabilistic bag-of-hyperlinks model for entity
linking. In Proceedings of WWW, pages 927–938.

Amir Globerson, Nevena Lazic, Soumen Chakrabarti,
Amarnag Subramanya, Michael Ringaard, and Fer-
nando Pereira. 2016. Collective entity resolution
with multi-focal attention. In Proceedings of ACL,
volume 1, pages 621–631.

Zhaochen Guo and Denilson Barbosa. 2014. Robust
entity linking via random walks. In Proceedings of
CIKM, pages 499–508.

Nitish Gupta, Sameer Singh, and Dan Roth. 2017. En-
tity linking via joint encoding of types, descrip-
tions, and context. In Proceedings of EMNLP, pages
2681–2690.

Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective
entity linking in web text: a graph-based method. In
Proceedings of SIGIR, pages 765–774.

Zhengyan He, Shujie Liu, Mu Li, Ming Zhou, Longkai
Zhang, and Houfeng Wang. 2013. Learning entity
In Pro-
representation for entity disambiguation.
ceedings of ACL, pages 30–34.

Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S Yu, and
Tianyi Wu. 2011. Pathsim: Meta path-based top-k
similarity search in heterogeneous information net-
works. VLDB, 4(11):992–1003.

Hongzhao Huang, Yunbo Cao, Xiaojiang Huang, Heng
Ji, and Chin-Yew Lin. 2014. Collective tweet wiki-
ﬁcation based on semi-supervised graph regulariza-
tion. In Proceedings of ACL, pages 380–390.

Chuanqi Tan, Furu Wei, Pengjie Ren, Weifeng Lv, and
Ming Zhou. 2017. Entity linking for queries by
In Proceedings of
searching wikipedia sentences.
EMNLP, pages 68–77.

Heng Ji, Joel Nothman, Hoa Trang Dang, and Syd-
ney Informatics Hub. 2016. Overview of tac-
kbp2016 tri-lingual edl and its impact on end-to-end
cold-start kbp. Proceedings of of TAC.

Yang Li, Chi Wang, Fangqiu Han, Jiawei Han, Dan
Roth, and Xifeng Yan. 2013. Mining evidences
for named entity disambiguation. In Proceedings of
KDD, pages 1070–1078.

Ying Lin, Chin-Yew Lin, and Heng Ji. 2017. List-only
In Proceedings of ACL, volume 2,

entity linking.
pages 536–541.

Xiao Ling, Sameer Singh, and Daniel S Weld. 2015.
Design challenges for entity linking. TACL, 3:315–
328.

Xiaohua Liu, Yitong Li, Haocheng Wu, Ming Zhou,
Furu Wei, and Yi Lu. 2013. Entity linking for
tweets. In Proceedings of ACL, pages 1304–1311.

Thien Huu Nguyen, Nicolas Fauceglia, Mariano Ro-
driguez Muro, Oktie Hassanzadeh, Alﬁo Massimil-
iano Gliozzo, and Mohammad Sadoghi. 2016. Joint
learning of local and global features for entity link-
In Proceedings of COL-
ing via neural networks.
ING, pages 2310–2320.

Xiaoman Pan, Taylor Cassidy, Ulf Hermjakob, Heng Ji,
and Kevin Knight. 2015. Unsupervised entity link-
In Pro-
ing with abstract meaning representation.
ceedings of NAACL-HLT, pages 1130–1139.

Wei Shen, Jiawei Han, Jianyong Wang, Xiaojie Yuan,
and Zhenglu Yang. 2017. Shine+: A general frame-
work for domain-speciﬁc entity linking with het-
erogeneous information networks. IEEE Trans. on
Knowl. and Data Eng.

Wei Shen, Jianyong Wang, and Jiawei Han. 2015. En-
Issues, tech-
IEEE Trans. Knowl. Data

tity linking with a knowledge base:
niques, and solutions.
Eng., 27(2):443–460.

Wei Shen, Jianyong Wang, Ping Luo, and Min Wang.
2013. Linking named entities in tweets with knowl-
In Proceed-
edge base via user interest modeling.
ings of KDD, pages 68–76.

Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhen-
zhou Ji, and Xiaolong Wang. 2015. Modeling men-
tion, context and entity with neural networks for en-
tity disambiguation. In Proceedings of IJCAI, pages
1333–1339.

Entity Linking within a Social Media Platform: A Case Study on Yelp

Hongliang Dai1, Yangqiu Song1, Liwei Qiu2 and Rijia Liu2
1Department of CSE, HKUST
2Tencent Technology (SZ) Co., Ltd.
1{hdai,yqsong}@cse.ust.hk
2{drolcaqiu,rijialiu}@tencent.com

Abstract

In this paper, we study a new entity linking
problem where both the entity mentions and
the target entities are within a same social me-
dia platform. Compared with traditional en-
tity linking problems that link mentions to a
knowledge base, this new problem have less
information about the target entities. How-
ever, if we can successfully link mentions to
entities within a social media platform, we can
improve a lot of applications such as compara-
tive study in business intelligence and opinion
leader ﬁnding. To study this problem, we con-
structed a dataset called Yelp-EL, where the
business mentions in Yelp reviews are linked
to their corresponding businesses on the plat-
form. We conducted comprehensive experi-
ments and analysis on this dataset with a learn-
ing to rank model that takes different types of
features as input, as well as a few state-of-the-
art entity linking approaches. Our experimen-
tal results show that two types of features that
are not available in traditional entity linking:
social features and location features, can be
very helpful for this task.

1

Introduction

Entity linking is the task of determining the iden-
tities of entities mentioned in texts. Most exist-
ing studies on entity linking have focused on link-
ing entity mentions to their referred entities in a
knowledge base (Cucerzan, 2007; Liu et al., 2013;
Ling et al., 2015). However, on social media plat-
forms such as Twitter, Instagram, Yelp, Facebook,
etc., the texts produced on them may often men-
tion entities that cannot be found in a knowledge
base, but can be found on the platform itself. For
example, consider Yelp, a platform where users
can write reviews about businesses such as restau-
rants, hotels, etc., a restaurant review on Yelp may
mention another restaurant to compare, which is
also likely to be on Yelp but cannot be found in

a knowledge base such as Wikipedia. As another
example, when people post a photo on a social me-
dia platform, their friends may be mentioned in
this post if they are also in the photo. Usually,
their friends are not included in a knowledge base
but may also have accounts on the same platform.
Thus for such entity mentions, linking them to an
account that is also on the platform is more practi-
cal than linking them to a knowledge base.

Performing this kind of entity linking can ben-
eﬁt many applications. For example, on Yelp, we
can perform analysis on the comparative sentences
in reviews after linking the business mentions in
them. The results can be directly used to either
provide recommendations for users or suggestions
for business owners.

Thus, in this paper, we focus on a new en-
tity linking problem where both the entity men-
tions and the target entities are within a social me-
dia platform. Speciﬁcally, the entity mentions are
from the texts (which we will refer to as men-
tion texts) produced by the users on a social me-
dia platform; and these mentions are linked to the
accounts on this platform.

It is not straightforward to apply existing entity
linking systems that link to a knowledge base to
this problem, because they usually take advantage
of the rich information knowledge bases provide
for the entities. For example, they can use detailed
text descriptions, varies kinds of attributes, etc., as
features (Francis-Landau et al., 2016; Gupta et al.,
2017; Tan et al., 2017), or even additional signals
such as the anchor texts in Wikipedia articles (Guo
and Barbosa, 2014; Globerson et al., 2016; Ganea
et al., 2016). However, on social media platforms,
most of these resources or information are either
unavailable or of poor quality.

On the other hand, social media platforms also
have some unique resources that can be exploited.
One that commonly exists on all of them is social

Figure 1: An example of entity linking within the Yelp social media platform. On Yelp, users can have friends
which makes it a social network. Users can also write reviews about a business and compare with other businesses.

information, which can be intuitively used in our
problem where mention texts and target entities
may be directly connected by users and their social
activities. Other than this, for location-based so-
cial media platforms such as Yelp and Foursquare,
location information can also be helpful since peo-
ple are more likely to mention and compare places
close to each other.

To study this problem, we construct a dataset
based on Yelp, which we name as Yelp-EL. As
shown in Figure 1, on Yelp, users can write re-
views for businesses and friend other users, and
the reviews they write may mention businesses
other than the reviewed ones. Thus, reviews,
users, and businesses are connected and form a
network through users’ activities on the platform.
In Yelp-EL, we link the business mentions in re-
views to their corresponding businesses on the
platform. We choose Yelp because other social
media platforms such as Facebook and Instagram
do not provide open dataset and there can be pri-
vacy issues related.

We then study the roles of three types of fea-
tures in our entity linking problem: social fea-
tures, location features, as well as conventional
features that are also frequently used in traditional
entity linking problems. We implemented a learn-
ing to rank model that takes the above features as
input. We conducted comprehensive experiments
and analysis on Yelp-EL with this model and also a
few state-of-the-art entity linking approaches that
we tailored to meet the requirements of Yelp-EL.
Experimental results show that both social and lo-
cation features can improve performance signiﬁ-
cantly.

Our contributions are summarized as follows.

• We are the ﬁrst attempt to study the new en-
tity linking problem where both entity men-
tions and target entities are within a same so-

cial media platform.

• We created a dataset based on Yelp to illus-
trate the usefulness of this problem and use
it as a benchmark to compare different ap-
proaches.

• We studied both traditional entity linking fea-
tures and social/location based features that
are available from the social media platform,
and show that they are indeed helpful for im-
proving the entity linking performance.

The code and data are available at https://
github.com/HKUST-KnowComp/ELWSMP.

2 Yelp-EL Dataset Construction

In this section we introduce how we create the
dataset Yelp-EL based on the Yelp social media
platform. We used the Round 9 version of the
Yelp challenge dataset1 to build Yelp-EL. There
are 4,153,150 reviews, 144,072 businesses, and
1,029,432 users in this dataset. In order to build
Yelp-EL, we ﬁrst ﬁnd possible entity mentions in
Yelp reviews, and then ask people to manually link
these mentions to Yelp businesses if possible.

Ideally, the mentions we need to extract from
the reviews should be only those that refer the
businesses in Yelp. Unfortunately, there is no
existing method or tool that can accomplish this
task. In fact, this problem itself is worth studying.
Nonetheless, since we focus on entity linking in
this paper, we only try to ﬁnd as many mentions
that may refer to Yelp businesses as we can, and
then let the annotators decide whether to link this
mention to a business. Thus, we use the following
two ways to ﬁnd mentions and then merge their
results.

1https://www.yelp.com/dataset/challenge

#Mentions
7,731

#Linked
1,749

#NIL #Disagreement1
5,117

842

#Disagreement2 Agreement%

23

88.8%

Table 1: Annotation statistics. “Linked” means the mentions that both annotators link to a same business. “NIL”
means the mentions that both annotators think are “unlinkable.” “Disagreement1” means the mentions that are
labeled by one annotator as “unlinkable,” but are linked to a business by the other annotator. “Disagreement2”
means the mentions that are linked by two annotators to two different businesses.

(1) We use the Standford NER tool (Finkel
et al., 2005) to ﬁnd ordinary entity mentions and
ﬁlter those that are unlikely to refer to businesses.
To do the ﬁltering, we ﬁrst construct a dictionary
which contains entity names that may occur in
Yelp reviews frequently but are unlikely to refer to
businesses, e.g., city names, country names, etc.
Then we run through the mentions found with the
NER tool and remove those whose mention strings
matches one of the names in the dictionary.

(2) We ﬁnd all the words/multi-word expres-
sions in reviews that match the name of a business,
and output them as mentions.

After extracting the mentions, we obtain the
ground-truth by asking annotators to label them.
Each time, we show the annotator one review with
the mentions in this review highlighted, the anno-
tator then needs to label each of the highlighted
mentions. For each mention, we show several can-
didate businesses whose names match the mention
string well. The annotator can also search the busi-
ness by querying its name and/or location, in case
the referred business is not included in the given
candidates. We also ask the annotators to label the
mention as “unlinkable” when its referred entity is
not a Yelp business or it is not an entity mention.
An important issue to note is franchises. There
are some mentions that refer to a franchise as a
whole, e.g., the mention “Panda Express” in the
sentence “If you want something different than the
usual Panda Express this is the place to come.”
There are also some mentions that refer to a spe-
ciﬁc location of a franchise. For example, the
mention “Best Buy” in “Every store you could
possibly need is no further than 3 miles from here,
which at that distance is Best Buy” refers to a spe-
ciﬁc “Best Buy” shop. As a location based social
network platform, Yelp only contains businesses
for different locations of franchises, not franchises
themselves. Thus in these cases, we ask the an-
notators to link the mentions when they refer to a
speciﬁc location of a franchise, but label them as
“unlinkable” when they refer to a franchise as a
whole.

We asked 14 annotators who are all undergradu-
ate or graduate students in an English environment
university to perform the annotation. They were
given a tutorial before starting to annotate, and
the annotation supervisor answered questions dur-
ing the procedure to ensure the annotation quality.
Each review is assigned to two annotators.

The statistics of the annotation results are
shown in Table 1. The total agree rate, calculated
as (#Linked + #NIL)/#Mentions, is 88.8%. Most
disagreements are on whether to link a mention
or not. We checked the data and ﬁnd that this
happens mostly when: they disagree on whether
the mention refers to a franchise as a whole or
just one speciﬁc location; one of the annotators
fails to ﬁnd the referred business. However, when
both annotators think the mention should be linked
to a business,
the disagree rate, calculated as
#Disagreement2/(#Linked + #Disagreement2), is
very low (only 1.3%).

We only use the mentions that both annotators
give the same labeling results to build the dataset.
As a result, we obtain 1,749 mentions that are
linked to a business. These mentions refer to 1,134
different businesses (mentioned businesses) and
are from 1,110 reviews. The reviews that contain
these mentions are for 967 different businesses (re-
viewed businesses).

The reviewed businesses are located in 96 dif-
ferent cities and belong to 419 different categories.
Note that a business can only locate in one city but
may have several different categories. The men-
tioned businesses are located in 98 different cities
and belong to 425 different categories. Figure 2
shows the numbers of reviewed businesses and
mentioned businesses in the most popular cities
and categories, from where we can see that these
mentions have an acceptable level of diversity.

The mentions that can be linked are our focus,
but we also include the 5,117 unlinkable mentions
in our dataset since they can be helpful for building
a complete entity discovery and linking system (Ji
et al., 2016).

(a) Top Cities

(b) Top Categories

Figure 2: Statistics of the related businesses in Yelp-EL. (a) The number of reviewed businesses in the six most
popular cities. (b) The number of mentioned businesses in the ﬁve most popular categories. Here, “popular” means
having the largest number of businesses in the dataset.

3 Entity Linking Algorithm

In this section, we introduce LinkYelp, an entity
linking approach we design for Yelp-EL to inves-
tigate the new proposed problem. LinkYelp con-
tains two main steps: candidate generation and
candidate ranking. The candidate generation step
ﬁnds a set of businesses that are plausible to be the
target of a mention based on the mention string.
Afterwards, the candidate ranking step ranks all
the candidates and chooses the top ranked one as
the target business.

3.1 Candidate Generation

For the ﬁrst step, candidate generation, we score
each business b with g(m, b) = gc(m, b) ·
gn(sm, sb) for a mention m, where sm is the men-
tion string of m, sb is the name of b. gc(m, b)
equals to a constant value that is larger than 1 (it
is set to 1.3 in practice) when the review that con-
tains m is for a business that is located in the same
city with b; Otherwise, it equals to 0. gn is deﬁned
as

gn(sm, sb) =

(cid:40)
1
sim(sm, sb)

if sm ∈ A(sb)
Otherwise,

(1)
where A(sb) is the set of possible acronyms for
sb, sim(sm, sb) is the cosine similarity between
the TF-IDF representations of sm and sb. In prac-
tice, A(sb) is empty when sb contains less than
two words; Otherwise, it contains one string: the
concatenation of the ﬁrst letter of each word in sb.
Then, we ﬁnd the top 30 highest scored businesses

as candidates. This approach has a recall of 0.955
on Yelp-EL.

3.2 Candidate Ranking

Let m be a mention and b be a candidate business
of m. We use the following function to score how
likely b is the correct business that m refers to:

f (m, b) = w · φ(m, b),

(2)

where φ(m, b) is the feature vector for mention-
candidate pair m and b, Section 4 describes how
to obtain it in detail; w is a parameter vector.

We use a max-margin based loss function to

train w:

J =

1
|T |

(cid:88)

max[0, 1 − f (m, bt)

(3)

<m,bt,bc>∈T

+ f (m, bc)] + λ(cid:107)w(cid:107)2,

where bt is the true business mention m refers to;
bc (cid:54)= bt is a corrupted business sample randomly
picked from the candidates of m; T is the set of
training samples; (cid:107) · (cid:107) is the l2-norm; λ is a hyper-
parameter that controls the regularization strength.
We use stochastic gradient descent to train this
model.

4 Feature Engineering

We study the effectiveness of three types of fea-
tures: conventional features, social features, and
location features. Among them, conventional fea-
tures are those that can also be use in traditional
entity linking tasks; social features and location
features are unique in our problem.

4.1 Conventional Features

Lots of information used in traditional entity link-
ing cannot be found for Yelp businesses, but we
try our best to include all such features that can be
used in our problem.

For Yelp-EL, we use the following conventional
features for a mention m and its candidate busi-
ness b:

u1 : The cosine similarity between the TF-IDF
representations of the mention string of m
and the name of b.

u2 : Whether the mention string of m is a pos-
sible acronym of b’s name (i.e., whether it is
an element of the set A(sb) in Equation 1).

u3 : The popularity of b. Let the number of re-
views received by b be n. Then this feature
value equals to n/C if n is smaller than a pa-
rameter C that’s used for normalization, oth-
erwise it equals to 1.

u4 : The cosine similarity between the TF-IDF
representations of the review that contains m
and combination of all reviews of b. This fea-
ture evaluates how well b ﬁts m semantically.

u5 : Whether b is the same as the reviewed busi-
ness. This feature is actually not available in
traditional EL, and it is usually not available
on other social media platforms either. But
it is obviously useful on Yelp-EL. Including
it here helps us to see how beneﬁcial social
features and location features truly are.

4.2 Social Features

Through the activities of the users on the plat-
form, the users, mentions, reviews and businesses
in Yelp-EL form a network where there are differ-
ent types of nodes and edges. Thus we use Hetero-
geneous Information Networks (HIN) to model it,
and then design meta-path based features to cap-
ture the relations between mentions and their can-
didate businesses. We skip the formal deﬁnitions
of HIN and meta-path here, readers can refer to
(Sun et al., 2011) for detailed introduction. The
HIN schema for Yelp-EL is shown in Figure 3.

The following meta-paths are used:

Figure 3: HIN schema of Yelp-EL. M: mention; R: Re-
view; U: user; B: business.

where we denote M for mention, R for Review, U
for user, and B for business.

Different meta-paths above capture different
kinds of relations between a mention and its can-
didate entities that are induced by users’ social ac-
tivities. For example, if an instance of P 1 exists
between a mention m and a business b, then m is
contained in a review that is written by a user who
also reviewed business b. If many such instances
of P 1 exist, then we may assume that m and b are
related, which makes it more possible for m to be
referring to b.

With the meta-paths above, we use the Path
Count feature deﬁned in (Sun et al., 2011) to feed
into the entity linking model described in Section
3. Given a meta-path P , for mention m and busi-
ness b, Path Count is the number of path instances
of P that start from m and end with b. In practice,
we normalize this value based on global statistics
before feeding it to a model.

4.3 Location Features

commonly

information

in
Location
location-based social media platforms such as
Yelp and Foursquare. Users on platforms such
as Twitter and Instagram may also be willing to
provide their locations.

exists

Here, we use the following two features for a

mention m and its candidate business b:

v1 : Whether the reviewed business is in the

same city as b.

v2 : The geographical distance between the re-
viewed business and b. This value is calcu-
lated based on the longitude and latitude co-
ordinates of the businesses.

P 1 : M − R − U − R − B
P 2 : M − R − U − U − R − B
P 3 : M − R − U − R − B − R − U − R − B

There are still some other location features that
can be designed. For example, we can also con-
sider the locations of the other businesses that are

reviewed by the user. We only use the above two
since we ﬁnd in our experiments that including
them already provides high performance boost.

5 Experiments

5.1 Compared Methods

We compare with a baseline method we name as
DirectLink, as well as two existing entity linking
methods including the method proposed by (Liu
et al., 2013) (which we refer to as ELT) and SS-
Regu proposed by (Huang et al., 2014).

DirectLink simply links each mention to the
corresponding reviewed business. Many business
mentions in Yelp reviews actually refer to the busi-
ness that is being reviewed. This baseline method
tells us how many of these mentions there are in
Yelp-EL.

ELT collectively links a set of mentions with
an objective to maximize local compatibility and
global consistence. It achieves this by integrating
three types of similarities: mention-entity similar-
ity, entity-entity similarity, and mention-mention
similarity. To apply ELT to Yelp-EL, we use the
conventional features introduced in Section 4.1 for
mention-entity similarities. The path count fea-
ture of meta-path B–R–U–R–B is used as entity-
entity similarity. For mention-mention similarity,
we use two features that are both TF-IDF based
cosine similarities, with one between the two men-
tion strings and the other between the reviews that
the two mentions belong to.

SSRegu is also a collective approach.

It is a
graph regularization model that incorporates both
local and global evidence through three principals:
local compatibility, coreference, and semantic re-
latedness. SSRegu computes a weight matrix for
each of these three principals, and then forms a
graph based on the weight matrices and performs
graph regularization to rank candidate entities. To
apply SSRegu, we need to compute three weight
matrices. The weight matrix for local compatibil-
ity is based on features extracted from the men-
tion and the candidate entity.
In our case, the
conventional features are used for computing this
matrix. Computing the coreference weight matrix
requires to determine whether two corresponding
mentions are coreferential. Huang et al. (2014)
assume two mentions to be coreferential if their
mention strings are the same and there exists at
least one meta-path instance of speciﬁc patterns
between them. In our case, the meta-paths used

Method

Accuracy (mean±std)

DirectLink
ELT
SSRegu
LinkYelp

0.6684±0.008
0.8451±0.012
0.7970±0.013
0.9034±0.014

Table 2: Entity linking performance of different meth-
ods on Yelp-EL.

are M–R–M and M–R–U–R–M. To compute the
semantic relatedness weight matrix, we apply the
entity-entity similarity used for ELT. Note that SS-
Regu is a semi-supervised approach and is capable
of using unsupervised data, but for fair comparison
we do not use this feature here.

ELT and SSRegu are originally proposed to
tackle the problem of entity linking for tweets,
but their linking target is Wikipedia. Evaluating
the performance of these two methods on Yelp-EL
shows the difference between their problem and
ours.

5.2 Experimental Settings

Throughout our experiments, the hyperparameter
λ in Equation (3) is set to 0.001. For each men-
tion, three corrupted samples are random selected
for the training with Equation (3). For ELT and
SSRegu, the hyperparameters are tuned on the val-
idation set with grid search. The candidate busi-
nesses for ELT and SSRegu are also obtained with
the method describe in Section 3.1. We run ﬁve
trials of random split of the linked mentions in the
dataset, where each trail uses 60% of the linked
mentions as training data and 40% as test data. In
each of the training set, we further select 20% as
validation set.

Note that we only use linked mentions to eval-
uate different methods since NIL detection is not
our focus, but NIL mentions are utilized in Section
5.6 to build a complete entity linking system.

5.3 Comparison Results

Table 2 shows the entity linking performance of
different methods on Yelp-EL. Here, all three
types of features described in Section 4 are fed
into LinkYelp. Within the compared methods,
LinkYelp performs substantially better.
This
shows that methods carefully designed for tradi-
tional entity linking problems may not work so
well when applied to entity linking within a so-
cial media platform, and this new problem we pro-

Features
C
S
L
C+S
C+L
S+L
C+S+L

All
84.05
79.65
80.05
86.45
89.42
85.19
90.34

Restaurants Nightlife Shopping
79.18
75.09
72.70
84.98
84.98
79.86
85.67

83.15
82.05
81.32
85.71
90.48
87.91
91.94

86.80
82.92
85.71
89.29
93.32
89.29
93.79

Food A & E Bars
83.49
71.67
81.15
80.66
66.95
77.69
82.55
50.21
85.77
87.26
78.54
85.00
88.21
76.39
88.46
85.38
72.10
87.31
88.68
78.97
90.00

E & S H & T
63.70
68.79
67.41
72.25
37.78
49.13
71.85
77.46
66.67
72.83
67.41
72.25
71.11
76.30

Table 3: Entity linking accuracy (%) on different categories of businesses with different types of features as input.
On the “Features” column, “C,” “S,” and “L” means conventional, social, and location features respectively. “All”
means all the categories combined, i.e., the whole test set; “A & E” means Arts & Entertainment; “E & S” means
Event Planning & Services; “H & T” means Hotels & Travel.

pose is worth studying differently from the tradi-
tional entity linking problem. The accuracy of Di-
rectLink means that many mentions (about 67%)
in Yelp-EL simply refer to the corresponding re-
viewed businesses. However, this does not mean
that our problem is less challenging than tradi-
tional entity linking, since simply using the popu-
larity measure of entities can achieve an accuracy
of about 82% in the latter task (Pan et al., 2015).

5.4 Ablation Study

We further investigate how the three different
types of features described in Section 4 contribute
to the ﬁnal performance of LinkYelp, and how
they perform differently in linking mentions that
refer to a speciﬁc category of Yelp businesses. The
results are listed in Table 3. The categories in Ta-
ble 3 are those that include the largest numbers
of businesses in the dataset. Entries in the “All”
column in Table 3 are the accuracies on all the cat-
egories combined. We can see from this column
that both social features and location features are
able to improve the performance when combined
with conventional features. Location features are
relatively more effective than social features, this
is because people’s activities are mainly restricted
to a certain area, so they are more likely to mention
businesses that are within this area. But social fea-
tures are still helpful even when both conventional
features and location features are already used, as
the best performance is achieved with all the three
types of features combined. Moreover, social fea-
tures can become more important for other social
media platforms that do not have location infor-
mation available.

There are also some interesting ﬁndings if we
consider the performance on different categories.
For example, compared with only using conven-

tional features (row C), incorporating social fea-
tures (row C+S) provides the largest improvement
for Event Planing & Services (e.g., wedding plan-
ning, party planning). This matches our intuition
because for these kinds of businesses, people tend
to be inﬂuenced more by their friends and make
choices that are socially related. Table 3 also
shows that on the categories Event Planning &
Services and Hotels & Travel, incorporating loca-
tion features is not that helpful as it does on other
categories. We manually checked the mentions
under these two categories that are linked correctly
by C+S but incorrectly by C+L. We ﬁnd that the
reasons why incorporating location features fails
on these mentions vary from case to case. Two
possible reasons are: location information is not
helpful to disambiguate a hotel and the shops in
this hotel; it also does not work well in disam-
biguating different locations of a hotel chain that
are all not far away from the reviewed business.

5.5 Error Examples

We also manually checked some of the errors
made by LinkYelp with all the three types of fea-
tures as input. A few examples are shown in Table
4.
In the ﬁrst case, since the reviewed business
“Jean Philippe Patisserie” is a restaurant, our sys-
tem tends to ﬁnd a similar business instead of a
hotel. Location features do not help here because
Cafe Bellagio has the same location as Bellagio
Hotel. The system is also incapable of identify-
ing that “stay at” should be probably followed by
a hotel instead of a Cafe. In the second case, the
algorithm outputs the reviewed business because
it is unable to understand what “the other Second
Sole in Rock River” means. The above two exam-
ples show that there are still some errors caused
by the failure of natural language understanding.

Name: Jean Philippe Patisserie
Reviewed Biz:
Review:
... Even if you are not staying at the Bellagio, you have to stop by anyway to ...
Name: Bellagio Hotel
True Referent:
System Prediction: Name: Cafe Bellagio

Addr.: 3600 S Las Vegas Blvd, Las Vegas
Addr.: 3600 S Las Vegas Blvd, Las Vegas

Addr.: 3600 S Las Vegas Blvd, Las Vegas

Reviewed Biz:
Review:
True Referent:
System Prediction: Name: Second Sole Athletic Footwear

Name: Second Sole Athletic Footwear
I did a review of the other Second Sole in Rocky River. This one is in Lyndhurst...
Addr.: 19341 Detroit Rd, Rocky River
Name: Second Sole

Addr.: 5114 Mayﬁeld Rd, Cleveland

Addr.: 5114 Mayﬁeld Rd, Cleveland

Reviewed Biz:
Review:
True Referent:
System Prediction: Name: Frys

Name: Hoot Owl
This place is a really fun neighborhood bar ... Its tucked away in the Frys parking lot...
Name: Fry’s Food and Drug

Addr.: 4315 W Bell Road, Phoenix

Addr.: 4361 W Bell Rd, Phoenix

Addr.: 2626 S 83rd Ave, Phoenix

Table 4: Examples of errors made by LinkYelp. Business mentions are underlined.

Reviewed Biz
Name: Burger King
Addr.: 1194 King St W, Toronto
Name: The Turf Public House
Addr.: 705 N 1st St, Phoenix

Sentence
When you compare it
across the street, the service is way better.
I have to say I like the atmosphere and sur-
roundings of the Turf better than Seamus.

to the McDonald’s

Mentioned Biz
Name: McDonald’s
Addr.: 1221 King Street W, Toronto
Name: Seamus McCaffrey’s
Addr.: 18 W Monroe St, Phoenix

Table 5: Examples of comparative sentences and linked mentions.

Name

City

Bacchanal Buffet Las Vegas
Las Vegas
Phoenix
Phoenix
Las Vegas
Las Vegas

Wicked Spoon
Cibo
Pizzeria Bianco
XS Nightclub
Marquee

Stars
4.0
3.5
4.5
4.0
4.0
3.5

#Better
33
24
9
2
8
1

Table 6: Comparative study using texts and average
ratings. Each row is a pair of two frequently compared
businesses. #Better means the number of sentences that
claim the corresponding business to be better than the
other one.

In the third case, “Fry’s Food and Drug” is lo-
cated at “4315 W Bell Road, Phoenix” which is
nearer to the reviewed business “Hoot Owl” lo-
cated at “4361 W Bell Rd, Phoenix.” However,
although location information favors the correct
business, the others features may contribute more
for the system output “Frys” since “Frys” has an
exact match of the candidate mention name.

5.6 Comparative Study

In this study, we provide some insight on the pos-
sible applications of our task by checking the com-
parative sentences in Yelp reviews.

First, we ﬁnd comparative sentences from the
whole Yelp review dataset with a simple pat-
tern matching method: we retrieve the sentences
that contain one of eight predeﬁned comparison
phrases such as “is better,” “not as good as,” etc.

Then we extract the named entity mentions within
these sentences and link them to Yelp businesses.
A threshold based approach is used to detect NIL
mentions (Dalton and Dietz, 2013).

As a result, we get 12,149 comparative sen-
tences from the total 4,153,150 reviews that con-
tains at least one linked mention. Some of the re-
sults are shown in Table 5. We can successfully
identify both the entity names and their locations
on Yelp. We also selected the top three frequently
compared pairs and compare with the stars pro-
vided by Yelp dataset. From Table 6 we can see
that the text comparison is consistent with star rat-
ings.

6 Related Work

The traditional entity linking task of mapping
mentions in articles to their corresponding enti-
ties in a knowledge base has been studied exten-
sively (Shen et al., 2015; Ling et al., 2015). Vari-
ous kinds of methods have been studied, e.g., neu-
ral network models (Sun et al., 2015; He et al.,
2013), generative models (Li et al., 2013), etc.
A large group of the existing entity linking ap-
proaches are called collective approaches, which
are based on the observation that the entities men-
tioned in a same context are usually related with
each other. Thus they usually form entity linking
as an optimization problem that tries to maximizes
both local mention-entity compatibility and global
entity-entity coherence (Han et al., 2011; Nguyen
et al., 2016). LinkYelp does not consider global

entity-entity coherence as it is not the focus of this
paper, but it can be applied to our problem too.

The prevalence of on-line social networks has
also motivated researchers to study entity linking
in such environments. (Huang et al., 2014; Shen
et al., 2013; Liu et al., 2013) proposed methods
that are specially designed for linking named en-
tities in tweets. They mainly address the problem
that tweets are usually short and informal, while
taking advantage of some of the extra information
that tweets may provide. For example, (Shen et al.,
2013) assumed that each user’s tweets have an un-
derlying interest distribution and proposed a graph
based interest propagation algorithm to rank the
entities. (Huang et al., 2014) also used meta-path
on HIN in their entity linking approach, but they
only used it to get an indication of whether two
mentions are related. Finally, although these stud-
ies focused on entity linking for tweets, they still
use entities in knowledge bases as the target.

There are a few entity linking studies that do not
link mentions to knowledge bases.
(Shen et al.,
2017) proposed to link entity mentions to an HIN
such as DBLP and IMDB. However, their articles
are collected from the Internet through searching
and thus are not related to the target entities. They
also used an HIN based method, but their use is
restricted to get the relatedness between different
entities. (Lin et al., 2017) studied the entity linking
problem where the entities are included in differ-
ent lists and entities of the same type belong to the
same list. They only used this information along
with the name of each entity to perform entity link-
ing. Thus their focus is very different from ours.

7 Conclusions

In this paper, we propose a new entity linking
problem where both entity mentions and target en-
tities are in a same social media platform. To
study this problem, we ﬁrst create a dataset called
Yelp-EL, and then conduct extensive experiments
and analysis on it with a learning to rank model
that takes three different types of features as input.
Through the experimental results, we ﬁnd that tra-
ditional entity linking approaches may not work
so well on our problem. The two types of features
that are usually not available for traditional entity
linking tasks – social features and location features
– can both improve the performance signiﬁcantly
on Yelp-EL. Our work can also motivate and en-
able a lot of downstream applications such as com-

parative analysis of location based businesses. In
the future, we plan to extract more patterns to ob-
tain more comparative sentences, so that we may
more accurately demonstrate how useful perform-
ing comparative analysis after linking the business
mentions can be.

Acknowledgments

This paper was supported by the Early Career
Scheme (ECS, No.
26206717) from Research
Grants Council in Hong Kong. The experiments
and conclusions contained herein are those of the
authors and should not be interpreted as necessar-
ily representing the afﬁliated company.

References

Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. Proceedings
of EMNLP-CoNLL, page 708.

Jeffrey Dalton and Laura Dietz. 2013. A neighborhood
relevance model for entity linking. In Proceedings
of the 10th Conference on Open Research Areas in
Information Retrieval, pages 149–156.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005.
Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of ACL, pages 363–370.

Matthew Francis-Landau, Greg Durrett, and Dan
Klein. 2016. Capturing semantic similarity for en-
tity linking with convolutional neural networks. In
Proceedings of NAACL, pages 1256–1261.

Octavian-Eugen Ganea, Marina Ganea, Aurelien Luc-
chi, Carsten Eickhoff, and Thomas Hofmann. 2016.
Probabilistic bag-of-hyperlinks model for entity
linking. In Proceedings of WWW, pages 927–938.

Amir Globerson, Nevena Lazic, Soumen Chakrabarti,
Amarnag Subramanya, Michael Ringaard, and Fer-
nando Pereira. 2016. Collective entity resolution
with multi-focal attention. In Proceedings of ACL,
volume 1, pages 621–631.

Zhaochen Guo and Denilson Barbosa. 2014. Robust
entity linking via random walks. In Proceedings of
CIKM, pages 499–508.

Nitish Gupta, Sameer Singh, and Dan Roth. 2017. En-
tity linking via joint encoding of types, descrip-
tions, and context. In Proceedings of EMNLP, pages
2681–2690.

Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective
entity linking in web text: a graph-based method. In
Proceedings of SIGIR, pages 765–774.

Zhengyan He, Shujie Liu, Mu Li, Ming Zhou, Longkai
Zhang, and Houfeng Wang. 2013. Learning entity
In Pro-
representation for entity disambiguation.
ceedings of ACL, pages 30–34.

Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S Yu, and
Tianyi Wu. 2011. Pathsim: Meta path-based top-k
similarity search in heterogeneous information net-
works. VLDB, 4(11):992–1003.

Hongzhao Huang, Yunbo Cao, Xiaojiang Huang, Heng
Ji, and Chin-Yew Lin. 2014. Collective tweet wiki-
ﬁcation based on semi-supervised graph regulariza-
tion. In Proceedings of ACL, pages 380–390.

Chuanqi Tan, Furu Wei, Pengjie Ren, Weifeng Lv, and
Ming Zhou. 2017. Entity linking for queries by
In Proceedings of
searching wikipedia sentences.
EMNLP, pages 68–77.

Heng Ji, Joel Nothman, Hoa Trang Dang, and Syd-
ney Informatics Hub. 2016. Overview of tac-
kbp2016 tri-lingual edl and its impact on end-to-end
cold-start kbp. Proceedings of of TAC.

Yang Li, Chi Wang, Fangqiu Han, Jiawei Han, Dan
Roth, and Xifeng Yan. 2013. Mining evidences
for named entity disambiguation. In Proceedings of
KDD, pages 1070–1078.

Ying Lin, Chin-Yew Lin, and Heng Ji. 2017. List-only
In Proceedings of ACL, volume 2,

entity linking.
pages 536–541.

Xiao Ling, Sameer Singh, and Daniel S Weld. 2015.
Design challenges for entity linking. TACL, 3:315–
328.

Xiaohua Liu, Yitong Li, Haocheng Wu, Ming Zhou,
Furu Wei, and Yi Lu. 2013. Entity linking for
tweets. In Proceedings of ACL, pages 1304–1311.

Thien Huu Nguyen, Nicolas Fauceglia, Mariano Ro-
driguez Muro, Oktie Hassanzadeh, Alﬁo Massimil-
iano Gliozzo, and Mohammad Sadoghi. 2016. Joint
learning of local and global features for entity link-
In Proceedings of COL-
ing via neural networks.
ING, pages 2310–2320.

Xiaoman Pan, Taylor Cassidy, Ulf Hermjakob, Heng Ji,
and Kevin Knight. 2015. Unsupervised entity link-
In Pro-
ing with abstract meaning representation.
ceedings of NAACL-HLT, pages 1130–1139.

Wei Shen, Jiawei Han, Jianyong Wang, Xiaojie Yuan,
and Zhenglu Yang. 2017. Shine+: A general frame-
work for domain-speciﬁc entity linking with het-
erogeneous information networks. IEEE Trans. on
Knowl. and Data Eng.

Wei Shen, Jianyong Wang, and Jiawei Han. 2015. En-
Issues, tech-
IEEE Trans. Knowl. Data

tity linking with a knowledge base:
niques, and solutions.
Eng., 27(2):443–460.

Wei Shen, Jianyong Wang, Ping Luo, and Min Wang.
2013. Linking named entities in tweets with knowl-
In Proceed-
edge base via user interest modeling.
ings of KDD, pages 68–76.

Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhen-
zhou Ji, and Xiaolong Wang. 2015. Modeling men-
tion, context and entity with neural networks for en-
tity disambiguation. In Proceedings of IJCAI, pages
1333–1339.

