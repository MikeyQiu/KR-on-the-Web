Guided Alignment Training for Topic-Aware Neural Machine Translation

Wenhu Chen2, Evgeny Matusov1, Shahram Khadivi1, and Jan-Thorsten Peter2

1eBay, Inc.
Kasernenstr. 25
52064 Aachen, Germany
ematusov@ebay.com,
skhadivi@ebay.com

2RWTH Aachen University
Ahornstr. 55
52056 Aachen, Germany
hustchenwenhu@gmail.com,
peter@cs.rwth-aachen.de

Abstract

In this paper, we propose an effective
way for biasing the attention mechanism
of a sequence-to-sequence neural ma-
chine translation (NMT) model towards
the well-studied statistical word align-
ment models. We show that our novel
guided alignment training approach im-
proves translation quality on real-life e-
commerce texts consisting of product titles
and descriptions, overcoming the prob-
lems posed by many unknown words and
a large type/token ratio. We also show
that meta-data associated with input texts
such as topic or category information can
signiﬁcantly improve translation quality
when used as an additional signal to the
decoder part of the network. With both
the BLEU score of the
novel features,
NMT system on a product title set im-
proves from 18.6 to 21.3%. Even larger
MT quality gains are obtained through
domain adaptation of a general domain
NMT system to e-commerce data. The de-
veloped NMT system also performs well
on the IWSLT speech translation task,
where an ensemble of four variant systems
outperforms the phrase-based baseline by
2.1% BLEU absolute.

1

Introduction

NMT systems were shown to reach state-of-the-
art translation quality on tasks established in MT
research community such as IWSLT speech trans-
lation task (Cettolo et al., 2012).
In this paper,
we also apply NMT approach to e-commerce data:
user-generated product titles and descriptions for
items put on sale. Such data are very different
from newswire and other texts typically consid-

ered in the MT research community. Titles in par-
ticular are short (usually fewer than 15 words),
contain many brand names which often do not
have to be translated, but also product feature val-
ues and speciﬁc abbreviations and jargon. Also,
the vocabulary size is very large due to the large
variety of product types, and many words are ob-
served in the training data only once. At the same
time, these data are provided with additional meta-
information about the item (e.g. product category
such as clothing or electronics), which can be used
as context to perform topic/domain adaptation for
improved translation quality.

At ﬁrst glance, established phrase-based sta-
tistical MT approaches are well-suited for e-
commerce data translation. In a phrase-based ap-
proach, singleton, but unambiguous words and
phrases are usually translated correctly. Also,
since the alignment between source and target
words is available, it is possible to transfer cer-
tain entities from the source sentence to the gen-
erated target sentence “in-context” without trans-
lating them. Such entities can include numbers,
product speciﬁcations such as “5S” or “.35XX”, or
brand names such as “Samsung” or “Lenovo”. In
training, these entities can be replaced with place-
holders to reduce the vocabulary size.

However, NMT approaches are more power-
ful at capturing context beyond phrase boundaries
and were shown to better exploit available training
data. They also successfully adapt themselves to a
domain, for which only a limited amount of paral-
lel training data is available (Luong and Manning,
2015). Also, previous research (Mathur et al.,
2015) has shown that it is difﬁcult to obtain trans-
lation quality improvements with topic adaptation
in phrase-based SMT because of data sparseness
and a large number of topics (e. g. correspond-
ing to product categories), which may or may not
be relevant for disambiguating between alternative

6
1
0
2
 
l
u
J
 
6
 
 
]
L
C
.
s
c
[
 
 
1
v
8
2
6
1
0
.
7
0
6
1
:
v
i
X
r
a

translations or solving other known MT problems.
In contrast, we expected NMT to better solve the
topic adaptation problem by using the additional
meta-information as an extra signal in the neural
network. To the best of our knowledge, this is the
ﬁrst work where the additional information about
the text topic is embedded into the vector space
and used to directly inﬂuence NMT decisions.

In an NMT system, the attention mechanism in-
troduced in (Luong et al., 2014) is important both
for decoding as well as for restoration of place-
holder content and insertion of unknown words
in the right positions in the target sentence. To
improve the estimation of the soft alignment, we
propose to use the Viterbi alignments of the IBM
model 4 (Brown et al., 1993) as an additional
source of knowledge during NMT training. The
additional alignment information helps the current
system to bias the attention mechanism towards
the Viterbi alignment.

This paper is structured as follows. After an
overview of related NMT work in Section 2, we
propose a novel approach in Section 3 on how
to improve the NMT translation quality by com-
bining two worlds:
the phrase-based SMT and
its statistical word alignment and the neural MT
In Section 4, we describe
attention mechanism.
in more detail how topic information can beneﬁt
NMT. Section 5 and Section 6 describes our do-
main adaptation approach. Experimental results
are presented in Section 7. The paper is concluded
with a discussion and outlook in Section 8.

2 Related Work

Neural machine translation is mainly based on us-
ing recurrent neural networks to grasp long term
dependencies in natural language. An NMT sys-
tem is trained on end-to-end basis to maximize
the conditional probability of a correct translation
given a source sentence (Sutskever et al., 2014),
(Bahdanau et al., 2014), (Cho et al., 2014b).
When using attention mechanism, large vocabu-
laries (Jean et al., 2014), and some other tech-
niques, NMT is reported to achieve compara-
ble translation quality to state-of-art phrase-based
translation systems. Most NMT approaches are
based on the encoder-decoder architecture (Cho
et al., 2014a), in which the input sentence is ﬁrst
encoded into a ﬁxed-length representation, from
which the recurrent neural network decoder gen-
erates the sequence of target words. Since ﬁxed-

length representation cannot give enough infor-
mation for decoding, a more sophisticated ap-
proach using attention mechanism is proposed by
(Bahdanau et al., 2014).
In this approach, the
neural network learns to attend to different parts
of source sentence to improve translation quality.
Since the source and target language vocabularies
for a neural network have to be limited, the rare
words problem deteriorates translation quality sig-
niﬁcantly. The rare word replacement technique
using soft alignment proposed by (Luong et al.,
2014) gives a promising solution for the problem.
Both encoder-decoder architecture and insertion
of unknown words into NMT output highly rely on
the quality of the attention mechanism, thus it be-
comes the crucial part of NMT. Some research has
been done to reﬁne it by (Luong et al., 2015), who
proposed global and local attention-based models,
and (Cohn et al., 2016), who used biases, fertility
and symmetric bilingual structure to improve the
attention model mechanism.

Research on topic adaptation most closely re-
lated to our work was performed by (Hasler et al.,
2014), but the features proposed there were added
to the log-linear model of a phrase-based system.
Here, we use the topic information as part of the
input to the NMT system. Another difference is
that we primarily work with human-labeled topics,
whereas in (Hasler et al., 2014) the topic distribu-
tion is inferred automatically from data.

When translating e-commerce content, we are
faced with a situation when only a few product ti-
tles and descriptions were manually translated, re-
sulting in a small in-domain parallel corpus, but
a large general-domain parallel corpus is avail-
able.
In such situations, domain adaption tech-
niques have been used both in phrase-based sys-
tems (Koehn and Schroeder, 2007) and NMT (Lu-
ong and Manning, 2015). In addition, while di-
verse NMT models using different features and
techniques are trained, an ensemble decoder can
be used to combine them together to make a more
robust model. This approach was used by (Lu-
ong et al., 2015) to outperform the state-of-art
phrase-based system with their NMT approach in
the WMT 2015 evaluation.

3 Guided Alignment Training

When using the attention-based NMT (Bahdanau
et al., 2014), we observed that the attention mech-
anism sometimes fails to yield appropriate soft

alignments, especially with increasing length of
the input sentence and many out-of-vocabulary
words or placeholders. In translation, this can lead
to disordered output and word repetition.

In contrast to a statistical phrase-based system,
the NMT decoder does not have explicit informa-
tion about the candidates of the current word, so
at each recurrent step, the attention weights only
rely on the previously generated word and de-
coder/encoder state, as depicted in Figure 1. The
target word itself is not used to compute its atten-
tion weights.
If the previous word is an out-of-
vocabulary (OOV) or a placeholder, then the in-
formation it provides for calculating the attention
weights for the current word is neither sufﬁcient
nor reliable anymore. This leads to incorrect tar-
get word prediction, and the error propagates to
the future steps due to feedback loop. The prob-
lem is even larger in the case of e-commerce data
where the number of OOVs and placeholders is
considerably higher.

To improve the estimation of the soft alignment,
we propose to use the Viterbi alignments of the
IBM model 4 as an additional source of knowledge
during the NMT training. Therefore, we ﬁrstly ex-
tract Viterbi alignments as trained by the GIZA++
toolkit (Och and Ney, 2003), then use them to bias
the attention mechanism. Our approach is to opti-
mize on both the decoder cost and the divergence
between the attention weights and the alignment
connections generated by statistical alignments.
The multi-objective optimization task is then ex-
pressed as a single-objective one by means of lin-
ear combination of two loss functions: the original
and the new alignment-guided loss.

3.1 Decoder Cost

NMT proposed by (Bahdanau et al., 2014) max-
imizes the conditional
log-likelihood of target
sentence y1, . . . , yT given the source sentence
x1, . . . , x(cid:48)

T :

HD(y, x) = −

log pθ(yn|xn)

(1)

1
N

N
(cid:88)

n=1

where (yn, xn) refers to nth training sentence pair,
and N denotes the total number of sentence pairs
in the training corpus. In the paper, we name the
negative log-likelihood as decoder cost to distin-
guish from alignment cost. When using encoder-
decoder architecture by (Cho et al., 2014b), the

conditional probability can be written as:

p(y1 . . . yT |x1 . . . xT (cid:48)) =

p(yt|yt−1 · · · y1, c)

T
(cid:89)

t=1

(2)

with

p(yt|yt−1 · · · y1, c) = g(st, yt−1, c)

where T is the length of the target sentence and T (cid:48)
is the length of source sentence, c is a ﬁxed-length
vector to encode source sentence, st is a hidden
state of RNN at time step t, and g(·) is a non-
linear function to approximate word probability. If
attention mechanism is used, the ﬁxed-length c is
replaced by variable-length representation ct that
is a weighted summary over a sequence of anno-
tations (h1, · · · , hT (cid:48)), and hi contains information
about the whole input sentence, but with a strong
focus on the parts surrounding the ith word (Bah-
danau et al., 2014). Then, the context vector can
be deﬁned as:

ct =

αtihi

(3)

T (cid:48)
(cid:88)

i

where αti for each annotation hi is computed by
normalizing the score function with softmax, as
described in Equation 4.

αti =

exp(eti)
k=1 exp(etk)

(cid:80)T (cid:48)

(4)

Here eti = a(st−1, hi) is the function to calcu-
late the score of t-th target word aligning to i-th
word in the source sentence. The alignment model
a(·, ·) is used to calculate similarity between pre-
vious state st−1 and bi-directional state hi. In our
experiments, we took the idea of dot global at-
tention model (Luong et al., 2015), but we still
keep the order ht−1 → at → ct → ht as pro-
posed by (Bahdanau et al., 2014). We calculate
the dot product of encoder state hi with the last
decoder state st−1 instead of the current decoder
state. We observe that this dot attention model
(Equation 5) works better than concatenation in
our experiments.

a(st−1, hi) = (Wsst−1)T (Whhi)

(5)

3.2 Alignment Cost

We introduce alignment cost to penalize attention
mechanism when it is not consistent with statisti-
cal word alignment. We represent the pre-trained
statistical alignments by a matrix A, where Ati
refers to the probability of the tth word in the tar-
get sentence of being aligned to the ith word in
the source sentence.
In case of multiple source
words aligning to the same target word, we nor-
malize to make sure (cid:80)
In attention-
based NMT, the matrix of attention weights α has
the same shape and semantics as A. We propose to
penalize NMT based on the divergence of the two
matrices during the training, the divergence func-
tion can e. g. be cross entropy Gce or mean square
error Gmse as in Equation 6. As shown in Fig-
ure 1, A comes from statistical alignment, feeding
into our guided-alignment NMT as an additional
input to penalize the attention mechanism.

i Ati = 1.

Gce(A, α) = −

Ati log αti

(6)

Gmse(A, α) =

(Ati − αti)2

T
(cid:88)

T (cid:48)
(cid:88)

1
T

t=1

i=1

T
(cid:88)

T (cid:48)
(cid:88)

t=1

i=1

1
T

We combine decoder cost and alignment cost to
build the new loss function H(y, x, A, α):

H(y, x, A, α) = w1G(A, α) + w2HD(y, x) (7)

During training, we optimize the new compound
loss function H(y, x, A, α) with regard to the
same parameters as before. The guided-alignment
training inﬂuences the attention mechanism to
generate alignment closer to Viterbi alignment and
has the advantage of unchanged parameter space
and model complexity. When training is done, we
assume that NMT can generate robust alignment
by itself, so there is no need to feed an alignment
matrix as input during evaluation. As indicated
in Equation 7, we set w1 and w2 for weights of
decoder cost and alignment cost to balance their
weight ratio. We performed further experiments
(see section 7) to analyze the impact of different
weight settings on translation quality.

4 Topic-aware Machine Translation

In the e-commerce domain, the information on the
product category (e.g., “men’s clothing”, “mobile
phones“, “kitchen appliances”) often accompanies
the product title and description and can be used

as an additional source of information both in the
training of a MT system and during translation.
In particular, such meta-information can help to
disambiguate between alternative translations of
the same word that have different meaning. The
choice of the right translation often depends on
the category. For example, the word “skin” has to
be translated differently in the categories “mobile
phone accessories” and “make-up”. Outside of
the e-commerce world, similar topic information
is available in the form of e.g. tags and keywords
for a given document (on-line article, blog post,
patent, etc.) and can also be used for word sense
disambiguation and topic adaptation. In general,
the same document can belong to multiple topics.
Here, we propose to feed such meta-information
into the recurrent neural network to help generate
words which are appropriate given a particular cat-
egory or topic.

4.1 Topic Representation

The idea is to represent topic information in a
D-dimensional vector l, where D is the num-
ber of topics. Since one sentence can belong to
multiple topics (possibly with different probabili-
ties/weights), we normalize the topic vector so that
the sum of its elements is 1. It is fed into the de-
coder to inﬂuence the proposed target word distri-
bution. The conditional probability given the topic
membership vector can be written as (cf. Equa-
tions 2 and 3):

p(yt|y<t−1, ct, st−1, l) = p(yt|yt−1, ct, st−1, l)

= g(yt−1, st−1, ct, l)

where g(·) is used to approximate the probabil-
ity distribution. In our implementation, we intro-
duce an intermediate readout layer to build func-
tion g(·), which is a feed-forward network as de-
picted in Figure 2.

4.2 Topic-aware Decoder

In the NMT decoder, we feed the topic member-
ship vector to the readout layer in each recurrent
step to enhance word selection. As shown in Fig-
ure 1, topic membership vector l is fed into NMT
decoder as an additional input besides source and
target sentences:

p(yt|y<t−1, ct, st−1, l) = p(yt|rt)

rt = Wr[ct; ft−1; st−1; l] + br

(8)

(9)

Figure 1: Topic-aware, alignment-guided encoder-decoder model.

where Wr is concatenation of original transforma-
tion matrix W (cid:48)
r and topic transformation matrix
Wc. Then adding topic into readout layer input is
equivalent to adding an additional topic vector Ec
into the original readout layer output. Assuming
l is a one-hot category vector, then Wcl is equiva-
lent to retrieving a speciﬁc column from the matrix
Wc. Hence, we can name this additional vector
Ec as topic embedding, regarded as a vector rep-
resentation of topic information. It is quite similar
to word embedding by (Mikolov et al., 2013), we
will further analyze the similarity between differ-
ent topics in Figure 3.

The readout layer depicted in Figure 2 merges
information from the last state st−1, previous word
embedding ft−1 (coming from word index yt−1,
which is sampled w.r.t. the proposed word distri-
bution), as well as the current context ct to gener-
ate output. It can be seen as a shallow network,
which consists of a max-out layer (Goodfellow et
al., 2013), a fully-connected layer, and a softmax
layer.

5 Bootstrapping

the
When trained on small amounts of data,
attention-based neural network approach does not
always produce reliable soft alignment. The prob-
lem gets worse when the sentence pairs available
for training are getting longer. To solve this prob-
lem, we extracted bilingual sub-sentence units
from existing sentence pairs to be used as addi-

Figure 2: Topic-aware readout layer.

where Wr is concatenation of original transforma-
tion matrix and l, rt is the output from readout
layer and ft is the embedding of the last target
word yt−1; st−1 refers the last decoder state. Wr
and br are weights and bias for the linear trans-
formation, respectively. We can rearrange the for-
mula as:

rt = [W (cid:48)
= W (cid:48)
= W (cid:48)
= r(cid:48)

r, Wc][ct; ft−1; st−1; l] + br
r[ct; ft−1; st−1] + Wcl + br
r[ct; ft−1; st−1] + Ec + br
t + Ec

(10)

tional training data. These units are exclusively
aligned to each other, i. e. all words within the
source sub-sentence are aligned only to the words
within the corresponding target sub-sentence and
vice versa. The alignment is determined with
the standard approach (IBM Model 4 alignment
trained with the GIZA++ toolkit (Och and Ney,
2003)). As boundaries for sub-sentence units, we
used punctuation marks, including period, comma,
semicolon, colon, dash, etc. To simplify bilingual
sentence splitting, we used the standard phrase
pair extraction algorithm for phrase-based SMT,
the minimum/maximum source phrase
but set
length to 8 and 30 tokens, respectively. From all
such long phrase pairs extracted by the algorithm,
we only kept those which are started or ended with
a punctuation mark or started/ended a sentence;
both on the source and on the target side.

For the bootstrapped training, we merged the
training data with the extracted sub-
original
sentence units and ran the neural training algo-
rithm on this extended training set. Since the
extracted bilingual sub-sentence units generally
showed good correspondence between source and
target due to the constraints described above, the
expectation was that having such units repeated in
the training data as stand-alone training instances
would guide the attention mechanism to become
more robust and make it easier for the neural train-
ing algorithm to ﬁnd better correspondences be-
tween more difﬁcult source/target sentence parts.
Also, having both short and long training instances
was expected to make neural translation quality
less dependent on the input length.

6 E-commerce Domain Adaptation

For the e-commerce English-to-French translation
task, we only had a limited amount of in-domain
parallel training data (item titles and descriptions).
To beneﬁt from large amounts of general-domain
training data, we followed the method described
in (Luong and Manning, 2015). We ﬁrst trained
a baseline NMT model on English-French WMT
data (common-crawl, Europarl v7, and news com-
mentary corpora) for two epochs to get the best
result on a development set, and then continued
training the same model on the in-domain training
set for a few more epochs. In contrast to (Luong
and Manning, 2015), however, we used the vo-
cabularies of the most frequent 52K source/target
words in the in-domain data (instead of the out-

of-domain data vocabularies). This allowed us to
focus the NN on translation of the most relevant
in-domain words.

7 Experimental Results

7.1 Datasets and Preprocessing

We performed MT experiments on the German-to-
English IWLST 2015 speech translation task (Cet-
tolo et al., 2012) and on an in-house English-to-
French e-commerce translation task. As part of
data preprocessing, we tokenized and lowercased
the corpora, as well as replaced numbers, prod-
uct speciﬁcations, and other special symbols with
placeholders such as $num. We only keep these
placeholders in training, but preserve their content
as XML markups in the dev/test sets, which we try
to restore using attention mechanism. This con-
tent is inserted for the generated placeholders on
the target side based on the attention mechanism
(see (Luong et al., 2014)). In the beam search for
the best translation, we make sure that each place-
holder content is used only once. Using the same
mechanism, we also pass OOV words to the target
side “as is” (without using any special unknown
word symbol).

On both tasks, we evaluate all systems and sys-
tem variants using case-insensitive BLEU (Pap-
ineni et al., 2002) and TER (Snover et al., 2006)
scores on held-out development and test data us-
ing a single human reference translation.

7.1.1

IWSLT TED Talk Data

For the IWSLT German-to-English task (transla-
tion of transcribed TED talks), we mapped the
topic keywords of each TED talk in the 2015 train-
ing/dev/test evaluation campaign release to ten
general topics such as politics, environment, ed-
ucation, and others. All sentences in the same talk
share the same topic, and one talk can belong to
several topics. Instead of using the ofﬁcial IWSLT
dev/test data, we set aside 81/159 talks for devel-
opment/test set, respectively. Out of these talks,
we used 567 dev and 1100 test sentences which
had the highest probability of relating to a particu-
lar topic (bag-of-words classiﬁcation using the re-
maining 1365 talks as the training data). The full
corpus statistics for the IWSLT data sets obtained
this way are given in Table 11.

1This IWSLT data set with topic labels is publicly
available at https://github.com/wenhuchen/iwslt-2015-de-en-
topics.

Data-set
Language

Sentences
Running words performed on
Full vocabulary
Sentences
Running words
Sentences
Running words

IWSLT

165 201

567

1100

German

English

English

French

3 873 816
103 390

3 656 038
45 068

2 592 202
119 607

2 895 089
129 848

9 812

10 695

10 339

11 283

19 019

22 895

10 817

11 016

e-commerce

516 000

910

910

Training

Dev

Test

Source OOV rate % w.r.t. full/NMT vocabulary

5.16/6.12

2.56/5.76

Table 1: Corpus statistics for the IWSLT and e-commerce translation tasks. OOV rate is calculated after
preprocessing, placeholders like $num, $url, etc. largely decrease the OOV rate in the e-commerce dev
and test sets.

7.1.2 E-commerce Data

E-commerce En→Fr

BLEU TER

For the e-commerce English-to-French task, we
used the product category such as “fashion” or
“electronics” as topic information (a total of 80
most widely used categories plus the category
“other” that combined all the less frequent cate-
gories). The training set contained both product
titles and product descriptions, while dev and test
set only contained product titles. Each title or de-
scription sentence was assigned to only one cate-
gory. The statistics of the e-commerce data sets
are given in Table 1.

7.2 Model Training

We implemented our neural translation model in
Python using the Blocks deep learning library (van
Merri¨enboer et al., 2015) based on the open-source
MILA translation project. We compared our im-
plementation of NMT baseline system with (Bah-
danau et al., 2014) on the WMT 2014 English-
to-French machine translation task and obtained
a similar BLEU score on the ofﬁcial test set as
they reported in (Bahdanau et al., 2014). Then
we implemented the topic-aware algorithm (sec-
tion 4), guided alignment training (section 3), and
the bootstrapped training (section 5) into the NMT
model. We trained separate models with various
feature combinations. We also created an ensem-
ble of different models to obtain the best NMT
translation results.

In our experiments, we set the word embedding
size to 620 and used a two-layer bi-directional
GRU encoder and one layer of GRU decoder, the
cell dimension of both were 1000. We selected the
50k most frequent German words and top 30k En-
glish words as vocabularies for the IWSLT task,

Baseline NMT
+preﬁxed human-labeled categs
+readout human-labeled categs
+readout LDA topics

%
18.6
18.3
19.7
14.5

%
68.5
69.3
65.3
74.9

Table 2: Comparison of different approaches for
topic-aware NMT.

and most frequent 52k English/French words for
the e-commerce task. The optimization of the ob-
jective function was performed by using AdaDelta
algorithm (Zeiler, 2012). We set the beam size to
10 for dev/test set beam search translation.

For training implementation, we use stochas-
tic gradient descent with batch size of 100, sav-
ing model parameters after a certain number of
epochs. We saved around 30 consecutive model
parameters. We selected the best parameter set ac-
cording to the sum of the established MT evalu-
ation measures BLEU (Papineni et al., 2002) and
1-TER (Snover et al., 2006) on the development
set. After model selection, we evaluated the best
model on the test set. We report the test set BLEU
and TER scores in Table 5 and Table 7.

We use TITAN X GPUs with 12GB of RAM
to run experiments on Ubuntu Linux 14.04. The
training converges in less than 24 hours on the
IWSLT talk task and around 30 hours on the e-
commerce task. The beam search on the test set for
both tasks takes around 10 minutes, the exact time
depends on the vocabulary size and beam size.

source
NMT
+ topics
reference
source

NMT

+ topics

reference

ich m¨ochte Ihnen heute Morgen gerne von meinem Projekt, Kunst Aufr¨aumen, erz¨ahlen.
I want to clean you this morning, from my project, to say Art.
I would like to talk to you today by my project, Art clean.
I would like to talk to you this morning about my project, Tidying Up Art.
. . . unsere Kollegen an Tufts verbinden Modelle wie diese mit durch Tissue Engineering erzeugten Knochen,
um zu sehen, wie Krebs sich von einem Teil des K¨orpers zum n¨achsten verbreiten k¨onnte.
. . . our NOAA colleagues combined models of models like this with tissue generated bones from bones to see
how cancer could spread from one part of the body, to the next distribution.
. . . our colleagues at Tufts are using models like this with tissue-based engineered bones to see
how cancer could spread from a part of the body to the next part.
. . . our colleagues at Tufts are mixing models like these with tissue-engineered bone to see
how cancer might spread from one part of the body to the next.

Table 3: Example of improved translation quality when topic information is used as input in the neural
MT system (German-to-English IWSLT test set).

E-commerce En→Fr
Baseline NMT
+cross-entropy (decay)
+cross-entropy (1:2)
+cross-entropy (1:1)
+cross-entropy (2:1)
+squared error (1:1)

BLEU % TER %
18.6
20.5
20.6
20.2
20.9
20.8

68.3
65.8
65.5
65.0
65.7
64.5

Table 4: Comparison of different loss functions
and weight ratios for guided alignment.

niﬁcantly (19.7% BLEU) outperforming the base-
line.

Replacing the human-labeled topic one-hot vec-
tors of size 80 with the LDA-predicted topic distri-
bution vectors of the same dimension in the read-
out layer of the neural network deteriorated the
BLEU and TER scores signiﬁcantly. We attribute
this to data sparseness problems when training the
LDA of dimension 80 on product titles.

On the German-to-English task, we also ob-
served MT quality improvements when using
human-labeled topic information as described in
Figure 1. Here, we extracted the topic embed-
ding Ec from different experiments and show
their cosine distance in Figure 3. It’s straightfor-
ward that in different experiments, the same topic
tends to share similar representation in continu-
ous embedding space. At the same time, closer
topic pairs like “politics” and “issues” tend to
have shorter distance from each other. Examples
of improved German-to-English NMT translations
when human-labeled topic information is used are
shown in Table 3.

7.4

Implementation of Guided Alignment

To balance decoder cost and the attention weight
cost, we experimented with different weights for

Figure 3: Topic embedding cosine distance.

7.3 Effect of Topic-aware NMT

We tested different approaches to ﬁnd out where
topic information ﬁts best into NMT, since topic
information can affect alignment, word selection,
etc. The most naive approach is to insert a pseudo
topic word in the beginning of a sentence to bias
the context of the sentence to a certain topic. We
also tried topic vectors of different origin in the
read-out layer of the network. We used both topics
predicted automatically with the Latent Dirichlet
Analysis (LDA) and human-labeled topics to feed
into the network as shown in Figure 1.

The results on the e-commerce task in Ta-
ble 2 show that category information as a pseudo
topic word does not carry enough semantic and
syntactic meaning in comparison to real source
words to have a positive effect on the target words
predicted in the decoder. The BLEU score of
such system (18.3%) is even below the baseline
the human-labeled cate-
(18.6%).
gories are more reliable and are able to positively
inﬂuence word selection in the NMT decoder, sig-

In contrast,

Vintage Ollech & Wajs Early Bird Diver watch, Excellent!
Vintage Ollech & Wajs d´ebut oiseau montre de plong´ee, excellent!
Montre de plong´ee vintage Ollech & Wajs early bird, excellent!

source
SMT
NMT
reference Montre de Plong´ee Vintage Ollech & Wajs Early Bird, Excellent !
APT Holman Model 1 Audiophile Power Amplifer made in Cambridge Mass
source
APT Holman modle 1 audiophile power fabricant d’ampli made in Cambridge Mass
SMT
L’ampliﬁcateur de puissance audiophile APT Holman mod`ele 1 fabriqu´ee ´a Cambridge Mass
NMT
reference Ampliﬁcateur de puissance pour audiophile APT Holman mod`ele 1 fabriqu´e ´a Cambridge Massachussets

Table 6: Example of improved translation quality of the NMT ensemble system vs. phrase-based baseline
system (English-to-French title test set).

Exp System Description
Phrase-based system
1
Phrase-based system + OSM
2
NMT
3
NMT + topic vectors
4
NMT + bootstrapping
5
NMT + guided alignment
6
NMT + topic vectors + bootstrapping
7
NMT + topic vectors + bootstrapping + guided alignment
8

BLEU % TER %

24.7
25.7
23.4
23.7
24.1
23.8
24.2
24.6

55.4
55.1
60.1
59.6
58.6
60.8
59.4
57.7

9

Ensemble

27.8

55.4

NMT + topic vectors
NMT + topic vectors + guided alignment
NMT + topic vectors + bootstrapping
NMT + topic v. + guided alignment + bootstrapping

Table 7: Overview of the translation results on the English-to-German IWSLT task.

these costs. We analyzed the relation between
weight ratio and the ﬁnal result in Table 4. Be-
sides ﬁxing the cost ratios during training, we also
apply a heuristic to adjust the ratio as the training
is progressing. One approach is to set a high value
for the alignment cost in the beginning, then decay
the weight to 90% after every epoch, ﬁnally elim-
inating the inﬂuence of the alignment after some
time. This approach helps for the IWSLT task,
but not on the e-commerce task. We assume that
the alignment for the TED talk sentences seems
to be easier for NMT to learn on its own than the
alignment between product titles and their transla-
tions. We also analyzed the effect of using differ-
ent loss functions for calculating alignment diver-
gence (see Section 3.2). The difference between
the squared error and cross-entropy is not so large
as shown in Table 4. Since the cross-entropy func-
tion has a consistent form as decoder cost, we
decided to use it in further experiments. We ex-
tracted the NMT attention weights and marked the
connection with the highest score as hard align-
ment for each word. We drew the alignment in
Figure 4 to compare baseline NMT and alignment-
guided NMT. It can be seen from the graph that
the guided alignment training truly improves the

alignment correspondence.

7.5 Overall Results

The overall results on the e-commerce translation
task and IWSLT task are shown in Table 5 and Ta-
ble 7, respectively. We observed consistency on
both tasks, in a sense that a feature that improves
BLEU/TER results on one task is also beneﬁcial
for the other.

For comparison, we trained phrase-based SMT
models using the Moses toolkit (Koehn et al.,
2007) on both translation tasks. We used the
standard Moses features, including a 4-gram LM
trained on the target side of the bilingual data,
word-level and phrase-level translation probabil-
ities, as well as the distortion model with the max-
imum distortion of 6. Our stronger phrase-based
baseline included additional 5 features of a 4-gram
operation sequence model – OSM (Durrani et al.,
2015).

On the e-commerce task, which is more chal-
lenging due to a high number of OOV words and
placeholders, we observed that NMT translation
output had many errors related to incorrect atten-
tion weights. To improve the attention mecha-
nism, we applied guided alignment and bootstrap-

System Description

1. NMT in-domain (ID)
2. 1) + topic vectors
3. 1) + bootstrapping
4. 1) + guided alignment
5. NMT with 2) and 4)
6. NMT with 2) and 3) and 4)
7. NMT out-of-domain (OOD)
8. 7) + guided alignment
9. 8) + domain adaptation

Ensemble

NMT
ID
Ensemble
NMT
OOD

system 4)
system 5)
system 6)
NMT w. 3) and 4)
system 9)
9) with DW
9) w. topic vectors

BLEU
%
18.6
19.7
20.1
20.9
21.3
20.7
13.8
16.3
25.0

TER
%
68.5
65.3
66.2
65.7
64.3
66.2
77.4
74.5
60.1

24.5

60.9

25.6

58.6

Table 5: Overview of the translation results on the
English-to-French e-commerce translation task.
(DW: decaying weight for the statistical align-
ment).

reverse was true for 460 titles. In particular, the
word order of noun phrases was observed to be
better in the NMT translations.

On the IWSLT task (Table 7), the baseline NMT
was not as far behind the phrase-based system as
on the e-commerce task, and thus the obtained
improvements were smaller than for product ti-
tle translations. We observed that topic informa-
tion is less helpful than bootstrapping and guided
alignment learning. When we combined them, we
reached the same BLEU score as the phrase-based
system (see Table 7). Finally, we combined four
variant systems to create an ensemble, which re-
sulted in the BLEU score of 27.8%, surpassing the
phrase-based translation with the OSM model by
2.1% BLEU absolute.

8 Conclusion

We have presented a novel guided alignment train-
ing for a NMT model that utilizes IBM model 4
Viterbi alignments to guide the attention mecha-
nism. This approach was shown experimentally to
bring consistent improvements of translation qual-
ity on e-commerce and spoken language trans-
lation tasks. Also on both tasks, the proposed
novel way of utilizing topic meta-information was

Figure 4: Reﬁned alignment examples using
guided-alignment learning (green blocks refer to
the identical alignments from Baseline NMT and
guided-alignment NMT, blue blocks refer to the
alignment from baseline NMT, yellow blocks re-
fer to guided-alignment NMT).

ping. Both boosted the translation performance.
Adding topic information increased the BLEU
score to 21.3%. We selected the four best model
parameters from various experiments to make an
ensemble system, this improved the BLEU score
to 24.5%. For the following experiment, we had
pre-trained a model on WMT15 parallel data with
the guided alignment technique, and then contin-
ued training on the e-commerce data for several
epocs as described in section 6, performing do-
main adaptation. This approach proved to be ex-
tremely helpful, giving an increase of over 3.0%
absolute in BLEU. Finally, we also applied en-
semble methods on variants of the domain-adapted
models to further increase the BLEU score to 25.6,
which is 7.0 BLEU higher than the NMT baseline
system, and only 0.6% BLEU behind the BLEU
score of 26.2% for the state-of-the-art phrase-
based baseline. Table 6 shows examples where the
ensemble NMT system is better than the phrase-
based system despite the slightly lower corpus-
level BLEU score. In fact, a more detailed anal-
ysis of the sentence-level BLEU scores showed
that the NMT translation of 386 titles out of 910
was ranked higher than the SMT translation, the

shown to improve BLEU and TER scores. We also
showed improvements when using domain adap-
tation by continuing training of an out-of-domain
NMT system on in-domain parallel data.
In the
future, we would like to investigate how to ef-
fectively make use of the abundant monolingual
data with human-labeled product category infor-
mation that we have available for the envisioned
e-commerce application.

References

[Bahdanau et al.2014] Dzmitry Bahdanau, Kyunghyun
Cho, and Yoshua Bengio. 2014. Neural machine
translation by jointly learning to align and translate.
arXiv preprint arXiv:1409.0473.

[Brown et al.1993] Peter F. Brown, Stephen A. Della
Pietra, Vincent J. Della Pietra, and Robert. L. Mer-
cer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
Linguistics, 19:263–311.

[Cettolo et al.2012] Mauro Cettolo, Christian Girardi,
and Marcello Federico. 2012. Wit3: Web inventory
of transcribed and translated talks. In Proceedings
of the 16th Conference of the European Association
for Machine Translation (EAMT), pages 261–268,
Trento, Italy, May.

[Cho et al.2014a] Kyunghyun

Bart
van
Merri¨enboer, Dzmitry Bahdanau,
and Yoshua
Bengio.
2014a. On the properties of neural
machine translation: Encoder-decoder approaches.
arXiv preprint arXiv:1409.1259.

Cho,

[Cho et al.2014b] Kyunghyun

Cho,

Bart
Van Merri¨enboer, Caglar Gulcehre, Dzmitry
Bahdanau, Fethi Bougares, Holger Schwenk,
Learning phrase
and Yoshua Bengio.
representations using RNN encoder-decoder
for
arXiv preprint
statistical machine translation.
arXiv:1406.1078.

2014b.

[Cohn et al.2016] Trevor Cohn, Cong Duy Vu Hoang,
and Ekaterina Vymolova.
Incorpo-
rating structural alignment biases into an atten-
arXiv preprint
tion neural
arXiv:1601.01085.

translation model.

2016.

[Durrani et al.2015] Nadir Durrani, Helmut Schmid,
Alexander Fraser, Philipp Koehn, and Hinrich
Sch¨utze. 2015. The operation sequence model-
combining n-gram-based and phrase-based statisti-
cal machine translation. Computational Linguistics.

[Goodfellow et al.2013] Ian

J Goodfellow, David
Warde-Farley, Mehdi Mirza, Aaron Courville, and
Yoshua Bengio. 2013. Maxout networks. arXiv
preprint arXiv:1302.4389.

[Hasler et al.2014] Eva Hasler, Phil Blunsom, Philipp
Koehn, and Barry Haddow. 2014. Dynamic topic
adaptation for phrase-based MT. In Proceedings of
EACL, pages 328–337.

[Jean et al.2014] S´ebastien Jean, Kyunghyun Cho,
Roland Memisevic, and Yoshua Bengio. 2014. On
using very large target vocabulary for neural ma-
chine translation. CoRR, abs/1412.2007.

[Koehn and Schroeder2007] Philipp Koehn and Josh
Schroeder. 2007. Experiments in domain adaptation
for statistical machine translation. In Proceedings of
the second workshop on statistical machine transla-
tion, pages 224–227. Association for Computational
Linguistics.

[Koehn et al.2007] Philipp Koehn, Hieu Hoang,
Alexandra Birch, Chris Callison-Burch, Mar-
cello Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard Zens, et al.
2007. Moses: Open source toolkit for statisti-
In Proceedings of the
cal machine translation.
45th annual meeting of
the ACL on interactive
poster and demonstration sessions, pages 177–180.
Association for Computational Linguistics.

[Luong and Manning2015] Minh-Thang Luong and
Christopher D. Manning. 2015. Stanford neural
machine translation systems for spoken language
domain.

[Luong et al.2014] Minh-Thang Luong, Ilya Sutskever,
Quoc V Le, Oriol Vinyals, and Wojciech Zaremba.
Addressing the rare word problem
2014.
arXiv preprint
in neural machine translation.
arXiv:1410.8206.

[Luong et al.2015] Minh-Thang Luong, Hieu Pham,
and Christopher D Manning. 2015. Effective ap-
proaches to attention-based neural machine transla-
tion. arXiv preprint arXiv:1508.04025.

[Mathur et al.2015] Prashant Mathur, Marcello Fed-
erico, Selc¸uk K¨opr¨u, Sharam Khadivi, and Hassan
Sawaf. 2015. Topic adaptation for machine trans-
lation of e-commerce content. Proceedings of MT
Summit XV, page 270.

[Mikolov et al.2013] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013. Efﬁcient estima-
tion of word representations in vector space. arXiv
preprint arXiv:1301.3781.

[Och and Ney2003] Franz Josef Och and Hermann Ney.
2003. A systematic comparison of various statisti-
cal alignment models. Computational Linguistics,
29(1):19–51.

[Papineni et al.2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu.
2002. Bleu: a
method for automatic evaluation of machine trans-
In Proceedings of the 40th annual meeting
lation.
on association for computational linguistics, pages
311–318. Association for Computational Linguis-
tics.

[Snover et al.2006] Matthew Snover, Bonnie Dorr,
Richard Schwartz, Linnea Micciulla, and John
Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of
association for machine translation in the Americas,
pages 223–231.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals,
and Quoc V Le. 2014. Sequence to sequence learn-
ing with neural networks. In Advances in neural in-
formation processing systems, pages 3104–3112.

[van Merri¨enboer et al.2015] Bart

van Merri¨enboer,
Dzmitry Bahdanau, Vincent Dumoulin, Dmitriy
Jan Chorowski,
Serdyuk, David Warde-Farley,
Blocks and fuel:
and Yoshua Bengio.
arXiv preprint
Frameworks for deep learning.
arXiv:1506.00619.

2015.

[Zeiler2012] Matthew D Zeiler.

2012. Adadelta:
an adaptive learning rate method. arXiv preprint
arXiv:1212.5701.

Guided Alignment Training for Topic-Aware Neural Machine Translation

Wenhu Chen2, Evgeny Matusov1, Shahram Khadivi1, and Jan-Thorsten Peter2

1eBay, Inc.
Kasernenstr. 25
52064 Aachen, Germany
ematusov@ebay.com,
skhadivi@ebay.com

2RWTH Aachen University
Ahornstr. 55
52056 Aachen, Germany
hustchenwenhu@gmail.com,
peter@cs.rwth-aachen.de

Abstract

In this paper, we propose an effective
way for biasing the attention mechanism
of a sequence-to-sequence neural ma-
chine translation (NMT) model towards
the well-studied statistical word align-
ment models. We show that our novel
guided alignment training approach im-
proves translation quality on real-life e-
commerce texts consisting of product titles
and descriptions, overcoming the prob-
lems posed by many unknown words and
a large type/token ratio. We also show
that meta-data associated with input texts
such as topic or category information can
signiﬁcantly improve translation quality
when used as an additional signal to the
decoder part of the network. With both
the BLEU score of the
novel features,
NMT system on a product title set im-
proves from 18.6 to 21.3%. Even larger
MT quality gains are obtained through
domain adaptation of a general domain
NMT system to e-commerce data. The de-
veloped NMT system also performs well
on the IWSLT speech translation task,
where an ensemble of four variant systems
outperforms the phrase-based baseline by
2.1% BLEU absolute.

1

Introduction

NMT systems were shown to reach state-of-the-
art translation quality on tasks established in MT
research community such as IWSLT speech trans-
lation task (Cettolo et al., 2012).
In this paper,
we also apply NMT approach to e-commerce data:
user-generated product titles and descriptions for
items put on sale. Such data are very different
from newswire and other texts typically consid-

ered in the MT research community. Titles in par-
ticular are short (usually fewer than 15 words),
contain many brand names which often do not
have to be translated, but also product feature val-
ues and speciﬁc abbreviations and jargon. Also,
the vocabulary size is very large due to the large
variety of product types, and many words are ob-
served in the training data only once. At the same
time, these data are provided with additional meta-
information about the item (e.g. product category
such as clothing or electronics), which can be used
as context to perform topic/domain adaptation for
improved translation quality.

At ﬁrst glance, established phrase-based sta-
tistical MT approaches are well-suited for e-
commerce data translation. In a phrase-based ap-
proach, singleton, but unambiguous words and
phrases are usually translated correctly. Also,
since the alignment between source and target
words is available, it is possible to transfer cer-
tain entities from the source sentence to the gen-
erated target sentence “in-context” without trans-
lating them. Such entities can include numbers,
product speciﬁcations such as “5S” or “.35XX”, or
brand names such as “Samsung” or “Lenovo”. In
training, these entities can be replaced with place-
holders to reduce the vocabulary size.

However, NMT approaches are more power-
ful at capturing context beyond phrase boundaries
and were shown to better exploit available training
data. They also successfully adapt themselves to a
domain, for which only a limited amount of paral-
lel training data is available (Luong and Manning,
2015). Also, previous research (Mathur et al.,
2015) has shown that it is difﬁcult to obtain trans-
lation quality improvements with topic adaptation
in phrase-based SMT because of data sparseness
and a large number of topics (e. g. correspond-
ing to product categories), which may or may not
be relevant for disambiguating between alternative

6
1
0
2
 
l
u
J
 
6
 
 
]
L
C
.
s
c
[
 
 
1
v
8
2
6
1
0
.
7
0
6
1
:
v
i
X
r
a

translations or solving other known MT problems.
In contrast, we expected NMT to better solve the
topic adaptation problem by using the additional
meta-information as an extra signal in the neural
network. To the best of our knowledge, this is the
ﬁrst work where the additional information about
the text topic is embedded into the vector space
and used to directly inﬂuence NMT decisions.

In an NMT system, the attention mechanism in-
troduced in (Luong et al., 2014) is important both
for decoding as well as for restoration of place-
holder content and insertion of unknown words
in the right positions in the target sentence. To
improve the estimation of the soft alignment, we
propose to use the Viterbi alignments of the IBM
model 4 (Brown et al., 1993) as an additional
source of knowledge during NMT training. The
additional alignment information helps the current
system to bias the attention mechanism towards
the Viterbi alignment.

This paper is structured as follows. After an
overview of related NMT work in Section 2, we
propose a novel approach in Section 3 on how
to improve the NMT translation quality by com-
bining two worlds:
the phrase-based SMT and
its statistical word alignment and the neural MT
In Section 4, we describe
attention mechanism.
in more detail how topic information can beneﬁt
NMT. Section 5 and Section 6 describes our do-
main adaptation approach. Experimental results
are presented in Section 7. The paper is concluded
with a discussion and outlook in Section 8.

2 Related Work

Neural machine translation is mainly based on us-
ing recurrent neural networks to grasp long term
dependencies in natural language. An NMT sys-
tem is trained on end-to-end basis to maximize
the conditional probability of a correct translation
given a source sentence (Sutskever et al., 2014),
(Bahdanau et al., 2014), (Cho et al., 2014b).
When using attention mechanism, large vocabu-
laries (Jean et al., 2014), and some other tech-
niques, NMT is reported to achieve compara-
ble translation quality to state-of-art phrase-based
translation systems. Most NMT approaches are
based on the encoder-decoder architecture (Cho
et al., 2014a), in which the input sentence is ﬁrst
encoded into a ﬁxed-length representation, from
which the recurrent neural network decoder gen-
erates the sequence of target words. Since ﬁxed-

length representation cannot give enough infor-
mation for decoding, a more sophisticated ap-
proach using attention mechanism is proposed by
(Bahdanau et al., 2014).
In this approach, the
neural network learns to attend to different parts
of source sentence to improve translation quality.
Since the source and target language vocabularies
for a neural network have to be limited, the rare
words problem deteriorates translation quality sig-
niﬁcantly. The rare word replacement technique
using soft alignment proposed by (Luong et al.,
2014) gives a promising solution for the problem.
Both encoder-decoder architecture and insertion
of unknown words into NMT output highly rely on
the quality of the attention mechanism, thus it be-
comes the crucial part of NMT. Some research has
been done to reﬁne it by (Luong et al., 2015), who
proposed global and local attention-based models,
and (Cohn et al., 2016), who used biases, fertility
and symmetric bilingual structure to improve the
attention model mechanism.

Research on topic adaptation most closely re-
lated to our work was performed by (Hasler et al.,
2014), but the features proposed there were added
to the log-linear model of a phrase-based system.
Here, we use the topic information as part of the
input to the NMT system. Another difference is
that we primarily work with human-labeled topics,
whereas in (Hasler et al., 2014) the topic distribu-
tion is inferred automatically from data.

When translating e-commerce content, we are
faced with a situation when only a few product ti-
tles and descriptions were manually translated, re-
sulting in a small in-domain parallel corpus, but
a large general-domain parallel corpus is avail-
able.
In such situations, domain adaption tech-
niques have been used both in phrase-based sys-
tems (Koehn and Schroeder, 2007) and NMT (Lu-
ong and Manning, 2015). In addition, while di-
verse NMT models using different features and
techniques are trained, an ensemble decoder can
be used to combine them together to make a more
robust model. This approach was used by (Lu-
ong et al., 2015) to outperform the state-of-art
phrase-based system with their NMT approach in
the WMT 2015 evaluation.

3 Guided Alignment Training

When using the attention-based NMT (Bahdanau
et al., 2014), we observed that the attention mech-
anism sometimes fails to yield appropriate soft

alignments, especially with increasing length of
the input sentence and many out-of-vocabulary
words or placeholders. In translation, this can lead
to disordered output and word repetition.

In contrast to a statistical phrase-based system,
the NMT decoder does not have explicit informa-
tion about the candidates of the current word, so
at each recurrent step, the attention weights only
rely on the previously generated word and de-
coder/encoder state, as depicted in Figure 1. The
target word itself is not used to compute its atten-
tion weights.
If the previous word is an out-of-
vocabulary (OOV) or a placeholder, then the in-
formation it provides for calculating the attention
weights for the current word is neither sufﬁcient
nor reliable anymore. This leads to incorrect tar-
get word prediction, and the error propagates to
the future steps due to feedback loop. The prob-
lem is even larger in the case of e-commerce data
where the number of OOVs and placeholders is
considerably higher.

To improve the estimation of the soft alignment,
we propose to use the Viterbi alignments of the
IBM model 4 as an additional source of knowledge
during the NMT training. Therefore, we ﬁrstly ex-
tract Viterbi alignments as trained by the GIZA++
toolkit (Och and Ney, 2003), then use them to bias
the attention mechanism. Our approach is to opti-
mize on both the decoder cost and the divergence
between the attention weights and the alignment
connections generated by statistical alignments.
The multi-objective optimization task is then ex-
pressed as a single-objective one by means of lin-
ear combination of two loss functions: the original
and the new alignment-guided loss.

3.1 Decoder Cost

NMT proposed by (Bahdanau et al., 2014) max-
imizes the conditional
log-likelihood of target
sentence y1, . . . , yT given the source sentence
x1, . . . , x(cid:48)

T :

HD(y, x) = −

log pθ(yn|xn)

(1)

1
N

N
(cid:88)

n=1

where (yn, xn) refers to nth training sentence pair,
and N denotes the total number of sentence pairs
in the training corpus. In the paper, we name the
negative log-likelihood as decoder cost to distin-
guish from alignment cost. When using encoder-
decoder architecture by (Cho et al., 2014b), the

conditional probability can be written as:

p(y1 . . . yT |x1 . . . xT (cid:48)) =

p(yt|yt−1 · · · y1, c)

T
(cid:89)

t=1

(2)

with

p(yt|yt−1 · · · y1, c) = g(st, yt−1, c)

where T is the length of the target sentence and T (cid:48)
is the length of source sentence, c is a ﬁxed-length
vector to encode source sentence, st is a hidden
state of RNN at time step t, and g(·) is a non-
linear function to approximate word probability. If
attention mechanism is used, the ﬁxed-length c is
replaced by variable-length representation ct that
is a weighted summary over a sequence of anno-
tations (h1, · · · , hT (cid:48)), and hi contains information
about the whole input sentence, but with a strong
focus on the parts surrounding the ith word (Bah-
danau et al., 2014). Then, the context vector can
be deﬁned as:

ct =

αtihi

(3)

T (cid:48)
(cid:88)

i

where αti for each annotation hi is computed by
normalizing the score function with softmax, as
described in Equation 4.

αti =

exp(eti)
k=1 exp(etk)

(cid:80)T (cid:48)

(4)

Here eti = a(st−1, hi) is the function to calcu-
late the score of t-th target word aligning to i-th
word in the source sentence. The alignment model
a(·, ·) is used to calculate similarity between pre-
vious state st−1 and bi-directional state hi. In our
experiments, we took the idea of dot global at-
tention model (Luong et al., 2015), but we still
keep the order ht−1 → at → ct → ht as pro-
posed by (Bahdanau et al., 2014). We calculate
the dot product of encoder state hi with the last
decoder state st−1 instead of the current decoder
state. We observe that this dot attention model
(Equation 5) works better than concatenation in
our experiments.

a(st−1, hi) = (Wsst−1)T (Whhi)

(5)

3.2 Alignment Cost

We introduce alignment cost to penalize attention
mechanism when it is not consistent with statisti-
cal word alignment. We represent the pre-trained
statistical alignments by a matrix A, where Ati
refers to the probability of the tth word in the tar-
get sentence of being aligned to the ith word in
the source sentence.
In case of multiple source
words aligning to the same target word, we nor-
malize to make sure (cid:80)
In attention-
based NMT, the matrix of attention weights α has
the same shape and semantics as A. We propose to
penalize NMT based on the divergence of the two
matrices during the training, the divergence func-
tion can e. g. be cross entropy Gce or mean square
error Gmse as in Equation 6. As shown in Fig-
ure 1, A comes from statistical alignment, feeding
into our guided-alignment NMT as an additional
input to penalize the attention mechanism.

i Ati = 1.

Gce(A, α) = −

Ati log αti

(6)

Gmse(A, α) =

(Ati − αti)2

T
(cid:88)

T (cid:48)
(cid:88)

1
T

t=1

i=1

T
(cid:88)

T (cid:48)
(cid:88)

t=1

i=1

1
T

We combine decoder cost and alignment cost to
build the new loss function H(y, x, A, α):

H(y, x, A, α) = w1G(A, α) + w2HD(y, x) (7)

During training, we optimize the new compound
loss function H(y, x, A, α) with regard to the
same parameters as before. The guided-alignment
training inﬂuences the attention mechanism to
generate alignment closer to Viterbi alignment and
has the advantage of unchanged parameter space
and model complexity. When training is done, we
assume that NMT can generate robust alignment
by itself, so there is no need to feed an alignment
matrix as input during evaluation. As indicated
in Equation 7, we set w1 and w2 for weights of
decoder cost and alignment cost to balance their
weight ratio. We performed further experiments
(see section 7) to analyze the impact of different
weight settings on translation quality.

4 Topic-aware Machine Translation

In the e-commerce domain, the information on the
product category (e.g., “men’s clothing”, “mobile
phones“, “kitchen appliances”) often accompanies
the product title and description and can be used

as an additional source of information both in the
training of a MT system and during translation.
In particular, such meta-information can help to
disambiguate between alternative translations of
the same word that have different meaning. The
choice of the right translation often depends on
the category. For example, the word “skin” has to
be translated differently in the categories “mobile
phone accessories” and “make-up”. Outside of
the e-commerce world, similar topic information
is available in the form of e.g. tags and keywords
for a given document (on-line article, blog post,
patent, etc.) and can also be used for word sense
disambiguation and topic adaptation. In general,
the same document can belong to multiple topics.
Here, we propose to feed such meta-information
into the recurrent neural network to help generate
words which are appropriate given a particular cat-
egory or topic.

4.1 Topic Representation

The idea is to represent topic information in a
D-dimensional vector l, where D is the num-
ber of topics. Since one sentence can belong to
multiple topics (possibly with different probabili-
ties/weights), we normalize the topic vector so that
the sum of its elements is 1. It is fed into the de-
coder to inﬂuence the proposed target word distri-
bution. The conditional probability given the topic
membership vector can be written as (cf. Equa-
tions 2 and 3):

p(yt|y<t−1, ct, st−1, l) = p(yt|yt−1, ct, st−1, l)

= g(yt−1, st−1, ct, l)

where g(·) is used to approximate the probabil-
ity distribution. In our implementation, we intro-
duce an intermediate readout layer to build func-
tion g(·), which is a feed-forward network as de-
picted in Figure 2.

4.2 Topic-aware Decoder

In the NMT decoder, we feed the topic member-
ship vector to the readout layer in each recurrent
step to enhance word selection. As shown in Fig-
ure 1, topic membership vector l is fed into NMT
decoder as an additional input besides source and
target sentences:

p(yt|y<t−1, ct, st−1, l) = p(yt|rt)

rt = Wr[ct; ft−1; st−1; l] + br

(8)

(9)

Figure 1: Topic-aware, alignment-guided encoder-decoder model.

where Wr is concatenation of original transforma-
tion matrix W (cid:48)
r and topic transformation matrix
Wc. Then adding topic into readout layer input is
equivalent to adding an additional topic vector Ec
into the original readout layer output. Assuming
l is a one-hot category vector, then Wcl is equiva-
lent to retrieving a speciﬁc column from the matrix
Wc. Hence, we can name this additional vector
Ec as topic embedding, regarded as a vector rep-
resentation of topic information. It is quite similar
to word embedding by (Mikolov et al., 2013), we
will further analyze the similarity between differ-
ent topics in Figure 3.

The readout layer depicted in Figure 2 merges
information from the last state st−1, previous word
embedding ft−1 (coming from word index yt−1,
which is sampled w.r.t. the proposed word distri-
bution), as well as the current context ct to gener-
ate output. It can be seen as a shallow network,
which consists of a max-out layer (Goodfellow et
al., 2013), a fully-connected layer, and a softmax
layer.

5 Bootstrapping

the
When trained on small amounts of data,
attention-based neural network approach does not
always produce reliable soft alignment. The prob-
lem gets worse when the sentence pairs available
for training are getting longer. To solve this prob-
lem, we extracted bilingual sub-sentence units
from existing sentence pairs to be used as addi-

Figure 2: Topic-aware readout layer.

where Wr is concatenation of original transforma-
tion matrix and l, rt is the output from readout
layer and ft is the embedding of the last target
word yt−1; st−1 refers the last decoder state. Wr
and br are weights and bias for the linear trans-
formation, respectively. We can rearrange the for-
mula as:

rt = [W (cid:48)
= W (cid:48)
= W (cid:48)
= r(cid:48)

r, Wc][ct; ft−1; st−1; l] + br
r[ct; ft−1; st−1] + Wcl + br
r[ct; ft−1; st−1] + Ec + br
t + Ec

(10)

tional training data. These units are exclusively
aligned to each other, i. e. all words within the
source sub-sentence are aligned only to the words
within the corresponding target sub-sentence and
vice versa. The alignment is determined with
the standard approach (IBM Model 4 alignment
trained with the GIZA++ toolkit (Och and Ney,
2003)). As boundaries for sub-sentence units, we
used punctuation marks, including period, comma,
semicolon, colon, dash, etc. To simplify bilingual
sentence splitting, we used the standard phrase
pair extraction algorithm for phrase-based SMT,
the minimum/maximum source phrase
but set
length to 8 and 30 tokens, respectively. From all
such long phrase pairs extracted by the algorithm,
we only kept those which are started or ended with
a punctuation mark or started/ended a sentence;
both on the source and on the target side.

For the bootstrapped training, we merged the
training data with the extracted sub-
original
sentence units and ran the neural training algo-
rithm on this extended training set. Since the
extracted bilingual sub-sentence units generally
showed good correspondence between source and
target due to the constraints described above, the
expectation was that having such units repeated in
the training data as stand-alone training instances
would guide the attention mechanism to become
more robust and make it easier for the neural train-
ing algorithm to ﬁnd better correspondences be-
tween more difﬁcult source/target sentence parts.
Also, having both short and long training instances
was expected to make neural translation quality
less dependent on the input length.

6 E-commerce Domain Adaptation

For the e-commerce English-to-French translation
task, we only had a limited amount of in-domain
parallel training data (item titles and descriptions).
To beneﬁt from large amounts of general-domain
training data, we followed the method described
in (Luong and Manning, 2015). We ﬁrst trained
a baseline NMT model on English-French WMT
data (common-crawl, Europarl v7, and news com-
mentary corpora) for two epochs to get the best
result on a development set, and then continued
training the same model on the in-domain training
set for a few more epochs. In contrast to (Luong
and Manning, 2015), however, we used the vo-
cabularies of the most frequent 52K source/target
words in the in-domain data (instead of the out-

of-domain data vocabularies). This allowed us to
focus the NN on translation of the most relevant
in-domain words.

7 Experimental Results

7.1 Datasets and Preprocessing

We performed MT experiments on the German-to-
English IWLST 2015 speech translation task (Cet-
tolo et al., 2012) and on an in-house English-to-
French e-commerce translation task. As part of
data preprocessing, we tokenized and lowercased
the corpora, as well as replaced numbers, prod-
uct speciﬁcations, and other special symbols with
placeholders such as $num. We only keep these
placeholders in training, but preserve their content
as XML markups in the dev/test sets, which we try
to restore using attention mechanism. This con-
tent is inserted for the generated placeholders on
the target side based on the attention mechanism
(see (Luong et al., 2014)). In the beam search for
the best translation, we make sure that each place-
holder content is used only once. Using the same
mechanism, we also pass OOV words to the target
side “as is” (without using any special unknown
word symbol).

On both tasks, we evaluate all systems and sys-
tem variants using case-insensitive BLEU (Pap-
ineni et al., 2002) and TER (Snover et al., 2006)
scores on held-out development and test data us-
ing a single human reference translation.

7.1.1

IWSLT TED Talk Data

For the IWSLT German-to-English task (transla-
tion of transcribed TED talks), we mapped the
topic keywords of each TED talk in the 2015 train-
ing/dev/test evaluation campaign release to ten
general topics such as politics, environment, ed-
ucation, and others. All sentences in the same talk
share the same topic, and one talk can belong to
several topics. Instead of using the ofﬁcial IWSLT
dev/test data, we set aside 81/159 talks for devel-
opment/test set, respectively. Out of these talks,
we used 567 dev and 1100 test sentences which
had the highest probability of relating to a particu-
lar topic (bag-of-words classiﬁcation using the re-
maining 1365 talks as the training data). The full
corpus statistics for the IWSLT data sets obtained
this way are given in Table 11.

1This IWSLT data set with topic labels is publicly
available at https://github.com/wenhuchen/iwslt-2015-de-en-
topics.

Data-set
Language

Sentences
Running words performed on
Full vocabulary
Sentences
Running words
Sentences
Running words

IWSLT

165 201

567

1100

German

English

English

French

3 873 816
103 390

3 656 038
45 068

2 592 202
119 607

2 895 089
129 848

9 812

10 695

10 339

11 283

19 019

22 895

10 817

11 016

e-commerce

516 000

910

910

Training

Dev

Test

Source OOV rate % w.r.t. full/NMT vocabulary

5.16/6.12

2.56/5.76

Table 1: Corpus statistics for the IWSLT and e-commerce translation tasks. OOV rate is calculated after
preprocessing, placeholders like $num, $url, etc. largely decrease the OOV rate in the e-commerce dev
and test sets.

7.1.2 E-commerce Data

E-commerce En→Fr

BLEU TER

For the e-commerce English-to-French task, we
used the product category such as “fashion” or
“electronics” as topic information (a total of 80
most widely used categories plus the category
“other” that combined all the less frequent cate-
gories). The training set contained both product
titles and product descriptions, while dev and test
set only contained product titles. Each title or de-
scription sentence was assigned to only one cate-
gory. The statistics of the e-commerce data sets
are given in Table 1.

7.2 Model Training

We implemented our neural translation model in
Python using the Blocks deep learning library (van
Merri¨enboer et al., 2015) based on the open-source
MILA translation project. We compared our im-
plementation of NMT baseline system with (Bah-
danau et al., 2014) on the WMT 2014 English-
to-French machine translation task and obtained
a similar BLEU score on the ofﬁcial test set as
they reported in (Bahdanau et al., 2014). Then
we implemented the topic-aware algorithm (sec-
tion 4), guided alignment training (section 3), and
the bootstrapped training (section 5) into the NMT
model. We trained separate models with various
feature combinations. We also created an ensem-
ble of different models to obtain the best NMT
translation results.

In our experiments, we set the word embedding
size to 620 and used a two-layer bi-directional
GRU encoder and one layer of GRU decoder, the
cell dimension of both were 1000. We selected the
50k most frequent German words and top 30k En-
glish words as vocabularies for the IWSLT task,

Baseline NMT
+preﬁxed human-labeled categs
+readout human-labeled categs
+readout LDA topics

%
18.6
18.3
19.7
14.5

%
68.5
69.3
65.3
74.9

Table 2: Comparison of different approaches for
topic-aware NMT.

and most frequent 52k English/French words for
the e-commerce task. The optimization of the ob-
jective function was performed by using AdaDelta
algorithm (Zeiler, 2012). We set the beam size to
10 for dev/test set beam search translation.

For training implementation, we use stochas-
tic gradient descent with batch size of 100, sav-
ing model parameters after a certain number of
epochs. We saved around 30 consecutive model
parameters. We selected the best parameter set ac-
cording to the sum of the established MT evalu-
ation measures BLEU (Papineni et al., 2002) and
1-TER (Snover et al., 2006) on the development
set. After model selection, we evaluated the best
model on the test set. We report the test set BLEU
and TER scores in Table 5 and Table 7.

We use TITAN X GPUs with 12GB of RAM
to run experiments on Ubuntu Linux 14.04. The
training converges in less than 24 hours on the
IWSLT talk task and around 30 hours on the e-
commerce task. The beam search on the test set for
both tasks takes around 10 minutes, the exact time
depends on the vocabulary size and beam size.

source
NMT
+ topics
reference
source

NMT

+ topics

reference

ich m¨ochte Ihnen heute Morgen gerne von meinem Projekt, Kunst Aufr¨aumen, erz¨ahlen.
I want to clean you this morning, from my project, to say Art.
I would like to talk to you today by my project, Art clean.
I would like to talk to you this morning about my project, Tidying Up Art.
. . . unsere Kollegen an Tufts verbinden Modelle wie diese mit durch Tissue Engineering erzeugten Knochen,
um zu sehen, wie Krebs sich von einem Teil des K¨orpers zum n¨achsten verbreiten k¨onnte.
. . . our NOAA colleagues combined models of models like this with tissue generated bones from bones to see
how cancer could spread from one part of the body, to the next distribution.
. . . our colleagues at Tufts are using models like this with tissue-based engineered bones to see
how cancer could spread from a part of the body to the next part.
. . . our colleagues at Tufts are mixing models like these with tissue-engineered bone to see
how cancer might spread from one part of the body to the next.

Table 3: Example of improved translation quality when topic information is used as input in the neural
MT system (German-to-English IWSLT test set).

E-commerce En→Fr
Baseline NMT
+cross-entropy (decay)
+cross-entropy (1:2)
+cross-entropy (1:1)
+cross-entropy (2:1)
+squared error (1:1)

BLEU % TER %
18.6
20.5
20.6
20.2
20.9
20.8

68.3
65.8
65.5
65.0
65.7
64.5

Table 4: Comparison of different loss functions
and weight ratios for guided alignment.

niﬁcantly (19.7% BLEU) outperforming the base-
line.

Replacing the human-labeled topic one-hot vec-
tors of size 80 with the LDA-predicted topic distri-
bution vectors of the same dimension in the read-
out layer of the neural network deteriorated the
BLEU and TER scores signiﬁcantly. We attribute
this to data sparseness problems when training the
LDA of dimension 80 on product titles.

On the German-to-English task, we also ob-
served MT quality improvements when using
human-labeled topic information as described in
Figure 1. Here, we extracted the topic embed-
ding Ec from different experiments and show
their cosine distance in Figure 3. It’s straightfor-
ward that in different experiments, the same topic
tends to share similar representation in continu-
ous embedding space. At the same time, closer
topic pairs like “politics” and “issues” tend to
have shorter distance from each other. Examples
of improved German-to-English NMT translations
when human-labeled topic information is used are
shown in Table 3.

7.4

Implementation of Guided Alignment

To balance decoder cost and the attention weight
cost, we experimented with different weights for

Figure 3: Topic embedding cosine distance.

7.3 Effect of Topic-aware NMT

We tested different approaches to ﬁnd out where
topic information ﬁts best into NMT, since topic
information can affect alignment, word selection,
etc. The most naive approach is to insert a pseudo
topic word in the beginning of a sentence to bias
the context of the sentence to a certain topic. We
also tried topic vectors of different origin in the
read-out layer of the network. We used both topics
predicted automatically with the Latent Dirichlet
Analysis (LDA) and human-labeled topics to feed
into the network as shown in Figure 1.

The results on the e-commerce task in Ta-
ble 2 show that category information as a pseudo
topic word does not carry enough semantic and
syntactic meaning in comparison to real source
words to have a positive effect on the target words
predicted in the decoder. The BLEU score of
such system (18.3%) is even below the baseline
the human-labeled cate-
(18.6%).
gories are more reliable and are able to positively
inﬂuence word selection in the NMT decoder, sig-

In contrast,

Vintage Ollech & Wajs Early Bird Diver watch, Excellent!
Vintage Ollech & Wajs d´ebut oiseau montre de plong´ee, excellent!
Montre de plong´ee vintage Ollech & Wajs early bird, excellent!

source
SMT
NMT
reference Montre de Plong´ee Vintage Ollech & Wajs Early Bird, Excellent !
APT Holman Model 1 Audiophile Power Amplifer made in Cambridge Mass
source
APT Holman modle 1 audiophile power fabricant d’ampli made in Cambridge Mass
SMT
L’ampliﬁcateur de puissance audiophile APT Holman mod`ele 1 fabriqu´ee ´a Cambridge Mass
NMT
reference Ampliﬁcateur de puissance pour audiophile APT Holman mod`ele 1 fabriqu´e ´a Cambridge Massachussets

Table 6: Example of improved translation quality of the NMT ensemble system vs. phrase-based baseline
system (English-to-French title test set).

Exp System Description
Phrase-based system
1
Phrase-based system + OSM
2
NMT
3
NMT + topic vectors
4
NMT + bootstrapping
5
NMT + guided alignment
6
NMT + topic vectors + bootstrapping
7
NMT + topic vectors + bootstrapping + guided alignment
8

BLEU % TER %

24.7
25.7
23.4
23.7
24.1
23.8
24.2
24.6

55.4
55.1
60.1
59.6
58.6
60.8
59.4
57.7

9

Ensemble

27.8

55.4

NMT + topic vectors
NMT + topic vectors + guided alignment
NMT + topic vectors + bootstrapping
NMT + topic v. + guided alignment + bootstrapping

Table 7: Overview of the translation results on the English-to-German IWSLT task.

these costs. We analyzed the relation between
weight ratio and the ﬁnal result in Table 4. Be-
sides ﬁxing the cost ratios during training, we also
apply a heuristic to adjust the ratio as the training
is progressing. One approach is to set a high value
for the alignment cost in the beginning, then decay
the weight to 90% after every epoch, ﬁnally elim-
inating the inﬂuence of the alignment after some
time. This approach helps for the IWSLT task,
but not on the e-commerce task. We assume that
the alignment for the TED talk sentences seems
to be easier for NMT to learn on its own than the
alignment between product titles and their transla-
tions. We also analyzed the effect of using differ-
ent loss functions for calculating alignment diver-
gence (see Section 3.2). The difference between
the squared error and cross-entropy is not so large
as shown in Table 4. Since the cross-entropy func-
tion has a consistent form as decoder cost, we
decided to use it in further experiments. We ex-
tracted the NMT attention weights and marked the
connection with the highest score as hard align-
ment for each word. We drew the alignment in
Figure 4 to compare baseline NMT and alignment-
guided NMT. It can be seen from the graph that
the guided alignment training truly improves the

alignment correspondence.

7.5 Overall Results

The overall results on the e-commerce translation
task and IWSLT task are shown in Table 5 and Ta-
ble 7, respectively. We observed consistency on
both tasks, in a sense that a feature that improves
BLEU/TER results on one task is also beneﬁcial
for the other.

For comparison, we trained phrase-based SMT
models using the Moses toolkit (Koehn et al.,
2007) on both translation tasks. We used the
standard Moses features, including a 4-gram LM
trained on the target side of the bilingual data,
word-level and phrase-level translation probabil-
ities, as well as the distortion model with the max-
imum distortion of 6. Our stronger phrase-based
baseline included additional 5 features of a 4-gram
operation sequence model – OSM (Durrani et al.,
2015).

On the e-commerce task, which is more chal-
lenging due to a high number of OOV words and
placeholders, we observed that NMT translation
output had many errors related to incorrect atten-
tion weights. To improve the attention mecha-
nism, we applied guided alignment and bootstrap-

System Description

1. NMT in-domain (ID)
2. 1) + topic vectors
3. 1) + bootstrapping
4. 1) + guided alignment
5. NMT with 2) and 4)
6. NMT with 2) and 3) and 4)
7. NMT out-of-domain (OOD)
8. 7) + guided alignment
9. 8) + domain adaptation

Ensemble

NMT
ID
Ensemble
NMT
OOD

system 4)
system 5)
system 6)
NMT w. 3) and 4)
system 9)
9) with DW
9) w. topic vectors

BLEU
%
18.6
19.7
20.1
20.9
21.3
20.7
13.8
16.3
25.0

TER
%
68.5
65.3
66.2
65.7
64.3
66.2
77.4
74.5
60.1

24.5

60.9

25.6

58.6

Table 5: Overview of the translation results on the
English-to-French e-commerce translation task.
(DW: decaying weight for the statistical align-
ment).

reverse was true for 460 titles. In particular, the
word order of noun phrases was observed to be
better in the NMT translations.

On the IWSLT task (Table 7), the baseline NMT
was not as far behind the phrase-based system as
on the e-commerce task, and thus the obtained
improvements were smaller than for product ti-
tle translations. We observed that topic informa-
tion is less helpful than bootstrapping and guided
alignment learning. When we combined them, we
reached the same BLEU score as the phrase-based
system (see Table 7). Finally, we combined four
variant systems to create an ensemble, which re-
sulted in the BLEU score of 27.8%, surpassing the
phrase-based translation with the OSM model by
2.1% BLEU absolute.

8 Conclusion

We have presented a novel guided alignment train-
ing for a NMT model that utilizes IBM model 4
Viterbi alignments to guide the attention mecha-
nism. This approach was shown experimentally to
bring consistent improvements of translation qual-
ity on e-commerce and spoken language trans-
lation tasks. Also on both tasks, the proposed
novel way of utilizing topic meta-information was

Figure 4: Reﬁned alignment examples using
guided-alignment learning (green blocks refer to
the identical alignments from Baseline NMT and
guided-alignment NMT, blue blocks refer to the
alignment from baseline NMT, yellow blocks re-
fer to guided-alignment NMT).

ping. Both boosted the translation performance.
Adding topic information increased the BLEU
score to 21.3%. We selected the four best model
parameters from various experiments to make an
ensemble system, this improved the BLEU score
to 24.5%. For the following experiment, we had
pre-trained a model on WMT15 parallel data with
the guided alignment technique, and then contin-
ued training on the e-commerce data for several
epocs as described in section 6, performing do-
main adaptation. This approach proved to be ex-
tremely helpful, giving an increase of over 3.0%
absolute in BLEU. Finally, we also applied en-
semble methods on variants of the domain-adapted
models to further increase the BLEU score to 25.6,
which is 7.0 BLEU higher than the NMT baseline
system, and only 0.6% BLEU behind the BLEU
score of 26.2% for the state-of-the-art phrase-
based baseline. Table 6 shows examples where the
ensemble NMT system is better than the phrase-
based system despite the slightly lower corpus-
level BLEU score. In fact, a more detailed anal-
ysis of the sentence-level BLEU scores showed
that the NMT translation of 386 titles out of 910
was ranked higher than the SMT translation, the

shown to improve BLEU and TER scores. We also
showed improvements when using domain adap-
tation by continuing training of an out-of-domain
NMT system on in-domain parallel data.
In the
future, we would like to investigate how to ef-
fectively make use of the abundant monolingual
data with human-labeled product category infor-
mation that we have available for the envisioned
e-commerce application.

References

[Bahdanau et al.2014] Dzmitry Bahdanau, Kyunghyun
Cho, and Yoshua Bengio. 2014. Neural machine
translation by jointly learning to align and translate.
arXiv preprint arXiv:1409.0473.

[Brown et al.1993] Peter F. Brown, Stephen A. Della
Pietra, Vincent J. Della Pietra, and Robert. L. Mer-
cer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
Linguistics, 19:263–311.

[Cettolo et al.2012] Mauro Cettolo, Christian Girardi,
and Marcello Federico. 2012. Wit3: Web inventory
of transcribed and translated talks. In Proceedings
of the 16th Conference of the European Association
for Machine Translation (EAMT), pages 261–268,
Trento, Italy, May.

[Cho et al.2014a] Kyunghyun

Bart
van
Merri¨enboer, Dzmitry Bahdanau,
and Yoshua
Bengio.
2014a. On the properties of neural
machine translation: Encoder-decoder approaches.
arXiv preprint arXiv:1409.1259.

Cho,

[Cho et al.2014b] Kyunghyun

Cho,

Bart
Van Merri¨enboer, Caglar Gulcehre, Dzmitry
Bahdanau, Fethi Bougares, Holger Schwenk,
Learning phrase
and Yoshua Bengio.
representations using RNN encoder-decoder
for
arXiv preprint
statistical machine translation.
arXiv:1406.1078.

2014b.

[Cohn et al.2016] Trevor Cohn, Cong Duy Vu Hoang,
and Ekaterina Vymolova.
Incorpo-
rating structural alignment biases into an atten-
arXiv preprint
tion neural
arXiv:1601.01085.

translation model.

2016.

[Durrani et al.2015] Nadir Durrani, Helmut Schmid,
Alexander Fraser, Philipp Koehn, and Hinrich
Sch¨utze. 2015. The operation sequence model-
combining n-gram-based and phrase-based statisti-
cal machine translation. Computational Linguistics.

[Goodfellow et al.2013] Ian

J Goodfellow, David
Warde-Farley, Mehdi Mirza, Aaron Courville, and
Yoshua Bengio. 2013. Maxout networks. arXiv
preprint arXiv:1302.4389.

[Hasler et al.2014] Eva Hasler, Phil Blunsom, Philipp
Koehn, and Barry Haddow. 2014. Dynamic topic
adaptation for phrase-based MT. In Proceedings of
EACL, pages 328–337.

[Jean et al.2014] S´ebastien Jean, Kyunghyun Cho,
Roland Memisevic, and Yoshua Bengio. 2014. On
using very large target vocabulary for neural ma-
chine translation. CoRR, abs/1412.2007.

[Koehn and Schroeder2007] Philipp Koehn and Josh
Schroeder. 2007. Experiments in domain adaptation
for statistical machine translation. In Proceedings of
the second workshop on statistical machine transla-
tion, pages 224–227. Association for Computational
Linguistics.

[Koehn et al.2007] Philipp Koehn, Hieu Hoang,
Alexandra Birch, Chris Callison-Burch, Mar-
cello Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard Zens, et al.
2007. Moses: Open source toolkit for statisti-
In Proceedings of the
cal machine translation.
45th annual meeting of
the ACL on interactive
poster and demonstration sessions, pages 177–180.
Association for Computational Linguistics.

[Luong and Manning2015] Minh-Thang Luong and
Christopher D. Manning. 2015. Stanford neural
machine translation systems for spoken language
domain.

[Luong et al.2014] Minh-Thang Luong, Ilya Sutskever,
Quoc V Le, Oriol Vinyals, and Wojciech Zaremba.
Addressing the rare word problem
2014.
arXiv preprint
in neural machine translation.
arXiv:1410.8206.

[Luong et al.2015] Minh-Thang Luong, Hieu Pham,
and Christopher D Manning. 2015. Effective ap-
proaches to attention-based neural machine transla-
tion. arXiv preprint arXiv:1508.04025.

[Mathur et al.2015] Prashant Mathur, Marcello Fed-
erico, Selc¸uk K¨opr¨u, Sharam Khadivi, and Hassan
Sawaf. 2015. Topic adaptation for machine trans-
lation of e-commerce content. Proceedings of MT
Summit XV, page 270.

[Mikolov et al.2013] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013. Efﬁcient estima-
tion of word representations in vector space. arXiv
preprint arXiv:1301.3781.

[Och and Ney2003] Franz Josef Och and Hermann Ney.
2003. A systematic comparison of various statisti-
cal alignment models. Computational Linguistics,
29(1):19–51.

[Papineni et al.2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu.
2002. Bleu: a
method for automatic evaluation of machine trans-
In Proceedings of the 40th annual meeting
lation.
on association for computational linguistics, pages
311–318. Association for Computational Linguis-
tics.

[Snover et al.2006] Matthew Snover, Bonnie Dorr,
Richard Schwartz, Linnea Micciulla, and John
Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of
association for machine translation in the Americas,
pages 223–231.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals,
and Quoc V Le. 2014. Sequence to sequence learn-
ing with neural networks. In Advances in neural in-
formation processing systems, pages 3104–3112.

[van Merri¨enboer et al.2015] Bart

van Merri¨enboer,
Dzmitry Bahdanau, Vincent Dumoulin, Dmitriy
Jan Chorowski,
Serdyuk, David Warde-Farley,
Blocks and fuel:
and Yoshua Bengio.
arXiv preprint
Frameworks for deep learning.
arXiv:1506.00619.

2015.

[Zeiler2012] Matthew D Zeiler.

2012. Adadelta:
an adaptive learning rate method. arXiv preprint
arXiv:1212.5701.

