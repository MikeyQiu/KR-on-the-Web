Learning Discourse-level Diversity for Neural Dialog Models using
Conditional Variational Autoencoders

Tiancheng Zhao, Ran Zhao and Maxine Eskenazi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, Pennsylvania, USA
{tianchez,ranzhao1,max+}@cs.cmu.edu

Abstract

recent neural

While
encoder-decoder
models have shown great promise in mod-
they
eling open-domain conversations,
often generate dull and generic responses.
Unlike past work that has focused on
diversifying the output of the decoder
at word-level to alleviate this problem,
we present a novel framework based on
conditional variational autoencoders that
captures the discourse-level diversity in
the encoder. Our model uses latent vari-
ables to learn a distribution over potential
conversational
generates
intents
diverse responses using only greedy de-
coders. We have further developed a novel
variant that is integrated with linguistic
prior knowledge for better performance.
Finally, the training procedure is improved
by introducing a bag-of-word loss. Our
proposed models have been validated
to generate signiﬁcantly more diverse
responses than baseline approaches and
exhibit competence in discourse-level
decision-making.1

and

Introduction

1
The dialog manager is one of the key components
of dialog systems, which is responsible for mod-
eling the decision-making process. Speciﬁcally, it
typically takes a new utterance and the dialog con-
text as input, and generates discourse-level deci-
sions (Bohus and Rudnicky, 2003; Williams and
Young, 2007). Advanced dialog managers usu-
ally have a list of potential actions that enable
them to have diverse behavior during a conver-
sation, e.g. different strategies to recover from
non-understanding (Yu et al., 2016). However,

1Data and an implementation of our model is avalaible at

https://github.com/snakeztc/NeuralDialog-CVAE

recent

the conventional approach of designing a dialog
manager (Williams and Young, 2007) does not
scale well to open-domain conversation models
because of the vast quantity of possible decisions.
Thus, there has been a growing interest in applying
encoder-decoder models (Sutskever et al., 2014)
for modeling open-domain conversation (Vinyals
and Le, 2015; Serban et al., 2016a). The basic ap-
proach treats a conversation as a transduction task,
in which the dialog history is the source sequence
and the next response is the target sequence. The
model is then trained end-to-end on large conver-
sation corpora using the maximum-likelihood esti-
mation (MLE) objective without the need for man-
ual crafting.
However

found that
encoder-decoder models tend to generate generic
and dull responses (e.g., I don’t know), rather
than meaningful and speciﬁc answers (Li et al.,
2015; Serban et al., 2016b). There have been
many attempts to explain and solve this limita-
tion, and they can be broadly divided into two cat-
egories (see Section 2 for details): (1) the ﬁrst cat-
egory argues that the dialog history is only one of
the factors that decide the next response. Other
features should be extracted and provided to the
models as conditionals in order to generate more
speciﬁc responses (Xing et al., 2016; Li et al.,
2016a); (2) the second category aims to improve
the encoder-decoder model itself, including de-
coding with beam search and its variations (Wise-
man and Rush, 2016), encouraging responses that
have long-term payoff (Li et al., 2016b), etc.

research has

Building upon the past work in dialog managers
and encoder-decoder models, the key idea of this
paper is to model dialogs as a one-to-many prob-
lem at the discourse level. Previous studies indi-
cate that there are many factors in open-domain
dialogs that decide the next response, and it is non-
Intuitively, given
trivial to extract all of them.

7
1
0
2
 
t
c
O
 
1
2
 
 
]
L
C
.
s
c
[
 
 
3
v
0
6
9
0
1
.
3
0
7
1
:
v
i
X
r
a

a similar dialog history (and other observed in-
puts), there may exist many valid responses (at the
discourse-level), each corresponding to a certain
conﬁguration of the latent variables that are not
presented in the input. To uncover the potential re-
sponses, we strive to model a probabilistic distri-
bution over the distributed utterance embeddings
of the potential responses using a latent variable
(Figure 1). This allows us to generate diverse re-
sponses by drawing samples from the learned dis-
tribution and reconstruct their words via a decoder
neural network.

Figure 1: Given A’s question, there exists many
valid responses from B for different assumptions
of the latent variables, e.g., B’s hobby.

Speciﬁcally, our contributions are three-fold:
1. We present a novel neural dialog model
adapted from conditional variational autoencoders
(CVAE) (Yan et al., 2015; Sohn et al., 2015),
which introduces a latent variable that can cap-
ture discourse-level variations as described above
2. We propose Knowledge-Guided CVAE (kgC-
VAE), which enables easy integration of expert
knowledge and results in performance improve-
ment and model interpretability. 3. We develop
a training method in addressing the difﬁculty of
optimizing CVAE for natural language genera-
tion (Bowman et al., 2015). We evaluate our
models on human-human conversation data and
yield promising results in: (a) generating appro-
priate and discourse-level diverse responses, and
(b) showing that the proposed training method is
more effective than the previous techniques.

2 Related Work

Our work is related to both recent advancement
in encoder-decoder dialog models and generative
models based on CVAE.

2.1 Encoder-decoder Dialog Models
Since the emergence of the neural dialog model,
the problem of output diversity has received much
attention in the research community.
Ideal out-
put responses should be both coherent and diverse.
However, most models end up with generic and
dull responses. To tackle this problem, one line
of research has focused on augmenting the in-

put of encoder-decoder models with richer con-
text information, in order to generate more spe-
ciﬁc responses. Li et al., (2016a) captured speak-
ers’ characteristics by encoding background infor-
mation and speaking style into the distributed em-
beddings, which are used to re-rank the generated
response from an encoder-decoder model. Xing et
al., (2016) maintain topic encoding based on La-
tent Dirichlet Allocation (LDA) (Blei et al., 2003)
of the conversation to encourage the model to out-
put more topic coherent responses.

On the other hand, many attempts have also
been made to improve the architecture of encoder-
decoder models. Li et al,. (2015) proposed to opti-
mize the standard encoder-decoder by maximizing
the mutual information between input and output,
which in turn reduces generic responses. This ap-
proach penalized unconditionally high frequency
responses, and favored responses that have high
conditional probability given the input. Wiseman
and Rush (2016) focused on improving the de-
coder network by alleviating the biases between
training and testing. They introduced a search-
based loss that directly optimizes the networks
for beam search decoding. The resulting model
achieves better performance on word ordering,
parsing and machine translation. Besides improv-
ing beam search, Li et al., (2016b) pointed out that
the MLE objective of an encoder-decoder model
is unable to approximate the real-world goal of
the conversation. Thus, they initialized a encoder-
decoder model with MLE objective and leveraged
reinforcement learning to ﬁne tune the model by
optimizing three heuristic rewards functions: in-
formativity, coherence, and ease of answering.

2.2 Conditional Variational Autoencoder
The variational autoencoder (VAE) (Kingma and
Welling, 2013; Rezende et al., 2014) is one of the
most popular frameworks for image generation.
The basic idea of VAE is to encode the input x
into a probability distribution z instead of a point
encoding in the autoencoder. Then VAE applies a
decoder network to reconstruct the original input
using samples from z. To generate images, VAE
ﬁrst obtains a sample of z from the prior distribu-
tion, e.g. N (0, I), and then produces an image via
the decoder network. A more advanced model, the
conditional VAE (CVAE), is a recent modiﬁcation
of VAE to generate diverse images conditioned on
certain attributes, e.g. generating different human
faces given skin color (Yan et al., 2015; Sohn et al.,

2015). Inspired by CVAE, we view the dialog con-
texts as the conditional attributes and adapt CVAE
to generate diverse responses instead of images.

Although VAE/CVAE has achieved impressive
results in image generation, adapting this to natu-
ral language generators is non-trivial. Bowman et
al., (2015) have used VAE with Long-Short Term
Memory (LSTM)-based recognition and decoder
networks to generate sentences from a latent Gaus-
sian variable. They showed that their model is able
to generate diverse sentences with even a greedy
LSTM decoder. They also reported the difﬁculty
of training because the LSTM decoder tends to ig-
nore the latent variable. We refer to this issue as
the vanishing latent variable problem. Serban et
al., (2016b) have applied a latent variable hierar-
chical encoder-decoder dialog model to introduce
utterance-level variations and facilitate longer re-
sponses. To improve upon the past models, we
ﬁrstly introduce a novel mechanism to leverage
linguistic knowledge in training end-to-end neural
dialog models, and we also propose a novel train-
ing technique that mitigates the vanishing latent
variable problem.

3 Proposed Models

Figure 2: Graphical models of CVAE (a) and kgC-
VAE (b)

3.1 Conditional Variational Autoencoder
(CVAE) for Dialog Generation

Each dyadic conversation can be represented via
three random variables: the dialog context c (con-
text window size k − 1), the response utterance x
(the kth utterance) and a latent variable z, which
is used to capture the latent distribution over the
valid responses. Further, c is composed of the dia-
log history: the preceding k-1 utterances; conver-
sational ﬂoor (1 if the utterance is from the same
speaker of x, otherwise 0) and meta features m
(e.g. the topic). We then deﬁne the conditional dis-
tribution p(x, z|c) = p(x|z, c)p(z|c) and our goal
is to use deep neural networks (parametrized by θ)
to approximate p(z|c) and p(x|z, c). We refer to

pθ(z|c) as the prior network and pθ(x, |z, c) as the
response decoder. Then the generative process of
x is (Figure 2 (a)):

1. Sample a latent variable z from the prior net-

work pθ(z|c).

pθ(x|z, c).

2. Generate x through the response decoder

CVAE is trained to maximize the conditional
log likelihood of x given c, which involves an in-
tractable marginalization over the latent variable
z. As proposed in (Sohn et al., 2015; Yan et al.,
2015), CVAE can be efﬁciently trained with the
Stochastic Gradient Variational Bayes (SGVB)
framework (Kingma and Welling, 2013) by maxi-
mizing the variational lower bound of the condi-
tional log likelihood. We assume the z follows
multivariate Gaussian distribution with a diago-
nal covariance matrix and introduce a recognition
network qφ(z|x, c) to approximate the true poste-
rior distribution p(z|x, c). Sohn and et al,. (2015)
have shown that the variational lower bound can
be written as:

L(θ, φ; x, c) = −KL(qφ(z|x, c)(cid:107)pθ(z|c))

+ Eqφ(z|c,x)[log pθ(x|z, c)]
≤ log p(x|c)

(1)

Figure 3 demonstrates an overview of our model.
The utterance encoder is a bidirectional recurrent
neural network (BRNN) (Schuster and Paliwal,
1997) with a gated recurrent unit (GRU) (Chung
et al., 2014) to encode each utterance into ﬁxed-
size vectors by concatenating the last hidden states
of the forward and backward RNN ui = [ (cid:126)hi, (cid:126)hi].
x is simply uk. The context encoder is a 1-layer
GRU network that encodes the preceding k-1 ut-
terances by taking u1:k−1 and the corresponding
conversation ﬂoor as inputs. The last hidden state
hc of the context encoder is concatenated with
meta features and c = [hc, m]. Since we assume z
follows isotropic Gaussian distribution, the recog-
nition network qφ(z|x, c) ∼ N (µ, σ2I) and the
prior network pθ(z|c) ∼ N (µ(cid:48), σ(cid:48)2I), and then we
have:

(cid:20)

(cid:21)
µ
log(σ2)
(cid:21)
µ(cid:48)
log(σ(cid:48)2)

(cid:20)

= Wr

+ br

(cid:21)

(cid:20)x
c

= MLPp(c)

(2)

(3)

Figure 3: The neural network architectures for the baseline and the proposed CVAE/kgCVAE models.
(cid:76) denotes the concatenation of the input vectors. The dashed blue connections only appear in kgCVAE.

We then use the reparametrization trick (Kingma
and Welling, 2013) to obtain samples of z either
from N (z; µ, σ2I) predicted by the recognition
network (training) or N (z; µ(cid:48), σ(cid:48)2I) predicted by
the prior network (testing). Finally, the response
decoder is a 1-layer GRU network with initial state
s0 = Wi[z, c]+bi. The response decoder then pre-
dicts the words in x sequentially.

3.2 Knowledge-Guided CVAE (kgCVAE)
In practice, training CVAE is a challenging opti-
mization problem and often requires large amount
of data. On the other hand, past research in spo-
ken dialog systems and discourse analysis has sug-
gested that many linguistic cues capture crucial
features in representing natural conversation. For
example, dialog acts (Poesio and Traum, 1998)
have been widely used in the dialog managers (Lit-
man and Allen, 1987; Raux et al., 2005; Zhao
and Eskenazi, 2016) to represent the propositional
function of the system. Therefore, we conjecture
that it will be beneﬁcial for the model to learn
meaningful latent z if it is provided with explicitly
extracted discourse features during the training.

In order to incorporate the linguistic features
into the basic CVAE model, we ﬁrst denote the set
of linguistic features as y. Then we assume that
the generation of x depends on c, z and y. y re-
lies on z and c as shown in Figure 2. Speciﬁcally,
during training the initial state of the response de-
coder is s0 = Wi[z, c, y] + bi and the input at ev-
ery step is [et, y] where et is the word embedding
of tth word in x. In addition, there is an MLP to
predict y(cid:48) = MLPy(z, c) based on z and c. In the
testing stage, the predicted y(cid:48) is used by the re-

sponse decoder instead of the oracle decoders. We
denote the modiﬁed model as knowledge-guided
CVAE (kgCVAE) and developers can add desired
discourse features that they wish the latent vari-
able z to capture. KgCVAE model is trained by
maximizing:

L(θ, φ; x, c, y) = −KL(qφ(z|x, c, y)(cid:107)Pθ(z|c))

+ Eqφ(z|c,x,y)[log p(x|z, c, y)]
+ Eqφ(z|c,x,y)[log p(y|z, c)]

(4)

Since now the reconstruction of y is a part of the
loss function, kgCVAE can more efﬁciently en-
code y-related information into z than discovering
it only based on the surface-level x and c. Another
advantage of kgCVAE is that it can output a high-
level label (e.g. dialog act) along with the word-
level responses, which allows easier interpretation
of the model’s outputs.

3.3 Optimization Challenges
A straightforward VAE with RNN decoder fails
to encode meaningful information in z due to the
vanishing latent variable problem (Bowman et al.,
2015). Bowman et al., (2015) proposed two solu-
tions: (1) KL annealing: gradually increasing the
weight of the KL term from 0 to 1 during training;
(2) word drop decoding: setting a certain percent-
age of the target words to 0. We found that CVAE
suffers from the same issue when the decoder is
an RNN. Also we did not consider word drop de-
coding because Bowman et al,. (2015) have shown
that it may hurt the performance when the drop
rate is too high.

As a result, we propose a simple yet novel tech-
nique to tackle the vanishing latent variable prob-
lem: bag-of-word loss. The idea is to introduce
an auxiliary loss that requires the decoder network
to predict the bag-of-words in the response x as
shown in Figure 3(b). We decompose x into two
variables: xo with word order and xbow without
order, and assume that xo and xbow are condi-
tionally independent given z and c: p(x, z|c) =
p(xo|z, c)p(xbow|z, c)p(z|c). Due to the condi-
tional independence assumption, the latent vari-
able is forced to capture global information about
the target response. Let f = MLPb(z, x) ∈ RV
where V is vocabulary size, and we have:

log p(xbow|z, c) = log

(5)

|x|
(cid:89)

t=1

efxt
j efj

(cid:80)V

where |x| is the length of x and xt is the word
index of tth word in x. The modiﬁed variational
lower bound for CVAE with bag-of-word loss is
(see Appendix A for kgCVAE):

L(cid:48)(θ, φ; x, c) = L(θ, φ; x, c)

+ Eqφ(z|c,x,y)[log p(xbow|z, c)] (6)

We will show that the bag-of-word loss in Equa-
tion 6 is very effective against the vanishing latent
variable and it is also complementary to the KL
annealing technique.

4 Experiment Setup
4.1 Dataset

We chose the Switchboard (SW) 1 Release 2 Cor-
pus (Godfrey and Holliman, 1997) to evaluate the
proposed models. SW has 2400 two-sided tele-
phone conversations with manually transcribed
speech and alignment.
In the beginning of the
call, a computer operator gave the callers recorded
prompts that deﬁne the desired topic of discus-
sion. There are 70 available topics. We ran-
domly split the data into 2316/60/62 dialogs for
train/validate/test. The pre-processing includes (1)
tokenize using the NLTK tokenizer (Bird et al.,
2009); (2) remove non-verbal symbols and re-
peated words due to false starts; (3) keep the
top 10K frequent word types as the vocabulary.
The ﬁnal data have 207, 833/5, 225/5, 481 (c, x)
pairs for train/validate/test. Furthermore, a sub-
set of SW was manually labeled with dialog
acts (Stolcke et al., 2000). We extracted dia-
log act labels based on the dialog act recognizer

proposed in (Ribeiro et al., 2015). The features
include the uni-gram and bi-gram of the utter-
ance, and the contextual features of the last 3 ut-
terances. We trained a Support Vector Machine
(SVM) (Suykens and Vandewalle, 1999) with lin-
ear kernel on the subset of SW with human anno-
tations. There are 42 types of dialog acts and the
SVM achieved 77.3% accuracy on held-out data.
Then the rest of SW data are labelled with dialog
acts using the trained SVM dialog act recognizer.

4.2 Training

We trained with the following hyperparameters
(according to the loss on the validate dataset):
word embedding has size 200 and is shared
across everywhere. We initialize the word embed-
ding from Glove embedding pre-trained on Twit-
ter (Pennington et al., 2014). The utterance en-
coder has a hidden size of 300 for each direc-
tion. The context encoder has a hidden size of
600 and the response decoder has a hidden size
of 400. The prior network and the MLP for pre-
dicting y both have 1 hidden layer of size 400 and
tanh non-linearity. The latent variable z has a
size of 200. The context window k is 10. All
the initial weights are sampled from a uniform
distribution [-0.08, 0.08]. The mini-batch size is
30. The models are trained end-to-end using the
Adam optimizer (Kingma and Ba, 2014) with a
learning rate of 0.001 and gradient clipping at 5.
We selected the best models based on the varia-
tional lower bound on the validate data. Finally,
we use the BOW loss along with KL annealing of
10,000 batches to achieve the best performance.
Section 5.4 gives a detailed argument for the im-
portance of the BOW loss.

5 Results
5.1 Experiments Setup

We compared three neural dialog models: a strong
baseline model, CVAE, and kgCVAE. The base-
line model is an encoder-decoder neural dialog
model without latent variables similar to (Serban
et al., 2016a). The baseline model’s encoder uses
the same context encoder to encode the dialog his-
tory and the meta features as shown in Figure 3.
The encoded context c is directly fed into the de-
coder networks as the initial state. The hyperpa-
rameters of the baseline are the same as the ones
reported in Section 4.2 and the baseline is trained
to minimize the standard cross entropy loss of the
decoder RNN model without any auxiliary loss.

Also, to compare the diversity introduced by the
stochasticity in the proposed latent variable ver-
sus the softmax of RNN at each decoding step, we
generate N responses from the baseline by sam-
pling from the softmax. For CVAE/kgCVAE, we
sample N times from the latent z and only use
greedy decoders so that the randomness comes en-
tirely from the latent variable z.

5.2 Quantitative Analysis

Automatically evaluating an open-domain gen-
erative dialog model is an open research chal-
lenge (Liu et al., 2016). Following our one-to-
many hypothesis, we propose the following met-
rics. We assume that for a given dialog context c,
there exist Mc reference responses rj, j ∈ [1, Mc].
Meanwhile a model can generate N hypothesis re-
sponses hi, i ∈ [1, N ]. The generalized response-
level precision/recall for a given dialog context is:

3. Dialog Act Match:

to measure the similar-
ity at the discourse level, the same dialog-
act tagger from 4.1 is applied to label all the
generated responses of each model. We set
d(rj, hi) = 1 if rj and hi have the same dia-
log acts, otherwise d(rj, hi) = 0.

One challenge of using the above metrics is that
there is only one, rather than multiple reference
responses/contexts. This impacts reliability of our
measures. Inspired by (Sordoni et al., 2015), we
utilized information retrieval techniques (see Ap-
pendix A) to gather 10 extra candidate reference
responses/context from other conversations with
the same topics. Then the 10 candidate references
are ﬁltered by two experts, which serve as the
ground truth to train the reference response classi-
ﬁer. The result is 6.69 extra references in average
per context. The average number of distinct refer-
ence dialog acts is 4.2. Table 1 shows the results.

precision(c) =

recall(c) =

(cid:80)N

i=1 maxj∈[1,Mc]d(rj, hi)
N
j=1 maxi∈[1,N ]d(rj, hi))
Mc

(cid:80)Mc

where d(rj, hi) is a distance function which lies
between 0 to 1 and measures the similarities be-
tween rj and hi. The ﬁnal score is averaged over
the entire test dataset and we report the perfor-
mance with 3 types of distance functions in or-
der to evaluate the systems from various linguistic
points of view:

1. Smoothed Sentence-level BLEU (Chen and
Cherry, 2014): BLEU is a popular metric that
measures the geometric mean of modiﬁed n-
gram precision with a length penalty (Pap-
ineni et al., 2002; Li et al., 2015). We use
BLEU-1 to 4 as our lexical similarity metric
and normalize the score to 0 to 1 scale.

2. Cosine Distance of Bag-of-word Embed-
ding: a simple method to obtain sentence
embeddings is to take the average or ex-
trema of all the word embeddings in the sen-
tences (Forgues et al., 2014; Adi et al., 2016).
The d(rj, hi) is the cosine distance of the two
embedding vectors. We used Glove embed-
ding described in Section 4 and denote the av-
erage method as A-bow and extrema method
as E-bow. The score is normalized to [0, 1].

Metrics
perplexity (KL)

BLEU-1 prec
BLEU-1 recall
BLEU-2 prec
BLEU-2 recall
BLEU-3 prec
BLEU-3 recall
BLEU-4 prec
BLEU-4 recall
A-bow prec
A-bow recall
E-bow prec
E-bow recall
DA prec
DA recall

Baseline CVAE
35.4
(n/a)
0.405
0.336
0.300
0.281
0.272
0.254
0.226
0.215
0.951
0.935
0.827
0.801
0.736
0.514

20.2
(11.36)
0.372
0.381
0.295
0.322
0.265
0.292
0.223
0.248
0.954
0.943
0.815
0.812
0.704
0.604

kgCVAE
16.02
(13.08)
0.412
0.411
0.350
0.356
0.310
0.318
0.262
0.272
0.961
0.944
0.804
0.807
0.721
0.598

Table 1: Performance of each model on automatic
measures. The highest score in each row is in
bold. Note that our BLEU scores are normalized
to [0, 1].

The proposed models outperform the baseline
in terms of recall in all the metrics with statis-
tical signiﬁcance. This conﬁrms our hypothesis
that generating responses with discourse-level di-
versity can lead to a more comprehensive cov-
erage of the potential responses than promoting
only word-level diversity. As for precision, we
observed that the baseline has higher or similar

scores than CVAE in all metrics, which is expected
since the baseline tends to generate the mostly
likely and safe responses repeatedly in the N hy-
potheses. However, kgCVAE is able to achieve
the highest precision and recall in the 4 metrics at
the same time (BLEU1-4, A-BOW). One reason
for kgCVAE’s good performance is that the pre-
dicted dialog act label in kgCVAE can regularize
the generation process of its RNN decoder by forc-
ing it to generate more coherent and precise words.
We further analyze the precision/recall of BLEU-
4 by looking at the average score versus the num-
ber of distinct reference dialog acts. A low num-
ber of distinct dialog acts represents the situation
where the dialog context has a strong constraint
on the range of the next response (low entropy),
while a high number indicates the opposite (high-
entropy). Figure 4 shows that CVAE/kgCVAE
achieves signiﬁcantly higher recall than the base-
line in higher entropy contexts. Also it shows
that CVAE suffers from lower precision, espe-
cially in low entropy contexts. Finally, kgCVAE
gets higher precision than both the baseline and
CVAE in the full spectrum of context entropy.

able to generate various ways of back-channeling.
This implies that the latent z is able to capture
context-sensitive variations, i.e. in low-entropy di-
alog contexts modeling lexical diversity while in
high-entropy ones modeling discourse-level diver-
sity. Moreover, kgCVAE is occasionally able to
generate more sophisticated grounding (sample 4)
beyond a simple back-channel, which is also an
acceptable response given the dialog context.

In addition, past work (Kingma and Welling,
2013) has shown that the recognition network is
able to learn to cluster high-dimension data, so
we conjecture that posterior z outputted from the
recognition network should cluster the responses
into meaningful groups. Figure 5 visualizes the
posterior z of responses in the test dataset in 2D
space using t-SNE (Maaten and Hinton, 2008).
We found that the learned latent space is highly
correlated with the dialog act and length of re-
sponses, which conﬁrms our assumption.

Figure 4: BLEU-4 precision/recall vs. the number
of distinct reference dialog acts.

5.3 Qualitative Analysis

Table 2 shows the outputs generated from the
baseline and kgCVAE. In example 1, caller A be-
gins with an open-ended question. The kgCVAE
model generated highly diverse answers that cover
multiple plausible dialog acts. Further, we notice
that the generated text exhibits similar dialog acts
compared to the ones predicted separately by the
model, implying the consistency of natural lan-
guage generation based on y. On the contrary, the
responses from the baseline model are limited to
local n-gram variations and share a similar preﬁx,
i.e. ”I’m”. Example 2 is a situation where caller
A is telling B stories. The ground truth response
is a back-channel and the range of valid answers is
more constrained than example 1 since B is play-
ing the role of a listener. The baseline successfully
predicts ”uh-huh”. The kgCVAE model is also

Figure 5: t-SNE visualization of the posterior z for
test responses with top 8 frequent dialog acts. The
size of circle represents the response length.

5.4 Results for Bag-of-Word Loss

Finally, we evaluate the effectiveness of bag-of-
word (BOW) loss for training VAE/CVAE with the
RNN decoder. To compare with past work (Bow-
man et al., 2015), we conducted the same lan-
guage modelling (LM) task on Penn Treebank us-
ing VAE. The network architecture is same ex-
cept we use GRU instead of LSTM. We compared
four different training setups: (1) standard VAE
without any heuristics; (2) VAE with KL anneal-
ing (KLA); (3) VAE with BOW loss; (4) VAE
with both BOW loss and KLA. Intuitively, a well
trained model should lead to a low reconstruction

Example 1-Topic: Recycling Context: A: are they doing a lot of recycling out in Georgia? Target-B (statement): well
at my workplace we have palaces for aluminium cans and we have a separate trash can for recyclable paper
Baseline+Sampling
1. well I’m a graduate student and have two kids
2. well I was in last year and so we’ve had lots of recycling

kgCVAE+Greedy
1. (non-understand) pardon
2. (statement) oh you’re not going to have a curbside pick
up here
3. (statement) okay I am sure about a recycling center
4. (yes-answer) yeah so

3. I’m not sure
4. well I don’t know I just moved here in new york

Example 2-Topic: Child Care Context: A: you know a private home to take their children to when they’re young until
they hit the preschool age and they Target-B (backchannel): uh-huh
Baseline+Sampling
1. um - hum
2. yeah
3. um - hum
4. uh-huh

kgCVAE+Greedy
1. (backchannel) uh-huh
2. (turn-exit) um-hum
3. (backchannel) yeah
4. (statement) oh yeah I think that’s part of the problem

Table 2: Generated responses from the baselines and kgCVAE in two examples. KgCVAE also provides
the predicted dialog act for each response. The context only shows the last utterance due to space limit
(the actual context window size is 10).

portance of BOW loss for training latent vari-
able models with the RNN decoder. Last but not
least, our experiments showed that the conclusions
drawn from LM using VAE also apply to training
CVAE/kgCVAE, so we used BOW loss together
with KLA for all previous experiments.

loss and small but non-trivial KL cost. For all
models with KLA, the KL weight increases lin-
early from 0 to 1 in the ﬁrst 5000 batches.

Table 3 shows the reconstruction perplexity and
the KL cost on the test dataset. The standard VAE
fails to learn a meaningful latent variable by hav-
ing a KL cost close to 0 and a reconstruction per-
plexity similar to a small LSTM LM (Zaremba
et al., 2014). KLA helps to improve the recon-
struction loss, but it requires early stopping since
the models will fall back to the standard VAE after
the KL weight becomes 1. At last, the models with
BOW loss achieved signiﬁcantly lower perplexity
and larger KL cost.

Perplexity
Model
122.0
Standard
111.5
KLA
BOW
97.72
BOW+KLA 73.04

KL cost
0.05
2.02
7.41
15.94

Table 3: The reconstruction perplexity and KL
terms on Penn Treebank test set.

Figure 6 visualizes the evolution of the KL cost.
We can see that for the standard model, the KL
cost crashes to 0 at the beginning of training and
never recovers. On the contrary, the model with
only KLA learns to encode substantial informa-
tion in latent z when the KL cost weight is small.
However, after the KL weight is increased to 1 (af-
ter 5000 batch), the model once again decides to
ignore the latent z and falls back to the naive im-
plementation. The model with BOW loss, how-
ever, consistently converges to a non-trivial KL
cost even without KLA, which conﬁrms the im-

Figure 6: The value of the KL divergence during
training with different setups on Penn Treebank.

6 Conclusion and Future Work
In conclusion, we identiﬁed the one-to-many na-
ture of open-domain conversation and proposed
two novel models that show superior performance
in generating diverse and appropriate responses at
the discourse level. While the current paper ad-
dresses diversifying responses in respect to dia-
logue acts, this work is part of a larger research
direction that targets leveraging both past linguis-
tic ﬁndings and the learning power of deep neural
networks to learn better representation of the la-
tent factors in dialog. In turn, the output of this
novel neural dialog model will be easier to ex-
plain and control by humans.
In addition to di-
alog acts, we plan to apply our kgCVAE model

to capture other different linguistic phenomena in-
cluding sentiment, named entities,etc. Last but
not least, the recognition network in our model
will serve as the foundation for designing a data-
driven dialog manager, which automatically dis-
covers useful high-level intents. All of the above
suggest a promising research direction.

7 Acknowledgements

This work was funded by NSF grant CNS-
1512973. The opinions expressed in this paper do
not necessarily reﬂect those of NSF.

References

Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer
Lavi, and Yoav Goldberg. 2016. Fine-grained anal-
ysis of sentence embeddings using auxiliary predic-
tion tasks. arXiv preprint arXiv:1608.04207 .

Steven Bird, Ewan Klein, and Edward Loper. 2009.
”

language processing with Python.

Natural
O’Reilly Media, Inc.”.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of ma-
chine Learning research 3(Jan):993–1022.

Dan Bohus and Alexander I Rudnicky. 2003. Raven-
claw: Dialog management using hierarchical task
decomposition and an expectation agenda .

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2015. A diversity-promoting objec-
tive function for neural conversation models. arXiv
preprint arXiv:1510.03055 .

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016a. A persona-based neural con-
versation model. arXiv preprint arXiv:1603.06155
.

Jiwei Li, Will Monroe, Alan Ritter, and Dan Jurafsky.
2016b. Deep reinforcement learning for dialogue
generation. arXiv preprint arXiv:1606.01541 .

Diane J Litman and James F Allen. 1987. A plan
recognition model for subdialogues in conversa-
tions. Cognitive science 11(2):163–200.

Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
2016. How not to evaluate your dialogue system:
An empirical study of unsupervised evaluation met-
rics for dialogue response generation. arXiv preprint
arXiv:1603.08023 .

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research 9(Nov):2579–2605.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics. Association for Computational
Linguistics, pages 311–318.

Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-
drew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. 2015. Generating sentences from a continuous
space. arXiv preprint arXiv:1511.06349 .

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP. volume 14, pages 1532–
43.

Boxing Chen and Colin Cherry. 2014. A systematic
comparison of smoothing techniques for sentence-
level bleu. ACL 2014 page 362.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555 .

Gabriel

Joelle

Pineau,

Forgues,

Jean-Marie
Larchevˆeque, and R´eal Tremblay. 2014. Boot-
strapping dialog systems with word embeddings.
In NIPS, Modern Machine Learning and Natural
Language Processing Workshop.

John J Godfrey and Edward Holliman. 1997.
Switchboard-1 release 2. Linguistic Data Consor-
tium, Philadelphia .

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Diederik P Kingma and Max Welling. 2013. Auto-
arXiv preprint

encoding variational bayes.
arXiv:1312.6114 .

Massimo Poesio and David Traum. 1998. Towards an
axiomatization of dialogue acts. In Proceedings of
the Twente Workshop on the Formal Semantics and
Pragmatics of Dialogues (13th Twente Workshop on
Language Technology. Citeseer.

Antoine Raux, Brian Langner, Dan Bohus, Alan W
Black, and Maxine Eskenazi. 2005. Lets go pub-
lic! taking a spoken dialog system to the real world.
In in Proc. of Interspeech 2005. Citeseer.

Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Stochastic backpropagation and
Wierstra. 2014.
approximate inference in deep generative models.
arXiv preprint arXiv:1401.4082 .

Eug´enio Ribeiro, Ricardo Ribeiro, and David Mar-
The inﬂuence of con-
arXiv preprint

tins de Matos. 2015.
text on dialogue act recognition.
arXiv:1506.00839 .

Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval.
Information processing & management 24(5):513–
523.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing 45(11):2673–2681.

Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016a. Building
end-to-end dialogue systems using generative hier-
archical neural network models. In Proceedings of
the 30th AAAI Conference on Artiﬁcial Intelligence
(AAAI-16).

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron Courville,
and Yoshua Bengio. 2016b. A hierarchical latent
variable encoder-decoder model for generating di-
alogues. arXiv preprint arXiv:1605.06069 .

Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015.
Learning structured output
representation using
In Advances
deep conditional generative models.
in Neural Information Processing Systems. pages
3483–3491.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. arXiv preprint
arXiv:1506.06714 .

Andreas Stolcke, Noah Coccaro, Rebecca Bates, Paul
Taylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-
beth Shriberg, Daniel Jurafsky, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for au-
tomatic tagging and recognition of conversational
speech. Computational linguistics 26(3):339–373.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.

Johan AK Suykens and Joos Vandewalle. 1999. Least
squares support vector machine classiﬁers. Neural
processing letters 9(3):293–300.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869 .

Jason D Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken
dialog systems. Computer Speech & Language
21(2):393–422.

Sam Wiseman and Alexander M Rush. 2016.
Sequence-to-sequence learning as beam-search op-
timization. arXiv preprint arXiv:1606.02960 .

Chen Xing, Wei Wu, Yu Wu,

Jie Liu, Yalou
Huang, Ming Zhou, and Wei-Ying Ma. 2016.
Topic augmented neural response generation with
arXiv preprint
a joint attention mechanism.
arXiv:1606.08340 .

Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak
Lee. 2015. Attribute2image: Conditional image
arXiv preprint
generation from visual attributes.
arXiv:1512.00570 .

Zhou Yu, Ziyu Xu, Alan W Black, and Alex I Rud-
nicky. 2016. Strategy and policy learning for non-
In 17th An-
task-oriented conversational systems.
nual Meeting of the Special Interest Group on Dis-
course and Dialogue. volume 2, page 7.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
Recurrent neural network regularization.

2014.
arXiv preprint arXiv:1409.2329 .

Tiancheng Zhao and Maxine Eskenazi. 2016. To-
wards end-to-end learning for dialog state tracking
and management using deep reinforcement learning.
arXiv preprint arXiv:1606.02560 .

A Supplemental Material

Variational Lower Bound for kgCVAE

We assume that even with the presence of linguis-
tic feature y regarding x, the prediction of xbow
still only depends on the z and c. Therefore, we
have:

L(θ, φ; x, c, y) = −KL(qφ(z|x, c, y)(cid:107)Pθ(z|c))

+ Eqφ(z|c,x,y)[log p(x|z, c, y)]
+ Eqφ(z|c,x,y)[log p(y|z, c)]
+ Eqφ(z|c,x,y)[log p(xbow|z, c)]

(7)

Collection of Multiple Reference Responses

We collected multiple reference responses for each
dialog context in the test set by information re-
trieval techniques combining with traditional a
machine learning method. First, we encode the di-
alog history using Term Frequency-Inverse Doc-
ument Frequency (TFIDF) (Salton and Buckley,
1988) weighted bag-of-words into vector repre-
sentation h. Then we denote the topic of the con-
versation as t and denote f as the conversation
if the speakers of the last utterance in
ﬂoor, i.e.
the dialog history and response utterance are the
same f = 1 otherwise f = 0. Then we com-
puted the similarity d(ci, cj) between two dialog
contexts using:

d(ci, cj) = 1(ti = tj)1(ti = tj)

hi · hj
||hi||||hj||

(8)

Unlike past work (Sordoni et al., 2015), this sim-
ilarity function only cares about the distance in

the context and imposes no constraints on the re-
sponse, therefore is suitbale for ﬁnding diverse re-
sponses regarding to the same dialog context. Sec-
ondly, for each dialog context in the test set, we
retrieved the 10 nearest neighbors from the train-
ing set and treated the responses from the training
set as candidate reference responses. Thirdly, we
further sampled 240 context-responses pairs from
5481 pairs in the total test set and post-processed
the selected candidate responses by two human
computational linguistic experts who were told to
give a binary label for each candidate response
about whether the response is appropriate regard-
ing its dialog context. The ﬁltered lists then served
as the ground truth to train our reference response
classiﬁer. For the next step, we extracted bigrams,
part-of-speech bigrams and word part-of-speech
pairs from both dialogue contexts and candidate
reference responses with rare threshold for feature
extraction being set to 20. Then L2-regularized
logistic regression with 10-fold cross validation
was applied as the machine learning algorithm.
Cross validation accuracy on the human-labelled
data was 71%. Finally, we automatically anno-
tated the rest of test set with this trained classiﬁer
and the resulting data were used for model evalu-
ation.

Learning Discourse-level Diversity for Neural Dialog Models using
Conditional Variational Autoencoders

Tiancheng Zhao, Ran Zhao and Maxine Eskenazi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, Pennsylvania, USA
{tianchez,ranzhao1,max+}@cs.cmu.edu

Abstract

recent neural

While
encoder-decoder
models have shown great promise in mod-
they
eling open-domain conversations,
often generate dull and generic responses.
Unlike past work that has focused on
diversifying the output of the decoder
at word-level to alleviate this problem,
we present a novel framework based on
conditional variational autoencoders that
captures the discourse-level diversity in
the encoder. Our model uses latent vari-
ables to learn a distribution over potential
conversational
generates
intents
diverse responses using only greedy de-
coders. We have further developed a novel
variant that is integrated with linguistic
prior knowledge for better performance.
Finally, the training procedure is improved
by introducing a bag-of-word loss. Our
proposed models have been validated
to generate signiﬁcantly more diverse
responses than baseline approaches and
exhibit competence in discourse-level
decision-making.1

and

Introduction

1
The dialog manager is one of the key components
of dialog systems, which is responsible for mod-
eling the decision-making process. Speciﬁcally, it
typically takes a new utterance and the dialog con-
text as input, and generates discourse-level deci-
sions (Bohus and Rudnicky, 2003; Williams and
Young, 2007). Advanced dialog managers usu-
ally have a list of potential actions that enable
them to have diverse behavior during a conver-
sation, e.g. different strategies to recover from
non-understanding (Yu et al., 2016). However,

1Data and an implementation of our model is avalaible at

https://github.com/snakeztc/NeuralDialog-CVAE

recent

the conventional approach of designing a dialog
manager (Williams and Young, 2007) does not
scale well to open-domain conversation models
because of the vast quantity of possible decisions.
Thus, there has been a growing interest in applying
encoder-decoder models (Sutskever et al., 2014)
for modeling open-domain conversation (Vinyals
and Le, 2015; Serban et al., 2016a). The basic ap-
proach treats a conversation as a transduction task,
in which the dialog history is the source sequence
and the next response is the target sequence. The
model is then trained end-to-end on large conver-
sation corpora using the maximum-likelihood esti-
mation (MLE) objective without the need for man-
ual crafting.
However

found that
encoder-decoder models tend to generate generic
and dull responses (e.g., I don’t know), rather
than meaningful and speciﬁc answers (Li et al.,
2015; Serban et al., 2016b). There have been
many attempts to explain and solve this limita-
tion, and they can be broadly divided into two cat-
egories (see Section 2 for details): (1) the ﬁrst cat-
egory argues that the dialog history is only one of
the factors that decide the next response. Other
features should be extracted and provided to the
models as conditionals in order to generate more
speciﬁc responses (Xing et al., 2016; Li et al.,
2016a); (2) the second category aims to improve
the encoder-decoder model itself, including de-
coding with beam search and its variations (Wise-
man and Rush, 2016), encouraging responses that
have long-term payoff (Li et al., 2016b), etc.

research has

Building upon the past work in dialog managers
and encoder-decoder models, the key idea of this
paper is to model dialogs as a one-to-many prob-
lem at the discourse level. Previous studies indi-
cate that there are many factors in open-domain
dialogs that decide the next response, and it is non-
Intuitively, given
trivial to extract all of them.

7
1
0
2
 
t
c
O
 
1
2
 
 
]
L
C
.
s
c
[
 
 
3
v
0
6
9
0
1
.
3
0
7
1
:
v
i
X
r
a

a similar dialog history (and other observed in-
puts), there may exist many valid responses (at the
discourse-level), each corresponding to a certain
conﬁguration of the latent variables that are not
presented in the input. To uncover the potential re-
sponses, we strive to model a probabilistic distri-
bution over the distributed utterance embeddings
of the potential responses using a latent variable
(Figure 1). This allows us to generate diverse re-
sponses by drawing samples from the learned dis-
tribution and reconstruct their words via a decoder
neural network.

Figure 1: Given A’s question, there exists many
valid responses from B for different assumptions
of the latent variables, e.g., B’s hobby.

Speciﬁcally, our contributions are three-fold:
1. We present a novel neural dialog model
adapted from conditional variational autoencoders
(CVAE) (Yan et al., 2015; Sohn et al., 2015),
which introduces a latent variable that can cap-
ture discourse-level variations as described above
2. We propose Knowledge-Guided CVAE (kgC-
VAE), which enables easy integration of expert
knowledge and results in performance improve-
ment and model interpretability. 3. We develop
a training method in addressing the difﬁculty of
optimizing CVAE for natural language genera-
tion (Bowman et al., 2015). We evaluate our
models on human-human conversation data and
yield promising results in: (a) generating appro-
priate and discourse-level diverse responses, and
(b) showing that the proposed training method is
more effective than the previous techniques.

2 Related Work

Our work is related to both recent advancement
in encoder-decoder dialog models and generative
models based on CVAE.

2.1 Encoder-decoder Dialog Models
Since the emergence of the neural dialog model,
the problem of output diversity has received much
attention in the research community.
Ideal out-
put responses should be both coherent and diverse.
However, most models end up with generic and
dull responses. To tackle this problem, one line
of research has focused on augmenting the in-

put of encoder-decoder models with richer con-
text information, in order to generate more spe-
ciﬁc responses. Li et al., (2016a) captured speak-
ers’ characteristics by encoding background infor-
mation and speaking style into the distributed em-
beddings, which are used to re-rank the generated
response from an encoder-decoder model. Xing et
al., (2016) maintain topic encoding based on La-
tent Dirichlet Allocation (LDA) (Blei et al., 2003)
of the conversation to encourage the model to out-
put more topic coherent responses.

On the other hand, many attempts have also
been made to improve the architecture of encoder-
decoder models. Li et al,. (2015) proposed to opti-
mize the standard encoder-decoder by maximizing
the mutual information between input and output,
which in turn reduces generic responses. This ap-
proach penalized unconditionally high frequency
responses, and favored responses that have high
conditional probability given the input. Wiseman
and Rush (2016) focused on improving the de-
coder network by alleviating the biases between
training and testing. They introduced a search-
based loss that directly optimizes the networks
for beam search decoding. The resulting model
achieves better performance on word ordering,
parsing and machine translation. Besides improv-
ing beam search, Li et al., (2016b) pointed out that
the MLE objective of an encoder-decoder model
is unable to approximate the real-world goal of
the conversation. Thus, they initialized a encoder-
decoder model with MLE objective and leveraged
reinforcement learning to ﬁne tune the model by
optimizing three heuristic rewards functions: in-
formativity, coherence, and ease of answering.

2.2 Conditional Variational Autoencoder
The variational autoencoder (VAE) (Kingma and
Welling, 2013; Rezende et al., 2014) is one of the
most popular frameworks for image generation.
The basic idea of VAE is to encode the input x
into a probability distribution z instead of a point
encoding in the autoencoder. Then VAE applies a
decoder network to reconstruct the original input
using samples from z. To generate images, VAE
ﬁrst obtains a sample of z from the prior distribu-
tion, e.g. N (0, I), and then produces an image via
the decoder network. A more advanced model, the
conditional VAE (CVAE), is a recent modiﬁcation
of VAE to generate diverse images conditioned on
certain attributes, e.g. generating different human
faces given skin color (Yan et al., 2015; Sohn et al.,

2015). Inspired by CVAE, we view the dialog con-
texts as the conditional attributes and adapt CVAE
to generate diverse responses instead of images.

Although VAE/CVAE has achieved impressive
results in image generation, adapting this to natu-
ral language generators is non-trivial. Bowman et
al., (2015) have used VAE with Long-Short Term
Memory (LSTM)-based recognition and decoder
networks to generate sentences from a latent Gaus-
sian variable. They showed that their model is able
to generate diverse sentences with even a greedy
LSTM decoder. They also reported the difﬁculty
of training because the LSTM decoder tends to ig-
nore the latent variable. We refer to this issue as
the vanishing latent variable problem. Serban et
al., (2016b) have applied a latent variable hierar-
chical encoder-decoder dialog model to introduce
utterance-level variations and facilitate longer re-
sponses. To improve upon the past models, we
ﬁrstly introduce a novel mechanism to leverage
linguistic knowledge in training end-to-end neural
dialog models, and we also propose a novel train-
ing technique that mitigates the vanishing latent
variable problem.

3 Proposed Models

Figure 2: Graphical models of CVAE (a) and kgC-
VAE (b)

3.1 Conditional Variational Autoencoder
(CVAE) for Dialog Generation

Each dyadic conversation can be represented via
three random variables: the dialog context c (con-
text window size k − 1), the response utterance x
(the kth utterance) and a latent variable z, which
is used to capture the latent distribution over the
valid responses. Further, c is composed of the dia-
log history: the preceding k-1 utterances; conver-
sational ﬂoor (1 if the utterance is from the same
speaker of x, otherwise 0) and meta features m
(e.g. the topic). We then deﬁne the conditional dis-
tribution p(x, z|c) = p(x|z, c)p(z|c) and our goal
is to use deep neural networks (parametrized by θ)
to approximate p(z|c) and p(x|z, c). We refer to

pθ(z|c) as the prior network and pθ(x, |z, c) as the
response decoder. Then the generative process of
x is (Figure 2 (a)):

1. Sample a latent variable z from the prior net-

work pθ(z|c).

pθ(x|z, c).

2. Generate x through the response decoder

CVAE is trained to maximize the conditional
log likelihood of x given c, which involves an in-
tractable marginalization over the latent variable
z. As proposed in (Sohn et al., 2015; Yan et al.,
2015), CVAE can be efﬁciently trained with the
Stochastic Gradient Variational Bayes (SGVB)
framework (Kingma and Welling, 2013) by maxi-
mizing the variational lower bound of the condi-
tional log likelihood. We assume the z follows
multivariate Gaussian distribution with a diago-
nal covariance matrix and introduce a recognition
network qφ(z|x, c) to approximate the true poste-
rior distribution p(z|x, c). Sohn and et al,. (2015)
have shown that the variational lower bound can
be written as:

L(θ, φ; x, c) = −KL(qφ(z|x, c)(cid:107)pθ(z|c))

+ Eqφ(z|c,x)[log pθ(x|z, c)]
≤ log p(x|c)

(1)

Figure 3 demonstrates an overview of our model.
The utterance encoder is a bidirectional recurrent
neural network (BRNN) (Schuster and Paliwal,
1997) with a gated recurrent unit (GRU) (Chung
et al., 2014) to encode each utterance into ﬁxed-
size vectors by concatenating the last hidden states
of the forward and backward RNN ui = [ (cid:126)hi, (cid:126)hi].
x is simply uk. The context encoder is a 1-layer
GRU network that encodes the preceding k-1 ut-
terances by taking u1:k−1 and the corresponding
conversation ﬂoor as inputs. The last hidden state
hc of the context encoder is concatenated with
meta features and c = [hc, m]. Since we assume z
follows isotropic Gaussian distribution, the recog-
nition network qφ(z|x, c) ∼ N (µ, σ2I) and the
prior network pθ(z|c) ∼ N (µ(cid:48), σ(cid:48)2I), and then we
have:

(cid:20)

(cid:21)
µ
log(σ2)
(cid:21)
µ(cid:48)
log(σ(cid:48)2)

(cid:20)

= Wr

+ br

(cid:21)

(cid:20)x
c

= MLPp(c)

(2)

(3)

Figure 3: The neural network architectures for the baseline and the proposed CVAE/kgCVAE models.
(cid:76) denotes the concatenation of the input vectors. The dashed blue connections only appear in kgCVAE.

We then use the reparametrization trick (Kingma
and Welling, 2013) to obtain samples of z either
from N (z; µ, σ2I) predicted by the recognition
network (training) or N (z; µ(cid:48), σ(cid:48)2I) predicted by
the prior network (testing). Finally, the response
decoder is a 1-layer GRU network with initial state
s0 = Wi[z, c]+bi. The response decoder then pre-
dicts the words in x sequentially.

3.2 Knowledge-Guided CVAE (kgCVAE)
In practice, training CVAE is a challenging opti-
mization problem and often requires large amount
of data. On the other hand, past research in spo-
ken dialog systems and discourse analysis has sug-
gested that many linguistic cues capture crucial
features in representing natural conversation. For
example, dialog acts (Poesio and Traum, 1998)
have been widely used in the dialog managers (Lit-
man and Allen, 1987; Raux et al., 2005; Zhao
and Eskenazi, 2016) to represent the propositional
function of the system. Therefore, we conjecture
that it will be beneﬁcial for the model to learn
meaningful latent z if it is provided with explicitly
extracted discourse features during the training.

In order to incorporate the linguistic features
into the basic CVAE model, we ﬁrst denote the set
of linguistic features as y. Then we assume that
the generation of x depends on c, z and y. y re-
lies on z and c as shown in Figure 2. Speciﬁcally,
during training the initial state of the response de-
coder is s0 = Wi[z, c, y] + bi and the input at ev-
ery step is [et, y] where et is the word embedding
of tth word in x. In addition, there is an MLP to
predict y(cid:48) = MLPy(z, c) based on z and c. In the
testing stage, the predicted y(cid:48) is used by the re-

sponse decoder instead of the oracle decoders. We
denote the modiﬁed model as knowledge-guided
CVAE (kgCVAE) and developers can add desired
discourse features that they wish the latent vari-
able z to capture. KgCVAE model is trained by
maximizing:

L(θ, φ; x, c, y) = −KL(qφ(z|x, c, y)(cid:107)Pθ(z|c))

+ Eqφ(z|c,x,y)[log p(x|z, c, y)]
+ Eqφ(z|c,x,y)[log p(y|z, c)]

(4)

Since now the reconstruction of y is a part of the
loss function, kgCVAE can more efﬁciently en-
code y-related information into z than discovering
it only based on the surface-level x and c. Another
advantage of kgCVAE is that it can output a high-
level label (e.g. dialog act) along with the word-
level responses, which allows easier interpretation
of the model’s outputs.

3.3 Optimization Challenges
A straightforward VAE with RNN decoder fails
to encode meaningful information in z due to the
vanishing latent variable problem (Bowman et al.,
2015). Bowman et al., (2015) proposed two solu-
tions: (1) KL annealing: gradually increasing the
weight of the KL term from 0 to 1 during training;
(2) word drop decoding: setting a certain percent-
age of the target words to 0. We found that CVAE
suffers from the same issue when the decoder is
an RNN. Also we did not consider word drop de-
coding because Bowman et al,. (2015) have shown
that it may hurt the performance when the drop
rate is too high.

As a result, we propose a simple yet novel tech-
nique to tackle the vanishing latent variable prob-
lem: bag-of-word loss. The idea is to introduce
an auxiliary loss that requires the decoder network
to predict the bag-of-words in the response x as
shown in Figure 3(b). We decompose x into two
variables: xo with word order and xbow without
order, and assume that xo and xbow are condi-
tionally independent given z and c: p(x, z|c) =
p(xo|z, c)p(xbow|z, c)p(z|c). Due to the condi-
tional independence assumption, the latent vari-
able is forced to capture global information about
the target response. Let f = MLPb(z, x) ∈ RV
where V is vocabulary size, and we have:

log p(xbow|z, c) = log

(5)

|x|
(cid:89)

t=1

efxt
j efj

(cid:80)V

where |x| is the length of x and xt is the word
index of tth word in x. The modiﬁed variational
lower bound for CVAE with bag-of-word loss is
(see Appendix A for kgCVAE):

L(cid:48)(θ, φ; x, c) = L(θ, φ; x, c)

+ Eqφ(z|c,x,y)[log p(xbow|z, c)] (6)

We will show that the bag-of-word loss in Equa-
tion 6 is very effective against the vanishing latent
variable and it is also complementary to the KL
annealing technique.

4 Experiment Setup
4.1 Dataset

We chose the Switchboard (SW) 1 Release 2 Cor-
pus (Godfrey and Holliman, 1997) to evaluate the
proposed models. SW has 2400 two-sided tele-
phone conversations with manually transcribed
speech and alignment.
In the beginning of the
call, a computer operator gave the callers recorded
prompts that deﬁne the desired topic of discus-
sion. There are 70 available topics. We ran-
domly split the data into 2316/60/62 dialogs for
train/validate/test. The pre-processing includes (1)
tokenize using the NLTK tokenizer (Bird et al.,
2009); (2) remove non-verbal symbols and re-
peated words due to false starts; (3) keep the
top 10K frequent word types as the vocabulary.
The ﬁnal data have 207, 833/5, 225/5, 481 (c, x)
pairs for train/validate/test. Furthermore, a sub-
set of SW was manually labeled with dialog
acts (Stolcke et al., 2000). We extracted dia-
log act labels based on the dialog act recognizer

proposed in (Ribeiro et al., 2015). The features
include the uni-gram and bi-gram of the utter-
ance, and the contextual features of the last 3 ut-
terances. We trained a Support Vector Machine
(SVM) (Suykens and Vandewalle, 1999) with lin-
ear kernel on the subset of SW with human anno-
tations. There are 42 types of dialog acts and the
SVM achieved 77.3% accuracy on held-out data.
Then the rest of SW data are labelled with dialog
acts using the trained SVM dialog act recognizer.

4.2 Training

We trained with the following hyperparameters
(according to the loss on the validate dataset):
word embedding has size 200 and is shared
across everywhere. We initialize the word embed-
ding from Glove embedding pre-trained on Twit-
ter (Pennington et al., 2014). The utterance en-
coder has a hidden size of 300 for each direc-
tion. The context encoder has a hidden size of
600 and the response decoder has a hidden size
of 400. The prior network and the MLP for pre-
dicting y both have 1 hidden layer of size 400 and
tanh non-linearity. The latent variable z has a
size of 200. The context window k is 10. All
the initial weights are sampled from a uniform
distribution [-0.08, 0.08]. The mini-batch size is
30. The models are trained end-to-end using the
Adam optimizer (Kingma and Ba, 2014) with a
learning rate of 0.001 and gradient clipping at 5.
We selected the best models based on the varia-
tional lower bound on the validate data. Finally,
we use the BOW loss along with KL annealing of
10,000 batches to achieve the best performance.
Section 5.4 gives a detailed argument for the im-
portance of the BOW loss.

5 Results
5.1 Experiments Setup

We compared three neural dialog models: a strong
baseline model, CVAE, and kgCVAE. The base-
line model is an encoder-decoder neural dialog
model without latent variables similar to (Serban
et al., 2016a). The baseline model’s encoder uses
the same context encoder to encode the dialog his-
tory and the meta features as shown in Figure 3.
The encoded context c is directly fed into the de-
coder networks as the initial state. The hyperpa-
rameters of the baseline are the same as the ones
reported in Section 4.2 and the baseline is trained
to minimize the standard cross entropy loss of the
decoder RNN model without any auxiliary loss.

Also, to compare the diversity introduced by the
stochasticity in the proposed latent variable ver-
sus the softmax of RNN at each decoding step, we
generate N responses from the baseline by sam-
pling from the softmax. For CVAE/kgCVAE, we
sample N times from the latent z and only use
greedy decoders so that the randomness comes en-
tirely from the latent variable z.

5.2 Quantitative Analysis

Automatically evaluating an open-domain gen-
erative dialog model is an open research chal-
lenge (Liu et al., 2016). Following our one-to-
many hypothesis, we propose the following met-
rics. We assume that for a given dialog context c,
there exist Mc reference responses rj, j ∈ [1, Mc].
Meanwhile a model can generate N hypothesis re-
sponses hi, i ∈ [1, N ]. The generalized response-
level precision/recall for a given dialog context is:

3. Dialog Act Match:

to measure the similar-
ity at the discourse level, the same dialog-
act tagger from 4.1 is applied to label all the
generated responses of each model. We set
d(rj, hi) = 1 if rj and hi have the same dia-
log acts, otherwise d(rj, hi) = 0.

One challenge of using the above metrics is that
there is only one, rather than multiple reference
responses/contexts. This impacts reliability of our
measures. Inspired by (Sordoni et al., 2015), we
utilized information retrieval techniques (see Ap-
pendix A) to gather 10 extra candidate reference
responses/context from other conversations with
the same topics. Then the 10 candidate references
are ﬁltered by two experts, which serve as the
ground truth to train the reference response classi-
ﬁer. The result is 6.69 extra references in average
per context. The average number of distinct refer-
ence dialog acts is 4.2. Table 1 shows the results.

precision(c) =

recall(c) =

(cid:80)N

i=1 maxj∈[1,Mc]d(rj, hi)
N
j=1 maxi∈[1,N ]d(rj, hi))
Mc

(cid:80)Mc

where d(rj, hi) is a distance function which lies
between 0 to 1 and measures the similarities be-
tween rj and hi. The ﬁnal score is averaged over
the entire test dataset and we report the perfor-
mance with 3 types of distance functions in or-
der to evaluate the systems from various linguistic
points of view:

1. Smoothed Sentence-level BLEU (Chen and
Cherry, 2014): BLEU is a popular metric that
measures the geometric mean of modiﬁed n-
gram precision with a length penalty (Pap-
ineni et al., 2002; Li et al., 2015). We use
BLEU-1 to 4 as our lexical similarity metric
and normalize the score to 0 to 1 scale.

2. Cosine Distance of Bag-of-word Embed-
ding: a simple method to obtain sentence
embeddings is to take the average or ex-
trema of all the word embeddings in the sen-
tences (Forgues et al., 2014; Adi et al., 2016).
The d(rj, hi) is the cosine distance of the two
embedding vectors. We used Glove embed-
ding described in Section 4 and denote the av-
erage method as A-bow and extrema method
as E-bow. The score is normalized to [0, 1].

Metrics
perplexity (KL)

BLEU-1 prec
BLEU-1 recall
BLEU-2 prec
BLEU-2 recall
BLEU-3 prec
BLEU-3 recall
BLEU-4 prec
BLEU-4 recall
A-bow prec
A-bow recall
E-bow prec
E-bow recall
DA prec
DA recall

Baseline CVAE
35.4
(n/a)
0.405
0.336
0.300
0.281
0.272
0.254
0.226
0.215
0.951
0.935
0.827
0.801
0.736
0.514

20.2
(11.36)
0.372
0.381
0.295
0.322
0.265
0.292
0.223
0.248
0.954
0.943
0.815
0.812
0.704
0.604

kgCVAE
16.02
(13.08)
0.412
0.411
0.350
0.356
0.310
0.318
0.262
0.272
0.961
0.944
0.804
0.807
0.721
0.598

Table 1: Performance of each model on automatic
measures. The highest score in each row is in
bold. Note that our BLEU scores are normalized
to [0, 1].

The proposed models outperform the baseline
in terms of recall in all the metrics with statis-
tical signiﬁcance. This conﬁrms our hypothesis
that generating responses with discourse-level di-
versity can lead to a more comprehensive cov-
erage of the potential responses than promoting
only word-level diversity. As for precision, we
observed that the baseline has higher or similar

scores than CVAE in all metrics, which is expected
since the baseline tends to generate the mostly
likely and safe responses repeatedly in the N hy-
potheses. However, kgCVAE is able to achieve
the highest precision and recall in the 4 metrics at
the same time (BLEU1-4, A-BOW). One reason
for kgCVAE’s good performance is that the pre-
dicted dialog act label in kgCVAE can regularize
the generation process of its RNN decoder by forc-
ing it to generate more coherent and precise words.
We further analyze the precision/recall of BLEU-
4 by looking at the average score versus the num-
ber of distinct reference dialog acts. A low num-
ber of distinct dialog acts represents the situation
where the dialog context has a strong constraint
on the range of the next response (low entropy),
while a high number indicates the opposite (high-
entropy). Figure 4 shows that CVAE/kgCVAE
achieves signiﬁcantly higher recall than the base-
line in higher entropy contexts. Also it shows
that CVAE suffers from lower precision, espe-
cially in low entropy contexts. Finally, kgCVAE
gets higher precision than both the baseline and
CVAE in the full spectrum of context entropy.

able to generate various ways of back-channeling.
This implies that the latent z is able to capture
context-sensitive variations, i.e. in low-entropy di-
alog contexts modeling lexical diversity while in
high-entropy ones modeling discourse-level diver-
sity. Moreover, kgCVAE is occasionally able to
generate more sophisticated grounding (sample 4)
beyond a simple back-channel, which is also an
acceptable response given the dialog context.

In addition, past work (Kingma and Welling,
2013) has shown that the recognition network is
able to learn to cluster high-dimension data, so
we conjecture that posterior z outputted from the
recognition network should cluster the responses
into meaningful groups. Figure 5 visualizes the
posterior z of responses in the test dataset in 2D
space using t-SNE (Maaten and Hinton, 2008).
We found that the learned latent space is highly
correlated with the dialog act and length of re-
sponses, which conﬁrms our assumption.

Figure 4: BLEU-4 precision/recall vs. the number
of distinct reference dialog acts.

5.3 Qualitative Analysis

Table 2 shows the outputs generated from the
baseline and kgCVAE. In example 1, caller A be-
gins with an open-ended question. The kgCVAE
model generated highly diverse answers that cover
multiple plausible dialog acts. Further, we notice
that the generated text exhibits similar dialog acts
compared to the ones predicted separately by the
model, implying the consistency of natural lan-
guage generation based on y. On the contrary, the
responses from the baseline model are limited to
local n-gram variations and share a similar preﬁx,
i.e. ”I’m”. Example 2 is a situation where caller
A is telling B stories. The ground truth response
is a back-channel and the range of valid answers is
more constrained than example 1 since B is play-
ing the role of a listener. The baseline successfully
predicts ”uh-huh”. The kgCVAE model is also

Figure 5: t-SNE visualization of the posterior z for
test responses with top 8 frequent dialog acts. The
size of circle represents the response length.

5.4 Results for Bag-of-Word Loss

Finally, we evaluate the effectiveness of bag-of-
word (BOW) loss for training VAE/CVAE with the
RNN decoder. To compare with past work (Bow-
man et al., 2015), we conducted the same lan-
guage modelling (LM) task on Penn Treebank us-
ing VAE. The network architecture is same ex-
cept we use GRU instead of LSTM. We compared
four different training setups: (1) standard VAE
without any heuristics; (2) VAE with KL anneal-
ing (KLA); (3) VAE with BOW loss; (4) VAE
with both BOW loss and KLA. Intuitively, a well
trained model should lead to a low reconstruction

Example 1-Topic: Recycling Context: A: are they doing a lot of recycling out in Georgia? Target-B (statement): well
at my workplace we have palaces for aluminium cans and we have a separate trash can for recyclable paper
Baseline+Sampling
1. well I’m a graduate student and have two kids
2. well I was in last year and so we’ve had lots of recycling

kgCVAE+Greedy
1. (non-understand) pardon
2. (statement) oh you’re not going to have a curbside pick
up here
3. (statement) okay I am sure about a recycling center
4. (yes-answer) yeah so

3. I’m not sure
4. well I don’t know I just moved here in new york

Example 2-Topic: Child Care Context: A: you know a private home to take their children to when they’re young until
they hit the preschool age and they Target-B (backchannel): uh-huh
Baseline+Sampling
1. um - hum
2. yeah
3. um - hum
4. uh-huh

kgCVAE+Greedy
1. (backchannel) uh-huh
2. (turn-exit) um-hum
3. (backchannel) yeah
4. (statement) oh yeah I think that’s part of the problem

Table 2: Generated responses from the baselines and kgCVAE in two examples. KgCVAE also provides
the predicted dialog act for each response. The context only shows the last utterance due to space limit
(the actual context window size is 10).

portance of BOW loss for training latent vari-
able models with the RNN decoder. Last but not
least, our experiments showed that the conclusions
drawn from LM using VAE also apply to training
CVAE/kgCVAE, so we used BOW loss together
with KLA for all previous experiments.

loss and small but non-trivial KL cost. For all
models with KLA, the KL weight increases lin-
early from 0 to 1 in the ﬁrst 5000 batches.

Table 3 shows the reconstruction perplexity and
the KL cost on the test dataset. The standard VAE
fails to learn a meaningful latent variable by hav-
ing a KL cost close to 0 and a reconstruction per-
plexity similar to a small LSTM LM (Zaremba
et al., 2014). KLA helps to improve the recon-
struction loss, but it requires early stopping since
the models will fall back to the standard VAE after
the KL weight becomes 1. At last, the models with
BOW loss achieved signiﬁcantly lower perplexity
and larger KL cost.

Perplexity
Model
122.0
Standard
111.5
KLA
BOW
97.72
BOW+KLA 73.04

KL cost
0.05
2.02
7.41
15.94

Table 3: The reconstruction perplexity and KL
terms on Penn Treebank test set.

Figure 6 visualizes the evolution of the KL cost.
We can see that for the standard model, the KL
cost crashes to 0 at the beginning of training and
never recovers. On the contrary, the model with
only KLA learns to encode substantial informa-
tion in latent z when the KL cost weight is small.
However, after the KL weight is increased to 1 (af-
ter 5000 batch), the model once again decides to
ignore the latent z and falls back to the naive im-
plementation. The model with BOW loss, how-
ever, consistently converges to a non-trivial KL
cost even without KLA, which conﬁrms the im-

Figure 6: The value of the KL divergence during
training with different setups on Penn Treebank.

6 Conclusion and Future Work
In conclusion, we identiﬁed the one-to-many na-
ture of open-domain conversation and proposed
two novel models that show superior performance
in generating diverse and appropriate responses at
the discourse level. While the current paper ad-
dresses diversifying responses in respect to dia-
logue acts, this work is part of a larger research
direction that targets leveraging both past linguis-
tic ﬁndings and the learning power of deep neural
networks to learn better representation of the la-
tent factors in dialog. In turn, the output of this
novel neural dialog model will be easier to ex-
plain and control by humans.
In addition to di-
alog acts, we plan to apply our kgCVAE model

to capture other different linguistic phenomena in-
cluding sentiment, named entities,etc. Last but
not least, the recognition network in our model
will serve as the foundation for designing a data-
driven dialog manager, which automatically dis-
covers useful high-level intents. All of the above
suggest a promising research direction.

7 Acknowledgements

This work was funded by NSF grant CNS-
1512973. The opinions expressed in this paper do
not necessarily reﬂect those of NSF.

References

Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer
Lavi, and Yoav Goldberg. 2016. Fine-grained anal-
ysis of sentence embeddings using auxiliary predic-
tion tasks. arXiv preprint arXiv:1608.04207 .

Steven Bird, Ewan Klein, and Edward Loper. 2009.
”

language processing with Python.

Natural
O’Reilly Media, Inc.”.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of ma-
chine Learning research 3(Jan):993–1022.

Dan Bohus and Alexander I Rudnicky. 2003. Raven-
claw: Dialog management using hierarchical task
decomposition and an expectation agenda .

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2015. A diversity-promoting objec-
tive function for neural conversation models. arXiv
preprint arXiv:1510.03055 .

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016a. A persona-based neural con-
versation model. arXiv preprint arXiv:1603.06155
.

Jiwei Li, Will Monroe, Alan Ritter, and Dan Jurafsky.
2016b. Deep reinforcement learning for dialogue
generation. arXiv preprint arXiv:1606.01541 .

Diane J Litman and James F Allen. 1987. A plan
recognition model for subdialogues in conversa-
tions. Cognitive science 11(2):163–200.

Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
2016. How not to evaluate your dialogue system:
An empirical study of unsupervised evaluation met-
rics for dialogue response generation. arXiv preprint
arXiv:1603.08023 .

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research 9(Nov):2579–2605.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics. Association for Computational
Linguistics, pages 311–318.

Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-
drew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. 2015. Generating sentences from a continuous
space. arXiv preprint arXiv:1511.06349 .

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP. volume 14, pages 1532–
43.

Boxing Chen and Colin Cherry. 2014. A systematic
comparison of smoothing techniques for sentence-
level bleu. ACL 2014 page 362.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555 .

Gabriel

Joelle

Pineau,

Forgues,

Jean-Marie
Larchevˆeque, and R´eal Tremblay. 2014. Boot-
strapping dialog systems with word embeddings.
In NIPS, Modern Machine Learning and Natural
Language Processing Workshop.

John J Godfrey and Edward Holliman. 1997.
Switchboard-1 release 2. Linguistic Data Consor-
tium, Philadelphia .

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Diederik P Kingma and Max Welling. 2013. Auto-
arXiv preprint

encoding variational bayes.
arXiv:1312.6114 .

Massimo Poesio and David Traum. 1998. Towards an
axiomatization of dialogue acts. In Proceedings of
the Twente Workshop on the Formal Semantics and
Pragmatics of Dialogues (13th Twente Workshop on
Language Technology. Citeseer.

Antoine Raux, Brian Langner, Dan Bohus, Alan W
Black, and Maxine Eskenazi. 2005. Lets go pub-
lic! taking a spoken dialog system to the real world.
In in Proc. of Interspeech 2005. Citeseer.

Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Stochastic backpropagation and
Wierstra. 2014.
approximate inference in deep generative models.
arXiv preprint arXiv:1401.4082 .

Eug´enio Ribeiro, Ricardo Ribeiro, and David Mar-
The inﬂuence of con-
arXiv preprint

tins de Matos. 2015.
text on dialogue act recognition.
arXiv:1506.00839 .

Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval.
Information processing & management 24(5):513–
523.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing 45(11):2673–2681.

Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016a. Building
end-to-end dialogue systems using generative hier-
archical neural network models. In Proceedings of
the 30th AAAI Conference on Artiﬁcial Intelligence
(AAAI-16).

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron Courville,
and Yoshua Bengio. 2016b. A hierarchical latent
variable encoder-decoder model for generating di-
alogues. arXiv preprint arXiv:1605.06069 .

Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015.
Learning structured output
representation using
In Advances
deep conditional generative models.
in Neural Information Processing Systems. pages
3483–3491.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. arXiv preprint
arXiv:1506.06714 .

Andreas Stolcke, Noah Coccaro, Rebecca Bates, Paul
Taylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-
beth Shriberg, Daniel Jurafsky, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for au-
tomatic tagging and recognition of conversational
speech. Computational linguistics 26(3):339–373.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.

Johan AK Suykens and Joos Vandewalle. 1999. Least
squares support vector machine classiﬁers. Neural
processing letters 9(3):293–300.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869 .

Jason D Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken
dialog systems. Computer Speech & Language
21(2):393–422.

Sam Wiseman and Alexander M Rush. 2016.
Sequence-to-sequence learning as beam-search op-
timization. arXiv preprint arXiv:1606.02960 .

Chen Xing, Wei Wu, Yu Wu,

Jie Liu, Yalou
Huang, Ming Zhou, and Wei-Ying Ma. 2016.
Topic augmented neural response generation with
arXiv preprint
a joint attention mechanism.
arXiv:1606.08340 .

Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak
Lee. 2015. Attribute2image: Conditional image
arXiv preprint
generation from visual attributes.
arXiv:1512.00570 .

Zhou Yu, Ziyu Xu, Alan W Black, and Alex I Rud-
nicky. 2016. Strategy and policy learning for non-
In 17th An-
task-oriented conversational systems.
nual Meeting of the Special Interest Group on Dis-
course and Dialogue. volume 2, page 7.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
Recurrent neural network regularization.

2014.
arXiv preprint arXiv:1409.2329 .

Tiancheng Zhao and Maxine Eskenazi. 2016. To-
wards end-to-end learning for dialog state tracking
and management using deep reinforcement learning.
arXiv preprint arXiv:1606.02560 .

A Supplemental Material

Variational Lower Bound for kgCVAE

We assume that even with the presence of linguis-
tic feature y regarding x, the prediction of xbow
still only depends on the z and c. Therefore, we
have:

L(θ, φ; x, c, y) = −KL(qφ(z|x, c, y)(cid:107)Pθ(z|c))

+ Eqφ(z|c,x,y)[log p(x|z, c, y)]
+ Eqφ(z|c,x,y)[log p(y|z, c)]
+ Eqφ(z|c,x,y)[log p(xbow|z, c)]

(7)

Collection of Multiple Reference Responses

We collected multiple reference responses for each
dialog context in the test set by information re-
trieval techniques combining with traditional a
machine learning method. First, we encode the di-
alog history using Term Frequency-Inverse Doc-
ument Frequency (TFIDF) (Salton and Buckley,
1988) weighted bag-of-words into vector repre-
sentation h. Then we denote the topic of the con-
versation as t and denote f as the conversation
if the speakers of the last utterance in
ﬂoor, i.e.
the dialog history and response utterance are the
same f = 1 otherwise f = 0. Then we com-
puted the similarity d(ci, cj) between two dialog
contexts using:

d(ci, cj) = 1(ti = tj)1(ti = tj)

hi · hj
||hi||||hj||

(8)

Unlike past work (Sordoni et al., 2015), this sim-
ilarity function only cares about the distance in

the context and imposes no constraints on the re-
sponse, therefore is suitbale for ﬁnding diverse re-
sponses regarding to the same dialog context. Sec-
ondly, for each dialog context in the test set, we
retrieved the 10 nearest neighbors from the train-
ing set and treated the responses from the training
set as candidate reference responses. Thirdly, we
further sampled 240 context-responses pairs from
5481 pairs in the total test set and post-processed
the selected candidate responses by two human
computational linguistic experts who were told to
give a binary label for each candidate response
about whether the response is appropriate regard-
ing its dialog context. The ﬁltered lists then served
as the ground truth to train our reference response
classiﬁer. For the next step, we extracted bigrams,
part-of-speech bigrams and word part-of-speech
pairs from both dialogue contexts and candidate
reference responses with rare threshold for feature
extraction being set to 20. Then L2-regularized
logistic regression with 10-fold cross validation
was applied as the machine learning algorithm.
Cross validation accuracy on the human-labelled
data was 71%. Finally, we automatically anno-
tated the rest of test set with this trained classiﬁer
and the resulting data were used for model evalu-
ation.

Learning Discourse-level Diversity for Neural Dialog Models using
Conditional Variational Autoencoders

Tiancheng Zhao, Ran Zhao and Maxine Eskenazi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, Pennsylvania, USA
{tianchez,ranzhao1,max+}@cs.cmu.edu

Abstract

recent neural

While
encoder-decoder
models have shown great promise in mod-
they
eling open-domain conversations,
often generate dull and generic responses.
Unlike past work that has focused on
diversifying the output of the decoder
at word-level to alleviate this problem,
we present a novel framework based on
conditional variational autoencoders that
captures the discourse-level diversity in
the encoder. Our model uses latent vari-
ables to learn a distribution over potential
conversational
generates
intents
diverse responses using only greedy de-
coders. We have further developed a novel
variant that is integrated with linguistic
prior knowledge for better performance.
Finally, the training procedure is improved
by introducing a bag-of-word loss. Our
proposed models have been validated
to generate signiﬁcantly more diverse
responses than baseline approaches and
exhibit competence in discourse-level
decision-making.1

and

Introduction

1
The dialog manager is one of the key components
of dialog systems, which is responsible for mod-
eling the decision-making process. Speciﬁcally, it
typically takes a new utterance and the dialog con-
text as input, and generates discourse-level deci-
sions (Bohus and Rudnicky, 2003; Williams and
Young, 2007). Advanced dialog managers usu-
ally have a list of potential actions that enable
them to have diverse behavior during a conver-
sation, e.g. different strategies to recover from
non-understanding (Yu et al., 2016). However,

1Data and an implementation of our model is avalaible at

https://github.com/snakeztc/NeuralDialog-CVAE

recent

the conventional approach of designing a dialog
manager (Williams and Young, 2007) does not
scale well to open-domain conversation models
because of the vast quantity of possible decisions.
Thus, there has been a growing interest in applying
encoder-decoder models (Sutskever et al., 2014)
for modeling open-domain conversation (Vinyals
and Le, 2015; Serban et al., 2016a). The basic ap-
proach treats a conversation as a transduction task,
in which the dialog history is the source sequence
and the next response is the target sequence. The
model is then trained end-to-end on large conver-
sation corpora using the maximum-likelihood esti-
mation (MLE) objective without the need for man-
ual crafting.
However

found that
encoder-decoder models tend to generate generic
and dull responses (e.g., I don’t know), rather
than meaningful and speciﬁc answers (Li et al.,
2015; Serban et al., 2016b). There have been
many attempts to explain and solve this limita-
tion, and they can be broadly divided into two cat-
egories (see Section 2 for details): (1) the ﬁrst cat-
egory argues that the dialog history is only one of
the factors that decide the next response. Other
features should be extracted and provided to the
models as conditionals in order to generate more
speciﬁc responses (Xing et al., 2016; Li et al.,
2016a); (2) the second category aims to improve
the encoder-decoder model itself, including de-
coding with beam search and its variations (Wise-
man and Rush, 2016), encouraging responses that
have long-term payoff (Li et al., 2016b), etc.

research has

Building upon the past work in dialog managers
and encoder-decoder models, the key idea of this
paper is to model dialogs as a one-to-many prob-
lem at the discourse level. Previous studies indi-
cate that there are many factors in open-domain
dialogs that decide the next response, and it is non-
Intuitively, given
trivial to extract all of them.

7
1
0
2
 
t
c
O
 
1
2
 
 
]
L
C
.
s
c
[
 
 
3
v
0
6
9
0
1
.
3
0
7
1
:
v
i
X
r
a

a similar dialog history (and other observed in-
puts), there may exist many valid responses (at the
discourse-level), each corresponding to a certain
conﬁguration of the latent variables that are not
presented in the input. To uncover the potential re-
sponses, we strive to model a probabilistic distri-
bution over the distributed utterance embeddings
of the potential responses using a latent variable
(Figure 1). This allows us to generate diverse re-
sponses by drawing samples from the learned dis-
tribution and reconstruct their words via a decoder
neural network.

Figure 1: Given A’s question, there exists many
valid responses from B for different assumptions
of the latent variables, e.g., B’s hobby.

Speciﬁcally, our contributions are three-fold:
1. We present a novel neural dialog model
adapted from conditional variational autoencoders
(CVAE) (Yan et al., 2015; Sohn et al., 2015),
which introduces a latent variable that can cap-
ture discourse-level variations as described above
2. We propose Knowledge-Guided CVAE (kgC-
VAE), which enables easy integration of expert
knowledge and results in performance improve-
ment and model interpretability. 3. We develop
a training method in addressing the difﬁculty of
optimizing CVAE for natural language genera-
tion (Bowman et al., 2015). We evaluate our
models on human-human conversation data and
yield promising results in: (a) generating appro-
priate and discourse-level diverse responses, and
(b) showing that the proposed training method is
more effective than the previous techniques.

2 Related Work

Our work is related to both recent advancement
in encoder-decoder dialog models and generative
models based on CVAE.

2.1 Encoder-decoder Dialog Models
Since the emergence of the neural dialog model,
the problem of output diversity has received much
attention in the research community.
Ideal out-
put responses should be both coherent and diverse.
However, most models end up with generic and
dull responses. To tackle this problem, one line
of research has focused on augmenting the in-

put of encoder-decoder models with richer con-
text information, in order to generate more spe-
ciﬁc responses. Li et al., (2016a) captured speak-
ers’ characteristics by encoding background infor-
mation and speaking style into the distributed em-
beddings, which are used to re-rank the generated
response from an encoder-decoder model. Xing et
al., (2016) maintain topic encoding based on La-
tent Dirichlet Allocation (LDA) (Blei et al., 2003)
of the conversation to encourage the model to out-
put more topic coherent responses.

On the other hand, many attempts have also
been made to improve the architecture of encoder-
decoder models. Li et al,. (2015) proposed to opti-
mize the standard encoder-decoder by maximizing
the mutual information between input and output,
which in turn reduces generic responses. This ap-
proach penalized unconditionally high frequency
responses, and favored responses that have high
conditional probability given the input. Wiseman
and Rush (2016) focused on improving the de-
coder network by alleviating the biases between
training and testing. They introduced a search-
based loss that directly optimizes the networks
for beam search decoding. The resulting model
achieves better performance on word ordering,
parsing and machine translation. Besides improv-
ing beam search, Li et al., (2016b) pointed out that
the MLE objective of an encoder-decoder model
is unable to approximate the real-world goal of
the conversation. Thus, they initialized a encoder-
decoder model with MLE objective and leveraged
reinforcement learning to ﬁne tune the model by
optimizing three heuristic rewards functions: in-
formativity, coherence, and ease of answering.

2.2 Conditional Variational Autoencoder
The variational autoencoder (VAE) (Kingma and
Welling, 2013; Rezende et al., 2014) is one of the
most popular frameworks for image generation.
The basic idea of VAE is to encode the input x
into a probability distribution z instead of a point
encoding in the autoencoder. Then VAE applies a
decoder network to reconstruct the original input
using samples from z. To generate images, VAE
ﬁrst obtains a sample of z from the prior distribu-
tion, e.g. N (0, I), and then produces an image via
the decoder network. A more advanced model, the
conditional VAE (CVAE), is a recent modiﬁcation
of VAE to generate diverse images conditioned on
certain attributes, e.g. generating different human
faces given skin color (Yan et al., 2015; Sohn et al.,

2015). Inspired by CVAE, we view the dialog con-
texts as the conditional attributes and adapt CVAE
to generate diverse responses instead of images.

Although VAE/CVAE has achieved impressive
results in image generation, adapting this to natu-
ral language generators is non-trivial. Bowman et
al., (2015) have used VAE with Long-Short Term
Memory (LSTM)-based recognition and decoder
networks to generate sentences from a latent Gaus-
sian variable. They showed that their model is able
to generate diverse sentences with even a greedy
LSTM decoder. They also reported the difﬁculty
of training because the LSTM decoder tends to ig-
nore the latent variable. We refer to this issue as
the vanishing latent variable problem. Serban et
al., (2016b) have applied a latent variable hierar-
chical encoder-decoder dialog model to introduce
utterance-level variations and facilitate longer re-
sponses. To improve upon the past models, we
ﬁrstly introduce a novel mechanism to leverage
linguistic knowledge in training end-to-end neural
dialog models, and we also propose a novel train-
ing technique that mitigates the vanishing latent
variable problem.

3 Proposed Models

Figure 2: Graphical models of CVAE (a) and kgC-
VAE (b)

3.1 Conditional Variational Autoencoder
(CVAE) for Dialog Generation

Each dyadic conversation can be represented via
three random variables: the dialog context c (con-
text window size k − 1), the response utterance x
(the kth utterance) and a latent variable z, which
is used to capture the latent distribution over the
valid responses. Further, c is composed of the dia-
log history: the preceding k-1 utterances; conver-
sational ﬂoor (1 if the utterance is from the same
speaker of x, otherwise 0) and meta features m
(e.g. the topic). We then deﬁne the conditional dis-
tribution p(x, z|c) = p(x|z, c)p(z|c) and our goal
is to use deep neural networks (parametrized by θ)
to approximate p(z|c) and p(x|z, c). We refer to

pθ(z|c) as the prior network and pθ(x, |z, c) as the
response decoder. Then the generative process of
x is (Figure 2 (a)):

1. Sample a latent variable z from the prior net-

work pθ(z|c).

pθ(x|z, c).

2. Generate x through the response decoder

CVAE is trained to maximize the conditional
log likelihood of x given c, which involves an in-
tractable marginalization over the latent variable
z. As proposed in (Sohn et al., 2015; Yan et al.,
2015), CVAE can be efﬁciently trained with the
Stochastic Gradient Variational Bayes (SGVB)
framework (Kingma and Welling, 2013) by maxi-
mizing the variational lower bound of the condi-
tional log likelihood. We assume the z follows
multivariate Gaussian distribution with a diago-
nal covariance matrix and introduce a recognition
network qφ(z|x, c) to approximate the true poste-
rior distribution p(z|x, c). Sohn and et al,. (2015)
have shown that the variational lower bound can
be written as:

L(θ, φ; x, c) = −KL(qφ(z|x, c)(cid:107)pθ(z|c))

+ Eqφ(z|c,x)[log pθ(x|z, c)]
≤ log p(x|c)

(1)

Figure 3 demonstrates an overview of our model.
The utterance encoder is a bidirectional recurrent
neural network (BRNN) (Schuster and Paliwal,
1997) with a gated recurrent unit (GRU) (Chung
et al., 2014) to encode each utterance into ﬁxed-
size vectors by concatenating the last hidden states
of the forward and backward RNN ui = [ (cid:126)hi, (cid:126)hi].
x is simply uk. The context encoder is a 1-layer
GRU network that encodes the preceding k-1 ut-
terances by taking u1:k−1 and the corresponding
conversation ﬂoor as inputs. The last hidden state
hc of the context encoder is concatenated with
meta features and c = [hc, m]. Since we assume z
follows isotropic Gaussian distribution, the recog-
nition network qφ(z|x, c) ∼ N (µ, σ2I) and the
prior network pθ(z|c) ∼ N (µ(cid:48), σ(cid:48)2I), and then we
have:

(cid:20)

(cid:21)
µ
log(σ2)
(cid:21)
µ(cid:48)
log(σ(cid:48)2)

(cid:20)

= Wr

+ br

(cid:21)

(cid:20)x
c

= MLPp(c)

(2)

(3)

Figure 3: The neural network architectures for the baseline and the proposed CVAE/kgCVAE models.
(cid:76) denotes the concatenation of the input vectors. The dashed blue connections only appear in kgCVAE.

We then use the reparametrization trick (Kingma
and Welling, 2013) to obtain samples of z either
from N (z; µ, σ2I) predicted by the recognition
network (training) or N (z; µ(cid:48), σ(cid:48)2I) predicted by
the prior network (testing). Finally, the response
decoder is a 1-layer GRU network with initial state
s0 = Wi[z, c]+bi. The response decoder then pre-
dicts the words in x sequentially.

3.2 Knowledge-Guided CVAE (kgCVAE)
In practice, training CVAE is a challenging opti-
mization problem and often requires large amount
of data. On the other hand, past research in spo-
ken dialog systems and discourse analysis has sug-
gested that many linguistic cues capture crucial
features in representing natural conversation. For
example, dialog acts (Poesio and Traum, 1998)
have been widely used in the dialog managers (Lit-
man and Allen, 1987; Raux et al., 2005; Zhao
and Eskenazi, 2016) to represent the propositional
function of the system. Therefore, we conjecture
that it will be beneﬁcial for the model to learn
meaningful latent z if it is provided with explicitly
extracted discourse features during the training.

In order to incorporate the linguistic features
into the basic CVAE model, we ﬁrst denote the set
of linguistic features as y. Then we assume that
the generation of x depends on c, z and y. y re-
lies on z and c as shown in Figure 2. Speciﬁcally,
during training the initial state of the response de-
coder is s0 = Wi[z, c, y] + bi and the input at ev-
ery step is [et, y] where et is the word embedding
of tth word in x. In addition, there is an MLP to
predict y(cid:48) = MLPy(z, c) based on z and c. In the
testing stage, the predicted y(cid:48) is used by the re-

sponse decoder instead of the oracle decoders. We
denote the modiﬁed model as knowledge-guided
CVAE (kgCVAE) and developers can add desired
discourse features that they wish the latent vari-
able z to capture. KgCVAE model is trained by
maximizing:

L(θ, φ; x, c, y) = −KL(qφ(z|x, c, y)(cid:107)Pθ(z|c))

+ Eqφ(z|c,x,y)[log p(x|z, c, y)]
+ Eqφ(z|c,x,y)[log p(y|z, c)]

(4)

Since now the reconstruction of y is a part of the
loss function, kgCVAE can more efﬁciently en-
code y-related information into z than discovering
it only based on the surface-level x and c. Another
advantage of kgCVAE is that it can output a high-
level label (e.g. dialog act) along with the word-
level responses, which allows easier interpretation
of the model’s outputs.

3.3 Optimization Challenges
A straightforward VAE with RNN decoder fails
to encode meaningful information in z due to the
vanishing latent variable problem (Bowman et al.,
2015). Bowman et al., (2015) proposed two solu-
tions: (1) KL annealing: gradually increasing the
weight of the KL term from 0 to 1 during training;
(2) word drop decoding: setting a certain percent-
age of the target words to 0. We found that CVAE
suffers from the same issue when the decoder is
an RNN. Also we did not consider word drop de-
coding because Bowman et al,. (2015) have shown
that it may hurt the performance when the drop
rate is too high.

As a result, we propose a simple yet novel tech-
nique to tackle the vanishing latent variable prob-
lem: bag-of-word loss. The idea is to introduce
an auxiliary loss that requires the decoder network
to predict the bag-of-words in the response x as
shown in Figure 3(b). We decompose x into two
variables: xo with word order and xbow without
order, and assume that xo and xbow are condi-
tionally independent given z and c: p(x, z|c) =
p(xo|z, c)p(xbow|z, c)p(z|c). Due to the condi-
tional independence assumption, the latent vari-
able is forced to capture global information about
the target response. Let f = MLPb(z, x) ∈ RV
where V is vocabulary size, and we have:

log p(xbow|z, c) = log

(5)

|x|
(cid:89)

t=1

efxt
j efj

(cid:80)V

where |x| is the length of x and xt is the word
index of tth word in x. The modiﬁed variational
lower bound for CVAE with bag-of-word loss is
(see Appendix A for kgCVAE):

L(cid:48)(θ, φ; x, c) = L(θ, φ; x, c)

+ Eqφ(z|c,x,y)[log p(xbow|z, c)] (6)

We will show that the bag-of-word loss in Equa-
tion 6 is very effective against the vanishing latent
variable and it is also complementary to the KL
annealing technique.

4 Experiment Setup
4.1 Dataset

We chose the Switchboard (SW) 1 Release 2 Cor-
pus (Godfrey and Holliman, 1997) to evaluate the
proposed models. SW has 2400 two-sided tele-
phone conversations with manually transcribed
speech and alignment.
In the beginning of the
call, a computer operator gave the callers recorded
prompts that deﬁne the desired topic of discus-
sion. There are 70 available topics. We ran-
domly split the data into 2316/60/62 dialogs for
train/validate/test. The pre-processing includes (1)
tokenize using the NLTK tokenizer (Bird et al.,
2009); (2) remove non-verbal symbols and re-
peated words due to false starts; (3) keep the
top 10K frequent word types as the vocabulary.
The ﬁnal data have 207, 833/5, 225/5, 481 (c, x)
pairs for train/validate/test. Furthermore, a sub-
set of SW was manually labeled with dialog
acts (Stolcke et al., 2000). We extracted dia-
log act labels based on the dialog act recognizer

proposed in (Ribeiro et al., 2015). The features
include the uni-gram and bi-gram of the utter-
ance, and the contextual features of the last 3 ut-
terances. We trained a Support Vector Machine
(SVM) (Suykens and Vandewalle, 1999) with lin-
ear kernel on the subset of SW with human anno-
tations. There are 42 types of dialog acts and the
SVM achieved 77.3% accuracy on held-out data.
Then the rest of SW data are labelled with dialog
acts using the trained SVM dialog act recognizer.

4.2 Training

We trained with the following hyperparameters
(according to the loss on the validate dataset):
word embedding has size 200 and is shared
across everywhere. We initialize the word embed-
ding from Glove embedding pre-trained on Twit-
ter (Pennington et al., 2014). The utterance en-
coder has a hidden size of 300 for each direc-
tion. The context encoder has a hidden size of
600 and the response decoder has a hidden size
of 400. The prior network and the MLP for pre-
dicting y both have 1 hidden layer of size 400 and
tanh non-linearity. The latent variable z has a
size of 200. The context window k is 10. All
the initial weights are sampled from a uniform
distribution [-0.08, 0.08]. The mini-batch size is
30. The models are trained end-to-end using the
Adam optimizer (Kingma and Ba, 2014) with a
learning rate of 0.001 and gradient clipping at 5.
We selected the best models based on the varia-
tional lower bound on the validate data. Finally,
we use the BOW loss along with KL annealing of
10,000 batches to achieve the best performance.
Section 5.4 gives a detailed argument for the im-
portance of the BOW loss.

5 Results
5.1 Experiments Setup

We compared three neural dialog models: a strong
baseline model, CVAE, and kgCVAE. The base-
line model is an encoder-decoder neural dialog
model without latent variables similar to (Serban
et al., 2016a). The baseline model’s encoder uses
the same context encoder to encode the dialog his-
tory and the meta features as shown in Figure 3.
The encoded context c is directly fed into the de-
coder networks as the initial state. The hyperpa-
rameters of the baseline are the same as the ones
reported in Section 4.2 and the baseline is trained
to minimize the standard cross entropy loss of the
decoder RNN model without any auxiliary loss.

Also, to compare the diversity introduced by the
stochasticity in the proposed latent variable ver-
sus the softmax of RNN at each decoding step, we
generate N responses from the baseline by sam-
pling from the softmax. For CVAE/kgCVAE, we
sample N times from the latent z and only use
greedy decoders so that the randomness comes en-
tirely from the latent variable z.

5.2 Quantitative Analysis

Automatically evaluating an open-domain gen-
erative dialog model is an open research chal-
lenge (Liu et al., 2016). Following our one-to-
many hypothesis, we propose the following met-
rics. We assume that for a given dialog context c,
there exist Mc reference responses rj, j ∈ [1, Mc].
Meanwhile a model can generate N hypothesis re-
sponses hi, i ∈ [1, N ]. The generalized response-
level precision/recall for a given dialog context is:

3. Dialog Act Match:

to measure the similar-
ity at the discourse level, the same dialog-
act tagger from 4.1 is applied to label all the
generated responses of each model. We set
d(rj, hi) = 1 if rj and hi have the same dia-
log acts, otherwise d(rj, hi) = 0.

One challenge of using the above metrics is that
there is only one, rather than multiple reference
responses/contexts. This impacts reliability of our
measures. Inspired by (Sordoni et al., 2015), we
utilized information retrieval techniques (see Ap-
pendix A) to gather 10 extra candidate reference
responses/context from other conversations with
the same topics. Then the 10 candidate references
are ﬁltered by two experts, which serve as the
ground truth to train the reference response classi-
ﬁer. The result is 6.69 extra references in average
per context. The average number of distinct refer-
ence dialog acts is 4.2. Table 1 shows the results.

precision(c) =

recall(c) =

(cid:80)N

i=1 maxj∈[1,Mc]d(rj, hi)
N
j=1 maxi∈[1,N ]d(rj, hi))
Mc

(cid:80)Mc

where d(rj, hi) is a distance function which lies
between 0 to 1 and measures the similarities be-
tween rj and hi. The ﬁnal score is averaged over
the entire test dataset and we report the perfor-
mance with 3 types of distance functions in or-
der to evaluate the systems from various linguistic
points of view:

1. Smoothed Sentence-level BLEU (Chen and
Cherry, 2014): BLEU is a popular metric that
measures the geometric mean of modiﬁed n-
gram precision with a length penalty (Pap-
ineni et al., 2002; Li et al., 2015). We use
BLEU-1 to 4 as our lexical similarity metric
and normalize the score to 0 to 1 scale.

2. Cosine Distance of Bag-of-word Embed-
ding: a simple method to obtain sentence
embeddings is to take the average or ex-
trema of all the word embeddings in the sen-
tences (Forgues et al., 2014; Adi et al., 2016).
The d(rj, hi) is the cosine distance of the two
embedding vectors. We used Glove embed-
ding described in Section 4 and denote the av-
erage method as A-bow and extrema method
as E-bow. The score is normalized to [0, 1].

Metrics
perplexity (KL)

BLEU-1 prec
BLEU-1 recall
BLEU-2 prec
BLEU-2 recall
BLEU-3 prec
BLEU-3 recall
BLEU-4 prec
BLEU-4 recall
A-bow prec
A-bow recall
E-bow prec
E-bow recall
DA prec
DA recall

Baseline CVAE
35.4
(n/a)
0.405
0.336
0.300
0.281
0.272
0.254
0.226
0.215
0.951
0.935
0.827
0.801
0.736
0.514

20.2
(11.36)
0.372
0.381
0.295
0.322
0.265
0.292
0.223
0.248
0.954
0.943
0.815
0.812
0.704
0.604

kgCVAE
16.02
(13.08)
0.412
0.411
0.350
0.356
0.310
0.318
0.262
0.272
0.961
0.944
0.804
0.807
0.721
0.598

Table 1: Performance of each model on automatic
measures. The highest score in each row is in
bold. Note that our BLEU scores are normalized
to [0, 1].

The proposed models outperform the baseline
in terms of recall in all the metrics with statis-
tical signiﬁcance. This conﬁrms our hypothesis
that generating responses with discourse-level di-
versity can lead to a more comprehensive cov-
erage of the potential responses than promoting
only word-level diversity. As for precision, we
observed that the baseline has higher or similar

scores than CVAE in all metrics, which is expected
since the baseline tends to generate the mostly
likely and safe responses repeatedly in the N hy-
potheses. However, kgCVAE is able to achieve
the highest precision and recall in the 4 metrics at
the same time (BLEU1-4, A-BOW). One reason
for kgCVAE’s good performance is that the pre-
dicted dialog act label in kgCVAE can regularize
the generation process of its RNN decoder by forc-
ing it to generate more coherent and precise words.
We further analyze the precision/recall of BLEU-
4 by looking at the average score versus the num-
ber of distinct reference dialog acts. A low num-
ber of distinct dialog acts represents the situation
where the dialog context has a strong constraint
on the range of the next response (low entropy),
while a high number indicates the opposite (high-
entropy). Figure 4 shows that CVAE/kgCVAE
achieves signiﬁcantly higher recall than the base-
line in higher entropy contexts. Also it shows
that CVAE suffers from lower precision, espe-
cially in low entropy contexts. Finally, kgCVAE
gets higher precision than both the baseline and
CVAE in the full spectrum of context entropy.

able to generate various ways of back-channeling.
This implies that the latent z is able to capture
context-sensitive variations, i.e. in low-entropy di-
alog contexts modeling lexical diversity while in
high-entropy ones modeling discourse-level diver-
sity. Moreover, kgCVAE is occasionally able to
generate more sophisticated grounding (sample 4)
beyond a simple back-channel, which is also an
acceptable response given the dialog context.

In addition, past work (Kingma and Welling,
2013) has shown that the recognition network is
able to learn to cluster high-dimension data, so
we conjecture that posterior z outputted from the
recognition network should cluster the responses
into meaningful groups. Figure 5 visualizes the
posterior z of responses in the test dataset in 2D
space using t-SNE (Maaten and Hinton, 2008).
We found that the learned latent space is highly
correlated with the dialog act and length of re-
sponses, which conﬁrms our assumption.

Figure 4: BLEU-4 precision/recall vs. the number
of distinct reference dialog acts.

5.3 Qualitative Analysis

Table 2 shows the outputs generated from the
baseline and kgCVAE. In example 1, caller A be-
gins with an open-ended question. The kgCVAE
model generated highly diverse answers that cover
multiple plausible dialog acts. Further, we notice
that the generated text exhibits similar dialog acts
compared to the ones predicted separately by the
model, implying the consistency of natural lan-
guage generation based on y. On the contrary, the
responses from the baseline model are limited to
local n-gram variations and share a similar preﬁx,
i.e. ”I’m”. Example 2 is a situation where caller
A is telling B stories. The ground truth response
is a back-channel and the range of valid answers is
more constrained than example 1 since B is play-
ing the role of a listener. The baseline successfully
predicts ”uh-huh”. The kgCVAE model is also

Figure 5: t-SNE visualization of the posterior z for
test responses with top 8 frequent dialog acts. The
size of circle represents the response length.

5.4 Results for Bag-of-Word Loss

Finally, we evaluate the effectiveness of bag-of-
word (BOW) loss for training VAE/CVAE with the
RNN decoder. To compare with past work (Bow-
man et al., 2015), we conducted the same lan-
guage modelling (LM) task on Penn Treebank us-
ing VAE. The network architecture is same ex-
cept we use GRU instead of LSTM. We compared
four different training setups: (1) standard VAE
without any heuristics; (2) VAE with KL anneal-
ing (KLA); (3) VAE with BOW loss; (4) VAE
with both BOW loss and KLA. Intuitively, a well
trained model should lead to a low reconstruction

Example 1-Topic: Recycling Context: A: are they doing a lot of recycling out in Georgia? Target-B (statement): well
at my workplace we have palaces for aluminium cans and we have a separate trash can for recyclable paper
Baseline+Sampling
1. well I’m a graduate student and have two kids
2. well I was in last year and so we’ve had lots of recycling

kgCVAE+Greedy
1. (non-understand) pardon
2. (statement) oh you’re not going to have a curbside pick
up here
3. (statement) okay I am sure about a recycling center
4. (yes-answer) yeah so

3. I’m not sure
4. well I don’t know I just moved here in new york

Example 2-Topic: Child Care Context: A: you know a private home to take their children to when they’re young until
they hit the preschool age and they Target-B (backchannel): uh-huh
Baseline+Sampling
1. um - hum
2. yeah
3. um - hum
4. uh-huh

kgCVAE+Greedy
1. (backchannel) uh-huh
2. (turn-exit) um-hum
3. (backchannel) yeah
4. (statement) oh yeah I think that’s part of the problem

Table 2: Generated responses from the baselines and kgCVAE in two examples. KgCVAE also provides
the predicted dialog act for each response. The context only shows the last utterance due to space limit
(the actual context window size is 10).

portance of BOW loss for training latent vari-
able models with the RNN decoder. Last but not
least, our experiments showed that the conclusions
drawn from LM using VAE also apply to training
CVAE/kgCVAE, so we used BOW loss together
with KLA for all previous experiments.

loss and small but non-trivial KL cost. For all
models with KLA, the KL weight increases lin-
early from 0 to 1 in the ﬁrst 5000 batches.

Table 3 shows the reconstruction perplexity and
the KL cost on the test dataset. The standard VAE
fails to learn a meaningful latent variable by hav-
ing a KL cost close to 0 and a reconstruction per-
plexity similar to a small LSTM LM (Zaremba
et al., 2014). KLA helps to improve the recon-
struction loss, but it requires early stopping since
the models will fall back to the standard VAE after
the KL weight becomes 1. At last, the models with
BOW loss achieved signiﬁcantly lower perplexity
and larger KL cost.

Perplexity
Model
122.0
Standard
111.5
KLA
BOW
97.72
BOW+KLA 73.04

KL cost
0.05
2.02
7.41
15.94

Table 3: The reconstruction perplexity and KL
terms on Penn Treebank test set.

Figure 6 visualizes the evolution of the KL cost.
We can see that for the standard model, the KL
cost crashes to 0 at the beginning of training and
never recovers. On the contrary, the model with
only KLA learns to encode substantial informa-
tion in latent z when the KL cost weight is small.
However, after the KL weight is increased to 1 (af-
ter 5000 batch), the model once again decides to
ignore the latent z and falls back to the naive im-
plementation. The model with BOW loss, how-
ever, consistently converges to a non-trivial KL
cost even without KLA, which conﬁrms the im-

Figure 6: The value of the KL divergence during
training with different setups on Penn Treebank.

6 Conclusion and Future Work
In conclusion, we identiﬁed the one-to-many na-
ture of open-domain conversation and proposed
two novel models that show superior performance
in generating diverse and appropriate responses at
the discourse level. While the current paper ad-
dresses diversifying responses in respect to dia-
logue acts, this work is part of a larger research
direction that targets leveraging both past linguis-
tic ﬁndings and the learning power of deep neural
networks to learn better representation of the la-
tent factors in dialog. In turn, the output of this
novel neural dialog model will be easier to ex-
plain and control by humans.
In addition to di-
alog acts, we plan to apply our kgCVAE model

to capture other different linguistic phenomena in-
cluding sentiment, named entities,etc. Last but
not least, the recognition network in our model
will serve as the foundation for designing a data-
driven dialog manager, which automatically dis-
covers useful high-level intents. All of the above
suggest a promising research direction.

7 Acknowledgements

This work was funded by NSF grant CNS-
1512973. The opinions expressed in this paper do
not necessarily reﬂect those of NSF.

References

Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer
Lavi, and Yoav Goldberg. 2016. Fine-grained anal-
ysis of sentence embeddings using auxiliary predic-
tion tasks. arXiv preprint arXiv:1608.04207 .

Steven Bird, Ewan Klein, and Edward Loper. 2009.
”

language processing with Python.

Natural
O’Reilly Media, Inc.”.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of ma-
chine Learning research 3(Jan):993–1022.

Dan Bohus and Alexander I Rudnicky. 2003. Raven-
claw: Dialog management using hierarchical task
decomposition and an expectation agenda .

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2015. A diversity-promoting objec-
tive function for neural conversation models. arXiv
preprint arXiv:1510.03055 .

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016a. A persona-based neural con-
versation model. arXiv preprint arXiv:1603.06155
.

Jiwei Li, Will Monroe, Alan Ritter, and Dan Jurafsky.
2016b. Deep reinforcement learning for dialogue
generation. arXiv preprint arXiv:1606.01541 .

Diane J Litman and James F Allen. 1987. A plan
recognition model for subdialogues in conversa-
tions. Cognitive science 11(2):163–200.

Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
2016. How not to evaluate your dialogue system:
An empirical study of unsupervised evaluation met-
rics for dialogue response generation. arXiv preprint
arXiv:1603.08023 .

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research 9(Nov):2579–2605.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics. Association for Computational
Linguistics, pages 311–318.

Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-
drew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. 2015. Generating sentences from a continuous
space. arXiv preprint arXiv:1511.06349 .

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP. volume 14, pages 1532–
43.

Boxing Chen and Colin Cherry. 2014. A systematic
comparison of smoothing techniques for sentence-
level bleu. ACL 2014 page 362.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555 .

Gabriel

Joelle

Pineau,

Forgues,

Jean-Marie
Larchevˆeque, and R´eal Tremblay. 2014. Boot-
strapping dialog systems with word embeddings.
In NIPS, Modern Machine Learning and Natural
Language Processing Workshop.

John J Godfrey and Edward Holliman. 1997.
Switchboard-1 release 2. Linguistic Data Consor-
tium, Philadelphia .

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Diederik P Kingma and Max Welling. 2013. Auto-
arXiv preprint

encoding variational bayes.
arXiv:1312.6114 .

Massimo Poesio and David Traum. 1998. Towards an
axiomatization of dialogue acts. In Proceedings of
the Twente Workshop on the Formal Semantics and
Pragmatics of Dialogues (13th Twente Workshop on
Language Technology. Citeseer.

Antoine Raux, Brian Langner, Dan Bohus, Alan W
Black, and Maxine Eskenazi. 2005. Lets go pub-
lic! taking a spoken dialog system to the real world.
In in Proc. of Interspeech 2005. Citeseer.

Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Stochastic backpropagation and
Wierstra. 2014.
approximate inference in deep generative models.
arXiv preprint arXiv:1401.4082 .

Eug´enio Ribeiro, Ricardo Ribeiro, and David Mar-
The inﬂuence of con-
arXiv preprint

tins de Matos. 2015.
text on dialogue act recognition.
arXiv:1506.00839 .

Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval.
Information processing & management 24(5):513–
523.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing 45(11):2673–2681.

Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016a. Building
end-to-end dialogue systems using generative hier-
archical neural network models. In Proceedings of
the 30th AAAI Conference on Artiﬁcial Intelligence
(AAAI-16).

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron Courville,
and Yoshua Bengio. 2016b. A hierarchical latent
variable encoder-decoder model for generating di-
alogues. arXiv preprint arXiv:1605.06069 .

Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015.
Learning structured output
representation using
In Advances
deep conditional generative models.
in Neural Information Processing Systems. pages
3483–3491.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. arXiv preprint
arXiv:1506.06714 .

Andreas Stolcke, Noah Coccaro, Rebecca Bates, Paul
Taylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-
beth Shriberg, Daniel Jurafsky, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for au-
tomatic tagging and recognition of conversational
speech. Computational linguistics 26(3):339–373.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.

Johan AK Suykens and Joos Vandewalle. 1999. Least
squares support vector machine classiﬁers. Neural
processing letters 9(3):293–300.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869 .

Jason D Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken
dialog systems. Computer Speech & Language
21(2):393–422.

Sam Wiseman and Alexander M Rush. 2016.
Sequence-to-sequence learning as beam-search op-
timization. arXiv preprint arXiv:1606.02960 .

Chen Xing, Wei Wu, Yu Wu,

Jie Liu, Yalou
Huang, Ming Zhou, and Wei-Ying Ma. 2016.
Topic augmented neural response generation with
arXiv preprint
a joint attention mechanism.
arXiv:1606.08340 .

Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak
Lee. 2015. Attribute2image: Conditional image
arXiv preprint
generation from visual attributes.
arXiv:1512.00570 .

Zhou Yu, Ziyu Xu, Alan W Black, and Alex I Rud-
nicky. 2016. Strategy and policy learning for non-
In 17th An-
task-oriented conversational systems.
nual Meeting of the Special Interest Group on Dis-
course and Dialogue. volume 2, page 7.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
Recurrent neural network regularization.

2014.
arXiv preprint arXiv:1409.2329 .

Tiancheng Zhao and Maxine Eskenazi. 2016. To-
wards end-to-end learning for dialog state tracking
and management using deep reinforcement learning.
arXiv preprint arXiv:1606.02560 .

A Supplemental Material

Variational Lower Bound for kgCVAE

We assume that even with the presence of linguis-
tic feature y regarding x, the prediction of xbow
still only depends on the z and c. Therefore, we
have:

L(θ, φ; x, c, y) = −KL(qφ(z|x, c, y)(cid:107)Pθ(z|c))

+ Eqφ(z|c,x,y)[log p(x|z, c, y)]
+ Eqφ(z|c,x,y)[log p(y|z, c)]
+ Eqφ(z|c,x,y)[log p(xbow|z, c)]

(7)

Collection of Multiple Reference Responses

We collected multiple reference responses for each
dialog context in the test set by information re-
trieval techniques combining with traditional a
machine learning method. First, we encode the di-
alog history using Term Frequency-Inverse Doc-
ument Frequency (TFIDF) (Salton and Buckley,
1988) weighted bag-of-words into vector repre-
sentation h. Then we denote the topic of the con-
versation as t and denote f as the conversation
if the speakers of the last utterance in
ﬂoor, i.e.
the dialog history and response utterance are the
same f = 1 otherwise f = 0. Then we com-
puted the similarity d(ci, cj) between two dialog
contexts using:

d(ci, cj) = 1(ti = tj)1(ti = tj)

hi · hj
||hi||||hj||

(8)

Unlike past work (Sordoni et al., 2015), this sim-
ilarity function only cares about the distance in

the context and imposes no constraints on the re-
sponse, therefore is suitbale for ﬁnding diverse re-
sponses regarding to the same dialog context. Sec-
ondly, for each dialog context in the test set, we
retrieved the 10 nearest neighbors from the train-
ing set and treated the responses from the training
set as candidate reference responses. Thirdly, we
further sampled 240 context-responses pairs from
5481 pairs in the total test set and post-processed
the selected candidate responses by two human
computational linguistic experts who were told to
give a binary label for each candidate response
about whether the response is appropriate regard-
ing its dialog context. The ﬁltered lists then served
as the ground truth to train our reference response
classiﬁer. For the next step, we extracted bigrams,
part-of-speech bigrams and word part-of-speech
pairs from both dialogue contexts and candidate
reference responses with rare threshold for feature
extraction being set to 20. Then L2-regularized
logistic regression with 10-fold cross validation
was applied as the machine learning algorithm.
Cross validation accuracy on the human-labelled
data was 71%. Finally, we automatically anno-
tated the rest of test set with this trained classiﬁer
and the resulting data were used for model evalu-
ation.

Learning Discourse-level Diversity for Neural Dialog Models using
Conditional Variational Autoencoders

Tiancheng Zhao, Ran Zhao and Maxine Eskenazi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, Pennsylvania, USA
{tianchez,ranzhao1,max+}@cs.cmu.edu

Abstract

recent neural

While
encoder-decoder
models have shown great promise in mod-
they
eling open-domain conversations,
often generate dull and generic responses.
Unlike past work that has focused on
diversifying the output of the decoder
at word-level to alleviate this problem,
we present a novel framework based on
conditional variational autoencoders that
captures the discourse-level diversity in
the encoder. Our model uses latent vari-
ables to learn a distribution over potential
conversational
generates
intents
diverse responses using only greedy de-
coders. We have further developed a novel
variant that is integrated with linguistic
prior knowledge for better performance.
Finally, the training procedure is improved
by introducing a bag-of-word loss. Our
proposed models have been validated
to generate signiﬁcantly more diverse
responses than baseline approaches and
exhibit competence in discourse-level
decision-making.1

and

Introduction

1
The dialog manager is one of the key components
of dialog systems, which is responsible for mod-
eling the decision-making process. Speciﬁcally, it
typically takes a new utterance and the dialog con-
text as input, and generates discourse-level deci-
sions (Bohus and Rudnicky, 2003; Williams and
Young, 2007). Advanced dialog managers usu-
ally have a list of potential actions that enable
them to have diverse behavior during a conver-
sation, e.g. different strategies to recover from
non-understanding (Yu et al., 2016). However,

1Data and an implementation of our model is avalaible at

https://github.com/snakeztc/NeuralDialog-CVAE

recent

the conventional approach of designing a dialog
manager (Williams and Young, 2007) does not
scale well to open-domain conversation models
because of the vast quantity of possible decisions.
Thus, there has been a growing interest in applying
encoder-decoder models (Sutskever et al., 2014)
for modeling open-domain conversation (Vinyals
and Le, 2015; Serban et al., 2016a). The basic ap-
proach treats a conversation as a transduction task,
in which the dialog history is the source sequence
and the next response is the target sequence. The
model is then trained end-to-end on large conver-
sation corpora using the maximum-likelihood esti-
mation (MLE) objective without the need for man-
ual crafting.
However

found that
encoder-decoder models tend to generate generic
and dull responses (e.g., I don’t know), rather
than meaningful and speciﬁc answers (Li et al.,
2015; Serban et al., 2016b). There have been
many attempts to explain and solve this limita-
tion, and they can be broadly divided into two cat-
egories (see Section 2 for details): (1) the ﬁrst cat-
egory argues that the dialog history is only one of
the factors that decide the next response. Other
features should be extracted and provided to the
models as conditionals in order to generate more
speciﬁc responses (Xing et al., 2016; Li et al.,
2016a); (2) the second category aims to improve
the encoder-decoder model itself, including de-
coding with beam search and its variations (Wise-
man and Rush, 2016), encouraging responses that
have long-term payoff (Li et al., 2016b), etc.

research has

Building upon the past work in dialog managers
and encoder-decoder models, the key idea of this
paper is to model dialogs as a one-to-many prob-
lem at the discourse level. Previous studies indi-
cate that there are many factors in open-domain
dialogs that decide the next response, and it is non-
Intuitively, given
trivial to extract all of them.

7
1
0
2
 
t
c
O
 
1
2
 
 
]
L
C
.
s
c
[
 
 
3
v
0
6
9
0
1
.
3
0
7
1
:
v
i
X
r
a

a similar dialog history (and other observed in-
puts), there may exist many valid responses (at the
discourse-level), each corresponding to a certain
conﬁguration of the latent variables that are not
presented in the input. To uncover the potential re-
sponses, we strive to model a probabilistic distri-
bution over the distributed utterance embeddings
of the potential responses using a latent variable
(Figure 1). This allows us to generate diverse re-
sponses by drawing samples from the learned dis-
tribution and reconstruct their words via a decoder
neural network.

Figure 1: Given A’s question, there exists many
valid responses from B for different assumptions
of the latent variables, e.g., B’s hobby.

Speciﬁcally, our contributions are three-fold:
1. We present a novel neural dialog model
adapted from conditional variational autoencoders
(CVAE) (Yan et al., 2015; Sohn et al., 2015),
which introduces a latent variable that can cap-
ture discourse-level variations as described above
2. We propose Knowledge-Guided CVAE (kgC-
VAE), which enables easy integration of expert
knowledge and results in performance improve-
ment and model interpretability. 3. We develop
a training method in addressing the difﬁculty of
optimizing CVAE for natural language genera-
tion (Bowman et al., 2015). We evaluate our
models on human-human conversation data and
yield promising results in: (a) generating appro-
priate and discourse-level diverse responses, and
(b) showing that the proposed training method is
more effective than the previous techniques.

2 Related Work

Our work is related to both recent advancement
in encoder-decoder dialog models and generative
models based on CVAE.

2.1 Encoder-decoder Dialog Models
Since the emergence of the neural dialog model,
the problem of output diversity has received much
attention in the research community.
Ideal out-
put responses should be both coherent and diverse.
However, most models end up with generic and
dull responses. To tackle this problem, one line
of research has focused on augmenting the in-

put of encoder-decoder models with richer con-
text information, in order to generate more spe-
ciﬁc responses. Li et al., (2016a) captured speak-
ers’ characteristics by encoding background infor-
mation and speaking style into the distributed em-
beddings, which are used to re-rank the generated
response from an encoder-decoder model. Xing et
al., (2016) maintain topic encoding based on La-
tent Dirichlet Allocation (LDA) (Blei et al., 2003)
of the conversation to encourage the model to out-
put more topic coherent responses.

On the other hand, many attempts have also
been made to improve the architecture of encoder-
decoder models. Li et al,. (2015) proposed to opti-
mize the standard encoder-decoder by maximizing
the mutual information between input and output,
which in turn reduces generic responses. This ap-
proach penalized unconditionally high frequency
responses, and favored responses that have high
conditional probability given the input. Wiseman
and Rush (2016) focused on improving the de-
coder network by alleviating the biases between
training and testing. They introduced a search-
based loss that directly optimizes the networks
for beam search decoding. The resulting model
achieves better performance on word ordering,
parsing and machine translation. Besides improv-
ing beam search, Li et al., (2016b) pointed out that
the MLE objective of an encoder-decoder model
is unable to approximate the real-world goal of
the conversation. Thus, they initialized a encoder-
decoder model with MLE objective and leveraged
reinforcement learning to ﬁne tune the model by
optimizing three heuristic rewards functions: in-
formativity, coherence, and ease of answering.

2.2 Conditional Variational Autoencoder
The variational autoencoder (VAE) (Kingma and
Welling, 2013; Rezende et al., 2014) is one of the
most popular frameworks for image generation.
The basic idea of VAE is to encode the input x
into a probability distribution z instead of a point
encoding in the autoencoder. Then VAE applies a
decoder network to reconstruct the original input
using samples from z. To generate images, VAE
ﬁrst obtains a sample of z from the prior distribu-
tion, e.g. N (0, I), and then produces an image via
the decoder network. A more advanced model, the
conditional VAE (CVAE), is a recent modiﬁcation
of VAE to generate diverse images conditioned on
certain attributes, e.g. generating different human
faces given skin color (Yan et al., 2015; Sohn et al.,

2015). Inspired by CVAE, we view the dialog con-
texts as the conditional attributes and adapt CVAE
to generate diverse responses instead of images.

Although VAE/CVAE has achieved impressive
results in image generation, adapting this to natu-
ral language generators is non-trivial. Bowman et
al., (2015) have used VAE with Long-Short Term
Memory (LSTM)-based recognition and decoder
networks to generate sentences from a latent Gaus-
sian variable. They showed that their model is able
to generate diverse sentences with even a greedy
LSTM decoder. They also reported the difﬁculty
of training because the LSTM decoder tends to ig-
nore the latent variable. We refer to this issue as
the vanishing latent variable problem. Serban et
al., (2016b) have applied a latent variable hierar-
chical encoder-decoder dialog model to introduce
utterance-level variations and facilitate longer re-
sponses. To improve upon the past models, we
ﬁrstly introduce a novel mechanism to leverage
linguistic knowledge in training end-to-end neural
dialog models, and we also propose a novel train-
ing technique that mitigates the vanishing latent
variable problem.

3 Proposed Models

Figure 2: Graphical models of CVAE (a) and kgC-
VAE (b)

3.1 Conditional Variational Autoencoder
(CVAE) for Dialog Generation

Each dyadic conversation can be represented via
three random variables: the dialog context c (con-
text window size k − 1), the response utterance x
(the kth utterance) and a latent variable z, which
is used to capture the latent distribution over the
valid responses. Further, c is composed of the dia-
log history: the preceding k-1 utterances; conver-
sational ﬂoor (1 if the utterance is from the same
speaker of x, otherwise 0) and meta features m
(e.g. the topic). We then deﬁne the conditional dis-
tribution p(x, z|c) = p(x|z, c)p(z|c) and our goal
is to use deep neural networks (parametrized by θ)
to approximate p(z|c) and p(x|z, c). We refer to

pθ(z|c) as the prior network and pθ(x, |z, c) as the
response decoder. Then the generative process of
x is (Figure 2 (a)):

1. Sample a latent variable z from the prior net-

work pθ(z|c).

pθ(x|z, c).

2. Generate x through the response decoder

CVAE is trained to maximize the conditional
log likelihood of x given c, which involves an in-
tractable marginalization over the latent variable
z. As proposed in (Sohn et al., 2015; Yan et al.,
2015), CVAE can be efﬁciently trained with the
Stochastic Gradient Variational Bayes (SGVB)
framework (Kingma and Welling, 2013) by maxi-
mizing the variational lower bound of the condi-
tional log likelihood. We assume the z follows
multivariate Gaussian distribution with a diago-
nal covariance matrix and introduce a recognition
network qφ(z|x, c) to approximate the true poste-
rior distribution p(z|x, c). Sohn and et al,. (2015)
have shown that the variational lower bound can
be written as:

L(θ, φ; x, c) = −KL(qφ(z|x, c)(cid:107)pθ(z|c))

+ Eqφ(z|c,x)[log pθ(x|z, c)]
≤ log p(x|c)

(1)

Figure 3 demonstrates an overview of our model.
The utterance encoder is a bidirectional recurrent
neural network (BRNN) (Schuster and Paliwal,
1997) with a gated recurrent unit (GRU) (Chung
et al., 2014) to encode each utterance into ﬁxed-
size vectors by concatenating the last hidden states
of the forward and backward RNN ui = [ (cid:126)hi, (cid:126)hi].
x is simply uk. The context encoder is a 1-layer
GRU network that encodes the preceding k-1 ut-
terances by taking u1:k−1 and the corresponding
conversation ﬂoor as inputs. The last hidden state
hc of the context encoder is concatenated with
meta features and c = [hc, m]. Since we assume z
follows isotropic Gaussian distribution, the recog-
nition network qφ(z|x, c) ∼ N (µ, σ2I) and the
prior network pθ(z|c) ∼ N (µ(cid:48), σ(cid:48)2I), and then we
have:

(cid:20)

(cid:21)
µ
log(σ2)
(cid:21)
µ(cid:48)
log(σ(cid:48)2)

(cid:20)

= Wr

+ br

(cid:21)

(cid:20)x
c

= MLPp(c)

(2)

(3)

Figure 3: The neural network architectures for the baseline and the proposed CVAE/kgCVAE models.
(cid:76) denotes the concatenation of the input vectors. The dashed blue connections only appear in kgCVAE.

We then use the reparametrization trick (Kingma
and Welling, 2013) to obtain samples of z either
from N (z; µ, σ2I) predicted by the recognition
network (training) or N (z; µ(cid:48), σ(cid:48)2I) predicted by
the prior network (testing). Finally, the response
decoder is a 1-layer GRU network with initial state
s0 = Wi[z, c]+bi. The response decoder then pre-
dicts the words in x sequentially.

3.2 Knowledge-Guided CVAE (kgCVAE)
In practice, training CVAE is a challenging opti-
mization problem and often requires large amount
of data. On the other hand, past research in spo-
ken dialog systems and discourse analysis has sug-
gested that many linguistic cues capture crucial
features in representing natural conversation. For
example, dialog acts (Poesio and Traum, 1998)
have been widely used in the dialog managers (Lit-
man and Allen, 1987; Raux et al., 2005; Zhao
and Eskenazi, 2016) to represent the propositional
function of the system. Therefore, we conjecture
that it will be beneﬁcial for the model to learn
meaningful latent z if it is provided with explicitly
extracted discourse features during the training.

In order to incorporate the linguistic features
into the basic CVAE model, we ﬁrst denote the set
of linguistic features as y. Then we assume that
the generation of x depends on c, z and y. y re-
lies on z and c as shown in Figure 2. Speciﬁcally,
during training the initial state of the response de-
coder is s0 = Wi[z, c, y] + bi and the input at ev-
ery step is [et, y] where et is the word embedding
of tth word in x. In addition, there is an MLP to
predict y(cid:48) = MLPy(z, c) based on z and c. In the
testing stage, the predicted y(cid:48) is used by the re-

sponse decoder instead of the oracle decoders. We
denote the modiﬁed model as knowledge-guided
CVAE (kgCVAE) and developers can add desired
discourse features that they wish the latent vari-
able z to capture. KgCVAE model is trained by
maximizing:

L(θ, φ; x, c, y) = −KL(qφ(z|x, c, y)(cid:107)Pθ(z|c))

+ Eqφ(z|c,x,y)[log p(x|z, c, y)]
+ Eqφ(z|c,x,y)[log p(y|z, c)]

(4)

Since now the reconstruction of y is a part of the
loss function, kgCVAE can more efﬁciently en-
code y-related information into z than discovering
it only based on the surface-level x and c. Another
advantage of kgCVAE is that it can output a high-
level label (e.g. dialog act) along with the word-
level responses, which allows easier interpretation
of the model’s outputs.

3.3 Optimization Challenges
A straightforward VAE with RNN decoder fails
to encode meaningful information in z due to the
vanishing latent variable problem (Bowman et al.,
2015). Bowman et al., (2015) proposed two solu-
tions: (1) KL annealing: gradually increasing the
weight of the KL term from 0 to 1 during training;
(2) word drop decoding: setting a certain percent-
age of the target words to 0. We found that CVAE
suffers from the same issue when the decoder is
an RNN. Also we did not consider word drop de-
coding because Bowman et al,. (2015) have shown
that it may hurt the performance when the drop
rate is too high.

As a result, we propose a simple yet novel tech-
nique to tackle the vanishing latent variable prob-
lem: bag-of-word loss. The idea is to introduce
an auxiliary loss that requires the decoder network
to predict the bag-of-words in the response x as
shown in Figure 3(b). We decompose x into two
variables: xo with word order and xbow without
order, and assume that xo and xbow are condi-
tionally independent given z and c: p(x, z|c) =
p(xo|z, c)p(xbow|z, c)p(z|c). Due to the condi-
tional independence assumption, the latent vari-
able is forced to capture global information about
the target response. Let f = MLPb(z, x) ∈ RV
where V is vocabulary size, and we have:

log p(xbow|z, c) = log

(5)

|x|
(cid:89)

t=1

efxt
j efj

(cid:80)V

where |x| is the length of x and xt is the word
index of tth word in x. The modiﬁed variational
lower bound for CVAE with bag-of-word loss is
(see Appendix A for kgCVAE):

L(cid:48)(θ, φ; x, c) = L(θ, φ; x, c)

+ Eqφ(z|c,x,y)[log p(xbow|z, c)] (6)

We will show that the bag-of-word loss in Equa-
tion 6 is very effective against the vanishing latent
variable and it is also complementary to the KL
annealing technique.

4 Experiment Setup
4.1 Dataset

We chose the Switchboard (SW) 1 Release 2 Cor-
pus (Godfrey and Holliman, 1997) to evaluate the
proposed models. SW has 2400 two-sided tele-
phone conversations with manually transcribed
speech and alignment.
In the beginning of the
call, a computer operator gave the callers recorded
prompts that deﬁne the desired topic of discus-
sion. There are 70 available topics. We ran-
domly split the data into 2316/60/62 dialogs for
train/validate/test. The pre-processing includes (1)
tokenize using the NLTK tokenizer (Bird et al.,
2009); (2) remove non-verbal symbols and re-
peated words due to false starts; (3) keep the
top 10K frequent word types as the vocabulary.
The ﬁnal data have 207, 833/5, 225/5, 481 (c, x)
pairs for train/validate/test. Furthermore, a sub-
set of SW was manually labeled with dialog
acts (Stolcke et al., 2000). We extracted dia-
log act labels based on the dialog act recognizer

proposed in (Ribeiro et al., 2015). The features
include the uni-gram and bi-gram of the utter-
ance, and the contextual features of the last 3 ut-
terances. We trained a Support Vector Machine
(SVM) (Suykens and Vandewalle, 1999) with lin-
ear kernel on the subset of SW with human anno-
tations. There are 42 types of dialog acts and the
SVM achieved 77.3% accuracy on held-out data.
Then the rest of SW data are labelled with dialog
acts using the trained SVM dialog act recognizer.

4.2 Training

We trained with the following hyperparameters
(according to the loss on the validate dataset):
word embedding has size 200 and is shared
across everywhere. We initialize the word embed-
ding from Glove embedding pre-trained on Twit-
ter (Pennington et al., 2014). The utterance en-
coder has a hidden size of 300 for each direc-
tion. The context encoder has a hidden size of
600 and the response decoder has a hidden size
of 400. The prior network and the MLP for pre-
dicting y both have 1 hidden layer of size 400 and
tanh non-linearity. The latent variable z has a
size of 200. The context window k is 10. All
the initial weights are sampled from a uniform
distribution [-0.08, 0.08]. The mini-batch size is
30. The models are trained end-to-end using the
Adam optimizer (Kingma and Ba, 2014) with a
learning rate of 0.001 and gradient clipping at 5.
We selected the best models based on the varia-
tional lower bound on the validate data. Finally,
we use the BOW loss along with KL annealing of
10,000 batches to achieve the best performance.
Section 5.4 gives a detailed argument for the im-
portance of the BOW loss.

5 Results
5.1 Experiments Setup

We compared three neural dialog models: a strong
baseline model, CVAE, and kgCVAE. The base-
line model is an encoder-decoder neural dialog
model without latent variables similar to (Serban
et al., 2016a). The baseline model’s encoder uses
the same context encoder to encode the dialog his-
tory and the meta features as shown in Figure 3.
The encoded context c is directly fed into the de-
coder networks as the initial state. The hyperpa-
rameters of the baseline are the same as the ones
reported in Section 4.2 and the baseline is trained
to minimize the standard cross entropy loss of the
decoder RNN model without any auxiliary loss.

Also, to compare the diversity introduced by the
stochasticity in the proposed latent variable ver-
sus the softmax of RNN at each decoding step, we
generate N responses from the baseline by sam-
pling from the softmax. For CVAE/kgCVAE, we
sample N times from the latent z and only use
greedy decoders so that the randomness comes en-
tirely from the latent variable z.

5.2 Quantitative Analysis

Automatically evaluating an open-domain gen-
erative dialog model is an open research chal-
lenge (Liu et al., 2016). Following our one-to-
many hypothesis, we propose the following met-
rics. We assume that for a given dialog context c,
there exist Mc reference responses rj, j ∈ [1, Mc].
Meanwhile a model can generate N hypothesis re-
sponses hi, i ∈ [1, N ]. The generalized response-
level precision/recall for a given dialog context is:

3. Dialog Act Match:

to measure the similar-
ity at the discourse level, the same dialog-
act tagger from 4.1 is applied to label all the
generated responses of each model. We set
d(rj, hi) = 1 if rj and hi have the same dia-
log acts, otherwise d(rj, hi) = 0.

One challenge of using the above metrics is that
there is only one, rather than multiple reference
responses/contexts. This impacts reliability of our
measures. Inspired by (Sordoni et al., 2015), we
utilized information retrieval techniques (see Ap-
pendix A) to gather 10 extra candidate reference
responses/context from other conversations with
the same topics. Then the 10 candidate references
are ﬁltered by two experts, which serve as the
ground truth to train the reference response classi-
ﬁer. The result is 6.69 extra references in average
per context. The average number of distinct refer-
ence dialog acts is 4.2. Table 1 shows the results.

precision(c) =

recall(c) =

(cid:80)N

i=1 maxj∈[1,Mc]d(rj, hi)
N
j=1 maxi∈[1,N ]d(rj, hi))
Mc

(cid:80)Mc

where d(rj, hi) is a distance function which lies
between 0 to 1 and measures the similarities be-
tween rj and hi. The ﬁnal score is averaged over
the entire test dataset and we report the perfor-
mance with 3 types of distance functions in or-
der to evaluate the systems from various linguistic
points of view:

1. Smoothed Sentence-level BLEU (Chen and
Cherry, 2014): BLEU is a popular metric that
measures the geometric mean of modiﬁed n-
gram precision with a length penalty (Pap-
ineni et al., 2002; Li et al., 2015). We use
BLEU-1 to 4 as our lexical similarity metric
and normalize the score to 0 to 1 scale.

2. Cosine Distance of Bag-of-word Embed-
ding: a simple method to obtain sentence
embeddings is to take the average or ex-
trema of all the word embeddings in the sen-
tences (Forgues et al., 2014; Adi et al., 2016).
The d(rj, hi) is the cosine distance of the two
embedding vectors. We used Glove embed-
ding described in Section 4 and denote the av-
erage method as A-bow and extrema method
as E-bow. The score is normalized to [0, 1].

Metrics
perplexity (KL)

BLEU-1 prec
BLEU-1 recall
BLEU-2 prec
BLEU-2 recall
BLEU-3 prec
BLEU-3 recall
BLEU-4 prec
BLEU-4 recall
A-bow prec
A-bow recall
E-bow prec
E-bow recall
DA prec
DA recall

Baseline CVAE
35.4
(n/a)
0.405
0.336
0.300
0.281
0.272
0.254
0.226
0.215
0.951
0.935
0.827
0.801
0.736
0.514

20.2
(11.36)
0.372
0.381
0.295
0.322
0.265
0.292
0.223
0.248
0.954
0.943
0.815
0.812
0.704
0.604

kgCVAE
16.02
(13.08)
0.412
0.411
0.350
0.356
0.310
0.318
0.262
0.272
0.961
0.944
0.804
0.807
0.721
0.598

Table 1: Performance of each model on automatic
measures. The highest score in each row is in
bold. Note that our BLEU scores are normalized
to [0, 1].

The proposed models outperform the baseline
in terms of recall in all the metrics with statis-
tical signiﬁcance. This conﬁrms our hypothesis
that generating responses with discourse-level di-
versity can lead to a more comprehensive cov-
erage of the potential responses than promoting
only word-level diversity. As for precision, we
observed that the baseline has higher or similar

scores than CVAE in all metrics, which is expected
since the baseline tends to generate the mostly
likely and safe responses repeatedly in the N hy-
potheses. However, kgCVAE is able to achieve
the highest precision and recall in the 4 metrics at
the same time (BLEU1-4, A-BOW). One reason
for kgCVAE’s good performance is that the pre-
dicted dialog act label in kgCVAE can regularize
the generation process of its RNN decoder by forc-
ing it to generate more coherent and precise words.
We further analyze the precision/recall of BLEU-
4 by looking at the average score versus the num-
ber of distinct reference dialog acts. A low num-
ber of distinct dialog acts represents the situation
where the dialog context has a strong constraint
on the range of the next response (low entropy),
while a high number indicates the opposite (high-
entropy). Figure 4 shows that CVAE/kgCVAE
achieves signiﬁcantly higher recall than the base-
line in higher entropy contexts. Also it shows
that CVAE suffers from lower precision, espe-
cially in low entropy contexts. Finally, kgCVAE
gets higher precision than both the baseline and
CVAE in the full spectrum of context entropy.

able to generate various ways of back-channeling.
This implies that the latent z is able to capture
context-sensitive variations, i.e. in low-entropy di-
alog contexts modeling lexical diversity while in
high-entropy ones modeling discourse-level diver-
sity. Moreover, kgCVAE is occasionally able to
generate more sophisticated grounding (sample 4)
beyond a simple back-channel, which is also an
acceptable response given the dialog context.

In addition, past work (Kingma and Welling,
2013) has shown that the recognition network is
able to learn to cluster high-dimension data, so
we conjecture that posterior z outputted from the
recognition network should cluster the responses
into meaningful groups. Figure 5 visualizes the
posterior z of responses in the test dataset in 2D
space using t-SNE (Maaten and Hinton, 2008).
We found that the learned latent space is highly
correlated with the dialog act and length of re-
sponses, which conﬁrms our assumption.

Figure 4: BLEU-4 precision/recall vs. the number
of distinct reference dialog acts.

5.3 Qualitative Analysis

Table 2 shows the outputs generated from the
baseline and kgCVAE. In example 1, caller A be-
gins with an open-ended question. The kgCVAE
model generated highly diverse answers that cover
multiple plausible dialog acts. Further, we notice
that the generated text exhibits similar dialog acts
compared to the ones predicted separately by the
model, implying the consistency of natural lan-
guage generation based on y. On the contrary, the
responses from the baseline model are limited to
local n-gram variations and share a similar preﬁx,
i.e. ”I’m”. Example 2 is a situation where caller
A is telling B stories. The ground truth response
is a back-channel and the range of valid answers is
more constrained than example 1 since B is play-
ing the role of a listener. The baseline successfully
predicts ”uh-huh”. The kgCVAE model is also

Figure 5: t-SNE visualization of the posterior z for
test responses with top 8 frequent dialog acts. The
size of circle represents the response length.

5.4 Results for Bag-of-Word Loss

Finally, we evaluate the effectiveness of bag-of-
word (BOW) loss for training VAE/CVAE with the
RNN decoder. To compare with past work (Bow-
man et al., 2015), we conducted the same lan-
guage modelling (LM) task on Penn Treebank us-
ing VAE. The network architecture is same ex-
cept we use GRU instead of LSTM. We compared
four different training setups: (1) standard VAE
without any heuristics; (2) VAE with KL anneal-
ing (KLA); (3) VAE with BOW loss; (4) VAE
with both BOW loss and KLA. Intuitively, a well
trained model should lead to a low reconstruction

Example 1-Topic: Recycling Context: A: are they doing a lot of recycling out in Georgia? Target-B (statement): well
at my workplace we have palaces for aluminium cans and we have a separate trash can for recyclable paper
Baseline+Sampling
1. well I’m a graduate student and have two kids
2. well I was in last year and so we’ve had lots of recycling

kgCVAE+Greedy
1. (non-understand) pardon
2. (statement) oh you’re not going to have a curbside pick
up here
3. (statement) okay I am sure about a recycling center
4. (yes-answer) yeah so

3. I’m not sure
4. well I don’t know I just moved here in new york

Example 2-Topic: Child Care Context: A: you know a private home to take their children to when they’re young until
they hit the preschool age and they Target-B (backchannel): uh-huh
Baseline+Sampling
1. um - hum
2. yeah
3. um - hum
4. uh-huh

kgCVAE+Greedy
1. (backchannel) uh-huh
2. (turn-exit) um-hum
3. (backchannel) yeah
4. (statement) oh yeah I think that’s part of the problem

Table 2: Generated responses from the baselines and kgCVAE in two examples. KgCVAE also provides
the predicted dialog act for each response. The context only shows the last utterance due to space limit
(the actual context window size is 10).

portance of BOW loss for training latent vari-
able models with the RNN decoder. Last but not
least, our experiments showed that the conclusions
drawn from LM using VAE also apply to training
CVAE/kgCVAE, so we used BOW loss together
with KLA for all previous experiments.

loss and small but non-trivial KL cost. For all
models with KLA, the KL weight increases lin-
early from 0 to 1 in the ﬁrst 5000 batches.

Table 3 shows the reconstruction perplexity and
the KL cost on the test dataset. The standard VAE
fails to learn a meaningful latent variable by hav-
ing a KL cost close to 0 and a reconstruction per-
plexity similar to a small LSTM LM (Zaremba
et al., 2014). KLA helps to improve the recon-
struction loss, but it requires early stopping since
the models will fall back to the standard VAE after
the KL weight becomes 1. At last, the models with
BOW loss achieved signiﬁcantly lower perplexity
and larger KL cost.

Perplexity
Model
122.0
Standard
111.5
KLA
BOW
97.72
BOW+KLA 73.04

KL cost
0.05
2.02
7.41
15.94

Table 3: The reconstruction perplexity and KL
terms on Penn Treebank test set.

Figure 6 visualizes the evolution of the KL cost.
We can see that for the standard model, the KL
cost crashes to 0 at the beginning of training and
never recovers. On the contrary, the model with
only KLA learns to encode substantial informa-
tion in latent z when the KL cost weight is small.
However, after the KL weight is increased to 1 (af-
ter 5000 batch), the model once again decides to
ignore the latent z and falls back to the naive im-
plementation. The model with BOW loss, how-
ever, consistently converges to a non-trivial KL
cost even without KLA, which conﬁrms the im-

Figure 6: The value of the KL divergence during
training with different setups on Penn Treebank.

6 Conclusion and Future Work
In conclusion, we identiﬁed the one-to-many na-
ture of open-domain conversation and proposed
two novel models that show superior performance
in generating diverse and appropriate responses at
the discourse level. While the current paper ad-
dresses diversifying responses in respect to dia-
logue acts, this work is part of a larger research
direction that targets leveraging both past linguis-
tic ﬁndings and the learning power of deep neural
networks to learn better representation of the la-
tent factors in dialog. In turn, the output of this
novel neural dialog model will be easier to ex-
plain and control by humans.
In addition to di-
alog acts, we plan to apply our kgCVAE model

to capture other different linguistic phenomena in-
cluding sentiment, named entities,etc. Last but
not least, the recognition network in our model
will serve as the foundation for designing a data-
driven dialog manager, which automatically dis-
covers useful high-level intents. All of the above
suggest a promising research direction.

7 Acknowledgements

This work was funded by NSF grant CNS-
1512973. The opinions expressed in this paper do
not necessarily reﬂect those of NSF.

References

Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer
Lavi, and Yoav Goldberg. 2016. Fine-grained anal-
ysis of sentence embeddings using auxiliary predic-
tion tasks. arXiv preprint arXiv:1608.04207 .

Steven Bird, Ewan Klein, and Edward Loper. 2009.
”

language processing with Python.

Natural
O’Reilly Media, Inc.”.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of ma-
chine Learning research 3(Jan):993–1022.

Dan Bohus and Alexander I Rudnicky. 2003. Raven-
claw: Dialog management using hierarchical task
decomposition and an expectation agenda .

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2015. A diversity-promoting objec-
tive function for neural conversation models. arXiv
preprint arXiv:1510.03055 .

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016a. A persona-based neural con-
versation model. arXiv preprint arXiv:1603.06155
.

Jiwei Li, Will Monroe, Alan Ritter, and Dan Jurafsky.
2016b. Deep reinforcement learning for dialogue
generation. arXiv preprint arXiv:1606.01541 .

Diane J Litman and James F Allen. 1987. A plan
recognition model for subdialogues in conversa-
tions. Cognitive science 11(2):163–200.

Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
2016. How not to evaluate your dialogue system:
An empirical study of unsupervised evaluation met-
rics for dialogue response generation. arXiv preprint
arXiv:1603.08023 .

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research 9(Nov):2579–2605.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics. Association for Computational
Linguistics, pages 311–318.

Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-
drew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. 2015. Generating sentences from a continuous
space. arXiv preprint arXiv:1511.06349 .

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP. volume 14, pages 1532–
43.

Boxing Chen and Colin Cherry. 2014. A systematic
comparison of smoothing techniques for sentence-
level bleu. ACL 2014 page 362.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555 .

Gabriel

Joelle

Pineau,

Forgues,

Jean-Marie
Larchevˆeque, and R´eal Tremblay. 2014. Boot-
strapping dialog systems with word embeddings.
In NIPS, Modern Machine Learning and Natural
Language Processing Workshop.

John J Godfrey and Edward Holliman. 1997.
Switchboard-1 release 2. Linguistic Data Consor-
tium, Philadelphia .

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Diederik P Kingma and Max Welling. 2013. Auto-
arXiv preprint

encoding variational bayes.
arXiv:1312.6114 .

Massimo Poesio and David Traum. 1998. Towards an
axiomatization of dialogue acts. In Proceedings of
the Twente Workshop on the Formal Semantics and
Pragmatics of Dialogues (13th Twente Workshop on
Language Technology. Citeseer.

Antoine Raux, Brian Langner, Dan Bohus, Alan W
Black, and Maxine Eskenazi. 2005. Lets go pub-
lic! taking a spoken dialog system to the real world.
In in Proc. of Interspeech 2005. Citeseer.

Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Stochastic backpropagation and
Wierstra. 2014.
approximate inference in deep generative models.
arXiv preprint arXiv:1401.4082 .

Eug´enio Ribeiro, Ricardo Ribeiro, and David Mar-
The inﬂuence of con-
arXiv preprint

tins de Matos. 2015.
text on dialogue act recognition.
arXiv:1506.00839 .

Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval.
Information processing & management 24(5):513–
523.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing 45(11):2673–2681.

Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016a. Building
end-to-end dialogue systems using generative hier-
archical neural network models. In Proceedings of
the 30th AAAI Conference on Artiﬁcial Intelligence
(AAAI-16).

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron Courville,
and Yoshua Bengio. 2016b. A hierarchical latent
variable encoder-decoder model for generating di-
alogues. arXiv preprint arXiv:1605.06069 .

Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015.
Learning structured output
representation using
In Advances
deep conditional generative models.
in Neural Information Processing Systems. pages
3483–3491.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. arXiv preprint
arXiv:1506.06714 .

Andreas Stolcke, Noah Coccaro, Rebecca Bates, Paul
Taylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-
beth Shriberg, Daniel Jurafsky, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for au-
tomatic tagging and recognition of conversational
speech. Computational linguistics 26(3):339–373.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.

Johan AK Suykens and Joos Vandewalle. 1999. Least
squares support vector machine classiﬁers. Neural
processing letters 9(3):293–300.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869 .

Jason D Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken
dialog systems. Computer Speech & Language
21(2):393–422.

Sam Wiseman and Alexander M Rush. 2016.
Sequence-to-sequence learning as beam-search op-
timization. arXiv preprint arXiv:1606.02960 .

Chen Xing, Wei Wu, Yu Wu,

Jie Liu, Yalou
Huang, Ming Zhou, and Wei-Ying Ma. 2016.
Topic augmented neural response generation with
arXiv preprint
a joint attention mechanism.
arXiv:1606.08340 .

Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak
Lee. 2015. Attribute2image: Conditional image
arXiv preprint
generation from visual attributes.
arXiv:1512.00570 .

Zhou Yu, Ziyu Xu, Alan W Black, and Alex I Rud-
nicky. 2016. Strategy and policy learning for non-
In 17th An-
task-oriented conversational systems.
nual Meeting of the Special Interest Group on Dis-
course and Dialogue. volume 2, page 7.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
Recurrent neural network regularization.

2014.
arXiv preprint arXiv:1409.2329 .

Tiancheng Zhao and Maxine Eskenazi. 2016. To-
wards end-to-end learning for dialog state tracking
and management using deep reinforcement learning.
arXiv preprint arXiv:1606.02560 .

A Supplemental Material

Variational Lower Bound for kgCVAE

We assume that even with the presence of linguis-
tic feature y regarding x, the prediction of xbow
still only depends on the z and c. Therefore, we
have:

L(θ, φ; x, c, y) = −KL(qφ(z|x, c, y)(cid:107)Pθ(z|c))

+ Eqφ(z|c,x,y)[log p(x|z, c, y)]
+ Eqφ(z|c,x,y)[log p(y|z, c)]
+ Eqφ(z|c,x,y)[log p(xbow|z, c)]

(7)

Collection of Multiple Reference Responses

We collected multiple reference responses for each
dialog context in the test set by information re-
trieval techniques combining with traditional a
machine learning method. First, we encode the di-
alog history using Term Frequency-Inverse Doc-
ument Frequency (TFIDF) (Salton and Buckley,
1988) weighted bag-of-words into vector repre-
sentation h. Then we denote the topic of the con-
versation as t and denote f as the conversation
if the speakers of the last utterance in
ﬂoor, i.e.
the dialog history and response utterance are the
same f = 1 otherwise f = 0. Then we com-
puted the similarity d(ci, cj) between two dialog
contexts using:

d(ci, cj) = 1(ti = tj)1(ti = tj)

hi · hj
||hi||||hj||

(8)

Unlike past work (Sordoni et al., 2015), this sim-
ilarity function only cares about the distance in

the context and imposes no constraints on the re-
sponse, therefore is suitbale for ﬁnding diverse re-
sponses regarding to the same dialog context. Sec-
ondly, for each dialog context in the test set, we
retrieved the 10 nearest neighbors from the train-
ing set and treated the responses from the training
set as candidate reference responses. Thirdly, we
further sampled 240 context-responses pairs from
5481 pairs in the total test set and post-processed
the selected candidate responses by two human
computational linguistic experts who were told to
give a binary label for each candidate response
about whether the response is appropriate regard-
ing its dialog context. The ﬁltered lists then served
as the ground truth to train our reference response
classiﬁer. For the next step, we extracted bigrams,
part-of-speech bigrams and word part-of-speech
pairs from both dialogue contexts and candidate
reference responses with rare threshold for feature
extraction being set to 20. Then L2-regularized
logistic regression with 10-fold cross validation
was applied as the machine learning algorithm.
Cross validation accuracy on the human-labelled
data was 71%. Finally, we automatically anno-
tated the rest of test set with this trained classiﬁer
and the resulting data were used for model evalu-
ation.

Learning Discourse-level Diversity for Neural Dialog Models using
Conditional Variational Autoencoders

Tiancheng Zhao, Ran Zhao and Maxine Eskenazi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, Pennsylvania, USA
{tianchez,ranzhao1,max+}@cs.cmu.edu

Abstract

recent neural

While
encoder-decoder
models have shown great promise in mod-
they
eling open-domain conversations,
often generate dull and generic responses.
Unlike past work that has focused on
diversifying the output of the decoder
at word-level to alleviate this problem,
we present a novel framework based on
conditional variational autoencoders that
captures the discourse-level diversity in
the encoder. Our model uses latent vari-
ables to learn a distribution over potential
conversational
generates
intents
diverse responses using only greedy de-
coders. We have further developed a novel
variant that is integrated with linguistic
prior knowledge for better performance.
Finally, the training procedure is improved
by introducing a bag-of-word loss. Our
proposed models have been validated
to generate signiﬁcantly more diverse
responses than baseline approaches and
exhibit competence in discourse-level
decision-making.1

and

Introduction

1
The dialog manager is one of the key components
of dialog systems, which is responsible for mod-
eling the decision-making process. Speciﬁcally, it
typically takes a new utterance and the dialog con-
text as input, and generates discourse-level deci-
sions (Bohus and Rudnicky, 2003; Williams and
Young, 2007). Advanced dialog managers usu-
ally have a list of potential actions that enable
them to have diverse behavior during a conver-
sation, e.g. different strategies to recover from
non-understanding (Yu et al., 2016). However,

1Data and an implementation of our model is avalaible at

https://github.com/snakeztc/NeuralDialog-CVAE

recent

the conventional approach of designing a dialog
manager (Williams and Young, 2007) does not
scale well to open-domain conversation models
because of the vast quantity of possible decisions.
Thus, there has been a growing interest in applying
encoder-decoder models (Sutskever et al., 2014)
for modeling open-domain conversation (Vinyals
and Le, 2015; Serban et al., 2016a). The basic ap-
proach treats a conversation as a transduction task,
in which the dialog history is the source sequence
and the next response is the target sequence. The
model is then trained end-to-end on large conver-
sation corpora using the maximum-likelihood esti-
mation (MLE) objective without the need for man-
ual crafting.
However

found that
encoder-decoder models tend to generate generic
and dull responses (e.g., I don’t know), rather
than meaningful and speciﬁc answers (Li et al.,
2015; Serban et al., 2016b). There have been
many attempts to explain and solve this limita-
tion, and they can be broadly divided into two cat-
egories (see Section 2 for details): (1) the ﬁrst cat-
egory argues that the dialog history is only one of
the factors that decide the next response. Other
features should be extracted and provided to the
models as conditionals in order to generate more
speciﬁc responses (Xing et al., 2016; Li et al.,
2016a); (2) the second category aims to improve
the encoder-decoder model itself, including de-
coding with beam search and its variations (Wise-
man and Rush, 2016), encouraging responses that
have long-term payoff (Li et al., 2016b), etc.

research has

Building upon the past work in dialog managers
and encoder-decoder models, the key idea of this
paper is to model dialogs as a one-to-many prob-
lem at the discourse level. Previous studies indi-
cate that there are many factors in open-domain
dialogs that decide the next response, and it is non-
Intuitively, given
trivial to extract all of them.

7
1
0
2
 
t
c
O
 
1
2
 
 
]
L
C
.
s
c
[
 
 
3
v
0
6
9
0
1
.
3
0
7
1
:
v
i
X
r
a

a similar dialog history (and other observed in-
puts), there may exist many valid responses (at the
discourse-level), each corresponding to a certain
conﬁguration of the latent variables that are not
presented in the input. To uncover the potential re-
sponses, we strive to model a probabilistic distri-
bution over the distributed utterance embeddings
of the potential responses using a latent variable
(Figure 1). This allows us to generate diverse re-
sponses by drawing samples from the learned dis-
tribution and reconstruct their words via a decoder
neural network.

Figure 1: Given A’s question, there exists many
valid responses from B for different assumptions
of the latent variables, e.g., B’s hobby.

Speciﬁcally, our contributions are three-fold:
1. We present a novel neural dialog model
adapted from conditional variational autoencoders
(CVAE) (Yan et al., 2015; Sohn et al., 2015),
which introduces a latent variable that can cap-
ture discourse-level variations as described above
2. We propose Knowledge-Guided CVAE (kgC-
VAE), which enables easy integration of expert
knowledge and results in performance improve-
ment and model interpretability. 3. We develop
a training method in addressing the difﬁculty of
optimizing CVAE for natural language genera-
tion (Bowman et al., 2015). We evaluate our
models on human-human conversation data and
yield promising results in: (a) generating appro-
priate and discourse-level diverse responses, and
(b) showing that the proposed training method is
more effective than the previous techniques.

2 Related Work

Our work is related to both recent advancement
in encoder-decoder dialog models and generative
models based on CVAE.

2.1 Encoder-decoder Dialog Models
Since the emergence of the neural dialog model,
the problem of output diversity has received much
attention in the research community.
Ideal out-
put responses should be both coherent and diverse.
However, most models end up with generic and
dull responses. To tackle this problem, one line
of research has focused on augmenting the in-

put of encoder-decoder models with richer con-
text information, in order to generate more spe-
ciﬁc responses. Li et al., (2016a) captured speak-
ers’ characteristics by encoding background infor-
mation and speaking style into the distributed em-
beddings, which are used to re-rank the generated
response from an encoder-decoder model. Xing et
al., (2016) maintain topic encoding based on La-
tent Dirichlet Allocation (LDA) (Blei et al., 2003)
of the conversation to encourage the model to out-
put more topic coherent responses.

On the other hand, many attempts have also
been made to improve the architecture of encoder-
decoder models. Li et al,. (2015) proposed to opti-
mize the standard encoder-decoder by maximizing
the mutual information between input and output,
which in turn reduces generic responses. This ap-
proach penalized unconditionally high frequency
responses, and favored responses that have high
conditional probability given the input. Wiseman
and Rush (2016) focused on improving the de-
coder network by alleviating the biases between
training and testing. They introduced a search-
based loss that directly optimizes the networks
for beam search decoding. The resulting model
achieves better performance on word ordering,
parsing and machine translation. Besides improv-
ing beam search, Li et al., (2016b) pointed out that
the MLE objective of an encoder-decoder model
is unable to approximate the real-world goal of
the conversation. Thus, they initialized a encoder-
decoder model with MLE objective and leveraged
reinforcement learning to ﬁne tune the model by
optimizing three heuristic rewards functions: in-
formativity, coherence, and ease of answering.

2.2 Conditional Variational Autoencoder
The variational autoencoder (VAE) (Kingma and
Welling, 2013; Rezende et al., 2014) is one of the
most popular frameworks for image generation.
The basic idea of VAE is to encode the input x
into a probability distribution z instead of a point
encoding in the autoencoder. Then VAE applies a
decoder network to reconstruct the original input
using samples from z. To generate images, VAE
ﬁrst obtains a sample of z from the prior distribu-
tion, e.g. N (0, I), and then produces an image via
the decoder network. A more advanced model, the
conditional VAE (CVAE), is a recent modiﬁcation
of VAE to generate diverse images conditioned on
certain attributes, e.g. generating different human
faces given skin color (Yan et al., 2015; Sohn et al.,

2015). Inspired by CVAE, we view the dialog con-
texts as the conditional attributes and adapt CVAE
to generate diverse responses instead of images.

Although VAE/CVAE has achieved impressive
results in image generation, adapting this to natu-
ral language generators is non-trivial. Bowman et
al., (2015) have used VAE with Long-Short Term
Memory (LSTM)-based recognition and decoder
networks to generate sentences from a latent Gaus-
sian variable. They showed that their model is able
to generate diverse sentences with even a greedy
LSTM decoder. They also reported the difﬁculty
of training because the LSTM decoder tends to ig-
nore the latent variable. We refer to this issue as
the vanishing latent variable problem. Serban et
al., (2016b) have applied a latent variable hierar-
chical encoder-decoder dialog model to introduce
utterance-level variations and facilitate longer re-
sponses. To improve upon the past models, we
ﬁrstly introduce a novel mechanism to leverage
linguistic knowledge in training end-to-end neural
dialog models, and we also propose a novel train-
ing technique that mitigates the vanishing latent
variable problem.

3 Proposed Models

Figure 2: Graphical models of CVAE (a) and kgC-
VAE (b)

3.1 Conditional Variational Autoencoder
(CVAE) for Dialog Generation

Each dyadic conversation can be represented via
three random variables: the dialog context c (con-
text window size k − 1), the response utterance x
(the kth utterance) and a latent variable z, which
is used to capture the latent distribution over the
valid responses. Further, c is composed of the dia-
log history: the preceding k-1 utterances; conver-
sational ﬂoor (1 if the utterance is from the same
speaker of x, otherwise 0) and meta features m
(e.g. the topic). We then deﬁne the conditional dis-
tribution p(x, z|c) = p(x|z, c)p(z|c) and our goal
is to use deep neural networks (parametrized by θ)
to approximate p(z|c) and p(x|z, c). We refer to

pθ(z|c) as the prior network and pθ(x, |z, c) as the
response decoder. Then the generative process of
x is (Figure 2 (a)):

1. Sample a latent variable z from the prior net-

work pθ(z|c).

pθ(x|z, c).

2. Generate x through the response decoder

CVAE is trained to maximize the conditional
log likelihood of x given c, which involves an in-
tractable marginalization over the latent variable
z. As proposed in (Sohn et al., 2015; Yan et al.,
2015), CVAE can be efﬁciently trained with the
Stochastic Gradient Variational Bayes (SGVB)
framework (Kingma and Welling, 2013) by maxi-
mizing the variational lower bound of the condi-
tional log likelihood. We assume the z follows
multivariate Gaussian distribution with a diago-
nal covariance matrix and introduce a recognition
network qφ(z|x, c) to approximate the true poste-
rior distribution p(z|x, c). Sohn and et al,. (2015)
have shown that the variational lower bound can
be written as:

L(θ, φ; x, c) = −KL(qφ(z|x, c)(cid:107)pθ(z|c))

+ Eqφ(z|c,x)[log pθ(x|z, c)]
≤ log p(x|c)

(1)

Figure 3 demonstrates an overview of our model.
The utterance encoder is a bidirectional recurrent
neural network (BRNN) (Schuster and Paliwal,
1997) with a gated recurrent unit (GRU) (Chung
et al., 2014) to encode each utterance into ﬁxed-
size vectors by concatenating the last hidden states
of the forward and backward RNN ui = [ (cid:126)hi, (cid:126)hi].
x is simply uk. The context encoder is a 1-layer
GRU network that encodes the preceding k-1 ut-
terances by taking u1:k−1 and the corresponding
conversation ﬂoor as inputs. The last hidden state
hc of the context encoder is concatenated with
meta features and c = [hc, m]. Since we assume z
follows isotropic Gaussian distribution, the recog-
nition network qφ(z|x, c) ∼ N (µ, σ2I) and the
prior network pθ(z|c) ∼ N (µ(cid:48), σ(cid:48)2I), and then we
have:

(cid:20)

(cid:21)
µ
log(σ2)
(cid:21)
µ(cid:48)
log(σ(cid:48)2)

(cid:20)

= Wr

+ br

(cid:21)

(cid:20)x
c

= MLPp(c)

(2)

(3)

Figure 3: The neural network architectures for the baseline and the proposed CVAE/kgCVAE models.
(cid:76) denotes the concatenation of the input vectors. The dashed blue connections only appear in kgCVAE.

We then use the reparametrization trick (Kingma
and Welling, 2013) to obtain samples of z either
from N (z; µ, σ2I) predicted by the recognition
network (training) or N (z; µ(cid:48), σ(cid:48)2I) predicted by
the prior network (testing). Finally, the response
decoder is a 1-layer GRU network with initial state
s0 = Wi[z, c]+bi. The response decoder then pre-
dicts the words in x sequentially.

3.2 Knowledge-Guided CVAE (kgCVAE)
In practice, training CVAE is a challenging opti-
mization problem and often requires large amount
of data. On the other hand, past research in spo-
ken dialog systems and discourse analysis has sug-
gested that many linguistic cues capture crucial
features in representing natural conversation. For
example, dialog acts (Poesio and Traum, 1998)
have been widely used in the dialog managers (Lit-
man and Allen, 1987; Raux et al., 2005; Zhao
and Eskenazi, 2016) to represent the propositional
function of the system. Therefore, we conjecture
that it will be beneﬁcial for the model to learn
meaningful latent z if it is provided with explicitly
extracted discourse features during the training.

In order to incorporate the linguistic features
into the basic CVAE model, we ﬁrst denote the set
of linguistic features as y. Then we assume that
the generation of x depends on c, z and y. y re-
lies on z and c as shown in Figure 2. Speciﬁcally,
during training the initial state of the response de-
coder is s0 = Wi[z, c, y] + bi and the input at ev-
ery step is [et, y] where et is the word embedding
of tth word in x. In addition, there is an MLP to
predict y(cid:48) = MLPy(z, c) based on z and c. In the
testing stage, the predicted y(cid:48) is used by the re-

sponse decoder instead of the oracle decoders. We
denote the modiﬁed model as knowledge-guided
CVAE (kgCVAE) and developers can add desired
discourse features that they wish the latent vari-
able z to capture. KgCVAE model is trained by
maximizing:

L(θ, φ; x, c, y) = −KL(qφ(z|x, c, y)(cid:107)Pθ(z|c))

+ Eqφ(z|c,x,y)[log p(x|z, c, y)]
+ Eqφ(z|c,x,y)[log p(y|z, c)]

(4)

Since now the reconstruction of y is a part of the
loss function, kgCVAE can more efﬁciently en-
code y-related information into z than discovering
it only based on the surface-level x and c. Another
advantage of kgCVAE is that it can output a high-
level label (e.g. dialog act) along with the word-
level responses, which allows easier interpretation
of the model’s outputs.

3.3 Optimization Challenges
A straightforward VAE with RNN decoder fails
to encode meaningful information in z due to the
vanishing latent variable problem (Bowman et al.,
2015). Bowman et al., (2015) proposed two solu-
tions: (1) KL annealing: gradually increasing the
weight of the KL term from 0 to 1 during training;
(2) word drop decoding: setting a certain percent-
age of the target words to 0. We found that CVAE
suffers from the same issue when the decoder is
an RNN. Also we did not consider word drop de-
coding because Bowman et al,. (2015) have shown
that it may hurt the performance when the drop
rate is too high.

As a result, we propose a simple yet novel tech-
nique to tackle the vanishing latent variable prob-
lem: bag-of-word loss. The idea is to introduce
an auxiliary loss that requires the decoder network
to predict the bag-of-words in the response x as
shown in Figure 3(b). We decompose x into two
variables: xo with word order and xbow without
order, and assume that xo and xbow are condi-
tionally independent given z and c: p(x, z|c) =
p(xo|z, c)p(xbow|z, c)p(z|c). Due to the condi-
tional independence assumption, the latent vari-
able is forced to capture global information about
the target response. Let f = MLPb(z, x) ∈ RV
where V is vocabulary size, and we have:

log p(xbow|z, c) = log

(5)

|x|
(cid:89)

t=1

efxt
j efj

(cid:80)V

where |x| is the length of x and xt is the word
index of tth word in x. The modiﬁed variational
lower bound for CVAE with bag-of-word loss is
(see Appendix A for kgCVAE):

L(cid:48)(θ, φ; x, c) = L(θ, φ; x, c)

+ Eqφ(z|c,x,y)[log p(xbow|z, c)] (6)

We will show that the bag-of-word loss in Equa-
tion 6 is very effective against the vanishing latent
variable and it is also complementary to the KL
annealing technique.

4 Experiment Setup
4.1 Dataset

We chose the Switchboard (SW) 1 Release 2 Cor-
pus (Godfrey and Holliman, 1997) to evaluate the
proposed models. SW has 2400 two-sided tele-
phone conversations with manually transcribed
speech and alignment.
In the beginning of the
call, a computer operator gave the callers recorded
prompts that deﬁne the desired topic of discus-
sion. There are 70 available topics. We ran-
domly split the data into 2316/60/62 dialogs for
train/validate/test. The pre-processing includes (1)
tokenize using the NLTK tokenizer (Bird et al.,
2009); (2) remove non-verbal symbols and re-
peated words due to false starts; (3) keep the
top 10K frequent word types as the vocabulary.
The ﬁnal data have 207, 833/5, 225/5, 481 (c, x)
pairs for train/validate/test. Furthermore, a sub-
set of SW was manually labeled with dialog
acts (Stolcke et al., 2000). We extracted dia-
log act labels based on the dialog act recognizer

proposed in (Ribeiro et al., 2015). The features
include the uni-gram and bi-gram of the utter-
ance, and the contextual features of the last 3 ut-
terances. We trained a Support Vector Machine
(SVM) (Suykens and Vandewalle, 1999) with lin-
ear kernel on the subset of SW with human anno-
tations. There are 42 types of dialog acts and the
SVM achieved 77.3% accuracy on held-out data.
Then the rest of SW data are labelled with dialog
acts using the trained SVM dialog act recognizer.

4.2 Training

We trained with the following hyperparameters
(according to the loss on the validate dataset):
word embedding has size 200 and is shared
across everywhere. We initialize the word embed-
ding from Glove embedding pre-trained on Twit-
ter (Pennington et al., 2014). The utterance en-
coder has a hidden size of 300 for each direc-
tion. The context encoder has a hidden size of
600 and the response decoder has a hidden size
of 400. The prior network and the MLP for pre-
dicting y both have 1 hidden layer of size 400 and
tanh non-linearity. The latent variable z has a
size of 200. The context window k is 10. All
the initial weights are sampled from a uniform
distribution [-0.08, 0.08]. The mini-batch size is
30. The models are trained end-to-end using the
Adam optimizer (Kingma and Ba, 2014) with a
learning rate of 0.001 and gradient clipping at 5.
We selected the best models based on the varia-
tional lower bound on the validate data. Finally,
we use the BOW loss along with KL annealing of
10,000 batches to achieve the best performance.
Section 5.4 gives a detailed argument for the im-
portance of the BOW loss.

5 Results
5.1 Experiments Setup

We compared three neural dialog models: a strong
baseline model, CVAE, and kgCVAE. The base-
line model is an encoder-decoder neural dialog
model without latent variables similar to (Serban
et al., 2016a). The baseline model’s encoder uses
the same context encoder to encode the dialog his-
tory and the meta features as shown in Figure 3.
The encoded context c is directly fed into the de-
coder networks as the initial state. The hyperpa-
rameters of the baseline are the same as the ones
reported in Section 4.2 and the baseline is trained
to minimize the standard cross entropy loss of the
decoder RNN model without any auxiliary loss.

Also, to compare the diversity introduced by the
stochasticity in the proposed latent variable ver-
sus the softmax of RNN at each decoding step, we
generate N responses from the baseline by sam-
pling from the softmax. For CVAE/kgCVAE, we
sample N times from the latent z and only use
greedy decoders so that the randomness comes en-
tirely from the latent variable z.

5.2 Quantitative Analysis

Automatically evaluating an open-domain gen-
erative dialog model is an open research chal-
lenge (Liu et al., 2016). Following our one-to-
many hypothesis, we propose the following met-
rics. We assume that for a given dialog context c,
there exist Mc reference responses rj, j ∈ [1, Mc].
Meanwhile a model can generate N hypothesis re-
sponses hi, i ∈ [1, N ]. The generalized response-
level precision/recall for a given dialog context is:

3. Dialog Act Match:

to measure the similar-
ity at the discourse level, the same dialog-
act tagger from 4.1 is applied to label all the
generated responses of each model. We set
d(rj, hi) = 1 if rj and hi have the same dia-
log acts, otherwise d(rj, hi) = 0.

One challenge of using the above metrics is that
there is only one, rather than multiple reference
responses/contexts. This impacts reliability of our
measures. Inspired by (Sordoni et al., 2015), we
utilized information retrieval techniques (see Ap-
pendix A) to gather 10 extra candidate reference
responses/context from other conversations with
the same topics. Then the 10 candidate references
are ﬁltered by two experts, which serve as the
ground truth to train the reference response classi-
ﬁer. The result is 6.69 extra references in average
per context. The average number of distinct refer-
ence dialog acts is 4.2. Table 1 shows the results.

precision(c) =

recall(c) =

(cid:80)N

i=1 maxj∈[1,Mc]d(rj, hi)
N
j=1 maxi∈[1,N ]d(rj, hi))
Mc

(cid:80)Mc

where d(rj, hi) is a distance function which lies
between 0 to 1 and measures the similarities be-
tween rj and hi. The ﬁnal score is averaged over
the entire test dataset and we report the perfor-
mance with 3 types of distance functions in or-
der to evaluate the systems from various linguistic
points of view:

1. Smoothed Sentence-level BLEU (Chen and
Cherry, 2014): BLEU is a popular metric that
measures the geometric mean of modiﬁed n-
gram precision with a length penalty (Pap-
ineni et al., 2002; Li et al., 2015). We use
BLEU-1 to 4 as our lexical similarity metric
and normalize the score to 0 to 1 scale.

2. Cosine Distance of Bag-of-word Embed-
ding: a simple method to obtain sentence
embeddings is to take the average or ex-
trema of all the word embeddings in the sen-
tences (Forgues et al., 2014; Adi et al., 2016).
The d(rj, hi) is the cosine distance of the two
embedding vectors. We used Glove embed-
ding described in Section 4 and denote the av-
erage method as A-bow and extrema method
as E-bow. The score is normalized to [0, 1].

Metrics
perplexity (KL)

BLEU-1 prec
BLEU-1 recall
BLEU-2 prec
BLEU-2 recall
BLEU-3 prec
BLEU-3 recall
BLEU-4 prec
BLEU-4 recall
A-bow prec
A-bow recall
E-bow prec
E-bow recall
DA prec
DA recall

Baseline CVAE
35.4
(n/a)
0.405
0.336
0.300
0.281
0.272
0.254
0.226
0.215
0.951
0.935
0.827
0.801
0.736
0.514

20.2
(11.36)
0.372
0.381
0.295
0.322
0.265
0.292
0.223
0.248
0.954
0.943
0.815
0.812
0.704
0.604

kgCVAE
16.02
(13.08)
0.412
0.411
0.350
0.356
0.310
0.318
0.262
0.272
0.961
0.944
0.804
0.807
0.721
0.598

Table 1: Performance of each model on automatic
measures. The highest score in each row is in
bold. Note that our BLEU scores are normalized
to [0, 1].

The proposed models outperform the baseline
in terms of recall in all the metrics with statis-
tical signiﬁcance. This conﬁrms our hypothesis
that generating responses with discourse-level di-
versity can lead to a more comprehensive cov-
erage of the potential responses than promoting
only word-level diversity. As for precision, we
observed that the baseline has higher or similar

scores than CVAE in all metrics, which is expected
since the baseline tends to generate the mostly
likely and safe responses repeatedly in the N hy-
potheses. However, kgCVAE is able to achieve
the highest precision and recall in the 4 metrics at
the same time (BLEU1-4, A-BOW). One reason
for kgCVAE’s good performance is that the pre-
dicted dialog act label in kgCVAE can regularize
the generation process of its RNN decoder by forc-
ing it to generate more coherent and precise words.
We further analyze the precision/recall of BLEU-
4 by looking at the average score versus the num-
ber of distinct reference dialog acts. A low num-
ber of distinct dialog acts represents the situation
where the dialog context has a strong constraint
on the range of the next response (low entropy),
while a high number indicates the opposite (high-
entropy). Figure 4 shows that CVAE/kgCVAE
achieves signiﬁcantly higher recall than the base-
line in higher entropy contexts. Also it shows
that CVAE suffers from lower precision, espe-
cially in low entropy contexts. Finally, kgCVAE
gets higher precision than both the baseline and
CVAE in the full spectrum of context entropy.

able to generate various ways of back-channeling.
This implies that the latent z is able to capture
context-sensitive variations, i.e. in low-entropy di-
alog contexts modeling lexical diversity while in
high-entropy ones modeling discourse-level diver-
sity. Moreover, kgCVAE is occasionally able to
generate more sophisticated grounding (sample 4)
beyond a simple back-channel, which is also an
acceptable response given the dialog context.

In addition, past work (Kingma and Welling,
2013) has shown that the recognition network is
able to learn to cluster high-dimension data, so
we conjecture that posterior z outputted from the
recognition network should cluster the responses
into meaningful groups. Figure 5 visualizes the
posterior z of responses in the test dataset in 2D
space using t-SNE (Maaten and Hinton, 2008).
We found that the learned latent space is highly
correlated with the dialog act and length of re-
sponses, which conﬁrms our assumption.

Figure 4: BLEU-4 precision/recall vs. the number
of distinct reference dialog acts.

5.3 Qualitative Analysis

Table 2 shows the outputs generated from the
baseline and kgCVAE. In example 1, caller A be-
gins with an open-ended question. The kgCVAE
model generated highly diverse answers that cover
multiple plausible dialog acts. Further, we notice
that the generated text exhibits similar dialog acts
compared to the ones predicted separately by the
model, implying the consistency of natural lan-
guage generation based on y. On the contrary, the
responses from the baseline model are limited to
local n-gram variations and share a similar preﬁx,
i.e. ”I’m”. Example 2 is a situation where caller
A is telling B stories. The ground truth response
is a back-channel and the range of valid answers is
more constrained than example 1 since B is play-
ing the role of a listener. The baseline successfully
predicts ”uh-huh”. The kgCVAE model is also

Figure 5: t-SNE visualization of the posterior z for
test responses with top 8 frequent dialog acts. The
size of circle represents the response length.

5.4 Results for Bag-of-Word Loss

Finally, we evaluate the effectiveness of bag-of-
word (BOW) loss for training VAE/CVAE with the
RNN decoder. To compare with past work (Bow-
man et al., 2015), we conducted the same lan-
guage modelling (LM) task on Penn Treebank us-
ing VAE. The network architecture is same ex-
cept we use GRU instead of LSTM. We compared
four different training setups: (1) standard VAE
without any heuristics; (2) VAE with KL anneal-
ing (KLA); (3) VAE with BOW loss; (4) VAE
with both BOW loss and KLA. Intuitively, a well
trained model should lead to a low reconstruction

Example 1-Topic: Recycling Context: A: are they doing a lot of recycling out in Georgia? Target-B (statement): well
at my workplace we have palaces for aluminium cans and we have a separate trash can for recyclable paper
Baseline+Sampling
1. well I’m a graduate student and have two kids
2. well I was in last year and so we’ve had lots of recycling

kgCVAE+Greedy
1. (non-understand) pardon
2. (statement) oh you’re not going to have a curbside pick
up here
3. (statement) okay I am sure about a recycling center
4. (yes-answer) yeah so

3. I’m not sure
4. well I don’t know I just moved here in new york

Example 2-Topic: Child Care Context: A: you know a private home to take their children to when they’re young until
they hit the preschool age and they Target-B (backchannel): uh-huh
Baseline+Sampling
1. um - hum
2. yeah
3. um - hum
4. uh-huh

kgCVAE+Greedy
1. (backchannel) uh-huh
2. (turn-exit) um-hum
3. (backchannel) yeah
4. (statement) oh yeah I think that’s part of the problem

Table 2: Generated responses from the baselines and kgCVAE in two examples. KgCVAE also provides
the predicted dialog act for each response. The context only shows the last utterance due to space limit
(the actual context window size is 10).

portance of BOW loss for training latent vari-
able models with the RNN decoder. Last but not
least, our experiments showed that the conclusions
drawn from LM using VAE also apply to training
CVAE/kgCVAE, so we used BOW loss together
with KLA for all previous experiments.

loss and small but non-trivial KL cost. For all
models with KLA, the KL weight increases lin-
early from 0 to 1 in the ﬁrst 5000 batches.

Table 3 shows the reconstruction perplexity and
the KL cost on the test dataset. The standard VAE
fails to learn a meaningful latent variable by hav-
ing a KL cost close to 0 and a reconstruction per-
plexity similar to a small LSTM LM (Zaremba
et al., 2014). KLA helps to improve the recon-
struction loss, but it requires early stopping since
the models will fall back to the standard VAE after
the KL weight becomes 1. At last, the models with
BOW loss achieved signiﬁcantly lower perplexity
and larger KL cost.

Perplexity
Model
122.0
Standard
111.5
KLA
BOW
97.72
BOW+KLA 73.04

KL cost
0.05
2.02
7.41
15.94

Table 3: The reconstruction perplexity and KL
terms on Penn Treebank test set.

Figure 6 visualizes the evolution of the KL cost.
We can see that for the standard model, the KL
cost crashes to 0 at the beginning of training and
never recovers. On the contrary, the model with
only KLA learns to encode substantial informa-
tion in latent z when the KL cost weight is small.
However, after the KL weight is increased to 1 (af-
ter 5000 batch), the model once again decides to
ignore the latent z and falls back to the naive im-
plementation. The model with BOW loss, how-
ever, consistently converges to a non-trivial KL
cost even without KLA, which conﬁrms the im-

Figure 6: The value of the KL divergence during
training with different setups on Penn Treebank.

6 Conclusion and Future Work
In conclusion, we identiﬁed the one-to-many na-
ture of open-domain conversation and proposed
two novel models that show superior performance
in generating diverse and appropriate responses at
the discourse level. While the current paper ad-
dresses diversifying responses in respect to dia-
logue acts, this work is part of a larger research
direction that targets leveraging both past linguis-
tic ﬁndings and the learning power of deep neural
networks to learn better representation of the la-
tent factors in dialog. In turn, the output of this
novel neural dialog model will be easier to ex-
plain and control by humans.
In addition to di-
alog acts, we plan to apply our kgCVAE model

to capture other different linguistic phenomena in-
cluding sentiment, named entities,etc. Last but
not least, the recognition network in our model
will serve as the foundation for designing a data-
driven dialog manager, which automatically dis-
covers useful high-level intents. All of the above
suggest a promising research direction.

7 Acknowledgements

This work was funded by NSF grant CNS-
1512973. The opinions expressed in this paper do
not necessarily reﬂect those of NSF.

References

Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer
Lavi, and Yoav Goldberg. 2016. Fine-grained anal-
ysis of sentence embeddings using auxiliary predic-
tion tasks. arXiv preprint arXiv:1608.04207 .

Steven Bird, Ewan Klein, and Edward Loper. 2009.
”

language processing with Python.

Natural
O’Reilly Media, Inc.”.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of ma-
chine Learning research 3(Jan):993–1022.

Dan Bohus and Alexander I Rudnicky. 2003. Raven-
claw: Dialog management using hierarchical task
decomposition and an expectation agenda .

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2015. A diversity-promoting objec-
tive function for neural conversation models. arXiv
preprint arXiv:1510.03055 .

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016a. A persona-based neural con-
versation model. arXiv preprint arXiv:1603.06155
.

Jiwei Li, Will Monroe, Alan Ritter, and Dan Jurafsky.
2016b. Deep reinforcement learning for dialogue
generation. arXiv preprint arXiv:1606.01541 .

Diane J Litman and James F Allen. 1987. A plan
recognition model for subdialogues in conversa-
tions. Cognitive science 11(2):163–200.

Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
2016. How not to evaluate your dialogue system:
An empirical study of unsupervised evaluation met-
rics for dialogue response generation. arXiv preprint
arXiv:1603.08023 .

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research 9(Nov):2579–2605.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics. Association for Computational
Linguistics, pages 311–318.

Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-
drew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. 2015. Generating sentences from a continuous
space. arXiv preprint arXiv:1511.06349 .

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP. volume 14, pages 1532–
43.

Boxing Chen and Colin Cherry. 2014. A systematic
comparison of smoothing techniques for sentence-
level bleu. ACL 2014 page 362.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555 .

Gabriel

Joelle

Pineau,

Forgues,

Jean-Marie
Larchevˆeque, and R´eal Tremblay. 2014. Boot-
strapping dialog systems with word embeddings.
In NIPS, Modern Machine Learning and Natural
Language Processing Workshop.

John J Godfrey and Edward Holliman. 1997.
Switchboard-1 release 2. Linguistic Data Consor-
tium, Philadelphia .

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Diederik P Kingma and Max Welling. 2013. Auto-
arXiv preprint

encoding variational bayes.
arXiv:1312.6114 .

Massimo Poesio and David Traum. 1998. Towards an
axiomatization of dialogue acts. In Proceedings of
the Twente Workshop on the Formal Semantics and
Pragmatics of Dialogues (13th Twente Workshop on
Language Technology. Citeseer.

Antoine Raux, Brian Langner, Dan Bohus, Alan W
Black, and Maxine Eskenazi. 2005. Lets go pub-
lic! taking a spoken dialog system to the real world.
In in Proc. of Interspeech 2005. Citeseer.

Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Stochastic backpropagation and
Wierstra. 2014.
approximate inference in deep generative models.
arXiv preprint arXiv:1401.4082 .

Eug´enio Ribeiro, Ricardo Ribeiro, and David Mar-
The inﬂuence of con-
arXiv preprint

tins de Matos. 2015.
text on dialogue act recognition.
arXiv:1506.00839 .

Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval.
Information processing & management 24(5):513–
523.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing 45(11):2673–2681.

Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016a. Building
end-to-end dialogue systems using generative hier-
archical neural network models. In Proceedings of
the 30th AAAI Conference on Artiﬁcial Intelligence
(AAAI-16).

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron Courville,
and Yoshua Bengio. 2016b. A hierarchical latent
variable encoder-decoder model for generating di-
alogues. arXiv preprint arXiv:1605.06069 .

Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015.
Learning structured output
representation using
In Advances
deep conditional generative models.
in Neural Information Processing Systems. pages
3483–3491.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. arXiv preprint
arXiv:1506.06714 .

Andreas Stolcke, Noah Coccaro, Rebecca Bates, Paul
Taylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-
beth Shriberg, Daniel Jurafsky, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for au-
tomatic tagging and recognition of conversational
speech. Computational linguistics 26(3):339–373.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.

Johan AK Suykens and Joos Vandewalle. 1999. Least
squares support vector machine classiﬁers. Neural
processing letters 9(3):293–300.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869 .

Jason D Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken
dialog systems. Computer Speech & Language
21(2):393–422.

Sam Wiseman and Alexander M Rush. 2016.
Sequence-to-sequence learning as beam-search op-
timization. arXiv preprint arXiv:1606.02960 .

Chen Xing, Wei Wu, Yu Wu,

Jie Liu, Yalou
Huang, Ming Zhou, and Wei-Ying Ma. 2016.
Topic augmented neural response generation with
arXiv preprint
a joint attention mechanism.
arXiv:1606.08340 .

Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak
Lee. 2015. Attribute2image: Conditional image
arXiv preprint
generation from visual attributes.
arXiv:1512.00570 .

Zhou Yu, Ziyu Xu, Alan W Black, and Alex I Rud-
nicky. 2016. Strategy and policy learning for non-
In 17th An-
task-oriented conversational systems.
nual Meeting of the Special Interest Group on Dis-
course and Dialogue. volume 2, page 7.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
Recurrent neural network regularization.

2014.
arXiv preprint arXiv:1409.2329 .

Tiancheng Zhao and Maxine Eskenazi. 2016. To-
wards end-to-end learning for dialog state tracking
and management using deep reinforcement learning.
arXiv preprint arXiv:1606.02560 .

A Supplemental Material

Variational Lower Bound for kgCVAE

We assume that even with the presence of linguis-
tic feature y regarding x, the prediction of xbow
still only depends on the z and c. Therefore, we
have:

L(θ, φ; x, c, y) = −KL(qφ(z|x, c, y)(cid:107)Pθ(z|c))

+ Eqφ(z|c,x,y)[log p(x|z, c, y)]
+ Eqφ(z|c,x,y)[log p(y|z, c)]
+ Eqφ(z|c,x,y)[log p(xbow|z, c)]

(7)

Collection of Multiple Reference Responses

We collected multiple reference responses for each
dialog context in the test set by information re-
trieval techniques combining with traditional a
machine learning method. First, we encode the di-
alog history using Term Frequency-Inverse Doc-
ument Frequency (TFIDF) (Salton and Buckley,
1988) weighted bag-of-words into vector repre-
sentation h. Then we denote the topic of the con-
versation as t and denote f as the conversation
if the speakers of the last utterance in
ﬂoor, i.e.
the dialog history and response utterance are the
same f = 1 otherwise f = 0. Then we com-
puted the similarity d(ci, cj) between two dialog
contexts using:

d(ci, cj) = 1(ti = tj)1(ti = tj)

hi · hj
||hi||||hj||

(8)

Unlike past work (Sordoni et al., 2015), this sim-
ilarity function only cares about the distance in

the context and imposes no constraints on the re-
sponse, therefore is suitbale for ﬁnding diverse re-
sponses regarding to the same dialog context. Sec-
ondly, for each dialog context in the test set, we
retrieved the 10 nearest neighbors from the train-
ing set and treated the responses from the training
set as candidate reference responses. Thirdly, we
further sampled 240 context-responses pairs from
5481 pairs in the total test set and post-processed
the selected candidate responses by two human
computational linguistic experts who were told to
give a binary label for each candidate response
about whether the response is appropriate regard-
ing its dialog context. The ﬁltered lists then served
as the ground truth to train our reference response
classiﬁer. For the next step, we extracted bigrams,
part-of-speech bigrams and word part-of-speech
pairs from both dialogue contexts and candidate
reference responses with rare threshold for feature
extraction being set to 20. Then L2-regularized
logistic regression with 10-fold cross validation
was applied as the machine learning algorithm.
Cross validation accuracy on the human-labelled
data was 71%. Finally, we automatically anno-
tated the rest of test set with this trained classiﬁer
and the resulting data were used for model evalu-
ation.

