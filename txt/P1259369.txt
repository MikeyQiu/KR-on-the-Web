0
2
0
2
 
n
a
J
 
4
2
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
4
6
0
4
0
.
2
0
8
1
:
v
i
X
r
a

A Contextual Bandit Bake-off

A Contextual Bandit Bake-oﬀ

Alberto Bietti
Inria, Ecole normale sup´erieure, PSL Research University, Paris, France∗
Alekh Agarwal
Microsoft Research, Redmond, WA
John Langford
Microsoft Research, New York, NY

alberto.bietti@inria.fr

alekha@microsoft.com

jcl@microsoft.com

Abstract

Contextual bandit algorithms are essential for solving many real-world interactive ma-
chine learning problems. Despite multiple recent successes on statistically and computation-
ally eﬃcient methods, the practical behavior of these algorithms is still poorly understood.
We leverage the availability of large numbers of supervised learning datasets to empirically
evaluate contextual bandit algorithms, focusing on practical methods that learn by relying
on optimization oracles from supervised learning. We ﬁnd that a recent method (Foster
et al., 2018) using optimism under uncertainty works the best overall. A surprisingly close
second is a simple greedy baseline that only explores implicitly through the diversity of
contexts, followed by a variant of Online Cover (Agarwal et al., 2014) which tends to be
more conservative but robust to problem speciﬁcation by design. Along the way, we also
evaluate various components of contextual bandit algorithm design such as loss estimators.
Overall, this is a thorough study and review of contextual bandit methodology.
Keywords:

contextual bandits, online learning, evaluation

1. Introduction

At a practical level, how should contextual bandit learning and exploration be done?

In the contextual bandit problem, a learner repeatedly observes a context, chooses an
action, and observes a loss for the chosen action only. Many real-world interactive machine
learning tasks are well-suited to this setting: a movie recommendation system selects a
movie for a given user and receives feedback (click or no click) only for that movie; a choice
of medical treatment may be prescribed to a patient with an outcome observed for (only)
the chosen treatment. The limited feedback (known as bandit feedback) received by the
learner highlights the importance of exploration, which needs to be addressed by contextual
bandit algorithms.

The focal point of contextual bandit (henceforth CB) learning research is eﬃcient ex-
ploration algorithms (Abbasi-Yadkori et al., 2011; Agarwal et al., 2012, 2014; Agrawal and
Goyal, 2013; Dudik et al., 2011a; Langford and Zhang, 2008; Russo et al., 2018). How-
ever, many of these algorithms remain far from practical, and even when considering more
practical variants, their empirical behavior is poorly understood, typically with limited eval-

∗. Part of this work was done while AB was visiting Microsoft Research NY, supported by the MSR-Inria

Joint Center.

1

Bietti, Agarwal and Langford

uation on just a handful of scenarios. In particular, strategies based on upper conﬁdence
bounds (Abbasi-Yadkori et al., 2011; Li et al., 2010) or Thompson sampling (Agrawal and
Goyal, 2013; Russo et al., 2018) are often intractable for sparse, high-dimensional datasets,
and make strong assumptions on the model representation. The method of Agarwal et al.
(2014) alleviates some of these diﬃculties while being statistically optimal under weak as-
sumptions, but the analyzed version is still far from practical, and the worst-case guarantees
may lead to overly conservative exploration that can be ineﬃcient in practice.

The main objective of our work is an evaluation of practical methods that are relevant
to practitioners. We focus on algorithms that rely on optimization oracles from supervised
learning such as cost-sensitive classiﬁcation or regression oracles, which provides computa-
tional eﬃciency and support for generic representations. We further rely on online learning
implementations of the oracles, which are desirable in practice due to the sequential na-
ture of contextual bandits. While conﬁdence-based strategies and Thompson sampling are
not directly adapted to this setting, we achieve it with online Bootstrap approximations for
Thompson sampling (Agarwal et al., 2014; Eckles and Kaptein, 2014; Osband and Van Roy,
2015), and with the conﬁdence-based method of Foster et al. (2018) based on regression
oracles, which contains LinUCB as a special case. Additionally, we consider practical de-
sign choices such as loss encodings (e.g., if values have a range of 1, should we encode the
costs of best and worst outcomes as 0/1 or -1/0?), and for methods that learn by reduc-
tion to oﬀ-policy learning, we study diﬀerent reduction techniques beyond the simple inverse
propensity scoring approach. All of our experiments are based on the online learning system
Vowpal Wabbit1 which has already been successfully used in production systems (Agarwal
et al., 2016).

The interactive aspect of CB problems makes them notoriously diﬃcult to evaluate in
real-world settings beyond a handful of tasks. Instead, we leverage the wide availability of
supervised learning datasets with diﬀerent cost structures on their predictions, and obtain
contextual bandit instances by simulating bandit feedback, treating labels as actions and
hiding the loss of all actions but the chosen one. This setup captures the generality of the
i.i.d. contextual bandit setting, while avoiding some diﬃcult aspects of real-world settings
that are not supported by most existing algorithms and are diﬃcult to evaluate, such as non-
stationarity. We consider a large collection of over 500 datasets with varying characteristics
and various cost structures, including multiclass, multilabel and more general cost-sensitive
datasets with real-valued costs. To our knowledge, this is the ﬁrst evaluation of contextual
bandit algorithms on such a large and diverse corpus of datasets.

Our evaluation considers online implementations of Bootstrap Thompson sampling (Agar-
wal et al., 2014; Eckles and Kaptein, 2014; Osband and Van Roy, 2015), the Cover approach
of Agarwal et al. (2014), (cid:15)-greedy (Langford and Zhang, 2008), RegCB (Foster et al., 2018,
which includes LinUCB as a special case), and a basic greedy method similar to the one
studied in Bastani et al. (2017); Kannan et al. (2018). As the ﬁrst conclusion of our study,
we ﬁnd that the recent RegCB method (Foster et al., 2018) performs the best overall across
a number of experimental conditions. Remarkably, we discover that a close second in our set
of methods is the simple greedy baseline, often outperforming most exploration algorithms.
Both these methods have drawbacks in theory; greedy can fail arbitrarily poorly in prob-

1. https://vowpalwabbit.org

2

A Contextual Bandit Bake-off

Figure 1: Comparison between three competitive approaches: RegCB (conﬁdence based),
Cover-NU (variant of Online Cover) and Greedy. The plots show relative loss compared
to supervised learning (lower is better) on all datasets with 5 actions or more. Red points
indicate datasets with a statistically signiﬁcant diﬀerence in loss between two methods. A
greedy approach can outperform exploration methods in many cases; yet both Greedy and
RegCB may fail to explore eﬃciently on some other datasets where Cover-NU dominates.

lems where intentional exploration matters, while UCB methods make stronger modeling
assumptions and can have an uncontrolled regret when the assumptions fail. The logs col-
lected by deploying these methods in practice are also unﬁt for later oﬀ-policy experiments,
an important practical consideration. Our third conclusion is that several methods which
are more robust in that they make only a relatively milder i.i.d. assumption on the problem
tend to be overly conservative and often pay a steep price on easier datasets. Nevertheless,
we ﬁnd that an adaptation of Online Cover (Agarwal et al., 2014) is quite competitive on a
large fraction of our datasets. We also evaluate the eﬀect of diﬀerent ways to encode losses
and study diﬀerent reduction mechanisms for exploration algorithms that rely on oﬀ-policy
learning (such as (cid:15)-greedy), ﬁnding that a technique based on importance-weighted regres-
sion tends to outperform other approaches when applicable. We show pairwise comparisons
between the top 3 methods in our evaluation in Figure 1 for datasets with 5 or more actions.
For future theoretical research, our results motivate an emphasis on understanding greedy
strategies, building on recent progress (Bastani et al., 2017; Kannan et al., 2018), as well
as eﬀectively leveraging easier datasets in exploration problems (Agarwal et al., 2017).

1.1 Organization of the paper

The paper is organized as follows:

• Section 2 provides relevant background on i.i.d. contextual bandits, optimization or-
acles, and mechanisms for reduction to oﬀ-policy learning, and introduces our exper-
imental setup.

• Section 3 describes the main algorithms we consider in our evaluation, as well as the

modiﬁcations that we found eﬀective in practice.

• Section 4 presents the results and insights from our experimental evaluation.
• Finally, we conclude in Section 5 with a discussion of our ﬁndings and a collection
of guidelines and recommendations for practitioners that come out of our empirical
study, as well as open questions for theoreticians.

3

Bietti, Agarwal and Langford

2. Contextual Bandit Setup

In this section, we present the learning setup considered in this work, recalling the stochastic
contextual bandit setting, the notion of optimization oracles, various techniques used by
contextual bandit algorithms for leveraging these oracles, and ﬁnally our experimental setup.

2.1 Learning Setting

The stochastic (i.i.d.) contextual bandit learning problem can be described as follows. At
each time step t, the environment produces a pair (xt, (cid:96)t) ∼ D independently from the past,
where xt ∈ X is a context vector and (cid:96)t = ((cid:96)t(1), . . . , (cid:96)t(K)) ∈ RK is a loss vector, with K
the number of possible actions, and the data distribution is denoted D. After observing the
context xt, the learner chooses an action at, and only observes the loss (cid:96)t(at) corresponding
to the chosen action. The goal of the learner is to trade-oﬀ exploration and exploitation in
order to incur a small cumulative regret

RT :=

(cid:96)t(at) −

(cid:96)t(π∗(xt)),

T
(cid:88)

t=1

T
(cid:88)

t=1

with respect to the optimal policy π∗ ∈ arg minπ∈Π E(x,(cid:96))∼D[(cid:96)(π(x))], where Π denotes a
(large, possibly inﬁnite) set of policies π : X → {1, . . . , K} which we would like to do well
against.
It is often important for the learner to use randomized strategies, for instance
in order to later evaluate or optimize new policies, hence we let pt(a) ∈ [0, 1] denote the
probability that the agent chooses action a ∈ {1, . . . , K} at time t, so that at ∼ pt.

2.2 Optimization Oracles

In this paper, we focus on CB algorithms which rely on access to an optimization oracle
for solving optimization problems similar to those that arise in supervised learning, leading
to methods that are suitable for general policy classes Π. The main example is the cost-
sensitive classiﬁcation (CSC) oracle (Agarwal et al., 2014; Dudik et al., 2011a; Langford
and Zhang, 2008), which given a collection (x1, c1), . . . , (xT , cT ) ∈ X × RK computes

T
(cid:88)

arg min
π∈Π

ct(π(xt)).

t=1
The cost vectors ct = (ct(1), . . . , ct(K)) ∈ RK are often constructed using counterfactual
estimates of the true (unobserved) losses, as we describe in the next section.

Another approach is to use regression oracles, which ﬁnd f : X ×{1, . . . , K} → R from
a class of regressor functions F to predict a cost yt, given a context xt and action at (see,
e.g., Agarwal et al., 2012; Foster et al., 2018).
In this paper, we consider the following
regression oracle with importance weights ωt > 0:

(1)

(2)

While the theory typically requires exact solutions to (1) or (2), this is often impractical
due to the diﬃculty of the underlying optimization problem (especially for CSC, which

arg min
f ∈F

T
(cid:88)

t=1

ωt(f (xt, at) − yt)2.

4

A Contextual Bandit Bake-off

yields a non-convex and non-smooth problem), and more importantly because the size of the
problems to be solved keeps increasing after each iteration. In this work, we consider instead
the use of online optimization oracles for solving problems (1) or (2), which incrementally
update a given policy or regression function after each new observation, using for instance
an online gradient method. Such an online learning approach is natural in the CB setting,
and is common in interactive production systems (e.g., Agarwal et al., 2016; He et al., 2014;
McMahan et al., 2013).

2.3 Loss Estimates and Reductions

A common approach to solving problems with bandit (partial) feedback is to compute an
estimate of the full feedback using the observed loss and then apply methods for the full-
information setting to these estimated values. In the case of CBs, this allows an algorithm
to ﬁnd a good policy based on oﬀ-policy exploration data collected by the algorithm. These
loss estimates are commonly used to create CSC instances to be solved by the optimization
oracle introduced above (Agarwal et al., 2014; Dudik et al., 2011a; Langford and Zhang,
2008), a process sometimes referred as reduction to cost-sensitive classiﬁcation. Given such
estimates ˆ(cid:96)t(a) of (cid:96)t(a) for all actions a and for t = 1, . . . , T , such a reduction constructs cost
vectors ct = (ˆ(cid:96)t(1), . . . , ˆ(cid:96)t(K)) ∈ RK and feeds them along with the observed contexts xt
to the CSC oracle (1) in order to obtain a policy. We now describe the three diﬀerent
estimation methods considered in this paper, and how each is typically used for reduction
to policy learning with a CSC or regression oracle. In what follows, we consider observed
interaction records (xt, at, (cid:96)t(at), pt(at)).

Perhaps the simplest approach is the inverse propensity-scoring (IPS) estimator:

ˆ(cid:96)t(a) :=

1{a = at}.

(cid:96)t(at)
pt(at)
For any action a with pt(a) > 0, this estimator is unbiased, i.e. Eat∼pt[ˆ(cid:96)t(a)] = (cid:96)t(a), but
can have high variance when pt(at) is small. The estimator leads to a straightforward
CSC example (xt, ˆ(cid:96)t). Using such examples in (1) provides a way to perform oﬀ-policy
(or counterfactual) evaluation and optimization, which in turn allows a CB algorithm to
identify good policies for exploration.
In order to obtain good unbiased estimates, one
needs to control the variance of the estimates, e.g., by enforcing a minimum exploration
probability pt(a) ≥ (cid:15) > 0 on all actions.

(3)

In order to reduce the variance of IPS, the doubly robust (DR) estimator (Dudik

et al., 2011b) uses a separate, possibly biased, estimator of the loss ˆ(cid:96)(x, a):

ˆ(cid:96)t(a) :=

(cid:96)t(at) − ˆ(cid:96)(xt, at)
pt(at)

1{a = at} + ˆ(cid:96)(xt, a).

When ˆ(cid:96)(xt, at) is a good estimate of (cid:96)t(at), the small numerator in the ﬁrst term helps
reduce the variance induced by a small denominator, while the second term ensures that
the estimator is unbiased. Typically, ˆ(cid:96)(x, a) is learned by regression on all past observed
losses, e.g.,

(4)

(5)

ˆ(cid:96) := arg min
f ∈F

T
(cid:88)

t=1

(f (xt, at) − (cid:96)t(at))2.

5

Bietti, Agarwal and Langford

The reduction to cost-sensitive classiﬁcation is similar to IPS, by feeding cost vectors ct = ˆ(cid:96)t
to the CSC oracle.

We consider a third method that directly reduces to the importance-weighted regression
oracle (2), which we refer to as IWR (for importance-weighted regression), and is
suitable for algorithms which rely on oﬀ-policy learning.2 This approach ﬁnds a regressor

ˆf := arg min
f ∈F

T
(cid:88)

t=1

1
pt(at)

(f (xt, at) − (cid:96)t(at))2,

(6)

and considers the policy ˆπ(x) = arg mina ˆf (x, a). Such an estimator has been used, e.g.,
in the context of oﬀ-policy learning for recommendations (Schnabel et al., 2016) and is
available in the Vowpal Wabbit library. Note that if pt has full support, then the objective
is an unbiased estimate of the full regression objective on all actions,

T
(cid:88)

K
(cid:88)

t=1

a=1

(f (xt, a) − (cid:96)t(a))2.

In contrast, if the learner only explores a single action (so that pt(at) = 1 for all t), the
obtained regressor ˆf is the same as the loss estimator ˆ(cid:96) in (5). In this csae, if we consider
a x with x ∈ Rd, then the IWR reduction
a linear class of regressors of the form f (x, a) = θ(cid:62)
computes least-squares estimates ˆθa from the data observed when action a was chosen.
When actions are selected according to the greedy policy at = arg mina ˆθ(cid:62)
a xt, this setup
corresponds to the greedy algorithm considered, e.g., by Bastani et al. (2017).

Note that while CSC is typically intractable and requires approximations in order to
In
work in practice, importance-weighted regression does not suﬀer from these issues.
addition, while the computational cost for an approximate CSC online update scales with
the number of actions K, IWR only requires an update for a single action, making the
approach more attractive computationally. Another beneﬁt of IWR in an online setting is
that it can leverage importance weight aware online updates (Karampatziakis and Langford,
2011), which makes it easier to handle large inverse propensity scores.

2.4 Experimental Setup

Our experiments are conducted by simulating the contextual bandit setting using multiclass
or cost-sensitive classiﬁcation datasets, and use the online learning system Vowpal Wabbit
(VW).

Simulated contextual bandit setting. The experiments in this paper are based on
leveraging supervised cost-sensitive classiﬁcation datasets for simulating CB learning. In
particular, we treat a CSC example (xt, ct) ∈ X × RK as a CB example, with xt given as
the context to a CB algorithm, and we only reveal the loss for the chosen action at. For
a multiclass example with label yt ∈ {1, . . . , K}, we set ct(a) := 1{a (cid:54)= yt}; for multilabel
examples with label set Yt ⊆ {1, . . . , K}, we set ct(a) := 1{a /∈ Yt}; the cost-sensitive

2. Note that IWR is not directly applicable to methods that explicitly reduce to CSC oracles, such as Agar-

wal et al. (2014); Dudik et al. (2011a).

6

A Contextual Bandit Bake-off

datasets we consider have ct ∈ [0, 1]K. We consider more general loss encodings deﬁned
with an additive oﬀset on the cost by:

(cid:96)c
t (a) = c + ct(a),

(7)

for some c ∈ R. Although some techniques attempt to remove a dependence on such en-
coding choices through appropriately designed counterfactual loss estimators (Dudik et al.,
2011b; Swaminathan and Joachims, 2015), these may be imperfect in practice, and partic-
ularly in an online scenario. The behavior observed for diﬀerent choices of c allows us to
get a sense of the robustness of the algorithms to the scale of observed losses, which might
be unknown. Separately, diﬀerent values of c can lead to lower variance for loss estimation
in diﬀerent scenarios: c = 0 might be preferred if ct(a) is often 0, while c = −1 is preferred
when ct(a) is often 1. In order to have a meaningful comparison between diﬀerent algo-
rithms, loss encodings, as well as supervised multiclass classiﬁcation, our evaluation metrics
consider the original costs ct.

Online learning in VW. Online learning is an important tool for having machine learn-
ing systems that quickly and eﬃciently adapt to observed data (Agarwal et al., 2016; He
et al., 2014; McMahan et al., 2013). We run our CB algorithms in an online fashion using
Vowpal Wabbit:
instead of exact solutions of the optimization oracles from Section 2.2,
we consider online variants of the CSC and regression oracles, which incrementally update
the policies or regressors with online gradient steps or variants thereof. Note that in VW,
online CSC itself reduces to multiple online regression problems in VW (one per action), so
that we are left with only online regression steps. More speciﬁcally, we use adaptive (Duchi
et al., 2011), normalized (Ross et al., 2013) and importance-weight-aware (Karampatziakis
and Langford, 2011) gradient updates, with a single tunable step-size parameter.

a x, or in the case of the IWR reduction, regressors f (x, a) = θ(cid:62)

Parameterization. We consider linearly parameterized policies taking the form π(x) =
arg mina θ(cid:62)
a x. For the DR
loss estimator, we use a similar linear parameterization ˆ(cid:96)(x, a) = φ(cid:62)
a x. We note that the
algorithms we consider do not rely on this speciﬁc form, and easily extend to more complex,
problem-dependent representations, such as action-dependent features. Some datasets in
our evaluation have such an action-dependent structure, with diﬀerent feature vectors xa
for diﬀerent actions a; in this case we use parameterizations of the form f (x, a) = θ(cid:62)xa,
and ˆ(cid:96)(x, a) = φ(cid:62)xa, where the parameters θ and φ are shared across all actions.

3. Algorithms

In this section, we present the main algorithms we study in this paper, along with sim-
ple modiﬁcations that achieve improved exploration eﬃciency. All methods are based on
the generic scheme in Algorithm 1. The function explore computes the exploration dis-
tribution pt over actions, and learn updates the algorithm’s policies. For simiplicity, we
consider a function oracle which performs an online update to a policy using IPS, DR or
IWR reductions given an interaction record (xt, at, (cid:96)t(at), pt). For some methods (mainly
Cover), the CSC oracle is called explicitly with modiﬁed cost vectors ct rather than the IPS
or DR loss estimates ˆ(cid:96)t deﬁned in Section 2.3; in this case, we denote such an oracle call by
csc_oracle, and also use a separate routine estimator which takes an interaction record

7

Bietti, Agarwal and Langford

Algorithm 1 Generic contextual bandit algorithm

for t = 1, . . . do

Observe context xt, compute pt = explore(xt);
Choose action at ∼ pt, observe loss (cid:96)t(at);
learn(xt, at, (cid:96)t(at), pt);

end for

Algorithm 2 (cid:15)-greedy
π1; (cid:15) > 0 (or (cid:15) = 0 for Greedy).
explore(xt):

return pt(a) = (cid:15)/K + (1 − (cid:15)) 1{πt(xt) = a};

learn(xt, at, (cid:96)t(at), pt):

πt+1 = oracle(πt, xt, at, (cid:96)t(at), pt(at));

and computes IPS or DR loss estimate vectors ˆ(cid:96)t. RegCB directly uses a regression oracle,
which we denote reg_oracle.

Our implementations of each algorithm are in the Vowpal Wabbit online learning library.

3.1 (cid:15)-greedy and greedy

We consider an importance-weighted variant of the epoch-greedy approach of Langford
and Zhang (2008), given in Algorithm 2. The method acts greedily with probability 1 − (cid:15),
and otherwise explores uniformly on all actions. Learning is achieved by reduction to oﬀ-
policy optimization, through any of the three reductions presented in Section 2.3.

We also experimented with a variant we call active (cid:15)-greedy, that uses notions from
disagreement-based active learning (Hanneke, 2014; Hsu, 2010) in order to reduce uniform
exploration to only actions that could plausibly be taken by the optimal policy. While this
variant often improves on the basic (cid:15)-greedy method, we found that it is often outperformed
empirically by other exploration algorithms, and thus defer its presentation to Appendix C,
along with a theoretical analysis, for reference.

Greedy. When taking (cid:15) = 0 in the (cid:15)-greedy approach, with the IWR reduction, we are left
with a fully greedy approach that always selects the action given by the current policy. This
gives us an online variant of the greedy algorithm of Bastani et al. (2017), which regresses on
observed losses and acts by selecting the action with minimum predicted loss. Although this
greedy strategy does not have an explicit mechanism for exploration in its choice of actions,
the inherent diversity in the distribution of contexts may provide suﬃcient exploration
for good performance and provable regret guarantees (Bastani et al., 2017; Kannan et al.,
2018). In particular, under appropriate assumptions including a diversity assumption on
the contexts, one can show that all actions have a non-zero probability of being selected at
each step, providing a form of “natural” exploration from which one can establish regret
guarantees. Empirically, we ﬁnd that Greedy can perform very well in practice on many
datasets (see Section 4). If multiple actions get the same score according to the current
regressor, we break ties randomly.

8

A Contextual Bandit Bake-off

3.2 Bagging (online Bootstrap Thompson sampling)

return pt(a) ∝ |{i : πi

t(xt) = a}|;3

Algorithm 3 Bag
π1
1, . . . , πN
1 .
explore(xt):

learn(xt, at, (cid:96)t(at), pt):
for i = 1, . . . , N do
τ i ∼ P oisson(1);
t+1 = oracleτ i(πi
πi
end for

t, xt, at, (cid:96)t(at), pt(at));

{with τ 1 = 1 for bag-greedy}

We now consider a variant of Thompson sampling which is usable in practice with opti-
mization oracles. Thompson sampling provides a generic approach to exploration problems,
which maintains a belief on the data generating model in the form of a posterior distribution
given the observed data, and explores by selecting actions according to a model sampled
from this posterior (see, e.g., Agrawal and Goyal, 2013; Chapelle and Li, 2011; Russo et al.,
2018; Thompson, 1933). While the generality of this strategy makes it attractive, main-
taining this posterior distribution can be intractable for complex policy classes, and may
require strong modeling assumptions. In order to overcome such diﬃculties and to support
the optimization oracles considered in this paper, we rely on an approximation of Thompson
sampling known as the online Bootstrap Thompson sampling (Eckles and Kaptein, 2014;
Osband and Van Roy, 2015), or bagging (Agarwal et al., 2014). This approach, shown in
Algorithm 3, maintains a collection of N policies π1
t meant to approximate the pos-
terior distribution over policies via the online Bootstrap (Agarwal et al., 2014; Eckles and
Kaptein, 2014; Osband and Van Roy, 2015; Oza and Russell, 2001; Qin et al., 2013), and
explores in a Thompson sampling fashion, by averaging action decisions across all policies
(hence the name bagging).

t , . . . , πN

Each policy is trained on a diﬀerent online Bootstrap sample of the observed data, in the
form of interaction records. The online Bootstrap performs a random number τ of online
updates to each policy instead of one (this is denoted by oracleτ in Algorithm 3). We use a
Poisson distribution with parameter 1 for τ , which ensures that in expectation, each policy
is trained on t examples after t steps. In contrast to Eckles and Kaptein (2014); Osband
and Van Roy (2015), which play the arm given by one of the N policies chosen at random,
we compute the full action distribution pt resulting from such a sampling, and leverage this
for loss estimation, allowing learning by reduction to oﬀ-policy optimization as in Agarwal
et al. (2014). As in the (cid:15)-greedy algorithm, Bagging directly relies on oﬀ-policy learning
and thus all three reductions are admissible.

Greedy bagging. We also consider a simple optimization that we call greedy bagging, for
which the ﬁrst policy π1 is trained on the true data sample (like Greedy), that is, with τ

3. When policies are parametrized using regressors as in our implementation, we let πi

t(x) be uniform over
all actions tied for the lowest cost, and the ﬁnal distribution is uniform across all actions tied for best
according to one of the policies in the bag. The added randomization gives useful variance reduction in
our experiments.

9

Bietti, Agarwal and Langford

Algorithm 4 Cover
π1
1, . . . , πN
explore(xt):

1 ; (cid:15)t = min(1/K, 1/

Kt); ψ > 0.

√

t(xt) = a}|;

pt(a) ∝ |{i : πi
return (cid:15)t + (1 − (cid:15)t)pt;
return pt;

learn(xt, at, (cid:96)t(at), pt):
π1
t+1 = oracle(π1
ˆ(cid:96)t = estimator(xt, at, (cid:96)t(at), pt(at));
for i = 2, . . . , N do

t , xt, at, (cid:96)t(at), pt(at));

t+1(xt) = a}|;

qi(a) ∝ |{j ≤ i − 1 : πj
ˆc(a) = ˆ(cid:96)t(a) −
ψ(cid:15)t
(cid:15)t+(1−(cid:15)t)qi(a) ;
πi
t+1 = csc_oracle(πi
end for

t, xt, ˆc);

{for cover}
{for cover-nu}

always equal to one, instead of a bootstrap sample with random choices of τ . We found this
approach to often improve on bagging, particularly when the number of policies N is small.

3.3 Cover

This method, given in Algorithm 4, is based on Online Cover, an online approxima-
tion of the “ILOVETOCONBANDITS” algorithm of Agarwal et al. (2014). The approach
maintains a collection of N policies, π1
t , meant to approximate a covering distri-
bution over policies that are good for both exploration and exploitation. The ﬁrst policy
π1
t is trained on observed data using the oracle as in previous algorithms, while subsequent
policies are trained using modiﬁed cost-sensitive examples which encourage diversity in the
predicted actions compared to the previous policies.

t , . . . , πN

Our implementation diﬀers from the Online Cover algorithm of Agarwal et al. (2014,
Algorithm 5) in how the diversity term in the deﬁnition of ˆc(a) is handled (the second term).
When creating cost-sensitive examples for a given policy πi, this term rewards an action a
that is not well-covered by previous policies (i.e., small qi(a)), by subtracting from the cost
a term that decreases with qi(a). While Online Cover considers a ﬁxed (cid:15)t = (cid:15), we let (cid:15)t
decay with t, and introduce a parameter ψ to control the overall reward term, which bears
more similarity with the analyzed algorithm. In particular, the magnitude of the reward
is ψ whenever action a is not covered by previous policies (i.e., qi(a) = 0), but decays with
ψ(cid:15)t whenever qi(a) > 0, so that the level of induced diversity can decrease over time as we
gain conﬁdence that good policies are covered.

Cover-NU. While Cover requires some uniform exploration across all actions, our ex-
periments suggest that this can make exploration highly ineﬃcient, thus we introduce a
variant, Cover-NU, with no uniform exploration outside the set of actions selected by cov-
ering policies.

10

A Contextual Bandit Bake-off

Algorithm 5 RegCB
f1; C0 > 0.
explore(xt):

lt(a) = lcb(ft, xt, a, ∆t,C0);
ut(a) = ucb(ft, xt, a, ∆t,C0);
pt(a) ∝ 1{a ∈ arg mina(cid:48) lt(a(cid:48))};
pt(a) ∝ 1{lt(a) ≤ mina(cid:48) ut(a(cid:48))};
return pt;

learn(xt, at, (cid:96)t(at), pt):

ft+1 = reg_oracle(ft, xt, at, (cid:96)t(at));

3.4 RegCB

{RegCB-opt variant}
{RegCB-elim variant}

We consider online approximations of the two algorithms introduced by Foster et al. (2018)
based on regression oracles, shown in Algorithm 5. Both algorithms estimate conﬁdence
intervals of the loss for each action given the current context xt, denoted [lt(a), ut(a)] in
Algorithm 5, by considering predictions from a subset of regressors with small squared loss.
The optimistic variant then selects the action with smallest lower bound estimate, similar
to LinUCB, while the elimination variant explores uniformly on actions that may plausibly
be the best.

More formally, the RegCB algorithm theoretically analyzed by Foster et al. (2018) de-

ﬁnes the conﬁdence bounds as follows:

lt(a) = min
f ∈Ft

f (xt, a),

and ut(a) = max
f ∈Ft

f (xt, a).

Here, Ft is a subset of regressors that is “good” for loss estimation, in the sense that it
achieves a small regression loss on observed data, ˆRt−1(f ) := 1
s=1(f (xt, at) − (cid:96)t(at))2,
t−1
compared to the best regressor in the full regressor class F:

(cid:80)t−1

Ft := {f ∈ F : ˆRt−1(f ) − min
f ∈F

ˆRt−1(f ) ≤ ∆t},

where ∆t is a quantity decreasing with t.

Our online implementation computes approximations of these upper and lower bounds
on the loss of each action, by using a sensitivity analysis of the current regressor based
on importance weighting taken from Krishnamurthy et al. (2019) in the context of active
learning (the computations are denoted lcb and ucb in Algorithm 5). The algorithm main-
tains a regressor ft : X × {1, . . . , K} and, given a new context xt, computes lower and
upper conﬁdence bounds lt(a) ≤ ft(xt, a) ≤ ut(a). These are computed by adding “virtual”
importance-weighted regression examples with low and high costs, and ﬁnding the largest
importance weight leading to an excess squared loss smaller than ∆t,C0, where

and C0 is a parameter controlling the width of the conﬁdence bounds. This importance
weight can be found using regressor sensitivities and a binary search procedure as described

∆t,C0 =

C0 log(Kt)
t

,

11

Bietti, Agarwal and Langford

in (Krishnamurthy et al., 2019, Section 7.1). Note that this requires knowledge of the loss
range [cmin, cmax], unlike other methods. In contrast to Krishnamurthy et al. (2019), we set
the labels of the “virtual” examples to cmin − 1 for the lower bound and cmax + 1 for the
upper bound, instead of cmin and cmax.

4. Evaluation

In this section, we present our evaluation of the contextual bandit algorithms described in
Section 3. The evaluation code is available at https://github.com/albietz/cb_bakeoff.
All methods presented in this section are available in Vowpal Wabbit.4

Evaluation setup. Our evaluation consists in simulating a CB setting from cost-sensitive
classiﬁcation datasets, as described in Section 2.4. We consider a collection of 525 multiclass
classiﬁcation datasets from the openml.org platform, including among others, medical,
gene expression, text, sensory or synthetic data, as well as 5 multilabel datasets5 and 3
cost-sensitive datasets, namely a cost-sensitive version of the RCV1 multilabel dataset used
in (Krishnamurthy et al., 2019), where the cost of a news topic is equal to the tree distance
to a correct topic, as well as the two learning to rank datasets used in (Foster et al., 2018).
More details on these datasets are given in Appendix A. Because of the online setup, we
consider one or more ﬁxed, shuﬄed orderings of each dataset. The datasets widely vary in
noise levels, and number of actions, features, examples etc., allowing us to model varying
diﬃculties in CB problems.

We evaluate the algorithms described in Section 3. We ran each method on every dataset
with diﬀerent choices of algorithm-speciﬁc hyperparameters, learning rates, reductions, and
loss encodings. Details are given in Appendix B.1. Unless otherwise speciﬁed, we consider
ﬁxed choices which are chosen to optimize performance on a subset of multiclass datasets
with a voting mechanism and are highlighted in Table 9 of Appendix B, except for the
learning rate, which is always optimized.

The performance of method A on a dataset of size n is measured by the progressive

validation loss (Blum et al., 1999):

P VA =

ct(at),

1
n

n
(cid:88)

t=1

where at is the action chosen by the algorithm on the t-th example, and ct the true cost
vector. This metric allows us to capture the explore-exploit trade-oﬀ, while providing a
measure of generalization that is independent of the choice of loss encodings, and compa-
rable with online supervised learning. We also consider a normalized loss variant given
by P VA−P VOAA
, where OAA denotes an online (supervised) cost-sensitive one against all
classiﬁer. This helps highlight the diﬃculty of exploration for some datasets in our plots.

P VOAA

In order to compare two methods on a given dataset with binary costs (multiclass or
multilabel), we consider a notion of statistically signiﬁcant win or loss. We use the

4. For reproducibility purposes, the precise version of VW used to run these experiments is available at

https://github.com/albietz/vowpal_wabbit/tree/bakeoff.

5. https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html

12

A Contextual Bandit Bake-off

Table 1: Entry (row, column) shows the statistically signiﬁcant win-loss diﬀerence of row
against column. Encoding ﬁxed to -1/0 (top) or 0/1 (bottom). (left) held-out datasets
only (330 in total); ﬁxed hyperparameters, only the learning rate is optimized; (right)
all datasets (530 in total); all choices of hyperparameters are optimized on each dataset
(diﬀerent methods have diﬀerent hyperparameter settings from 1 to 18, see Table 9 in
Appendix B).

↓ vs → G RO C-nu B-g
50
-7
G
49
-
RO
22
-26
C-nu
-
-49
B-g
-17
-68
(cid:15)G

10
26
-
-22
-57

-
7
-10
-50
-54

↓ vs → G RO C-nu B-g
-22
-30
G
8
-
RO
64
37
C-nu
-
-8
B-g
-86
-84
(cid:15)G

-93
-37
-
-64
-145

-
30
93
22
-58

(cid:15)G
58
84
145
86
-

-1/0 encoding

↓ vs → G
-
G
64
RO
17
C-nu
-36
B-g
-52
(cid:15)G

RO C-nu B-g
-17
-64
36
100
45
-
45
-
-45
-45
-100
-
-19
-75
-120

↓ vs → G
-
G
124
RO
150
C-nu
75
B-g
-5
(cid:15)G

RO C-nu B-g
-75
-150
-124
65
-11
-
67
-
11
-
-67
-65
-83
-144
-125

(cid:15)G
5
125
144
83
-

0/1 encoding

(cid:15)G
54
68
57
17
-

(cid:15)G
52
120
75
19
-

following (heuristic) deﬁnition of signiﬁcance based on an approximate Z-test: if pa and pb
denote the PV loss of a and b on a given dataset of size n, then a wins over b if





1 − Φ

pa − pb

(cid:113) pa(1−pa)
n

+ pb(1−pb)
n



 < 0.05,

where Φ is the Gauss error function. We also deﬁne the signiﬁcant win-loss diﬀerence
of one algorithm against another to be the diﬀerence between the number of signiﬁcant
wins and signiﬁcant losses. We have found these metrics to provide more insight into the
behavior of diﬀerent methods, compared to strategies based on aggregation of loss measures
across all datasets. Indeed, we often found the relative performance of two methods to vary
signiﬁcantly across datasets, making aggregate metrics less informative.

Results in this section focus on Greedy (G), RegCB-optimistic (RO), Cover-NU (C-
nu), Bag-greedy (B-g) and (cid:15)-greedy ((cid:15)G), deferring other variants to Appendix B, as their
performance is typically comparable to or dominated by these methods. We combine results
on multiclass and multilabel datasets, but show them separately in Appendix B.

Eﬃcient exploration methods. Our experiments suggest that the best performing
method is the RegCB approach (Foster et al., 2018), as shown in Table 1 (left), where
the signiﬁcant wins of RO against all other methods exceed signiﬁcant losses, which yields
the best performance overall. This is particularly prominent with 0/1 encodings. With -1/0
encodings, which are generally preferred on our corpus as discussed below, the simple greedy
approach comes a close second, outperforming other methods on a large number of datasets,

13

Bietti, Agarwal and Langford

Table 2: Progressive validation loss for cost-sensitive datasets with real-valued costs in [0, 1].
Hyperparameters are ﬁxed as in Table 10. We show mean and standard error based on 10
diﬀerent random reshuﬄings. For RCV1, costs are based on tree distance to correct topics.
For MSLR and Yahoo, costs encode 5 regularly-spaced discrete relevance scores (0: perfectly
relevant, 1: irrelevant), and we include results for a loss encoding oﬀset c = −1 in Eq. (7).

G
0.215 ± 0.010

RO
0.225 ± 0.008

B-g
0.251 ± 0.005

(cid:15)G
0.230 ± 0.009

G
0.798 ± 0.0023

RO
0.794 ± 0.0007

B-g
0.799 ± 0.0013

(cid:15)G
0.807 ± 0.0020

G
0.791 ± 0.0012

RO
0.790 ± 0.0009

B-g
0.791 ± 0.0008

(cid:15)G
0.806 ± 0.0018

G
0.593 ± 0.0004

RO
0.592 ± 0.0005

B-g
0.596 ± 0.0006

(cid:15)G
0.598 ± 0.0005

G
0.589 ± 0.0005

RO
0.588 ± 0.0004

B-g
0.590 ± 0.0005

(cid:15)G
0.594 ± 0.0006

C-nu
0.215 ± 0.006
(a) RCV1

C-nu
0.798 ± 0.0012
(b) MSLR

C-nu
0.792 ± 0.0007
(c) MSLR, c = −1

C-nu
0.594 ± 0.0004
(d) Yahoo

C-nu
0.589 ± 0.0004
(e) Yahoo, c = −1

despite the lack of an explicit exploration mechanism. A possible reason for this success is
the diversity that is inherently present in the distribution of contexts across actions, which
has been shown to yield no-regret guarantees under various assumptions (Bastani et al.,
2017; Kannan et al., 2018). The noise induced by the dynamics of online learning and
random tie-breaking may also be a source of more exploration. RO and Greedy also show
strong performance on the 8 UCI datasets and the learning-to-rank datasets from Foster
et al. (2018), as shown in Tables 2 and 13. Nevertheless, both Greedy and RegCB have
known failure modes which the Cover approach is robust to by design. While the basic
approach with uniform exploration is too conservative, we found our Cover-NU variant
to be quite competitive overall. The randomization in its choice of actions yields explo-
ration logs which may be additionally used for oﬄine evaluation, in contrast to Greedy and
RegCB-opt, which choose actions deterministically. A more granular comparison of these
methods is given in Figure 2, which highlight the failure of Greedy and RegCB against
Cover-NU on some datasets which may be more diﬃcult perhaps due to a failure of mod-
eling assumptions. Bagging also outperforms other methods on some datasets, however it
is outperformed on most datasets, possibly because of the additional variance induced by
the bootstrap sampling. Table 1 (right) optimizes over hyperparameters for each dataset,
which captures the best potential of each method. Cover-NU does the best here, but also
has the most hyperparameters, indicating that a more adaptive variant could be desirable.
RegCB stays competitive, while Greedy pales possibly due to fewer hyperparameters.

Variability with dataset characteristics. Table 5 shows win-loss statistics for subsets
of the datasets with constraints on diﬀerent characteristics, such as number of actions,

14

A Contextual Bandit Bake-off

Figure 2: Pairwise comparisons among four successful methods: Greedy, Cover-nu, Bag-
greedy, and RegCB-opt. Hyperparameters ﬁxed as in Table 9, with encoding -1/0. All
held-out multiclass and multilabel datasets are shown, in contrast to Figure 1, which only
shows held-out datasets with 5 or more actions. The plots consider normalized loss, with
red points indicating signiﬁcant wins.

dimensionality, size, and performance in the supervised setting. The values for these splits
were chosen in order to have a reasonably balanced number of datasets in each table.

√

We ﬁnd that RegCB-opt is the preferred method in most situations, while Greedy and
Cover-NU can also provide good performance in diﬀerent settings. When only considering
larger datasets, RegCB-opt dominates all other methods, with Greedy a close second, while
Cover-NU seems to explore less eﬃciently. This could be related to the better adaptivity
properties of RegCB to favorable noise conditions, which can achieve improved (even log-
arithmic) regret (Foster et al., 2018), in contrast to Cover-NU, for which a slower rate (of
O(
n)) may be unavoidable since it is baked into the algorithm of Agarwal et al. (2014)
by design, and is reﬂected in the diversity terms of the costs ˆc in our online variant given
in Algorithm 4. In contrast, when n is small, RegCB-opt and Greedy may struggle to ﬁnd
good policies (in fact, their analysis typically requires a number of “warm-start” iterations
with uniform exploration), while Cover-NU seems to explore more eﬃciently from the be-
ginning, and behaves well with large action spaces or high-dimensional features. Finally,
Table 5(d,e) shows that Greedy can be the best choice when the dataset is “easy”, in the
sense that a supervised learning method achieves small loss. Achieving good performance
on such easy datasets is related to the open problem of Agarwal et al. (2017), and vari-
ants of methods designed to be agnostic to the data distribution—such as Cover(-NU) and
(cid:15)-Greedy (Agarwal et al., 2014; Langford and Zhang, 2008)—seem to be the weakest on
these datasets.

15

Bietti, Agarwal and Langford

Table 3: Impact of reductions for Bag (left) and (cid:15)-greedy (right), with hyperparameters
optimized and encoding ﬁxed to -1/0. Each (row, column) entry shows the statistically
signiﬁcant win-loss diﬀerence of row against column. IWR outperforms the other reductions
for both methods, which are the only two methods that directly reduce to oﬀ-policy learning,
and thus where such a comparison applies.

↓ vs → ips
-
ips
42
dr
59
iwr

dr
-42
-
28

iwr
-59
-28
-

↓ vs → ips
-
ips
-63
dr
133
iwr

dr
63
-
155

iwr
-133
-155
-

Table 4: Impact of encoding on diﬀerent algorithms, with hyperparameters optimized. Each
entry indicates the number of statistically signiﬁcant wins/losses of -1/0 against 0/1. -1/0
is the better overall choice of encoding, but 0/1 can be preferable on larger datasets (the
bottom row considers the 64 datasets in our corpus with more than 10,000 examples).

datasets
all
≥ 10,000

G
136 / 42
19 / 12

RO
60 / 47
10 / 18

C-nu
76 / 46
14 / 20

B-g
77 / 27
15 / 11

(cid:15)G
99 / 27
14 / 5

Reductions. Among the reduction mechanisms introduced in Section 2.3, IWR has de-
sirable properties such as tractability (the other reductions rely on a CSC objective, which
requires approximations due to non-convexity), and a computational cost that is indepen-
dent of the total number of actions, only requiring updates for the chosen action. In addition
to Greedy, which can be seen as using a form of IWR, we found IWR to work very well
for bagging and (cid:15)-greedy approaches, as shown in Table 3 (see also Table 9 in Appendix B,
which shows that IWR is also preferred when considering ﬁxed hyperparameters for these
methods). This may be attributed to the diﬃculty of the CSC problem compared to regres-
sion, as well as importance weight aware online updates, which can be helpful for small (cid:15).
Together with its computational beneﬁts, our results suggest that IWR is often a com-
pelling alternative to CSC reductions based on IPS or DR. In particular, when the number
of actions is prohibitively large for using Cover-NU or RegCB, Bag with IWR may be a
good default choice of exploration algorithm. While Cover-NU does not directly support
the IWR reduction, making them work together well would be a promising future direction.

Encodings. Table 4 indicates that the -1/0 encoding is preferred to 0/1 on many of the
datasets, and for all methods. We now give one possible explanation. As discussed in
Section 2.4, the -1/0 encoding yields low variance loss estimates when the cost is often
close to 1. For datasets with binary costs, since the learner may often be wrong in early
iterations, a cost of 1 is a good initial bias for learning. With enough data, however, the
learner should reach better accuracies and observe losses closer to 0, in which case the 0/1
encoding should lead to lower variance estimates, yielding better performance as observed
in Table 4. We tried shifting the loss range in the RCV1 dataset with real-valued costs
from [0, 1] to [−1, 0], but saw no improvements compared to the results in Table 2. Indeed,

16

A Contextual Bandit Bake-off

(a) all multiclass datasets

(b) n ≥ 10 000 only

Figure 3: Errors of IPS counterfactual estimates for the uniform random policy using explo-
ration logs collected by various algorithms on multiclass datasets. The boxes show quartiles
(with the median shown as a blue line) of the distribution of squared errors across all
multiclass datasets or only those with at least 10 000 examples. The logs are obtained by
running each algorithm with -1/0 encodings, ﬁxed hyperparameters from Table 9, and the
best learning rate on each dataset according to progressive validation loss.

a cost of 1 may not be a good initial guess in this case, in contrast to the binary cost
setting. On the MSLR and Yahoo learning-to-rank datasets, we do see some improvement
from shifting costs to the range [−1, 0], perhaps because in this case the costs are discrete
values, and the cost of 1 corresponds to the document label “irrelevant”, which appears
frequently in these datasets.

Counterfactual evaluation. After running an exploration algorithm, a desirable goal
for practitioners is to evaluate new policies oﬄine, given access to interaction logs from
exploration data. While various exploration algorithms rely on such counterfactual eval-
uation through a reduction, the need for eﬃcient exploration might restrict the ability to
evaluate arbitrary policies oﬄine, since the algorithm may only need to estimate “good”
policies in the sense of small regret with respect to the optimal policy. To illustrate this,
in Figure 3, we consider the evaluation of the uniform random stochastic policy, given by
πunif(a|x) = 1/K for all actions a = 1, . . . , K, using a simple inverse propensity scoring
(IPS) approach. While such a task is somewhat extreme and of limited practical interest
in itself, we use it mainly as a proxy for the ability to evaluate any arbitrary policy (in par-
ticular, it should be possible to evaluate any policy if we can evaluate the uniform random
policy). On a multiclass dataset, the expected loss of such a policy is simply 1 − 1/K, while
its IPS estimate is given by

ˆLIP S = 1 −

1
n

n
(cid:88)

t=1

1 − (cid:96)t(at)
Kpt(at)

,

where the loss is given by (cid:96)t(at) = 1{at (cid:54)= yt} when the correct multiclass label is yt.
Note that this quantity is well-deﬁned since the denominator is always non-zero when at is
sampled from pt, but the estimator is biased when data is collected by a method without
some amount of uniform exploration (i.e., when pt does not have full support). This bias

17

Bietti, Agarwal and Langford

is particularly evident in Figure 3b where epsilon-greedy shows very good counterfactual
evaluation performance while the other algorithms induce biased counterfactual evaluation
due to lack of full support. The plots in Figure 3 show the distribution of squared errors
( ˆLIP S − (1 − 1/K))2 across multiclass datasets. We consider IPS on the rewards 1 − (cid:96)t(at)
here as it is more adapted to the -1/0 encodings used to collect exploration logs, but we
also show IPS on losses in Figure 6 of Appendix B.2. Figure 3 shows that the more eﬃcient
exploration methods (Greedy, RegCB-optimistic, and Cover-NU) give poor estimates for
this policy, probably because their exploration logs provide biased estimates and are quite
focused on few actions that may be taken by good policies, while the uniform exploration in
(cid:15)-Greedy (and Cover-U, see Figure 6) yields better estimates, particularly on larger datasets.
The elimination version of RegCB provides slightly better logs compared to the optimistic
version (see Figure 6), and Bagging may also be preferred in this context. Overall, these
results show that there may be a trade-oﬀ between eﬃcient exploration and the ability to
perform good counterfactual evaluation, and that uniform exploration may be needed if one
would like to perform accurate oﬄine experiments for a broad range of questions.

5. Discussion and Takeaways

In this paper, we presented an evaluation of practical contextual bandit algorithms on a
large collection of supervised learning datasets with simulated bandit feedback. We ﬁnd that
a worst-case theoretical robustness forces several common methods to often over-explore,
damaging their empirical performance, and strategies that limit (RegCB and Cover-NU) or
simply forgo (Greedy) explicit exploration dominate the ﬁeld. For practitioners, our study
also provides a reference for practical implementations, while stressing the importance of
loss estimation and other design choices such as how to encode observed feedback.

Guidelines for practitioners. We now summarize some practical guidelines that come
out of our empirical study:

• Methods relying on modeling assumptions on the data distribution such as RegCB are
often preferred in practice, and even Greedy can work well (see, e.g., Table 1). They
tend to dominate more robust approaches such as Cover-NU even more prominently
on larger datasets, or on datasets where prediction is easy, e.g., due to low noise
(see Table 5). While it may be diﬃcult to assess such favorable conditions of the
data in advance, practitioners may use speciﬁc domain knowledge to design better
feature representations for prediction, which may in turn improve exploration for
these methods.

• Uniform exploration hurts empirical performance in most cases (see, e.g., the poor
performance of (cid:15)-greedy and Cover-u in Table 11 of Appendix B). Nevertheless, it
may be necessary on the hardest datasets, and may be crucial if one needs to perform
oﬀ-policy counterfactual evaluation (see Figures 3 and 6).

• Loss estimation is an essential component in many CB algorithms for good practical
performance, and DR should be preferred over IPS. For methods based on reduction
to oﬀ-policy learning, such as (cid:15)-Greedy and Bagging, the IWR reduction is typically
best, in addition to providing computational beneﬁts (see Table 3).

18

A Contextual Bandit Bake-off

• From our early experiments, we found randomization on tied choices of actions to
always be useful. For instance, it avoids odd behavior which may arise from deter-
ministic, implementation-speciﬁc biases (e.g., always favoring one speciﬁc action over
the others).

• The choice of cost encodings makes a big diﬀerence in practice and should be carefully
considered when designing an contextual bandit problem, even when loss estimation
techniques such as DR are used. For binary outcomes, -1/0 is a good default choice
of encoding in the common situation where the observed loss is often 1 (see Table 4).

• Modeling choices and encodings sometimes provide pessimistic initial estimates that
can hurt initial exploration on some problems, particularly for Greedy and RegCB-
optimistic. Random tie-breaking as well as using a shared additive baseline can help
mitigate this issue (see Section B.3).

• The hyperparameters highlighted in Appendix B.1 obtained on our datasets may be
good default choices in practice. Nevertheless, these may need to be balanced in
order to address other conﬂicting requirements in real-world settings, such as non-
stationarity or the ability to run oﬄine experiments.

Open questions for theoreticians. Our study raises some questions of interest for
theoretical research on contextual bandits. The good performance of greedy methods calls
for a better understanding of greedy methods, building upon the work of Bastani et al.
(2017); Kannan et al. (2018), as well as methods that are more robust to more diﬃcult
datasets while adapting to such favorable scenarios, such as when the context distribution
has enough diversity. A related question is that of adaptivity to easy datasets for which
the optimal policy has small loss, an open problem pointed out by Agarwal et al. (2017)
in the form of “ﬁrst-order” regret bounds. While methods satisfying such regret bounds
have now been developed theoretically (Allen-Zhu et al., 2018), these methods are currently
not computationally eﬃcient, and obtaining eﬃcient methods based on optimization oracles
remains an important open problem. We also note that while our experiments are based on
online optimization oracles, most analyzed versions of the algorithms rely on solving the full
optimization problems; it would be interesting to better understand the behavior of online
variants, and to characterize the implicit exploration eﬀect for the greedy method.

Limitations of the study. Our study is primarily concerned with prediction perfor-
mance, while real world applications often additionally consider the value of counterfactual
evaluation for oﬄine policy evaluation.

A key limitation of our study is that it is concerned with stationary datasets. Many real-
world contextual bandit applications involve nonstationary datasources. This limitation
is simply due to the nature of readily available public datasets. The lack of public CB
datasets as well as challenges in counterfactual evaluation of CB algorithms make a more
realistic study challenging, but we hope that an emergence of platforms (Agarwal et al.,
2016; Jamieson et al., 2015) to easily deploy CB algorithms will enable studies with real
CB datasets in the future.

19

Bietti, Agarwal and Langford

References

Y. Abbasi-Yadkori, D. P´al, and C. Szepesv´ari. Improved algorithms for linear stochastic

bandits. In Advances in Neural Information Processing Systems (NIPS), 2011.

A. Agarwal, M. Dud´ık, S. Kale, J. Langford, and R. E. Schapire. Contextual bandit learning
In Proceedings of the International Conference on Artiﬁcial

with predictable rewards.
Intelligence and Statistics (AISTATS), 2012.

A. Agarwal, D. Hsu, S. Kale, J. Langford, L. Li, and R. E. Schapire. Taming the monster: A
fast and simple algorithm for contextual bandits. arXiv preprint arXiv:1402.0555, 2014.

A. Agarwal, S. Bird, M. Cozowicz, L. Hoang, J. Langford, S. Lee, J. Li, D. Melamed,
arXiv preprint

G. Oshri, O. Ribas, et al. A multiworld testing decision service.
arXiv:1606.03966, 2016.

A. Agarwal, A. Krishnamurthy, J. Langford, H. Luo, et al. Open problem: First-order
regret bounds for contextual bandits. In Conference on Learning Theory (COLT), 2017.

S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoﬀs.
In Proceedings of the International Conference on Machine Learning (ICML), 2013.

Z. Allen-Zhu, S. Bubeck, and Y. Li. Make the minority great again: First-order regret
bound for contextual bandits. In Proceedings of the International Conference on Machine
Learning (ICML), 2018.

H. Bastani, M. Bayati, and K. Khosravi. Mostly exploration-free algorithms for contextual

bandits. arXiv preprint arXiv:1704.09011, 2017.

A. Blum, A. Kalai, and J. Langford. Beating the hold-out: Bounds for k-fold and progressive

cross-validation. In Conference on Learning Theory (COLT), 1999.

O. Chapelle and L. Li. An empirical evaluation of thompson sampling.

In Advances in

Neural Information Processing Systems (NIPS), 2011.

J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine Learning Research (JMLR), 12(Jul):
2121–2159, 2011.

M. Dudik, D. Hsu, S. Kale, N. Karampatziakis, J. Langford, L. Reyzin, and T. Zhang. Eﬃ-
cient optimal learning for contextual bandits. In Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), 2011a.

M. Dudik, J. Langford, and L. Li. Doubly robust policy evaluation and learning.
Proceedings of the International Conference on Machine Learning (ICML), 2011b.

In

D. Eckles and M. Kaptein. Thompson sampling with the online bootstrap. arXiv preprint

arXiv:1410.4009, 2014.

20

A Contextual Bandit Bake-off

D. J. Foster, A. Agarwal, M. Dud´ık, H. Luo, and R. E. Schapire. Practical contextual ban-
dits with regression oracles. In Proceedings of the International Conference on Machine
Learning (ICML), 2018.

S. Hanneke. Theory of disagreement-based active learning. Foundations and Trends in

Machine Learning, 7(2-3), 2014.

X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers,
et al. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the
Eighth International Workshop on Data Mining for Online Advertising, 2014.

D. J. Hsu. Algorithms for active learning. PhD thesis, UC San Diego, 2010.

T.-K. Huang, A. Agarwal, D. J. Hsu, J. Langford, and R. E. Schapire. Eﬃcient and
In Advances in Neural Information Processing

parsimonious agnostic active learning.
Systems (NIPS), 2015.

K. G. Jamieson, L. Jain, C. Fernandez, N. J. Glattard, and R. Nowak. Next: A system for
real-world development, evaluation, and application of active learning. In Advances in
Neural Information Processing Systems (NIPS), 2015.

S. M. Kakade and A. Tewari. On the generalization ability of online strongly convex pro-
gramming algorithms. In Advances in Neural Information Processing Systems (NIPS),
2009.

S. Kannan, J. Morgenstern, A. Roth, B. Waggoner, and Z. S. Wu. A smoothed analysis
of the greedy algorithm for the linear contextual bandit problem. In Advances in Neural
Information Processing Systems (NIPS), 2018.

N. Karampatziakis and J. Langford. Online importance weight aware updates. In Confer-

ence on Uncertainty in Artiﬁcial Intelligence (UAI), 2011.

A. Krishnamurthy, A. Agarwal, T.-K. Huang, H. Daume III, and J. Langford. Active
learning for cost-sensitive classiﬁcation. Journal of Machine Learning Research (JMLR),
20(65):1–50, 2019.

J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side

information. In Advances in Neural Information Processing Systems (NIPS), 2008.

L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to person-
alized news article recommendation. In Proceedings of the 19th international conference
on World wide web. ACM, 2010.

P. Massart, ´E. N´ed´elec, et al. Risk bounds for statistical learning. The Annals of Statistics,

34(5), 2006.

H. B. McMahan, G. Holt, D. Sculley, M. Young, D. Ebner, J. Grady, L. Nie, T. Phillips,
E. Davydov, D. Golovin, et al. Ad click prediction: a view from the trenches. In Proceed-
ings of the 19th ACM international conference on Knowledge discovery and data mining
(KDD), 2013.

21

Bietti, Agarwal and Langford

I. Osband and B. Van Roy. Bootstrapped thompson sampling and deep exploration. arXiv

preprint arXiv:1507.00300, 2015.

N. C. Oza and S. Russell. Online bagging and boosting. In Proceedings of the International

Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2001.

Z. Qin, V. Petricek, N. Karampatziakis, L. Li, and J. Langford. Eﬃcient online bootstrap-
ping for large scale learning. In Workshop on Parallel and Large-scale Machine Learning
(BigLearning@NIPS), 2013.

S. Ross, P. Mineiro, and J. Langford. Normalized online learning. In Conference on Uncer-

tainty in Artiﬁcial Intelligence (UAI), 2013.

D. J. Russo, B. Van Roy, A. Kazerouni, I. Osband, Z. Wen, et al. A tutorial on thompson

sampling. Foundations and Trends R(cid:13) in Machine Learning, 11(1):1–96, 2018.

T. Schnabel, A. Swaminathan, A. Singh, N. Chandak, and T. Joachims. Recommendations
In Proceedings of the International

as treatments: debiasing learning and evaluation.
Conference on Machine Learning (ICML), 2016.

A. Swaminathan and T. Joachims. The self-normalized estimator for counterfactual learn-

ing. In Advances in Neural Information Processing Systems (NIPS), 2015.

W. R. Thompson. On the likelihood that one unknown probability exceeds another in view

of the evidence of two samples. Biometrika, 25(3/4), 1933.

22

A Contextual Bandit Bake-off

Table 5: Statistically signiﬁcant win-loss diﬀerence with hyperparameters ﬁxed as in Ta-
ble 10, encodings ﬁxed to -1/0, on all held-out datasets or subsets with diﬀerent charac-
teristics: (a) number of actions K; (b) number of features d; (c) number of examples n;
(d) PV loss of the one-against-all (OAA) method. The corresponding table for all held-out
datasets is shown in Table 1(top left).

(cid:15)G
18
25
26
-6
-

(cid:15)G
10
21
17
-7
-

(cid:15)G
53
60
37
18
-

↓ vs → G RO C-nu B-g
22
-5
G
24
-
RO
25
C-nu
-5
-
-24
B-g
6
-25
(cid:15)G

-6
5
-
-25
-26

-
5
6
-22
-18

↓ vs → G RO C-nu B-g
15
1
G
14
-
RO
22
0
C-nu
-
-14
B-g
9
-9
(cid:15)G

-
-1
4
-15
-7

-4
0
-
-22
-13

(a) K ≥ 3 (left, 89 datasets), K ≥ 10 (right, 36 datasets)

↓ vs → G RO C-nu B-g
20
-5
G
26
-
RO
20
-8
C-nu
-
-26
B-g
7
-21
(cid:15)G

-1
8
-
-20
-17

-
5
1
-20
-10

↓ vs → G RO C-nu B-g
-5
G
-
RO
-1
C-nu
-8
B-g
-8
(cid:15)G

-6
1
-
-15
-7

7
8
15
-
6

-
5
6
-7
0

(b) d ≥ 100 (left, 76 datasets), d ≥ 10 000 (right, 41 datasets)

↓ vs → G RO C-nu B-g
38
-4
G
40
-
RO
9
-33
C-nu
-
-40
B-g
-18
-60
(cid:15)G

-
4
-23
-38
-53

23
33
-
-9
-37

↓ vs → G RO C-nu B-g
10
-3
G
14
-
RO
2
-17
C-nu
-
-14
B-g
-25
-36
(cid:15)G

16
17
-
-2
-30

-
3
-16
-10
-34

(c) n ≥ 1 000 (left, 119 datasets), n ≥ 10 000 (right, 43 datasets)

↓ vs → G RO C-nu B-g
40
1
G
36
-
RO
7
-26
C-nu
-
-36
B-g
-4
-43
(cid:15)G

↓ vs → G RO C-nu B-g
8
1
G
5
-
RO
-12
-12
C-nu
-
-5
B-g
-10
-16
(cid:15)G
(d) P VOAA ≤ 0.2 (left, 135 datasets), P VOAA ≤ 0.05 (right, 28 datasets)

(cid:15)G
36
43
24
4
-

-
-1
-25
-40
-36

25
26
-
-7
-24

-
-1
-14
-8
-15

14
12
-
12
-5

(cid:15)G
7
9
13
-9
-

(cid:15)G
0
8
7
-6
-

(cid:15)G
34
36
30
25
-

(cid:15)G
15
16
5
10
-

↓ vs → G RO C-nu B-g
5
2
G
4
-
RO
-1
-8
C-nu
-
-4
B-g
-11
-12
(cid:15)G

8
8
-
1
-12

-
-2
-8
-5
-12

(cid:15)G
12
12
12
11
-

(e) n ≥ 10 000 and P VOAA ≤ 0.1 (13 datasets)

23

Bietti, Agarwal and Langford

Appendix A. Datasets

This section gives some details on the cost-sensitive classiﬁcation datasets considered in our
study.

Multiclass classiﬁcation datasets. We consider 525 multiclass datasets from the openml.
org platform, including among others, medical, gene expression, text, sensory or synthetic
data. Table 6 provides some statistics about these datasets. These also include the 8 clas-
siﬁcation datasets considered in (Foster et al., 2018) from the UCI database. The full list
of datasets is given below.

Table 6: Statistics on number of multiclass datasets by number of examples, actions and
unique features, as well as by progressive validation 0-1 loss for the supervised one-against-
all online classiﬁer, in our collection of 525 multiclass datasets.

actions #
404
73
48

2
3-9
10+

examples #
94
270
133
28

≤ 102
102-103
103-105
> 105

features
≤ 50
51-100
101-1000
1000+

#
392
35
17
81

P VOAA
≤ 0.01
(0.01, 0.1]
(0.1, 0.2]
(0.2, 0.5]
> 0.5

#
10
88
103
273
51

Multilabel classiﬁcation datasets. We consider 5 multilabel datasets from the LibSVM
website6, listed in Table 7.

Table 7: List of multilabel datasets.

Dataset # examples # features # actions P VOAA
0.1664
mediamill
0.0446
rcv1
0.0066
scene
0.1661
tmc
0.2553
yeast

30,993
23,149
1,211
21,519
1,500

120
47,236
294
30,438
103

101
103
6
22
14

Cost-sensitive classiﬁcation datasets. For more general real-valued costs in [0, 1], we
use a modiﬁcation of the multilabel RCV1 dataset introduced in (Krishnamurthy et al.,
2019). Each example consists of a news article labeled with the topics it belongs to, in
a collection of 103 topics. Instead of ﬁxing the cost to 1 for incorrect topics, the cost is
deﬁned as the tree distance to the set of correct topics in a topic hierarchy.

We also include the learning-to-rank datasets considered in (Foster et al., 2018), where
we limit the number of documents (actions) per query, and consider all the training folds.
We convert relevance scores to losses in {0, 0.25, 0.5, 0.75, 1}, with 0 indicating a perfectly
relevant document, and 1 an irrelevant one. The datasets considered are the Microsoft

6. https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html

24

A Contextual Bandit Bake-off

Learning to Rank dataset, variant MSLR-30K at https://www.microsoft.com/en-us/
research/project/mslr/, and the Yahoo! Learning to Rank Challenge V2.0, variant C14B
at https://webscope.sandbox.yahoo.com/catalog.php?datatype=c. Details are shown
in Table 8. We note that for these datasets we consider action-dependent features, with a
ﬁxed parameter vector for all documents.

Table 8: Learning to rank datasets.

Dataset
MSLR-30K
Yahoo

# examples # features max # documents P VOAA
0.7892
0.5876

31,531
36,251

136
415

10
6

List of multiclass datasets. The datasets we used can be accessed at https://www.
openml.org/d/<id>, with id in the following list:

3, 6, 8, 10, 11, 12, 14, 16, 18, 20, 21, 22, 23, 26, 28, 30, 31, 32, 36, 37, 39, 40, 41, 43,
44, 46, 48, 50, 53, 54, 59, 60, 61, 62, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161,
162, 180, 181, 182, 183, 184, 187, 189, 197, 209, 223, 227, 273, 275, 276, 277, 278, 279, 285,
287, 292, 293, 294, 298, 300, 307, 310, 312, 313, 329, 333, 334, 335, 336, 337, 338, 339, 343,
346, 351, 354, 357, 375, 377, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395,
396, 397, 398, 399, 400, 401, 444, 446, 448, 450, 457, 458, 459, 461, 462, 463, 464, 465, 467,
468, 469, 472, 475, 476, 477, 478, 479, 480, 554, 679, 682, 683, 685, 694, 713, 714, 715, 716,
717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735,
736, 737, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756,
758, 759, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777,
778, 779, 780, 782, 783, 784, 785, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 799,
800, 801, 803, 804, 805, 806, 807, 808, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821,
822, 823, 824, 825, 826, 827, 828, 829, 830, 832, 833, 834, 835, 836, 837, 838, 841, 843, 845,
846, 847, 848, 849, 850, 851, 853, 855, 857, 859, 860, 862, 863, 864, 865, 866, 867, 868, 869,
870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 884, 885, 886, 888, 891, 892,
893, 894, 895, 896, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914,
915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 931, 932, 933, 934,
935, 936, 937, 938, 941, 942, 943, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956,
958, 959, 962, 964, 965, 969, 970, 971, 973, 974, 976, 977, 978, 979, 980, 983, 987, 988, 991,
994, 995, 996, 997, 1004, 1005, 1006, 1009, 1011, 1012, 1013, 1014, 1015, 1016, 1019, 1020,
1021, 1022, 1025, 1026, 1036, 1038, 1040, 1041, 1043, 1044, 1045, 1046, 1048, 1049, 1050,
1054, 1055, 1056, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1071,
1073, 1075, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1100,
1104, 1106, 1107, 1110, 1113, 1115, 1116, 1117, 1120, 1121, 1122, 1123, 1124, 1125, 1126,
1127, 1128, 1129, 1130, 1131, 1132, 1133, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142,
1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157,
1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1169, 1216, 1217, 1218, 1233, 1235,
1236, 1237, 1238, 1241, 1242, 1412, 1413, 1441, 1442, 1443, 1444, 1449, 1451, 1453, 1454,
1455, 1457, 1459, 1460, 1464, 1467, 1470, 1471, 1472, 1473, 1475, 1481, 1482, 1483, 1486,
1487, 1488, 1489, 1496, 1498, 1590.

25

Bietti, Agarwal and Langford

Table 9: Choices of hyperparameters and reduction for each method. Fixed choices of hy-
perparameters for -1/0 encodings are in bold. These were obtained for each method with
an instant-runoﬀ voting mechanism on 200 of the multiclass datasets with -1/0 encoding,
where each dataset ranks hyperparameter choices according to the diﬀerence between sig-
niﬁcant wins and losses against all other choices (the vote of each dataset is divided by the
number of tied choices ranked ﬁrst). Table 10 shows optimized choices of hyperparameters
for diﬀerent encoding settings used in our study.

Name
G

Method
Greedy

R/RO RegCB-elim/RegCB-opt

C-nu

Cover-NU

C-u

B/B-g
(cid:15)G

Cover

Bag/Bag-greedy
(cid:15)-greedy

A

active (cid:15)-greedy

Hyperparameters
-
C0 ∈ 10−{1,2,3}
N ∈ {4, 8, 16}
ψ ∈ {0.01, 0.1, 1}
N ∈ {4, 8, 16}
ψ ∈ {0.01, 0.1, 1}
N ∈ {4, 8, 16}
(cid:15) ∈ {0.02, 0.05, 0.1}
(cid:15) ∈ {0.02, 1}
C0 ∈ 10−{2,4,6}

Reduction
IWR
-

IPS/DR

IPS/DR

IPS/DR/IWR
IPS/DR/IWR

IPS/DR/IWR

Appendix B. Evaluation Details

B.1 Algorithms and Hyperparameters

We ran each method on every dataset with the following hyperparameters:

• algorithm-speciﬁc hyperparameters, shown in Table 9.

• 9 choices of learning rates, on a logarithmic grid from 0.001 to 10 (see Section 2.4).

• 3 choices of reductions: IPS, DR and IWR (see Section 2.3). Note that these mainly
apply to methods that reduce to oﬀ-policy optimization (i.e., ((cid:15)-)greedy and bagging),
and to some extent, methods that reduce to cost-sensitive classiﬁcation (i.e., cover
and active (cid:15)-greedy, though the IWR reduction is heuristic in this case). Both RegCB
variants directly reduce to regression.

• 3 choices of loss encodings: 0/1, -1/0 and 9/10 (see Eq. (7)). 0/1 and -1/0 encodings
are typically a design choice, while the experiments with 9/10 are aimed at assessing
some robustness to loss range.

B.2 Additional Evaluation Results

This sections provides additional experimental results, and more detailed win/loss statistics
for tables in the main paper, showing both signiﬁcant wins and signiﬁcant losses, rather
than just their diﬀerence.

26

A Contextual Bandit Bake-off

Table 10: Optimized choices of hyperparameters for diﬀerent encoding settings, obtained
using the voting mechanism described in Table 9: -1/0 (same as bold choices in Table 9, used
in Tables 1(top left), 2ce, 5, 11a, 12a, 13a and in the ﬁgures); 0/1 (used in Tables 1(bottom
left), 2abd, 11b, 12b, 13b, 14).

Algorithm
G
R/RO
C-nu
C-u
B
B-g
(cid:15)G
A

-1/0
-
C0 = 10−3
N = 4, ψ = 0.1, DR
N = 4, ψ = 0.1, IPS
N = 4, IWR
N = 4, IWR
(cid:15) = 0.02, IWR

0/1
-
C0 = 10−3
N = 4, ψ = 0.01, DR
N = 4, ψ = 0.1, DR
N = 16, IWR
N = 8, IWR
(cid:15) = 0.02, IWR

(cid:15) = 0.02, C0 = 10−6, IWR (cid:15) = 0.02, C0 = 10−6, IWR

Table 11: Statistically signiﬁcant wins / losses of all methods on the 324 held-out multiclass
classiﬁcation datasets. Hyperparameters are ﬁxed as given in Table 10.

↓ vs →
G
R
RO
C-nu
B
B-g
(cid:15)G
C-u
A

↓ vs →
G
R
RO
C-nu
B
B-g
(cid:15)G
C-u
A

G
-
25 / 22
16 / 8
33 / 41
15 / 77
16 / 65
6 / 59
10 / 159
10 / 33

G
-
66 / 27
69 / 5
51 / 36
38 / 75
39 / 72
21 / 69
30 / 151
19 / 36

R
22 / 25
-
23 / 16
31 / 49
19 / 74
20 / 61
12 / 70
11 / 159
15 / 48

R
27 / 66
-
38 / 15
24 / 42
12 / 87
16 / 83
19 / 108
4 / 172
22 / 77

RO
8 / 16
16 / 23
-
21 / 47
13 / 76
13 / 60
4 / 71
6 / 166
7 / 46

RO
5 / 69
15 / 38
-
13 / 59
7 / 109
9 / 105
3 / 121
3 / 175
4 / 89

C-nu
41 / 33
49 / 31
47 / 21
-
26 / 66
31 / 51
19 / 75
10 / 159
29 / 50

B
77 / 15
74 / 19
76 / 13
66 / 26
-
32 / 11
41 / 46
16 / 126
54 / 26
(a) -1/0 encoding

C-nu
36 / 51
42 / 24
59 / 13
-
27 / 83
29 / 72
25 / 96
6 / 170
31 / 63

B
75 / 38
87 / 12
109 / 7
83 / 27
-
34 / 21
49 / 59
14 / 131
67 / 37
(b) 0/1 encoding

B-g
65 / 16
61 / 20
60 / 13
51 / 31
11 / 32
-
31 / 49
11 / 130
42 / 27

(cid:15)G
59 / 6
70 / 12
71 / 4
75 / 19
46 / 41
49 / 31
-
14 / 125
37 / 2

C-u
159 / 10
159 / 11
166 / 6
159 / 10
126 / 16
130 / 11
125 / 14
-
152 / 9

B-g
72 / 39
83 / 16
105 / 9
72 / 29
21 / 34
-
46 / 65
18 / 129
54 / 42

(cid:15)G
69 / 21
108 / 19
121 / 3
96 / 25
59 / 49
65 / 46
-
28 / 122
43 / 3

C-u
151 / 30
172 / 4
175 / 3
170 / 6
131 / 14
129 / 18
122 / 28
-
151 / 22

A
33 / 10
48 / 15
46 / 7
50 / 29
26 / 54
27 / 42
2 / 37
9 / 152
-

A
36 / 19
77 / 22
89 / 4
63 / 31
37 / 67
42 / 54
3 / 43
22 / 151
-

Extended tables. Tables 11 and 12 are extended versions of Table 1, showing both sig-
niﬁcant wins and loss, more methods, and separate statistics for multiclass and multilabel
datasets. In particular, we can see that both variants of RegCB become even more com-
petitive against all other methods when using 0/1 encodings. Table 14 extends Table 2(a)
with additional methods. Table 15 is a more detailed win/loss version of Table 3, and
additionally shows statistics for 0/1 encodings.

We also show separate statistics in Table 13 for the 8 datasets from the UCI repository
considered in (Foster et al., 2018), which highlight that Greedy can outperform RegCB on
some of these datasets, and that the optimistic variant of RegCB is often superior to the
elimination variant. We note that our experimental setup is quite diﬀerent from Foster et al.
(2018), who consider batch learning on an doubling epoch schedule, which might explain
some of the diﬀerences in the results.

27

Bietti, Agarwal and Langford

Table 12: Statistically signiﬁcant wins / losses of all methods on the 5 multilabel classiﬁca-
tion datasets. Hyperparameters are ﬁxed as given in Table 10.

↓ vs → G
-
G
2 / 2
R
0 / 1
RO
1 / 3
C-nu
2 / 2
B
2 / 3
B-g
1 / 2
(cid:15)G
1 / 4
C-u
1 / 2
A

↓ vs → G
-
G
1 / 3
R
2 / 2
RO
3 / 1
C-nu
0 / 4
B
1 / 4
B-g
0 / 4
(cid:15)G
0 / 5
C-u
0 / 3
A

R
2 / 2
-
2 / 2
0 / 2
0 / 3
0 / 3
2 / 3
0 / 5
2 / 3

R
3 / 1
-
3 / 0
4 / 1
2 / 2
3 / 2
2 / 2
1 / 4
2 / 2

RO C-nu
3 / 1
1 / 0
2 / 0
2 / 2
2 / 2
-
-
2 / 2
1 / 3
2 / 2
1 / 3
1 / 3
2 / 3
1 / 2
0 / 5
1 / 4
2 / 3
1 / 2

B
2 / 2
3 / 0
2 / 2
3 / 1
-
1 / 1
2 / 3
0 / 5
2 / 3
(a) -1/0 encoding

RO C-nu
1 / 3
2 / 2
1 / 4
0 / 3
1 / 2
-
-
2 / 1
1 / 4
0 / 5
1 / 3
0 / 4
0 / 4
1 / 3
1 / 4
0 / 5
1 / 4
1 / 3

B
4 / 0
2 / 2
5 / 0
4 / 1
-
2 / 0
2 / 1
1 / 2
3 / 1
(b) 0/1 encoding

B-g
3 / 2
3 / 0
3 / 1
3 / 1
1 / 1
-
3 / 2
1 / 4
3 / 2

B-g
4 / 1
2 / 3
4 / 0
3 / 1
0 / 2
-
2 / 2
1 / 4
3 / 1

(cid:15)G
2 / 1
3 / 2
2 / 1
3 / 2
3 / 2
2 / 3
-
1 / 4
0 / 1

(cid:15)G
4 / 0
2 / 2
3 / 1
4 / 0
1 / 2
2 / 2
-
1 / 4
1 / 0

C-u
4 / 1
5 / 0
4 / 1
5 / 0
5 / 0
4 / 1
4 / 1
-
4 / 1

C-u
5 / 0
4 / 1
5 / 0
4 / 1
2 / 1
4 / 1
4 / 1
-
5 / 0

A
2 / 1
3 / 2
2 / 1
3 / 2
3 / 2
2 / 3
1 / 0
1 / 4
-

A
3 / 0
2 / 2
3 / 1
4 / 1
1 / 3
1 / 3
0 / 1
0 / 5
-

Varying C0 in RegCB-opt and active (cid:15)-greedy. Figure 4 shows a comparison be-
tween RegCB-opt and Greedy or Cover-NU on our corpus, for diﬀerent values of C0, which
controls the level of exploration through the width of conﬁdence bounds. Figure 5 shows the
improvements that the active (cid:15)-greedy algorithm can achieve compared to (cid:15)-greedy, under
diﬀerent settings.

Counterfactual evaluation. Figure 6 extends Figure 3 to include all algorithms, and
additionally shows results of using IPS estimates directly on the losses (cid:96)t(at) instead of
rewards 1 − (cid:96)t(at), which tend to be signiﬁcantly worse.

B.3 Shared baseline parameterization

We also experimented with the use of an action-independent additive baseline term in our
loss estimators, which can help learn better estimates with fewer samples in some situations.
In this case the regressors take the form f (x, a) = θ0+θ(cid:62)
a x (DR).
In order to learn the baseline term more quickly, we propose to use a separate online update
for the parameters θ0 or φ0 to regress on observed losses, followed by an online update on
the residual for the action-dependent part. We scale the step-size of these baseline updates
by the largest observed magnitude of the loss, in order to adapt to the observed loss range
for normalized updates (Ross et al., 2013).

a x (IWR) or ˆ(cid:96)(x, a) = φ0+φ(cid:62)

28

A Contextual Bandit Bake-off

Table 13: Statistically signiﬁcant wins / losses of all methods on the 8 classiﬁcation datasets
from the UCI repository considered in (Foster et al., 2018). Hyperparameters are ﬁxed as
given in Table 10.

↓ vs → G
-
G
0 / 4
R
1 / 2
RO
2 / 5
C-nu
0 / 5
B
1 / 4
B-g
0 / 6
(cid:15)G
0 / 6
C-u
0 / 5
A

↓ vs → G
-
G
5 / 1
R
5 / 0
RO
2 / 2
C-nu
2 / 3
B
1 / 3
B-g
1 / 3
(cid:15)G
0 / 7
C-u
1 / 2
A

R
4 / 0
-
1 / 0
1 / 3
1 / 4
1 / 3
2 / 4
0 / 6
2 / 2

R
1 / 5
-
1 / 0
0 / 5
0 / 6
0 / 6
0 / 6
0 / 8
0 / 5

RO C-nu
5 / 2
2 / 1
3 / 1
0 / 1
4 / 1
-
-
1 / 4
1 / 3
0 / 5
2 / 2
0 / 5
2 / 3
0 / 5
0 / 7
0 / 6
3 / 2
0 / 3

B
5 / 0
4 / 1
5 / 0
3 / 1
-
2 / 0
2 / 3
0 / 6
2 / 2
(a) -1/0 encoding

RO C-nu
2 / 2
0 / 5
5 / 0
0 / 1
5 / 0
-
-
0 / 5
1 / 2
0 / 6
0 / 3
0 / 6
1 / 2
0 / 7
0 / 7
0 / 7
1 / 1
0 / 6

B
3 / 2
6 / 0
6 / 0
2 / 1
-
0 / 3
3 / 1
0 / 7
3 / 0
(b) 0/1 encoding

B-g
4 / 1
3 / 1
5 / 0
2 / 2
0 / 2
-
1 / 4
0 / 6
2 / 2

B-g
3 / 1
6 / 0
6 / 0
3 / 0
3 / 0
-
3 / 1
0 / 7
3 / 0

(cid:15)G
6 / 0
4 / 2
5 / 0
3 / 2
3 / 2
4 / 1
-
0 / 6
2 / 0

(cid:15)G
3 / 1
6 / 0
7 / 0
2 / 1
1 / 3
1 / 3
-
0 / 7
1 / 0

C-u
6 / 0
6 / 0
6 / 0
7 / 0
6 / 0
6 / 0
6 / 0
-
6 / 0

C-u
7 / 0
8 / 0
7 / 0
7 / 0
7 / 0
7 / 0
7 / 0
-
7 / 0

A
5 / 0
2 / 2
3 / 0
2 / 3
2 / 2
2 / 2
0 / 2
0 / 6
-

A
2 / 1
5 / 0
6 / 0
1 / 1
0 / 3
0 / 3
0 / 1
0 / 7
-

Table 14: Progressive validation loss for RCV1 with real-valued costs. Same as Table 2(a),
but with all methods. Hyperparameters are ﬁxed as given in Table 10. The learning rate
is optimized once on the original dataset, and we show mean and standard error based on
10 diﬀerent random reshuﬄings of the dataset.

G
0.215 ± 0.010

R
0.408 ± 0.003

RO
0.225 ± 0.008

C-nu
0.215 ± 0.006

C-u
0.570 ± 0.023

B
0.256 ± 0.006

B-g
0.251 ± 0.005

(cid:15)G
0.230 ± 0.009

A
0.230 ± 0.010

Such an additive baseline can be helpful to quickly adapt to a constant loss estimate
thanks to the separate online update. This appears particularly useful with the -1/0 en-
coding, for which the initialization at 0 may give pessimistic loss estimates which can be
damaging in particular for the greedy method, that often gets some initial exploration from
an optimistic cost encoding. This can be seen in Figure 7(top). Table 16 shows that op-
timizing over the use of baseline on each dataset can improve the performance of Greedy
and RegCB-opt when compared to other methods such as Cover-NU.

In an online learning setting, baseline can also help to quickly reach an unknown target
range of loss estimates. This is demonstrated in Figure 7(bottom), where the addition

29

Bietti, Agarwal and Langford

Table 15: Impact of reductions for Bag (left) and (cid:15)-greedy (right), with hyperparameters
optimized and encoding ﬁxed to -1/0 or 0/1. Extended version of Table 3. Each (row,
column) entry shows the statistically signiﬁcant wins and losses of row against column.

↓ vs →
ips
dr
iwr

ips
-
72 / 30
84 / 25

dr
30 / 72
-
58 / 30

iwr
25 / 84
30 / 58
-

↓ vs →
ips
dr
iwr

ips
-
22 / 85
149 / 16

dr
85 / 22
-
164 / 9

iwr
16 / 149
9 / 164
-

(a) -1/0 encoding

↓ vs →
ips
dr
iwr

ips
-
240 / 46
245 / 17

dr
46 / 240
-
97 / 33

iwr
17 / 245
33 / 97
-

↓ vs →
ips
dr
iwr

ips
-
136 / 40
182 / 36

dr
40 / 136
-
148 / 23

iwr
36 / 182
23 / 148
-

(b) 0/1 encoding

Figure 4: Comparison of RegCB-opt with Greedy (top) and Cover-NU (bottom) for diﬀerent
values of C0. Hyperparameters for Greedy and Cover-NU ﬁxed as in Table 9. Encoding
ﬁxed to -1/0. The plots consider normalized loss on held-out datasets, with red points
indicating signiﬁcant wins.

of baseline is shown to help various methods with 9/10 encodings on a large number of
datasets. We do not evaluate RegCB for 9/10 encodings as it needs a priori known upper
and lower bounds on costs.

30

Table 16: Statistically signiﬁcant wins / losses of all methods on held-out datasets, with -1/0
encoding and ﬁxed hyperparameters, except for baseline, which is optimized on each dataset
together with the learning rate. The ﬁxed hyperparameters are shown in the table below,
and were selected with the same voting approach described in Table 9. This optimization
beneﬁts Greedy and RegCB-opt in particular.

↓ vs →
G
R
RO
C-nu
B
B-g
(cid:15)G
C-u
A

G
-
14 / 32
16 / 14
15 / 60
13 / 88
16 / 69
1 / 78
1 / 178
6 / 49

R
32 / 14
-
36 / 9
25 / 43
19 / 69
24 / 52
14 / 66
9 / 167
19 / 42

RO
14 / 16
9 / 36
-
13 / 64
10 / 90
10 / 66
3 / 84
1 / 187
5 / 61

C-nu
60 / 15
43 / 25
64 / 13
-
30 / 58
41 / 34
24 / 60
6 / 163
36 / 42

B
88 / 13
69 / 19
90 / 10
58 / 30
-
33 / 10
35 / 54
7 / 133
59 / 29

B-g
69 / 16
52 / 24
66 / 10
34 / 41
10 / 33
-
18 / 57
2 / 147
38 / 37

(cid:15)G
78 / 1
66 / 14
84 / 3
60 / 24
54 / 35
57 / 18
-
10 / 129
44 / 3

C-u
178 / 1
167 / 9
187 / 1
163 / 6
133 / 7
147 / 2
129 / 10
-
164 / 5

A
49 / 6
42 / 19
61 / 5
42 / 36
29 / 59
37 / 38
3 / 44
5 / 164
-

A Contextual Bandit Bake-off

Algorithm
G
R/RO
C-nu
C-u
B
B-g
(cid:15)G
A

Hyperparameters
-
C0 = 10−3
N = 16, ψ = 0.1, DR
N = 4, ψ = 0.1, IPS
N = 4, IWR
N = 4, IWR
(cid:15) = 0.02, IWR
(cid:15) = 0.02, C0 = 10−6, IWR

31

Bietti, Agarwal and Langford

(a) DR

(b) IWR

32

Figure 5: Improvements to (cid:15)-greedy from our active learning strategy. Encoding ﬁxed to
-1/0. The IWR implementation described in Section C.1 still manages to often outperform
(cid:15)-greedy, despite only providing an approximation to Algorithm 6.

A Contextual Bandit Bake-off

(a) all multiclass datasets

(b) n ≥ 10 000 only

(c) all multiclass datasets

(d) n ≥ 10 000 only

Figure 6: Errors of IPS counterfactual estimates for the uniform random policy using ex-
ploration logs collected by various algorithms on multiclass datasets (extended version of
Figure 3). The boxes show quartiles (with the median shown as a blue line) of the dis-
tribution of squared errors across all multiclass datasets or only those with at least 10 000
examples. The logs are obtained by running each algorithm with -1/0 encodings, ﬁxed
hyperparameters from Table 9, and the best learning rate on each dataset according to pro-
gressive validation loss. The top plots consider IPS with reward estimates (as in Figure 3),
while the bottom plots consider IPS on the loss.

33

Bietti, Agarwal and Langford

Figure 7: (top) Impact of baseline on diﬀerent algorithms with encoding ﬁxed to -1/0; for
Greedy and RegCB-opt, it can signiﬁcantly help against pessimistic initial costs in some
datasets. Hyperparameters ﬁxed as in Table 9. (bottom) Baseline improves robustness to
the range of losses. The plots consider normalized loss on held-out datasets, with red points
indicating signiﬁcant wins.

34

A Contextual Bandit Bake-off

Algorithm 6 Active (cid:15)-greedy
π1; (cid:15); C0 > 0.
explore(xt):

At = {a : loss_diff(πt, xt, a) ≤ ∆t,C0};
1{a ∈ At} + (1 − (cid:15)|At|
pt(a) = (cid:15)
K
return pt;

K ) 1{πt(xt) = a};

learn(xt, at, (cid:96)t(at), pt):

ˆ(cid:96)t = estimator(xt, at, (cid:96)t(at), pt(at));

ˆct(a) =

(cid:40)ˆ(cid:96)t(a),
1,

if pt(a) > 0
otherwise.

πt+1 = csc_oracle(πt, xt, ˆct);

Appendix C. Active (cid:15)-greedy: Practical Algorithm and Analysis

This section presents our active (cid:15)-greedy method, a variant of (cid:15)-greedy that reduces the
amount of uniform exploration using techniques from active learning. Section C.1 introduces
the practical algorithm,7 while Section C.2 provides a theoretical analysis of the method,
showing that it achieves a regret of O(T 1/3) under speciﬁc favorable settings, compared
to O(T 2/3) for vanilla (cid:15)-greedy.

C.1 Algorithm

The simplicity of the (cid:15)-greedy method described in Section 3.1 often makes it the method
of choice for practitioners. However, the uniform exploration over randomly selected actions
can be quite ineﬃcient and costly in practice. A natural consideration is to restrict this
randomization over actions which could plausibly be selected by the optimal policy π∗ =
arg minπ∈Π L(π), where L(π) = E(x,(cid:96))∼D[(cid:96)(π(x))] is the expected loss of a policy π.

To achieve this, we use techniques from disagreement-based active learning (Hanneke,
2014; Hsu, 2010). After observing a context xt, for any action a, if we can ﬁnd a policy π
that would choose this action (π(xt) = a) instead of the empirically best action πt(xt),
while achieving a small loss on past data, then there is disagreement about how good
such an action is, and we allow exploring it. Otherwise, we are conﬁdent that the best
policy would not choose this action, thus we avoid exploring it, and assign it a high cost.
The resulting method is in Algorithm 6. Like RegCB, the method requires a known loss
range [cmin, cmax], and assigns a loss cmax to such unexplored actions (we consider the
range [0, 1] in Algorithm 6 for simplicity). The disagreement test we use is based on empirical
loss diﬀerences, similar to the Oracular CAL active learning method (Hsu, 2010), denoted
loss_diff, together with a threshold:

7. Our implementation is available in the following branch of Vowpal Wabbit: https://github.com/

albietz/vowpal_wabbit/tree/bakeoff.

(cid:114)

∆t,C0 =

C0

K log t
(cid:15)t

+ C0

K log t
(cid:15)t

.

35

Bietti, Agarwal and Langford

A practical implementation of loss_diff for an online setting is given below. We analyze a
theoretical form of this algorithm in Section C.2, showing a formal version of the following
theorem:

Theorem 1 With high-probability, and under favorable conditions on disagreement and on
the problem noise, active (cid:15)-greedy achieves expected regret O(T 1/3).

Note that this data-dependent guarantee improves on worst-case guarantees achieved by
the optimal algorithms Agarwal et al. (2014); Dudik et al. (2011a). In the extreme case
where the loss of any suboptimal policy is bounded away from that of π∗, we show that
our algorithm can achieve constant regret. While active learning algorithms suggest that
data-dependent thresholds ∆t can yield better guarantees (e.g., Huang et al., 2015), this
may require more work in our setting due to open problems related to data-dependent
guarantees for contextual bandits (Agarwal et al., 2017). In a worst-case scenario, active
(cid:15)-greedy behaves similarly to (cid:15)-greedy (Langford and Zhang, 2008), achieving an O(T 2/3)
expected regret with high probability.

Practical implementation of the disagreement test. We now present a practical
way to implement the disagreement tests in the active (cid:15)-greedy method, in the context of
online cost-sensitive classiﬁcation oracles based on regression, as in Vowpal Wabbit. This
corresponds to the loss_diff method in Algorithm 6.

Let ˆLt−1(π) denote the empirical loss of policy π on the (biased) sample of cost-sensitive
examples collected up to time t − 1 (see Section C.2 for details). After observing a context
xt, we want to estimate

loss_diff(πt, xt, ¯a) ≈ ˆLt−1(πt,¯a) − ˆLt−1(πt),

for any action ¯a, where

πt = arg min

ˆLt−1(π)

π

πt,¯a = arg min

π:π(xt)=¯a

ˆLt−1(π).

In our online setup, we take πt to be the current online policy (as in Algorithm 6), and
we estimate the loss diﬀerence by looking at how many online CSC examples of the form
¯c := (1{a (cid:54)= ¯a})a=1..K are needed (or the importance weight on such an example) in order
to switch prediction from πt(xt) to ¯a. If we denote this importance weight by τ¯a, then we
can estimate ˆLt−1(πt,¯a) − ˆLt−1(πt) ≈ τ¯a/t.

Computing τ¯a for IPS/DR.
In the case of IPS/DR, we use an online CSC oracle,
which is based on K regressors f (x, a) in VW, each predicting the cost for an action a.
Let ft be the current regressors for policy πt, yt(a) := ft(xt, a), and denote by st(a) the
sensitivity of regressor ft(·, a) on example (xt, ¯c(a)). This sensitivity is essentially deﬁned
to be the derivative with respect to an importance weight w of the prediction y(cid:48)(a) obtained
from the regressor after an online update (xt, ¯c(a)) with importance weight w. A similar
quantity has been used, e.g., by Huang et al. (2015); Karampatziakis and Langford (2011);
Krishnamurthy et al. (2019). Then, the predictions on actions ¯a and a cross when the

36

A Contextual Bandit Bake-off

importance weight w satisﬁes yt(¯a) − st(¯a)w = yt(a) + st(a)w. Thus, the importance weight
required for action ¯a to be preferred (i.e., smaller predicted loss) to action a is given by:

wa

¯a =

yt(¯a) − yt(a)
st(¯a) + st(a)

.

Action ¯a will thus be preferred to all other actions when using an importance weight τ¯a =
maxa wa
¯a.

Computing τ¯a for IWR. Although Algorithm 6 and the theoretical analysis require
CSC in order to assign a loss of 1 to unexplored actions, and hence does not directly
support IWR, we can consider an approximation which leverages the beneﬁts of IWR by
performing standard IWR updates as in (cid:15)-greedy, while exploring only on actions that pass
a similar disagreement test. In this case, we estimate τ¯a as the importance weight on an
online regression example (xt, 0) for the regressor ft(·, ¯a), needed to switch prediction to
¯a. If st(¯a) is the sensitivity for such an example, we have τ¯a = (yt(¯a) − y∗
t )/st(¯a), where
y∗
t = mina yt(a).

C.2 Theoretical Analysis

This section presents a theoretical analysis of the active (cid:15)-greedy method introduced in
Section C.1. We begin by presenting the analyzed version of the algorithm together with
deﬁnitions in Section C.2.1. Section C.2.2 then studies the correctness of the method,
showing that with high probability, the actions chosen by the optimal policy are always
explored, and that policies considered by the algorithm are always as good as those obtained
under standard (cid:15)-greedy exploration. This section also introduces a Massart-type low-noise
condition similar to the one considered by Krishnamurthy et al. (2019) for cost-sensitive
classiﬁcation. Finally, Section C.2.3 provides a regret analysis of the algorithm, both in the
worst case and under disagreement conditions together with the Massart noise condition.
In particular, a formal version of Theorem 1 is given by Theorem 8, and a more extreme
but informative situation is considered in Proposition 9, where our algorithm can achieve
constant regret.

C.2.1 Algorithm and definitions

We consider a version of the active (cid:15)-greedy strategy that is more suitable for theoretical
analysis, given in Algorithm 7. This method considers exact CSC oracles, as well as a CSC
oracle with one constraint on the policy (π(xt) = a in Eq.(10)). The threshold ∆t is deﬁned
later in Section C.2.2. Computing it would require some knowledge about the size of the
policy class, which we avoid by introducing a parameter C0 in the practical variant. The
disagreement strategy is based on the Oracular CAL active learning method of Hsu (2010),
which tests for disagreement using empirical error diﬀerences, and considers biased samples
when no label is queried. Here, similar tests are used to decide which actions should be
explored, in the diﬀerent context of cost-sensitive classiﬁcation, and the unexplored actions
are assigned a loss of 1, making the empirical sample biased ( ˆZT in Algorithm 7).
Deﬁnitions. Deﬁne ZT = {(xt, (cid:96)t)}t=1..T ⊂ X × RK, ˜ZT = {(xt, ˜(cid:96)t)}t=1..T (biased sam-
ple) and ˆZT = {(xt, ˆ(cid:96)t)}t=1..T (IPS estimate of biased sample), where (cid:96)t ∈ [0, 1]K is the

37

Bietti, Agarwal and Langford

Algorithm 7 active (cid:15)-greedy: analyzed version

Input: exploration probability (cid:15).
Initialize: ˆZ0 := ∅.
for t = 1, . . . do

Observe context xt. Let

πt := arg min

L(π, ˆZt−1)

π

πt,a := arg min

L(π, ˆZt−1)

π:π(xt)=a

At := {a : L(πt,a, ˆZt−1) − L(πt, ˆZt−1) ≤ ∆t}

(10)

(11)

(8)

(9)

Let

pt(a) =






1 − (|At| − 1)(cid:15)/K,
(cid:15)/K,
0,

if a = πt(xt)
if a ∈ At \ {πt(xt)}
otherwise.

Play action at ∼ pt, observe (cid:96)t(at) and set ˆZt = ˆZt−1 ∪ {(xt, ˆ(cid:96)t)}, where ˆ(cid:96)t is deﬁned
in (9).
end for

(unobserved) loss vector at time t and

˜(cid:96)t(a) =

(cid:40)

(cid:96)t(a),
1,

if a ∈ At
o/w

ˆ(cid:96)t(a) =

(cid:40) 1{a=at}

pt(at) (cid:96)t(at),
1,

if a ∈ At
o/w.

L(π, Z) =

(cid:88)

c(π(x)).

1
|Z|

(x,c)∈Z

For any set Z ⊂ X × RK deﬁned as above, we denote, for π ∈ Π,

We then deﬁne the empirical losses LT (π) := L(π, ZT ), ˆLT (π) := L(π, ˆZT ) and ˜LT (π) :=
L(π, ˜ZT ). Let L(π) := E(x,(cid:96))∼D[(cid:96)(π(x))] be the expected loss of policy π, and π∗ :=
arg minπ∈Π L(π). We also deﬁne ρ(π, π(cid:48)) := Px(π(x) (cid:54)= π(cid:48)(x)), the expected disagreement
between policies π and π(cid:48), where Px denotes the marginal distribution of D on contexts.

C.2.2 Correctness

We begin by stating a lemma that controls deviations of empirical loss diﬀerences, which
relies on Freedman’s inequality for martingales (see, e.g., Kakade and Tewari, 2009, Lemma
3).

38

A Contextual Bandit Bake-off

Lemma 2 (Deviation bounds) With probability 1 − δ, the following event holds: for all
π ∈ Π, for all T ≥ 1,

|( ˆLT (π) − ˆLT (π∗)) − ( ˜LT (π) − ˜LT (π∗))| ≤

(cid:114)

2Kρ(π, π∗)eT
(cid:15)

+

(cid:18) K
(cid:15)

(cid:19)

+ 1

eT

|(LT (π) − LT (π∗)) − (L(π) − L(π∗))| ≤ (cid:112)ρ(π, π∗)eT + 2eT ,

(12)

(13)

where eT = log(2|Π|/δT )/T and δT = δ/(T 2 + T ). We denote this event by E in what
follows.

Proof We prove the result using Freedman’s inequality (see, e.g., Kakade and Tewari,
2009, Lemma 3), which controls deviations of a sum using the conditional variance of each
term in the sum and an almost sure bound on their magnitude, along with a union bound.

For (12), let ( ˆLT (π) − ˆLT (π∗)) − ( ˜LT (π) − ˜LT (π∗)) = 1
T

(cid:80)T

t=1 Rt, with

Rt = ˆ(cid:96)t(π(xt)) − ˆ(cid:96)t(π∗(xt)) − (˜(cid:96)t(π(xt)) − ˜(cid:96)t(π∗(xt))).

We deﬁne the σ-ﬁelds Ft := σ({xi, (cid:96)i, ai}t

i=1). Note that Rt is Ft-measurable and

E[ˆ(cid:96)t(π(xt)) − ˆ(cid:96)t(π∗(xt))|xt, (cid:96)t] = ˜(cid:96)t(π(xt)) − ˜(cid:96)t(π∗(xt)),

so that E[Rt|Ft−1] = E[E[Rt|xt, (cid:96)t]|Ft−1] = 0. Thus, (Rt)t≥1 is a martingale diﬀerence
sequence adapted to the ﬁltration (Ft)t≥1. We have

|Rt| ≤ |ˆ(cid:96)t(π(xt)) − ˆ(cid:96)t(π∗(xt))| + |˜(cid:96)t(π(xt)) − ˜(cid:96)t(π∗(xt))| ≤

+ 1.

K
(cid:15)

Note that E[ˆ(cid:96)t(π(xt)) − ˆ(cid:96)t(π∗(xt))|xt, (cid:96)t] = ˜(cid:96)t(π(xt)) − ˜(cid:96)t(π∗(xt)), so that

E[R2

t |Ft−1] = E[E[R2

t |xt, (cid:96)t, At]|Ft−1]

≤ E[E[(ˆ(cid:96)t(π(xt)) − ˆ(cid:96)t(π∗(xt)))2|xt, (cid:96)t, At]|Ft−1]

(cid:20)

(cid:20)

≤ E

E

≤ E

E

(cid:20) (1{π(xt) = at} − 1{π∗(xt) = at})2
pt(at)2

|Ft−1
(cid:20) 1{π(xt) (cid:54)= π∗(xt)}(1{π(xt) = at} + 1{π∗(xt) = at})
pt(at)2
(cid:21)

|xt, (cid:96)t, At

(cid:21)

(cid:21)

= E

(cid:20) 2K 1{π(xt) (cid:54)= π∗(xt)}
(cid:15)

|Ft−1

=

ρ(π, π∗).

2K
(cid:15)

(cid:21)

(cid:21)

|xt, (cid:96)t, At

|Ft−1

Freedman’s inequality then states that (12) holds with probability 1 − δT /2|Π|.

For (13), we consider a similar setup with

Rt = (cid:96)t(π(xt)) − (cid:96)t(π∗(xt)) − (L(π) − L(π∗)).

We have E[Rt|Ft−1] = 0, |Rt| ≤ 2 and E[R2
t |Ft−1] ≤ ρ(π, π∗), which yields that (13) holds
with probability 1 − δT /2|Π| using Freedman’s inequality. A union bound on π ∈ Π and
T ≥ 1 gives the desired result.

39

Bietti, Agarwal and Langford

Threshold. We deﬁne the threshold ∆T used in (11) in Algorithm 7 as:

∆T :=

+ 1

eT −1 +

+ 3

eT −1.

(14)

(cid:32)(cid:114)

2K
(cid:15)

(cid:33)

√

(cid:19)

(cid:18) K
(cid:15)

We also deﬁne the following more precise deviation quantity for a given policy, which follows
directly from the deviation bounds in Lemma 2

∆∗

T (π) :=

(cid:32)(cid:114)

(cid:33)

2K
(cid:15)

+ 1

(cid:112)ρ(π, π∗)eT −1 +

(cid:18) K
(cid:15)

(cid:19)

+ 3

eT −1.

(15)

Note that we have ∆∗

T (π) ≤ ∆T for any policy π.

The next lemma shows that the bias introduced in the empirical sample by assigning
a loss of 1 to unexplored actions is favorable, in the sense that it will not hurt us in
identifying π∗.

Lemma 3 (Favorable bias) Assume π∗(xt) ∈ At for all t ≤ T . We have

˜LT (π) − ˜LT (π∗) ≥ LT (π) − LT (π∗).

(16)

Proof For any t ≤ T , we have ˜(cid:96)t(a) ≥ (cid:96)t(a), so that ˜LT (π) ≥ LT (π). Separately, we
have ˜(cid:96)t(π∗(xt)) = (cid:96)t(π∗(xt)) for all t ≤ T using the deﬁnition of ˜(cid:96)t and the assumption
π∗(xt) ∈ At, hence ˜LT (π∗) ≥ LT (π∗).

We now show that with high probability, the optimal action is always explored by the

algorithm.

Lemma 4 Assume that event E holds. The actions given by the optimal policy are always
explored for all t ≥ 1, i.e., π∗(xt) ∈ At for all t ≥ 1.

Proof We show by induction on T ≥ 1 that π∗(xt) ∈ At for all t = 1, . . . , T . For the base
case, we have A1 = [K] since ˆZ0 = ∅ and hence empirical errors are always equal to 0, so
that π∗(x1) ∈ A1. Let us now assume as the inductive hypothesis that π∗(xt) ∈ At for all
t ≤ T − 1.

From deviation bounds, we have

ˆLT −1(πT ) − ˆLT −1(π∗) ≥ ˜LT −1(πT ) − ˜LT −1(π∗) −

(cid:32)(cid:114)

2Kρ(π, π∗)eT −1
(cid:15)

(cid:33)

+ (K/(cid:15) + 1)eT −1

LT −1(πT ) − LT −1(π∗) ≥ L(πT ) − L(π∗) −

(cid:16)(cid:112)ρ(π, π∗)eT −1 + 2eT −1

(cid:17)

.

Using Lemma 3 together with the inductive hypothesis, the above inequalities yield

ˆLT −1(πT ) − ˆLT −1(π∗) ≥ L(πT ) − L(π∗) − ∆∗

T (πT ).

Now consider an action a /∈ At. Using the deﬁnition (11) of At, we have

ˆLT −1(πT,a) − ˆLT −1(π∗) = ˆLT −1(πT,a) − ˆLT −1(πT ) + ˆLT −1(πT ) − ˆLT −1(π∗)

> ∆T − ∆∗

T (πT ) = 0,

40

A Contextual Bandit Bake-off

which implies π∗(xT ) (cid:54)= a, since ˆLT −1(πT,a) is the minimum of ˆLT −1 over policies satisfying
π(xT ) = a. This yields π∗(xT ) ∈ AT , which concludes the proof.

With the previous results, we can now prove that with high probability, discarding some
of the actions from the exploration process does not hurt us in identifying good policies.
In particular, πT +1 is about as good as it would have been with uniform (cid:15)-exploration all
along.

Theorem 5 Under the event E, which holds with probability 1 − δ,

L(πT +1) − L(π∗) ≤ ∆∗

T +1(πT +1).

In particular, L(πT +1) − L(π∗) ≤ ∆T +1.
Proof Assume event E holds. Using (12-13) combined with Lemma 3 (which holds by
Lemma 4), we have

L(πT +1) − L(π∗) ≤ ˆLT (πT +1) − ˆLT (π∗) + ∆∗

T +1(πT +1) ≤ ∆∗

T +1(πT +1).

Massart noise condition. We introduce a low-noise condition that will help us obtain
improved regret guarantees. Similar conditions have been frequently used in supervised
learning (Massart et al., 2006) and active learning (Hsu, 2010; Huang et al., 2015; Krish-
namurthy et al., 2019) for obtaining better data-dependent guarantees. We consider the
following Massart noise condition with parameter τ > 0:

ρ(π, π∗) ≤

(L(π) − L(π∗)).

1
τ

(M)

This condition holds when E[mina(cid:54)=π∗(x) (cid:96)(a) − (cid:96)(π∗(x))|x] ≥ τ , Px-almost surely, which is
similar to the Massart condition considered in Krishnamurthy et al. (2019) in the context
of active learning for cost-sensitive classiﬁcation. Indeed, we have

L(π) − L(π∗) = E[1{π(x) (cid:54)= π∗(x)}((cid:96)(π(x)) − (cid:96)(π∗(x))]

+ E[1{π(x) = π∗(x)}((cid:96)(π∗(x)) − (cid:96)(π∗(x)))]
(cid:20)
1{π(x) (cid:54)= π∗(x)}

(cid:96)(a) − (cid:96)(π∗(x))

(cid:18)

(cid:19)(cid:21)

≥ E

= E[1{π(x) (cid:54)= π∗(x)} E[ min

(cid:96)(a) − (cid:96)(π∗(x))|x]]

min
a(cid:54)=π∗(x)

a(cid:54)=π∗(x)

≥ E[1{π(x) (cid:54)= π∗(x)}τ ] = τ ρ(π, π∗),

which is precisely (M). The condition allows us to obtain a fast rate for the policies con-
sidered by our algorithm, as we now show.

Theorem 6 Assume the Massart condition (M) holds with parameter τ . Under the event
E, which holds w.p. 1 − δ,

for some numeric constant C.

L(πT +1) − L(π∗) ≤ C

eT ,

K
τ (cid:15)

41

Bietti, Agarwal and Langford

Proof Using Theorem 5 and the Massart condition, we have

L(πT +1) − L(π∗) ≤ ∆∗

T +1(πT +1) =

(cid:32)(cid:114)

(cid:33)

2K
(cid:15)

+ 1

(cid:112)ρ(πT +1, π∗)eT +

(cid:18) K
(cid:15)

(cid:19)

+ 3

eT

+ 1

(cid:112)(L(πT +1) − L(π∗))eT /τ +

(cid:18) K
(cid:15)

(cid:19)

+ 3

eT

(cid:32)(cid:114)

(cid:33)

≤

≤

2K
(cid:15)

(cid:114)

8KeT
τ (cid:15)

(L(πT +1) − L(π∗)) +

4KeT
(cid:15)

.

Solving the quadratic inequality in L(πT +1) − L(π∗) yields the result.

C.2.3 Regret Analysis

In a worst-case scenario, the following result shows that Algorithm 7 enjoys a similar O(T 2/3)
regret guarantee to the vanilla (cid:15)-greedy approach (Langford and Zhang, 2008).

Theorem 7 Conditioned on the event E, which holds with probability 1 − δ, the expected
regret of the algorithm is

E[RT |E] ≤ O

(cid:32)(cid:114)

KT log(T |Π|/δ)
(cid:15)

(cid:33)

+ T (cid:15)

.

Optimizing over the choice of (cid:15) yields a regret O(T 2/3(K log(T |Π|/δ))1/3).

Proof We condition on the 1−δ probability event E that the deviation bounds of Lemma 2
hold. We have

E[(cid:96)t(at) − (cid:96)t(π∗(xt))|Ft−1] = E[1{at = πt(xt)}((cid:96)t(πt(xt)) − (cid:96)t(π∗(xt)))|Ft−1]
+ E[1{at (cid:54)= πt(xt)}((cid:96)t(at) − (cid:96)t(π∗(xt)))|Ft−1]

≤ E[(cid:96)t(πt(xt)) − (cid:96)t(π∗(xt))|Ft−1] + E[E[1 − pt(πt(xt))|xt]|Ft−1]
≤ L(πt) − L(π∗) + (cid:15).

Summing over t and applying Theorem 5 together with ∆∗

t (π) ≤ ∆t, we obtain

E[RT |E] = E

(cid:96)t(at) − (cid:96)t(π∗(xt))|E

(cid:35)

(cid:34) T

(cid:88)

t=1
T
(cid:88)

t=2

≤ 1 + T (cid:15) +

∆t.

T
(cid:88)

t=2

≤ 1 +

E[L(πt) − L(π∗) + (cid:15)|Ft−1, E]

Using (cid:80)T

t=2

√

et ≤ O((cid:112)T log(8T 2|Π|/δ)) and (cid:80)T
(cid:114)

(cid:32)

E[RT |E] ≤ O

1 +

KT log(T |Π|/δ)
(cid:15)

+

K log(T |Π|/δ)
(cid:15)

(cid:33)

log T + T (cid:15)

,

t=2 et ≤ O(log(8T 2|Π|/δ) log T ), we obtain

42

A Contextual Bandit Bake-off

which yields the result.

Disagreement deﬁnitions.
In order to obtain improvements in regret guarantees over
the worst case, we consider notions of disagreement that extend standard deﬁnitions from
the active learning literature (e.g., Hanneke, 2014; Hsu, 2010; Huang et al., 2015) to the
multiclass case. Let B(π∗, r) := {π ∈ Π : ρ(π, π∗) ≤ r} be the ball centered at π∗ under
the (pseudo)-metric ρ(·, ·). We deﬁne the disagreement region DIS(r) and disagreement
coeﬃcient θ as follows:

DIS(r) := {x : ∃π ∈ B(π∗, r) π(x) (cid:54)= π∗(x)}
P (x ∈ DIS(r))
r

θ := sup
r>0

.

The next result shows that under the Massart condition and with a ﬁnite disagreement
coeﬃcient θ, our algorithm achieves a regret that scales as O(T 1/3) (up to logarithmic
factors), thus improving on worst-case guarantees obtained by optimal algorithms such
as Agarwal et al. (2012, 2014); Dudik et al. (2011a).

Theorem 8 Assume the Massart condition (M) holds with parameter τ . Conditioning on
the event E which holds w.p. 1 − δ, the algorithm has expected regret

E[RT |E] ≤ O

(cid:18) K log(T |Π|/δ)
τ (cid:15)

log T +

(cid:112)(cid:15)KT log(T |Π|/δ)

(cid:19)

.

θ
τ

Optimizing over the choice of (cid:15) yields a regret

E[RT |E] ≤ O

(θK log(T |Π|/δ))2/3(T log T )1/3

.

(cid:19)

(cid:18) 1
τ

Proof Assume E holds. Let t ≥ 2, and assume a ∈ At \ {π∗(xt)}. Deﬁne
(cid:40)

πa =

πt,
πt,a,

if πt(xt) = a
if πt(xt) (cid:54)= a,

so that we have πa(xt) = a (cid:54)= π∗(xt).

• If πa = πt, then L(πa) − L(π∗) ≤ ∆∗

t (πa) ≤ ∆t by Theorem 5

• If πa = πt,a, using deviation bounds, Lemma 4 and 3, we have

L(πa) − L(π∗) = L(πt,a) − L(π∗)

≤ ˆLt−1(πt,a) − ˆLt−1(π∗) + ∆∗
= ˆLt−1(πt,a) − ˆLt−1(πt)
(cid:123)(cid:122)
(cid:125)
≤∆t

(cid:124)

(cid:124)

+ ˆLt−1(πt) − ˆLt−1(π∗)
(cid:123)(cid:122)
(cid:125)
≤0

t (πt,a)

+∆∗

t (πt,a)

≤ 2∆t,

where the last inequality uses a ∈ At.

43

Bietti, Agarwal and Langford

By the Massart assumption, we then have ρ(πa, π∗) ≤ 2∆t/τ . Hence, we have xt ∈
DIS(2∆t/τ ). We have thus shown

E[E[1{a ∈ At \ {π∗(xt)}}|xt]|Ft−1] ≤ E[P (xt ∈ DIS(2∆t/τ ))|Ft−1] ≤ 2θ∆t/τ.

We then have

E[(cid:96)t(at) − (cid:96)t(π∗(xt))|Ft−1] = E[1{at = πt(xt)}((cid:96)t(πt(xt)) − (cid:96)t(π∗(xt)))

+ 1{at = π∗(xt) ∧ at (cid:54)= πt(xt)}((cid:96)t(π∗(xt)) − (cid:96)t(π∗(xt)))

1{at = a ∧ a /∈ {πt(xt), π∗(xt)}}((cid:96)t(a) − (cid:96)t(π∗(xt)))|Ft−1]

≤ E[(cid:96)t(πt(xt)) − (cid:96)t(π∗(xt))|Ft−1]

+ E

E[1{at = a} 1{a /∈ {πt(xt), π∗(xt)}}|xt]|Ft−1

(cid:35)

+

K
(cid:88)

a=1

(cid:34) K
(cid:88)

a=1

= L(πt) − L(π∗) +

E[E[pt(a) 1{a /∈ {πt(xt), π∗(xt)}}|xt]|Ft−1]

≤ L(πt) − L(π∗) +

E[E[1{a ∈ At \ {π∗(xt)}}|xt]|Ft−1]

≤ C

et−1 + 2(cid:15)θ∆t/τ,

K
τ (cid:15)

where we used

pt(a) 1{a /∈ {πt(xt), π∗(xt)}} =

1{a ∈ At \ {πt(xt), π∗(xt)}}

≤

1{a ∈ At \ {π∗(xt)}}.

Summing over t and taking total expectations (conditioned on E) yields

E[RT |E] ≤ O

(cid:32)

K log(T |Π|/δ)
τ (cid:15)

log T +

(cid:32)(cid:114)

(cid:15)θ
τ

KT log(T |Π|/δ)
(cid:15)

+

K log(T |Π|/δ)
(cid:15)

(cid:33)(cid:33)

log(T )

,

and the result follows.

Finally, we look at a simpler instructive example, which considers an extreme situation
where the expected loss of any suboptimal policy is bounded away from that of the opti-
mal policy. In this case, Algorithm 7 can achieve constant regret when the disagreement
coeﬃcient is bounded, as shown by the following result.

Proposition 9 Assume that L(π) − L(π∗) ≥ τ > 0 for all π (cid:54)= π∗, and that θ < ∞. Under
the event E, the algorithm achieves constant expected regret. In particular, the algorithm
stops incurring regret for T > T0 := max{t : 2∆t > τ }.

K
(cid:88)

a=1

(cid:15)
K

K
(cid:88)

a=1

(cid:15)
K
(cid:15)
K

44

A Contextual Bandit Bake-off

Proof By Theorem 5 and our assumption, we have L(πt) − L(π∗) ≤ 1{∆t ≥ τ }∆t.
Similarly, the assumption implies that ρ(π, π∗) ≤ 1{L(π) − L(π∗) ≥ τ }, so that using
similar arguments to the proof of Theorem 8, we have

E[E[1{a ∈ At \ {π∗(xt)}}|xt]|Ft−1] ≤ θ 1{2∆t ≥ τ }.

Following the proof of Theorem 8, this implies that when t is such that 2∆t < τ , then we
have

Let T0 := max{t : 2∆t ≥ τ }. We thus have

E[(cid:96)t(at) − (cid:96)t(π∗(xt))|Ft−1] = 0.

E[RT |E] ≤ 1 +

(∆t + (cid:15)).

T0(cid:88)

t=2

45

0
2
0
2
 
n
a
J
 
4
2
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
4
6
0
4
0
.
2
0
8
1
:
v
i
X
r
a

A Contextual Bandit Bake-off

A Contextual Bandit Bake-oﬀ

Alberto Bietti
Inria, Ecole normale sup´erieure, PSL Research University, Paris, France∗
Alekh Agarwal
Microsoft Research, Redmond, WA
John Langford
Microsoft Research, New York, NY

alberto.bietti@inria.fr

alekha@microsoft.com

jcl@microsoft.com

Abstract

Contextual bandit algorithms are essential for solving many real-world interactive ma-
chine learning problems. Despite multiple recent successes on statistically and computation-
ally eﬃcient methods, the practical behavior of these algorithms is still poorly understood.
We leverage the availability of large numbers of supervised learning datasets to empirically
evaluate contextual bandit algorithms, focusing on practical methods that learn by relying
on optimization oracles from supervised learning. We ﬁnd that a recent method (Foster
et al., 2018) using optimism under uncertainty works the best overall. A surprisingly close
second is a simple greedy baseline that only explores implicitly through the diversity of
contexts, followed by a variant of Online Cover (Agarwal et al., 2014) which tends to be
more conservative but robust to problem speciﬁcation by design. Along the way, we also
evaluate various components of contextual bandit algorithm design such as loss estimators.
Overall, this is a thorough study and review of contextual bandit methodology.
Keywords:

contextual bandits, online learning, evaluation

1. Introduction

At a practical level, how should contextual bandit learning and exploration be done?

In the contextual bandit problem, a learner repeatedly observes a context, chooses an
action, and observes a loss for the chosen action only. Many real-world interactive machine
learning tasks are well-suited to this setting: a movie recommendation system selects a
movie for a given user and receives feedback (click or no click) only for that movie; a choice
of medical treatment may be prescribed to a patient with an outcome observed for (only)
the chosen treatment. The limited feedback (known as bandit feedback) received by the
learner highlights the importance of exploration, which needs to be addressed by contextual
bandit algorithms.

The focal point of contextual bandit (henceforth CB) learning research is eﬃcient ex-
ploration algorithms (Abbasi-Yadkori et al., 2011; Agarwal et al., 2012, 2014; Agrawal and
Goyal, 2013; Dudik et al., 2011a; Langford and Zhang, 2008; Russo et al., 2018). How-
ever, many of these algorithms remain far from practical, and even when considering more
practical variants, their empirical behavior is poorly understood, typically with limited eval-

∗. Part of this work was done while AB was visiting Microsoft Research NY, supported by the MSR-Inria

Joint Center.

1

Bietti, Agarwal and Langford

uation on just a handful of scenarios. In particular, strategies based on upper conﬁdence
bounds (Abbasi-Yadkori et al., 2011; Li et al., 2010) or Thompson sampling (Agrawal and
Goyal, 2013; Russo et al., 2018) are often intractable for sparse, high-dimensional datasets,
and make strong assumptions on the model representation. The method of Agarwal et al.
(2014) alleviates some of these diﬃculties while being statistically optimal under weak as-
sumptions, but the analyzed version is still far from practical, and the worst-case guarantees
may lead to overly conservative exploration that can be ineﬃcient in practice.

The main objective of our work is an evaluation of practical methods that are relevant
to practitioners. We focus on algorithms that rely on optimization oracles from supervised
learning such as cost-sensitive classiﬁcation or regression oracles, which provides computa-
tional eﬃciency and support for generic representations. We further rely on online learning
implementations of the oracles, which are desirable in practice due to the sequential na-
ture of contextual bandits. While conﬁdence-based strategies and Thompson sampling are
not directly adapted to this setting, we achieve it with online Bootstrap approximations for
Thompson sampling (Agarwal et al., 2014; Eckles and Kaptein, 2014; Osband and Van Roy,
2015), and with the conﬁdence-based method of Foster et al. (2018) based on regression
oracles, which contains LinUCB as a special case. Additionally, we consider practical de-
sign choices such as loss encodings (e.g., if values have a range of 1, should we encode the
costs of best and worst outcomes as 0/1 or -1/0?), and for methods that learn by reduc-
tion to oﬀ-policy learning, we study diﬀerent reduction techniques beyond the simple inverse
propensity scoring approach. All of our experiments are based on the online learning system
Vowpal Wabbit1 which has already been successfully used in production systems (Agarwal
et al., 2016).

The interactive aspect of CB problems makes them notoriously diﬃcult to evaluate in
real-world settings beyond a handful of tasks. Instead, we leverage the wide availability of
supervised learning datasets with diﬀerent cost structures on their predictions, and obtain
contextual bandit instances by simulating bandit feedback, treating labels as actions and
hiding the loss of all actions but the chosen one. This setup captures the generality of the
i.i.d. contextual bandit setting, while avoiding some diﬃcult aspects of real-world settings
that are not supported by most existing algorithms and are diﬃcult to evaluate, such as non-
stationarity. We consider a large collection of over 500 datasets with varying characteristics
and various cost structures, including multiclass, multilabel and more general cost-sensitive
datasets with real-valued costs. To our knowledge, this is the ﬁrst evaluation of contextual
bandit algorithms on such a large and diverse corpus of datasets.

Our evaluation considers online implementations of Bootstrap Thompson sampling (Agar-
wal et al., 2014; Eckles and Kaptein, 2014; Osband and Van Roy, 2015), the Cover approach
of Agarwal et al. (2014), (cid:15)-greedy (Langford and Zhang, 2008), RegCB (Foster et al., 2018,
which includes LinUCB as a special case), and a basic greedy method similar to the one
studied in Bastani et al. (2017); Kannan et al. (2018). As the ﬁrst conclusion of our study,
we ﬁnd that the recent RegCB method (Foster et al., 2018) performs the best overall across
a number of experimental conditions. Remarkably, we discover that a close second in our set
of methods is the simple greedy baseline, often outperforming most exploration algorithms.
Both these methods have drawbacks in theory; greedy can fail arbitrarily poorly in prob-

1. https://vowpalwabbit.org

2

A Contextual Bandit Bake-off

Figure 1: Comparison between three competitive approaches: RegCB (conﬁdence based),
Cover-NU (variant of Online Cover) and Greedy. The plots show relative loss compared
to supervised learning (lower is better) on all datasets with 5 actions or more. Red points
indicate datasets with a statistically signiﬁcant diﬀerence in loss between two methods. A
greedy approach can outperform exploration methods in many cases; yet both Greedy and
RegCB may fail to explore eﬃciently on some other datasets where Cover-NU dominates.

lems where intentional exploration matters, while UCB methods make stronger modeling
assumptions and can have an uncontrolled regret when the assumptions fail. The logs col-
lected by deploying these methods in practice are also unﬁt for later oﬀ-policy experiments,
an important practical consideration. Our third conclusion is that several methods which
are more robust in that they make only a relatively milder i.i.d. assumption on the problem
tend to be overly conservative and often pay a steep price on easier datasets. Nevertheless,
we ﬁnd that an adaptation of Online Cover (Agarwal et al., 2014) is quite competitive on a
large fraction of our datasets. We also evaluate the eﬀect of diﬀerent ways to encode losses
and study diﬀerent reduction mechanisms for exploration algorithms that rely on oﬀ-policy
learning (such as (cid:15)-greedy), ﬁnding that a technique based on importance-weighted regres-
sion tends to outperform other approaches when applicable. We show pairwise comparisons
between the top 3 methods in our evaluation in Figure 1 for datasets with 5 or more actions.
For future theoretical research, our results motivate an emphasis on understanding greedy
strategies, building on recent progress (Bastani et al., 2017; Kannan et al., 2018), as well
as eﬀectively leveraging easier datasets in exploration problems (Agarwal et al., 2017).

1.1 Organization of the paper

The paper is organized as follows:

• Section 2 provides relevant background on i.i.d. contextual bandits, optimization or-
acles, and mechanisms for reduction to oﬀ-policy learning, and introduces our exper-
imental setup.

• Section 3 describes the main algorithms we consider in our evaluation, as well as the

modiﬁcations that we found eﬀective in practice.

• Section 4 presents the results and insights from our experimental evaluation.
• Finally, we conclude in Section 5 with a discussion of our ﬁndings and a collection
of guidelines and recommendations for practitioners that come out of our empirical
study, as well as open questions for theoreticians.

3

Bietti, Agarwal and Langford

2. Contextual Bandit Setup

In this section, we present the learning setup considered in this work, recalling the stochastic
contextual bandit setting, the notion of optimization oracles, various techniques used by
contextual bandit algorithms for leveraging these oracles, and ﬁnally our experimental setup.

2.1 Learning Setting

The stochastic (i.i.d.) contextual bandit learning problem can be described as follows. At
each time step t, the environment produces a pair (xt, (cid:96)t) ∼ D independently from the past,
where xt ∈ X is a context vector and (cid:96)t = ((cid:96)t(1), . . . , (cid:96)t(K)) ∈ RK is a loss vector, with K
the number of possible actions, and the data distribution is denoted D. After observing the
context xt, the learner chooses an action at, and only observes the loss (cid:96)t(at) corresponding
to the chosen action. The goal of the learner is to trade-oﬀ exploration and exploitation in
order to incur a small cumulative regret

RT :=

(cid:96)t(at) −

(cid:96)t(π∗(xt)),

T
(cid:88)

t=1

T
(cid:88)

t=1

with respect to the optimal policy π∗ ∈ arg minπ∈Π E(x,(cid:96))∼D[(cid:96)(π(x))], where Π denotes a
(large, possibly inﬁnite) set of policies π : X → {1, . . . , K} which we would like to do well
against.
It is often important for the learner to use randomized strategies, for instance
in order to later evaluate or optimize new policies, hence we let pt(a) ∈ [0, 1] denote the
probability that the agent chooses action a ∈ {1, . . . , K} at time t, so that at ∼ pt.

2.2 Optimization Oracles

In this paper, we focus on CB algorithms which rely on access to an optimization oracle
for solving optimization problems similar to those that arise in supervised learning, leading
to methods that are suitable for general policy classes Π. The main example is the cost-
sensitive classiﬁcation (CSC) oracle (Agarwal et al., 2014; Dudik et al., 2011a; Langford
and Zhang, 2008), which given a collection (x1, c1), . . . , (xT , cT ) ∈ X × RK computes

T
(cid:88)

arg min
π∈Π

ct(π(xt)).

t=1
The cost vectors ct = (ct(1), . . . , ct(K)) ∈ RK are often constructed using counterfactual
estimates of the true (unobserved) losses, as we describe in the next section.

Another approach is to use regression oracles, which ﬁnd f : X ×{1, . . . , K} → R from
a class of regressor functions F to predict a cost yt, given a context xt and action at (see,
e.g., Agarwal et al., 2012; Foster et al., 2018).
In this paper, we consider the following
regression oracle with importance weights ωt > 0:

(1)

(2)

While the theory typically requires exact solutions to (1) or (2), this is often impractical
due to the diﬃculty of the underlying optimization problem (especially for CSC, which

arg min
f ∈F

T
(cid:88)

t=1

ωt(f (xt, at) − yt)2.

4

A Contextual Bandit Bake-off

yields a non-convex and non-smooth problem), and more importantly because the size of the
problems to be solved keeps increasing after each iteration. In this work, we consider instead
the use of online optimization oracles for solving problems (1) or (2), which incrementally
update a given policy or regression function after each new observation, using for instance
an online gradient method. Such an online learning approach is natural in the CB setting,
and is common in interactive production systems (e.g., Agarwal et al., 2016; He et al., 2014;
McMahan et al., 2013).

2.3 Loss Estimates and Reductions

A common approach to solving problems with bandit (partial) feedback is to compute an
estimate of the full feedback using the observed loss and then apply methods for the full-
information setting to these estimated values. In the case of CBs, this allows an algorithm
to ﬁnd a good policy based on oﬀ-policy exploration data collected by the algorithm. These
loss estimates are commonly used to create CSC instances to be solved by the optimization
oracle introduced above (Agarwal et al., 2014; Dudik et al., 2011a; Langford and Zhang,
2008), a process sometimes referred as reduction to cost-sensitive classiﬁcation. Given such
estimates ˆ(cid:96)t(a) of (cid:96)t(a) for all actions a and for t = 1, . . . , T , such a reduction constructs cost
vectors ct = (ˆ(cid:96)t(1), . . . , ˆ(cid:96)t(K)) ∈ RK and feeds them along with the observed contexts xt
to the CSC oracle (1) in order to obtain a policy. We now describe the three diﬀerent
estimation methods considered in this paper, and how each is typically used for reduction
to policy learning with a CSC or regression oracle. In what follows, we consider observed
interaction records (xt, at, (cid:96)t(at), pt(at)).

Perhaps the simplest approach is the inverse propensity-scoring (IPS) estimator:

ˆ(cid:96)t(a) :=

1{a = at}.

(cid:96)t(at)
pt(at)
For any action a with pt(a) > 0, this estimator is unbiased, i.e. Eat∼pt[ˆ(cid:96)t(a)] = (cid:96)t(a), but
can have high variance when pt(at) is small. The estimator leads to a straightforward
CSC example (xt, ˆ(cid:96)t). Using such examples in (1) provides a way to perform oﬀ-policy
(or counterfactual) evaluation and optimization, which in turn allows a CB algorithm to
identify good policies for exploration.
In order to obtain good unbiased estimates, one
needs to control the variance of the estimates, e.g., by enforcing a minimum exploration
probability pt(a) ≥ (cid:15) > 0 on all actions.

(3)

In order to reduce the variance of IPS, the doubly robust (DR) estimator (Dudik

et al., 2011b) uses a separate, possibly biased, estimator of the loss ˆ(cid:96)(x, a):

ˆ(cid:96)t(a) :=

(cid:96)t(at) − ˆ(cid:96)(xt, at)
pt(at)

1{a = at} + ˆ(cid:96)(xt, a).

When ˆ(cid:96)(xt, at) is a good estimate of (cid:96)t(at), the small numerator in the ﬁrst term helps
reduce the variance induced by a small denominator, while the second term ensures that
the estimator is unbiased. Typically, ˆ(cid:96)(x, a) is learned by regression on all past observed
losses, e.g.,

(4)

(5)

ˆ(cid:96) := arg min
f ∈F

T
(cid:88)

t=1

(f (xt, at) − (cid:96)t(at))2.

5

Bietti, Agarwal and Langford

The reduction to cost-sensitive classiﬁcation is similar to IPS, by feeding cost vectors ct = ˆ(cid:96)t
to the CSC oracle.

We consider a third method that directly reduces to the importance-weighted regression
oracle (2), which we refer to as IWR (for importance-weighted regression), and is
suitable for algorithms which rely on oﬀ-policy learning.2 This approach ﬁnds a regressor

ˆf := arg min
f ∈F

T
(cid:88)

t=1

1
pt(at)

(f (xt, at) − (cid:96)t(at))2,

(6)

and considers the policy ˆπ(x) = arg mina ˆf (x, a). Such an estimator has been used, e.g.,
in the context of oﬀ-policy learning for recommendations (Schnabel et al., 2016) and is
available in the Vowpal Wabbit library. Note that if pt has full support, then the objective
is an unbiased estimate of the full regression objective on all actions,

T
(cid:88)

K
(cid:88)

t=1

a=1

(f (xt, a) − (cid:96)t(a))2.

In contrast, if the learner only explores a single action (so that pt(at) = 1 for all t), the
obtained regressor ˆf is the same as the loss estimator ˆ(cid:96) in (5). In this csae, if we consider
a x with x ∈ Rd, then the IWR reduction
a linear class of regressors of the form f (x, a) = θ(cid:62)
computes least-squares estimates ˆθa from the data observed when action a was chosen.
When actions are selected according to the greedy policy at = arg mina ˆθ(cid:62)
a xt, this setup
corresponds to the greedy algorithm considered, e.g., by Bastani et al. (2017).

Note that while CSC is typically intractable and requires approximations in order to
In
work in practice, importance-weighted regression does not suﬀer from these issues.
addition, while the computational cost for an approximate CSC online update scales with
the number of actions K, IWR only requires an update for a single action, making the
approach more attractive computationally. Another beneﬁt of IWR in an online setting is
that it can leverage importance weight aware online updates (Karampatziakis and Langford,
2011), which makes it easier to handle large inverse propensity scores.

2.4 Experimental Setup

Our experiments are conducted by simulating the contextual bandit setting using multiclass
or cost-sensitive classiﬁcation datasets, and use the online learning system Vowpal Wabbit
(VW).

Simulated contextual bandit setting. The experiments in this paper are based on
leveraging supervised cost-sensitive classiﬁcation datasets for simulating CB learning. In
particular, we treat a CSC example (xt, ct) ∈ X × RK as a CB example, with xt given as
the context to a CB algorithm, and we only reveal the loss for the chosen action at. For
a multiclass example with label yt ∈ {1, . . . , K}, we set ct(a) := 1{a (cid:54)= yt}; for multilabel
examples with label set Yt ⊆ {1, . . . , K}, we set ct(a) := 1{a /∈ Yt}; the cost-sensitive

2. Note that IWR is not directly applicable to methods that explicitly reduce to CSC oracles, such as Agar-

wal et al. (2014); Dudik et al. (2011a).

6

A Contextual Bandit Bake-off

datasets we consider have ct ∈ [0, 1]K. We consider more general loss encodings deﬁned
with an additive oﬀset on the cost by:

(cid:96)c
t (a) = c + ct(a),

(7)

for some c ∈ R. Although some techniques attempt to remove a dependence on such en-
coding choices through appropriately designed counterfactual loss estimators (Dudik et al.,
2011b; Swaminathan and Joachims, 2015), these may be imperfect in practice, and partic-
ularly in an online scenario. The behavior observed for diﬀerent choices of c allows us to
get a sense of the robustness of the algorithms to the scale of observed losses, which might
be unknown. Separately, diﬀerent values of c can lead to lower variance for loss estimation
in diﬀerent scenarios: c = 0 might be preferred if ct(a) is often 0, while c = −1 is preferred
when ct(a) is often 1. In order to have a meaningful comparison between diﬀerent algo-
rithms, loss encodings, as well as supervised multiclass classiﬁcation, our evaluation metrics
consider the original costs ct.

Online learning in VW. Online learning is an important tool for having machine learn-
ing systems that quickly and eﬃciently adapt to observed data (Agarwal et al., 2016; He
et al., 2014; McMahan et al., 2013). We run our CB algorithms in an online fashion using
Vowpal Wabbit:
instead of exact solutions of the optimization oracles from Section 2.2,
we consider online variants of the CSC and regression oracles, which incrementally update
the policies or regressors with online gradient steps or variants thereof. Note that in VW,
online CSC itself reduces to multiple online regression problems in VW (one per action), so
that we are left with only online regression steps. More speciﬁcally, we use adaptive (Duchi
et al., 2011), normalized (Ross et al., 2013) and importance-weight-aware (Karampatziakis
and Langford, 2011) gradient updates, with a single tunable step-size parameter.

a x, or in the case of the IWR reduction, regressors f (x, a) = θ(cid:62)

Parameterization. We consider linearly parameterized policies taking the form π(x) =
arg mina θ(cid:62)
a x. For the DR
loss estimator, we use a similar linear parameterization ˆ(cid:96)(x, a) = φ(cid:62)
a x. We note that the
algorithms we consider do not rely on this speciﬁc form, and easily extend to more complex,
problem-dependent representations, such as action-dependent features. Some datasets in
our evaluation have such an action-dependent structure, with diﬀerent feature vectors xa
for diﬀerent actions a; in this case we use parameterizations of the form f (x, a) = θ(cid:62)xa,
and ˆ(cid:96)(x, a) = φ(cid:62)xa, where the parameters θ and φ are shared across all actions.

3. Algorithms

In this section, we present the main algorithms we study in this paper, along with sim-
ple modiﬁcations that achieve improved exploration eﬃciency. All methods are based on
the generic scheme in Algorithm 1. The function explore computes the exploration dis-
tribution pt over actions, and learn updates the algorithm’s policies. For simiplicity, we
consider a function oracle which performs an online update to a policy using IPS, DR or
IWR reductions given an interaction record (xt, at, (cid:96)t(at), pt). For some methods (mainly
Cover), the CSC oracle is called explicitly with modiﬁed cost vectors ct rather than the IPS
or DR loss estimates ˆ(cid:96)t deﬁned in Section 2.3; in this case, we denote such an oracle call by
csc_oracle, and also use a separate routine estimator which takes an interaction record

7

Bietti, Agarwal and Langford

Algorithm 1 Generic contextual bandit algorithm

for t = 1, . . . do

Observe context xt, compute pt = explore(xt);
Choose action at ∼ pt, observe loss (cid:96)t(at);
learn(xt, at, (cid:96)t(at), pt);

end for

Algorithm 2 (cid:15)-greedy
π1; (cid:15) > 0 (or (cid:15) = 0 for Greedy).
explore(xt):

return pt(a) = (cid:15)/K + (1 − (cid:15)) 1{πt(xt) = a};

learn(xt, at, (cid:96)t(at), pt):

πt+1 = oracle(πt, xt, at, (cid:96)t(at), pt(at));

and computes IPS or DR loss estimate vectors ˆ(cid:96)t. RegCB directly uses a regression oracle,
which we denote reg_oracle.

Our implementations of each algorithm are in the Vowpal Wabbit online learning library.

3.1 (cid:15)-greedy and greedy

We consider an importance-weighted variant of the epoch-greedy approach of Langford
and Zhang (2008), given in Algorithm 2. The method acts greedily with probability 1 − (cid:15),
and otherwise explores uniformly on all actions. Learning is achieved by reduction to oﬀ-
policy optimization, through any of the three reductions presented in Section 2.3.

We also experimented with a variant we call active (cid:15)-greedy, that uses notions from
disagreement-based active learning (Hanneke, 2014; Hsu, 2010) in order to reduce uniform
exploration to only actions that could plausibly be taken by the optimal policy. While this
variant often improves on the basic (cid:15)-greedy method, we found that it is often outperformed
empirically by other exploration algorithms, and thus defer its presentation to Appendix C,
along with a theoretical analysis, for reference.

Greedy. When taking (cid:15) = 0 in the (cid:15)-greedy approach, with the IWR reduction, we are left
with a fully greedy approach that always selects the action given by the current policy. This
gives us an online variant of the greedy algorithm of Bastani et al. (2017), which regresses on
observed losses and acts by selecting the action with minimum predicted loss. Although this
greedy strategy does not have an explicit mechanism for exploration in its choice of actions,
the inherent diversity in the distribution of contexts may provide suﬃcient exploration
for good performance and provable regret guarantees (Bastani et al., 2017; Kannan et al.,
2018). In particular, under appropriate assumptions including a diversity assumption on
the contexts, one can show that all actions have a non-zero probability of being selected at
each step, providing a form of “natural” exploration from which one can establish regret
guarantees. Empirically, we ﬁnd that Greedy can perform very well in practice on many
datasets (see Section 4). If multiple actions get the same score according to the current
regressor, we break ties randomly.

8

A Contextual Bandit Bake-off

3.2 Bagging (online Bootstrap Thompson sampling)

return pt(a) ∝ |{i : πi

t(xt) = a}|;3

Algorithm 3 Bag
π1
1, . . . , πN
1 .
explore(xt):

learn(xt, at, (cid:96)t(at), pt):
for i = 1, . . . , N do
τ i ∼ P oisson(1);
t+1 = oracleτ i(πi
πi
end for

t, xt, at, (cid:96)t(at), pt(at));

{with τ 1 = 1 for bag-greedy}

We now consider a variant of Thompson sampling which is usable in practice with opti-
mization oracles. Thompson sampling provides a generic approach to exploration problems,
which maintains a belief on the data generating model in the form of a posterior distribution
given the observed data, and explores by selecting actions according to a model sampled
from this posterior (see, e.g., Agrawal and Goyal, 2013; Chapelle and Li, 2011; Russo et al.,
2018; Thompson, 1933). While the generality of this strategy makes it attractive, main-
taining this posterior distribution can be intractable for complex policy classes, and may
require strong modeling assumptions. In order to overcome such diﬃculties and to support
the optimization oracles considered in this paper, we rely on an approximation of Thompson
sampling known as the online Bootstrap Thompson sampling (Eckles and Kaptein, 2014;
Osband and Van Roy, 2015), or bagging (Agarwal et al., 2014). This approach, shown in
Algorithm 3, maintains a collection of N policies π1
t meant to approximate the pos-
terior distribution over policies via the online Bootstrap (Agarwal et al., 2014; Eckles and
Kaptein, 2014; Osband and Van Roy, 2015; Oza and Russell, 2001; Qin et al., 2013), and
explores in a Thompson sampling fashion, by averaging action decisions across all policies
(hence the name bagging).

t , . . . , πN

Each policy is trained on a diﬀerent online Bootstrap sample of the observed data, in the
form of interaction records. The online Bootstrap performs a random number τ of online
updates to each policy instead of one (this is denoted by oracleτ in Algorithm 3). We use a
Poisson distribution with parameter 1 for τ , which ensures that in expectation, each policy
is trained on t examples after t steps. In contrast to Eckles and Kaptein (2014); Osband
and Van Roy (2015), which play the arm given by one of the N policies chosen at random,
we compute the full action distribution pt resulting from such a sampling, and leverage this
for loss estimation, allowing learning by reduction to oﬀ-policy optimization as in Agarwal
et al. (2014). As in the (cid:15)-greedy algorithm, Bagging directly relies on oﬀ-policy learning
and thus all three reductions are admissible.

Greedy bagging. We also consider a simple optimization that we call greedy bagging, for
which the ﬁrst policy π1 is trained on the true data sample (like Greedy), that is, with τ

3. When policies are parametrized using regressors as in our implementation, we let πi

t(x) be uniform over
all actions tied for the lowest cost, and the ﬁnal distribution is uniform across all actions tied for best
according to one of the policies in the bag. The added randomization gives useful variance reduction in
our experiments.

9

Bietti, Agarwal and Langford

Algorithm 4 Cover
π1
1, . . . , πN
explore(xt):

1 ; (cid:15)t = min(1/K, 1/

Kt); ψ > 0.

√

t(xt) = a}|;

pt(a) ∝ |{i : πi
return (cid:15)t + (1 − (cid:15)t)pt;
return pt;

learn(xt, at, (cid:96)t(at), pt):
π1
t+1 = oracle(π1
ˆ(cid:96)t = estimator(xt, at, (cid:96)t(at), pt(at));
for i = 2, . . . , N do

t , xt, at, (cid:96)t(at), pt(at));

t+1(xt) = a}|;

qi(a) ∝ |{j ≤ i − 1 : πj
ˆc(a) = ˆ(cid:96)t(a) −
ψ(cid:15)t
(cid:15)t+(1−(cid:15)t)qi(a) ;
πi
t+1 = csc_oracle(πi
end for

t, xt, ˆc);

{for cover}
{for cover-nu}

always equal to one, instead of a bootstrap sample with random choices of τ . We found this
approach to often improve on bagging, particularly when the number of policies N is small.

3.3 Cover

This method, given in Algorithm 4, is based on Online Cover, an online approxima-
tion of the “ILOVETOCONBANDITS” algorithm of Agarwal et al. (2014). The approach
maintains a collection of N policies, π1
t , meant to approximate a covering distri-
bution over policies that are good for both exploration and exploitation. The ﬁrst policy
π1
t is trained on observed data using the oracle as in previous algorithms, while subsequent
policies are trained using modiﬁed cost-sensitive examples which encourage diversity in the
predicted actions compared to the previous policies.

t , . . . , πN

Our implementation diﬀers from the Online Cover algorithm of Agarwal et al. (2014,
Algorithm 5) in how the diversity term in the deﬁnition of ˆc(a) is handled (the second term).
When creating cost-sensitive examples for a given policy πi, this term rewards an action a
that is not well-covered by previous policies (i.e., small qi(a)), by subtracting from the cost
a term that decreases with qi(a). While Online Cover considers a ﬁxed (cid:15)t = (cid:15), we let (cid:15)t
decay with t, and introduce a parameter ψ to control the overall reward term, which bears
more similarity with the analyzed algorithm. In particular, the magnitude of the reward
is ψ whenever action a is not covered by previous policies (i.e., qi(a) = 0), but decays with
ψ(cid:15)t whenever qi(a) > 0, so that the level of induced diversity can decrease over time as we
gain conﬁdence that good policies are covered.

Cover-NU. While Cover requires some uniform exploration across all actions, our ex-
periments suggest that this can make exploration highly ineﬃcient, thus we introduce a
variant, Cover-NU, with no uniform exploration outside the set of actions selected by cov-
ering policies.

10

A Contextual Bandit Bake-off

Algorithm 5 RegCB
f1; C0 > 0.
explore(xt):

lt(a) = lcb(ft, xt, a, ∆t,C0);
ut(a) = ucb(ft, xt, a, ∆t,C0);
pt(a) ∝ 1{a ∈ arg mina(cid:48) lt(a(cid:48))};
pt(a) ∝ 1{lt(a) ≤ mina(cid:48) ut(a(cid:48))};
return pt;

learn(xt, at, (cid:96)t(at), pt):

ft+1 = reg_oracle(ft, xt, at, (cid:96)t(at));

3.4 RegCB

{RegCB-opt variant}
{RegCB-elim variant}

We consider online approximations of the two algorithms introduced by Foster et al. (2018)
based on regression oracles, shown in Algorithm 5. Both algorithms estimate conﬁdence
intervals of the loss for each action given the current context xt, denoted [lt(a), ut(a)] in
Algorithm 5, by considering predictions from a subset of regressors with small squared loss.
The optimistic variant then selects the action with smallest lower bound estimate, similar
to LinUCB, while the elimination variant explores uniformly on actions that may plausibly
be the best.

More formally, the RegCB algorithm theoretically analyzed by Foster et al. (2018) de-

ﬁnes the conﬁdence bounds as follows:

lt(a) = min
f ∈Ft

f (xt, a),

and ut(a) = max
f ∈Ft

f (xt, a).

Here, Ft is a subset of regressors that is “good” for loss estimation, in the sense that it
achieves a small regression loss on observed data, ˆRt−1(f ) := 1
s=1(f (xt, at) − (cid:96)t(at))2,
t−1
compared to the best regressor in the full regressor class F:

(cid:80)t−1

Ft := {f ∈ F : ˆRt−1(f ) − min
f ∈F

ˆRt−1(f ) ≤ ∆t},

where ∆t is a quantity decreasing with t.

Our online implementation computes approximations of these upper and lower bounds
on the loss of each action, by using a sensitivity analysis of the current regressor based
on importance weighting taken from Krishnamurthy et al. (2019) in the context of active
learning (the computations are denoted lcb and ucb in Algorithm 5). The algorithm main-
tains a regressor ft : X × {1, . . . , K} and, given a new context xt, computes lower and
upper conﬁdence bounds lt(a) ≤ ft(xt, a) ≤ ut(a). These are computed by adding “virtual”
importance-weighted regression examples with low and high costs, and ﬁnding the largest
importance weight leading to an excess squared loss smaller than ∆t,C0, where

and C0 is a parameter controlling the width of the conﬁdence bounds. This importance
weight can be found using regressor sensitivities and a binary search procedure as described

∆t,C0 =

C0 log(Kt)
t

,

11

Bietti, Agarwal and Langford

in (Krishnamurthy et al., 2019, Section 7.1). Note that this requires knowledge of the loss
range [cmin, cmax], unlike other methods. In contrast to Krishnamurthy et al. (2019), we set
the labels of the “virtual” examples to cmin − 1 for the lower bound and cmax + 1 for the
upper bound, instead of cmin and cmax.

4. Evaluation

In this section, we present our evaluation of the contextual bandit algorithms described in
Section 3. The evaluation code is available at https://github.com/albietz/cb_bakeoff.
All methods presented in this section are available in Vowpal Wabbit.4

Evaluation setup. Our evaluation consists in simulating a CB setting from cost-sensitive
classiﬁcation datasets, as described in Section 2.4. We consider a collection of 525 multiclass
classiﬁcation datasets from the openml.org platform, including among others, medical,
gene expression, text, sensory or synthetic data, as well as 5 multilabel datasets5 and 3
cost-sensitive datasets, namely a cost-sensitive version of the RCV1 multilabel dataset used
in (Krishnamurthy et al., 2019), where the cost of a news topic is equal to the tree distance
to a correct topic, as well as the two learning to rank datasets used in (Foster et al., 2018).
More details on these datasets are given in Appendix A. Because of the online setup, we
consider one or more ﬁxed, shuﬄed orderings of each dataset. The datasets widely vary in
noise levels, and number of actions, features, examples etc., allowing us to model varying
diﬃculties in CB problems.

We evaluate the algorithms described in Section 3. We ran each method on every dataset
with diﬀerent choices of algorithm-speciﬁc hyperparameters, learning rates, reductions, and
loss encodings. Details are given in Appendix B.1. Unless otherwise speciﬁed, we consider
ﬁxed choices which are chosen to optimize performance on a subset of multiclass datasets
with a voting mechanism and are highlighted in Table 9 of Appendix B, except for the
learning rate, which is always optimized.

The performance of method A on a dataset of size n is measured by the progressive

validation loss (Blum et al., 1999):

P VA =

ct(at),

1
n

n
(cid:88)

t=1

where at is the action chosen by the algorithm on the t-th example, and ct the true cost
vector. This metric allows us to capture the explore-exploit trade-oﬀ, while providing a
measure of generalization that is independent of the choice of loss encodings, and compa-
rable with online supervised learning. We also consider a normalized loss variant given
by P VA−P VOAA
, where OAA denotes an online (supervised) cost-sensitive one against all
classiﬁer. This helps highlight the diﬃculty of exploration for some datasets in our plots.

P VOAA

In order to compare two methods on a given dataset with binary costs (multiclass or
multilabel), we consider a notion of statistically signiﬁcant win or loss. We use the

4. For reproducibility purposes, the precise version of VW used to run these experiments is available at

https://github.com/albietz/vowpal_wabbit/tree/bakeoff.

5. https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html

12

A Contextual Bandit Bake-off

Table 1: Entry (row, column) shows the statistically signiﬁcant win-loss diﬀerence of row
against column. Encoding ﬁxed to -1/0 (top) or 0/1 (bottom). (left) held-out datasets
only (330 in total); ﬁxed hyperparameters, only the learning rate is optimized; (right)
all datasets (530 in total); all choices of hyperparameters are optimized on each dataset
(diﬀerent methods have diﬀerent hyperparameter settings from 1 to 18, see Table 9 in
Appendix B).

↓ vs → G RO C-nu B-g
50
-7
G
49
-
RO
22
-26
C-nu
-
-49
B-g
-17
-68
(cid:15)G

10
26
-
-22
-57

-
7
-10
-50
-54

↓ vs → G RO C-nu B-g
-22
-30
G
8
-
RO
64
37
C-nu
-
-8
B-g
-86
-84
(cid:15)G

-93
-37
-
-64
-145

-
30
93
22
-58

(cid:15)G
58
84
145
86
-

-1/0 encoding

↓ vs → G
-
G
64
RO
17
C-nu
-36
B-g
-52
(cid:15)G

RO C-nu B-g
-17
-64
36
100
45
-
45
-
-45
-45
-100
-
-19
-75
-120

↓ vs → G
-
G
124
RO
150
C-nu
75
B-g
-5
(cid:15)G

RO C-nu B-g
-75
-150
-124
65
-11
-
67
-
11
-
-67
-65
-83
-144
-125

(cid:15)G
5
125
144
83
-

0/1 encoding

(cid:15)G
54
68
57
17
-

(cid:15)G
52
120
75
19
-

following (heuristic) deﬁnition of signiﬁcance based on an approximate Z-test: if pa and pb
denote the PV loss of a and b on a given dataset of size n, then a wins over b if





1 − Φ

pa − pb

(cid:113) pa(1−pa)
n

+ pb(1−pb)
n



 < 0.05,

where Φ is the Gauss error function. We also deﬁne the signiﬁcant win-loss diﬀerence
of one algorithm against another to be the diﬀerence between the number of signiﬁcant
wins and signiﬁcant losses. We have found these metrics to provide more insight into the
behavior of diﬀerent methods, compared to strategies based on aggregation of loss measures
across all datasets. Indeed, we often found the relative performance of two methods to vary
signiﬁcantly across datasets, making aggregate metrics less informative.

Results in this section focus on Greedy (G), RegCB-optimistic (RO), Cover-NU (C-
nu), Bag-greedy (B-g) and (cid:15)-greedy ((cid:15)G), deferring other variants to Appendix B, as their
performance is typically comparable to or dominated by these methods. We combine results
on multiclass and multilabel datasets, but show them separately in Appendix B.

Eﬃcient exploration methods. Our experiments suggest that the best performing
method is the RegCB approach (Foster et al., 2018), as shown in Table 1 (left), where
the signiﬁcant wins of RO against all other methods exceed signiﬁcant losses, which yields
the best performance overall. This is particularly prominent with 0/1 encodings. With -1/0
encodings, which are generally preferred on our corpus as discussed below, the simple greedy
approach comes a close second, outperforming other methods on a large number of datasets,

13

Bietti, Agarwal and Langford

Table 2: Progressive validation loss for cost-sensitive datasets with real-valued costs in [0, 1].
Hyperparameters are ﬁxed as in Table 10. We show mean and standard error based on 10
diﬀerent random reshuﬄings. For RCV1, costs are based on tree distance to correct topics.
For MSLR and Yahoo, costs encode 5 regularly-spaced discrete relevance scores (0: perfectly
relevant, 1: irrelevant), and we include results for a loss encoding oﬀset c = −1 in Eq. (7).

G
0.215 ± 0.010

RO
0.225 ± 0.008

B-g
0.251 ± 0.005

(cid:15)G
0.230 ± 0.009

G
0.798 ± 0.0023

RO
0.794 ± 0.0007

B-g
0.799 ± 0.0013

(cid:15)G
0.807 ± 0.0020

G
0.791 ± 0.0012

RO
0.790 ± 0.0009

B-g
0.791 ± 0.0008

(cid:15)G
0.806 ± 0.0018

G
0.593 ± 0.0004

RO
0.592 ± 0.0005

B-g
0.596 ± 0.0006

(cid:15)G
0.598 ± 0.0005

G
0.589 ± 0.0005

RO
0.588 ± 0.0004

B-g
0.590 ± 0.0005

(cid:15)G
0.594 ± 0.0006

C-nu
0.215 ± 0.006
(a) RCV1

C-nu
0.798 ± 0.0012
(b) MSLR

C-nu
0.792 ± 0.0007
(c) MSLR, c = −1

C-nu
0.594 ± 0.0004
(d) Yahoo

C-nu
0.589 ± 0.0004
(e) Yahoo, c = −1

despite the lack of an explicit exploration mechanism. A possible reason for this success is
the diversity that is inherently present in the distribution of contexts across actions, which
has been shown to yield no-regret guarantees under various assumptions (Bastani et al.,
2017; Kannan et al., 2018). The noise induced by the dynamics of online learning and
random tie-breaking may also be a source of more exploration. RO and Greedy also show
strong performance on the 8 UCI datasets and the learning-to-rank datasets from Foster
et al. (2018), as shown in Tables 2 and 13. Nevertheless, both Greedy and RegCB have
known failure modes which the Cover approach is robust to by design. While the basic
approach with uniform exploration is too conservative, we found our Cover-NU variant
to be quite competitive overall. The randomization in its choice of actions yields explo-
ration logs which may be additionally used for oﬄine evaluation, in contrast to Greedy and
RegCB-opt, which choose actions deterministically. A more granular comparison of these
methods is given in Figure 2, which highlight the failure of Greedy and RegCB against
Cover-NU on some datasets which may be more diﬃcult perhaps due to a failure of mod-
eling assumptions. Bagging also outperforms other methods on some datasets, however it
is outperformed on most datasets, possibly because of the additional variance induced by
the bootstrap sampling. Table 1 (right) optimizes over hyperparameters for each dataset,
which captures the best potential of each method. Cover-NU does the best here, but also
has the most hyperparameters, indicating that a more adaptive variant could be desirable.
RegCB stays competitive, while Greedy pales possibly due to fewer hyperparameters.

Variability with dataset characteristics. Table 5 shows win-loss statistics for subsets
of the datasets with constraints on diﬀerent characteristics, such as number of actions,

14

A Contextual Bandit Bake-off

Figure 2: Pairwise comparisons among four successful methods: Greedy, Cover-nu, Bag-
greedy, and RegCB-opt. Hyperparameters ﬁxed as in Table 9, with encoding -1/0. All
held-out multiclass and multilabel datasets are shown, in contrast to Figure 1, which only
shows held-out datasets with 5 or more actions. The plots consider normalized loss, with
red points indicating signiﬁcant wins.

dimensionality, size, and performance in the supervised setting. The values for these splits
were chosen in order to have a reasonably balanced number of datasets in each table.

√

We ﬁnd that RegCB-opt is the preferred method in most situations, while Greedy and
Cover-NU can also provide good performance in diﬀerent settings. When only considering
larger datasets, RegCB-opt dominates all other methods, with Greedy a close second, while
Cover-NU seems to explore less eﬃciently. This could be related to the better adaptivity
properties of RegCB to favorable noise conditions, which can achieve improved (even log-
arithmic) regret (Foster et al., 2018), in contrast to Cover-NU, for which a slower rate (of
O(
n)) may be unavoidable since it is baked into the algorithm of Agarwal et al. (2014)
by design, and is reﬂected in the diversity terms of the costs ˆc in our online variant given
in Algorithm 4. In contrast, when n is small, RegCB-opt and Greedy may struggle to ﬁnd
good policies (in fact, their analysis typically requires a number of “warm-start” iterations
with uniform exploration), while Cover-NU seems to explore more eﬃciently from the be-
ginning, and behaves well with large action spaces or high-dimensional features. Finally,
Table 5(d,e) shows that Greedy can be the best choice when the dataset is “easy”, in the
sense that a supervised learning method achieves small loss. Achieving good performance
on such easy datasets is related to the open problem of Agarwal et al. (2017), and vari-
ants of methods designed to be agnostic to the data distribution—such as Cover(-NU) and
(cid:15)-Greedy (Agarwal et al., 2014; Langford and Zhang, 2008)—seem to be the weakest on
these datasets.

15

Bietti, Agarwal and Langford

Table 3: Impact of reductions for Bag (left) and (cid:15)-greedy (right), with hyperparameters
optimized and encoding ﬁxed to -1/0. Each (row, column) entry shows the statistically
signiﬁcant win-loss diﬀerence of row against column. IWR outperforms the other reductions
for both methods, which are the only two methods that directly reduce to oﬀ-policy learning,
and thus where such a comparison applies.

↓ vs → ips
-
ips
42
dr
59
iwr

dr
-42
-
28

iwr
-59
-28
-

↓ vs → ips
-
ips
-63
dr
133
iwr

dr
63
-
155

iwr
-133
-155
-

Table 4: Impact of encoding on diﬀerent algorithms, with hyperparameters optimized. Each
entry indicates the number of statistically signiﬁcant wins/losses of -1/0 against 0/1. -1/0
is the better overall choice of encoding, but 0/1 can be preferable on larger datasets (the
bottom row considers the 64 datasets in our corpus with more than 10,000 examples).

datasets
all
≥ 10,000

G
136 / 42
19 / 12

RO
60 / 47
10 / 18

C-nu
76 / 46
14 / 20

B-g
77 / 27
15 / 11

(cid:15)G
99 / 27
14 / 5

Reductions. Among the reduction mechanisms introduced in Section 2.3, IWR has de-
sirable properties such as tractability (the other reductions rely on a CSC objective, which
requires approximations due to non-convexity), and a computational cost that is indepen-
dent of the total number of actions, only requiring updates for the chosen action. In addition
to Greedy, which can be seen as using a form of IWR, we found IWR to work very well
for bagging and (cid:15)-greedy approaches, as shown in Table 3 (see also Table 9 in Appendix B,
which shows that IWR is also preferred when considering ﬁxed hyperparameters for these
methods). This may be attributed to the diﬃculty of the CSC problem compared to regres-
sion, as well as importance weight aware online updates, which can be helpful for small (cid:15).
Together with its computational beneﬁts, our results suggest that IWR is often a com-
pelling alternative to CSC reductions based on IPS or DR. In particular, when the number
of actions is prohibitively large for using Cover-NU or RegCB, Bag with IWR may be a
good default choice of exploration algorithm. While Cover-NU does not directly support
the IWR reduction, making them work together well would be a promising future direction.

Encodings. Table 4 indicates that the -1/0 encoding is preferred to 0/1 on many of the
datasets, and for all methods. We now give one possible explanation. As discussed in
Section 2.4, the -1/0 encoding yields low variance loss estimates when the cost is often
close to 1. For datasets with binary costs, since the learner may often be wrong in early
iterations, a cost of 1 is a good initial bias for learning. With enough data, however, the
learner should reach better accuracies and observe losses closer to 0, in which case the 0/1
encoding should lead to lower variance estimates, yielding better performance as observed
in Table 4. We tried shifting the loss range in the RCV1 dataset with real-valued costs
from [0, 1] to [−1, 0], but saw no improvements compared to the results in Table 2. Indeed,

16

A Contextual Bandit Bake-off

(a) all multiclass datasets

(b) n ≥ 10 000 only

Figure 3: Errors of IPS counterfactual estimates for the uniform random policy using explo-
ration logs collected by various algorithms on multiclass datasets. The boxes show quartiles
(with the median shown as a blue line) of the distribution of squared errors across all
multiclass datasets or only those with at least 10 000 examples. The logs are obtained by
running each algorithm with -1/0 encodings, ﬁxed hyperparameters from Table 9, and the
best learning rate on each dataset according to progressive validation loss.

a cost of 1 may not be a good initial guess in this case, in contrast to the binary cost
setting. On the MSLR and Yahoo learning-to-rank datasets, we do see some improvement
from shifting costs to the range [−1, 0], perhaps because in this case the costs are discrete
values, and the cost of 1 corresponds to the document label “irrelevant”, which appears
frequently in these datasets.

Counterfactual evaluation. After running an exploration algorithm, a desirable goal
for practitioners is to evaluate new policies oﬄine, given access to interaction logs from
exploration data. While various exploration algorithms rely on such counterfactual eval-
uation through a reduction, the need for eﬃcient exploration might restrict the ability to
evaluate arbitrary policies oﬄine, since the algorithm may only need to estimate “good”
policies in the sense of small regret with respect to the optimal policy. To illustrate this,
in Figure 3, we consider the evaluation of the uniform random stochastic policy, given by
πunif(a|x) = 1/K for all actions a = 1, . . . , K, using a simple inverse propensity scoring
(IPS) approach. While such a task is somewhat extreme and of limited practical interest
in itself, we use it mainly as a proxy for the ability to evaluate any arbitrary policy (in par-
ticular, it should be possible to evaluate any policy if we can evaluate the uniform random
policy). On a multiclass dataset, the expected loss of such a policy is simply 1 − 1/K, while
its IPS estimate is given by

ˆLIP S = 1 −

1
n

n
(cid:88)

t=1

1 − (cid:96)t(at)
Kpt(at)

,

where the loss is given by (cid:96)t(at) = 1{at (cid:54)= yt} when the correct multiclass label is yt.
Note that this quantity is well-deﬁned since the denominator is always non-zero when at is
sampled from pt, but the estimator is biased when data is collected by a method without
some amount of uniform exploration (i.e., when pt does not have full support). This bias

17

Bietti, Agarwal and Langford

is particularly evident in Figure 3b where epsilon-greedy shows very good counterfactual
evaluation performance while the other algorithms induce biased counterfactual evaluation
due to lack of full support. The plots in Figure 3 show the distribution of squared errors
( ˆLIP S − (1 − 1/K))2 across multiclass datasets. We consider IPS on the rewards 1 − (cid:96)t(at)
here as it is more adapted to the -1/0 encodings used to collect exploration logs, but we
also show IPS on losses in Figure 6 of Appendix B.2. Figure 3 shows that the more eﬃcient
exploration methods (Greedy, RegCB-optimistic, and Cover-NU) give poor estimates for
this policy, probably because their exploration logs provide biased estimates and are quite
focused on few actions that may be taken by good policies, while the uniform exploration in
(cid:15)-Greedy (and Cover-U, see Figure 6) yields better estimates, particularly on larger datasets.
The elimination version of RegCB provides slightly better logs compared to the optimistic
version (see Figure 6), and Bagging may also be preferred in this context. Overall, these
results show that there may be a trade-oﬀ between eﬃcient exploration and the ability to
perform good counterfactual evaluation, and that uniform exploration may be needed if one
would like to perform accurate oﬄine experiments for a broad range of questions.

5. Discussion and Takeaways

In this paper, we presented an evaluation of practical contextual bandit algorithms on a
large collection of supervised learning datasets with simulated bandit feedback. We ﬁnd that
a worst-case theoretical robustness forces several common methods to often over-explore,
damaging their empirical performance, and strategies that limit (RegCB and Cover-NU) or
simply forgo (Greedy) explicit exploration dominate the ﬁeld. For practitioners, our study
also provides a reference for practical implementations, while stressing the importance of
loss estimation and other design choices such as how to encode observed feedback.

Guidelines for practitioners. We now summarize some practical guidelines that come
out of our empirical study:

• Methods relying on modeling assumptions on the data distribution such as RegCB are
often preferred in practice, and even Greedy can work well (see, e.g., Table 1). They
tend to dominate more robust approaches such as Cover-NU even more prominently
on larger datasets, or on datasets where prediction is easy, e.g., due to low noise
(see Table 5). While it may be diﬃcult to assess such favorable conditions of the
data in advance, practitioners may use speciﬁc domain knowledge to design better
feature representations for prediction, which may in turn improve exploration for
these methods.

• Uniform exploration hurts empirical performance in most cases (see, e.g., the poor
performance of (cid:15)-greedy and Cover-u in Table 11 of Appendix B). Nevertheless, it
may be necessary on the hardest datasets, and may be crucial if one needs to perform
oﬀ-policy counterfactual evaluation (see Figures 3 and 6).

• Loss estimation is an essential component in many CB algorithms for good practical
performance, and DR should be preferred over IPS. For methods based on reduction
to oﬀ-policy learning, such as (cid:15)-Greedy and Bagging, the IWR reduction is typically
best, in addition to providing computational beneﬁts (see Table 3).

18

A Contextual Bandit Bake-off

• From our early experiments, we found randomization on tied choices of actions to
always be useful. For instance, it avoids odd behavior which may arise from deter-
ministic, implementation-speciﬁc biases (e.g., always favoring one speciﬁc action over
the others).

• The choice of cost encodings makes a big diﬀerence in practice and should be carefully
considered when designing an contextual bandit problem, even when loss estimation
techniques such as DR are used. For binary outcomes, -1/0 is a good default choice
of encoding in the common situation where the observed loss is often 1 (see Table 4).

• Modeling choices and encodings sometimes provide pessimistic initial estimates that
can hurt initial exploration on some problems, particularly for Greedy and RegCB-
optimistic. Random tie-breaking as well as using a shared additive baseline can help
mitigate this issue (see Section B.3).

• The hyperparameters highlighted in Appendix B.1 obtained on our datasets may be
good default choices in practice. Nevertheless, these may need to be balanced in
order to address other conﬂicting requirements in real-world settings, such as non-
stationarity or the ability to run oﬄine experiments.

Open questions for theoreticians. Our study raises some questions of interest for
theoretical research on contextual bandits. The good performance of greedy methods calls
for a better understanding of greedy methods, building upon the work of Bastani et al.
(2017); Kannan et al. (2018), as well as methods that are more robust to more diﬃcult
datasets while adapting to such favorable scenarios, such as when the context distribution
has enough diversity. A related question is that of adaptivity to easy datasets for which
the optimal policy has small loss, an open problem pointed out by Agarwal et al. (2017)
in the form of “ﬁrst-order” regret bounds. While methods satisfying such regret bounds
have now been developed theoretically (Allen-Zhu et al., 2018), these methods are currently
not computationally eﬃcient, and obtaining eﬃcient methods based on optimization oracles
remains an important open problem. We also note that while our experiments are based on
online optimization oracles, most analyzed versions of the algorithms rely on solving the full
optimization problems; it would be interesting to better understand the behavior of online
variants, and to characterize the implicit exploration eﬀect for the greedy method.

Limitations of the study. Our study is primarily concerned with prediction perfor-
mance, while real world applications often additionally consider the value of counterfactual
evaluation for oﬄine policy evaluation.

A key limitation of our study is that it is concerned with stationary datasets. Many real-
world contextual bandit applications involve nonstationary datasources. This limitation
is simply due to the nature of readily available public datasets. The lack of public CB
datasets as well as challenges in counterfactual evaluation of CB algorithms make a more
realistic study challenging, but we hope that an emergence of platforms (Agarwal et al.,
2016; Jamieson et al., 2015) to easily deploy CB algorithms will enable studies with real
CB datasets in the future.

19

Bietti, Agarwal and Langford

References

Y. Abbasi-Yadkori, D. P´al, and C. Szepesv´ari. Improved algorithms for linear stochastic

bandits. In Advances in Neural Information Processing Systems (NIPS), 2011.

A. Agarwal, M. Dud´ık, S. Kale, J. Langford, and R. E. Schapire. Contextual bandit learning
In Proceedings of the International Conference on Artiﬁcial

with predictable rewards.
Intelligence and Statistics (AISTATS), 2012.

A. Agarwal, D. Hsu, S. Kale, J. Langford, L. Li, and R. E. Schapire. Taming the monster: A
fast and simple algorithm for contextual bandits. arXiv preprint arXiv:1402.0555, 2014.

A. Agarwal, S. Bird, M. Cozowicz, L. Hoang, J. Langford, S. Lee, J. Li, D. Melamed,
arXiv preprint

G. Oshri, O. Ribas, et al. A multiworld testing decision service.
arXiv:1606.03966, 2016.

A. Agarwal, A. Krishnamurthy, J. Langford, H. Luo, et al. Open problem: First-order
regret bounds for contextual bandits. In Conference on Learning Theory (COLT), 2017.

S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoﬀs.
In Proceedings of the International Conference on Machine Learning (ICML), 2013.

Z. Allen-Zhu, S. Bubeck, and Y. Li. Make the minority great again: First-order regret
bound for contextual bandits. In Proceedings of the International Conference on Machine
Learning (ICML), 2018.

H. Bastani, M. Bayati, and K. Khosravi. Mostly exploration-free algorithms for contextual

bandits. arXiv preprint arXiv:1704.09011, 2017.

A. Blum, A. Kalai, and J. Langford. Beating the hold-out: Bounds for k-fold and progressive

cross-validation. In Conference on Learning Theory (COLT), 1999.

O. Chapelle and L. Li. An empirical evaluation of thompson sampling.

In Advances in

Neural Information Processing Systems (NIPS), 2011.

J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine Learning Research (JMLR), 12(Jul):
2121–2159, 2011.

M. Dudik, D. Hsu, S. Kale, N. Karampatziakis, J. Langford, L. Reyzin, and T. Zhang. Eﬃ-
cient optimal learning for contextual bandits. In Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), 2011a.

M. Dudik, J. Langford, and L. Li. Doubly robust policy evaluation and learning.
Proceedings of the International Conference on Machine Learning (ICML), 2011b.

In

D. Eckles and M. Kaptein. Thompson sampling with the online bootstrap. arXiv preprint

arXiv:1410.4009, 2014.

20

A Contextual Bandit Bake-off

D. J. Foster, A. Agarwal, M. Dud´ık, H. Luo, and R. E. Schapire. Practical contextual ban-
dits with regression oracles. In Proceedings of the International Conference on Machine
Learning (ICML), 2018.

S. Hanneke. Theory of disagreement-based active learning. Foundations and Trends in

Machine Learning, 7(2-3), 2014.

X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers,
et al. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the
Eighth International Workshop on Data Mining for Online Advertising, 2014.

D. J. Hsu. Algorithms for active learning. PhD thesis, UC San Diego, 2010.

T.-K. Huang, A. Agarwal, D. J. Hsu, J. Langford, and R. E. Schapire. Eﬃcient and
In Advances in Neural Information Processing

parsimonious agnostic active learning.
Systems (NIPS), 2015.

K. G. Jamieson, L. Jain, C. Fernandez, N. J. Glattard, and R. Nowak. Next: A system for
real-world development, evaluation, and application of active learning. In Advances in
Neural Information Processing Systems (NIPS), 2015.

S. M. Kakade and A. Tewari. On the generalization ability of online strongly convex pro-
gramming algorithms. In Advances in Neural Information Processing Systems (NIPS),
2009.

S. Kannan, J. Morgenstern, A. Roth, B. Waggoner, and Z. S. Wu. A smoothed analysis
of the greedy algorithm for the linear contextual bandit problem. In Advances in Neural
Information Processing Systems (NIPS), 2018.

N. Karampatziakis and J. Langford. Online importance weight aware updates. In Confer-

ence on Uncertainty in Artiﬁcial Intelligence (UAI), 2011.

A. Krishnamurthy, A. Agarwal, T.-K. Huang, H. Daume III, and J. Langford. Active
learning for cost-sensitive classiﬁcation. Journal of Machine Learning Research (JMLR),
20(65):1–50, 2019.

J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side

information. In Advances in Neural Information Processing Systems (NIPS), 2008.

L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to person-
alized news article recommendation. In Proceedings of the 19th international conference
on World wide web. ACM, 2010.

P. Massart, ´E. N´ed´elec, et al. Risk bounds for statistical learning. The Annals of Statistics,

34(5), 2006.

H. B. McMahan, G. Holt, D. Sculley, M. Young, D. Ebner, J. Grady, L. Nie, T. Phillips,
E. Davydov, D. Golovin, et al. Ad click prediction: a view from the trenches. In Proceed-
ings of the 19th ACM international conference on Knowledge discovery and data mining
(KDD), 2013.

21

Bietti, Agarwal and Langford

I. Osband and B. Van Roy. Bootstrapped thompson sampling and deep exploration. arXiv

preprint arXiv:1507.00300, 2015.

N. C. Oza and S. Russell. Online bagging and boosting. In Proceedings of the International

Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2001.

Z. Qin, V. Petricek, N. Karampatziakis, L. Li, and J. Langford. Eﬃcient online bootstrap-
ping for large scale learning. In Workshop on Parallel and Large-scale Machine Learning
(BigLearning@NIPS), 2013.

S. Ross, P. Mineiro, and J. Langford. Normalized online learning. In Conference on Uncer-

tainty in Artiﬁcial Intelligence (UAI), 2013.

D. J. Russo, B. Van Roy, A. Kazerouni, I. Osband, Z. Wen, et al. A tutorial on thompson

sampling. Foundations and Trends R(cid:13) in Machine Learning, 11(1):1–96, 2018.

T. Schnabel, A. Swaminathan, A. Singh, N. Chandak, and T. Joachims. Recommendations
In Proceedings of the International

as treatments: debiasing learning and evaluation.
Conference on Machine Learning (ICML), 2016.

A. Swaminathan and T. Joachims. The self-normalized estimator for counterfactual learn-

ing. In Advances in Neural Information Processing Systems (NIPS), 2015.

W. R. Thompson. On the likelihood that one unknown probability exceeds another in view

of the evidence of two samples. Biometrika, 25(3/4), 1933.

22

A Contextual Bandit Bake-off

Table 5: Statistically signiﬁcant win-loss diﬀerence with hyperparameters ﬁxed as in Ta-
ble 10, encodings ﬁxed to -1/0, on all held-out datasets or subsets with diﬀerent charac-
teristics: (a) number of actions K; (b) number of features d; (c) number of examples n;
(d) PV loss of the one-against-all (OAA) method. The corresponding table for all held-out
datasets is shown in Table 1(top left).

(cid:15)G
18
25
26
-6
-

(cid:15)G
10
21
17
-7
-

(cid:15)G
53
60
37
18
-

↓ vs → G RO C-nu B-g
22
-5
G
24
-
RO
25
C-nu
-5
-
-24
B-g
6
-25
(cid:15)G

-6
5
-
-25
-26

-
5
6
-22
-18

↓ vs → G RO C-nu B-g
15
1
G
14
-
RO
22
0
C-nu
-
-14
B-g
9
-9
(cid:15)G

-
-1
4
-15
-7

-4
0
-
-22
-13

(a) K ≥ 3 (left, 89 datasets), K ≥ 10 (right, 36 datasets)

↓ vs → G RO C-nu B-g
20
-5
G
26
-
RO
20
-8
C-nu
-
-26
B-g
7
-21
(cid:15)G

-
5
1
-20
-10

-1
8
-
-20
-17

↓ vs → G RO C-nu B-g
-5
G
-
RO
-1
C-nu
-8
B-g
-8
(cid:15)G

-6
1
-
-15
-7

7
8
15
-
6

-
5
6
-7
0

(b) d ≥ 100 (left, 76 datasets), d ≥ 10 000 (right, 41 datasets)

↓ vs → G RO C-nu B-g
38
-4
G
40
-
RO
9
-33
C-nu
-
-40
B-g
-18
-60
(cid:15)G

23
33
-
-9
-37

-
4
-23
-38
-53

↓ vs → G RO C-nu B-g
10
-3
G
14
-
RO
2
-17
C-nu
-
-14
B-g
-25
-36
(cid:15)G

-
3
-16
-10
-34

16
17
-
-2
-30

(c) n ≥ 1 000 (left, 119 datasets), n ≥ 10 000 (right, 43 datasets)

↓ vs → G RO C-nu B-g
40
1
G
36
-
RO
7
-26
C-nu
-
-36
B-g
-4
-43
(cid:15)G

↓ vs → G RO C-nu B-g
8
1
G
5
-
RO
-12
-12
C-nu
-
-5
B-g
-10
-16
(cid:15)G
(d) P VOAA ≤ 0.2 (left, 135 datasets), P VOAA ≤ 0.05 (right, 28 datasets)

(cid:15)G
36
43
24
4
-

-
-1
-25
-40
-36

25
26
-
-7
-24

-
-1
-14
-8
-15

14
12
-
12
-5

(cid:15)G
7
9
13
-9
-

(cid:15)G
0
8
7
-6
-

(cid:15)G
34
36
30
25
-

(cid:15)G
15
16
5
10
-

↓ vs → G RO C-nu B-g
5
2
G
4
-
RO
-1
-8
C-nu
-
-4
B-g
-11
-12
(cid:15)G

8
8
-
1
-12

-
-2
-8
-5
-12

(cid:15)G
12
12
12
11
-

(e) n ≥ 10 000 and P VOAA ≤ 0.1 (13 datasets)

23

Bietti, Agarwal and Langford

Appendix A. Datasets

This section gives some details on the cost-sensitive classiﬁcation datasets considered in our
study.

Multiclass classiﬁcation datasets. We consider 525 multiclass datasets from the openml.
org platform, including among others, medical, gene expression, text, sensory or synthetic
data. Table 6 provides some statistics about these datasets. These also include the 8 clas-
siﬁcation datasets considered in (Foster et al., 2018) from the UCI database. The full list
of datasets is given below.

Table 6: Statistics on number of multiclass datasets by number of examples, actions and
unique features, as well as by progressive validation 0-1 loss for the supervised one-against-
all online classiﬁer, in our collection of 525 multiclass datasets.

actions #
404
73
48

2
3-9
10+

examples #
94
270
133
28

≤ 102
102-103
103-105
> 105

features
≤ 50
51-100
101-1000
1000+

#
392
35
17
81

P VOAA
≤ 0.01
(0.01, 0.1]
(0.1, 0.2]
(0.2, 0.5]
> 0.5

#
10
88
103
273
51

Multilabel classiﬁcation datasets. We consider 5 multilabel datasets from the LibSVM
website6, listed in Table 7.

Table 7: List of multilabel datasets.

Dataset # examples # features # actions P VOAA
0.1664
mediamill
0.0446
rcv1
0.0066
scene
0.1661
tmc
0.2553
yeast

30,993
23,149
1,211
21,519
1,500

120
47,236
294
30,438
103

101
103
6
22
14

Cost-sensitive classiﬁcation datasets. For more general real-valued costs in [0, 1], we
use a modiﬁcation of the multilabel RCV1 dataset introduced in (Krishnamurthy et al.,
2019). Each example consists of a news article labeled with the topics it belongs to, in
a collection of 103 topics. Instead of ﬁxing the cost to 1 for incorrect topics, the cost is
deﬁned as the tree distance to the set of correct topics in a topic hierarchy.

We also include the learning-to-rank datasets considered in (Foster et al., 2018), where
we limit the number of documents (actions) per query, and consider all the training folds.
We convert relevance scores to losses in {0, 0.25, 0.5, 0.75, 1}, with 0 indicating a perfectly
relevant document, and 1 an irrelevant one. The datasets considered are the Microsoft

6. https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html

24

A Contextual Bandit Bake-off

Learning to Rank dataset, variant MSLR-30K at https://www.microsoft.com/en-us/
research/project/mslr/, and the Yahoo! Learning to Rank Challenge V2.0, variant C14B
at https://webscope.sandbox.yahoo.com/catalog.php?datatype=c. Details are shown
in Table 8. We note that for these datasets we consider action-dependent features, with a
ﬁxed parameter vector for all documents.

Table 8: Learning to rank datasets.

Dataset
MSLR-30K
Yahoo

# examples # features max # documents P VOAA
0.7892
0.5876

31,531
36,251

136
415

10
6

List of multiclass datasets. The datasets we used can be accessed at https://www.
openml.org/d/<id>, with id in the following list:

3, 6, 8, 10, 11, 12, 14, 16, 18, 20, 21, 22, 23, 26, 28, 30, 31, 32, 36, 37, 39, 40, 41, 43,
44, 46, 48, 50, 53, 54, 59, 60, 61, 62, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161,
162, 180, 181, 182, 183, 184, 187, 189, 197, 209, 223, 227, 273, 275, 276, 277, 278, 279, 285,
287, 292, 293, 294, 298, 300, 307, 310, 312, 313, 329, 333, 334, 335, 336, 337, 338, 339, 343,
346, 351, 354, 357, 375, 377, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395,
396, 397, 398, 399, 400, 401, 444, 446, 448, 450, 457, 458, 459, 461, 462, 463, 464, 465, 467,
468, 469, 472, 475, 476, 477, 478, 479, 480, 554, 679, 682, 683, 685, 694, 713, 714, 715, 716,
717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735,
736, 737, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756,
758, 759, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777,
778, 779, 780, 782, 783, 784, 785, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 799,
800, 801, 803, 804, 805, 806, 807, 808, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821,
822, 823, 824, 825, 826, 827, 828, 829, 830, 832, 833, 834, 835, 836, 837, 838, 841, 843, 845,
846, 847, 848, 849, 850, 851, 853, 855, 857, 859, 860, 862, 863, 864, 865, 866, 867, 868, 869,
870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 884, 885, 886, 888, 891, 892,
893, 894, 895, 896, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914,
915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 931, 932, 933, 934,
935, 936, 937, 938, 941, 942, 943, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956,
958, 959, 962, 964, 965, 969, 970, 971, 973, 974, 976, 977, 978, 979, 980, 983, 987, 988, 991,
994, 995, 996, 997, 1004, 1005, 1006, 1009, 1011, 1012, 1013, 1014, 1015, 1016, 1019, 1020,
1021, 1022, 1025, 1026, 1036, 1038, 1040, 1041, 1043, 1044, 1045, 1046, 1048, 1049, 1050,
1054, 1055, 1056, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1071,
1073, 1075, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1100,
1104, 1106, 1107, 1110, 1113, 1115, 1116, 1117, 1120, 1121, 1122, 1123, 1124, 1125, 1126,
1127, 1128, 1129, 1130, 1131, 1132, 1133, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142,
1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157,
1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1169, 1216, 1217, 1218, 1233, 1235,
1236, 1237, 1238, 1241, 1242, 1412, 1413, 1441, 1442, 1443, 1444, 1449, 1451, 1453, 1454,
1455, 1457, 1459, 1460, 1464, 1467, 1470, 1471, 1472, 1473, 1475, 1481, 1482, 1483, 1486,
1487, 1488, 1489, 1496, 1498, 1590.

25

Bietti, Agarwal and Langford

Table 9: Choices of hyperparameters and reduction for each method. Fixed choices of hy-
perparameters for -1/0 encodings are in bold. These were obtained for each method with
an instant-runoﬀ voting mechanism on 200 of the multiclass datasets with -1/0 encoding,
where each dataset ranks hyperparameter choices according to the diﬀerence between sig-
niﬁcant wins and losses against all other choices (the vote of each dataset is divided by the
number of tied choices ranked ﬁrst). Table 10 shows optimized choices of hyperparameters
for diﬀerent encoding settings used in our study.

Name
G

Method
Greedy

R/RO RegCB-elim/RegCB-opt

C-nu

Cover-NU

C-u

B/B-g
(cid:15)G

Cover

Bag/Bag-greedy
(cid:15)-greedy

A

active (cid:15)-greedy

Hyperparameters
-
C0 ∈ 10−{1,2,3}
N ∈ {4, 8, 16}
ψ ∈ {0.01, 0.1, 1}
N ∈ {4, 8, 16}
ψ ∈ {0.01, 0.1, 1}
N ∈ {4, 8, 16}
(cid:15) ∈ {0.02, 0.05, 0.1}
(cid:15) ∈ {0.02, 1}
C0 ∈ 10−{2,4,6}

Reduction
IWR
-

IPS/DR

IPS/DR

IPS/DR/IWR
IPS/DR/IWR

IPS/DR/IWR

Appendix B. Evaluation Details

B.1 Algorithms and Hyperparameters

We ran each method on every dataset with the following hyperparameters:

• algorithm-speciﬁc hyperparameters, shown in Table 9.

• 9 choices of learning rates, on a logarithmic grid from 0.001 to 10 (see Section 2.4).

• 3 choices of reductions: IPS, DR and IWR (see Section 2.3). Note that these mainly
apply to methods that reduce to oﬀ-policy optimization (i.e., ((cid:15)-)greedy and bagging),
and to some extent, methods that reduce to cost-sensitive classiﬁcation (i.e., cover
and active (cid:15)-greedy, though the IWR reduction is heuristic in this case). Both RegCB
variants directly reduce to regression.

• 3 choices of loss encodings: 0/1, -1/0 and 9/10 (see Eq. (7)). 0/1 and -1/0 encodings
are typically a design choice, while the experiments with 9/10 are aimed at assessing
some robustness to loss range.

B.2 Additional Evaluation Results

This sections provides additional experimental results, and more detailed win/loss statistics
for tables in the main paper, showing both signiﬁcant wins and signiﬁcant losses, rather
than just their diﬀerence.

26

A Contextual Bandit Bake-off

Table 10: Optimized choices of hyperparameters for diﬀerent encoding settings, obtained
using the voting mechanism described in Table 9: -1/0 (same as bold choices in Table 9, used
in Tables 1(top left), 2ce, 5, 11a, 12a, 13a and in the ﬁgures); 0/1 (used in Tables 1(bottom
left), 2abd, 11b, 12b, 13b, 14).

Algorithm
G
R/RO
C-nu
C-u
B
B-g
(cid:15)G
A

-1/0
-
C0 = 10−3
N = 4, ψ = 0.1, DR
N = 4, ψ = 0.1, IPS
N = 4, IWR
N = 4, IWR
(cid:15) = 0.02, IWR

0/1
-
C0 = 10−3
N = 4, ψ = 0.01, DR
N = 4, ψ = 0.1, DR
N = 16, IWR
N = 8, IWR
(cid:15) = 0.02, IWR

(cid:15) = 0.02, C0 = 10−6, IWR (cid:15) = 0.02, C0 = 10−6, IWR

Table 11: Statistically signiﬁcant wins / losses of all methods on the 324 held-out multiclass
classiﬁcation datasets. Hyperparameters are ﬁxed as given in Table 10.

↓ vs →
G
R
RO
C-nu
B
B-g
(cid:15)G
C-u
A

↓ vs →
G
R
RO
C-nu
B
B-g
(cid:15)G
C-u
A

G
-
25 / 22
16 / 8
33 / 41
15 / 77
16 / 65
6 / 59
10 / 159
10 / 33

G
-
66 / 27
69 / 5
51 / 36
38 / 75
39 / 72
21 / 69
30 / 151
19 / 36

R
22 / 25
-
23 / 16
31 / 49
19 / 74
20 / 61
12 / 70
11 / 159
15 / 48

R
27 / 66
-
38 / 15
24 / 42
12 / 87
16 / 83
19 / 108
4 / 172
22 / 77

RO
8 / 16
16 / 23
-
21 / 47
13 / 76
13 / 60
4 / 71
6 / 166
7 / 46

RO
5 / 69
15 / 38
-
13 / 59
7 / 109
9 / 105
3 / 121
3 / 175
4 / 89

C-nu
41 / 33
49 / 31
47 / 21
-
26 / 66
31 / 51
19 / 75
10 / 159
29 / 50

B
77 / 15
74 / 19
76 / 13
66 / 26
-
32 / 11
41 / 46
16 / 126
54 / 26
(a) -1/0 encoding

C-nu
36 / 51
42 / 24
59 / 13
-
27 / 83
29 / 72
25 / 96
6 / 170
31 / 63

B
75 / 38
87 / 12
109 / 7
83 / 27
-
34 / 21
49 / 59
14 / 131
67 / 37
(b) 0/1 encoding

B-g
65 / 16
61 / 20
60 / 13
51 / 31
11 / 32
-
31 / 49
11 / 130
42 / 27

(cid:15)G
59 / 6
70 / 12
71 / 4
75 / 19
46 / 41
49 / 31
-
14 / 125
37 / 2

C-u
159 / 10
159 / 11
166 / 6
159 / 10
126 / 16
130 / 11
125 / 14
-
152 / 9

B-g
72 / 39
83 / 16
105 / 9
72 / 29
21 / 34
-
46 / 65
18 / 129
54 / 42

(cid:15)G
69 / 21
108 / 19
121 / 3
96 / 25
59 / 49
65 / 46
-
28 / 122
43 / 3

C-u
151 / 30
172 / 4
175 / 3
170 / 6
131 / 14
129 / 18
122 / 28
-
151 / 22

A
33 / 10
48 / 15
46 / 7
50 / 29
26 / 54
27 / 42
2 / 37
9 / 152
-

A
36 / 19
77 / 22
89 / 4
63 / 31
37 / 67
42 / 54
3 / 43
22 / 151
-

Extended tables. Tables 11 and 12 are extended versions of Table 1, showing both sig-
niﬁcant wins and loss, more methods, and separate statistics for multiclass and multilabel
datasets. In particular, we can see that both variants of RegCB become even more com-
petitive against all other methods when using 0/1 encodings. Table 14 extends Table 2(a)
with additional methods. Table 15 is a more detailed win/loss version of Table 3, and
additionally shows statistics for 0/1 encodings.

We also show separate statistics in Table 13 for the 8 datasets from the UCI repository
considered in (Foster et al., 2018), which highlight that Greedy can outperform RegCB on
some of these datasets, and that the optimistic variant of RegCB is often superior to the
elimination variant. We note that our experimental setup is quite diﬀerent from Foster et al.
(2018), who consider batch learning on an doubling epoch schedule, which might explain
some of the diﬀerences in the results.

27

Bietti, Agarwal and Langford

Table 12: Statistically signiﬁcant wins / losses of all methods on the 5 multilabel classiﬁca-
tion datasets. Hyperparameters are ﬁxed as given in Table 10.

↓ vs → G
-
G
2 / 2
R
0 / 1
RO
1 / 3
C-nu
2 / 2
B
2 / 3
B-g
1 / 2
(cid:15)G
1 / 4
C-u
1 / 2
A

↓ vs → G
-
G
1 / 3
R
2 / 2
RO
3 / 1
C-nu
0 / 4
B
1 / 4
B-g
0 / 4
(cid:15)G
0 / 5
C-u
0 / 3
A

R
2 / 2
-
2 / 2
0 / 2
0 / 3
0 / 3
2 / 3
0 / 5
2 / 3

R
3 / 1
-
3 / 0
4 / 1
2 / 2
3 / 2
2 / 2
1 / 4
2 / 2

RO C-nu
3 / 1
1 / 0
2 / 0
2 / 2
2 / 2
-
-
2 / 2
1 / 3
2 / 2
1 / 3
1 / 3
2 / 3
1 / 2
0 / 5
1 / 4
2 / 3
1 / 2

B
2 / 2
3 / 0
2 / 2
3 / 1
-
1 / 1
2 / 3
0 / 5
2 / 3
(a) -1/0 encoding

RO C-nu
1 / 3
2 / 2
1 / 4
0 / 3
1 / 2
-
-
2 / 1
1 / 4
0 / 5
1 / 3
0 / 4
0 / 4
1 / 3
1 / 4
0 / 5
1 / 4
1 / 3

B
4 / 0
2 / 2
5 / 0
4 / 1
-
2 / 0
2 / 1
1 / 2
3 / 1
(b) 0/1 encoding

B-g
3 / 2
3 / 0
3 / 1
3 / 1
1 / 1
-
3 / 2
1 / 4
3 / 2

B-g
4 / 1
2 / 3
4 / 0
3 / 1
0 / 2
-
2 / 2
1 / 4
3 / 1

(cid:15)G
2 / 1
3 / 2
2 / 1
3 / 2
3 / 2
2 / 3
-
1 / 4
0 / 1

(cid:15)G
4 / 0
2 / 2
3 / 1
4 / 0
1 / 2
2 / 2
-
1 / 4
1 / 0

C-u
4 / 1
5 / 0
4 / 1
5 / 0
5 / 0
4 / 1
4 / 1
-
4 / 1

C-u
5 / 0
4 / 1
5 / 0
4 / 1
2 / 1
4 / 1
4 / 1
-
5 / 0

A
2 / 1
3 / 2
2 / 1
3 / 2
3 / 2
2 / 3
1 / 0
1 / 4
-

A
3 / 0
2 / 2
3 / 1
4 / 1
1 / 3
1 / 3
0 / 1
0 / 5
-

Varying C0 in RegCB-opt and active (cid:15)-greedy. Figure 4 shows a comparison be-
tween RegCB-opt and Greedy or Cover-NU on our corpus, for diﬀerent values of C0, which
controls the level of exploration through the width of conﬁdence bounds. Figure 5 shows the
improvements that the active (cid:15)-greedy algorithm can achieve compared to (cid:15)-greedy, under
diﬀerent settings.

Counterfactual evaluation. Figure 6 extends Figure 3 to include all algorithms, and
additionally shows results of using IPS estimates directly on the losses (cid:96)t(at) instead of
rewards 1 − (cid:96)t(at), which tend to be signiﬁcantly worse.

B.3 Shared baseline parameterization

We also experimented with the use of an action-independent additive baseline term in our
loss estimators, which can help learn better estimates with fewer samples in some situations.
In this case the regressors take the form f (x, a) = θ0+θ(cid:62)
a x (DR).
In order to learn the baseline term more quickly, we propose to use a separate online update
for the parameters θ0 or φ0 to regress on observed losses, followed by an online update on
the residual for the action-dependent part. We scale the step-size of these baseline updates
by the largest observed magnitude of the loss, in order to adapt to the observed loss range
for normalized updates (Ross et al., 2013).

a x (IWR) or ˆ(cid:96)(x, a) = φ0+φ(cid:62)

28

A Contextual Bandit Bake-off

Table 13: Statistically signiﬁcant wins / losses of all methods on the 8 classiﬁcation datasets
from the UCI repository considered in (Foster et al., 2018). Hyperparameters are ﬁxed as
given in Table 10.

↓ vs → G
-
G
0 / 4
R
1 / 2
RO
2 / 5
C-nu
0 / 5
B
1 / 4
B-g
0 / 6
(cid:15)G
0 / 6
C-u
0 / 5
A

↓ vs → G
-
G
5 / 1
R
5 / 0
RO
2 / 2
C-nu
2 / 3
B
1 / 3
B-g
1 / 3
(cid:15)G
0 / 7
C-u
1 / 2
A

R
4 / 0
-
1 / 0
1 / 3
1 / 4
1 / 3
2 / 4
0 / 6
2 / 2

R
1 / 5
-
1 / 0
0 / 5
0 / 6
0 / 6
0 / 6
0 / 8
0 / 5

RO C-nu
5 / 2
2 / 1
3 / 1
0 / 1
4 / 1
-
-
1 / 4
1 / 3
0 / 5
2 / 2
0 / 5
2 / 3
0 / 5
0 / 7
0 / 6
3 / 2
0 / 3

B
5 / 0
4 / 1
5 / 0
3 / 1
-
2 / 0
2 / 3
0 / 6
2 / 2
(a) -1/0 encoding

RO C-nu
2 / 2
0 / 5
5 / 0
0 / 1
5 / 0
-
-
0 / 5
1 / 2
0 / 6
0 / 3
0 / 6
1 / 2
0 / 7
0 / 7
0 / 7
1 / 1
0 / 6

B
3 / 2
6 / 0
6 / 0
2 / 1
-
0 / 3
3 / 1
0 / 7
3 / 0
(b) 0/1 encoding

B-g
4 / 1
3 / 1
5 / 0
2 / 2
0 / 2
-
1 / 4
0 / 6
2 / 2

B-g
3 / 1
6 / 0
6 / 0
3 / 0
3 / 0
-
3 / 1
0 / 7
3 / 0

(cid:15)G
6 / 0
4 / 2
5 / 0
3 / 2
3 / 2
4 / 1
-
0 / 6
2 / 0

(cid:15)G
3 / 1
6 / 0
7 / 0
2 / 1
1 / 3
1 / 3
-
0 / 7
1 / 0

C-u
6 / 0
6 / 0
6 / 0
7 / 0
6 / 0
6 / 0
6 / 0
-
6 / 0

C-u
7 / 0
8 / 0
7 / 0
7 / 0
7 / 0
7 / 0
7 / 0
-
7 / 0

A
5 / 0
2 / 2
3 / 0
2 / 3
2 / 2
2 / 2
0 / 2
0 / 6
-

A
2 / 1
5 / 0
6 / 0
1 / 1
0 / 3
0 / 3
0 / 1
0 / 7
-

Table 14: Progressive validation loss for RCV1 with real-valued costs. Same as Table 2(a),
but with all methods. Hyperparameters are ﬁxed as given in Table 10. The learning rate
is optimized once on the original dataset, and we show mean and standard error based on
10 diﬀerent random reshuﬄings of the dataset.

G
0.215 ± 0.010

R
0.408 ± 0.003

RO
0.225 ± 0.008

C-nu
0.215 ± 0.006

C-u
0.570 ± 0.023

B
0.256 ± 0.006

B-g
0.251 ± 0.005

(cid:15)G
0.230 ± 0.009

A
0.230 ± 0.010

Such an additive baseline can be helpful to quickly adapt to a constant loss estimate
thanks to the separate online update. This appears particularly useful with the -1/0 en-
coding, for which the initialization at 0 may give pessimistic loss estimates which can be
damaging in particular for the greedy method, that often gets some initial exploration from
an optimistic cost encoding. This can be seen in Figure 7(top). Table 16 shows that op-
timizing over the use of baseline on each dataset can improve the performance of Greedy
and RegCB-opt when compared to other methods such as Cover-NU.

In an online learning setting, baseline can also help to quickly reach an unknown target
range of loss estimates. This is demonstrated in Figure 7(bottom), where the addition

29

Bietti, Agarwal and Langford

Table 15: Impact of reductions for Bag (left) and (cid:15)-greedy (right), with hyperparameters
optimized and encoding ﬁxed to -1/0 or 0/1. Extended version of Table 3. Each (row,
column) entry shows the statistically signiﬁcant wins and losses of row against column.

↓ vs →
ips
dr
iwr

ips
-
72 / 30
84 / 25

dr
30 / 72
-
58 / 30

iwr
25 / 84
30 / 58
-

↓ vs →
ips
dr
iwr

ips
-
22 / 85
149 / 16

dr
85 / 22
-
164 / 9

iwr
16 / 149
9 / 164
-

(a) -1/0 encoding

↓ vs →
ips
dr
iwr

ips
-
240 / 46
245 / 17

dr
46 / 240
-
97 / 33

iwr
17 / 245
33 / 97
-

↓ vs →
ips
dr
iwr

ips
-
136 / 40
182 / 36

dr
40 / 136
-
148 / 23

iwr
36 / 182
23 / 148
-

(b) 0/1 encoding

Figure 4: Comparison of RegCB-opt with Greedy (top) and Cover-NU (bottom) for diﬀerent
values of C0. Hyperparameters for Greedy and Cover-NU ﬁxed as in Table 9. Encoding
ﬁxed to -1/0. The plots consider normalized loss on held-out datasets, with red points
indicating signiﬁcant wins.

of baseline is shown to help various methods with 9/10 encodings on a large number of
datasets. We do not evaluate RegCB for 9/10 encodings as it needs a priori known upper
and lower bounds on costs.

30

Table 16: Statistically signiﬁcant wins / losses of all methods on held-out datasets, with -1/0
encoding and ﬁxed hyperparameters, except for baseline, which is optimized on each dataset
together with the learning rate. The ﬁxed hyperparameters are shown in the table below,
and were selected with the same voting approach described in Table 9. This optimization
beneﬁts Greedy and RegCB-opt in particular.

↓ vs →
G
R
RO
C-nu
B
B-g
(cid:15)G
C-u
A

G
-
14 / 32
16 / 14
15 / 60
13 / 88
16 / 69
1 / 78
1 / 178
6 / 49

R
32 / 14
-
36 / 9
25 / 43
19 / 69
24 / 52
14 / 66
9 / 167
19 / 42

RO
14 / 16
9 / 36
-
13 / 64
10 / 90
10 / 66
3 / 84
1 / 187
5 / 61

C-nu
60 / 15
43 / 25
64 / 13
-
30 / 58
41 / 34
24 / 60
6 / 163
36 / 42

B
88 / 13
69 / 19
90 / 10
58 / 30
-
33 / 10
35 / 54
7 / 133
59 / 29

B-g
69 / 16
52 / 24
66 / 10
34 / 41
10 / 33
-
18 / 57
2 / 147
38 / 37

(cid:15)G
78 / 1
66 / 14
84 / 3
60 / 24
54 / 35
57 / 18
-
10 / 129
44 / 3

C-u
178 / 1
167 / 9
187 / 1
163 / 6
133 / 7
147 / 2
129 / 10
-
164 / 5

A
49 / 6
42 / 19
61 / 5
42 / 36
29 / 59
37 / 38
3 / 44
5 / 164
-

A Contextual Bandit Bake-off

Algorithm
G
R/RO
C-nu
C-u
B
B-g
(cid:15)G
A

Hyperparameters
-
C0 = 10−3
N = 16, ψ = 0.1, DR
N = 4, ψ = 0.1, IPS
N = 4, IWR
N = 4, IWR
(cid:15) = 0.02, IWR
(cid:15) = 0.02, C0 = 10−6, IWR

31

Bietti, Agarwal and Langford

(a) DR

(b) IWR

32

Figure 5: Improvements to (cid:15)-greedy from our active learning strategy. Encoding ﬁxed to
-1/0. The IWR implementation described in Section C.1 still manages to often outperform
(cid:15)-greedy, despite only providing an approximation to Algorithm 6.

A Contextual Bandit Bake-off

(a) all multiclass datasets

(b) n ≥ 10 000 only

(c) all multiclass datasets

(d) n ≥ 10 000 only

Figure 6: Errors of IPS counterfactual estimates for the uniform random policy using ex-
ploration logs collected by various algorithms on multiclass datasets (extended version of
Figure 3). The boxes show quartiles (with the median shown as a blue line) of the dis-
tribution of squared errors across all multiclass datasets or only those with at least 10 000
examples. The logs are obtained by running each algorithm with -1/0 encodings, ﬁxed
hyperparameters from Table 9, and the best learning rate on each dataset according to pro-
gressive validation loss. The top plots consider IPS with reward estimates (as in Figure 3),
while the bottom plots consider IPS on the loss.

33

Bietti, Agarwal and Langford

Figure 7: (top) Impact of baseline on diﬀerent algorithms with encoding ﬁxed to -1/0; for
Greedy and RegCB-opt, it can signiﬁcantly help against pessimistic initial costs in some
datasets. Hyperparameters ﬁxed as in Table 9. (bottom) Baseline improves robustness to
the range of losses. The plots consider normalized loss on held-out datasets, with red points
indicating signiﬁcant wins.

34

A Contextual Bandit Bake-off

Algorithm 6 Active (cid:15)-greedy
π1; (cid:15); C0 > 0.
explore(xt):

At = {a : loss_diff(πt, xt, a) ≤ ∆t,C0};
1{a ∈ At} + (1 − (cid:15)|At|
pt(a) = (cid:15)
K
return pt;

K ) 1{πt(xt) = a};

learn(xt, at, (cid:96)t(at), pt):

ˆ(cid:96)t = estimator(xt, at, (cid:96)t(at), pt(at));

ˆct(a) =

(cid:40)ˆ(cid:96)t(a),
1,

if pt(a) > 0
otherwise.

πt+1 = csc_oracle(πt, xt, ˆct);

Appendix C. Active (cid:15)-greedy: Practical Algorithm and Analysis

This section presents our active (cid:15)-greedy method, a variant of (cid:15)-greedy that reduces the
amount of uniform exploration using techniques from active learning. Section C.1 introduces
the practical algorithm,7 while Section C.2 provides a theoretical analysis of the method,
showing that it achieves a regret of O(T 1/3) under speciﬁc favorable settings, compared
to O(T 2/3) for vanilla (cid:15)-greedy.

C.1 Algorithm

The simplicity of the (cid:15)-greedy method described in Section 3.1 often makes it the method
of choice for practitioners. However, the uniform exploration over randomly selected actions
can be quite ineﬃcient and costly in practice. A natural consideration is to restrict this
randomization over actions which could plausibly be selected by the optimal policy π∗ =
arg minπ∈Π L(π), where L(π) = E(x,(cid:96))∼D[(cid:96)(π(x))] is the expected loss of a policy π.

To achieve this, we use techniques from disagreement-based active learning (Hanneke,
2014; Hsu, 2010). After observing a context xt, for any action a, if we can ﬁnd a policy π
that would choose this action (π(xt) = a) instead of the empirically best action πt(xt),
while achieving a small loss on past data, then there is disagreement about how good
such an action is, and we allow exploring it. Otherwise, we are conﬁdent that the best
policy would not choose this action, thus we avoid exploring it, and assign it a high cost.
The resulting method is in Algorithm 6. Like RegCB, the method requires a known loss
range [cmin, cmax], and assigns a loss cmax to such unexplored actions (we consider the
range [0, 1] in Algorithm 6 for simplicity). The disagreement test we use is based on empirical
loss diﬀerences, similar to the Oracular CAL active learning method (Hsu, 2010), denoted
loss_diff, together with a threshold:

7. Our implementation is available in the following branch of Vowpal Wabbit: https://github.com/

albietz/vowpal_wabbit/tree/bakeoff.

(cid:114)

∆t,C0 =

C0

K log t
(cid:15)t

+ C0

K log t
(cid:15)t

.

35

Bietti, Agarwal and Langford

A practical implementation of loss_diff for an online setting is given below. We analyze a
theoretical form of this algorithm in Section C.2, showing a formal version of the following
theorem:

Theorem 1 With high-probability, and under favorable conditions on disagreement and on
the problem noise, active (cid:15)-greedy achieves expected regret O(T 1/3).

Note that this data-dependent guarantee improves on worst-case guarantees achieved by
the optimal algorithms Agarwal et al. (2014); Dudik et al. (2011a). In the extreme case
where the loss of any suboptimal policy is bounded away from that of π∗, we show that
our algorithm can achieve constant regret. While active learning algorithms suggest that
data-dependent thresholds ∆t can yield better guarantees (e.g., Huang et al., 2015), this
may require more work in our setting due to open problems related to data-dependent
guarantees for contextual bandits (Agarwal et al., 2017). In a worst-case scenario, active
(cid:15)-greedy behaves similarly to (cid:15)-greedy (Langford and Zhang, 2008), achieving an O(T 2/3)
expected regret with high probability.

Practical implementation of the disagreement test. We now present a practical
way to implement the disagreement tests in the active (cid:15)-greedy method, in the context of
online cost-sensitive classiﬁcation oracles based on regression, as in Vowpal Wabbit. This
corresponds to the loss_diff method in Algorithm 6.

Let ˆLt−1(π) denote the empirical loss of policy π on the (biased) sample of cost-sensitive
examples collected up to time t − 1 (see Section C.2 for details). After observing a context
xt, we want to estimate

loss_diff(πt, xt, ¯a) ≈ ˆLt−1(πt,¯a) − ˆLt−1(πt),

for any action ¯a, where

πt = arg min

ˆLt−1(π)

π

πt,¯a = arg min

π:π(xt)=¯a

ˆLt−1(π).

In our online setup, we take πt to be the current online policy (as in Algorithm 6), and
we estimate the loss diﬀerence by looking at how many online CSC examples of the form
¯c := (1{a (cid:54)= ¯a})a=1..K are needed (or the importance weight on such an example) in order
to switch prediction from πt(xt) to ¯a. If we denote this importance weight by τ¯a, then we
can estimate ˆLt−1(πt,¯a) − ˆLt−1(πt) ≈ τ¯a/t.

Computing τ¯a for IPS/DR.
In the case of IPS/DR, we use an online CSC oracle,
which is based on K regressors f (x, a) in VW, each predicting the cost for an action a.
Let ft be the current regressors for policy πt, yt(a) := ft(xt, a), and denote by st(a) the
sensitivity of regressor ft(·, a) on example (xt, ¯c(a)). This sensitivity is essentially deﬁned
to be the derivative with respect to an importance weight w of the prediction y(cid:48)(a) obtained
from the regressor after an online update (xt, ¯c(a)) with importance weight w. A similar
quantity has been used, e.g., by Huang et al. (2015); Karampatziakis and Langford (2011);
Krishnamurthy et al. (2019). Then, the predictions on actions ¯a and a cross when the

36

A Contextual Bandit Bake-off

importance weight w satisﬁes yt(¯a) − st(¯a)w = yt(a) + st(a)w. Thus, the importance weight
required for action ¯a to be preferred (i.e., smaller predicted loss) to action a is given by:

wa

¯a =

yt(¯a) − yt(a)
st(¯a) + st(a)

.

Action ¯a will thus be preferred to all other actions when using an importance weight τ¯a =
maxa wa
¯a.

Computing τ¯a for IWR. Although Algorithm 6 and the theoretical analysis require
CSC in order to assign a loss of 1 to unexplored actions, and hence does not directly
support IWR, we can consider an approximation which leverages the beneﬁts of IWR by
performing standard IWR updates as in (cid:15)-greedy, while exploring only on actions that pass
a similar disagreement test. In this case, we estimate τ¯a as the importance weight on an
online regression example (xt, 0) for the regressor ft(·, ¯a), needed to switch prediction to
¯a. If st(¯a) is the sensitivity for such an example, we have τ¯a = (yt(¯a) − y∗
t )/st(¯a), where
y∗
t = mina yt(a).

C.2 Theoretical Analysis

This section presents a theoretical analysis of the active (cid:15)-greedy method introduced in
Section C.1. We begin by presenting the analyzed version of the algorithm together with
deﬁnitions in Section C.2.1. Section C.2.2 then studies the correctness of the method,
showing that with high probability, the actions chosen by the optimal policy are always
explored, and that policies considered by the algorithm are always as good as those obtained
under standard (cid:15)-greedy exploration. This section also introduces a Massart-type low-noise
condition similar to the one considered by Krishnamurthy et al. (2019) for cost-sensitive
classiﬁcation. Finally, Section C.2.3 provides a regret analysis of the algorithm, both in the
worst case and under disagreement conditions together with the Massart noise condition.
In particular, a formal version of Theorem 1 is given by Theorem 8, and a more extreme
but informative situation is considered in Proposition 9, where our algorithm can achieve
constant regret.

C.2.1 Algorithm and definitions

We consider a version of the active (cid:15)-greedy strategy that is more suitable for theoretical
analysis, given in Algorithm 7. This method considers exact CSC oracles, as well as a CSC
oracle with one constraint on the policy (π(xt) = a in Eq.(10)). The threshold ∆t is deﬁned
later in Section C.2.2. Computing it would require some knowledge about the size of the
policy class, which we avoid by introducing a parameter C0 in the practical variant. The
disagreement strategy is based on the Oracular CAL active learning method of Hsu (2010),
which tests for disagreement using empirical error diﬀerences, and considers biased samples
when no label is queried. Here, similar tests are used to decide which actions should be
explored, in the diﬀerent context of cost-sensitive classiﬁcation, and the unexplored actions
are assigned a loss of 1, making the empirical sample biased ( ˆZT in Algorithm 7).
Deﬁnitions. Deﬁne ZT = {(xt, (cid:96)t)}t=1..T ⊂ X × RK, ˜ZT = {(xt, ˜(cid:96)t)}t=1..T (biased sam-
ple) and ˆZT = {(xt, ˆ(cid:96)t)}t=1..T (IPS estimate of biased sample), where (cid:96)t ∈ [0, 1]K is the

37

Bietti, Agarwal and Langford

Algorithm 7 active (cid:15)-greedy: analyzed version

Input: exploration probability (cid:15).
Initialize: ˆZ0 := ∅.
for t = 1, . . . do

Observe context xt. Let

πt := arg min

L(π, ˆZt−1)

π

πt,a := arg min

L(π, ˆZt−1)

π:π(xt)=a

At := {a : L(πt,a, ˆZt−1) − L(πt, ˆZt−1) ≤ ∆t}

(10)

(11)

(8)

(9)

Let

pt(a) =






1 − (|At| − 1)(cid:15)/K,
(cid:15)/K,
0,

if a = πt(xt)
if a ∈ At \ {πt(xt)}
otherwise.

Play action at ∼ pt, observe (cid:96)t(at) and set ˆZt = ˆZt−1 ∪ {(xt, ˆ(cid:96)t)}, where ˆ(cid:96)t is deﬁned
in (9).
end for

(unobserved) loss vector at time t and

˜(cid:96)t(a) =

(cid:40)

(cid:96)t(a),
1,

if a ∈ At
o/w

ˆ(cid:96)t(a) =

(cid:40) 1{a=at}

pt(at) (cid:96)t(at),
1,

if a ∈ At
o/w.

L(π, Z) =

(cid:88)

c(π(x)).

1
|Z|

(x,c)∈Z

For any set Z ⊂ X × RK deﬁned as above, we denote, for π ∈ Π,

We then deﬁne the empirical losses LT (π) := L(π, ZT ), ˆLT (π) := L(π, ˆZT ) and ˜LT (π) :=
L(π, ˜ZT ). Let L(π) := E(x,(cid:96))∼D[(cid:96)(π(x))] be the expected loss of policy π, and π∗ :=
arg minπ∈Π L(π). We also deﬁne ρ(π, π(cid:48)) := Px(π(x) (cid:54)= π(cid:48)(x)), the expected disagreement
between policies π and π(cid:48), where Px denotes the marginal distribution of D on contexts.

C.2.2 Correctness

We begin by stating a lemma that controls deviations of empirical loss diﬀerences, which
relies on Freedman’s inequality for martingales (see, e.g., Kakade and Tewari, 2009, Lemma
3).

38

A Contextual Bandit Bake-off

Lemma 2 (Deviation bounds) With probability 1 − δ, the following event holds: for all
π ∈ Π, for all T ≥ 1,

|( ˆLT (π) − ˆLT (π∗)) − ( ˜LT (π) − ˜LT (π∗))| ≤

(cid:114)

2Kρ(π, π∗)eT
(cid:15)

+

(cid:18) K
(cid:15)

(cid:19)

+ 1

eT

|(LT (π) − LT (π∗)) − (L(π) − L(π∗))| ≤ (cid:112)ρ(π, π∗)eT + 2eT ,

(12)

(13)

where eT = log(2|Π|/δT )/T and δT = δ/(T 2 + T ). We denote this event by E in what
follows.

Proof We prove the result using Freedman’s inequality (see, e.g., Kakade and Tewari,
2009, Lemma 3), which controls deviations of a sum using the conditional variance of each
term in the sum and an almost sure bound on their magnitude, along with a union bound.

For (12), let ( ˆLT (π) − ˆLT (π∗)) − ( ˜LT (π) − ˜LT (π∗)) = 1
T

(cid:80)T

t=1 Rt, with

Rt = ˆ(cid:96)t(π(xt)) − ˆ(cid:96)t(π∗(xt)) − (˜(cid:96)t(π(xt)) − ˜(cid:96)t(π∗(xt))).

We deﬁne the σ-ﬁelds Ft := σ({xi, (cid:96)i, ai}t

i=1). Note that Rt is Ft-measurable and

E[ˆ(cid:96)t(π(xt)) − ˆ(cid:96)t(π∗(xt))|xt, (cid:96)t] = ˜(cid:96)t(π(xt)) − ˜(cid:96)t(π∗(xt)),

so that E[Rt|Ft−1] = E[E[Rt|xt, (cid:96)t]|Ft−1] = 0. Thus, (Rt)t≥1 is a martingale diﬀerence
sequence adapted to the ﬁltration (Ft)t≥1. We have

|Rt| ≤ |ˆ(cid:96)t(π(xt)) − ˆ(cid:96)t(π∗(xt))| + |˜(cid:96)t(π(xt)) − ˜(cid:96)t(π∗(xt))| ≤

+ 1.

K
(cid:15)

Note that E[ˆ(cid:96)t(π(xt)) − ˆ(cid:96)t(π∗(xt))|xt, (cid:96)t] = ˜(cid:96)t(π(xt)) − ˜(cid:96)t(π∗(xt)), so that

E[R2

t |Ft−1] = E[E[R2

t |xt, (cid:96)t, At]|Ft−1]

≤ E[E[(ˆ(cid:96)t(π(xt)) − ˆ(cid:96)t(π∗(xt)))2|xt, (cid:96)t, At]|Ft−1]

(cid:20)

(cid:20)

≤ E

E

≤ E

E

(cid:20) (1{π(xt) = at} − 1{π∗(xt) = at})2
pt(at)2

|Ft−1
(cid:20) 1{π(xt) (cid:54)= π∗(xt)}(1{π(xt) = at} + 1{π∗(xt) = at})
pt(at)2
(cid:21)

|xt, (cid:96)t, At

(cid:21)

(cid:21)

= E

(cid:20) 2K 1{π(xt) (cid:54)= π∗(xt)}
(cid:15)

|Ft−1

=

ρ(π, π∗).

2K
(cid:15)

(cid:21)

(cid:21)

|xt, (cid:96)t, At

|Ft−1

Freedman’s inequality then states that (12) holds with probability 1 − δT /2|Π|.

For (13), we consider a similar setup with

Rt = (cid:96)t(π(xt)) − (cid:96)t(π∗(xt)) − (L(π) − L(π∗)).

We have E[Rt|Ft−1] = 0, |Rt| ≤ 2 and E[R2
t |Ft−1] ≤ ρ(π, π∗), which yields that (13) holds
with probability 1 − δT /2|Π| using Freedman’s inequality. A union bound on π ∈ Π and
T ≥ 1 gives the desired result.

39

Bietti, Agarwal and Langford

Threshold. We deﬁne the threshold ∆T used in (11) in Algorithm 7 as:

∆T :=

+ 1

eT −1 +

+ 3

eT −1.

(14)

(cid:32)(cid:114)

2K
(cid:15)

(cid:33)

√

(cid:19)

(cid:18) K
(cid:15)

We also deﬁne the following more precise deviation quantity for a given policy, which follows
directly from the deviation bounds in Lemma 2

∆∗

T (π) :=

(cid:32)(cid:114)

(cid:33)

2K
(cid:15)

+ 1

(cid:112)ρ(π, π∗)eT −1 +

(cid:18) K
(cid:15)

(cid:19)

+ 3

eT −1.

(15)

Note that we have ∆∗

T (π) ≤ ∆T for any policy π.

The next lemma shows that the bias introduced in the empirical sample by assigning
a loss of 1 to unexplored actions is favorable, in the sense that it will not hurt us in
identifying π∗.

Lemma 3 (Favorable bias) Assume π∗(xt) ∈ At for all t ≤ T . We have

˜LT (π) − ˜LT (π∗) ≥ LT (π) − LT (π∗).

(16)

Proof For any t ≤ T , we have ˜(cid:96)t(a) ≥ (cid:96)t(a), so that ˜LT (π) ≥ LT (π). Separately, we
have ˜(cid:96)t(π∗(xt)) = (cid:96)t(π∗(xt)) for all t ≤ T using the deﬁnition of ˜(cid:96)t and the assumption
π∗(xt) ∈ At, hence ˜LT (π∗) ≥ LT (π∗).

We now show that with high probability, the optimal action is always explored by the

algorithm.

Lemma 4 Assume that event E holds. The actions given by the optimal policy are always
explored for all t ≥ 1, i.e., π∗(xt) ∈ At for all t ≥ 1.

Proof We show by induction on T ≥ 1 that π∗(xt) ∈ At for all t = 1, . . . , T . For the base
case, we have A1 = [K] since ˆZ0 = ∅ and hence empirical errors are always equal to 0, so
that π∗(x1) ∈ A1. Let us now assume as the inductive hypothesis that π∗(xt) ∈ At for all
t ≤ T − 1.

From deviation bounds, we have

ˆLT −1(πT ) − ˆLT −1(π∗) ≥ ˜LT −1(πT ) − ˜LT −1(π∗) −

(cid:32)(cid:114)

2Kρ(π, π∗)eT −1
(cid:15)

(cid:33)

+ (K/(cid:15) + 1)eT −1

LT −1(πT ) − LT −1(π∗) ≥ L(πT ) − L(π∗) −

(cid:16)(cid:112)ρ(π, π∗)eT −1 + 2eT −1

(cid:17)

.

Using Lemma 3 together with the inductive hypothesis, the above inequalities yield

ˆLT −1(πT ) − ˆLT −1(π∗) ≥ L(πT ) − L(π∗) − ∆∗

T (πT ).

Now consider an action a /∈ At. Using the deﬁnition (11) of At, we have

ˆLT −1(πT,a) − ˆLT −1(π∗) = ˆLT −1(πT,a) − ˆLT −1(πT ) + ˆLT −1(πT ) − ˆLT −1(π∗)

> ∆T − ∆∗

T (πT ) = 0,

40

A Contextual Bandit Bake-off

which implies π∗(xT ) (cid:54)= a, since ˆLT −1(πT,a) is the minimum of ˆLT −1 over policies satisfying
π(xT ) = a. This yields π∗(xT ) ∈ AT , which concludes the proof.

With the previous results, we can now prove that with high probability, discarding some
of the actions from the exploration process does not hurt us in identifying good policies.
In particular, πT +1 is about as good as it would have been with uniform (cid:15)-exploration all
along.

Theorem 5 Under the event E, which holds with probability 1 − δ,

L(πT +1) − L(π∗) ≤ ∆∗

T +1(πT +1).

In particular, L(πT +1) − L(π∗) ≤ ∆T +1.
Proof Assume event E holds. Using (12-13) combined with Lemma 3 (which holds by
Lemma 4), we have

L(πT +1) − L(π∗) ≤ ˆLT (πT +1) − ˆLT (π∗) + ∆∗

T +1(πT +1) ≤ ∆∗

T +1(πT +1).

Massart noise condition. We introduce a low-noise condition that will help us obtain
improved regret guarantees. Similar conditions have been frequently used in supervised
learning (Massart et al., 2006) and active learning (Hsu, 2010; Huang et al., 2015; Krish-
namurthy et al., 2019) for obtaining better data-dependent guarantees. We consider the
following Massart noise condition with parameter τ > 0:

ρ(π, π∗) ≤

(L(π) − L(π∗)).

1
τ

(M)

This condition holds when E[mina(cid:54)=π∗(x) (cid:96)(a) − (cid:96)(π∗(x))|x] ≥ τ , Px-almost surely, which is
similar to the Massart condition considered in Krishnamurthy et al. (2019) in the context
of active learning for cost-sensitive classiﬁcation. Indeed, we have

L(π) − L(π∗) = E[1{π(x) (cid:54)= π∗(x)}((cid:96)(π(x)) − (cid:96)(π∗(x))]

+ E[1{π(x) = π∗(x)}((cid:96)(π∗(x)) − (cid:96)(π∗(x)))]
(cid:20)
1{π(x) (cid:54)= π∗(x)}

(cid:96)(a) − (cid:96)(π∗(x))

(cid:18)

(cid:19)(cid:21)

≥ E

= E[1{π(x) (cid:54)= π∗(x)} E[ min

(cid:96)(a) − (cid:96)(π∗(x))|x]]

min
a(cid:54)=π∗(x)

a(cid:54)=π∗(x)

≥ E[1{π(x) (cid:54)= π∗(x)}τ ] = τ ρ(π, π∗),

which is precisely (M). The condition allows us to obtain a fast rate for the policies con-
sidered by our algorithm, as we now show.

Theorem 6 Assume the Massart condition (M) holds with parameter τ . Under the event
E, which holds w.p. 1 − δ,

for some numeric constant C.

L(πT +1) − L(π∗) ≤ C

eT ,

K
τ (cid:15)

41

Bietti, Agarwal and Langford

Proof Using Theorem 5 and the Massart condition, we have

L(πT +1) − L(π∗) ≤ ∆∗

T +1(πT +1) =

(cid:32)(cid:114)

(cid:33)

2K
(cid:15)

+ 1

(cid:112)ρ(πT +1, π∗)eT +

(cid:18) K
(cid:15)

(cid:19)

+ 3

eT

+ 1

(cid:112)(L(πT +1) − L(π∗))eT /τ +

(cid:18) K
(cid:15)

(cid:19)

+ 3

eT

(cid:32)(cid:114)

(cid:33)

≤

≤

2K
(cid:15)

(cid:114)

8KeT
τ (cid:15)

(L(πT +1) − L(π∗)) +

4KeT
(cid:15)

.

Solving the quadratic inequality in L(πT +1) − L(π∗) yields the result.

C.2.3 Regret Analysis

In a worst-case scenario, the following result shows that Algorithm 7 enjoys a similar O(T 2/3)
regret guarantee to the vanilla (cid:15)-greedy approach (Langford and Zhang, 2008).

Theorem 7 Conditioned on the event E, which holds with probability 1 − δ, the expected
regret of the algorithm is

E[RT |E] ≤ O

(cid:32)(cid:114)

KT log(T |Π|/δ)
(cid:15)

(cid:33)

+ T (cid:15)

.

Optimizing over the choice of (cid:15) yields a regret O(T 2/3(K log(T |Π|/δ))1/3).

Proof We condition on the 1−δ probability event E that the deviation bounds of Lemma 2
hold. We have

E[(cid:96)t(at) − (cid:96)t(π∗(xt))|Ft−1] = E[1{at = πt(xt)}((cid:96)t(πt(xt)) − (cid:96)t(π∗(xt)))|Ft−1]
+ E[1{at (cid:54)= πt(xt)}((cid:96)t(at) − (cid:96)t(π∗(xt)))|Ft−1]

≤ E[(cid:96)t(πt(xt)) − (cid:96)t(π∗(xt))|Ft−1] + E[E[1 − pt(πt(xt))|xt]|Ft−1]
≤ L(πt) − L(π∗) + (cid:15).

Summing over t and applying Theorem 5 together with ∆∗

t (π) ≤ ∆t, we obtain

E[RT |E] = E

(cid:96)t(at) − (cid:96)t(π∗(xt))|E

(cid:35)

(cid:34) T

(cid:88)

t=1
T
(cid:88)

t=2

≤ 1 + T (cid:15) +

∆t.

T
(cid:88)

t=2

≤ 1 +

E[L(πt) − L(π∗) + (cid:15)|Ft−1, E]

Using (cid:80)T

t=2

√

et ≤ O((cid:112)T log(8T 2|Π|/δ)) and (cid:80)T
(cid:114)

(cid:32)

E[RT |E] ≤ O

1 +

KT log(T |Π|/δ)
(cid:15)

+

K log(T |Π|/δ)
(cid:15)

(cid:33)

log T + T (cid:15)

,

t=2 et ≤ O(log(8T 2|Π|/δ) log T ), we obtain

42

A Contextual Bandit Bake-off

which yields the result.

Disagreement deﬁnitions.
In order to obtain improvements in regret guarantees over
the worst case, we consider notions of disagreement that extend standard deﬁnitions from
the active learning literature (e.g., Hanneke, 2014; Hsu, 2010; Huang et al., 2015) to the
multiclass case. Let B(π∗, r) := {π ∈ Π : ρ(π, π∗) ≤ r} be the ball centered at π∗ under
the (pseudo)-metric ρ(·, ·). We deﬁne the disagreement region DIS(r) and disagreement
coeﬃcient θ as follows:

DIS(r) := {x : ∃π ∈ B(π∗, r) π(x) (cid:54)= π∗(x)}
P (x ∈ DIS(r))
r

θ := sup
r>0

.

The next result shows that under the Massart condition and with a ﬁnite disagreement
coeﬃcient θ, our algorithm achieves a regret that scales as O(T 1/3) (up to logarithmic
factors), thus improving on worst-case guarantees obtained by optimal algorithms such
as Agarwal et al. (2012, 2014); Dudik et al. (2011a).

Theorem 8 Assume the Massart condition (M) holds with parameter τ . Conditioning on
the event E which holds w.p. 1 − δ, the algorithm has expected regret

E[RT |E] ≤ O

(cid:18) K log(T |Π|/δ)
τ (cid:15)

log T +

(cid:112)(cid:15)KT log(T |Π|/δ)

(cid:19)

.

θ
τ

Optimizing over the choice of (cid:15) yields a regret

E[RT |E] ≤ O

(θK log(T |Π|/δ))2/3(T log T )1/3

.

(cid:19)

(cid:18) 1
τ

Proof Assume E holds. Let t ≥ 2, and assume a ∈ At \ {π∗(xt)}. Deﬁne
(cid:40)

πa =

πt,
πt,a,

if πt(xt) = a
if πt(xt) (cid:54)= a,

so that we have πa(xt) = a (cid:54)= π∗(xt).

• If πa = πt, then L(πa) − L(π∗) ≤ ∆∗

t (πa) ≤ ∆t by Theorem 5

• If πa = πt,a, using deviation bounds, Lemma 4 and 3, we have

L(πa) − L(π∗) = L(πt,a) − L(π∗)

≤ ˆLt−1(πt,a) − ˆLt−1(π∗) + ∆∗
= ˆLt−1(πt,a) − ˆLt−1(πt)
(cid:123)(cid:122)
(cid:125)
≤∆t

(cid:124)

+ ˆLt−1(πt) − ˆLt−1(π∗)
(cid:123)(cid:122)
(cid:125)
≤0

(cid:124)

t (πt,a)

+∆∗

t (πt,a)

≤ 2∆t,

where the last inequality uses a ∈ At.

43

Bietti, Agarwal and Langford

By the Massart assumption, we then have ρ(πa, π∗) ≤ 2∆t/τ . Hence, we have xt ∈
DIS(2∆t/τ ). We have thus shown

E[E[1{a ∈ At \ {π∗(xt)}}|xt]|Ft−1] ≤ E[P (xt ∈ DIS(2∆t/τ ))|Ft−1] ≤ 2θ∆t/τ.

We then have

E[(cid:96)t(at) − (cid:96)t(π∗(xt))|Ft−1] = E[1{at = πt(xt)}((cid:96)t(πt(xt)) − (cid:96)t(π∗(xt)))

+ 1{at = π∗(xt) ∧ at (cid:54)= πt(xt)}((cid:96)t(π∗(xt)) − (cid:96)t(π∗(xt)))

1{at = a ∧ a /∈ {πt(xt), π∗(xt)}}((cid:96)t(a) − (cid:96)t(π∗(xt)))|Ft−1]

≤ E[(cid:96)t(πt(xt)) − (cid:96)t(π∗(xt))|Ft−1]

+ E

E[1{at = a} 1{a /∈ {πt(xt), π∗(xt)}}|xt]|Ft−1

(cid:35)

+

K
(cid:88)

a=1

(cid:34) K
(cid:88)

a=1

= L(πt) − L(π∗) +

E[E[pt(a) 1{a /∈ {πt(xt), π∗(xt)}}|xt]|Ft−1]

≤ L(πt) − L(π∗) +

E[E[1{a ∈ At \ {π∗(xt)}}|xt]|Ft−1]

≤ C

et−1 + 2(cid:15)θ∆t/τ,

K
τ (cid:15)

where we used

pt(a) 1{a /∈ {πt(xt), π∗(xt)}} =

1{a ∈ At \ {πt(xt), π∗(xt)}}

≤

1{a ∈ At \ {π∗(xt)}}.

Summing over t and taking total expectations (conditioned on E) yields

E[RT |E] ≤ O

(cid:32)

K log(T |Π|/δ)
τ (cid:15)

log T +

(cid:32)(cid:114)

(cid:15)θ
τ

KT log(T |Π|/δ)
(cid:15)

+

K log(T |Π|/δ)
(cid:15)

(cid:33)(cid:33)

log(T )

,

and the result follows.

Finally, we look at a simpler instructive example, which considers an extreme situation
where the expected loss of any suboptimal policy is bounded away from that of the opti-
mal policy. In this case, Algorithm 7 can achieve constant regret when the disagreement
coeﬃcient is bounded, as shown by the following result.

Proposition 9 Assume that L(π) − L(π∗) ≥ τ > 0 for all π (cid:54)= π∗, and that θ < ∞. Under
the event E, the algorithm achieves constant expected regret. In particular, the algorithm
stops incurring regret for T > T0 := max{t : 2∆t > τ }.

K
(cid:88)

a=1

(cid:15)
K

K
(cid:88)

a=1

(cid:15)
K
(cid:15)
K

44

A Contextual Bandit Bake-off

Proof By Theorem 5 and our assumption, we have L(πt) − L(π∗) ≤ 1{∆t ≥ τ }∆t.
Similarly, the assumption implies that ρ(π, π∗) ≤ 1{L(π) − L(π∗) ≥ τ }, so that using
similar arguments to the proof of Theorem 8, we have

E[E[1{a ∈ At \ {π∗(xt)}}|xt]|Ft−1] ≤ θ 1{2∆t ≥ τ }.

Following the proof of Theorem 8, this implies that when t is such that 2∆t < τ , then we
have

Let T0 := max{t : 2∆t ≥ τ }. We thus have

E[(cid:96)t(at) − (cid:96)t(π∗(xt))|Ft−1] = 0.

E[RT |E] ≤ 1 +

(∆t + (cid:15)).

T0(cid:88)

t=2

45

0
2
0
2
 
n
a
J
 
4
2
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
4
6
0
4
0
.
2
0
8
1
:
v
i
X
r
a

A Contextual Bandit Bake-off

A Contextual Bandit Bake-oﬀ

Alberto Bietti
Inria, Ecole normale sup´erieure, PSL Research University, Paris, France∗
Alekh Agarwal
Microsoft Research, Redmond, WA
John Langford
Microsoft Research, New York, NY

alberto.bietti@inria.fr

alekha@microsoft.com

jcl@microsoft.com

Abstract

Contextual bandit algorithms are essential for solving many real-world interactive ma-
chine learning problems. Despite multiple recent successes on statistically and computation-
ally eﬃcient methods, the practical behavior of these algorithms is still poorly understood.
We leverage the availability of large numbers of supervised learning datasets to empirically
evaluate contextual bandit algorithms, focusing on practical methods that learn by relying
on optimization oracles from supervised learning. We ﬁnd that a recent method (Foster
et al., 2018) using optimism under uncertainty works the best overall. A surprisingly close
second is a simple greedy baseline that only explores implicitly through the diversity of
contexts, followed by a variant of Online Cover (Agarwal et al., 2014) which tends to be
more conservative but robust to problem speciﬁcation by design. Along the way, we also
evaluate various components of contextual bandit algorithm design such as loss estimators.
Overall, this is a thorough study and review of contextual bandit methodology.
Keywords:

contextual bandits, online learning, evaluation

1. Introduction

At a practical level, how should contextual bandit learning and exploration be done?

In the contextual bandit problem, a learner repeatedly observes a context, chooses an
action, and observes a loss for the chosen action only. Many real-world interactive machine
learning tasks are well-suited to this setting: a movie recommendation system selects a
movie for a given user and receives feedback (click or no click) only for that movie; a choice
of medical treatment may be prescribed to a patient with an outcome observed for (only)
the chosen treatment. The limited feedback (known as bandit feedback) received by the
learner highlights the importance of exploration, which needs to be addressed by contextual
bandit algorithms.

The focal point of contextual bandit (henceforth CB) learning research is eﬃcient ex-
ploration algorithms (Abbasi-Yadkori et al., 2011; Agarwal et al., 2012, 2014; Agrawal and
Goyal, 2013; Dudik et al., 2011a; Langford and Zhang, 2008; Russo et al., 2018). How-
ever, many of these algorithms remain far from practical, and even when considering more
practical variants, their empirical behavior is poorly understood, typically with limited eval-

∗. Part of this work was done while AB was visiting Microsoft Research NY, supported by the MSR-Inria

Joint Center.

1

Bietti, Agarwal and Langford

uation on just a handful of scenarios. In particular, strategies based on upper conﬁdence
bounds (Abbasi-Yadkori et al., 2011; Li et al., 2010) or Thompson sampling (Agrawal and
Goyal, 2013; Russo et al., 2018) are often intractable for sparse, high-dimensional datasets,
and make strong assumptions on the model representation. The method of Agarwal et al.
(2014) alleviates some of these diﬃculties while being statistically optimal under weak as-
sumptions, but the analyzed version is still far from practical, and the worst-case guarantees
may lead to overly conservative exploration that can be ineﬃcient in practice.

The main objective of our work is an evaluation of practical methods that are relevant
to practitioners. We focus on algorithms that rely on optimization oracles from supervised
learning such as cost-sensitive classiﬁcation or regression oracles, which provides computa-
tional eﬃciency and support for generic representations. We further rely on online learning
implementations of the oracles, which are desirable in practice due to the sequential na-
ture of contextual bandits. While conﬁdence-based strategies and Thompson sampling are
not directly adapted to this setting, we achieve it with online Bootstrap approximations for
Thompson sampling (Agarwal et al., 2014; Eckles and Kaptein, 2014; Osband and Van Roy,
2015), and with the conﬁdence-based method of Foster et al. (2018) based on regression
oracles, which contains LinUCB as a special case. Additionally, we consider practical de-
sign choices such as loss encodings (e.g., if values have a range of 1, should we encode the
costs of best and worst outcomes as 0/1 or -1/0?), and for methods that learn by reduc-
tion to oﬀ-policy learning, we study diﬀerent reduction techniques beyond the simple inverse
propensity scoring approach. All of our experiments are based on the online learning system
Vowpal Wabbit1 which has already been successfully used in production systems (Agarwal
et al., 2016).

The interactive aspect of CB problems makes them notoriously diﬃcult to evaluate in
real-world settings beyond a handful of tasks. Instead, we leverage the wide availability of
supervised learning datasets with diﬀerent cost structures on their predictions, and obtain
contextual bandit instances by simulating bandit feedback, treating labels as actions and
hiding the loss of all actions but the chosen one. This setup captures the generality of the
i.i.d. contextual bandit setting, while avoiding some diﬃcult aspects of real-world settings
that are not supported by most existing algorithms and are diﬃcult to evaluate, such as non-
stationarity. We consider a large collection of over 500 datasets with varying characteristics
and various cost structures, including multiclass, multilabel and more general cost-sensitive
datasets with real-valued costs. To our knowledge, this is the ﬁrst evaluation of contextual
bandit algorithms on such a large and diverse corpus of datasets.

Our evaluation considers online implementations of Bootstrap Thompson sampling (Agar-
wal et al., 2014; Eckles and Kaptein, 2014; Osband and Van Roy, 2015), the Cover approach
of Agarwal et al. (2014), (cid:15)-greedy (Langford and Zhang, 2008), RegCB (Foster et al., 2018,
which includes LinUCB as a special case), and a basic greedy method similar to the one
studied in Bastani et al. (2017); Kannan et al. (2018). As the ﬁrst conclusion of our study,
we ﬁnd that the recent RegCB method (Foster et al., 2018) performs the best overall across
a number of experimental conditions. Remarkably, we discover that a close second in our set
of methods is the simple greedy baseline, often outperforming most exploration algorithms.
Both these methods have drawbacks in theory; greedy can fail arbitrarily poorly in prob-

1. https://vowpalwabbit.org

2

A Contextual Bandit Bake-off

Figure 1: Comparison between three competitive approaches: RegCB (conﬁdence based),
Cover-NU (variant of Online Cover) and Greedy. The plots show relative loss compared
to supervised learning (lower is better) on all datasets with 5 actions or more. Red points
indicate datasets with a statistically signiﬁcant diﬀerence in loss between two methods. A
greedy approach can outperform exploration methods in many cases; yet both Greedy and
RegCB may fail to explore eﬃciently on some other datasets where Cover-NU dominates.

lems where intentional exploration matters, while UCB methods make stronger modeling
assumptions and can have an uncontrolled regret when the assumptions fail. The logs col-
lected by deploying these methods in practice are also unﬁt for later oﬀ-policy experiments,
an important practical consideration. Our third conclusion is that several methods which
are more robust in that they make only a relatively milder i.i.d. assumption on the problem
tend to be overly conservative and often pay a steep price on easier datasets. Nevertheless,
we ﬁnd that an adaptation of Online Cover (Agarwal et al., 2014) is quite competitive on a
large fraction of our datasets. We also evaluate the eﬀect of diﬀerent ways to encode losses
and study diﬀerent reduction mechanisms for exploration algorithms that rely on oﬀ-policy
learning (such as (cid:15)-greedy), ﬁnding that a technique based on importance-weighted regres-
sion tends to outperform other approaches when applicable. We show pairwise comparisons
between the top 3 methods in our evaluation in Figure 1 for datasets with 5 or more actions.
For future theoretical research, our results motivate an emphasis on understanding greedy
strategies, building on recent progress (Bastani et al., 2017; Kannan et al., 2018), as well
as eﬀectively leveraging easier datasets in exploration problems (Agarwal et al., 2017).

1.1 Organization of the paper

The paper is organized as follows:

• Section 2 provides relevant background on i.i.d. contextual bandits, optimization or-
acles, and mechanisms for reduction to oﬀ-policy learning, and introduces our exper-
imental setup.

• Section 3 describes the main algorithms we consider in our evaluation, as well as the

modiﬁcations that we found eﬀective in practice.

• Section 4 presents the results and insights from our experimental evaluation.
• Finally, we conclude in Section 5 with a discussion of our ﬁndings and a collection
of guidelines and recommendations for practitioners that come out of our empirical
study, as well as open questions for theoreticians.

3

Bietti, Agarwal and Langford

2. Contextual Bandit Setup

In this section, we present the learning setup considered in this work, recalling the stochastic
contextual bandit setting, the notion of optimization oracles, various techniques used by
contextual bandit algorithms for leveraging these oracles, and ﬁnally our experimental setup.

2.1 Learning Setting

The stochastic (i.i.d.) contextual bandit learning problem can be described as follows. At
each time step t, the environment produces a pair (xt, (cid:96)t) ∼ D independently from the past,
where xt ∈ X is a context vector and (cid:96)t = ((cid:96)t(1), . . . , (cid:96)t(K)) ∈ RK is a loss vector, with K
the number of possible actions, and the data distribution is denoted D. After observing the
context xt, the learner chooses an action at, and only observes the loss (cid:96)t(at) corresponding
to the chosen action. The goal of the learner is to trade-oﬀ exploration and exploitation in
order to incur a small cumulative regret

RT :=

(cid:96)t(at) −

(cid:96)t(π∗(xt)),

T
(cid:88)

t=1

T
(cid:88)

t=1

with respect to the optimal policy π∗ ∈ arg minπ∈Π E(x,(cid:96))∼D[(cid:96)(π(x))], where Π denotes a
(large, possibly inﬁnite) set of policies π : X → {1, . . . , K} which we would like to do well
against.
It is often important for the learner to use randomized strategies, for instance
in order to later evaluate or optimize new policies, hence we let pt(a) ∈ [0, 1] denote the
probability that the agent chooses action a ∈ {1, . . . , K} at time t, so that at ∼ pt.

2.2 Optimization Oracles

In this paper, we focus on CB algorithms which rely on access to an optimization oracle
for solving optimization problems similar to those that arise in supervised learning, leading
to methods that are suitable for general policy classes Π. The main example is the cost-
sensitive classiﬁcation (CSC) oracle (Agarwal et al., 2014; Dudik et al., 2011a; Langford
and Zhang, 2008), which given a collection (x1, c1), . . . , (xT , cT ) ∈ X × RK computes

T
(cid:88)

arg min
π∈Π

ct(π(xt)).

t=1
The cost vectors ct = (ct(1), . . . , ct(K)) ∈ RK are often constructed using counterfactual
estimates of the true (unobserved) losses, as we describe in the next section.

Another approach is to use regression oracles, which ﬁnd f : X ×{1, . . . , K} → R from
a class of regressor functions F to predict a cost yt, given a context xt and action at (see,
e.g., Agarwal et al., 2012; Foster et al., 2018).
In this paper, we consider the following
regression oracle with importance weights ωt > 0:

(1)

(2)

While the theory typically requires exact solutions to (1) or (2), this is often impractical
due to the diﬃculty of the underlying optimization problem (especially for CSC, which

arg min
f ∈F

T
(cid:88)

t=1

ωt(f (xt, at) − yt)2.

4

A Contextual Bandit Bake-off

yields a non-convex and non-smooth problem), and more importantly because the size of the
problems to be solved keeps increasing after each iteration. In this work, we consider instead
the use of online optimization oracles for solving problems (1) or (2), which incrementally
update a given policy or regression function after each new observation, using for instance
an online gradient method. Such an online learning approach is natural in the CB setting,
and is common in interactive production systems (e.g., Agarwal et al., 2016; He et al., 2014;
McMahan et al., 2013).

2.3 Loss Estimates and Reductions

A common approach to solving problems with bandit (partial) feedback is to compute an
estimate of the full feedback using the observed loss and then apply methods for the full-
information setting to these estimated values. In the case of CBs, this allows an algorithm
to ﬁnd a good policy based on oﬀ-policy exploration data collected by the algorithm. These
loss estimates are commonly used to create CSC instances to be solved by the optimization
oracle introduced above (Agarwal et al., 2014; Dudik et al., 2011a; Langford and Zhang,
2008), a process sometimes referred as reduction to cost-sensitive classiﬁcation. Given such
estimates ˆ(cid:96)t(a) of (cid:96)t(a) for all actions a and for t = 1, . . . , T , such a reduction constructs cost
vectors ct = (ˆ(cid:96)t(1), . . . , ˆ(cid:96)t(K)) ∈ RK and feeds them along with the observed contexts xt
to the CSC oracle (1) in order to obtain a policy. We now describe the three diﬀerent
estimation methods considered in this paper, and how each is typically used for reduction
to policy learning with a CSC or regression oracle. In what follows, we consider observed
interaction records (xt, at, (cid:96)t(at), pt(at)).

Perhaps the simplest approach is the inverse propensity-scoring (IPS) estimator:

ˆ(cid:96)t(a) :=

1{a = at}.

(cid:96)t(at)
pt(at)
For any action a with pt(a) > 0, this estimator is unbiased, i.e. Eat∼pt[ˆ(cid:96)t(a)] = (cid:96)t(a), but
can have high variance when pt(at) is small. The estimator leads to a straightforward
CSC example (xt, ˆ(cid:96)t). Using such examples in (1) provides a way to perform oﬀ-policy
(or counterfactual) evaluation and optimization, which in turn allows a CB algorithm to
identify good policies for exploration.
In order to obtain good unbiased estimates, one
needs to control the variance of the estimates, e.g., by enforcing a minimum exploration
probability pt(a) ≥ (cid:15) > 0 on all actions.

(3)

In order to reduce the variance of IPS, the doubly robust (DR) estimator (Dudik

et al., 2011b) uses a separate, possibly biased, estimator of the loss ˆ(cid:96)(x, a):

ˆ(cid:96)t(a) :=

(cid:96)t(at) − ˆ(cid:96)(xt, at)
pt(at)

1{a = at} + ˆ(cid:96)(xt, a).

When ˆ(cid:96)(xt, at) is a good estimate of (cid:96)t(at), the small numerator in the ﬁrst term helps
reduce the variance induced by a small denominator, while the second term ensures that
the estimator is unbiased. Typically, ˆ(cid:96)(x, a) is learned by regression on all past observed
losses, e.g.,

(4)

(5)

ˆ(cid:96) := arg min
f ∈F

T
(cid:88)

t=1

(f (xt, at) − (cid:96)t(at))2.

5

Bietti, Agarwal and Langford

The reduction to cost-sensitive classiﬁcation is similar to IPS, by feeding cost vectors ct = ˆ(cid:96)t
to the CSC oracle.

We consider a third method that directly reduces to the importance-weighted regression
oracle (2), which we refer to as IWR (for importance-weighted regression), and is
suitable for algorithms which rely on oﬀ-policy learning.2 This approach ﬁnds a regressor

ˆf := arg min
f ∈F

T
(cid:88)

t=1

1
pt(at)

(f (xt, at) − (cid:96)t(at))2,

(6)

and considers the policy ˆπ(x) = arg mina ˆf (x, a). Such an estimator has been used, e.g.,
in the context of oﬀ-policy learning for recommendations (Schnabel et al., 2016) and is
available in the Vowpal Wabbit library. Note that if pt has full support, then the objective
is an unbiased estimate of the full regression objective on all actions,

T
(cid:88)

K
(cid:88)

t=1

a=1

(f (xt, a) − (cid:96)t(a))2.

In contrast, if the learner only explores a single action (so that pt(at) = 1 for all t), the
obtained regressor ˆf is the same as the loss estimator ˆ(cid:96) in (5). In this csae, if we consider
a x with x ∈ Rd, then the IWR reduction
a linear class of regressors of the form f (x, a) = θ(cid:62)
computes least-squares estimates ˆθa from the data observed when action a was chosen.
When actions are selected according to the greedy policy at = arg mina ˆθ(cid:62)
a xt, this setup
corresponds to the greedy algorithm considered, e.g., by Bastani et al. (2017).

Note that while CSC is typically intractable and requires approximations in order to
In
work in practice, importance-weighted regression does not suﬀer from these issues.
addition, while the computational cost for an approximate CSC online update scales with
the number of actions K, IWR only requires an update for a single action, making the
approach more attractive computationally. Another beneﬁt of IWR in an online setting is
that it can leverage importance weight aware online updates (Karampatziakis and Langford,
2011), which makes it easier to handle large inverse propensity scores.

2.4 Experimental Setup

Our experiments are conducted by simulating the contextual bandit setting using multiclass
or cost-sensitive classiﬁcation datasets, and use the online learning system Vowpal Wabbit
(VW).

Simulated contextual bandit setting. The experiments in this paper are based on
leveraging supervised cost-sensitive classiﬁcation datasets for simulating CB learning. In
particular, we treat a CSC example (xt, ct) ∈ X × RK as a CB example, with xt given as
the context to a CB algorithm, and we only reveal the loss for the chosen action at. For
a multiclass example with label yt ∈ {1, . . . , K}, we set ct(a) := 1{a (cid:54)= yt}; for multilabel
examples with label set Yt ⊆ {1, . . . , K}, we set ct(a) := 1{a /∈ Yt}; the cost-sensitive

2. Note that IWR is not directly applicable to methods that explicitly reduce to CSC oracles, such as Agar-

wal et al. (2014); Dudik et al. (2011a).

6

A Contextual Bandit Bake-off

datasets we consider have ct ∈ [0, 1]K. We consider more general loss encodings deﬁned
with an additive oﬀset on the cost by:

(cid:96)c
t (a) = c + ct(a),

(7)

for some c ∈ R. Although some techniques attempt to remove a dependence on such en-
coding choices through appropriately designed counterfactual loss estimators (Dudik et al.,
2011b; Swaminathan and Joachims, 2015), these may be imperfect in practice, and partic-
ularly in an online scenario. The behavior observed for diﬀerent choices of c allows us to
get a sense of the robustness of the algorithms to the scale of observed losses, which might
be unknown. Separately, diﬀerent values of c can lead to lower variance for loss estimation
in diﬀerent scenarios: c = 0 might be preferred if ct(a) is often 0, while c = −1 is preferred
when ct(a) is often 1. In order to have a meaningful comparison between diﬀerent algo-
rithms, loss encodings, as well as supervised multiclass classiﬁcation, our evaluation metrics
consider the original costs ct.

Online learning in VW. Online learning is an important tool for having machine learn-
ing systems that quickly and eﬃciently adapt to observed data (Agarwal et al., 2016; He
et al., 2014; McMahan et al., 2013). We run our CB algorithms in an online fashion using
Vowpal Wabbit:
instead of exact solutions of the optimization oracles from Section 2.2,
we consider online variants of the CSC and regression oracles, which incrementally update
the policies or regressors with online gradient steps or variants thereof. Note that in VW,
online CSC itself reduces to multiple online regression problems in VW (one per action), so
that we are left with only online regression steps. More speciﬁcally, we use adaptive (Duchi
et al., 2011), normalized (Ross et al., 2013) and importance-weight-aware (Karampatziakis
and Langford, 2011) gradient updates, with a single tunable step-size parameter.

a x, or in the case of the IWR reduction, regressors f (x, a) = θ(cid:62)

Parameterization. We consider linearly parameterized policies taking the form π(x) =
arg mina θ(cid:62)
a x. For the DR
loss estimator, we use a similar linear parameterization ˆ(cid:96)(x, a) = φ(cid:62)
a x. We note that the
algorithms we consider do not rely on this speciﬁc form, and easily extend to more complex,
problem-dependent representations, such as action-dependent features. Some datasets in
our evaluation have such an action-dependent structure, with diﬀerent feature vectors xa
for diﬀerent actions a; in this case we use parameterizations of the form f (x, a) = θ(cid:62)xa,
and ˆ(cid:96)(x, a) = φ(cid:62)xa, where the parameters θ and φ are shared across all actions.

3. Algorithms

In this section, we present the main algorithms we study in this paper, along with sim-
ple modiﬁcations that achieve improved exploration eﬃciency. All methods are based on
the generic scheme in Algorithm 1. The function explore computes the exploration dis-
tribution pt over actions, and learn updates the algorithm’s policies. For simiplicity, we
consider a function oracle which performs an online update to a policy using IPS, DR or
IWR reductions given an interaction record (xt, at, (cid:96)t(at), pt). For some methods (mainly
Cover), the CSC oracle is called explicitly with modiﬁed cost vectors ct rather than the IPS
or DR loss estimates ˆ(cid:96)t deﬁned in Section 2.3; in this case, we denote such an oracle call by
csc_oracle, and also use a separate routine estimator which takes an interaction record

7

Bietti, Agarwal and Langford

Algorithm 1 Generic contextual bandit algorithm

for t = 1, . . . do

Observe context xt, compute pt = explore(xt);
Choose action at ∼ pt, observe loss (cid:96)t(at);
learn(xt, at, (cid:96)t(at), pt);

end for

Algorithm 2 (cid:15)-greedy
π1; (cid:15) > 0 (or (cid:15) = 0 for Greedy).
explore(xt):

return pt(a) = (cid:15)/K + (1 − (cid:15)) 1{πt(xt) = a};

learn(xt, at, (cid:96)t(at), pt):

πt+1 = oracle(πt, xt, at, (cid:96)t(at), pt(at));

and computes IPS or DR loss estimate vectors ˆ(cid:96)t. RegCB directly uses a regression oracle,
which we denote reg_oracle.

Our implementations of each algorithm are in the Vowpal Wabbit online learning library.

3.1 (cid:15)-greedy and greedy

We consider an importance-weighted variant of the epoch-greedy approach of Langford
and Zhang (2008), given in Algorithm 2. The method acts greedily with probability 1 − (cid:15),
and otherwise explores uniformly on all actions. Learning is achieved by reduction to oﬀ-
policy optimization, through any of the three reductions presented in Section 2.3.

We also experimented with a variant we call active (cid:15)-greedy, that uses notions from
disagreement-based active learning (Hanneke, 2014; Hsu, 2010) in order to reduce uniform
exploration to only actions that could plausibly be taken by the optimal policy. While this
variant often improves on the basic (cid:15)-greedy method, we found that it is often outperformed
empirically by other exploration algorithms, and thus defer its presentation to Appendix C,
along with a theoretical analysis, for reference.

Greedy. When taking (cid:15) = 0 in the (cid:15)-greedy approach, with the IWR reduction, we are left
with a fully greedy approach that always selects the action given by the current policy. This
gives us an online variant of the greedy algorithm of Bastani et al. (2017), which regresses on
observed losses and acts by selecting the action with minimum predicted loss. Although this
greedy strategy does not have an explicit mechanism for exploration in its choice of actions,
the inherent diversity in the distribution of contexts may provide suﬃcient exploration
for good performance and provable regret guarantees (Bastani et al., 2017; Kannan et al.,
2018). In particular, under appropriate assumptions including a diversity assumption on
the contexts, one can show that all actions have a non-zero probability of being selected at
each step, providing a form of “natural” exploration from which one can establish regret
guarantees. Empirically, we ﬁnd that Greedy can perform very well in practice on many
datasets (see Section 4). If multiple actions get the same score according to the current
regressor, we break ties randomly.

8

A Contextual Bandit Bake-off

3.2 Bagging (online Bootstrap Thompson sampling)

return pt(a) ∝ |{i : πi

t(xt) = a}|;3

Algorithm 3 Bag
π1
1, . . . , πN
1 .
explore(xt):

learn(xt, at, (cid:96)t(at), pt):
for i = 1, . . . , N do
τ i ∼ P oisson(1);
t+1 = oracleτ i(πi
πi
end for

t, xt, at, (cid:96)t(at), pt(at));

{with τ 1 = 1 for bag-greedy}

We now consider a variant of Thompson sampling which is usable in practice with opti-
mization oracles. Thompson sampling provides a generic approach to exploration problems,
which maintains a belief on the data generating model in the form of a posterior distribution
given the observed data, and explores by selecting actions according to a model sampled
from this posterior (see, e.g., Agrawal and Goyal, 2013; Chapelle and Li, 2011; Russo et al.,
2018; Thompson, 1933). While the generality of this strategy makes it attractive, main-
taining this posterior distribution can be intractable for complex policy classes, and may
require strong modeling assumptions. In order to overcome such diﬃculties and to support
the optimization oracles considered in this paper, we rely on an approximation of Thompson
sampling known as the online Bootstrap Thompson sampling (Eckles and Kaptein, 2014;
Osband and Van Roy, 2015), or bagging (Agarwal et al., 2014). This approach, shown in
Algorithm 3, maintains a collection of N policies π1
t meant to approximate the pos-
terior distribution over policies via the online Bootstrap (Agarwal et al., 2014; Eckles and
Kaptein, 2014; Osband and Van Roy, 2015; Oza and Russell, 2001; Qin et al., 2013), and
explores in a Thompson sampling fashion, by averaging action decisions across all policies
(hence the name bagging).

t , . . . , πN

Each policy is trained on a diﬀerent online Bootstrap sample of the observed data, in the
form of interaction records. The online Bootstrap performs a random number τ of online
updates to each policy instead of one (this is denoted by oracleτ in Algorithm 3). We use a
Poisson distribution with parameter 1 for τ , which ensures that in expectation, each policy
is trained on t examples after t steps. In contrast to Eckles and Kaptein (2014); Osband
and Van Roy (2015), which play the arm given by one of the N policies chosen at random,
we compute the full action distribution pt resulting from such a sampling, and leverage this
for loss estimation, allowing learning by reduction to oﬀ-policy optimization as in Agarwal
et al. (2014). As in the (cid:15)-greedy algorithm, Bagging directly relies on oﬀ-policy learning
and thus all three reductions are admissible.

Greedy bagging. We also consider a simple optimization that we call greedy bagging, for
which the ﬁrst policy π1 is trained on the true data sample (like Greedy), that is, with τ

3. When policies are parametrized using regressors as in our implementation, we let πi

t(x) be uniform over
all actions tied for the lowest cost, and the ﬁnal distribution is uniform across all actions tied for best
according to one of the policies in the bag. The added randomization gives useful variance reduction in
our experiments.

9

Bietti, Agarwal and Langford

Algorithm 4 Cover
π1
1, . . . , πN
explore(xt):

1 ; (cid:15)t = min(1/K, 1/

Kt); ψ > 0.

√

t(xt) = a}|;

pt(a) ∝ |{i : πi
return (cid:15)t + (1 − (cid:15)t)pt;
return pt;

learn(xt, at, (cid:96)t(at), pt):
π1
t+1 = oracle(π1
ˆ(cid:96)t = estimator(xt, at, (cid:96)t(at), pt(at));
for i = 2, . . . , N do

t , xt, at, (cid:96)t(at), pt(at));

t+1(xt) = a}|;

qi(a) ∝ |{j ≤ i − 1 : πj
ˆc(a) = ˆ(cid:96)t(a) −
ψ(cid:15)t
(cid:15)t+(1−(cid:15)t)qi(a) ;
πi
t+1 = csc_oracle(πi
end for

t, xt, ˆc);

{for cover}
{for cover-nu}

always equal to one, instead of a bootstrap sample with random choices of τ . We found this
approach to often improve on bagging, particularly when the number of policies N is small.

3.3 Cover

This method, given in Algorithm 4, is based on Online Cover, an online approxima-
tion of the “ILOVETOCONBANDITS” algorithm of Agarwal et al. (2014). The approach
maintains a collection of N policies, π1
t , meant to approximate a covering distri-
bution over policies that are good for both exploration and exploitation. The ﬁrst policy
π1
t is trained on observed data using the oracle as in previous algorithms, while subsequent
policies are trained using modiﬁed cost-sensitive examples which encourage diversity in the
predicted actions compared to the previous policies.

t , . . . , πN

Our implementation diﬀers from the Online Cover algorithm of Agarwal et al. (2014,
Algorithm 5) in how the diversity term in the deﬁnition of ˆc(a) is handled (the second term).
When creating cost-sensitive examples for a given policy πi, this term rewards an action a
that is not well-covered by previous policies (i.e., small qi(a)), by subtracting from the cost
a term that decreases with qi(a). While Online Cover considers a ﬁxed (cid:15)t = (cid:15), we let (cid:15)t
decay with t, and introduce a parameter ψ to control the overall reward term, which bears
more similarity with the analyzed algorithm. In particular, the magnitude of the reward
is ψ whenever action a is not covered by previous policies (i.e., qi(a) = 0), but decays with
ψ(cid:15)t whenever qi(a) > 0, so that the level of induced diversity can decrease over time as we
gain conﬁdence that good policies are covered.

Cover-NU. While Cover requires some uniform exploration across all actions, our ex-
periments suggest that this can make exploration highly ineﬃcient, thus we introduce a
variant, Cover-NU, with no uniform exploration outside the set of actions selected by cov-
ering policies.

10

A Contextual Bandit Bake-off

Algorithm 5 RegCB
f1; C0 > 0.
explore(xt):

lt(a) = lcb(ft, xt, a, ∆t,C0);
ut(a) = ucb(ft, xt, a, ∆t,C0);
pt(a) ∝ 1{a ∈ arg mina(cid:48) lt(a(cid:48))};
pt(a) ∝ 1{lt(a) ≤ mina(cid:48) ut(a(cid:48))};
return pt;

learn(xt, at, (cid:96)t(at), pt):

ft+1 = reg_oracle(ft, xt, at, (cid:96)t(at));

3.4 RegCB

{RegCB-opt variant}
{RegCB-elim variant}

We consider online approximations of the two algorithms introduced by Foster et al. (2018)
based on regression oracles, shown in Algorithm 5. Both algorithms estimate conﬁdence
intervals of the loss for each action given the current context xt, denoted [lt(a), ut(a)] in
Algorithm 5, by considering predictions from a subset of regressors with small squared loss.
The optimistic variant then selects the action with smallest lower bound estimate, similar
to LinUCB, while the elimination variant explores uniformly on actions that may plausibly
be the best.

More formally, the RegCB algorithm theoretically analyzed by Foster et al. (2018) de-

ﬁnes the conﬁdence bounds as follows:

lt(a) = min
f ∈Ft

f (xt, a),

and ut(a) = max
f ∈Ft

f (xt, a).

Here, Ft is a subset of regressors that is “good” for loss estimation, in the sense that it
achieves a small regression loss on observed data, ˆRt−1(f ) := 1
s=1(f (xt, at) − (cid:96)t(at))2,
t−1
compared to the best regressor in the full regressor class F:

(cid:80)t−1

Ft := {f ∈ F : ˆRt−1(f ) − min
f ∈F

ˆRt−1(f ) ≤ ∆t},

where ∆t is a quantity decreasing with t.

Our online implementation computes approximations of these upper and lower bounds
on the loss of each action, by using a sensitivity analysis of the current regressor based
on importance weighting taken from Krishnamurthy et al. (2019) in the context of active
learning (the computations are denoted lcb and ucb in Algorithm 5). The algorithm main-
tains a regressor ft : X × {1, . . . , K} and, given a new context xt, computes lower and
upper conﬁdence bounds lt(a) ≤ ft(xt, a) ≤ ut(a). These are computed by adding “virtual”
importance-weighted regression examples with low and high costs, and ﬁnding the largest
importance weight leading to an excess squared loss smaller than ∆t,C0, where

and C0 is a parameter controlling the width of the conﬁdence bounds. This importance
weight can be found using regressor sensitivities and a binary search procedure as described

∆t,C0 =

C0 log(Kt)
t

,

11

Bietti, Agarwal and Langford

in (Krishnamurthy et al., 2019, Section 7.1). Note that this requires knowledge of the loss
range [cmin, cmax], unlike other methods. In contrast to Krishnamurthy et al. (2019), we set
the labels of the “virtual” examples to cmin − 1 for the lower bound and cmax + 1 for the
upper bound, instead of cmin and cmax.

4. Evaluation

In this section, we present our evaluation of the contextual bandit algorithms described in
Section 3. The evaluation code is available at https://github.com/albietz/cb_bakeoff.
All methods presented in this section are available in Vowpal Wabbit.4

Evaluation setup. Our evaluation consists in simulating a CB setting from cost-sensitive
classiﬁcation datasets, as described in Section 2.4. We consider a collection of 525 multiclass
classiﬁcation datasets from the openml.org platform, including among others, medical,
gene expression, text, sensory or synthetic data, as well as 5 multilabel datasets5 and 3
cost-sensitive datasets, namely a cost-sensitive version of the RCV1 multilabel dataset used
in (Krishnamurthy et al., 2019), where the cost of a news topic is equal to the tree distance
to a correct topic, as well as the two learning to rank datasets used in (Foster et al., 2018).
More details on these datasets are given in Appendix A. Because of the online setup, we
consider one or more ﬁxed, shuﬄed orderings of each dataset. The datasets widely vary in
noise levels, and number of actions, features, examples etc., allowing us to model varying
diﬃculties in CB problems.

We evaluate the algorithms described in Section 3. We ran each method on every dataset
with diﬀerent choices of algorithm-speciﬁc hyperparameters, learning rates, reductions, and
loss encodings. Details are given in Appendix B.1. Unless otherwise speciﬁed, we consider
ﬁxed choices which are chosen to optimize performance on a subset of multiclass datasets
with a voting mechanism and are highlighted in Table 9 of Appendix B, except for the
learning rate, which is always optimized.

The performance of method A on a dataset of size n is measured by the progressive

validation loss (Blum et al., 1999):

P VA =

ct(at),

1
n

n
(cid:88)

t=1

where at is the action chosen by the algorithm on the t-th example, and ct the true cost
vector. This metric allows us to capture the explore-exploit trade-oﬀ, while providing a
measure of generalization that is independent of the choice of loss encodings, and compa-
rable with online supervised learning. We also consider a normalized loss variant given
by P VA−P VOAA
, where OAA denotes an online (supervised) cost-sensitive one against all
classiﬁer. This helps highlight the diﬃculty of exploration for some datasets in our plots.

P VOAA

In order to compare two methods on a given dataset with binary costs (multiclass or
multilabel), we consider a notion of statistically signiﬁcant win or loss. We use the

4. For reproducibility purposes, the precise version of VW used to run these experiments is available at

https://github.com/albietz/vowpal_wabbit/tree/bakeoff.

5. https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html

12

A Contextual Bandit Bake-off

Table 1: Entry (row, column) shows the statistically signiﬁcant win-loss diﬀerence of row
against column. Encoding ﬁxed to -1/0 (top) or 0/1 (bottom). (left) held-out datasets
only (330 in total); ﬁxed hyperparameters, only the learning rate is optimized; (right)
all datasets (530 in total); all choices of hyperparameters are optimized on each dataset
(diﬀerent methods have diﬀerent hyperparameter settings from 1 to 18, see Table 9 in
Appendix B).

↓ vs → G RO C-nu B-g
50
-7
G
49
-
RO
22
-26
C-nu
-
-49
B-g
-17
-68
(cid:15)G

10
26
-
-22
-57

-
7
-10
-50
-54

↓ vs → G RO C-nu B-g
-22
-30
G
8
-
RO
64
37
C-nu
-
-8
B-g
-86
-84
(cid:15)G

-93
-37
-
-64
-145

-
30
93
22
-58

(cid:15)G
58
84
145
86
-

-1/0 encoding

↓ vs → G
-
G
64
RO
17
C-nu
-36
B-g
-52
(cid:15)G

RO C-nu B-g
-17
-64
36
100
45
-
45
-
-45
-45
-100
-
-19
-75
-120

↓ vs → G
-
G
124
RO
150
C-nu
75
B-g
-5
(cid:15)G

RO C-nu B-g
-75
-150
-124
65
-11
-
67
-
11
-
-67
-65
-83
-144
-125

(cid:15)G
5
125
144
83
-

0/1 encoding

(cid:15)G
54
68
57
17
-

(cid:15)G
52
120
75
19
-

following (heuristic) deﬁnition of signiﬁcance based on an approximate Z-test: if pa and pb
denote the PV loss of a and b on a given dataset of size n, then a wins over b if





1 − Φ

pa − pb

(cid:113) pa(1−pa)
n

+ pb(1−pb)
n



 < 0.05,

where Φ is the Gauss error function. We also deﬁne the signiﬁcant win-loss diﬀerence
of one algorithm against another to be the diﬀerence between the number of signiﬁcant
wins and signiﬁcant losses. We have found these metrics to provide more insight into the
behavior of diﬀerent methods, compared to strategies based on aggregation of loss measures
across all datasets. Indeed, we often found the relative performance of two methods to vary
signiﬁcantly across datasets, making aggregate metrics less informative.

Results in this section focus on Greedy (G), RegCB-optimistic (RO), Cover-NU (C-
nu), Bag-greedy (B-g) and (cid:15)-greedy ((cid:15)G), deferring other variants to Appendix B, as their
performance is typically comparable to or dominated by these methods. We combine results
on multiclass and multilabel datasets, but show them separately in Appendix B.

Eﬃcient exploration methods. Our experiments suggest that the best performing
method is the RegCB approach (Foster et al., 2018), as shown in Table 1 (left), where
the signiﬁcant wins of RO against all other methods exceed signiﬁcant losses, which yields
the best performance overall. This is particularly prominent with 0/1 encodings. With -1/0
encodings, which are generally preferred on our corpus as discussed below, the simple greedy
approach comes a close second, outperforming other methods on a large number of datasets,

13

Bietti, Agarwal and Langford

Table 2: Progressive validation loss for cost-sensitive datasets with real-valued costs in [0, 1].
Hyperparameters are ﬁxed as in Table 10. We show mean and standard error based on 10
diﬀerent random reshuﬄings. For RCV1, costs are based on tree distance to correct topics.
For MSLR and Yahoo, costs encode 5 regularly-spaced discrete relevance scores (0: perfectly
relevant, 1: irrelevant), and we include results for a loss encoding oﬀset c = −1 in Eq. (7).

G
0.215 ± 0.010

RO
0.225 ± 0.008

B-g
0.251 ± 0.005

(cid:15)G
0.230 ± 0.009

G
0.798 ± 0.0023

RO
0.794 ± 0.0007

B-g
0.799 ± 0.0013

(cid:15)G
0.807 ± 0.0020

G
0.791 ± 0.0012

RO
0.790 ± 0.0009

B-g
0.791 ± 0.0008

(cid:15)G
0.806 ± 0.0018

G
0.593 ± 0.0004

RO
0.592 ± 0.0005

B-g
0.596 ± 0.0006

(cid:15)G
0.598 ± 0.0005

G
0.589 ± 0.0005

RO
0.588 ± 0.0004

B-g
0.590 ± 0.0005

(cid:15)G
0.594 ± 0.0006

C-nu
0.215 ± 0.006
(a) RCV1

C-nu
0.798 ± 0.0012
(b) MSLR

C-nu
0.792 ± 0.0007
(c) MSLR, c = −1

C-nu
0.594 ± 0.0004
(d) Yahoo

C-nu
0.589 ± 0.0004
(e) Yahoo, c = −1

despite the lack of an explicit exploration mechanism. A possible reason for this success is
the diversity that is inherently present in the distribution of contexts across actions, which
has been shown to yield no-regret guarantees under various assumptions (Bastani et al.,
2017; Kannan et al., 2018). The noise induced by the dynamics of online learning and
random tie-breaking may also be a source of more exploration. RO and Greedy also show
strong performance on the 8 UCI datasets and the learning-to-rank datasets from Foster
et al. (2018), as shown in Tables 2 and 13. Nevertheless, both Greedy and RegCB have
known failure modes which the Cover approach is robust to by design. While the basic
approach with uniform exploration is too conservative, we found our Cover-NU variant
to be quite competitive overall. The randomization in its choice of actions yields explo-
ration logs which may be additionally used for oﬄine evaluation, in contrast to Greedy and
RegCB-opt, which choose actions deterministically. A more granular comparison of these
methods is given in Figure 2, which highlight the failure of Greedy and RegCB against
Cover-NU on some datasets which may be more diﬃcult perhaps due to a failure of mod-
eling assumptions. Bagging also outperforms other methods on some datasets, however it
is outperformed on most datasets, possibly because of the additional variance induced by
the bootstrap sampling. Table 1 (right) optimizes over hyperparameters for each dataset,
which captures the best potential of each method. Cover-NU does the best here, but also
has the most hyperparameters, indicating that a more adaptive variant could be desirable.
RegCB stays competitive, while Greedy pales possibly due to fewer hyperparameters.

Variability with dataset characteristics. Table 5 shows win-loss statistics for subsets
of the datasets with constraints on diﬀerent characteristics, such as number of actions,

14

A Contextual Bandit Bake-off

Figure 2: Pairwise comparisons among four successful methods: Greedy, Cover-nu, Bag-
greedy, and RegCB-opt. Hyperparameters ﬁxed as in Table 9, with encoding -1/0. All
held-out multiclass and multilabel datasets are shown, in contrast to Figure 1, which only
shows held-out datasets with 5 or more actions. The plots consider normalized loss, with
red points indicating signiﬁcant wins.

dimensionality, size, and performance in the supervised setting. The values for these splits
were chosen in order to have a reasonably balanced number of datasets in each table.

√

We ﬁnd that RegCB-opt is the preferred method in most situations, while Greedy and
Cover-NU can also provide good performance in diﬀerent settings. When only considering
larger datasets, RegCB-opt dominates all other methods, with Greedy a close second, while
Cover-NU seems to explore less eﬃciently. This could be related to the better adaptivity
properties of RegCB to favorable noise conditions, which can achieve improved (even log-
arithmic) regret (Foster et al., 2018), in contrast to Cover-NU, for which a slower rate (of
O(
n)) may be unavoidable since it is baked into the algorithm of Agarwal et al. (2014)
by design, and is reﬂected in the diversity terms of the costs ˆc in our online variant given
in Algorithm 4. In contrast, when n is small, RegCB-opt and Greedy may struggle to ﬁnd
good policies (in fact, their analysis typically requires a number of “warm-start” iterations
with uniform exploration), while Cover-NU seems to explore more eﬃciently from the be-
ginning, and behaves well with large action spaces or high-dimensional features. Finally,
Table 5(d,e) shows that Greedy can be the best choice when the dataset is “easy”, in the
sense that a supervised learning method achieves small loss. Achieving good performance
on such easy datasets is related to the open problem of Agarwal et al. (2017), and vari-
ants of methods designed to be agnostic to the data distribution—such as Cover(-NU) and
(cid:15)-Greedy (Agarwal et al., 2014; Langford and Zhang, 2008)—seem to be the weakest on
these datasets.

15

Bietti, Agarwal and Langford

Table 3: Impact of reductions for Bag (left) and (cid:15)-greedy (right), with hyperparameters
optimized and encoding ﬁxed to -1/0. Each (row, column) entry shows the statistically
signiﬁcant win-loss diﬀerence of row against column. IWR outperforms the other reductions
for both methods, which are the only two methods that directly reduce to oﬀ-policy learning,
and thus where such a comparison applies.

↓ vs → ips
-
ips
42
dr
59
iwr

dr
-42
-
28

iwr
-59
-28
-

↓ vs → ips
-
ips
-63
dr
133
iwr

dr
63
-
155

iwr
-133
-155
-

Table 4: Impact of encoding on diﬀerent algorithms, with hyperparameters optimized. Each
entry indicates the number of statistically signiﬁcant wins/losses of -1/0 against 0/1. -1/0
is the better overall choice of encoding, but 0/1 can be preferable on larger datasets (the
bottom row considers the 64 datasets in our corpus with more than 10,000 examples).

datasets
all
≥ 10,000

G
136 / 42
19 / 12

RO
60 / 47
10 / 18

C-nu
76 / 46
14 / 20

B-g
77 / 27
15 / 11

(cid:15)G
99 / 27
14 / 5

Reductions. Among the reduction mechanisms introduced in Section 2.3, IWR has de-
sirable properties such as tractability (the other reductions rely on a CSC objective, which
requires approximations due to non-convexity), and a computational cost that is indepen-
dent of the total number of actions, only requiring updates for the chosen action. In addition
to Greedy, which can be seen as using a form of IWR, we found IWR to work very well
for bagging and (cid:15)-greedy approaches, as shown in Table 3 (see also Table 9 in Appendix B,
which shows that IWR is also preferred when considering ﬁxed hyperparameters for these
methods). This may be attributed to the diﬃculty of the CSC problem compared to regres-
sion, as well as importance weight aware online updates, which can be helpful for small (cid:15).
Together with its computational beneﬁts, our results suggest that IWR is often a com-
pelling alternative to CSC reductions based on IPS or DR. In particular, when the number
of actions is prohibitively large for using Cover-NU or RegCB, Bag with IWR may be a
good default choice of exploration algorithm. While Cover-NU does not directly support
the IWR reduction, making them work together well would be a promising future direction.

Encodings. Table 4 indicates that the -1/0 encoding is preferred to 0/1 on many of the
datasets, and for all methods. We now give one possible explanation. As discussed in
Section 2.4, the -1/0 encoding yields low variance loss estimates when the cost is often
close to 1. For datasets with binary costs, since the learner may often be wrong in early
iterations, a cost of 1 is a good initial bias for learning. With enough data, however, the
learner should reach better accuracies and observe losses closer to 0, in which case the 0/1
encoding should lead to lower variance estimates, yielding better performance as observed
in Table 4. We tried shifting the loss range in the RCV1 dataset with real-valued costs
from [0, 1] to [−1, 0], but saw no improvements compared to the results in Table 2. Indeed,

16

A Contextual Bandit Bake-off

(a) all multiclass datasets

(b) n ≥ 10 000 only

Figure 3: Errors of IPS counterfactual estimates for the uniform random policy using explo-
ration logs collected by various algorithms on multiclass datasets. The boxes show quartiles
(with the median shown as a blue line) of the distribution of squared errors across all
multiclass datasets or only those with at least 10 000 examples. The logs are obtained by
running each algorithm with -1/0 encodings, ﬁxed hyperparameters from Table 9, and the
best learning rate on each dataset according to progressive validation loss.

a cost of 1 may not be a good initial guess in this case, in contrast to the binary cost
setting. On the MSLR and Yahoo learning-to-rank datasets, we do see some improvement
from shifting costs to the range [−1, 0], perhaps because in this case the costs are discrete
values, and the cost of 1 corresponds to the document label “irrelevant”, which appears
frequently in these datasets.

Counterfactual evaluation. After running an exploration algorithm, a desirable goal
for practitioners is to evaluate new policies oﬄine, given access to interaction logs from
exploration data. While various exploration algorithms rely on such counterfactual eval-
uation through a reduction, the need for eﬃcient exploration might restrict the ability to
evaluate arbitrary policies oﬄine, since the algorithm may only need to estimate “good”
policies in the sense of small regret with respect to the optimal policy. To illustrate this,
in Figure 3, we consider the evaluation of the uniform random stochastic policy, given by
πunif(a|x) = 1/K for all actions a = 1, . . . , K, using a simple inverse propensity scoring
(IPS) approach. While such a task is somewhat extreme and of limited practical interest
in itself, we use it mainly as a proxy for the ability to evaluate any arbitrary policy (in par-
ticular, it should be possible to evaluate any policy if we can evaluate the uniform random
policy). On a multiclass dataset, the expected loss of such a policy is simply 1 − 1/K, while
its IPS estimate is given by

ˆLIP S = 1 −

1
n

n
(cid:88)

t=1

1 − (cid:96)t(at)
Kpt(at)

,

where the loss is given by (cid:96)t(at) = 1{at (cid:54)= yt} when the correct multiclass label is yt.
Note that this quantity is well-deﬁned since the denominator is always non-zero when at is
sampled from pt, but the estimator is biased when data is collected by a method without
some amount of uniform exploration (i.e., when pt does not have full support). This bias

17

Bietti, Agarwal and Langford

is particularly evident in Figure 3b where epsilon-greedy shows very good counterfactual
evaluation performance while the other algorithms induce biased counterfactual evaluation
due to lack of full support. The plots in Figure 3 show the distribution of squared errors
( ˆLIP S − (1 − 1/K))2 across multiclass datasets. We consider IPS on the rewards 1 − (cid:96)t(at)
here as it is more adapted to the -1/0 encodings used to collect exploration logs, but we
also show IPS on losses in Figure 6 of Appendix B.2. Figure 3 shows that the more eﬃcient
exploration methods (Greedy, RegCB-optimistic, and Cover-NU) give poor estimates for
this policy, probably because their exploration logs provide biased estimates and are quite
focused on few actions that may be taken by good policies, while the uniform exploration in
(cid:15)-Greedy (and Cover-U, see Figure 6) yields better estimates, particularly on larger datasets.
The elimination version of RegCB provides slightly better logs compared to the optimistic
version (see Figure 6), and Bagging may also be preferred in this context. Overall, these
results show that there may be a trade-oﬀ between eﬃcient exploration and the ability to
perform good counterfactual evaluation, and that uniform exploration may be needed if one
would like to perform accurate oﬄine experiments for a broad range of questions.

5. Discussion and Takeaways

In this paper, we presented an evaluation of practical contextual bandit algorithms on a
large collection of supervised learning datasets with simulated bandit feedback. We ﬁnd that
a worst-case theoretical robustness forces several common methods to often over-explore,
damaging their empirical performance, and strategies that limit (RegCB and Cover-NU) or
simply forgo (Greedy) explicit exploration dominate the ﬁeld. For practitioners, our study
also provides a reference for practical implementations, while stressing the importance of
loss estimation and other design choices such as how to encode observed feedback.

Guidelines for practitioners. We now summarize some practical guidelines that come
out of our empirical study:

• Methods relying on modeling assumptions on the data distribution such as RegCB are
often preferred in practice, and even Greedy can work well (see, e.g., Table 1). They
tend to dominate more robust approaches such as Cover-NU even more prominently
on larger datasets, or on datasets where prediction is easy, e.g., due to low noise
(see Table 5). While it may be diﬃcult to assess such favorable conditions of the
data in advance, practitioners may use speciﬁc domain knowledge to design better
feature representations for prediction, which may in turn improve exploration for
these methods.

• Uniform exploration hurts empirical performance in most cases (see, e.g., the poor
performance of (cid:15)-greedy and Cover-u in Table 11 of Appendix B). Nevertheless, it
may be necessary on the hardest datasets, and may be crucial if one needs to perform
oﬀ-policy counterfactual evaluation (see Figures 3 and 6).

• Loss estimation is an essential component in many CB algorithms for good practical
performance, and DR should be preferred over IPS. For methods based on reduction
to oﬀ-policy learning, such as (cid:15)-Greedy and Bagging, the IWR reduction is typically
best, in addition to providing computational beneﬁts (see Table 3).

18

A Contextual Bandit Bake-off

• From our early experiments, we found randomization on tied choices of actions to
always be useful. For instance, it avoids odd behavior which may arise from deter-
ministic, implementation-speciﬁc biases (e.g., always favoring one speciﬁc action over
the others).

• The choice of cost encodings makes a big diﬀerence in practice and should be carefully
considered when designing an contextual bandit problem, even when loss estimation
techniques such as DR are used. For binary outcomes, -1/0 is a good default choice
of encoding in the common situation where the observed loss is often 1 (see Table 4).

• Modeling choices and encodings sometimes provide pessimistic initial estimates that
can hurt initial exploration on some problems, particularly for Greedy and RegCB-
optimistic. Random tie-breaking as well as using a shared additive baseline can help
mitigate this issue (see Section B.3).

• The hyperparameters highlighted in Appendix B.1 obtained on our datasets may be
good default choices in practice. Nevertheless, these may need to be balanced in
order to address other conﬂicting requirements in real-world settings, such as non-
stationarity or the ability to run oﬄine experiments.

Open questions for theoreticians. Our study raises some questions of interest for
theoretical research on contextual bandits. The good performance of greedy methods calls
for a better understanding of greedy methods, building upon the work of Bastani et al.
(2017); Kannan et al. (2018), as well as methods that are more robust to more diﬃcult
datasets while adapting to such favorable scenarios, such as when the context distribution
has enough diversity. A related question is that of adaptivity to easy datasets for which
the optimal policy has small loss, an open problem pointed out by Agarwal et al. (2017)
in the form of “ﬁrst-order” regret bounds. While methods satisfying such regret bounds
have now been developed theoretically (Allen-Zhu et al., 2018), these methods are currently
not computationally eﬃcient, and obtaining eﬃcient methods based on optimization oracles
remains an important open problem. We also note that while our experiments are based on
online optimization oracles, most analyzed versions of the algorithms rely on solving the full
optimization problems; it would be interesting to better understand the behavior of online
variants, and to characterize the implicit exploration eﬀect for the greedy method.

Limitations of the study. Our study is primarily concerned with prediction perfor-
mance, while real world applications often additionally consider the value of counterfactual
evaluation for oﬄine policy evaluation.

A key limitation of our study is that it is concerned with stationary datasets. Many real-
world contextual bandit applications involve nonstationary datasources. This limitation
is simply due to the nature of readily available public datasets. The lack of public CB
datasets as well as challenges in counterfactual evaluation of CB algorithms make a more
realistic study challenging, but we hope that an emergence of platforms (Agarwal et al.,
2016; Jamieson et al., 2015) to easily deploy CB algorithms will enable studies with real
CB datasets in the future.

19

Bietti, Agarwal and Langford

References

Y. Abbasi-Yadkori, D. P´al, and C. Szepesv´ari. Improved algorithms for linear stochastic

bandits. In Advances in Neural Information Processing Systems (NIPS), 2011.

A. Agarwal, M. Dud´ık, S. Kale, J. Langford, and R. E. Schapire. Contextual bandit learning
In Proceedings of the International Conference on Artiﬁcial

with predictable rewards.
Intelligence and Statistics (AISTATS), 2012.

A. Agarwal, D. Hsu, S. Kale, J. Langford, L. Li, and R. E. Schapire. Taming the monster: A
fast and simple algorithm for contextual bandits. arXiv preprint arXiv:1402.0555, 2014.

A. Agarwal, S. Bird, M. Cozowicz, L. Hoang, J. Langford, S. Lee, J. Li, D. Melamed,
arXiv preprint

G. Oshri, O. Ribas, et al. A multiworld testing decision service.
arXiv:1606.03966, 2016.

A. Agarwal, A. Krishnamurthy, J. Langford, H. Luo, et al. Open problem: First-order
regret bounds for contextual bandits. In Conference on Learning Theory (COLT), 2017.

S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoﬀs.
In Proceedings of the International Conference on Machine Learning (ICML), 2013.

Z. Allen-Zhu, S. Bubeck, and Y. Li. Make the minority great again: First-order regret
bound for contextual bandits. In Proceedings of the International Conference on Machine
Learning (ICML), 2018.

H. Bastani, M. Bayati, and K. Khosravi. Mostly exploration-free algorithms for contextual

bandits. arXiv preprint arXiv:1704.09011, 2017.

A. Blum, A. Kalai, and J. Langford. Beating the hold-out: Bounds for k-fold and progressive

cross-validation. In Conference on Learning Theory (COLT), 1999.

O. Chapelle and L. Li. An empirical evaluation of thompson sampling.

In Advances in

Neural Information Processing Systems (NIPS), 2011.

J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine Learning Research (JMLR), 12(Jul):
2121–2159, 2011.

M. Dudik, D. Hsu, S. Kale, N. Karampatziakis, J. Langford, L. Reyzin, and T. Zhang. Eﬃ-
cient optimal learning for contextual bandits. In Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), 2011a.

M. Dudik, J. Langford, and L. Li. Doubly robust policy evaluation and learning.
Proceedings of the International Conference on Machine Learning (ICML), 2011b.

In

D. Eckles and M. Kaptein. Thompson sampling with the online bootstrap. arXiv preprint

arXiv:1410.4009, 2014.

20

A Contextual Bandit Bake-off

D. J. Foster, A. Agarwal, M. Dud´ık, H. Luo, and R. E. Schapire. Practical contextual ban-
dits with regression oracles. In Proceedings of the International Conference on Machine
Learning (ICML), 2018.

S. Hanneke. Theory of disagreement-based active learning. Foundations and Trends in

Machine Learning, 7(2-3), 2014.

X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers,
et al. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the
Eighth International Workshop on Data Mining for Online Advertising, 2014.

D. J. Hsu. Algorithms for active learning. PhD thesis, UC San Diego, 2010.

T.-K. Huang, A. Agarwal, D. J. Hsu, J. Langford, and R. E. Schapire. Eﬃcient and
In Advances in Neural Information Processing

parsimonious agnostic active learning.
Systems (NIPS), 2015.

K. G. Jamieson, L. Jain, C. Fernandez, N. J. Glattard, and R. Nowak. Next: A system for
real-world development, evaluation, and application of active learning. In Advances in
Neural Information Processing Systems (NIPS), 2015.

S. M. Kakade and A. Tewari. On the generalization ability of online strongly convex pro-
gramming algorithms. In Advances in Neural Information Processing Systems (NIPS),
2009.

S. Kannan, J. Morgenstern, A. Roth, B. Waggoner, and Z. S. Wu. A smoothed analysis
of the greedy algorithm for the linear contextual bandit problem. In Advances in Neural
Information Processing Systems (NIPS), 2018.

N. Karampatziakis and J. Langford. Online importance weight aware updates. In Confer-

ence on Uncertainty in Artiﬁcial Intelligence (UAI), 2011.

A. Krishnamurthy, A. Agarwal, T.-K. Huang, H. Daume III, and J. Langford. Active
learning for cost-sensitive classiﬁcation. Journal of Machine Learning Research (JMLR),
20(65):1–50, 2019.

J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side

information. In Advances in Neural Information Processing Systems (NIPS), 2008.

L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to person-
alized news article recommendation. In Proceedings of the 19th international conference
on World wide web. ACM, 2010.

P. Massart, ´E. N´ed´elec, et al. Risk bounds for statistical learning. The Annals of Statistics,

34(5), 2006.

H. B. McMahan, G. Holt, D. Sculley, M. Young, D. Ebner, J. Grady, L. Nie, T. Phillips,
E. Davydov, D. Golovin, et al. Ad click prediction: a view from the trenches. In Proceed-
ings of the 19th ACM international conference on Knowledge discovery and data mining
(KDD), 2013.

21

Bietti, Agarwal and Langford

I. Osband and B. Van Roy. Bootstrapped thompson sampling and deep exploration. arXiv

preprint arXiv:1507.00300, 2015.

N. C. Oza and S. Russell. Online bagging and boosting. In Proceedings of the International

Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2001.

Z. Qin, V. Petricek, N. Karampatziakis, L. Li, and J. Langford. Eﬃcient online bootstrap-
ping for large scale learning. In Workshop on Parallel and Large-scale Machine Learning
(BigLearning@NIPS), 2013.

S. Ross, P. Mineiro, and J. Langford. Normalized online learning. In Conference on Uncer-

tainty in Artiﬁcial Intelligence (UAI), 2013.

D. J. Russo, B. Van Roy, A. Kazerouni, I. Osband, Z. Wen, et al. A tutorial on thompson

sampling. Foundations and Trends R(cid:13) in Machine Learning, 11(1):1–96, 2018.

T. Schnabel, A. Swaminathan, A. Singh, N. Chandak, and T. Joachims. Recommendations
In Proceedings of the International

as treatments: debiasing learning and evaluation.
Conference on Machine Learning (ICML), 2016.

A. Swaminathan and T. Joachims. The self-normalized estimator for counterfactual learn-

ing. In Advances in Neural Information Processing Systems (NIPS), 2015.

W. R. Thompson. On the likelihood that one unknown probability exceeds another in view

of the evidence of two samples. Biometrika, 25(3/4), 1933.

22

A Contextual Bandit Bake-off

Table 5: Statistically signiﬁcant win-loss diﬀerence with hyperparameters ﬁxed as in Ta-
ble 10, encodings ﬁxed to -1/0, on all held-out datasets or subsets with diﬀerent charac-
teristics: (a) number of actions K; (b) number of features d; (c) number of examples n;
(d) PV loss of the one-against-all (OAA) method. The corresponding table for all held-out
datasets is shown in Table 1(top left).

(cid:15)G
18
25
26
-6
-

(cid:15)G
10
21
17
-7
-

(cid:15)G
53
60
37
18
-

↓ vs → G RO C-nu B-g
22
-5
G
24
-
RO
25
C-nu
-5
-
-24
B-g
6
-25
(cid:15)G

-6
5
-
-25
-26

-
5
6
-22
-18

↓ vs → G RO C-nu B-g
15
1
G
14
-
RO
22
0
C-nu
-
-14
B-g
9
-9
(cid:15)G

-
-1
4
-15
-7

-4
0
-
-22
-13

(a) K ≥ 3 (left, 89 datasets), K ≥ 10 (right, 36 datasets)

↓ vs → G RO C-nu B-g
20
-5
G
26
-
RO
20
-8
C-nu
-
-26
B-g
7
-21
(cid:15)G

-1
8
-
-20
-17

-
5
1
-20
-10

↓ vs → G RO C-nu B-g
-5
G
-
RO
-1
C-nu
-8
B-g
-8
(cid:15)G

-6
1
-
-15
-7

7
8
15
-
6

-
5
6
-7
0

(b) d ≥ 100 (left, 76 datasets), d ≥ 10 000 (right, 41 datasets)

↓ vs → G RO C-nu B-g
38
-4
G
40
-
RO
9
-33
C-nu
-
-40
B-g
-18
-60
(cid:15)G

-
4
-23
-38
-53

23
33
-
-9
-37

↓ vs → G RO C-nu B-g
10
-3
G
14
-
RO
2
-17
C-nu
-
-14
B-g
-25
-36
(cid:15)G

16
17
-
-2
-30

-
3
-16
-10
-34

(c) n ≥ 1 000 (left, 119 datasets), n ≥ 10 000 (right, 43 datasets)

↓ vs → G RO C-nu B-g
40
1
G
36
-
RO
7
-26
C-nu
-
-36
B-g
-4
-43
(cid:15)G

↓ vs → G RO C-nu B-g
8
1
G
5
-
RO
-12
-12
C-nu
-
-5
B-g
-10
-16
(cid:15)G
(d) P VOAA ≤ 0.2 (left, 135 datasets), P VOAA ≤ 0.05 (right, 28 datasets)

(cid:15)G
36
43
24
4
-

-
-1
-25
-40
-36

25
26
-
-7
-24

-
-1
-14
-8
-15

14
12
-
12
-5

(cid:15)G
7
9
13
-9
-

(cid:15)G
0
8
7
-6
-

(cid:15)G
34
36
30
25
-

(cid:15)G
15
16
5
10
-

↓ vs → G RO C-nu B-g
5
2
G
4
-
RO
-1
-8
C-nu
-
-4
B-g
-11
-12
(cid:15)G

-
-2
-8
-5
-12

8
8
-
1
-12

(cid:15)G
12
12
12
11
-

(e) n ≥ 10 000 and P VOAA ≤ 0.1 (13 datasets)

23

Bietti, Agarwal and Langford

Appendix A. Datasets

This section gives some details on the cost-sensitive classiﬁcation datasets considered in our
study.

Multiclass classiﬁcation datasets. We consider 525 multiclass datasets from the openml.
org platform, including among others, medical, gene expression, text, sensory or synthetic
data. Table 6 provides some statistics about these datasets. These also include the 8 clas-
siﬁcation datasets considered in (Foster et al., 2018) from the UCI database. The full list
of datasets is given below.

Table 6: Statistics on number of multiclass datasets by number of examples, actions and
unique features, as well as by progressive validation 0-1 loss for the supervised one-against-
all online classiﬁer, in our collection of 525 multiclass datasets.

actions #
404
73
48

2
3-9
10+

examples #
94
270
133
28

≤ 102
102-103
103-105
> 105

features
≤ 50
51-100
101-1000
1000+

#
392
35
17
81

P VOAA
≤ 0.01
(0.01, 0.1]
(0.1, 0.2]
(0.2, 0.5]
> 0.5

#
10
88
103
273
51

Multilabel classiﬁcation datasets. We consider 5 multilabel datasets from the LibSVM
website6, listed in Table 7.

Table 7: List of multilabel datasets.

Dataset # examples # features # actions P VOAA
0.1664
mediamill
0.0446
rcv1
0.0066
scene
0.1661
tmc
0.2553
yeast

120
47,236
294
30,438
103

30,993
23,149
1,211
21,519
1,500

101
103
6
22
14

Cost-sensitive classiﬁcation datasets. For more general real-valued costs in [0, 1], we
use a modiﬁcation of the multilabel RCV1 dataset introduced in (Krishnamurthy et al.,
2019). Each example consists of a news article labeled with the topics it belongs to, in
a collection of 103 topics. Instead of ﬁxing the cost to 1 for incorrect topics, the cost is
deﬁned as the tree distance to the set of correct topics in a topic hierarchy.

We also include the learning-to-rank datasets considered in (Foster et al., 2018), where
we limit the number of documents (actions) per query, and consider all the training folds.
We convert relevance scores to losses in {0, 0.25, 0.5, 0.75, 1}, with 0 indicating a perfectly
relevant document, and 1 an irrelevant one. The datasets considered are the Microsoft

6. https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html

24

A Contextual Bandit Bake-off

Learning to Rank dataset, variant MSLR-30K at https://www.microsoft.com/en-us/
research/project/mslr/, and the Yahoo! Learning to Rank Challenge V2.0, variant C14B
at https://webscope.sandbox.yahoo.com/catalog.php?datatype=c. Details are shown
in Table 8. We note that for these datasets we consider action-dependent features, with a
ﬁxed parameter vector for all documents.

Table 8: Learning to rank datasets.

Dataset
MSLR-30K
Yahoo

# examples # features max # documents P VOAA
0.7892
0.5876

31,531
36,251

136
415

10
6

List of multiclass datasets. The datasets we used can be accessed at https://www.
openml.org/d/<id>, with id in the following list:

3, 6, 8, 10, 11, 12, 14, 16, 18, 20, 21, 22, 23, 26, 28, 30, 31, 32, 36, 37, 39, 40, 41, 43,
44, 46, 48, 50, 53, 54, 59, 60, 61, 62, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161,
162, 180, 181, 182, 183, 184, 187, 189, 197, 209, 223, 227, 273, 275, 276, 277, 278, 279, 285,
287, 292, 293, 294, 298, 300, 307, 310, 312, 313, 329, 333, 334, 335, 336, 337, 338, 339, 343,
346, 351, 354, 357, 375, 377, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395,
396, 397, 398, 399, 400, 401, 444, 446, 448, 450, 457, 458, 459, 461, 462, 463, 464, 465, 467,
468, 469, 472, 475, 476, 477, 478, 479, 480, 554, 679, 682, 683, 685, 694, 713, 714, 715, 716,
717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735,
736, 737, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756,
758, 759, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777,
778, 779, 780, 782, 783, 784, 785, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 799,
800, 801, 803, 804, 805, 806, 807, 808, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821,
822, 823, 824, 825, 826, 827, 828, 829, 830, 832, 833, 834, 835, 836, 837, 838, 841, 843, 845,
846, 847, 848, 849, 850, 851, 853, 855, 857, 859, 860, 862, 863, 864, 865, 866, 867, 868, 869,
870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 884, 885, 886, 888, 891, 892,
893, 894, 895, 896, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914,
915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 931, 932, 933, 934,
935, 936, 937, 938, 941, 942, 943, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956,
958, 959, 962, 964, 965, 969, 970, 971, 973, 974, 976, 977, 978, 979, 980, 983, 987, 988, 991,
994, 995, 996, 997, 1004, 1005, 1006, 1009, 1011, 1012, 1013, 1014, 1015, 1016, 1019, 1020,
1021, 1022, 1025, 1026, 1036, 1038, 1040, 1041, 1043, 1044, 1045, 1046, 1048, 1049, 1050,
1054, 1055, 1056, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1071,
1073, 1075, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1100,
1104, 1106, 1107, 1110, 1113, 1115, 1116, 1117, 1120, 1121, 1122, 1123, 1124, 1125, 1126,
1127, 1128, 1129, 1130, 1131, 1132, 1133, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142,
1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157,
1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1169, 1216, 1217, 1218, 1233, 1235,
1236, 1237, 1238, 1241, 1242, 1412, 1413, 1441, 1442, 1443, 1444, 1449, 1451, 1453, 1454,
1455, 1457, 1459, 1460, 1464, 1467, 1470, 1471, 1472, 1473, 1475, 1481, 1482, 1483, 1486,
1487, 1488, 1489, 1496, 1498, 1590.

25

Bietti, Agarwal and Langford

Table 9: Choices of hyperparameters and reduction for each method. Fixed choices of hy-
perparameters for -1/0 encodings are in bold. These were obtained for each method with
an instant-runoﬀ voting mechanism on 200 of the multiclass datasets with -1/0 encoding,
where each dataset ranks hyperparameter choices according to the diﬀerence between sig-
niﬁcant wins and losses against all other choices (the vote of each dataset is divided by the
number of tied choices ranked ﬁrst). Table 10 shows optimized choices of hyperparameters
for diﬀerent encoding settings used in our study.

Name
G

Method
Greedy

R/RO RegCB-elim/RegCB-opt

C-nu

Cover-NU

C-u

B/B-g
(cid:15)G

Cover

Bag/Bag-greedy
(cid:15)-greedy

A

active (cid:15)-greedy

Hyperparameters
-
C0 ∈ 10−{1,2,3}
N ∈ {4, 8, 16}
ψ ∈ {0.01, 0.1, 1}
N ∈ {4, 8, 16}
ψ ∈ {0.01, 0.1, 1}
N ∈ {4, 8, 16}
(cid:15) ∈ {0.02, 0.05, 0.1}
(cid:15) ∈ {0.02, 1}
C0 ∈ 10−{2,4,6}

Reduction
IWR
-

IPS/DR

IPS/DR

IPS/DR/IWR
IPS/DR/IWR

IPS/DR/IWR

Appendix B. Evaluation Details

B.1 Algorithms and Hyperparameters

We ran each method on every dataset with the following hyperparameters:

• algorithm-speciﬁc hyperparameters, shown in Table 9.

• 9 choices of learning rates, on a logarithmic grid from 0.001 to 10 (see Section 2.4).

• 3 choices of reductions: IPS, DR and IWR (see Section 2.3). Note that these mainly
apply to methods that reduce to oﬀ-policy optimization (i.e., ((cid:15)-)greedy and bagging),
and to some extent, methods that reduce to cost-sensitive classiﬁcation (i.e., cover
and active (cid:15)-greedy, though the IWR reduction is heuristic in this case). Both RegCB
variants directly reduce to regression.

• 3 choices of loss encodings: 0/1, -1/0 and 9/10 (see Eq. (7)). 0/1 and -1/0 encodings
are typically a design choice, while the experiments with 9/10 are aimed at assessing
some robustness to loss range.

B.2 Additional Evaluation Results

This sections provides additional experimental results, and more detailed win/loss statistics
for tables in the main paper, showing both signiﬁcant wins and signiﬁcant losses, rather
than just their diﬀerence.

26

A Contextual Bandit Bake-off

Table 10: Optimized choices of hyperparameters for diﬀerent encoding settings, obtained
using the voting mechanism described in Table 9: -1/0 (same as bold choices in Table 9, used
in Tables 1(top left), 2ce, 5, 11a, 12a, 13a and in the ﬁgures); 0/1 (used in Tables 1(bottom
left), 2abd, 11b, 12b, 13b, 14).

Algorithm
G
R/RO
C-nu
C-u
B
B-g
(cid:15)G
A

-1/0
-
C0 = 10−3
N = 4, ψ = 0.1, DR
N = 4, ψ = 0.1, IPS
N = 4, IWR
N = 4, IWR
(cid:15) = 0.02, IWR

0/1
-
C0 = 10−3
N = 4, ψ = 0.01, DR
N = 4, ψ = 0.1, DR
N = 16, IWR
N = 8, IWR
(cid:15) = 0.02, IWR

(cid:15) = 0.02, C0 = 10−6, IWR (cid:15) = 0.02, C0 = 10−6, IWR

Table 11: Statistically signiﬁcant wins / losses of all methods on the 324 held-out multiclass
classiﬁcation datasets. Hyperparameters are ﬁxed as given in Table 10.

↓ vs →
G
R
RO
C-nu
B
B-g
(cid:15)G
C-u
A

↓ vs →
G
R
RO
C-nu
B
B-g
(cid:15)G
C-u
A

G
-
25 / 22
16 / 8
33 / 41
15 / 77
16 / 65
6 / 59
10 / 159
10 / 33

G
-
66 / 27
69 / 5
51 / 36
38 / 75
39 / 72
21 / 69
30 / 151
19 / 36

R
22 / 25
-
23 / 16
31 / 49
19 / 74
20 / 61
12 / 70
11 / 159
15 / 48

R
27 / 66
-
38 / 15
24 / 42
12 / 87
16 / 83
19 / 108
4 / 172
22 / 77

RO
8 / 16
16 / 23
-
21 / 47
13 / 76
13 / 60
4 / 71
6 / 166
7 / 46

RO
5 / 69
15 / 38
-
13 / 59
7 / 109
9 / 105
3 / 121
3 / 175
4 / 89

C-nu
41 / 33
49 / 31
47 / 21
-
26 / 66
31 / 51
19 / 75
10 / 159
29 / 50

B
77 / 15
74 / 19
76 / 13
66 / 26
-
32 / 11
41 / 46
16 / 126
54 / 26
(a) -1/0 encoding

C-nu
36 / 51
42 / 24
59 / 13
-
27 / 83
29 / 72
25 / 96
6 / 170
31 / 63

B
75 / 38
87 / 12
109 / 7
83 / 27
-
34 / 21
49 / 59
14 / 131
67 / 37
(b) 0/1 encoding

B-g
65 / 16
61 / 20
60 / 13
51 / 31
11 / 32
-
31 / 49
11 / 130
42 / 27

(cid:15)G
59 / 6
70 / 12
71 / 4
75 / 19
46 / 41
49 / 31
-
14 / 125
37 / 2

C-u
159 / 10
159 / 11
166 / 6
159 / 10
126 / 16
130 / 11
125 / 14
-
152 / 9

B-g
72 / 39
83 / 16
105 / 9
72 / 29
21 / 34
-
46 / 65
18 / 129
54 / 42

(cid:15)G
69 / 21
108 / 19
121 / 3
96 / 25
59 / 49
65 / 46
-
28 / 122
43 / 3

C-u
151 / 30
172 / 4
175 / 3
170 / 6
131 / 14
129 / 18
122 / 28
-
151 / 22

A
33 / 10
48 / 15
46 / 7
50 / 29
26 / 54
27 / 42
2 / 37
9 / 152
-

A
36 / 19
77 / 22
89 / 4
63 / 31
37 / 67
42 / 54
3 / 43
22 / 151
-

Extended tables. Tables 11 and 12 are extended versions of Table 1, showing both sig-
niﬁcant wins and loss, more methods, and separate statistics for multiclass and multilabel
datasets. In particular, we can see that both variants of RegCB become even more com-
petitive against all other methods when using 0/1 encodings. Table 14 extends Table 2(a)
with additional methods. Table 15 is a more detailed win/loss version of Table 3, and
additionally shows statistics for 0/1 encodings.

We also show separate statistics in Table 13 for the 8 datasets from the UCI repository
considered in (Foster et al., 2018), which highlight that Greedy can outperform RegCB on
some of these datasets, and that the optimistic variant of RegCB is often superior to the
elimination variant. We note that our experimental setup is quite diﬀerent from Foster et al.
(2018), who consider batch learning on an doubling epoch schedule, which might explain
some of the diﬀerences in the results.

27

Bietti, Agarwal and Langford

Table 12: Statistically signiﬁcant wins / losses of all methods on the 5 multilabel classiﬁca-
tion datasets. Hyperparameters are ﬁxed as given in Table 10.

↓ vs → G
-
G
2 / 2
R
0 / 1
RO
1 / 3
C-nu
2 / 2
B
2 / 3
B-g
1 / 2
(cid:15)G
1 / 4
C-u
1 / 2
A

↓ vs → G
-
G
1 / 3
R
2 / 2
RO
3 / 1
C-nu
0 / 4
B
1 / 4
B-g
0 / 4
(cid:15)G
0 / 5
C-u
0 / 3
A

R
2 / 2
-
2 / 2
0 / 2
0 / 3
0 / 3
2 / 3
0 / 5
2 / 3

R
3 / 1
-
3 / 0
4 / 1
2 / 2
3 / 2
2 / 2
1 / 4
2 / 2

RO C-nu
3 / 1
1 / 0
2 / 0
2 / 2
2 / 2
-
-
2 / 2
1 / 3
2 / 2
1 / 3
1 / 3
2 / 3
1 / 2
0 / 5
1 / 4
2 / 3
1 / 2

B
2 / 2
3 / 0
2 / 2
3 / 1
-
1 / 1
2 / 3
0 / 5
2 / 3
(a) -1/0 encoding

RO C-nu
1 / 3
2 / 2
1 / 4
0 / 3
1 / 2
-
-
2 / 1
1 / 4
0 / 5
1 / 3
0 / 4
0 / 4
1 / 3
1 / 4
0 / 5
1 / 4
1 / 3

B
4 / 0
2 / 2
5 / 0
4 / 1
-
2 / 0
2 / 1
1 / 2
3 / 1
(b) 0/1 encoding

B-g
3 / 2
3 / 0
3 / 1
3 / 1
1 / 1
-
3 / 2
1 / 4
3 / 2

B-g
4 / 1
2 / 3
4 / 0
3 / 1
0 / 2
-
2 / 2
1 / 4
3 / 1

(cid:15)G
2 / 1
3 / 2
2 / 1
3 / 2
3 / 2
2 / 3
-
1 / 4
0 / 1

(cid:15)G
4 / 0
2 / 2
3 / 1
4 / 0
1 / 2
2 / 2
-
1 / 4
1 / 0

C-u
4 / 1
5 / 0
4 / 1
5 / 0
5 / 0
4 / 1
4 / 1
-
4 / 1

C-u
5 / 0
4 / 1
5 / 0
4 / 1
2 / 1
4 / 1
4 / 1
-
5 / 0

A
2 / 1
3 / 2
2 / 1
3 / 2
3 / 2
2 / 3
1 / 0
1 / 4
-

A
3 / 0
2 / 2
3 / 1
4 / 1
1 / 3
1 / 3
0 / 1
0 / 5
-

Varying C0 in RegCB-opt and active (cid:15)-greedy. Figure 4 shows a comparison be-
tween RegCB-opt and Greedy or Cover-NU on our corpus, for diﬀerent values of C0, which
controls the level of exploration through the width of conﬁdence bounds. Figure 5 shows the
improvements that the active (cid:15)-greedy algorithm can achieve compared to (cid:15)-greedy, under
diﬀerent settings.

Counterfactual evaluation. Figure 6 extends Figure 3 to include all algorithms, and
additionally shows results of using IPS estimates directly on the losses (cid:96)t(at) instead of
rewards 1 − (cid:96)t(at), which tend to be signiﬁcantly worse.

B.3 Shared baseline parameterization

We also experimented with the use of an action-independent additive baseline term in our
loss estimators, which can help learn better estimates with fewer samples in some situations.
In this case the regressors take the form f (x, a) = θ0+θ(cid:62)
a x (DR).
In order to learn the baseline term more quickly, we propose to use a separate online update
for the parameters θ0 or φ0 to regress on observed losses, followed by an online update on
the residual for the action-dependent part. We scale the step-size of these baseline updates
by the largest observed magnitude of the loss, in order to adapt to the observed loss range
for normalized updates (Ross et al., 2013).

a x (IWR) or ˆ(cid:96)(x, a) = φ0+φ(cid:62)

28

A Contextual Bandit Bake-off

Table 13: Statistically signiﬁcant wins / losses of all methods on the 8 classiﬁcation datasets
from the UCI repository considered in (Foster et al., 2018). Hyperparameters are ﬁxed as
given in Table 10.

↓ vs → G
-
G
0 / 4
R
1 / 2
RO
2 / 5
C-nu
0 / 5
B
1 / 4
B-g
0 / 6
(cid:15)G
0 / 6
C-u
0 / 5
A

↓ vs → G
-
G
5 / 1
R
5 / 0
RO
2 / 2
C-nu
2 / 3
B
1 / 3
B-g
1 / 3
(cid:15)G
0 / 7
C-u
1 / 2
A

R
4 / 0
-
1 / 0
1 / 3
1 / 4
1 / 3
2 / 4
0 / 6
2 / 2

R
1 / 5
-
1 / 0
0 / 5
0 / 6
0 / 6
0 / 6
0 / 8
0 / 5

RO C-nu
5 / 2
2 / 1
3 / 1
0 / 1
4 / 1
-
-
1 / 4
1 / 3
0 / 5
2 / 2
0 / 5
2 / 3
0 / 5
0 / 7
0 / 6
3 / 2
0 / 3

B
5 / 0
4 / 1
5 / 0
3 / 1
-
2 / 0
2 / 3
0 / 6
2 / 2
(a) -1/0 encoding

RO C-nu
2 / 2
0 / 5
5 / 0
0 / 1
5 / 0
-
-
0 / 5
1 / 2
0 / 6
0 / 3
0 / 6
1 / 2
0 / 7
0 / 7
0 / 7
1 / 1
0 / 6

B
3 / 2
6 / 0
6 / 0
2 / 1
-
0 / 3
3 / 1
0 / 7
3 / 0
(b) 0/1 encoding

B-g
4 / 1
3 / 1
5 / 0
2 / 2
0 / 2
-
1 / 4
0 / 6
2 / 2

B-g
3 / 1
6 / 0
6 / 0
3 / 0
3 / 0
-
3 / 1
0 / 7
3 / 0

(cid:15)G
6 / 0
4 / 2
5 / 0
3 / 2
3 / 2
4 / 1
-
0 / 6
2 / 0

(cid:15)G
3 / 1
6 / 0
7 / 0
2 / 1
1 / 3
1 / 3
-
0 / 7
1 / 0

C-u
6 / 0
6 / 0
6 / 0
7 / 0
6 / 0
6 / 0
6 / 0
-
6 / 0

C-u
7 / 0
8 / 0
7 / 0
7 / 0
7 / 0
7 / 0
7 / 0
-
7 / 0

A
5 / 0
2 / 2
3 / 0
2 / 3
2 / 2
2 / 2
0 / 2
0 / 6
-

A
2 / 1
5 / 0
6 / 0
1 / 1
0 / 3
0 / 3
0 / 1
0 / 7
-

Table 14: Progressive validation loss for RCV1 with real-valued costs. Same as Table 2(a),
but with all methods. Hyperparameters are ﬁxed as given in Table 10. The learning rate
is optimized once on the original dataset, and we show mean and standard error based on
10 diﬀerent random reshuﬄings of the dataset.

G
0.215 ± 0.010

R
0.408 ± 0.003

RO
0.225 ± 0.008

C-nu
0.215 ± 0.006

C-u
0.570 ± 0.023

B
0.256 ± 0.006

B-g
0.251 ± 0.005

(cid:15)G
0.230 ± 0.009

A
0.230 ± 0.010

Such an additive baseline can be helpful to quickly adapt to a constant loss estimate
thanks to the separate online update. This appears particularly useful with the -1/0 en-
coding, for which the initialization at 0 may give pessimistic loss estimates which can be
damaging in particular for the greedy method, that often gets some initial exploration from
an optimistic cost encoding. This can be seen in Figure 7(top). Table 16 shows that op-
timizing over the use of baseline on each dataset can improve the performance of Greedy
and RegCB-opt when compared to other methods such as Cover-NU.

In an online learning setting, baseline can also help to quickly reach an unknown target
range of loss estimates. This is demonstrated in Figure 7(bottom), where the addition

29

Bietti, Agarwal and Langford

Table 15: Impact of reductions for Bag (left) and (cid:15)-greedy (right), with hyperparameters
optimized and encoding ﬁxed to -1/0 or 0/1. Extended version of Table 3. Each (row,
column) entry shows the statistically signiﬁcant wins and losses of row against column.

↓ vs →
ips
dr
iwr

ips
-
72 / 30
84 / 25

dr
30 / 72
-
58 / 30

iwr
25 / 84
30 / 58
-

↓ vs →
ips
dr
iwr

ips
-
22 / 85
149 / 16

dr
85 / 22
-
164 / 9

iwr
16 / 149
9 / 164
-

(a) -1/0 encoding

↓ vs →
ips
dr
iwr

ips
-
240 / 46
245 / 17

dr
46 / 240
-
97 / 33

iwr
17 / 245
33 / 97
-

↓ vs →
ips
dr
iwr

ips
-
136 / 40
182 / 36

dr
40 / 136
-
148 / 23

iwr
36 / 182
23 / 148
-

(b) 0/1 encoding

Figure 4: Comparison of RegCB-opt with Greedy (top) and Cover-NU (bottom) for diﬀerent
values of C0. Hyperparameters for Greedy and Cover-NU ﬁxed as in Table 9. Encoding
ﬁxed to -1/0. The plots consider normalized loss on held-out datasets, with red points
indicating signiﬁcant wins.

of baseline is shown to help various methods with 9/10 encodings on a large number of
datasets. We do not evaluate RegCB for 9/10 encodings as it needs a priori known upper
and lower bounds on costs.

30

Table 16: Statistically signiﬁcant wins / losses of all methods on held-out datasets, with -1/0
encoding and ﬁxed hyperparameters, except for baseline, which is optimized on each dataset
together with the learning rate. The ﬁxed hyperparameters are shown in the table below,
and were selected with the same voting approach described in Table 9. This optimization
beneﬁts Greedy and RegCB-opt in particular.

↓ vs →
G
R
RO
C-nu
B
B-g
(cid:15)G
C-u
A

G
-
14 / 32
16 / 14
15 / 60
13 / 88
16 / 69
1 / 78
1 / 178
6 / 49

R
32 / 14
-
36 / 9
25 / 43
19 / 69
24 / 52
14 / 66
9 / 167
19 / 42

RO
14 / 16
9 / 36
-
13 / 64
10 / 90
10 / 66
3 / 84
1 / 187
5 / 61

C-nu
60 / 15
43 / 25
64 / 13
-
30 / 58
41 / 34
24 / 60
6 / 163
36 / 42

B
88 / 13
69 / 19
90 / 10
58 / 30
-
33 / 10
35 / 54
7 / 133
59 / 29

B-g
69 / 16
52 / 24
66 / 10
34 / 41
10 / 33
-
18 / 57
2 / 147
38 / 37

(cid:15)G
78 / 1
66 / 14
84 / 3
60 / 24
54 / 35
57 / 18
-
10 / 129
44 / 3

C-u
178 / 1
167 / 9
187 / 1
163 / 6
133 / 7
147 / 2
129 / 10
-
164 / 5

A
49 / 6
42 / 19
61 / 5
42 / 36
29 / 59
37 / 38
3 / 44
5 / 164
-

A Contextual Bandit Bake-off

Algorithm
G
R/RO
C-nu
C-u
B
B-g
(cid:15)G
A

Hyperparameters
-
C0 = 10−3
N = 16, ψ = 0.1, DR
N = 4, ψ = 0.1, IPS
N = 4, IWR
N = 4, IWR
(cid:15) = 0.02, IWR
(cid:15) = 0.02, C0 = 10−6, IWR

31

Bietti, Agarwal and Langford

(a) DR

(b) IWR

32

Figure 5: Improvements to (cid:15)-greedy from our active learning strategy. Encoding ﬁxed to
-1/0. The IWR implementation described in Section C.1 still manages to often outperform
(cid:15)-greedy, despite only providing an approximation to Algorithm 6.

A Contextual Bandit Bake-off

(a) all multiclass datasets

(b) n ≥ 10 000 only

(c) all multiclass datasets

(d) n ≥ 10 000 only

Figure 6: Errors of IPS counterfactual estimates for the uniform random policy using ex-
ploration logs collected by various algorithms on multiclass datasets (extended version of
Figure 3). The boxes show quartiles (with the median shown as a blue line) of the dis-
tribution of squared errors across all multiclass datasets or only those with at least 10 000
examples. The logs are obtained by running each algorithm with -1/0 encodings, ﬁxed
hyperparameters from Table 9, and the best learning rate on each dataset according to pro-
gressive validation loss. The top plots consider IPS with reward estimates (as in Figure 3),
while the bottom plots consider IPS on the loss.

33

Bietti, Agarwal and Langford

Figure 7: (top) Impact of baseline on diﬀerent algorithms with encoding ﬁxed to -1/0; for
Greedy and RegCB-opt, it can signiﬁcantly help against pessimistic initial costs in some
datasets. Hyperparameters ﬁxed as in Table 9. (bottom) Baseline improves robustness to
the range of losses. The plots consider normalized loss on held-out datasets, with red points
indicating signiﬁcant wins.

34

A Contextual Bandit Bake-off

Algorithm 6 Active (cid:15)-greedy
π1; (cid:15); C0 > 0.
explore(xt):

At = {a : loss_diff(πt, xt, a) ≤ ∆t,C0};
1{a ∈ At} + (1 − (cid:15)|At|
pt(a) = (cid:15)
K
return pt;

K ) 1{πt(xt) = a};

learn(xt, at, (cid:96)t(at), pt):

ˆ(cid:96)t = estimator(xt, at, (cid:96)t(at), pt(at));

ˆct(a) =

(cid:40)ˆ(cid:96)t(a),
1,

if pt(a) > 0
otherwise.

πt+1 = csc_oracle(πt, xt, ˆct);

Appendix C. Active (cid:15)-greedy: Practical Algorithm and Analysis

This section presents our active (cid:15)-greedy method, a variant of (cid:15)-greedy that reduces the
amount of uniform exploration using techniques from active learning. Section C.1 introduces
the practical algorithm,7 while Section C.2 provides a theoretical analysis of the method,
showing that it achieves a regret of O(T 1/3) under speciﬁc favorable settings, compared
to O(T 2/3) for vanilla (cid:15)-greedy.

C.1 Algorithm

The simplicity of the (cid:15)-greedy method described in Section 3.1 often makes it the method
of choice for practitioners. However, the uniform exploration over randomly selected actions
can be quite ineﬃcient and costly in practice. A natural consideration is to restrict this
randomization over actions which could plausibly be selected by the optimal policy π∗ =
arg minπ∈Π L(π), where L(π) = E(x,(cid:96))∼D[(cid:96)(π(x))] is the expected loss of a policy π.

To achieve this, we use techniques from disagreement-based active learning (Hanneke,
2014; Hsu, 2010). After observing a context xt, for any action a, if we can ﬁnd a policy π
that would choose this action (π(xt) = a) instead of the empirically best action πt(xt),
while achieving a small loss on past data, then there is disagreement about how good
such an action is, and we allow exploring it. Otherwise, we are conﬁdent that the best
policy would not choose this action, thus we avoid exploring it, and assign it a high cost.
The resulting method is in Algorithm 6. Like RegCB, the method requires a known loss
range [cmin, cmax], and assigns a loss cmax to such unexplored actions (we consider the
range [0, 1] in Algorithm 6 for simplicity). The disagreement test we use is based on empirical
loss diﬀerences, similar to the Oracular CAL active learning method (Hsu, 2010), denoted
loss_diff, together with a threshold:

7. Our implementation is available in the following branch of Vowpal Wabbit: https://github.com/

albietz/vowpal_wabbit/tree/bakeoff.

(cid:114)

∆t,C0 =

C0

K log t
(cid:15)t

+ C0

K log t
(cid:15)t

.

35

Bietti, Agarwal and Langford

A practical implementation of loss_diff for an online setting is given below. We analyze a
theoretical form of this algorithm in Section C.2, showing a formal version of the following
theorem:

Theorem 1 With high-probability, and under favorable conditions on disagreement and on
the problem noise, active (cid:15)-greedy achieves expected regret O(T 1/3).

Note that this data-dependent guarantee improves on worst-case guarantees achieved by
the optimal algorithms Agarwal et al. (2014); Dudik et al. (2011a). In the extreme case
where the loss of any suboptimal policy is bounded away from that of π∗, we show that
our algorithm can achieve constant regret. While active learning algorithms suggest that
data-dependent thresholds ∆t can yield better guarantees (e.g., Huang et al., 2015), this
may require more work in our setting due to open problems related to data-dependent
guarantees for contextual bandits (Agarwal et al., 2017). In a worst-case scenario, active
(cid:15)-greedy behaves similarly to (cid:15)-greedy (Langford and Zhang, 2008), achieving an O(T 2/3)
expected regret with high probability.

Practical implementation of the disagreement test. We now present a practical
way to implement the disagreement tests in the active (cid:15)-greedy method, in the context of
online cost-sensitive classiﬁcation oracles based on regression, as in Vowpal Wabbit. This
corresponds to the loss_diff method in Algorithm 6.

Let ˆLt−1(π) denote the empirical loss of policy π on the (biased) sample of cost-sensitive
examples collected up to time t − 1 (see Section C.2 for details). After observing a context
xt, we want to estimate

loss_diff(πt, xt, ¯a) ≈ ˆLt−1(πt,¯a) − ˆLt−1(πt),

for any action ¯a, where

πt = arg min

ˆLt−1(π)

π

πt,¯a = arg min

π:π(xt)=¯a

ˆLt−1(π).

In our online setup, we take πt to be the current online policy (as in Algorithm 6), and
we estimate the loss diﬀerence by looking at how many online CSC examples of the form
¯c := (1{a (cid:54)= ¯a})a=1..K are needed (or the importance weight on such an example) in order
to switch prediction from πt(xt) to ¯a. If we denote this importance weight by τ¯a, then we
can estimate ˆLt−1(πt,¯a) − ˆLt−1(πt) ≈ τ¯a/t.

Computing τ¯a for IPS/DR.
In the case of IPS/DR, we use an online CSC oracle,
which is based on K regressors f (x, a) in VW, each predicting the cost for an action a.
Let ft be the current regressors for policy πt, yt(a) := ft(xt, a), and denote by st(a) the
sensitivity of regressor ft(·, a) on example (xt, ¯c(a)). This sensitivity is essentially deﬁned
to be the derivative with respect to an importance weight w of the prediction y(cid:48)(a) obtained
from the regressor after an online update (xt, ¯c(a)) with importance weight w. A similar
quantity has been used, e.g., by Huang et al. (2015); Karampatziakis and Langford (2011);
Krishnamurthy et al. (2019). Then, the predictions on actions ¯a and a cross when the

36

A Contextual Bandit Bake-off

importance weight w satisﬁes yt(¯a) − st(¯a)w = yt(a) + st(a)w. Thus, the importance weight
required for action ¯a to be preferred (i.e., smaller predicted loss) to action a is given by:

wa

¯a =

yt(¯a) − yt(a)
st(¯a) + st(a)

.

Action ¯a will thus be preferred to all other actions when using an importance weight τ¯a =
maxa wa
¯a.

Computing τ¯a for IWR. Although Algorithm 6 and the theoretical analysis require
CSC in order to assign a loss of 1 to unexplored actions, and hence does not directly
support IWR, we can consider an approximation which leverages the beneﬁts of IWR by
performing standard IWR updates as in (cid:15)-greedy, while exploring only on actions that pass
a similar disagreement test. In this case, we estimate τ¯a as the importance weight on an
online regression example (xt, 0) for the regressor ft(·, ¯a), needed to switch prediction to
¯a. If st(¯a) is the sensitivity for such an example, we have τ¯a = (yt(¯a) − y∗
t )/st(¯a), where
y∗
t = mina yt(a).

C.2 Theoretical Analysis

This section presents a theoretical analysis of the active (cid:15)-greedy method introduced in
Section C.1. We begin by presenting the analyzed version of the algorithm together with
deﬁnitions in Section C.2.1. Section C.2.2 then studies the correctness of the method,
showing that with high probability, the actions chosen by the optimal policy are always
explored, and that policies considered by the algorithm are always as good as those obtained
under standard (cid:15)-greedy exploration. This section also introduces a Massart-type low-noise
condition similar to the one considered by Krishnamurthy et al. (2019) for cost-sensitive
classiﬁcation. Finally, Section C.2.3 provides a regret analysis of the algorithm, both in the
worst case and under disagreement conditions together with the Massart noise condition.
In particular, a formal version of Theorem 1 is given by Theorem 8, and a more extreme
but informative situation is considered in Proposition 9, where our algorithm can achieve
constant regret.

C.2.1 Algorithm and definitions

We consider a version of the active (cid:15)-greedy strategy that is more suitable for theoretical
analysis, given in Algorithm 7. This method considers exact CSC oracles, as well as a CSC
oracle with one constraint on the policy (π(xt) = a in Eq.(10)). The threshold ∆t is deﬁned
later in Section C.2.2. Computing it would require some knowledge about the size of the
policy class, which we avoid by introducing a parameter C0 in the practical variant. The
disagreement strategy is based on the Oracular CAL active learning method of Hsu (2010),
which tests for disagreement using empirical error diﬀerences, and considers biased samples
when no label is queried. Here, similar tests are used to decide which actions should be
explored, in the diﬀerent context of cost-sensitive classiﬁcation, and the unexplored actions
are assigned a loss of 1, making the empirical sample biased ( ˆZT in Algorithm 7).
Deﬁnitions. Deﬁne ZT = {(xt, (cid:96)t)}t=1..T ⊂ X × RK, ˜ZT = {(xt, ˜(cid:96)t)}t=1..T (biased sam-
ple) and ˆZT = {(xt, ˆ(cid:96)t)}t=1..T (IPS estimate of biased sample), where (cid:96)t ∈ [0, 1]K is the

37

Bietti, Agarwal and Langford

Algorithm 7 active (cid:15)-greedy: analyzed version

Input: exploration probability (cid:15).
Initialize: ˆZ0 := ∅.
for t = 1, . . . do

Observe context xt. Let

πt := arg min

L(π, ˆZt−1)

π

πt,a := arg min

L(π, ˆZt−1)

π:π(xt)=a

At := {a : L(πt,a, ˆZt−1) − L(πt, ˆZt−1) ≤ ∆t}

(10)

(11)

(8)

(9)

Let

pt(a) =






1 − (|At| − 1)(cid:15)/K,
(cid:15)/K,
0,

if a = πt(xt)
if a ∈ At \ {πt(xt)}
otherwise.

Play action at ∼ pt, observe (cid:96)t(at) and set ˆZt = ˆZt−1 ∪ {(xt, ˆ(cid:96)t)}, where ˆ(cid:96)t is deﬁned
in (9).
end for

(unobserved) loss vector at time t and

˜(cid:96)t(a) =

(cid:40)

(cid:96)t(a),
1,

if a ∈ At
o/w

ˆ(cid:96)t(a) =

(cid:40) 1{a=at}

pt(at) (cid:96)t(at),
1,

if a ∈ At
o/w.

L(π, Z) =

(cid:88)

c(π(x)).

1
|Z|

(x,c)∈Z

For any set Z ⊂ X × RK deﬁned as above, we denote, for π ∈ Π,

We then deﬁne the empirical losses LT (π) := L(π, ZT ), ˆLT (π) := L(π, ˆZT ) and ˜LT (π) :=
L(π, ˜ZT ). Let L(π) := E(x,(cid:96))∼D[(cid:96)(π(x))] be the expected loss of policy π, and π∗ :=
arg minπ∈Π L(π). We also deﬁne ρ(π, π(cid:48)) := Px(π(x) (cid:54)= π(cid:48)(x)), the expected disagreement
between policies π and π(cid:48), where Px denotes the marginal distribution of D on contexts.

C.2.2 Correctness

We begin by stating a lemma that controls deviations of empirical loss diﬀerences, which
relies on Freedman’s inequality for martingales (see, e.g., Kakade and Tewari, 2009, Lemma
3).

38

A Contextual Bandit Bake-off

Lemma 2 (Deviation bounds) With probability 1 − δ, the following event holds: for all
π ∈ Π, for all T ≥ 1,

|( ˆLT (π) − ˆLT (π∗)) − ( ˜LT (π) − ˜LT (π∗))| ≤

(cid:114)

2Kρ(π, π∗)eT
(cid:15)

+

(cid:18) K
(cid:15)

(cid:19)

+ 1

eT

|(LT (π) − LT (π∗)) − (L(π) − L(π∗))| ≤ (cid:112)ρ(π, π∗)eT + 2eT ,

(12)

(13)

where eT = log(2|Π|/δT )/T and δT = δ/(T 2 + T ). We denote this event by E in what
follows.

Proof We prove the result using Freedman’s inequality (see, e.g., Kakade and Tewari,
2009, Lemma 3), which controls deviations of a sum using the conditional variance of each
term in the sum and an almost sure bound on their magnitude, along with a union bound.

For (12), let ( ˆLT (π) − ˆLT (π∗)) − ( ˜LT (π) − ˜LT (π∗)) = 1
T

(cid:80)T

t=1 Rt, with

Rt = ˆ(cid:96)t(π(xt)) − ˆ(cid:96)t(π∗(xt)) − (˜(cid:96)t(π(xt)) − ˜(cid:96)t(π∗(xt))).

We deﬁne the σ-ﬁelds Ft := σ({xi, (cid:96)i, ai}t

i=1). Note that Rt is Ft-measurable and

E[ˆ(cid:96)t(π(xt)) − ˆ(cid:96)t(π∗(xt))|xt, (cid:96)t] = ˜(cid:96)t(π(xt)) − ˜(cid:96)t(π∗(xt)),

so that E[Rt|Ft−1] = E[E[Rt|xt, (cid:96)t]|Ft−1] = 0. Thus, (Rt)t≥1 is a martingale diﬀerence
sequence adapted to the ﬁltration (Ft)t≥1. We have

|Rt| ≤ |ˆ(cid:96)t(π(xt)) − ˆ(cid:96)t(π∗(xt))| + |˜(cid:96)t(π(xt)) − ˜(cid:96)t(π∗(xt))| ≤

+ 1.

K
(cid:15)

Note that E[ˆ(cid:96)t(π(xt)) − ˆ(cid:96)t(π∗(xt))|xt, (cid:96)t] = ˜(cid:96)t(π(xt)) − ˜(cid:96)t(π∗(xt)), so that

E[R2

t |Ft−1] = E[E[R2

t |xt, (cid:96)t, At]|Ft−1]

≤ E[E[(ˆ(cid:96)t(π(xt)) − ˆ(cid:96)t(π∗(xt)))2|xt, (cid:96)t, At]|Ft−1]

(cid:20)

(cid:20)

≤ E

E

≤ E

E

(cid:20) (1{π(xt) = at} − 1{π∗(xt) = at})2
pt(at)2

|Ft−1
(cid:20) 1{π(xt) (cid:54)= π∗(xt)}(1{π(xt) = at} + 1{π∗(xt) = at})
pt(at)2
(cid:21)

|xt, (cid:96)t, At

(cid:21)

(cid:21)

= E

(cid:20) 2K 1{π(xt) (cid:54)= π∗(xt)}
(cid:15)

|Ft−1

=

ρ(π, π∗).

2K
(cid:15)

(cid:21)

(cid:21)

|xt, (cid:96)t, At

|Ft−1

Freedman’s inequality then states that (12) holds with probability 1 − δT /2|Π|.

For (13), we consider a similar setup with

Rt = (cid:96)t(π(xt)) − (cid:96)t(π∗(xt)) − (L(π) − L(π∗)).

We have E[Rt|Ft−1] = 0, |Rt| ≤ 2 and E[R2
t |Ft−1] ≤ ρ(π, π∗), which yields that (13) holds
with probability 1 − δT /2|Π| using Freedman’s inequality. A union bound on π ∈ Π and
T ≥ 1 gives the desired result.

39

Bietti, Agarwal and Langford

Threshold. We deﬁne the threshold ∆T used in (11) in Algorithm 7 as:

∆T :=

+ 1

eT −1 +

+ 3

eT −1.

(14)

(cid:32)(cid:114)

2K
(cid:15)

(cid:33)

√

(cid:19)

(cid:18) K
(cid:15)

We also deﬁne the following more precise deviation quantity for a given policy, which follows
directly from the deviation bounds in Lemma 2

∆∗

T (π) :=

(cid:32)(cid:114)

(cid:33)

2K
(cid:15)

+ 1

(cid:112)ρ(π, π∗)eT −1 +

(cid:18) K
(cid:15)

(cid:19)

+ 3

eT −1.

(15)

Note that we have ∆∗

T (π) ≤ ∆T for any policy π.

The next lemma shows that the bias introduced in the empirical sample by assigning
a loss of 1 to unexplored actions is favorable, in the sense that it will not hurt us in
identifying π∗.

Lemma 3 (Favorable bias) Assume π∗(xt) ∈ At for all t ≤ T . We have

˜LT (π) − ˜LT (π∗) ≥ LT (π) − LT (π∗).

(16)

Proof For any t ≤ T , we have ˜(cid:96)t(a) ≥ (cid:96)t(a), so that ˜LT (π) ≥ LT (π). Separately, we
have ˜(cid:96)t(π∗(xt)) = (cid:96)t(π∗(xt)) for all t ≤ T using the deﬁnition of ˜(cid:96)t and the assumption
π∗(xt) ∈ At, hence ˜LT (π∗) ≥ LT (π∗).

We now show that with high probability, the optimal action is always explored by the

algorithm.

Lemma 4 Assume that event E holds. The actions given by the optimal policy are always
explored for all t ≥ 1, i.e., π∗(xt) ∈ At for all t ≥ 1.

Proof We show by induction on T ≥ 1 that π∗(xt) ∈ At for all t = 1, . . . , T . For the base
case, we have A1 = [K] since ˆZ0 = ∅ and hence empirical errors are always equal to 0, so
that π∗(x1) ∈ A1. Let us now assume as the inductive hypothesis that π∗(xt) ∈ At for all
t ≤ T − 1.

From deviation bounds, we have

ˆLT −1(πT ) − ˆLT −1(π∗) ≥ ˜LT −1(πT ) − ˜LT −1(π∗) −

(cid:32)(cid:114)

2Kρ(π, π∗)eT −1
(cid:15)

(cid:33)

+ (K/(cid:15) + 1)eT −1

LT −1(πT ) − LT −1(π∗) ≥ L(πT ) − L(π∗) −

(cid:16)(cid:112)ρ(π, π∗)eT −1 + 2eT −1

(cid:17)

.

Using Lemma 3 together with the inductive hypothesis, the above inequalities yield

ˆLT −1(πT ) − ˆLT −1(π∗) ≥ L(πT ) − L(π∗) − ∆∗

T (πT ).

Now consider an action a /∈ At. Using the deﬁnition (11) of At, we have

ˆLT −1(πT,a) − ˆLT −1(π∗) = ˆLT −1(πT,a) − ˆLT −1(πT ) + ˆLT −1(πT ) − ˆLT −1(π∗)

> ∆T − ∆∗

T (πT ) = 0,

40

A Contextual Bandit Bake-off

which implies π∗(xT ) (cid:54)= a, since ˆLT −1(πT,a) is the minimum of ˆLT −1 over policies satisfying
π(xT ) = a. This yields π∗(xT ) ∈ AT , which concludes the proof.

With the previous results, we can now prove that with high probability, discarding some
of the actions from the exploration process does not hurt us in identifying good policies.
In particular, πT +1 is about as good as it would have been with uniform (cid:15)-exploration all
along.

Theorem 5 Under the event E, which holds with probability 1 − δ,

L(πT +1) − L(π∗) ≤ ∆∗

T +1(πT +1).

In particular, L(πT +1) − L(π∗) ≤ ∆T +1.
Proof Assume event E holds. Using (12-13) combined with Lemma 3 (which holds by
Lemma 4), we have

L(πT +1) − L(π∗) ≤ ˆLT (πT +1) − ˆLT (π∗) + ∆∗

T +1(πT +1) ≤ ∆∗

T +1(πT +1).

Massart noise condition. We introduce a low-noise condition that will help us obtain
improved regret guarantees. Similar conditions have been frequently used in supervised
learning (Massart et al., 2006) and active learning (Hsu, 2010; Huang et al., 2015; Krish-
namurthy et al., 2019) for obtaining better data-dependent guarantees. We consider the
following Massart noise condition with parameter τ > 0:

ρ(π, π∗) ≤

(L(π) − L(π∗)).

1
τ

(M)

This condition holds when E[mina(cid:54)=π∗(x) (cid:96)(a) − (cid:96)(π∗(x))|x] ≥ τ , Px-almost surely, which is
similar to the Massart condition considered in Krishnamurthy et al. (2019) in the context
of active learning for cost-sensitive classiﬁcation. Indeed, we have

L(π) − L(π∗) = E[1{π(x) (cid:54)= π∗(x)}((cid:96)(π(x)) − (cid:96)(π∗(x))]

+ E[1{π(x) = π∗(x)}((cid:96)(π∗(x)) − (cid:96)(π∗(x)))]
(cid:20)
1{π(x) (cid:54)= π∗(x)}

(cid:96)(a) − (cid:96)(π∗(x))

(cid:18)

(cid:19)(cid:21)

≥ E

= E[1{π(x) (cid:54)= π∗(x)} E[ min

(cid:96)(a) − (cid:96)(π∗(x))|x]]

min
a(cid:54)=π∗(x)

a(cid:54)=π∗(x)

≥ E[1{π(x) (cid:54)= π∗(x)}τ ] = τ ρ(π, π∗),

which is precisely (M). The condition allows us to obtain a fast rate for the policies con-
sidered by our algorithm, as we now show.

Theorem 6 Assume the Massart condition (M) holds with parameter τ . Under the event
E, which holds w.p. 1 − δ,

for some numeric constant C.

L(πT +1) − L(π∗) ≤ C

eT ,

K
τ (cid:15)

41

Bietti, Agarwal and Langford

Proof Using Theorem 5 and the Massart condition, we have

L(πT +1) − L(π∗) ≤ ∆∗

T +1(πT +1) =

(cid:32)(cid:114)

(cid:33)

2K
(cid:15)

+ 1

(cid:112)ρ(πT +1, π∗)eT +

(cid:18) K
(cid:15)

(cid:19)

+ 3

eT

+ 1

(cid:112)(L(πT +1) − L(π∗))eT /τ +

(cid:18) K
(cid:15)

(cid:19)

+ 3

eT

(cid:32)(cid:114)

(cid:33)

≤

≤

2K
(cid:15)

(cid:114)

8KeT
τ (cid:15)

(L(πT +1) − L(π∗)) +

4KeT
(cid:15)

.

Solving the quadratic inequality in L(πT +1) − L(π∗) yields the result.

C.2.3 Regret Analysis

In a worst-case scenario, the following result shows that Algorithm 7 enjoys a similar O(T 2/3)
regret guarantee to the vanilla (cid:15)-greedy approach (Langford and Zhang, 2008).

Theorem 7 Conditioned on the event E, which holds with probability 1 − δ, the expected
regret of the algorithm is

E[RT |E] ≤ O

(cid:32)(cid:114)

KT log(T |Π|/δ)
(cid:15)

(cid:33)

+ T (cid:15)

.

Optimizing over the choice of (cid:15) yields a regret O(T 2/3(K log(T |Π|/δ))1/3).

Proof We condition on the 1−δ probability event E that the deviation bounds of Lemma 2
hold. We have

E[(cid:96)t(at) − (cid:96)t(π∗(xt))|Ft−1] = E[1{at = πt(xt)}((cid:96)t(πt(xt)) − (cid:96)t(π∗(xt)))|Ft−1]
+ E[1{at (cid:54)= πt(xt)}((cid:96)t(at) − (cid:96)t(π∗(xt)))|Ft−1]

≤ E[(cid:96)t(πt(xt)) − (cid:96)t(π∗(xt))|Ft−1] + E[E[1 − pt(πt(xt))|xt]|Ft−1]
≤ L(πt) − L(π∗) + (cid:15).

Summing over t and applying Theorem 5 together with ∆∗

t (π) ≤ ∆t, we obtain

E[RT |E] = E

(cid:96)t(at) − (cid:96)t(π∗(xt))|E

(cid:35)

(cid:34) T

(cid:88)

t=1
T
(cid:88)

t=2

≤ 1 + T (cid:15) +

∆t.

T
(cid:88)

t=2

≤ 1 +

E[L(πt) − L(π∗) + (cid:15)|Ft−1, E]

Using (cid:80)T

t=2

√

et ≤ O((cid:112)T log(8T 2|Π|/δ)) and (cid:80)T
(cid:114)

(cid:32)

E[RT |E] ≤ O

1 +

KT log(T |Π|/δ)
(cid:15)

+

K log(T |Π|/δ)
(cid:15)

(cid:33)

log T + T (cid:15)

,

t=2 et ≤ O(log(8T 2|Π|/δ) log T ), we obtain

42

A Contextual Bandit Bake-off

which yields the result.

Disagreement deﬁnitions.
In order to obtain improvements in regret guarantees over
the worst case, we consider notions of disagreement that extend standard deﬁnitions from
the active learning literature (e.g., Hanneke, 2014; Hsu, 2010; Huang et al., 2015) to the
multiclass case. Let B(π∗, r) := {π ∈ Π : ρ(π, π∗) ≤ r} be the ball centered at π∗ under
the (pseudo)-metric ρ(·, ·). We deﬁne the disagreement region DIS(r) and disagreement
coeﬃcient θ as follows:

DIS(r) := {x : ∃π ∈ B(π∗, r) π(x) (cid:54)= π∗(x)}
P (x ∈ DIS(r))
r

θ := sup
r>0

.

The next result shows that under the Massart condition and with a ﬁnite disagreement
coeﬃcient θ, our algorithm achieves a regret that scales as O(T 1/3) (up to logarithmic
factors), thus improving on worst-case guarantees obtained by optimal algorithms such
as Agarwal et al. (2012, 2014); Dudik et al. (2011a).

Theorem 8 Assume the Massart condition (M) holds with parameter τ . Conditioning on
the event E which holds w.p. 1 − δ, the algorithm has expected regret

E[RT |E] ≤ O

(cid:18) K log(T |Π|/δ)
τ (cid:15)

log T +

(cid:112)(cid:15)KT log(T |Π|/δ)

(cid:19)

.

θ
τ

Optimizing over the choice of (cid:15) yields a regret

E[RT |E] ≤ O

(θK log(T |Π|/δ))2/3(T log T )1/3

.

(cid:19)

(cid:18) 1
τ

Proof Assume E holds. Let t ≥ 2, and assume a ∈ At \ {π∗(xt)}. Deﬁne
(cid:40)

πa =

πt,
πt,a,

if πt(xt) = a
if πt(xt) (cid:54)= a,

so that we have πa(xt) = a (cid:54)= π∗(xt).

• If πa = πt, then L(πa) − L(π∗) ≤ ∆∗

t (πa) ≤ ∆t by Theorem 5

• If πa = πt,a, using deviation bounds, Lemma 4 and 3, we have

L(πa) − L(π∗) = L(πt,a) − L(π∗)

≤ ˆLt−1(πt,a) − ˆLt−1(π∗) + ∆∗
= ˆLt−1(πt,a) − ˆLt−1(πt)
(cid:123)(cid:122)
(cid:125)
≤∆t

(cid:124)

+ ˆLt−1(πt) − ˆLt−1(π∗)
(cid:123)(cid:122)
(cid:125)
≤0

(cid:124)

t (πt,a)

+∆∗

t (πt,a)

≤ 2∆t,

where the last inequality uses a ∈ At.

43

Bietti, Agarwal and Langford

By the Massart assumption, we then have ρ(πa, π∗) ≤ 2∆t/τ . Hence, we have xt ∈
DIS(2∆t/τ ). We have thus shown

E[E[1{a ∈ At \ {π∗(xt)}}|xt]|Ft−1] ≤ E[P (xt ∈ DIS(2∆t/τ ))|Ft−1] ≤ 2θ∆t/τ.

We then have

E[(cid:96)t(at) − (cid:96)t(π∗(xt))|Ft−1] = E[1{at = πt(xt)}((cid:96)t(πt(xt)) − (cid:96)t(π∗(xt)))

+ 1{at = π∗(xt) ∧ at (cid:54)= πt(xt)}((cid:96)t(π∗(xt)) − (cid:96)t(π∗(xt)))

1{at = a ∧ a /∈ {πt(xt), π∗(xt)}}((cid:96)t(a) − (cid:96)t(π∗(xt)))|Ft−1]

≤ E[(cid:96)t(πt(xt)) − (cid:96)t(π∗(xt))|Ft−1]

+ E

E[1{at = a} 1{a /∈ {πt(xt), π∗(xt)}}|xt]|Ft−1

(cid:35)

+

K
(cid:88)

a=1

(cid:34) K
(cid:88)

a=1

= L(πt) − L(π∗) +

E[E[pt(a) 1{a /∈ {πt(xt), π∗(xt)}}|xt]|Ft−1]

≤ L(πt) − L(π∗) +

E[E[1{a ∈ At \ {π∗(xt)}}|xt]|Ft−1]

≤ C

et−1 + 2(cid:15)θ∆t/τ,

K
τ (cid:15)

where we used

pt(a) 1{a /∈ {πt(xt), π∗(xt)}} =

1{a ∈ At \ {πt(xt), π∗(xt)}}

≤

1{a ∈ At \ {π∗(xt)}}.

Summing over t and taking total expectations (conditioned on E) yields

E[RT |E] ≤ O

(cid:32)

K log(T |Π|/δ)
τ (cid:15)

log T +

(cid:32)(cid:114)

(cid:15)θ
τ

KT log(T |Π|/δ)
(cid:15)

+

K log(T |Π|/δ)
(cid:15)

(cid:33)(cid:33)

log(T )

,

and the result follows.

Finally, we look at a simpler instructive example, which considers an extreme situation
where the expected loss of any suboptimal policy is bounded away from that of the opti-
mal policy. In this case, Algorithm 7 can achieve constant regret when the disagreement
coeﬃcient is bounded, as shown by the following result.

Proposition 9 Assume that L(π) − L(π∗) ≥ τ > 0 for all π (cid:54)= π∗, and that θ < ∞. Under
the event E, the algorithm achieves constant expected regret. In particular, the algorithm
stops incurring regret for T > T0 := max{t : 2∆t > τ }.

K
(cid:88)

a=1

(cid:15)
K

K
(cid:88)

a=1

(cid:15)
K
(cid:15)
K

44

A Contextual Bandit Bake-off

Proof By Theorem 5 and our assumption, we have L(πt) − L(π∗) ≤ 1{∆t ≥ τ }∆t.
Similarly, the assumption implies that ρ(π, π∗) ≤ 1{L(π) − L(π∗) ≥ τ }, so that using
similar arguments to the proof of Theorem 8, we have

E[E[1{a ∈ At \ {π∗(xt)}}|xt]|Ft−1] ≤ θ 1{2∆t ≥ τ }.

Following the proof of Theorem 8, this implies that when t is such that 2∆t < τ , then we
have

Let T0 := max{t : 2∆t ≥ τ }. We thus have

E[(cid:96)t(at) − (cid:96)t(π∗(xt))|Ft−1] = 0.

E[RT |E] ≤ 1 +

(∆t + (cid:15)).

T0(cid:88)

t=2

45

0
2
0
2
 
n
a
J
 
4
2
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
4
6
0
4
0
.
2
0
8
1
:
v
i
X
r
a

A Contextual Bandit Bake-off

A Contextual Bandit Bake-oﬀ

Alberto Bietti
Inria, Ecole normale sup´erieure, PSL Research University, Paris, France∗
Alekh Agarwal
Microsoft Research, Redmond, WA
John Langford
Microsoft Research, New York, NY

alberto.bietti@inria.fr

alekha@microsoft.com

jcl@microsoft.com

Abstract

Contextual bandit algorithms are essential for solving many real-world interactive ma-
chine learning problems. Despite multiple recent successes on statistically and computation-
ally eﬃcient methods, the practical behavior of these algorithms is still poorly understood.
We leverage the availability of large numbers of supervised learning datasets to empirically
evaluate contextual bandit algorithms, focusing on practical methods that learn by relying
on optimization oracles from supervised learning. We ﬁnd that a recent method (Foster
et al., 2018) using optimism under uncertainty works the best overall. A surprisingly close
second is a simple greedy baseline that only explores implicitly through the diversity of
contexts, followed by a variant of Online Cover (Agarwal et al., 2014) which tends to be
more conservative but robust to problem speciﬁcation by design. Along the way, we also
evaluate various components of contextual bandit algorithm design such as loss estimators.
Overall, this is a thorough study and review of contextual bandit methodology.
Keywords:

contextual bandits, online learning, evaluation

1. Introduction

At a practical level, how should contextual bandit learning and exploration be done?

In the contextual bandit problem, a learner repeatedly observes a context, chooses an
action, and observes a loss for the chosen action only. Many real-world interactive machine
learning tasks are well-suited to this setting: a movie recommendation system selects a
movie for a given user and receives feedback (click or no click) only for that movie; a choice
of medical treatment may be prescribed to a patient with an outcome observed for (only)
the chosen treatment. The limited feedback (known as bandit feedback) received by the
learner highlights the importance of exploration, which needs to be addressed by contextual
bandit algorithms.

The focal point of contextual bandit (henceforth CB) learning research is eﬃcient ex-
ploration algorithms (Abbasi-Yadkori et al., 2011; Agarwal et al., 2012, 2014; Agrawal and
Goyal, 2013; Dudik et al., 2011a; Langford and Zhang, 2008; Russo et al., 2018). How-
ever, many of these algorithms remain far from practical, and even when considering more
practical variants, their empirical behavior is poorly understood, typically with limited eval-

∗. Part of this work was done while AB was visiting Microsoft Research NY, supported by the MSR-Inria

Joint Center.

1

Bietti, Agarwal and Langford

uation on just a handful of scenarios. In particular, strategies based on upper conﬁdence
bounds (Abbasi-Yadkori et al., 2011; Li et al., 2010) or Thompson sampling (Agrawal and
Goyal, 2013; Russo et al., 2018) are often intractable for sparse, high-dimensional datasets,
and make strong assumptions on the model representation. The method of Agarwal et al.
(2014) alleviates some of these diﬃculties while being statistically optimal under weak as-
sumptions, but the analyzed version is still far from practical, and the worst-case guarantees
may lead to overly conservative exploration that can be ineﬃcient in practice.

The main objective of our work is an evaluation of practical methods that are relevant
to practitioners. We focus on algorithms that rely on optimization oracles from supervised
learning such as cost-sensitive classiﬁcation or regression oracles, which provides computa-
tional eﬃciency and support for generic representations. We further rely on online learning
implementations of the oracles, which are desirable in practice due to the sequential na-
ture of contextual bandits. While conﬁdence-based strategies and Thompson sampling are
not directly adapted to this setting, we achieve it with online Bootstrap approximations for
Thompson sampling (Agarwal et al., 2014; Eckles and Kaptein, 2014; Osband and Van Roy,
2015), and with the conﬁdence-based method of Foster et al. (2018) based on regression
oracles, which contains LinUCB as a special case. Additionally, we consider practical de-
sign choices such as loss encodings (e.g., if values have a range of 1, should we encode the
costs of best and worst outcomes as 0/1 or -1/0?), and for methods that learn by reduc-
tion to oﬀ-policy learning, we study diﬀerent reduction techniques beyond the simple inverse
propensity scoring approach. All of our experiments are based on the online learning system
Vowpal Wabbit1 which has already been successfully used in production systems (Agarwal
et al., 2016).

The interactive aspect of CB problems makes them notoriously diﬃcult to evaluate in
real-world settings beyond a handful of tasks. Instead, we leverage the wide availability of
supervised learning datasets with diﬀerent cost structures on their predictions, and obtain
contextual bandit instances by simulating bandit feedback, treating labels as actions and
hiding the loss of all actions but the chosen one. This setup captures the generality of the
i.i.d. contextual bandit setting, while avoiding some diﬃcult aspects of real-world settings
that are not supported by most existing algorithms and are diﬃcult to evaluate, such as non-
stationarity. We consider a large collection of over 500 datasets with varying characteristics
and various cost structures, including multiclass, multilabel and more general cost-sensitive
datasets with real-valued costs. To our knowledge, this is the ﬁrst evaluation of contextual
bandit algorithms on such a large and diverse corpus of datasets.

Our evaluation considers online implementations of Bootstrap Thompson sampling (Agar-
wal et al., 2014; Eckles and Kaptein, 2014; Osband and Van Roy, 2015), the Cover approach
of Agarwal et al. (2014), (cid:15)-greedy (Langford and Zhang, 2008), RegCB (Foster et al., 2018,
which includes LinUCB as a special case), and a basic greedy method similar to the one
studied in Bastani et al. (2017); Kannan et al. (2018). As the ﬁrst conclusion of our study,
we ﬁnd that the recent RegCB method (Foster et al., 2018) performs the best overall across
a number of experimental conditions. Remarkably, we discover that a close second in our set
of methods is the simple greedy baseline, often outperforming most exploration algorithms.
Both these methods have drawbacks in theory; greedy can fail arbitrarily poorly in prob-

1. https://vowpalwabbit.org

2

A Contextual Bandit Bake-off

Figure 1: Comparison between three competitive approaches: RegCB (conﬁdence based),
Cover-NU (variant of Online Cover) and Greedy. The plots show relative loss compared
to supervised learning (lower is better) on all datasets with 5 actions or more. Red points
indicate datasets with a statistically signiﬁcant diﬀerence in loss between two methods. A
greedy approach can outperform exploration methods in many cases; yet both Greedy and
RegCB may fail to explore eﬃciently on some other datasets where Cover-NU dominates.

lems where intentional exploration matters, while UCB methods make stronger modeling
assumptions and can have an uncontrolled regret when the assumptions fail. The logs col-
lected by deploying these methods in practice are also unﬁt for later oﬀ-policy experiments,
an important practical consideration. Our third conclusion is that several methods which
are more robust in that they make only a relatively milder i.i.d. assumption on the problem
tend to be overly conservative and often pay a steep price on easier datasets. Nevertheless,
we ﬁnd that an adaptation of Online Cover (Agarwal et al., 2014) is quite competitive on a
large fraction of our datasets. We also evaluate the eﬀect of diﬀerent ways to encode losses
and study diﬀerent reduction mechanisms for exploration algorithms that rely on oﬀ-policy
learning (such as (cid:15)-greedy), ﬁnding that a technique based on importance-weighted regres-
sion tends to outperform other approaches when applicable. We show pairwise comparisons
between the top 3 methods in our evaluation in Figure 1 for datasets with 5 or more actions.
For future theoretical research, our results motivate an emphasis on understanding greedy
strategies, building on recent progress (Bastani et al., 2017; Kannan et al., 2018), as well
as eﬀectively leveraging easier datasets in exploration problems (Agarwal et al., 2017).

1.1 Organization of the paper

The paper is organized as follows:

• Section 2 provides relevant background on i.i.d. contextual bandits, optimization or-
acles, and mechanisms for reduction to oﬀ-policy learning, and introduces our exper-
imental setup.

• Section 3 describes the main algorithms we consider in our evaluation, as well as the

modiﬁcations that we found eﬀective in practice.

• Section 4 presents the results and insights from our experimental evaluation.
• Finally, we conclude in Section 5 with a discussion of our ﬁndings and a collection
of guidelines and recommendations for practitioners that come out of our empirical
study, as well as open questions for theoreticians.

3

Bietti, Agarwal and Langford

2. Contextual Bandit Setup

In this section, we present the learning setup considered in this work, recalling the stochastic
contextual bandit setting, the notion of optimization oracles, various techniques used by
contextual bandit algorithms for leveraging these oracles, and ﬁnally our experimental setup.

2.1 Learning Setting

The stochastic (i.i.d.) contextual bandit learning problem can be described as follows. At
each time step t, the environment produces a pair (xt, (cid:96)t) ∼ D independently from the past,
where xt ∈ X is a context vector and (cid:96)t = ((cid:96)t(1), . . . , (cid:96)t(K)) ∈ RK is a loss vector, with K
the number of possible actions, and the data distribution is denoted D. After observing the
context xt, the learner chooses an action at, and only observes the loss (cid:96)t(at) corresponding
to the chosen action. The goal of the learner is to trade-oﬀ exploration and exploitation in
order to incur a small cumulative regret

RT :=

(cid:96)t(at) −

(cid:96)t(π∗(xt)),

T
(cid:88)

t=1

T
(cid:88)

t=1

with respect to the optimal policy π∗ ∈ arg minπ∈Π E(x,(cid:96))∼D[(cid:96)(π(x))], where Π denotes a
(large, possibly inﬁnite) set of policies π : X → {1, . . . , K} which we would like to do well
against.
It is often important for the learner to use randomized strategies, for instance
in order to later evaluate or optimize new policies, hence we let pt(a) ∈ [0, 1] denote the
probability that the agent chooses action a ∈ {1, . . . , K} at time t, so that at ∼ pt.

2.2 Optimization Oracles

In this paper, we focus on CB algorithms which rely on access to an optimization oracle
for solving optimization problems similar to those that arise in supervised learning, leading
to methods that are suitable for general policy classes Π. The main example is the cost-
sensitive classiﬁcation (CSC) oracle (Agarwal et al., 2014; Dudik et al., 2011a; Langford
and Zhang, 2008), which given a collection (x1, c1), . . . , (xT , cT ) ∈ X × RK computes

T
(cid:88)

arg min
π∈Π

ct(π(xt)).

t=1
The cost vectors ct = (ct(1), . . . , ct(K)) ∈ RK are often constructed using counterfactual
estimates of the true (unobserved) losses, as we describe in the next section.

Another approach is to use regression oracles, which ﬁnd f : X ×{1, . . . , K} → R from
a class of regressor functions F to predict a cost yt, given a context xt and action at (see,
e.g., Agarwal et al., 2012; Foster et al., 2018).
In this paper, we consider the following
regression oracle with importance weights ωt > 0:

(1)

(2)

While the theory typically requires exact solutions to (1) or (2), this is often impractical
due to the diﬃculty of the underlying optimization problem (especially for CSC, which

arg min
f ∈F

T
(cid:88)

t=1

ωt(f (xt, at) − yt)2.

4

A Contextual Bandit Bake-off

yields a non-convex and non-smooth problem), and more importantly because the size of the
problems to be solved keeps increasing after each iteration. In this work, we consider instead
the use of online optimization oracles for solving problems (1) or (2), which incrementally
update a given policy or regression function after each new observation, using for instance
an online gradient method. Such an online learning approach is natural in the CB setting,
and is common in interactive production systems (e.g., Agarwal et al., 2016; He et al., 2014;
McMahan et al., 2013).

2.3 Loss Estimates and Reductions

A common approach to solving problems with bandit (partial) feedback is to compute an
estimate of the full feedback using the observed loss and then apply methods for the full-
information setting to these estimated values. In the case of CBs, this allows an algorithm
to ﬁnd a good policy based on oﬀ-policy exploration data collected by the algorithm. These
loss estimates are commonly used to create CSC instances to be solved by the optimization
oracle introduced above (Agarwal et al., 2014; Dudik et al., 2011a; Langford and Zhang,
2008), a process sometimes referred as reduction to cost-sensitive classiﬁcation. Given such
estimates ˆ(cid:96)t(a) of (cid:96)t(a) for all actions a and for t = 1, . . . , T , such a reduction constructs cost
vectors ct = (ˆ(cid:96)t(1), . . . , ˆ(cid:96)t(K)) ∈ RK and feeds them along with the observed contexts xt
to the CSC oracle (1) in order to obtain a policy. We now describe the three diﬀerent
estimation methods considered in this paper, and how each is typically used for reduction
to policy learning with a CSC or regression oracle. In what follows, we consider observed
interaction records (xt, at, (cid:96)t(at), pt(at)).

Perhaps the simplest approach is the inverse propensity-scoring (IPS) estimator:

ˆ(cid:96)t(a) :=

1{a = at}.

(cid:96)t(at)
pt(at)
For any action a with pt(a) > 0, this estimator is unbiased, i.e. Eat∼pt[ˆ(cid:96)t(a)] = (cid:96)t(a), but
can have high variance when pt(at) is small. The estimator leads to a straightforward
CSC example (xt, ˆ(cid:96)t). Using such examples in (1) provides a way to perform oﬀ-policy
(or counterfactual) evaluation and optimization, which in turn allows a CB algorithm to
identify good policies for exploration.
In order to obtain good unbiased estimates, one
needs to control the variance of the estimates, e.g., by enforcing a minimum exploration
probability pt(a) ≥ (cid:15) > 0 on all actions.

(3)

In order to reduce the variance of IPS, the doubly robust (DR) estimator (Dudik

et al., 2011b) uses a separate, possibly biased, estimator of the loss ˆ(cid:96)(x, a):

ˆ(cid:96)t(a) :=

(cid:96)t(at) − ˆ(cid:96)(xt, at)
pt(at)

1{a = at} + ˆ(cid:96)(xt, a).

When ˆ(cid:96)(xt, at) is a good estimate of (cid:96)t(at), the small numerator in the ﬁrst term helps
reduce the variance induced by a small denominator, while the second term ensures that
the estimator is unbiased. Typically, ˆ(cid:96)(x, a) is learned by regression on all past observed
losses, e.g.,

(4)

(5)

ˆ(cid:96) := arg min
f ∈F

T
(cid:88)

t=1

(f (xt, at) − (cid:96)t(at))2.

5

Bietti, Agarwal and Langford

The reduction to cost-sensitive classiﬁcation is similar to IPS, by feeding cost vectors ct = ˆ(cid:96)t
to the CSC oracle.

We consider a third method that directly reduces to the importance-weighted regression
oracle (2), which we refer to as IWR (for importance-weighted regression), and is
suitable for algorithms which rely on oﬀ-policy learning.2 This approach ﬁnds a regressor

ˆf := arg min
f ∈F

T
(cid:88)

t=1

1
pt(at)

(f (xt, at) − (cid:96)t(at))2,

(6)

and considers the policy ˆπ(x) = arg mina ˆf (x, a). Such an estimator has been used, e.g.,
in the context of oﬀ-policy learning for recommendations (Schnabel et al., 2016) and is
available in the Vowpal Wabbit library. Note that if pt has full support, then the objective
is an unbiased estimate of the full regression objective on all actions,

T
(cid:88)

K
(cid:88)

t=1

a=1

(f (xt, a) − (cid:96)t(a))2.

In contrast, if the learner only explores a single action (so that pt(at) = 1 for all t), the
obtained regressor ˆf is the same as the loss estimator ˆ(cid:96) in (5). In this csae, if we consider
a x with x ∈ Rd, then the IWR reduction
a linear class of regressors of the form f (x, a) = θ(cid:62)
computes least-squares estimates ˆθa from the data observed when action a was chosen.
When actions are selected according to the greedy policy at = arg mina ˆθ(cid:62)
a xt, this setup
corresponds to the greedy algorithm considered, e.g., by Bastani et al. (2017).

Note that while CSC is typically intractable and requires approximations in order to
In
work in practice, importance-weighted regression does not suﬀer from these issues.
addition, while the computational cost for an approximate CSC online update scales with
the number of actions K, IWR only requires an update for a single action, making the
approach more attractive computationally. Another beneﬁt of IWR in an online setting is
that it can leverage importance weight aware online updates (Karampatziakis and Langford,
2011), which makes it easier to handle large inverse propensity scores.

2.4 Experimental Setup

Our experiments are conducted by simulating the contextual bandit setting using multiclass
or cost-sensitive classiﬁcation datasets, and use the online learning system Vowpal Wabbit
(VW).

Simulated contextual bandit setting. The experiments in this paper are based on
leveraging supervised cost-sensitive classiﬁcation datasets for simulating CB learning. In
particular, we treat a CSC example (xt, ct) ∈ X × RK as a CB example, with xt given as
the context to a CB algorithm, and we only reveal the loss for the chosen action at. For
a multiclass example with label yt ∈ {1, . . . , K}, we set ct(a) := 1{a (cid:54)= yt}; for multilabel
examples with label set Yt ⊆ {1, . . . , K}, we set ct(a) := 1{a /∈ Yt}; the cost-sensitive

2. Note that IWR is not directly applicable to methods that explicitly reduce to CSC oracles, such as Agar-

wal et al. (2014); Dudik et al. (2011a).

6

A Contextual Bandit Bake-off

datasets we consider have ct ∈ [0, 1]K. We consider more general loss encodings deﬁned
with an additive oﬀset on the cost by:

(cid:96)c
t (a) = c + ct(a),

(7)

for some c ∈ R. Although some techniques attempt to remove a dependence on such en-
coding choices through appropriately designed counterfactual loss estimators (Dudik et al.,
2011b; Swaminathan and Joachims, 2015), these may be imperfect in practice, and partic-
ularly in an online scenario. The behavior observed for diﬀerent choices of c allows us to
get a sense of the robustness of the algorithms to the scale of observed losses, which might
be unknown. Separately, diﬀerent values of c can lead to lower variance for loss estimation
in diﬀerent scenarios: c = 0 might be preferred if ct(a) is often 0, while c = −1 is preferred
when ct(a) is often 1. In order to have a meaningful comparison between diﬀerent algo-
rithms, loss encodings, as well as supervised multiclass classiﬁcation, our evaluation metrics
consider the original costs ct.

Online learning in VW. Online learning is an important tool for having machine learn-
ing systems that quickly and eﬃciently adapt to observed data (Agarwal et al., 2016; He
et al., 2014; McMahan et al., 2013). We run our CB algorithms in an online fashion using
Vowpal Wabbit:
instead of exact solutions of the optimization oracles from Section 2.2,
we consider online variants of the CSC and regression oracles, which incrementally update
the policies or regressors with online gradient steps or variants thereof. Note that in VW,
online CSC itself reduces to multiple online regression problems in VW (one per action), so
that we are left with only online regression steps. More speciﬁcally, we use adaptive (Duchi
et al., 2011), normalized (Ross et al., 2013) and importance-weight-aware (Karampatziakis
and Langford, 2011) gradient updates, with a single tunable step-size parameter.

a x, or in the case of the IWR reduction, regressors f (x, a) = θ(cid:62)

Parameterization. We consider linearly parameterized policies taking the form π(x) =
arg mina θ(cid:62)
a x. For the DR
loss estimator, we use a similar linear parameterization ˆ(cid:96)(x, a) = φ(cid:62)
a x. We note that the
algorithms we consider do not rely on this speciﬁc form, and easily extend to more complex,
problem-dependent representations, such as action-dependent features. Some datasets in
our evaluation have such an action-dependent structure, with diﬀerent feature vectors xa
for diﬀerent actions a; in this case we use parameterizations of the form f (x, a) = θ(cid:62)xa,
and ˆ(cid:96)(x, a) = φ(cid:62)xa, where the parameters θ and φ are shared across all actions.

3. Algorithms

In this section, we present the main algorithms we study in this paper, along with sim-
ple modiﬁcations that achieve improved exploration eﬃciency. All methods are based on
the generic scheme in Algorithm 1. The function explore computes the exploration dis-
tribution pt over actions, and learn updates the algorithm’s policies. For simiplicity, we
consider a function oracle which performs an online update to a policy using IPS, DR or
IWR reductions given an interaction record (xt, at, (cid:96)t(at), pt). For some methods (mainly
Cover), the CSC oracle is called explicitly with modiﬁed cost vectors ct rather than the IPS
or DR loss estimates ˆ(cid:96)t deﬁned in Section 2.3; in this case, we denote such an oracle call by
csc_oracle, and also use a separate routine estimator which takes an interaction record

7

Bietti, Agarwal and Langford

Algorithm 1 Generic contextual bandit algorithm

for t = 1, . . . do

Observe context xt, compute pt = explore(xt);
Choose action at ∼ pt, observe loss (cid:96)t(at);
learn(xt, at, (cid:96)t(at), pt);

end for

Algorithm 2 (cid:15)-greedy
π1; (cid:15) > 0 (or (cid:15) = 0 for Greedy).
explore(xt):

return pt(a) = (cid:15)/K + (1 − (cid:15)) 1{πt(xt) = a};

learn(xt, at, (cid:96)t(at), pt):

πt+1 = oracle(πt, xt, at, (cid:96)t(at), pt(at));

and computes IPS or DR loss estimate vectors ˆ(cid:96)t. RegCB directly uses a regression oracle,
which we denote reg_oracle.

Our implementations of each algorithm are in the Vowpal Wabbit online learning library.

3.1 (cid:15)-greedy and greedy

We consider an importance-weighted variant of the epoch-greedy approach of Langford
and Zhang (2008), given in Algorithm 2. The method acts greedily with probability 1 − (cid:15),
and otherwise explores uniformly on all actions. Learning is achieved by reduction to oﬀ-
policy optimization, through any of the three reductions presented in Section 2.3.

We also experimented with a variant we call active (cid:15)-greedy, that uses notions from
disagreement-based active learning (Hanneke, 2014; Hsu, 2010) in order to reduce uniform
exploration to only actions that could plausibly be taken by the optimal policy. While this
variant often improves on the basic (cid:15)-greedy method, we found that it is often outperformed
empirically by other exploration algorithms, and thus defer its presentation to Appendix C,
along with a theoretical analysis, for reference.

Greedy. When taking (cid:15) = 0 in the (cid:15)-greedy approach, with the IWR reduction, we are left
with a fully greedy approach that always selects the action given by the current policy. This
gives us an online variant of the greedy algorithm of Bastani et al. (2017), which regresses on
observed losses and acts by selecting the action with minimum predicted loss. Although this
greedy strategy does not have an explicit mechanism for exploration in its choice of actions,
the inherent diversity in the distribution of contexts may provide suﬃcient exploration
for good performance and provable regret guarantees (Bastani et al., 2017; Kannan et al.,
2018). In particular, under appropriate assumptions including a diversity assumption on
the contexts, one can show that all actions have a non-zero probability of being selected at
each step, providing a form of “natural” exploration from which one can establish regret
guarantees. Empirically, we ﬁnd that Greedy can perform very well in practice on many
datasets (see Section 4). If multiple actions get the same score according to the current
regressor, we break ties randomly.

8

A Contextual Bandit Bake-off

3.2 Bagging (online Bootstrap Thompson sampling)

return pt(a) ∝ |{i : πi

t(xt) = a}|;3

Algorithm 3 Bag
π1
1, . . . , πN
1 .
explore(xt):

learn(xt, at, (cid:96)t(at), pt):
for i = 1, . . . , N do
τ i ∼ P oisson(1);
t+1 = oracleτ i(πi
πi
end for

t, xt, at, (cid:96)t(at), pt(at));

{with τ 1 = 1 for bag-greedy}

We now consider a variant of Thompson sampling which is usable in practice with opti-
mization oracles. Thompson sampling provides a generic approach to exploration problems,
which maintains a belief on the data generating model in the form of a posterior distribution
given the observed data, and explores by selecting actions according to a model sampled
from this posterior (see, e.g., Agrawal and Goyal, 2013; Chapelle and Li, 2011; Russo et al.,
2018; Thompson, 1933). While the generality of this strategy makes it attractive, main-
taining this posterior distribution can be intractable for complex policy classes, and may
require strong modeling assumptions. In order to overcome such diﬃculties and to support
the optimization oracles considered in this paper, we rely on an approximation of Thompson
sampling known as the online Bootstrap Thompson sampling (Eckles and Kaptein, 2014;
Osband and Van Roy, 2015), or bagging (Agarwal et al., 2014). This approach, shown in
Algorithm 3, maintains a collection of N policies π1
t meant to approximate the pos-
terior distribution over policies via the online Bootstrap (Agarwal et al., 2014; Eckles and
Kaptein, 2014; Osband and Van Roy, 2015; Oza and Russell, 2001; Qin et al., 2013), and
explores in a Thompson sampling fashion, by averaging action decisions across all policies
(hence the name bagging).

t , . . . , πN

Each policy is trained on a diﬀerent online Bootstrap sample of the observed data, in the
form of interaction records. The online Bootstrap performs a random number τ of online
updates to each policy instead of one (this is denoted by oracleτ in Algorithm 3). We use a
Poisson distribution with parameter 1 for τ , which ensures that in expectation, each policy
is trained on t examples after t steps. In contrast to Eckles and Kaptein (2014); Osband
and Van Roy (2015), which play the arm given by one of the N policies chosen at random,
we compute the full action distribution pt resulting from such a sampling, and leverage this
for loss estimation, allowing learning by reduction to oﬀ-policy optimization as in Agarwal
et al. (2014). As in the (cid:15)-greedy algorithm, Bagging directly relies on oﬀ-policy learning
and thus all three reductions are admissible.

Greedy bagging. We also consider a simple optimization that we call greedy bagging, for
which the ﬁrst policy π1 is trained on the true data sample (like Greedy), that is, with τ

3. When policies are parametrized using regressors as in our implementation, we let πi

t(x) be uniform over
all actions tied for the lowest cost, and the ﬁnal distribution is uniform across all actions tied for best
according to one of the policies in the bag. The added randomization gives useful variance reduction in
our experiments.

9

Bietti, Agarwal and Langford

Algorithm 4 Cover
π1
1, . . . , πN
explore(xt):

1 ; (cid:15)t = min(1/K, 1/

Kt); ψ > 0.

√

t(xt) = a}|;

pt(a) ∝ |{i : πi
return (cid:15)t + (1 − (cid:15)t)pt;
return pt;

learn(xt, at, (cid:96)t(at), pt):
π1
t+1 = oracle(π1
ˆ(cid:96)t = estimator(xt, at, (cid:96)t(at), pt(at));
for i = 2, . . . , N do

t , xt, at, (cid:96)t(at), pt(at));

t+1(xt) = a}|;

qi(a) ∝ |{j ≤ i − 1 : πj
ˆc(a) = ˆ(cid:96)t(a) −
ψ(cid:15)t
(cid:15)t+(1−(cid:15)t)qi(a) ;
πi
t+1 = csc_oracle(πi
end for

t, xt, ˆc);

{for cover}
{for cover-nu}

always equal to one, instead of a bootstrap sample with random choices of τ . We found this
approach to often improve on bagging, particularly when the number of policies N is small.

3.3 Cover

This method, given in Algorithm 4, is based on Online Cover, an online approxima-
tion of the “ILOVETOCONBANDITS” algorithm of Agarwal et al. (2014). The approach
maintains a collection of N policies, π1
t , meant to approximate a covering distri-
bution over policies that are good for both exploration and exploitation. The ﬁrst policy
π1
t is trained on observed data using the oracle as in previous algorithms, while subsequent
policies are trained using modiﬁed cost-sensitive examples which encourage diversity in the
predicted actions compared to the previous policies.

t , . . . , πN

Our implementation diﬀers from the Online Cover algorithm of Agarwal et al. (2014,
Algorithm 5) in how the diversity term in the deﬁnition of ˆc(a) is handled (the second term).
When creating cost-sensitive examples for a given policy πi, this term rewards an action a
that is not well-covered by previous policies (i.e., small qi(a)), by subtracting from the cost
a term that decreases with qi(a). While Online Cover considers a ﬁxed (cid:15)t = (cid:15), we let (cid:15)t
decay with t, and introduce a parameter ψ to control the overall reward term, which bears
more similarity with the analyzed algorithm. In particular, the magnitude of the reward
is ψ whenever action a is not covered by previous policies (i.e., qi(a) = 0), but decays with
ψ(cid:15)t whenever qi(a) > 0, so that the level of induced diversity can decrease over time as we
gain conﬁdence that good policies are covered.

Cover-NU. While Cover requires some uniform exploration across all actions, our ex-
periments suggest that this can make exploration highly ineﬃcient, thus we introduce a
variant, Cover-NU, with no uniform exploration outside the set of actions selected by cov-
ering policies.

10

A Contextual Bandit Bake-off

Algorithm 5 RegCB
f1; C0 > 0.
explore(xt):

lt(a) = lcb(ft, xt, a, ∆t,C0);
ut(a) = ucb(ft, xt, a, ∆t,C0);
pt(a) ∝ 1{a ∈ arg mina(cid:48) lt(a(cid:48))};
pt(a) ∝ 1{lt(a) ≤ mina(cid:48) ut(a(cid:48))};
return pt;

learn(xt, at, (cid:96)t(at), pt):

ft+1 = reg_oracle(ft, xt, at, (cid:96)t(at));

3.4 RegCB

{RegCB-opt variant}
{RegCB-elim variant}

We consider online approximations of the two algorithms introduced by Foster et al. (2018)
based on regression oracles, shown in Algorithm 5. Both algorithms estimate conﬁdence
intervals of the loss for each action given the current context xt, denoted [lt(a), ut(a)] in
Algorithm 5, by considering predictions from a subset of regressors with small squared loss.
The optimistic variant then selects the action with smallest lower bound estimate, similar
to LinUCB, while the elimination variant explores uniformly on actions that may plausibly
be the best.

More formally, the RegCB algorithm theoretically analyzed by Foster et al. (2018) de-

ﬁnes the conﬁdence bounds as follows:

lt(a) = min
f ∈Ft

f (xt, a),

and ut(a) = max
f ∈Ft

f (xt, a).

Here, Ft is a subset of regressors that is “good” for loss estimation, in the sense that it
achieves a small regression loss on observed data, ˆRt−1(f ) := 1
s=1(f (xt, at) − (cid:96)t(at))2,
t−1
compared to the best regressor in the full regressor class F:

(cid:80)t−1

Ft := {f ∈ F : ˆRt−1(f ) − min
f ∈F

ˆRt−1(f ) ≤ ∆t},

where ∆t is a quantity decreasing with t.

Our online implementation computes approximations of these upper and lower bounds
on the loss of each action, by using a sensitivity analysis of the current regressor based
on importance weighting taken from Krishnamurthy et al. (2019) in the context of active
learning (the computations are denoted lcb and ucb in Algorithm 5). The algorithm main-
tains a regressor ft : X × {1, . . . , K} and, given a new context xt, computes lower and
upper conﬁdence bounds lt(a) ≤ ft(xt, a) ≤ ut(a). These are computed by adding “virtual”
importance-weighted regression examples with low and high costs, and ﬁnding the largest
importance weight leading to an excess squared loss smaller than ∆t,C0, where

and C0 is a parameter controlling the width of the conﬁdence bounds. This importance
weight can be found using regressor sensitivities and a binary search procedure as described

∆t,C0 =

C0 log(Kt)
t

,

11

Bietti, Agarwal and Langford

in (Krishnamurthy et al., 2019, Section 7.1). Note that this requires knowledge of the loss
range [cmin, cmax], unlike other methods. In contrast to Krishnamurthy et al. (2019), we set
the labels of the “virtual” examples to cmin − 1 for the lower bound and cmax + 1 for the
upper bound, instead of cmin and cmax.

4. Evaluation

In this section, we present our evaluation of the contextual bandit algorithms described in
Section 3. The evaluation code is available at https://github.com/albietz/cb_bakeoff.
All methods presented in this section are available in Vowpal Wabbit.4

Evaluation setup. Our evaluation consists in simulating a CB setting from cost-sensitive
classiﬁcation datasets, as described in Section 2.4. We consider a collection of 525 multiclass
classiﬁcation datasets from the openml.org platform, including among others, medical,
gene expression, text, sensory or synthetic data, as well as 5 multilabel datasets5 and 3
cost-sensitive datasets, namely a cost-sensitive version of the RCV1 multilabel dataset used
in (Krishnamurthy et al., 2019), where the cost of a news topic is equal to the tree distance
to a correct topic, as well as the two learning to rank datasets used in (Foster et al., 2018).
More details on these datasets are given in Appendix A. Because of the online setup, we
consider one or more ﬁxed, shuﬄed orderings of each dataset. The datasets widely vary in
noise levels, and number of actions, features, examples etc., allowing us to model varying
diﬃculties in CB problems.

We evaluate the algorithms described in Section 3. We ran each method on every dataset
with diﬀerent choices of algorithm-speciﬁc hyperparameters, learning rates, reductions, and
loss encodings. Details are given in Appendix B.1. Unless otherwise speciﬁed, we consider
ﬁxed choices which are chosen to optimize performance on a subset of multiclass datasets
with a voting mechanism and are highlighted in Table 9 of Appendix B, except for the
learning rate, which is always optimized.

The performance of method A on a dataset of size n is measured by the progressive

validation loss (Blum et al., 1999):

P VA =

ct(at),

1
n

n
(cid:88)

t=1

where at is the action chosen by the algorithm on the t-th example, and ct the true cost
vector. This metric allows us to capture the explore-exploit trade-oﬀ, while providing a
measure of generalization that is independent of the choice of loss encodings, and compa-
rable with online supervised learning. We also consider a normalized loss variant given
by P VA−P VOAA
, where OAA denotes an online (supervised) cost-sensitive one against all
classiﬁer. This helps highlight the diﬃculty of exploration for some datasets in our plots.

P VOAA

In order to compare two methods on a given dataset with binary costs (multiclass or
multilabel), we consider a notion of statistically signiﬁcant win or loss. We use the

4. For reproducibility purposes, the precise version of VW used to run these experiments is available at

https://github.com/albietz/vowpal_wabbit/tree/bakeoff.

5. https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html

12

A Contextual Bandit Bake-off

Table 1: Entry (row, column) shows the statistically signiﬁcant win-loss diﬀerence of row
against column. Encoding ﬁxed to -1/0 (top) or 0/1 (bottom). (left) held-out datasets
only (330 in total); ﬁxed hyperparameters, only the learning rate is optimized; (right)
all datasets (530 in total); all choices of hyperparameters are optimized on each dataset
(diﬀerent methods have diﬀerent hyperparameter settings from 1 to 18, see Table 9 in
Appendix B).

↓ vs → G RO C-nu B-g
50
-7
G
49
-
RO
22
-26
C-nu
-
-49
B-g
-17
-68
(cid:15)G

10
26
-
-22
-57

-
7
-10
-50
-54

↓ vs → G RO C-nu B-g
-22
-30
G
8
-
RO
64
37
C-nu
-
-8
B-g
-86
-84
(cid:15)G

-93
-37
-
-64
-145

-
30
93
22
-58

(cid:15)G
58
84
145
86
-

-1/0 encoding

↓ vs → G
-
G
64
RO
17
C-nu
-36
B-g
-52
(cid:15)G

RO C-nu B-g
-17
-64
36
100
45
-
45
-
-45
-45
-100
-
-19
-75
-120

↓ vs → G
-
G
124
RO
150
C-nu
75
B-g
-5
(cid:15)G

RO C-nu B-g
-75
-150
-124
65
-11
-
67
-
11
-
-67
-65
-83
-144
-125

(cid:15)G
5
125
144
83
-

0/1 encoding

(cid:15)G
54
68
57
17
-

(cid:15)G
52
120
75
19
-

following (heuristic) deﬁnition of signiﬁcance based on an approximate Z-test: if pa and pb
denote the PV loss of a and b on a given dataset of size n, then a wins over b if





1 − Φ

pa − pb

(cid:113) pa(1−pa)
n

+ pb(1−pb)
n



 < 0.05,

where Φ is the Gauss error function. We also deﬁne the signiﬁcant win-loss diﬀerence
of one algorithm against another to be the diﬀerence between the number of signiﬁcant
wins and signiﬁcant losses. We have found these metrics to provide more insight into the
behavior of diﬀerent methods, compared to strategies based on aggregation of loss measures
across all datasets. Indeed, we often found the relative performance of two methods to vary
signiﬁcantly across datasets, making aggregate metrics less informative.

Results in this section focus on Greedy (G), RegCB-optimistic (RO), Cover-NU (C-
nu), Bag-greedy (B-g) and (cid:15)-greedy ((cid:15)G), deferring other variants to Appendix B, as their
performance is typically comparable to or dominated by these methods. We combine results
on multiclass and multilabel datasets, but show them separately in Appendix B.

Eﬃcient exploration methods. Our experiments suggest that the best performing
method is the RegCB approach (Foster et al., 2018), as shown in Table 1 (left), where
the signiﬁcant wins of RO against all other methods exceed signiﬁcant losses, which yields
the best performance overall. This is particularly prominent with 0/1 encodings. With -1/0
encodings, which are generally preferred on our corpus as discussed below, the simple greedy
approach comes a close second, outperforming other methods on a large number of datasets,

13

Bietti, Agarwal and Langford

Table 2: Progressive validation loss for cost-sensitive datasets with real-valued costs in [0, 1].
Hyperparameters are ﬁxed as in Table 10. We show mean and standard error based on 10
diﬀerent random reshuﬄings. For RCV1, costs are based on tree distance to correct topics.
For MSLR and Yahoo, costs encode 5 regularly-spaced discrete relevance scores (0: perfectly
relevant, 1: irrelevant), and we include results for a loss encoding oﬀset c = −1 in Eq. (7).

G
0.215 ± 0.010

RO
0.225 ± 0.008

B-g
0.251 ± 0.005

(cid:15)G
0.230 ± 0.009

G
0.798 ± 0.0023

RO
0.794 ± 0.0007

B-g
0.799 ± 0.0013

(cid:15)G
0.807 ± 0.0020

G
0.791 ± 0.0012

RO
0.790 ± 0.0009

B-g
0.791 ± 0.0008

(cid:15)G
0.806 ± 0.0018

G
0.593 ± 0.0004

RO
0.592 ± 0.0005

B-g
0.596 ± 0.0006

(cid:15)G
0.598 ± 0.0005

G
0.589 ± 0.0005

RO
0.588 ± 0.0004

B-g
0.590 ± 0.0005

(cid:15)G
0.594 ± 0.0006

C-nu
0.215 ± 0.006
(a) RCV1

C-nu
0.798 ± 0.0012
(b) MSLR

C-nu
0.792 ± 0.0007
(c) MSLR, c = −1

C-nu
0.594 ± 0.0004
(d) Yahoo

C-nu
0.589 ± 0.0004
(e) Yahoo, c = −1

despite the lack of an explicit exploration mechanism. A possible reason for this success is
the diversity that is inherently present in the distribution of contexts across actions, which
has been shown to yield no-regret guarantees under various assumptions (Bastani et al.,
2017; Kannan et al., 2018). The noise induced by the dynamics of online learning and
random tie-breaking may also be a source of more exploration. RO and Greedy also show
strong performance on the 8 UCI datasets and the learning-to-rank datasets from Foster
et al. (2018), as shown in Tables 2 and 13. Nevertheless, both Greedy and RegCB have
known failure modes which the Cover approach is robust to by design. While the basic
approach with uniform exploration is too conservative, we found our Cover-NU variant
to be quite competitive overall. The randomization in its choice of actions yields explo-
ration logs which may be additionally used for oﬄine evaluation, in contrast to Greedy and
RegCB-opt, which choose actions deterministically. A more granular comparison of these
methods is given in Figure 2, which highlight the failure of Greedy and RegCB against
Cover-NU on some datasets which may be more diﬃcult perhaps due to a failure of mod-
eling assumptions. Bagging also outperforms other methods on some datasets, however it
is outperformed on most datasets, possibly because of the additional variance induced by
the bootstrap sampling. Table 1 (right) optimizes over hyperparameters for each dataset,
which captures the best potential of each method. Cover-NU does the best here, but also
has the most hyperparameters, indicating that a more adaptive variant could be desirable.
RegCB stays competitive, while Greedy pales possibly due to fewer hyperparameters.

Variability with dataset characteristics. Table 5 shows win-loss statistics for subsets
of the datasets with constraints on diﬀerent characteristics, such as number of actions,

14

A Contextual Bandit Bake-off

Figure 2: Pairwise comparisons among four successful methods: Greedy, Cover-nu, Bag-
greedy, and RegCB-opt. Hyperparameters ﬁxed as in Table 9, with encoding -1/0. All
held-out multiclass and multilabel datasets are shown, in contrast to Figure 1, which only
shows held-out datasets with 5 or more actions. The plots consider normalized loss, with
red points indicating signiﬁcant wins.

dimensionality, size, and performance in the supervised setting. The values for these splits
were chosen in order to have a reasonably balanced number of datasets in each table.

√

We ﬁnd that RegCB-opt is the preferred method in most situations, while Greedy and
Cover-NU can also provide good performance in diﬀerent settings. When only considering
larger datasets, RegCB-opt dominates all other methods, with Greedy a close second, while
Cover-NU seems to explore less eﬃciently. This could be related to the better adaptivity
properties of RegCB to favorable noise conditions, which can achieve improved (even log-
arithmic) regret (Foster et al., 2018), in contrast to Cover-NU, for which a slower rate (of
O(
n)) may be unavoidable since it is baked into the algorithm of Agarwal et al. (2014)
by design, and is reﬂected in the diversity terms of the costs ˆc in our online variant given
in Algorithm 4. In contrast, when n is small, RegCB-opt and Greedy may struggle to ﬁnd
good policies (in fact, their analysis typically requires a number of “warm-start” iterations
with uniform exploration), while Cover-NU seems to explore more eﬃciently from the be-
ginning, and behaves well with large action spaces or high-dimensional features. Finally,
Table 5(d,e) shows that Greedy can be the best choice when the dataset is “easy”, in the
sense that a supervised learning method achieves small loss. Achieving good performance
on such easy datasets is related to the open problem of Agarwal et al. (2017), and vari-
ants of methods designed to be agnostic to the data distribution—such as Cover(-NU) and
(cid:15)-Greedy (Agarwal et al., 2014; Langford and Zhang, 2008)—seem to be the weakest on
these datasets.

15

Bietti, Agarwal and Langford

Table 3: Impact of reductions for Bag (left) and (cid:15)-greedy (right), with hyperparameters
optimized and encoding ﬁxed to -1/0. Each (row, column) entry shows the statistically
signiﬁcant win-loss diﬀerence of row against column. IWR outperforms the other reductions
for both methods, which are the only two methods that directly reduce to oﬀ-policy learning,
and thus where such a comparison applies.

↓ vs → ips
-
ips
42
dr
59
iwr

dr
-42
-
28

iwr
-59
-28
-

↓ vs → ips
-
ips
-63
dr
133
iwr

dr
63
-
155

iwr
-133
-155
-

Table 4: Impact of encoding on diﬀerent algorithms, with hyperparameters optimized. Each
entry indicates the number of statistically signiﬁcant wins/losses of -1/0 against 0/1. -1/0
is the better overall choice of encoding, but 0/1 can be preferable on larger datasets (the
bottom row considers the 64 datasets in our corpus with more than 10,000 examples).

datasets
all
≥ 10,000

G
136 / 42
19 / 12

RO
60 / 47
10 / 18

C-nu
76 / 46
14 / 20

B-g
77 / 27
15 / 11

(cid:15)G
99 / 27
14 / 5

Reductions. Among the reduction mechanisms introduced in Section 2.3, IWR has de-
sirable properties such as tractability (the other reductions rely on a CSC objective, which
requires approximations due to non-convexity), and a computational cost that is indepen-
dent of the total number of actions, only requiring updates for the chosen action. In addition
to Greedy, which can be seen as using a form of IWR, we found IWR to work very well
for bagging and (cid:15)-greedy approaches, as shown in Table 3 (see also Table 9 in Appendix B,
which shows that IWR is also preferred when considering ﬁxed hyperparameters for these
methods). This may be attributed to the diﬃculty of the CSC problem compared to regres-
sion, as well as importance weight aware online updates, which can be helpful for small (cid:15).
Together with its computational beneﬁts, our results suggest that IWR is often a com-
pelling alternative to CSC reductions based on IPS or DR. In particular, when the number
of actions is prohibitively large for using Cover-NU or RegCB, Bag with IWR may be a
good default choice of exploration algorithm. While Cover-NU does not directly support
the IWR reduction, making them work together well would be a promising future direction.

Encodings. Table 4 indicates that the -1/0 encoding is preferred to 0/1 on many of the
datasets, and for all methods. We now give one possible explanation. As discussed in
Section 2.4, the -1/0 encoding yields low variance loss estimates when the cost is often
close to 1. For datasets with binary costs, since the learner may often be wrong in early
iterations, a cost of 1 is a good initial bias for learning. With enough data, however, the
learner should reach better accuracies and observe losses closer to 0, in which case the 0/1
encoding should lead to lower variance estimates, yielding better performance as observed
in Table 4. We tried shifting the loss range in the RCV1 dataset with real-valued costs
from [0, 1] to [−1, 0], but saw no improvements compared to the results in Table 2. Indeed,

16

A Contextual Bandit Bake-off

(a) all multiclass datasets

(b) n ≥ 10 000 only

Figure 3: Errors of IPS counterfactual estimates for the uniform random policy using explo-
ration logs collected by various algorithms on multiclass datasets. The boxes show quartiles
(with the median shown as a blue line) of the distribution of squared errors across all
multiclass datasets or only those with at least 10 000 examples. The logs are obtained by
running each algorithm with -1/0 encodings, ﬁxed hyperparameters from Table 9, and the
best learning rate on each dataset according to progressive validation loss.

a cost of 1 may not be a good initial guess in this case, in contrast to the binary cost
setting. On the MSLR and Yahoo learning-to-rank datasets, we do see some improvement
from shifting costs to the range [−1, 0], perhaps because in this case the costs are discrete
values, and the cost of 1 corresponds to the document label “irrelevant”, which appears
frequently in these datasets.

Counterfactual evaluation. After running an exploration algorithm, a desirable goal
for practitioners is to evaluate new policies oﬄine, given access to interaction logs from
exploration data. While various exploration algorithms rely on such counterfactual eval-
uation through a reduction, the need for eﬃcient exploration might restrict the ability to
evaluate arbitrary policies oﬄine, since the algorithm may only need to estimate “good”
policies in the sense of small regret with respect to the optimal policy. To illustrate this,
in Figure 3, we consider the evaluation of the uniform random stochastic policy, given by
πunif(a|x) = 1/K for all actions a = 1, . . . , K, using a simple inverse propensity scoring
(IPS) approach. While such a task is somewhat extreme and of limited practical interest
in itself, we use it mainly as a proxy for the ability to evaluate any arbitrary policy (in par-
ticular, it should be possible to evaluate any policy if we can evaluate the uniform random
policy). On a multiclass dataset, the expected loss of such a policy is simply 1 − 1/K, while
its IPS estimate is given by

ˆLIP S = 1 −

1
n

n
(cid:88)

t=1

1 − (cid:96)t(at)
Kpt(at)

,

where the loss is given by (cid:96)t(at) = 1{at (cid:54)= yt} when the correct multiclass label is yt.
Note that this quantity is well-deﬁned since the denominator is always non-zero when at is
sampled from pt, but the estimator is biased when data is collected by a method without
some amount of uniform exploration (i.e., when pt does not have full support). This bias

17

Bietti, Agarwal and Langford

is particularly evident in Figure 3b where epsilon-greedy shows very good counterfactual
evaluation performance while the other algorithms induce biased counterfactual evaluation
due to lack of full support. The plots in Figure 3 show the distribution of squared errors
( ˆLIP S − (1 − 1/K))2 across multiclass datasets. We consider IPS on the rewards 1 − (cid:96)t(at)
here as it is more adapted to the -1/0 encodings used to collect exploration logs, but we
also show IPS on losses in Figure 6 of Appendix B.2. Figure 3 shows that the more eﬃcient
exploration methods (Greedy, RegCB-optimistic, and Cover-NU) give poor estimates for
this policy, probably because their exploration logs provide biased estimates and are quite
focused on few actions that may be taken by good policies, while the uniform exploration in
(cid:15)-Greedy (and Cover-U, see Figure 6) yields better estimates, particularly on larger datasets.
The elimination version of RegCB provides slightly better logs compared to the optimistic
version (see Figure 6), and Bagging may also be preferred in this context. Overall, these
results show that there may be a trade-oﬀ between eﬃcient exploration and the ability to
perform good counterfactual evaluation, and that uniform exploration may be needed if one
would like to perform accurate oﬄine experiments for a broad range of questions.

5. Discussion and Takeaways

In this paper, we presented an evaluation of practical contextual bandit algorithms on a
large collection of supervised learning datasets with simulated bandit feedback. We ﬁnd that
a worst-case theoretical robustness forces several common methods to often over-explore,
damaging their empirical performance, and strategies that limit (RegCB and Cover-NU) or
simply forgo (Greedy) explicit exploration dominate the ﬁeld. For practitioners, our study
also provides a reference for practical implementations, while stressing the importance of
loss estimation and other design choices such as how to encode observed feedback.

Guidelines for practitioners. We now summarize some practical guidelines that come
out of our empirical study:

• Methods relying on modeling assumptions on the data distribution such as RegCB are
often preferred in practice, and even Greedy can work well (see, e.g., Table 1). They
tend to dominate more robust approaches such as Cover-NU even more prominently
on larger datasets, or on datasets where prediction is easy, e.g., due to low noise
(see Table 5). While it may be diﬃcult to assess such favorable conditions of the
data in advance, practitioners may use speciﬁc domain knowledge to design better
feature representations for prediction, which may in turn improve exploration for
these methods.

• Uniform exploration hurts empirical performance in most cases (see, e.g., the poor
performance of (cid:15)-greedy and Cover-u in Table 11 of Appendix B). Nevertheless, it
may be necessary on the hardest datasets, and may be crucial if one needs to perform
oﬀ-policy counterfactual evaluation (see Figures 3 and 6).

• Loss estimation is an essential component in many CB algorithms for good practical
performance, and DR should be preferred over IPS. For methods based on reduction
to oﬀ-policy learning, such as (cid:15)-Greedy and Bagging, the IWR reduction is typically
best, in addition to providing computational beneﬁts (see Table 3).

18

A Contextual Bandit Bake-off

• From our early experiments, we found randomization on tied choices of actions to
always be useful. For instance, it avoids odd behavior which may arise from deter-
ministic, implementation-speciﬁc biases (e.g., always favoring one speciﬁc action over
the others).

• The choice of cost encodings makes a big diﬀerence in practice and should be carefully
considered when designing an contextual bandit problem, even when loss estimation
techniques such as DR are used. For binary outcomes, -1/0 is a good default choice
of encoding in the common situation where the observed loss is often 1 (see Table 4).

• Modeling choices and encodings sometimes provide pessimistic initial estimates that
can hurt initial exploration on some problems, particularly for Greedy and RegCB-
optimistic. Random tie-breaking as well as using a shared additive baseline can help
mitigate this issue (see Section B.3).

• The hyperparameters highlighted in Appendix B.1 obtained on our datasets may be
good default choices in practice. Nevertheless, these may need to be balanced in
order to address other conﬂicting requirements in real-world settings, such as non-
stationarity or the ability to run oﬄine experiments.

Open questions for theoreticians. Our study raises some questions of interest for
theoretical research on contextual bandits. The good performance of greedy methods calls
for a better understanding of greedy methods, building upon the work of Bastani et al.
(2017); Kannan et al. (2018), as well as methods that are more robust to more diﬃcult
datasets while adapting to such favorable scenarios, such as when the context distribution
has enough diversity. A related question is that of adaptivity to easy datasets for which
the optimal policy has small loss, an open problem pointed out by Agarwal et al. (2017)
in the form of “ﬁrst-order” regret bounds. While methods satisfying such regret bounds
have now been developed theoretically (Allen-Zhu et al., 2018), these methods are currently
not computationally eﬃcient, and obtaining eﬃcient methods based on optimization oracles
remains an important open problem. We also note that while our experiments are based on
online optimization oracles, most analyzed versions of the algorithms rely on solving the full
optimization problems; it would be interesting to better understand the behavior of online
variants, and to characterize the implicit exploration eﬀect for the greedy method.

Limitations of the study. Our study is primarily concerned with prediction perfor-
mance, while real world applications often additionally consider the value of counterfactual
evaluation for oﬄine policy evaluation.

A key limitation of our study is that it is concerned with stationary datasets. Many real-
world contextual bandit applications involve nonstationary datasources. This limitation
is simply due to the nature of readily available public datasets. The lack of public CB
datasets as well as challenges in counterfactual evaluation of CB algorithms make a more
realistic study challenging, but we hope that an emergence of platforms (Agarwal et al.,
2016; Jamieson et al., 2015) to easily deploy CB algorithms will enable studies with real
CB datasets in the future.

19

Bietti, Agarwal and Langford

References

Y. Abbasi-Yadkori, D. P´al, and C. Szepesv´ari. Improved algorithms for linear stochastic

bandits. In Advances in Neural Information Processing Systems (NIPS), 2011.

A. Agarwal, M. Dud´ık, S. Kale, J. Langford, and R. E. Schapire. Contextual bandit learning
In Proceedings of the International Conference on Artiﬁcial

with predictable rewards.
Intelligence and Statistics (AISTATS), 2012.

A. Agarwal, D. Hsu, S. Kale, J. Langford, L. Li, and R. E. Schapire. Taming the monster: A
fast and simple algorithm for contextual bandits. arXiv preprint arXiv:1402.0555, 2014.

A. Agarwal, S. Bird, M. Cozowicz, L. Hoang, J. Langford, S. Lee, J. Li, D. Melamed,
arXiv preprint

G. Oshri, O. Ribas, et al. A multiworld testing decision service.
arXiv:1606.03966, 2016.

A. Agarwal, A. Krishnamurthy, J. Langford, H. Luo, et al. Open problem: First-order
regret bounds for contextual bandits. In Conference on Learning Theory (COLT), 2017.

S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoﬀs.
In Proceedings of the International Conference on Machine Learning (ICML), 2013.

Z. Allen-Zhu, S. Bubeck, and Y. Li. Make the minority great again: First-order regret
bound for contextual bandits. In Proceedings of the International Conference on Machine
Learning (ICML), 2018.

H. Bastani, M. Bayati, and K. Khosravi. Mostly exploration-free algorithms for contextual

bandits. arXiv preprint arXiv:1704.09011, 2017.

A. Blum, A. Kalai, and J. Langford. Beating the hold-out: Bounds for k-fold and progressive

cross-validation. In Conference on Learning Theory (COLT), 1999.

O. Chapelle and L. Li. An empirical evaluation of thompson sampling.

In Advances in

Neural Information Processing Systems (NIPS), 2011.

J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine Learning Research (JMLR), 12(Jul):
2121–2159, 2011.

M. Dudik, D. Hsu, S. Kale, N. Karampatziakis, J. Langford, L. Reyzin, and T. Zhang. Eﬃ-
cient optimal learning for contextual bandits. In Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), 2011a.

M. Dudik, J. Langford, and L. Li. Doubly robust policy evaluation and learning.
Proceedings of the International Conference on Machine Learning (ICML), 2011b.

In

D. Eckles and M. Kaptein. Thompson sampling with the online bootstrap. arXiv preprint

arXiv:1410.4009, 2014.

20

A Contextual Bandit Bake-off

D. J. Foster, A. Agarwal, M. Dud´ık, H. Luo, and R. E. Schapire. Practical contextual ban-
dits with regression oracles. In Proceedings of the International Conference on Machine
Learning (ICML), 2018.

S. Hanneke. Theory of disagreement-based active learning. Foundations and Trends in

Machine Learning, 7(2-3), 2014.

X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers,
et al. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the
Eighth International Workshop on Data Mining for Online Advertising, 2014.

D. J. Hsu. Algorithms for active learning. PhD thesis, UC San Diego, 2010.

T.-K. Huang, A. Agarwal, D. J. Hsu, J. Langford, and R. E. Schapire. Eﬃcient and
In Advances in Neural Information Processing

parsimonious agnostic active learning.
Systems (NIPS), 2015.

K. G. Jamieson, L. Jain, C. Fernandez, N. J. Glattard, and R. Nowak. Next: A system for
real-world development, evaluation, and application of active learning. In Advances in
Neural Information Processing Systems (NIPS), 2015.

S. M. Kakade and A. Tewari. On the generalization ability of online strongly convex pro-
gramming algorithms. In Advances in Neural Information Processing Systems (NIPS),
2009.

S. Kannan, J. Morgenstern, A. Roth, B. Waggoner, and Z. S. Wu. A smoothed analysis
of the greedy algorithm for the linear contextual bandit problem. In Advances in Neural
Information Processing Systems (NIPS), 2018.

N. Karampatziakis and J. Langford. Online importance weight aware updates. In Confer-

ence on Uncertainty in Artiﬁcial Intelligence (UAI), 2011.

A. Krishnamurthy, A. Agarwal, T.-K. Huang, H. Daume III, and J. Langford. Active
learning for cost-sensitive classiﬁcation. Journal of Machine Learning Research (JMLR),
20(65):1–50, 2019.

J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side

information. In Advances in Neural Information Processing Systems (NIPS), 2008.

L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to person-
alized news article recommendation. In Proceedings of the 19th international conference
on World wide web. ACM, 2010.

P. Massart, ´E. N´ed´elec, et al. Risk bounds for statistical learning. The Annals of Statistics,

34(5), 2006.

H. B. McMahan, G. Holt, D. Sculley, M. Young, D. Ebner, J. Grady, L. Nie, T. Phillips,
E. Davydov, D. Golovin, et al. Ad click prediction: a view from the trenches. In Proceed-
ings of the 19th ACM international conference on Knowledge discovery and data mining
(KDD), 2013.

21

Bietti, Agarwal and Langford

I. Osband and B. Van Roy. Bootstrapped thompson sampling and deep exploration. arXiv

preprint arXiv:1507.00300, 2015.

N. C. Oza and S. Russell. Online bagging and boosting. In Proceedings of the International

Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2001.

Z. Qin, V. Petricek, N. Karampatziakis, L. Li, and J. Langford. Eﬃcient online bootstrap-
ping for large scale learning. In Workshop on Parallel and Large-scale Machine Learning
(BigLearning@NIPS), 2013.

S. Ross, P. Mineiro, and J. Langford. Normalized online learning. In Conference on Uncer-

tainty in Artiﬁcial Intelligence (UAI), 2013.

D. J. Russo, B. Van Roy, A. Kazerouni, I. Osband, Z. Wen, et al. A tutorial on thompson

sampling. Foundations and Trends R(cid:13) in Machine Learning, 11(1):1–96, 2018.

T. Schnabel, A. Swaminathan, A. Singh, N. Chandak, and T. Joachims. Recommendations
In Proceedings of the International

as treatments: debiasing learning and evaluation.
Conference on Machine Learning (ICML), 2016.

A. Swaminathan and T. Joachims. The self-normalized estimator for counterfactual learn-

ing. In Advances in Neural Information Processing Systems (NIPS), 2015.

W. R. Thompson. On the likelihood that one unknown probability exceeds another in view

of the evidence of two samples. Biometrika, 25(3/4), 1933.

22

A Contextual Bandit Bake-off

Table 5: Statistically signiﬁcant win-loss diﬀerence with hyperparameters ﬁxed as in Ta-
ble 10, encodings ﬁxed to -1/0, on all held-out datasets or subsets with diﬀerent charac-
teristics: (a) number of actions K; (b) number of features d; (c) number of examples n;
(d) PV loss of the one-against-all (OAA) method. The corresponding table for all held-out
datasets is shown in Table 1(top left).

(cid:15)G
18
25
26
-6
-

(cid:15)G
10
21
17
-7
-

(cid:15)G
53
60
37
18
-

↓ vs → G RO C-nu B-g
22
-5
G
24
-
RO
25
C-nu
-5
-
-24
B-g
6
-25
(cid:15)G

-
5
6
-22
-18

-6
5
-
-25
-26

↓ vs → G RO C-nu B-g
15
1
G
14
-
RO
22
0
C-nu
-
-14
B-g
9
-9
(cid:15)G

-4
0
-
-22
-13

-
-1
4
-15
-7

(a) K ≥ 3 (left, 89 datasets), K ≥ 10 (right, 36 datasets)

↓ vs → G RO C-nu B-g
20
-5
G
26
-
RO
20
-8
C-nu
-
-26
B-g
7
-21
(cid:15)G

-
5
1
-20
-10

-1
8
-
-20
-17

↓ vs → G RO C-nu B-g
-5
G
-
RO
-1
C-nu
-8
B-g
-8
(cid:15)G

-6
1
-
-15
-7

7
8
15
-
6

-
5
6
-7
0

(b) d ≥ 100 (left, 76 datasets), d ≥ 10 000 (right, 41 datasets)

↓ vs → G RO C-nu B-g
38
-4
G
40
-
RO
9
-33
C-nu
-
-40
B-g
-18
-60
(cid:15)G

23
33
-
-9
-37

-
4
-23
-38
-53

↓ vs → G RO C-nu B-g
10
-3
G
14
-
RO
2
-17
C-nu
-
-14
B-g
-25
-36
(cid:15)G

16
17
-
-2
-30

-
3
-16
-10
-34

(c) n ≥ 1 000 (left, 119 datasets), n ≥ 10 000 (right, 43 datasets)

↓ vs → G RO C-nu B-g
40
1
G
36
-
RO
7
-26
C-nu
-
-36
B-g
-4
-43
(cid:15)G

↓ vs → G RO C-nu B-g
8
1
G
5
-
RO
-12
-12
C-nu
-
-5
B-g
-10
-16
(cid:15)G
(d) P VOAA ≤ 0.2 (left, 135 datasets), P VOAA ≤ 0.05 (right, 28 datasets)

(cid:15)G
36
43
24
4
-

25
26
-
-7
-24

-
-1
-25
-40
-36

-
-1
-14
-8
-15

14
12
-
12
-5

(cid:15)G
7
9
13
-9
-

(cid:15)G
0
8
7
-6
-

(cid:15)G
34
36
30
25
-

(cid:15)G
15
16
5
10
-

↓ vs → G RO C-nu B-g
5
2
G
4
-
RO
-1
-8
C-nu
-
-4
B-g
-11
-12
(cid:15)G

8
8
-
1
-12

-
-2
-8
-5
-12

(cid:15)G
12
12
12
11
-

(e) n ≥ 10 000 and P VOAA ≤ 0.1 (13 datasets)

23

Bietti, Agarwal and Langford

Appendix A. Datasets

This section gives some details on the cost-sensitive classiﬁcation datasets considered in our
study.

Multiclass classiﬁcation datasets. We consider 525 multiclass datasets from the openml.
org platform, including among others, medical, gene expression, text, sensory or synthetic
data. Table 6 provides some statistics about these datasets. These also include the 8 clas-
siﬁcation datasets considered in (Foster et al., 2018) from the UCI database. The full list
of datasets is given below.

Table 6: Statistics on number of multiclass datasets by number of examples, actions and
unique features, as well as by progressive validation 0-1 loss for the supervised one-against-
all online classiﬁer, in our collection of 525 multiclass datasets.

actions #
404
73
48

2
3-9
10+

examples #
94
270
133
28

≤ 102
102-103
103-105
> 105

features
≤ 50
51-100
101-1000
1000+

#
392
35
17
81

P VOAA
≤ 0.01
(0.01, 0.1]
(0.1, 0.2]
(0.2, 0.5]
> 0.5

#
10
88
103
273
51

Multilabel classiﬁcation datasets. We consider 5 multilabel datasets from the LibSVM
website6, listed in Table 7.

Table 7: List of multilabel datasets.

Dataset # examples # features # actions P VOAA
0.1664
mediamill
0.0446
rcv1
0.0066
scene
0.1661
tmc
0.2553
yeast

30,993
23,149
1,211
21,519
1,500

120
47,236
294
30,438
103

101
103
6
22
14

Cost-sensitive classiﬁcation datasets. For more general real-valued costs in [0, 1], we
use a modiﬁcation of the multilabel RCV1 dataset introduced in (Krishnamurthy et al.,
2019). Each example consists of a news article labeled with the topics it belongs to, in
a collection of 103 topics. Instead of ﬁxing the cost to 1 for incorrect topics, the cost is
deﬁned as the tree distance to the set of correct topics in a topic hierarchy.

We also include the learning-to-rank datasets considered in (Foster et al., 2018), where
we limit the number of documents (actions) per query, and consider all the training folds.
We convert relevance scores to losses in {0, 0.25, 0.5, 0.75, 1}, with 0 indicating a perfectly
relevant document, and 1 an irrelevant one. The datasets considered are the Microsoft

6. https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html

24

A Contextual Bandit Bake-off

Learning to Rank dataset, variant MSLR-30K at https://www.microsoft.com/en-us/
research/project/mslr/, and the Yahoo! Learning to Rank Challenge V2.0, variant C14B
at https://webscope.sandbox.yahoo.com/catalog.php?datatype=c. Details are shown
in Table 8. We note that for these datasets we consider action-dependent features, with a
ﬁxed parameter vector for all documents.

Table 8: Learning to rank datasets.

Dataset
MSLR-30K
Yahoo

# examples # features max # documents P VOAA
0.7892
0.5876

31,531
36,251

136
415

10
6

List of multiclass datasets. The datasets we used can be accessed at https://www.
openml.org/d/<id>, with id in the following list:

3, 6, 8, 10, 11, 12, 14, 16, 18, 20, 21, 22, 23, 26, 28, 30, 31, 32, 36, 37, 39, 40, 41, 43,
44, 46, 48, 50, 53, 54, 59, 60, 61, 62, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161,
162, 180, 181, 182, 183, 184, 187, 189, 197, 209, 223, 227, 273, 275, 276, 277, 278, 279, 285,
287, 292, 293, 294, 298, 300, 307, 310, 312, 313, 329, 333, 334, 335, 336, 337, 338, 339, 343,
346, 351, 354, 357, 375, 377, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395,
396, 397, 398, 399, 400, 401, 444, 446, 448, 450, 457, 458, 459, 461, 462, 463, 464, 465, 467,
468, 469, 472, 475, 476, 477, 478, 479, 480, 554, 679, 682, 683, 685, 694, 713, 714, 715, 716,
717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735,
736, 737, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756,
758, 759, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777,
778, 779, 780, 782, 783, 784, 785, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 799,
800, 801, 803, 804, 805, 806, 807, 808, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821,
822, 823, 824, 825, 826, 827, 828, 829, 830, 832, 833, 834, 835, 836, 837, 838, 841, 843, 845,
846, 847, 848, 849, 850, 851, 853, 855, 857, 859, 860, 862, 863, 864, 865, 866, 867, 868, 869,
870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 884, 885, 886, 888, 891, 892,
893, 894, 895, 896, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914,
915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 931, 932, 933, 934,
935, 936, 937, 938, 941, 942, 943, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956,
958, 959, 962, 964, 965, 969, 970, 971, 973, 974, 976, 977, 978, 979, 980, 983, 987, 988, 991,
994, 995, 996, 997, 1004, 1005, 1006, 1009, 1011, 1012, 1013, 1014, 1015, 1016, 1019, 1020,
1021, 1022, 1025, 1026, 1036, 1038, 1040, 1041, 1043, 1044, 1045, 1046, 1048, 1049, 1050,
1054, 1055, 1056, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1071,
1073, 1075, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1100,
1104, 1106, 1107, 1110, 1113, 1115, 1116, 1117, 1120, 1121, 1122, 1123, 1124, 1125, 1126,
1127, 1128, 1129, 1130, 1131, 1132, 1133, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142,
1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157,
1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1169, 1216, 1217, 1218, 1233, 1235,
1236, 1237, 1238, 1241, 1242, 1412, 1413, 1441, 1442, 1443, 1444, 1449, 1451, 1453, 1454,
1455, 1457, 1459, 1460, 1464, 1467, 1470, 1471, 1472, 1473, 1475, 1481, 1482, 1483, 1486,
1487, 1488, 1489, 1496, 1498, 1590.

25

Bietti, Agarwal and Langford

Table 9: Choices of hyperparameters and reduction for each method. Fixed choices of hy-
perparameters for -1/0 encodings are in bold. These were obtained for each method with
an instant-runoﬀ voting mechanism on 200 of the multiclass datasets with -1/0 encoding,
where each dataset ranks hyperparameter choices according to the diﬀerence between sig-
niﬁcant wins and losses against all other choices (the vote of each dataset is divided by the
number of tied choices ranked ﬁrst). Table 10 shows optimized choices of hyperparameters
for diﬀerent encoding settings used in our study.

Name
G

Method
Greedy

R/RO RegCB-elim/RegCB-opt

C-nu

Cover-NU

C-u

B/B-g
(cid:15)G

Cover

Bag/Bag-greedy
(cid:15)-greedy

A

active (cid:15)-greedy

Hyperparameters
-
C0 ∈ 10−{1,2,3}
N ∈ {4, 8, 16}
ψ ∈ {0.01, 0.1, 1}
N ∈ {4, 8, 16}
ψ ∈ {0.01, 0.1, 1}
N ∈ {4, 8, 16}
(cid:15) ∈ {0.02, 0.05, 0.1}
(cid:15) ∈ {0.02, 1}
C0 ∈ 10−{2,4,6}

Reduction
IWR
-

IPS/DR

IPS/DR

IPS/DR/IWR
IPS/DR/IWR

IPS/DR/IWR

Appendix B. Evaluation Details

B.1 Algorithms and Hyperparameters

We ran each method on every dataset with the following hyperparameters:

• algorithm-speciﬁc hyperparameters, shown in Table 9.

• 9 choices of learning rates, on a logarithmic grid from 0.001 to 10 (see Section 2.4).

• 3 choices of reductions: IPS, DR and IWR (see Section 2.3). Note that these mainly
apply to methods that reduce to oﬀ-policy optimization (i.e., ((cid:15)-)greedy and bagging),
and to some extent, methods that reduce to cost-sensitive classiﬁcation (i.e., cover
and active (cid:15)-greedy, though the IWR reduction is heuristic in this case). Both RegCB
variants directly reduce to regression.

• 3 choices of loss encodings: 0/1, -1/0 and 9/10 (see Eq. (7)). 0/1 and -1/0 encodings
are typically a design choice, while the experiments with 9/10 are aimed at assessing
some robustness to loss range.

B.2 Additional Evaluation Results

This sections provides additional experimental results, and more detailed win/loss statistics
for tables in the main paper, showing both signiﬁcant wins and signiﬁcant losses, rather
than just their diﬀerence.

26

A Contextual Bandit Bake-off

Table 10: Optimized choices of hyperparameters for diﬀerent encoding settings, obtained
using the voting mechanism described in Table 9: -1/0 (same as bold choices in Table 9, used
in Tables 1(top left), 2ce, 5, 11a, 12a, 13a and in the ﬁgures); 0/1 (used in Tables 1(bottom
left), 2abd, 11b, 12b, 13b, 14).

Algorithm
G
R/RO
C-nu
C-u
B
B-g
(cid:15)G
A

-1/0
-
C0 = 10−3
N = 4, ψ = 0.1, DR
N = 4, ψ = 0.1, IPS
N = 4, IWR
N = 4, IWR
(cid:15) = 0.02, IWR

0/1
-
C0 = 10−3
N = 4, ψ = 0.01, DR
N = 4, ψ = 0.1, DR
N = 16, IWR
N = 8, IWR
(cid:15) = 0.02, IWR

(cid:15) = 0.02, C0 = 10−6, IWR (cid:15) = 0.02, C0 = 10−6, IWR

Table 11: Statistically signiﬁcant wins / losses of all methods on the 324 held-out multiclass
classiﬁcation datasets. Hyperparameters are ﬁxed as given in Table 10.

↓ vs →
G
R
RO
C-nu
B
B-g
(cid:15)G
C-u
A

↓ vs →
G
R
RO
C-nu
B
B-g
(cid:15)G
C-u
A

G
-
25 / 22
16 / 8
33 / 41
15 / 77
16 / 65
6 / 59
10 / 159
10 / 33

G
-
66 / 27
69 / 5
51 / 36
38 / 75
39 / 72
21 / 69
30 / 151
19 / 36

R
22 / 25
-
23 / 16
31 / 49
19 / 74
20 / 61
12 / 70
11 / 159
15 / 48

R
27 / 66
-
38 / 15
24 / 42
12 / 87
16 / 83
19 / 108
4 / 172
22 / 77

RO
8 / 16
16 / 23
-
21 / 47
13 / 76
13 / 60
4 / 71
6 / 166
7 / 46

RO
5 / 69
15 / 38
-
13 / 59
7 / 109
9 / 105
3 / 121
3 / 175
4 / 89

C-nu
41 / 33
49 / 31
47 / 21
-
26 / 66
31 / 51
19 / 75
10 / 159
29 / 50

B
77 / 15
74 / 19
76 / 13
66 / 26
-
32 / 11
41 / 46
16 / 126
54 / 26
(a) -1/0 encoding

C-nu
36 / 51
42 / 24
59 / 13
-
27 / 83
29 / 72
25 / 96
6 / 170
31 / 63

B
75 / 38
87 / 12
109 / 7
83 / 27
-
34 / 21
49 / 59
14 / 131
67 / 37
(b) 0/1 encoding

B-g
65 / 16
61 / 20
60 / 13
51 / 31
11 / 32
-
31 / 49
11 / 130
42 / 27

(cid:15)G
59 / 6
70 / 12
71 / 4
75 / 19
46 / 41
49 / 31
-
14 / 125
37 / 2

C-u
159 / 10
159 / 11
166 / 6
159 / 10
126 / 16
130 / 11
125 / 14
-
152 / 9

B-g
72 / 39
83 / 16
105 / 9
72 / 29
21 / 34
-
46 / 65
18 / 129
54 / 42

(cid:15)G
69 / 21
108 / 19
121 / 3
96 / 25
59 / 49
65 / 46
-
28 / 122
43 / 3

C-u
151 / 30
172 / 4
175 / 3
170 / 6
131 / 14
129 / 18
122 / 28
-
151 / 22

A
33 / 10
48 / 15
46 / 7
50 / 29
26 / 54
27 / 42
2 / 37
9 / 152
-

A
36 / 19
77 / 22
89 / 4
63 / 31
37 / 67
42 / 54
3 / 43
22 / 151
-

Extended tables. Tables 11 and 12 are extended versions of Table 1, showing both sig-
niﬁcant wins and loss, more methods, and separate statistics for multiclass and multilabel
datasets. In particular, we can see that both variants of RegCB become even more com-
petitive against all other methods when using 0/1 encodings. Table 14 extends Table 2(a)
with additional methods. Table 15 is a more detailed win/loss version of Table 3, and
additionally shows statistics for 0/1 encodings.

We also show separate statistics in Table 13 for the 8 datasets from the UCI repository
considered in (Foster et al., 2018), which highlight that Greedy can outperform RegCB on
some of these datasets, and that the optimistic variant of RegCB is often superior to the
elimination variant. We note that our experimental setup is quite diﬀerent from Foster et al.
(2018), who consider batch learning on an doubling epoch schedule, which might explain
some of the diﬀerences in the results.

27

Bietti, Agarwal and Langford

Table 12: Statistically signiﬁcant wins / losses of all methods on the 5 multilabel classiﬁca-
tion datasets. Hyperparameters are ﬁxed as given in Table 10.

↓ vs → G
-
G
2 / 2
R
0 / 1
RO
1 / 3
C-nu
2 / 2
B
2 / 3
B-g
1 / 2
(cid:15)G
1 / 4
C-u
1 / 2
A

↓ vs → G
-
G
1 / 3
R
2 / 2
RO
3 / 1
C-nu
0 / 4
B
1 / 4
B-g
0 / 4
(cid:15)G
0 / 5
C-u
0 / 3
A

R
2 / 2
-
2 / 2
0 / 2
0 / 3
0 / 3
2 / 3
0 / 5
2 / 3

R
3 / 1
-
3 / 0
4 / 1
2 / 2
3 / 2
2 / 2
1 / 4
2 / 2

RO C-nu
3 / 1
1 / 0
2 / 0
2 / 2
2 / 2
-
-
2 / 2
1 / 3
2 / 2
1 / 3
1 / 3
2 / 3
1 / 2
0 / 5
1 / 4
2 / 3
1 / 2

B
2 / 2
3 / 0
2 / 2
3 / 1
-
1 / 1
2 / 3
0 / 5
2 / 3
(a) -1/0 encoding

RO C-nu
1 / 3
2 / 2
1 / 4
0 / 3
1 / 2
-
-
2 / 1
1 / 4
0 / 5
1 / 3
0 / 4
0 / 4
1 / 3
1 / 4
0 / 5
1 / 4
1 / 3

B
4 / 0
2 / 2
5 / 0
4 / 1
-
2 / 0
2 / 1
1 / 2
3 / 1
(b) 0/1 encoding

B-g
3 / 2
3 / 0
3 / 1
3 / 1
1 / 1
-
3 / 2
1 / 4
3 / 2

B-g
4 / 1
2 / 3
4 / 0
3 / 1
0 / 2
-
2 / 2
1 / 4
3 / 1

(cid:15)G
2 / 1
3 / 2
2 / 1
3 / 2
3 / 2
2 / 3
-
1 / 4
0 / 1

(cid:15)G
4 / 0
2 / 2
3 / 1
4 / 0
1 / 2
2 / 2
-
1 / 4
1 / 0

C-u
4 / 1
5 / 0
4 / 1
5 / 0
5 / 0
4 / 1
4 / 1
-
4 / 1

C-u
5 / 0
4 / 1
5 / 0
4 / 1
2 / 1
4 / 1
4 / 1
-
5 / 0

A
2 / 1
3 / 2
2 / 1
3 / 2
3 / 2
2 / 3
1 / 0
1 / 4
-

A
3 / 0
2 / 2
3 / 1
4 / 1
1 / 3
1 / 3
0 / 1
0 / 5
-

Varying C0 in RegCB-opt and active (cid:15)-greedy. Figure 4 shows a comparison be-
tween RegCB-opt and Greedy or Cover-NU on our corpus, for diﬀerent values of C0, which
controls the level of exploration through the width of conﬁdence bounds. Figure 5 shows the
improvements that the active (cid:15)-greedy algorithm can achieve compared to (cid:15)-greedy, under
diﬀerent settings.

Counterfactual evaluation. Figure 6 extends Figure 3 to include all algorithms, and
additionally shows results of using IPS estimates directly on the losses (cid:96)t(at) instead of
rewards 1 − (cid:96)t(at), which tend to be signiﬁcantly worse.

B.3 Shared baseline parameterization

We also experimented with the use of an action-independent additive baseline term in our
loss estimators, which can help learn better estimates with fewer samples in some situations.
In this case the regressors take the form f (x, a) = θ0+θ(cid:62)
a x (DR).
In order to learn the baseline term more quickly, we propose to use a separate online update
for the parameters θ0 or φ0 to regress on observed losses, followed by an online update on
the residual for the action-dependent part. We scale the step-size of these baseline updates
by the largest observed magnitude of the loss, in order to adapt to the observed loss range
for normalized updates (Ross et al., 2013).

a x (IWR) or ˆ(cid:96)(x, a) = φ0+φ(cid:62)

28

A Contextual Bandit Bake-off

Table 13: Statistically signiﬁcant wins / losses of all methods on the 8 classiﬁcation datasets
from the UCI repository considered in (Foster et al., 2018). Hyperparameters are ﬁxed as
given in Table 10.

↓ vs → G
-
G
0 / 4
R
1 / 2
RO
2 / 5
C-nu
0 / 5
B
1 / 4
B-g
0 / 6
(cid:15)G
0 / 6
C-u
0 / 5
A

↓ vs → G
-
G
5 / 1
R
5 / 0
RO
2 / 2
C-nu
2 / 3
B
1 / 3
B-g
1 / 3
(cid:15)G
0 / 7
C-u
1 / 2
A

R
4 / 0
-
1 / 0
1 / 3
1 / 4
1 / 3
2 / 4
0 / 6
2 / 2

R
1 / 5
-
1 / 0
0 / 5
0 / 6
0 / 6
0 / 6
0 / 8
0 / 5

RO C-nu
5 / 2
2 / 1
3 / 1
0 / 1
4 / 1
-
-
1 / 4
1 / 3
0 / 5
2 / 2
0 / 5
2 / 3
0 / 5
0 / 7
0 / 6
3 / 2
0 / 3

B
5 / 0
4 / 1
5 / 0
3 / 1
-
2 / 0
2 / 3
0 / 6
2 / 2
(a) -1/0 encoding

RO C-nu
2 / 2
0 / 5
5 / 0
0 / 1
5 / 0
-
-
0 / 5
1 / 2
0 / 6
0 / 3
0 / 6
1 / 2
0 / 7
0 / 7
0 / 7
1 / 1
0 / 6

B
3 / 2
6 / 0
6 / 0
2 / 1
-
0 / 3
3 / 1
0 / 7
3 / 0
(b) 0/1 encoding

B-g
4 / 1
3 / 1
5 / 0
2 / 2
0 / 2
-
1 / 4
0 / 6
2 / 2

B-g
3 / 1
6 / 0
6 / 0
3 / 0
3 / 0
-
3 / 1
0 / 7
3 / 0

(cid:15)G
6 / 0
4 / 2
5 / 0
3 / 2
3 / 2
4 / 1
-
0 / 6
2 / 0

(cid:15)G
3 / 1
6 / 0
7 / 0
2 / 1
1 / 3
1 / 3
-
0 / 7
1 / 0

C-u
6 / 0
6 / 0
6 / 0
7 / 0
6 / 0
6 / 0
6 / 0
-
6 / 0

C-u
7 / 0
8 / 0
7 / 0
7 / 0
7 / 0
7 / 0
7 / 0
-
7 / 0

A
5 / 0
2 / 2
3 / 0
2 / 3
2 / 2
2 / 2
0 / 2
0 / 6
-

A
2 / 1
5 / 0
6 / 0
1 / 1
0 / 3
0 / 3
0 / 1
0 / 7
-

Table 14: Progressive validation loss for RCV1 with real-valued costs. Same as Table 2(a),
but with all methods. Hyperparameters are ﬁxed as given in Table 10. The learning rate
is optimized once on the original dataset, and we show mean and standard error based on
10 diﬀerent random reshuﬄings of the dataset.

G
0.215 ± 0.010

R
0.408 ± 0.003

RO
0.225 ± 0.008

C-nu
0.215 ± 0.006

C-u
0.570 ± 0.023

B
0.256 ± 0.006

B-g
0.251 ± 0.005

(cid:15)G
0.230 ± 0.009

A
0.230 ± 0.010

Such an additive baseline can be helpful to quickly adapt to a constant loss estimate
thanks to the separate online update. This appears particularly useful with the -1/0 en-
coding, for which the initialization at 0 may give pessimistic loss estimates which can be
damaging in particular for the greedy method, that often gets some initial exploration from
an optimistic cost encoding. This can be seen in Figure 7(top). Table 16 shows that op-
timizing over the use of baseline on each dataset can improve the performance of Greedy
and RegCB-opt when compared to other methods such as Cover-NU.

In an online learning setting, baseline can also help to quickly reach an unknown target
range of loss estimates. This is demonstrated in Figure 7(bottom), where the addition

29

Bietti, Agarwal and Langford

Table 15: Impact of reductions for Bag (left) and (cid:15)-greedy (right), with hyperparameters
optimized and encoding ﬁxed to -1/0 or 0/1. Extended version of Table 3. Each (row,
column) entry shows the statistically signiﬁcant wins and losses of row against column.

↓ vs →
ips
dr
iwr

ips
-
72 / 30
84 / 25

dr
30 / 72
-
58 / 30

iwr
25 / 84
30 / 58
-

↓ vs →
ips
dr
iwr

ips
-
22 / 85
149 / 16

dr
85 / 22
-
164 / 9

iwr
16 / 149
9 / 164
-

(a) -1/0 encoding

↓ vs →
ips
dr
iwr

ips
-
240 / 46
245 / 17

dr
46 / 240
-
97 / 33

iwr
17 / 245
33 / 97
-

↓ vs →
ips
dr
iwr

ips
-
136 / 40
182 / 36

dr
40 / 136
-
148 / 23

iwr
36 / 182
23 / 148
-

(b) 0/1 encoding

Figure 4: Comparison of RegCB-opt with Greedy (top) and Cover-NU (bottom) for diﬀerent
values of C0. Hyperparameters for Greedy and Cover-NU ﬁxed as in Table 9. Encoding
ﬁxed to -1/0. The plots consider normalized loss on held-out datasets, with red points
indicating signiﬁcant wins.

of baseline is shown to help various methods with 9/10 encodings on a large number of
datasets. We do not evaluate RegCB for 9/10 encodings as it needs a priori known upper
and lower bounds on costs.

30

Table 16: Statistically signiﬁcant wins / losses of all methods on held-out datasets, with -1/0
encoding and ﬁxed hyperparameters, except for baseline, which is optimized on each dataset
together with the learning rate. The ﬁxed hyperparameters are shown in the table below,
and were selected with the same voting approach described in Table 9. This optimization
beneﬁts Greedy and RegCB-opt in particular.

↓ vs →
G
R
RO
C-nu
B
B-g
(cid:15)G
C-u
A

G
-
14 / 32
16 / 14
15 / 60
13 / 88
16 / 69
1 / 78
1 / 178
6 / 49

R
32 / 14
-
36 / 9
25 / 43
19 / 69
24 / 52
14 / 66
9 / 167
19 / 42

RO
14 / 16
9 / 36
-
13 / 64
10 / 90
10 / 66
3 / 84
1 / 187
5 / 61

C-nu
60 / 15
43 / 25
64 / 13
-
30 / 58
41 / 34
24 / 60
6 / 163
36 / 42

B
88 / 13
69 / 19
90 / 10
58 / 30
-
33 / 10
35 / 54
7 / 133
59 / 29

B-g
69 / 16
52 / 24
66 / 10
34 / 41
10 / 33
-
18 / 57
2 / 147
38 / 37

(cid:15)G
78 / 1
66 / 14
84 / 3
60 / 24
54 / 35
57 / 18
-
10 / 129
44 / 3

C-u
178 / 1
167 / 9
187 / 1
163 / 6
133 / 7
147 / 2
129 / 10
-
164 / 5

A
49 / 6
42 / 19
61 / 5
42 / 36
29 / 59
37 / 38
3 / 44
5 / 164
-

A Contextual Bandit Bake-off

Algorithm
G
R/RO
C-nu
C-u
B
B-g
(cid:15)G
A

Hyperparameters
-
C0 = 10−3
N = 16, ψ = 0.1, DR
N = 4, ψ = 0.1, IPS
N = 4, IWR
N = 4, IWR
(cid:15) = 0.02, IWR
(cid:15) = 0.02, C0 = 10−6, IWR

31

Bietti, Agarwal and Langford

(a) DR

(b) IWR

32

Figure 5: Improvements to (cid:15)-greedy from our active learning strategy. Encoding ﬁxed to
-1/0. The IWR implementation described in Section C.1 still manages to often outperform
(cid:15)-greedy, despite only providing an approximation to Algorithm 6.

A Contextual Bandit Bake-off

(a) all multiclass datasets

(b) n ≥ 10 000 only

(c) all multiclass datasets

(d) n ≥ 10 000 only

Figure 6: Errors of IPS counterfactual estimates for the uniform random policy using ex-
ploration logs collected by various algorithms on multiclass datasets (extended version of
Figure 3). The boxes show quartiles (with the median shown as a blue line) of the dis-
tribution of squared errors across all multiclass datasets or only those with at least 10 000
examples. The logs are obtained by running each algorithm with -1/0 encodings, ﬁxed
hyperparameters from Table 9, and the best learning rate on each dataset according to pro-
gressive validation loss. The top plots consider IPS with reward estimates (as in Figure 3),
while the bottom plots consider IPS on the loss.

33

Bietti, Agarwal and Langford

Figure 7: (top) Impact of baseline on diﬀerent algorithms with encoding ﬁxed to -1/0; for
Greedy and RegCB-opt, it can signiﬁcantly help against pessimistic initial costs in some
datasets. Hyperparameters ﬁxed as in Table 9. (bottom) Baseline improves robustness to
the range of losses. The plots consider normalized loss on held-out datasets, with red points
indicating signiﬁcant wins.

34

A Contextual Bandit Bake-off

Algorithm 6 Active (cid:15)-greedy
π1; (cid:15); C0 > 0.
explore(xt):

At = {a : loss_diff(πt, xt, a) ≤ ∆t,C0};
1{a ∈ At} + (1 − (cid:15)|At|
pt(a) = (cid:15)
K
return pt;

K ) 1{πt(xt) = a};

learn(xt, at, (cid:96)t(at), pt):

ˆ(cid:96)t = estimator(xt, at, (cid:96)t(at), pt(at));

ˆct(a) =

(cid:40)ˆ(cid:96)t(a),
1,

if pt(a) > 0
otherwise.

πt+1 = csc_oracle(πt, xt, ˆct);

Appendix C. Active (cid:15)-greedy: Practical Algorithm and Analysis

This section presents our active (cid:15)-greedy method, a variant of (cid:15)-greedy that reduces the
amount of uniform exploration using techniques from active learning. Section C.1 introduces
the practical algorithm,7 while Section C.2 provides a theoretical analysis of the method,
showing that it achieves a regret of O(T 1/3) under speciﬁc favorable settings, compared
to O(T 2/3) for vanilla (cid:15)-greedy.

C.1 Algorithm

The simplicity of the (cid:15)-greedy method described in Section 3.1 often makes it the method
of choice for practitioners. However, the uniform exploration over randomly selected actions
can be quite ineﬃcient and costly in practice. A natural consideration is to restrict this
randomization over actions which could plausibly be selected by the optimal policy π∗ =
arg minπ∈Π L(π), where L(π) = E(x,(cid:96))∼D[(cid:96)(π(x))] is the expected loss of a policy π.

To achieve this, we use techniques from disagreement-based active learning (Hanneke,
2014; Hsu, 2010). After observing a context xt, for any action a, if we can ﬁnd a policy π
that would choose this action (π(xt) = a) instead of the empirically best action πt(xt),
while achieving a small loss on past data, then there is disagreement about how good
such an action is, and we allow exploring it. Otherwise, we are conﬁdent that the best
policy would not choose this action, thus we avoid exploring it, and assign it a high cost.
The resulting method is in Algorithm 6. Like RegCB, the method requires a known loss
range [cmin, cmax], and assigns a loss cmax to such unexplored actions (we consider the
range [0, 1] in Algorithm 6 for simplicity). The disagreement test we use is based on empirical
loss diﬀerences, similar to the Oracular CAL active learning method (Hsu, 2010), denoted
loss_diff, together with a threshold:

7. Our implementation is available in the following branch of Vowpal Wabbit: https://github.com/

albietz/vowpal_wabbit/tree/bakeoff.

(cid:114)

∆t,C0 =

C0

K log t
(cid:15)t

+ C0

K log t
(cid:15)t

.

35

Bietti, Agarwal and Langford

A practical implementation of loss_diff for an online setting is given below. We analyze a
theoretical form of this algorithm in Section C.2, showing a formal version of the following
theorem:

Theorem 1 With high-probability, and under favorable conditions on disagreement and on
the problem noise, active (cid:15)-greedy achieves expected regret O(T 1/3).

Note that this data-dependent guarantee improves on worst-case guarantees achieved by
the optimal algorithms Agarwal et al. (2014); Dudik et al. (2011a). In the extreme case
where the loss of any suboptimal policy is bounded away from that of π∗, we show that
our algorithm can achieve constant regret. While active learning algorithms suggest that
data-dependent thresholds ∆t can yield better guarantees (e.g., Huang et al., 2015), this
may require more work in our setting due to open problems related to data-dependent
guarantees for contextual bandits (Agarwal et al., 2017). In a worst-case scenario, active
(cid:15)-greedy behaves similarly to (cid:15)-greedy (Langford and Zhang, 2008), achieving an O(T 2/3)
expected regret with high probability.

Practical implementation of the disagreement test. We now present a practical
way to implement the disagreement tests in the active (cid:15)-greedy method, in the context of
online cost-sensitive classiﬁcation oracles based on regression, as in Vowpal Wabbit. This
corresponds to the loss_diff method in Algorithm 6.

Let ˆLt−1(π) denote the empirical loss of policy π on the (biased) sample of cost-sensitive
examples collected up to time t − 1 (see Section C.2 for details). After observing a context
xt, we want to estimate

loss_diff(πt, xt, ¯a) ≈ ˆLt−1(πt,¯a) − ˆLt−1(πt),

for any action ¯a, where

πt = arg min

ˆLt−1(π)

π

πt,¯a = arg min

π:π(xt)=¯a

ˆLt−1(π).

In our online setup, we take πt to be the current online policy (as in Algorithm 6), and
we estimate the loss diﬀerence by looking at how many online CSC examples of the form
¯c := (1{a (cid:54)= ¯a})a=1..K are needed (or the importance weight on such an example) in order
to switch prediction from πt(xt) to ¯a. If we denote this importance weight by τ¯a, then we
can estimate ˆLt−1(πt,¯a) − ˆLt−1(πt) ≈ τ¯a/t.

Computing τ¯a for IPS/DR.
In the case of IPS/DR, we use an online CSC oracle,
which is based on K regressors f (x, a) in VW, each predicting the cost for an action a.
Let ft be the current regressors for policy πt, yt(a) := ft(xt, a), and denote by st(a) the
sensitivity of regressor ft(·, a) on example (xt, ¯c(a)). This sensitivity is essentially deﬁned
to be the derivative with respect to an importance weight w of the prediction y(cid:48)(a) obtained
from the regressor after an online update (xt, ¯c(a)) with importance weight w. A similar
quantity has been used, e.g., by Huang et al. (2015); Karampatziakis and Langford (2011);
Krishnamurthy et al. (2019). Then, the predictions on actions ¯a and a cross when the

36

A Contextual Bandit Bake-off

importance weight w satisﬁes yt(¯a) − st(¯a)w = yt(a) + st(a)w. Thus, the importance weight
required for action ¯a to be preferred (i.e., smaller predicted loss) to action a is given by:

wa

¯a =

yt(¯a) − yt(a)
st(¯a) + st(a)

.

Action ¯a will thus be preferred to all other actions when using an importance weight τ¯a =
maxa wa
¯a.

Computing τ¯a for IWR. Although Algorithm 6 and the theoretical analysis require
CSC in order to assign a loss of 1 to unexplored actions, and hence does not directly
support IWR, we can consider an approximation which leverages the beneﬁts of IWR by
performing standard IWR updates as in (cid:15)-greedy, while exploring only on actions that pass
a similar disagreement test. In this case, we estimate τ¯a as the importance weight on an
online regression example (xt, 0) for the regressor ft(·, ¯a), needed to switch prediction to
¯a. If st(¯a) is the sensitivity for such an example, we have τ¯a = (yt(¯a) − y∗
t )/st(¯a), where
y∗
t = mina yt(a).

C.2 Theoretical Analysis

This section presents a theoretical analysis of the active (cid:15)-greedy method introduced in
Section C.1. We begin by presenting the analyzed version of the algorithm together with
deﬁnitions in Section C.2.1. Section C.2.2 then studies the correctness of the method,
showing that with high probability, the actions chosen by the optimal policy are always
explored, and that policies considered by the algorithm are always as good as those obtained
under standard (cid:15)-greedy exploration. This section also introduces a Massart-type low-noise
condition similar to the one considered by Krishnamurthy et al. (2019) for cost-sensitive
classiﬁcation. Finally, Section C.2.3 provides a regret analysis of the algorithm, both in the
worst case and under disagreement conditions together with the Massart noise condition.
In particular, a formal version of Theorem 1 is given by Theorem 8, and a more extreme
but informative situation is considered in Proposition 9, where our algorithm can achieve
constant regret.

C.2.1 Algorithm and definitions

We consider a version of the active (cid:15)-greedy strategy that is more suitable for theoretical
analysis, given in Algorithm 7. This method considers exact CSC oracles, as well as a CSC
oracle with one constraint on the policy (π(xt) = a in Eq.(10)). The threshold ∆t is deﬁned
later in Section C.2.2. Computing it would require some knowledge about the size of the
policy class, which we avoid by introducing a parameter C0 in the practical variant. The
disagreement strategy is based on the Oracular CAL active learning method of Hsu (2010),
which tests for disagreement using empirical error diﬀerences, and considers biased samples
when no label is queried. Here, similar tests are used to decide which actions should be
explored, in the diﬀerent context of cost-sensitive classiﬁcation, and the unexplored actions
are assigned a loss of 1, making the empirical sample biased ( ˆZT in Algorithm 7).
Deﬁnitions. Deﬁne ZT = {(xt, (cid:96)t)}t=1..T ⊂ X × RK, ˜ZT = {(xt, ˜(cid:96)t)}t=1..T (biased sam-
ple) and ˆZT = {(xt, ˆ(cid:96)t)}t=1..T (IPS estimate of biased sample), where (cid:96)t ∈ [0, 1]K is the

37

Bietti, Agarwal and Langford

Algorithm 7 active (cid:15)-greedy: analyzed version

Input: exploration probability (cid:15).
Initialize: ˆZ0 := ∅.
for t = 1, . . . do

Observe context xt. Let

πt := arg min

L(π, ˆZt−1)

π

πt,a := arg min

L(π, ˆZt−1)

π:π(xt)=a

At := {a : L(πt,a, ˆZt−1) − L(πt, ˆZt−1) ≤ ∆t}

(10)

(11)

(8)

(9)

Let

pt(a) =






1 − (|At| − 1)(cid:15)/K,
(cid:15)/K,
0,

if a = πt(xt)
if a ∈ At \ {πt(xt)}
otherwise.

Play action at ∼ pt, observe (cid:96)t(at) and set ˆZt = ˆZt−1 ∪ {(xt, ˆ(cid:96)t)}, where ˆ(cid:96)t is deﬁned
in (9).
end for

(unobserved) loss vector at time t and

˜(cid:96)t(a) =

(cid:40)

(cid:96)t(a),
1,

if a ∈ At
o/w

ˆ(cid:96)t(a) =

(cid:40) 1{a=at}

pt(at) (cid:96)t(at),
1,

if a ∈ At
o/w.

L(π, Z) =

(cid:88)

c(π(x)).

1
|Z|

(x,c)∈Z

For any set Z ⊂ X × RK deﬁned as above, we denote, for π ∈ Π,

We then deﬁne the empirical losses LT (π) := L(π, ZT ), ˆLT (π) := L(π, ˆZT ) and ˜LT (π) :=
L(π, ˜ZT ). Let L(π) := E(x,(cid:96))∼D[(cid:96)(π(x))] be the expected loss of policy π, and π∗ :=
arg minπ∈Π L(π). We also deﬁne ρ(π, π(cid:48)) := Px(π(x) (cid:54)= π(cid:48)(x)), the expected disagreement
between policies π and π(cid:48), where Px denotes the marginal distribution of D on contexts.

C.2.2 Correctness

We begin by stating a lemma that controls deviations of empirical loss diﬀerences, which
relies on Freedman’s inequality for martingales (see, e.g., Kakade and Tewari, 2009, Lemma
3).

38

A Contextual Bandit Bake-off

Lemma 2 (Deviation bounds) With probability 1 − δ, the following event holds: for all
π ∈ Π, for all T ≥ 1,

|( ˆLT (π) − ˆLT (π∗)) − ( ˜LT (π) − ˜LT (π∗))| ≤

(cid:114)

2Kρ(π, π∗)eT
(cid:15)

+

(cid:18) K
(cid:15)

(cid:19)

+ 1

eT

|(LT (π) − LT (π∗)) − (L(π) − L(π∗))| ≤ (cid:112)ρ(π, π∗)eT + 2eT ,

(12)

(13)

where eT = log(2|Π|/δT )/T and δT = δ/(T 2 + T ). We denote this event by E in what
follows.

Proof We prove the result using Freedman’s inequality (see, e.g., Kakade and Tewari,
2009, Lemma 3), which controls deviations of a sum using the conditional variance of each
term in the sum and an almost sure bound on their magnitude, along with a union bound.

For (12), let ( ˆLT (π) − ˆLT (π∗)) − ( ˜LT (π) − ˜LT (π∗)) = 1
T

(cid:80)T

t=1 Rt, with

Rt = ˆ(cid:96)t(π(xt)) − ˆ(cid:96)t(π∗(xt)) − (˜(cid:96)t(π(xt)) − ˜(cid:96)t(π∗(xt))).

We deﬁne the σ-ﬁelds Ft := σ({xi, (cid:96)i, ai}t

i=1). Note that Rt is Ft-measurable and

E[ˆ(cid:96)t(π(xt)) − ˆ(cid:96)t(π∗(xt))|xt, (cid:96)t] = ˜(cid:96)t(π(xt)) − ˜(cid:96)t(π∗(xt)),

so that E[Rt|Ft−1] = E[E[Rt|xt, (cid:96)t]|Ft−1] = 0. Thus, (Rt)t≥1 is a martingale diﬀerence
sequence adapted to the ﬁltration (Ft)t≥1. We have

|Rt| ≤ |ˆ(cid:96)t(π(xt)) − ˆ(cid:96)t(π∗(xt))| + |˜(cid:96)t(π(xt)) − ˜(cid:96)t(π∗(xt))| ≤

+ 1.

K
(cid:15)

Note that E[ˆ(cid:96)t(π(xt)) − ˆ(cid:96)t(π∗(xt))|xt, (cid:96)t] = ˜(cid:96)t(π(xt)) − ˜(cid:96)t(π∗(xt)), so that

E[R2

t |Ft−1] = E[E[R2

t |xt, (cid:96)t, At]|Ft−1]

≤ E[E[(ˆ(cid:96)t(π(xt)) − ˆ(cid:96)t(π∗(xt)))2|xt, (cid:96)t, At]|Ft−1]

(cid:20)

(cid:20)

≤ E

E

≤ E

E

(cid:20) (1{π(xt) = at} − 1{π∗(xt) = at})2
pt(at)2

|Ft−1
(cid:20) 1{π(xt) (cid:54)= π∗(xt)}(1{π(xt) = at} + 1{π∗(xt) = at})
pt(at)2
(cid:21)

|xt, (cid:96)t, At

(cid:21)

(cid:21)

= E

(cid:20) 2K 1{π(xt) (cid:54)= π∗(xt)}
(cid:15)

|Ft−1

=

ρ(π, π∗).

2K
(cid:15)

(cid:21)

(cid:21)

|xt, (cid:96)t, At

|Ft−1

Freedman’s inequality then states that (12) holds with probability 1 − δT /2|Π|.

For (13), we consider a similar setup with

Rt = (cid:96)t(π(xt)) − (cid:96)t(π∗(xt)) − (L(π) − L(π∗)).

We have E[Rt|Ft−1] = 0, |Rt| ≤ 2 and E[R2
t |Ft−1] ≤ ρ(π, π∗), which yields that (13) holds
with probability 1 − δT /2|Π| using Freedman’s inequality. A union bound on π ∈ Π and
T ≥ 1 gives the desired result.

39

Bietti, Agarwal and Langford

Threshold. We deﬁne the threshold ∆T used in (11) in Algorithm 7 as:

∆T :=

+ 1

eT −1 +

+ 3

eT −1.

(14)

(cid:32)(cid:114)

2K
(cid:15)

(cid:33)

√

(cid:19)

(cid:18) K
(cid:15)

We also deﬁne the following more precise deviation quantity for a given policy, which follows
directly from the deviation bounds in Lemma 2

∆∗

T (π) :=

(cid:32)(cid:114)

(cid:33)

2K
(cid:15)

+ 1

(cid:112)ρ(π, π∗)eT −1 +

(cid:18) K
(cid:15)

(cid:19)

+ 3

eT −1.

(15)

Note that we have ∆∗

T (π) ≤ ∆T for any policy π.

The next lemma shows that the bias introduced in the empirical sample by assigning
a loss of 1 to unexplored actions is favorable, in the sense that it will not hurt us in
identifying π∗.

Lemma 3 (Favorable bias) Assume π∗(xt) ∈ At for all t ≤ T . We have

˜LT (π) − ˜LT (π∗) ≥ LT (π) − LT (π∗).

(16)

Proof For any t ≤ T , we have ˜(cid:96)t(a) ≥ (cid:96)t(a), so that ˜LT (π) ≥ LT (π). Separately, we
have ˜(cid:96)t(π∗(xt)) = (cid:96)t(π∗(xt)) for all t ≤ T using the deﬁnition of ˜(cid:96)t and the assumption
π∗(xt) ∈ At, hence ˜LT (π∗) ≥ LT (π∗).

We now show that with high probability, the optimal action is always explored by the

algorithm.

Lemma 4 Assume that event E holds. The actions given by the optimal policy are always
explored for all t ≥ 1, i.e., π∗(xt) ∈ At for all t ≥ 1.

Proof We show by induction on T ≥ 1 that π∗(xt) ∈ At for all t = 1, . . . , T . For the base
case, we have A1 = [K] since ˆZ0 = ∅ and hence empirical errors are always equal to 0, so
that π∗(x1) ∈ A1. Let us now assume as the inductive hypothesis that π∗(xt) ∈ At for all
t ≤ T − 1.

From deviation bounds, we have

ˆLT −1(πT ) − ˆLT −1(π∗) ≥ ˜LT −1(πT ) − ˜LT −1(π∗) −

(cid:32)(cid:114)

2Kρ(π, π∗)eT −1
(cid:15)

(cid:33)

+ (K/(cid:15) + 1)eT −1

LT −1(πT ) − LT −1(π∗) ≥ L(πT ) − L(π∗) −

(cid:16)(cid:112)ρ(π, π∗)eT −1 + 2eT −1

(cid:17)

.

Using Lemma 3 together with the inductive hypothesis, the above inequalities yield

ˆLT −1(πT ) − ˆLT −1(π∗) ≥ L(πT ) − L(π∗) − ∆∗

T (πT ).

Now consider an action a /∈ At. Using the deﬁnition (11) of At, we have

ˆLT −1(πT,a) − ˆLT −1(π∗) = ˆLT −1(πT,a) − ˆLT −1(πT ) + ˆLT −1(πT ) − ˆLT −1(π∗)

> ∆T − ∆∗

T (πT ) = 0,

40

A Contextual Bandit Bake-off

which implies π∗(xT ) (cid:54)= a, since ˆLT −1(πT,a) is the minimum of ˆLT −1 over policies satisfying
π(xT ) = a. This yields π∗(xT ) ∈ AT , which concludes the proof.

With the previous results, we can now prove that with high probability, discarding some
of the actions from the exploration process does not hurt us in identifying good policies.
In particular, πT +1 is about as good as it would have been with uniform (cid:15)-exploration all
along.

Theorem 5 Under the event E, which holds with probability 1 − δ,

L(πT +1) − L(π∗) ≤ ∆∗

T +1(πT +1).

In particular, L(πT +1) − L(π∗) ≤ ∆T +1.
Proof Assume event E holds. Using (12-13) combined with Lemma 3 (which holds by
Lemma 4), we have

L(πT +1) − L(π∗) ≤ ˆLT (πT +1) − ˆLT (π∗) + ∆∗

T +1(πT +1) ≤ ∆∗

T +1(πT +1).

Massart noise condition. We introduce a low-noise condition that will help us obtain
improved regret guarantees. Similar conditions have been frequently used in supervised
learning (Massart et al., 2006) and active learning (Hsu, 2010; Huang et al., 2015; Krish-
namurthy et al., 2019) for obtaining better data-dependent guarantees. We consider the
following Massart noise condition with parameter τ > 0:

ρ(π, π∗) ≤

(L(π) − L(π∗)).

1
τ

(M)

This condition holds when E[mina(cid:54)=π∗(x) (cid:96)(a) − (cid:96)(π∗(x))|x] ≥ τ , Px-almost surely, which is
similar to the Massart condition considered in Krishnamurthy et al. (2019) in the context
of active learning for cost-sensitive classiﬁcation. Indeed, we have

L(π) − L(π∗) = E[1{π(x) (cid:54)= π∗(x)}((cid:96)(π(x)) − (cid:96)(π∗(x))]

+ E[1{π(x) = π∗(x)}((cid:96)(π∗(x)) − (cid:96)(π∗(x)))]
(cid:20)
1{π(x) (cid:54)= π∗(x)}

(cid:96)(a) − (cid:96)(π∗(x))

(cid:18)

(cid:19)(cid:21)

≥ E

= E[1{π(x) (cid:54)= π∗(x)} E[ min

(cid:96)(a) − (cid:96)(π∗(x))|x]]

min
a(cid:54)=π∗(x)

a(cid:54)=π∗(x)

≥ E[1{π(x) (cid:54)= π∗(x)}τ ] = τ ρ(π, π∗),

which is precisely (M). The condition allows us to obtain a fast rate for the policies con-
sidered by our algorithm, as we now show.

Theorem 6 Assume the Massart condition (M) holds with parameter τ . Under the event
E, which holds w.p. 1 − δ,

for some numeric constant C.

L(πT +1) − L(π∗) ≤ C

eT ,

K
τ (cid:15)

41

Bietti, Agarwal and Langford

Proof Using Theorem 5 and the Massart condition, we have

L(πT +1) − L(π∗) ≤ ∆∗

T +1(πT +1) =

(cid:32)(cid:114)

(cid:33)

2K
(cid:15)

+ 1

(cid:112)ρ(πT +1, π∗)eT +

(cid:18) K
(cid:15)

(cid:19)

+ 3

eT

+ 1

(cid:112)(L(πT +1) − L(π∗))eT /τ +

(cid:18) K
(cid:15)

(cid:19)

+ 3

eT

(cid:32)(cid:114)

(cid:33)

≤

≤

2K
(cid:15)

(cid:114)

8KeT
τ (cid:15)

(L(πT +1) − L(π∗)) +

4KeT
(cid:15)

.

Solving the quadratic inequality in L(πT +1) − L(π∗) yields the result.

C.2.3 Regret Analysis

In a worst-case scenario, the following result shows that Algorithm 7 enjoys a similar O(T 2/3)
regret guarantee to the vanilla (cid:15)-greedy approach (Langford and Zhang, 2008).

Theorem 7 Conditioned on the event E, which holds with probability 1 − δ, the expected
regret of the algorithm is

E[RT |E] ≤ O

(cid:32)(cid:114)

KT log(T |Π|/δ)
(cid:15)

(cid:33)

+ T (cid:15)

.

Optimizing over the choice of (cid:15) yields a regret O(T 2/3(K log(T |Π|/δ))1/3).

Proof We condition on the 1−δ probability event E that the deviation bounds of Lemma 2
hold. We have

E[(cid:96)t(at) − (cid:96)t(π∗(xt))|Ft−1] = E[1{at = πt(xt)}((cid:96)t(πt(xt)) − (cid:96)t(π∗(xt)))|Ft−1]
+ E[1{at (cid:54)= πt(xt)}((cid:96)t(at) − (cid:96)t(π∗(xt)))|Ft−1]

≤ E[(cid:96)t(πt(xt)) − (cid:96)t(π∗(xt))|Ft−1] + E[E[1 − pt(πt(xt))|xt]|Ft−1]
≤ L(πt) − L(π∗) + (cid:15).

Summing over t and applying Theorem 5 together with ∆∗

t (π) ≤ ∆t, we obtain

E[RT |E] = E

(cid:96)t(at) − (cid:96)t(π∗(xt))|E

(cid:35)

(cid:34) T

(cid:88)

t=1
T
(cid:88)

t=2

≤ 1 + T (cid:15) +

∆t.

T
(cid:88)

t=2

≤ 1 +

E[L(πt) − L(π∗) + (cid:15)|Ft−1, E]

Using (cid:80)T

t=2

√

et ≤ O((cid:112)T log(8T 2|Π|/δ)) and (cid:80)T
(cid:114)

(cid:32)

E[RT |E] ≤ O

1 +

KT log(T |Π|/δ)
(cid:15)

+

K log(T |Π|/δ)
(cid:15)

(cid:33)

log T + T (cid:15)

,

t=2 et ≤ O(log(8T 2|Π|/δ) log T ), we obtain

42

A Contextual Bandit Bake-off

which yields the result.

Disagreement deﬁnitions.
In order to obtain improvements in regret guarantees over
the worst case, we consider notions of disagreement that extend standard deﬁnitions from
the active learning literature (e.g., Hanneke, 2014; Hsu, 2010; Huang et al., 2015) to the
multiclass case. Let B(π∗, r) := {π ∈ Π : ρ(π, π∗) ≤ r} be the ball centered at π∗ under
the (pseudo)-metric ρ(·, ·). We deﬁne the disagreement region DIS(r) and disagreement
coeﬃcient θ as follows:

DIS(r) := {x : ∃π ∈ B(π∗, r) π(x) (cid:54)= π∗(x)}
P (x ∈ DIS(r))
r

θ := sup
r>0

.

The next result shows that under the Massart condition and with a ﬁnite disagreement
coeﬃcient θ, our algorithm achieves a regret that scales as O(T 1/3) (up to logarithmic
factors), thus improving on worst-case guarantees obtained by optimal algorithms such
as Agarwal et al. (2012, 2014); Dudik et al. (2011a).

Theorem 8 Assume the Massart condition (M) holds with parameter τ . Conditioning on
the event E which holds w.p. 1 − δ, the algorithm has expected regret

E[RT |E] ≤ O

(cid:18) K log(T |Π|/δ)
τ (cid:15)

log T +

(cid:112)(cid:15)KT log(T |Π|/δ)

(cid:19)

.

θ
τ

Optimizing over the choice of (cid:15) yields a regret

E[RT |E] ≤ O

(θK log(T |Π|/δ))2/3(T log T )1/3

.

(cid:19)

(cid:18) 1
τ

Proof Assume E holds. Let t ≥ 2, and assume a ∈ At \ {π∗(xt)}. Deﬁne
(cid:40)

πa =

πt,
πt,a,

if πt(xt) = a
if πt(xt) (cid:54)= a,

so that we have πa(xt) = a (cid:54)= π∗(xt).

• If πa = πt, then L(πa) − L(π∗) ≤ ∆∗

t (πa) ≤ ∆t by Theorem 5

• If πa = πt,a, using deviation bounds, Lemma 4 and 3, we have

L(πa) − L(π∗) = L(πt,a) − L(π∗)

≤ ˆLt−1(πt,a) − ˆLt−1(π∗) + ∆∗
= ˆLt−1(πt,a) − ˆLt−1(πt)
(cid:123)(cid:122)
(cid:125)
≤∆t

(cid:124)

+ ˆLt−1(πt) − ˆLt−1(π∗)
(cid:123)(cid:122)
(cid:125)
≤0

(cid:124)

t (πt,a)

+∆∗

t (πt,a)

≤ 2∆t,

where the last inequality uses a ∈ At.

43

Bietti, Agarwal and Langford

By the Massart assumption, we then have ρ(πa, π∗) ≤ 2∆t/τ . Hence, we have xt ∈
DIS(2∆t/τ ). We have thus shown

E[E[1{a ∈ At \ {π∗(xt)}}|xt]|Ft−1] ≤ E[P (xt ∈ DIS(2∆t/τ ))|Ft−1] ≤ 2θ∆t/τ.

We then have

E[(cid:96)t(at) − (cid:96)t(π∗(xt))|Ft−1] = E[1{at = πt(xt)}((cid:96)t(πt(xt)) − (cid:96)t(π∗(xt)))

+ 1{at = π∗(xt) ∧ at (cid:54)= πt(xt)}((cid:96)t(π∗(xt)) − (cid:96)t(π∗(xt)))

1{at = a ∧ a /∈ {πt(xt), π∗(xt)}}((cid:96)t(a) − (cid:96)t(π∗(xt)))|Ft−1]

≤ E[(cid:96)t(πt(xt)) − (cid:96)t(π∗(xt))|Ft−1]

+ E

E[1{at = a} 1{a /∈ {πt(xt), π∗(xt)}}|xt]|Ft−1

(cid:35)

+

K
(cid:88)

a=1

(cid:34) K
(cid:88)

a=1

= L(πt) − L(π∗) +

E[E[pt(a) 1{a /∈ {πt(xt), π∗(xt)}}|xt]|Ft−1]

≤ L(πt) − L(π∗) +

E[E[1{a ∈ At \ {π∗(xt)}}|xt]|Ft−1]

≤ C

et−1 + 2(cid:15)θ∆t/τ,

K
τ (cid:15)

where we used

pt(a) 1{a /∈ {πt(xt), π∗(xt)}} =

1{a ∈ At \ {πt(xt), π∗(xt)}}

≤

1{a ∈ At \ {π∗(xt)}}.

Summing over t and taking total expectations (conditioned on E) yields

E[RT |E] ≤ O

(cid:32)

K log(T |Π|/δ)
τ (cid:15)

log T +

(cid:32)(cid:114)

(cid:15)θ
τ

KT log(T |Π|/δ)
(cid:15)

+

K log(T |Π|/δ)
(cid:15)

(cid:33)(cid:33)

log(T )

,

and the result follows.

Finally, we look at a simpler instructive example, which considers an extreme situation
where the expected loss of any suboptimal policy is bounded away from that of the opti-
mal policy. In this case, Algorithm 7 can achieve constant regret when the disagreement
coeﬃcient is bounded, as shown by the following result.

Proposition 9 Assume that L(π) − L(π∗) ≥ τ > 0 for all π (cid:54)= π∗, and that θ < ∞. Under
the event E, the algorithm achieves constant expected regret. In particular, the algorithm
stops incurring regret for T > T0 := max{t : 2∆t > τ }.

K
(cid:88)

a=1

(cid:15)
K

K
(cid:88)

a=1

(cid:15)
K
(cid:15)
K

44

A Contextual Bandit Bake-off

Proof By Theorem 5 and our assumption, we have L(πt) − L(π∗) ≤ 1{∆t ≥ τ }∆t.
Similarly, the assumption implies that ρ(π, π∗) ≤ 1{L(π) − L(π∗) ≥ τ }, so that using
similar arguments to the proof of Theorem 8, we have

E[E[1{a ∈ At \ {π∗(xt)}}|xt]|Ft−1] ≤ θ 1{2∆t ≥ τ }.

Following the proof of Theorem 8, this implies that when t is such that 2∆t < τ , then we
have

Let T0 := max{t : 2∆t ≥ τ }. We thus have

E[(cid:96)t(at) − (cid:96)t(π∗(xt))|Ft−1] = 0.

E[RT |E] ≤ 1 +

(∆t + (cid:15)).

T0(cid:88)

t=2

45

