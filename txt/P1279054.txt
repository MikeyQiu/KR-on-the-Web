8
1
0
2
 
v
o
N
 
3
2
 
 
]

G
L
.
s
c
[
 
 
2
v
8
2
3
1
1
.
5
0
8
1
:
v
i
X
r
a

Hamiltonian Variational Auto-Encoder

Anthony L. Caterini1, Arnaud Doucet1,2, Dino Sejdinovic1,2
1Department of Statistics, University of Oxford
2Alan Turing Institute for Data Science
{anthony.caterini, doucet, dino.sejdinovic}@stats.ox.ac.uk

Abstract

Variational Auto-Encoders (VAEs) have become very popular techniques to per-
form inference and learning in latent variable models: they allow us to leverage the
rich representational power of neural networks to obtain ﬂexible approximations of
the posterior of latent variables as well as tight evidence lower bounds (ELBOs).
Combined with stochastic variational inference, this provides a methodology scal-
ing to large datasets. However, for this methodology to be practically efﬁcient,
it is necessary to obtain low-variance unbiased estimators of the ELBO and its
gradients with respect to the parameters of interest. While the use of Markov chain
Monte Carlo (MCMC) techniques such as Hamiltonian Monte Carlo (HMC) has
been previously suggested to achieve this [25, 28], the proposed methods require
specifying reverse kernels which have a large impact on performance. Additionally,
the resulting unbiased estimator of the ELBO for most MCMC kernels is typically
not amenable to the reparameterization trick. We show here how to optimally
select reverse kernels in this setting and, by building upon Hamiltonian Importance
Sampling (HIS) [19], we obtain a scheme that provides low-variance unbiased
estimators of the ELBO and its gradients using the reparameterization trick. This al-
lows us to develop a Hamiltonian Variational Auto-Encoder (HVAE). This method
can be re-interpreted as a target-informed normalizing ﬂow [22] which, within our
context, only requires a few evaluations of the gradient of the sampled likelihood
and trivial Jacobian calculations at each iteration.

1

Introduction

Variational Auto-Encoders (VAEs), introduced by Kingma and Welling [15] and Rezende et al.
[23], are popular techniques to carry out inference and learning in complex latent variable models.
However, the standard mean-ﬁeld parametrization of the approximate posterior distribution can limit
its ﬂexibility. Recent work has sought to augment the VAE approach by sampling from the VAE
posterior approximation and transforming these samples through mappings with additional trainable
parameters to achieve richer posterior approximations. The most popular application of this idea
is the Normalizing Flows (NFs) approach [22] in which the samples are deterministically evolved
through a series of parameterized invertible transformations called a ﬂow. NFs have demonstrated
success in various domains [2, 16], but the ﬂows do not explicitly use information about the target
posterior. Therefore, it is unclear whether the improved performance is caused by an accurate posterior
approximation or simply a result of overparametrization. The related Hamiltonian Variational
Inference (HVI) [25] instead stochastically evolves the base samples according to Hamiltonian Monte
Carlo (HMC) [20] and thus uses target information, but relies on deﬁning reverse dynamics in the
ﬂow, which, as we will see, turns out to be unnecessary and suboptimal.

One of the key components in the formulation of VAEs is the maximization of the evidence lower
bound (ELBO). The main idea put forward in Salimans et al. [25] is that it is possible to use K
MCMC iterations to obtain an unbiased estimator of the ELBO and its gradients. This estimator

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

is obtained using an importance sampling argument on an augmented space, with the importance
distribution being the joint distribution of the K + 1 states of the ‘forward’ Markov chain, while
the augmented target distribution is constructed using a sequence of ‘reverse’ Markov kernels such
that it admits the original posterior distribution as a marginal. The performance of this estimator is
strongly dependent on the selection of these forward and reverse kernels, but no clear guideline for
selection has been provided therein. By linking this approach to earlier work by Del Moral et al. [6],
we show how to select these components. We focus, in particular, on the use of time-inhomogeneous
Hamiltonian dynamics, proposed originally in Neal [19]. This method uses reverse kernels which are
optimal for reducing variance of the likelihood estimators and allows for simple calculation of the
approximate posteriors of the latent variables. Additionally, we can easily use the reparameterization
trick to calculate unbiased gradients of the ELBO with respect to the parameters of interest. The
resulting method, which we refer to as the Hamiltonian Variational Auto-Encoder (HVAE), can
be thought of as a Normalizing Flow scheme in which the ﬂow depends explicitly on the target
distribution. This combines the best properties of HVI and NFs, resulting in target-informed and
inhomogeneous deterministic Hamiltonian dynamics, while being scalable to large datasets and high
dimensions.

2 Evidence Lower Bounds, MCMC and Hamiltonian Importance Sampling

2.1 Unbiased likelihood and evidence lower bound estimators

For data x ∈ X ⊆ Rd and parameter θ ∈ Θ, consider the likelihood function

(cid:90)

(cid:90)

pθ(x) =

pθ(x, z)dz =

pθ(x|z)pθ(z)dz,

(cid:90)

(cid:90)

where z ∈ Z are some latent variables. If we assume we have access to a strictly positive unbiased
estimate of pθ(x), denoted ˆpθ(x), then

ˆpθ(x)qθ,φ(u|x)du = pθ(x),

(1)

with u ∼ qθ,φ(·), u ∈ U denoting all the random variables used to compute ˆpθ(x). Here, φ denotes
additional parameters of the sampling distribution. We emphasize that ˆpθ(x) depends on both u and
potentially φ as this is not done notationally. By applying Jensen’s inequality to (1), we thus obtain,
for all θ and φ,

LELBO(θ, φ; x) :=

log ˆpθ(x) qθ,φ(u|x)du ≤ log pθ(x) =: L(θ; x).

(2)

i=1 qθ,φ(zi|x) and ˆpθ(x) = 1
L

It can be shown that |LELBO(θ, φ; x)−L(θ; x)| decreases as the variance of ˆpθ(x) decreases; see, e.g.,
[3, 17]. The standard variational framework corresponds to U = Z and ˆpθ(x) = pθ(x, z)/qθ,φ(z|x),
while the Importance Weighted Auto-Encoder (IWAE) [3] with L importance samples corresponds to
U = Z L, qθ,φ(u|x) = (cid:81)L
In the general case, we do not have an analytical expression for LELBO(θ, φ; x). When performing
stochastic gradient ascent for variational inference, however, we only require an unbiased estimator of
∇θLELBO(θ, φ; x). This is given by ∇θ log ˆpθ(x) if the reparameterization trick [8, 15] is used, i.e.
qθ,φ(u|x) = q(u), and ˆpθ(x) is a ‘smooth enough’ function of u. As a guiding principle, one should
attempt to obtain a low-variance estimator of pθ(x), which typically translates into a low-variance
estimator of ∇θLELBO(θ, φ; x). We can analogously optimize LELBO(θ, φ; x) with respect to φ
through stochastic gradient ascent to obtain tighter bounds.

i=1 pθ(x, zi)/qθ,φ(zi|x).

(cid:80)L

2.2 Unbiased likelihood estimator using time-inhomogeneous MCMC

Salimans et al. [25] propose to build an unbiased estimator of pθ(x) by sampling a (potentially
time-inhomogeneous) ‘forward’ Markov chain of length K + 1 using z0 ∼ q0
θ,φ(·) and zk ∼
qk
θ,φ(·|zk−1) for k = 1, ..., K. Using artiﬁcial ‘reverse’ Markov transition kernels rk
θ,φ(zk|zk+1) for
k = 0, ..., K − 1, it follows easily from an importance sampling argument that

ˆpθ(x) =

pθ(x, zK) (cid:81)K−1
k=0 rk
θ,φ(z0) (cid:81)K
k=1 qk
q0

θ,φ(zk|zk+1)
θ,φ(zk|zk−1)

(3)

2

is an unbiased estimator of pθ(x) as long as the ratio in (3) is well-deﬁned. In the framework
of the previous section, we have U = Z K+1 and qθ,φ(u|x) is given by the denominator of (3).
Although we did not use measure-theoretic notation, the kernels qk
θ,φ are typically MCMC kernels
which do not admit a density with respect to the Lebesgue measure (e.g. the Metropolis–Hastings
kernel). This makes it difﬁcult to deﬁne reverse kernels for which (3) is well-deﬁned as evidenced
in Salimans et al. [25, Section 4] or Wolf et al. [28]. The estimator (3) was originally introduced
in Del Moral et al. [6] where generic recommendations are provided for this estimator to admit a
low relative variance: select qk
θ,φ as MCMC kernels which are invariant, or approximately invariant
[pθ(x, z)]βk is a sequence
θ,φ(z) to pθ(z|x) smoothly using β0 = 0 < β1 < · · · < βK−1 <
θ,φ}k,
θ,φ (zk|zk+1) =
θ,φ (zk) denotes the marginal density of zk under the

as in [9], with respect to pk
of artiﬁcial densities bridging q0
βK = 1. It is also established in Del Moral et al. [6] that, given any sequence of kernels {qk
the sequence of reverse kernels minimizing the variance of ˆpθ(x) is given by rk,opt
θ,φ(zk)qk+1
qk
forward dynamics, yielding

θ,φ (zk+1), where qk

θ,φ (zk+1|zk)/qk+1

θ (x, zk), where pk

θ,φ(z|x) ∝

q0
θ,φ(z)

(cid:105)1−βk

(cid:104)

ˆpθ(x) =

pθ(x, zK)
qK
θ,φ(zK)

.

(4)

θ,φ

θ,φ

θ,φ should be approximating rk,opt

For stochastic forward transitions, it is typically not possible to compute rk,opt
and the corresponding
estimator (4) as the marginal densities qk
θ,φ(zk) do not admit closed-form expressions. However
this suggests that rk
and various schemes are presented in [6].
As noticed by Del Moral et al. [6] and Salimans et al. [25], Annealed Importance Sampling (AIS)
[18] – also known as the Jarzynski-Crooks identity ([4, 12]) in physics – is a special case of (3)
using, for qk
θ,φ, a pk
θ (z|x)-invariant MCMC kernel and the reversal of this kernel as the reverse
transition kernel rk−1
1. This choice of reverse kernels is suboptimal but leads to a simple expression
θ,φ
for estimator (3). AIS provides state-of-the-art estimators of the marginal likelihood and has been
widely used in machine learning. Unfortunately, it typically cannot be used in conjunction with the
reparameterization trick. Indeed, although it is very often possible to reparameterize the forward
simulation of (z1, ..., zT ) in terms of the deterministic transformation of some random variables
u ∼ q independent of θ and φ, this mapping is not continuous because the MCMC kernels it uses
typically include singular components. In this context, although (1) holds, ∇θ log ˆpθ(x) is not an
unbiased estimator of ∇θLELBO(θ, φ; x); see, e.g., Glasserman [8] for a careful discussion of these
issues.

2.3 Using Hamiltonian dynamics

Given the empirical success of Hamiltonian Monte Carlo (HMC) [11, 20], various contributions have
proposed to develop algorithms exploiting Hamiltonian dynamics to obtain unbiased estimates of
the ELBO and its gradients when Z = R(cid:96). This was proposed in Salimans et al. [25]. However, the
algorithm suggested therein relies on a time-homogeneous leapfrog where momentum resampling is
performed at each step and no Metropolis correction is used. It also relies on learned reverse kernels.
To address the limitations of Salimans et al. [25], Wolf et al. [28] have proposed to include some
Metropolis acceptance steps, but they still limit themselves to homogeneous dynamics and their
estimator is not amenable to the reparameterization trick. Finally, in Hoffman [10], an alternative
approach is used where the gradient of the true likelihood, ∇θL(θ; x), is directly approximated by
using Fisher’s identity and HMC to obtain approximate samples from pθ(z|x). However, the MCMC
bias can be very signiﬁcant when one has multimodal latent posteriors and is strongly dependent on
both the initial distribution and θ.

Here, we follow an alternative approach where we use Hamiltonian dynamics that are time-
inhomogeneous as in [6] and [18], and use optimal reverse Markov kernels to compute ˆpθ(x).
This estimator can be used in conjunction with the reparameterization trick to obtain an unbiased
estimator of ∇LELBO(θ, φ; x). This method is based on the Hamiltonian Importance Sampling (HIS)
scheme proposed in Neal [19]; one can also ﬁnd several instances of related ideas in physics [13, 26].

1The reversal of a µ-invariant kernel K(z(cid:48)|z) is given by Krev(z(cid:48)|z) = µ(z(cid:48))K(z|z(cid:48))

. If K is µ-reversible

µ(z)

then Krev = K.

3

We work in an extended space (z, ρ) ∈ U := R(cid:96) × R(cid:96), introducing momentum variables ρ to pair
with the position variables z, with new target ¯pθ(x, z, ρ) := pθ(x, z)N (ρ|0, I(cid:96)). Essentially, the idea
is to sample using deterministic transitions qk
θ,φ(zk−1,ρk−1)(zk, ρk)
θ,φ(·, ·) and
θ,φ)k≥1, deﬁne diffeomorphisms corresponding to a time-discretized and inhomogeneous Hamilto-

so that (zK, ρK) = Hθ,φ(z0, ρ0) :=
(Φk
nian dynamics. In this case, it is easy to show that

θ,φ((zk, ρk)|(zk−1, ρk−1)) = δΦk

(z0, ρ0), where (z0, ρ0) ∼ q0

θ,φ ◦ · · · ◦ Φ1

ΦK

θ,φ

(cid:16)

(cid:17)

θ,φ(zK, ρK) = q0
qK

θ,φ(z0, ρ0)

(cid:12)
(cid:12)det ∇Φk

θ,φ(zk, ρk)(cid:12)
(cid:12)

−1

and

ˆpθ(x) =

¯pθ(x, zK, ρK)
qK
θ,φ(zK, ρK)

.

(5)

K
(cid:89)

k=1

It can also be shown that this is nothing but a special case of (3) (on the extended position-momentum
space) using the optimal reverse kernels2 rk,opt
θ,φ . This setup is similar to the one of Normalizing
Flows [22], except here we use a ﬂow informed by the target distribution. Salimans et al. [25] is in
fact mentioned in Rezende and Mohamed [22], but the ﬂow therein is homogeneous and yields a
high-variance estimator of the normalizing constants even if rk,opt
is used, as demonstrated in our
simulations in section 4.

θ

Under these dynamics, the estimator ˆpθ(x) deﬁned in (5) can be rewritten as

ˆpθ(x) =

¯pθ (x, Hθ,φ (z0, ρ0))
q0
θ,φ (z0, ρ0)

K
(cid:89)

k=1

(cid:12)
(cid:12)det ∇Φk

θ,φ(zk, ρk)(cid:12)
(cid:12) .

(6)

Hence, if we can simulate (z0, ρ0) ∼ q0
smooth mapping, then we can use the reparameterization trick since Φk

θ,φ(·, ·) using (z0, ρ0) = Ψθ,φ(u), where u ∼ q and Ψθ,φ is a
θ,φ are also smooth mappings.

In our case, the deterministic transformation Φk
θ,φ has two components: a leapfrog step, which
discretizes the Hamiltonian dynamics, and a tempering step, which adds inhomogeneity to the
dynamics and allows us to explore isolated modes of the target [19]. To describe the leapfrog step,
we ﬁrst deﬁne the potential energy of the system as Uθ(z|x) ≡ − log pθ(x, z) for a single datapoint
x ∈ X . Leapfrog then takes the system from (z, ρ) into (z(cid:48), ρ(cid:48)) via the following transformations:

ε
2

(cid:101)ρ = ρ −
z(cid:48) = z + ε (cid:12) (cid:101)ρ,
ρ(cid:48) = (cid:101)ρ −

ε
2

(cid:12) ∇Uθ(z|x),

(cid:12) ∇Uθ(z(cid:48)|x),

(7)

(8)

(9)

where ε ∈ (R+)(cid:96) are the individual leapfrog step sizes per dimension, (cid:12) denotes elementwise
multiplication, and the gradient of Uθ(z|x) is taken with respect to z. The composition of equations
(7) - (9) has unit Jacobian since each equation describes a shear transformation. For the tempering
portion, we multiply the momentum output of each leapfrog step by αk ∈ (0, 1) for k ∈ [K] where
[K] ≡ {1, . . . , K}. We consider two methods for setting the values αk. First, ﬁxed tempering
involves allowing an inverse temperature β0 ∈ (0, 1) to vary, and then setting αk = (cid:112)βk−1/βk,
where each βk is a deterministic function of β0 and 0 < β0 < β1 < . . . < βK = 1. In the second
method, known as free tempering, we allow each of the αk values to be learned, and then set the initial
inverse temperature to β0 = (cid:81)K
k. For both methods, the tempering operation has Jacobian α(cid:96)
k.
We obtain Φk
θ,φ by composing the leapfrog integrator with the cooling operation, which implies that
the Jacobian is given by | det ∇Φk

k = (βk−1/βk)(cid:96)/2, which in turns implies

θ,φ(zk, ρk)| = α(cid:96)

k=1 α2

K
(cid:89)

k=1

| det ∇Φk

θ,φ(zk, ρk)| =

(cid:19)(cid:96)/2

K
(cid:89)

k=1

(cid:18) βk−1
βk

= β(cid:96)/2
0

.

The only remaining component to specify is the initial distribution. We will set q0
q0
θ,φ(z0) · N (ρ0|0, β−1

θ,φ(z0, ρ0) =
θ,φ(z0) will be referred to as the variational prior over the latent

0 I(cid:96)), where q0

2Since this is a deterministic ﬂow, the density can be evaluated directly. However, direct evaluation corre-

sponds to optimal reverse kernels in the deterministic case.

4

variables and N (ρ0|0, β−1
0 I(cid:96)) is the canonical momentum distribution at inverse temperature β0.
The full procedure to generate an unbiased estimate of the ELBO from (2) on the extended space
U for a single point x ∈ X and ﬁxed tempering is given in Algorithm 1. The set of variational
parameters to optimize contains the ﬂow parameters β0 and ε, along with additional parameters
of the variational prior.3 We can see from (6) that we will obtain unbiased gradients with respect
to θ and φ from our estimate of the ELBO if we write (z0, ρ0) = (cid:0)z0, γ0/
θ,φ(·)
and γ0 ∼ N (·|0, I(cid:96)) ≡ N(cid:96)(·), provided we are not also optimizing with respect to parameters of
the variational prior. We will require additional reparameterization when we elect to optimize with
respect to the parameters of the variational prior, but this is generally quite easy to implement on a
problem-speciﬁc basis and is well-known in the literature; see, e.g. [15, 22, 23] and section 4.

(cid:1), for z0 ∼ q0

β0

√

Algorithm 1 Hamiltonian ELBO, Fixed Tempering

Require: pθ(x, ·) is the unnormalized posterior for x ∈ X and θ ∈ Θ
Require: q0

θ,φ(·) is the variational prior on R(cid:96)

function HIS(x, θ, K, β0, ε)

θ,φ(·), γ0 ∼ N(cid:96)(·)

Sample z0 ∼ q0
ρ0 ← γ0/
for k ← 1 to K do

β0

√

(cid:101)ρ ← ρ − ε/2 (cid:12) ∇Uθ(zk−1|x)
zk ← zk−1 + ε (cid:12) (cid:101)ρ
ρ(cid:48) ← (cid:101)ρ − ε/2 (cid:12) ∇Uθ(zk|x)
√
1 − 1√
β0

βk ←

(cid:16)(cid:16)

(cid:17)

ρk ← (cid:112)βk−1/βk · ρ(cid:48)
¯p ← pθ(x, zK)N (ρK|0, I(cid:96))
¯q ← q0
ˆLH
return ˆLH

θ,φ(z0)N (ρ0|0, β−1
0
ELBO(θ, φ; x) ← log ¯p − log ¯q
ELBO(θ, φ; x)

0 I(cid:96))β−(cid:96)/2

(cid:46) ρ0 ∼ N (·|0, β−1

0 I(cid:96))
(cid:46) Run K steps of alternating leapfrog and tempering
(cid:46) Start of leapfrog; Equation (7)
(cid:46) Equation (8)
(cid:46) Equation (9)

(cid:46) Quadratic tempering scheme

· k2/K 2 + 1√
β0

(cid:17)−1

(cid:46) Equation (5), left side
(cid:46) Take the log of equation (5), right side
(cid:46) Can take unbiased gradients of this estimate wrt θ, φ

3 Stochastic Variational Inference

We will now describe how to use Algorithm 1 within a stochastic variational inference procedure,
moving to the setting where we have a dataset D = {x1, . . . , xN } and xi ∈ X for all i ∈ [N ]. In this
case, we are interested in ﬁnding

θ∗ ∈ argmax

Ex∼νD(·)[L(θ; x)],

θ∈Θ

(10)

(cid:80)N

where νD(·) ≡ 1
i=1 δxi(·) is the empirical measure of the data. We must resort to variational
N
methods since L(θ; x) cannot generally be calculated exactly and instead maximize the surrogate
ELBO objective function

LELBO(θ, φ) ≡ Ex∼νD(·) [LELBO(θ, φ; x)]
(11)
for LELBO(θ, φ; x) deﬁned as in (2). We can now turn to stochastic gradient ascent (or a variant
thereof) to jointly maximize (11) with respect to θ and φ by approximating the expectation over νD(·)
using minibatches of observed data.

For our speciﬁc problem, we can reduce the variance of the ELBO calculation by analytically
evaluating some terms in the expectation (i.e. Rao-Blackwellization) as follows:
(cid:34)

(cid:33)(cid:35)

(cid:32)

LH

ELBO(θ, φ; x) = E(z0,ρ0)∼q0

θ,φ(·,·)

log

= Ez0∼q0

θ,φ(·),γ0∼N(cid:96)(·)

KρK − log q0
ρT

(cid:21)
θ,φ(z0)

+

(cid:96)
2

,

(12)

0

¯pθ(x, zK, ρK)β(cid:96)/2
q0
θ,φ(z0, ρ0)
1
2

(cid:20)
log pθ(x, zK) −

3We avoid reference to a mass matrix M throughout this formulation because we can capture the same effect

by optimizing individual leapfrog step sizes per dimension as pointed out in [20, Section 4.2].

5

(cid:1) under reparameterization. We can now consider the
where we write (zK, ρK) = Hθ,φ
output of Algorithm 1 as taking a sample from the inner expectation for a given sample x from the
outer expectation. Algorithm 2 provides a full procedure to stochastically optimize (12). In practice,
we take the gradients of (12) using automatic differentation packages. This is achieved by using
TensorFlow [1] in our implementation.

(cid:0)z0, γ0/

β0

√

Algorithm 2 Hamiltonian Variational Auto-Encoder

Require: pθ(x, ·) is the unnormalized posterior for x ∈ X and θ ∈ Θ

function HVAE(D, K, nB)

Initialize θ, φ
while θ, φ not converged do

Sample {x1, . . . , xnB } ∼ νD(·) independently
ˆLH
ELBO(θ, φ) ← 0
for i ← 1 to nB do

ELBO(θ, φ) ← HIS(xi, θ, K, β0, ε) + ˆLH
ˆLH
ELBO(θ, φ)/nB

ˆLH
ELBO(θ, φ) ← ˆLH
(cid:46)
θ ← UPDATETHETA(∇θ ˆLH
φ ← UPDATEPHI(∇φ ˆLH

ELBO(θ, φ), θ)

ELBO(θ, φ), φ)

ELBO(θ, φ)

(cid:46) nB is minibatch size

(cid:46) Stochastic optimization loop

(cid:46) Average ELBO estimators over mini-batch

Optimize the ELBO using gradient-based techniques such as RMSProp, ADAM, etc.

return θ, φ

4 Experiments

4.1 Gaussian Model

In this section, we discuss the experiments used to validate our method. We ﬁrst test HVAE on an
example with a tractable full log likelihood (where no neural networks are needed), and then perform
larger-scale tests on the MNIST dataset. Code is available online.4 All models were trained using
TensorFlow [1].

The generative model that we will consider ﬁrst is a Gaussian likelihood with an offset and a Gaussian
prior on the mean, given by

z ∼ N (0, I(cid:96)),
xi|z ∼ N (z + ∆, Σ)

independently,

i ∈ [N ]

where Σ is constrained to be diagonal. We will again write D ≡ {x1, . . . , xN } to denote an observed
dataset under this model, where each xi ∈ X ⊆ Rd. In this example, we have (cid:96) = d. The goal of the
d) and ∆ ∈ Rd.
problem is to learn the model parameters θ ≡ {Σ, ∆}, where Σ = diag(σ2
Here, we have only one latent variable generating the entire set of data. Thus, our variational lower
bound is now given by

1, . . . , σ2

LELBO(θ, φ; D) := Ez∼qθ,φ(·|D) [log pθ(D, z) − log qθ,φ(z|D)] ≤ log pθ(D),

for the variational posteroir approximation qθ,φ(·|D). We note that this is not exactly the same as
the auto-encoder setting, in which an individual latent variable is associated with each observation,
however it provides a tractable framework to analyze effectiveness of various variational inference
methods. We also note that we can calculate the log-likelihood log pθ(D) exactly in this case, but we
use variational methods for the sake of comparison.

From the model, we see that the logarithm of the unnormalized target is given by

log pθ(D, z) =

log N (xi|z + ∆, Σ) + log N (z|0, Id).

4https://github.com/anthonycaterini/hvae-nips

N
(cid:88)

i=1

6

(a) Comparison across all methods

(b) HVAE with and without tempering

Figure 1: Averages of
where ˆθ is the estimated maximizer of the ELBO for each method and θ is the true parameter.

for several variational methods and choices of dimensionality d,

(cid:13)
(cid:13)
2
(cid:13)θ − ˆθ
(cid:13)
(cid:13)
(cid:13)
2

For this example, we will use a HVAE with variational prior equal to the true prior, i.e. q0 = N (0, I(cid:96)),
and ﬁxed tempering. The potential, given by Uθ(z|D) = log pθ(D, z), has gradient

∇Uθ(z|D) = z + N Σ−1(z + ∆ − x).

The set of variational parameters here is φ ≡ {ε, β0}, where ε ∈ Rd contains the per-dimension
leapfrog stepsizes and β0 ∈ (0, 1) is the initial inverse temperature. We constrain each of the leapfrog
step sizes such that εj ∈ (0, ξ) for some ξ > 0, for all j ∈ [d] – this is to prevent the leapfrog
discretization from entering unstable regimes. Note that φ ∈ Rd+1 in this example; in particular, we
do not optimize any parameters of the variational prior and thus require no further reparameterization.

We will compare HVAE with a basic Variational Bayes (VB) scheme with mean-ﬁeld approximate
posterior qφV (z|D) = N (z|µZ, ΣZ), where ΣZ is diagonal and φV ≡ {µZ, ΣZ} denotes the set
of learned variational parameters. We will also include a planar normalizing ﬂow of the form of
equation (10) in Rezende and Mohamed [22], but with the same ﬂow parameters across iterations to
keep the number of variational parameters of the same order as the other methods. The variational
prior here is also set to the true prior as in HVAE above. The log variational posterior log qφN (z|D)
is given by equation (13) of Rezende and Mohamed [22], where φN ≡ {u, v, b}5 ∈ R2d+1.
We set our true offset vector to be ∆ = (cid:0)− d−1
(cid:1) /5, and our scale parameters to range
quadratically from σ1 = 1, reaching a minimum at σ(d+1)/2 = 0.1, and increasing back to σd = 1.6
All experiments have N = 10,000 and all training was done using RMSProp [27] with a learning
rate of 10−3.

2 , . . . , d−1

2

To compare the results across methods, we train each method ten times on different datasets. For
, where ˆθ is the estimated value of θ given by the variational
each training run, we calculate
method on a particular run, and plot the average of this across the 10 runs for various dimensions in
Figure 1a. We note that, as the dimension increases, HVAE performs best in parameter estimation.
The VB method suffers most on prediction of ∆ as the dimension increases, whereas the NF method
does poorly on predicting Σ.

(cid:13)
(cid:13)
2
(cid:13)θ − ˆθ
(cid:13)
(cid:13)
(cid:13)
2

We also compare HVAE with tempering to HVAE without tempering, i.e. where β0 is ﬁxed to 1 in
training. This has the effect of making our Hamiltonian dynamics homogeneous in time. We perform
the same comparison as above and present the results in Figure 1b. We can see that the tempered
methods perform better than their non-tempered counterparts; this shows that time-inhomogeneous
dynamics are a key ingredient in the effectiveness of the method.

5Boldface vectors used to match notation of Rezende and Mohamed [22].
6When d is even, σ(d+1)/2 does not exist, although we could still consider (d + 1)/2 to be the location of

the minimum of the parabola deﬁning the true standard deviations.

7

4.2 Generative Model for MNIST

The next experiment that we consider is using HVAE to improve upon a convolutional variational
auto-encoder (VAE) for the binarized MNIST handwritten digit dataset. Again, our training data is
D = {x1, . . . , xN }, where each xi ∈ X ⊆ {0, 1}d for d = 28 × 28 = 784. The generative model is
as follows:

zi ∼ N (0, I(cid:96)),
d
(cid:89)

j=1

xi|zi ∼

Bernoulli((xi)j|πθ(zi)j),

for i ∈ [N ], where (xi)j is the jth component of xi, zi ∈ Z ≡ R(cid:96) is the latent variable associated
with xi, and πθ : Z → X is a convolutional neural network (i.e. the generative network, or encoder)
parametrized by the model parameters θ. This is the standard generative model used in VAEs in
which each pixel in the image xi is conditionally independent given the latent variable. The VAE
approximate posterior – and the HVAE variational prior across the latent variables in this case – is
given by qθ,φ(zi|xi) = N (zi|µφ(xi), Σφ(xi)), where µφ and Σφ are separate outputs of the same
neural network (the inference network, or encoder) parametrized by φ, and Σφ is constrained to be
diagonal.

We attempted to match the network structure of Salimans et al. [25]. The inference network consists
of three convolutional layers, each with ﬁlters of size 5 × 5 and a stride of 2. The convolutional
layers output 16, 32, and 32 feature maps, respectively. The output of the third layer is fed into a
fully-connected layer with hidden dimension nh = 450, whose output is then fully connected to the
output means and standard deviations each of size (cid:96). Softplus activation functions are used throughout
the network except immediately before the outputted mean. The generative network mirrors this
structure in reverse, replacing the stride with upsampling as in Dosovitskiy et al. [7] and replicated in
Salimans et al. [25].

We apply HVAE on top of the base convolutional VAE. We evolve samples from the variational
prior according to Algorithm 1 and optimize the new objective given in (12). We reparameterize
z0|x ∼ N (µφ(x), Σφ(x)) as z0 = µφ(x) + Σ1/2
φ (x) · (cid:15), for (cid:15) ∼ N (0, I(cid:96)) and x ∈ X , to generate
unbiased gradients of the ELBO with respect to φ. We select various values for K and set (cid:96) = 64. In
contrast with normalizing ﬂows, we do not need our ﬂow parameters ε and β0 to be outputs of the
inference network because our ﬂow is guided by the target. This allows our method to have fewer
overall parameters than normalizing ﬂow schemes. We use the standard stochastic binarization of
MNIST [24] as training data, and train using Adamax [14] with learning rate 10−3. We also employ
early stopping by halting the training procedure if there is no improvement in the loss on validation
data over 100 epochs.

To evaluate HVAE after training is complete, we estimate out-of-sample negative log likelihoods
(NLLs) using 1000 importance samples from the HVAE approximate posterior. For each trained
model, we estimate NLL three times, noting that the standard deviation over these three estimates
is no larger than 0.12 nats. We report the average NLL values over either two or three different
initializations (in addition to the three NLL estimates for each trained model) for several choices of
tempering and leapfrog steps in Table 1. A full accounting of the tests is given in the supplementary
material. We also consider an HVAE scheme in which we allow ε to vary across layers of the ﬂow
and report the results.

From Table 1, we notice that generally increasing the inhomogeneity in the dynamics improves the
test NLL values. For example, free tempering is the most successful tempering scheme, and varying
the leapfrog step size ε across layers also improves results. We also notice that increasing the number
of leapfrog steps does not always improve the performance, as K = 15 provides the best results in
free tempering schemes. We believe that the improvement in HVAE over the base VAE scheme can
be attributed to a more expressive approximate posterior, as we can see that samples from the HVAE
approximate posterior exhibit non-negligible covariance across dimensions. As in Salimans et al. [25],
we are also able to improve upon the base model by adding our time-inhomogeneous Hamiltonian
dynamics on top, but in a simpliﬁed regime without referring to learned reverse kernels. Rezende and
Mohamed [22] report only lower bounds on the log-likelihood for NFs, which are indeed lower than
our log-likelihood estimates, although they use a much larger number of variational parameters.

8

Table 1: Estimated NLL values for HVAE on MNIST. The base VAE achieves an NLL of 83.20. A
more detailed version of this table is included in the supplementary material.

ε ﬁxed across layers

ε varied across layers

T = Free

T = Fixed

T =None

T = Free

T = Fixed

T =None

K = 1
K = 5
K = 10
K = 15
K = 20

N/A
83.09
82.97
82.78
82.93

83.32
83.26
83.26
83.56
83.18

83.17
83.68
83.40
83.82
83.33

N/A
83.01
82.62
82.62
82.83

N/A
82.94
82.87
83.09
82.85

N/A
83.35
83.25
82.94
82.93

5 Conclusion and Discussion

We have proposed a principled way to exploit Hamiltonian dynamics within stochastic variational
inference. Contrary to previous methods [25, 28], our algorithm does not rely on learned reverse
Markov kernels and beneﬁts from the use of tempering ideas. Additionally, we can use the reparam-
eterization trick to obtain unbiased estimators of gradients of the ELBO. The resulting HVAE can
be interpreted as a target-driven normalizing ﬂow which requires the evaluation of a few gradients
of the log-likelihood associated to a single data point at each stochastic gradient step. However, the
Jacobian computations required for the ELBO are trivial. In our experiments, the robustness brought
about by the use of target-informed dynamics can reduce the number of parameters that must be
trained and improve generalizability.

We note that, although we have fewer parameters to optimize, the memory cost of using HVAE and
target-informed dynamics could become prohibitively large if the memory required to store evalua-
tions of ∇z log pθ(x, z) is already extremely large. Evaluating these gradients is not a requirement
of VAEs or standard normalizing ﬂows. However, we have shown that in the case of a fairly large
generative network we are still able to evaluate gradients and backpropagate through the layers of the
ﬂow. Further tests explicitly comparing HVAE with VAEs and normalizing ﬂows in various memory
regimes are required to determine in what cases one method should be used over the other.

There are numerous possible extensions of this work. Hamiltonian dynamics preserves the Hamilto-
nian and hence also the corresponding target distribution, but there exist other deterministic dynamics
which leave the target distribution invariant but not the Hamiltonian. This includes the Nosé-Hoover
thermostat. It is possible to directly use these dynamics instead of the Hamiltonian dynamics within
the framework developed in subsection 2.3. In continuous-time, related ideas have appeared in
physics [5, 21, 26]. This comes at the cost of more complicated Jacobian calculations. The ideas
presented here could also be coupled with the methodology proposed in [9] – we conjecture that this
could reduce the variance of the estimator (3) by an order of magnitude.

Anthony L. Caterini is a Commonwealth Scholar, funded by the UK government.

Acknowledgments

References

[1] Martín Abadi et al. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015.

URL https://www.tensorflow.org/. Software available from tensorﬂow.org.

[2] Rianne van den Berg, Leonard Hasenclever, Jakub M Tomczak, and Max Welling. Sylvester

normalizing ﬂows for variational inference. arXiv preprint arXiv:1803.05649, 2018.

[3] Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In

The 4th International Conference on Learning Representations (ICLR), 2016.

[4] Gavin E Crooks. Nonequilibrium measurements of free energy differences for microscopically
reversible Markovian systems. Journal of Statistical Physics, 90(5-6):1481–1487, 1998.

9

[5] Michel A Cuendet. Statistical mechanical derivation of Jarzynski’s identity for thermostated

non-hamiltonian dynamics. Physical Review Letters, 96(12):120602, 2006.

[6] Pierre Del Moral, Arnaud Doucet, and Ajay Jasra. Sequential Monte Carlo samplers. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 68(3):411–436, 2006.

[7] Alexey Dosovitskiy, Jost Tobias Springenberg, and Thomas Brox. Learning to generate chairs
with convolutional neural networks. In Computer Vision and Pattern Recognition (CVPR), 2015
IEEE Conference on, pages 1538–1546. IEEE, 2015.

[8] Paul Glasserman. Gradient estimation via perturbation analysis, volume 116. Springer Science

& Business Media, 1991.

[9] Jeremy Heng, Adrian N Bishop, George Deligiannidis, and Arnaud Doucet. Controlled sequen-

tial Monte Carlo. arXiv preprint arXiv:1708.08396, 2017.

[10] Matthew D Hoffman. Learning deep latent Gaussian models with Markov chain Monte Carlo.

In International Conference on Machine Learning, pages 1510–1519, 2017.

[11] Matthew D Hoffman and Andrew Gelman. The no-u-turn sampler: adaptively setting path
lengths in hamiltonian monte carlo. Journal of Machine Learning Research, 15(1):1593–1623,
2014.

[12] Christopher Jarzynski. Nonequilibrium equality for free energy differences. Physical Review

Letters, 78(14):2690, 1997.

[13] Christopher Jarzynski. Hamiltonian derivation of a detailed ﬂuctuation theorem. Journal of

Statistical Physics, 98(1-2):77–102, 2000.

[14] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

[15] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In The 2nd Interna-

tional Conference on Learning Representations (ICLR), 2014.

[16] Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max
Welling. Improved variational inference with inverse autoregressive ﬂow. In Advances in Neural
Information Processing Systems, pages 4743–4751, 2016.

[17] Chris J Maddison, John Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy
Mnih, Arnaud Doucet, and Yee Teh. Filtering variational objectives. In Advances in Neural
Information Processing Systems, pages 6576–6586, 2017.

[18] Radford M Neal. Annealed importance sampling. Statistics and Computing, 11(2):125–139,

2001.

[19] Radford M Neal. Hamiltonian importance sampling. www.cs.toronto.edu/pub/radford/
his-talk.ps, 2005. Talk presented at the Banff International Research Station (BIRS) work-
shop on Mathematical Issues in Molecular Dynamics.

[20] Radford M Neal et al. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte

Carlo, 2(11), 2011.

[21] Piero Procacci, Simone Marsili, Alessandro Barducci, Giorgio F Signorini, and Riccardo Chelli.
Crooks equation for steered molecular dynamics using a Nosé-Hoover thermostat. The Journal
of Chemical Physics, 125(16):164101, 2006.

[22] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows.

In

International Conference on Machine Learning, pages 1530–1538, 2015.

[23] Danilo Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approx-
imate inference in deep generative models. In International Conference on Machine Learning,
pages 1278–1286, 2014.

10

[24] Ruslan Salakhutdinov and Iain Murray. On the quantitative analysis of deep belief networks. In
Proceedings of the 25th international conference on Machine learning, pages 872–879. ACM,
2008.

[25] Tim Salimans, Diederik P Kingma, and Max Welling. Markov chain Monte Carlo and variational
inference: Bridging the gap. In International Conference on Machine Learning, pages 1218–
1226, 2015.

[26] E Schöll-Paschinger and Christoph Dellago. A proof of Jarzynski’s nonequilibrium work
theorem for dynamical systems that conserve the canonical distribution. The Journal of Chemical
Physics, 125(5):054105, 2006.

[27] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):
26–31, 2012.

[28] Christopher Wolf, Maximilian Karl, and Patrick van der Smagt. Variational inference with

Hamiltonian Monte Carlo. arXiv preprint arXiv:1609.08203, 2016.

11

Supplementary Material

A Full List of Tests on MNIST

Table 2 and Table 3 display the list of test runs of HVAE on MNIST. The number of ﬂow steps
is denoted by K. The total number of epochs varies in training because of early stopping. The
ELBO and NLL estimates are generated using 1000 importance samples from the HVAE approximate
posterior; this procedure is run 3 times for each seed. The average and standard deviation of these
estimates over the three runs is displayed. Table 2 refers to HVAE tests in which ε was ﬁxed across
ﬂow layers, whereas Table 3 refers to HVAE tests in which ε was allowed to vary across ﬂow layers.

Table 2: List of tests of HVAE for ε ﬁxed across ﬂow layers

K Tempering
ﬁxed
1
ﬁxed
1
none
1
none
1
ﬁxed
5
ﬁxed
5
free
5
free
5
none
5
none
5
ﬁxed
10
ﬁxed
10
free
10
free
10
free
10
none
10
none
10
ﬁxed
15
ﬁxed
15
free
15
free
15
none
15
none
15
ﬁxed
20
ﬁxed
20
free
20
free
20
none
20
none
20

Seed Total Epochs ELBO Estimate NLL Estimate
83.30 (0.06)
85547
83.33 (0.02)
12345
83.29 (0.07)
85547
83.04 (0.04)
12345
83.36 (0.02)
85547
83.16 (0.04)
12345
83.16 (0.04)
85547
83.02 (0.10)
12345
83.68 (0.05)
85547
83.68 (0.02)
12345
83.50 (0.03)
85547
83.02 (0.04)
12345
82.73 (0.05)
85547
83.18 (0.03)
12345
82.99 (0.08)
98765
83.24 (0.06)
85547
83.56 (0.03)
12345
83.93 (0.02)
85547
83.19 (0.07)
12345
82.75 (0.04)
85547
82.81 (0.06)
12345
83.87 (0.06)
85547
83.76 (0.03)
12345
83.18 (0.07)
85547
83.18 (0.09)
12345
82.96 (0.03)
85547
82.89 (0.02)
12345
83.30 (0.06)
85547
83.35 (0.05)
12345

86.49 (0.05)
86.53 (0.02)
86.43 (0.08)
86.21 (0.04)
86.69 (0.01)
86.40 (0.04)
86.45 (0.05)
86.31 (0.11)
86.82 (0.04)
86.82 (0.02)
86.75 (0.03)
86.26 (0.05)
86.06 (0.05)
86.65 (0.04)
86.47 (0.08)
86.47 (0.07)
86.78 (0.02)
87.11 (0.02)
86.51 (0.08)
86.19 (0.03)
86.16 (0.06)
87.05 (0.06)
86.95 (0.03)
86.30 (0.08)
86.32 (0.09)
86.28 (0.03)
86.27 (0.02)
86.36 (0.06)
86.44 (0.05)

1158
1131
1374
1519
998
1070
1046
1141
975
959
991
1340
1404
819
846
1425
1035
666
1204
1303
1028
785
989
1103
1126
1028
1131
1318
1299

12

Table 3: List of tests of HVAE for ε ﬁxed across ﬂow layers

K Tempering
ﬁxed
5
ﬁxed
5
free
5
free
5
none
5
none
5
ﬁxed
10
ﬁxed
10
free
10
free
10
free
10
none
10
none
10
ﬁxed
15
ﬁxed
15
free
15
free
15
none
15
none
15
ﬁxed
20
ﬁxed
20
free
20
free
20
none
20
none
20

Seed Total Epochs ELBO Estimate NLL Estimate
83.10 (0.04)
85547
82.77 (0.02)
12345
83.11 (0.02)
85547
82.91 (0.07)
12345
83.38 (0.06)
85547
83.32 (0.01)
12345
82.88 (0.05)
85547
82.85 (0.01)
12345
82.69 (0.03)
85547
82.37 (0.02)
12345
82.80 (0.09)
98765
83.35 (0.07)
85547
83.14 (0.12)
12345
83.20 (0.02)
85547
82.97 (0.02)
12345
82.57 (0.02)
85547
82.66 (0.03)
12345
82.96 (0.05)
85547
82.92 (0.01)
12345
82.71 (0.06)
85547
82.98 (0.05)
12345
82.75 (0.05)
85547
82.90 (0.02)
12345
82.83 (0.02)
85547
83.03 (0.06)
12345

86.28 (0.04)
85.89 (0.02)
86.24 (0.02)
86.07 (0.07)
86.50 (0.06)
86.46 (0.01)
86.01 (0.05)
86.01 (0.01)
85.98 (0.03)
85.61 (0.02)
86.17 (0.08)
86.44 (0.08)
86.29 (0.12)
86.41 (0.01)
86.21 (0.02)
85.92 (0.03)
85.96 (0.03)
86.08 (0.06)
86.06 (0.01)
85.85 (0.05)
86.18 (0.06)
86.02 (0.05)
86.09 (0.03)
85.97 (0.03)
86.17 (0.06)

1229
1579
1116
1302
1014
959
1473
1508
1303
1680
846
1035
1276
998
1070
1303
1131
1425
1355
1473
1200
1404
1131
1596
1327

Table 4 displays the list of test runs of the base VAE on MNIST. ELBO and NLL estimates are again
generated by importance sampling, but this time from the learned VAE approximate posterior.

Table 4: List of tests of VAE

Seed Total Epochs ELBO Estimate NLL Estimate
83.25 (0.01)
83.19 (0.07)
83.15 (0.07)

86.41 (0.01)
86.42 (0.07)
86.43 (0.07)

1524
1301
1381

85547
98765
12345

13

