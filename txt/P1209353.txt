6
1
0
2
 
r
p
A
 
1
1
 
 
]

V
C
.
s
c
[
 
 
1
v
1
0
8
2
0
.
4
0
6
1
:
v
i
X
r
a

Capturing Dynamic Textured Surfaces
of Moving Targets

Ruizhe Wang1, Lingyu Wei1, Etienne Vouga2,
Qixing Huang3, Duygu Ceylan4, G´erard Medioni1, and Hao Li1

1University of Southern California
{ruizhewa,lingyu.wei,medioni}@usc.edu
hao@hao-li.com
2University of Texas at Austin
evouga@cs.utexas.edu
3Toyota Technological Institute at Chicago
huangqx@ttic.edu
4Adobe Research
ceylan@adobe.com

Abstract. We present an end-to-end system for reconstructing complete water-
tight and textured models of moving subjects such as clothed humans and an-
imals, using only three or four handheld sensors. The heart of our framework
is a new pairwise registration algorithm that minimizes, using a particle swarm
strategy, an alignment error metric based on mutual visibility and occlusion. We
show that this algorithm reliably registers partial scans with as little as 15% over-
lap without requiring any initial correspondences, and outperforms alternative
global registration algorithms. This registration algorithm allows us to reconstruct
moving subjects from free-viewpoint video produced by consumer-grade sensors,
without extensive sensor calibration, constrained capture volume, expensive ar-
rays of cameras, or templates of the subject geometry.

Keywords: range image registration, particle swarm optimization, dynamic sur-
face reconstruction, free-viewpoint video, moving target, texture reconstruction

1 Introduction

The rekindling of interest in immersive, 360-degree virtual environments, spurred on by
the Oculus, Hololens, and other breakthroughs in consumer AR and VR hardware, has
birthed a need for digitizing objects with full geometry and texture from all views. One
of the most important objects to digitize in this way are moving, clothed humans, yet
they are also among the most challenging: the human body can undergo large deforma-
tions over short time spans, has complex geometry with occluded regions that can only
be seen from a small number of angles, and has regions like the face with important
high-frequency features that must be faithfully preserved.

Most techniques for capturing high-quality digital humans rely on a large array of
sensors mounted around a ﬁxed capture volume. The recent work of Collet et al. [1] uses
such a setup to capture live performances and compresses them to enable streaming of

2

R. Wang et al.

free-viewpoint videos. Unfortunately, these techniques are severely restrictive: ﬁrst, to
ensure high-quality reconstruction and sufﬁcient coverage, a large number of expensive
sensors must be used, leaving human capture out of reach of consumers without the
resources of a professional studio. Second, the subject must remain within the small
working volume enclosed by the sensors, ruling out subjects interacting with large,
open environments or undergoing large motions.

Using free-viewpoint sensors is an attractive alternative, since it does not constrain
the capture volume and allows ordinary consumers, with access to only portable, low-
cost devices, to capture human motion. The typical challenge with using hand-held
active sensors is that, obviously, multiple sensors must be used simultaneously from
different angles to achieve adequate coverage of the subject. In overlapping regions, sig-
nal interference causes signiﬁcant deterioration in the quality of the captured geometry.
This problem can be avoided by minimizing the amount of overlap between sensors,
but on the other hand, existing registration algorithms for aligning the captured par-
tial scans only work reliably if the partial scans signiﬁcantly overlap. Template-based
methods like the work of Ye et al [2] circumvent these difﬁculties by warping a full ge-
ometric template to track the moving sparse partial scans, but templates are only readily
available for naked humans [3]; for clothed humans a template must be precomputed
on a case-by-case basis.

We thus introduce a new shape registration method that can reliably register partial
scans even with almost no overlap, sidestepping the need for shape templates or sensor
arrays. This method is based on a visibility error metric which encodes the intuition that
if a set of partial scans are properly registered, each partial scan, when viewed from the
same angle at which it was captured, should occlude all other partial scans. We solve
the global registration problem by minimizing this error metric using a particle swarm
strategy, to ensure sufﬁcient coverage of the solution space to avoid local minima. This
registration method signiﬁcantly outperforms state of the art global registration tech-
niques like 4PCS [4] for challenging cases of small overlap.

Contributions. We present the ﬁrst end-to-end free-viewpoint reconstruction frame-
work that produces watertight, fully-textured surfaces of moving, clothed humans using
only three to four handheld depth sensors, without the need of shape templates or ex-
tensive calibration. The most signiﬁcant technical component of this system is a robust
pairwise global registration algorithm, based on minimizing a visibility error metric,
that can align depth maps even in the presence of very little (15%) overlap.

2 Related Work

Digitizing realistic, moving characters has traditionally involved an intricate pipeline in-
cluding modeling, rigging, and animation. This process has been occasionally assisted
by 3D motion and geometry capture systems such as marker-based motion capture or
markerless capture methods involving large arrays of sensors [5]. Both approaches sup-
ply artists with accurate reference geometry and motion, but they require specialized
hardware and a controlled studio setting.

Real-time 3D scanning and reconstruction systems requiring only a single sensor,
like KinectFusion [6], allow casual users to easily scan everyday objects; however, as

Capturing Dynamic Textured Surfaces of Moving Targets

3

with most simultaneous localization and mapping (SLAM) techniques, the major as-
sumption is that the scanned scene is rigid. This assumption is invalid for humans, even
for humans attempting to maintain a single pose; several follow-up works have ad-
dressed this limitation by allowing near-rigid motion, and using non-rigid partial scan
alignment algorithms [7, 8]. While the recent DynamicFusion framework [9] and simi-
lar systems [10] show impressive results in capturing non-rigidly deforming scenes, our
goal of capturing and tracking freely moving targets is fundamentally different: we seek
to reconstruct a complete model of the moving target at all times, which requires either
extensive prior knowledge of the subject’s geometry, or the use of multiple sensors to
provide better coverage.

Prior work has proposed various simplifying assumptions to make the problem of
capturing entire shapes in motion tractable. Examples include assuming availability of
a template, high-quality data, smooth motion, and a controlled capture environment.

Template-based Tracking: The vast majority of related work on capturing dynamic
motion focuses on speciﬁc human parts, such as faces [11] and hands [12,13], for which
specialized shapes and motion templates are available. In the case of tracking the full
human body, parameterized body models [14] have been used. However, such models
work best on naked subjects or subjects wearing very tight clothing, and are difﬁcult to
adapt to moving people wearing more typical garments.

Another category of methods ﬁrst capture a template in a static pose and then track it
across time. Vlasic et al [15] use a rigged template model, and De Aguiar et al [16] apply
a skeleton-less shape deformation model to the template to track human performances
from multi-view video data. Other methods [17, 18] use a smoothed template to track
motion from a capture sequence. The more recent work of Wu et al. [19] and Liu et
al. [20] track both the surface and the skeleton of a template from stereo cameras and
sparse set of depth sensors respectively.

All of these template-based approaches handle with ease the problem of tracking
moving targets, since the entire geometry of the target is known. However, in addition
to requiring constructing or ﬁtting said template, these methods share the common limi-
tation that they cannot handle geometry or topology changes which are likely to happen
during typical human motion (picking up an object; crossing arms; etc).

Dynamic Shape Capture: Several works have proposed to reconstruct both shape and
motion from a dynamic motion sequence. Given a series of time-varying point clouds,
Wand et al. [21] use a uniform deformation model to capture both geometry and motion.
A follow-up work [22] proposes to separate the deformation models used for geome-
try and motion capture. Both methods make the strong assumption that the motion is
smooth, and thus suffer from popping artifacts in the case of large motions between time
steps. S¨ußmuth et al. [23] ﬁt a 4D space-time surface to the given sequence but they as-
sume that the complete shape is visible in the ﬁrst frame. Finally, Tevs et al. [24] detect
landmark correspondences which are then extended to dense correspondences. While
this method can handle a considerable amount of topological change, it is sensitive to
large acquisition holes, which are typical for commercial depth sensors.

Another category of related work aims to reconstruct a deforming watertight mesh
from a dynamic capture sequence by imposing either visual hull [25] or temporal co-

4

R. Wang et al.

herency constraints [26]. Such constraints either limit the capture volume or are not
sufﬁcient to handle large holes. Furthermore, neither of these methods focus on propa-
gating texture to invisible areas; in contrast, we use dense correspondences to perform
texture inpainting in non-visible regions. Bojsen-Hansen et al. [27] also use dense corre-
spondences to track surfaces with evolving topologies. However, their method requires
the input to be a closed manifold surface. Our goal, on the other hand, is to reconstruct
such complete meshes from sparse partial scans.

The recent work of Collet et al. [1] uses multimodal input data from a stage setup to
capture topologically-varying scenes. While this method produces impressive results, it
requires a pre-calibrated complex setup. In contrast, we use a signiﬁcantly cheaper and
more convenient setup composed of three to four commercial depth sensors.

Global Range Image Registration: At the heart of our approach is a robust algorithm
that registers noisy data coming from each commercial depth sensor with very little
overlap. A typical approach is to ﬁrst perform global registration to compute an ap-
proximate rigid transformation between a pair of range images, which is then used
to initialize local registration methods (e.g., Iterative Closest Point (ICP) [28, 29]) for
further reﬁnement. A popular approach for global registration is to construct feature
descriptors for a set of interest points which are then correlated to estimate a rigid
transformation. Spin-images [30], integral volume descriptors [31], and point feature
histograms (PFH, FPFH) [32, 33] are among the popular descriptors proposed by prior
work. Makadia et al. [34] represent each range image as a translation-invariant em-
phextended gaussian Image (EGI) [35] using surface normals. They ﬁrst compute the
optimum rotation by correlating two EGIs and further estimate the corresponding trans-
lation using Fourier transform. For noisy data as coming from a commercial depth sen-
sor, however, it is challenging to compute reliable feature descriptors. Another approach
for global registration is to align either main axes extracted by principal component
analysis (PCA) [36] or a sparse set of control points in a RANSAC loop [37]. Silva
et al. [38] introduce a robust surface interpenetration measure (SIM) and search the 6
DoF parameter space with a genetic algorithm. More recently, Yang et al. [39] adopt a
branch-and-bound strategy to extend the basic ICP algorithm in a global manner. 4PCS
[4] and its latest variant Super-4PCS [40] register a pair of range images by extracting
all coplanar 4-points sets. Such approaches, however, are likely to converge to wrong
alignments in cases of very little overlap between the range images (see Section 5).

Several prior works have adopted silhouette-based constraints for aligning multiple
images [41–47]. While the idea is similar to our approach, our registration algorithm
also takes advantage of depth information, and employs a particle-swarm optimization
strategy that efﬁciently explores the space of alignments.

3 System Overview

Our pipeline for reconstructing fully-textured, watertight meshes from three to four
depth sensors can be decomposed into four major steps. See Figure 1 for an overview
of how these steps interrelate.

1. Data Capture: We capture the subject (who is free to move arbitrarily) using
uncalibrated hand-held real-time RGBD sensors. We experimented with both Kinect

Capturing Dynamic Textured Surfaces of Moving Targets

5

Fig. 1. An overview of our textured dynamic surface capturing system.

One time-of-ﬂight cameras mounted on laptops, and Occipital Structure IO sensors
mounted on iPad Air 2 tablets (section 6).

2. Global Rigid Registration: The relative positions of the depth sensors constantly
change over time, and the captured depth maps often have little overlap (10%-30%).
For each frame, we globally register sparse depth images from all views (section 4).
This step produces registered, but incomplete, textured partial scans of the subject for
each frame.

3. Surface Reconstruction: To reduce ﬂickering artifacts, we adopt the shape com-
pletion pipeline of Li et al [26] to warp partial scans from temporally-proximate frames
to the current frame geometry. A weighted Poisson reconstruction step then extracts a
single watertight surface. There is no guarantee, however, that the resulted fused surface
has complete texture coverage (and indeed typically texture will be missing at partial
scan seams and in occluded regions.)

4. Dense Correspondences for Texture Reconstruction: We complete regions of
missing or unreliable texture on one frame by propagating data from other (perhaps
very temporally-distant) frames with reliable texture in that region. We adopt a recently-
proposed correspondence computation framework [48] based on a deep neural network
to build dense correspondences between any two frames, even if the subject has un-
dergone large relative deformations. Upon building dense correspondences, we transfer
texture from reliable regions to less reliable ones.

We next describe the details of the global registration method as it constitutes the
core of our pipeline. Please refer to the supplementary material for more details of the
other components.

4 Robust Rigid Registration

The key technical challenge in our pipeline is registering a set of depth images accu-
rately without assuming any initialization, even when the geometry visible in each depth
image has very little overlap with any other depth image. We attack this problem by de-
veloping a robust pairwise global registration method: let P1 and P2 be partial meshes
generated from two depth images captured simultaneously. We seek a global Euclidean
transformation T12 which aligns P2 to P1. Traditional pairwise registration based on
ﬁnding corresponding points on P1 and P2, and minimizing the distance between them,
has notorious difﬁculty in this setting. As such we propose a novel visibility error metric
(VEM) (Section 4.1), and we minimize the VEM to ﬁnd T12 (Section 4.2). We further
extend this pairwise method to handle multi-view global registration (Section 4.3).

6

R. Wang et al.

4.1 Visibility Error Metric

Suppose P1 and P2 are correctly aligned,
and consider looking at the pair of scans
through a camera whose position and ori-
entation matches that of the sensor used
to capture P1. The only parts of P2 that
should be visible from this view are those
that overlap with P1: parts of P2 that
do not overlap should be completely oc-
cluded by P1 (otherwise they would have
been detected and included in P1). Simi-
larly, when looking at the scene through
the camera that captured P2, only parts of
P1 that overlap with P2 should be visible.
Visibility-Based Alignment Error We
now formalize the above idea. Let P1, P2
be two partial scans, with P1 captured us-
ing a sensor at position cp and view direc-
tion cv. For every point x ∈ P2, let I(x)
be the ﬁrst intersection point of P1 and the ray −→cpx. We can partition P2 into three re-
gions, and associate to each region an energy density d(x, P1) measuring the extent to
which points x in that region violate the above visibility criteria:

Fig. 2. Left: two partial scans P1 (dotted) and
P2 (solid) of a 2D bunny. Middle: when viewed
from P1’s camera, P2 is entirely occluded
(blue). Therefore all of P2 is in O. Right: when
viewed from P2’s camera, parts of P1 are in O
(blue), parts occlude P2 and are thus in F (yel-
low), and parts are in B (red).

– points x ∈ O that are occluded by P1: (cid:107)x − cp(cid:107) ≥ (cid:107)I(x) − cp(cid:107). To points in this

region we associate no energy:

dO(x, P1) = 0.

– points x ∈ F that are in front of P1: (cid:107)x − cp(cid:107) < (cid:107)I(x) − cp(cid:107). Such points might
exist even when P1 and P2 are well-aligned, due to surface noise and roughness,
etc. However, we penalize large violations using:

dF (x, P1) = (cid:107)x − I(x)(cid:107)2.

– points x ∈ B for which I(x) does not exist. Such points also violate the visibility
criteria. It is tempting to penalize such points proportionally to the distance between
x and its closest point on P1, but a small misalignment could create a point in B that
is very distant from P1 in Euclidean space, despite being very close to P1 on the
camera image plane. We therefore penalize x using squared distance on the image
plane,

dB(x, P1) = min
y∈S1

(cid:107)Pcv x − Pcv y(cid:107)2 ,

where Pcv is the projection I − cvcT

v onto the plane orthogonal to cv.

Figure 2 illustrates these regions on a didactic 2D example. Alignment of P1 and
P2 from the point of view of P1 is then measured by the aggregate energy d(P2, P1) =
(cid:80)
d(x, P1). Finally, every Euclidean transformation T12 that produces a possible

x∈P2

Capturing Dynamic Textured Surfaces of Moving Targets

7

alignment between P1 and P2 can be associated with an energy to deﬁne our visibility
error metric on SE(3),

E(T12) = d (cid:0)T −1

12 P1, P2

(cid:1) + d (T12P2, P1) .

(1)

4.2 Finding the Transformation

(a)

(b)

Fig. 3. (a) Left: a pair of range images to be registered. Right: VEM evaluated on the entire ro-
tation space. Each point within the unit ball represents the vector part of a unit quaternion; for
each quaternion, we estimate its corresponding translation component and evaluate the VEM
on the composite transformation. The red rectangles indicate areas with local minima, and the
red cross is the global minimum. (b) Example particle locations and displacements at iteration
1 and k. Blue vectors indicate displacement of regular (non-guide) particles following a tradi-
tional particle swarm scheme. Red vectors are displacements of guide particles. Guide particles
draw neighboring regular particles more efﬁciently towards local minima to search for the global
minimum.

Minimizing the error metric (1) consists of solving a nonlinear least squares prob-
lem and so in principle can be optimized using e.g. the Gauss-Newton method. How-
ever, it is non-convex, and prone to local minima (Figure 3(a)). Absent a straightforward
heuristic for picking a good initial guess, we instead adopt a Particle Swarm Optimiza-
tion (PSO) [49] method to efﬁciently minimize (1), where “particles” are candidate rigid
transformations that move towards smaller energy landscapes in SE(3). We could in-
dependently minimize E starting from each particle as an initial guess, but this strategy
is not computationally tractable. So we iteratively update all particle positions in lock-
step: a small set of the most promising guide particles, that are most likely to be close
to the global minimum, are updated using an iteration of Levenberg-Marquardt. The
rest of the particles receive PSO-style weighted random perturbations. This procedure
is summarized in Algorithm 1, and each step is described in more detail below.

Initial Particle Sampling We begin by sampling N particles (we use N = 1600),
where each particle represents a rigid motion mi ∈ SE(3). Since SE(3) is not com-
pact, it is not straightforward to directly sample the initial particles. We instead uni-
formly sample only the rotational component Ri of each particle [50], and solve for the

1, ..., T0

N } ∈ SE(3)N

8

R. Wang et al.

select guide particles
for each guide particle do

Algorithm 1 Modiﬁed Particle Swarm Optimization
1: Input: A set of initial “particles” (orientations) {T0
2: evaluate VEM on initial particles
3: for each iteration do
4:
5:
6:
7:
8:
9:
10:
11:
12: end for
13: Output: The best particle Tb

end for
recalculate VEM at new locations

end for
for each regular particle do

update guide particle using Levenberg-Marquardt

update particle using weighted random displacement

best translation using the following Hough-transform-like procedure. For every x ∈ P1
and y ∈ RiP2, we measure the angle between their respective normals, and if it is less
than 20◦, the pair (x, y) votes for a translation of y − x. These translations are binned
(we use 10mm × 10mm × 10mm bins) and the best translation t0
i is extracted from
the bin with the most votes. The translation estimation procedure is robust even in the
presence of limited overlap amount (Figure 4).

The above procedure yields a set T 0 = {T 0

i )} of N initial particles.
We next describe how to step the orientation particles from their values T k at iteration
k to T k+1 at iteration k + 1.

i } = {(R0

i , t0

Identifying Guide Particles We want to select
as guide particles those particles with lowest vis-
ibility error metric; however we don’t want many
clustered redundant guide particles. Therefore we
ﬁrst promote the particle T k
i with lowest error
metric to guide particle, then remove from consid-
eration all nearby particles, e.g. those that satisfy

dθ(Rk

j , Rk

i ) ≤ θr,

(cid:17)

(cid:16)

log (cid:2)Rk

(cid:3)−1

j

Rk
i

i , Rk

j ) = θ

where dθ(Rk
is the
bi-invariant metric on SO(3), e.g. the least an-
gle of all rotations R with Rk
j . We use
θτ = 30◦. We then repeat this process (promot-
ing the remaining particle with lowest VEM, re-
moving nearby particles, etc) until no candidates
remain.

i = RRk

Guide Particle Update We update each guide
i to decrease its VEM. We parameterize
i by two vectors

particle T k
the tangent space of SE(3) at T k
u, v ∈ R3 with exp(u, v) = (cid:0)exp([u]×)Rk

i , tk

Fig. 4. Translation estimation exam-
ples of our Hough Transform method
on range scans with limited over-
lap. The na¨ıve method, which sim-
ply aligns the corresponding centroids,
fails to estimate the correct translation.

i + v(cid:1), where [u]× is the cross-product

Capturing Dynamic Textured Surfaces of Moving Targets

9

matrix. We then use the Levenberg-Marquardt method to ﬁnd an energy-decreasing
direction (u, v), and set T k+1
= exp(u, v). Please see the supplementary material for
more details.

i

Other Particle Update Performing a Levenberg-Marquardt iteration on all parti-
cles is too expensive, so we move the remaining non-guide particles by applying a
randomly weighted summation of each particle’s displacement during the previous it-
eration, the displacement towards its best past position, and the displacement towards
the local best particle within radius θr (measured using dθ) with lowest energy, as in
standard PSO [49]. While the guide particles rapidly descend to local minima, they are
also local best particles and drag neighboring regular particles with them for a more ef-
ﬁcient search of all local minima, from which the global one is extracted (Figure 3(b)).
Please refer to the supplementary material for more details.

Termination Since the VEM of each guide particle is guaranteed to decrease during
every iteration, the particle with lowest energy is always selected as a guide particle, and
the local minima of E must lie in a bounded subset of SE(3). In the above procedure
the particle with lowest energy is guaranteed to converge to a local minimum of E. We
terminate the optimization when mini |E(T k
)| ≤ 10−4. In practice this
occurs within 5–10 iterations.

i ) − E(T k+1

i

4.3 Multi-view Extension

We extend our VEM-based pairwise registration method to globally align a total of M
partial scans (cid:8)P1, ..., PM
(cid:9).
(cid:9) by estimating the optimum transformation set (cid:8)T12, ..., T1M
First we perform pairwise registration between all pairs to build a registration graph,
where each vertex represents a partial scan and each pair of vertices are linked by
an edge of the estimated transformation. We then extract all spanning trees from the
graph, and for each spanning tree we calculate its corresponding transformation set
(cid:8)T12, ..., T1M

(cid:9) and estimate the overall VEM as,

EM =

d (cid:0)T −1

1j T1iPi, Pj

(cid:1) + d (cid:0)T −1

1i T1jPj, Pi

(cid:1) .

(2)

(cid:88)

i(cid:54)=j

We select the transformation set with the minimum overall VEM. We perform several
iterations of Levenberg-Marquardt algorithm to minimize Equation 2 to further jointly
reﬁne the transformation set.

Temporal Coherence When globally registering depth images from multiple sensors
frame by frame, we can easily incorporate temporal coherence into the global registra-
tion framework by adding the ﬁnal estimated transformation set of the previous frame
to the pool of transformation sets of the current frame before selecting the best one. It is
worth mentioning, however, that our capturing system does not rely on the assumption
of temporal coherence and the transformation set is estimated globally for each frame.
This is especially crucial for a system with handheld sensors, where the temporal co-
herence assumption is easily violated.

10

R. Wang et al.

5 Global Registration Evaluation

Data Sets. We evaluate our registration algorithm on the Stanford 3D Scanning Repos-
itory and the Princeton Shape Benchmark [51]. We use 4 models from the Stanford 3D
Scanning Repository (the Bunny, the Happy Buddha, the Dragon, and the Amardillo),
and use all 1814 models from the Princeton Shape Benchmark. We believe these two
data sets, especially the latter, are general enough to cover shape variation of real world
objects. For each data set, we generated 1000 pairs of synthetic depth images with uni-
formly varying degrees of overlap; these range maps were synthesized using randomly-
selected 3D models and randomly-selected camera angles. Each pair is then initialized
with a random initial relative transformation. As such, for each pair of range images,
we have the ground truth transformation as well as their overlap ratio.

Evaluation Metric. The extracted transformation, if not correctly estimated, can be at
any distance from the ground truth transformation, depending on the speciﬁc shape of
the underlying surfaces and the local minima distribution of the solution space. Thus,
it is not very informative to directly use the RMSE of rotation and translation estima-
tion. It is rather straightforward to use success percentage as the evaluation metric. We
claim the global registration to be successful if the error dθ(Rest, Rgt) of the estimated
rotation Rest is smaller than a small angle 10◦. We do not enforce the translation to be
close since it is scale-dependent and the translation component is easily recovered by
a robust local registration method if the rotation component is close enough (e.g., by
using surface normals to prune incorrect correspondences [52]).

Effectiveness of the PSO Strategy. To demonstrate the advantage of the particle-swarm
optimization strategy, we compare our full algorithm to three alternatives on the Stan-
ford 3D Scanning Repository: 1) a baseline method that simply reports the minimum
particles from all initially-sampled particles, with no attempt at optimization; 2) using
only a traditional PSO formulation, without guide particles; and 3) updating only the
guide particles, and applying no displacement to ordinary particles.

Figure 5 compares the performance of the four alternatives. While updating guide
particles alone achieves good registration results, incorporating the swarm intelligence
further improves the performance, especially on range scans with overlap ratios below
30%.

Comparisons. To demonstrate the effectiveness of the proposed registration method, we
compare it against four other alternatives: 1) a baseline method that aligns principal axes
extracted with weighted PCA [36], where the weight of each vertex is proportional to its
local surface area; 2) Go-ICP [39], which combines local ICP with a branch-and-bound
search to ﬁnd the global minima; 3) FPFH [33, 53], which matches FPFH descriptors;
4) 4PCS, a state-of-the-art method that performs global registration by constructing a
congruent set of 4 points between range images [4]. We do not compare with its latest
variant SUPER-4PCS [40] as only efﬁciency is improved for the latter. For Go-ICP,
FPFH and 4PCS, we use the authors’ original implementation and tune parameters to
achieve optimum performance.

Capturing Dynamic Textured Surfaces of Moving Targets

11

Fig. 5. Success percentage of the global registration method employing different optimization
schemes on the Stanford 3D Scanning Repository.

Figure 6 compares the performance of the ﬁve methods on the two data sets respec-
tively. The overall performance on the Princeton Shape Benchmark is lower as this data
set is more challenging with many symmetric objects. As expected the baseline PCA
method only works well when there is sufﬁcient overlap. All previous methods expe-
rience a dramatic fall in accuracy once the overlap amount drops below 40%; 4PCS
performs the best out of these, but because 4PCS is essentially searching for the most
consistent area shared by two shapes, for small overlap ratio, it can converge to false
alignments (Figure 1). Our method outperforms all previous approaches, and doesn’t
experience degraded performance until overlap falls below 15%. The average perfor-
mance is summarized in Table 1.

Table 1. Performance of global registration algorithms on two data sets. Average running time is
measured using a single thread on an Intel Core i7-4710MQ CPU clocked at 2.5 GHz.

Stanford (%)
Princeton (%)
Runtime (sec)

PCA
19.5
18.5
0.01

GO-ICP
34.1
22.0
25

FPFH
49.3
33.0
3

4PCS
73.0
73.2
10

Our Method
93.6
81.5
0.5

Performance on Real Data. We further compare the performance of our registration
method with 4PCS on pairs of depth maps captured from Kinect One and Structure
IO sensors. The hardware setup used to obtain this data is described in detail in the
next section. These depth maps share only 10%-30% overlap and 4PCS often fails to
compute the correct alignment as shown in Figure 8.

12

R. Wang et al.

Fig. 6. Success percentage of our global registration method compared with other methods. Left:
Comparison on the Stanford 3D Scanning Repository. Right: Comparison on the Princeton Shape
Benchmark.

Limitations. Our global registration method, like most other methods, fails to align
scans with dominant symmetries since in such cases depth alone is not enough to resolve
the ambiguity. This limitation holds for scans depicting large planar surfaces (e.g. walls
and ground) due to continuous symmetry.

6 Dynamic Capture Results

Hardware. We provide results of our dynamic scene capture system. We experiment
with two popular depth sensors, namely the Kinect One (V2) sensor and the Structure
IO sensor. We mount the former on laptops and extend the capture range with long
power extension cables. For the latter, we attach it to iPad Air 2 tablets and stream data
to laptops through wireless network. Kinect One sensors stream high-ﬁdelity 512x424
depth images and 1920x1080 color images at 30 fps. We use it to cover the entire
human body from 3 or 4 views at approximately 2 meters away. Structure IO sensors
stream 640x480 for both depth and color (iPad RGB camera after compression) images
at 30 fps. Per pixel depth accuracy of the Structure IO sensor is relatively low and
unreliable, especially when used outdoor beyond 2 meters. Thus, we use it to capture
small objects, e. g. , dogs and children, at approximately 1 meter away. Our mobile
capture setting allows the subject to move freely in space in stead of being restricted to
a speciﬁc capture volume.

Pre-processing. For each depth image, ﬁrst we remove background by threshold-
ing depth value and removing dominant planar segments in a RANSAC fashion. For
temporal synchronization across depth sensors, we use visual cues, i. e. , jumping and
clapping hands, to manually initialize the starting frame. Then we automatically syn-
chronize all remaining frames by using the system time stamp of each frame, which is
accurate up to milliseconds.

Capturing Dynamic Textured Surfaces of Moving Targets

13

Fig. 7. Example registration results of range images with limited overlap. First and second row
show examples from the Stanford 3D Scanning Repository and the Princeton Shape Benchmark
respectively. Please see the supplementary material for more examples.

Fig. 8. Our registration method compared with 4PCS on real data. First two examples are captured
by Kinect One sensors while the last example is captured by Structure IO sensors.

Performance. We process data using a single thread Intel Core i7-4710MQ CPU
clocked at 2.5 GHz. It takes on average 15 seconds to globally align all the views for
each frame, 5 minutes for surface denoising and reconstruction, and 3 minutes for build-
ing dense correspondences and texture reconstruction.

Results. We capture a variety of motions and objects, including walking, jumping,
playing Tai Chi and dog training (see the supplementary material for a complete list).
For all captures, the performer(s) are able to move freely in space while 3 or 4 people
follow them with depth sensors. As shown in Figure 9, our geometry reconstruction
method reduces ﬂickering artifacts of the original Poisson reconstruction, and our tex-
ture reconstruction method recovers reliable texture on occluded areas. Figure 10 pro-
vides several examples that demonstrate the effectiveness and ﬂexibility of our capture
system. Our global registration method plays a key role as most range images share
only 10% to 30% overlap. While we demonstrate successful sequences with 3 depth
sensors, using an additional sensor typically improves the reconstruction quality since
it provides higher overlap between neighboring views leading to a more robust registra-
tion.

As opposed to most existing free-form surface reconstruction techniques, our method
can handle performances of subjects that move through a long trajectory instead of be-
ing constrained to a capture volume. Since our method does not require a template,
it is not restricted to human performances and can successfully capture animals for

14

R. Wang et al.

Fig. 9. From left to right: Globally aligned partial scans from multiple depth sensors; The water-
tight mesh model after Poisson reconstruction [54]; Denoised mesh after merging neighboring
meshes by using [26]; Model after our dense correspondences based texture reconstruction;
Model after directly applying texture-stitcher [55].

Fig. 10. Example capturing results. The sequence in the lower right corner is reconstructed from
Structure IO sensors, while other sequences are reconstructed from Kinect One Sensors.

which obtaining a static template would be challenging. The global registration method
employed for each frame effectively reduces drift for long capture sequences. We can
recover plausible textures even in regions that are not fully captured by the sensors
using textures from frames where they are visible.

7 Conclusion

We have demonstrated that it is possible, using only a small number of synchronized
consumer-grade handheld sensors, to reconstruct fully-textured moving humans, and
without restricting the subject to the constrained environment required by stage se-
tups with calibrated sensor arrays. Our system does not require a template geometry
in advance and thus can generalize well to a variety of subjects including animals and
small children. Since our system is based on low-cost devices and works in fully un-
constrained environments, we believe our system is an important step toward accessible

Capturing Dynamic Textured Surfaces of Moving Targets

15

creation of VR and AR content for consumers. Our results depend critically on our new
alignment algorithm based on the visibility error metric, which can reliably align partial
scans with much less overlap than is required by current state-of-the-art registration al-
gorithms. Without this alignment algorithm, we would need to use many more sensors,
and solve the sensor interference problem that would arise. We believe this algorithm is
an important contribution on its own, as it represents a signiﬁcant step forward in global
registration.

References

1. Collet, A., Chuang, M., Sweeney, P., Gillett, D., Evseev, D., Calabrese, D., Hoppe, H., Kirk,
A., Sullivan, S.: High-quality streamable free-viewpoint video. In: ACM SIGGRAPH. Vol-
ume 34., ACM (July 2015) 69:1–69:13

2. Ye, G., Deng, Y., Hasler, N., Ji, X., Dai, Q., Theobalt, C.: Free-viewpoint video of human
IEEE Transactions on Cybernetics 43(5) (2013)

actors using multiple handheld kinects.
1370–1382

3. Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., Davis, J.: Scape: Shape

completion and animation of people. ACM Trans. Graph. 24(3) (July 2005) 408–416

4. Aiger, D., Mitra, N.J., Cohen-Or, D.: 4-points congruent sets for robust pairwise surface

registration. In: ACM Transactions on Graphics (TOG). Volume 27., ACM (2008) 85
5. Debevec, P.: The Light Stages and Their Applications to Photoreal Digital Actors.

In:

SIGGRAPH Asia, Singapore (November 2012)

6. Izadi, S., Kim, D., Hilliges, O., Molyneaux, D., Newcombe, R., Kohli, P., Shotton, J.,
Hodges, S., Freeman, D., Davison, A., Fitzgibbon, A.: Kinectfusion: Real-time 3d recon-
struction and interaction using a moving depth camera.
In: UIST, New York, NY, USA,
ACM (2011) 559–568

7. Tong, J., Zhou, J., Liu, L., Pan, Z., Yan, H.: Scanning 3d full human bodies using kinects.

IEEE TVCG 18(4) (April 2012) 643–650

8. Li, H., Vouga, E., Gudym, A., Luo, L., Barron, J.T., Gusev, G.: 3d self-portraits. In: ACM

SIGGRAPH Asia. Volume 32., ACM (November 2013) 187:1–187:9

9. Newcombe, R.A., Fox, D., Seitz, S.M.: Dynamicfusion: Reconstruction and tracking of

non-rigid scenes in real-time. In: IEEE CVPR. (June 2015)

10. Dou, M., Taylor, J., Fuchs, H., Fitzgibbon, A., Izadi, S.: 3d scanning deformable objects

with a single rgbd sensor. In: IEEE CVPR. (June 2015) 493–501

11. Li, H., Yu, J., Ye, Y., Bregler, C.: Realtime facial animation with on-the-ﬂy correctives. In:

ACM SIGGRAPH. Volume 32., ACM (July 2013) 42:1–42:10

12. Qian, C., Sun, X., Wei, Y., Tang, X., Sun, J.: Realtime and robust hand tracking from depth.

In: IEEE CVPR, IEEE (2014) 1106–1113

13. Oikonomidis, I., Kyriazis, N., Argyros, A.A.: Tracking the articulated motion of two strongly

interacting hands. In: IEEE CVPR, IEEE (2012) 1862–1869

14. Bogo, F., Black, M.J., Loper, M., Romero, J.: Detailed full-body reconstructions of moving

people from monocular RGB-D sequences. (December 2015) 2300–2308

15. Vlasic, D., Baran, I., Matusik, W., Popovi´c, J.: Articulated mesh animation from multi-view
In: ACM SIGGRAPH. SIGGRAPH ’08, New York, NY, USA, ACM (2008)

silhouettes.
97:1–97:9

16. de Aguiar, E., Stoll, C., Theobalt, C., Ahmed, N., Seidel, H.P., Thrun, S.: Performance
capture from sparse multi-view video. In: ACM SIGGRAPH, New York, NY, USA, ACM
(2008) 98:1–98:10

16

R. Wang et al.

17. Li, H., Adams, B., Guibas, L.J., Pauly, M.: Robust single-view geometry and motion recon-
struction. In: ACM SIGGRAPH Asia. SIGGRAPH Asia ’09, New York, NY, USA, ACM
(2009) 175:1–175:10

18. Zollh¨ofer, M., Nießner, M., Izadi, S., Rehmann, C., Zach, C., Fisher, M., Wu, C., Fitzgibbon,
A., Loop, C., Theobalt, C., Stamminger, M.: Real-time non-rigid reconstruction using an
rgb-d camera. In: ACM SIGGRAPH. Volume 33., New York, NY, USA, ACM (July 2014)
156:1–156:12

19. Wu, C., Stoll, C., Valgaerts, L., Theobalt, C.: On-set performance capture of multiple actors

with a stereo camera. ACM Trans. Graph. 32(6) (November 2013) 161:1–161:11

20. Liu, Y., Ye, G., Wang, Y., Dai, Q., Theobalt, C.: Human Performance Capture Using Mul-
tiple Handheld Kinects. In: Computer Vision and Machine Learning with RGB-D Sensors.
Springer International Publishing, Cham (2014) 91–108

21. Wand, M., Jenke, P., Huang, Q., Bokeloh, M., Guibas, L., Schilling, A.: Reconstruction of
deforming geometry from time-varying point clouds. In: SGP. SGP ’07 (2007) 49–58
22. Wand, M., Adams, B., Ovsjanikov, M., Berner, A., Bokeloh, M., Jenke, P., Guibas, L., Seidel,
H.P., Schilling, A.: Efﬁcient reconstruction of nonrigid shape and motion from real-time 3d
scanner data. ACM TOG 28(2) (May 2009) 15:1–15:15

23. S¨ußmuth, J., Winter, M., Greiner, G.: Reconstructing animated meshes from time-varying

point clouds. In: SGP. SGP ’08 (2008) 1469–1476

24. Tevs, A., Berner, A., Wand, M., Ihrke, I., Bokeloh, M., Kerber, J., Seidel, H.P.: Animation
cartography&mdash;intrinsic reconstruction of shape and motion. ACM TOG 31(2) (April
2012) 12:1–12:15

25. Vlasic, D., Peers, P., Baran, I., Debevec, P., Popovi´c, J., Rusinkiewicz, S., Matusik, W.: Dy-
namic shape capture using multi-view photometric stereo. In: ACM SIGGRAPH Asia. SIG-
GRAPH Asia ’09 (2009) 174:1–174:11

26. Li, H., Luo, L., Vlasic, D., Peers, P., Popovi´c, J., Pauly, M., Rusinkiewicz, S.: Temporally
coherent completion of dynamic shapes. ACM TOG 31(1) (February 2012) 2:1–2:11
27. Bojsen-Hansen, M., Li, H., Wojtan, C.: Tracking surfaces with evolving topology. ACM

Transactions on Graphics (SIGGRAPH 2012) 31(4) (2012) 53:1–53:10

28. Zhang, Z.: Iterative point matching for registration of free-form curves and surfaces. IJCV

13(2) (1994) 119–152

IEEE (1991) 2724–2729

29. Chen, Y., Medioni, G.: Object modeling by registration of multiple range images. In: ICRA,

30. Johnson, A.E., Hebert, M.: Using spin images for efﬁcient object recognition in cluttered
3d scenes. Pattern Analysis and Machine Intelligence, IEEE Transactions on 21(5) (1999)
433–449

31. Gelfand, N., Mitra, N.J., Guibas, L.J., Pottmann, H.: Robust global registration. In: Sympo-

sium on geometry processing. Volume 2. (2005) 5

32. Rusu, R.B., Blodow, N., Marton, Z.C., Beetz, M.: Aligning point cloud views using persis-
tent feature histograms. In: Intelligent Robots and Systems, 2008 IEEE/RSJ International
Conference on, IEEE (2008) 3384–3391

33. Rusu, R.B., Blodow, N., Beetz, M.: Fast point feature histograms (fpfh) for 3d registration.
In: Robotics and Automation, 2009 IEEE International Conference on, IEEE (2009) 3212–
3217

34. Makadia, A., Patterson, A., Daniilidis, K.: Fully automatic registration of 3d point clouds.

In: CVPR, 2006 IEEE Conference on. Volume 1., IEEE (2006) 1297–1304

35. Horn, B.K.: Extended gaussian images. Proceedings of the IEEE 72(12) (1984) 1671–1686
36. Chung, D.H., Yun, I.D., Lee, S.U.: Registration of multiple-range views using the reverse-

calibration technique. Pattern Recognition 31(4) (1998) 457–464

Capturing Dynamic Textured Surfaces of Moving Targets

17

37. Chen, C.S., Hung, Y.P., Cheng, J.B.: Ransac-based darces: A new approach to fast automatic
registration of partially overlapping range images. Pattern Analysis and Machine Intelli-
gence, IEEE Transactions on 21(11) (1999) 1229–1234

38. Silva, L., Bellon, O.R., Boyer, K.L.: Precision range image registration using a robust surface
interpenetration measure and enhanced genetic algorithms. Pattern Analysis and Machine
Intelligence, IEEE Transactions on 27(5) (2005) 762–776

39. Yang, J., Li, H., Jia, Y.: Go-icp: Solving 3d registration efﬁciently and globally optimally. In:
Computer Vision (ICCV), 2013 IEEE International Conference on, IEEE (2013) 1457–1464
40. Mellado, N., Aiger, D., Mitra, N.J.: Super 4pcs fast global pointcloud registration via smart
indexing. In: Computer Graphics Forum. Volume 33., Wiley Online Library (2014) 205–215
41. Moezzi, S., Tai, L.C., Gerard, P.: Virtual view generation for 3d digital video. MultiMedia,

IEEE 4(1) (Jan 1997) 18–26

42. Matusik, W., Buehler, C., Raskar, R., Gortler, S.J., McMillan, L.: Image-based visual hulls.
In: Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Tech-
niques. SIGGRAPH ’00, New York, NY, USA, ACM Press/Addison-Wesley Publishing Co.
(2000) 369–374

43. Franco, J., Lapierre, M., Boyer, E.: Visual shapes of silhouette sets. In: 3D Data Processing,

Visualization, and Transmission, Third International Symposium on. (June 2006) 397–404

44. Ahmed, N., Theobalt, C., Dobrev, P., Seidel, H.P., Thrun, S.: Robust fusion of dynamic
shape and normal capture for high-quality reconstruction of time-varying geometry. In: IEEE
CVPR. (June 2008) 1–8

45. Starck, J., Hilton, A.: Surface capture for performance-based animation.

IEEE Comput.

Graph. Appl. 27(3) (May 2007) 21–31

46. Wu, C., Varanasi, K., Liu, Y., Seidel, H.P., Theobalt, C.: Shading-based dynamic shape
reﬁnement from multi-view video under general illumination, IEEE (November 2011) 1108–
1115

47. Hern´andez, C., Schmitt, F., Cipolla, R.: Silhouette coherence for camera calibration under
circular motion. Pattern Analysis and Machine Intelligence, IEEE Transactions on 29(2)
(2007) 343–349

48. Wei, L., Huang, Q., Ceylan, D., Vouga, E., Li, H.: Dense human body correspondences using

convolutional networks. In: IEEE CVPR, IEEE (2016)

49. Kennedy, J.: Particle swarm optimization. In: Encyclopedia of Machine Learning. Springer

(2010) 760–766

sional, Inc. (1992) 124–132

50. Shoemake, K.: Uniform random rotations. In: Graphics Gems III, Academic Press Profes-

51. Shilane, P., Min, P., Kazhdan, M., Funkhouser, T.: The princeton shape benchmark.

In:

Shape modeling applications, 2004. Proceedings, IEEE (2004) 167–178

52. Rusinkiewicz, S., Levoy, M.: Efﬁcient variants of the icp algorithm. In: 3-D Digital Imaging

and Modeling, IEEE (2001) 145–152

53. Rusu, R.B., Cousins, S.: 3d is here: Point cloud library (pcl). In: Robotics and Automation

(ICRA), 2011 IEEE International Conference on, IEEE (2011) 1–4

54. Kazhdan, M., Bolitho, M., Hoppe, H.: Poisson surface reconstruction. In: Proceedings of

the fourth Eurographics symposium on Geometry processing. Volume 7. (2006)

55. Chuang, M., Luo, L., Brown, B.J., Rusinkiewicz, S., Kazhdan, M.: Estimating the laplace-
beltrami operator by restricting 3d functions. In: Computer Graphics Forum. Volume 28.,
Wiley Online Library (2009) 1475–1484

56. Chuang, M., Luo, L., Brown, B.J., Rusinkiewicz, S., Kazhdan, M.: Estimating the Laplace-
Beltrami operator by restricting 3D functions. Symposium on Geometry Processing (July
2009)

57. Zhou, Q.Y., Koltun, V.: Color map optimization for 3d reconstruction with consumer depth

cameras. In: ACM SIGGRAPH. Volume 33., ACM (July 2014) 155:1–155:10

Capturing Dynamic Textured Surfaces
of Moving Targets
Supplementary Material

Supplemental to Section 4.2 – Particle Update Methods

This section discusses in detail the update methods for guide particles and regular par-
ticles during the particle swam optimization.

i , tk
Guide Particle Update. Here we describe how we update guide particle T k
i )
(i is the particle index and k is the current frame number). We parameterize the tangent
i + v(cid:1),
space of SE(3) at T k
where [u]× is the cross-product matrix [u]×w = u × w. For any ﬁxed m, and partial
scans P1, P2, we can separate exp(−m)P1 and exp(m)P2 into regions Oj, F j, Bj, j ∈
{1, 2}, as described in the main text. We then have

i by m = [u, v] ∈ R3⊕R3 with exp(m) = (cid:0)exp([u]×)Rk

i = (Rk

i , tk

E(exp(m)) = d (exp(−m)P1, P2) + d (exp(m)P2, P1)

dF (x, P2) +

dB(x, P2)

(cid:88)

x∈B1
(cid:88)

dF (x, P1) +

dB(x, P1)

x∈F 2
(cid:88)

x∈B2
(cid:107)x − I2(x)(cid:107)2 +

(cid:88)

=

+

=

=

(cid:88)

x∈F 1
(cid:88)

x∈F 1
(cid:88)

x∈F 2

(cid:107)x − I1(x)(cid:107)2 +

min
y∈P2

min
y∈P1

x∈B1
(cid:88)

x∈B2

(cid:13)
(cid:13)Pc2

v

x − Pc2

v

y(cid:13)
2
(cid:13)

(cid:13)
(cid:13)Pc1

v

x − Pc1

v

y(cid:13)
2
(cid:13)

.

Minimizing E(exp(m)) with respect to m is then a non-linear least squares prob-
lem, which we use Levenberg-Marquardt. We begin with initial guess m = (0, 0) and
iteratively apply the quasi-Newton update

where ∆m is obtained from solving the linear system

m ⇐ m + ∆m,

(Jr

T Jr + λI)∆m = −Jr

T r.

(1)

(2)

r is a stacked column vector such that E(exp(m)) = rT r, and Jr is the Jacobian
matrix of r calculated using chain rule. The damping factor λ is set as 0.1 throughout
all experiments. After m converges, we set T k+1

= exp(m).

i

Capturing Dynamic Textured Surfaces of Moving Targets

19

j , tk
j as p = [q, tk

Regular Particle Update. Here we describe how we update regular (non-guide) par-
j = (Rk
ticle T k
j ) (j is the particle index and k is the current frame number). We
j ] ∈ R3 ⊕ R3, where q is the imaginary part of the quater-
parameterize T k
nion representation of Rk
j . Up to the sign of the real part, which is assumed positive,
q determines a unique unit quaternion representing the rotation Rk
j . p is updated in a
traditional PSO fashion,

p ⇐ p + ωpv + ωb(b − p) + ωg(g − p),

(3)

where v is the velocity of previous iteration, b is the best location particle p has
been at, and g is the best particle location within radius θr. Please refer to [49] for
more details. The ﬁxed weights ωp, ωb and ωg are set as 0.2, 0.3 and 0.3 throughout all
experiments. After update, the boundary condition ((cid:107)q(cid:107) ≤ 1) is checked and enforced
by normalization if violated.

Supplemental to Section 3 – Surface Reconstruction Algorithm

This section summarizes the surface reconstruction method. After globally registering
partial scans of each frame, we perform Poisson surface reconstruction [54] to fuse three
or four partial scans Si,j (i and j denote the frame and the sensor number respectively),
and we obtain a sequence of complete, watertight surfaces W1, W2, . . . , WM . To re-
duce ﬂickering artifacts and to ﬁll holes, we adopt the shape completion pipeline of Li
et al [26] to warp partial scans from temporally-proximate frames to the current frame
geometry. For Wi, we warp Wi−1 and Wi+1 to align with Wi using a mesh deforma-
tion model based on pairwise correspondences and Laplacian coordinates. We further
combine them all using Poisson surface reconstruction with the following weights: 10
for the reconstructed mesh of the current frame and the warped neighboring frames, 2
for the hole-ﬁlled regions of the current frame, and 1 for the hole-ﬁlled regions of the
warped neighboring frames. This imposes a mild temporal ﬁlter on the reconstructed
surfaces, and a strong ﬁlter on the hole-ﬁlled regions. This step reduces the temporal
ﬂicker, and propagates some of the reconstructed surface detail from the neighboring
frames onto the current frame (this stems from the neighboring reconstructed mesh
weight being larger than any hole-ﬁlled region weight). Please refer to [26] for more
details.

Supplemental to Section 3 – Texture Reconstruction Algorithm

This section explains in detail the texture reconstruction step based on dense corre-
spondences. After the surface reconstruction step, we ﬁrst perform texture reconstruc-
tion [56] to obtain texture for Wi, by fusing and interpolating the texture from partial
scans Si,j (i and j denote the frame and the sensor number respectively). However, each
surface contains regions where this texture is unreliable, either because the region had
poor coverage in the partial scans, or is located near the seam between two partial scans
where the texture is inconsistent due to sensor noise and variations in lighting. When

20

R. Wang et al.

capturing clothed humans using three sensors, we observe that roughly 10–20% of the
texture on each surface is unreliable.

The recent work of Zhou et al [57] presents impressive results on texturing scanned
data. This method, however, assumes that the captured scene is static and thus is not ap-
plicable in our setting. Tracking methods like optical ﬂow can be used to transfer texture
between consecutive surfaces in our capture sequence, but we found them to be too frag-
ile for our purposes: they fail if the deformation between frames is either too large (so
that tracking fails) or too small (so that holes in coverage persist over large numbers of
frames). Instead we replace unreliable texture on each surface Wi by computing dense
correspondences between Wi and other surfaces in the sequence (including temporally
distant frames), and transferring texture from surfaces whose texture at the correspond-
ing point is reliable. With this approach we can reconstruct reliable texture even in the
presence of large geometry or topology changes over time.

Reliability Weight. We ﬁrst need a measure wp ∈ [0, 1] of how reliable the recon-
structed texture is at each point p of each surface Wi. Intuitively, texture is most reliable
at points that directly face the camera; therefore for partial scans Si,j where p is visible,
we set

wp = max (0, −np · cv)

where np is the surface normal at p and cv is the view direction of the sensor that
captured Si,j. If p is visible in multiple partial scans, we take the maximum weight, and
if it is visible in none, we set wp = 0. Furthermore we feather the weights of points that
lie close to the boundaries of any partial scans, as texture at the seams tends to also be
unreliable.

Computing Correspondences. We adopt the method of Wei et al [48] to predict a pose-
invariant descriptor for every vertex of each Wi. The network of Wei et al is trained on
a large dataset of captured and artiﬁcial human depth images, and can reliably compute
a 16-dimensional unit length descriptor for every vertex, where nearby points in feature
space are nearly-corresponding on the surfaces.

Texture Transfer. We declare all points with wp < (cid:15) unreliable and all others reliable.
We set (cid:15) = 0.3 throughout all experiments. We compute descriptors for all reliable
points (across all frames) and place them in a KD-tree; for each unreliable point p, we
compute its 50 nearest neighbors (in feature space) among reliable points, and take as
the color of p the weighted average of those neighbors, with each neighbor q weighted
by its distance from p in feature space and by wq.

Supplemental to Section 5 – Qualitative Registration Results

Fig 1 below extends ﬁgure 7 in the main text, and shows more global registration results.

Supplemental to Section 6 – List of Captured Sequence

This section lists statistics for all captured sequences in Table 1.

Capturing Dynamic Textured Surfaces of Moving Targets

21

Fig. 1. Example registration results of range images with limited overlap. First two rows display
range image from the Stanford 3D Scanning Repository while the last four rows exhibit data from
the Princeton Shape Benchmark.

Supplemental to Section 6 – Limitations of Capture System

This section covers limitations of the proposed capture system. The global registration
fails when there is barely no overlap, i. e. , below 5%, potentially caused by two neigh-
boring sensors drifting apart. Our method fails to capture fast motion, e. g. , jumping,
due to minor asynchronization across different sensors (Figure 2). Because of the sparse
views, there can be potentially consistent occluded regions, for which the texture can-
not be accurately recovered from other frames (Figure 2). Finally, in large occluded
regions, Poisson reconstruction might ﬁll in missing surface data with geometry far
from the ground truth human shape. In the future we wish to repair these regions by
propagating details using a similar approach to how we ﬁx the texture.

22

R. Wang et al.

Table 1. List of all captured sequences.

Sequence

Sensor

Walking 1 Kinect One
Jumping Kinect One
Kicking Kinect One
Tai Chi Kinect One
Swimming Kinect One
Walking 2 Kinect One
Kinect One
Structure IO

Dog 1
Dog 2

Sensor
Count
3
3
3
4
4
4
4
4

Frame
Count
250
209
198
491
370
201
441
300

Av. Vertex
Count
250,000
270,000
260,000
128,000
115,000
160,000
150,000
145,000

Fig. 2. Left: Registration failure of frames with fast motion due to minor asynchronization across
different sensors. Right: Failed texture reconstruction on consistently occluded regions.

