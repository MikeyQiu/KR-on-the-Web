Multi-task Learning over Graph Structures

Pengfei Liu†‡, Jie Fu‡∗, Yue Dong‡(cid:93)∗, Xipeng Qiu†, Jackie Chi Kit Cheung‡(cid:93)
†School of Computer Science, Fudan University, Shanghai Insitute of Intelligent Electroics & Systems
& ‡MILA & (cid:93)McGill University
{pﬂiu14,xpqiu}@fudan.edu.cn, jie.fu@polymtl.ca,yue.dong2@mail.mcgill.ca,jcheung@cs.mcgill.ca

8
1
0
2
 
v
o
N
 
6
2
 
 
]
L
C
.
s
c
[
 
 
1
v
1
1
2
0
1
.
1
1
8
1
:
v
i
X
r
a

Abstract

We present two architectures for multi-task learning with neu-
ral sequence models. Our approach allows the relationships
between different tasks to be learned dynamically, rather than
using an ad-hoc pre-deﬁned structure as in previous work. We
adopt the idea from message-passing graph neural networks,
and propose a general graph multi-task learning framework
in which different tasks can communicate with each other in
an effective and interpretable way. We conduct extensive ex-
periments in text classiﬁcation and sequence labelling to eval-
uate our approach on multi-task learning and transfer learn-
ing. The empirical results show that our models not only out-
perform competitive baselines, but also learn interpretable
and transferable patterns across tasks.

Introduction

Neural multi-task learning models have driven state-of-the-
art results to new levels in a number of language process-
ing tasks, ranging from part-of-speech (POS) tagging (Yang,
Salakhutdinov, and Cohen, 2016; Søgaard and Goldberg,
2016), parsing (Peng, Thomson, and Smith, 2017; Guo et
al., 2016), text classiﬁcation (Liu, Qiu, and Huang, 2016,
2017) to machine translation (Luong et al., 2015; Firat, Cho,
and Bengio, 2016).

Multi-task learning utilizes the correlation between re-
lated tasks to improve the performance of each task. In prac-
tice, existing work often models task relatedness by simply
deﬁning shared common parameters over some pre-deﬁned
task structures. Figure 1-(a,b) shows two typical pre-deﬁned
topology structures which have been popular. A ﬂat struc-
ture (Collobert and Weston, 2008) assumes that all tasks
jointly share a hidden space, while a hierarchical struc-
ture (Søgaard and Goldberg, 2016; Hashimoto et al., 2017)
speciﬁes a partial order of the direction of information ﬂow
between tasks.

There are two major limitations to the above approaches.
First, static pre-deﬁned structures represent a strong assump-
tion about the nature of the interaction between tasks, re-
stricting the model’s capacity to make use of shared infor-
mation. For example, the structure in 1(a) does not allow

∗These two authors contributed equally

Copyright c(cid:13) 2019, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Figure 1: Different topology structures for multi-task learn-
ing: ﬂat, hierarchical and graph structures. Each blue circle
represents a task (A, B, C, D), while the red box denotes
a virtual node, which stores shared information and facili-
tates communication. The boldness of the line indicates the
strength of the relationships. The directed edges deﬁne the
direction of information ﬂow. For example, in sub-ﬁgure (b),
task C receives information from task A, but not vice versa.

the model to explicitly learn the strength of relatedness be-
tween tasks. This restriction prevents the model from fully
utilizing and handling the complexity of the data (Li and
Tian, 2015). Note that the strength of relatedness between
tasks is itself not static but subject to change, depending on
the data samples at hand. Second, these models are not in-
terpretable to researchers and system developers, meaning
that we learn little about what kinds of patterns have been
shared besides the parameters themselves. Previous non-
neural-network models (Bakker and Heskes, 2003; Kim and
Xing, 2010; Chen et al., 2010) have demonstrated the im-
portance of learning inter-task relationships for multi-task
learning. However, there is little work giving an in-depth
analysis in the neural setting.

The above issues motivate the following research ques-
tions: 1) How can we explicitly model complex relationships
between different tasks? 2) Can we design models that learn
interpretable shared structures?

To address these questions, we propose to model the rela-

tionships between language processing tasks over a graph
structure, in which each task is regarded as a node. We
take inspiration from the idea of message passing (Berend-
sen, van der Spoel, and van Drunen, 1995; Serlet, Boynton,
and Tevanian, 1996; Gilmer et al., 2017; Kipf and Welling,
2016), designing two methods for communication between
tasks, in which messages can be passed between any two
nodes in a direct (Complete-graph in Figure 1-(c)) or an
indirect way (Star-graph in Figure 1-(d)). Importantly, the
strength of the relatedness is learned dynamically, rather
than being pre-speciﬁed, which allows tasks to selectively
share information when needed.

We evaluate our proposed models on two types of se-
quence learning tasks, text classiﬁcation and sequence tag-
ging, both well-studied NLP tasks (Li and Zong, 2008; Liu,
Qiu, and Huang, 2017; Yang, Salakhutdinov, and Cohen,
2016). Moreover, we conduct experiments in both the multi-
task setting and in the transfer learning setting to demon-
strate that the shared knowledge learned by our models can
be useful for new tasks. Our experimental results not only
show the effectiveness of our methods in terms of reduced
error rates, but also provide good interpretablility of the
shared knowledge.

The contributions of this paper can be summarized as fol-

lows:
1. We explore the problem of learning the relationship be-
tween multiple tasks and formulate this problem as mes-
sage passing over a graph neural network.

2. We present a state-of-the-art approach that allows multi-
ple tasks to communicate dynamically rather than follow-
ing a pre-deﬁned structure.

3. Different from traditional black-box learned models, this
paper makes a step towards learning transferable and in-
terpretable representations, which enables us to know
what types of patterns are shared.

Message Passing Framework for Multi-task
Communication
We propose to use graph neural networks with mes-
sage passing to deal with the problem of multi-task se-
quence learning. Two well-studied sequence learning tasks,
text classiﬁcation and sequence tagging, are used in our
experiments. We denote the text sequence as X =
{x1, x2, . . . , xT } and the output as Y . In text classiﬁca-
tion, Y is a single label; whereas in sequence labelling,
Y = {y1, y2, . . . , yT } is a sequence.

Assuming that there are K related tasks, we refer to Dk

as a dataset with Nk samples for task k. Speciﬁcally,

Dk = {(X (k)

i

, Y (k)
i

)}Nk

i=1,

(1)

i

and Y (k)

where X (k)
denote a sentence and a correspond-
ing label sequence for task k, respectively. The goal is to
learn a neural network to estimate the conditional probabil-
ity P (Y |X).

i

Generally, when combining multi-task learning with se-
quence learning, two kinds of interactions should be mod-
elled: the ﬁrst is the interactions between different words

within a sentence, and the other is the interactions across
different tasks.

For the ﬁrst type of interaction (interaction of words
within a sentence), many models have been proposed by
applying a composition function in order to obtain rep-
resentation of the sentence. Typical choices for deﬁning
the composition function include recurrent neural networks
(Hochreiter and Schmidhuber, 1997), convolutional neural
networks (Kalchbrenner, Grefenstette, and Blunsom, 2014),
and tree-structured neural networks (Tai, Socher, and Man-
ning, 2015). In this paper, we adopt the LSTM architecture
to learn the dependencies within a sentence, due to their im-
pressive performance on many NLP tasks (Cheng, Dong,
and Lapata, 2016). Formally, we refer to ht as the hidden
state of the word at time t, wt. Then, ht can be computed as:

ht = LSTM(xt, ht−1, θ).

(2)

Here, the θ represents all the parameters of LSTM.

For the second type of interaction (interaction across dif-
ferent tasks), we propose to conceptualize tasks and their
interactions as a graph, and utilize message passing mech-
anisms to allow them to communicate. Our framework is
inspired by the idea of message passing, which is used ubiq-
uitously in modern computer software (Berendsen, van der
Spoel, and van Drunen, 1995) and programming languages
(Serlet, Boynton, and Tevanian, 1996). The general idea of
this framework is that we provide a graph network that al-
lows different tasks to cooperate and interact with one an-
other. Below, we describe our conceptualization of the graph
construction process, then we describe the message passing
mechanism used for inter-task communication.

Formally, a graph G can be deﬁned as an ordered pair
G = (V, E), where V is a set of nodes {V1, . . . , Vm} and
E is a set of edges. In this work, we use directed graphs
to model the communication ﬂows between tasks and an
edge is therefore deﬁned as an ordered set of two nodes
(Vi, Vj), i (cid:54)= j.

In our models, we represent each task as a node. In addi-
tion, we allow virtual nodes to be introduced. These virtual
nodes do not correspond to a task. Rather, their purpose is to
facilitate communication among different tasks. Intuitively,
the virtual node functions as a mailbox, storing messages
from other nodes and distributing them as needed.

Tasks and virtual nodes are connected by weighted edges,
which represent communication between different nodes.
Previous ﬂat and hierarchical architectures for multi-task
learning can be considered as graphs with ﬁxed edge con-
nections. Our models dynamically learn the weight of each
edge, which allows the models to adjust the strength of the
communication signals.

Message Passing

In our graph structures, we use directed edges to model the
communication between tasks. In other words, nodes com-
municate with each other by sending and receiving mes-
sages over edges. Given a sentence with a sequence of words
(wk
t to represent the aggre-
gated messages that the word of task k at time step t can

T ) from task k, we use rk

1 , ..., wk

(a) Complete-graph MTL

(b) Star-graph MTL

Figure 2: Two frameworks for multi-task communication. (a) shows how task A and B send information to task C. Here, we
only present the information ﬂow relating to task C and omit some edges for easy understanding. (b) shows how the mailbox
(shared layer) sends information to task C. The α and β values correspond to the edge weights. α controls the strength of the
association from a word in a source task to a word in the target task while β controls the amount of information we should
take from the shared layer. For example, βC
32 can be understood as: to compute the hidden state h3 of task C, the amount of
information should be taken from the second hidden state of shared layers is βC
32.

get, and we use hk
resentation of the word wk
t .

t to denote the task-dependent hidden rep-

Below, we propose two basic communication architec-
tures for message passing: Complete-graph and Star-graph
which differ according to whether they allow direct commu-
nication between any pair of tasks, or whether the commu-
nication is mediated by an intermediate virtual node.

1. Complete-graph (CG): Direct Communication for
Multi-Task Learning: in this model, each node can directly
send (or receive) messages to (or from) any other nodes.
Speciﬁcally, as shown in Fig.2-(a), we ﬁrst assign each task a
task-dependent LSTM layer. Each sentence in task k can be
passed to all the other task-dependent LSTMs1 to get corre-
sponding representations h(i)
, i = 1...K, i (cid:54)= k. Then, these
t
messages will be aggregated as:

r(k)
t =

(cid:88)

α(i→k)
t

h(i)
t

i=1...Ks.t.i(cid:54)=k

t

Here, α(ki)
is a scalar, which controls the relatedness be-
tween two tasks k and i, and can be dynamically computed
as:

s(i→k)
t

t

t−1, h(i)
t )

= f (x(k)
, h(k)
= u(s) tanh(W(s)[xt, h(k)

t−1, h(i)

t

])

where u(s) and W(s) are learnable parameters. And the re-
latedness scores will be normalized into a probability distri-
bution:

αt = softmax(st)

2. Star-graph (SG): Indirect Communciation for
Multi-Task Learning: the potential limitation of our pro-
posed CG-MTL model lies in its computational cost, be-
cause the number of pairwise interactions grows quadrat-
ically with the number of tasks. Inspired by the mailbox
idea used in the traditional message passing paradigm (Net-
zer and Miller, 1995), we introduce an extra virtual node

1At training time, the loss is only calculated and used to com-

pute the gradient for the task from which the sentence is drawn.

(3)

(4)

(5)

(6)

into our graph to address this problem. In this setting, mes-
sages are not sent directly from one node to another, but
are bridged by the virtual node. Intuitively, the virtual node
stores the shared messages across all the tasks; different
tasks can put messages into this global space, then other
tasks can take out the useful messages for themselves from
the same space. Fig.2-(b) shows how one task collects infor-
mation from the mailbox (shared layer).

In details, we introduce an extra LSTM to act as the vir-
tual node, whose parameters are shared across tasks. Given
a sentence from task k, its information can be written into
the shared LSTM by the following operation:
t = LSTM(x(k)
h(s)
t−1, θ(s)),

, h(s)

(7)

t

where θ(s) denotes the parameters are shared across all the
tasks.

Then, the aggregated messages at time t can be read from

the shared LSTM:

r(k)
t =

t→ih(s)
β(k)

i

,

T
(cid:88)

i=1

(8)

where T denotes the length of the sentence and β(k)
t→i is used
to retrieval useful messages from the shared LSTM, which
can be computed in a similar fashion to Equations 5 and 6.

t

Once the graphs and message passing between the nodes
are deﬁned, the next question to ask is how to update the
task-dependent representation h(k)
for node k using the cur-
rent input information xt and the aggregated messages r(k)
.
We employ a gating unit that allows the model to decide
how many aggregated messages should be used for the target
tasks, which avoids unnecessary information redundancy.
Formally, the h(k)
can be computed as:
t−1, r(k)
t = LSTM†(xt, h(k)
h(k)

(9)
The function LSTM† is the same as Eq.2 except that we
replace the memory update step of the inner function in Eq.2
with the following equation:

, θ(k), θ(s)).

t

t

t

ht = ot (cid:12) tanh(ct + gt (cid:12) (W(s)

f rt)),

(10)

where W(s)
is a parameter matrix, and gt is a fusion gate
f
that selects the aggregated messages. gt is computed as fol-
lows:

gt = σ(W(s)

r r(k)

t + W(s)

c ct),

(11)

where W(s)
r

and W(s)

c

are parameter matrices.

Comparison of Complete-graph (CG) and Star-graph
(SG) For CG-MTL, the advantage is that we can ﬁgure out
the strength of the association from a word in a source task to
a word in the target task. However, the computation of CG-
MTL is not efﬁcient if the number of tasks is too large. For
SG-MTL, the advantage is that the learned shared structures
are interpretable and more importantly, the learned knowl-
edge of SG-MTL can be used for unseen tasks. To conclude,
the CG-MTL can be used in these scenarios: 1) The number
of tasks is not too large; 2) We need to explicitly analyze the
relatedness between different tasks (as shown in Fig.3). By
contrast, the SG-MTL can be used in the following scenar-
ios: 1) The number of tasks is large; 2) We need to transfer
shared knowledge to new tasks; 3) We need to analyze what
types of shared patterns have been learned by the model (As
shown in Tab.2).

Task-dependent Layers
Given a sentence X from task k with its label Y (note Y is
either a classiﬁcation label or sequential labels) and its fea-
ture vector h(k) emitted by the two communication methods
above, we can adapt our models to different tasks by using
different task-speciﬁc layers. We call the task-speciﬁc layer
as the OUTPUT-LAYER. For text classiﬁcation tasks, the
commonly used OUTPUT-LAYER is a softmax layer, while
for sequence labelling tasks, it can be a conditional random
ﬁeld (CRF) layer. Finally, the output probability P (Y |X)
can be computed as:

P (Y |X) = OUTPUT-LAYER(X, h(k)

T , θ(k)).

(12)

Then, we can maximize the above probability to optimize

the parameters of each task:

Lk(X, Y, θ(k), θ(s)) = −P (Y |X).
Once the loss function for a speciﬁc task is deﬁned, we
could compute the overall loss for our multi-task models as
the following:

(13)

L =

λkLk(X, Y, θ(k), θ(s))

(14)

K
(cid:88)

k=1

where λk is the weight for task k. The parameters of the
whole network are trained over all datasets and the overall
training procedure is presented in Algorithm 1.

Experiments and Results
In this section, we describe our hyperparameter settings and
present the empirical performance of our proposed models
on two types of multi-task learning datasets, ﬁrst on text
classiﬁcation, then on sequence tagging. Each dataset con-
tains several related tasks.

Algorithm 1 Training Process for Multi-task Learning over
Graph Structures
Require: A set of training tasks {DTi }K

i=1, where Ti is drawn

Sample a batch of samples BTk = {X, Y } ∈ DTk
// Message Passing
if Direct-Communication then

from p(T )

for Tj ∼ p(T ) do
h(j→k)

for Tk ∼ p(T ) do

t
end for
t = (cid:80)
r(k)

1: Initialize Θ := {θ(s), θ(k)}
2: while not done do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
end for
19:
20: end while

= LSTM(BTk , ht−1, θ(k))

(cid:46) Tk (cid:54)= Tj

j α(j→k)

t

h(j)
t

(cid:46) Aggregating

else if Indirect-Communication then

h(s)
t = LSTM(BTk , h(s)
t = (cid:80)T
i=1 β(k)
r(k)

i→th(s)

i

t−1, θ(s))

end if
// Node Updating
h(k)
t = LSTM†(xt, h(k)
// Task-dependent Output
P (Y |X) = OUTPUT-LAYER(X, h(k)

t−1, r(k)

t

, θ(k), θ(s))

T , θ(k))

(cid:46) Aggregating

Hyperparameters
The word embeddings for all of the models are initialized
with the 200-dimensional GloVe vectors (840B token ver-
sion (Pennington, Socher, and Manning, 2014)). The other
parameters are initialized by randomly sampling from the
uniform distribution of [−0.1, 0.1]. The mini-batch size is
set to 8.

For each task, we take the hyperparameters which achieve
the best performance on the development set via a grid
search over combinations of the hidden size [100, 200, 300]
and l2 regularization [0.0, 5E − 5, 1E − 5]. Additionally,
for text classiﬁcation tasks, we set an equal lambda for each
task; while for tagging tasks, we run a grid search of lambda
in the range of [1, 0.8, 0.5] and take the hyperparameters
which achieve the best performance on the development set.
Based on the validation performance, we choose the size of
hidden state as 200 and l2 as 0.0. We apply stochastic gradi-
ent descent with the diagonal variant of AdaDelta for opti-
mization (Zeiler, 2012).

Text Classiﬁcation
To investigate the effectiveness of multi-task learning, we
experimented with 16 different text classiﬁcation tasks in-
volving different popular review corpora, such as books, ap-
parel and movie (Liu, Qiu, and Huang, 2017). Each sub-task
aims at predicting a correct sentiment label (positive or neg-
ative) for a given sentence. All the datasets in each task are
partitioned into training, validating, and testing with the pro-
portions of 1400, 200 and 400 samples respectively.

We choose several relevant and representative models as

baselines.
• MT-CNN: This model is proposed by Collobert and We-
ston (2008) with a convolutional layer, in which lookup-

Task

Single Task

Multiple Tasks

Transfer

Avg.

MT-CNN

FS-MTL

SP-MTL

CG-MTL*

SG-MTL*

SP-MTL

SG-MTL*

Books
Electronics
DVD
Kitchen
Apparel
Camera
Health
Music
Toys
Video
Baby
Magazines
Software
Sports
IMDB
MR

AVG

19.2
21.4
19.9
20.1
15.7
14.6
17.8
23.0
16.3
17.0
15.9
10.5
14.7
17.3
17.3
26.9

18.0

15.5(−3.7)
16.8(−4.6)
16.0(−3.9)
16.8(−3.3)
16.3(+0.6)
14.0(−0.6)
12.8(−5.0)
16.3(−6.7)
10.8(−5.5)
18.5(+1.5)
12.3(−3.6)
12.3(+1.8)
13.5(−1.2)
16.0(−1.3)
13.8(−3.5)
25.5(−1.4)

17.5(−1.7)
14.3(−7.1)
16.5(−3.4)
14.0(−6.1)
15.5(−0.2)
13.5(−1.1)
12.0(−5.8)
18.8(−4.2)
15.5(−0.8)
16.3(−0.7)
12.0(−3.9)
7.5(−3.0)
13.8(−0.9)
14.5(−2.8)
17.5(+0.2)
25.3(−1.6)

16.0(−3.2)
13.2(−8.2)
14.5(−5.4)
13.8(−6.3)
13.0(−2.7)
10.8(−3.8)
11.8(−6.0)
17.5(−5.5)
12.0(−4.3)
15.5(−1.5)
11.8(−4.1)
7.8(−2.7)
12.8(−1.9)
14.3(−3.0)
14.5(−2.8)
23.3(−3.6)

13.3(−5.9)
11.5(−9.9)
13.5(−6.4)
12.3(−7.8)
13.0(−2.7)
10.5(−4.1)
10.5(−7.3)
14.8(−8.2)
11.0(−5.3)
13.0(−4.0)
10.8(−5.1)
8.0(−2.5)
10.3(−4.4)
12.3(−5.0)
13.0(−4.3)
21.5(−5.4)

13.8(−5.4)
11.5(−9.9)
12.0(−7.9)
11.8(−8.3)
12.5(−3.2)
11.0(−3.6)
10.5(−7.3)
14.3(−8.7)
10.8(−5.5)
14.0(−3.0)
11.3(−4.6)
7.8(−2.7)
12.8(−1.9)
13.3(−4.0)
13.5(−3.8)
22.0(−4.9)

16.3(−2.9)
16.8(−4.6)
14.3(−5.6)
15.0(−5.1)
13.8(−1.9)
10.3(−4.3)
13.5(−4.3)
18.3(−4.7)
11.8(−4.5)
14.8(−2.2)
12.0(−3.9)
9.5(−1.0)
11.8(−2.9)
13.5(−3.8)
13.3(−4.0)
23.5(−3.4)

14.5(−4.7)
13.8(−7.6)
14.0(−5.9)
12.8(−7.3)
13.5(−2.2)
11.0(−3.6)
11.0(−6.8)
14.8(−8.2)
10.8(−5.5)
13.5(−3.5)
11.5(−4.4)
8.8(−1.7)
11.0(−3.7)
12.8(−4.5)
13.3(−4.0)
22.8(−4.1)

15.5(−2.5)

15.3(−2.7)

13.9(−4.1)

12.5(−5.5)

12.7(−5.3)

14.3(−3.7)

13.1(−4.9)

Table 1: Text classiﬁcation error rates of our models on 16 datasets against typical baselines. The smaller values indicate
better performances. The column of Single Task (Avg.) gives the average error rates of vanilla LSTM, bidirectional LSTM,
and stacked LSTM while the column of Multiple Tasks shows the results achieved by corresponding multi-task models. The
Transfer column lists the results of different models on transfer learning. “*” indicates our proposed models. The numbers in
brackets represent the improvements relative to the average performance (Avg.) of three single task baselines.

tables are shared partially while other layers are task-
speciﬁc.

• FS-MTL: Fully shared multi-task learning framework.

Different tasks fully share a neural layer (LSTM).

• SP-MTL: Shared-private multi-task learning framework
with adversarial learning (Liu, Qiu, and Huang, 2017).
Different tasks not only have common layers to share in-
formation, but have their own private layers.

Results on Multi-task Learning: The experimental re-
sults show that our proposed models outperform all single-
task baselines by a large margin, and here we show the av-
eraged error due to the following reasons: 1) it is easier to
show the performance gain of multi-task learning models
over single task models. 2) BiLSTM and stacked LSTM are
also the necessary baselines for SG-MTL, since the combi-
nation of shared and private layers in SG-MTL is similar to
two-layer LSTM.

Table 1 shows the overall results on the 16 different tasks
under three settings: single task, multiple task, and transfer
learning. Generally, we can see that almost all tasks bene-
ﬁt from multi-task learning, which boosts the performance
by a large margin. Speciﬁcally, CG-MTL achieves the best
performance, surpassing SP-MTL by 1.4%, which suggests
that explicit communication makes it easier to shared infor-
mation. Although the improvement of SG-MTL is not as
large as CG-MTL, SG-MTL is efﬁcient to train. Addition-
ally, the comparison between SG-MTL and SP-MTL shows
the effectiveness of selectively sharing schema. Moreover,
we may further improve our models by incorporating the
adversarial training mechanism introduced in SP-MTL, as it
is an orthogonal innovation to our methods.

Evaluation on Transfer Learning: We next present the

potential of our methods on transfer learning, as we expect
that the shared knowledge learned by our model architec-
tures can be useful for new tasks. In particular, the virtual
node in the SG-MTL model can condense shared informa-
tion into a common space after multi-task learning, which
allows us to transfer this knowledge to new tasks. In order
to test the transferability of the shared knowledge learned
by SG-MTL, we design an experiment following the super-
vised pre-training paradigm. Speciﬁcally, we adopt a 16-fold
“leave-one-task-out” paradigm; we take turns choosing 15
tasks to train our model via multi-task learning, then the
learned shared layer is transferred to a second network that is
used to test on the remaining target task k. The parameters
of the transferred layer are kept frozen, and the remaining
parameters of the new network are randomly initialized.

Table 1 shows these results in the “Transfer” column,
in which the task in each row is regarded as the target task.
We observe that our model achieves a 4.9% average im-
provement in terms of the error rate over the single tasking
setting (13.1 vs. 18.0), surpassing SP-MTL by 1.2% in av-
erage (13.1 vs. 14.3). This improvement suggests that our
retrieval method with the selective mechanism (the attention
layer in eq. 8) is more efﬁcient in ﬁnding the relevant infor-
mation from the shared space compared to SP-MTL, which
reads the shared information without any selective mecha-
nism and ignores the relationship between tasks.

Sequence Tagging
In this section, we present the results of our models on
the second task of sequence tagging. We conducted exper-
iments by following the same settings as Yang, Salakhut-
dinov, and Cohen (2016). We use the following benchmark

Figure 3: Illustrations of the most relevant tasks (top 3) for each word in “Kitchen” and “Video” tasks. Here we choose
some typical words to visualize in blue. And the orange words represent the relevant tasks.

Interpretable Sub-Structures

Explanations

Short-term

Long-term

Interpretable phrases to be
shared across tasks

Interpretable sentence patterns,
they usually determine the
sentence meanings.

Table 2: Multiple interpretable sub-structures learned by shared layers. “senti-words” refers to “boring,
interesting, amazing” etc. “adv-words” refers to “easily, fine, great” etc. “adj-words” refers to
“stable, great, fantastic” etc. These structures show that short-term and long-term dependencies between different
words can be learned, which usually control the sentiment of corresponding sentences.

Dataset

Task

Training Dev.

Test

Model

CoNLL2000 CoNLL2003

PTB

CoNLL 2000 Chunking 211,727

-

47,377

CoNLL 2003 NER

204,567

51,578

46,666

PTB

POS

912,344

131,768 129,654

Table 3: The sizes of the sequence labelling datasets in our
experiments, in terms of the number of tokens.

LSTM + CRF
MT-CNN
FS-MTL
SP-MTL∗

CG-MTL
SG-MTL

94.46
94.32
95.41
95.27

95.49
95.61

90.10
89.59
90.94
90.90

91.25
91.47

97.55
97.29
97.55
97.49

97.61
97.69

datasets in our experiments: Penn Treebank (PTB) POS tag-
ging, CoNLL 2000 chunking, CoNLL 2003 English NER.
The statistics of the datasets are described in Table 3.

Results and Analysis: Table 4 shows the performance of
the models on the sequence tagging tasks. CG-MTL and SG-
MTL signiﬁcantly outperform the three strong multi-task
learning baselines, Speciﬁcally, SG-MTL achieves a perfor-
mance gain of 0.53% in terms of F1 score over the best
competitor FS-MTL on the CoNLL2003 dataset, indicating
that our models are able to make use of the shared informa-
tion by modelling the relationship between different tasks.
Our models also achieve slightly better F1 scores on the
CoNLL2000 and PTB datasets when compared to the best
baseline model FS-MTL.

Table 4: Performance of different models on Chunking,
NER, and POS respectively. All the results without marks
are reported in the corresponding paper. LSTM+ CRF: Pro-
posed by (Huang, Xu, and Yu, 2015). MT-CNN: Proposed
by (Collobert and Weston, 2008). FS-MTL: Proposed by
(Yang, Salakhutdinov, and Cohen, 2016).

Discussion and Qualitative Analysis
In order to obtain more insights and detailed interpretability
of how messages are passed between tasks in our proposed
models, we design a series of experiments targeting the fol-
lowing aspects:

1. Can the relationship between different tasks be learned by

CG-MTL?

2. Are there interpretable structures that the shared layer in
SG-MTL can learn? Are these shared patterns similar to

linguistic structures, and can they be transferred for other
tasks?

Explicit Relationship Learning To answer the ﬁrst ques-
tion, we visualize the weight αt of CG-MTL in equation 3.
As each task can receive messages from any other task in
CG-MTL, αt directly indicates the relevance of other tasks
to the current task at time step t. As shown in Figure 3,
we analyze the relationships learned by our models on ran-
domly sampled sentences from different tasks. We ﬁnd that
the relationship between tasks cannot be modelled by a static
score. Rather, it depends on the speciﬁc sample and context.
Consider the example sentence in Figure 3-(a), drawn from
the KITCHEN task. Here, the words “easily” and “ads”
are inﬂuenced by different sets of external tasks, in which
those words express sentiment. For example, in the CAM-
ERA and TOYS tasks, “breaks easily” is usually used
to express negative sentiment, while the word “ads” often
appears in the MAGAZINE task to express negative senti-
ment. Figure 3-(b) shows a similar case on “quality” and
“name-brand”.

Interpretable Structure Learning To answer the second
question, we visualize β in equation 8 inside the SG-MTL
model. As different tasks can read information from shared
layers in SG-MTL, visualizing β allows us to analyze what
kinds of sentence structures are shared. Speciﬁcally, each
word w(k)
T and the
amount of messages is controlled by the scores β. To illus-
trate the interpretable structures learned by the shared layer
in SG-MTL, we randomly sample several examples from
different tasks and visualize their shared structures. Three
random sampled cases are described as in Figure 4.

can receive shared messages: w(s)
1

... w(s)

t

From the experiments we conducted in visulizing β in

SG-MTL, we observed the following:

• The proposed model can not only utilize the shared
information across different
tasks, but can tell us
what kinds of features are shared. As shown in Ta-
ble 2, the short-term and long-term dependencies be-
tween different words can be captured. For example,
the word “movie” is prone to connecting to emotional
words, such as “boring, amazing, exciting”
while “products” is more likely to make friends with
“stable, great, fantastic”.

• Comparing Figure 4-(b) and (c), we can see how
task SOFTWARE borrows useful
information from
the sentence “I would
task KITCHEN. Concretely,
have to buy the software again” in the task
“Software” has negative emotion. In this sentence, the
key pattern is “would have”, which does not appear
too much in the training set of SOFTWARE. Fortunately,
the training samples in the task KITCHEN provide more
hints about this pattern.

• As shown in Figure 4-(a) and (b), the shared layer has
learned an informative sentence pattern “would have
to ...” from the training set of task KITCHEN. This
pattern is useful for the sentiment prediction of another

Figure 4: Illustrations of the learned patterns captured by the
shared layer under different tasks. The boldness of the line
indicates the strength β of the relationship. The ﬁrst sentence
comes from the development set of task SOFTWARE while
the second one belongs to the training set of task KITCHEN.
We choose three typical words “buy”, “software” and
“machine” to visualize in these sampled sentences.

task SOFTWARE, which suggests that we can analyze the
sharabla patterns in an interpretable way for SG-MTL
model.

Related Work
Neural network-based multi-task frameworks have achieved
success on many NLP tasks, such as POS tagging (Yang,
Salakhutdinov, and Cohen, 2016; Søgaard and Goldberg,
2016), parsing (Peng, Thomson, and Smith, 2017; Guo et al.,
2016), machine translation (Dong et al., 2015; Luong et al.,
2015; Firat, Cho, and Bengio, 2016), and text classiﬁcation
(Liu, Qiu, and Huang, 2016, 2017). However, previous work
does not focus on explicitly modelling the relationships be-
tween different tasks. These models are often trained with
an opaque neural component, which makes it hard to un-
derstand what kind of knowledge is shared. By contrast, in
this paper, we propose to explicitly learn the communication
between different tasks, and learn some interpretable shared
structures.

Before the bloom of neural-based models, non-neural
multi-task learning methods have also been proposed to
model the relationships between tasks. For example, Bakker
and Heskes (2003) learn to cluster tasks by using Bayesian
approaches. Kim and Xing (2010) utilizes a given tree struc-
ture to design a regularizer, while Chen et al. (2010) learns
a structured multi-task problem over a given graph. These
models adopt complex learning strategies and introduce a
priori information between different tasks, which are usu-
ally not suitable for sequence modelling. In this paper, we
provide a new perspective on how to model the relation-
ships using distributed graph models and message passing,
which can be learned dynamically rather than following a
pre-deﬁned structure.

The technique of message passing is used ubiquitously
in computer software (Berendsen, van der Spoel, and van

Drunen, 1995) and programming languages (Serlet, Boyn-
ton, and Tevanian, 1996). Recently, there has also been
growing interest in developing graph neural networks (Kipf
and Welling, 2016) or neural message passing algorithms
(Gilmer et al., 2017) for learning representations of irregular
graph-structured data. In this paper, we formulate multi-task
learning as a communication problem over graph structures,
allowing different tasks to communicate via message pass-
ing.

More recently, Liu and Huang (2018) propose to learn
multi-task communication by explicitly passing gradients.
Both our work try to incorporate inductive bias to multi-
task learning. However, the difference is that we focus on
the structural bias while Liu and Huang (2018) introduced
an additional loss function.

Conclusion and Outlook
We have explored the problem of learning the relationships
between multiple tasks, formulating the problem as message
passing over a graph neural network. Our proposed methods
explicitly model the relationships between different tasks
and achieve improved performance in several multi-task and
transfer learning settings. We also show that we can extract
interpretable shared patterns from the outputs of our models.
From our experiments, we believe that learning interpretable
shared structures is a promising direction, which is also very
useful for knowledge transfer.

Acknowledgments
The authors wish to thank the anonymous reviewers for their
helpful comments. This work was partially funded by Na-
tional Natural Science Foundation of China (No. 61751201,
61672162), STCSM (No.16JC1420401, No.17JC1404100),
and Natural Sciences and Engineering Research Council of
Canada (NSERC).

References
Bakker, B., and Heskes, T. 2003. Task clustering and gating
for bayesian multitask learning. JMLR 4(May):83–99.
Berendsen, H. J.; van der Spoel, D.; and van Drunen, R.
1995. Gromacs: a message-passing parallel molecular dy-
namics implementation. Computer Physics Communica-
tions 91(1-3):43–56.

Chen, X.; Kim, S.; Lin, Q.; Carbonell, J. G.; and Xing, E. P.
2010. Graph-structured multi-task regression and an ef-
ﬁcient optimization method for general fused lasso. stat
1050:20.

Cheng, J.; Dong, L.; and Lapata, M. 2016. Long short-term
memory-networks for machine reading. In Proceedings
of the 2016 Conference on EMNLP, 551–561.

Collobert, R., and Weston, J. 2008. A uniﬁed architecture for
natural language processing: Deep neural networks with
multitask learning. In Proceedings of ICML.

Firat, O.; Cho, K.; and Bengio, Y. 2016. Multi-way, multi-
lingual neural machine translation with a shared attention
mechanism. In Proceedings of NAACL-HLT, 866–875.

Gilmer, J.; Schoenholz, S. S.; Riley, P. F.; Vinyals, O.; and
Dahl, G. E. 2017. Neural message passing for quantum
chemistry. In ICML, 1263–1272.

Guo, J.; Che, W.; Wang, H.; and Liu, T. 2016. Exploit-
ing multi-typed treebanks for parsing with deep multi-task
learning. arXiv preprint arXiv:1606.01161.

Hashimoto, K.; Tsuruoka, Y.; Socher, R.; et al. 2017. A joint
many-task model: Growing a neural network for multi-
ple nlp tasks. In Proceedings of the 2017 Conference on
EMNLP, 1923–1933.

Hochreiter, S., and Schmidhuber, J. 1997. Long short-term

memory. Neural computation 9(8):1735–1780.

Huang, Z.; Xu, W.; and Yu, K.

lstm-crf models for sequence tagging.
arXiv:1508.01991.

2015. Bidirectional
arXiv preprint

Kalchbrenner, N.; Grefenstette, E.; and Blunsom, P. 2014.
A convolutional neural network for modelling sentences.
In Proceedings of ACL.

Kim, S., and Xing, E. P. 2010. Tree-guided group lasso for

multi-task regression with structured sparsity.

Kipf, T. N., and Welling, M. 2016. Semi-supervised classiﬁ-
cation with graph convolutional networks. arXiv preprint
arXiv:1609.02907.

Li, Y., and Tian, X. 2015. Graph-based multi-task learning.
In Communication Technology (ICCT), 2015 IEEE 16th
International Conference on, 730–733. IEEE.

Li, S., and Zong, C. 2008. Multi-domain sentiment classiﬁ-

cation. In Proceedings of the ACL, 257–260.

Liu, P., and Huang, X. 2018. Meta-learning multi-task com-

munication. arXiv preprint arXiv:1810.09988.

Liu, P.; Qiu, X.; and Huang, X. 2016. Recurrent neural
network for text classiﬁcation with multi-task learning. In
Proceedings of IJCAI.

Liu, P.; Qiu, X.; and Huang, X. 2017. Adversarial multi-task
learning for text classiﬁcation. In Proceedings of the 55th
Annual Meeting of ACL, volume 1, 1–10.

Luong, M.-T.; Le, Q. V.; Sutskever, I.; Vinyals, O.; and
Kaiser, L. 2015. Multi-task sequence to sequence learn-
ing. arXiv preprint arXiv:1511.06114.

Netzer, R. H., and Miller, B. P. 1995. Optimal tracing and
replay for debugging message-passing parallel programs.
The Journal of Supercomputing 8(4):371–388.

Peng, H.; Thomson, S.; and Smith, N. A. 2017. Deep mul-
titask learning for semantic dependency parsing. In Pro-
ceedings of the 55th ACL, volume 1, 2037–2048.

Dong, D.; Wu, H.; He, W.; Yu, D.; and Wang, H. 2015.
Multi-task learning for multiple language translation. In
Proceedings of the ACL.

Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:
Global vectors for word representation. Proceedings of
the EMNLP 12:1532–1543.

Serlet, B.; Boynton, L.; and Tevanian, A. 1996. Method
for providing automatic and dynamic translation into op-
eration system message passing using proxy objects. US
Patent 5,481,721.

Søgaard, A., and Goldberg, Y. 2016. Deep multi-task learn-
ing with low level tasks supervised at lower layers.
In
Proceedings of ACL.

Tai, K. S.; Socher, R.; and Manning, C. D. 2015. Improved
semantic representations from tree-structured long short-
term memory networks. In Proceedings of the 53rd ACL,
1556–1566.

Yang, Z.; Salakhutdinov, R.; and Cohen, W. 2016. Multi-
task cross-lingual sequence tagging from scratch. arXiv
preprint arXiv:1603.06270.

Zeiler, M. D. 2012. Adadelta: An adaptive learning rate

method. arXiv preprint arXiv:1212.5701.

