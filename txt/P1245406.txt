9
1
0
2
 
n
u
J
 
6
2
 
 
]

G
L
.
s
c
[
 
 
1
v
3
7
9
0
1
.
6
0
9
1
:
v
i
X
r
a

Defending Adversarial Attacks by Correcting logits

Yifeng Li1, Lingxi Xie2, Ya Zhang1, Rui Zhang1, Yanfeng Wang1, Qi Tian2
1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University
2Noah’s Ark Lab, Huawei Inc.
tobylyf@live.com 198808xc@gmail.com
{ya_zhang,zhang_rui,wangyanfeng}@sjtu.edu.cn tian.qi1@huawei.com

Abstract

Generating and eliminating adversarial examples has been an intriguing topic
in the ﬁeld of deep learning. While previous research veriﬁed that adversarial
attacks are often fragile and can be defended via image-level processing, it remains
unclear how high-level features are perturbed by such attacks. We investigate
this issue from a new perspective, which purely relies on logits, the class scores
before softmax, to detect and defend adversarial attacks. Our defender is a two-
layer network trained on a mixed set of clean and perturbed logits, with the goal
being recovering the original prediction. Upon a wide range of adversarial attacks,
our simple approach shows promising results with relatively high accuracy in
defense, and the defender can transfer across attackers with similar properties.
More importantly, our defender can work in the scenarios that image data are
unavailable, and enjoys high interpretability especially at the semantic level.

1

Introduction

In the past years, with the blooming development of deep learning, researchers started focusing on
the ‘dark side’ of deep neural networks, such as the lack of interpretability. In this research ﬁeld,
an important topic is the existence of adversarial examples, which claims that slightly modifying
the input image, sometimes being imperceptible to human, can lead to a catastrophic change in the
output prediction [32, 8]. Driven by the requirements of understanding deep networks and securing
AI-based systems, there emerge a lot of efforts in generating adversarial examples to attack deep
networks [32, 8, 20, 4, 16, 5] and, in the opposite direction, designing algorithms to recognize
adversarial examples and thus eliminate their impacts [24, 17, 19, 33, 9, 35, 28, 18].

This paper focuses on the defense part. On the one hand, researchers have demonstrated that
adversarial attacks are fragile and thus their impacts can be weakened or eliminated by some pre-
processing on the input image [9, 35, 28, 18]; on the other hand, there are also efforts in revealing how
such attacks change mid-level and high-level features and thus eventually break up prediction [18, 36].
Here we make a hypothesis: the cues of recovering low-level features and high-level features from
adversarial attacks are quite different: for low-level features, it is natural to use image-space priors
such as intensity distribution and smoothness to ﬁlter out adversaries; for high-level features, however,
the cues may lie in the intrinsic connections between the semantics of different classes.

To the best of our knowledge, this is the ﬁrst work that purely focuses on defending adversarial attacks
from topmost semantics — this forces our system to ﬁnd semantic cues, which were not studied
before, for defending adversaries. To this end, we study logits, which are the class scores produced by
the last fully-connected layers — before being fed into softmax normalization. The major discovery
of this paper lies in that we can take merely logits as input to defend adversarial attacks. To
reveal this, we ﬁrst train a 1,000-way classiﬁer on the ILSVRC2012 dataset [30]. Then, we use an
off-the-shelf attacker such as PGD [19] to generate adversarial examples, in particular logits are
recorded. Finally, we train a two-layer fully-connected network to predict the original label. We mix

Preprint. Under review.

both clean and adversarial logits into the training set, so that the trained defender has the ability of
recovering attacked prediction meanwhile keeping non-attacked prediction unchanged.

Working at the semantic level, our approach enjoys two-way beneﬁts. First, we only use logits for
defense, which reduces the requirements of image data and mid-level features — this is especially
useful in the scenarios that image data are inaccessible due to security reasons, and that very low bits
of data can be transferred for defense (note that compared to low-level and mid-level features, logits
have a much lower dimension). Second and more importantly, we make it possible to explain how
adversaries are defended by directly digging into the two-layer network. An interesting phenomenon
is that almost all attackers can leave ﬁngerprints on a few ﬁxed classes, which are named supporting
classes, even if the adversarial class can vary among the entire dataset. Our defender works by
detecting these supporting classes, and whether a defender can transfer to different attacks and/or
network models can be judged by evaluating how these supporting classes overlap. By revealing
these properties, we move one step further towards understanding the mechanisms of adversaries.

2 Backgrounds

In this section, we provide background knowledge about adversarial examples and review some
representative attack and defense methods. Let x denote a clean/natural image sample, and y is the
corresponding label which has C choices. A deep neural network f (·) is deﬁned as y = f (x) : X →
.
Y. In practice, given an input x, the classiﬁer f (·) outputs a ﬁnal feature vector z
= Z(x) called
logits where each element zk corresponds to the k-th class. The logits are then fed into a softmax
function to produce the predicted probability p(y|x) = softmax(z), and the predicted class is chosen
by f (x) = arg maxyp(y|x). There exist adversarial attacks that went beyond classiﬁcation [34], but
we focus on classiﬁcation because it is the fundamental technique extended to other tasks.

2.1 Adversarial attacks

An adversarial example x∗ is crafted by adding an imperceptible perturbation to input x, so that
the prediction of classiﬁer f (·) becomes incorrect. If an attack aims at just forcing the classiﬁer to
produce a wrong prediction, i.e., f (x∗) (cid:54)= f (x), it is called a non-targeted attack. On the other hand,
a targeted attack is designed to cause the classiﬁer to output a speciﬁc prediction as f (x∗) = y∗,
where y∗ is the target label and y∗ (cid:54)= f (x). In this paper, we mainly focus on the non-targeted attack,
as it is more common and easier to achieved by a few recent approaches [8, 16, 19, 5, 20, 4]. Another
way of categorizing adversarial attacks is to consider the amount of information the attacker has.
A white-box attack means that the adversary has full access to both the target model f (·) and how
the defender works. In the opposite, a black-box attack indicates that the attacker knows neither
the classiﬁer nor the defender. Also there are some intermediate cases, termed the gray-box attack,
in which partial information is unknown. Generally, two types of methods to generate adversarial
examples were most frequently used by researchers:

Gradient-sign ((cid:96)∞) methods. To generate adversarial examples efﬁciently, Goodfellow et al. [8]
proposed the Fast Gradient Sign Method (FGSM) that took a single step in the signed-gradient
direction of the cross-entropy loss, based on an assumption that the classiﬁer is approximately linear
locally. However, this assumption may not hold perfectly and the success rates of FGSM are relatively
low. To address this issue, Kurakin et al. [16] proposed the Basic Iterative Method (BIM) that applied
fast gradient iteratively with smaller steps. Madry et al. [19] further extended this approach to a
‘universal’ ﬁrst-order adversary by introducing a random start. The proposed Projected Gradient
Descent (PGD) method served as a very strong (cid:96)∞ attack in the white-box scenario. Dong et al. [5]
stated that FGSM adversarial examples ‘under-ﬁt’ the target model and BIM ‘over-ﬁt’ it, respectively,
making them hard to transfer across models. They proposed the Momentum-based Iterative Method
(MIM) that integrated a momentum term into the iterative process to stabilize update directions.

(cid:96)2 attacks. Moosavi-Dezfooli et al. [20] also considered a linear approximation of the decision
boundaries and proposed the DeepFool attack to generate adversarial examples that minimize pertur-
bations measured by (cid:96)2-norm. It iteratively moved an image towards the nearest decision boundary
until the image crosses it and becomes misclassiﬁed. Unlike previous iterative gradient-based ap-
proaches, Carlini and Wagner [4] proposed to directly minimize a loss function so that the generated
perturbation makes the example adversarial meanwhile its (cid:96)2-norm is optimized to be small. The
C&W attack always generates very strong adversarial examples with low distortions.

2

2.2 Adversarial defenses

Many methods defending against adversarial examples have been proposed recently. These methods
can be roughly divided into two categories. One type of defenses worked by modifying the training
or inference strategies of models to improve their inherent robustness against adversarial examples.
Adversarial training is one of the most popular and effective method of this kind [8, 17, 19, 33, 13].
It augmented or replaced the training data with adversarial examples and aimed at training a robust
model being aware of the existence of adversaries. While effectively improving the robustness to seen
attacks, this type of approaches often consumed much more computational resources than normal
training. Other methods include defensive distillation [24], saturating networks [21], thermometer
encoding [3], etc., which beneﬁted from gradient masking effect [25] or obfuscated gradients [2] but
were still vulnerable to black-box attacks.

Another line of defenses was based on removing adversarial perturbations by processing the input
image before feeding them to the target model. Dziugaite et al. [6] studied JPEG compression to
reduce the effect of adversarial noises. Osadchy et al. [22] applied a set of ﬁlters like the median ﬁlter
and averaging ﬁlter to remove perturbations. Guo et al. [9] studied ﬁve image transformations and
showed that total-variance minimization and image quilting were effective for defense. Xie et al. [35]
leveraged random resizing and padding to mitigate adversarial effects. Prakash et al. [28] proposed
pixel deﬂection that redistributed pixel values locally and thus broke adversarial patterns. Liao et al.
[18] utilized a high-level representation guided denoiser to defend adversaries. Nonetheless, these
methods did not show high effectiveness against very strong perturbations.

Another approach most similar to our work is [29], which detected and attempted to recover adversar-
ial examples by measuring how logits change when the input is randomly perturbed. Our method
differs from it in that we purely leverage the ﬁnal logits but no input image.

3 Settings: dataset, network models and attacks

Throughout this paper, we evaluate our approach on the ILSVRC2012 dataset [30], a large-scale
image classiﬁcation task with C = 1,000 classes, around 1.3M training images and 50K validation
images. This dataset was also widely used by other researchers for adversarial attacks and defenses.
We directly use a few pre-trained network models from the model zoo of the PyTorch platform [26],
including VGG-16 [31], ResNet-50 [10] and DenseNet-121 [12], which report top-1 classiﬁcation
accuracies of 73.48%, 76.13% and 74.47%, respectively.

As for adversarial attacks, we evaluate a few popular attacks mentioned in Section 2.1 using the
CleverHans library [23], including PGD [19], MIM [5], DeepFool [20] and C&W [4], each of which
causes dramatic accuracy drop to the pre-trained models (see Section 4.3). Below we elaborate the
technical details of these attacks, most of which simply follow their original implementations.

0 ∼ B∞
x∗

PGD [19] adversarial examples are generated by the following method:
(cid:15) (x), x∗
t+1 = ΠB∞
(cid:15) (x) is the (cid:96)∞ ball around x, x∗

(1)
Here, B∞
(cid:15) (x)(·)
means clipping the examples so that they stay in the ball and satisfy the (cid:96)∞ constraint. We set the
maximum perturbation size to be (cid:15) = 16/255, number of iterations T = 10 and step size α = 2/255.

J(x∗
0 is randomly sampled from inside the ball and ΠB∞

t , y))(cid:1) , x∗ = x∗
T .

t + α · sign(∇x∗

(cid:0)x∗

(cid:15) (x)

t

MIM [5] adversarial examples are generated using the following algorithm:

g0 = 0,
0 = x, x∗
x∗

gt+1 = µ · gt + ∇x∗
t+1 = x∗

J(x∗
J(x∗
t , y)||1,
t + α · sign(gt+1), x∗ = x∗
T .

t , y)/||∇x∗

t

t

(2)

With maximum perturbation size (cid:15) = 16/255, we set the number of iterations T = 10, step size
α = (cid:15)/T and decay factor µ = 1.0.

DeepFool [20] iteratively ﬁnds the minimal perturbations to cross the linearization of the nearest
decision boundaries. 10 most-likely classes are sampled when looking for the nearest boundaries,
and the maximum number of iterations is set to be 100.

C&W [4] jointly optimizes the norm of perturbation and a hinge loss of giving an incorrect prediction.
For efﬁciency, we set the maximum number of iterations to 25 with 4 binary search steps for the
trade-off constant. The initial trade-off constant, hinge margin and learning rate are set to be 10, 0
and 0.01, respectively.

3

4 Defending adversarial attacks by logits

Our goal is to detect and defend adversarial attacks by logits. The main reason of investigating logits
lies in two parts. First, it makes the algorithm easier to be deployed in the scenarios that the original
input image and/or mid-level features are not available. Second, it enables us to understand the
mechanisms of adversaries as well as how they are related to the interpretability of deep networks.

Mathematically, we are given a set of logits z which is the ﬁnal output of the deep network f (·)
before softmax and the input is either a clean image x or an adversarial image x∗. Most often, f (x)
leads to the correct class while f (x∗) can be dramatically wrong. The goal is to recover the original
label from f (x∗) while remaining the prediction of f (x) unchanged.

4.1 The possibility of defending adversaries by logits

The basis of our research lies in the possibility of de-
fending adversarial attacks by merely checking the log-
its. In other words, the numerical values of logits before
and after being attacked are quite different. To reveal
this, we use an example of the PGD attack [19], while
we also observe similar phenomena in other cases. We
apply PGD over all 50,000 validation images, and plot
the distributions of the average values of the logits, be-
fore and after the dataset is attacked. Figure 1 shows
the histogram of the average response. One can ob-
serve that adversaries cast signiﬁcant changes which
are obvious even under such simple statistic.

The above experiment indicate that adversarial attacks
indeed leave some kind of ‘ﬁngerprints’ in high-level
feature vectors, in particular logits. This is interesting
because logits features are produced by the last layer
of a deep network, so (i) do not contain any spatial
information and (ii) each element in it corresponds to
one class of the dataset. The former property makes
our defender quite different from those working on
image data or intermediate features [6, 22, 9, 35, 28,
36], which often made use of spatial information for
noise removal. The latter property eases the defender
to learn inter-class relationship to determine whether a case was adversarial and to recover it from
attacks. However, due to the high dimensionality of logits — the dimension is C = 1,000 in our
problem and can be higher in the future, it is difﬁcult to achieve the goal of defense upon a few ﬁxed
rules. This motivates us to design a learning-based approach for this purpose.

Figure 1: Average response of logits
on clean and PGD adversarial exam-
ples, counted on the validation set of
ILSVRC2012. We ﬁx the number of bins to
be 20 for both types of data. In most cases,
the PGD attack has made the mean value of
logits greater.

4.2 Adversarial logits correction

We ﬁrst consider training a defender for a speciﬁc deep network f (·), say ResNet-50 [10], and a
speciﬁc attack A(·), say PGD [19]. We should discuss on the possibility of transfer this defender to
other combinations of network and attack in Section 5, after we explain how it works. The ﬁrst step
is to build up a training dataset. Recall that our goal is to recover the prediction of contaminated data
while remain that of clean data unchanged, so we collect both types of data by feeding each training
case x into A(·) so as to produce its adversarial version x∗ = A(x). Then, both x and x∗ are fed
into f (·) and the corresponding logits are recorded as z and z∗, respectively. We ignore the softmax
layer to generate the ﬁnal labels y and y∗, since it is trivial and does not impact the defense process.

We then train a two-layer fully-connected network upon these C-dimensional logits. Speciﬁcally,
the output layer is still a C-dimensional vector which represents the corrected logits, and the hidden
layer contains D = 10,000 neurons, each of which is equipped with a ReLU activation function [15]
and Dropout [11] with a keep ratio of 0.5. This design is to maximally simplify the network structure
to facilitate explanation, while being able to deal with non-linearity in learning relationship between
classes. The relatively large amount of hidden neurons eases learning complicated rules for logits

4

n=1, pre-trained classiﬁer f (·), adversarial attacker A(·);

Algorithm 1 Adversarial logits Correction
Input: Clean training set S = {xn, yn}N
Input: # of hidden neurons D, # of training iterations T , clean training probability p;
Output: logits correction network g(·);
1: Perturb S with A(·) and obtain an adversarial counterpart x∗
2: Feed all examples, xn and x∗
n, into f (·) and extract the corresponding logits zn and z∗
n;
3: Randomly initialize g(·) as a two-layer fully-connected network with D hidden neurons;
4: for t ← 1 to T do
5:

n for each xn;

m takes either zm or z∗

Sample a mini-batch Bt = {z◦
probability of p to take the clean logits, zm;
Do the training step of g(·) using Bt and the cross-entropy loss;

m=1 from S, in which z◦

m, ym}M

m, with a

6:
7: end for
8: return g(·).

correction. While we believe more complicated network design can improve its ability of defense, this
is not the most important topic of this paper. As a side note, we tried to simply use more hidden layers
to train a deeper correction network, but observed performance decrease on adversarial examples.
The training process of this defender network follows a standard gradient-based optimization, with
very few bells and whistles added. A detailed illustration is provided in Algorithm 1. Note that we
mix clean and contaminated images with a probability p that a clean image is sampled. This strategy
is simple yet effective in enabling the defender to maintain the original prediction of clean data.

Before going into experiments, we brieﬂy discuss the relationship between our approach, adversarial
logits correction, and prior work. The ﬁrst family was named ‘adversarial training’ [8, 17, 19, 33, 13],
which generated adversarial examples online and added them into the training set. Our approach is
different in many aspects, including the goal (we hope to defend adversaries towards trained models
rather than increasing the robustness of models themselves) and the overheads (we do not require
a costly online generation process). More importantly, we generate and defend adversaries in the
level of logits, which, to the best of our knowledge, was never studied in the previous literature.
The second one was often called ‘learning from noisy labels’ [1], in which researchers proposed to
estimate a transition matrix [7, 27] in order to address the corrupted labels. Although our approach
also relies on the same assumption that corrupted or adversarial labels can be recovered by checking
class-level relationship, we emphasize that the knowledge required for correcting adversaries is quite
different from that for correcting label noises. This is because the effects brought by adversaries
are often less deterministic, i.e., the adversarial label is often completely irrelevant to the original
one, but a noisy label can somewhat reﬂect the correct one. This is also the reason why we used
a learning-based approach and built a two-layer network with a large number of hidden neurons.
In practice, using a single-layer network or reducing hidden neurons can cause dramatic accuracy
drop in recovering adversaries, e.g., under the PGD attack towards ResNet-50, the recovery rate of a
two-layer network is 75.4% but drops to 41.9% when a single-layer network is used (see the next
part for detailed experimental settings).

4.3 Experimental results

We ﬁrst evaluate our approach on the PGD attack [19] with different pre-trained network models. We
use the Adam optimizer [14] to train the defenders for 50 epochs on the entire ILSVRC2012 [30]
training set, with a batch size of 256, learning rate of 0.00005, weight decay of 0.0001, and a
probability of choosing clean data, p, of 0.3. We evaluate both a full testing set (50,000 images) and
a selected testing set (1,000 correctly classiﬁed images, to compare with other attackers). Table 1
shows the classiﬁcation accuracy of different networks on both clean and PGD-attacked examples
with and without adversarial logits correction. One the one hand, although the PGD attack is very
strong in this scenario, reducing the classiﬁcation accuracy of all networks to nearly 0%, it is possible
to recover the correct prediction by merely checking logits, which (i) do not preserve any spatial
information, and (ii) as high-level features, are supposed to be perturbed much more severely than the
input data [18, 36]. After correction, the classiﬁcation accuracy on VGG-16 [31] and ResNet-50 [10]
are only reduced by 6.01% and 3.74%, respectively. To the best of our knowledge, this is the only
work which purely relies on logits to defend adversaries, so it is difﬁcult to compare these results with
prior work. On the other hand, on clean (un-attacked) examples, our approach reports slight accuracy

5

Table 1: Classiﬁcation accuracy on clean and PGD-attacked images of different networks. Here, full
indicates that the entire ILSVRC2012 validation set (50,000 images) are used, while selected refers
to a set of 1,000 images that are correctly classiﬁed by ResNet-50.

Clean, full

PGD, full

Clean, selected

PGD, selected

No Defense Corrected No defense Corrected No defense Corrected No defense Corrected

VGG-16
ResNet-50
DenseNet-121

73.48%
76.13%
74.47%

69.43%
73.85%
72.20%

0.04%
0.00%
0.01%

67.47%
72.39%
88.29%

94.7%
100.0%
100.0%

90.8%
96.5%
95.3%

0.0%
0.0%
0.0%

71.8%
75.4%
90.5%

Table 2: Classiﬁcation accuracy on clean and different
adversarial images of ResNet-50. For fair comparison,
we use the selected test set containing 1,000 images
whose clean version is correctly recognized by ResNet-
50.

Table 3: Classiﬁcation accuracy when a
defender trained on one attack is used to
defend other attacks. The target model is
ResNet-50, and all results are produced on
the selected test set.

Clean, selected

Adversarial, selected

Defender Trained on

No defense Corrected No defense Corrected

PGD MIM DeepFool C&W

PGD
MIM
DeepFool
C&W

100.0%
100.0%
100.0%
100.0%

96.5%
96.3%
97.0%
82.8%

0.0%
0.1%
0.0%
0.0%

75.4%
87.8%
76.1%
66.8%

PGD
MIM
DeepFool
C&W

75.4% 70.3%
85.7% 87.8%
54.5% 52.7%
10.3% 9.1%

0.0%
0.1%
76.1%
19.2%

4.4%
5.2%
73.5%
66.8%

drop, because some of them are considered to be adversaries and thus mistakenly ‘corrected’. This is
somewhat inevitable, because logits lose all spatial information and non-targeted attacks sometimes
cast large but random changes in logits. These cases are not recoverable without extra information.

Besides, a surprising result produced by our approach is that the classiﬁcation accuracy on DenseNet-
121, after being attacked and recovered, even improves from 74.47% to 88.29%. This phenomenon
is similar to ‘label leaking’ [17], which claimed that the accuracy on adversarial images gets much
higher than that on clean images for a model adversarially trained on FGSM adversarial examples.
However, label leaking was found to only occur with one-step attacks that use the true labels and
vanish if an iterative method is used. In our experiment settings, PGD attack is used which is a very
strong iterative method with randomness that further increases uncertainty. In addition, our correction
network merely takes logits as input but cannot directly access the transformed image. This ﬁnding
extends the problem of label leaking and reveals the possibility of helping classiﬁers with adversarial
examples.

Next, we evaluate our approach on several state-of-the-art adversarial attacks, including PGD [19],
MIM [5], DeepFool [20] and C&W [4]. A pre-trained ResNet-50 model is used as the target, and all
training settings simply remain unchanged as in previous experiments. Differently, we only use 1 test
image per class (1,000 in total) that can be correctly classiﬁed by ResNet-50 in the test stage, which is
mainly due to the slowness of the DeepFool attack. Experimental results are summarized in Table 2,
from which one can observe quite similar phenomena as in the previous experiments. Here, we
draw a few comments on the different properties among these attackers. For PGD, logits correction
manages to recover 75.4% of adversarial examples and still maintains a sufﬁciently high accuracy
(96.5%) on clean examples. MIM differs from PGD in that no random start is used and a momentum
term is introduced to stabilize gradient updates, which results in less diversity and uncertainty. As
a result, logits correction is able to learn better class-level relationship and thus recovers a larger
fraction (87.8%) of adversarial examples. As for DeepFool, note that image distortions are relatively
smaller since it minimizes the perturbations in (cid:96)2-norm. Nevertheless, our method still succeeds in
correcting 76.1% of adversarial logits. Among all evaluated attacks, C&W is the most difﬁcult to
defend, with our approach yielding an accuracy of less than 70% on adversarial examples and the
accuracy on clean examples is also largely affected. This is partly because C&W, besides controlling
the (cid:96)2-norm like DeepFool, uses a different kind of objective function and explicitly optimizes the
magnitude of perturbations, which results in quite different behaviors in adversarial logits and thus
increases the difﬁculty of correction.

Finally, we evaluate transferability, i.e., whether a logits correction network trained on a speciﬁc
attack can be used to defend other attacks on the same model. We ﬁx ResNet-50 to be the target

6

model, and evaluate the defender trained on each of the four attackers. Results are summarized in
Table 3, in which each row corresponds to an attacker and each column a defender — note that the
diagonal is the same as the last column of Table 2. One can see from the table that the defenders
trained on PGD and MIM transfer well to each other, mainly due to the similar nature of these
two attackers. These two defenders are also able to correct more than half of adversarial examples
generated by DeepFool, which shows a wider aspect in generalization. On the contrary, the defender
trained on DeepFool can hardly recover those adversarial examples produced by PGD and MIM,
indicating that DeepFool, being an (cid:96)2-norm attacker, has different properties and thus the learned
patterns for defense are less transferable. Similarly, C&W has a closer behavior to DeepFool, an
(cid:96)2-norm attacker, than to PGD and MIM, two (cid:96)∞-norm attackers, which also reﬂects in the low
recovery rates in the last row and column of Table 3. It is interesting to see a high transfer accuracy
from C&W to DeepFool, but a low accuracy in the opposite direction. This is because both DeepFool
and C&W are (cid:96)2-norm attackers, but the adversarial patterns generated by DeepFool are relatively
simpler. So, the defender trained on C&W can cover the patterns of DeepFool (73.5% of cases are
defended), but the opposite is not true (only 19.2% of cases are defended).

In the next section, we will provide a new insight to transferability, which focuses on ﬁnding the
supporting classes of each defense and measuring the overlapping ratio between different sets of
supporting classes.

5 Explaining logits-based defense

5.1 How logits correction works

It remains an important topic to explain how our defender works. Thanks to the semantic basis and
simplicity of our approach, for each defender, we can ﬁnd a small set of classes that make most
signiﬁcant contributions to defense.
Given a clean vector of logits, z, or an adversarial one, z∗, the logits correction network g(·) acts as a
multi-variate mapping between two RC spaces where C = 1,000 in the ILSVRC2012 dataset [30].
Let zk denote the score of the k-th class in z, and g(z)i the score of the i-th class in g(z). The core
.
= ∂g(z)i
idea is to compute the partial derivative Hi,k(z)
, so that we can estimate the contribution of
∂zk
each element in the input logits to the corrected logits.
Suppose we have an adversarial sample z∗ that has a ground-truth label of i but is misclassiﬁed as
class j after being attacked. When feeding it into the correction network g(·), it should be recovered
and thus g(z∗)i should have the greatest score. To ﬁnd out how g(·) manages to perform correction,
we can either investigate how g(z∗)i is ‘pulled up’ by ﬁnding the classes that have high positive
Hi,k(z∗) values, or how other classes are ‘pushed down’ by ﬁnding the classes that have high negative
impacts on the average of all logits, namely, Hmean,k(z∗) = 1
C
We ﬁrst explore an example using the logits correction network trained to defend the PGD attack [19]
on ResNet-50 [10]. For each adversarial input z∗ in the validation set, we ﬁnd out 10 greatest entries
of Hi,k(z∗), with i being the original label and k ranges among all 1,000 classes. Interestingly, for
a large amount of cases, no matter what the ground-truth label or the input image is, there always
exist some speciﬁc classes that contribute most to recovering the correct label. We count over all
50,000 validation images, and ﬁnd out that the greatest Hi,k(z∗) appears mostly when k equals to
705, 830, 600, 356, 850 and 447. When we compute −Hmean,k(z∗) instead, most cases have the
highest response in the 640-th class, (i.e., ‘manhole cover’). Similar phenomena are also found when
we use PGD to attack other target networks, including VGG-16 [31] and DenseNet-121 [12]. Some
of the classes that contribute most to Hi,k(z∗) overlap with those for ResNet-50, implying that these
classes are fragile to PGD. When we compute −Hmean,k(z∗) instead, the 640-th class still dominants
for both DenseNet-121 and VGG-16.

lHl,k(z∗).

(cid:80)

5.2 Supporting classes and their relationship to transferability of defense

Here, we deﬁne a new concept named supporting classes as those classes that contribute most to
logits-based defense. Taking both positive (‘pulling up’) and negative (‘pushing down’) effects into
.
= Hi,k(z∗) − Hmean,k(z∗). Among all classes, 10 classes of k with
consideration, we compute Sk
greatest Sk values are taken out for each image. We count the occurrences over the selected test

7

set (1,000 images), and ﬁnally pick up 10 classes that appear most frequently as the supporting
classes for defending a speciﬁc attack from the model. The supporting classes of defending the PGD
attack on ResNet-50 are illustrated in Figure 2, and we also show other cases in the supplementary
material. We ﬁrst emphasize that, indeed, these classes are the key to defense. On an arbitrary image,
if we reduce the logit values of these supporting classes by 20 and feed the modiﬁed logits into the
trained defender, it is almost for sure that logits correction fails dramatically and, very likely, the
classiﬁcation result remains to be j, the originally misclassiﬁed class, even when the original or
adversarial classes of this case are almost irrelevant to these supporting classes.

As a further analysis, we reveal the relation-
ship between the overlapping ratio of supporting
classes and the transferability of our defender
from one setting to another. We compute the
Bhattacharyya coefﬁcients between the sets of
supporting classes produced by the four attacks.
The ﬁrst pair is PGD and MIM, which reports
a high coefﬁcient of 0.9662. This implies that
PGD and MIM have quite similar behavior in
attacks, which is mainly due to their similar
mechanisms (e.g., (cid:96)∞-bounded, iteration-based,
etc.). Consequently, as shown in Table 3, the
defenders trained on PGD and MIM transfer
well to each other. A similar phenomenon ap-
pears in the pair of DeepFool and C&W, two
(cid:96)2-norm attackers, with an coefﬁcient of 0.9222.
As a result, the defenders trained on DeepFool
and C&W produce the best transfer accuracy
on each other (as we explained before, the
weak transferability of the DeepFool defender
is mainly because DeepFool is very easy to de-
fend). Across (cid:96)∞-norm and (cid:96)2-norm attackers,
we report coefﬁcients of 0.9004 for the pair of
PGD and DeepFool, and 0.8623 for PGD and
C&W, respectively. Note that both numbers are less than those between the same type of attacks.
In addition, the defender trained on PGD achieves an accuracy of 54.5% on DeepFool, and merely
10.3% on C&W, which aligns with the coefﬁcient values. These results verify our motivation —
logits-based correction is easier to be explained, in particular at the semantic level.

Figure 2: Supporting classes of the PGD defender
on ResNet-50. We list 10 classes that appear most
frequently in the top-10 of Sk, with the frequency
of occurrence recorded on the vertical axis. For
better visualization, we list the name of each class
and attach a representative image above the bar.

Last but not least, the impact of supporting classes can also be analyzed in the instance level, i.e.,
ﬁnding the classes that contribute most to defending each attack on each single image. We observe a
few interesting phenomena, e.g., for PGD and MIM, the top-1 supporting class is very likely to be the
640-th class, while for DeepFool, the most important class is always the ground-truth class which
differs from case to case. This partly explains the transferability between PGD/MIM and DeepFool.
More instance-level analysis is provided in the supplementary material.

6 Conclusions

In this paper, we ﬁnd that a wide range of state-of-the-art adversarial attacks can be defended by
merely correcting logits, the features produced by the last layer of a deep neural network. This
implies that each attacker leaves ‘ﬁngerprints’ during the attack. Although it is difﬁcult to make
rules to detect and eliminate such impacts, we design a learning-based approach, simple but effective,
which achieves high recovery rates in a few combinations of attacks and target networks. Going
one step forward, we reveal that our defender works by ﬁnding a few supporting classes for each
attack-network combination, and by checking the overlapping ratio of these classes, we can estimate
the transferability of a defense across different scenarios.

Our research leaves a few unsolved problems. For example, it is unclear whether there exists an attack
algorithm that cannot be corrected by our defender, or if we can ﬁnd deeper connections between our
discovery and the mechanism of deep neural networks. In addition, we believe that improving the
transferability of this defender is a promising direction, in which we shall continue in the future.

8

References

[1] Dana Angluin and Philip D. Laird. Learning from noisy examples. Machine Learning, 2:343–370, 1988.

[2] Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of security:

Circumventing defenses to adversarial examples. In ICML, 2018.

[3] Jacob Buckman, Aurko Roy, Colin A. Raffel, and Ian J. Goodfellow. Thermometer encoding: One hot way

to resist adversarial examples. In ICLR, 2018.

[4] Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In IEEE

Symposium on Security and Privacy (SP), 2017.

[5] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting

adversarial attacks with momentum. In CVPR, 2018.

[6] Gintare Karolina Dziugaite, Zoubin Ghahramani, and Daniel M. Roy. A study of the effect of jpg

compression on adversarial images. CoRR, abs/1608.00853, 2016.

[7] Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In

[8] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

ICLR, 2017.

examples. In ICLR, 2015.

In CVPR, 2016.

2017.

[9] Chuan Guo, Mayank Rana, Moustapha Cissé, and Laurens van der Maaten. Countering adversarial images

using input transformations. In ICLR, 2018.

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.

[11] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov.
Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580, 2012.

[12] Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely connected convolutional networks. In CVPR,

[13] Harini Kannan, Alexey Kurakin, and Ian J. Goodfellow. Adversarial logit pairing. In NeurIPS, 2018.

[14] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.

[15] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convolutional

neural networks. In NeurIPS, 2012.

[16] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In

[17] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In ICLR,

ICLR Workshop, 2017.

2017.

[18] Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Jun Zhu, and Xiaolin Hu. Defense against

adversarial attacks using high-level representation guided denoiser. In CVPR, 2018.

[19] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards

deep learning models resistant to adversarial attacks. In ICLR, 2018.

[20] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and accurate

method to fool deep neural networks. In CVPR, 2016.

[21] Aran Nayebi and Surya Ganguli. Biologically inspired protection of deep networks from adversarial

attacks. CoRR, abs/1703.09202, 2017.

[22] Margarita Osadchy, Julio Hernandez-Castro, Stuart J. Gibson, Orr Dunkelman, and Daniel Pérez-Cabo.
No bot expects the deepcaptcha! introducing immutable adversarial examples, with applications to captcha
generation. In IEEE Transactions on Information Forensics and Security, volume 12, pages 2640–2653,
2017.

9

[23] Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, Alexey Kurakin,
Cihang Xie, Yash Sharma, Tom Brown, Aurko Roy, Alexander Matyasko, Vahid Behzadan, Karen
Hambardzumyan, Zhishuai Zhang, Yi-Lin Juang, Zhi Li, Ryan Sheatsley, Abhibhav Garg, Jonathan Uesato,
Willi Gierke, Yinpeng Dong, David Berthelot, Paul Hendricks, Jonas Rauber, Rujun Long, and Patrick
McDaniel. Technical report on the cleverhans v2.1.0 adversarial examples library. CoRR, abs/1610.00768,
2016.

[24] Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In IEEE Symposium on Security and
Privacy (SP), 2016.

[25] Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia
Conference on Computer and Communications Security, 2017.

[26] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming
Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NeurIPS
Workshop, 2017.

[27] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep

neural networks robust to label noise: A loss correction approach. In CVPR, 2017.

[28] Aaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo, and James A. Storer. Deﬂecting

adversarial attacks with pixel deﬂection. In CVPR, 2018.

[29] Kevin Roth, Yannic Kilcher, and Thomas Hofmann. The odds are odd: A statistical test for detecting

adversarial examples. CoRR, abs/1902.04818, 2019.

[30] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large
scale visual recognition challenge. In IJCV, volume 115, pages 211–252, 2015.

[31] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. In ICLR, 2015.

[32] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and

Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014.

[33] Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian J. Goodfellow, Dan Boneh, and Patrick D. McDaniel.

Ensemble adversarial training: Attacks and defenses. In ICLR, 2018.

[34] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan L. Yuille. Adversarial

examples for semantic segmentation and object detection. In ICCV, 2017.

[35] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Loddon Yuille. Mitigating adversarial

effects through randomization. In ICLR, 2018.

[36] Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Loddon Yuille, and Kaiming He. Feature denoising

for improving adversarial robustness. CoRR, abs/1812.03411, 2018.

10

A Supporting classes of different attacks

In Figure 3, we illustrate the supporting classes of defending PGD [19], MIM [5], DeepFool [20]
and C&W [4] on ResNet-50 [10], respectively. Just like the cases of PGD attack on different target
networks, these different attacks also share some supporting classes in common. Note that the
supporting classes of PGD and MIM are nearly the same, even considering their relative order
in frequency. This aligns with the high Bhattacharyya coefﬁcients between them and the good
transferability of defenders trained on them. The two (cid:96)2-norm attacks, DeepFool and C&W, also
have very similar supporting classes, and this similarity yet accounts for the transferability of their
corresponding defenders. Besides, one can observe from Figure 3 that MIM and DeepFool have
stronger response in the 640-th class, i.e., ‘manhole cover’, which may explain why they are easier to
be defended. Similarly, the difﬁculty of defending against C&W may also reside in the fact that the
supporting classes of this attack are not as strong as other attackers, e.g., the dominant class is also
the 640-th class, but its frequency is relatively lower (also closer to the second class).

Figure 3: Supporting classes of each adversarial attack on ResNet-50. We list 10 classes that appear
most frequently in the top-10 of Sk (see Section 5.2), with the frequency of occurrences recorded on
the vertical axis. For better visualization, we list the name of each class on the horizontal axis, and
also attach a representative image above the bar. Please zoom in for better clarity.

B Delving into supporting classes at instance level

To better understand the impact of supporting classes, we further inspect the classes that contribute
most to defending a single example, i.e., at an instance level. The top-10 classes of Sk and their
corresponding values are taken out for each image. Figure 4 shows such classes and values for an
example with ground-truth label 999 and attacked by the four attackers.

We ﬁrst explore the PGD attacker [19] as usual, and ﬁnd that while the supporting classes we obtain
in the last part frequently appear in the top-10, the 640-th class always occupies the top-3 and even

11

Figure 4: Top-10 classes of Sk and their corresponding values for an example with ground-truth label
999 and attacked by PGD, MIM, DeepFool and C&W, respectively. For better visualization, we list
the name of each class. Please zoom in for better clarity.

top-1, especially when the attacked example is successfully corrected by the defender. This once
again shows the importance of the 640-th class, and similar phenomenon is found when the MIM
attacker [5] is used.

As for DeepFool [20], things become different, as the top-1 of Sk always lies in i, the ground-truth
label, with a much greater value than the second most signiﬁcant supporting class. This is mainly due
to the design nature of DeepFool, which moves an example across the nearest decision boundary and
thus the original class should still have a high score in the adversarial logits z∗. In other words, the
adversary of DeepFool can be recovered by assigning a greater weight to i. Consequently, although
DeepFool shares a similar property that the 640-th class still appears most frequently, it shows quite a
different behavior in defense, which partly reﬂects in the transfer experiments between DeepFool
and PGD. Given an example attacked by DeepFool and a defender trained on PGD, the defender
can correct the example basing on the supporting classes rather than i, yielding a relatively good
performance; On the contrary, given an example attacked by PGD and a defender trained on DeepFool,
the defender will focus too much on the ground-truth class i of the example, and thus fail to correct
the attack of PGD which does not have such preference.

Finally, we study the case of C&W [4]. We ﬁnd that the set of supporting classes of Sk lies between
PGD and DeepFool, and is more similar to that of DeepFool (i.e., the most signiﬁcant class is usually
the ground-truth class i, but sometimes 640). This corresponds to the fact that the defender trained
on C&W can transfer to DeepFool better than PGD. However, since the behaviour of C&W is more
irregular (e.g., i is not always the most important contributor), it is more difﬁcult to defend by
defenders trained on other attacks.

12

