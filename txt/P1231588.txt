Efﬁcient Video Object Segmentation via Network Modulation

Linjie Yang1

Yanran Wang2∗

Xuehan Xiong3

Jianchao Yang1

Aggelos K. Katsaggelos2

1Snap Inc.
2Image and Video Processing Laboratory, Northwestern University
3Google Inc.
{linjie.yang,jianchao.yang}@snap.com {joycewang1026@u,aggk@eecs}.northwestern.edu xiong828@gmail.com

8
1
0
2
 
b
e
F
 
4
 
 
]

V
C
.
s
c
[
 
 
1
v
8
1
2
1
0
.
2
0
8
1
:
v
i
X
r
a

Abstract

Video object segmentation targets at segmenting a spe-
ciﬁc object throughout a video sequence, given only an an-
notated ﬁrst frame. Recent deep learning based approaches
ﬁnd it effective by ﬁne-tuning a general-purpose segmen-
tation model on the annotated frame using hundreds of it-
erations of gradient descent. Despite the high accuracy
these methods achieve, the ﬁne-tuning process is inefﬁcient
and fail to meet the requirements of real world applica-
tions. We propose a novel approach that uses a single
forward pass to adapt the segmentation model to the ap-
pearance of a speciﬁc object. Speciﬁcally, a second meta
neural network named modulator is learned to manipu-
late the intermediate layers of the segmentation network
given limited visual and spatial information of the target
object. The experiments show that our approach is 70×
faster than ﬁne-tuning approaches while achieving similar
accuracy. Our model and code are released at https:
//github.com/linjieyangsc/video_seg.

1. Introduction

Semantic segmentation plays an important role in un-
derstanding visual content of an image as it assigns pre-
deﬁned object or scene labels to each pixel and thus trans-
lates the image into a segmentation map. When dealing with
video content, a human can easily segment an object in the
whole video without knowing its semantic meaning, which
inspired a research topic named semi-supervised video seg-
mentation. In a typical scenario of semi-supervised video
segmentation, one is given the ﬁrst frame of a video along
with an annotated object mask, and the task is to accurately
locate the object in all following frames [24, 20]. The ability
of performing accurate pixel-level video segmentation with
minimum supervision (e.g., one annotated frame) can fos-
ter a large amount of applications, such as accurate object

∗This work is done when Yanran was an intern at Snap Inc.

Figure 1. An overview of our approach. Our model is consisted of
a modulator and a segmentation network. The modulator can adapt
the segmentation model instantly to segment an arbitrary object
through a video sequence.

tracking for video understanding, interactive video editing,
augmented reality, and video-based advertisement. When
the supervision is limited to only one annotated frame, re-
searchers refer to this scenario as one-shot learning. In the
recent years, we have witnessed a rising amount of interests
in developing one-shot learning techniques for video seg-
mentation [2, 23, 35, 22, 32, 4]. Most of these work share a
similar two-stage paradigm: ﬁrst, train a general-purpose
Fully Convolutional Network (FCN) [31] to segment the
foreground object; Second, ﬁne-tune this network based on
the ﬁrst frame of the video for several hundred forward-
backward iterations to adapt the model to the speciﬁc video
sequence. Despite the high accuracies achieved by these ap-
proaches, the ﬁne-tuning process is arguably time consum-
ing, which makes it prohibited for real-time applications.
Some of these approaches [4] [23] also utilize optical ﬂow
information, which is computationally heavy for state-of-
the-art algorithms[29] [15].

1

In order to alleviate the computational cost of semi-
supervised segmentation, we propose a novel approach to
adapt the generic segmentation network to the appearance
of a speciﬁc object instance in one single feed-forward pass.
We propose to employ another meta neural network called
modulator to learn to adjust the intermediate layers of the
generic segmentation network given an arbitrary target ob-
ject instance. Fig. 1 shows an illustration of our approach.
By extracting information from the image of the annotated
object and the spatial prior of the object, the modulator pro-
duces a list of parameters, which are injected into the seg-
mentation model for layer-wise feature manipulation. With-
out one-shot ﬁne-tuning, our model is able to change the
behavior of the segmentation network with minimum ex-
tracted information from the target object. We name this
process network modulation.

Our proposed model is efﬁcient, requiring only one for-
ward pass from the modulator to produce all parameters
needed for the segmentation model to adapt to the speciﬁc
object instance. Network modulation guided by the spatial
prior facilitates the model to track the object even with the
presence of multiple similar instances. The whole pipeline
is differentiable and can be learned end-to-end using the
standard stochastic gradient descent. The experiments show
that our approach outperforms previous approaches without
one-shot ﬁne-tuning by a large margin, and achieves com-
parable performance with these approaches after one-shot
ﬁne-tuning with a 70× speed up.

2. Related Work

Semi-supervised video segmentation. Semi-supervised
video object segmentation aims at tracking an object mask
given from the ﬁrst annotated frame throughout the rest of
video. Many approaches have been proposed in the lit-
erature, including those propagating superpixels [17] [35],
patches [9], object proposals [25], or in bilateral space [22],
and graphical model based optimization is usually per-
formed to consider multiple frames simultaneously. With
the success of FCN on static image segmentation [12],
deep learning based methods [23, 2, 32, 34, 18, 4] have
been recently proposed for video segmentation and promis-
ing results have been achieved. To model the temporal
motion information, some works heavily rely on optical
ﬂow [34] [4], and use CNNs to learn mask reﬁnement of
an object from current frame to the next one [23], or com-
bine the training of CNN with bilateral ﬁltering between
adjacent frames [18]. Chen et al. [4] use a CNN to jointly
estimate the optical ﬂow and provide the learned motion
representation to generate motion consistent segmentation
across time. Different from these approaches, Caelles et
al. [2] combine ofﬂine and online training process on static
images without using temporal information. While it saves
the computation of optical ﬂow and/or conditional random

ﬁelds (CRF) [19] involved in some previous methods, on-
line ﬁne-tuning still requires many iterations of optimiza-
tion, which poses a challenge for real-world applications
that need rapid inference.

Meta-learning for low-shot learning. Current success of
deep learning relies on the ability of learning from large-
scale labeled datasets through gradient descent optimiza-
tion. However, if we want our model to learn many tasks
adapted to many environments, it is not affordable to learn
each task for each setting from scratch. Instead, we want
our deep learning system to be able to learn new tasks very
fast and from very limited quantities of data. In the extreme
of “one-shot learning”, the algorithm needs to learn the new
task with a single observation. One potential strategy for
learning a versatile model is the notion of meta-learning,
or learning to learn, which can date back to the late 80s.
Recently, meta-learning has become a hot research topic
with publications on neural network optimization [3], ﬁnd-
ing good network architectures, fast reinforcement learning,
and few-shot image recognition [36, 28, 13, 10, 30]. Ravi
and Larochelle [28] proposed a LSTM meta-learner to learn
the update rules for few shot learning. The meta optimiza-
tion over a large number of tasks in [10] targets at learning
a model that can quickly adapt to the new task with limited
number of updates. Hariharan and Girschick [13] trained a
learner that generated new samples and used new samples
for training new tasks. Our approach shares the similarity
with meta-learning that it learns to update the segmentation
model rapidly with another meta learner, i.e. the modulator.

Network manipulation Several previous work try to in-
corporate modules to manipulate the behavior of a deep
neural network, either to manipulate spatial arrangement of
data [16] or ﬁlter connections [5]. Our method is also heav-
ily motivated by conditional batch normalization [8, 11, 14,
26], where the behavior of the deep model is manipulated by
batch normalization parameters conditioned on a guidance
input, e.g. a style image for image stylization or a language
sentence for visual question answering.

3. Video Object Segmentation with Network

Modulation

In our proposed framework, we utilize modulators to in-
stantly adapt the segmentation network to a speciﬁc object,
rather than performing hundreds of iterations of gradient
descent. We can achieve similar accuracy by adjusting a
limited number of parameters in the segmentation network,
compared with the updating the whole network in one-shot
learning approaches [23, 2]. There are two important cues
for video object segmentation: visual appearance and con-
tinuous motion in space. To use information from both vi-

Figure 2. An illustration of our model with three components: a segmentation network, a visual modulator, and a spatial modulator. The
two modulators produce a set of parameters that manipulates the intermediate feature maps of the segmentation network and adapt it to
segment the speciﬁc object.

sual and spatial domains, we incorporate two network mod-
ulators, namely visual modulator and spatial modulator, to
learn to adjust intermediate layers in the main segmentation
network, based on the annotated ﬁrst frame and spatial lo-
cation of the object, respectively.

3.1. Conditional batch normalization

Our approach is inspired by recent works using Condi-
tional Batch Normalization (CBN) [6, 14, 26], where the
scale and bias parameters of each batch-normalization layer
are produced by a second controller network. These param-
eters are used to control the behavior of the main network
for tasks such as image stylization and question answering.
Mathematically, each CBN layer can be formulated as fol-
lows:

yc = γcxc + βc,

(1)

where xc and yc are the input and output feature maps in the
cth channel, and γc and βc are the scale and bias parameters
produced by the controller network, respectively. The mean
and variance parameters are omitted for clarity.

3.2. Visual and spatial modulation

The CBN layer is a special case of the more general
scale-and-shift operation on feature maps. Following each
convolution layer, we deﬁne a new modulation layer with
parameters generated by both visual and spatial modula-
tors that are jointly trained. We design the two modula-
tors such that the visual modulator produces channel-wise
scale parameters to adjust the weights of different channels

in the feature maps, while the spatial modulator generates
element-wise bias parameters to inject spatial prior to the
modulated features. Speciﬁcally, our modulation layer can
be formulated as follows:

yc = γcxc + βc,

(2)

where γc and βc are modulation parameters from the vi-
sual and spatial modulators, respectively. γc is a scalar for
channel-wise weighting, while βc is a two-dimensional ma-
trix to apply point-wise bias values.

Fig. 2 shows an illustration of the proposed approach,
which consists of three networks: a fully-convolutional
main segmentation network, a visual modulator network,
and a spatial modulator network. The visual modulator net-
work is a CNN that takes the annotated visual object image
as input and produces a vector of scale parameters for all
modulation layers, while the spatial modulator network is a
very efﬁcient network that produces bias parameters based
on the spatial prior input. We will discuss the two modula-
tors in more detail in the following sections.

3.3. Visual modulator

The visual modulator is used to adapt the segmentation
network to focus on a speciﬁc object instance, which is the
annotated object in the ﬁrst frame. The annotated object is
referred to as visual guide hereafter for convenience. The
visual modulator extracts semantic information such as cat-
egory, color, shape, and texture, from the visual guide and
generates corresponding channel-wise weights so as to re-
target the segmentation network to segment the object. We

use VGG16 [33] neural network as the model for the vi-
sual modulator. We modify its last layer trained for Ima-
geNet classiﬁcation to match the number of parameters in
the modulation layers for the segmentation network.

The visual modulator implicitly learns an embedding of
different types of objects. It should produce similar param-
eters to adjust the segmentation network for similar objects
while different parameters for different objects. This is in-
deed true as we show in Sec. 4.2 that the embedding of the
modulator outputs correlates with object appearance very
well. One big advantage of using such a visual modulator is
that we can potentially transfer the knowledge learned with
a large number of object classes, e.g., ImageNet, in order to
learn a good embedding.

3.4. Spatial modulator

Our spatial modulator takes a prior location of the ob-
ject in the image as input. Since objects move continu-
ously in a video, we set the prior to be the predicted loca-
tion of the object mask in the previous frame. Speciﬁcally,
we encode the location information as a heatmap with a
two-dimensional Gaussian distribution on the image plane.
The center and standard deviations of the Gaussian distribu-
tion are computed from the predicted mask of the previous
frame. This heatmap is referred as spatial guide hereafter
for convenience. The spatial modulator downsamples the
spatial guide into different scales, to match the resolution
of different feature maps in the segmentation network, and
then applies a scale-and-shift operation on each downsam-
pled heatmap to generate the bias parameters of the corre-
sponding modulation layer. Mathematically,

βc = ˜γcm + ˜βc

(3)

where m is a down-sampled Gaussian heat map for the cor-
responding modulation layer, ˜γc and ˜βc are the scale-and-
shift parameters for the c-th channel, respectively. This is
implemented with a computationally efﬁcient 1 × 1 convo-
lution. In the bottom of Fig. 2, we illustrate the structure of
the spatial modulator.

Our method shares some similarities with the previous
work MaskTrack [23] in utilizing information from the pre-
vious mask. Comparing with their approach that uses the
exact foreground mask of the previous frame, we only use a
very coarse location prior.
It may seem that our method
throws away more information from the previous frame.
However, we argue that the rough position and size in the
previous frame possess enough information to infer the ob-
ject mask with the RGB image, and it prevents the model
from relying too much on the mask and as a result the error
propagation, which can be catastrophic when the object has
large movements in the video. As a drawback of such over-
utilization of the mask, MaskTrack has to apply plenty of
well-engineered data augmentation to prevent over-ﬁtting,

while we only apply simple shift and scaling as augmenta-
tion.

3.5. Implementation details

Our FCN structure follows the one used by [2], which
is a VGG16 [33] model with a hyper-column structure [12].
Intuitively, we should add modulation layers after each con-
volution layer in the FCN. However, we found that adding
modulation layers in-between the early convolution layers
actually makes the model perform worse. One possible rea-
son is that early layers extract low-level features that are
very sensitive to the scale-and-shift operations introduced
by the modulator. In our implementation, we add modula-
tion operations to all convolution layers in VGG16 except
the ﬁrst four layers, which results in nine modulation layers.
Similar to MaskTrack [23], we also utilize static im-
Ideally, the visual modula-
ages for training our model.
tor should learn a mapping from any object to modulation
weights of different layers in a FCN, which requires the
model to see all possible different objects. However, most
video semantic segmentation datasets only contain a very
limited number of categories. We tackle this challenge by
using the largest public semantic segmentation dataset MS-
COCO [21], which has 80 object categories. We select ob-
jects that are larger than 3% of the image size for training,
resulting in a total number of 217, 516 objects. For prepro-
cessing the input for the visual modulator, we ﬁrst crop the
object using the annotated mask, then set the background
pixels to mean image values, and then resize the cropped
image to a constant resolution of 224 × 224. The object
is also augmented with up to 10% random scaling and 10◦
random rotation. For preprocessing the spatial guide as in-
put to the spatial modulator, we ﬁrst compute the mean and
standard deviation of the mask, and then augment the mask
with up to 20% random shift and 40% random scaling. For
the whole image fed into the FCN, we use a random size
from 320, 400, and 480 with a square shape.

The visual modulator and segmentation network are
both initialized with VGG16 model pretrained on the Im-
ageNet [7] classiﬁcation task. The modulation parameters
{γc} are initialized to ones by setting the weights and biases
of the last fully-connected layer of the visual modulator to
zeros and ones, respectively. The weights of spatial modu-
lator are initialized randomly. We used the same balanced
cross-entropy loss as in [2]. A mini-batch size of 8 is used.
We use Adam optimizer with default momentum 0.9 and
0.999 for β1 and β2, respectively. The model is ﬁrst trained
for 10 epochs with learning rate 10−5 and then trained for
another 5 epochs with learning rate 10−6.

Further, in order to model appearance variations of mov-
ing objects in videos, the model can be ﬁnetuned on video
segmentation dataset such as DAVIS 2017 [27]. To be more
robust to appearance variations, we randomly pick a fore-

ground object from the whole video sequence as the visual
guide for each frame. The spatial guide is obtained from
the ground truth mask of the object in the previous frame.
The same data augmentations are applied as training on MS-
COCO. The model is ﬁnetuned for 20 epochs with learning
rate 10−6.

4. Experiments

In this section, we will introduce three parts of experi-
ment: the comparison of our approach with previous meth-
ods, the visualization of the modulation parameters, and ab-
lation study. Our model is trained on MS-COCO [21] 2017
dataset, and is tested on several popular video segmenta-
tion datasets, including DAVIS [24] [27] and YoutubeOb-
jects [17].

4.1. Semi-supervised Video Segmentation

In this section, we compare with traditional approaches
including OFL [35], BVS[22], and deep learning-based
approaches including PLM [32], MaskTrack [23], OS-
VOS [2], VPN [18], SFL [4], and ConvGRU [34].

4.1.1 DAVIS 2016 & YoutubeObjects

First, we compare our approach with previous approaches
on DAVIS 2016 and YoutubeObjects. Some approaches
(MaskTrack[23], SFL [4] and OSVOS[2]) reported results
both with and without model ﬁne-tuning on the target se-
quences. We include both of them and denote the variants
without ﬁne-tuning as MaskTrack-B, SFL-B, and OSVOS-
B, respectively. Our model has two variants,with the ﬁrst
only trained on static images (Stage 1) and the second ﬁne-
tuned on video data (Stage 1&2). Since there are several
popular add-ons for this line of research, such as optical
ﬂow and CRF [19], which both have a lot of variants and
make a fair comparison hard, we only include the perfor-
mances without optical ﬂow and CRF if possible, and mark
those with add-ons in Table 1.

In Table 1, by comparing our method with OFL [35],
an expensive graphical model based approach, we achieve
better accuracy on both DAVIS 2016 and YoutubeOb-
jects. Comparing with deep learning approaches without
model ﬁne-tuning, and therefore, similar speed as ours, our
method achieves the best accuracy on both DAVIS 2016 and
YoutubeObjects. Comparing with the four approaches using
model ﬁne-tuning on target videos (PLM, MaskTrack, SFL,
and OSVOS), our approach achieves better performance
than PLM and MaskTrack, and is on-par with SFL. OS-
VOS achieves higher accuracy but it also utilizes a bound-
ary snapping approach which contributes 2.4% in mean
IU. Our method is 70× faster than MaskTrack and OS-
VOS, 50× faster than SFL. We measure the running time
of MaskTrack-B, OSVOS-B, and our method on a NVIDIA

Quadro M6000 GPU using Tensorﬂow [1]. Speed of other
methods are derived from the corresponding papers 1.

In our method, the adaptation of the segmentation model
by the modulators is done with one forward pass for visual
modulator, so it is much more efﬁcient than the approaches
with model ﬁne-tuning on target videos. The visual modu-
lator only needs to be computed once for the whole video,
while the spatial modulator needs to be computed for every
frame but the overhead is negligible, i.e., the average speed
of our model on a video sequence is about the same as FCN
itself. Our method is the second fastest of all compared
methods, with only MaskTrack-B and OSVOS-B achieving
similar speed but with much worse accuracies.

4.1.2 DAVIS 2017

To further investigate the capability of our model, we con-
duct more experiments on DAVIS 2017 [27], which is the
largest video segmentation dataset to date. DAVIS 2017 is
more challenging than DAVIS 2016 and YoutubeObjects in
that it has multiple objects for each video sequences and
some of the objects are very similar. We compare our
method with two most related approaches, MaskTrack [23]
and OSVOS [2]. For fair comparison, we only use their
single network and adds-on free versions. We directly use
open source code of OSVOS and adapt MaskTrack model
to Tensorﬂow [1]. For each video sequence, OSVOS and
MaskTrack are ﬁnetuned with 1000 iterations. To show
that network modulation is capable of adapting different
model structures to speciﬁc object instances, we also ex-
periment with modiﬁed OSVOS and MaskTrack models by
adding a visual modulator to each of them, which are named
OSVOS-M and MaskTrack-M respectively. For these two
models, we only update the weights of the visual modula-
tors and keep the weights of the segmentation model ﬁxed
in training.

Table 2 shows the results of different approaches on
DAVIS 2017. We utilize the ofﬁcial evaluation metrics of
DAVIS dataset: mean, recall, and decay of region simi-
larity J and contour accuracy F, respectively. Note J
mean is equivalent to mean IU we used above. Again,
our model outperforms OSVOS-B and MaskTrack-B with
a large margin, while obtaining comparable performance
with the two methods with model ﬁne-tuning. OSVOS-M
and MaskTrack-M are both better than their baseline im-
plementations with a 18% and 9.3% gain in J mean, re-
spectively. Since the weights of the segmentation model
are ﬁxed, the accuracy gain comes solely from the modu-
lator, which proves that the visual modulator is capable of
improving different model structures by manipulating the
scales of the intermediate feature maps. Noticeably, our

1Speed of ConvGRU is estimated with the expensive optical ﬂow they

use, speed of PLM is derived through communication with the authors.

Table 1. Performance comparison of our approach with recent approaches on DAVIS 2016 and YoutubeObjects. Performance measured in
mean IU.

DAVIS 16 YoutubeObjs with FT OptFlow CRF

Method
OFL [35]
BVS [22]
ConvGRU[34]
VPN[18]
MaskTrack-B [23]
SFL-B [4]
OSVOS-B [2]
Ours (Stage 1)
Ours (Stage 1&2)
PLM [32]
MaskTrack [23]
SFL [4]
OSVOS [2]

68.0
60.0
70.1
70.2
63.2
67.4
52.5
72.2
74.0
70.0
69.8
74.8
79.8

67.5
58.4
-
-
66.5
-
44.7
66.4
69.0
-
71.7
-
74.1

-
-
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:51)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:55)

(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

Speed (s)
42.2
0.37
20
0.63
0.24
0.30
0.14
0.14
0.14
0.50
12
7.9
10

Figure 3. Some qualitative results of our approach compared with two recent state-of-the-art approaches on DAVIS 2017.

method obtains much lower decay rate for both region simi-
larity and contour accuracy compared to OSVOS and Mask-
Track. The accuracy changes of the different methods over
time are illustrated in Fig. 4. In the beginning of the video,
our method lags behind OSVOS and MaskTrack. However,
when it proceeds to around 40% of the video, our method
is on par with OSVOS and outperforms MaskTrack towards
the end of the video. With one-shot ﬁne-tuning, OSVOS
and MaskTrack ﬁt to the ﬁrst frame very well. They are
able to obtain high accuracy in the beginning of the video
since these frames are all similar to the ﬁrst one. But as time
goes on and the object turns into different poses and appear-
ances, it gets harder for the ﬁne-tuned model to generalize
to new object appearances. Our model is more robust to
the appearance changes since it learns a feature embedding

(see Section 4.2) for the annotated object which is more tol-
erant to pose and appearance changes compared to one-shot
ﬁne-tuning.

Some qualitative results of our methods compared with
the two previous approaches are shown in Fig. 3. Com-
pared with MaskTrack, our method generally obtains more
accurate boundaries, partially due to that the coarse spatial
prior forces the model to explore more cues on the image
rather than the mask in the previous frame. Compared with
OSVOS, our method shows better results when there are
multiple similar objects in the image, thanks to the tracking
capability provided by the spatial modulator. On the other
hand, our method is also shown to work well on unseen ob-
ject categories in training data. In Fig. 3, the camel and the
pigs are unseen object categories in MS-COCO dataset.

Table 2. Comparisons of our approach and two state-of-the-art algorithm on DAVIS 2017 validation set.

Method
OSVOS-B [2]
MaskTrack-B [23]
OSVOS-M
MaskTrack-M
OSVOS [2]
MaskTrack [23]
Ours

with FT J mean↑ J recall↑ J decay↓ F mean↑ F recall↑ F decay↓
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:55)

30.0
36.4
39.5
47.6
62.1
57.3
57.1

0.1
42.0
9.1
27.9
29.3
29.1
24.3

-0.8
39.3
14.8
27.1
28.2
28.3
21.5

20.0
36.0
35.3
49.3
71.3
65.5
66.1

15.9
37.8
34.8
48.7
60.2
59.7
60.9

18.5
35.3
36.4
44.6
55.1
51.2
52.5

Figure 4. The J mean performance of different methods over time
on DAVIS 2017. Best viewed in color.

Figure 6. Histograms of standard deviations of γc from the visual
modulator in different modulation layers. The annotated names
are the corresponding convolution layers in VGG16.

Figure 5. Visualization of learned modulation parameters for 100
objects from 10 categories: bicycle, motorcycle, car, bus, truck,
dog, cat, horse, cow, person. Zoom in to see details.

4.2. Visualization of the modulation parameters

Figure 7. Histograms of magnitude of ˜γc from the spatial modu-
lator in different modulation layers. The annotated names are the
corresponding convolution layers in VGG16.

Our model implicitly learns an embedding with the mod-
ulation parameters from the visual modulator for the anno-
tated objects. Intuitively, similar objects should have similar
modulation parameters, while different objects should have
dramatically different modulation parameters. To visual-

ize this embedding, we extract modulation parameters from
100 object instances in 10 object classes in MS-COCO, and
visualize the parameters in a two-dimensional embedding
space using multi-dimensional scaling in Fig. 5. We can see
that objects in the same category are mostly clustered to-

Table 3. Ablation study of our method on DAVIS 2017.

4.3. Ablation Study

Add-on

Model

Data

Variants
Ours + Online ﬁnetuning
Ours + CRF
Ours
no visual modulator
no spatial modulator
- random crop
- visual guide augmentation
- spatial guide augmentation

+8.3
+1.9

mIU ∆ mIU
60.8
54.4
52.5
33.0
40.1
50.6
49.5
35.6

-19.5
-12.4
-1.9
-1.1
-13.9

gether, and similar categories are closer to each other than
dissimilar categories. For example, cats and dogs, cars and
buses are mixed up due to their similar appearance, while
bicycles and dogs, buses and horses are far from each other
due to the big visual difference. Mammal classes (cats,
dogs, cows, horses, human) are generally clustered together,
and man-made objects (cars, buses, bicycles, motorcycles,
trucks) are clustered together.

We also investigate the magnitude of the modulation pa-
rameters in different layers. The modulation parameters
{γc} changes according to the visual guide. Therefore, we
compute the standard deviations of modulation parameters
{γc} in each modulation layer for images in MS-COCO val-
idation set and illustrate them in Fig. 6. An interesting ob-
servation is that towards deeper level of the network, the
variations of modulation parameters get larger. This shows
that the manipulation of feature maps is more dramatic in
the last few layers than in early layers of the network. The
last few layers of a deep neural network usually learn high-
level semantic meanings [37], which could be used to ad-
just the segmentation model to a speciﬁc object more effec-
tively.

We also look into the spatial modulator by extracting the
scale parameters {˜γc} in each layer of the spatial modula-
tor and visualize them in Fig. 7. The magnitudes of {˜γc}
are the relative scales of the spatial guide added to the fea-
ture maps in the FCN. The scale of {˜γc} is proportional to
the impact of spatial prior on the intermediate feature maps.
Interestingly, we observe sparsity in the values of {˜γc}. Ex-
cept the last convolution layer conv5 3, around 60% of
the parameters have zero values, which means only 40% of
the feature maps are affected by the spatial prior in these
layers. In the layer conv5 3, around 70% of the feature
maps interact with the spatial guide and most of them are
added with a similar scale (note the peak around 0.4) of the
spatial guide. This shows that the spatial prior is fused into
the feature maps gradually, rather than being effective at the
beginning of the network. After all feature extractions are
done, the spatial modulator makes a large adjustment to the
feature maps, which provides a strong prior of the location
of the target object.

We study the impact of different ingredients in our
method. We conduct experiments on DAVIS 2017 and mea-
sure the performance using mean IU. For variants of model
structures, we experiment with only using spatial or visual
modulator. For data augmentation methods, we experiment
with no random crop augmentation for the FCN input, and
no afﬁne transformation for the visual guide and the spa-
tial guide. We experiment with CRF as a post-processing
step. To investigate the effect of one-shot ﬁne-tuning on
our model, we also experiment with standard one-shot ﬁne-
tuning using a small number of iterations. Results are
shown in Table 3.

By adding a CRF post-processing, our method achieves
mIU (mean IU) of 54.4. By one-shot ﬁne-tuning with only
100 iterations for each sequence, our method achieves mIU
of 60.8, which is 5.7 better than OSVOS with 1000 itera-
tions. With ﬁne-tuning, our method is still relatively efﬁ-
cient with average running time around 1 s/frame. With-
out visual modulator, our model deteriorates to 33.0, while
without spatial modulator, our model obtains mIU of 40.1,
which shows that the visual guide is more important than the
spatial guide. For data augmentation, without random crop,
the accuracy drops by 1.9. Without afﬁne data augmenta-
tion on the visual guide, the accuracy further decreases by
1.1. Without augmentation on the spatial guide, our model
only obtains mIU of 35.6, which is a dramatic drop from
49.5. The results indicates that the spatial guide augmen-
tation is the most signiﬁcant on the performance. Without
perturbation, the model might rely on the location of the
spatial prior too much that it cannot deal with moving ob-
jects in real video sequences.

5. Conclusions

In this work, we propose a novel framework to process
one-shot video segmentation efﬁciently. To alleviate the
slow speed of one-shot ﬁne-tuning developed by previous
FCN-based methods, we propose to use a network modula-
tion approach mimicking the ﬁne-tuning process with one
forward pass of the modulator network. We show in experi-
ments that by injecting a limited number of parameters com-
puted by the modulators, the segmentation model can be re-
purposed to segment an arbitrary object. The proposed net-
work modulation method is a general learning method for
few-shot learning problems, which could be applied to other
tasks such as visual tracking and image stylization. Our
approach falls into the general category of meta-learning,
and it would also be interesting to investigate other meta-
learning approaches for video segmentation. Another piece
of future work would be to learn a recurrent representation
of the modulation parameters to manipulate the FCN based
on temporal information.

References

[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-
mawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,
R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e,
R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,
J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker,
V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. War-
den, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. Tensor-
Flow: Large-scale machine learning on heterogeneous sys-
tems, 2015. Software available from tensorﬂow.org. 5
[2] S. Caelles, K.-K. Maninis, J. Pont-Tuset, L. Leal-Taix´e,
D. Cremers, and L. Van Gool. One-shot video object seg-
mentation. In CVPR, 2017. 1, 2, 4, 5, 6, 7

[3] Y. Chen, M. W. Hoffman, S. G. Colmenarejo, M. Denil,
T. P. Lillicrap, M. Botvinick, and N. de Freitas. Learning to
learn without gradient descent by gradient descent. In ICML,
2016. 2

[4] J. Cheng, Y.-H. Tsai, S. Wang, and M.-H. Yang. Segﬂow:
Joint learning for video object segmentation and optical
ﬂow. In IEEE International Conference on Computer Vision
(ICCV), 2017. 1, 2, 5, 6

[5] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei.

Deformable convolutional networks. ICCV, 2017. 2

[6] H. de Vries, F. Strub, J. Mary, H. Larochelle, O. Pietquin,
and A. C. Courville. Modulating early visual processing by
language. CoRR, abs/1707.00683, 2017. 3

[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009. 4

[8] V. Dumoulin, J. Shlens, and M. Kudlur. A learned represen-

tation for artistic style, 2017. 2

[9] Q. Fan, F. Zhong, D. Lischinski, D. Cohen-Or, and B. Chen.
Jumpcut:non-successive mask transfer and interpolation for
video cutout. In ACM Trans. Graph., 34(6), 2015. 2

[10] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-
In ICML,

learning for fast adaptation of deep networks.
2017. 2

[11] G. Ghiasi, H. Lee, M. Kudlur, V. Dumoulin, and J. Shlens.
exploring the structure of a real-time, arbitrary neural artistic
stylization network. In BMVC, 2017. 2

[12] B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Hyper-
columns for object segmentation and ﬁne-grained localiza-
tion. In CVPR, 2015. 2, 4

[13] B. Hariharan and R. Girshick. Low-shot visual recognition

by shrinking and hallucinating features. In ICCV, 2017. 2

[14] X. Huang and S. J. Belongie. Arbitrary style transfer
in real-time with adaptive instance normalization. CoRR,
abs/1703.06868, 2017. 2, 3

[15] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. Flownet 2.0: Evolution of optical ﬂow estimation
with deep networks. CVPR, 2017. 1

[16] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial

transformer networks. In NIPS, pages 2017–2025, 2015. 2

[17] S. D. Jain and K. Grauman. Supervoxel-consistent fore-

ground propagation in video. In ECCV, 2014. 2, 5

[18] V. Jampani, R. Gadde, and P. V. Gehler. Video propagation

networks. In CVPR, 2017. 2, 5, 6

[19] P. Kr¨ahenb¨uhl and V. Koltun. Efﬁcient inference in fully
connected crfs with gaussian edge potentials. In NIPS, pages
109–117, 2011. 2, 5

[20] F. Li, T. Kim, A. Humayun, D. Tsai, and J. M. Rehg. Video
segmentation by tracking many ﬁgure-ground segments. In
ICCV, 2013. 1

[21] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In ECCV, 2014. 4, 5

[22] N. M¨arki, F. Perazzi, O. Wang, and A. Sorkine-Hornung. Bi-
lateral space video segmentation. In CVPR, 2016. 1, 2, 5, 6
[23] F. Perazzi, A. Khoreva, R. Benenson, B. Schiele, and
A.Sorkine-Hornung. Learning video object segmentation
from static images. In CVPR, 2017. 1, 2, 4, 5, 6, 7

[24] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool,
M. Gross, and A. Sorkine-Hornung. A benchmark dataset
and evaluation methodology for video object segmentation.
In CVPR, 2016. 1, 5

[25] F. Perazzi, O. Wang, M. Gross, and A. Sorkine-Hornung.
Fully connected object proposals for video segmentation. In
ICCV, 2015. 2

[26] E. Perez, H. de Vries, F. Strub, V. Dumoulin, and A. C.
Courville. Learning visual reasoning without strong priors.
CoRR, abs/1707.03017, 2017. 2, 3

[27] J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbel´aez, A. Sorkine-
Hornung, and L. Van Gool. The 2017 davis challenge on
video object segmentation. arXiv:1704.00675, 2017. 4, 5

[28] S. Ravi and H. Larochelle. Optimization as a model for few-

shot learning. ICLR, 2017. 2

[29] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
Epicﬂow: Edge-preserving interpolation of correspondences
for optical ﬂow. In CVPR, 2015. 1

[30] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and
T. Lillicrap. Meta-learning with memory-augmented neural
networks. In ICML, 2016. 2

[31] E. Shelhamer, J. Long, and T. Darrell. Fully convolutional
networks for semantic segmentation. IEEE transactions on
pattern analysis and machine intelligence, 39(4):640–651,
2017. 1

[32] J. Shin Yoon, F. Rameau, J. Kim, S. Lee, S. Shin, and
I. So Kweon. Pixel-level matching for video object segmen-
tation using convolutional neural networks. In CVPR, 2017.
1, 2, 5, 6

[33] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 3, 4

[34] P. Tokmakov, K. Alahari, and C. Schmid. Learning video
object segmentation with visual memory. In ICCV, 2017. 2,
5, 6

[35] Y.-H. Tsai, M.-H. Yang, and M. J. Black. Video segmenta-

tion via object ﬂow. In CVPR, 2016. 1, 2, 5, 6

[36] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and
D. Wierstra. Matching networks for one shot learning. In
NIPS, 2016. 2

[37] M. D. Zeiler and R. Fergus. Visualizing and understanding

convolutional networks. In ECCV, 2014. 8

Efﬁcient Video Object Segmentation via Network Modulation

Linjie Yang1

Yanran Wang2∗

Xuehan Xiong3

Jianchao Yang1

Aggelos K. Katsaggelos2

1Snap Inc.
2Image and Video Processing Laboratory, Northwestern University
3Google Inc.
{linjie.yang,jianchao.yang}@snap.com {joycewang1026@u,aggk@eecs}.northwestern.edu xiong828@gmail.com

8
1
0
2
 
b
e
F
 
4
 
 
]

V
C
.
s
c
[
 
 
1
v
8
1
2
1
0
.
2
0
8
1
:
v
i
X
r
a

Abstract

Video object segmentation targets at segmenting a spe-
ciﬁc object throughout a video sequence, given only an an-
notated ﬁrst frame. Recent deep learning based approaches
ﬁnd it effective by ﬁne-tuning a general-purpose segmen-
tation model on the annotated frame using hundreds of it-
erations of gradient descent. Despite the high accuracy
these methods achieve, the ﬁne-tuning process is inefﬁcient
and fail to meet the requirements of real world applica-
tions. We propose a novel approach that uses a single
forward pass to adapt the segmentation model to the ap-
pearance of a speciﬁc object. Speciﬁcally, a second meta
neural network named modulator is learned to manipu-
late the intermediate layers of the segmentation network
given limited visual and spatial information of the target
object. The experiments show that our approach is 70×
faster than ﬁne-tuning approaches while achieving similar
accuracy. Our model and code are released at https:
//github.com/linjieyangsc/video_seg.

1. Introduction

Semantic segmentation plays an important role in un-
derstanding visual content of an image as it assigns pre-
deﬁned object or scene labels to each pixel and thus trans-
lates the image into a segmentation map. When dealing with
video content, a human can easily segment an object in the
whole video without knowing its semantic meaning, which
inspired a research topic named semi-supervised video seg-
mentation. In a typical scenario of semi-supervised video
segmentation, one is given the ﬁrst frame of a video along
with an annotated object mask, and the task is to accurately
locate the object in all following frames [24, 20]. The ability
of performing accurate pixel-level video segmentation with
minimum supervision (e.g., one annotated frame) can fos-
ter a large amount of applications, such as accurate object

∗This work is done when Yanran was an intern at Snap Inc.

Figure 1. An overview of our approach. Our model is consisted of
a modulator and a segmentation network. The modulator can adapt
the segmentation model instantly to segment an arbitrary object
through a video sequence.

tracking for video understanding, interactive video editing,
augmented reality, and video-based advertisement. When
the supervision is limited to only one annotated frame, re-
searchers refer to this scenario as one-shot learning. In the
recent years, we have witnessed a rising amount of interests
in developing one-shot learning techniques for video seg-
mentation [2, 23, 35, 22, 32, 4]. Most of these work share a
similar two-stage paradigm: ﬁrst, train a general-purpose
Fully Convolutional Network (FCN) [31] to segment the
foreground object; Second, ﬁne-tune this network based on
the ﬁrst frame of the video for several hundred forward-
backward iterations to adapt the model to the speciﬁc video
sequence. Despite the high accuracies achieved by these ap-
proaches, the ﬁne-tuning process is arguably time consum-
ing, which makes it prohibited for real-time applications.
Some of these approaches [4] [23] also utilize optical ﬂow
information, which is computationally heavy for state-of-
the-art algorithms[29] [15].

1

In order to alleviate the computational cost of semi-
supervised segmentation, we propose a novel approach to
adapt the generic segmentation network to the appearance
of a speciﬁc object instance in one single feed-forward pass.
We propose to employ another meta neural network called
modulator to learn to adjust the intermediate layers of the
generic segmentation network given an arbitrary target ob-
ject instance. Fig. 1 shows an illustration of our approach.
By extracting information from the image of the annotated
object and the spatial prior of the object, the modulator pro-
duces a list of parameters, which are injected into the seg-
mentation model for layer-wise feature manipulation. With-
out one-shot ﬁne-tuning, our model is able to change the
behavior of the segmentation network with minimum ex-
tracted information from the target object. We name this
process network modulation.

Our proposed model is efﬁcient, requiring only one for-
ward pass from the modulator to produce all parameters
needed for the segmentation model to adapt to the speciﬁc
object instance. Network modulation guided by the spatial
prior facilitates the model to track the object even with the
presence of multiple similar instances. The whole pipeline
is differentiable and can be learned end-to-end using the
standard stochastic gradient descent. The experiments show
that our approach outperforms previous approaches without
one-shot ﬁne-tuning by a large margin, and achieves com-
parable performance with these approaches after one-shot
ﬁne-tuning with a 70× speed up.

2. Related Work

Semi-supervised video segmentation. Semi-supervised
video object segmentation aims at tracking an object mask
given from the ﬁrst annotated frame throughout the rest of
video. Many approaches have been proposed in the lit-
erature, including those propagating superpixels [17] [35],
patches [9], object proposals [25], or in bilateral space [22],
and graphical model based optimization is usually per-
formed to consider multiple frames simultaneously. With
the success of FCN on static image segmentation [12],
deep learning based methods [23, 2, 32, 34, 18, 4] have
been recently proposed for video segmentation and promis-
ing results have been achieved. To model the temporal
motion information, some works heavily rely on optical
ﬂow [34] [4], and use CNNs to learn mask reﬁnement of
an object from current frame to the next one [23], or com-
bine the training of CNN with bilateral ﬁltering between
adjacent frames [18]. Chen et al. [4] use a CNN to jointly
estimate the optical ﬂow and provide the learned motion
representation to generate motion consistent segmentation
across time. Different from these approaches, Caelles et
al. [2] combine ofﬂine and online training process on static
images without using temporal information. While it saves
the computation of optical ﬂow and/or conditional random

ﬁelds (CRF) [19] involved in some previous methods, on-
line ﬁne-tuning still requires many iterations of optimiza-
tion, which poses a challenge for real-world applications
that need rapid inference.

Meta-learning for low-shot learning. Current success of
deep learning relies on the ability of learning from large-
scale labeled datasets through gradient descent optimiza-
tion. However, if we want our model to learn many tasks
adapted to many environments, it is not affordable to learn
each task for each setting from scratch. Instead, we want
our deep learning system to be able to learn new tasks very
fast and from very limited quantities of data. In the extreme
of “one-shot learning”, the algorithm needs to learn the new
task with a single observation. One potential strategy for
learning a versatile model is the notion of meta-learning,
or learning to learn, which can date back to the late 80s.
Recently, meta-learning has become a hot research topic
with publications on neural network optimization [3], ﬁnd-
ing good network architectures, fast reinforcement learning,
and few-shot image recognition [36, 28, 13, 10, 30]. Ravi
and Larochelle [28] proposed a LSTM meta-learner to learn
the update rules for few shot learning. The meta optimiza-
tion over a large number of tasks in [10] targets at learning
a model that can quickly adapt to the new task with limited
number of updates. Hariharan and Girschick [13] trained a
learner that generated new samples and used new samples
for training new tasks. Our approach shares the similarity
with meta-learning that it learns to update the segmentation
model rapidly with another meta learner, i.e. the modulator.

Network manipulation Several previous work try to in-
corporate modules to manipulate the behavior of a deep
neural network, either to manipulate spatial arrangement of
data [16] or ﬁlter connections [5]. Our method is also heav-
ily motivated by conditional batch normalization [8, 11, 14,
26], where the behavior of the deep model is manipulated by
batch normalization parameters conditioned on a guidance
input, e.g. a style image for image stylization or a language
sentence for visual question answering.

3. Video Object Segmentation with Network

Modulation

In our proposed framework, we utilize modulators to in-
stantly adapt the segmentation network to a speciﬁc object,
rather than performing hundreds of iterations of gradient
descent. We can achieve similar accuracy by adjusting a
limited number of parameters in the segmentation network,
compared with the updating the whole network in one-shot
learning approaches [23, 2]. There are two important cues
for video object segmentation: visual appearance and con-
tinuous motion in space. To use information from both vi-

Figure 2. An illustration of our model with three components: a segmentation network, a visual modulator, and a spatial modulator. The
two modulators produce a set of parameters that manipulates the intermediate feature maps of the segmentation network and adapt it to
segment the speciﬁc object.

sual and spatial domains, we incorporate two network mod-
ulators, namely visual modulator and spatial modulator, to
learn to adjust intermediate layers in the main segmentation
network, based on the annotated ﬁrst frame and spatial lo-
cation of the object, respectively.

3.1. Conditional batch normalization

Our approach is inspired by recent works using Condi-
tional Batch Normalization (CBN) [6, 14, 26], where the
scale and bias parameters of each batch-normalization layer
are produced by a second controller network. These param-
eters are used to control the behavior of the main network
for tasks such as image stylization and question answering.
Mathematically, each CBN layer can be formulated as fol-
lows:

yc = γcxc + βc,

(1)

where xc and yc are the input and output feature maps in the
cth channel, and γc and βc are the scale and bias parameters
produced by the controller network, respectively. The mean
and variance parameters are omitted for clarity.

3.2. Visual and spatial modulation

The CBN layer is a special case of the more general
scale-and-shift operation on feature maps. Following each
convolution layer, we deﬁne a new modulation layer with
parameters generated by both visual and spatial modula-
tors that are jointly trained. We design the two modula-
tors such that the visual modulator produces channel-wise
scale parameters to adjust the weights of different channels

in the feature maps, while the spatial modulator generates
element-wise bias parameters to inject spatial prior to the
modulated features. Speciﬁcally, our modulation layer can
be formulated as follows:

yc = γcxc + βc,

(2)

where γc and βc are modulation parameters from the vi-
sual and spatial modulators, respectively. γc is a scalar for
channel-wise weighting, while βc is a two-dimensional ma-
trix to apply point-wise bias values.

Fig. 2 shows an illustration of the proposed approach,
which consists of three networks: a fully-convolutional
main segmentation network, a visual modulator network,
and a spatial modulator network. The visual modulator net-
work is a CNN that takes the annotated visual object image
as input and produces a vector of scale parameters for all
modulation layers, while the spatial modulator network is a
very efﬁcient network that produces bias parameters based
on the spatial prior input. We will discuss the two modula-
tors in more detail in the following sections.

3.3. Visual modulator

The visual modulator is used to adapt the segmentation
network to focus on a speciﬁc object instance, which is the
annotated object in the ﬁrst frame. The annotated object is
referred to as visual guide hereafter for convenience. The
visual modulator extracts semantic information such as cat-
egory, color, shape, and texture, from the visual guide and
generates corresponding channel-wise weights so as to re-
target the segmentation network to segment the object. We

use VGG16 [33] neural network as the model for the vi-
sual modulator. We modify its last layer trained for Ima-
geNet classiﬁcation to match the number of parameters in
the modulation layers for the segmentation network.

The visual modulator implicitly learns an embedding of
different types of objects. It should produce similar param-
eters to adjust the segmentation network for similar objects
while different parameters for different objects. This is in-
deed true as we show in Sec. 4.2 that the embedding of the
modulator outputs correlates with object appearance very
well. One big advantage of using such a visual modulator is
that we can potentially transfer the knowledge learned with
a large number of object classes, e.g., ImageNet, in order to
learn a good embedding.

3.4. Spatial modulator

Our spatial modulator takes a prior location of the ob-
ject in the image as input. Since objects move continu-
ously in a video, we set the prior to be the predicted loca-
tion of the object mask in the previous frame. Speciﬁcally,
we encode the location information as a heatmap with a
two-dimensional Gaussian distribution on the image plane.
The center and standard deviations of the Gaussian distribu-
tion are computed from the predicted mask of the previous
frame. This heatmap is referred as spatial guide hereafter
for convenience. The spatial modulator downsamples the
spatial guide into different scales, to match the resolution
of different feature maps in the segmentation network, and
then applies a scale-and-shift operation on each downsam-
pled heatmap to generate the bias parameters of the corre-
sponding modulation layer. Mathematically,

βc = ˜γcm + ˜βc

(3)

where m is a down-sampled Gaussian heat map for the cor-
responding modulation layer, ˜γc and ˜βc are the scale-and-
shift parameters for the c-th channel, respectively. This is
implemented with a computationally efﬁcient 1 × 1 convo-
lution. In the bottom of Fig. 2, we illustrate the structure of
the spatial modulator.

Our method shares some similarities with the previous
work MaskTrack [23] in utilizing information from the pre-
vious mask. Comparing with their approach that uses the
exact foreground mask of the previous frame, we only use a
very coarse location prior.
It may seem that our method
throws away more information from the previous frame.
However, we argue that the rough position and size in the
previous frame possess enough information to infer the ob-
ject mask with the RGB image, and it prevents the model
from relying too much on the mask and as a result the error
propagation, which can be catastrophic when the object has
large movements in the video. As a drawback of such over-
utilization of the mask, MaskTrack has to apply plenty of
well-engineered data augmentation to prevent over-ﬁtting,

while we only apply simple shift and scaling as augmenta-
tion.

3.5. Implementation details

Our FCN structure follows the one used by [2], which
is a VGG16 [33] model with a hyper-column structure [12].
Intuitively, we should add modulation layers after each con-
volution layer in the FCN. However, we found that adding
modulation layers in-between the early convolution layers
actually makes the model perform worse. One possible rea-
son is that early layers extract low-level features that are
very sensitive to the scale-and-shift operations introduced
by the modulator. In our implementation, we add modula-
tion operations to all convolution layers in VGG16 except
the ﬁrst four layers, which results in nine modulation layers.
Similar to MaskTrack [23], we also utilize static im-
Ideally, the visual modula-
ages for training our model.
tor should learn a mapping from any object to modulation
weights of different layers in a FCN, which requires the
model to see all possible different objects. However, most
video semantic segmentation datasets only contain a very
limited number of categories. We tackle this challenge by
using the largest public semantic segmentation dataset MS-
COCO [21], which has 80 object categories. We select ob-
jects that are larger than 3% of the image size for training,
resulting in a total number of 217, 516 objects. For prepro-
cessing the input for the visual modulator, we ﬁrst crop the
object using the annotated mask, then set the background
pixels to mean image values, and then resize the cropped
image to a constant resolution of 224 × 224. The object
is also augmented with up to 10% random scaling and 10◦
random rotation. For preprocessing the spatial guide as in-
put to the spatial modulator, we ﬁrst compute the mean and
standard deviation of the mask, and then augment the mask
with up to 20% random shift and 40% random scaling. For
the whole image fed into the FCN, we use a random size
from 320, 400, and 480 with a square shape.

The visual modulator and segmentation network are
both initialized with VGG16 model pretrained on the Im-
ageNet [7] classiﬁcation task. The modulation parameters
{γc} are initialized to ones by setting the weights and biases
of the last fully-connected layer of the visual modulator to
zeros and ones, respectively. The weights of spatial modu-
lator are initialized randomly. We used the same balanced
cross-entropy loss as in [2]. A mini-batch size of 8 is used.
We use Adam optimizer with default momentum 0.9 and
0.999 for β1 and β2, respectively. The model is ﬁrst trained
for 10 epochs with learning rate 10−5 and then trained for
another 5 epochs with learning rate 10−6.

Further, in order to model appearance variations of mov-
ing objects in videos, the model can be ﬁnetuned on video
segmentation dataset such as DAVIS 2017 [27]. To be more
robust to appearance variations, we randomly pick a fore-

ground object from the whole video sequence as the visual
guide for each frame. The spatial guide is obtained from
the ground truth mask of the object in the previous frame.
The same data augmentations are applied as training on MS-
COCO. The model is ﬁnetuned for 20 epochs with learning
rate 10−6.

4. Experiments

In this section, we will introduce three parts of experi-
ment: the comparison of our approach with previous meth-
ods, the visualization of the modulation parameters, and ab-
lation study. Our model is trained on MS-COCO [21] 2017
dataset, and is tested on several popular video segmenta-
tion datasets, including DAVIS [24] [27] and YoutubeOb-
jects [17].

4.1. Semi-supervised Video Segmentation

In this section, we compare with traditional approaches
including OFL [35], BVS[22], and deep learning-based
approaches including PLM [32], MaskTrack [23], OS-
VOS [2], VPN [18], SFL [4], and ConvGRU [34].

4.1.1 DAVIS 2016 & YoutubeObjects

First, we compare our approach with previous approaches
on DAVIS 2016 and YoutubeObjects. Some approaches
(MaskTrack[23], SFL [4] and OSVOS[2]) reported results
both with and without model ﬁne-tuning on the target se-
quences. We include both of them and denote the variants
without ﬁne-tuning as MaskTrack-B, SFL-B, and OSVOS-
B, respectively. Our model has two variants,with the ﬁrst
only trained on static images (Stage 1) and the second ﬁne-
tuned on video data (Stage 1&2). Since there are several
popular add-ons for this line of research, such as optical
ﬂow and CRF [19], which both have a lot of variants and
make a fair comparison hard, we only include the perfor-
mances without optical ﬂow and CRF if possible, and mark
those with add-ons in Table 1.

In Table 1, by comparing our method with OFL [35],
an expensive graphical model based approach, we achieve
better accuracy on both DAVIS 2016 and YoutubeOb-
jects. Comparing with deep learning approaches without
model ﬁne-tuning, and therefore, similar speed as ours, our
method achieves the best accuracy on both DAVIS 2016 and
YoutubeObjects. Comparing with the four approaches using
model ﬁne-tuning on target videos (PLM, MaskTrack, SFL,
and OSVOS), our approach achieves better performance
than PLM and MaskTrack, and is on-par with SFL. OS-
VOS achieves higher accuracy but it also utilizes a bound-
ary snapping approach which contributes 2.4% in mean
IU. Our method is 70× faster than MaskTrack and OS-
VOS, 50× faster than SFL. We measure the running time
of MaskTrack-B, OSVOS-B, and our method on a NVIDIA

Quadro M6000 GPU using Tensorﬂow [1]. Speed of other
methods are derived from the corresponding papers 1.

In our method, the adaptation of the segmentation model
by the modulators is done with one forward pass for visual
modulator, so it is much more efﬁcient than the approaches
with model ﬁne-tuning on target videos. The visual modu-
lator only needs to be computed once for the whole video,
while the spatial modulator needs to be computed for every
frame but the overhead is negligible, i.e., the average speed
of our model on a video sequence is about the same as FCN
itself. Our method is the second fastest of all compared
methods, with only MaskTrack-B and OSVOS-B achieving
similar speed but with much worse accuracies.

4.1.2 DAVIS 2017

To further investigate the capability of our model, we con-
duct more experiments on DAVIS 2017 [27], which is the
largest video segmentation dataset to date. DAVIS 2017 is
more challenging than DAVIS 2016 and YoutubeObjects in
that it has multiple objects for each video sequences and
some of the objects are very similar. We compare our
method with two most related approaches, MaskTrack [23]
and OSVOS [2]. For fair comparison, we only use their
single network and adds-on free versions. We directly use
open source code of OSVOS and adapt MaskTrack model
to Tensorﬂow [1]. For each video sequence, OSVOS and
MaskTrack are ﬁnetuned with 1000 iterations. To show
that network modulation is capable of adapting different
model structures to speciﬁc object instances, we also ex-
periment with modiﬁed OSVOS and MaskTrack models by
adding a visual modulator to each of them, which are named
OSVOS-M and MaskTrack-M respectively. For these two
models, we only update the weights of the visual modula-
tors and keep the weights of the segmentation model ﬁxed
in training.

Table 2 shows the results of different approaches on
DAVIS 2017. We utilize the ofﬁcial evaluation metrics of
DAVIS dataset: mean, recall, and decay of region simi-
larity J and contour accuracy F, respectively. Note J
mean is equivalent to mean IU we used above. Again,
our model outperforms OSVOS-B and MaskTrack-B with
a large margin, while obtaining comparable performance
with the two methods with model ﬁne-tuning. OSVOS-M
and MaskTrack-M are both better than their baseline im-
plementations with a 18% and 9.3% gain in J mean, re-
spectively. Since the weights of the segmentation model
are ﬁxed, the accuracy gain comes solely from the modu-
lator, which proves that the visual modulator is capable of
improving different model structures by manipulating the
scales of the intermediate feature maps. Noticeably, our

1Speed of ConvGRU is estimated with the expensive optical ﬂow they

use, speed of PLM is derived through communication with the authors.

Table 1. Performance comparison of our approach with recent approaches on DAVIS 2016 and YoutubeObjects. Performance measured in
mean IU.

DAVIS 16 YoutubeObjs with FT OptFlow CRF

Method
OFL [35]
BVS [22]
ConvGRU[34]
VPN[18]
MaskTrack-B [23]
SFL-B [4]
OSVOS-B [2]
Ours (Stage 1)
Ours (Stage 1&2)
PLM [32]
MaskTrack [23]
SFL [4]
OSVOS [2]

68.0
60.0
70.1
70.2
63.2
67.4
52.5
72.2
74.0
70.0
69.8
74.8
79.8

67.5
58.4
-
-
66.5
-
44.7
66.4
69.0
-
71.7
-
74.1

-
-
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:51)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:55)

(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

Speed (s)
42.2
0.37
20
0.63
0.24
0.30
0.14
0.14
0.14
0.50
12
7.9
10

Figure 3. Some qualitative results of our approach compared with two recent state-of-the-art approaches on DAVIS 2017.

method obtains much lower decay rate for both region simi-
larity and contour accuracy compared to OSVOS and Mask-
Track. The accuracy changes of the different methods over
time are illustrated in Fig. 4. In the beginning of the video,
our method lags behind OSVOS and MaskTrack. However,
when it proceeds to around 40% of the video, our method
is on par with OSVOS and outperforms MaskTrack towards
the end of the video. With one-shot ﬁne-tuning, OSVOS
and MaskTrack ﬁt to the ﬁrst frame very well. They are
able to obtain high accuracy in the beginning of the video
since these frames are all similar to the ﬁrst one. But as time
goes on and the object turns into different poses and appear-
ances, it gets harder for the ﬁne-tuned model to generalize
to new object appearances. Our model is more robust to
the appearance changes since it learns a feature embedding

(see Section 4.2) for the annotated object which is more tol-
erant to pose and appearance changes compared to one-shot
ﬁne-tuning.

Some qualitative results of our methods compared with
the two previous approaches are shown in Fig. 3. Com-
pared with MaskTrack, our method generally obtains more
accurate boundaries, partially due to that the coarse spatial
prior forces the model to explore more cues on the image
rather than the mask in the previous frame. Compared with
OSVOS, our method shows better results when there are
multiple similar objects in the image, thanks to the tracking
capability provided by the spatial modulator. On the other
hand, our method is also shown to work well on unseen ob-
ject categories in training data. In Fig. 3, the camel and the
pigs are unseen object categories in MS-COCO dataset.

Table 2. Comparisons of our approach and two state-of-the-art algorithm on DAVIS 2017 validation set.

Method
OSVOS-B [2]
MaskTrack-B [23]
OSVOS-M
MaskTrack-M
OSVOS [2]
MaskTrack [23]
Ours

with FT J mean↑ J recall↑ J decay↓ F mean↑ F recall↑ F decay↓
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:55)

20.0
36.0
35.3
49.3
71.3
65.5
66.1

-0.8
39.3
14.8
27.1
28.2
28.3
21.5

30.0
36.4
39.5
47.6
62.1
57.3
57.1

18.5
35.3
36.4
44.6
55.1
51.2
52.5

15.9
37.8
34.8
48.7
60.2
59.7
60.9

0.1
42.0
9.1
27.9
29.3
29.1
24.3

Figure 4. The J mean performance of different methods over time
on DAVIS 2017. Best viewed in color.

Figure 6. Histograms of standard deviations of γc from the visual
modulator in different modulation layers. The annotated names
are the corresponding convolution layers in VGG16.

Figure 5. Visualization of learned modulation parameters for 100
objects from 10 categories: bicycle, motorcycle, car, bus, truck,
dog, cat, horse, cow, person. Zoom in to see details.

4.2. Visualization of the modulation parameters

Figure 7. Histograms of magnitude of ˜γc from the spatial modu-
lator in different modulation layers. The annotated names are the
corresponding convolution layers in VGG16.

Our model implicitly learns an embedding with the mod-
ulation parameters from the visual modulator for the anno-
tated objects. Intuitively, similar objects should have similar
modulation parameters, while different objects should have
dramatically different modulation parameters. To visual-

ize this embedding, we extract modulation parameters from
100 object instances in 10 object classes in MS-COCO, and
visualize the parameters in a two-dimensional embedding
space using multi-dimensional scaling in Fig. 5. We can see
that objects in the same category are mostly clustered to-

Table 3. Ablation study of our method on DAVIS 2017.

4.3. Ablation Study

Add-on

Model

Data

Variants
Ours + Online ﬁnetuning
Ours + CRF
Ours
no visual modulator
no spatial modulator
- random crop
- visual guide augmentation
- spatial guide augmentation

+8.3
+1.9

mIU ∆ mIU
60.8
54.4
52.5
33.0
40.1
50.6
49.5
35.6

-19.5
-12.4
-1.9
-1.1
-13.9

gether, and similar categories are closer to each other than
dissimilar categories. For example, cats and dogs, cars and
buses are mixed up due to their similar appearance, while
bicycles and dogs, buses and horses are far from each other
due to the big visual difference. Mammal classes (cats,
dogs, cows, horses, human) are generally clustered together,
and man-made objects (cars, buses, bicycles, motorcycles,
trucks) are clustered together.

We also investigate the magnitude of the modulation pa-
rameters in different layers. The modulation parameters
{γc} changes according to the visual guide. Therefore, we
compute the standard deviations of modulation parameters
{γc} in each modulation layer for images in MS-COCO val-
idation set and illustrate them in Fig. 6. An interesting ob-
servation is that towards deeper level of the network, the
variations of modulation parameters get larger. This shows
that the manipulation of feature maps is more dramatic in
the last few layers than in early layers of the network. The
last few layers of a deep neural network usually learn high-
level semantic meanings [37], which could be used to ad-
just the segmentation model to a speciﬁc object more effec-
tively.

We also look into the spatial modulator by extracting the
scale parameters {˜γc} in each layer of the spatial modula-
tor and visualize them in Fig. 7. The magnitudes of {˜γc}
are the relative scales of the spatial guide added to the fea-
ture maps in the FCN. The scale of {˜γc} is proportional to
the impact of spatial prior on the intermediate feature maps.
Interestingly, we observe sparsity in the values of {˜γc}. Ex-
cept the last convolution layer conv5 3, around 60% of
the parameters have zero values, which means only 40% of
the feature maps are affected by the spatial prior in these
layers. In the layer conv5 3, around 70% of the feature
maps interact with the spatial guide and most of them are
added with a similar scale (note the peak around 0.4) of the
spatial guide. This shows that the spatial prior is fused into
the feature maps gradually, rather than being effective at the
beginning of the network. After all feature extractions are
done, the spatial modulator makes a large adjustment to the
feature maps, which provides a strong prior of the location
of the target object.

We study the impact of different ingredients in our
method. We conduct experiments on DAVIS 2017 and mea-
sure the performance using mean IU. For variants of model
structures, we experiment with only using spatial or visual
modulator. For data augmentation methods, we experiment
with no random crop augmentation for the FCN input, and
no afﬁne transformation for the visual guide and the spa-
tial guide. We experiment with CRF as a post-processing
step. To investigate the effect of one-shot ﬁne-tuning on
our model, we also experiment with standard one-shot ﬁne-
tuning using a small number of iterations. Results are
shown in Table 3.

By adding a CRF post-processing, our method achieves
mIU (mean IU) of 54.4. By one-shot ﬁne-tuning with only
100 iterations for each sequence, our method achieves mIU
of 60.8, which is 5.7 better than OSVOS with 1000 itera-
tions. With ﬁne-tuning, our method is still relatively efﬁ-
cient with average running time around 1 s/frame. With-
out visual modulator, our model deteriorates to 33.0, while
without spatial modulator, our model obtains mIU of 40.1,
which shows that the visual guide is more important than the
spatial guide. For data augmentation, without random crop,
the accuracy drops by 1.9. Without afﬁne data augmenta-
tion on the visual guide, the accuracy further decreases by
1.1. Without augmentation on the spatial guide, our model
only obtains mIU of 35.6, which is a dramatic drop from
49.5. The results indicates that the spatial guide augmen-
tation is the most signiﬁcant on the performance. Without
perturbation, the model might rely on the location of the
spatial prior too much that it cannot deal with moving ob-
jects in real video sequences.

5. Conclusions

In this work, we propose a novel framework to process
one-shot video segmentation efﬁciently. To alleviate the
slow speed of one-shot ﬁne-tuning developed by previous
FCN-based methods, we propose to use a network modula-
tion approach mimicking the ﬁne-tuning process with one
forward pass of the modulator network. We show in experi-
ments that by injecting a limited number of parameters com-
puted by the modulators, the segmentation model can be re-
purposed to segment an arbitrary object. The proposed net-
work modulation method is a general learning method for
few-shot learning problems, which could be applied to other
tasks such as visual tracking and image stylization. Our
approach falls into the general category of meta-learning,
and it would also be interesting to investigate other meta-
learning approaches for video segmentation. Another piece
of future work would be to learn a recurrent representation
of the modulation parameters to manipulate the FCN based
on temporal information.

References

[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-
mawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,
R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e,
R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,
J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker,
V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. War-
den, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. Tensor-
Flow: Large-scale machine learning on heterogeneous sys-
tems, 2015. Software available from tensorﬂow.org. 5
[2] S. Caelles, K.-K. Maninis, J. Pont-Tuset, L. Leal-Taix´e,
D. Cremers, and L. Van Gool. One-shot video object seg-
mentation. In CVPR, 2017. 1, 2, 4, 5, 6, 7

[3] Y. Chen, M. W. Hoffman, S. G. Colmenarejo, M. Denil,
T. P. Lillicrap, M. Botvinick, and N. de Freitas. Learning to
learn without gradient descent by gradient descent. In ICML,
2016. 2

[4] J. Cheng, Y.-H. Tsai, S. Wang, and M.-H. Yang. Segﬂow:
Joint learning for video object segmentation and optical
ﬂow. In IEEE International Conference on Computer Vision
(ICCV), 2017. 1, 2, 5, 6

[5] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei.

Deformable convolutional networks. ICCV, 2017. 2

[6] H. de Vries, F. Strub, J. Mary, H. Larochelle, O. Pietquin,
and A. C. Courville. Modulating early visual processing by
language. CoRR, abs/1707.00683, 2017. 3

[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009. 4

[8] V. Dumoulin, J. Shlens, and M. Kudlur. A learned represen-

tation for artistic style, 2017. 2

[9] Q. Fan, F. Zhong, D. Lischinski, D. Cohen-Or, and B. Chen.
Jumpcut:non-successive mask transfer and interpolation for
video cutout. In ACM Trans. Graph., 34(6), 2015. 2

[10] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-
In ICML,

learning for fast adaptation of deep networks.
2017. 2

[11] G. Ghiasi, H. Lee, M. Kudlur, V. Dumoulin, and J. Shlens.
exploring the structure of a real-time, arbitrary neural artistic
stylization network. In BMVC, 2017. 2

[12] B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Hyper-
columns for object segmentation and ﬁne-grained localiza-
tion. In CVPR, 2015. 2, 4

[13] B. Hariharan and R. Girshick. Low-shot visual recognition

by shrinking and hallucinating features. In ICCV, 2017. 2

[14] X. Huang and S. J. Belongie. Arbitrary style transfer
in real-time with adaptive instance normalization. CoRR,
abs/1703.06868, 2017. 2, 3

[15] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. Flownet 2.0: Evolution of optical ﬂow estimation
with deep networks. CVPR, 2017. 1

[16] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial

transformer networks. In NIPS, pages 2017–2025, 2015. 2

[17] S. D. Jain and K. Grauman. Supervoxel-consistent fore-

ground propagation in video. In ECCV, 2014. 2, 5

[18] V. Jampani, R. Gadde, and P. V. Gehler. Video propagation

networks. In CVPR, 2017. 2, 5, 6

[19] P. Kr¨ahenb¨uhl and V. Koltun. Efﬁcient inference in fully
connected crfs with gaussian edge potentials. In NIPS, pages
109–117, 2011. 2, 5

[20] F. Li, T. Kim, A. Humayun, D. Tsai, and J. M. Rehg. Video
segmentation by tracking many ﬁgure-ground segments. In
ICCV, 2013. 1

[21] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In ECCV, 2014. 4, 5

[22] N. M¨arki, F. Perazzi, O. Wang, and A. Sorkine-Hornung. Bi-
lateral space video segmentation. In CVPR, 2016. 1, 2, 5, 6
[23] F. Perazzi, A. Khoreva, R. Benenson, B. Schiele, and
A.Sorkine-Hornung. Learning video object segmentation
from static images. In CVPR, 2017. 1, 2, 4, 5, 6, 7

[24] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool,
M. Gross, and A. Sorkine-Hornung. A benchmark dataset
and evaluation methodology for video object segmentation.
In CVPR, 2016. 1, 5

[25] F. Perazzi, O. Wang, M. Gross, and A. Sorkine-Hornung.
Fully connected object proposals for video segmentation. In
ICCV, 2015. 2

[26] E. Perez, H. de Vries, F. Strub, V. Dumoulin, and A. C.
Courville. Learning visual reasoning without strong priors.
CoRR, abs/1707.03017, 2017. 2, 3

[27] J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbel´aez, A. Sorkine-
Hornung, and L. Van Gool. The 2017 davis challenge on
video object segmentation. arXiv:1704.00675, 2017. 4, 5

[28] S. Ravi and H. Larochelle. Optimization as a model for few-

shot learning. ICLR, 2017. 2

[29] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
Epicﬂow: Edge-preserving interpolation of correspondences
for optical ﬂow. In CVPR, 2015. 1

[30] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and
T. Lillicrap. Meta-learning with memory-augmented neural
networks. In ICML, 2016. 2

[31] E. Shelhamer, J. Long, and T. Darrell. Fully convolutional
networks for semantic segmentation. IEEE transactions on
pattern analysis and machine intelligence, 39(4):640–651,
2017. 1

[32] J. Shin Yoon, F. Rameau, J. Kim, S. Lee, S. Shin, and
I. So Kweon. Pixel-level matching for video object segmen-
tation using convolutional neural networks. In CVPR, 2017.
1, 2, 5, 6

[33] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 3, 4

[34] P. Tokmakov, K. Alahari, and C. Schmid. Learning video
object segmentation with visual memory. In ICCV, 2017. 2,
5, 6

[35] Y.-H. Tsai, M.-H. Yang, and M. J. Black. Video segmenta-

tion via object ﬂow. In CVPR, 2016. 1, 2, 5, 6

[36] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and
D. Wierstra. Matching networks for one shot learning. In
NIPS, 2016. 2

[37] M. D. Zeiler and R. Fergus. Visualizing and understanding

convolutional networks. In ECCV, 2014. 8

Efﬁcient Video Object Segmentation via Network Modulation

Linjie Yang1

Yanran Wang2∗

Xuehan Xiong3

Jianchao Yang1

Aggelos K. Katsaggelos2

1Snap Inc.
2Image and Video Processing Laboratory, Northwestern University
3Google Inc.
{linjie.yang,jianchao.yang}@snap.com {joycewang1026@u,aggk@eecs}.northwestern.edu xiong828@gmail.com

8
1
0
2
 
b
e
F
 
4
 
 
]

V
C
.
s
c
[
 
 
1
v
8
1
2
1
0
.
2
0
8
1
:
v
i
X
r
a

Abstract

Video object segmentation targets at segmenting a spe-
ciﬁc object throughout a video sequence, given only an an-
notated ﬁrst frame. Recent deep learning based approaches
ﬁnd it effective by ﬁne-tuning a general-purpose segmen-
tation model on the annotated frame using hundreds of it-
erations of gradient descent. Despite the high accuracy
these methods achieve, the ﬁne-tuning process is inefﬁcient
and fail to meet the requirements of real world applica-
tions. We propose a novel approach that uses a single
forward pass to adapt the segmentation model to the ap-
pearance of a speciﬁc object. Speciﬁcally, a second meta
neural network named modulator is learned to manipu-
late the intermediate layers of the segmentation network
given limited visual and spatial information of the target
object. The experiments show that our approach is 70×
faster than ﬁne-tuning approaches while achieving similar
accuracy. Our model and code are released at https:
//github.com/linjieyangsc/video_seg.

1. Introduction

Semantic segmentation plays an important role in un-
derstanding visual content of an image as it assigns pre-
deﬁned object or scene labels to each pixel and thus trans-
lates the image into a segmentation map. When dealing with
video content, a human can easily segment an object in the
whole video without knowing its semantic meaning, which
inspired a research topic named semi-supervised video seg-
mentation. In a typical scenario of semi-supervised video
segmentation, one is given the ﬁrst frame of a video along
with an annotated object mask, and the task is to accurately
locate the object in all following frames [24, 20]. The ability
of performing accurate pixel-level video segmentation with
minimum supervision (e.g., one annotated frame) can fos-
ter a large amount of applications, such as accurate object

∗This work is done when Yanran was an intern at Snap Inc.

Figure 1. An overview of our approach. Our model is consisted of
a modulator and a segmentation network. The modulator can adapt
the segmentation model instantly to segment an arbitrary object
through a video sequence.

tracking for video understanding, interactive video editing,
augmented reality, and video-based advertisement. When
the supervision is limited to only one annotated frame, re-
searchers refer to this scenario as one-shot learning. In the
recent years, we have witnessed a rising amount of interests
in developing one-shot learning techniques for video seg-
mentation [2, 23, 35, 22, 32, 4]. Most of these work share a
similar two-stage paradigm: ﬁrst, train a general-purpose
Fully Convolutional Network (FCN) [31] to segment the
foreground object; Second, ﬁne-tune this network based on
the ﬁrst frame of the video for several hundred forward-
backward iterations to adapt the model to the speciﬁc video
sequence. Despite the high accuracies achieved by these ap-
proaches, the ﬁne-tuning process is arguably time consum-
ing, which makes it prohibited for real-time applications.
Some of these approaches [4] [23] also utilize optical ﬂow
information, which is computationally heavy for state-of-
the-art algorithms[29] [15].

1

In order to alleviate the computational cost of semi-
supervised segmentation, we propose a novel approach to
adapt the generic segmentation network to the appearance
of a speciﬁc object instance in one single feed-forward pass.
We propose to employ another meta neural network called
modulator to learn to adjust the intermediate layers of the
generic segmentation network given an arbitrary target ob-
ject instance. Fig. 1 shows an illustration of our approach.
By extracting information from the image of the annotated
object and the spatial prior of the object, the modulator pro-
duces a list of parameters, which are injected into the seg-
mentation model for layer-wise feature manipulation. With-
out one-shot ﬁne-tuning, our model is able to change the
behavior of the segmentation network with minimum ex-
tracted information from the target object. We name this
process network modulation.

Our proposed model is efﬁcient, requiring only one for-
ward pass from the modulator to produce all parameters
needed for the segmentation model to adapt to the speciﬁc
object instance. Network modulation guided by the spatial
prior facilitates the model to track the object even with the
presence of multiple similar instances. The whole pipeline
is differentiable and can be learned end-to-end using the
standard stochastic gradient descent. The experiments show
that our approach outperforms previous approaches without
one-shot ﬁne-tuning by a large margin, and achieves com-
parable performance with these approaches after one-shot
ﬁne-tuning with a 70× speed up.

2. Related Work

Semi-supervised video segmentation. Semi-supervised
video object segmentation aims at tracking an object mask
given from the ﬁrst annotated frame throughout the rest of
video. Many approaches have been proposed in the lit-
erature, including those propagating superpixels [17] [35],
patches [9], object proposals [25], or in bilateral space [22],
and graphical model based optimization is usually per-
formed to consider multiple frames simultaneously. With
the success of FCN on static image segmentation [12],
deep learning based methods [23, 2, 32, 34, 18, 4] have
been recently proposed for video segmentation and promis-
ing results have been achieved. To model the temporal
motion information, some works heavily rely on optical
ﬂow [34] [4], and use CNNs to learn mask reﬁnement of
an object from current frame to the next one [23], or com-
bine the training of CNN with bilateral ﬁltering between
adjacent frames [18]. Chen et al. [4] use a CNN to jointly
estimate the optical ﬂow and provide the learned motion
representation to generate motion consistent segmentation
across time. Different from these approaches, Caelles et
al. [2] combine ofﬂine and online training process on static
images without using temporal information. While it saves
the computation of optical ﬂow and/or conditional random

ﬁelds (CRF) [19] involved in some previous methods, on-
line ﬁne-tuning still requires many iterations of optimiza-
tion, which poses a challenge for real-world applications
that need rapid inference.

Meta-learning for low-shot learning. Current success of
deep learning relies on the ability of learning from large-
scale labeled datasets through gradient descent optimiza-
tion. However, if we want our model to learn many tasks
adapted to many environments, it is not affordable to learn
each task for each setting from scratch. Instead, we want
our deep learning system to be able to learn new tasks very
fast and from very limited quantities of data. In the extreme
of “one-shot learning”, the algorithm needs to learn the new
task with a single observation. One potential strategy for
learning a versatile model is the notion of meta-learning,
or learning to learn, which can date back to the late 80s.
Recently, meta-learning has become a hot research topic
with publications on neural network optimization [3], ﬁnd-
ing good network architectures, fast reinforcement learning,
and few-shot image recognition [36, 28, 13, 10, 30]. Ravi
and Larochelle [28] proposed a LSTM meta-learner to learn
the update rules for few shot learning. The meta optimiza-
tion over a large number of tasks in [10] targets at learning
a model that can quickly adapt to the new task with limited
number of updates. Hariharan and Girschick [13] trained a
learner that generated new samples and used new samples
for training new tasks. Our approach shares the similarity
with meta-learning that it learns to update the segmentation
model rapidly with another meta learner, i.e. the modulator.

Network manipulation Several previous work try to in-
corporate modules to manipulate the behavior of a deep
neural network, either to manipulate spatial arrangement of
data [16] or ﬁlter connections [5]. Our method is also heav-
ily motivated by conditional batch normalization [8, 11, 14,
26], where the behavior of the deep model is manipulated by
batch normalization parameters conditioned on a guidance
input, e.g. a style image for image stylization or a language
sentence for visual question answering.

3. Video Object Segmentation with Network

Modulation

In our proposed framework, we utilize modulators to in-
stantly adapt the segmentation network to a speciﬁc object,
rather than performing hundreds of iterations of gradient
descent. We can achieve similar accuracy by adjusting a
limited number of parameters in the segmentation network,
compared with the updating the whole network in one-shot
learning approaches [23, 2]. There are two important cues
for video object segmentation: visual appearance and con-
tinuous motion in space. To use information from both vi-

Figure 2. An illustration of our model with three components: a segmentation network, a visual modulator, and a spatial modulator. The
two modulators produce a set of parameters that manipulates the intermediate feature maps of the segmentation network and adapt it to
segment the speciﬁc object.

sual and spatial domains, we incorporate two network mod-
ulators, namely visual modulator and spatial modulator, to
learn to adjust intermediate layers in the main segmentation
network, based on the annotated ﬁrst frame and spatial lo-
cation of the object, respectively.

3.1. Conditional batch normalization

Our approach is inspired by recent works using Condi-
tional Batch Normalization (CBN) [6, 14, 26], where the
scale and bias parameters of each batch-normalization layer
are produced by a second controller network. These param-
eters are used to control the behavior of the main network
for tasks such as image stylization and question answering.
Mathematically, each CBN layer can be formulated as fol-
lows:

yc = γcxc + βc,

(1)

where xc and yc are the input and output feature maps in the
cth channel, and γc and βc are the scale and bias parameters
produced by the controller network, respectively. The mean
and variance parameters are omitted for clarity.

3.2. Visual and spatial modulation

The CBN layer is a special case of the more general
scale-and-shift operation on feature maps. Following each
convolution layer, we deﬁne a new modulation layer with
parameters generated by both visual and spatial modula-
tors that are jointly trained. We design the two modula-
tors such that the visual modulator produces channel-wise
scale parameters to adjust the weights of different channels

in the feature maps, while the spatial modulator generates
element-wise bias parameters to inject spatial prior to the
modulated features. Speciﬁcally, our modulation layer can
be formulated as follows:

yc = γcxc + βc,

(2)

where γc and βc are modulation parameters from the vi-
sual and spatial modulators, respectively. γc is a scalar for
channel-wise weighting, while βc is a two-dimensional ma-
trix to apply point-wise bias values.

Fig. 2 shows an illustration of the proposed approach,
which consists of three networks: a fully-convolutional
main segmentation network, a visual modulator network,
and a spatial modulator network. The visual modulator net-
work is a CNN that takes the annotated visual object image
as input and produces a vector of scale parameters for all
modulation layers, while the spatial modulator network is a
very efﬁcient network that produces bias parameters based
on the spatial prior input. We will discuss the two modula-
tors in more detail in the following sections.

3.3. Visual modulator

The visual modulator is used to adapt the segmentation
network to focus on a speciﬁc object instance, which is the
annotated object in the ﬁrst frame. The annotated object is
referred to as visual guide hereafter for convenience. The
visual modulator extracts semantic information such as cat-
egory, color, shape, and texture, from the visual guide and
generates corresponding channel-wise weights so as to re-
target the segmentation network to segment the object. We

use VGG16 [33] neural network as the model for the vi-
sual modulator. We modify its last layer trained for Ima-
geNet classiﬁcation to match the number of parameters in
the modulation layers for the segmentation network.

The visual modulator implicitly learns an embedding of
different types of objects. It should produce similar param-
eters to adjust the segmentation network for similar objects
while different parameters for different objects. This is in-
deed true as we show in Sec. 4.2 that the embedding of the
modulator outputs correlates with object appearance very
well. One big advantage of using such a visual modulator is
that we can potentially transfer the knowledge learned with
a large number of object classes, e.g., ImageNet, in order to
learn a good embedding.

3.4. Spatial modulator

Our spatial modulator takes a prior location of the ob-
ject in the image as input. Since objects move continu-
ously in a video, we set the prior to be the predicted loca-
tion of the object mask in the previous frame. Speciﬁcally,
we encode the location information as a heatmap with a
two-dimensional Gaussian distribution on the image plane.
The center and standard deviations of the Gaussian distribu-
tion are computed from the predicted mask of the previous
frame. This heatmap is referred as spatial guide hereafter
for convenience. The spatial modulator downsamples the
spatial guide into different scales, to match the resolution
of different feature maps in the segmentation network, and
then applies a scale-and-shift operation on each downsam-
pled heatmap to generate the bias parameters of the corre-
sponding modulation layer. Mathematically,

βc = ˜γcm + ˜βc

(3)

where m is a down-sampled Gaussian heat map for the cor-
responding modulation layer, ˜γc and ˜βc are the scale-and-
shift parameters for the c-th channel, respectively. This is
implemented with a computationally efﬁcient 1 × 1 convo-
lution. In the bottom of Fig. 2, we illustrate the structure of
the spatial modulator.

Our method shares some similarities with the previous
work MaskTrack [23] in utilizing information from the pre-
vious mask. Comparing with their approach that uses the
exact foreground mask of the previous frame, we only use a
very coarse location prior.
It may seem that our method
throws away more information from the previous frame.
However, we argue that the rough position and size in the
previous frame possess enough information to infer the ob-
ject mask with the RGB image, and it prevents the model
from relying too much on the mask and as a result the error
propagation, which can be catastrophic when the object has
large movements in the video. As a drawback of such over-
utilization of the mask, MaskTrack has to apply plenty of
well-engineered data augmentation to prevent over-ﬁtting,

while we only apply simple shift and scaling as augmenta-
tion.

3.5. Implementation details

Our FCN structure follows the one used by [2], which
is a VGG16 [33] model with a hyper-column structure [12].
Intuitively, we should add modulation layers after each con-
volution layer in the FCN. However, we found that adding
modulation layers in-between the early convolution layers
actually makes the model perform worse. One possible rea-
son is that early layers extract low-level features that are
very sensitive to the scale-and-shift operations introduced
by the modulator. In our implementation, we add modula-
tion operations to all convolution layers in VGG16 except
the ﬁrst four layers, which results in nine modulation layers.
Similar to MaskTrack [23], we also utilize static im-
Ideally, the visual modula-
ages for training our model.
tor should learn a mapping from any object to modulation
weights of different layers in a FCN, which requires the
model to see all possible different objects. However, most
video semantic segmentation datasets only contain a very
limited number of categories. We tackle this challenge by
using the largest public semantic segmentation dataset MS-
COCO [21], which has 80 object categories. We select ob-
jects that are larger than 3% of the image size for training,
resulting in a total number of 217, 516 objects. For prepro-
cessing the input for the visual modulator, we ﬁrst crop the
object using the annotated mask, then set the background
pixels to mean image values, and then resize the cropped
image to a constant resolution of 224 × 224. The object
is also augmented with up to 10% random scaling and 10◦
random rotation. For preprocessing the spatial guide as in-
put to the spatial modulator, we ﬁrst compute the mean and
standard deviation of the mask, and then augment the mask
with up to 20% random shift and 40% random scaling. For
the whole image fed into the FCN, we use a random size
from 320, 400, and 480 with a square shape.

The visual modulator and segmentation network are
both initialized with VGG16 model pretrained on the Im-
ageNet [7] classiﬁcation task. The modulation parameters
{γc} are initialized to ones by setting the weights and biases
of the last fully-connected layer of the visual modulator to
zeros and ones, respectively. The weights of spatial modu-
lator are initialized randomly. We used the same balanced
cross-entropy loss as in [2]. A mini-batch size of 8 is used.
We use Adam optimizer with default momentum 0.9 and
0.999 for β1 and β2, respectively. The model is ﬁrst trained
for 10 epochs with learning rate 10−5 and then trained for
another 5 epochs with learning rate 10−6.

Further, in order to model appearance variations of mov-
ing objects in videos, the model can be ﬁnetuned on video
segmentation dataset such as DAVIS 2017 [27]. To be more
robust to appearance variations, we randomly pick a fore-

ground object from the whole video sequence as the visual
guide for each frame. The spatial guide is obtained from
the ground truth mask of the object in the previous frame.
The same data augmentations are applied as training on MS-
COCO. The model is ﬁnetuned for 20 epochs with learning
rate 10−6.

4. Experiments

In this section, we will introduce three parts of experi-
ment: the comparison of our approach with previous meth-
ods, the visualization of the modulation parameters, and ab-
lation study. Our model is trained on MS-COCO [21] 2017
dataset, and is tested on several popular video segmenta-
tion datasets, including DAVIS [24] [27] and YoutubeOb-
jects [17].

4.1. Semi-supervised Video Segmentation

In this section, we compare with traditional approaches
including OFL [35], BVS[22], and deep learning-based
approaches including PLM [32], MaskTrack [23], OS-
VOS [2], VPN [18], SFL [4], and ConvGRU [34].

4.1.1 DAVIS 2016 & YoutubeObjects

First, we compare our approach with previous approaches
on DAVIS 2016 and YoutubeObjects. Some approaches
(MaskTrack[23], SFL [4] and OSVOS[2]) reported results
both with and without model ﬁne-tuning on the target se-
quences. We include both of them and denote the variants
without ﬁne-tuning as MaskTrack-B, SFL-B, and OSVOS-
B, respectively. Our model has two variants,with the ﬁrst
only trained on static images (Stage 1) and the second ﬁne-
tuned on video data (Stage 1&2). Since there are several
popular add-ons for this line of research, such as optical
ﬂow and CRF [19], which both have a lot of variants and
make a fair comparison hard, we only include the perfor-
mances without optical ﬂow and CRF if possible, and mark
those with add-ons in Table 1.

In Table 1, by comparing our method with OFL [35],
an expensive graphical model based approach, we achieve
better accuracy on both DAVIS 2016 and YoutubeOb-
jects. Comparing with deep learning approaches without
model ﬁne-tuning, and therefore, similar speed as ours, our
method achieves the best accuracy on both DAVIS 2016 and
YoutubeObjects. Comparing with the four approaches using
model ﬁne-tuning on target videos (PLM, MaskTrack, SFL,
and OSVOS), our approach achieves better performance
than PLM and MaskTrack, and is on-par with SFL. OS-
VOS achieves higher accuracy but it also utilizes a bound-
ary snapping approach which contributes 2.4% in mean
IU. Our method is 70× faster than MaskTrack and OS-
VOS, 50× faster than SFL. We measure the running time
of MaskTrack-B, OSVOS-B, and our method on a NVIDIA

Quadro M6000 GPU using Tensorﬂow [1]. Speed of other
methods are derived from the corresponding papers 1.

In our method, the adaptation of the segmentation model
by the modulators is done with one forward pass for visual
modulator, so it is much more efﬁcient than the approaches
with model ﬁne-tuning on target videos. The visual modu-
lator only needs to be computed once for the whole video,
while the spatial modulator needs to be computed for every
frame but the overhead is negligible, i.e., the average speed
of our model on a video sequence is about the same as FCN
itself. Our method is the second fastest of all compared
methods, with only MaskTrack-B and OSVOS-B achieving
similar speed but with much worse accuracies.

4.1.2 DAVIS 2017

To further investigate the capability of our model, we con-
duct more experiments on DAVIS 2017 [27], which is the
largest video segmentation dataset to date. DAVIS 2017 is
more challenging than DAVIS 2016 and YoutubeObjects in
that it has multiple objects for each video sequences and
some of the objects are very similar. We compare our
method with two most related approaches, MaskTrack [23]
and OSVOS [2]. For fair comparison, we only use their
single network and adds-on free versions. We directly use
open source code of OSVOS and adapt MaskTrack model
to Tensorﬂow [1]. For each video sequence, OSVOS and
MaskTrack are ﬁnetuned with 1000 iterations. To show
that network modulation is capable of adapting different
model structures to speciﬁc object instances, we also ex-
periment with modiﬁed OSVOS and MaskTrack models by
adding a visual modulator to each of them, which are named
OSVOS-M and MaskTrack-M respectively. For these two
models, we only update the weights of the visual modula-
tors and keep the weights of the segmentation model ﬁxed
in training.

Table 2 shows the results of different approaches on
DAVIS 2017. We utilize the ofﬁcial evaluation metrics of
DAVIS dataset: mean, recall, and decay of region simi-
larity J and contour accuracy F, respectively. Note J
mean is equivalent to mean IU we used above. Again,
our model outperforms OSVOS-B and MaskTrack-B with
a large margin, while obtaining comparable performance
with the two methods with model ﬁne-tuning. OSVOS-M
and MaskTrack-M are both better than their baseline im-
plementations with a 18% and 9.3% gain in J mean, re-
spectively. Since the weights of the segmentation model
are ﬁxed, the accuracy gain comes solely from the modu-
lator, which proves that the visual modulator is capable of
improving different model structures by manipulating the
scales of the intermediate feature maps. Noticeably, our

1Speed of ConvGRU is estimated with the expensive optical ﬂow they

use, speed of PLM is derived through communication with the authors.

Table 1. Performance comparison of our approach with recent approaches on DAVIS 2016 and YoutubeObjects. Performance measured in
mean IU.

DAVIS 16 YoutubeObjs with FT OptFlow CRF

Method
OFL [35]
BVS [22]
ConvGRU[34]
VPN[18]
MaskTrack-B [23]
SFL-B [4]
OSVOS-B [2]
Ours (Stage 1)
Ours (Stage 1&2)
PLM [32]
MaskTrack [23]
SFL [4]
OSVOS [2]

68.0
60.0
70.1
70.2
63.2
67.4
52.5
72.2
74.0
70.0
69.8
74.8
79.8

67.5
58.4
-
-
66.5
-
44.7
66.4
69.0
-
71.7
-
74.1

-
-
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:51)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:55)

(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

Speed (s)
42.2
0.37
20
0.63
0.24
0.30
0.14
0.14
0.14
0.50
12
7.9
10

Figure 3. Some qualitative results of our approach compared with two recent state-of-the-art approaches on DAVIS 2017.

method obtains much lower decay rate for both region simi-
larity and contour accuracy compared to OSVOS and Mask-
Track. The accuracy changes of the different methods over
time are illustrated in Fig. 4. In the beginning of the video,
our method lags behind OSVOS and MaskTrack. However,
when it proceeds to around 40% of the video, our method
is on par with OSVOS and outperforms MaskTrack towards
the end of the video. With one-shot ﬁne-tuning, OSVOS
and MaskTrack ﬁt to the ﬁrst frame very well. They are
able to obtain high accuracy in the beginning of the video
since these frames are all similar to the ﬁrst one. But as time
goes on and the object turns into different poses and appear-
ances, it gets harder for the ﬁne-tuned model to generalize
to new object appearances. Our model is more robust to
the appearance changes since it learns a feature embedding

(see Section 4.2) for the annotated object which is more tol-
erant to pose and appearance changes compared to one-shot
ﬁne-tuning.

Some qualitative results of our methods compared with
the two previous approaches are shown in Fig. 3. Com-
pared with MaskTrack, our method generally obtains more
accurate boundaries, partially due to that the coarse spatial
prior forces the model to explore more cues on the image
rather than the mask in the previous frame. Compared with
OSVOS, our method shows better results when there are
multiple similar objects in the image, thanks to the tracking
capability provided by the spatial modulator. On the other
hand, our method is also shown to work well on unseen ob-
ject categories in training data. In Fig. 3, the camel and the
pigs are unseen object categories in MS-COCO dataset.

Table 2. Comparisons of our approach and two state-of-the-art algorithm on DAVIS 2017 validation set.

Method
OSVOS-B [2]
MaskTrack-B [23]
OSVOS-M
MaskTrack-M
OSVOS [2]
MaskTrack [23]
Ours

with FT J mean↑ J recall↑ J decay↓ F mean↑ F recall↑ F decay↓
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:55)

30.0
36.4
39.5
47.6
62.1
57.3
57.1

15.9
37.8
34.8
48.7
60.2
59.7
60.9

-0.8
39.3
14.8
27.1
28.2
28.3
21.5

18.5
35.3
36.4
44.6
55.1
51.2
52.5

20.0
36.0
35.3
49.3
71.3
65.5
66.1

0.1
42.0
9.1
27.9
29.3
29.1
24.3

Figure 4. The J mean performance of different methods over time
on DAVIS 2017. Best viewed in color.

Figure 6. Histograms of standard deviations of γc from the visual
modulator in different modulation layers. The annotated names
are the corresponding convolution layers in VGG16.

Figure 5. Visualization of learned modulation parameters for 100
objects from 10 categories: bicycle, motorcycle, car, bus, truck,
dog, cat, horse, cow, person. Zoom in to see details.

4.2. Visualization of the modulation parameters

Figure 7. Histograms of magnitude of ˜γc from the spatial modu-
lator in different modulation layers. The annotated names are the
corresponding convolution layers in VGG16.

Our model implicitly learns an embedding with the mod-
ulation parameters from the visual modulator for the anno-
tated objects. Intuitively, similar objects should have similar
modulation parameters, while different objects should have
dramatically different modulation parameters. To visual-

ize this embedding, we extract modulation parameters from
100 object instances in 10 object classes in MS-COCO, and
visualize the parameters in a two-dimensional embedding
space using multi-dimensional scaling in Fig. 5. We can see
that objects in the same category are mostly clustered to-

Table 3. Ablation study of our method on DAVIS 2017.

4.3. Ablation Study

Add-on

Model

Data

Variants
Ours + Online ﬁnetuning
Ours + CRF
Ours
no visual modulator
no spatial modulator
- random crop
- visual guide augmentation
- spatial guide augmentation

+8.3
+1.9

mIU ∆ mIU
60.8
54.4
52.5
33.0
40.1
50.6
49.5
35.6

-19.5
-12.4
-1.9
-1.1
-13.9

gether, and similar categories are closer to each other than
dissimilar categories. For example, cats and dogs, cars and
buses are mixed up due to their similar appearance, while
bicycles and dogs, buses and horses are far from each other
due to the big visual difference. Mammal classes (cats,
dogs, cows, horses, human) are generally clustered together,
and man-made objects (cars, buses, bicycles, motorcycles,
trucks) are clustered together.

We also investigate the magnitude of the modulation pa-
rameters in different layers. The modulation parameters
{γc} changes according to the visual guide. Therefore, we
compute the standard deviations of modulation parameters
{γc} in each modulation layer for images in MS-COCO val-
idation set and illustrate them in Fig. 6. An interesting ob-
servation is that towards deeper level of the network, the
variations of modulation parameters get larger. This shows
that the manipulation of feature maps is more dramatic in
the last few layers than in early layers of the network. The
last few layers of a deep neural network usually learn high-
level semantic meanings [37], which could be used to ad-
just the segmentation model to a speciﬁc object more effec-
tively.

We also look into the spatial modulator by extracting the
scale parameters {˜γc} in each layer of the spatial modula-
tor and visualize them in Fig. 7. The magnitudes of {˜γc}
are the relative scales of the spatial guide added to the fea-
ture maps in the FCN. The scale of {˜γc} is proportional to
the impact of spatial prior on the intermediate feature maps.
Interestingly, we observe sparsity in the values of {˜γc}. Ex-
cept the last convolution layer conv5 3, around 60% of
the parameters have zero values, which means only 40% of
the feature maps are affected by the spatial prior in these
layers. In the layer conv5 3, around 70% of the feature
maps interact with the spatial guide and most of them are
added with a similar scale (note the peak around 0.4) of the
spatial guide. This shows that the spatial prior is fused into
the feature maps gradually, rather than being effective at the
beginning of the network. After all feature extractions are
done, the spatial modulator makes a large adjustment to the
feature maps, which provides a strong prior of the location
of the target object.

We study the impact of different ingredients in our
method. We conduct experiments on DAVIS 2017 and mea-
sure the performance using mean IU. For variants of model
structures, we experiment with only using spatial or visual
modulator. For data augmentation methods, we experiment
with no random crop augmentation for the FCN input, and
no afﬁne transformation for the visual guide and the spa-
tial guide. We experiment with CRF as a post-processing
step. To investigate the effect of one-shot ﬁne-tuning on
our model, we also experiment with standard one-shot ﬁne-
tuning using a small number of iterations. Results are
shown in Table 3.

By adding a CRF post-processing, our method achieves
mIU (mean IU) of 54.4. By one-shot ﬁne-tuning with only
100 iterations for each sequence, our method achieves mIU
of 60.8, which is 5.7 better than OSVOS with 1000 itera-
tions. With ﬁne-tuning, our method is still relatively efﬁ-
cient with average running time around 1 s/frame. With-
out visual modulator, our model deteriorates to 33.0, while
without spatial modulator, our model obtains mIU of 40.1,
which shows that the visual guide is more important than the
spatial guide. For data augmentation, without random crop,
the accuracy drops by 1.9. Without afﬁne data augmenta-
tion on the visual guide, the accuracy further decreases by
1.1. Without augmentation on the spatial guide, our model
only obtains mIU of 35.6, which is a dramatic drop from
49.5. The results indicates that the spatial guide augmen-
tation is the most signiﬁcant on the performance. Without
perturbation, the model might rely on the location of the
spatial prior too much that it cannot deal with moving ob-
jects in real video sequences.

5. Conclusions

In this work, we propose a novel framework to process
one-shot video segmentation efﬁciently. To alleviate the
slow speed of one-shot ﬁne-tuning developed by previous
FCN-based methods, we propose to use a network modula-
tion approach mimicking the ﬁne-tuning process with one
forward pass of the modulator network. We show in experi-
ments that by injecting a limited number of parameters com-
puted by the modulators, the segmentation model can be re-
purposed to segment an arbitrary object. The proposed net-
work modulation method is a general learning method for
few-shot learning problems, which could be applied to other
tasks such as visual tracking and image stylization. Our
approach falls into the general category of meta-learning,
and it would also be interesting to investigate other meta-
learning approaches for video segmentation. Another piece
of future work would be to learn a recurrent representation
of the modulation parameters to manipulate the FCN based
on temporal information.

References

[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-
mawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,
R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e,
R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,
J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker,
V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. War-
den, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. Tensor-
Flow: Large-scale machine learning on heterogeneous sys-
tems, 2015. Software available from tensorﬂow.org. 5
[2] S. Caelles, K.-K. Maninis, J. Pont-Tuset, L. Leal-Taix´e,
D. Cremers, and L. Van Gool. One-shot video object seg-
mentation. In CVPR, 2017. 1, 2, 4, 5, 6, 7

[3] Y. Chen, M. W. Hoffman, S. G. Colmenarejo, M. Denil,
T. P. Lillicrap, M. Botvinick, and N. de Freitas. Learning to
learn without gradient descent by gradient descent. In ICML,
2016. 2

[4] J. Cheng, Y.-H. Tsai, S. Wang, and M.-H. Yang. Segﬂow:
Joint learning for video object segmentation and optical
ﬂow. In IEEE International Conference on Computer Vision
(ICCV), 2017. 1, 2, 5, 6

[5] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei.

Deformable convolutional networks. ICCV, 2017. 2

[6] H. de Vries, F. Strub, J. Mary, H. Larochelle, O. Pietquin,
and A. C. Courville. Modulating early visual processing by
language. CoRR, abs/1707.00683, 2017. 3

[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009. 4

[8] V. Dumoulin, J. Shlens, and M. Kudlur. A learned represen-

tation for artistic style, 2017. 2

[9] Q. Fan, F. Zhong, D. Lischinski, D. Cohen-Or, and B. Chen.
Jumpcut:non-successive mask transfer and interpolation for
video cutout. In ACM Trans. Graph., 34(6), 2015. 2

[10] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-
In ICML,

learning for fast adaptation of deep networks.
2017. 2

[11] G. Ghiasi, H. Lee, M. Kudlur, V. Dumoulin, and J. Shlens.
exploring the structure of a real-time, arbitrary neural artistic
stylization network. In BMVC, 2017. 2

[12] B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Hyper-
columns for object segmentation and ﬁne-grained localiza-
tion. In CVPR, 2015. 2, 4

[13] B. Hariharan and R. Girshick. Low-shot visual recognition

by shrinking and hallucinating features. In ICCV, 2017. 2

[14] X. Huang and S. J. Belongie. Arbitrary style transfer
in real-time with adaptive instance normalization. CoRR,
abs/1703.06868, 2017. 2, 3

[15] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. Flownet 2.0: Evolution of optical ﬂow estimation
with deep networks. CVPR, 2017. 1

[16] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial

transformer networks. In NIPS, pages 2017–2025, 2015. 2

[17] S. D. Jain and K. Grauman. Supervoxel-consistent fore-

ground propagation in video. In ECCV, 2014. 2, 5

[18] V. Jampani, R. Gadde, and P. V. Gehler. Video propagation

networks. In CVPR, 2017. 2, 5, 6

[19] P. Kr¨ahenb¨uhl and V. Koltun. Efﬁcient inference in fully
connected crfs with gaussian edge potentials. In NIPS, pages
109–117, 2011. 2, 5

[20] F. Li, T. Kim, A. Humayun, D. Tsai, and J. M. Rehg. Video
segmentation by tracking many ﬁgure-ground segments. In
ICCV, 2013. 1

[21] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In ECCV, 2014. 4, 5

[22] N. M¨arki, F. Perazzi, O. Wang, and A. Sorkine-Hornung. Bi-
lateral space video segmentation. In CVPR, 2016. 1, 2, 5, 6
[23] F. Perazzi, A. Khoreva, R. Benenson, B. Schiele, and
A.Sorkine-Hornung. Learning video object segmentation
from static images. In CVPR, 2017. 1, 2, 4, 5, 6, 7

[24] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool,
M. Gross, and A. Sorkine-Hornung. A benchmark dataset
and evaluation methodology for video object segmentation.
In CVPR, 2016. 1, 5

[25] F. Perazzi, O. Wang, M. Gross, and A. Sorkine-Hornung.
Fully connected object proposals for video segmentation. In
ICCV, 2015. 2

[26] E. Perez, H. de Vries, F. Strub, V. Dumoulin, and A. C.
Courville. Learning visual reasoning without strong priors.
CoRR, abs/1707.03017, 2017. 2, 3

[27] J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbel´aez, A. Sorkine-
Hornung, and L. Van Gool. The 2017 davis challenge on
video object segmentation. arXiv:1704.00675, 2017. 4, 5

[28] S. Ravi and H. Larochelle. Optimization as a model for few-

shot learning. ICLR, 2017. 2

[29] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
Epicﬂow: Edge-preserving interpolation of correspondences
for optical ﬂow. In CVPR, 2015. 1

[30] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and
T. Lillicrap. Meta-learning with memory-augmented neural
networks. In ICML, 2016. 2

[31] E. Shelhamer, J. Long, and T. Darrell. Fully convolutional
networks for semantic segmentation. IEEE transactions on
pattern analysis and machine intelligence, 39(4):640–651,
2017. 1

[32] J. Shin Yoon, F. Rameau, J. Kim, S. Lee, S. Shin, and
I. So Kweon. Pixel-level matching for video object segmen-
tation using convolutional neural networks. In CVPR, 2017.
1, 2, 5, 6

[33] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 3, 4

[34] P. Tokmakov, K. Alahari, and C. Schmid. Learning video
object segmentation with visual memory. In ICCV, 2017. 2,
5, 6

[35] Y.-H. Tsai, M.-H. Yang, and M. J. Black. Video segmenta-

tion via object ﬂow. In CVPR, 2016. 1, 2, 5, 6

[36] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and
D. Wierstra. Matching networks for one shot learning. In
NIPS, 2016. 2

[37] M. D. Zeiler and R. Fergus. Visualizing and understanding

convolutional networks. In ECCV, 2014. 8

Efﬁcient Video Object Segmentation via Network Modulation

Linjie Yang1

Yanran Wang2∗

Xuehan Xiong3

Jianchao Yang1

Aggelos K. Katsaggelos2

1Snap Inc.
2Image and Video Processing Laboratory, Northwestern University
3Google Inc.
{linjie.yang,jianchao.yang}@snap.com {joycewang1026@u,aggk@eecs}.northwestern.edu xiong828@gmail.com

8
1
0
2
 
b
e
F
 
4
 
 
]

V
C
.
s
c
[
 
 
1
v
8
1
2
1
0
.
2
0
8
1
:
v
i
X
r
a

Abstract

Video object segmentation targets at segmenting a spe-
ciﬁc object throughout a video sequence, given only an an-
notated ﬁrst frame. Recent deep learning based approaches
ﬁnd it effective by ﬁne-tuning a general-purpose segmen-
tation model on the annotated frame using hundreds of it-
erations of gradient descent. Despite the high accuracy
these methods achieve, the ﬁne-tuning process is inefﬁcient
and fail to meet the requirements of real world applica-
tions. We propose a novel approach that uses a single
forward pass to adapt the segmentation model to the ap-
pearance of a speciﬁc object. Speciﬁcally, a second meta
neural network named modulator is learned to manipu-
late the intermediate layers of the segmentation network
given limited visual and spatial information of the target
object. The experiments show that our approach is 70×
faster than ﬁne-tuning approaches while achieving similar
accuracy. Our model and code are released at https:
//github.com/linjieyangsc/video_seg.

1. Introduction

Semantic segmentation plays an important role in un-
derstanding visual content of an image as it assigns pre-
deﬁned object or scene labels to each pixel and thus trans-
lates the image into a segmentation map. When dealing with
video content, a human can easily segment an object in the
whole video without knowing its semantic meaning, which
inspired a research topic named semi-supervised video seg-
mentation. In a typical scenario of semi-supervised video
segmentation, one is given the ﬁrst frame of a video along
with an annotated object mask, and the task is to accurately
locate the object in all following frames [24, 20]. The ability
of performing accurate pixel-level video segmentation with
minimum supervision (e.g., one annotated frame) can fos-
ter a large amount of applications, such as accurate object

∗This work is done when Yanran was an intern at Snap Inc.

Figure 1. An overview of our approach. Our model is consisted of
a modulator and a segmentation network. The modulator can adapt
the segmentation model instantly to segment an arbitrary object
through a video sequence.

tracking for video understanding, interactive video editing,
augmented reality, and video-based advertisement. When
the supervision is limited to only one annotated frame, re-
searchers refer to this scenario as one-shot learning. In the
recent years, we have witnessed a rising amount of interests
in developing one-shot learning techniques for video seg-
mentation [2, 23, 35, 22, 32, 4]. Most of these work share a
similar two-stage paradigm: ﬁrst, train a general-purpose
Fully Convolutional Network (FCN) [31] to segment the
foreground object; Second, ﬁne-tune this network based on
the ﬁrst frame of the video for several hundred forward-
backward iterations to adapt the model to the speciﬁc video
sequence. Despite the high accuracies achieved by these ap-
proaches, the ﬁne-tuning process is arguably time consum-
ing, which makes it prohibited for real-time applications.
Some of these approaches [4] [23] also utilize optical ﬂow
information, which is computationally heavy for state-of-
the-art algorithms[29] [15].

1

In order to alleviate the computational cost of semi-
supervised segmentation, we propose a novel approach to
adapt the generic segmentation network to the appearance
of a speciﬁc object instance in one single feed-forward pass.
We propose to employ another meta neural network called
modulator to learn to adjust the intermediate layers of the
generic segmentation network given an arbitrary target ob-
ject instance. Fig. 1 shows an illustration of our approach.
By extracting information from the image of the annotated
object and the spatial prior of the object, the modulator pro-
duces a list of parameters, which are injected into the seg-
mentation model for layer-wise feature manipulation. With-
out one-shot ﬁne-tuning, our model is able to change the
behavior of the segmentation network with minimum ex-
tracted information from the target object. We name this
process network modulation.

Our proposed model is efﬁcient, requiring only one for-
ward pass from the modulator to produce all parameters
needed for the segmentation model to adapt to the speciﬁc
object instance. Network modulation guided by the spatial
prior facilitates the model to track the object even with the
presence of multiple similar instances. The whole pipeline
is differentiable and can be learned end-to-end using the
standard stochastic gradient descent. The experiments show
that our approach outperforms previous approaches without
one-shot ﬁne-tuning by a large margin, and achieves com-
parable performance with these approaches after one-shot
ﬁne-tuning with a 70× speed up.

2. Related Work

Semi-supervised video segmentation. Semi-supervised
video object segmentation aims at tracking an object mask
given from the ﬁrst annotated frame throughout the rest of
video. Many approaches have been proposed in the lit-
erature, including those propagating superpixels [17] [35],
patches [9], object proposals [25], or in bilateral space [22],
and graphical model based optimization is usually per-
formed to consider multiple frames simultaneously. With
the success of FCN on static image segmentation [12],
deep learning based methods [23, 2, 32, 34, 18, 4] have
been recently proposed for video segmentation and promis-
ing results have been achieved. To model the temporal
motion information, some works heavily rely on optical
ﬂow [34] [4], and use CNNs to learn mask reﬁnement of
an object from current frame to the next one [23], or com-
bine the training of CNN with bilateral ﬁltering between
adjacent frames [18]. Chen et al. [4] use a CNN to jointly
estimate the optical ﬂow and provide the learned motion
representation to generate motion consistent segmentation
across time. Different from these approaches, Caelles et
al. [2] combine ofﬂine and online training process on static
images without using temporal information. While it saves
the computation of optical ﬂow and/or conditional random

ﬁelds (CRF) [19] involved in some previous methods, on-
line ﬁne-tuning still requires many iterations of optimiza-
tion, which poses a challenge for real-world applications
that need rapid inference.

Meta-learning for low-shot learning. Current success of
deep learning relies on the ability of learning from large-
scale labeled datasets through gradient descent optimiza-
tion. However, if we want our model to learn many tasks
adapted to many environments, it is not affordable to learn
each task for each setting from scratch. Instead, we want
our deep learning system to be able to learn new tasks very
fast and from very limited quantities of data. In the extreme
of “one-shot learning”, the algorithm needs to learn the new
task with a single observation. One potential strategy for
learning a versatile model is the notion of meta-learning,
or learning to learn, which can date back to the late 80s.
Recently, meta-learning has become a hot research topic
with publications on neural network optimization [3], ﬁnd-
ing good network architectures, fast reinforcement learning,
and few-shot image recognition [36, 28, 13, 10, 30]. Ravi
and Larochelle [28] proposed a LSTM meta-learner to learn
the update rules for few shot learning. The meta optimiza-
tion over a large number of tasks in [10] targets at learning
a model that can quickly adapt to the new task with limited
number of updates. Hariharan and Girschick [13] trained a
learner that generated new samples and used new samples
for training new tasks. Our approach shares the similarity
with meta-learning that it learns to update the segmentation
model rapidly with another meta learner, i.e. the modulator.

Network manipulation Several previous work try to in-
corporate modules to manipulate the behavior of a deep
neural network, either to manipulate spatial arrangement of
data [16] or ﬁlter connections [5]. Our method is also heav-
ily motivated by conditional batch normalization [8, 11, 14,
26], where the behavior of the deep model is manipulated by
batch normalization parameters conditioned on a guidance
input, e.g. a style image for image stylization or a language
sentence for visual question answering.

3. Video Object Segmentation with Network

Modulation

In our proposed framework, we utilize modulators to in-
stantly adapt the segmentation network to a speciﬁc object,
rather than performing hundreds of iterations of gradient
descent. We can achieve similar accuracy by adjusting a
limited number of parameters in the segmentation network,
compared with the updating the whole network in one-shot
learning approaches [23, 2]. There are two important cues
for video object segmentation: visual appearance and con-
tinuous motion in space. To use information from both vi-

Figure 2. An illustration of our model with three components: a segmentation network, a visual modulator, and a spatial modulator. The
two modulators produce a set of parameters that manipulates the intermediate feature maps of the segmentation network and adapt it to
segment the speciﬁc object.

sual and spatial domains, we incorporate two network mod-
ulators, namely visual modulator and spatial modulator, to
learn to adjust intermediate layers in the main segmentation
network, based on the annotated ﬁrst frame and spatial lo-
cation of the object, respectively.

3.1. Conditional batch normalization

Our approach is inspired by recent works using Condi-
tional Batch Normalization (CBN) [6, 14, 26], where the
scale and bias parameters of each batch-normalization layer
are produced by a second controller network. These param-
eters are used to control the behavior of the main network
for tasks such as image stylization and question answering.
Mathematically, each CBN layer can be formulated as fol-
lows:

yc = γcxc + βc,

(1)

where xc and yc are the input and output feature maps in the
cth channel, and γc and βc are the scale and bias parameters
produced by the controller network, respectively. The mean
and variance parameters are omitted for clarity.

3.2. Visual and spatial modulation

The CBN layer is a special case of the more general
scale-and-shift operation on feature maps. Following each
convolution layer, we deﬁne a new modulation layer with
parameters generated by both visual and spatial modula-
tors that are jointly trained. We design the two modula-
tors such that the visual modulator produces channel-wise
scale parameters to adjust the weights of different channels

in the feature maps, while the spatial modulator generates
element-wise bias parameters to inject spatial prior to the
modulated features. Speciﬁcally, our modulation layer can
be formulated as follows:

yc = γcxc + βc,

(2)

where γc and βc are modulation parameters from the vi-
sual and spatial modulators, respectively. γc is a scalar for
channel-wise weighting, while βc is a two-dimensional ma-
trix to apply point-wise bias values.

Fig. 2 shows an illustration of the proposed approach,
which consists of three networks: a fully-convolutional
main segmentation network, a visual modulator network,
and a spatial modulator network. The visual modulator net-
work is a CNN that takes the annotated visual object image
as input and produces a vector of scale parameters for all
modulation layers, while the spatial modulator network is a
very efﬁcient network that produces bias parameters based
on the spatial prior input. We will discuss the two modula-
tors in more detail in the following sections.

3.3. Visual modulator

The visual modulator is used to adapt the segmentation
network to focus on a speciﬁc object instance, which is the
annotated object in the ﬁrst frame. The annotated object is
referred to as visual guide hereafter for convenience. The
visual modulator extracts semantic information such as cat-
egory, color, shape, and texture, from the visual guide and
generates corresponding channel-wise weights so as to re-
target the segmentation network to segment the object. We

use VGG16 [33] neural network as the model for the vi-
sual modulator. We modify its last layer trained for Ima-
geNet classiﬁcation to match the number of parameters in
the modulation layers for the segmentation network.

The visual modulator implicitly learns an embedding of
different types of objects. It should produce similar param-
eters to adjust the segmentation network for similar objects
while different parameters for different objects. This is in-
deed true as we show in Sec. 4.2 that the embedding of the
modulator outputs correlates with object appearance very
well. One big advantage of using such a visual modulator is
that we can potentially transfer the knowledge learned with
a large number of object classes, e.g., ImageNet, in order to
learn a good embedding.

3.4. Spatial modulator

Our spatial modulator takes a prior location of the ob-
ject in the image as input. Since objects move continu-
ously in a video, we set the prior to be the predicted loca-
tion of the object mask in the previous frame. Speciﬁcally,
we encode the location information as a heatmap with a
two-dimensional Gaussian distribution on the image plane.
The center and standard deviations of the Gaussian distribu-
tion are computed from the predicted mask of the previous
frame. This heatmap is referred as spatial guide hereafter
for convenience. The spatial modulator downsamples the
spatial guide into different scales, to match the resolution
of different feature maps in the segmentation network, and
then applies a scale-and-shift operation on each downsam-
pled heatmap to generate the bias parameters of the corre-
sponding modulation layer. Mathematically,

βc = ˜γcm + ˜βc

(3)

where m is a down-sampled Gaussian heat map for the cor-
responding modulation layer, ˜γc and ˜βc are the scale-and-
shift parameters for the c-th channel, respectively. This is
implemented with a computationally efﬁcient 1 × 1 convo-
lution. In the bottom of Fig. 2, we illustrate the structure of
the spatial modulator.

Our method shares some similarities with the previous
work MaskTrack [23] in utilizing information from the pre-
vious mask. Comparing with their approach that uses the
exact foreground mask of the previous frame, we only use a
very coarse location prior.
It may seem that our method
throws away more information from the previous frame.
However, we argue that the rough position and size in the
previous frame possess enough information to infer the ob-
ject mask with the RGB image, and it prevents the model
from relying too much on the mask and as a result the error
propagation, which can be catastrophic when the object has
large movements in the video. As a drawback of such over-
utilization of the mask, MaskTrack has to apply plenty of
well-engineered data augmentation to prevent over-ﬁtting,

while we only apply simple shift and scaling as augmenta-
tion.

3.5. Implementation details

Our FCN structure follows the one used by [2], which
is a VGG16 [33] model with a hyper-column structure [12].
Intuitively, we should add modulation layers after each con-
volution layer in the FCN. However, we found that adding
modulation layers in-between the early convolution layers
actually makes the model perform worse. One possible rea-
son is that early layers extract low-level features that are
very sensitive to the scale-and-shift operations introduced
by the modulator. In our implementation, we add modula-
tion operations to all convolution layers in VGG16 except
the ﬁrst four layers, which results in nine modulation layers.
Similar to MaskTrack [23], we also utilize static im-
Ideally, the visual modula-
ages for training our model.
tor should learn a mapping from any object to modulation
weights of different layers in a FCN, which requires the
model to see all possible different objects. However, most
video semantic segmentation datasets only contain a very
limited number of categories. We tackle this challenge by
using the largest public semantic segmentation dataset MS-
COCO [21], which has 80 object categories. We select ob-
jects that are larger than 3% of the image size for training,
resulting in a total number of 217, 516 objects. For prepro-
cessing the input for the visual modulator, we ﬁrst crop the
object using the annotated mask, then set the background
pixels to mean image values, and then resize the cropped
image to a constant resolution of 224 × 224. The object
is also augmented with up to 10% random scaling and 10◦
random rotation. For preprocessing the spatial guide as in-
put to the spatial modulator, we ﬁrst compute the mean and
standard deviation of the mask, and then augment the mask
with up to 20% random shift and 40% random scaling. For
the whole image fed into the FCN, we use a random size
from 320, 400, and 480 with a square shape.

The visual modulator and segmentation network are
both initialized with VGG16 model pretrained on the Im-
ageNet [7] classiﬁcation task. The modulation parameters
{γc} are initialized to ones by setting the weights and biases
of the last fully-connected layer of the visual modulator to
zeros and ones, respectively. The weights of spatial modu-
lator are initialized randomly. We used the same balanced
cross-entropy loss as in [2]. A mini-batch size of 8 is used.
We use Adam optimizer with default momentum 0.9 and
0.999 for β1 and β2, respectively. The model is ﬁrst trained
for 10 epochs with learning rate 10−5 and then trained for
another 5 epochs with learning rate 10−6.

Further, in order to model appearance variations of mov-
ing objects in videos, the model can be ﬁnetuned on video
segmentation dataset such as DAVIS 2017 [27]. To be more
robust to appearance variations, we randomly pick a fore-

ground object from the whole video sequence as the visual
guide for each frame. The spatial guide is obtained from
the ground truth mask of the object in the previous frame.
The same data augmentations are applied as training on MS-
COCO. The model is ﬁnetuned for 20 epochs with learning
rate 10−6.

4. Experiments

In this section, we will introduce three parts of experi-
ment: the comparison of our approach with previous meth-
ods, the visualization of the modulation parameters, and ab-
lation study. Our model is trained on MS-COCO [21] 2017
dataset, and is tested on several popular video segmenta-
tion datasets, including DAVIS [24] [27] and YoutubeOb-
jects [17].

4.1. Semi-supervised Video Segmentation

In this section, we compare with traditional approaches
including OFL [35], BVS[22], and deep learning-based
approaches including PLM [32], MaskTrack [23], OS-
VOS [2], VPN [18], SFL [4], and ConvGRU [34].

4.1.1 DAVIS 2016 & YoutubeObjects

First, we compare our approach with previous approaches
on DAVIS 2016 and YoutubeObjects. Some approaches
(MaskTrack[23], SFL [4] and OSVOS[2]) reported results
both with and without model ﬁne-tuning on the target se-
quences. We include both of them and denote the variants
without ﬁne-tuning as MaskTrack-B, SFL-B, and OSVOS-
B, respectively. Our model has two variants,with the ﬁrst
only trained on static images (Stage 1) and the second ﬁne-
tuned on video data (Stage 1&2). Since there are several
popular add-ons for this line of research, such as optical
ﬂow and CRF [19], which both have a lot of variants and
make a fair comparison hard, we only include the perfor-
mances without optical ﬂow and CRF if possible, and mark
those with add-ons in Table 1.

In Table 1, by comparing our method with OFL [35],
an expensive graphical model based approach, we achieve
better accuracy on both DAVIS 2016 and YoutubeOb-
jects. Comparing with deep learning approaches without
model ﬁne-tuning, and therefore, similar speed as ours, our
method achieves the best accuracy on both DAVIS 2016 and
YoutubeObjects. Comparing with the four approaches using
model ﬁne-tuning on target videos (PLM, MaskTrack, SFL,
and OSVOS), our approach achieves better performance
than PLM and MaskTrack, and is on-par with SFL. OS-
VOS achieves higher accuracy but it also utilizes a bound-
ary snapping approach which contributes 2.4% in mean
IU. Our method is 70× faster than MaskTrack and OS-
VOS, 50× faster than SFL. We measure the running time
of MaskTrack-B, OSVOS-B, and our method on a NVIDIA

Quadro M6000 GPU using Tensorﬂow [1]. Speed of other
methods are derived from the corresponding papers 1.

In our method, the adaptation of the segmentation model
by the modulators is done with one forward pass for visual
modulator, so it is much more efﬁcient than the approaches
with model ﬁne-tuning on target videos. The visual modu-
lator only needs to be computed once for the whole video,
while the spatial modulator needs to be computed for every
frame but the overhead is negligible, i.e., the average speed
of our model on a video sequence is about the same as FCN
itself. Our method is the second fastest of all compared
methods, with only MaskTrack-B and OSVOS-B achieving
similar speed but with much worse accuracies.

4.1.2 DAVIS 2017

To further investigate the capability of our model, we con-
duct more experiments on DAVIS 2017 [27], which is the
largest video segmentation dataset to date. DAVIS 2017 is
more challenging than DAVIS 2016 and YoutubeObjects in
that it has multiple objects for each video sequences and
some of the objects are very similar. We compare our
method with two most related approaches, MaskTrack [23]
and OSVOS [2]. For fair comparison, we only use their
single network and adds-on free versions. We directly use
open source code of OSVOS and adapt MaskTrack model
to Tensorﬂow [1]. For each video sequence, OSVOS and
MaskTrack are ﬁnetuned with 1000 iterations. To show
that network modulation is capable of adapting different
model structures to speciﬁc object instances, we also ex-
periment with modiﬁed OSVOS and MaskTrack models by
adding a visual modulator to each of them, which are named
OSVOS-M and MaskTrack-M respectively. For these two
models, we only update the weights of the visual modula-
tors and keep the weights of the segmentation model ﬁxed
in training.

Table 2 shows the results of different approaches on
DAVIS 2017. We utilize the ofﬁcial evaluation metrics of
DAVIS dataset: mean, recall, and decay of region simi-
larity J and contour accuracy F, respectively. Note J
mean is equivalent to mean IU we used above. Again,
our model outperforms OSVOS-B and MaskTrack-B with
a large margin, while obtaining comparable performance
with the two methods with model ﬁne-tuning. OSVOS-M
and MaskTrack-M are both better than their baseline im-
plementations with a 18% and 9.3% gain in J mean, re-
spectively. Since the weights of the segmentation model
are ﬁxed, the accuracy gain comes solely from the modu-
lator, which proves that the visual modulator is capable of
improving different model structures by manipulating the
scales of the intermediate feature maps. Noticeably, our

1Speed of ConvGRU is estimated with the expensive optical ﬂow they

use, speed of PLM is derived through communication with the authors.

Table 1. Performance comparison of our approach with recent approaches on DAVIS 2016 and YoutubeObjects. Performance measured in
mean IU.

DAVIS 16 YoutubeObjs with FT OptFlow CRF

Method
OFL [35]
BVS [22]
ConvGRU[34]
VPN[18]
MaskTrack-B [23]
SFL-B [4]
OSVOS-B [2]
Ours (Stage 1)
Ours (Stage 1&2)
PLM [32]
MaskTrack [23]
SFL [4]
OSVOS [2]

68.0
60.0
70.1
70.2
63.2
67.4
52.5
72.2
74.0
70.0
69.8
74.8
79.8

67.5
58.4
-
-
66.5
-
44.7
66.4
69.0
-
71.7
-
74.1

-
-
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:51)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:55)

(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

Speed (s)
42.2
0.37
20
0.63
0.24
0.30
0.14
0.14
0.14
0.50
12
7.9
10

Figure 3. Some qualitative results of our approach compared with two recent state-of-the-art approaches on DAVIS 2017.

method obtains much lower decay rate for both region simi-
larity and contour accuracy compared to OSVOS and Mask-
Track. The accuracy changes of the different methods over
time are illustrated in Fig. 4. In the beginning of the video,
our method lags behind OSVOS and MaskTrack. However,
when it proceeds to around 40% of the video, our method
is on par with OSVOS and outperforms MaskTrack towards
the end of the video. With one-shot ﬁne-tuning, OSVOS
and MaskTrack ﬁt to the ﬁrst frame very well. They are
able to obtain high accuracy in the beginning of the video
since these frames are all similar to the ﬁrst one. But as time
goes on and the object turns into different poses and appear-
ances, it gets harder for the ﬁne-tuned model to generalize
to new object appearances. Our model is more robust to
the appearance changes since it learns a feature embedding

(see Section 4.2) for the annotated object which is more tol-
erant to pose and appearance changes compared to one-shot
ﬁne-tuning.

Some qualitative results of our methods compared with
the two previous approaches are shown in Fig. 3. Com-
pared with MaskTrack, our method generally obtains more
accurate boundaries, partially due to that the coarse spatial
prior forces the model to explore more cues on the image
rather than the mask in the previous frame. Compared with
OSVOS, our method shows better results when there are
multiple similar objects in the image, thanks to the tracking
capability provided by the spatial modulator. On the other
hand, our method is also shown to work well on unseen ob-
ject categories in training data. In Fig. 3, the camel and the
pigs are unseen object categories in MS-COCO dataset.

Table 2. Comparisons of our approach and two state-of-the-art algorithm on DAVIS 2017 validation set.

Method
OSVOS-B [2]
MaskTrack-B [23]
OSVOS-M
MaskTrack-M
OSVOS [2]
MaskTrack [23]
Ours

with FT J mean↑ J recall↑ J decay↓ F mean↑ F recall↑ F decay↓
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:55)

30.0
36.4
39.5
47.6
62.1
57.3
57.1

0.1
42.0
9.1
27.9
29.3
29.1
24.3

18.5
35.3
36.4
44.6
55.1
51.2
52.5

-0.8
39.3
14.8
27.1
28.2
28.3
21.5

20.0
36.0
35.3
49.3
71.3
65.5
66.1

15.9
37.8
34.8
48.7
60.2
59.7
60.9

Figure 4. The J mean performance of different methods over time
on DAVIS 2017. Best viewed in color.

Figure 6. Histograms of standard deviations of γc from the visual
modulator in different modulation layers. The annotated names
are the corresponding convolution layers in VGG16.

Figure 5. Visualization of learned modulation parameters for 100
objects from 10 categories: bicycle, motorcycle, car, bus, truck,
dog, cat, horse, cow, person. Zoom in to see details.

4.2. Visualization of the modulation parameters

Figure 7. Histograms of magnitude of ˜γc from the spatial modu-
lator in different modulation layers. The annotated names are the
corresponding convolution layers in VGG16.

Our model implicitly learns an embedding with the mod-
ulation parameters from the visual modulator for the anno-
tated objects. Intuitively, similar objects should have similar
modulation parameters, while different objects should have
dramatically different modulation parameters. To visual-

ize this embedding, we extract modulation parameters from
100 object instances in 10 object classes in MS-COCO, and
visualize the parameters in a two-dimensional embedding
space using multi-dimensional scaling in Fig. 5. We can see
that objects in the same category are mostly clustered to-

Table 3. Ablation study of our method on DAVIS 2017.

4.3. Ablation Study

Add-on

Model

Data

Variants
Ours + Online ﬁnetuning
Ours + CRF
Ours
no visual modulator
no spatial modulator
- random crop
- visual guide augmentation
- spatial guide augmentation

+8.3
+1.9

mIU ∆ mIU
60.8
54.4
52.5
33.0
40.1
50.6
49.5
35.6

-19.5
-12.4
-1.9
-1.1
-13.9

gether, and similar categories are closer to each other than
dissimilar categories. For example, cats and dogs, cars and
buses are mixed up due to their similar appearance, while
bicycles and dogs, buses and horses are far from each other
due to the big visual difference. Mammal classes (cats,
dogs, cows, horses, human) are generally clustered together,
and man-made objects (cars, buses, bicycles, motorcycles,
trucks) are clustered together.

We also investigate the magnitude of the modulation pa-
rameters in different layers. The modulation parameters
{γc} changes according to the visual guide. Therefore, we
compute the standard deviations of modulation parameters
{γc} in each modulation layer for images in MS-COCO val-
idation set and illustrate them in Fig. 6. An interesting ob-
servation is that towards deeper level of the network, the
variations of modulation parameters get larger. This shows
that the manipulation of feature maps is more dramatic in
the last few layers than in early layers of the network. The
last few layers of a deep neural network usually learn high-
level semantic meanings [37], which could be used to ad-
just the segmentation model to a speciﬁc object more effec-
tively.

We also look into the spatial modulator by extracting the
scale parameters {˜γc} in each layer of the spatial modula-
tor and visualize them in Fig. 7. The magnitudes of {˜γc}
are the relative scales of the spatial guide added to the fea-
ture maps in the FCN. The scale of {˜γc} is proportional to
the impact of spatial prior on the intermediate feature maps.
Interestingly, we observe sparsity in the values of {˜γc}. Ex-
cept the last convolution layer conv5 3, around 60% of
the parameters have zero values, which means only 40% of
the feature maps are affected by the spatial prior in these
layers. In the layer conv5 3, around 70% of the feature
maps interact with the spatial guide and most of them are
added with a similar scale (note the peak around 0.4) of the
spatial guide. This shows that the spatial prior is fused into
the feature maps gradually, rather than being effective at the
beginning of the network. After all feature extractions are
done, the spatial modulator makes a large adjustment to the
feature maps, which provides a strong prior of the location
of the target object.

We study the impact of different ingredients in our
method. We conduct experiments on DAVIS 2017 and mea-
sure the performance using mean IU. For variants of model
structures, we experiment with only using spatial or visual
modulator. For data augmentation methods, we experiment
with no random crop augmentation for the FCN input, and
no afﬁne transformation for the visual guide and the spa-
tial guide. We experiment with CRF as a post-processing
step. To investigate the effect of one-shot ﬁne-tuning on
our model, we also experiment with standard one-shot ﬁne-
tuning using a small number of iterations. Results are
shown in Table 3.

By adding a CRF post-processing, our method achieves
mIU (mean IU) of 54.4. By one-shot ﬁne-tuning with only
100 iterations for each sequence, our method achieves mIU
of 60.8, which is 5.7 better than OSVOS with 1000 itera-
tions. With ﬁne-tuning, our method is still relatively efﬁ-
cient with average running time around 1 s/frame. With-
out visual modulator, our model deteriorates to 33.0, while
without spatial modulator, our model obtains mIU of 40.1,
which shows that the visual guide is more important than the
spatial guide. For data augmentation, without random crop,
the accuracy drops by 1.9. Without afﬁne data augmenta-
tion on the visual guide, the accuracy further decreases by
1.1. Without augmentation on the spatial guide, our model
only obtains mIU of 35.6, which is a dramatic drop from
49.5. The results indicates that the spatial guide augmen-
tation is the most signiﬁcant on the performance. Without
perturbation, the model might rely on the location of the
spatial prior too much that it cannot deal with moving ob-
jects in real video sequences.

5. Conclusions

In this work, we propose a novel framework to process
one-shot video segmentation efﬁciently. To alleviate the
slow speed of one-shot ﬁne-tuning developed by previous
FCN-based methods, we propose to use a network modula-
tion approach mimicking the ﬁne-tuning process with one
forward pass of the modulator network. We show in experi-
ments that by injecting a limited number of parameters com-
puted by the modulators, the segmentation model can be re-
purposed to segment an arbitrary object. The proposed net-
work modulation method is a general learning method for
few-shot learning problems, which could be applied to other
tasks such as visual tracking and image stylization. Our
approach falls into the general category of meta-learning,
and it would also be interesting to investigate other meta-
learning approaches for video segmentation. Another piece
of future work would be to learn a recurrent representation
of the modulation parameters to manipulate the FCN based
on temporal information.

References

[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-
mawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,
R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e,
R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,
J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker,
V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. War-
den, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. Tensor-
Flow: Large-scale machine learning on heterogeneous sys-
tems, 2015. Software available from tensorﬂow.org. 5
[2] S. Caelles, K.-K. Maninis, J. Pont-Tuset, L. Leal-Taix´e,
D. Cremers, and L. Van Gool. One-shot video object seg-
mentation. In CVPR, 2017. 1, 2, 4, 5, 6, 7

[3] Y. Chen, M. W. Hoffman, S. G. Colmenarejo, M. Denil,
T. P. Lillicrap, M. Botvinick, and N. de Freitas. Learning to
learn without gradient descent by gradient descent. In ICML,
2016. 2

[4] J. Cheng, Y.-H. Tsai, S. Wang, and M.-H. Yang. Segﬂow:
Joint learning for video object segmentation and optical
ﬂow. In IEEE International Conference on Computer Vision
(ICCV), 2017. 1, 2, 5, 6

[5] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei.

Deformable convolutional networks. ICCV, 2017. 2

[6] H. de Vries, F. Strub, J. Mary, H. Larochelle, O. Pietquin,
and A. C. Courville. Modulating early visual processing by
language. CoRR, abs/1707.00683, 2017. 3

[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009. 4

[8] V. Dumoulin, J. Shlens, and M. Kudlur. A learned represen-

tation for artistic style, 2017. 2

[9] Q. Fan, F. Zhong, D. Lischinski, D. Cohen-Or, and B. Chen.
Jumpcut:non-successive mask transfer and interpolation for
video cutout. In ACM Trans. Graph., 34(6), 2015. 2

[10] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-
In ICML,

learning for fast adaptation of deep networks.
2017. 2

[11] G. Ghiasi, H. Lee, M. Kudlur, V. Dumoulin, and J. Shlens.
exploring the structure of a real-time, arbitrary neural artistic
stylization network. In BMVC, 2017. 2

[12] B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Hyper-
columns for object segmentation and ﬁne-grained localiza-
tion. In CVPR, 2015. 2, 4

[13] B. Hariharan and R. Girshick. Low-shot visual recognition

by shrinking and hallucinating features. In ICCV, 2017. 2

[14] X. Huang and S. J. Belongie. Arbitrary style transfer
in real-time with adaptive instance normalization. CoRR,
abs/1703.06868, 2017. 2, 3

[15] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. Flownet 2.0: Evolution of optical ﬂow estimation
with deep networks. CVPR, 2017. 1

[16] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial

transformer networks. In NIPS, pages 2017–2025, 2015. 2

[17] S. D. Jain and K. Grauman. Supervoxel-consistent fore-

ground propagation in video. In ECCV, 2014. 2, 5

[18] V. Jampani, R. Gadde, and P. V. Gehler. Video propagation

networks. In CVPR, 2017. 2, 5, 6

[19] P. Kr¨ahenb¨uhl and V. Koltun. Efﬁcient inference in fully
connected crfs with gaussian edge potentials. In NIPS, pages
109–117, 2011. 2, 5

[20] F. Li, T. Kim, A. Humayun, D. Tsai, and J. M. Rehg. Video
segmentation by tracking many ﬁgure-ground segments. In
ICCV, 2013. 1

[21] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In ECCV, 2014. 4, 5

[22] N. M¨arki, F. Perazzi, O. Wang, and A. Sorkine-Hornung. Bi-
lateral space video segmentation. In CVPR, 2016. 1, 2, 5, 6
[23] F. Perazzi, A. Khoreva, R. Benenson, B. Schiele, and
A.Sorkine-Hornung. Learning video object segmentation
from static images. In CVPR, 2017. 1, 2, 4, 5, 6, 7

[24] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool,
M. Gross, and A. Sorkine-Hornung. A benchmark dataset
and evaluation methodology for video object segmentation.
In CVPR, 2016. 1, 5

[25] F. Perazzi, O. Wang, M. Gross, and A. Sorkine-Hornung.
Fully connected object proposals for video segmentation. In
ICCV, 2015. 2

[26] E. Perez, H. de Vries, F. Strub, V. Dumoulin, and A. C.
Courville. Learning visual reasoning without strong priors.
CoRR, abs/1707.03017, 2017. 2, 3

[27] J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbel´aez, A. Sorkine-
Hornung, and L. Van Gool. The 2017 davis challenge on
video object segmentation. arXiv:1704.00675, 2017. 4, 5

[28] S. Ravi and H. Larochelle. Optimization as a model for few-

shot learning. ICLR, 2017. 2

[29] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
Epicﬂow: Edge-preserving interpolation of correspondences
for optical ﬂow. In CVPR, 2015. 1

[30] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and
T. Lillicrap. Meta-learning with memory-augmented neural
networks. In ICML, 2016. 2

[31] E. Shelhamer, J. Long, and T. Darrell. Fully convolutional
networks for semantic segmentation. IEEE transactions on
pattern analysis and machine intelligence, 39(4):640–651,
2017. 1

[32] J. Shin Yoon, F. Rameau, J. Kim, S. Lee, S. Shin, and
I. So Kweon. Pixel-level matching for video object segmen-
tation using convolutional neural networks. In CVPR, 2017.
1, 2, 5, 6

[33] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 3, 4

[34] P. Tokmakov, K. Alahari, and C. Schmid. Learning video
object segmentation with visual memory. In ICCV, 2017. 2,
5, 6

[35] Y.-H. Tsai, M.-H. Yang, and M. J. Black. Video segmenta-

tion via object ﬂow. In CVPR, 2016. 1, 2, 5, 6

[36] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and
D. Wierstra. Matching networks for one shot learning. In
NIPS, 2016. 2

[37] M. D. Zeiler and R. Fergus. Visualizing and understanding

convolutional networks. In ECCV, 2014. 8

