8
1
0
2
 
b
e
F
 
7
2

 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
6
5
6
9
0
.
2
0
8
1
:
v
i
X
r
a

Learning Binary Latent Variable Models: A Tensor Eigenpair
Approach

Ariel Jaffe1∗, Roi Weiss1, Shai Carmi2, Yuval Kluger3,4,5 and Boaz Nadler1

2

1

4

Dept. of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot 7610001, Israel

Braun School of Public Health and Community Medicine, The Hebrew University of Jerusalem, Jerusalem 9112102, Israel
3

Interdepartmental Program in Computational Biology and Bioinformatics, Yale University, New Haven, CT 06511, USA

Dept. of Pathology and Yale Cancer Center, Yale University School of Medicine, New Haven, CT 06520, USA

5

Program of Applied Mathematics, Yale University , New Haven, CT 06511, USA

Abstract

Latent variable models with hidden binary units appear in various applications. Learning such models, in
particular in the presence of noise, is a challenging computational problem. In this paper we propose a novel
spectral approach to this problem, based on the eigenvectors of both the second order moment matrix and
third order moment tensor of the observed data. We prove that under mild non-degeneracy conditions, our
method consistently estimates the model parameters at the optimal parametric rate. Our tensor-based method
generalizes previous orthogonal tensor decomposition approaches, where the hidden units were assumed to be
either statistically independent or mutually exclusive. We illustrate the consistency of our method on simulated
data and demonstrate its usefulness in learning a common model for population mixtures in genetics.

1

Introduction

In this paper we propose a spectral method for learning the following binary latent variable model, shown in
Figure 1. The hidden layer, h = (h1, . . . , hd), consists of d binary random variables with an unknown joint
distribution Ph :

[0, 1]. The observed vector x

d features is modeled as

Rm of m

0, 1

d

{

}

→

∈

≥

x = W ⊤h + σξ,

(1)

∈

Rd×m is an unknown weight matrix assumed to be full rank d. Here, σ

where W
0 is the noise level and
ξ is an additive noise vector independent of h, whose m coordinates are all i.i.d. zero mean and unit variance
random variables. For simplicity we assume it is Gaussian, though our method can be modiﬁed to handle other
noise distributions.

≥

The model in (1) appears, for example, in overlapping clustering [7, 8], in various problems in bioinformatics
[9, 45, 47], and in blind source separation [49]. A special instance of model (1) is the Gaussian-Bernoulli re-
stricted Boltzmann machine (G-RBM) where the distribution Ph is further assumed to have a parametric energy-
based structure [15, 25, 50]. G-RBMs were used, e.g., in modeling human motion [48] and natural image patches
[37].

Given n i.i.d. samples x1, . . . , xn from model (1), the goal is to estimate the weight matrix W . A common
approach for learning W is by maximum likelihood. As this function is non-convex, common optimization

∗Email addresses: ariel.jaffe@weizmann.ac.il, roi.weiss@weizmann.ac.il, boaz.nadler@weizmann.ac.il,

shai.carmi@huji.ac.il, yuval.kluger@yale.edu

1

h1

h2

. . .

hd

W11

Wdm

x1

. . .

xi

. . .

xm

Figure 1: The binary latent variable model.

schemes include the EM algorithm and alternating least squares (ALS). In addition, several works developed
iterative methods specialized to G-RBMs [15, 24]. All these methods, however, often lack consistency guarantees
and may not be well suited for large datasets due to their potential slow convergence. This is not surprising, as
learning W under model (1) is believed to be computationally hard; see for example Mossel and Roch [39].

Over the past years, several works considered variants and speciﬁc instances of model (1) under additional
assumptions on the distribution Ph or on the weight matrix W . For example, when Ph is a product distribution,
the learning problem becomes that of independent component analysis (ICA) with binary signals [28]. In this
case, several methods have been derived for estimating W and under suitable non-degeneracy conditions were
proven to be both computationally efﬁcient and statistically consistent [4, 18, 28, 30, 44, 46]. Similarly, when
d
the hidden units are mutually exclusive, namely Ph has support h
i=1, the model is a Gaussian mixture
(GMM) with d spherical components with linearly independent means. Efﬁcient and consistent algorithms have
been derived for this case as well [2, 3, 26, 38]. Among those, most relevant to this work are orthogonal tensor
decomposition methods [4]. Interestingly, these methods can learn some additional latent models, with hidden
units that are not necessarily binary, such as Dirichlet allocation and other correlated topic models [5].

ei}

∈ {

Learning W given the observed data

n
j=1 can also be viewed as a noisy matrix factorization problem. If
W is known to be non-negative, then various non-negative matrix factorization methods can be used. Moreover,
under appropriate conditions, some of these methods were proven to be computationally efﬁcient and consistent
[6, 17]. For general full rank W , the matrix factorization method in Slawski et al. [47] (SHL) exactly recovers
W when σ = 0 with a runtime exponential in d. This method, however, can handle only low levels of noise and
has no consistency guarantees when σ > 0.

xj}

{

A tensor eigenpair approach In this paper we propose a novel spectral method for learning W which is based
on the eigenvectors of both the second order moment matrix and the third order moment tensor of the observed
data. We prove that our method is consistent under mild non-degeneracy conditions and achieves the parametric
rate OP (n− 1

2 ) for any noise level σ

0.

The non-degeneracy conditions we pose are signiﬁcantly weaker than those required by previous tensor
decomposition methods mentioned above. In particular, their assumptions and resulting methods can be viewed
as speciﬁc cases of our more general approach.

≥

Similarly to the matrix factorization method in Slawski et al. [47], our algorithm has runtime linear in n,
polynomial in m, and in general exponential in d. With our current Matlab implementation, most of the runtime
d tensor. Practically, our method, implemented without
is spent on computing the eigenpairs of a d
any particular optimization, can learn a model with 12 hidden units in less than ten minutes on a standard PC.
Furthermore, the overall runtime can be signiﬁcantly reduced, since the step of computing the tensor eigenpairs
can be embarrassingly parallelized.

×

×

d

Paper outline
In Section 3 we introduce our method in the case σ = 0. The case σ

In the next section we ﬁx the notation and provide necessary background on tensor eigenpairs.
0 is treated in Section 4. Experiments with

≥

2

our method and comparison to other approaches appear in Section 5. All proofs are deferred to the appendices.

2 Preliminaries

Notation We abbreviate [d] =
1, . . . , d
}
slightly abuse notation and view a matrix W also as the set of its columns, namely w
W and span(W ) is the span of all its columns. The unit sphere is denoted by Sd−1 =

and denote ei as the i-th unit vector with entries (ei)j = δij. We
W is some column of

Rd :

= 1

u

{

.

T ∈

Rd×d×d is symmetric if

can also be seen as a multi-linear operator: for matrices W 1, W 2, W 3 with W i

}
Tπ(i,j,k) for all permutations π of i, j, k. Here, we consider only
Rd×di,

Tijk =

∈

k

k

A tensor

∈
u
{

symmetric tensors.
the tensor-mode product, denoted

T

∈
d3 tensor whose (i1, i2, i3)-th entry is

T

(W 1, W 2, W 3), is a d1 ×
j2i2 W 3
W 1

j1i1 W 2

d2 ×
j3i3 Tj1j2j3 .

Xj1,j2,j3∈[d]

Tensor eigenpairs Several types of eigenpairs of a tensor have been proposed. Here, we consider the follow-
ing deﬁnition, termed Z-eigenpairs by Qi [43] and l2-eigenpairs by Lim [36]. Henceforth we just call them
eigenpairs.

Deﬁnition 1. (u, λ)

Rd

R is an eigenpair of

if

∈

×

T

(I, u, u) = λu and

T

= 1.

u

k

k

(2)

Note that if (u, λ) is an eigenpair then the eigenvalue is simply λ =

λ) is
also an eigenpair. Following common practice, we treat these two pairs as one. So, without loss of generality, we
make the convention that λ

(u, u, u). In addition, (

0.
In contrast to the matrix case, the number of eigenvalues

Rd×d×d can be much larger
than d. As shown by Cartwright and Sturmfels [12], for a d
1 of them.
With precise deﬁnitions appearing in Cartwright and Sturmfels [12], for a generic tensor, all its eigenvalues have
multiplicity one and the number of eigenpairs

of a tensor
d tensor, there can be at most 2d

×
is at most 2d

λ
}
{
d
×

T ∈

u,

1.

−

−

≥

−

T

In principle, computing the set of all eigenpairs of a general symmetric tensor is a #P problem [23]. Neverthe-
less, several methods have been proposed for computing at least some eigenpairs, including iterative higher-order
power methods [31, 32], homotopy continuation [13], semideﬁnite programming [16], and iterative Newton-
based methods [22, 29]. We conclude this section with the deﬁnition of Newton-stable eigenpairs [29] which are
most relevant to our work.

(u, λ)
}

{

−

Rd,

→

Newton-stable eigenpairs Equivalently to (2), eigenpairs of
Rd

T

can also be characterized by the function g :

g(u) =

(I, u, u)

(u, u, u)

u.

·
= 1 is an eigenpair of

− T

T
u

k

k

It is easy to verify that a pair (u, λ) with

(u, u, u). The stability of an eigenpair is determined by its Jacobian matrix

g(u)
1 dimensional subspace orthogonal to u. Formally, let Lu
1 orthonormal columns that span the subspace orthogonal to u and deﬁne the (d

−

T
by its projection into the d
with d
−
Jacobian matrix

∈
∈
−

T

(3)
if and only if g(u) = 0 and λ =
Rd×d, more precisely,
Rd×(d−1) be a matrix
1) projected
1)

(d

∇

×

−

Deﬁnition 2. An eigenpair (u, λ) of

T ∈

Jp(u) = L⊤
u
Rd×d×d is Newton-stable if the matrix Jp(u) has full rank d

g(u)Lu.

∇

(4)

1.

−

The homotopy continuation method in Chen et al. [13] is guaranteed to compute all the Newton-stable eigen-
pairs of a tensor. Alternatively, Newton-stable eigenpairs are attracting ﬁxed points for the iterative orthogonal
Newton correction method (O–NCM) in Jaffe et al. [29]. Moreover, O–NCM converges to any Newton-stable
eigenpair at a quadratic rate given a sufﬁciently close initial guess. Finally, for a generic tensor, all its eigenpairs
are Newton-stable.

3

3 Learning in the noiseless case

To motivate our approach for estimating the matrix W it is instructive to ﬁrst consider the ideal noiseless case
where σ = 0. In this case, model (1) takes the form x = W ⊤h. Our problem then becomes that of factorizing the
observed matrix X = [x1, . . . , xn]

Rm×n of n samples into a product of real and binary low-rank matrices,

∈

Find W

Rd×m, H

0, 1

d×n s.t. X = W ⊤H.

∈

}
To be able to recover W we ﬁrst need conditions under which the decomposition of X into W and H is unique.
Clearly, such a factorization can be unique at most up to a permutation of its components; we henceforth ignore
this degeneracy. A sufﬁcient condition for uniqueness, similar to the one posed in Slawski et al. [47], is that H
d×n is rigid if any non-trivial linear combination of its rows yields a non-binary
is rigid. Formally, H
u
vector:

∈ {

∈ {

0, 1

= 0,

}

∀

}
Condition (6) is satisﬁed, for example, when the columns of H include ei and ei + ej for all i

∈ {

∈ {

⇔

u⊤H

n

0, 1

u

d
i=1.

ei}

The following proposition, similar in nature to the (afﬁne constrained) uniqueness guarantee in Slawski et al.
[47], shows that under condition (6) the factorization in (5) is unique and fully characterized by the binary
constraints.

= j

[d].

∈

d. Let
Rm×d be the unique right pseudo-inverse of W so W W † = Id. Then W and H are unique and for all

d×n rigid and W

Rd×m full rank with m

Proposition 1. Let X = W ⊤H with H
W †
v

∈
span(X)

∈ {

0, 1

≥

∈

0

}

,

∈

\ {

}

v⊤X

n

0, 1

v

W †.

⇔
Hence, under the rigidity condition (6), the matrix factorization problem in (5) is equivalent to the problem of
span(X) of d non-zero vectors that satisfy the binary constraints

∈ {

∈

}

ﬁnding the unique set W † =
v∗
i

1, . . . , v∗
v∗
n. The weight matrix is then W = (W †)†.

d} ⊆

⊤X

0, 1

{

∈ {

}

Algorithm outline We recover W † via a two step procedure. First, a ﬁnite set V =
of candidate vectors is computed with a guarantee that W †
eigenpairs of a d
much larger than d, so in the second step V is ﬁltered by selecting all v

span(X)
{
V . Speciﬁcally, V is computed from the set of
d tensor, constructed from the low order moments of X. Typically, the size of V will be

V that satisfy v⊤X

v1, v2, . . .

} ⊆

0, 1

n.

⊆

×

×

d

Before describing the two steps in more detail we ﬁrst state the additional non-degeneracy conditions we

∈

∈ {

}

pose. To this end, denote the unknown ﬁrst, second, and third order moments of the latent binary vector h by

(5)

(6)

(7)

(8)

p = E[h]
C = E[h
= E[h

C

Rd,

∈

h]

h

∈

⊗

⊗

⊗

Rd×d,

Rd×d×d.

h]

∈

Non-degeneracy conditions We assume the following:

(I) H is rigid.

(II) rank(2

(I, I, ei)

C) = d for all i

[d].

C

−

∈

Condition (I) implies rank(C) = rank(HH ⊤) = d. This in turn implies pi = E[hi] > 0 for all i

[d] and
that at most one variable hi has pi = 1. Such an “always on” variable can model a ﬁxed bias to x. As far as we
know, condition (II) is new and its nature will become clear shortly.
We now describe each step of our algorithm in more detail.

∈

4

(9)

(10)

(11)

(12)

(13)

(14)

(15)

Computing the candidate set To compute a set V that is guaranteed to include the columns of W † we make
use of the second and third order moments of x,

M = E[x
= E[x

⊗

x]

x

∈

Rm×m,
x]

Rm×m×m.

Given a large number of samples n
simplicity, in this section we consider the population setting where n
and

, so M and
are related to the unknown second and third order moments of h in (8) via [4]

M
1, these can be easily and accurately estimated from the sample X. For
are known exactly. M

→ ∞

M

≫

⊗

⊗

∈

M

M
Since both C and W are full rank, the number of latent units can be deduced by rank(M ) = d. Since C is
positive deﬁnite, there is a whitening matrix K

Rm×d such that

C

M = W ⊤CW,

=

(W, W, W ).

∈
K ⊤M K = Id.

Such a K can be computed, for example, by an eigen-decomposition of M . Although K is not unique, any
d lower dimensional whitened
K
⊆
tensor

span(M ) that satisﬁes (11) sufﬁces for our purpose. Deﬁne the d

×

×

d

Denote the set of eigenpairs of

by

Our set of candidates is then

=

(K, K, K).

W

M

W
U =

(u, λ)

{

Sd−1 ×

∈

R+ :

W

(I, u, u) = λu

.

}

∈
The following lemma shows that under condition (I) the set V is guaranteed to contain the d columns of W †.

} ⊆

≥

{

V =

Ku/λ : (u, λ)

U with λ

1

Rm.

Lemma 1. Let
condition (I) holds then W †

W

⊆

be the tensor in (12) corresponding to model (1) with σ = 0 and let V be as in (14). If

V . In particular, each (ui, λi) in the set of d relevant eigenpairs

U ∗ =

(u, λ)

U : Ku/λ

{

∈

W †

}

∈

1 where pi = E[hi] > 0.

has the eigenvalue λi = 1/√pi ≥
Computing the tensor eigenpairs By Lemma 1, we may construct a candidate set V that contains W † by
ﬁrst calculating the set U of eigenpairs of
. Unfortunately, computing the set of all eigenpairs of a general
symmetric tensor is computationally hard [23]. Moreover, besides the d columns of W †, the set V in (14) may
is typically O(2d) which is much larger
contain many spurious candidates, as the number of eigenpairs of
than d [12].

W

W

Nevertheless, as discussed in Section 2, several methods have been proposed for computing some eigenpairs
of a tensor under appropriate stability conditions. The following lemma highlights the importance of condition
(II) for the stability of the eigenpairs in U ∗. Note that conditions (I)-(II) do not depend on W , but only on the
distribution of the latent variables h.

Lemma 2. Let
hold, then all (u, λ)

W

∈

U ∗ are Newton-stable eigenpairs of

.

W

be the whitened tensor in (12) corresponding to model (1) with σ = 0. If conditions (I)-(II)

Hence, under conditions (I)-(II), the homotopy method in Chen et al. [13], or alternatively the O–NCM with a
sufﬁciently large number of random initializations [29], are guaranteed to compute a candidate set which includes
all the columns of W †. The next step is to extract W † out of V .

5

Algorithm 1 Recover W when σ = 0

Input: sample matrix X

⊆

M
span(M ) such that K ⊤M K = Id

1: estimate second and third order moments M ,
2: set d = rank(M )
3: compute K
4: compute whitened tensor
M
5: compute the set U of eigenpairs of
6: compute the candidate set V in (14)
7: ﬁlter ¯V =
n
}
∈
8: return the pseudo-inverse W = ¯V †

(K, K, K)

V : v⊤X

∈ {

0, 1

W

W

=

v

{

}

Filtering As suggested by Eq. (7) we select the subset of vectors ¯V

V that satisfy the binary constraints,

¯V =

v

{

∈

V : vT X

0, 1

∈ {

⊆
n
.

}

}

(16)

Indeed, under condition (I), Proposition 1 implies that ¯V = W † and the weight matrix is thus W = ¯V †.

Algorithm 1 summarizes our method for estimating W in the noiseless case and has the following recovery

guarantee.

Theorem 1. Let X be a matrix of n samples from model (1) with σ = 0. If conditions (I)-(II) hold, then Algorithm
1 recovers W exactly.

C

M

), Algorithm 1 exactly recovers W when M and

We note that when σ = 0 and conditions (I)-(II) hold for the empirical latent moments ˆC and ˆ
C

(rather than
are replaced by their ﬁnite sample estimates. The
C and
matrix factorization method SHL in Slawski et al. [47] also exactly recovers W in the case σ = 0. While its
runtime is also exponential in d, practically it may be much faster than our proposed tensor based approach. This
is because SHL constructs a candidate set of size 2d that can be computed by a suitable linear transformation of
the ﬁxed set
d tensor.
×
However, SHL does not take advantage of the large number of samples n, since only m
d sub-matrices of the
n sample matrix X are used for constructing its candidate set. Indeed, in the noisy case where σ > 0, SHL
m
has no consistency guarantees and as demonstrated by the simulation results in Section 5 it may fail at high levels
of noise. In the next section we derive a robust version of our method that consistently estimates W for any noise
level σ

d, as opposed to our candidate set which is constructed by eigenpairs of a d

0, 1

0.

×

×

×

d

}

{

≥

4 Learning in the presence of noise

The method in Section 3 to estimate W is clearly inadequate when σ > 0. However, we now show that by
making several adjustments, the two steps of computing the candidate set and its ﬁltering can be both made
robust to noise, yielding a consistent estimator of W for any σ

0.

≥

Computing the candidate set As in the case σ = 0 our goal in the ﬁrst step is to compute a ﬁnite candidate
Rm that is guaranteed to contain accurate estimates for the d columns of W †. To this end, in addition
set Vσ ⊆
in (9), we also consider the ﬁrst order moment µ = E[x] and
to the second and third order moments M and
deﬁne the following noise corrected moments,

M

Mσ = M

σ2Im,
m

−

Mσ =

M −

σ2

i=1 (cid:16)
X

µ

ei ⊗

ei + ei ⊗

µ

ei + ei ⊗

ei ⊗

⊗

⊗

µ

.

(cid:17)

(17)

6

By assumption, the noise satisﬁes E[ξ3
moments in (17) are related to these of h by [4]

i ] = 0. Thus, similarly to the moment equations in (10), the modiﬁed

Mσ = W ⊤CW,

Mσ =

C

(W, W, W ).

Hence, if Mσ and
the noiseless case, but with M and
Kσ such that K ⊤

Mσ were known exactly, a candidate set Vσ that contains W † could be obtained exactly as in
Mσ; namely, ﬁrst calculate the whitening matrix

σ MσKσ = Id and then compute the eigenpairs of the population whitened tensor

replaced with Mσ and

M

Wσ =

Mσ(Kσ, Kσ, Kσ).

M

In practice, σ2, d, µ, M and
are all unknown and need to be estimated from the sample matrix X. Assuming
m > d, the parameters σ2 and d can be consistently estimated, for example, by the methods in Kritchman and
Nadler [33]. For simplicity, we assume they are known exactly. Similarly, µ, M ,
are consistently estimated
ˆMσ ˆKσ = Id
by their empirical means, ˆµ, ˆM , and ˆ
M
Wσ and for some small 0 < τ = O(n− 1
and ˆ
2 )
take our candidate set as

Mσ( ˆKσ, ˆKσ, ˆKσ), we compute the set ˆUσ of eigenpairs of ˆ

. So, after computing the plugin estimates ˆKσ such that ˆK ⊤
σ

Wσ = ˆ

M

ˆVσ =

ˆKσu/λ : (u, λ)

ˆUσ with 1

τ

.

λ
}

{

∈
The following lemma shows that under conditions (I)-(II) the above procedure is stable to small perturbations.
Wσ and Kσ, the method computes a candidate set ˆVσ that contains
Namely, for perturbations of order δ
a subset of d vectors that are O(δ) close to the columns of W †. Furthermore, these d vectors all correspond to
Newton-stable eigenpairs of the perturbed tensor and are Ω(1) separated from the other candidates in ˆVσ.

1 in

≪

−

≤

(20)

Lemma 3. Let Kσ,
the candidate set ˆVσ in (20). If conditions (I)-(II) hold, then there are c, δ0, δ1 > 0 such that for all 0
the following holds: If the perturbed versions satisfy

Wσ be the population quantities in (19) and let ˆKσ, ˆ

Wσ be their perturbed versions, inducing
δ0

≤

≤

δ

max

ˆ
Wσ − WσkF ,

{k
ˆVσ such that

∈

ˆKσ −

k

KσkF } ≤

δ,

then any v∗

W † has a unique ˆv

∈

k ≤
Moreover, ˆv corresponds to a Newton-stable eigenpair of ˆ

−

k

ˆv

v∗

cδ.

Wσ with eigenvalue λ
δ1 > 2cδ.

v∗

˜v

k

−

k ≥

1

cδ and for all ˜v

≥

−

ˆVσ \{

∈

The proof is based on the implicit function theorem [27]; small perturbations to a tensor result in small

perturbations to its Newton-stable eigenpairs.

Now, by the delta method, the plugin estimates ˆKσ and ˆ

quantities,

Wσ are both OP (n− 1

2 ) close to their population

(18)

(19)

(21)

(22)

ˆv

,

}
(23)

(24)

k

KσkF = OP (n− 1
ˆKσ −
Wσ − WσkF = OP (n− 1
ˆ

k

2 ),

2 ).

By (24), we have that (21) holds with δ = OP (n− 1
Wσ provide a
candidate set ˆVσ that contains d vectors that are OP (n− 1
2 ) close to the columns of W †. In addition, any irrelevant
candidate is ΩP (1) far away from W †. As we show next, these properties ensure that with high probability the d
relevant candidates can be identiﬁed in ˆVσ.

2 ). Hence, by Lemma 3, the eigenpairs of ˆ

7

Filtering Given the candidate set ˆVσ computed in the ﬁrst step, our goal now is to ﬁnd a set ¯Vσ ⊆
ˆVσ of d
vectors that accurately estimate the d columns of W †. To simplify the theoretical analysis, we assume we are
given a fresh sample X of size n that is independent of ˆVσ. This can be achieved by ﬁrst splitting a sample of
size 2n into two sets of size n, one for each step.

Recall that for a vector x from model (1) and any v

Rm

∈

v⊤x = v⊤W ⊤h + σv⊤ξ.

W † will exactly satisfy v∗⊤X

Obviously, when σ > 0, the ﬁltering procedure in (16) for the noiseless case is inadequate, as typically no
n. Nevertheless, we expect that for a sufﬁciently small noise level
v∗
}
∈
W † will result in v⊤X that is close to being binary, while any v
σ, any v
sufﬁciently far from W † will result in v⊤X that is far from being binary. A natural measure for how v⊤X is
“far from being binary”, similar to the one used for ﬁltering in Slawski et al. [47], is simply its deviation from its
binary rounding,

∈ {
ˆVσ that is close to some v∗

0, 1

∈

∈

min
b∈{0,1}n k

2

.

vT X
b
−
k
2
v
n
k

k
Eq. (26) works extremely well for small σ, but fails for high noise levels. Here we instead propose a ﬁltering
procedure based on the classical Kolmogorov-Smirnov goodness of ﬁt test [34]. As we show below, this approach
gives consistent estimates of W for any σ > 0.

Before describing the test, we ﬁrst introduce the probabilistic analogue of the rigidity condition (6). For any
Rd, deﬁne its corresponding expected binary rounding,

u

∈

Clearly, r(0) = 0 and r(ei) = 0 for all i

[d]. We pose the following expected rigidity condition: for all u

r(u) = Eh∼Ph

min
b∈{0,1}

(cid:20)

(u⊤h

b)2

.

−

(cid:21)

∈
r(u) = 0

u

d
i=1.

ei}

∈ {

⇔

Analogously to the deterministic rigidity condition in (6), condition (27) is satisﬁed, for example, when Ph(ei) >
0 and Ph(ei + ej) > 0 for all i

= j
To introduce our ﬁltering test, recall that under model (1), ξ

(0, Im). Hence, for any ﬁxed v, the random

[d].

∈

variable v⊤x in (25) is distributed according to the following univariate Gaussian mixture model (GMM),

∼ N

v⊤x

∼

Ph[h]

(v⊤W ⊤h, σ2

· N

v

2).
k

k

h∈{0,1}d
X

Denote the cumulative distribution function of v⊤x by Fv. For general v, this mixture may have up to 2d distinct
W †, it reduces to a mixture of two components with means at 0 and 1. More
components. However, for v∗
∈
precisely, for any candidate v with corresponding eigenvalue λ(v)
1, deﬁne the GMM with two components

(1

−

1
λ(v)2 )

(0, σ2

v

2) + 1
k

λ(v)2

k

· N

· N

v

2).
k

k

≥
(1, σ2

Denote its cumulative distribution function by Gv. The following lemma shows that under condition (27), Gv
fully characterizes the columns of W †.

Lemma 4. Let Kσ,
computed from the eigenpairs of
any v

Wσ be the population quantities in (19) and let Vσ be the set of population candidates as
Wσ. If conditions (I)-(II) and the expected rigidity condition (27) hold, then for

Vσ and its corresponding eigenvalue λ(v),

∈

(25)

(26)

= 0,

(27)

(28)

(29)

Fv = Gv

W †.

v

∈

⇔

8

1

∞
≪
and plugin moments ˆMσ and ˆ

Mσ of (17)

Algorithm 2 Estimate W when σ > 0 and n <
Rm×n and 0 < τ

Input: sample matrix X

ˆMσ ˆKσ = Id

∈
1: estimate number of hidden units d and noise level σ2
2: compute empirical moments ˆµ, ˆM and ˆ
M
3: compute ˆKσ such that ˆK ⊤
σ
4: construct ˆ
Mσ( ˆKσ, ˆKσ, ˆKσ)
Wσ = ˆ
5: compute the set ˆUσ of eigenpairs of ˆ
Wσ
6: compute the candidate set ˆVσ in (20)
ˆVσ compute its KS score ∆n(ˆv) in (30)
7: for each ˆv
∈
ˆVσ of d vectors with smallest ∆n(ˆv)
8: select ¯Vσ ⊆
9: return the pseudo-inverse ˆW = ¯V †
σ

Given the empirical candidate set ˆVσ, Lemma 4 suggests ranking all ˆv

ˆVσ according to their goodness of
ﬁt to Gˆv and taking the d candidates with the best ﬁt. More precisely, given a sample X = [x1, . . . , xn] that is
independent of ˆVσ, for each candidate ˆv

ˆVσ we compute the empirical cumulative distribution function,

∈

∈

ˆFˆv(t) =

1
n

n

j=1
X

ˆv⊤xj ≤

t

,

}

1{

R,

t

∈

∆n(ˆv) = sup
t∈R |

ˆFˆv(t)

Gˆv(t)
|

.

−

and calculate its Kolmogorov-Smirnov score

Our estimator ¯Vσ ⊆
is the pseudo-inverse, ˆW = ¯V †
σ .

ˆVσ for W † is then the set of d vectors with the smallest scores ∆n(ˆv). The estimator for W

The following lemma shows that for sufﬁciently large n, ∆n(ˆv) accurately distinguishes between ˆv

that are close to the columns of W † from these that are not.

Lemma 5. Let v∗
Then,

∈

W † and ˆv(1), ˆv(2), . . . a sequence of random vectors such that

ˆv(n) −

k

v∗

k

= OP (n− 1

2 ).

(30)

ˆVσ

∈

In contrast, if minv∗∈W †

ˆv(n) −

k

v∗

k

= ΩP (1), then

provided the expected rigidity condition (27) holds.

∆n(ˆv(n)) = oP (1).

∆n(ˆv(n)) = ΩP (1),

Lemma 5 follows from classical and well studied properties of the Kolmogorov-Smirnov test, see for example

Billingsley [10], Lehmann and Romano [34].

Algorithm 2 summarizes our method for estimating W in the general case where σ

0 and n <

. The

≥

∞

following theorem establishes its consistency.

Theorem 2. Let x1, . . . , xn be n i.i.d. samples from model (1). If conditions (I)-(II) and the expected rigidity
condition (27) hold, then the estimator ˆW computed by Algorithm 2 is consistent, achieving the parametric rate,

ˆW = W + OP (n− 1

2 ).

Runtime The runtime of Algorithm 2 is composed of three main parts. First, O(nm3) operations are needed
to compute all the relevant moments from the data and to construct the d
Wσ. The most
time consuming task is computing the eigenpairs of ˆ
Wσ, which can be done by either the homotopy method
or O–NCM. Currently, no runtime guarantees are available for either of these methods. In practice, since there
are O(2d) eigenpairs, these methods spend O(2d
poly(d)) operations in total. Finally, since there are O(2d)
·
candidates and each KS test takes O(dn) operations [20], the ﬁltering procedure runtime is O(d2dn).

d whitened tensor ˆ

×

×

d

9

10-2

10-4

10-6

10-8

100

10-5

104

105

106

10-10

0.01

0.1

0.2

0.4

0.8

Figure 2: Left panel: Error in W vs. sample size n with σ = 0.4. Right panel: Error in W vs. noise level σ with
n = 105.

Power-stability and orthogonal decomposition The exponential runtime of our algorithm stems from the fact
Wσ is typically exponentially large. Indeed, the above algorithm
that the set UN of Newton-stable eigenpairs of
becomes intractable for large values of d. However, in some cases, the set U ∗ of d relevant eigenpairs has
additional structure so that a smaller candidate set may be computed instead of UN . Speciﬁcally, consider the
subset UP ⊆
Deﬁnition 3. An eigenpair (u, λ) is power-stable if its projected Jacobian Jp(u) is either positive or negative
deﬁnite.

UN of power-stable eigenpairs of

Wσ.

Typically, the number of power-stable eigenpairs is signiﬁcantly smaller than the number of Newton-stable

eigenpairs.1 In addition, UP can be computed by the shifted higher-order power method [31, 32].

Similarly to Lemma 2, one can show that UP is guaranteed to contain U ∗ whenever the following stronger

version of condition (II) holds: for all (ui, λi)

U ∗, the matrix

∈
(W KLui)⊤(2

(I, I, ei)

C)(W KLui)

C

−

(31)

is either positive-deﬁnite or negative-deﬁnite.

As an example, consider the case where Ph has the support h

Id. Then model (1) corresponds to a GMM
with d spherical components with linearly independent means. In this case, both C and
are diagonal with
ei diag(p)Lei , which by condition (I) are all
p on their diagonal. Thus, the matrices in (31) take the form
negative-deﬁnite. In fact, in this case,
Wσ has an orthogonal decomposition and the d orthogonal eigenpairs in
U ∗ are the only negative-deﬁnite power-stable eigenpairs of
Wσ [4]. Similarly, when Ph is a product distribution,
the same orthogonal structure appears if the centered moments of x are used instead of M and
. As shown
in Anandkumar et al. [4], the power method, accompanied with a deﬂation procedure, decompose an orthogonal
tensor in polynomial time, thus implying an efﬁcient algorithm in these cases.

∈
L⊤

M

−

C

5 Experiments

We demonstrate our method in two scenarios: (I) simulations from the exact binary model (1); and (II) learn-
ing a common population genetic admixture model. Code to reproduce the simulation results can be found at
https://github.com/arJaffe/BinaryLatentVariables.

1We currently do not know whether the number of power-stable eigenpairs of a generic tensor is polynomial or exponential in d.

10

Figure 3: Illustration of the admixture model.

∈
∼ N

Simulations We generated n samples from model (1) with d = 6 hidden units, m = 30 observable features, and
(0, Im). The m columns of W were drawn uniformly from the unit sphere Sd−1. Fixing a
Gaussian noise ξ
Rd×d, each hidden vector h was generated independently by
mean vector a
ﬁrst drawing r

∼ N
Rd and a covariance matrix R

(a, R) and then taking its binary rounding.

∈

Figure 2 shows the error, in Frobenius norm, averaged over 50 independent realizations of X as a function
of n (upper panel) and σ (lower panel) for ﬁve methods: (i) our spectral approach, detailed in Algorithm 2
(Spectral); (ii) Algorithm 2 followed by an additional single weighted least square step detailed in Appendix I
(Spectral+WLS); (iii) SHL, the matrix decomposition approach of Slawski et al. [47]2; (iv) ALS with a random
starting point (see Appendix J); and (v) an oracle estimator that is given the exact matrix H and computes W via
least squares.

As one can see, as opposed to SHL, our method is consistent for σ > 0 and achieves an error rate O(n− 1
2 )
corresponding to a slope of
1 in the left panel of Fig. 2. In addition, as seen in the right panel of Fig. 2, at low
level of noise our method is comparable to SHL, whereas at high level of noise it is far more accurate. Finally,
adding a weighted least square step reduces the error for low noise levels, but increases the error for high noise
levels.

−

Population genetic admixture We present an application of our method to a fundamental problem in popu-
lation genetics, known as admixture, illustrated in Fig. 3. Admixture refers to the mixing of d
2 ancestral
populations that were long separated, e.g., due to geographical or cultural barriers [1, 35, 42]. The observed data
X is an m
n matrix where m is the number of modern “admixed” individuals and n is the number of relevant
locations in their DNA, known as SNPs. Each SNP corresponds to two alleles and different individuals may have
according to the number
different alleles. Fixing a reference allele for each location, Xij takes values in
[m] at locus j
of reference alleles appearing in the genotype of individual i

0, 1
[n].

2 , 1

×

≥

}

Given the genotypes X, an important problem in population genetics is to estimate the following two quanti-
[0, 1]d×n whose entry Hkj is the frequency of the reference allele at locus
[0, 1]d×m whose columns sum

ties. The allele frequency matrix H
[n] in ancestral population k
j
to 1 and its entry Wki is the proportion of individual i’s genome that was inherited from population k.
A common model for X in terms of W and H is to assume that the number of alleles 2Xij ∈ {

∈
[d]; and the admixture proportion matrix W

0, 1, 2

is the

∈

∈

∈

}

sum of two i.i.d. Bernoulli random variables with success probability Fij =

d
k=1 WkiHkj. Namely,

{
∈

∈

2Code taken from https://sites.google.com/site/slawskimartin/code. For each realization, we aggregated over 50

runs of SHL and chose the output H, W that minimized kX − W ⊤HkF .

H

Xij|

∼

1
2 ·

Binomial(2, Fij).

P

11

cALS
Admixture
Spectral

)
d
·

m
(
/

|
j
i

W
−

j
i

ˆW

|

j
,
i

P

r
o
r
r
E

0.4

0.3

0.2

0.1

0

0.1

1
Dirichlet parameter α

10

Figure 4: Average absolute error in ˆW vs. Dirichlet parameter α for d = 3 ancestral populations and m = 50
admixed individuals.

Note that under this model

E[X

H] = F = W ⊤H.

|

(32)

Although (32) has similar form to model (1), there are two main differences; the noise is not normally distributed
and the matrix H is non-binary. Yet, model (1) is expected to be a good approximation whenever various alleles
are rare in some populations but abundant in others. Speciﬁcally, for ancestral populations that have been long
separated, some alleles may become ﬁxed in one population (i.e., reach frequency of 1) while being totally absent
in others.

Genetic simulations We followed a standard simulation scheme appearing, for example, in Gravel [21], Price
et al. [41], Xue et al. [51]. First, using SCRM [40], we simulated d = 3 ancestral populations separated for
4000 generations and generated the genomes of 40 individuals for each. H was then computed as the frequency
of the reference alleles in each population. Next, the columns of W were sampled from a symmetric Dirichlet
0. Finally, the genomes of m = 50 admixed individuals were generated as
distribution with parameter α
mosaics of genomic segments of individuals from the ancestral populations with proportions W . The mosaic
nature of the admixed genomes is an important realistic detail, due to the linkage (correlation) between SNPs
[51]. A detailed description is in Appendix K.

≥

We compare our algorithm to two methods. The ﬁrst is Admixture [1], one of the most widely used algorithms
in population genetics, which aims to maximize the likelihood of X. A recent spectral approach is ALStructure
[11], where an estimation of span(W ⊤) via Chen and Storey [14] is followed by constrained ALS iterations
of W and H. For our method, two modiﬁcation are needed for Algorithm 2. First, since the distribution of
Mσ as calculated by (17) do not satisfy (18). Instead,
Xij −
we implemented a matrix completion algorithm derived in [30] for a similar setup, see Appendix H for more
details. In addition, the ﬁltering process described in Section 4 is no longer valid. However, as d is relatively
small, we were able to perform exhaustive search over all candidate subsets of size d and choose the one that
maximized the likelihood.

i hj is not Gaussian, the corrected moments ˆMσ, ˆ

wT

Figure 4 compares the results of the 3 methods for α = 0.1, 1, 10. The spectral method outperforms Admix-

ture and ALStructure for α = 1, 10 and performs similarly to Admixture for α = 0.1.

Acknowledgments This research was funded in part by NIH Grant 1R01HG008383-01A1.

12

References

[1] David H Alexander, John Novembre, and Kenneth Lange. Fast model-based estimation of ancestry in

unrelated individuals. Genome research, 19(9):1655–1664, 2009.

[2] Animashree Anandkumar, Dean P Foster, Daniel J Hsu, Sham M Kakade, and Yi-Kai Liu. A spectral
In Advances in Neural Information Processing Systems, pages

algorithm for latent dirichlet allocation.
917–925, 2012.

[3] Animashree Anandkumar, Daniel Hsu, and Sham M Kakade. A method of moments for mixture models

and hidden markov models. In COLT, volume 1, page 4, 2012.

[4] Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decom-
positions for learning latent variable models. Journal of Machine Learning Research, 15(1):2773–2832,
2014.

[5] Forough Arabshahi and Anima Anandkumar. Spectral methods for correlated topic models. In Artiﬁcial

Intelligence and Statistics, pages 1439–1447, 2017.

[6] Sanjeev Arora, Rong Ge, Ravindran Kannan, and Ankur Moitra. Computing a nonnegative matrix
factorization–provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of computing,
pages 145–162. ACM, 2012.

[7] Said Baadel, Fadi Thabtah, and Joan Lu. Overlapping clustering: A review. In SAI Computing Conference

(SAI), 2016, pages 233–237. IEEE, 2016.

[8] Arindam Banerjee, Chase Krumpelman, Joydeep Ghosh, Sugato Basu, and Raymond J Mooney. Model-
based overlapping clustering. In Proceedings of the eleventh ACM SIGKDD international conference on
Knowledge discovery in data mining, pages 532–537. ACM, 2005.

[9] Emmanuelle Becker, Benoˆıt Robisson, Charles E Chapple, Alain Gu´enoche, and Christine Brun. Multi-
functional proteins revealed by overlapping clustering in protein interaction network. Bioinformatics, 28
(1):84–90, 2011.

[10] Patrick Billingsley. Convergence of probability measures. John Wiley & Sons, 2013.

[11] Irineo Cabreros and John D Storey. A nonparametric estimator of population structure unifying admixture

models and principal components analysis. bioRxiv, page 240812, 2017.

[12] Dustin Cartwright and Bernd Sturmfels. The number of eigenvalues of a tensor. Linear algebra and its

applications, 438(2):942–952, 2013.

[13] Liping Chen, Lixing Han, and Liangmin Zhou. Computing tensor eigenvalues via homotopy methods.

SIAM Journal on Matrix Analysis and Applications, 37(1):290–319, 2016.

[14] Xiongzhi Chen and John D Storey. Consistent estimation of low-dimensional latent structure in high-

dimensional data. arXiv preprint arXiv:1510.03497, 2015.

[15] KyungHyun Cho, Alexander Ilin, and Tapani Raiko. Improved learning of gaussian-bernoulli restricted
boltzmann machines. Artiﬁcial Neural Networks and Machine Learning–ICANN 2011, pages 10–17, 2011.

[16] Chun-Feng Cui, Yu-Hong Dai, and Jiawang Nie. All real eigenvalues of symmetric tensors. SIAM Journal

on Matrix Analysis and Applications, 35(4):1582–1601, 2014.

[17] David Donoho and Victoria Stodden. When does non-negative matrix factorization give a correct decom-
position into parts? In Advances in neural information processing systems, pages 1141–1148, 2004.

13

[18] Alan Frieze, Mark Jerrum, and Ravi Kannan. Learning linear transformations. In Foundations of Computer

Science, 1996. Proceedings., 37th Annual Symposium on, pages 359–368. IEEE, 1996.

[19] Gene H Golub and Charles F Van Loan. Matrix computations, volume 3. JHU Press, 2012.

[20] Teoﬁlo Gonzalez, Sartaj Sahni, and William R. Franta. An efﬁcient algorithm for the kolmogorov-smirnov

and lilliefors tests. ACM Transactions on Mathematical Software (TOMS), 3(1):60–64, 1977.

[21] Simon Gravel. Population genetics models of local ancestry. Genetics, 191(2):607–619, 2012.

[22] Chun-Hua Guo, Wen-Wei Lin, and Ching-Sung Liu. A modiﬁed newton iteration for ﬁnding nonnegative

z-eigenpairs of a nonnegative tensor. arXiv preprint arXiv:1705.07487, 2017.

[23] Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM (JACM),

60(6):45, 2013.

[24] Geoffrey Hinton. A practical guide to training restricted boltzmann machines. Momentum, 9(1):926, 2010.

[25] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks.

science, 313(5786):504–507, 2006.

[26] Daniel Hsu and Sham M Kakade. Learning mixtures of spherical gaussians: moment methods and spectral
decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer Science,
pages 11–20. ACM, 2013.

[27] John H Hubbard and Barbara Burke Hubbard. Vector calculus, linear algebra, and differential forms: a

uniﬁed approach. Matrix Editions, 2015.

[28] Aapo Hyv¨arinen, Juha Karhunen, and Erkki Oja. Independent component analysis, volume 46. John Wiley

& Sons, 2004.

[29] Ariel Jaffe, Roi Weiss, and Boaz Nadler. Newton correction methods for computing real eigenpairs of

symmetric tensors. arXiv preprint arXiv:1706.02132, 2017.

[30] Prateek Jain and Sewoong Oh. Learning mixtures of discrete product distributions using spectral decompo-

sitions. In Conference on Learning Theory, pages 824–856, 2014.

[31] Tamara G Kolda and Jackson R Mayo. Shifted power method for computing tensor eigenpairs. SIAM

Journal on Matrix Analysis and Applications, 32(4):1095–1124, 2011.

[32] Tamara G Kolda and Jackson R Mayo. An adaptive shifted power method for computing generalized tensor

eigenpairs. SIAM Journal on Matrix Analysis and Applications, 35(4):1563–1581, 2014.

[33] Shira Kritchman and Boaz Nadler. Non-parametric detection of the number of signals: Hypothesis testing

and random matrix theory. IEEE Transactions on Signal Processing, 57(10):3930–3941, 2009.

[34] Erich L Lehmann and Joseph P Romano. Testing statistical hypotheses. Springer Science & Business

Media, 2006.

[35] Jun Z Li, Devin M Absher, Hua Tang, Audrey M Southwick, Amanda M Casto, Sohini Ramachandran,
Howard M Cann, Gregory S Barsh, Marcus Feldman, Luigi L Cavalli-Sforza, et al. Worldwide human
relationships inferred from genome-wide patterns of variation. science, 319(5866):1100–1104, 2008.

[36] Lek-Heng Lim. Singular values and eigenvalues of tensors: a variational approach.

In Computational
Advances in Multi-Sensor Adaptive Processing, 2005 1st IEEE International Workshop on, pages 129–132.
IEEE, 2005.

14

[37] Jan Melchior, Nan Wang, and Laurenz Wiskott. Gaussian-binary restricted boltzmann machines for model-

ing natural image statistics. PloS one, 12(2):e0171015, 2017.

[38] Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of gaussians.

In
Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Symposium on, pages 93–102. IEEE,
2010.

[39] Elchanan Mossel and S´ebastien Roch. Learning nonsingular phylogenies and hidden markov models. In
Proceedings of the thirty-seventh annual ACM symposium on Theory of computing, pages 366–375. ACM,
2005.

[40] Paul R. Staab, Sha Zhu, Dirk Metzler, and Gerton Lunter. scrm: efﬁciently simulating long sequences using

the approximated coalescent with recombination. Bioinformatics, 31(10):1680–1682, 2015.

[41] Alkes L Price, Arti Tandon, Nick Patterson, Kathleen C Barnes, Nicholas Rafaels, Ingo Ruczinski, Terri H
Beaty, Rasika Mathias, David Reich, and Simon Myers. Sensitive detection of chromosomal segments of
distinct ancestry in admixed populations. PLoS genetics, 5(6):e1000519, 2009.

[42] Jonathan K Pritchard, Matthew Stephens, and Peter Donnelly.
multilocus genotype data. Genetics, 155(2):945–959, 2000.

Inference of population structure using

[43] Liqun Qi. Eigenvalues of a real supersymmetric tensor. Journal of Symbolic Computation, 40(6):1302–

1324, 2005.

[44] Phillip A Regalia and Eleftherios Koﬁdis. Monotonic convergence of ﬁxed-point algorithms for ica. IEEE

Transactions on Neural Networks, 14(4):943–949, 2003.

[45] Eran Segal, Alexis Battle, and Daphne Koller. Decomposing gene expression into cellular processes. In

Proceedings of the Paciﬁc Symposium on Biocomputing, volume 8, pages 89–100, 2002.

[46] Oﬁr Shalvi and Ehud Weinstein. Super-exponential methods for blind deconvolution. IEEE Transactions

on Information Theory, 39(2):504–519, 1993.

[47] Martin Slawski, Matthias Hein, and Pavlo Lutsik. Matrix factorization with binary components. In Ad-

vances in Neural Information Processing Systems, pages 3210–3218, 2013.

[48] Graham W Taylor, Geoffrey E Hinton, and Sam T Roweis. Modeling human motion using binary latent

variables. In Advances in neural information processing systems, pages 1345–1352, 2007.

[49] A-J Van der Veen. Analytical method for blind binary signal separation. IEEE Transactions on Signal

Processing, 45(4):1078–1082, 1997.

[50] Nan Wang, Jan Melchior, and Laurenz Wiskott. An analysis of gaussian-binary restricted boltzmann ma-

chines for natural images. In ESANN, 2012.

[51] James Xue, Todd Lencz, Ariel Darvasi, Itsik Peer, and Shai Carmi. The time and place of european admix-

ture in ashkenazi jewish history. PLoS genetics, 13(4):e1006644, 2017.

15

A Proof of Proposition 1

Uniqueness of the factorization readily follows from (7) so we proceed to prove (7). First note that span(X) =
span(W ⊤) = span(W †). Since W is full rank, we have W W † = Id. Hence,

(W †)⊤X = (W W †)⊤H = H

0, 1

d×n.

∈ {

}

So any v∗
be such that v⊤X
W is full rank and v

W † satisﬁes the binary constraint v∗⊤X

n. For the other direction, let v
0, 1
n. Since v⊤X = (W v)⊤H, the rigidity condition (6) implies W v

}
0, 1
span(W †), v must be a column of W †.

∈ {

∈

}

0
span(X)
\ {
}
d
ei}
i=1. Since

∈
∈ {

∈ {
∈

B Proof of Lemma 1

Since the vector h is binary, its second and third order moments are related as follows. For all i, j
Cjii = E[h2

i hj] = E[hihj] = Cij.

Ciij =

Ciji =

[d],

∈

(33)

Since W is full rank, W W † = Id. Hence, applying W † multi-linearly on the moment equations in (10) we
obtain

C = (W †)⊤M W †,

=

C

M

(W †, W †, W †).

Thus, the equality in (33) is equivalent to

(W †, W †, W †)]iij = [(W †)⊤M W †]ij .

[

M

Let Y ∗

Rd×d be the full rank matrix that satisﬁes W † = KY ∗ where K is the whitening matrix in (11). Then,

∈

W

(W †, W †, W †) =

(KY ∗, KY ∗, KY ∗) =

(Y ∗, Y ∗, Y ∗)

M

M

W

where

is the whitened tensor in (12). Similarly, by (11),

(W †)⊤M W † = (Y ∗)⊤(K T M K)(Y ∗) = (Y ∗)⊤Y ∗.

Inserting these into (34), the matrix Y ∗ must satisfy

(Y ∗, Y ∗, Y ∗)]iij = [(Y ∗)⊤(Y ∗)]ij ,

[

W

i, j

∀

∈

[d].

The following lemma, proved in Appendix G, shows that Eq. (36) is nothing but a tensor eigen-problem. Specif-
ically, the columns of Y ∗, up to scaling, are eigenvectors of

.

W
Rd×d×d be an arbitrary symmetric tensor. Then, a matrix Y = [y1, . . . , yd]

[d], yk = uk/λk, where (uk, λk)d

∈
k=1 are d eigenpairs of

Rd×d of
with

W

W ∈

Lemma 6. Let
rank d satisﬁes (36) if and only if for all k
linearly independent

uk}
By Lemma 6, the set of scaled eigenpairs

d
k=1.

∈

{

Since W † = KY ∗, the set

Ky
To show that each y = u/λ

{

}
∈

≥

∈

of
{
is guaranteed to contain W †.
1, note that the vector Ky is a column of W †, so W Ky = ei for
Y ∗ has λ

is guaranteed to contain the d columns of Y ∗.

y = u/λ
}

W

some i

[d]. Hence, by the deﬁnition of the whitened tensor (12) and the moment equation (10),

(34)

(35)

(36)

W

(y, y, y) =
=

(Ky, Ky, Ky) =

(W Ky, W Ky, W Ky)

M

C

(ei, ei, ei) =

C
Ciii = E[hi]

1.

≤

16

(37)

(38)

On the other hand, since (u, λ) is an eigenpair of

with eigenvalue λ =

(u, u, u),

(y, y, y) =

(u, u, u) =

W

W
1
λ3 W

W

1
λ2 .

By convention, λ

0. Hence,

≥

concluding the proof.

C Proof of Lemma 2

λ = 1/

E[hi]

1,

≥

p

Let (u, λ)
W
that under conditions (I)-(II) the projected Jacobian matrix Jp(u) = L⊤
u

such that v∗ = Ku/λ

U ∗ be an eigenpair of

W †. To show Newton-stability we need to show
g(u)Lu in (4) is full rank d

1.

∈

∈

∇

−

The Jacobian matrix

g(u) is

∇

∇

g(u) = 2
= 2

(I, I, u)
(I, I, u)

W
W

3u
W
3λuu⊤

(I, u, u)⊤
λId.

−

−
−

(u, u, u)Id

− W

u u = 0, the second term in (38) does not contribute to Jp(u). For the ﬁrst term in (38), by (12) and

Since L⊤
(10),

Since v∗ = Ku/λ is a column of W †, W Ku = λei for some i

[d]. Thus,

(I, I, u) =

(K, K, Ku) =

(W K, W K, W Ku).

W

W

M

C

C

∈

C

For the third term in (38), by the deﬁnition of K in (11),

(I, I, u) = λ

(W K, W K, ei) = λK ⊤W ⊤

(I, I, ei)W K.

(39)

Putting the last two equalities in (38) and applying the projection Lu we obtain

Id = K ⊤M K = K ⊤W ⊤CW K.

Jp(u) = L⊤
u

g(u)Lu = λL⊤

u K ⊤W ⊤(2

(I, I, ei)

C)W KLu.

C
1 and W and K are full rank, condition (II) implies that Jp(u) is full rank as well. Thus, (u, λ) is a

∇

−

Since λ
Newton-stable eigenpair of

≥

.

W

D Proof of Lemma 3

Lemma 3 follows from the following lemma which establishes the stability of Newton-stable eigenpairs of a
tensor

+ ∆

=

.

to small perturbations ˜
W

W

W

W

Lemma 7. Let (u, λ) be a Newton-stable eigenpair of
all sufﬁciently small ε > 0 the following holds. For any ˜
W
eigenpair ( ˜u, ˜λ) of ˜
W

such that

W

with λ

≥

such that

1. There are c1, c2, ε0 > 0 such that for
ε there exists a unique

˜
W − WkF ≤

k

k
In addition, ( ˜u, ˜λ) is Newton-stable and any other eigenvector ˜v of ˜
W

k ≤

| ≤

−

−

|

u

˜u

c1ε

and

˜λ

λ

c2ε.

satisﬁes

˜v

k

u

−

k ≥

ε0.

(40)

17

Proof of Lemma 7. For a tensor
function Q : Rd+s

Rd by

T ∈

→

Rd×d×d let t

Rs be the vector of s = d3 entries

∈

. Deﬁne the

{Tijk}

Note that for any t
(v, β) is an eigenpair of t with eigenvalue β =

Rs and (v, β)

Rd

×

∈

∈

= 0, we have that Q(v, t) = 0 if and only if
(v, v, v).3 Denote the gradients of Q with respect to v and t by

Q(v, t) =

(I, v, v)

T
R with v

− T
= 0 and β

(v, v, v)

v.

·

T

A(v, t) =
B(v, t) =

∇
∇
and let (u, λ)

vQ(v, t)
tQ(v, t)

∈
∈
Sd−1 ×

∈

W

Rd×d,
Rd×s.

D(u, w) =

A(u, w) B(u, w)
Is (cid:19)

0

.

(cid:18)

Rs be the vectorization of

Let w
λ
matrix is invertible,

≥

∈

1. Since u is Newton-stable and λ > 0, A(u, w) is invertible. In addition, the following (d + s)

R+ be a Newton-stable eigenpair of w with
(d + s)

×

D(u, w)−1

Let γD = 1/
constant of
in the neighborhood,

∇

k
k
Q(v, t) = [A(v, t), B(v, t)]

∈

> 0 be the smallest singular value of D(u, w) and let LD <

Rd×(d+s) in a small neighborhood of (u, w), namely,

∞

be the Lipschitz
(v, t), (˜v, ˜t)
∀

Q(v, t)

Q(˜v, ˜t)

LDk

k ≤

(v, t)

(˜v, ˜t)
.
k

−

− ∇

k∇

≤

⊂
ε1 := γ2
D/(2LD), there exists a unique continuously differentiable mapping ˜u : Bε(w)
w

Let Bε(w)
ε
that Q( ˜u( ˜w), ˜w) = 0 for all ˜w
Bε(w). In other words, for any ˜w such that
unique vector ˜u in all B2ε/γD (u) that is an eigenpair of ˜w. Equivalently, for ˜
W
there exists a unique eigenvector ˜u of ˜
W

Rs be the ball of radius ε centered at w. Then by the implicit function theorem [27], for any
B2ε/γD (u) such
ε, there exist a
˜
ε,
W − WkF ≤

˜w
k
−
such that

→
k ≤
k

such that

∈

The bound on
be the Lipschitz constant of q in the neighborhood of (u, w). Then,

readily follows from (41). Indeed, let q : Rd+s

λ
|

−

|

˜λ

→

R be q(v, t) =

(v, v, v) and let Lλ

T

˜u

k

u

−

k ≤

2ε/γD := c1ε.

(41)

˜λ

|

λ
|

−

q(u, w)
|
˜w

2 +
k

k

u

−

−

w

2
k

−

+ 1

ε := c2ε.

·

=

≤

≤

q( ˜u, ˜w)

|
Lλ

Lλ

˜u

k
q
2
γD

r

→

As for the Newton-stability of ˜u, let r : Rd+s

R+ be r(v, t) = 1/

A(v, t)−1

value of A(v, t). Since (u, λ) is a Newton-stable eigenpair of w,
Lγ be the Lipschitz constant of r(v, t) in the neighborhood [19]. Then, for ε
γA/2 > 0, so ( ˜u, ˜λ) is a Newton-stable eigenpair of ˜w.
r( ˜u, ˜w)

≤

k

γA > 0 such that r(u, w)
∃

k

, the minimal singular
γA. Let
ε2 := γ/(2Lγ), we have

≥

≥

Finally, we show that any other eigenvector ˜v of ˜
W

ε0 > 0 such that

˜v

k

−

˜u

2ε0 for any other eigenvector ˜v. Hence, for ε

ε0,

is apart from u. Since ˜u is Newton-stable, there exists

k ≥
˜v

k
ε0, ε1, ε2}

{

u

˜v

˜u

˜u

u

˜v

˜u

−

k ≥

−
k
(cid:12)
and c1, c2, ε0 as above concludes the proof of the lemma.
(cid:12)

k
(cid:12)
(cid:12)

k − k

≥ k

k −

−

≥

−

ε0.

≤
ε

3This does not precisely hold when β = 0 since Q(v, t) = 0 does not imply kvk = 1 in this case, but only that v is proportional to an

Taking ε

min

≤

eigenvector.

18

Lastly, for completeness, we show that γD ≥

γA
√γ2
A+d

.

γ−1
D =

k

D(u, w)−1

k
A(u, w)−1

≤

k
q

≤ s

1 +

k

1 +

2(1 +

k
B(v, w)
k
γ2
A

k
2

.

B(u, w)
k

2) +

2

Isk

k

(42)

To bound

B(u, w)
k

k

, note that Q(u, w) is linear in w and its i-th entry is given by

[Q(u, w)]i =

wjklujukul)ui.

wiklukul −

(
Xj,k,l

Xk,l
m matrix B(u, w) has entries

Thus, the d

×

which is independent of w. Recalling that

[B(u, w)]i,(jkl) = [

uiuj)ukul,

(43)

wQ(u, w)]i,(jkl) = (δij −
∇
= 1,

u

k

k

2

B(u)
k

k

2
B(u)
F =
k

≤ k

(δij −

uiuj)2u2

ku2
l

d

Xi,j,k,l=1

(δ2

ij −

2δijuiuj + u2

i u2

j ) = d

1.

−

d

=

i,j=1
X
γA
√γ2
A+d

.

Putting this bound in (42), we obtain γD ≥

E Proof of Lemma 4

Let v∗
corresponds to the two component GMM

W †. Then

[d] such that v∗⊤W ⊤h = hi ∈ {

i
∃

∈

∈

}

0, 1

. Hence, by (28), the c.d.f. Fv∗ of v∗⊤x

(1

pi)

−

· N

(0, σ2

v∗

k

2) + pi · N
k

(1, σ2

v∗

2).

k

k

By Lemma 1 we have pi = 1/λ(v∗)2. Thus, Fv∗ = Gv∗ .

For the other direction, let v

d
i=1. Moreover, by Eq. (23) of Lemma 3,

e⊤
i }

{

Vσ \

W †. Since W is full rank, the d-dimensional vector u⊤ = v⊤W ⊤ /
∈

∈

Hence, there exists ε0 > 0 such that

inf
v∈Vσ \W †

min
v∗∈W † k

v

−

v∗

k ≥

δ1 > 0.

u

min
i∈[d] k

−

eik ≥

ε0.

So by the expected rigidity condition (27), there exists η0 > 0 such that r(u)
component with mean that is bounded away from both 0 and 1 and thus Fv
η1 > 0 such that

η0. It follows that Fv has a
= Gv. In particular, there exists

≥

sup
t∈R |

Fv(t)

Gv(t)

η1.

−

| ≥

19

(44)

F Proof of Lemma 5

Recall that our sample of size 2n was split into two separate parts each of size n. The ﬁrst n samples were used
to estimate the tensor eigenvectors, and the last n samples to estimate the empirical cdf’s of their projections onto
the eigenvectors.

For any ˆv that is close to a vector v, we bound ∆n(ˆv) =

ˆFˆv

k
k∞ +

Gˆv

−
Fv

k∞ by the triangle inequality,
Gv
Gv

Gˆv

k∞ +

k∞.

ˆFˆv

Gˆv

ˆFˆv

k∞ ≤ k

Fˆv

k∞ +

Fˆv

Fv

k

−

−

−
We now consider each of the four terms separately, starting with the ﬁrst one. Since σ > 0, the cdf Fˆv : R
[0, 1] is continuous and the distribution of
Wolfowitz inequality,
n

→
k∞ is independent of ˆv. Then, by the Dvoretzky-Kiefer-
k∞ is w.h.p. of order O(1/√n) for any ˆv, and in particular tends to zero as

ˆFˆv

ˆFˆv

Fˆv

Fˆv

0.

−

−

−

−

k

k

k

k

k

(45)

→

As for the second term, write ˆv = v + η. Then,

ˆv⊤x = v⊤x + η⊤x.

Recall that x = W ⊤h + σξ. Hence,
W
Gaussian random variable with standard deviation σ
with probability tending to one as n
can be bounded by Knk

. This, in turn, implies that
k

→ ∞

η⊤x

+ σ

√d
k2
|
k
|
| ≤ k
. So, there exists Kn > √d
η
W
k
k
k
η⊤xj| ≤
η
, for all n samples xj ∈
Knk

. The term η⊤ξ is simply a zero mean
k2 + σn1/3 such that
v⊤x
ˆv⊤x
. Thus,
|
k

η⊤ξ

X,

−

−

ˆv

η

v

k

|

|

|

Fˆv

k

−

Fv

k∞ ≤

LKnk

ˆv

v

,

k

−

where L = maxt F ′
rate OP (1/√n). Since Kn grows much more slowly with n, this term tends to zero.

v(t), which is ﬁnite for any σ > 0. Now, suppose the sequence ˆv(n) converges to some v at

Let us next consider the fourth term, and leave the third term to the end. Here note that Gv is continuous in

its parameter v. So if the sequence ˆv(n) converges to some v, then this term tends to zero.

Finally, consider the third term. If the limiting vector v belongs to the correct set, namely v∗
k∞ tends to zero as required.

Fv = Gv, and thus overall

ˆFˆv

Gˆv

−

k

∈

W †, then instead of Eq. (45) we invoke the following inequality:

W †, then

In contrast, if ˆv converges to a vector v /
∈

ˆFˆv

Gˆv

k∞ ≥ k

−

k
k∞ is strictly larger than zero whereas the three other remaining terms tend to zero as n
Gv

−

−

−

−

k∞ − k

k∞ − k

Gˆv

k∞ − k

Gv

k∞.

Fv

Gv

Fv

Fˆv

Fˆv

ˆFˆv

as

→ ∞

Fv

Here
k
above.

−

G Proof of Lemma 6

Multiplying (36) from the right by the full rank matrix Y −1 we obtain the equations

Note that for all i

[d],

∈

Since

is symmetric, we thus have

W

Writing yi = ui/λi we obtain the eigenpair equation

(Y, Y, I)]iij = [Y ⊤]ij,

[

W

i, j

∀

∈

[d].

[

W

(Y, Y, I)]iij = [

(yi, yi, I)]j.

W

(I, yi, yi) = yi,

W

i
∀

∈

[d].

(I, ui, ui) = λiui,

W

[d].

i
∀

∈

20

The other direction readily follows from the deﬁnition of eigenpairs.

H Matrix and tensor denoising

by (17). This modiﬁcation is suited for additive
In Algorithm 2, we modify the diagonal elements of M,
Gaussian noise, but is not applicable for the case where X = binomial(2, W T H). Instead, we implemented a
method derived in [30] for a similar setup.

M

First, we treat the diagonal elements of Mσ as missing data, and complete them with the following iterative
=

of R(k); and (ii) update the diagonal elements by R(k+1)

i )jj .

i λiviv⊤
Next, instead of computing

steps. (i) compute the ﬁrst d eigenpairs
(

vi, λi}
{
Mσ via (17) and then
P
following system of linear equations. Let K † be the pseudo-inverse matrix of K, and PΩ(
T
operation over the tensor

Wσ via (19), we compute

such that,

Wσ directly by solving the
) denote a masking

jj

T

PΩ(

) =

T

Tijk
0

(

= k

= j

i
o.w

We estimate

by the following minimization problem,

W

ˆ
W

= argmin

PΩ

W k

(K †, K †, K †)

PΩ(

2
F

)
k

M

−

W
(cid:0)

This method depends only on the off-diagonal elements of M and
W T H and the noise has bounded variance.

(cid:1)

M

and hence is applicable whenever E[X

H] =

|

I Adding a weighted least square step to the spectral method

In section 5, we compare the results of algorithm 2 with and without an additional single weighted least square
step. Given an estimate ˆW , for each observed instance xj we calculate the conditional likelihood
h) for
the 2d possible binary vectors h

(xj|
L

0, 1

d,

∈ {

}

(xj|
L

h) =

1
√2πσ2

exp

xj −

2

ˆW T h
k

− k

(2σ2)

(cid:16)
For each instance xj, we keep the top K = 6 vectors h1j , . . . , hKj with the highest likelihood. Let Π
[0, 1]K×n be a weight matrix such that Πkj is proportional to
(xj |
L
estimate ˆWwls is the minimizer of the weighted least square problem,

∈
k Πkj = 1 for all j. The new

hkj ), and

.

(cid:17)

P

ˆWwls = argmin

W

Πkj k

xj −

W T hkj k

2.

n

K

j=1
X

Xk=1

J Alternating least squares for W and H

In section 5, we compare the results of the spectral approach to the following ALS iterations, with a random
starting point.

W (k) = argmin

X

W T H (k−1)

W ∈Rd×m k

−

X

(W (k))T H

ˆH (k) = argmin

H∈Rd×n k
H (k) = argmin

−

H

H∈{0,1}d×n k

−

ˆH (k)

2
F ,

k

2
F

k

2
F

k

21

K Genetic admixture simulations

The simulated admixture data was generated via the following steps:

1. We used SCRM [40] to simulate a split between d = 3 ancestral populations, with separation time of 4000
106 for each of the three populations.
generations. The simulator generated 40 chromosomes of length 250
The simulation parameters were determined as N0 = 104 effective population size, 10−8 mutation rate (per
base pair per generation), and 10−8 recombination rate (per base pair per generation).

·

2. We sampled the proportion matrix W from a Dirichlet distribution with parameter α.

·

3. Two chromosomes of length 250

106 were created for each of the m = 50 admixed individuals with the
following steps: (i) An ancestral population was sampled according to W, say, population hA. (ii) One
of the 40 chromosomes was sampled from hA, say hA(k) (iii) A block length l was sampled from an
exponential distribution with rate 20 per Morgan corresponding admixture event happening 20 generations
ago (in our case, 1 Morgan was 108 base pairs). (iv) A block of length l was copied from chromosome
hA(k) to the corresponding locations in the new admixed chromosome. We repeated steps (i)-(iv) until
completion of the chromosome.

22

