6
1
0
2
 
c
e
D
 
6
 
 
]

V
C
.
s
c
[
 
 
1
v
9
3
9
1
0
.
2
1
6
1
:
v
i
X
r
a

Correlation Alignment for Unsupervised
Domain Adaptation

Baochen Sun, Jiashi Feng, and Kate Saenko

Abstract In this chapter, we present CORrelation ALignment (CORAL), a sim-
ple yet effective method for unsupervised domain adaptation. CORAL minimizes
domain shift by aligning the second-order statistics of source and target distribu-
tions, without requiring any target labels. In contrast to subspace manifold meth-
ods, it aligns the original feature distributions of the source and target domains,
rather than the bases of lower-dimensional subspaces. It is also much simpler
than other distribution matching methods. CORAL performs remarkably well in
extensive evaluations on standard benchmark datasets. We ﬁrst describe a solu-
tion that applies a linear transformation to source features to align them with tar-
get features before classiﬁer training. For linear classiﬁers, we propose to equiva-
lently apply CORAL to the classiﬁer weights, leading to added efﬁciency when the
number of classiﬁers is small but the number and dimensionality of target exam-
ples are very high. The resulting CORAL Linear Discriminant Analysis (CORAL-
LDA) outperforms LDA by a large margin on standard domain adaptation bench-
marks. Finally, we extend CORAL to learn a nonlinear transformation that aligns
correlations of layer activations in deep neural networks (DNNs). The resulting
Deep CORAL approach works seamlessly with DNNs and achieves state-of-the-
art performance on standard benchmark datasets. Our code is available at: https:
//github.com/VisionLearningGroup/CORAL

Baochen Sun
Microsoft, e-mail: baochens@gmail.com

Jiashi Feng
National University of Singapore, e-mail: elefjia@nus.edu.sg

Kate Saenko
Boston University, e-mail: saenko@bu.edu

1

2

1 Introduction

Baochen Sun, Jiashi Feng, and Kate Saenko

Machine learning is very different from human learning. Humans are able to learn
from very few labeled examples and apply the learned knowledge to new examples
in novel conditions. In contrast, traditional machine learning algorithms assume that
the training and test data are independent and identically distributed (i.i.d.) and su-
pervised machine learning methods only perform well when the given extensive
labeled data are from the same distribution as the test distribution. However, this
assumption rarely holds in practice, as the data are likely to change over time and
space. To compensate for the degradation in performance due to domain shift, do-
main adaptation [23, 8, 11, 20, 28, 25, 30, 2] tries to transfer knowledge from source
(training) domain to target (test) domain. Our approach is in line with most existing
unsupervised domain adaptation approaches in that we ﬁrst transform the source
data to be as close to the target data as possible. Then a classiﬁer trained on the
transformed source domain is applied to the target data.

In this chapter, we mainly focus on the unsupervised scenario as we believe that
leveraging the unlabeled target data is key to the success of domain adaptation. In
real world applications, unlabeled target data are often much more abundant and
easier to obtain. On the other side, labeled examples are very limited and require
human annotation. So the question of how to utilize the unlabeled target data is
more important for practical visual domain adaptation. For example, it would be
more convenient and applicable to have a pedestrian detector automatically adapt to
the changing visual appearance of pedestrians rather than having a human annotator
label every frame that has a different visual appearance.

Our goal is to propose a simple yet effective approach for domain adaptation that
researchers with little knowledge of domain adaptation or machine learning could
easily integrate into their application. In this chapter, we describe a “frustratingly
easy” (to borrow a phrase from [2]) unsupervised domain adaptation method called
CORrelation ALignment or CORAL [25] in short. CORAL aligns the input feature
distributions of the source and target domains by minimizing the difference between
their second-order statistics. The intuition is that we want to capture the structure
of the domain using feature correlations. As an example, imagine a target domain
of Western movies where most people wear hats; then the “head” feature may be
positively correlated with the “hat” feature. Our goal is to transfer such correlations
to the source domain by transforming the source features.

In Section 2, we describe a linear solution to CORAL, where the distributions
are aligned by re-coloring whitened source features with the covariance of the tar-
get data distribution. This solution is simple yet efﬁcient, as the only computations
it needs are (1) computing covariance statistics in each domain and (2) applying
the whitening and re-coloring linear transformation to the source features. Then, su-
pervised learning proceeds as usual–training a classiﬁer on the transformed source
features.

For linear classiﬁers, we can equivalently apply the CORAL transformation to
the classiﬁer weights, leading to better efﬁciency when the number of classiﬁers is
small but the number and dimensionality of the target examples are very high. We

Correlation Alignment for Unsupervised Domain Adaptation

3

present the resulting CORAL–Linear Discriminant Analysis (CORAL-LDA) [27]
in Section 3, and show that it outperforms standard Linear Discriminant Analysis
(LDA) by a large margin on cross domain applications. We also extend CORAL to
work seamlessly with deep neural networks by designing a layer that consists of a
differentiable CORAL loss [29], detailed in Section 4. On the contrary to the linear
CORAL, Deep CORAL learns a nonlinear transformation and also provides end-
to-end adaptation. Section 5 describes extensive quantitative experiments on several
benchmarks, and Section 6 concludes the chapter.

2 Linear Correlation Alignment

In this section, we present CORrelation ALignment (CORAL) for unsupervised do-
main adaptation and derive a linear solution. We ﬁrst describe the formulation and
derivation, followed by the main linear CORAL algorithm and its relationship to ex-
isting approaches. In this section and Section 3, we constrain the transformation to
be linear. Section 4 extends CORAL to learn a nonlinear transformation that aligns
correlations of layer activations in deep neural networks (Deep CORAL).

2.1 Formulation and Derivation

We describe our method by taking a multi-class classiﬁcation problem as the run-
ning example. Suppose we are given source-domain training examples DS = {xi},
x ∈ Rd with labels LS = {yi}, y ∈ {1, ..., L}, and target data DT = {ui}, u ∈ Rd. Here
both x and u are the d-dimensional feature representations φ (I) of input I. Suppose
µs, µt and CS,CT are the feature vector means and covariance matrices. Assuming
that all features are normalized to have zero mean and unit variance, µt = µs = 0
after the normalization step, while CS (cid:54)= CT .

To minimize the distance between the second-order statistics (covariance) of the
source and target features, we apply a linear transformation A to the original source
features and use the Frobenius norm as the matrix distance metric:

(cid:107)C ˆS −CT (cid:107)2

F

min
A

= min
A

2
(cid:107)A(cid:62)CSA −CT (cid:107)
F

(1)

where C ˆS is covariance of the transformed source features DsA and (cid:107) · (cid:107)2
the squared matrix Frobenius norm.

F denotes

If rank(CS) ≥ rank(CT ), then an analytical solution can be obtained by choosing
A such that C ˆS = CT . However, the data typically lie on a lower dimensional man-
ifold [13, 11, 8], and so the covariance matrices are likely to be low rank [14]. We
derive a solution for this general case, using the following lemma.

4

Baochen Sun, Jiashi Feng, and Kate Saenko

Lemma 1. Let Y be a real matrix of rank rY and X a real matrix of rank at most
r, where r (cid:54) rY ; let Y = UY ΣYVY be the SVD of Y , and ΣY [1:r], UY [1:r], VY [1:r] be the
largest r singular values and the corresponding left and right singular vectors of Y
(cid:62) is the optimal solution to the problem
respectively. Then, X ∗ = UY [1:r]ΣY [1:r]VY [1:r]
of min
X

(cid:107)X −Y (cid:107)2

F . [1]

Theorem 1. Let Σ + be the Moore-Penrose pseudoinverse of Σ , rCS and rCT denote
the rank of CS and CT respectively. Then, A∗ = USΣ +
(cid:62) is
S
the optimal solution to the problem in Equation (12) with r = min(rCS , rCT ).

(cid:62)UT [1:r]ΣT [1:r]

1
2 UT [1:r]

1
2 US

Proof. Since A is a linear transformation, A(cid:62)CSA does not increase the rank of CS.
(cid:54) rCS . Since CS and CT are symmetric matrices, conducting SVD on CS
Thus, rC ˆS
and CT gives CS = USΣSUS
T respectively. We ﬁrst ﬁnd the op-
timal value of C ˆS through considering the following two cases:
Case 1. rCS > rCT . The optimal solution is C ˆS = CT . Thus, C ˆS = UT ΣTUT
UT [1:r]ΣT [1:r]UT [1:r]

(cid:62) is the optimal solution to Equation (12) where r = rCT .

(cid:62) and CT = UT ΣTU (cid:62)

(cid:62) =

Case 2. rCS
optimal solution to Equation (12) where r = rCS .

(cid:54) rCT . Then, according to Lemma 1, C ˆS = UT [1:r]ΣT [1:r]UT [1:r]

(cid:62) is the

Combining the results in the above two cases yields that C ˆS = UT [1:r]ΣT [1:r]UT [1:r]
is the optimal solution to Equation (12) with r = min(rCS , rCT ). We then proceed to
solve for A based on the above result. Letting C ˆS = A(cid:62)CSA, we can write

(cid:62)

A(cid:62)CSA = UT [1:r]ΣT [1:r]UT [1:r]

(cid:62).

Since CS = USΣSUS

(cid:62), we have

A(cid:62)USΣSUS

(cid:62)A = UT [1:r]ΣT [1:r]UT [1:r]

(cid:62).

This gives:

(cid:62)
(cid:62)A)

(US

ΣS(US

(cid:62)A) = UT [1:r]ΣT [1:r]UT [1:r]

(cid:62).

Let E = Σ +
(cid:62)UT [1:r]ΣT [1:r]
S
tion can be re-written as E(cid:62)ΣSE. This gives

1
2 UT [1:r]

1
2 US

(cid:62), then the right hand side of the above equa-

(cid:62)
(cid:62)A)

(US

ΣS(US

(cid:62)A) = E(cid:62)ΣSE

By setting US

(cid:62)A to E, we obtain the optimal solution of A as

A∗ = USE

= (USΣ +
S

1
2 US

(cid:62))(UT [1:r]ΣT [1:r]

1
2 UT [1:r]

(cid:62)).

(2)

Correlation Alignment for Unsupervised Domain Adaptation

5

Fig. 1 (a-c) Illustration of CORrelation ALignment (CORAL) for Domain Adaptation: (a) The
original source and target domains have different distribution covariances, despite the features be-
ing normalized to zero mean and unit standard deviation. This presents a problem for transferring
classiﬁers trained on source to target. (b) The same two domains after source decorrelation, i.e.
removing the feature correlations of the source domain. (c) Target re-correlation, adding the corre-
lation of the target domain to the source features. After this step, the source and target distributions
are well aligned and the classiﬁer trained on the adjusted source domain is expected to work well
in the target domain. (d) One might instead attempt to align the distributions by whitening both
source and target. However, this will fail since the source and target data are likely to lie on different
subspaces due to domain shift. (Best viewed in color)

2.2 Algorithm

Figures 1(a-c) illustrate the linear CORAL approach. Figure 1(a) shows example
original source and target data distributions. We can think of the transformation A
1
intuitively as follows: the ﬁrst part USΣ +
(cid:62) whitens the source data, while the
2 US
S
(cid:62) re-colors it with the target covariance. This is
second part UT [1:r]ΣT [1:r]
illustrated in Figure 1(b) and Figure 1(c) respectively.

1
2 UT [1:r]

In practice, for the sake of efﬁciency and stability, we can avoid the expensive
SVD steps and perform traditional data whitening and coloring. Traditional whiten-
ing adds a small regularization parameter λ to the diagonal elements of the covari-
ance matrix to explicitly make it full rank and then multiplies the original features
by its inverse square root (or square root for coloring.) This is advantageous be-
cause: (1) it is faster1 and more stable, as SVD on the original covariance matrices

1 the entire CORAL transformation takes less than one minute on a regular laptop for dimensions
as large as DS ∈ R795×4096 and DT ∈ R2817×4096

6

Baochen Sun, Jiashi Feng, and Kate Saenko

might not be stable and might be slow to converge; (2) as illustrated in Figure 2,
the performance is similar to the analytical solution in Equation (2) and very stable
with respect to λ . In the experiments provided at the end of this chapter we set λ to
1. The ﬁnal algorithm can be written in four lines of MATLAB code as illustrated
in Algorithm 1.

Fig. 2 Sensitivity of CORAL to the covariance regularization parameter λ with λ ∈ {0, 0.001,
0.01, 0.1, 1}. The plots show classiﬁcation accuracy on target data for two domain shifts (blue and
red). When λ = 0, there is no regularization and we use the analytical solution in Equation (2).
Please refer to Section 5.1 for details of the experiment.

Algorithm 1 CORAL for Unsupervised Domain Adaptation

Input: Source Data DS, Target Data DT
Output: Adjusted Source Data D∗
s
CS = cov(DS) + eye(size(DS, 2))
CT = cov(DT ) + eye(size(DT , 2))

−1
2

DS = DS ∗C
S
1
D∗
2
S = DS ∗C
T

% whitening source

% re-coloring with target covariance

One might instead attempt to align the distributions by whitening both source
and target. As shown in Figure 1(d), this will fail as the source and target data are
likely to lie on different subspaces due to domain shift. An alternative approach
would be whitening the target and then re-coloring it with the source covariance.
However, as demonstrated in [13, 8] and our experiments, transforming data from
source to target space gives better performance. This might be due to the fact that by
transforming the source to target space the classiﬁer is trained using both the label
information from the source and the unlabelled structure from the target.

After CORAL transforms the source features to the target space, a classiﬁer fw
parametrized by w can be trained on the adjusted source features and directly ap-
plied to target features. For a linear classiﬁer fw(I) = wT φ (I), we can apply an
equivalent transformation to the parameter vector w (e.g., fw(I) = (wT A)φ (I)) in-
stead of the features (e.g., fw(I) = wT (Aφ (I))). This results in added efﬁciency
when the number of classiﬁers is small but the number and dimensionality of target

Correlation Alignment for Unsupervised Domain Adaptation

7

examples is very high. For linear SVM, this extension is straightforward. In Sec-
tion 3, we apply the idea of CORAL to another commonly used linear classiﬁer–
Linear Discriminant Analysis (LDA). LDA is special in the sense that its weights
also use the covariance of the data. It is also extremely efﬁcient for training a large
number of classiﬁers [14].

2.3 Relationship to Existing Methods

It has long been known that input feature normalization improves many machine
learning methods, e.g., [15]. However, CORAL does not simply perform feature nor-
malization, but rather aligns two different distributions. Batch Normalization [15]
tries to compensate for internal covariate shift by normalizing each mini-batch to be
zero-mean and unit-variance. However, as illustrated in Figure 1(a), such normal-
ization might not be enough. Even if used with full whitening, Batch Normalization
may not compensate for external covariate shift: the layer activations will be decor-
related for a source point but not for a target point. What’s more, as mentioned in
Section 2.2, whitening both domains is not a successful strategy.

Recent state-of-the-art unsupervised approaches project the source and target dis-
tributions into a lower-dimensional manifold and ﬁnd a transformation that brings
the subspaces closer together [12, 11, 8, 13]. CORAL avoids subspace projection,
which can be costly and requires selecting the hyper-parameter that controls the di-
mensionality of the subspace, k. We note that subspace-mapping approaches [13, 8]
only align the top k eigenvectors of the source and target covariance matrices. On the
contrary, CORAL aligns the covariance matrices, which can only be re-constructed
using all eigenvectors and eigenvalues. Even though the eigenvectors can be aligned
well, the distributions can still differ a lot due to the difference of eigenvalues be-
tween the corresponding eigenvectors of the source and target data. CORAL is a
more general and much simpler method than the above two as it takes into ac-
count both eigenvectors and eigenvalues of the covariance matrix without the burden
of subspace dimensionality selection.

Maximum Mean Discrepancy (MMD) based methods (e.g., TCA [22], DAN [19])
for domain adaptation can be interpreted as “moment matching” and can ex-
press arbitrary statistics of the data. Minimizing MMD with a polynomial kernel
(k(x, y) = (1 + x(cid:48)y)q with q = 2) is similar to the CORAL objective, however, no
previous work has used this kernel for domain adaptation nor proposed a closed
form solution to the best of our knowledge. The other difference is that MMD based
approaches usually apply the same transformation to both the source and target do-
main. As demonstrated in [18, 13, 8], asymmetric transformations are more ﬂexible
and often yield better performance for domain adaptation tasks. Intuitively, symmet-
ric transformations ﬁnd a space that “ignores” the differences between the source
and target domain while asymmetric transformations try to “bridge” the two do-
mains.

8

Baochen Sun, Jiashi Feng, and Kate Saenko

3 CORAL Linear Discriminant Analysis

In this section, we introduce how CORAL can be applied for aligning multiple linear
classiﬁers. In particular, we take LDA as the example for illustration, considering
LDA is a commonly used and effective linear classiﬁer. Combining CORAL and
LDA gives a new efﬁcient adpative learning approach CORAL-LDA. We use the
task of object detection as a running example to explain CORAL-LDA.

We begin by describing the decorrelation-based approach to detection proposed
in [14]. Given an image I, it follows the sliding-window paradigm, extracting a
d-dimensional feature vector φ (I, b) at each window b across all locations and at
multiple scales. It then scores the windows using a scoring function

fw(I, b) = w(cid:62)φ (I, b).

(3)

In practice, all windows with values of fw above a predetermined threshold are
considered positive detections.

In recent years, use of the linear SVM as the scoring function fw, usually with
Histogram of Gradients (HOG) as the features φ , has emerged as the predominant
object detection paradigm. Yet, as observed by Hariharan et al. [14], training SVMs
can be expensive, especially because it usually involves costly rounds of hard neg-
ative mining. Furthermore, the training must be repeated for each object category,
which makes it scale poorly with the number of categories.

Hariharan et al. [14] proposed a much more efﬁcient alternative, learning fw
with Linear Discriminant Analysis (LDA). LDA is a well-known linear classiﬁer
that models the training set of examples x with labels y ∈ {0, 1} as being generated
by p(x, y) = p(x|y)p(y). p(y) is the prior on class labels and the class-conditional
densities are normal distributions

p(x|y) = N (x; µ y, CS),

where the feature vector covariance CS is assumed to be the same for both positive
and negative (background) classes. In our case, the feature is represented by x =
φ (I, b). The resulting classiﬁer is given by

w = CS

−1(µ1 − µ0)

The innovation in [14] was to re-use CS and µ0, the background mean, for all cate-
gories, reducing the task of learning a new category model to computing the average
positive feature, µ1. This was accomplished by calculating CS and µ0 for the largest
possible window and subsampling to estimate all other smaller window sizes. Also,
CS was shown to have a sparse local structure, with correlation falling off sharply
beyond a few nearby image locations.

Like other classiﬁers, LDA learns to suppress non-discriminative structures and
enhance the contours of the object. However it does so by learning the global co-
variance statistics once for all natural images, and then using the inverse covariance
matrix to remove the non-discriminative correlations, and the negative mean to re-

(4)

(5)

Correlation Alignment for Unsupervised Domain Adaptation

9

Fig. 3 (a) Applying a linear classiﬁer w learned by LDA to source data x is equivalent to (b)
−1/2x. (c) However, target points u
applying classiﬁer ˆw = CS
−1/2u, hurting performance. (d) Our method uses target-speciﬁc
may still be correlated after CS
covariance to obtain properly decorrelated ˆu.

−1/2w to decorrelated points CS

move the average feature. LDA was shown in [14] to have competitive performance
to SVM, and can be implemented both as an exemplar-based [21] or as deformable
parts model (DPM) [7].

We observe that estimating global statistics CS and µ0 once and re-using them for
all tasks may work when training and testing in the same domain, but in our case,
the source training data is likely to have different statistics from the target data.
Figure 4 illustrates the effect of centering and decorrelating a positive mean using
global statistics from the wrong domain. The effect is clear: important discriminative
information is removed while irrelevant structures are not.

Based on this observation, we propose an adaptive decorrelation approach to
detection. Assume that we are given labeled training data {x, y} in the source domain
(e.g., virtual images rendered from 3D models), and unlabeled examples u in the
target domain (e.g., real images collected in an ofﬁce environment). Evaluating the
scoring function fw(x) in the source domain is equivalent to ﬁrst de-correlating the
−1/2x, computing their positive and negative class means
training features ˆx = CS
−1/2µ0 and then projecting the decorrelated feature onto
ˆµ1 = CS
the decorrelated difference between means, fw(x) = ˆw(cid:62) ˆx, where ˆw = ( ˆµ1 − ˆµ0).
This is illustrated in Figure 3(a-b).

−1/2µ1 and ˆµ0 = CS

However, as we saw in Figure 4, the assumption that the input is properly decor-
related does not hold if the input comes from a target domain with a different co-
−1/2u does not
variance structure. Figure 3(c) illustrates this case, showing that CS
have isotropic covariance. Therefore, w cannot be used directly.

Fig. 4 Visualization of classiﬁer weights of bicycle (decorrelated with mismatched-domain co-
variance (left) v.s. with same-domain covariance (right)).

10

Baochen Sun, Jiashi Feng, and Kate Saenko

We may be able to compute the covariance of the target domain on the unlabeled
target points u, but not the positive class mean. Therefore, we would like to re-use
the decorrelated mean difference ˆw, but adapt to the covariance of the target do-
main. In the rest of the chapter, we make the assumption that the difference between
positive and negative means is the same in the source and target. This may or may
not hold in practice, and we discuss this further in Section 5.

Let the estimated target covariance be CT. We ﬁrst decorrelate the target input
feature with its inverse square root, and then apply ˆw directly, as shown in Fig-
ure 3(d). The resulting scoring function is:

f ˆw(u) = ˆw(cid:62) ˆu

= (CS

= ((CT

(cid:62)
−1/2(µ1 − µ0))
−1/2)(cid:62)CS

−1/2u)
(CT
(cid:62)
−1/2(µ1 − µ0))

u

(6)

This corresponds to a transformation (CT

−1/2) instead of the original
−1/2)(cid:62)(CS
−1 being applied to the difference between means to compute w. Note
−1/2) equals to
−1 since both CT and CS are symmetric. In this case, Equation 6 ends up the

whitening CS
that if source and target domains are the same, then (CT
CS
same as Equation 5.

−1/2)(cid:62)(CS

In practice, either the source or the target component of the above transformation
may also work, or even statistics from similar domains. However, as we will see in
Section 5.2, dissimilar domain statistics can signiﬁcantly hurt performance. Further-
more, if either source or target has only images of the positive category available,
and cannot be used to properly compute background statistics, the other domain can
still be used.

CORAL-LDA works in a purely unsupervised way. Here, we extend it to semi-
supervised adaptation when a few labeled examples are available in the target do-
main. Following [10], a simple adaptation method is used whereby the template
learned on source positives is combined with a template learned on target positives,
using a weighted combination. The key difference with our approach is that the
target template uses target-speciﬁc statistics.

In [10], the author uses the same background statistics as [14] which were es-
timated on 10,000 natural images from the PASCAL VOC 2010 dataset. Based on
our analysis above, even though these background statistics were estimated from a
very large amount of real image data, it will not work for all domains. In section 5.2,
our results conﬁrm this claim.

4 Deep CORAL

In this section, we extend CORAL to work seamlessly with deep neural networks
by designing a differentiable CORAL loss. Deep CORAL enables end-to-end adap-
tation and also learns more a powerful nonlinear transformation. It can be easily

Correlation Alignment for Unsupervised Domain Adaptation

11

Fig. 5 Sample Deep CORAL architecture based on a CNN with a classiﬁer layer. For generaliza-
tion and simplicity, here we apply the CORAL loss to the f c8 layer of AlexNet [17]. Integrating it
into other layers or network architectures is also possible.

integrated into different layers or network architectures. Figure 5 shows a sample
Deep CORAL architecture using our proposed correlation alignment layer for deep
domain adaptation. We refer to Deep CORAL as any deep network incorporating
the CORAL loss for domain adaptation.

We ﬁrst describe the CORAL loss between two domains for a single feature layer.
Suppose the numbers of source and target data are nS and nT respectively. Here both
x and u are the d-dimensional deep layer activations φ (I) of input I that we are
trying to learn. Suppose Di j
T ) indicates the j-th dimension of the i-th source
(target) data example and CS (CT ) denote the feature covariance matrices.

S (Di j

We deﬁne the CORAL loss as the distance between the second-order statistics

(covariances) of the source and target features:

LCORAL =

1
4d2 (cid:107)CS −CT (cid:107)2

F

(7)

(8)

(9)

where (cid:107) · (cid:107)2
of the source and target data are given by:

F denotes the squared matrix Frobenius norm. The covariance matrices

CS =

1
nS − 1

(D(cid:62)

S DS −

(1(cid:62)DS)(cid:62)(1(cid:62)DS))

1
nT − 1
where 1 is a column vector with all elements equal to 1.

T DT −

CT =

1
nT

(D(cid:62)

(1(cid:62)DT )(cid:62)(1(cid:62)DT ))

The gradient with respect to the input features can be calculated using the chain

rule:

∂ LCORAL
∂ Di j
S

=

1
d2(nS − 1)

((D(cid:62)

S −

(1(cid:62)DS)(cid:62)1(cid:62))(cid:62)(CS −CT ))i j

(10)

1
nS

1
nS

12

Baochen Sun, Jiashi Feng, and Kate Saenko

∂ LCORAL
∂ Di j
T

= −

1
d2(nT − 1)

1
nT

((D(cid:62)

T −

(1(cid:62)DT )(cid:62)1(cid:62))(cid:62)(CS −CT ))i j

(11)

In our experiments, we use batch covariances and the network parameters are shared
between the two networks, but other settings are also possible.

To see how this loss can be used to adapt an existing neural network, let us
return to the multi-class classiﬁcation problem. Suppose we start with a network
with a ﬁnal classiﬁcation layer, such as the ConvNet shown in Figure 5. As men-
tioned before, the ﬁnal deep features need to be both discriminative enough to train
a strong classiﬁer and invariant to the difference between source and target domains.
Minimizing the classiﬁcation loss itself is likely to lead to overﬁtting to the source
domain, causing reduced performance on the target domain. On the other hand, min-
imizing the CORAL loss alone might lead to degenerated features. For example, the
network could project all of the source and target data to a single point, making
the CORAL loss trivially zero. However, no strong classiﬁer can be constructed on
these features. Joint training with both the classiﬁcation loss and CORAL loss is
likely to learn features that work well on the target domain:

L = LCLASS +

λiLCORALi

(12)

t
∑
i=1

where t denotes the number of CORAL loss layers in a deep network and λi is a
weight that trades off the adaptation with classiﬁcation accuracy on the source do-
main. As we show below, these two losses play counterparts and reach an equilib-
rium at the end of training, where the ﬁnal features are discriminative and generalize
well to the target domain.

5 Experiments

We evaluate CORAL and Deep CORAL on object recognition [23] using standard
benchmarks and protocols. In all experiments we assume the target domain is unla-
beled. For CORAL, we follow the standard procedure [8, 4] and use a linear SVM as
the base classiﬁer. The model selection approach of [8] is used to set the C parame-
ter for the SVM by doing cross-validation on the source domain. For CORAL-LDA,
as efﬁciency is the main concern, we evaluate it on the more time constrained task–
object detection. We follow the protocol of [10] and use HOG features. To have a
fair comparison, we use accuracies reported by other authors with exactly the same
setting or conduct experiments using the source code provided by the authors.

Correlation Alignment for Unsupervised Domain Adaptation

13

5.1 Object Recognition

In this set of experiments, domain adaptation is used to improve the accuracy of
an object classiﬁer on novel image domains. Both the standard Ofﬁce [23] and
extended Ofﬁce-Caltech10 [11] datasets are used as benchmarks in this chap-
ter. Ofﬁce-Caltech10 contains 10 object categories from an ofﬁce environment
(e.g., keyboard, laptop, etc.) in 4 image domains: Webcam, DSLR, Amazon, and
Caltech256. The standard Ofﬁce dataset contains 31 (the same 10 categories from
Ofﬁce-Caltech10 plus 21 additional ones) object categories in 3 domains: Webcam,
DSLR, and Amazon.

A→C A→D A→W C→A C→D C→W D→A D→C D→W W→A W→C W→D AVG
37.8
26.4
35.8
NA
41.0
33.4
34.8
SVMA
40.2
33.5
34.9
DAM
43.4
37.9
38.3
GFK
45.7
39.6
40.0
TCA
42.0
45.9
SA
39.9
46.7
CORAL 40.3
38.1

25.7
33.5
31.2
29.1
33.7
31.8
34.6

32.3
36.6
34.7
37.1
40.2
39.3
37.8

39.4
34.5
34.7
36.1
41.4
39.4
40.7

27.1
31.4
31.5
31.4
34.0
35.0
34.2

24.9
32.5
32.5
39.8
40.1
39.6
38.7

33.1
34.1
34.3
37.9
39.1
38.8
38.3

30.0
32.9
33.1
34.9
36.2
38.9
39.2

56.4
74.4
74.7
79.1
80.4
82.3
85.9

78.9
75.0
68.3
74.6
77.5
77.9
84.9

43.7
39.1
39.2
44.8
46.7
46.1
47.2

Table 1 Object recognition accuracies of all 12 domain shifts on the Ofﬁce-Caltech10 dataset [11]
with SURF features, following the protocol of [11, 8, 12, 18, 23].

Object Recognition with Shallow Features

We follow the standard protocol of [11, 8, 12, 18, 23] and conduct experiments
on the Ofﬁce-Caltech10 dataset with shallow features (SURF). The SURF features
were encoded with 800-bin bag-of-words histograms and normalized to have zero
mean and unit standard deviation in each dimension. Since there are four domains,
there are 12 experiment settings, namely, A→C (train classiﬁer on (A)mazon, test
on (C)altech), A→D (train on (A)mazon, test on (D)SLR), A→W, and so on. We
follow the standard protocol and conduct experiments in 20 randomized trials for
each domain shift and average the accuracy over the trials. In each trial, we use the
standard setting [11, 8, 12, 18, 23] and randomly sample the same number (20 for
Amazon, Caltech, and Webcam; 8 for DSLR as there are only 8 images per category
in the DSLR domain) of labelled images in the source domain as training set, and
use all the unlabelled data in the target domain as the test set.

In Table 1, we compare our method to ﬁve recent published methods: SVMA [6],
DAM [5], GFK [11], SA [8], and TCA [22] as well as the no adaptation base-
line (NA). GFK, SA, and TCA are manifold based methods that project the source
and target distributions into a lower-dimensional manifold. GFK integrates over an
inﬁnite number of subspaces along the subspace manifold using the kernel trick.
SA aligns the source and target subspaces by computing a linear map that mini-
mizes the Frobenius norm of their difference. TCA performs domain adaptation via
a new parametric kernel using feature extraction methods by projecting data onto

14

Baochen Sun, Jiashi Feng, and Kate Saenko

the learned transfer components. DAM introduces smoothness assumption to en-
force the target classiﬁer share similar decision values with the source classiﬁers.
Even though these methods are far more complicated than ours and require tuning
of hyperparameters (e.g., subspace dimensionality), our method achieves the best
average performance across all the 12 domain shifts. Our method also improves on
the no adaptation baseline (NA), in some cases increasing accuracy signiﬁcantly
(from 56% to 86% for D→W).

Object Recognition with Deep Features

For visual domain adaptation with deep features, we follow the standard protocol
of [11, 19, 4, 30, 9] and use all the labeled source data and all the target data without
labels on the standard Ofﬁce dataset [23]. Since there are 3 domains, we conduct
experiments on all 6 shifts (5 runs per shift), taking one domain as the source and
another as the target.

In this experiment, we apply the CORAL loss to the last classiﬁcation layer as it
is the most general case–most deep classiﬁer architectures (e.g., convolutional neu-
ral networks, recurrent neural networks) contain a fully connected layer for classi-
ﬁcation. Applying the CORAL loss to other layers or other network architectures is
also possible. The dimension of the last fully connected layer ( f c8) was set to the
number of categories (31) and initialized with N (0, 0.005). The learning rate of
f c8 was set to 10 times the other layers as it was training from scratch. We initial-
ized the other layers with the parameters pre-trained on ImageNet [3] and kept the
original layer-wise parameter settings. In the training phase, we set the batch size to
128, base learning rate to 10−3, weight decay to 5 × 10−4, and momentum to 0.9.
The weight of the CORAL loss (λ ) is set in such way that at the end of training
the classiﬁcation loss and CORAL loss are roughly the same. It seems be a reason-
able choice as we want to have a feature representation that is both discriminative
and also minimizes the distance between the source and target domains. We used
Caffe [16] and BVLC Reference CaffeNet for all of our experiments.

We compare to 7 recently published methods: CNN [17] (no adaptation), GFK [11],

SA [8], TCA [22], CORAL [25], DDC [30], DAN [19]. GFK, SA, and TCA are
manifold based methods that project the source and target distributions into a lower-
dimensional manifold and are not end-to-end deep methods. DDC adds a domain
confusion loss to AlexNet [17] and ﬁne-tunes it on both the source and target do-
main. DAN is similar to DDC but utilizes a multi-kernel selection method for better
mean embedding matching and adapts in multiple layers. For direct comparison,
DAN in this paper uses the hidden layer f c8. For GFK, SA, TCA, and CORAL, we
use the f c7 feature ﬁne-tuned on the source domain (FT 7 in [25]) as it achieves bet-
ter performance than generic pre-trained features, and train a linear SVM [8, 25]. To
have a fair comparison, we use accuracies reported by other authors with exactly the
same setting or conduct experiments using the source code provided by the authors.
From Table 2 we can see that Deep CORAL (D-CORAL) achieves better average
performance than CORAL and the other 6 baseline methods. In 3 out of 6 shifts, it

Correlation Alignment for Unsupervised Domain Adaptation

15

A→D A→W D→A D→W W→A W→D AVG
52.4±0.0 54.7±0.0 43.2±0.0 92.1±0.0 41.8±0.0 96.2±0.0 63.4
GFK
50.6±0.0 47.4±0.0 39.5±0.0 89.1±0.0 37.6±0.0 93.8±0.0 59.7
SA
46.8±0.0 45.5±0.0 36.4±0.0 81.1±0.0 39.5±0.0 92.2±0.0 56.9
TCA
65.7±0.0 64.3±0.0 48.5±0.0 96.1±0.0 48.2±0.0 99.8±0.0 70.4
CORAL
63.8±0.5 61.6±0.5 51.1±0.6 95.4±0.3 49.8±0.4 99.0±0.2 70.1
CNN
64.4±0.3 61.8±0.4 52.1±0.8 95.0±0.5 52.2±0.4 98.5±0.4 70.6
DDC
65.8±0.4 63.8±0.4 52.8±0.4 94.6±0.5 51.9±0.5 98.8±0.6 71.3
DAN
D-CORAL 66.8±0.6 66.4±0.4 52.8±0.2 95.7±0.3 51.5±0.3 99.2±0.1 72.1

Table 2 Object recognition accuracies for all 6 domain shifts on the standard Ofﬁce dataset with
deep features, following the standard unsupervised adaptation protocol.

Fig. 6 Detailed analysis of shift A→W for training w/ v.s. w/o CORAL loss. (a): training and test
accuracies for training w/ v.s. w/o CORAL loss. We can see that adding CORAL loss helps achieve
much better performance on the target domain while maintaining strong classiﬁcation accuracy on
the source domain. (b): classiﬁcation loss and CORAL loss for training w/ CORAL loss. As the
last fully connected layer is randomly initialized with N (0, 0.005), CORAL loss is very small
while classiﬁcation loss is very large at the beginning. After training for a few hundred iterations,
these two losses are about the same. (c): CORAL distance for training w/o CORAL loss (setting
the weight to 0). The distance is getting much larger ((cid:62) 100 times larger compared to training w/
CORAL loss).

achieves the highest accuracy. For the other 3 shifts, the margin between D-CORAL
and the best baseline method is very small ((cid:54) 0.7).

Domain Adaptation Equilibrium

To get a better understanding of Deep CORAL, we generate three plots for domain
shift A→W. In Figure 6(a) we show the training (source) and testing (target) accu-
racies for training with v.s. without CORAL loss. We can clearly see that adding
the CORAL loss helps achieve much better performance on the target domain while
maintaining strong classiﬁcation accuracy on the source domain.

In Figure 6(b) we visualize both the classiﬁcation loss and the CORAL loss for
training w/ CORAL loss. As the last fully connected layer is randomly initialized
with N (0, 0.005), in the beginning the CORAL loss is very small while the clas-
siﬁcation loss is very large. After training for a few hundred iterations, these two

16

Baochen Sun, Jiashi Feng, and Kate Saenko

losses are about the same and reach an equilibrium. In Figure 6(c) we show the
CORAL distance between the domains for training w/o CORAL loss (setting the
weight to 0). We can see that the distance is getting much larger ((cid:62) 100 times larger
compared to training w/ CORAL loss). Comparing Figure 6(b) and Figure 6(c), we
can see that even though the CORAL loss is not always decreasing during training,
if we set its weight to 0, the distance between source and target domains becomes
much larger. This is reasonable as ﬁne-tuning without domain adaptation is likely
to overﬁt the features to the source domain. Our CORAL loss constrains the dis-
tance between source and target domain during the ﬁne-tuning process and helps to
maintain an equilibrium where the ﬁnal features work well on the target domain.

5.2 Object Detection

Following protocol of [10], we conduct object detection experiment on the Ofﬁce
dataset [23] with HOG features. We use the same setting as [10], performing detec-
tion on the Webcam domain as the target (test) domain, and evaluating on the same
783 image test set of 20 categories (out of 31). As source (training) domains, we
use: the two remaining real-image domains in Ofﬁce, Amazon and DSLR, and two
domains that contain virtual images only, Virtual and Virtual-Gray, generated from
3d CAD models. The inclusion of the two virtual domains is to reduce human effort
in annotation and facilitate future research [24]. Examples of Virtual and Virtual-
Gray are shown in Figure 8. Please refer to [26] for detailed explanation of the data
generation process. We also compare to [10] who use corresponding ImageNet [3]
synsets as the source. Thus, there are four potential source domains (two synthetic
and three real) and one (real) target domain. The number of positive training images
per category in each domain is shown in Table 3. Figure 7 shows an overview of our
evaluation.

Fig. 7 Overview of our evaluation on object detection. With CORAL-Linear Discriminant Analy-
sis (LDA), we show that a strong object detector can be trained from virtual data only.

Correlation Alignment for Unsupervised Domain Adaptation

17

(a)

(b)

Fig. 8 Two sets of virtual images used in this section: (a) Virtual: background and texturemap from
a random real ImageNet image; (b) Virtual-Gray: uniform gray texturemap and white background.
Please refer to [26] for detailed explanation of the data generation process.

Domain

# Training sample

Amazon
DSLR
Virtual(-Gray)
ImageNet

20
8
30
150-2000

Table 3 # training examples for each source domain.

Effect of Mismatched Image Statistics

First, we explore the effect of mismatched precomputed image statistics on detection
performance. For each source domain, we train CORAL-LDA detectors using the
positive mean from the source, and pair it with the covariance and negative mean
of other domains. The virtual and the Ofﬁce domains are used as sources, and the
test domain is always Webcam. The statistics for each of the four domains were
calculated using all of the training data, following the same approach as [14]. The
pre-computed statistics of 10,000 real images from PASCAL, as proposed in [14,
10], are also evaluated.

Detection performance, measured in Mean Average Precision (MAP), is shown
in Table 4. We also calculate the normalized Euclidean distance between pairs of
domains as ((cid:107)C1 − C2(cid:107))/((cid:107)C1(cid:107) + (cid:107)C2(cid:107)) + ((cid:107)µ 1
0 (cid:107)), and show
the average distance between source and target in parentheses in Table 4.

0 (cid:107))/((cid:107)µ 1

0 (cid:107) + (cid:107)µ 2

0 − µ 2

From these results we can see a trend that larger domain difference leads to
poorer performance. Note that larger difference to the target domain also leads to
lower performance, conﬁrming our hypothesis that both source and target statistics
matter. Some of the variation could also stem from our assumption about the differ-
ence of means being the same not quite holding true. Finally, the PASCAL statistics
from [14] perform the worst. Thus, in practice, statistics from either source domain
or target domain or domains close to them could be used. However, unrelated statis-
tics will not work even though they might be estimated from a very large amount of
data as [14].

18

Baochen Sun, Jiashi Feng, and Kate Saenko

30.8 (0.1)
Virtual
Virtual-Gray 32.3 (0.6)
39.9 (0.4)
Amazon
68.2 (0.2)
DSLR

Virtual Virtual-Gray Amazon DSLR PASCAL
24.1 (0.6) 28.3 (0.2) 10.7 (0.5)
27.3 (0.8) 32.7 (0.6) 17.9 (0.7)
39.2 (0.4) 37.9 (0.4) 18.6 (0.6)
68.1 (0.6) 66.5 (0.1) 37.7 (0.5)

16.5 (1.0)
32.3 (0.5)
30.0 (1.0)
62.1 (1.0)

Table 4 MAP of CORAL-LDA trained on positive examples from each row’s source domain and
background statistics from each column’s domain. The average distance between each set of back-
ground statistics and the true source and target statistics is shown in parentheses.

Unsupervised and Semi-supervised Adaptation

Next, we report the results of our unsupervised and semi-supervised adaptation tech-
nique. We use the same setting as [10], in which three positive and nine negative
labeled images per category were used for semi-supervised adaptation. Target co-
variance in Equation 6 is estimated from 305 unlabeled training examples. We also
followed the same approach to learn a linear combination between the unsupervised
and supervised model via cross-validation. The results are presented in Table 5.
Please note that our target-only MAP is 52.9 compared to 36.6 in [10]. This also
conﬁrms our conclusion that the statistics should come from a related domain. It is
clear that both of our unsupervised and semi-supervised adaptation techniques out-
perform the method in [10]. Furthermore, Virtual-Gray data outperforms Virtual,
and DSLR does best, as it is very close to the target domain (the main difference
between DLSR and Webcam domains is in the camera used to capture images).

Finally, we compare our method trained on Virtual-Gray to the results of adapt-
ing from ImageNet reported by [10], in Figure 9. While their unsupervised models
are learned from 150-2000 real ImageNet images per category and the background
statistics are estimated from 10,000 PASCAL images, we only have 30 virtual im-
ages per category and the background statistics is learned from about 1,000 images.
What’s more, all the virtual images used are with uniform gray texturemap and white
background. This clearly demonstrates the importance of domain-speciﬁc decorre-
lation, and shows that the there is no need to collect a large amount of real images
to train a good classiﬁer.

Fig. 9 Comparison of unsupervised and semi-supervised adaptation of virtual detectors using our
method with the results of training on ImageNet and supervised adaptation from ImageNet reported
in [10]. Our semi-supervised adapted detectors achieve comparable performance despite not using
any real source training data, and using only 3 positive images for adaptation, and even outperform
ImageNet signiﬁcantly for several categories (e.g., ruler).

Correlation Alignment for Unsupervised Domain Adaptation

19

Source
Virtual
Virtual-Gray
Amazon
DSLR

Source-only [14] Unsup-Ours SemiSup [10] SemiSup-Ours

10.7
17.9
18.6
37.7

27.9
33.0
38.9
67.1

30.7
35.0
35.8
42.9

45.2
54.7
53.0
71.4

Table 5 Top: Comparison of the source-only [14] and semi-supervised adapted model of [10]
with our unsupervised-adapted and semi-supervised adapted models. Target domain is Webcam.
Mean AP across categories is reported on the Webcam test data, using different source domains for
training. Bottom: Sample detections of the DSLR-UnsupAdapt-Ours detectors.

6 Conclusion

In this chapter, we described a simple, effective, and efﬁcient method for unsu-
pervised domain adaptation called CORrelation ALignment (CORAL). CORAL
minimizes domain shift by aligning the second-order statistics of source and target
distributions, without requiring any target labels. We also developed novel domain
adaptation algorithms by applying the idea of CORAL to three different scenarios.
In the ﬁrst scenario, we applied a linear transformation that minimizes the CORAL
objective to source features prior to training the classiﬁer. In the case of linear classi-
ﬁers, we equivalently applied the linear CORAL transform to the classiﬁer weights,
signiﬁcantly improving efﬁciency and classiﬁcation accuracy over standard LDA
on several domain adaptation benchmarks. We further extended CORAL to learn
a nonlinear transformation that aligns correlations of layer activations in deep neu-
ral networks. The resulting Deep CORAL approach works seamlessly with deep
networks and can be integrated into any arbitrary network architecture to enable
end-to-end unsupervised adaptation. One limitation of CORAL is that it captures
second-order statistics only and may not preserve higher-order structure in the data.
However, as demonstrated in this chapter, it works fairly well in practice, and can
also potentially be combined with other domain-alignment loss functions.

References

1. Jian-Feng Cai, Emmanuel J. Cand`es, and Zuowei Shen. A singular value thresholding algo-

rithm for matrix completion. SIAM J. on Optimization, 20(4):1956–1982, March 2010.

2. H. Daume III. Frustratingly easy domain adaptation. In ACL, 2007.
3. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale

hierarchical image database. In CVPR, 2009.

20

Baochen Sun, Jiashi Feng, and Kate Saenko

4. Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and
Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition.
In ICML, 2014.

5. L. Duan, I. W. Tsang, D. Xu, and T. Chua. Domain adaptation from multiple sources via

6. Lixin Duan, Ivor W. Tsang, and Dong Xu. Domain transfer multiple kernel learning. TPAMI,

auxiliary classiﬁers. In ICML, 2009.

34(3):465–479, March 2012.

7. Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object de-
tection with discriminatively trained part-based models. TPAMI, 32(9):1627–1645, 2010.
8. Basura Fernando, Amaury Habrard, Marc Sebban, and Tinne Tuytelaars. Unsupervised visual

domain adaptation using subspace alignment. In ICCV, 2013.

9. Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation.

In ICML, 2015.

10. Daniel Goehring, Judy Hoffman, Erik Rodner, Kate Saenko, and Trevor Darrell. Interactive

adaptation of real-time object detectors. In ICRA, 2014.

11. B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic ﬂow kernel for unsupervised domain

12. R. Gopalan, R. Li, and R. Chellappa. Domain adaptation for object recognition: An unsuper-

adaptation. In CVPR, 2012.

vised approach. In ICCV, 2011.

13. Maayan Harel and Shie Mannor. Learning from multiple outlooks. In ICML, 2011.
14. Bharath Hariharan, Jitendra Malik, and Deva Ramanan. Discriminative decorrelation for clus-

tering and classiﬁcation. In ECCV. 2012.

15. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training

by reducing internal covariate shift. In ICML, 2015.

16. Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick,
Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature
embedding. arXiv preprint, 2014.

17. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep

convolutional neural networks. In NIPS, 2012.

18. B. Kulis, K. Saenko, and T. Darrell. What you saw is not what you get: Domain adaptation

using asymmetric kernel transforms. In CVPR, 2011.

19. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable

features with deep adaptation networks. In ICML, 2015.

20. Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and P.S. Yu. Transfer joint

matching for unsupervised domain adaptation. In CVPR, 2014.

21. Tomasz Malisiewicz, Abhinav Gupta, and Alexei A Efros. Ensemble of exemplar-svms for

22. Sinno Jialin Pan, Ivor W. Tsang, James T. Kwok, and Qiang Yang. Domain adaptation via

object detection and beyond. In ICCV, 2011.

transfer component analysis. In IJCAI, 2009.

23. Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models

24. Baochen Sun. Correlation Alignment for Domain Adaptation. PhD thesis, University of

25. Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation.

26. Baochen Sun, Xingchao Peng, and Kate Saenko. Generating large scale image datasets from

3d cad models. In CVPR’15 Workshop on The Future of Datasets in Vision, 2015.

27. Baochen Sun and Kate Saenko. From virtual to reality: Fast adaptation of virtual object de-

tectors to real domains. In BMVC, 2014.

28. Baochen Sun and Kate Saenko. Subspace distribution alignment for unsupervised domain

29. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation.

adaptation. In BMVC, 2015.

In ECCV 2016 Workshops, 2016.

30. Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain

confusion: Maximizing for domain invariance. CoRR, abs/1412.3474, 2014.

to new domains. In ECCV, 2010.

Massachusetts Lowell, 2016.

In AAAI, 2016.

6
1
0
2
 
c
e
D
 
6
 
 
]

V
C
.
s
c
[
 
 
1
v
9
3
9
1
0
.
2
1
6
1
:
v
i
X
r
a

Correlation Alignment for Unsupervised
Domain Adaptation

Baochen Sun, Jiashi Feng, and Kate Saenko

Abstract In this chapter, we present CORrelation ALignment (CORAL), a sim-
ple yet effective method for unsupervised domain adaptation. CORAL minimizes
domain shift by aligning the second-order statistics of source and target distribu-
tions, without requiring any target labels. In contrast to subspace manifold meth-
ods, it aligns the original feature distributions of the source and target domains,
rather than the bases of lower-dimensional subspaces. It is also much simpler
than other distribution matching methods. CORAL performs remarkably well in
extensive evaluations on standard benchmark datasets. We ﬁrst describe a solu-
tion that applies a linear transformation to source features to align them with tar-
get features before classiﬁer training. For linear classiﬁers, we propose to equiva-
lently apply CORAL to the classiﬁer weights, leading to added efﬁciency when the
number of classiﬁers is small but the number and dimensionality of target exam-
ples are very high. The resulting CORAL Linear Discriminant Analysis (CORAL-
LDA) outperforms LDA by a large margin on standard domain adaptation bench-
marks. Finally, we extend CORAL to learn a nonlinear transformation that aligns
correlations of layer activations in deep neural networks (DNNs). The resulting
Deep CORAL approach works seamlessly with DNNs and achieves state-of-the-
art performance on standard benchmark datasets. Our code is available at: https:
//github.com/VisionLearningGroup/CORAL

Baochen Sun
Microsoft, e-mail: baochens@gmail.com

Jiashi Feng
National University of Singapore, e-mail: elefjia@nus.edu.sg

Kate Saenko
Boston University, e-mail: saenko@bu.edu

1

2

1 Introduction

Baochen Sun, Jiashi Feng, and Kate Saenko

Machine learning is very different from human learning. Humans are able to learn
from very few labeled examples and apply the learned knowledge to new examples
in novel conditions. In contrast, traditional machine learning algorithms assume that
the training and test data are independent and identically distributed (i.i.d.) and su-
pervised machine learning methods only perform well when the given extensive
labeled data are from the same distribution as the test distribution. However, this
assumption rarely holds in practice, as the data are likely to change over time and
space. To compensate for the degradation in performance due to domain shift, do-
main adaptation [23, 8, 11, 20, 28, 25, 30, 2] tries to transfer knowledge from source
(training) domain to target (test) domain. Our approach is in line with most existing
unsupervised domain adaptation approaches in that we ﬁrst transform the source
data to be as close to the target data as possible. Then a classiﬁer trained on the
transformed source domain is applied to the target data.

In this chapter, we mainly focus on the unsupervised scenario as we believe that
leveraging the unlabeled target data is key to the success of domain adaptation. In
real world applications, unlabeled target data are often much more abundant and
easier to obtain. On the other side, labeled examples are very limited and require
human annotation. So the question of how to utilize the unlabeled target data is
more important for practical visual domain adaptation. For example, it would be
more convenient and applicable to have a pedestrian detector automatically adapt to
the changing visual appearance of pedestrians rather than having a human annotator
label every frame that has a different visual appearance.

Our goal is to propose a simple yet effective approach for domain adaptation that
researchers with little knowledge of domain adaptation or machine learning could
easily integrate into their application. In this chapter, we describe a “frustratingly
easy” (to borrow a phrase from [2]) unsupervised domain adaptation method called
CORrelation ALignment or CORAL [25] in short. CORAL aligns the input feature
distributions of the source and target domains by minimizing the difference between
their second-order statistics. The intuition is that we want to capture the structure
of the domain using feature correlations. As an example, imagine a target domain
of Western movies where most people wear hats; then the “head” feature may be
positively correlated with the “hat” feature. Our goal is to transfer such correlations
to the source domain by transforming the source features.

In Section 2, we describe a linear solution to CORAL, where the distributions
are aligned by re-coloring whitened source features with the covariance of the tar-
get data distribution. This solution is simple yet efﬁcient, as the only computations
it needs are (1) computing covariance statistics in each domain and (2) applying
the whitening and re-coloring linear transformation to the source features. Then, su-
pervised learning proceeds as usual–training a classiﬁer on the transformed source
features.

For linear classiﬁers, we can equivalently apply the CORAL transformation to
the classiﬁer weights, leading to better efﬁciency when the number of classiﬁers is
small but the number and dimensionality of the target examples are very high. We

Correlation Alignment for Unsupervised Domain Adaptation

3

present the resulting CORAL–Linear Discriminant Analysis (CORAL-LDA) [27]
in Section 3, and show that it outperforms standard Linear Discriminant Analysis
(LDA) by a large margin on cross domain applications. We also extend CORAL to
work seamlessly with deep neural networks by designing a layer that consists of a
differentiable CORAL loss [29], detailed in Section 4. On the contrary to the linear
CORAL, Deep CORAL learns a nonlinear transformation and also provides end-
to-end adaptation. Section 5 describes extensive quantitative experiments on several
benchmarks, and Section 6 concludes the chapter.

2 Linear Correlation Alignment

In this section, we present CORrelation ALignment (CORAL) for unsupervised do-
main adaptation and derive a linear solution. We ﬁrst describe the formulation and
derivation, followed by the main linear CORAL algorithm and its relationship to ex-
isting approaches. In this section and Section 3, we constrain the transformation to
be linear. Section 4 extends CORAL to learn a nonlinear transformation that aligns
correlations of layer activations in deep neural networks (Deep CORAL).

2.1 Formulation and Derivation

We describe our method by taking a multi-class classiﬁcation problem as the run-
ning example. Suppose we are given source-domain training examples DS = {xi},
x ∈ Rd with labels LS = {yi}, y ∈ {1, ..., L}, and target data DT = {ui}, u ∈ Rd. Here
both x and u are the d-dimensional feature representations φ (I) of input I. Suppose
µs, µt and CS,CT are the feature vector means and covariance matrices. Assuming
that all features are normalized to have zero mean and unit variance, µt = µs = 0
after the normalization step, while CS (cid:54)= CT .

To minimize the distance between the second-order statistics (covariance) of the
source and target features, we apply a linear transformation A to the original source
features and use the Frobenius norm as the matrix distance metric:

(cid:107)C ˆS −CT (cid:107)2

F

min
A

= min
A

2
(cid:107)A(cid:62)CSA −CT (cid:107)
F

(1)

where C ˆS is covariance of the transformed source features DsA and (cid:107) · (cid:107)2
the squared matrix Frobenius norm.

F denotes

If rank(CS) ≥ rank(CT ), then an analytical solution can be obtained by choosing
A such that C ˆS = CT . However, the data typically lie on a lower dimensional man-
ifold [13, 11, 8], and so the covariance matrices are likely to be low rank [14]. We
derive a solution for this general case, using the following lemma.

4

Baochen Sun, Jiashi Feng, and Kate Saenko

Lemma 1. Let Y be a real matrix of rank rY and X a real matrix of rank at most
r, where r (cid:54) rY ; let Y = UY ΣYVY be the SVD of Y , and ΣY [1:r], UY [1:r], VY [1:r] be the
largest r singular values and the corresponding left and right singular vectors of Y
(cid:62) is the optimal solution to the problem
respectively. Then, X ∗ = UY [1:r]ΣY [1:r]VY [1:r]
of min
X

(cid:107)X −Y (cid:107)2

F . [1]

Theorem 1. Let Σ + be the Moore-Penrose pseudoinverse of Σ , rCS and rCT denote
the rank of CS and CT respectively. Then, A∗ = USΣ +
(cid:62) is
S
the optimal solution to the problem in Equation (12) with r = min(rCS , rCT ).

(cid:62)UT [1:r]ΣT [1:r]

1
2 UT [1:r]

1
2 US

Proof. Since A is a linear transformation, A(cid:62)CSA does not increase the rank of CS.
(cid:54) rCS . Since CS and CT are symmetric matrices, conducting SVD on CS
Thus, rC ˆS
and CT gives CS = USΣSUS
T respectively. We ﬁrst ﬁnd the op-
timal value of C ˆS through considering the following two cases:
Case 1. rCS > rCT . The optimal solution is C ˆS = CT . Thus, C ˆS = UT ΣTUT
UT [1:r]ΣT [1:r]UT [1:r]

(cid:62) is the optimal solution to Equation (12) where r = rCT .

(cid:62) and CT = UT ΣTU (cid:62)

(cid:62) =

Case 2. rCS
optimal solution to Equation (12) where r = rCS .

(cid:54) rCT . Then, according to Lemma 1, C ˆS = UT [1:r]ΣT [1:r]UT [1:r]

(cid:62) is the

Combining the results in the above two cases yields that C ˆS = UT [1:r]ΣT [1:r]UT [1:r]
is the optimal solution to Equation (12) with r = min(rCS , rCT ). We then proceed to
solve for A based on the above result. Letting C ˆS = A(cid:62)CSA, we can write

(cid:62)

A(cid:62)CSA = UT [1:r]ΣT [1:r]UT [1:r]

(cid:62).

Since CS = USΣSUS

(cid:62), we have

A(cid:62)USΣSUS

(cid:62)A = UT [1:r]ΣT [1:r]UT [1:r]

(cid:62).

This gives:

(cid:62)
(cid:62)A)

(US

ΣS(US

(cid:62)A) = UT [1:r]ΣT [1:r]UT [1:r]

(cid:62).

Let E = Σ +
(cid:62)UT [1:r]ΣT [1:r]
S
tion can be re-written as E(cid:62)ΣSE. This gives

1
2 UT [1:r]

1
2 US

(cid:62), then the right hand side of the above equa-

(cid:62)
(cid:62)A)

(US

ΣS(US

(cid:62)A) = E(cid:62)ΣSE

By setting US

(cid:62)A to E, we obtain the optimal solution of A as

A∗ = USE

= (USΣ +
S

1
2 US

(cid:62))(UT [1:r]ΣT [1:r]

1
2 UT [1:r]

(cid:62)).

(2)

Correlation Alignment for Unsupervised Domain Adaptation

5

Fig. 1 (a-c) Illustration of CORrelation ALignment (CORAL) for Domain Adaptation: (a) The
original source and target domains have different distribution covariances, despite the features be-
ing normalized to zero mean and unit standard deviation. This presents a problem for transferring
classiﬁers trained on source to target. (b) The same two domains after source decorrelation, i.e.
removing the feature correlations of the source domain. (c) Target re-correlation, adding the corre-
lation of the target domain to the source features. After this step, the source and target distributions
are well aligned and the classiﬁer trained on the adjusted source domain is expected to work well
in the target domain. (d) One might instead attempt to align the distributions by whitening both
source and target. However, this will fail since the source and target data are likely to lie on different
subspaces due to domain shift. (Best viewed in color)

2.2 Algorithm

Figures 1(a-c) illustrate the linear CORAL approach. Figure 1(a) shows example
original source and target data distributions. We can think of the transformation A
1
intuitively as follows: the ﬁrst part USΣ +
(cid:62) whitens the source data, while the
2 US
S
(cid:62) re-colors it with the target covariance. This is
second part UT [1:r]ΣT [1:r]
illustrated in Figure 1(b) and Figure 1(c) respectively.

1
2 UT [1:r]

In practice, for the sake of efﬁciency and stability, we can avoid the expensive
SVD steps and perform traditional data whitening and coloring. Traditional whiten-
ing adds a small regularization parameter λ to the diagonal elements of the covari-
ance matrix to explicitly make it full rank and then multiplies the original features
by its inverse square root (or square root for coloring.) This is advantageous be-
cause: (1) it is faster1 and more stable, as SVD on the original covariance matrices

1 the entire CORAL transformation takes less than one minute on a regular laptop for dimensions
as large as DS ∈ R795×4096 and DT ∈ R2817×4096

6

Baochen Sun, Jiashi Feng, and Kate Saenko

might not be stable and might be slow to converge; (2) as illustrated in Figure 2,
the performance is similar to the analytical solution in Equation (2) and very stable
with respect to λ . In the experiments provided at the end of this chapter we set λ to
1. The ﬁnal algorithm can be written in four lines of MATLAB code as illustrated
in Algorithm 1.

Fig. 2 Sensitivity of CORAL to the covariance regularization parameter λ with λ ∈ {0, 0.001,
0.01, 0.1, 1}. The plots show classiﬁcation accuracy on target data for two domain shifts (blue and
red). When λ = 0, there is no regularization and we use the analytical solution in Equation (2).
Please refer to Section 5.1 for details of the experiment.

Algorithm 1 CORAL for Unsupervised Domain Adaptation

Input: Source Data DS, Target Data DT
Output: Adjusted Source Data D∗
s
CS = cov(DS) + eye(size(DS, 2))
CT = cov(DT ) + eye(size(DT , 2))

−1
2

DS = DS ∗C
S
1
D∗
2
S = DS ∗C
T

% whitening source

% re-coloring with target covariance

One might instead attempt to align the distributions by whitening both source
and target. As shown in Figure 1(d), this will fail as the source and target data are
likely to lie on different subspaces due to domain shift. An alternative approach
would be whitening the target and then re-coloring it with the source covariance.
However, as demonstrated in [13, 8] and our experiments, transforming data from
source to target space gives better performance. This might be due to the fact that by
transforming the source to target space the classiﬁer is trained using both the label
information from the source and the unlabelled structure from the target.

After CORAL transforms the source features to the target space, a classiﬁer fw
parametrized by w can be trained on the adjusted source features and directly ap-
plied to target features. For a linear classiﬁer fw(I) = wT φ (I), we can apply an
equivalent transformation to the parameter vector w (e.g., fw(I) = (wT A)φ (I)) in-
stead of the features (e.g., fw(I) = wT (Aφ (I))). This results in added efﬁciency
when the number of classiﬁers is small but the number and dimensionality of target

Correlation Alignment for Unsupervised Domain Adaptation

7

examples is very high. For linear SVM, this extension is straightforward. In Sec-
tion 3, we apply the idea of CORAL to another commonly used linear classiﬁer–
Linear Discriminant Analysis (LDA). LDA is special in the sense that its weights
also use the covariance of the data. It is also extremely efﬁcient for training a large
number of classiﬁers [14].

2.3 Relationship to Existing Methods

It has long been known that input feature normalization improves many machine
learning methods, e.g., [15]. However, CORAL does not simply perform feature nor-
malization, but rather aligns two different distributions. Batch Normalization [15]
tries to compensate for internal covariate shift by normalizing each mini-batch to be
zero-mean and unit-variance. However, as illustrated in Figure 1(a), such normal-
ization might not be enough. Even if used with full whitening, Batch Normalization
may not compensate for external covariate shift: the layer activations will be decor-
related for a source point but not for a target point. What’s more, as mentioned in
Section 2.2, whitening both domains is not a successful strategy.

Recent state-of-the-art unsupervised approaches project the source and target dis-
tributions into a lower-dimensional manifold and ﬁnd a transformation that brings
the subspaces closer together [12, 11, 8, 13]. CORAL avoids subspace projection,
which can be costly and requires selecting the hyper-parameter that controls the di-
mensionality of the subspace, k. We note that subspace-mapping approaches [13, 8]
only align the top k eigenvectors of the source and target covariance matrices. On the
contrary, CORAL aligns the covariance matrices, which can only be re-constructed
using all eigenvectors and eigenvalues. Even though the eigenvectors can be aligned
well, the distributions can still differ a lot due to the difference of eigenvalues be-
tween the corresponding eigenvectors of the source and target data. CORAL is a
more general and much simpler method than the above two as it takes into ac-
count both eigenvectors and eigenvalues of the covariance matrix without the burden
of subspace dimensionality selection.

Maximum Mean Discrepancy (MMD) based methods (e.g., TCA [22], DAN [19])
for domain adaptation can be interpreted as “moment matching” and can ex-
press arbitrary statistics of the data. Minimizing MMD with a polynomial kernel
(k(x, y) = (1 + x(cid:48)y)q with q = 2) is similar to the CORAL objective, however, no
previous work has used this kernel for domain adaptation nor proposed a closed
form solution to the best of our knowledge. The other difference is that MMD based
approaches usually apply the same transformation to both the source and target do-
main. As demonstrated in [18, 13, 8], asymmetric transformations are more ﬂexible
and often yield better performance for domain adaptation tasks. Intuitively, symmet-
ric transformations ﬁnd a space that “ignores” the differences between the source
and target domain while asymmetric transformations try to “bridge” the two do-
mains.

8

Baochen Sun, Jiashi Feng, and Kate Saenko

3 CORAL Linear Discriminant Analysis

In this section, we introduce how CORAL can be applied for aligning multiple linear
classiﬁers. In particular, we take LDA as the example for illustration, considering
LDA is a commonly used and effective linear classiﬁer. Combining CORAL and
LDA gives a new efﬁcient adpative learning approach CORAL-LDA. We use the
task of object detection as a running example to explain CORAL-LDA.

We begin by describing the decorrelation-based approach to detection proposed
in [14]. Given an image I, it follows the sliding-window paradigm, extracting a
d-dimensional feature vector φ (I, b) at each window b across all locations and at
multiple scales. It then scores the windows using a scoring function

fw(I, b) = w(cid:62)φ (I, b).

(3)

In practice, all windows with values of fw above a predetermined threshold are
considered positive detections.

In recent years, use of the linear SVM as the scoring function fw, usually with
Histogram of Gradients (HOG) as the features φ , has emerged as the predominant
object detection paradigm. Yet, as observed by Hariharan et al. [14], training SVMs
can be expensive, especially because it usually involves costly rounds of hard neg-
ative mining. Furthermore, the training must be repeated for each object category,
which makes it scale poorly with the number of categories.

Hariharan et al. [14] proposed a much more efﬁcient alternative, learning fw
with Linear Discriminant Analysis (LDA). LDA is a well-known linear classiﬁer
that models the training set of examples x with labels y ∈ {0, 1} as being generated
by p(x, y) = p(x|y)p(y). p(y) is the prior on class labels and the class-conditional
densities are normal distributions

p(x|y) = N (x; µ y, CS),

where the feature vector covariance CS is assumed to be the same for both positive
and negative (background) classes. In our case, the feature is represented by x =
φ (I, b). The resulting classiﬁer is given by

w = CS

−1(µ1 − µ0)

The innovation in [14] was to re-use CS and µ0, the background mean, for all cate-
gories, reducing the task of learning a new category model to computing the average
positive feature, µ1. This was accomplished by calculating CS and µ0 for the largest
possible window and subsampling to estimate all other smaller window sizes. Also,
CS was shown to have a sparse local structure, with correlation falling off sharply
beyond a few nearby image locations.

Like other classiﬁers, LDA learns to suppress non-discriminative structures and
enhance the contours of the object. However it does so by learning the global co-
variance statistics once for all natural images, and then using the inverse covariance
matrix to remove the non-discriminative correlations, and the negative mean to re-

(4)

(5)

Correlation Alignment for Unsupervised Domain Adaptation

9

Fig. 3 (a) Applying a linear classiﬁer w learned by LDA to source data x is equivalent to (b)
−1/2x. (c) However, target points u
applying classiﬁer ˆw = CS
−1/2u, hurting performance. (d) Our method uses target-speciﬁc
may still be correlated after CS
covariance to obtain properly decorrelated ˆu.

−1/2w to decorrelated points CS

move the average feature. LDA was shown in [14] to have competitive performance
to SVM, and can be implemented both as an exemplar-based [21] or as deformable
parts model (DPM) [7].

We observe that estimating global statistics CS and µ0 once and re-using them for
all tasks may work when training and testing in the same domain, but in our case,
the source training data is likely to have different statistics from the target data.
Figure 4 illustrates the effect of centering and decorrelating a positive mean using
global statistics from the wrong domain. The effect is clear: important discriminative
information is removed while irrelevant structures are not.

Based on this observation, we propose an adaptive decorrelation approach to
detection. Assume that we are given labeled training data {x, y} in the source domain
(e.g., virtual images rendered from 3D models), and unlabeled examples u in the
target domain (e.g., real images collected in an ofﬁce environment). Evaluating the
scoring function fw(x) in the source domain is equivalent to ﬁrst de-correlating the
−1/2x, computing their positive and negative class means
training features ˆx = CS
−1/2µ0 and then projecting the decorrelated feature onto
ˆµ1 = CS
the decorrelated difference between means, fw(x) = ˆw(cid:62) ˆx, where ˆw = ( ˆµ1 − ˆµ0).
This is illustrated in Figure 3(a-b).

−1/2µ1 and ˆµ0 = CS

However, as we saw in Figure 4, the assumption that the input is properly decor-
related does not hold if the input comes from a target domain with a different co-
−1/2u does not
variance structure. Figure 3(c) illustrates this case, showing that CS
have isotropic covariance. Therefore, w cannot be used directly.

Fig. 4 Visualization of classiﬁer weights of bicycle (decorrelated with mismatched-domain co-
variance (left) v.s. with same-domain covariance (right)).

10

Baochen Sun, Jiashi Feng, and Kate Saenko

We may be able to compute the covariance of the target domain on the unlabeled
target points u, but not the positive class mean. Therefore, we would like to re-use
the decorrelated mean difference ˆw, but adapt to the covariance of the target do-
main. In the rest of the chapter, we make the assumption that the difference between
positive and negative means is the same in the source and target. This may or may
not hold in practice, and we discuss this further in Section 5.

Let the estimated target covariance be CT. We ﬁrst decorrelate the target input
feature with its inverse square root, and then apply ˆw directly, as shown in Fig-
ure 3(d). The resulting scoring function is:

f ˆw(u) = ˆw(cid:62) ˆu

= (CS

= ((CT

(cid:62)
−1/2(µ1 − µ0))
−1/2)(cid:62)CS

−1/2u)
(CT
(cid:62)
−1/2(µ1 − µ0))

u

(6)

This corresponds to a transformation (CT

−1/2) instead of the original
−1/2)(cid:62)(CS
−1 being applied to the difference between means to compute w. Note
−1/2) equals to
−1 since both CT and CS are symmetric. In this case, Equation 6 ends up the

whitening CS
that if source and target domains are the same, then (CT
CS
same as Equation 5.

−1/2)(cid:62)(CS

In practice, either the source or the target component of the above transformation
may also work, or even statistics from similar domains. However, as we will see in
Section 5.2, dissimilar domain statistics can signiﬁcantly hurt performance. Further-
more, if either source or target has only images of the positive category available,
and cannot be used to properly compute background statistics, the other domain can
still be used.

CORAL-LDA works in a purely unsupervised way. Here, we extend it to semi-
supervised adaptation when a few labeled examples are available in the target do-
main. Following [10], a simple adaptation method is used whereby the template
learned on source positives is combined with a template learned on target positives,
using a weighted combination. The key difference with our approach is that the
target template uses target-speciﬁc statistics.

In [10], the author uses the same background statistics as [14] which were es-
timated on 10,000 natural images from the PASCAL VOC 2010 dataset. Based on
our analysis above, even though these background statistics were estimated from a
very large amount of real image data, it will not work for all domains. In section 5.2,
our results conﬁrm this claim.

4 Deep CORAL

In this section, we extend CORAL to work seamlessly with deep neural networks
by designing a differentiable CORAL loss. Deep CORAL enables end-to-end adap-
tation and also learns more a powerful nonlinear transformation. It can be easily

Correlation Alignment for Unsupervised Domain Adaptation

11

Fig. 5 Sample Deep CORAL architecture based on a CNN with a classiﬁer layer. For generaliza-
tion and simplicity, here we apply the CORAL loss to the f c8 layer of AlexNet [17]. Integrating it
into other layers or network architectures is also possible.

integrated into different layers or network architectures. Figure 5 shows a sample
Deep CORAL architecture using our proposed correlation alignment layer for deep
domain adaptation. We refer to Deep CORAL as any deep network incorporating
the CORAL loss for domain adaptation.

We ﬁrst describe the CORAL loss between two domains for a single feature layer.
Suppose the numbers of source and target data are nS and nT respectively. Here both
x and u are the d-dimensional deep layer activations φ (I) of input I that we are
trying to learn. Suppose Di j
T ) indicates the j-th dimension of the i-th source
(target) data example and CS (CT ) denote the feature covariance matrices.

S (Di j

We deﬁne the CORAL loss as the distance between the second-order statistics

(covariances) of the source and target features:

LCORAL =

1
4d2 (cid:107)CS −CT (cid:107)2

F

(7)

(8)

(9)

where (cid:107) · (cid:107)2
of the source and target data are given by:

F denotes the squared matrix Frobenius norm. The covariance matrices

CS =

1
nS − 1

(D(cid:62)

S DS −

(1(cid:62)DS)(cid:62)(1(cid:62)DS))

1
nT − 1
where 1 is a column vector with all elements equal to 1.

T DT −

CT =

1
nT

(D(cid:62)

(1(cid:62)DT )(cid:62)(1(cid:62)DT ))

The gradient with respect to the input features can be calculated using the chain

rule:

∂ LCORAL
∂ Di j
S

=

1
d2(nS − 1)

((D(cid:62)

S −

(1(cid:62)DS)(cid:62)1(cid:62))(cid:62)(CS −CT ))i j

(10)

1
nS

1
nS

12

Baochen Sun, Jiashi Feng, and Kate Saenko

∂ LCORAL
∂ Di j
T

= −

1
d2(nT − 1)

1
nT

((D(cid:62)

T −

(1(cid:62)DT )(cid:62)1(cid:62))(cid:62)(CS −CT ))i j

(11)

In our experiments, we use batch covariances and the network parameters are shared
between the two networks, but other settings are also possible.

To see how this loss can be used to adapt an existing neural network, let us
return to the multi-class classiﬁcation problem. Suppose we start with a network
with a ﬁnal classiﬁcation layer, such as the ConvNet shown in Figure 5. As men-
tioned before, the ﬁnal deep features need to be both discriminative enough to train
a strong classiﬁer and invariant to the difference between source and target domains.
Minimizing the classiﬁcation loss itself is likely to lead to overﬁtting to the source
domain, causing reduced performance on the target domain. On the other hand, min-
imizing the CORAL loss alone might lead to degenerated features. For example, the
network could project all of the source and target data to a single point, making
the CORAL loss trivially zero. However, no strong classiﬁer can be constructed on
these features. Joint training with both the classiﬁcation loss and CORAL loss is
likely to learn features that work well on the target domain:

L = LCLASS +

λiLCORALi

(12)

t
∑
i=1

where t denotes the number of CORAL loss layers in a deep network and λi is a
weight that trades off the adaptation with classiﬁcation accuracy on the source do-
main. As we show below, these two losses play counterparts and reach an equilib-
rium at the end of training, where the ﬁnal features are discriminative and generalize
well to the target domain.

5 Experiments

We evaluate CORAL and Deep CORAL on object recognition [23] using standard
benchmarks and protocols. In all experiments we assume the target domain is unla-
beled. For CORAL, we follow the standard procedure [8, 4] and use a linear SVM as
the base classiﬁer. The model selection approach of [8] is used to set the C parame-
ter for the SVM by doing cross-validation on the source domain. For CORAL-LDA,
as efﬁciency is the main concern, we evaluate it on the more time constrained task–
object detection. We follow the protocol of [10] and use HOG features. To have a
fair comparison, we use accuracies reported by other authors with exactly the same
setting or conduct experiments using the source code provided by the authors.

Correlation Alignment for Unsupervised Domain Adaptation

13

5.1 Object Recognition

In this set of experiments, domain adaptation is used to improve the accuracy of
an object classiﬁer on novel image domains. Both the standard Ofﬁce [23] and
extended Ofﬁce-Caltech10 [11] datasets are used as benchmarks in this chap-
ter. Ofﬁce-Caltech10 contains 10 object categories from an ofﬁce environment
(e.g., keyboard, laptop, etc.) in 4 image domains: Webcam, DSLR, Amazon, and
Caltech256. The standard Ofﬁce dataset contains 31 (the same 10 categories from
Ofﬁce-Caltech10 plus 21 additional ones) object categories in 3 domains: Webcam,
DSLR, and Amazon.

A→C A→D A→W C→A C→D C→W D→A D→C D→W W→A W→C W→D AVG
37.8
26.4
35.8
NA
41.0
33.4
34.8
SVMA
40.2
33.5
34.9
DAM
43.4
37.9
38.3
GFK
45.7
39.6
40.0
TCA
42.0
45.9
SA
39.9
46.7
CORAL 40.3
38.1

25.7
33.5
31.2
29.1
33.7
31.8
34.6

27.1
31.4
31.5
31.4
34.0
35.0
34.2

32.3
36.6
34.7
37.1
40.2
39.3
37.8

39.4
34.5
34.7
36.1
41.4
39.4
40.7

24.9
32.5
32.5
39.8
40.1
39.6
38.7

33.1
34.1
34.3
37.9
39.1
38.8
38.3

30.0
32.9
33.1
34.9
36.2
38.9
39.2

56.4
74.4
74.7
79.1
80.4
82.3
85.9

78.9
75.0
68.3
74.6
77.5
77.9
84.9

43.7
39.1
39.2
44.8
46.7
46.1
47.2

Table 1 Object recognition accuracies of all 12 domain shifts on the Ofﬁce-Caltech10 dataset [11]
with SURF features, following the protocol of [11, 8, 12, 18, 23].

Object Recognition with Shallow Features

We follow the standard protocol of [11, 8, 12, 18, 23] and conduct experiments
on the Ofﬁce-Caltech10 dataset with shallow features (SURF). The SURF features
were encoded with 800-bin bag-of-words histograms and normalized to have zero
mean and unit standard deviation in each dimension. Since there are four domains,
there are 12 experiment settings, namely, A→C (train classiﬁer on (A)mazon, test
on (C)altech), A→D (train on (A)mazon, test on (D)SLR), A→W, and so on. We
follow the standard protocol and conduct experiments in 20 randomized trials for
each domain shift and average the accuracy over the trials. In each trial, we use the
standard setting [11, 8, 12, 18, 23] and randomly sample the same number (20 for
Amazon, Caltech, and Webcam; 8 for DSLR as there are only 8 images per category
in the DSLR domain) of labelled images in the source domain as training set, and
use all the unlabelled data in the target domain as the test set.

In Table 1, we compare our method to ﬁve recent published methods: SVMA [6],
DAM [5], GFK [11], SA [8], and TCA [22] as well as the no adaptation base-
line (NA). GFK, SA, and TCA are manifold based methods that project the source
and target distributions into a lower-dimensional manifold. GFK integrates over an
inﬁnite number of subspaces along the subspace manifold using the kernel trick.
SA aligns the source and target subspaces by computing a linear map that mini-
mizes the Frobenius norm of their difference. TCA performs domain adaptation via
a new parametric kernel using feature extraction methods by projecting data onto

14

Baochen Sun, Jiashi Feng, and Kate Saenko

the learned transfer components. DAM introduces smoothness assumption to en-
force the target classiﬁer share similar decision values with the source classiﬁers.
Even though these methods are far more complicated than ours and require tuning
of hyperparameters (e.g., subspace dimensionality), our method achieves the best
average performance across all the 12 domain shifts. Our method also improves on
the no adaptation baseline (NA), in some cases increasing accuracy signiﬁcantly
(from 56% to 86% for D→W).

Object Recognition with Deep Features

For visual domain adaptation with deep features, we follow the standard protocol
of [11, 19, 4, 30, 9] and use all the labeled source data and all the target data without
labels on the standard Ofﬁce dataset [23]. Since there are 3 domains, we conduct
experiments on all 6 shifts (5 runs per shift), taking one domain as the source and
another as the target.

In this experiment, we apply the CORAL loss to the last classiﬁcation layer as it
is the most general case–most deep classiﬁer architectures (e.g., convolutional neu-
ral networks, recurrent neural networks) contain a fully connected layer for classi-
ﬁcation. Applying the CORAL loss to other layers or other network architectures is
also possible. The dimension of the last fully connected layer ( f c8) was set to the
number of categories (31) and initialized with N (0, 0.005). The learning rate of
f c8 was set to 10 times the other layers as it was training from scratch. We initial-
ized the other layers with the parameters pre-trained on ImageNet [3] and kept the
original layer-wise parameter settings. In the training phase, we set the batch size to
128, base learning rate to 10−3, weight decay to 5 × 10−4, and momentum to 0.9.
The weight of the CORAL loss (λ ) is set in such way that at the end of training
the classiﬁcation loss and CORAL loss are roughly the same. It seems be a reason-
able choice as we want to have a feature representation that is both discriminative
and also minimizes the distance between the source and target domains. We used
Caffe [16] and BVLC Reference CaffeNet for all of our experiments.

We compare to 7 recently published methods: CNN [17] (no adaptation), GFK [11],

SA [8], TCA [22], CORAL [25], DDC [30], DAN [19]. GFK, SA, and TCA are
manifold based methods that project the source and target distributions into a lower-
dimensional manifold and are not end-to-end deep methods. DDC adds a domain
confusion loss to AlexNet [17] and ﬁne-tunes it on both the source and target do-
main. DAN is similar to DDC but utilizes a multi-kernel selection method for better
mean embedding matching and adapts in multiple layers. For direct comparison,
DAN in this paper uses the hidden layer f c8. For GFK, SA, TCA, and CORAL, we
use the f c7 feature ﬁne-tuned on the source domain (FT 7 in [25]) as it achieves bet-
ter performance than generic pre-trained features, and train a linear SVM [8, 25]. To
have a fair comparison, we use accuracies reported by other authors with exactly the
same setting or conduct experiments using the source code provided by the authors.
From Table 2 we can see that Deep CORAL (D-CORAL) achieves better average
performance than CORAL and the other 6 baseline methods. In 3 out of 6 shifts, it

Correlation Alignment for Unsupervised Domain Adaptation

15

A→D A→W D→A D→W W→A W→D AVG
52.4±0.0 54.7±0.0 43.2±0.0 92.1±0.0 41.8±0.0 96.2±0.0 63.4
GFK
50.6±0.0 47.4±0.0 39.5±0.0 89.1±0.0 37.6±0.0 93.8±0.0 59.7
SA
46.8±0.0 45.5±0.0 36.4±0.0 81.1±0.0 39.5±0.0 92.2±0.0 56.9
TCA
65.7±0.0 64.3±0.0 48.5±0.0 96.1±0.0 48.2±0.0 99.8±0.0 70.4
CORAL
63.8±0.5 61.6±0.5 51.1±0.6 95.4±0.3 49.8±0.4 99.0±0.2 70.1
CNN
64.4±0.3 61.8±0.4 52.1±0.8 95.0±0.5 52.2±0.4 98.5±0.4 70.6
DDC
65.8±0.4 63.8±0.4 52.8±0.4 94.6±0.5 51.9±0.5 98.8±0.6 71.3
DAN
D-CORAL 66.8±0.6 66.4±0.4 52.8±0.2 95.7±0.3 51.5±0.3 99.2±0.1 72.1

Table 2 Object recognition accuracies for all 6 domain shifts on the standard Ofﬁce dataset with
deep features, following the standard unsupervised adaptation protocol.

Fig. 6 Detailed analysis of shift A→W for training w/ v.s. w/o CORAL loss. (a): training and test
accuracies for training w/ v.s. w/o CORAL loss. We can see that adding CORAL loss helps achieve
much better performance on the target domain while maintaining strong classiﬁcation accuracy on
the source domain. (b): classiﬁcation loss and CORAL loss for training w/ CORAL loss. As the
last fully connected layer is randomly initialized with N (0, 0.005), CORAL loss is very small
while classiﬁcation loss is very large at the beginning. After training for a few hundred iterations,
these two losses are about the same. (c): CORAL distance for training w/o CORAL loss (setting
the weight to 0). The distance is getting much larger ((cid:62) 100 times larger compared to training w/
CORAL loss).

achieves the highest accuracy. For the other 3 shifts, the margin between D-CORAL
and the best baseline method is very small ((cid:54) 0.7).

Domain Adaptation Equilibrium

To get a better understanding of Deep CORAL, we generate three plots for domain
shift A→W. In Figure 6(a) we show the training (source) and testing (target) accu-
racies for training with v.s. without CORAL loss. We can clearly see that adding
the CORAL loss helps achieve much better performance on the target domain while
maintaining strong classiﬁcation accuracy on the source domain.

In Figure 6(b) we visualize both the classiﬁcation loss and the CORAL loss for
training w/ CORAL loss. As the last fully connected layer is randomly initialized
with N (0, 0.005), in the beginning the CORAL loss is very small while the clas-
siﬁcation loss is very large. After training for a few hundred iterations, these two

16

Baochen Sun, Jiashi Feng, and Kate Saenko

losses are about the same and reach an equilibrium. In Figure 6(c) we show the
CORAL distance between the domains for training w/o CORAL loss (setting the
weight to 0). We can see that the distance is getting much larger ((cid:62) 100 times larger
compared to training w/ CORAL loss). Comparing Figure 6(b) and Figure 6(c), we
can see that even though the CORAL loss is not always decreasing during training,
if we set its weight to 0, the distance between source and target domains becomes
much larger. This is reasonable as ﬁne-tuning without domain adaptation is likely
to overﬁt the features to the source domain. Our CORAL loss constrains the dis-
tance between source and target domain during the ﬁne-tuning process and helps to
maintain an equilibrium where the ﬁnal features work well on the target domain.

5.2 Object Detection

Following protocol of [10], we conduct object detection experiment on the Ofﬁce
dataset [23] with HOG features. We use the same setting as [10], performing detec-
tion on the Webcam domain as the target (test) domain, and evaluating on the same
783 image test set of 20 categories (out of 31). As source (training) domains, we
use: the two remaining real-image domains in Ofﬁce, Amazon and DSLR, and two
domains that contain virtual images only, Virtual and Virtual-Gray, generated from
3d CAD models. The inclusion of the two virtual domains is to reduce human effort
in annotation and facilitate future research [24]. Examples of Virtual and Virtual-
Gray are shown in Figure 8. Please refer to [26] for detailed explanation of the data
generation process. We also compare to [10] who use corresponding ImageNet [3]
synsets as the source. Thus, there are four potential source domains (two synthetic
and three real) and one (real) target domain. The number of positive training images
per category in each domain is shown in Table 3. Figure 7 shows an overview of our
evaluation.

Fig. 7 Overview of our evaluation on object detection. With CORAL-Linear Discriminant Analy-
sis (LDA), we show that a strong object detector can be trained from virtual data only.

Correlation Alignment for Unsupervised Domain Adaptation

17

(a)

(b)

Fig. 8 Two sets of virtual images used in this section: (a) Virtual: background and texturemap from
a random real ImageNet image; (b) Virtual-Gray: uniform gray texturemap and white background.
Please refer to [26] for detailed explanation of the data generation process.

Domain

# Training sample

Amazon
DSLR
Virtual(-Gray)
ImageNet

20
8
30
150-2000

Table 3 # training examples for each source domain.

Effect of Mismatched Image Statistics

First, we explore the effect of mismatched precomputed image statistics on detection
performance. For each source domain, we train CORAL-LDA detectors using the
positive mean from the source, and pair it with the covariance and negative mean
of other domains. The virtual and the Ofﬁce domains are used as sources, and the
test domain is always Webcam. The statistics for each of the four domains were
calculated using all of the training data, following the same approach as [14]. The
pre-computed statistics of 10,000 real images from PASCAL, as proposed in [14,
10], are also evaluated.

Detection performance, measured in Mean Average Precision (MAP), is shown
in Table 4. We also calculate the normalized Euclidean distance between pairs of
domains as ((cid:107)C1 − C2(cid:107))/((cid:107)C1(cid:107) + (cid:107)C2(cid:107)) + ((cid:107)µ 1
0 (cid:107)), and show
the average distance between source and target in parentheses in Table 4.

0 (cid:107))/((cid:107)µ 1

0 (cid:107) + (cid:107)µ 2

0 − µ 2

From these results we can see a trend that larger domain difference leads to
poorer performance. Note that larger difference to the target domain also leads to
lower performance, conﬁrming our hypothesis that both source and target statistics
matter. Some of the variation could also stem from our assumption about the differ-
ence of means being the same not quite holding true. Finally, the PASCAL statistics
from [14] perform the worst. Thus, in practice, statistics from either source domain
or target domain or domains close to them could be used. However, unrelated statis-
tics will not work even though they might be estimated from a very large amount of
data as [14].

18

Baochen Sun, Jiashi Feng, and Kate Saenko

30.8 (0.1)
Virtual
Virtual-Gray 32.3 (0.6)
39.9 (0.4)
Amazon
68.2 (0.2)
DSLR

Virtual Virtual-Gray Amazon DSLR PASCAL
24.1 (0.6) 28.3 (0.2) 10.7 (0.5)
27.3 (0.8) 32.7 (0.6) 17.9 (0.7)
39.2 (0.4) 37.9 (0.4) 18.6 (0.6)
68.1 (0.6) 66.5 (0.1) 37.7 (0.5)

16.5 (1.0)
32.3 (0.5)
30.0 (1.0)
62.1 (1.0)

Table 4 MAP of CORAL-LDA trained on positive examples from each row’s source domain and
background statistics from each column’s domain. The average distance between each set of back-
ground statistics and the true source and target statistics is shown in parentheses.

Unsupervised and Semi-supervised Adaptation

Next, we report the results of our unsupervised and semi-supervised adaptation tech-
nique. We use the same setting as [10], in which three positive and nine negative
labeled images per category were used for semi-supervised adaptation. Target co-
variance in Equation 6 is estimated from 305 unlabeled training examples. We also
followed the same approach to learn a linear combination between the unsupervised
and supervised model via cross-validation. The results are presented in Table 5.
Please note that our target-only MAP is 52.9 compared to 36.6 in [10]. This also
conﬁrms our conclusion that the statistics should come from a related domain. It is
clear that both of our unsupervised and semi-supervised adaptation techniques out-
perform the method in [10]. Furthermore, Virtual-Gray data outperforms Virtual,
and DSLR does best, as it is very close to the target domain (the main difference
between DLSR and Webcam domains is in the camera used to capture images).

Finally, we compare our method trained on Virtual-Gray to the results of adapt-
ing from ImageNet reported by [10], in Figure 9. While their unsupervised models
are learned from 150-2000 real ImageNet images per category and the background
statistics are estimated from 10,000 PASCAL images, we only have 30 virtual im-
ages per category and the background statistics is learned from about 1,000 images.
What’s more, all the virtual images used are with uniform gray texturemap and white
background. This clearly demonstrates the importance of domain-speciﬁc decorre-
lation, and shows that the there is no need to collect a large amount of real images
to train a good classiﬁer.

Fig. 9 Comparison of unsupervised and semi-supervised adaptation of virtual detectors using our
method with the results of training on ImageNet and supervised adaptation from ImageNet reported
in [10]. Our semi-supervised adapted detectors achieve comparable performance despite not using
any real source training data, and using only 3 positive images for adaptation, and even outperform
ImageNet signiﬁcantly for several categories (e.g., ruler).

Correlation Alignment for Unsupervised Domain Adaptation

19

Source
Virtual
Virtual-Gray
Amazon
DSLR

Source-only [14] Unsup-Ours SemiSup [10] SemiSup-Ours

10.7
17.9
18.6
37.7

27.9
33.0
38.9
67.1

30.7
35.0
35.8
42.9

45.2
54.7
53.0
71.4

Table 5 Top: Comparison of the source-only [14] and semi-supervised adapted model of [10]
with our unsupervised-adapted and semi-supervised adapted models. Target domain is Webcam.
Mean AP across categories is reported on the Webcam test data, using different source domains for
training. Bottom: Sample detections of the DSLR-UnsupAdapt-Ours detectors.

6 Conclusion

In this chapter, we described a simple, effective, and efﬁcient method for unsu-
pervised domain adaptation called CORrelation ALignment (CORAL). CORAL
minimizes domain shift by aligning the second-order statistics of source and target
distributions, without requiring any target labels. We also developed novel domain
adaptation algorithms by applying the idea of CORAL to three different scenarios.
In the ﬁrst scenario, we applied a linear transformation that minimizes the CORAL
objective to source features prior to training the classiﬁer. In the case of linear classi-
ﬁers, we equivalently applied the linear CORAL transform to the classiﬁer weights,
signiﬁcantly improving efﬁciency and classiﬁcation accuracy over standard LDA
on several domain adaptation benchmarks. We further extended CORAL to learn
a nonlinear transformation that aligns correlations of layer activations in deep neu-
ral networks. The resulting Deep CORAL approach works seamlessly with deep
networks and can be integrated into any arbitrary network architecture to enable
end-to-end unsupervised adaptation. One limitation of CORAL is that it captures
second-order statistics only and may not preserve higher-order structure in the data.
However, as demonstrated in this chapter, it works fairly well in practice, and can
also potentially be combined with other domain-alignment loss functions.

References

1. Jian-Feng Cai, Emmanuel J. Cand`es, and Zuowei Shen. A singular value thresholding algo-

rithm for matrix completion. SIAM J. on Optimization, 20(4):1956–1982, March 2010.

2. H. Daume III. Frustratingly easy domain adaptation. In ACL, 2007.
3. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale

hierarchical image database. In CVPR, 2009.

20

Baochen Sun, Jiashi Feng, and Kate Saenko

4. Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and
Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition.
In ICML, 2014.

5. L. Duan, I. W. Tsang, D. Xu, and T. Chua. Domain adaptation from multiple sources via

6. Lixin Duan, Ivor W. Tsang, and Dong Xu. Domain transfer multiple kernel learning. TPAMI,

auxiliary classiﬁers. In ICML, 2009.

34(3):465–479, March 2012.

7. Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object de-
tection with discriminatively trained part-based models. TPAMI, 32(9):1627–1645, 2010.
8. Basura Fernando, Amaury Habrard, Marc Sebban, and Tinne Tuytelaars. Unsupervised visual

domain adaptation using subspace alignment. In ICCV, 2013.

9. Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation.

In ICML, 2015.

10. Daniel Goehring, Judy Hoffman, Erik Rodner, Kate Saenko, and Trevor Darrell. Interactive

adaptation of real-time object detectors. In ICRA, 2014.

11. B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic ﬂow kernel for unsupervised domain

12. R. Gopalan, R. Li, and R. Chellappa. Domain adaptation for object recognition: An unsuper-

adaptation. In CVPR, 2012.

vised approach. In ICCV, 2011.

13. Maayan Harel and Shie Mannor. Learning from multiple outlooks. In ICML, 2011.
14. Bharath Hariharan, Jitendra Malik, and Deva Ramanan. Discriminative decorrelation for clus-

tering and classiﬁcation. In ECCV. 2012.

15. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training

by reducing internal covariate shift. In ICML, 2015.

16. Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick,
Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature
embedding. arXiv preprint, 2014.

17. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep

convolutional neural networks. In NIPS, 2012.

18. B. Kulis, K. Saenko, and T. Darrell. What you saw is not what you get: Domain adaptation

using asymmetric kernel transforms. In CVPR, 2011.

19. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable

features with deep adaptation networks. In ICML, 2015.

20. Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and P.S. Yu. Transfer joint

matching for unsupervised domain adaptation. In CVPR, 2014.

21. Tomasz Malisiewicz, Abhinav Gupta, and Alexei A Efros. Ensemble of exemplar-svms for

22. Sinno Jialin Pan, Ivor W. Tsang, James T. Kwok, and Qiang Yang. Domain adaptation via

object detection and beyond. In ICCV, 2011.

transfer component analysis. In IJCAI, 2009.

23. Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models

24. Baochen Sun. Correlation Alignment for Domain Adaptation. PhD thesis, University of

25. Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation.

26. Baochen Sun, Xingchao Peng, and Kate Saenko. Generating large scale image datasets from

3d cad models. In CVPR’15 Workshop on The Future of Datasets in Vision, 2015.

27. Baochen Sun and Kate Saenko. From virtual to reality: Fast adaptation of virtual object de-

tectors to real domains. In BMVC, 2014.

28. Baochen Sun and Kate Saenko. Subspace distribution alignment for unsupervised domain

29. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation.

adaptation. In BMVC, 2015.

In ECCV 2016 Workshops, 2016.

30. Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain

confusion: Maximizing for domain invariance. CoRR, abs/1412.3474, 2014.

to new domains. In ECCV, 2010.

Massachusetts Lowell, 2016.

In AAAI, 2016.

6
1
0
2
 
c
e
D
 
6
 
 
]

V
C
.
s
c
[
 
 
1
v
9
3
9
1
0
.
2
1
6
1
:
v
i
X
r
a

Correlation Alignment for Unsupervised
Domain Adaptation

Baochen Sun, Jiashi Feng, and Kate Saenko

Abstract In this chapter, we present CORrelation ALignment (CORAL), a sim-
ple yet effective method for unsupervised domain adaptation. CORAL minimizes
domain shift by aligning the second-order statistics of source and target distribu-
tions, without requiring any target labels. In contrast to subspace manifold meth-
ods, it aligns the original feature distributions of the source and target domains,
rather than the bases of lower-dimensional subspaces. It is also much simpler
than other distribution matching methods. CORAL performs remarkably well in
extensive evaluations on standard benchmark datasets. We ﬁrst describe a solu-
tion that applies a linear transformation to source features to align them with tar-
get features before classiﬁer training. For linear classiﬁers, we propose to equiva-
lently apply CORAL to the classiﬁer weights, leading to added efﬁciency when the
number of classiﬁers is small but the number and dimensionality of target exam-
ples are very high. The resulting CORAL Linear Discriminant Analysis (CORAL-
LDA) outperforms LDA by a large margin on standard domain adaptation bench-
marks. Finally, we extend CORAL to learn a nonlinear transformation that aligns
correlations of layer activations in deep neural networks (DNNs). The resulting
Deep CORAL approach works seamlessly with DNNs and achieves state-of-the-
art performance on standard benchmark datasets. Our code is available at: https:
//github.com/VisionLearningGroup/CORAL

Baochen Sun
Microsoft, e-mail: baochens@gmail.com

Jiashi Feng
National University of Singapore, e-mail: elefjia@nus.edu.sg

Kate Saenko
Boston University, e-mail: saenko@bu.edu

1

2

1 Introduction

Baochen Sun, Jiashi Feng, and Kate Saenko

Machine learning is very different from human learning. Humans are able to learn
from very few labeled examples and apply the learned knowledge to new examples
in novel conditions. In contrast, traditional machine learning algorithms assume that
the training and test data are independent and identically distributed (i.i.d.) and su-
pervised machine learning methods only perform well when the given extensive
labeled data are from the same distribution as the test distribution. However, this
assumption rarely holds in practice, as the data are likely to change over time and
space. To compensate for the degradation in performance due to domain shift, do-
main adaptation [23, 8, 11, 20, 28, 25, 30, 2] tries to transfer knowledge from source
(training) domain to target (test) domain. Our approach is in line with most existing
unsupervised domain adaptation approaches in that we ﬁrst transform the source
data to be as close to the target data as possible. Then a classiﬁer trained on the
transformed source domain is applied to the target data.

In this chapter, we mainly focus on the unsupervised scenario as we believe that
leveraging the unlabeled target data is key to the success of domain adaptation. In
real world applications, unlabeled target data are often much more abundant and
easier to obtain. On the other side, labeled examples are very limited and require
human annotation. So the question of how to utilize the unlabeled target data is
more important for practical visual domain adaptation. For example, it would be
more convenient and applicable to have a pedestrian detector automatically adapt to
the changing visual appearance of pedestrians rather than having a human annotator
label every frame that has a different visual appearance.

Our goal is to propose a simple yet effective approach for domain adaptation that
researchers with little knowledge of domain adaptation or machine learning could
easily integrate into their application. In this chapter, we describe a “frustratingly
easy” (to borrow a phrase from [2]) unsupervised domain adaptation method called
CORrelation ALignment or CORAL [25] in short. CORAL aligns the input feature
distributions of the source and target domains by minimizing the difference between
their second-order statistics. The intuition is that we want to capture the structure
of the domain using feature correlations. As an example, imagine a target domain
of Western movies where most people wear hats; then the “head” feature may be
positively correlated with the “hat” feature. Our goal is to transfer such correlations
to the source domain by transforming the source features.

In Section 2, we describe a linear solution to CORAL, where the distributions
are aligned by re-coloring whitened source features with the covariance of the tar-
get data distribution. This solution is simple yet efﬁcient, as the only computations
it needs are (1) computing covariance statistics in each domain and (2) applying
the whitening and re-coloring linear transformation to the source features. Then, su-
pervised learning proceeds as usual–training a classiﬁer on the transformed source
features.

For linear classiﬁers, we can equivalently apply the CORAL transformation to
the classiﬁer weights, leading to better efﬁciency when the number of classiﬁers is
small but the number and dimensionality of the target examples are very high. We

Correlation Alignment for Unsupervised Domain Adaptation

3

present the resulting CORAL–Linear Discriminant Analysis (CORAL-LDA) [27]
in Section 3, and show that it outperforms standard Linear Discriminant Analysis
(LDA) by a large margin on cross domain applications. We also extend CORAL to
work seamlessly with deep neural networks by designing a layer that consists of a
differentiable CORAL loss [29], detailed in Section 4. On the contrary to the linear
CORAL, Deep CORAL learns a nonlinear transformation and also provides end-
to-end adaptation. Section 5 describes extensive quantitative experiments on several
benchmarks, and Section 6 concludes the chapter.

2 Linear Correlation Alignment

In this section, we present CORrelation ALignment (CORAL) for unsupervised do-
main adaptation and derive a linear solution. We ﬁrst describe the formulation and
derivation, followed by the main linear CORAL algorithm and its relationship to ex-
isting approaches. In this section and Section 3, we constrain the transformation to
be linear. Section 4 extends CORAL to learn a nonlinear transformation that aligns
correlations of layer activations in deep neural networks (Deep CORAL).

2.1 Formulation and Derivation

We describe our method by taking a multi-class classiﬁcation problem as the run-
ning example. Suppose we are given source-domain training examples DS = {xi},
x ∈ Rd with labels LS = {yi}, y ∈ {1, ..., L}, and target data DT = {ui}, u ∈ Rd. Here
both x and u are the d-dimensional feature representations φ (I) of input I. Suppose
µs, µt and CS,CT are the feature vector means and covariance matrices. Assuming
that all features are normalized to have zero mean and unit variance, µt = µs = 0
after the normalization step, while CS (cid:54)= CT .

To minimize the distance between the second-order statistics (covariance) of the
source and target features, we apply a linear transformation A to the original source
features and use the Frobenius norm as the matrix distance metric:

(cid:107)C ˆS −CT (cid:107)2

F

min
A

= min
A

2
(cid:107)A(cid:62)CSA −CT (cid:107)
F

(1)

where C ˆS is covariance of the transformed source features DsA and (cid:107) · (cid:107)2
the squared matrix Frobenius norm.

F denotes

If rank(CS) ≥ rank(CT ), then an analytical solution can be obtained by choosing
A such that C ˆS = CT . However, the data typically lie on a lower dimensional man-
ifold [13, 11, 8], and so the covariance matrices are likely to be low rank [14]. We
derive a solution for this general case, using the following lemma.

4

Baochen Sun, Jiashi Feng, and Kate Saenko

Lemma 1. Let Y be a real matrix of rank rY and X a real matrix of rank at most
r, where r (cid:54) rY ; let Y = UY ΣYVY be the SVD of Y , and ΣY [1:r], UY [1:r], VY [1:r] be the
largest r singular values and the corresponding left and right singular vectors of Y
(cid:62) is the optimal solution to the problem
respectively. Then, X ∗ = UY [1:r]ΣY [1:r]VY [1:r]
of min
X

(cid:107)X −Y (cid:107)2

F . [1]

Theorem 1. Let Σ + be the Moore-Penrose pseudoinverse of Σ , rCS and rCT denote
the rank of CS and CT respectively. Then, A∗ = USΣ +
(cid:62) is
S
the optimal solution to the problem in Equation (12) with r = min(rCS , rCT ).

(cid:62)UT [1:r]ΣT [1:r]

1
2 UT [1:r]

1
2 US

Proof. Since A is a linear transformation, A(cid:62)CSA does not increase the rank of CS.
(cid:54) rCS . Since CS and CT are symmetric matrices, conducting SVD on CS
Thus, rC ˆS
and CT gives CS = USΣSUS
T respectively. We ﬁrst ﬁnd the op-
timal value of C ˆS through considering the following two cases:
Case 1. rCS > rCT . The optimal solution is C ˆS = CT . Thus, C ˆS = UT ΣTUT
UT [1:r]ΣT [1:r]UT [1:r]

(cid:62) is the optimal solution to Equation (12) where r = rCT .

(cid:62) and CT = UT ΣTU (cid:62)

(cid:62) =

Case 2. rCS
optimal solution to Equation (12) where r = rCS .

(cid:54) rCT . Then, according to Lemma 1, C ˆS = UT [1:r]ΣT [1:r]UT [1:r]

(cid:62) is the

Combining the results in the above two cases yields that C ˆS = UT [1:r]ΣT [1:r]UT [1:r]
is the optimal solution to Equation (12) with r = min(rCS , rCT ). We then proceed to
solve for A based on the above result. Letting C ˆS = A(cid:62)CSA, we can write

(cid:62)

A(cid:62)CSA = UT [1:r]ΣT [1:r]UT [1:r]

(cid:62).

Since CS = USΣSUS

(cid:62), we have

A(cid:62)USΣSUS

(cid:62)A = UT [1:r]ΣT [1:r]UT [1:r]

(cid:62).

This gives:

(cid:62)
(cid:62)A)

(US

ΣS(US

(cid:62)A) = UT [1:r]ΣT [1:r]UT [1:r]

(cid:62).

Let E = Σ +
(cid:62)UT [1:r]ΣT [1:r]
S
tion can be re-written as E(cid:62)ΣSE. This gives

1
2 UT [1:r]

1
2 US

(cid:62), then the right hand side of the above equa-

(cid:62)
(cid:62)A)

(US

ΣS(US

(cid:62)A) = E(cid:62)ΣSE

By setting US

(cid:62)A to E, we obtain the optimal solution of A as

A∗ = USE

= (USΣ +
S

1
2 US

(cid:62))(UT [1:r]ΣT [1:r]

1
2 UT [1:r]

(cid:62)).

(2)

Correlation Alignment for Unsupervised Domain Adaptation

5

Fig. 1 (a-c) Illustration of CORrelation ALignment (CORAL) for Domain Adaptation: (a) The
original source and target domains have different distribution covariances, despite the features be-
ing normalized to zero mean and unit standard deviation. This presents a problem for transferring
classiﬁers trained on source to target. (b) The same two domains after source decorrelation, i.e.
removing the feature correlations of the source domain. (c) Target re-correlation, adding the corre-
lation of the target domain to the source features. After this step, the source and target distributions
are well aligned and the classiﬁer trained on the adjusted source domain is expected to work well
in the target domain. (d) One might instead attempt to align the distributions by whitening both
source and target. However, this will fail since the source and target data are likely to lie on different
subspaces due to domain shift. (Best viewed in color)

2.2 Algorithm

Figures 1(a-c) illustrate the linear CORAL approach. Figure 1(a) shows example
original source and target data distributions. We can think of the transformation A
1
intuitively as follows: the ﬁrst part USΣ +
(cid:62) whitens the source data, while the
2 US
S
(cid:62) re-colors it with the target covariance. This is
second part UT [1:r]ΣT [1:r]
illustrated in Figure 1(b) and Figure 1(c) respectively.

1
2 UT [1:r]

In practice, for the sake of efﬁciency and stability, we can avoid the expensive
SVD steps and perform traditional data whitening and coloring. Traditional whiten-
ing adds a small regularization parameter λ to the diagonal elements of the covari-
ance matrix to explicitly make it full rank and then multiplies the original features
by its inverse square root (or square root for coloring.) This is advantageous be-
cause: (1) it is faster1 and more stable, as SVD on the original covariance matrices

1 the entire CORAL transformation takes less than one minute on a regular laptop for dimensions
as large as DS ∈ R795×4096 and DT ∈ R2817×4096

6

Baochen Sun, Jiashi Feng, and Kate Saenko

might not be stable and might be slow to converge; (2) as illustrated in Figure 2,
the performance is similar to the analytical solution in Equation (2) and very stable
with respect to λ . In the experiments provided at the end of this chapter we set λ to
1. The ﬁnal algorithm can be written in four lines of MATLAB code as illustrated
in Algorithm 1.

Fig. 2 Sensitivity of CORAL to the covariance regularization parameter λ with λ ∈ {0, 0.001,
0.01, 0.1, 1}. The plots show classiﬁcation accuracy on target data for two domain shifts (blue and
red). When λ = 0, there is no regularization and we use the analytical solution in Equation (2).
Please refer to Section 5.1 for details of the experiment.

Algorithm 1 CORAL for Unsupervised Domain Adaptation

Input: Source Data DS, Target Data DT
Output: Adjusted Source Data D∗
s
CS = cov(DS) + eye(size(DS, 2))
CT = cov(DT ) + eye(size(DT , 2))

−1
2

DS = DS ∗C
S
1
D∗
2
S = DS ∗C
T

% whitening source

% re-coloring with target covariance

One might instead attempt to align the distributions by whitening both source
and target. As shown in Figure 1(d), this will fail as the source and target data are
likely to lie on different subspaces due to domain shift. An alternative approach
would be whitening the target and then re-coloring it with the source covariance.
However, as demonstrated in [13, 8] and our experiments, transforming data from
source to target space gives better performance. This might be due to the fact that by
transforming the source to target space the classiﬁer is trained using both the label
information from the source and the unlabelled structure from the target.

After CORAL transforms the source features to the target space, a classiﬁer fw
parametrized by w can be trained on the adjusted source features and directly ap-
plied to target features. For a linear classiﬁer fw(I) = wT φ (I), we can apply an
equivalent transformation to the parameter vector w (e.g., fw(I) = (wT A)φ (I)) in-
stead of the features (e.g., fw(I) = wT (Aφ (I))). This results in added efﬁciency
when the number of classiﬁers is small but the number and dimensionality of target

Correlation Alignment for Unsupervised Domain Adaptation

7

examples is very high. For linear SVM, this extension is straightforward. In Sec-
tion 3, we apply the idea of CORAL to another commonly used linear classiﬁer–
Linear Discriminant Analysis (LDA). LDA is special in the sense that its weights
also use the covariance of the data. It is also extremely efﬁcient for training a large
number of classiﬁers [14].

2.3 Relationship to Existing Methods

It has long been known that input feature normalization improves many machine
learning methods, e.g., [15]. However, CORAL does not simply perform feature nor-
malization, but rather aligns two different distributions. Batch Normalization [15]
tries to compensate for internal covariate shift by normalizing each mini-batch to be
zero-mean and unit-variance. However, as illustrated in Figure 1(a), such normal-
ization might not be enough. Even if used with full whitening, Batch Normalization
may not compensate for external covariate shift: the layer activations will be decor-
related for a source point but not for a target point. What’s more, as mentioned in
Section 2.2, whitening both domains is not a successful strategy.

Recent state-of-the-art unsupervised approaches project the source and target dis-
tributions into a lower-dimensional manifold and ﬁnd a transformation that brings
the subspaces closer together [12, 11, 8, 13]. CORAL avoids subspace projection,
which can be costly and requires selecting the hyper-parameter that controls the di-
mensionality of the subspace, k. We note that subspace-mapping approaches [13, 8]
only align the top k eigenvectors of the source and target covariance matrices. On the
contrary, CORAL aligns the covariance matrices, which can only be re-constructed
using all eigenvectors and eigenvalues. Even though the eigenvectors can be aligned
well, the distributions can still differ a lot due to the difference of eigenvalues be-
tween the corresponding eigenvectors of the source and target data. CORAL is a
more general and much simpler method than the above two as it takes into ac-
count both eigenvectors and eigenvalues of the covariance matrix without the burden
of subspace dimensionality selection.

Maximum Mean Discrepancy (MMD) based methods (e.g., TCA [22], DAN [19])
for domain adaptation can be interpreted as “moment matching” and can ex-
press arbitrary statistics of the data. Minimizing MMD with a polynomial kernel
(k(x, y) = (1 + x(cid:48)y)q with q = 2) is similar to the CORAL objective, however, no
previous work has used this kernel for domain adaptation nor proposed a closed
form solution to the best of our knowledge. The other difference is that MMD based
approaches usually apply the same transformation to both the source and target do-
main. As demonstrated in [18, 13, 8], asymmetric transformations are more ﬂexible
and often yield better performance for domain adaptation tasks. Intuitively, symmet-
ric transformations ﬁnd a space that “ignores” the differences between the source
and target domain while asymmetric transformations try to “bridge” the two do-
mains.

8

Baochen Sun, Jiashi Feng, and Kate Saenko

3 CORAL Linear Discriminant Analysis

In this section, we introduce how CORAL can be applied for aligning multiple linear
classiﬁers. In particular, we take LDA as the example for illustration, considering
LDA is a commonly used and effective linear classiﬁer. Combining CORAL and
LDA gives a new efﬁcient adpative learning approach CORAL-LDA. We use the
task of object detection as a running example to explain CORAL-LDA.

We begin by describing the decorrelation-based approach to detection proposed
in [14]. Given an image I, it follows the sliding-window paradigm, extracting a
d-dimensional feature vector φ (I, b) at each window b across all locations and at
multiple scales. It then scores the windows using a scoring function

fw(I, b) = w(cid:62)φ (I, b).

(3)

In practice, all windows with values of fw above a predetermined threshold are
considered positive detections.

In recent years, use of the linear SVM as the scoring function fw, usually with
Histogram of Gradients (HOG) as the features φ , has emerged as the predominant
object detection paradigm. Yet, as observed by Hariharan et al. [14], training SVMs
can be expensive, especially because it usually involves costly rounds of hard neg-
ative mining. Furthermore, the training must be repeated for each object category,
which makes it scale poorly with the number of categories.

Hariharan et al. [14] proposed a much more efﬁcient alternative, learning fw
with Linear Discriminant Analysis (LDA). LDA is a well-known linear classiﬁer
that models the training set of examples x with labels y ∈ {0, 1} as being generated
by p(x, y) = p(x|y)p(y). p(y) is the prior on class labels and the class-conditional
densities are normal distributions

p(x|y) = N (x; µ y, CS),

where the feature vector covariance CS is assumed to be the same for both positive
and negative (background) classes. In our case, the feature is represented by x =
φ (I, b). The resulting classiﬁer is given by

w = CS

−1(µ1 − µ0)

The innovation in [14] was to re-use CS and µ0, the background mean, for all cate-
gories, reducing the task of learning a new category model to computing the average
positive feature, µ1. This was accomplished by calculating CS and µ0 for the largest
possible window and subsampling to estimate all other smaller window sizes. Also,
CS was shown to have a sparse local structure, with correlation falling off sharply
beyond a few nearby image locations.

Like other classiﬁers, LDA learns to suppress non-discriminative structures and
enhance the contours of the object. However it does so by learning the global co-
variance statistics once for all natural images, and then using the inverse covariance
matrix to remove the non-discriminative correlations, and the negative mean to re-

(4)

(5)

Correlation Alignment for Unsupervised Domain Adaptation

9

Fig. 3 (a) Applying a linear classiﬁer w learned by LDA to source data x is equivalent to (b)
−1/2x. (c) However, target points u
applying classiﬁer ˆw = CS
−1/2u, hurting performance. (d) Our method uses target-speciﬁc
may still be correlated after CS
covariance to obtain properly decorrelated ˆu.

−1/2w to decorrelated points CS

move the average feature. LDA was shown in [14] to have competitive performance
to SVM, and can be implemented both as an exemplar-based [21] or as deformable
parts model (DPM) [7].

We observe that estimating global statistics CS and µ0 once and re-using them for
all tasks may work when training and testing in the same domain, but in our case,
the source training data is likely to have different statistics from the target data.
Figure 4 illustrates the effect of centering and decorrelating a positive mean using
global statistics from the wrong domain. The effect is clear: important discriminative
information is removed while irrelevant structures are not.

Based on this observation, we propose an adaptive decorrelation approach to
detection. Assume that we are given labeled training data {x, y} in the source domain
(e.g., virtual images rendered from 3D models), and unlabeled examples u in the
target domain (e.g., real images collected in an ofﬁce environment). Evaluating the
scoring function fw(x) in the source domain is equivalent to ﬁrst de-correlating the
−1/2x, computing their positive and negative class means
training features ˆx = CS
−1/2µ0 and then projecting the decorrelated feature onto
ˆµ1 = CS
the decorrelated difference between means, fw(x) = ˆw(cid:62) ˆx, where ˆw = ( ˆµ1 − ˆµ0).
This is illustrated in Figure 3(a-b).

−1/2µ1 and ˆµ0 = CS

However, as we saw in Figure 4, the assumption that the input is properly decor-
related does not hold if the input comes from a target domain with a different co-
−1/2u does not
variance structure. Figure 3(c) illustrates this case, showing that CS
have isotropic covariance. Therefore, w cannot be used directly.

Fig. 4 Visualization of classiﬁer weights of bicycle (decorrelated with mismatched-domain co-
variance (left) v.s. with same-domain covariance (right)).

10

Baochen Sun, Jiashi Feng, and Kate Saenko

We may be able to compute the covariance of the target domain on the unlabeled
target points u, but not the positive class mean. Therefore, we would like to re-use
the decorrelated mean difference ˆw, but adapt to the covariance of the target do-
main. In the rest of the chapter, we make the assumption that the difference between
positive and negative means is the same in the source and target. This may or may
not hold in practice, and we discuss this further in Section 5.

Let the estimated target covariance be CT. We ﬁrst decorrelate the target input
feature with its inverse square root, and then apply ˆw directly, as shown in Fig-
ure 3(d). The resulting scoring function is:

f ˆw(u) = ˆw(cid:62) ˆu

= (CS

= ((CT

(cid:62)
−1/2(µ1 − µ0))
−1/2)(cid:62)CS

−1/2u)
(CT
(cid:62)
−1/2(µ1 − µ0))

u

(6)

This corresponds to a transformation (CT

−1/2) instead of the original
−1/2)(cid:62)(CS
−1 being applied to the difference between means to compute w. Note
−1/2) equals to
−1 since both CT and CS are symmetric. In this case, Equation 6 ends up the

whitening CS
that if source and target domains are the same, then (CT
CS
same as Equation 5.

−1/2)(cid:62)(CS

In practice, either the source or the target component of the above transformation
may also work, or even statistics from similar domains. However, as we will see in
Section 5.2, dissimilar domain statistics can signiﬁcantly hurt performance. Further-
more, if either source or target has only images of the positive category available,
and cannot be used to properly compute background statistics, the other domain can
still be used.

CORAL-LDA works in a purely unsupervised way. Here, we extend it to semi-
supervised adaptation when a few labeled examples are available in the target do-
main. Following [10], a simple adaptation method is used whereby the template
learned on source positives is combined with a template learned on target positives,
using a weighted combination. The key difference with our approach is that the
target template uses target-speciﬁc statistics.

In [10], the author uses the same background statistics as [14] which were es-
timated on 10,000 natural images from the PASCAL VOC 2010 dataset. Based on
our analysis above, even though these background statistics were estimated from a
very large amount of real image data, it will not work for all domains. In section 5.2,
our results conﬁrm this claim.

4 Deep CORAL

In this section, we extend CORAL to work seamlessly with deep neural networks
by designing a differentiable CORAL loss. Deep CORAL enables end-to-end adap-
tation and also learns more a powerful nonlinear transformation. It can be easily

Correlation Alignment for Unsupervised Domain Adaptation

11

Fig. 5 Sample Deep CORAL architecture based on a CNN with a classiﬁer layer. For generaliza-
tion and simplicity, here we apply the CORAL loss to the f c8 layer of AlexNet [17]. Integrating it
into other layers or network architectures is also possible.

integrated into different layers or network architectures. Figure 5 shows a sample
Deep CORAL architecture using our proposed correlation alignment layer for deep
domain adaptation. We refer to Deep CORAL as any deep network incorporating
the CORAL loss for domain adaptation.

We ﬁrst describe the CORAL loss between two domains for a single feature layer.
Suppose the numbers of source and target data are nS and nT respectively. Here both
x and u are the d-dimensional deep layer activations φ (I) of input I that we are
trying to learn. Suppose Di j
T ) indicates the j-th dimension of the i-th source
(target) data example and CS (CT ) denote the feature covariance matrices.

S (Di j

We deﬁne the CORAL loss as the distance between the second-order statistics

(covariances) of the source and target features:

LCORAL =

1
4d2 (cid:107)CS −CT (cid:107)2

F

(7)

(8)

(9)

where (cid:107) · (cid:107)2
of the source and target data are given by:

F denotes the squared matrix Frobenius norm. The covariance matrices

CS =

1
nS − 1

(D(cid:62)

S DS −

(1(cid:62)DS)(cid:62)(1(cid:62)DS))

1
nT − 1
where 1 is a column vector with all elements equal to 1.

T DT −

CT =

1
nT

(D(cid:62)

(1(cid:62)DT )(cid:62)(1(cid:62)DT ))

The gradient with respect to the input features can be calculated using the chain

rule:

∂ LCORAL
∂ Di j
S

=

1
d2(nS − 1)

((D(cid:62)

S −

(1(cid:62)DS)(cid:62)1(cid:62))(cid:62)(CS −CT ))i j

(10)

1
nS

1
nS

12

Baochen Sun, Jiashi Feng, and Kate Saenko

∂ LCORAL
∂ Di j
T

= −

1
d2(nT − 1)

1
nT

((D(cid:62)

T −

(1(cid:62)DT )(cid:62)1(cid:62))(cid:62)(CS −CT ))i j

(11)

In our experiments, we use batch covariances and the network parameters are shared
between the two networks, but other settings are also possible.

To see how this loss can be used to adapt an existing neural network, let us
return to the multi-class classiﬁcation problem. Suppose we start with a network
with a ﬁnal classiﬁcation layer, such as the ConvNet shown in Figure 5. As men-
tioned before, the ﬁnal deep features need to be both discriminative enough to train
a strong classiﬁer and invariant to the difference between source and target domains.
Minimizing the classiﬁcation loss itself is likely to lead to overﬁtting to the source
domain, causing reduced performance on the target domain. On the other hand, min-
imizing the CORAL loss alone might lead to degenerated features. For example, the
network could project all of the source and target data to a single point, making
the CORAL loss trivially zero. However, no strong classiﬁer can be constructed on
these features. Joint training with both the classiﬁcation loss and CORAL loss is
likely to learn features that work well on the target domain:

L = LCLASS +

λiLCORALi

(12)

t
∑
i=1

where t denotes the number of CORAL loss layers in a deep network and λi is a
weight that trades off the adaptation with classiﬁcation accuracy on the source do-
main. As we show below, these two losses play counterparts and reach an equilib-
rium at the end of training, where the ﬁnal features are discriminative and generalize
well to the target domain.

5 Experiments

We evaluate CORAL and Deep CORAL on object recognition [23] using standard
benchmarks and protocols. In all experiments we assume the target domain is unla-
beled. For CORAL, we follow the standard procedure [8, 4] and use a linear SVM as
the base classiﬁer. The model selection approach of [8] is used to set the C parame-
ter for the SVM by doing cross-validation on the source domain. For CORAL-LDA,
as efﬁciency is the main concern, we evaluate it on the more time constrained task–
object detection. We follow the protocol of [10] and use HOG features. To have a
fair comparison, we use accuracies reported by other authors with exactly the same
setting or conduct experiments using the source code provided by the authors.

Correlation Alignment for Unsupervised Domain Adaptation

13

5.1 Object Recognition

In this set of experiments, domain adaptation is used to improve the accuracy of
an object classiﬁer on novel image domains. Both the standard Ofﬁce [23] and
extended Ofﬁce-Caltech10 [11] datasets are used as benchmarks in this chap-
ter. Ofﬁce-Caltech10 contains 10 object categories from an ofﬁce environment
(e.g., keyboard, laptop, etc.) in 4 image domains: Webcam, DSLR, Amazon, and
Caltech256. The standard Ofﬁce dataset contains 31 (the same 10 categories from
Ofﬁce-Caltech10 plus 21 additional ones) object categories in 3 domains: Webcam,
DSLR, and Amazon.

A→C A→D A→W C→A C→D C→W D→A D→C D→W W→A W→C W→D AVG
37.8
26.4
35.8
NA
41.0
33.4
34.8
SVMA
40.2
33.5
34.9
DAM
43.4
37.9
38.3
GFK
45.7
39.6
40.0
TCA
42.0
45.9
SA
39.9
46.7
CORAL 40.3
38.1

25.7
33.5
31.2
29.1
33.7
31.8
34.6

32.3
36.6
34.7
37.1
40.2
39.3
37.8

39.4
34.5
34.7
36.1
41.4
39.4
40.7

27.1
31.4
31.5
31.4
34.0
35.0
34.2

24.9
32.5
32.5
39.8
40.1
39.6
38.7

33.1
34.1
34.3
37.9
39.1
38.8
38.3

30.0
32.9
33.1
34.9
36.2
38.9
39.2

56.4
74.4
74.7
79.1
80.4
82.3
85.9

78.9
75.0
68.3
74.6
77.5
77.9
84.9

43.7
39.1
39.2
44.8
46.7
46.1
47.2

Table 1 Object recognition accuracies of all 12 domain shifts on the Ofﬁce-Caltech10 dataset [11]
with SURF features, following the protocol of [11, 8, 12, 18, 23].

Object Recognition with Shallow Features

We follow the standard protocol of [11, 8, 12, 18, 23] and conduct experiments
on the Ofﬁce-Caltech10 dataset with shallow features (SURF). The SURF features
were encoded with 800-bin bag-of-words histograms and normalized to have zero
mean and unit standard deviation in each dimension. Since there are four domains,
there are 12 experiment settings, namely, A→C (train classiﬁer on (A)mazon, test
on (C)altech), A→D (train on (A)mazon, test on (D)SLR), A→W, and so on. We
follow the standard protocol and conduct experiments in 20 randomized trials for
each domain shift and average the accuracy over the trials. In each trial, we use the
standard setting [11, 8, 12, 18, 23] and randomly sample the same number (20 for
Amazon, Caltech, and Webcam; 8 for DSLR as there are only 8 images per category
in the DSLR domain) of labelled images in the source domain as training set, and
use all the unlabelled data in the target domain as the test set.

In Table 1, we compare our method to ﬁve recent published methods: SVMA [6],
DAM [5], GFK [11], SA [8], and TCA [22] as well as the no adaptation base-
line (NA). GFK, SA, and TCA are manifold based methods that project the source
and target distributions into a lower-dimensional manifold. GFK integrates over an
inﬁnite number of subspaces along the subspace manifold using the kernel trick.
SA aligns the source and target subspaces by computing a linear map that mini-
mizes the Frobenius norm of their difference. TCA performs domain adaptation via
a new parametric kernel using feature extraction methods by projecting data onto

14

Baochen Sun, Jiashi Feng, and Kate Saenko

the learned transfer components. DAM introduces smoothness assumption to en-
force the target classiﬁer share similar decision values with the source classiﬁers.
Even though these methods are far more complicated than ours and require tuning
of hyperparameters (e.g., subspace dimensionality), our method achieves the best
average performance across all the 12 domain shifts. Our method also improves on
the no adaptation baseline (NA), in some cases increasing accuracy signiﬁcantly
(from 56% to 86% for D→W).

Object Recognition with Deep Features

For visual domain adaptation with deep features, we follow the standard protocol
of [11, 19, 4, 30, 9] and use all the labeled source data and all the target data without
labels on the standard Ofﬁce dataset [23]. Since there are 3 domains, we conduct
experiments on all 6 shifts (5 runs per shift), taking one domain as the source and
another as the target.

In this experiment, we apply the CORAL loss to the last classiﬁcation layer as it
is the most general case–most deep classiﬁer architectures (e.g., convolutional neu-
ral networks, recurrent neural networks) contain a fully connected layer for classi-
ﬁcation. Applying the CORAL loss to other layers or other network architectures is
also possible. The dimension of the last fully connected layer ( f c8) was set to the
number of categories (31) and initialized with N (0, 0.005). The learning rate of
f c8 was set to 10 times the other layers as it was training from scratch. We initial-
ized the other layers with the parameters pre-trained on ImageNet [3] and kept the
original layer-wise parameter settings. In the training phase, we set the batch size to
128, base learning rate to 10−3, weight decay to 5 × 10−4, and momentum to 0.9.
The weight of the CORAL loss (λ ) is set in such way that at the end of training
the classiﬁcation loss and CORAL loss are roughly the same. It seems be a reason-
able choice as we want to have a feature representation that is both discriminative
and also minimizes the distance between the source and target domains. We used
Caffe [16] and BVLC Reference CaffeNet for all of our experiments.

We compare to 7 recently published methods: CNN [17] (no adaptation), GFK [11],

SA [8], TCA [22], CORAL [25], DDC [30], DAN [19]. GFK, SA, and TCA are
manifold based methods that project the source and target distributions into a lower-
dimensional manifold and are not end-to-end deep methods. DDC adds a domain
confusion loss to AlexNet [17] and ﬁne-tunes it on both the source and target do-
main. DAN is similar to DDC but utilizes a multi-kernel selection method for better
mean embedding matching and adapts in multiple layers. For direct comparison,
DAN in this paper uses the hidden layer f c8. For GFK, SA, TCA, and CORAL, we
use the f c7 feature ﬁne-tuned on the source domain (FT 7 in [25]) as it achieves bet-
ter performance than generic pre-trained features, and train a linear SVM [8, 25]. To
have a fair comparison, we use accuracies reported by other authors with exactly the
same setting or conduct experiments using the source code provided by the authors.
From Table 2 we can see that Deep CORAL (D-CORAL) achieves better average
performance than CORAL and the other 6 baseline methods. In 3 out of 6 shifts, it

Correlation Alignment for Unsupervised Domain Adaptation

15

A→D A→W D→A D→W W→A W→D AVG
52.4±0.0 54.7±0.0 43.2±0.0 92.1±0.0 41.8±0.0 96.2±0.0 63.4
GFK
50.6±0.0 47.4±0.0 39.5±0.0 89.1±0.0 37.6±0.0 93.8±0.0 59.7
SA
46.8±0.0 45.5±0.0 36.4±0.0 81.1±0.0 39.5±0.0 92.2±0.0 56.9
TCA
65.7±0.0 64.3±0.0 48.5±0.0 96.1±0.0 48.2±0.0 99.8±0.0 70.4
CORAL
63.8±0.5 61.6±0.5 51.1±0.6 95.4±0.3 49.8±0.4 99.0±0.2 70.1
CNN
64.4±0.3 61.8±0.4 52.1±0.8 95.0±0.5 52.2±0.4 98.5±0.4 70.6
DDC
65.8±0.4 63.8±0.4 52.8±0.4 94.6±0.5 51.9±0.5 98.8±0.6 71.3
DAN
D-CORAL 66.8±0.6 66.4±0.4 52.8±0.2 95.7±0.3 51.5±0.3 99.2±0.1 72.1

Table 2 Object recognition accuracies for all 6 domain shifts on the standard Ofﬁce dataset with
deep features, following the standard unsupervised adaptation protocol.

Fig. 6 Detailed analysis of shift A→W for training w/ v.s. w/o CORAL loss. (a): training and test
accuracies for training w/ v.s. w/o CORAL loss. We can see that adding CORAL loss helps achieve
much better performance on the target domain while maintaining strong classiﬁcation accuracy on
the source domain. (b): classiﬁcation loss and CORAL loss for training w/ CORAL loss. As the
last fully connected layer is randomly initialized with N (0, 0.005), CORAL loss is very small
while classiﬁcation loss is very large at the beginning. After training for a few hundred iterations,
these two losses are about the same. (c): CORAL distance for training w/o CORAL loss (setting
the weight to 0). The distance is getting much larger ((cid:62) 100 times larger compared to training w/
CORAL loss).

achieves the highest accuracy. For the other 3 shifts, the margin between D-CORAL
and the best baseline method is very small ((cid:54) 0.7).

Domain Adaptation Equilibrium

To get a better understanding of Deep CORAL, we generate three plots for domain
shift A→W. In Figure 6(a) we show the training (source) and testing (target) accu-
racies for training with v.s. without CORAL loss. We can clearly see that adding
the CORAL loss helps achieve much better performance on the target domain while
maintaining strong classiﬁcation accuracy on the source domain.

In Figure 6(b) we visualize both the classiﬁcation loss and the CORAL loss for
training w/ CORAL loss. As the last fully connected layer is randomly initialized
with N (0, 0.005), in the beginning the CORAL loss is very small while the clas-
siﬁcation loss is very large. After training for a few hundred iterations, these two

16

Baochen Sun, Jiashi Feng, and Kate Saenko

losses are about the same and reach an equilibrium. In Figure 6(c) we show the
CORAL distance between the domains for training w/o CORAL loss (setting the
weight to 0). We can see that the distance is getting much larger ((cid:62) 100 times larger
compared to training w/ CORAL loss). Comparing Figure 6(b) and Figure 6(c), we
can see that even though the CORAL loss is not always decreasing during training,
if we set its weight to 0, the distance between source and target domains becomes
much larger. This is reasonable as ﬁne-tuning without domain adaptation is likely
to overﬁt the features to the source domain. Our CORAL loss constrains the dis-
tance between source and target domain during the ﬁne-tuning process and helps to
maintain an equilibrium where the ﬁnal features work well on the target domain.

5.2 Object Detection

Following protocol of [10], we conduct object detection experiment on the Ofﬁce
dataset [23] with HOG features. We use the same setting as [10], performing detec-
tion on the Webcam domain as the target (test) domain, and evaluating on the same
783 image test set of 20 categories (out of 31). As source (training) domains, we
use: the two remaining real-image domains in Ofﬁce, Amazon and DSLR, and two
domains that contain virtual images only, Virtual and Virtual-Gray, generated from
3d CAD models. The inclusion of the two virtual domains is to reduce human effort
in annotation and facilitate future research [24]. Examples of Virtual and Virtual-
Gray are shown in Figure 8. Please refer to [26] for detailed explanation of the data
generation process. We also compare to [10] who use corresponding ImageNet [3]
synsets as the source. Thus, there are four potential source domains (two synthetic
and three real) and one (real) target domain. The number of positive training images
per category in each domain is shown in Table 3. Figure 7 shows an overview of our
evaluation.

Fig. 7 Overview of our evaluation on object detection. With CORAL-Linear Discriminant Analy-
sis (LDA), we show that a strong object detector can be trained from virtual data only.

Correlation Alignment for Unsupervised Domain Adaptation

17

(a)

(b)

Fig. 8 Two sets of virtual images used in this section: (a) Virtual: background and texturemap from
a random real ImageNet image; (b) Virtual-Gray: uniform gray texturemap and white background.
Please refer to [26] for detailed explanation of the data generation process.

Domain

# Training sample

Amazon
DSLR
Virtual(-Gray)
ImageNet

20
8
30
150-2000

Table 3 # training examples for each source domain.

Effect of Mismatched Image Statistics

First, we explore the effect of mismatched precomputed image statistics on detection
performance. For each source domain, we train CORAL-LDA detectors using the
positive mean from the source, and pair it with the covariance and negative mean
of other domains. The virtual and the Ofﬁce domains are used as sources, and the
test domain is always Webcam. The statistics for each of the four domains were
calculated using all of the training data, following the same approach as [14]. The
pre-computed statistics of 10,000 real images from PASCAL, as proposed in [14,
10], are also evaluated.

Detection performance, measured in Mean Average Precision (MAP), is shown
in Table 4. We also calculate the normalized Euclidean distance between pairs of
domains as ((cid:107)C1 − C2(cid:107))/((cid:107)C1(cid:107) + (cid:107)C2(cid:107)) + ((cid:107)µ 1
0 (cid:107)), and show
the average distance between source and target in parentheses in Table 4.

0 (cid:107))/((cid:107)µ 1

0 (cid:107) + (cid:107)µ 2

0 − µ 2

From these results we can see a trend that larger domain difference leads to
poorer performance. Note that larger difference to the target domain also leads to
lower performance, conﬁrming our hypothesis that both source and target statistics
matter. Some of the variation could also stem from our assumption about the differ-
ence of means being the same not quite holding true. Finally, the PASCAL statistics
from [14] perform the worst. Thus, in practice, statistics from either source domain
or target domain or domains close to them could be used. However, unrelated statis-
tics will not work even though they might be estimated from a very large amount of
data as [14].

18

Baochen Sun, Jiashi Feng, and Kate Saenko

30.8 (0.1)
Virtual
Virtual-Gray 32.3 (0.6)
39.9 (0.4)
Amazon
68.2 (0.2)
DSLR

Virtual Virtual-Gray Amazon DSLR PASCAL
24.1 (0.6) 28.3 (0.2) 10.7 (0.5)
27.3 (0.8) 32.7 (0.6) 17.9 (0.7)
39.2 (0.4) 37.9 (0.4) 18.6 (0.6)
68.1 (0.6) 66.5 (0.1) 37.7 (0.5)

16.5 (1.0)
32.3 (0.5)
30.0 (1.0)
62.1 (1.0)

Table 4 MAP of CORAL-LDA trained on positive examples from each row’s source domain and
background statistics from each column’s domain. The average distance between each set of back-
ground statistics and the true source and target statistics is shown in parentheses.

Unsupervised and Semi-supervised Adaptation

Next, we report the results of our unsupervised and semi-supervised adaptation tech-
nique. We use the same setting as [10], in which three positive and nine negative
labeled images per category were used for semi-supervised adaptation. Target co-
variance in Equation 6 is estimated from 305 unlabeled training examples. We also
followed the same approach to learn a linear combination between the unsupervised
and supervised model via cross-validation. The results are presented in Table 5.
Please note that our target-only MAP is 52.9 compared to 36.6 in [10]. This also
conﬁrms our conclusion that the statistics should come from a related domain. It is
clear that both of our unsupervised and semi-supervised adaptation techniques out-
perform the method in [10]. Furthermore, Virtual-Gray data outperforms Virtual,
and DSLR does best, as it is very close to the target domain (the main difference
between DLSR and Webcam domains is in the camera used to capture images).

Finally, we compare our method trained on Virtual-Gray to the results of adapt-
ing from ImageNet reported by [10], in Figure 9. While their unsupervised models
are learned from 150-2000 real ImageNet images per category and the background
statistics are estimated from 10,000 PASCAL images, we only have 30 virtual im-
ages per category and the background statistics is learned from about 1,000 images.
What’s more, all the virtual images used are with uniform gray texturemap and white
background. This clearly demonstrates the importance of domain-speciﬁc decorre-
lation, and shows that the there is no need to collect a large amount of real images
to train a good classiﬁer.

Fig. 9 Comparison of unsupervised and semi-supervised adaptation of virtual detectors using our
method with the results of training on ImageNet and supervised adaptation from ImageNet reported
in [10]. Our semi-supervised adapted detectors achieve comparable performance despite not using
any real source training data, and using only 3 positive images for adaptation, and even outperform
ImageNet signiﬁcantly for several categories (e.g., ruler).

Correlation Alignment for Unsupervised Domain Adaptation

19

Source
Virtual
Virtual-Gray
Amazon
DSLR

Source-only [14] Unsup-Ours SemiSup [10] SemiSup-Ours

10.7
17.9
18.6
37.7

27.9
33.0
38.9
67.1

30.7
35.0
35.8
42.9

45.2
54.7
53.0
71.4

Table 5 Top: Comparison of the source-only [14] and semi-supervised adapted model of [10]
with our unsupervised-adapted and semi-supervised adapted models. Target domain is Webcam.
Mean AP across categories is reported on the Webcam test data, using different source domains for
training. Bottom: Sample detections of the DSLR-UnsupAdapt-Ours detectors.

6 Conclusion

In this chapter, we described a simple, effective, and efﬁcient method for unsu-
pervised domain adaptation called CORrelation ALignment (CORAL). CORAL
minimizes domain shift by aligning the second-order statistics of source and target
distributions, without requiring any target labels. We also developed novel domain
adaptation algorithms by applying the idea of CORAL to three different scenarios.
In the ﬁrst scenario, we applied a linear transformation that minimizes the CORAL
objective to source features prior to training the classiﬁer. In the case of linear classi-
ﬁers, we equivalently applied the linear CORAL transform to the classiﬁer weights,
signiﬁcantly improving efﬁciency and classiﬁcation accuracy over standard LDA
on several domain adaptation benchmarks. We further extended CORAL to learn
a nonlinear transformation that aligns correlations of layer activations in deep neu-
ral networks. The resulting Deep CORAL approach works seamlessly with deep
networks and can be integrated into any arbitrary network architecture to enable
end-to-end unsupervised adaptation. One limitation of CORAL is that it captures
second-order statistics only and may not preserve higher-order structure in the data.
However, as demonstrated in this chapter, it works fairly well in practice, and can
also potentially be combined with other domain-alignment loss functions.

References

1. Jian-Feng Cai, Emmanuel J. Cand`es, and Zuowei Shen. A singular value thresholding algo-

rithm for matrix completion. SIAM J. on Optimization, 20(4):1956–1982, March 2010.

2. H. Daume III. Frustratingly easy domain adaptation. In ACL, 2007.
3. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale

hierarchical image database. In CVPR, 2009.

20

Baochen Sun, Jiashi Feng, and Kate Saenko

4. Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and
Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition.
In ICML, 2014.

5. L. Duan, I. W. Tsang, D. Xu, and T. Chua. Domain adaptation from multiple sources via

6. Lixin Duan, Ivor W. Tsang, and Dong Xu. Domain transfer multiple kernel learning. TPAMI,

auxiliary classiﬁers. In ICML, 2009.

34(3):465–479, March 2012.

7. Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object de-
tection with discriminatively trained part-based models. TPAMI, 32(9):1627–1645, 2010.
8. Basura Fernando, Amaury Habrard, Marc Sebban, and Tinne Tuytelaars. Unsupervised visual

domain adaptation using subspace alignment. In ICCV, 2013.

9. Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation.

In ICML, 2015.

10. Daniel Goehring, Judy Hoffman, Erik Rodner, Kate Saenko, and Trevor Darrell. Interactive

adaptation of real-time object detectors. In ICRA, 2014.

11. B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic ﬂow kernel for unsupervised domain

12. R. Gopalan, R. Li, and R. Chellappa. Domain adaptation for object recognition: An unsuper-

adaptation. In CVPR, 2012.

vised approach. In ICCV, 2011.

13. Maayan Harel and Shie Mannor. Learning from multiple outlooks. In ICML, 2011.
14. Bharath Hariharan, Jitendra Malik, and Deva Ramanan. Discriminative decorrelation for clus-

tering and classiﬁcation. In ECCV. 2012.

15. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training

by reducing internal covariate shift. In ICML, 2015.

16. Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick,
Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature
embedding. arXiv preprint, 2014.

17. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep

convolutional neural networks. In NIPS, 2012.

18. B. Kulis, K. Saenko, and T. Darrell. What you saw is not what you get: Domain adaptation

using asymmetric kernel transforms. In CVPR, 2011.

19. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable

features with deep adaptation networks. In ICML, 2015.

20. Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and P.S. Yu. Transfer joint

matching for unsupervised domain adaptation. In CVPR, 2014.

21. Tomasz Malisiewicz, Abhinav Gupta, and Alexei A Efros. Ensemble of exemplar-svms for

22. Sinno Jialin Pan, Ivor W. Tsang, James T. Kwok, and Qiang Yang. Domain adaptation via

object detection and beyond. In ICCV, 2011.

transfer component analysis. In IJCAI, 2009.

23. Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models

24. Baochen Sun. Correlation Alignment for Domain Adaptation. PhD thesis, University of

25. Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation.

26. Baochen Sun, Xingchao Peng, and Kate Saenko. Generating large scale image datasets from

3d cad models. In CVPR’15 Workshop on The Future of Datasets in Vision, 2015.

27. Baochen Sun and Kate Saenko. From virtual to reality: Fast adaptation of virtual object de-

tectors to real domains. In BMVC, 2014.

28. Baochen Sun and Kate Saenko. Subspace distribution alignment for unsupervised domain

29. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation.

adaptation. In BMVC, 2015.

In ECCV 2016 Workshops, 2016.

30. Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain

confusion: Maximizing for domain invariance. CoRR, abs/1412.3474, 2014.

to new domains. In ECCV, 2010.

Massachusetts Lowell, 2016.

In AAAI, 2016.

6
1
0
2
 
c
e
D
 
6
 
 
]

V
C
.
s
c
[
 
 
1
v
9
3
9
1
0
.
2
1
6
1
:
v
i
X
r
a

Correlation Alignment for Unsupervised
Domain Adaptation

Baochen Sun, Jiashi Feng, and Kate Saenko

Abstract In this chapter, we present CORrelation ALignment (CORAL), a sim-
ple yet effective method for unsupervised domain adaptation. CORAL minimizes
domain shift by aligning the second-order statistics of source and target distribu-
tions, without requiring any target labels. In contrast to subspace manifold meth-
ods, it aligns the original feature distributions of the source and target domains,
rather than the bases of lower-dimensional subspaces. It is also much simpler
than other distribution matching methods. CORAL performs remarkably well in
extensive evaluations on standard benchmark datasets. We ﬁrst describe a solu-
tion that applies a linear transformation to source features to align them with tar-
get features before classiﬁer training. For linear classiﬁers, we propose to equiva-
lently apply CORAL to the classiﬁer weights, leading to added efﬁciency when the
number of classiﬁers is small but the number and dimensionality of target exam-
ples are very high. The resulting CORAL Linear Discriminant Analysis (CORAL-
LDA) outperforms LDA by a large margin on standard domain adaptation bench-
marks. Finally, we extend CORAL to learn a nonlinear transformation that aligns
correlations of layer activations in deep neural networks (DNNs). The resulting
Deep CORAL approach works seamlessly with DNNs and achieves state-of-the-
art performance on standard benchmark datasets. Our code is available at: https:
//github.com/VisionLearningGroup/CORAL

Baochen Sun
Microsoft, e-mail: baochens@gmail.com

Jiashi Feng
National University of Singapore, e-mail: elefjia@nus.edu.sg

Kate Saenko
Boston University, e-mail: saenko@bu.edu

1

2

1 Introduction

Baochen Sun, Jiashi Feng, and Kate Saenko

Machine learning is very different from human learning. Humans are able to learn
from very few labeled examples and apply the learned knowledge to new examples
in novel conditions. In contrast, traditional machine learning algorithms assume that
the training and test data are independent and identically distributed (i.i.d.) and su-
pervised machine learning methods only perform well when the given extensive
labeled data are from the same distribution as the test distribution. However, this
assumption rarely holds in practice, as the data are likely to change over time and
space. To compensate for the degradation in performance due to domain shift, do-
main adaptation [23, 8, 11, 20, 28, 25, 30, 2] tries to transfer knowledge from source
(training) domain to target (test) domain. Our approach is in line with most existing
unsupervised domain adaptation approaches in that we ﬁrst transform the source
data to be as close to the target data as possible. Then a classiﬁer trained on the
transformed source domain is applied to the target data.

In this chapter, we mainly focus on the unsupervised scenario as we believe that
leveraging the unlabeled target data is key to the success of domain adaptation. In
real world applications, unlabeled target data are often much more abundant and
easier to obtain. On the other side, labeled examples are very limited and require
human annotation. So the question of how to utilize the unlabeled target data is
more important for practical visual domain adaptation. For example, it would be
more convenient and applicable to have a pedestrian detector automatically adapt to
the changing visual appearance of pedestrians rather than having a human annotator
label every frame that has a different visual appearance.

Our goal is to propose a simple yet effective approach for domain adaptation that
researchers with little knowledge of domain adaptation or machine learning could
easily integrate into their application. In this chapter, we describe a “frustratingly
easy” (to borrow a phrase from [2]) unsupervised domain adaptation method called
CORrelation ALignment or CORAL [25] in short. CORAL aligns the input feature
distributions of the source and target domains by minimizing the difference between
their second-order statistics. The intuition is that we want to capture the structure
of the domain using feature correlations. As an example, imagine a target domain
of Western movies where most people wear hats; then the “head” feature may be
positively correlated with the “hat” feature. Our goal is to transfer such correlations
to the source domain by transforming the source features.

In Section 2, we describe a linear solution to CORAL, where the distributions
are aligned by re-coloring whitened source features with the covariance of the tar-
get data distribution. This solution is simple yet efﬁcient, as the only computations
it needs are (1) computing covariance statistics in each domain and (2) applying
the whitening and re-coloring linear transformation to the source features. Then, su-
pervised learning proceeds as usual–training a classiﬁer on the transformed source
features.

For linear classiﬁers, we can equivalently apply the CORAL transformation to
the classiﬁer weights, leading to better efﬁciency when the number of classiﬁers is
small but the number and dimensionality of the target examples are very high. We

Correlation Alignment for Unsupervised Domain Adaptation

3

present the resulting CORAL–Linear Discriminant Analysis (CORAL-LDA) [27]
in Section 3, and show that it outperforms standard Linear Discriminant Analysis
(LDA) by a large margin on cross domain applications. We also extend CORAL to
work seamlessly with deep neural networks by designing a layer that consists of a
differentiable CORAL loss [29], detailed in Section 4. On the contrary to the linear
CORAL, Deep CORAL learns a nonlinear transformation and also provides end-
to-end adaptation. Section 5 describes extensive quantitative experiments on several
benchmarks, and Section 6 concludes the chapter.

2 Linear Correlation Alignment

In this section, we present CORrelation ALignment (CORAL) for unsupervised do-
main adaptation and derive a linear solution. We ﬁrst describe the formulation and
derivation, followed by the main linear CORAL algorithm and its relationship to ex-
isting approaches. In this section and Section 3, we constrain the transformation to
be linear. Section 4 extends CORAL to learn a nonlinear transformation that aligns
correlations of layer activations in deep neural networks (Deep CORAL).

2.1 Formulation and Derivation

We describe our method by taking a multi-class classiﬁcation problem as the run-
ning example. Suppose we are given source-domain training examples DS = {xi},
x ∈ Rd with labels LS = {yi}, y ∈ {1, ..., L}, and target data DT = {ui}, u ∈ Rd. Here
both x and u are the d-dimensional feature representations φ (I) of input I. Suppose
µs, µt and CS,CT are the feature vector means and covariance matrices. Assuming
that all features are normalized to have zero mean and unit variance, µt = µs = 0
after the normalization step, while CS (cid:54)= CT .

To minimize the distance between the second-order statistics (covariance) of the
source and target features, we apply a linear transformation A to the original source
features and use the Frobenius norm as the matrix distance metric:

(cid:107)C ˆS −CT (cid:107)2

F

min
A

= min
A

2
(cid:107)A(cid:62)CSA −CT (cid:107)
F

(1)

where C ˆS is covariance of the transformed source features DsA and (cid:107) · (cid:107)2
the squared matrix Frobenius norm.

F denotes

If rank(CS) ≥ rank(CT ), then an analytical solution can be obtained by choosing
A such that C ˆS = CT . However, the data typically lie on a lower dimensional man-
ifold [13, 11, 8], and so the covariance matrices are likely to be low rank [14]. We
derive a solution for this general case, using the following lemma.

4

Baochen Sun, Jiashi Feng, and Kate Saenko

Lemma 1. Let Y be a real matrix of rank rY and X a real matrix of rank at most
r, where r (cid:54) rY ; let Y = UY ΣYVY be the SVD of Y , and ΣY [1:r], UY [1:r], VY [1:r] be the
largest r singular values and the corresponding left and right singular vectors of Y
(cid:62) is the optimal solution to the problem
respectively. Then, X ∗ = UY [1:r]ΣY [1:r]VY [1:r]
of min
X

(cid:107)X −Y (cid:107)2

F . [1]

Theorem 1. Let Σ + be the Moore-Penrose pseudoinverse of Σ , rCS and rCT denote
the rank of CS and CT respectively. Then, A∗ = USΣ +
(cid:62) is
S
the optimal solution to the problem in Equation (12) with r = min(rCS , rCT ).

(cid:62)UT [1:r]ΣT [1:r]

1
2 UT [1:r]

1
2 US

Proof. Since A is a linear transformation, A(cid:62)CSA does not increase the rank of CS.
(cid:54) rCS . Since CS and CT are symmetric matrices, conducting SVD on CS
Thus, rC ˆS
and CT gives CS = USΣSUS
T respectively. We ﬁrst ﬁnd the op-
timal value of C ˆS through considering the following two cases:
Case 1. rCS > rCT . The optimal solution is C ˆS = CT . Thus, C ˆS = UT ΣTUT
UT [1:r]ΣT [1:r]UT [1:r]

(cid:62) is the optimal solution to Equation (12) where r = rCT .

(cid:62) and CT = UT ΣTU (cid:62)

(cid:62) =

Case 2. rCS
optimal solution to Equation (12) where r = rCS .

(cid:54) rCT . Then, according to Lemma 1, C ˆS = UT [1:r]ΣT [1:r]UT [1:r]

(cid:62) is the

Combining the results in the above two cases yields that C ˆS = UT [1:r]ΣT [1:r]UT [1:r]
is the optimal solution to Equation (12) with r = min(rCS , rCT ). We then proceed to
solve for A based on the above result. Letting C ˆS = A(cid:62)CSA, we can write

(cid:62)

A(cid:62)CSA = UT [1:r]ΣT [1:r]UT [1:r]

(cid:62).

Since CS = USΣSUS

(cid:62), we have

A(cid:62)USΣSUS

(cid:62)A = UT [1:r]ΣT [1:r]UT [1:r]

(cid:62).

This gives:

(cid:62)
(cid:62)A)

(US

ΣS(US

(cid:62)A) = UT [1:r]ΣT [1:r]UT [1:r]

(cid:62).

Let E = Σ +
(cid:62)UT [1:r]ΣT [1:r]
S
tion can be re-written as E(cid:62)ΣSE. This gives

1
2 UT [1:r]

1
2 US

(cid:62), then the right hand side of the above equa-

(cid:62)
(cid:62)A)

(US

ΣS(US

(cid:62)A) = E(cid:62)ΣSE

By setting US

(cid:62)A to E, we obtain the optimal solution of A as

A∗ = USE

= (USΣ +
S

1
2 US

(cid:62))(UT [1:r]ΣT [1:r]

1
2 UT [1:r]

(cid:62)).

(2)

Correlation Alignment for Unsupervised Domain Adaptation

5

Fig. 1 (a-c) Illustration of CORrelation ALignment (CORAL) for Domain Adaptation: (a) The
original source and target domains have different distribution covariances, despite the features be-
ing normalized to zero mean and unit standard deviation. This presents a problem for transferring
classiﬁers trained on source to target. (b) The same two domains after source decorrelation, i.e.
removing the feature correlations of the source domain. (c) Target re-correlation, adding the corre-
lation of the target domain to the source features. After this step, the source and target distributions
are well aligned and the classiﬁer trained on the adjusted source domain is expected to work well
in the target domain. (d) One might instead attempt to align the distributions by whitening both
source and target. However, this will fail since the source and target data are likely to lie on different
subspaces due to domain shift. (Best viewed in color)

2.2 Algorithm

Figures 1(a-c) illustrate the linear CORAL approach. Figure 1(a) shows example
original source and target data distributions. We can think of the transformation A
1
intuitively as follows: the ﬁrst part USΣ +
(cid:62) whitens the source data, while the
2 US
S
(cid:62) re-colors it with the target covariance. This is
second part UT [1:r]ΣT [1:r]
illustrated in Figure 1(b) and Figure 1(c) respectively.

1
2 UT [1:r]

In practice, for the sake of efﬁciency and stability, we can avoid the expensive
SVD steps and perform traditional data whitening and coloring. Traditional whiten-
ing adds a small regularization parameter λ to the diagonal elements of the covari-
ance matrix to explicitly make it full rank and then multiplies the original features
by its inverse square root (or square root for coloring.) This is advantageous be-
cause: (1) it is faster1 and more stable, as SVD on the original covariance matrices

1 the entire CORAL transformation takes less than one minute on a regular laptop for dimensions
as large as DS ∈ R795×4096 and DT ∈ R2817×4096

6

Baochen Sun, Jiashi Feng, and Kate Saenko

might not be stable and might be slow to converge; (2) as illustrated in Figure 2,
the performance is similar to the analytical solution in Equation (2) and very stable
with respect to λ . In the experiments provided at the end of this chapter we set λ to
1. The ﬁnal algorithm can be written in four lines of MATLAB code as illustrated
in Algorithm 1.

Fig. 2 Sensitivity of CORAL to the covariance regularization parameter λ with λ ∈ {0, 0.001,
0.01, 0.1, 1}. The plots show classiﬁcation accuracy on target data for two domain shifts (blue and
red). When λ = 0, there is no regularization and we use the analytical solution in Equation (2).
Please refer to Section 5.1 for details of the experiment.

Algorithm 1 CORAL for Unsupervised Domain Adaptation

Input: Source Data DS, Target Data DT
Output: Adjusted Source Data D∗
s
CS = cov(DS) + eye(size(DS, 2))
CT = cov(DT ) + eye(size(DT , 2))

−1
2

DS = DS ∗C
S
1
D∗
2
S = DS ∗C
T

% whitening source

% re-coloring with target covariance

One might instead attempt to align the distributions by whitening both source
and target. As shown in Figure 1(d), this will fail as the source and target data are
likely to lie on different subspaces due to domain shift. An alternative approach
would be whitening the target and then re-coloring it with the source covariance.
However, as demonstrated in [13, 8] and our experiments, transforming data from
source to target space gives better performance. This might be due to the fact that by
transforming the source to target space the classiﬁer is trained using both the label
information from the source and the unlabelled structure from the target.

After CORAL transforms the source features to the target space, a classiﬁer fw
parametrized by w can be trained on the adjusted source features and directly ap-
plied to target features. For a linear classiﬁer fw(I) = wT φ (I), we can apply an
equivalent transformation to the parameter vector w (e.g., fw(I) = (wT A)φ (I)) in-
stead of the features (e.g., fw(I) = wT (Aφ (I))). This results in added efﬁciency
when the number of classiﬁers is small but the number and dimensionality of target

Correlation Alignment for Unsupervised Domain Adaptation

7

examples is very high. For linear SVM, this extension is straightforward. In Sec-
tion 3, we apply the idea of CORAL to another commonly used linear classiﬁer–
Linear Discriminant Analysis (LDA). LDA is special in the sense that its weights
also use the covariance of the data. It is also extremely efﬁcient for training a large
number of classiﬁers [14].

2.3 Relationship to Existing Methods

It has long been known that input feature normalization improves many machine
learning methods, e.g., [15]. However, CORAL does not simply perform feature nor-
malization, but rather aligns two different distributions. Batch Normalization [15]
tries to compensate for internal covariate shift by normalizing each mini-batch to be
zero-mean and unit-variance. However, as illustrated in Figure 1(a), such normal-
ization might not be enough. Even if used with full whitening, Batch Normalization
may not compensate for external covariate shift: the layer activations will be decor-
related for a source point but not for a target point. What’s more, as mentioned in
Section 2.2, whitening both domains is not a successful strategy.

Recent state-of-the-art unsupervised approaches project the source and target dis-
tributions into a lower-dimensional manifold and ﬁnd a transformation that brings
the subspaces closer together [12, 11, 8, 13]. CORAL avoids subspace projection,
which can be costly and requires selecting the hyper-parameter that controls the di-
mensionality of the subspace, k. We note that subspace-mapping approaches [13, 8]
only align the top k eigenvectors of the source and target covariance matrices. On the
contrary, CORAL aligns the covariance matrices, which can only be re-constructed
using all eigenvectors and eigenvalues. Even though the eigenvectors can be aligned
well, the distributions can still differ a lot due to the difference of eigenvalues be-
tween the corresponding eigenvectors of the source and target data. CORAL is a
more general and much simpler method than the above two as it takes into ac-
count both eigenvectors and eigenvalues of the covariance matrix without the burden
of subspace dimensionality selection.

Maximum Mean Discrepancy (MMD) based methods (e.g., TCA [22], DAN [19])
for domain adaptation can be interpreted as “moment matching” and can ex-
press arbitrary statistics of the data. Minimizing MMD with a polynomial kernel
(k(x, y) = (1 + x(cid:48)y)q with q = 2) is similar to the CORAL objective, however, no
previous work has used this kernel for domain adaptation nor proposed a closed
form solution to the best of our knowledge. The other difference is that MMD based
approaches usually apply the same transformation to both the source and target do-
main. As demonstrated in [18, 13, 8], asymmetric transformations are more ﬂexible
and often yield better performance for domain adaptation tasks. Intuitively, symmet-
ric transformations ﬁnd a space that “ignores” the differences between the source
and target domain while asymmetric transformations try to “bridge” the two do-
mains.

8

Baochen Sun, Jiashi Feng, and Kate Saenko

3 CORAL Linear Discriminant Analysis

In this section, we introduce how CORAL can be applied for aligning multiple linear
classiﬁers. In particular, we take LDA as the example for illustration, considering
LDA is a commonly used and effective linear classiﬁer. Combining CORAL and
LDA gives a new efﬁcient adpative learning approach CORAL-LDA. We use the
task of object detection as a running example to explain CORAL-LDA.

We begin by describing the decorrelation-based approach to detection proposed
in [14]. Given an image I, it follows the sliding-window paradigm, extracting a
d-dimensional feature vector φ (I, b) at each window b across all locations and at
multiple scales. It then scores the windows using a scoring function

fw(I, b) = w(cid:62)φ (I, b).

(3)

In practice, all windows with values of fw above a predetermined threshold are
considered positive detections.

In recent years, use of the linear SVM as the scoring function fw, usually with
Histogram of Gradients (HOG) as the features φ , has emerged as the predominant
object detection paradigm. Yet, as observed by Hariharan et al. [14], training SVMs
can be expensive, especially because it usually involves costly rounds of hard neg-
ative mining. Furthermore, the training must be repeated for each object category,
which makes it scale poorly with the number of categories.

Hariharan et al. [14] proposed a much more efﬁcient alternative, learning fw
with Linear Discriminant Analysis (LDA). LDA is a well-known linear classiﬁer
that models the training set of examples x with labels y ∈ {0, 1} as being generated
by p(x, y) = p(x|y)p(y). p(y) is the prior on class labels and the class-conditional
densities are normal distributions

p(x|y) = N (x; µ y, CS),

where the feature vector covariance CS is assumed to be the same for both positive
and negative (background) classes. In our case, the feature is represented by x =
φ (I, b). The resulting classiﬁer is given by

w = CS

−1(µ1 − µ0)

The innovation in [14] was to re-use CS and µ0, the background mean, for all cate-
gories, reducing the task of learning a new category model to computing the average
positive feature, µ1. This was accomplished by calculating CS and µ0 for the largest
possible window and subsampling to estimate all other smaller window sizes. Also,
CS was shown to have a sparse local structure, with correlation falling off sharply
beyond a few nearby image locations.

Like other classiﬁers, LDA learns to suppress non-discriminative structures and
enhance the contours of the object. However it does so by learning the global co-
variance statistics once for all natural images, and then using the inverse covariance
matrix to remove the non-discriminative correlations, and the negative mean to re-

(4)

(5)

Correlation Alignment for Unsupervised Domain Adaptation

9

Fig. 3 (a) Applying a linear classiﬁer w learned by LDA to source data x is equivalent to (b)
−1/2x. (c) However, target points u
applying classiﬁer ˆw = CS
−1/2u, hurting performance. (d) Our method uses target-speciﬁc
may still be correlated after CS
covariance to obtain properly decorrelated ˆu.

−1/2w to decorrelated points CS

move the average feature. LDA was shown in [14] to have competitive performance
to SVM, and can be implemented both as an exemplar-based [21] or as deformable
parts model (DPM) [7].

We observe that estimating global statistics CS and µ0 once and re-using them for
all tasks may work when training and testing in the same domain, but in our case,
the source training data is likely to have different statistics from the target data.
Figure 4 illustrates the effect of centering and decorrelating a positive mean using
global statistics from the wrong domain. The effect is clear: important discriminative
information is removed while irrelevant structures are not.

Based on this observation, we propose an adaptive decorrelation approach to
detection. Assume that we are given labeled training data {x, y} in the source domain
(e.g., virtual images rendered from 3D models), and unlabeled examples u in the
target domain (e.g., real images collected in an ofﬁce environment). Evaluating the
scoring function fw(x) in the source domain is equivalent to ﬁrst de-correlating the
−1/2x, computing their positive and negative class means
training features ˆx = CS
−1/2µ0 and then projecting the decorrelated feature onto
ˆµ1 = CS
the decorrelated difference between means, fw(x) = ˆw(cid:62) ˆx, where ˆw = ( ˆµ1 − ˆµ0).
This is illustrated in Figure 3(a-b).

−1/2µ1 and ˆµ0 = CS

However, as we saw in Figure 4, the assumption that the input is properly decor-
related does not hold if the input comes from a target domain with a different co-
−1/2u does not
variance structure. Figure 3(c) illustrates this case, showing that CS
have isotropic covariance. Therefore, w cannot be used directly.

Fig. 4 Visualization of classiﬁer weights of bicycle (decorrelated with mismatched-domain co-
variance (left) v.s. with same-domain covariance (right)).

10

Baochen Sun, Jiashi Feng, and Kate Saenko

We may be able to compute the covariance of the target domain on the unlabeled
target points u, but not the positive class mean. Therefore, we would like to re-use
the decorrelated mean difference ˆw, but adapt to the covariance of the target do-
main. In the rest of the chapter, we make the assumption that the difference between
positive and negative means is the same in the source and target. This may or may
not hold in practice, and we discuss this further in Section 5.

Let the estimated target covariance be CT. We ﬁrst decorrelate the target input
feature with its inverse square root, and then apply ˆw directly, as shown in Fig-
ure 3(d). The resulting scoring function is:

f ˆw(u) = ˆw(cid:62) ˆu

= (CS

= ((CT

(cid:62)
−1/2(µ1 − µ0))
−1/2)(cid:62)CS

−1/2u)
(CT
(cid:62)
−1/2(µ1 − µ0))

u

(6)

This corresponds to a transformation (CT

−1/2) instead of the original
−1/2)(cid:62)(CS
−1 being applied to the difference between means to compute w. Note
−1/2) equals to
−1 since both CT and CS are symmetric. In this case, Equation 6 ends up the

whitening CS
that if source and target domains are the same, then (CT
CS
same as Equation 5.

−1/2)(cid:62)(CS

In practice, either the source or the target component of the above transformation
may also work, or even statistics from similar domains. However, as we will see in
Section 5.2, dissimilar domain statistics can signiﬁcantly hurt performance. Further-
more, if either source or target has only images of the positive category available,
and cannot be used to properly compute background statistics, the other domain can
still be used.

CORAL-LDA works in a purely unsupervised way. Here, we extend it to semi-
supervised adaptation when a few labeled examples are available in the target do-
main. Following [10], a simple adaptation method is used whereby the template
learned on source positives is combined with a template learned on target positives,
using a weighted combination. The key difference with our approach is that the
target template uses target-speciﬁc statistics.

In [10], the author uses the same background statistics as [14] which were es-
timated on 10,000 natural images from the PASCAL VOC 2010 dataset. Based on
our analysis above, even though these background statistics were estimated from a
very large amount of real image data, it will not work for all domains. In section 5.2,
our results conﬁrm this claim.

4 Deep CORAL

In this section, we extend CORAL to work seamlessly with deep neural networks
by designing a differentiable CORAL loss. Deep CORAL enables end-to-end adap-
tation and also learns more a powerful nonlinear transformation. It can be easily

Correlation Alignment for Unsupervised Domain Adaptation

11

Fig. 5 Sample Deep CORAL architecture based on a CNN with a classiﬁer layer. For generaliza-
tion and simplicity, here we apply the CORAL loss to the f c8 layer of AlexNet [17]. Integrating it
into other layers or network architectures is also possible.

integrated into different layers or network architectures. Figure 5 shows a sample
Deep CORAL architecture using our proposed correlation alignment layer for deep
domain adaptation. We refer to Deep CORAL as any deep network incorporating
the CORAL loss for domain adaptation.

We ﬁrst describe the CORAL loss between two domains for a single feature layer.
Suppose the numbers of source and target data are nS and nT respectively. Here both
x and u are the d-dimensional deep layer activations φ (I) of input I that we are
trying to learn. Suppose Di j
T ) indicates the j-th dimension of the i-th source
(target) data example and CS (CT ) denote the feature covariance matrices.

S (Di j

We deﬁne the CORAL loss as the distance between the second-order statistics

(covariances) of the source and target features:

LCORAL =

1
4d2 (cid:107)CS −CT (cid:107)2

F

(7)

(8)

(9)

where (cid:107) · (cid:107)2
of the source and target data are given by:

F denotes the squared matrix Frobenius norm. The covariance matrices

CS =

1
nS − 1

(D(cid:62)

S DS −

(1(cid:62)DS)(cid:62)(1(cid:62)DS))

1
nT − 1
where 1 is a column vector with all elements equal to 1.

T DT −

CT =

1
nT

(D(cid:62)

(1(cid:62)DT )(cid:62)(1(cid:62)DT ))

The gradient with respect to the input features can be calculated using the chain

rule:

∂ LCORAL
∂ Di j
S

=

1
d2(nS − 1)

((D(cid:62)

S −

(1(cid:62)DS)(cid:62)1(cid:62))(cid:62)(CS −CT ))i j

(10)

1
nS

1
nS

12

Baochen Sun, Jiashi Feng, and Kate Saenko

∂ LCORAL
∂ Di j
T

= −

1
d2(nT − 1)

1
nT

((D(cid:62)

T −

(1(cid:62)DT )(cid:62)1(cid:62))(cid:62)(CS −CT ))i j

(11)

In our experiments, we use batch covariances and the network parameters are shared
between the two networks, but other settings are also possible.

To see how this loss can be used to adapt an existing neural network, let us
return to the multi-class classiﬁcation problem. Suppose we start with a network
with a ﬁnal classiﬁcation layer, such as the ConvNet shown in Figure 5. As men-
tioned before, the ﬁnal deep features need to be both discriminative enough to train
a strong classiﬁer and invariant to the difference between source and target domains.
Minimizing the classiﬁcation loss itself is likely to lead to overﬁtting to the source
domain, causing reduced performance on the target domain. On the other hand, min-
imizing the CORAL loss alone might lead to degenerated features. For example, the
network could project all of the source and target data to a single point, making
the CORAL loss trivially zero. However, no strong classiﬁer can be constructed on
these features. Joint training with both the classiﬁcation loss and CORAL loss is
likely to learn features that work well on the target domain:

L = LCLASS +

λiLCORALi

(12)

t
∑
i=1

where t denotes the number of CORAL loss layers in a deep network and λi is a
weight that trades off the adaptation with classiﬁcation accuracy on the source do-
main. As we show below, these two losses play counterparts and reach an equilib-
rium at the end of training, where the ﬁnal features are discriminative and generalize
well to the target domain.

5 Experiments

We evaluate CORAL and Deep CORAL on object recognition [23] using standard
benchmarks and protocols. In all experiments we assume the target domain is unla-
beled. For CORAL, we follow the standard procedure [8, 4] and use a linear SVM as
the base classiﬁer. The model selection approach of [8] is used to set the C parame-
ter for the SVM by doing cross-validation on the source domain. For CORAL-LDA,
as efﬁciency is the main concern, we evaluate it on the more time constrained task–
object detection. We follow the protocol of [10] and use HOG features. To have a
fair comparison, we use accuracies reported by other authors with exactly the same
setting or conduct experiments using the source code provided by the authors.

Correlation Alignment for Unsupervised Domain Adaptation

13

5.1 Object Recognition

In this set of experiments, domain adaptation is used to improve the accuracy of
an object classiﬁer on novel image domains. Both the standard Ofﬁce [23] and
extended Ofﬁce-Caltech10 [11] datasets are used as benchmarks in this chap-
ter. Ofﬁce-Caltech10 contains 10 object categories from an ofﬁce environment
(e.g., keyboard, laptop, etc.) in 4 image domains: Webcam, DSLR, Amazon, and
Caltech256. The standard Ofﬁce dataset contains 31 (the same 10 categories from
Ofﬁce-Caltech10 plus 21 additional ones) object categories in 3 domains: Webcam,
DSLR, and Amazon.

A→C A→D A→W C→A C→D C→W D→A D→C D→W W→A W→C W→D AVG
37.8
26.4
35.8
NA
41.0
33.4
34.8
SVMA
40.2
33.5
34.9
DAM
43.4
37.9
38.3
GFK
45.7
39.6
40.0
TCA
42.0
45.9
SA
39.9
46.7
CORAL 40.3
38.1

25.7
33.5
31.2
29.1
33.7
31.8
34.6

27.1
31.4
31.5
31.4
34.0
35.0
34.2

32.3
36.6
34.7
37.1
40.2
39.3
37.8

39.4
34.5
34.7
36.1
41.4
39.4
40.7

24.9
32.5
32.5
39.8
40.1
39.6
38.7

33.1
34.1
34.3
37.9
39.1
38.8
38.3

78.9
75.0
68.3
74.6
77.5
77.9
84.9

56.4
74.4
74.7
79.1
80.4
82.3
85.9

30.0
32.9
33.1
34.9
36.2
38.9
39.2

43.7
39.1
39.2
44.8
46.7
46.1
47.2

Table 1 Object recognition accuracies of all 12 domain shifts on the Ofﬁce-Caltech10 dataset [11]
with SURF features, following the protocol of [11, 8, 12, 18, 23].

Object Recognition with Shallow Features

We follow the standard protocol of [11, 8, 12, 18, 23] and conduct experiments
on the Ofﬁce-Caltech10 dataset with shallow features (SURF). The SURF features
were encoded with 800-bin bag-of-words histograms and normalized to have zero
mean and unit standard deviation in each dimension. Since there are four domains,
there are 12 experiment settings, namely, A→C (train classiﬁer on (A)mazon, test
on (C)altech), A→D (train on (A)mazon, test on (D)SLR), A→W, and so on. We
follow the standard protocol and conduct experiments in 20 randomized trials for
each domain shift and average the accuracy over the trials. In each trial, we use the
standard setting [11, 8, 12, 18, 23] and randomly sample the same number (20 for
Amazon, Caltech, and Webcam; 8 for DSLR as there are only 8 images per category
in the DSLR domain) of labelled images in the source domain as training set, and
use all the unlabelled data in the target domain as the test set.

In Table 1, we compare our method to ﬁve recent published methods: SVMA [6],
DAM [5], GFK [11], SA [8], and TCA [22] as well as the no adaptation base-
line (NA). GFK, SA, and TCA are manifold based methods that project the source
and target distributions into a lower-dimensional manifold. GFK integrates over an
inﬁnite number of subspaces along the subspace manifold using the kernel trick.
SA aligns the source and target subspaces by computing a linear map that mini-
mizes the Frobenius norm of their difference. TCA performs domain adaptation via
a new parametric kernel using feature extraction methods by projecting data onto

14

Baochen Sun, Jiashi Feng, and Kate Saenko

the learned transfer components. DAM introduces smoothness assumption to en-
force the target classiﬁer share similar decision values with the source classiﬁers.
Even though these methods are far more complicated than ours and require tuning
of hyperparameters (e.g., subspace dimensionality), our method achieves the best
average performance across all the 12 domain shifts. Our method also improves on
the no adaptation baseline (NA), in some cases increasing accuracy signiﬁcantly
(from 56% to 86% for D→W).

Object Recognition with Deep Features

For visual domain adaptation with deep features, we follow the standard protocol
of [11, 19, 4, 30, 9] and use all the labeled source data and all the target data without
labels on the standard Ofﬁce dataset [23]. Since there are 3 domains, we conduct
experiments on all 6 shifts (5 runs per shift), taking one domain as the source and
another as the target.

In this experiment, we apply the CORAL loss to the last classiﬁcation layer as it
is the most general case–most deep classiﬁer architectures (e.g., convolutional neu-
ral networks, recurrent neural networks) contain a fully connected layer for classi-
ﬁcation. Applying the CORAL loss to other layers or other network architectures is
also possible. The dimension of the last fully connected layer ( f c8) was set to the
number of categories (31) and initialized with N (0, 0.005). The learning rate of
f c8 was set to 10 times the other layers as it was training from scratch. We initial-
ized the other layers with the parameters pre-trained on ImageNet [3] and kept the
original layer-wise parameter settings. In the training phase, we set the batch size to
128, base learning rate to 10−3, weight decay to 5 × 10−4, and momentum to 0.9.
The weight of the CORAL loss (λ ) is set in such way that at the end of training
the classiﬁcation loss and CORAL loss are roughly the same. It seems be a reason-
able choice as we want to have a feature representation that is both discriminative
and also minimizes the distance between the source and target domains. We used
Caffe [16] and BVLC Reference CaffeNet for all of our experiments.

We compare to 7 recently published methods: CNN [17] (no adaptation), GFK [11],

SA [8], TCA [22], CORAL [25], DDC [30], DAN [19]. GFK, SA, and TCA are
manifold based methods that project the source and target distributions into a lower-
dimensional manifold and are not end-to-end deep methods. DDC adds a domain
confusion loss to AlexNet [17] and ﬁne-tunes it on both the source and target do-
main. DAN is similar to DDC but utilizes a multi-kernel selection method for better
mean embedding matching and adapts in multiple layers. For direct comparison,
DAN in this paper uses the hidden layer f c8. For GFK, SA, TCA, and CORAL, we
use the f c7 feature ﬁne-tuned on the source domain (FT 7 in [25]) as it achieves bet-
ter performance than generic pre-trained features, and train a linear SVM [8, 25]. To
have a fair comparison, we use accuracies reported by other authors with exactly the
same setting or conduct experiments using the source code provided by the authors.
From Table 2 we can see that Deep CORAL (D-CORAL) achieves better average
performance than CORAL and the other 6 baseline methods. In 3 out of 6 shifts, it

Correlation Alignment for Unsupervised Domain Adaptation

15

A→D A→W D→A D→W W→A W→D AVG
52.4±0.0 54.7±0.0 43.2±0.0 92.1±0.0 41.8±0.0 96.2±0.0 63.4
GFK
50.6±0.0 47.4±0.0 39.5±0.0 89.1±0.0 37.6±0.0 93.8±0.0 59.7
SA
46.8±0.0 45.5±0.0 36.4±0.0 81.1±0.0 39.5±0.0 92.2±0.0 56.9
TCA
65.7±0.0 64.3±0.0 48.5±0.0 96.1±0.0 48.2±0.0 99.8±0.0 70.4
CORAL
63.8±0.5 61.6±0.5 51.1±0.6 95.4±0.3 49.8±0.4 99.0±0.2 70.1
CNN
64.4±0.3 61.8±0.4 52.1±0.8 95.0±0.5 52.2±0.4 98.5±0.4 70.6
DDC
65.8±0.4 63.8±0.4 52.8±0.4 94.6±0.5 51.9±0.5 98.8±0.6 71.3
DAN
D-CORAL 66.8±0.6 66.4±0.4 52.8±0.2 95.7±0.3 51.5±0.3 99.2±0.1 72.1

Table 2 Object recognition accuracies for all 6 domain shifts on the standard Ofﬁce dataset with
deep features, following the standard unsupervised adaptation protocol.

Fig. 6 Detailed analysis of shift A→W for training w/ v.s. w/o CORAL loss. (a): training and test
accuracies for training w/ v.s. w/o CORAL loss. We can see that adding CORAL loss helps achieve
much better performance on the target domain while maintaining strong classiﬁcation accuracy on
the source domain. (b): classiﬁcation loss and CORAL loss for training w/ CORAL loss. As the
last fully connected layer is randomly initialized with N (0, 0.005), CORAL loss is very small
while classiﬁcation loss is very large at the beginning. After training for a few hundred iterations,
these two losses are about the same. (c): CORAL distance for training w/o CORAL loss (setting
the weight to 0). The distance is getting much larger ((cid:62) 100 times larger compared to training w/
CORAL loss).

achieves the highest accuracy. For the other 3 shifts, the margin between D-CORAL
and the best baseline method is very small ((cid:54) 0.7).

Domain Adaptation Equilibrium

To get a better understanding of Deep CORAL, we generate three plots for domain
shift A→W. In Figure 6(a) we show the training (source) and testing (target) accu-
racies for training with v.s. without CORAL loss. We can clearly see that adding
the CORAL loss helps achieve much better performance on the target domain while
maintaining strong classiﬁcation accuracy on the source domain.

In Figure 6(b) we visualize both the classiﬁcation loss and the CORAL loss for
training w/ CORAL loss. As the last fully connected layer is randomly initialized
with N (0, 0.005), in the beginning the CORAL loss is very small while the clas-
siﬁcation loss is very large. After training for a few hundred iterations, these two

16

Baochen Sun, Jiashi Feng, and Kate Saenko

losses are about the same and reach an equilibrium. In Figure 6(c) we show the
CORAL distance between the domains for training w/o CORAL loss (setting the
weight to 0). We can see that the distance is getting much larger ((cid:62) 100 times larger
compared to training w/ CORAL loss). Comparing Figure 6(b) and Figure 6(c), we
can see that even though the CORAL loss is not always decreasing during training,
if we set its weight to 0, the distance between source and target domains becomes
much larger. This is reasonable as ﬁne-tuning without domain adaptation is likely
to overﬁt the features to the source domain. Our CORAL loss constrains the dis-
tance between source and target domain during the ﬁne-tuning process and helps to
maintain an equilibrium where the ﬁnal features work well on the target domain.

5.2 Object Detection

Following protocol of [10], we conduct object detection experiment on the Ofﬁce
dataset [23] with HOG features. We use the same setting as [10], performing detec-
tion on the Webcam domain as the target (test) domain, and evaluating on the same
783 image test set of 20 categories (out of 31). As source (training) domains, we
use: the two remaining real-image domains in Ofﬁce, Amazon and DSLR, and two
domains that contain virtual images only, Virtual and Virtual-Gray, generated from
3d CAD models. The inclusion of the two virtual domains is to reduce human effort
in annotation and facilitate future research [24]. Examples of Virtual and Virtual-
Gray are shown in Figure 8. Please refer to [26] for detailed explanation of the data
generation process. We also compare to [10] who use corresponding ImageNet [3]
synsets as the source. Thus, there are four potential source domains (two synthetic
and three real) and one (real) target domain. The number of positive training images
per category in each domain is shown in Table 3. Figure 7 shows an overview of our
evaluation.

Fig. 7 Overview of our evaluation on object detection. With CORAL-Linear Discriminant Analy-
sis (LDA), we show that a strong object detector can be trained from virtual data only.

Correlation Alignment for Unsupervised Domain Adaptation

17

(a)

(b)

Fig. 8 Two sets of virtual images used in this section: (a) Virtual: background and texturemap from
a random real ImageNet image; (b) Virtual-Gray: uniform gray texturemap and white background.
Please refer to [26] for detailed explanation of the data generation process.

Domain

# Training sample

Amazon
DSLR
Virtual(-Gray)
ImageNet

20
8
30
150-2000

Table 3 # training examples for each source domain.

Effect of Mismatched Image Statistics

First, we explore the effect of mismatched precomputed image statistics on detection
performance. For each source domain, we train CORAL-LDA detectors using the
positive mean from the source, and pair it with the covariance and negative mean
of other domains. The virtual and the Ofﬁce domains are used as sources, and the
test domain is always Webcam. The statistics for each of the four domains were
calculated using all of the training data, following the same approach as [14]. The
pre-computed statistics of 10,000 real images from PASCAL, as proposed in [14,
10], are also evaluated.

Detection performance, measured in Mean Average Precision (MAP), is shown
in Table 4. We also calculate the normalized Euclidean distance between pairs of
domains as ((cid:107)C1 − C2(cid:107))/((cid:107)C1(cid:107) + (cid:107)C2(cid:107)) + ((cid:107)µ 1
0 (cid:107)), and show
the average distance between source and target in parentheses in Table 4.

0 (cid:107))/((cid:107)µ 1

0 (cid:107) + (cid:107)µ 2

0 − µ 2

From these results we can see a trend that larger domain difference leads to
poorer performance. Note that larger difference to the target domain also leads to
lower performance, conﬁrming our hypothesis that both source and target statistics
matter. Some of the variation could also stem from our assumption about the differ-
ence of means being the same not quite holding true. Finally, the PASCAL statistics
from [14] perform the worst. Thus, in practice, statistics from either source domain
or target domain or domains close to them could be used. However, unrelated statis-
tics will not work even though they might be estimated from a very large amount of
data as [14].

18

Baochen Sun, Jiashi Feng, and Kate Saenko

30.8 (0.1)
Virtual
Virtual-Gray 32.3 (0.6)
39.9 (0.4)
Amazon
68.2 (0.2)
DSLR

Virtual Virtual-Gray Amazon DSLR PASCAL
24.1 (0.6) 28.3 (0.2) 10.7 (0.5)
27.3 (0.8) 32.7 (0.6) 17.9 (0.7)
39.2 (0.4) 37.9 (0.4) 18.6 (0.6)
68.1 (0.6) 66.5 (0.1) 37.7 (0.5)

16.5 (1.0)
32.3 (0.5)
30.0 (1.0)
62.1 (1.0)

Table 4 MAP of CORAL-LDA trained on positive examples from each row’s source domain and
background statistics from each column’s domain. The average distance between each set of back-
ground statistics and the true source and target statistics is shown in parentheses.

Unsupervised and Semi-supervised Adaptation

Next, we report the results of our unsupervised and semi-supervised adaptation tech-
nique. We use the same setting as [10], in which three positive and nine negative
labeled images per category were used for semi-supervised adaptation. Target co-
variance in Equation 6 is estimated from 305 unlabeled training examples. We also
followed the same approach to learn a linear combination between the unsupervised
and supervised model via cross-validation. The results are presented in Table 5.
Please note that our target-only MAP is 52.9 compared to 36.6 in [10]. This also
conﬁrms our conclusion that the statistics should come from a related domain. It is
clear that both of our unsupervised and semi-supervised adaptation techniques out-
perform the method in [10]. Furthermore, Virtual-Gray data outperforms Virtual,
and DSLR does best, as it is very close to the target domain (the main difference
between DLSR and Webcam domains is in the camera used to capture images).

Finally, we compare our method trained on Virtual-Gray to the results of adapt-
ing from ImageNet reported by [10], in Figure 9. While their unsupervised models
are learned from 150-2000 real ImageNet images per category and the background
statistics are estimated from 10,000 PASCAL images, we only have 30 virtual im-
ages per category and the background statistics is learned from about 1,000 images.
What’s more, all the virtual images used are with uniform gray texturemap and white
background. This clearly demonstrates the importance of domain-speciﬁc decorre-
lation, and shows that the there is no need to collect a large amount of real images
to train a good classiﬁer.

Fig. 9 Comparison of unsupervised and semi-supervised adaptation of virtual detectors using our
method with the results of training on ImageNet and supervised adaptation from ImageNet reported
in [10]. Our semi-supervised adapted detectors achieve comparable performance despite not using
any real source training data, and using only 3 positive images for adaptation, and even outperform
ImageNet signiﬁcantly for several categories (e.g., ruler).

Correlation Alignment for Unsupervised Domain Adaptation

19

Source
Virtual
Virtual-Gray
Amazon
DSLR

Source-only [14] Unsup-Ours SemiSup [10] SemiSup-Ours

10.7
17.9
18.6
37.7

27.9
33.0
38.9
67.1

30.7
35.0
35.8
42.9

45.2
54.7
53.0
71.4

Table 5 Top: Comparison of the source-only [14] and semi-supervised adapted model of [10]
with our unsupervised-adapted and semi-supervised adapted models. Target domain is Webcam.
Mean AP across categories is reported on the Webcam test data, using different source domains for
training. Bottom: Sample detections of the DSLR-UnsupAdapt-Ours detectors.

6 Conclusion

In this chapter, we described a simple, effective, and efﬁcient method for unsu-
pervised domain adaptation called CORrelation ALignment (CORAL). CORAL
minimizes domain shift by aligning the second-order statistics of source and target
distributions, without requiring any target labels. We also developed novel domain
adaptation algorithms by applying the idea of CORAL to three different scenarios.
In the ﬁrst scenario, we applied a linear transformation that minimizes the CORAL
objective to source features prior to training the classiﬁer. In the case of linear classi-
ﬁers, we equivalently applied the linear CORAL transform to the classiﬁer weights,
signiﬁcantly improving efﬁciency and classiﬁcation accuracy over standard LDA
on several domain adaptation benchmarks. We further extended CORAL to learn
a nonlinear transformation that aligns correlations of layer activations in deep neu-
ral networks. The resulting Deep CORAL approach works seamlessly with deep
networks and can be integrated into any arbitrary network architecture to enable
end-to-end unsupervised adaptation. One limitation of CORAL is that it captures
second-order statistics only and may not preserve higher-order structure in the data.
However, as demonstrated in this chapter, it works fairly well in practice, and can
also potentially be combined with other domain-alignment loss functions.

References

1. Jian-Feng Cai, Emmanuel J. Cand`es, and Zuowei Shen. A singular value thresholding algo-

rithm for matrix completion. SIAM J. on Optimization, 20(4):1956–1982, March 2010.

2. H. Daume III. Frustratingly easy domain adaptation. In ACL, 2007.
3. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale

hierarchical image database. In CVPR, 2009.

20

Baochen Sun, Jiashi Feng, and Kate Saenko

4. Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and
Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition.
In ICML, 2014.

5. L. Duan, I. W. Tsang, D. Xu, and T. Chua. Domain adaptation from multiple sources via

6. Lixin Duan, Ivor W. Tsang, and Dong Xu. Domain transfer multiple kernel learning. TPAMI,

auxiliary classiﬁers. In ICML, 2009.

34(3):465–479, March 2012.

7. Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object de-
tection with discriminatively trained part-based models. TPAMI, 32(9):1627–1645, 2010.
8. Basura Fernando, Amaury Habrard, Marc Sebban, and Tinne Tuytelaars. Unsupervised visual

domain adaptation using subspace alignment. In ICCV, 2013.

9. Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation.

In ICML, 2015.

10. Daniel Goehring, Judy Hoffman, Erik Rodner, Kate Saenko, and Trevor Darrell. Interactive

adaptation of real-time object detectors. In ICRA, 2014.

11. B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic ﬂow kernel for unsupervised domain

12. R. Gopalan, R. Li, and R. Chellappa. Domain adaptation for object recognition: An unsuper-

adaptation. In CVPR, 2012.

vised approach. In ICCV, 2011.

13. Maayan Harel and Shie Mannor. Learning from multiple outlooks. In ICML, 2011.
14. Bharath Hariharan, Jitendra Malik, and Deva Ramanan. Discriminative decorrelation for clus-

tering and classiﬁcation. In ECCV. 2012.

15. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training

by reducing internal covariate shift. In ICML, 2015.

16. Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick,
Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature
embedding. arXiv preprint, 2014.

17. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep

convolutional neural networks. In NIPS, 2012.

18. B. Kulis, K. Saenko, and T. Darrell. What you saw is not what you get: Domain adaptation

using asymmetric kernel transforms. In CVPR, 2011.

19. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable

features with deep adaptation networks. In ICML, 2015.

20. Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and P.S. Yu. Transfer joint

matching for unsupervised domain adaptation. In CVPR, 2014.

21. Tomasz Malisiewicz, Abhinav Gupta, and Alexei A Efros. Ensemble of exemplar-svms for

22. Sinno Jialin Pan, Ivor W. Tsang, James T. Kwok, and Qiang Yang. Domain adaptation via

object detection and beyond. In ICCV, 2011.

transfer component analysis. In IJCAI, 2009.

23. Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models

24. Baochen Sun. Correlation Alignment for Domain Adaptation. PhD thesis, University of

25. Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation.

26. Baochen Sun, Xingchao Peng, and Kate Saenko. Generating large scale image datasets from

3d cad models. In CVPR’15 Workshop on The Future of Datasets in Vision, 2015.

27. Baochen Sun and Kate Saenko. From virtual to reality: Fast adaptation of virtual object de-

tectors to real domains. In BMVC, 2014.

28. Baochen Sun and Kate Saenko. Subspace distribution alignment for unsupervised domain

29. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation.

adaptation. In BMVC, 2015.

In ECCV 2016 Workshops, 2016.

30. Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain

confusion: Maximizing for domain invariance. CoRR, abs/1412.3474, 2014.

to new domains. In ECCV, 2010.

Massachusetts Lowell, 2016.

In AAAI, 2016.

