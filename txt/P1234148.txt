The One Hundred Layers Tiramisu:
Fully Convolutional DenseNets for Semantic Segmentation

Simon J´egou1 Michal Drozdzal2,3 David Vazquez1,4 Adriana Romero1 Yoshua Bengio1
1Montreal Institute for Learning Algorithms 2 ´Ecole Polytechnique de Montr´eal
3Imagia Inc., Montr´eal, 4Computer Vision Center, Barcelona
simon.jegou@gmail.com, michal@imagia.com, dvazquez@cvc.uab.es,

adriana.romero.soriano@umontreal.ca, yoshua.umontreal@gmail.com

7
1
0
2
 
t
c
O
 
1
3
 
 
]

V
C
.
s
c
[
 
 
3
v
6
2
3
9
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

State-of-the-art approaches for semantic image segmen-
tation are built on Convolutional Neural Networks (CNNs).
The typical segmentation architecture is composed of (a)
a downsampling path responsible for extracting coarse se-
mantic features, followed by (b) an upsampling path trained
to recover the input image resolution at the output of the
model and, optionally, (c) a post-processing module (e.g.
Conditional Random Fields) to reﬁne the model predictions.
Recently, a new CNN architecture, Densely Connected
Convolutional Networks (DenseNets), has shown excellent
results on image classiﬁcation tasks. The idea of DenseNets
is based on the observation that if each layer is directly con-
nected to every other layer in a feed-forward fashion then
the network will be more accurate and easier to train.

In this paper, we extend DenseNets to deal with the prob-
lem of semantic segmentation. We achieve state-of-the-art
results on urban scene benchmark datasets such as CamVid
and Gatech, without any further post-processing module
nor pretraining. Moreover, due to smart construction of
the model, our approach has much less parameters than
currently published best entries for these datasets. Code
to reproduce the experiments is publicly available here :
https://github.com/SimJeg/FC-DenseNet

1. Introduction

Convolutional Neural Networks (CNNs) are driving ma-
jor advances in many computer vision tasks, such as im-
age classiﬁcation [29], object detection [25, 24] and seman-
tic image segmentation [20]. The last few years have wit-
nessed outstanding improvements on CNN-based models.
Very deep architectures [29, 11, 31] have shown impres-
sive results on standard benchmarks such as ImageNet [6]
or MSCOCO [19]. State-of-the-art CNNs heavily reduce
the input resolution through successive pooling layers and,

1

Figure 1. Diagram of our architecture for semantic segmentation.
Our architecture is built from dense blocks. The diagram is com-
posed of a downsampling path with 2 Transitions Down (TD) and
an upsampling path with 2 Transitions Up (TU). A circle repre-
sents concatenation and arrows represent connectivity patterns in
the network. Gray horizontal arrows represent skip connections,
the feature maps from the downsampling path are concatenated
with the corresponding feature maps in the upsampling path. Note
that the connectivity pattern in the upsampling and the downsam-
pling paths are different. In the downsampling path, the input to
a dense block is concatenated with its output, leading to a linear
growth of the number of feature maps, whereas in the upsampling
path, it is not.

thus, are well suited for applications where a single predic-
tion per input image is expected (e.g. image classiﬁcation
task).

Fully Convolutional Networks (FCNs) [20, 27] were in-
troduced in the literature as a natural extension of CNNs to
tackle per pixel prediction problems such as semantic im-
age segmentation. FCNs add upsampling layers to standard
CNNs to recover the spatial resolution of the input at the
output layer. As a consequence, FCNs can process images
of arbitrary size.
In order to compensate for the resolu-
tion loss induced by pooling layers, FCNs introduce skip
connections between their downsampling and upsampling
paths. Skip connections help the upsampling path recover
ﬁne-grained information from the downsampling layers.

Among CNN architectures extended as FCNs for seman-
tic segmentation purposes, Residual Networks (ResNets)
[11] make an interesting case. ResNets are designed to
ease the training of very deep networks (of hundreds of
layers) by introducing a residual block that sums two sig-
nals: a non-linear transformation of the input and its identity
mapping. The identity mapping is implemented by means
of a shortcut connection. ResNets have been extended to
work as FCNs [4, 8] yielding very good results in differ-
ent segmentation benchmarks. ResNets incorporate addi-
tional paths to FCN (shortcut paths) and, thus, increase the
number of connections within a segmentation network. This
additional shortcut paths have been shown not only to im-
prove the segmentation accuracy but also to help the net-
work optimization process, resulting in faster convergence
of the training [8].

Recently, a new CNN architecture, called DenseNet, was
introduced in [13]. DenseNets are built from dense blocks
and pooling operations, where each dense block is an itera-
tive concatenation of previous feature maps. This architec-
ture can be seen as an extension of ResNets [11], which per-
forms iterative summation of previous feature maps. How-
ever, this small modiﬁcation has some interesting implica-
tions: (1) parameter efﬁciency, DenseNets are more efﬁ-
cient in the parameter usage; (2) implicit deep supervision,
DenseNets perform deep supervision thanks to short paths
to all feature maps in the architecture (similar to Deeply
Supervised Networks [18]); and (3) feature reuse, all lay-
ers can easily access their preceding layers making it easy
to reuse the information from previously computed feature
maps. The characteristics of DenseNets make them a very
good ﬁt for semantic segmentation as they naturally induce
skip connections and multi-scale supervision.

In this paper, we extend DenseNets to work as FCNs by
adding an upsampling path to recover the full input reso-
lution. Naively building an upsampling path would result
in a computationally intractable number of feature maps
with very high resolution prior to the softmax layer. This
is because one would multiply the high resolution feature

maps with a large number of input ﬁlters (from all the lay-
ers below), resulting in both very large amount of compu-
tation and number of parameters. In order to mitigate this
effect, we only upsample the feature maps created by the
preceding dense block. Doing so allows to have a number of
dense blocks at each resolution of the upsampling path inde-
pendent of the number of pooling layers. Moreover, given
the network architecture, the upsampled dense block com-
bines the information contained in the other dense blocks
of the same resolution. The higher resolution information
is passed by means of a standard skip connection between
the downsampling and the upsampling paths. The details of
the proposed architecture are shown in Figure 1. We eval-
uate our model on two challenging benchmarks for urban
scene understanding, Camvid [2] and Gatech [22], and con-
ﬁrm the potential of DenseNets for semantic segmentation
by improving the state-of-the-art.

Thus, the contributions of the paper can be summarized

as follows:

• We carefully extend the DenseNet architecture [13] to
fully convolutional networks for semantic segmenta-
tion, while mitigating the feature map explosion.

• We highlight that the proposed upsampling path, built
from dense blocks, performs better than upsampling
path with more standard operations, such as the ones
in [27].

• We show that such a network can outperform current
state-of-the-art results on standard benchmarks for ur-
ban scene understanding without neither using pre-
trained parameters nor any further post-processing.

2. Related Work

Recent advances in semantic segmentation have been de-
voted to improve architectural designs by (1) improving
the upsampling path and increasing the connectivity within
FCNs [27, 1, 21, 8]; (2) introducing modules to account for
broader context understanding [36, 5, 37]; and/or (3) en-
dowing FCN architectures with the ability to provide struc-
tured outputs [16, 5, 38].

First, different alternatives have been proposed in the lit-
erature to address the resolution recovery in FCN’s upsam-
pling path; from simple bilinear interpolation [10, 20, 1]
to more sophisticated operators such as unpooling [1, 21]
or transposed convolutions [20]. Skip connections from
the downsampling to the upsampling path have also been
adopted to allow for a ﬁner information recovery [27]. More
recently, [8] presented a thorough analysis on the combina-
tion of identity mapping [11] and long skip connections [27]
for semantic segmentation.

Second, approaches that introduce larger context to se-
mantic segmentation networks include [10, 36, 5, 37]. In

3. Fully Convolutional DenseNets

As mentioned in Section 1, FCNs are built from a down-
sampling path, an upsampling path and skip connections.
Skip connections help the upsampling path recover spa-
tially detailed information from the downsampling path, by
reusing features maps. The goal of our model is to further
exploit the feature reuse by extending the more sophisti-
cated DenseNet architecture, while avoiding the feature ex-
plosion at the upsampling path of the network.

In this section, we detail the proposed model for seman-
tic segmentation. First, we review the recently proposed
DenseNet architecture. Second, we introduce the construc-
tion of the novel upsampling path and discuss its advantages
w.r.t. a naive DenseNet extension and more classical archi-
tectures. Finally, we wrap up with the details of the main
architecture used in Section 4.

3.1. Review of DenseNets

Let x(cid:96) be the output of the (cid:96)th layer. In a standard CNN,
x(cid:96) is computed by applying a non-linear transformation H(cid:96)
to the output of the previous layer x(cid:96)−1

x(cid:96) = H(cid:96)(x(cid:96)−1),

(1)

where H is commonly deﬁned as a convolution followed by
a rectiﬁer non-linearity (ReLU) and often dropout [30].

In order to ease the training of very deep networks,
ResNets [11] introduce a residual block that sums the iden-
tity mapping of the input to the output of a layer. The re-
sulting output x(cid:96) becomes

x(cid:96) = H(cid:96)(x(cid:96)−1) + x(cid:96)−1,

(2)

allowing for the reuse of features and permitting the gra-
dient to ﬂow directly to earlier layers. In this case, H is
deﬁned as the repetition (2 or 3 times) of a block composed
of Batch Normalization (BN) [14], followed by ReLU and
a convolution.

Pushing this idea further, DenseNets [13] design a more
sophisticated connectivity pattern that iteratively concate-
nates all feature outputs in a feedforward fashion. Thus, the
output of the (cid:96)th layer is deﬁned as

x(cid:96) = H(cid:96)([x(cid:96)−1, x(cid:96)−2, ..., x0]),

(3)

where [ ... ] represents the concatenation operation. In this
case, H is deﬁned as BN, followed by ReLU, a convolution
and dropout. Such connectivity pattern strongly encourages
the reuse of features and makes all layers in the architec-
ture receive direct supervision signal. The output dimension
of each layer (cid:96) has k feature maps where k, hereafter re-
ferred as to growth rate parameter, is typically set to a small
value (e.g. k = 12). Thus, the number of feature maps in
DenseNets grows linearly with the depth (e.g. after (cid:96) layers,
the input [x(cid:96)−1, x(cid:96)−2, ..., x0] will have (cid:96) × k feature maps).

Figure 2. Diagram of a dense block of 4 layers. A ﬁrst layer is ap-
plied to the input to create k feature maps, which are concatenated
to the input. A second layer is then applied to create another k
features maps, which are again concatenated to the previous fea-
ture maps. The operation is repeated 4 times. The output of the
block is the concatenation of the outputs of the 4 layers, and thus
contains 4 ∗ k feature maps

[10], an unsupervised global image descriptor is computed
added to the feature maps for each pixel. In [36], Recur-
rent Neural Networks (RNNs) are used to retrieve contex-
tual information by sweeping the image horizontally and
vertically in both directions.
In [5], dilated convolutions
are introduced as an alternative to late CNN pooling layers
to capture larger context without reducing the image reso-
lution. Following the same spirit, [37] propose to provide
FCNs with a context module built as a stack of dilated con-
volutional layers to enlarge the ﬁeld of view of the network.

Third, Conditional Random Fields (CRF) have long been
a popular choice to enforce structure consistency to seg-
mentation outputs. More recently, fully connected CRFs
[16] have been used to include structural properties of the
output of FCNs [5]. Interestingly, in [38], RNN have been
introduced to approximate mean-ﬁeld iterations of CRF op-
timization, allowing for an end-to-end training of both the
FCN and the RNN.

Finally, it is worth noting that current state-of-the-art
FCN architectures for semantic segmentation often rely on
pre-trained models (e.g. VGG [29] or ResNet101 [11]) to
improve their segmentation results [20, 1, 4].

A transition down is introduced to reduce the spatial di-
mensionality of the feature maps. Such transformation is
composed of a 1×1 convolution (which conserves the num-
ber of feature maps) followed by a 2 × 2 pooling operation.
In the remainder of the article, we will call dense block
the concatenation of the new feature maps created at a given
resolution. Figure 2 shows an example of dense block con-
struction. Starting from an input x0 (input image or output
of a transition down) with m feature maps, the ﬁrst layer of
the block generates an output x1 of dimension k by applying
H1(x0). These k feature maps are then stacked to the pre-
vious m feature maps by concatenation ([x1, x0]) and used
as input to the second layer. The same operation is repeated
n times, leading to a new dense block with n × k feature
maps.

3.2. From DenseNets

to Fully Convolutional

DenseNets

The DenseNet architecture described in Subsection 3.1
constitutes the downsampling path of our Fully Convolu-
tional DenseNet (FC-DenseNet). Note that, in the down-
sampling path, the linear growth in the number of features
is compensated by the reduction in spatial resolution of each
feature map after the pooling operation. The last layer of the
downsampling path is referred to as bottleneck.

In order to recover the input spatial resolution, FCNs in-
troduce an upsampling path composed of convolution, up-
sampling operations (transposed convolutions or unpooling
operations) and skip connections.
In FC-DenseNets, we
substitute the convolution operation by a dense block and
an upsampling operation referred to as transition up. Tran-
sition up modules consist of a transposed convolution that
upsamples the previous feature maps. The upsampled fea-
ture maps are then concatenated to the ones coming from
the skip connection to form the input of a new dense block.
Since the upsampling path increases the feature maps spa-
tial resolution, the linear growth in the number of features
would be too memory demanding, especially for the full
resolution features in the pre-softmax layer.

In order to overcome this limitation, the input of a dense
block is not concatenated with its output. Thus, the trans-
posed convolution is applied only to the feature maps ob-
tained by the last dense block and not to all feature maps
concatenated so far. The last dense block summarizes the
information contained in all the previous dense blocks at
the same resolution. Note that some information from ear-
lier dense blocks is lost in the transition down due to the
pooling operation. Nevertheless, this information is avail-
able in the downsampling path of the network and can be
passed via skip connections. Hence, the dense blocks of the
upsampling path are computed using all the available fea-
ture maps at a given resolution. Figure 1 illustrates this idea
in detail.

Therefore, our upsampling path approach allows us to
build very deep FC-DenseNets without a feature map explo-
sion. An alternative way of implementing the upsampling
path would be to perform consecutive transposed convolu-
tions and complement them with skip connections from the
downsampling path in a U-Net [27] or FCN-like [20] fash-
ion. This will be further discussed in Section 4

3.3. Semantic Segmentation Architecture

In this subsection, we detail the main architecture, FC-

DenseNet103, used in Section 4.

First, in Table 1, we deﬁne the dense block layer, tran-
sition down and transition up of the architecture. Dense
block layers are composed of BN, followed by ReLU, a
3 × 3 same convolution (no resolution loss) and dropout
with probability p = 0.2. The growth rate of the layer is set
to k = 16. Transition down is composed of BN, followed
by ReLU, a 1 × 1 convolution, dropout with p = 0.2 and a
non-overlapping max pooling of size 2 × 2. Transition up
is composed of a 3 × 3 transposed convolution with stride 2
to compensate for the pooling operation.

Second, in Table 2, we summarize all Dense103 layers.
This architecture is built from 103 convolutional layers : a
ﬁrst one on the input, 38 in the downsampling path, 15 in
the bottleneck and 38 in the upsampling path. We use 5
Transition Down (TD), each one containing one extra con-
volution, and 5 Transition Up (TU), each one containing a
transposed convolution. The ﬁnal layer in the network is a
1 × 1 convolution followed by a softmax non-linearity to
provide the per class distribution at each pixel.

It is worth noting that, as discussed in Subsection 3.2, the
proposed upsampling path properly mitigates the DenseNet
feature map explosion, leading to reasonable pre-softmax
feature map number of 256.

Finally, the model is trained by minimizing the pixel-

wise cross-entropy loss.

4. Experiments

We evaluate our method on two urban scene understand-
ing datasets: CamVid [2], and Gatech [22]. We trained our
models from scratch without using any extra-data nor post-
processing module. We report the results using the Inter-
section over Union (IoU) metric and the global accuracy
(pixel-wise accuracy on the dataset). For a given class c,
predictions (oi) and targets (yi), the IoU is deﬁned by

IoU (c) =

(cid:80)
(cid:80)

i (oi == c ∧ yi == c)
i (oi == c ∨ yi == c)

,

(4)

where ∧ is a logical and operation, while ∨ is a logical or
operation. We compute IoU by summing over all the pixels
i of the dataset.

Layer
Batch Normalization
ReLU
3 × 3 Convolution
Dropout p = 0.2

Transition Down (TD)
Batch Normalization
ReLU
1 × 1 Convolution
Dropout p = 0.2
2 × 2 Max Pooling

Transition Up (TU)
3 × 3 Transposed Convolution
stride = 2

Table 1. Building blocks of fully convolutional DenseNets. From left to right: layer used in the model, Transition Down (TD) and Transition
Up (TU). See text for details.

4.1. Architecture and training details

4.2. CamVid dataset

We initialize our models using HeUniform [12] and train
them with RMSprop [33], with an initial learning rate of
1e − 3 and an exponential decay of 0.995 after each epoch.
All models are trained on data augmented with random
crops and vertical ﬂips. For all experiments, we ﬁnetune
our models with full size images and learning rate of 1e − 4.
We use validation set to earlystop the training and the ﬁne-
tuning. We monitor mean IoU or mean accuracy and use
patience of 100 (50 during ﬁnetuning).

We regularized our models with a weight decay of 1e−4
and a dropout rate of 0.2. For batch normalization, we use
current batch statistics at training, validation and test time.

Architecture
Input, m = 3
3 × 3 Convolution, m = 48
DB (4 layers) + TD, m = 112
DB (5 layers) + TD, m = 192
DB (7 layers) + TD, m = 304
DB (10 layers) + TD, m = 464
DB (12 layers) + TD, m = 656
DB (15 layers), m = 896
TU + DB (12 layers), m = 1088
TU + DB (10 layers), m = 816
TU + DB (7 layers), m = 578
TU + DB (5 layers), m = 384
TU + DB (4 layers), m = 256
1 × 1 Convolution, m = c
Softmax

Table 2. Architecture details of FC-DenseNet103 model used in
our experiments. This model is built from 103 convolutional lay-
ers. In the Table we use following notations: DB stands for Dense
Block, TD stands for Transition Down, TU stands for Transition
Up, BN stands for Batch Normalization and m corresponds to the
total number of feature maps at the end of a block. c stands for the
number of classes.

CamVid1 [2] is a dataset of fully segmented videos for
urban scene understanding. We used the split and image
resolution from [1], which consists of 367 frames for train-
ing, 101 frames for validation and 233 frames for test. Each
frame has a size 360 × 480 and its pixels are labeled with
11 semantic classes. Our models were trained with crops
of 224 × 224 and batch size 3. At the end, the model is
ﬁnetuned with full size images.

In Table 3, we report our results for three networks with
respectively (1) 56 layers (FC-DenseNet56), with 4 layers
per dense block and a growth rate of 12; (2) 67 layers (FC-
DenseNet67) with 5 layers per dense block and a growth
rate of 16; and (3) 103 layers (FC-DenseNet103) with a
growth rate k = 16 (see Table 2 for details). We also trained
an architecture using standard convolutions in the upsam-
pling path instead of dense blocks (Classic Upsampling). In
the latter architecture, we used 3 convolutions per resolution
level with respectively 512, 256, 128, 128 and 64 ﬁlters, as
in [27]. Results show clear superiority of the proposed up-
sampling path w.r.t.
the classic one, consistently improv-
ing the IoU signiﬁcantly for all classes. Particularly, we
observe that unrepresented classes beneﬁt notably from the
FC-DenseNet architecture, namely sign, pedestrian, fence,
cyclist experience a crucial boost in performance (ranging
from 15% to 25%).

As expected, when comparing FC-DenseNet56 or FC-
DenseNet67 to FC-DenseNet103, we see that the model
beneﬁts from having more depth as well as more parame-
ters.

When compared to other methods, we show that FC-
DenseNet architectures achieve state-of-the-art, improving
upon models with 10 times more parameters. It is worth
mentioning that our small model FC-DenseNet56 already
outperforms popular architectures with at least 100 times
more parameters.

It is worth noting that images in CamVid correspond to
video frames and, thus, the dataset contains temporal infor-
mation. Some state-of-the-art methods such as [17] incor-
porate long range spatio-temporal regularization to the out-

1http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/

put of a FCN to boost their performance. Our model is able
to outperform such state-of-the-art model, without requir-
ing any temporal smoothing. However, any post-processing
temporal regularization is complementary to our approach
and could bring additional improvements.

Unlike most of the current state-of-the-art methods, FC-
DenseNets have not been pretrained on large datasets such
as ImageNet [6] and could most likely beneﬁt from such
pretraining. More recently, it has been shown that deep net-
works can also boost their performance when pretrained on
data other than natural images, such as video games [26, 28]
or clipart [3], and this an interesting direction to explore.

Figure 3 shows some qualitative segmentation results on
the CamVid dataset. Qualitative results are well aligned
with the quantitative ones, showing sharp segmentations
that account for a lot of details. For example, trees, column
poles, sidewalk and pedestrians appear very well sketched.
Among common errors, we ﬁnd that thin details found in
trees can be confused with column poles (see ﬁfth row),
buses and trucks can be confused with buildings (fourth
row), and shop signs can be confused with road signs (sec-
ond row).

4.3. Gatech dataset

Gatech2 [23] is a geometric scene understanding dataset,
which consists of 63 videos for training/validation and 38
for testing. Each video has between 50 and 300 frames
(with an average of 190). A pixel-wise segmentation map is
provided for each frame. There are 8 classes in the dataset:
sky, ground, buildings, porous (mainly trees), humans, cars,
vertical mix and main mix. The dataset was originally built
to learn 3D geometric structure of outdoor video scenes and
the standard metric for this dataset is mean global accuracy.
We used the FC-DenseNet103 model pretrained on
CamVid, removed the softmax layer, and ﬁnetuned it for
10 epochs with crops of 224 × 224 and batch size 5. Given
the high redundancy in Gatech frames, we used only one
out of 10 frames to train the model and tested it on all full
resolution test set frames.

In Table 4, we report the obtained results. We compare
the results to the recently proposed method for video seg-
mentation of [34], which reports results of their architecture
with 2D and 3D convolutions. Frame-based 2D convolu-
tions do not have temporal information. As it can be seen
in Table 4, our method gives an impressive improvement of
23.7% in global accuracy with respect to previously pub-
lished state-of-the-art with 2D convolutions. Moreover, our
model (trained with only 2D convolutions) also achieves a
signiﬁcant improvement over state-of-the-art models based
on spatio-temporal 3D convolutions (3.4% improvement).

2http://www.cc.gatech.edu/cpl/projects/videogeometriccontext/

5. Discussion

Our fully convolutional DenseNet

implicitly inherits
the advantages of DenseNets, namely: (1) parameter ef-
ﬁciency, as our network has substantially less parame-
ters than other segmentation architectures published for the
Camvid dataset; (2) implicit deep supervision, we tried in-
cluding additional levels of supervision to different layers of
our network without noticeable change in performance; and
(3) feature reuse, as all layers can easily access their pre-
ceding layers not only due to the iterative concatenation of
feature maps in a dense block but also thanks to skip con-
nections that enforce connectivity between downsampling
and upsampling path.

Recent evidence suggest that ResNets behave like en-
semble of relatively shallow networks [35]: ”Residual net-
works avoid the vanishing gradient problem by introducing
short paths which can carry gradient throughout the extent
of very deep networks”. It would be interesting to revisit
this ﬁnding in the context of fully convolutional DenseNets.
Due to iterative feature map concatenation in the dense
block, the gradients are forced to be passed through net-
works of different depth (with different numbers of non-
linearities). Thus, thanks to the smart connectivity pat-
terns, FC-DenseNets might represent an ensemble of vari-
able depth networks. This particular ensemble behavior
would be very interesting for semantic segmentation mod-
els, where the ensemble of different paths throughout the
model would capture the multi-scale appearance of objects
in urban scene.

6. Conclusion

In this paper, we have extended DenseNets and made
them fully convolutional to tackle the problem semantic im-
age segmentation. The main idea behind DenseNets is cap-
tured in dense blocks that perform iterative concatenation
of feature maps. We designed an upsampling path mitigat-
ing the linear growth of feature maps that would appear in
a naive extension of DenseNets.

The resulting network is very deep (from 56 to 103 lay-
ers) and has very few parameters, about 10 fold reduction
w.r.t. state-of-the-art models. Moreover, it improves state-
of-the-art performance on challenging urban scene under-
standing datasets (CamVid and Gatech), without neither ad-
ditional post-processing, pretraining, nor including tempo-
ral information.

Aknowledgements

The authors would like to thank the developers of
Theano [32] and Lasagne [7]. Special thanks to Fr´ed´eric
Bastien for his work assessing the compilation issues.
Thanks to Francesco Visin for his well designed data-
loader [9], as well as Harm de Vries for his support

)

M

(

s
r
e
t
e
m
a
r
a
p

#

d
e
n
i
a
r
t
e
r
P

g
n
i
d
l
i
u
B

e
e
r
T

y
k
S

r
a
C

n
g
i

S

n
a
i
r
t
s
e
d
e
P

e
c
n
e
F

e
l
o
P

k
l
a
w
e
d
i

S

t
s
i
l
c
y
C

68.7

52.0

87.0

58.5

13.4

25.3

17.9

16.0

60.5

24.8

77.8
81.5
82.6
84.0

73.5
77.6
80.2
83.0

71.0
74.6
76.2
77.2

72.2
72.0
75.4
77.3

88.7
89.0
89.0
91.3

92.4
92.4
93.0
93.0

76.1
82.2
84.0
85.6

66.2
73.2
78.2
77.3

32.7
42.3
46.9
49.9

26.9
31.8
40.9
43.9

41.7
48.4
56.3
59.1

37.7
37.9
58.4
59.6

24.4
27.2
35.8
37.6

22.7
26.2
30.7
37.1

19.9
14.3
23.4
16.9

30.8
32.6
38.4
37.8

72.7
75.4
75.3
76.0

69.6
79.9
81.9
82.2

31.0
50.1
55.5
57.2

25.1
31.1
52.1
50.5

Table 3. Results on CamVid dataset. Note that we trained our own pretrained FCN8 model

d
a
o
R

86.2
n/a
n/a
n/a
91.2
92.2
92.2
92.5

90.0
92.8
94.7
94.5

Model

SegNet [1]

DeconvNet [21]
Visin et al. [36]
FCN8 [20]
DeepLab-LFOV [5]
Dilation8 [37]
Dilation8 + FSO [17]

! 29.5
Bayesian SegNet [15] ! 29.5
! 252
! 32.3
! 134.5
! 37.3
! 140.8
! 140.8
(cid:55)
(cid:55)
(cid:55)
(cid:55)

Classic Upsampling
FC-DenseNet56 (k=12)
FC-DenseNet67 (k=16)
FC-DenseNet103 (k=16)

20
1.5
3.5
9.4

y
c
a
r
u
c
c
a

l
a
b
o
l
G

62.5
86.9
85.9
88.7
88.0
−
79.0
88.3

86.8
88.9
90.8
91.5

U
o
I

n
a
e

M

46.4
63.1
48.9
58.8
57.0
61.6
65.3
66.1

55.2
58.9
65.8
66.9

Model

2D models (no time)
2D-V2V-from scratch [34]
FC-DenseNet103

Acc.

55.7
79.4

3D models (incorporate time)
3D-V2V-from scratch [34]
3D-V2V-pretrained [34]

66.7
76.0

Table 4. Results on Gatech dataset

in network parallelization, and Tristan Sylvain. We ac-
knowledge the support of the following agencies for re-
search funding and computing support: Imagia Inc., Span-
ish projects TRA2014-57088-C2-1-R & 2014-SGR-1506,
TECNIOspring-FP7-ACCI grant.

References

[1] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A
deep convolutional encoder-decoder architecture for image
segmentation. CoRR, abs/1511.00561, 2015.

[2] G. J. Brostow, J. Shotton, J. Fauqueur, and R. Cipolla.
Segmentation and recognition using structure from motion
point clouds. In European Conference on Computer Vision
(ECCV), 2008.

[3] L. Castrejon, Y. Aytar, C. Vondrick, H. Pirsiavash, and
A. Torralba. Learning aligned cross-modal representations
from weakly aligned data. CoRR, abs/1607.07295, 2016.
[4] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille. Deeplab: Semantic image segmentation with deep
convolutional nets, atrous convolution, and fully connected
crfs. CoRR, abs/1606.00915, 2016.

[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Semantic image segmentation with deep con-

volutional nets and fully connected crfs.
Conference of Learning Representations (ICLR), 2015.
[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2009.

In International

[7] S. Dieleman, J. Schlter, C. Raffel, E. Olson, and et al.

Lasagne: First release., Aug. 2015.

[8] M. Drozdzal, E. Vorontsov, G. Chartrand, S. Kadoury, and
C. Pal. The importance of skip connections in biomedical
image segmentation. CoRR, abs/1608.04117, 2016.

[9] A. R. F. Visin. Dataset loaders: a python library to load and
preprocess datasets. https://github.com/fvisin/
dataset_loaders, 2017.

[10] C. Gatta, A. Romero, and J. van de Weijer. Unrolling loopy
top-down semantic feedback in convolutional deep networks.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR) workshop, 2014.

[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. CoRR, abs/1512.03385, 2015.
[12] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. CoRR, abs/1502.01852, 2015.

[13] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der
Maaten. Densely connected convolutional networks. CoRR,
abs/1608.06993, 2016.

[14] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
CoRR, abs/1502.03167, 2015.

[15] A. Kendall, V. Badrinarayanan, and R. Cipolla. Bayesian
segnet: Model uncertainty in deep convolutional encoder-
CoRR,
decoder architectures for scene understanding.
abs/1511.02680, 2015.

[16] P. Kr¨ahenb¨uhl and V. Koltun. Efﬁcient inference in fully
connected crfs with gaussian edge potentials. In Advances in
Neural Information Processing Systems (NIPS). 2011.

Figure 3. Qualitative results on the CamVid test set. Pixels labeled in yellow are void class. Each row represents (from left to right):
original image, original annotation (ground truth) and prediction of our model.

[34] D. Tran, L. D. Bourdev, R. Fergus, L. Torresani, and
M. Paluri. Deep end2end voxel2voxel prediction. CoRR,
abs/1511.06681, 2015.

[35] A. Veit, M. J. Wilber, and S. J. Belongie. Residual networks
are exponential ensembles of relatively shallow networks.
CoRR, abs/1605.06431, 2016.

[36] F. Visin, M. Ciccone, A. Romero, K. Kastner, K. Cho,
Y. Bengio, M. Matteucci, and A. Courville. Reseg: A re-
current neural network-based model for semantic segmenta-
tion. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) workshop, 2016.

[37] F. Yu and V. Koltun. Multi-scale context aggregation by di-
lated convolutions. In International Conference of Learning
Representations (ICLR), 2016.

[38] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,
Z. Su, D. Du, C. Huang, and P. Torr. Conditional random
ﬁelds as recurrent neural networks. In International Confer-
ence on Computer Vision (ICCV), 2015.

[17] A. Kundu, V. Vineet, and V. Koltun. Feature space optimiza-
tion for semantic video segmentation. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2016.
[18] C. Lee, S. Xie, P. W. Gallagher, Z. Zhang, and Z. Tu. Deeply-
In International Conference on Artiﬁcial

supervised nets.
Intelligence and Statistics (AISTATS), 2015.

[19] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Dollr, and C. L. Zitnick. Microsoft coco: Common
objects in context. In European Conference on Computer Vi-
sion (ECCV), 2014.

[20] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2015.

[21] H. Noh, S. Hong, and B. Han.

tion network for semantic segmentation.
arXiv:1505.04366, 2015.

Learning deconvolu-
arXiv preprint

[22] S. H. Raza, M. Grundmann, and I. Essa. Geometric con-
text from video. IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2013.

[23] S. H. Raza, M. Grundmann, and I. Essa. Geometric context
In IEEE Conference on Computer Vision and

from video.
Pattern Recognition (CVPR), 2013.

[24] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi.
You only look once: Uniﬁed, real-time object detection.
CoRR, abs/1506.02640, 2015.

[25] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN:
towards real-time object detection with region proposal net-
works. CoRR, abs/1506.01497, 2015.

[26] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing
for data: Ground truth from computer games. In European
Conference on Computer Vision (ECCV), 2016.

[27] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-
tional networks for biomedical image segmentation. In In-
ternational Conference on Medical Image Computing and
Computer-Assisted Intervention (MICAI), 2015.

[28] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M.
Lopez. The synthia dataset: A large collection of synthetic
images for semantic segmentation of urban scenes. In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2016.

[29] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014.

[30] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: A simple way to prevent neu-
ral networks from overﬁtting. Journal of Machine Learning
Research, 15:1929–1958, 2014.

[31] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. CoRR, abs/1409.4842,
2014.

[32] Theano Development Team. Theano: A Python framework
for fast computation of mathematical expressions. arXiv e-
prints, abs/1605.02688, May 2016.

[33] T. Tieleman and G. Hinton. rmsprop adaptive learning. In
COURSERA: Neural Networks for Machine Learning, 2012.

The One Hundred Layers Tiramisu:
Fully Convolutional DenseNets for Semantic Segmentation

Simon J´egou1 Michal Drozdzal2,3 David Vazquez1,4 Adriana Romero1 Yoshua Bengio1
1Montreal Institute for Learning Algorithms 2 ´Ecole Polytechnique de Montr´eal
3Imagia Inc., Montr´eal, 4Computer Vision Center, Barcelona
simon.jegou@gmail.com, michal@imagia.com, dvazquez@cvc.uab.es,

adriana.romero.soriano@umontreal.ca, yoshua.umontreal@gmail.com

7
1
0
2
 
t
c
O
 
1
3
 
 
]

V
C
.
s
c
[
 
 
3
v
6
2
3
9
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

State-of-the-art approaches for semantic image segmen-
tation are built on Convolutional Neural Networks (CNNs).
The typical segmentation architecture is composed of (a)
a downsampling path responsible for extracting coarse se-
mantic features, followed by (b) an upsampling path trained
to recover the input image resolution at the output of the
model and, optionally, (c) a post-processing module (e.g.
Conditional Random Fields) to reﬁne the model predictions.
Recently, a new CNN architecture, Densely Connected
Convolutional Networks (DenseNets), has shown excellent
results on image classiﬁcation tasks. The idea of DenseNets
is based on the observation that if each layer is directly con-
nected to every other layer in a feed-forward fashion then
the network will be more accurate and easier to train.

In this paper, we extend DenseNets to deal with the prob-
lem of semantic segmentation. We achieve state-of-the-art
results on urban scene benchmark datasets such as CamVid
and Gatech, without any further post-processing module
nor pretraining. Moreover, due to smart construction of
the model, our approach has much less parameters than
currently published best entries for these datasets. Code
to reproduce the experiments is publicly available here :
https://github.com/SimJeg/FC-DenseNet

1. Introduction

Convolutional Neural Networks (CNNs) are driving ma-
jor advances in many computer vision tasks, such as im-
age classiﬁcation [29], object detection [25, 24] and seman-
tic image segmentation [20]. The last few years have wit-
nessed outstanding improvements on CNN-based models.
Very deep architectures [29, 11, 31] have shown impres-
sive results on standard benchmarks such as ImageNet [6]
or MSCOCO [19]. State-of-the-art CNNs heavily reduce
the input resolution through successive pooling layers and,

1

Figure 1. Diagram of our architecture for semantic segmentation.
Our architecture is built from dense blocks. The diagram is com-
posed of a downsampling path with 2 Transitions Down (TD) and
an upsampling path with 2 Transitions Up (TU). A circle repre-
sents concatenation and arrows represent connectivity patterns in
the network. Gray horizontal arrows represent skip connections,
the feature maps from the downsampling path are concatenated
with the corresponding feature maps in the upsampling path. Note
that the connectivity pattern in the upsampling and the downsam-
pling paths are different. In the downsampling path, the input to
a dense block is concatenated with its output, leading to a linear
growth of the number of feature maps, whereas in the upsampling
path, it is not.

thus, are well suited for applications where a single predic-
tion per input image is expected (e.g. image classiﬁcation
task).

Fully Convolutional Networks (FCNs) [20, 27] were in-
troduced in the literature as a natural extension of CNNs to
tackle per pixel prediction problems such as semantic im-
age segmentation. FCNs add upsampling layers to standard
CNNs to recover the spatial resolution of the input at the
output layer. As a consequence, FCNs can process images
of arbitrary size.
In order to compensate for the resolu-
tion loss induced by pooling layers, FCNs introduce skip
connections between their downsampling and upsampling
paths. Skip connections help the upsampling path recover
ﬁne-grained information from the downsampling layers.

Among CNN architectures extended as FCNs for seman-
tic segmentation purposes, Residual Networks (ResNets)
[11] make an interesting case. ResNets are designed to
ease the training of very deep networks (of hundreds of
layers) by introducing a residual block that sums two sig-
nals: a non-linear transformation of the input and its identity
mapping. The identity mapping is implemented by means
of a shortcut connection. ResNets have been extended to
work as FCNs [4, 8] yielding very good results in differ-
ent segmentation benchmarks. ResNets incorporate addi-
tional paths to FCN (shortcut paths) and, thus, increase the
number of connections within a segmentation network. This
additional shortcut paths have been shown not only to im-
prove the segmentation accuracy but also to help the net-
work optimization process, resulting in faster convergence
of the training [8].

Recently, a new CNN architecture, called DenseNet, was
introduced in [13]. DenseNets are built from dense blocks
and pooling operations, where each dense block is an itera-
tive concatenation of previous feature maps. This architec-
ture can be seen as an extension of ResNets [11], which per-
forms iterative summation of previous feature maps. How-
ever, this small modiﬁcation has some interesting implica-
tions: (1) parameter efﬁciency, DenseNets are more efﬁ-
cient in the parameter usage; (2) implicit deep supervision,
DenseNets perform deep supervision thanks to short paths
to all feature maps in the architecture (similar to Deeply
Supervised Networks [18]); and (3) feature reuse, all lay-
ers can easily access their preceding layers making it easy
to reuse the information from previously computed feature
maps. The characteristics of DenseNets make them a very
good ﬁt for semantic segmentation as they naturally induce
skip connections and multi-scale supervision.

In this paper, we extend DenseNets to work as FCNs by
adding an upsampling path to recover the full input reso-
lution. Naively building an upsampling path would result
in a computationally intractable number of feature maps
with very high resolution prior to the softmax layer. This
is because one would multiply the high resolution feature

maps with a large number of input ﬁlters (from all the lay-
ers below), resulting in both very large amount of compu-
tation and number of parameters. In order to mitigate this
effect, we only upsample the feature maps created by the
preceding dense block. Doing so allows to have a number of
dense blocks at each resolution of the upsampling path inde-
pendent of the number of pooling layers. Moreover, given
the network architecture, the upsampled dense block com-
bines the information contained in the other dense blocks
of the same resolution. The higher resolution information
is passed by means of a standard skip connection between
the downsampling and the upsampling paths. The details of
the proposed architecture are shown in Figure 1. We eval-
uate our model on two challenging benchmarks for urban
scene understanding, Camvid [2] and Gatech [22], and con-
ﬁrm the potential of DenseNets for semantic segmentation
by improving the state-of-the-art.

Thus, the contributions of the paper can be summarized

as follows:

• We carefully extend the DenseNet architecture [13] to
fully convolutional networks for semantic segmenta-
tion, while mitigating the feature map explosion.

• We highlight that the proposed upsampling path, built
from dense blocks, performs better than upsampling
path with more standard operations, such as the ones
in [27].

• We show that such a network can outperform current
state-of-the-art results on standard benchmarks for ur-
ban scene understanding without neither using pre-
trained parameters nor any further post-processing.

2. Related Work

Recent advances in semantic segmentation have been de-
voted to improve architectural designs by (1) improving
the upsampling path and increasing the connectivity within
FCNs [27, 1, 21, 8]; (2) introducing modules to account for
broader context understanding [36, 5, 37]; and/or (3) en-
dowing FCN architectures with the ability to provide struc-
tured outputs [16, 5, 38].

First, different alternatives have been proposed in the lit-
erature to address the resolution recovery in FCN’s upsam-
pling path; from simple bilinear interpolation [10, 20, 1]
to more sophisticated operators such as unpooling [1, 21]
or transposed convolutions [20]. Skip connections from
the downsampling to the upsampling path have also been
adopted to allow for a ﬁner information recovery [27]. More
recently, [8] presented a thorough analysis on the combina-
tion of identity mapping [11] and long skip connections [27]
for semantic segmentation.

Second, approaches that introduce larger context to se-
mantic segmentation networks include [10, 36, 5, 37]. In

3. Fully Convolutional DenseNets

As mentioned in Section 1, FCNs are built from a down-
sampling path, an upsampling path and skip connections.
Skip connections help the upsampling path recover spa-
tially detailed information from the downsampling path, by
reusing features maps. The goal of our model is to further
exploit the feature reuse by extending the more sophisti-
cated DenseNet architecture, while avoiding the feature ex-
plosion at the upsampling path of the network.

In this section, we detail the proposed model for seman-
tic segmentation. First, we review the recently proposed
DenseNet architecture. Second, we introduce the construc-
tion of the novel upsampling path and discuss its advantages
w.r.t. a naive DenseNet extension and more classical archi-
tectures. Finally, we wrap up with the details of the main
architecture used in Section 4.

3.1. Review of DenseNets

Let x(cid:96) be the output of the (cid:96)th layer. In a standard CNN,
x(cid:96) is computed by applying a non-linear transformation H(cid:96)
to the output of the previous layer x(cid:96)−1

x(cid:96) = H(cid:96)(x(cid:96)−1),

(1)

where H is commonly deﬁned as a convolution followed by
a rectiﬁer non-linearity (ReLU) and often dropout [30].

In order to ease the training of very deep networks,
ResNets [11] introduce a residual block that sums the iden-
tity mapping of the input to the output of a layer. The re-
sulting output x(cid:96) becomes

x(cid:96) = H(cid:96)(x(cid:96)−1) + x(cid:96)−1,

(2)

allowing for the reuse of features and permitting the gra-
dient to ﬂow directly to earlier layers. In this case, H is
deﬁned as the repetition (2 or 3 times) of a block composed
of Batch Normalization (BN) [14], followed by ReLU and
a convolution.

Pushing this idea further, DenseNets [13] design a more
sophisticated connectivity pattern that iteratively concate-
nates all feature outputs in a feedforward fashion. Thus, the
output of the (cid:96)th layer is deﬁned as

x(cid:96) = H(cid:96)([x(cid:96)−1, x(cid:96)−2, ..., x0]),

(3)

where [ ... ] represents the concatenation operation. In this
case, H is deﬁned as BN, followed by ReLU, a convolution
and dropout. Such connectivity pattern strongly encourages
the reuse of features and makes all layers in the architec-
ture receive direct supervision signal. The output dimension
of each layer (cid:96) has k feature maps where k, hereafter re-
ferred as to growth rate parameter, is typically set to a small
value (e.g. k = 12). Thus, the number of feature maps in
DenseNets grows linearly with the depth (e.g. after (cid:96) layers,
the input [x(cid:96)−1, x(cid:96)−2, ..., x0] will have (cid:96) × k feature maps).

Figure 2. Diagram of a dense block of 4 layers. A ﬁrst layer is ap-
plied to the input to create k feature maps, which are concatenated
to the input. A second layer is then applied to create another k
features maps, which are again concatenated to the previous fea-
ture maps. The operation is repeated 4 times. The output of the
block is the concatenation of the outputs of the 4 layers, and thus
contains 4 ∗ k feature maps

[10], an unsupervised global image descriptor is computed
added to the feature maps for each pixel. In [36], Recur-
rent Neural Networks (RNNs) are used to retrieve contex-
tual information by sweeping the image horizontally and
vertically in both directions.
In [5], dilated convolutions
are introduced as an alternative to late CNN pooling layers
to capture larger context without reducing the image reso-
lution. Following the same spirit, [37] propose to provide
FCNs with a context module built as a stack of dilated con-
volutional layers to enlarge the ﬁeld of view of the network.

Third, Conditional Random Fields (CRF) have long been
a popular choice to enforce structure consistency to seg-
mentation outputs. More recently, fully connected CRFs
[16] have been used to include structural properties of the
output of FCNs [5]. Interestingly, in [38], RNN have been
introduced to approximate mean-ﬁeld iterations of CRF op-
timization, allowing for an end-to-end training of both the
FCN and the RNN.

Finally, it is worth noting that current state-of-the-art
FCN architectures for semantic segmentation often rely on
pre-trained models (e.g. VGG [29] or ResNet101 [11]) to
improve their segmentation results [20, 1, 4].

A transition down is introduced to reduce the spatial di-
mensionality of the feature maps. Such transformation is
composed of a 1×1 convolution (which conserves the num-
ber of feature maps) followed by a 2 × 2 pooling operation.
In the remainder of the article, we will call dense block
the concatenation of the new feature maps created at a given
resolution. Figure 2 shows an example of dense block con-
struction. Starting from an input x0 (input image or output
of a transition down) with m feature maps, the ﬁrst layer of
the block generates an output x1 of dimension k by applying
H1(x0). These k feature maps are then stacked to the pre-
vious m feature maps by concatenation ([x1, x0]) and used
as input to the second layer. The same operation is repeated
n times, leading to a new dense block with n × k feature
maps.

3.2. From DenseNets

to Fully Convolutional

DenseNets

The DenseNet architecture described in Subsection 3.1
constitutes the downsampling path of our Fully Convolu-
tional DenseNet (FC-DenseNet). Note that, in the down-
sampling path, the linear growth in the number of features
is compensated by the reduction in spatial resolution of each
feature map after the pooling operation. The last layer of the
downsampling path is referred to as bottleneck.

In order to recover the input spatial resolution, FCNs in-
troduce an upsampling path composed of convolution, up-
sampling operations (transposed convolutions or unpooling
operations) and skip connections.
In FC-DenseNets, we
substitute the convolution operation by a dense block and
an upsampling operation referred to as transition up. Tran-
sition up modules consist of a transposed convolution that
upsamples the previous feature maps. The upsampled fea-
ture maps are then concatenated to the ones coming from
the skip connection to form the input of a new dense block.
Since the upsampling path increases the feature maps spa-
tial resolution, the linear growth in the number of features
would be too memory demanding, especially for the full
resolution features in the pre-softmax layer.

In order to overcome this limitation, the input of a dense
block is not concatenated with its output. Thus, the trans-
posed convolution is applied only to the feature maps ob-
tained by the last dense block and not to all feature maps
concatenated so far. The last dense block summarizes the
information contained in all the previous dense blocks at
the same resolution. Note that some information from ear-
lier dense blocks is lost in the transition down due to the
pooling operation. Nevertheless, this information is avail-
able in the downsampling path of the network and can be
passed via skip connections. Hence, the dense blocks of the
upsampling path are computed using all the available fea-
ture maps at a given resolution. Figure 1 illustrates this idea
in detail.

Therefore, our upsampling path approach allows us to
build very deep FC-DenseNets without a feature map explo-
sion. An alternative way of implementing the upsampling
path would be to perform consecutive transposed convolu-
tions and complement them with skip connections from the
downsampling path in a U-Net [27] or FCN-like [20] fash-
ion. This will be further discussed in Section 4

3.3. Semantic Segmentation Architecture

In this subsection, we detail the main architecture, FC-

DenseNet103, used in Section 4.

First, in Table 1, we deﬁne the dense block layer, tran-
sition down and transition up of the architecture. Dense
block layers are composed of BN, followed by ReLU, a
3 × 3 same convolution (no resolution loss) and dropout
with probability p = 0.2. The growth rate of the layer is set
to k = 16. Transition down is composed of BN, followed
by ReLU, a 1 × 1 convolution, dropout with p = 0.2 and a
non-overlapping max pooling of size 2 × 2. Transition up
is composed of a 3 × 3 transposed convolution with stride 2
to compensate for the pooling operation.

Second, in Table 2, we summarize all Dense103 layers.
This architecture is built from 103 convolutional layers : a
ﬁrst one on the input, 38 in the downsampling path, 15 in
the bottleneck and 38 in the upsampling path. We use 5
Transition Down (TD), each one containing one extra con-
volution, and 5 Transition Up (TU), each one containing a
transposed convolution. The ﬁnal layer in the network is a
1 × 1 convolution followed by a softmax non-linearity to
provide the per class distribution at each pixel.

It is worth noting that, as discussed in Subsection 3.2, the
proposed upsampling path properly mitigates the DenseNet
feature map explosion, leading to reasonable pre-softmax
feature map number of 256.

Finally, the model is trained by minimizing the pixel-

wise cross-entropy loss.

4. Experiments

We evaluate our method on two urban scene understand-
ing datasets: CamVid [2], and Gatech [22]. We trained our
models from scratch without using any extra-data nor post-
processing module. We report the results using the Inter-
section over Union (IoU) metric and the global accuracy
(pixel-wise accuracy on the dataset). For a given class c,
predictions (oi) and targets (yi), the IoU is deﬁned by

IoU (c) =

(cid:80)
(cid:80)

i (oi == c ∧ yi == c)
i (oi == c ∨ yi == c)

,

(4)

where ∧ is a logical and operation, while ∨ is a logical or
operation. We compute IoU by summing over all the pixels
i of the dataset.

Layer
Batch Normalization
ReLU
3 × 3 Convolution
Dropout p = 0.2

Transition Down (TD)
Batch Normalization
ReLU
1 × 1 Convolution
Dropout p = 0.2
2 × 2 Max Pooling

Transition Up (TU)
3 × 3 Transposed Convolution
stride = 2

Table 1. Building blocks of fully convolutional DenseNets. From left to right: layer used in the model, Transition Down (TD) and Transition
Up (TU). See text for details.

4.1. Architecture and training details

4.2. CamVid dataset

We initialize our models using HeUniform [12] and train
them with RMSprop [33], with an initial learning rate of
1e − 3 and an exponential decay of 0.995 after each epoch.
All models are trained on data augmented with random
crops and vertical ﬂips. For all experiments, we ﬁnetune
our models with full size images and learning rate of 1e − 4.
We use validation set to earlystop the training and the ﬁne-
tuning. We monitor mean IoU or mean accuracy and use
patience of 100 (50 during ﬁnetuning).

We regularized our models with a weight decay of 1e−4
and a dropout rate of 0.2. For batch normalization, we use
current batch statistics at training, validation and test time.

Architecture
Input, m = 3
3 × 3 Convolution, m = 48
DB (4 layers) + TD, m = 112
DB (5 layers) + TD, m = 192
DB (7 layers) + TD, m = 304
DB (10 layers) + TD, m = 464
DB (12 layers) + TD, m = 656
DB (15 layers), m = 896
TU + DB (12 layers), m = 1088
TU + DB (10 layers), m = 816
TU + DB (7 layers), m = 578
TU + DB (5 layers), m = 384
TU + DB (4 layers), m = 256
1 × 1 Convolution, m = c
Softmax

Table 2. Architecture details of FC-DenseNet103 model used in
our experiments. This model is built from 103 convolutional lay-
ers. In the Table we use following notations: DB stands for Dense
Block, TD stands for Transition Down, TU stands for Transition
Up, BN stands for Batch Normalization and m corresponds to the
total number of feature maps at the end of a block. c stands for the
number of classes.

CamVid1 [2] is a dataset of fully segmented videos for
urban scene understanding. We used the split and image
resolution from [1], which consists of 367 frames for train-
ing, 101 frames for validation and 233 frames for test. Each
frame has a size 360 × 480 and its pixels are labeled with
11 semantic classes. Our models were trained with crops
of 224 × 224 and batch size 3. At the end, the model is
ﬁnetuned with full size images.

In Table 3, we report our results for three networks with
respectively (1) 56 layers (FC-DenseNet56), with 4 layers
per dense block and a growth rate of 12; (2) 67 layers (FC-
DenseNet67) with 5 layers per dense block and a growth
rate of 16; and (3) 103 layers (FC-DenseNet103) with a
growth rate k = 16 (see Table 2 for details). We also trained
an architecture using standard convolutions in the upsam-
pling path instead of dense blocks (Classic Upsampling). In
the latter architecture, we used 3 convolutions per resolution
level with respectively 512, 256, 128, 128 and 64 ﬁlters, as
in [27]. Results show clear superiority of the proposed up-
sampling path w.r.t.
the classic one, consistently improv-
ing the IoU signiﬁcantly for all classes. Particularly, we
observe that unrepresented classes beneﬁt notably from the
FC-DenseNet architecture, namely sign, pedestrian, fence,
cyclist experience a crucial boost in performance (ranging
from 15% to 25%).

As expected, when comparing FC-DenseNet56 or FC-
DenseNet67 to FC-DenseNet103, we see that the model
beneﬁts from having more depth as well as more parame-
ters.

When compared to other methods, we show that FC-
DenseNet architectures achieve state-of-the-art, improving
upon models with 10 times more parameters. It is worth
mentioning that our small model FC-DenseNet56 already
outperforms popular architectures with at least 100 times
more parameters.

It is worth noting that images in CamVid correspond to
video frames and, thus, the dataset contains temporal infor-
mation. Some state-of-the-art methods such as [17] incor-
porate long range spatio-temporal regularization to the out-

1http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/

put of a FCN to boost their performance. Our model is able
to outperform such state-of-the-art model, without requir-
ing any temporal smoothing. However, any post-processing
temporal regularization is complementary to our approach
and could bring additional improvements.

Unlike most of the current state-of-the-art methods, FC-
DenseNets have not been pretrained on large datasets such
as ImageNet [6] and could most likely beneﬁt from such
pretraining. More recently, it has been shown that deep net-
works can also boost their performance when pretrained on
data other than natural images, such as video games [26, 28]
or clipart [3], and this an interesting direction to explore.

Figure 3 shows some qualitative segmentation results on
the CamVid dataset. Qualitative results are well aligned
with the quantitative ones, showing sharp segmentations
that account for a lot of details. For example, trees, column
poles, sidewalk and pedestrians appear very well sketched.
Among common errors, we ﬁnd that thin details found in
trees can be confused with column poles (see ﬁfth row),
buses and trucks can be confused with buildings (fourth
row), and shop signs can be confused with road signs (sec-
ond row).

4.3. Gatech dataset

Gatech2 [23] is a geometric scene understanding dataset,
which consists of 63 videos for training/validation and 38
for testing. Each video has between 50 and 300 frames
(with an average of 190). A pixel-wise segmentation map is
provided for each frame. There are 8 classes in the dataset:
sky, ground, buildings, porous (mainly trees), humans, cars,
vertical mix and main mix. The dataset was originally built
to learn 3D geometric structure of outdoor video scenes and
the standard metric for this dataset is mean global accuracy.
We used the FC-DenseNet103 model pretrained on
CamVid, removed the softmax layer, and ﬁnetuned it for
10 epochs with crops of 224 × 224 and batch size 5. Given
the high redundancy in Gatech frames, we used only one
out of 10 frames to train the model and tested it on all full
resolution test set frames.

In Table 4, we report the obtained results. We compare
the results to the recently proposed method for video seg-
mentation of [34], which reports results of their architecture
with 2D and 3D convolutions. Frame-based 2D convolu-
tions do not have temporal information. As it can be seen
in Table 4, our method gives an impressive improvement of
23.7% in global accuracy with respect to previously pub-
lished state-of-the-art with 2D convolutions. Moreover, our
model (trained with only 2D convolutions) also achieves a
signiﬁcant improvement over state-of-the-art models based
on spatio-temporal 3D convolutions (3.4% improvement).

2http://www.cc.gatech.edu/cpl/projects/videogeometriccontext/

5. Discussion

Our fully convolutional DenseNet

implicitly inherits
the advantages of DenseNets, namely: (1) parameter ef-
ﬁciency, as our network has substantially less parame-
ters than other segmentation architectures published for the
Camvid dataset; (2) implicit deep supervision, we tried in-
cluding additional levels of supervision to different layers of
our network without noticeable change in performance; and
(3) feature reuse, as all layers can easily access their pre-
ceding layers not only due to the iterative concatenation of
feature maps in a dense block but also thanks to skip con-
nections that enforce connectivity between downsampling
and upsampling path.

Recent evidence suggest that ResNets behave like en-
semble of relatively shallow networks [35]: ”Residual net-
works avoid the vanishing gradient problem by introducing
short paths which can carry gradient throughout the extent
of very deep networks”. It would be interesting to revisit
this ﬁnding in the context of fully convolutional DenseNets.
Due to iterative feature map concatenation in the dense
block, the gradients are forced to be passed through net-
works of different depth (with different numbers of non-
linearities). Thus, thanks to the smart connectivity pat-
terns, FC-DenseNets might represent an ensemble of vari-
able depth networks. This particular ensemble behavior
would be very interesting for semantic segmentation mod-
els, where the ensemble of different paths throughout the
model would capture the multi-scale appearance of objects
in urban scene.

6. Conclusion

In this paper, we have extended DenseNets and made
them fully convolutional to tackle the problem semantic im-
age segmentation. The main idea behind DenseNets is cap-
tured in dense blocks that perform iterative concatenation
of feature maps. We designed an upsampling path mitigat-
ing the linear growth of feature maps that would appear in
a naive extension of DenseNets.

The resulting network is very deep (from 56 to 103 lay-
ers) and has very few parameters, about 10 fold reduction
w.r.t. state-of-the-art models. Moreover, it improves state-
of-the-art performance on challenging urban scene under-
standing datasets (CamVid and Gatech), without neither ad-
ditional post-processing, pretraining, nor including tempo-
ral information.

Aknowledgements

The authors would like to thank the developers of
Theano [32] and Lasagne [7]. Special thanks to Fr´ed´eric
Bastien for his work assessing the compilation issues.
Thanks to Francesco Visin for his well designed data-
loader [9], as well as Harm de Vries for his support

)

M

(

s
r
e
t
e
m
a
r
a
p

#

d
e
n
i
a
r
t
e
r
P

g
n
i
d
l
i
u
B

e
e
r
T

y
k
S

r
a
C

n
g
i

S

n
a
i
r
t
s
e
d
e
P

e
c
n
e
F

e
l
o
P

k
l
a
w
e
d
i

S

t
s
i
l
c
y
C

68.7

52.0

87.0

58.5

13.4

25.3

17.9

16.0

60.5

24.8

77.8
81.5
82.6
84.0

73.5
77.6
80.2
83.0

71.0
74.6
76.2
77.2

72.2
72.0
75.4
77.3

88.7
89.0
89.0
91.3

92.4
92.4
93.0
93.0

76.1
82.2
84.0
85.6

66.2
73.2
78.2
77.3

32.7
42.3
46.9
49.9

26.9
31.8
40.9
43.9

41.7
48.4
56.3
59.1

37.7
37.9
58.4
59.6

24.4
27.2
35.8
37.6

22.7
26.2
30.7
37.1

19.9
14.3
23.4
16.9

30.8
32.6
38.4
37.8

72.7
75.4
75.3
76.0

69.6
79.9
81.9
82.2

31.0
50.1
55.5
57.2

25.1
31.1
52.1
50.5

Table 3. Results on CamVid dataset. Note that we trained our own pretrained FCN8 model

d
a
o
R

86.2
n/a
n/a
n/a
91.2
92.2
92.2
92.5

90.0
92.8
94.7
94.5

Model

SegNet [1]

DeconvNet [21]
Visin et al. [36]
FCN8 [20]
DeepLab-LFOV [5]
Dilation8 [37]
Dilation8 + FSO [17]

! 29.5
Bayesian SegNet [15] ! 29.5
! 252
! 32.3
! 134.5
! 37.3
! 140.8
! 140.8
(cid:55)
(cid:55)
(cid:55)
(cid:55)

Classic Upsampling
FC-DenseNet56 (k=12)
FC-DenseNet67 (k=16)
FC-DenseNet103 (k=16)

20
1.5
3.5
9.4

y
c
a
r
u
c
c
a

l
a
b
o
l
G

62.5
86.9
85.9
88.7
88.0
−
79.0
88.3

86.8
88.9
90.8
91.5

U
o
I

n
a
e

M

46.4
63.1
48.9
58.8
57.0
61.6
65.3
66.1

55.2
58.9
65.8
66.9

Model

2D models (no time)
2D-V2V-from scratch [34]
FC-DenseNet103

Acc.

55.7
79.4

3D models (incorporate time)
3D-V2V-from scratch [34]
3D-V2V-pretrained [34]

66.7
76.0

Table 4. Results on Gatech dataset

in network parallelization, and Tristan Sylvain. We ac-
knowledge the support of the following agencies for re-
search funding and computing support: Imagia Inc., Span-
ish projects TRA2014-57088-C2-1-R & 2014-SGR-1506,
TECNIOspring-FP7-ACCI grant.

References

[1] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A
deep convolutional encoder-decoder architecture for image
segmentation. CoRR, abs/1511.00561, 2015.

[2] G. J. Brostow, J. Shotton, J. Fauqueur, and R. Cipolla.
Segmentation and recognition using structure from motion
point clouds. In European Conference on Computer Vision
(ECCV), 2008.

[3] L. Castrejon, Y. Aytar, C. Vondrick, H. Pirsiavash, and
A. Torralba. Learning aligned cross-modal representations
from weakly aligned data. CoRR, abs/1607.07295, 2016.
[4] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille. Deeplab: Semantic image segmentation with deep
convolutional nets, atrous convolution, and fully connected
crfs. CoRR, abs/1606.00915, 2016.

[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Semantic image segmentation with deep con-

volutional nets and fully connected crfs.
Conference of Learning Representations (ICLR), 2015.
[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2009.

In International

[7] S. Dieleman, J. Schlter, C. Raffel, E. Olson, and et al.

Lasagne: First release., Aug. 2015.

[8] M. Drozdzal, E. Vorontsov, G. Chartrand, S. Kadoury, and
C. Pal. The importance of skip connections in biomedical
image segmentation. CoRR, abs/1608.04117, 2016.

[9] A. R. F. Visin. Dataset loaders: a python library to load and
preprocess datasets. https://github.com/fvisin/
dataset_loaders, 2017.

[10] C. Gatta, A. Romero, and J. van de Weijer. Unrolling loopy
top-down semantic feedback in convolutional deep networks.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR) workshop, 2014.

[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. CoRR, abs/1512.03385, 2015.
[12] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. CoRR, abs/1502.01852, 2015.

[13] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der
Maaten. Densely connected convolutional networks. CoRR,
abs/1608.06993, 2016.

[14] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
CoRR, abs/1502.03167, 2015.

[15] A. Kendall, V. Badrinarayanan, and R. Cipolla. Bayesian
segnet: Model uncertainty in deep convolutional encoder-
CoRR,
decoder architectures for scene understanding.
abs/1511.02680, 2015.

[16] P. Kr¨ahenb¨uhl and V. Koltun. Efﬁcient inference in fully
connected crfs with gaussian edge potentials. In Advances in
Neural Information Processing Systems (NIPS). 2011.

Figure 3. Qualitative results on the CamVid test set. Pixels labeled in yellow are void class. Each row represents (from left to right):
original image, original annotation (ground truth) and prediction of our model.

[34] D. Tran, L. D. Bourdev, R. Fergus, L. Torresani, and
M. Paluri. Deep end2end voxel2voxel prediction. CoRR,
abs/1511.06681, 2015.

[35] A. Veit, M. J. Wilber, and S. J. Belongie. Residual networks
are exponential ensembles of relatively shallow networks.
CoRR, abs/1605.06431, 2016.

[36] F. Visin, M. Ciccone, A. Romero, K. Kastner, K. Cho,
Y. Bengio, M. Matteucci, and A. Courville. Reseg: A re-
current neural network-based model for semantic segmenta-
tion. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) workshop, 2016.

[37] F. Yu and V. Koltun. Multi-scale context aggregation by di-
lated convolutions. In International Conference of Learning
Representations (ICLR), 2016.

[38] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,
Z. Su, D. Du, C. Huang, and P. Torr. Conditional random
ﬁelds as recurrent neural networks. In International Confer-
ence on Computer Vision (ICCV), 2015.

[17] A. Kundu, V. Vineet, and V. Koltun. Feature space optimiza-
tion for semantic video segmentation. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2016.
[18] C. Lee, S. Xie, P. W. Gallagher, Z. Zhang, and Z. Tu. Deeply-
In International Conference on Artiﬁcial

supervised nets.
Intelligence and Statistics (AISTATS), 2015.

[19] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Dollr, and C. L. Zitnick. Microsoft coco: Common
objects in context. In European Conference on Computer Vi-
sion (ECCV), 2014.

[20] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2015.

[21] H. Noh, S. Hong, and B. Han.

tion network for semantic segmentation.
arXiv:1505.04366, 2015.

Learning deconvolu-
arXiv preprint

[22] S. H. Raza, M. Grundmann, and I. Essa. Geometric con-
text from video. IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2013.

[23] S. H. Raza, M. Grundmann, and I. Essa. Geometric context
In IEEE Conference on Computer Vision and

from video.
Pattern Recognition (CVPR), 2013.

[24] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi.
You only look once: Uniﬁed, real-time object detection.
CoRR, abs/1506.02640, 2015.

[25] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN:
towards real-time object detection with region proposal net-
works. CoRR, abs/1506.01497, 2015.

[26] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing
for data: Ground truth from computer games. In European
Conference on Computer Vision (ECCV), 2016.

[27] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-
tional networks for biomedical image segmentation. In In-
ternational Conference on Medical Image Computing and
Computer-Assisted Intervention (MICAI), 2015.

[28] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M.
Lopez. The synthia dataset: A large collection of synthetic
images for semantic segmentation of urban scenes. In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2016.

[29] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014.

[30] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: A simple way to prevent neu-
ral networks from overﬁtting. Journal of Machine Learning
Research, 15:1929–1958, 2014.

[31] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. CoRR, abs/1409.4842,
2014.

[32] Theano Development Team. Theano: A Python framework
for fast computation of mathematical expressions. arXiv e-
prints, abs/1605.02688, May 2016.

[33] T. Tieleman and G. Hinton. rmsprop adaptive learning. In
COURSERA: Neural Networks for Machine Learning, 2012.

The One Hundred Layers Tiramisu:
Fully Convolutional DenseNets for Semantic Segmentation

Simon J´egou1 Michal Drozdzal2,3 David Vazquez1,4 Adriana Romero1 Yoshua Bengio1
1Montreal Institute for Learning Algorithms 2 ´Ecole Polytechnique de Montr´eal
3Imagia Inc., Montr´eal, 4Computer Vision Center, Barcelona
simon.jegou@gmail.com, michal@imagia.com, dvazquez@cvc.uab.es,

adriana.romero.soriano@umontreal.ca, yoshua.umontreal@gmail.com

7
1
0
2
 
t
c
O
 
1
3
 
 
]

V
C
.
s
c
[
 
 
3
v
6
2
3
9
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

State-of-the-art approaches for semantic image segmen-
tation are built on Convolutional Neural Networks (CNNs).
The typical segmentation architecture is composed of (a)
a downsampling path responsible for extracting coarse se-
mantic features, followed by (b) an upsampling path trained
to recover the input image resolution at the output of the
model and, optionally, (c) a post-processing module (e.g.
Conditional Random Fields) to reﬁne the model predictions.
Recently, a new CNN architecture, Densely Connected
Convolutional Networks (DenseNets), has shown excellent
results on image classiﬁcation tasks. The idea of DenseNets
is based on the observation that if each layer is directly con-
nected to every other layer in a feed-forward fashion then
the network will be more accurate and easier to train.

In this paper, we extend DenseNets to deal with the prob-
lem of semantic segmentation. We achieve state-of-the-art
results on urban scene benchmark datasets such as CamVid
and Gatech, without any further post-processing module
nor pretraining. Moreover, due to smart construction of
the model, our approach has much less parameters than
currently published best entries for these datasets. Code
to reproduce the experiments is publicly available here :
https://github.com/SimJeg/FC-DenseNet

1. Introduction

Convolutional Neural Networks (CNNs) are driving ma-
jor advances in many computer vision tasks, such as im-
age classiﬁcation [29], object detection [25, 24] and seman-
tic image segmentation [20]. The last few years have wit-
nessed outstanding improvements on CNN-based models.
Very deep architectures [29, 11, 31] have shown impres-
sive results on standard benchmarks such as ImageNet [6]
or MSCOCO [19]. State-of-the-art CNNs heavily reduce
the input resolution through successive pooling layers and,

1

Figure 1. Diagram of our architecture for semantic segmentation.
Our architecture is built from dense blocks. The diagram is com-
posed of a downsampling path with 2 Transitions Down (TD) and
an upsampling path with 2 Transitions Up (TU). A circle repre-
sents concatenation and arrows represent connectivity patterns in
the network. Gray horizontal arrows represent skip connections,
the feature maps from the downsampling path are concatenated
with the corresponding feature maps in the upsampling path. Note
that the connectivity pattern in the upsampling and the downsam-
pling paths are different. In the downsampling path, the input to
a dense block is concatenated with its output, leading to a linear
growth of the number of feature maps, whereas in the upsampling
path, it is not.

thus, are well suited for applications where a single predic-
tion per input image is expected (e.g. image classiﬁcation
task).

Fully Convolutional Networks (FCNs) [20, 27] were in-
troduced in the literature as a natural extension of CNNs to
tackle per pixel prediction problems such as semantic im-
age segmentation. FCNs add upsampling layers to standard
CNNs to recover the spatial resolution of the input at the
output layer. As a consequence, FCNs can process images
of arbitrary size.
In order to compensate for the resolu-
tion loss induced by pooling layers, FCNs introduce skip
connections between their downsampling and upsampling
paths. Skip connections help the upsampling path recover
ﬁne-grained information from the downsampling layers.

Among CNN architectures extended as FCNs for seman-
tic segmentation purposes, Residual Networks (ResNets)
[11] make an interesting case. ResNets are designed to
ease the training of very deep networks (of hundreds of
layers) by introducing a residual block that sums two sig-
nals: a non-linear transformation of the input and its identity
mapping. The identity mapping is implemented by means
of a shortcut connection. ResNets have been extended to
work as FCNs [4, 8] yielding very good results in differ-
ent segmentation benchmarks. ResNets incorporate addi-
tional paths to FCN (shortcut paths) and, thus, increase the
number of connections within a segmentation network. This
additional shortcut paths have been shown not only to im-
prove the segmentation accuracy but also to help the net-
work optimization process, resulting in faster convergence
of the training [8].

Recently, a new CNN architecture, called DenseNet, was
introduced in [13]. DenseNets are built from dense blocks
and pooling operations, where each dense block is an itera-
tive concatenation of previous feature maps. This architec-
ture can be seen as an extension of ResNets [11], which per-
forms iterative summation of previous feature maps. How-
ever, this small modiﬁcation has some interesting implica-
tions: (1) parameter efﬁciency, DenseNets are more efﬁ-
cient in the parameter usage; (2) implicit deep supervision,
DenseNets perform deep supervision thanks to short paths
to all feature maps in the architecture (similar to Deeply
Supervised Networks [18]); and (3) feature reuse, all lay-
ers can easily access their preceding layers making it easy
to reuse the information from previously computed feature
maps. The characteristics of DenseNets make them a very
good ﬁt for semantic segmentation as they naturally induce
skip connections and multi-scale supervision.

In this paper, we extend DenseNets to work as FCNs by
adding an upsampling path to recover the full input reso-
lution. Naively building an upsampling path would result
in a computationally intractable number of feature maps
with very high resolution prior to the softmax layer. This
is because one would multiply the high resolution feature

maps with a large number of input ﬁlters (from all the lay-
ers below), resulting in both very large amount of compu-
tation and number of parameters. In order to mitigate this
effect, we only upsample the feature maps created by the
preceding dense block. Doing so allows to have a number of
dense blocks at each resolution of the upsampling path inde-
pendent of the number of pooling layers. Moreover, given
the network architecture, the upsampled dense block com-
bines the information contained in the other dense blocks
of the same resolution. The higher resolution information
is passed by means of a standard skip connection between
the downsampling and the upsampling paths. The details of
the proposed architecture are shown in Figure 1. We eval-
uate our model on two challenging benchmarks for urban
scene understanding, Camvid [2] and Gatech [22], and con-
ﬁrm the potential of DenseNets for semantic segmentation
by improving the state-of-the-art.

Thus, the contributions of the paper can be summarized

as follows:

• We carefully extend the DenseNet architecture [13] to
fully convolutional networks for semantic segmenta-
tion, while mitigating the feature map explosion.

• We highlight that the proposed upsampling path, built
from dense blocks, performs better than upsampling
path with more standard operations, such as the ones
in [27].

• We show that such a network can outperform current
state-of-the-art results on standard benchmarks for ur-
ban scene understanding without neither using pre-
trained parameters nor any further post-processing.

2. Related Work

Recent advances in semantic segmentation have been de-
voted to improve architectural designs by (1) improving
the upsampling path and increasing the connectivity within
FCNs [27, 1, 21, 8]; (2) introducing modules to account for
broader context understanding [36, 5, 37]; and/or (3) en-
dowing FCN architectures with the ability to provide struc-
tured outputs [16, 5, 38].

First, different alternatives have been proposed in the lit-
erature to address the resolution recovery in FCN’s upsam-
pling path; from simple bilinear interpolation [10, 20, 1]
to more sophisticated operators such as unpooling [1, 21]
or transposed convolutions [20]. Skip connections from
the downsampling to the upsampling path have also been
adopted to allow for a ﬁner information recovery [27]. More
recently, [8] presented a thorough analysis on the combina-
tion of identity mapping [11] and long skip connections [27]
for semantic segmentation.

Second, approaches that introduce larger context to se-
mantic segmentation networks include [10, 36, 5, 37]. In

3. Fully Convolutional DenseNets

As mentioned in Section 1, FCNs are built from a down-
sampling path, an upsampling path and skip connections.
Skip connections help the upsampling path recover spa-
tially detailed information from the downsampling path, by
reusing features maps. The goal of our model is to further
exploit the feature reuse by extending the more sophisti-
cated DenseNet architecture, while avoiding the feature ex-
plosion at the upsampling path of the network.

In this section, we detail the proposed model for seman-
tic segmentation. First, we review the recently proposed
DenseNet architecture. Second, we introduce the construc-
tion of the novel upsampling path and discuss its advantages
w.r.t. a naive DenseNet extension and more classical archi-
tectures. Finally, we wrap up with the details of the main
architecture used in Section 4.

3.1. Review of DenseNets

Let x(cid:96) be the output of the (cid:96)th layer. In a standard CNN,
x(cid:96) is computed by applying a non-linear transformation H(cid:96)
to the output of the previous layer x(cid:96)−1

x(cid:96) = H(cid:96)(x(cid:96)−1),

(1)

where H is commonly deﬁned as a convolution followed by
a rectiﬁer non-linearity (ReLU) and often dropout [30].

In order to ease the training of very deep networks,
ResNets [11] introduce a residual block that sums the iden-
tity mapping of the input to the output of a layer. The re-
sulting output x(cid:96) becomes

x(cid:96) = H(cid:96)(x(cid:96)−1) + x(cid:96)−1,

(2)

allowing for the reuse of features and permitting the gra-
dient to ﬂow directly to earlier layers. In this case, H is
deﬁned as the repetition (2 or 3 times) of a block composed
of Batch Normalization (BN) [14], followed by ReLU and
a convolution.

Pushing this idea further, DenseNets [13] design a more
sophisticated connectivity pattern that iteratively concate-
nates all feature outputs in a feedforward fashion. Thus, the
output of the (cid:96)th layer is deﬁned as

x(cid:96) = H(cid:96)([x(cid:96)−1, x(cid:96)−2, ..., x0]),

(3)

where [ ... ] represents the concatenation operation. In this
case, H is deﬁned as BN, followed by ReLU, a convolution
and dropout. Such connectivity pattern strongly encourages
the reuse of features and makes all layers in the architec-
ture receive direct supervision signal. The output dimension
of each layer (cid:96) has k feature maps where k, hereafter re-
ferred as to growth rate parameter, is typically set to a small
value (e.g. k = 12). Thus, the number of feature maps in
DenseNets grows linearly with the depth (e.g. after (cid:96) layers,
the input [x(cid:96)−1, x(cid:96)−2, ..., x0] will have (cid:96) × k feature maps).

Figure 2. Diagram of a dense block of 4 layers. A ﬁrst layer is ap-
plied to the input to create k feature maps, which are concatenated
to the input. A second layer is then applied to create another k
features maps, which are again concatenated to the previous fea-
ture maps. The operation is repeated 4 times. The output of the
block is the concatenation of the outputs of the 4 layers, and thus
contains 4 ∗ k feature maps

[10], an unsupervised global image descriptor is computed
added to the feature maps for each pixel. In [36], Recur-
rent Neural Networks (RNNs) are used to retrieve contex-
tual information by sweeping the image horizontally and
vertically in both directions.
In [5], dilated convolutions
are introduced as an alternative to late CNN pooling layers
to capture larger context without reducing the image reso-
lution. Following the same spirit, [37] propose to provide
FCNs with a context module built as a stack of dilated con-
volutional layers to enlarge the ﬁeld of view of the network.

Third, Conditional Random Fields (CRF) have long been
a popular choice to enforce structure consistency to seg-
mentation outputs. More recently, fully connected CRFs
[16] have been used to include structural properties of the
output of FCNs [5]. Interestingly, in [38], RNN have been
introduced to approximate mean-ﬁeld iterations of CRF op-
timization, allowing for an end-to-end training of both the
FCN and the RNN.

Finally, it is worth noting that current state-of-the-art
FCN architectures for semantic segmentation often rely on
pre-trained models (e.g. VGG [29] or ResNet101 [11]) to
improve their segmentation results [20, 1, 4].

A transition down is introduced to reduce the spatial di-
mensionality of the feature maps. Such transformation is
composed of a 1×1 convolution (which conserves the num-
ber of feature maps) followed by a 2 × 2 pooling operation.
In the remainder of the article, we will call dense block
the concatenation of the new feature maps created at a given
resolution. Figure 2 shows an example of dense block con-
struction. Starting from an input x0 (input image or output
of a transition down) with m feature maps, the ﬁrst layer of
the block generates an output x1 of dimension k by applying
H1(x0). These k feature maps are then stacked to the pre-
vious m feature maps by concatenation ([x1, x0]) and used
as input to the second layer. The same operation is repeated
n times, leading to a new dense block with n × k feature
maps.

3.2. From DenseNets

to Fully Convolutional

DenseNets

The DenseNet architecture described in Subsection 3.1
constitutes the downsampling path of our Fully Convolu-
tional DenseNet (FC-DenseNet). Note that, in the down-
sampling path, the linear growth in the number of features
is compensated by the reduction in spatial resolution of each
feature map after the pooling operation. The last layer of the
downsampling path is referred to as bottleneck.

In order to recover the input spatial resolution, FCNs in-
troduce an upsampling path composed of convolution, up-
sampling operations (transposed convolutions or unpooling
operations) and skip connections.
In FC-DenseNets, we
substitute the convolution operation by a dense block and
an upsampling operation referred to as transition up. Tran-
sition up modules consist of a transposed convolution that
upsamples the previous feature maps. The upsampled fea-
ture maps are then concatenated to the ones coming from
the skip connection to form the input of a new dense block.
Since the upsampling path increases the feature maps spa-
tial resolution, the linear growth in the number of features
would be too memory demanding, especially for the full
resolution features in the pre-softmax layer.

In order to overcome this limitation, the input of a dense
block is not concatenated with its output. Thus, the trans-
posed convolution is applied only to the feature maps ob-
tained by the last dense block and not to all feature maps
concatenated so far. The last dense block summarizes the
information contained in all the previous dense blocks at
the same resolution. Note that some information from ear-
lier dense blocks is lost in the transition down due to the
pooling operation. Nevertheless, this information is avail-
able in the downsampling path of the network and can be
passed via skip connections. Hence, the dense blocks of the
upsampling path are computed using all the available fea-
ture maps at a given resolution. Figure 1 illustrates this idea
in detail.

Therefore, our upsampling path approach allows us to
build very deep FC-DenseNets without a feature map explo-
sion. An alternative way of implementing the upsampling
path would be to perform consecutive transposed convolu-
tions and complement them with skip connections from the
downsampling path in a U-Net [27] or FCN-like [20] fash-
ion. This will be further discussed in Section 4

3.3. Semantic Segmentation Architecture

In this subsection, we detail the main architecture, FC-

DenseNet103, used in Section 4.

First, in Table 1, we deﬁne the dense block layer, tran-
sition down and transition up of the architecture. Dense
block layers are composed of BN, followed by ReLU, a
3 × 3 same convolution (no resolution loss) and dropout
with probability p = 0.2. The growth rate of the layer is set
to k = 16. Transition down is composed of BN, followed
by ReLU, a 1 × 1 convolution, dropout with p = 0.2 and a
non-overlapping max pooling of size 2 × 2. Transition up
is composed of a 3 × 3 transposed convolution with stride 2
to compensate for the pooling operation.

Second, in Table 2, we summarize all Dense103 layers.
This architecture is built from 103 convolutional layers : a
ﬁrst one on the input, 38 in the downsampling path, 15 in
the bottleneck and 38 in the upsampling path. We use 5
Transition Down (TD), each one containing one extra con-
volution, and 5 Transition Up (TU), each one containing a
transposed convolution. The ﬁnal layer in the network is a
1 × 1 convolution followed by a softmax non-linearity to
provide the per class distribution at each pixel.

It is worth noting that, as discussed in Subsection 3.2, the
proposed upsampling path properly mitigates the DenseNet
feature map explosion, leading to reasonable pre-softmax
feature map number of 256.

Finally, the model is trained by minimizing the pixel-

wise cross-entropy loss.

4. Experiments

We evaluate our method on two urban scene understand-
ing datasets: CamVid [2], and Gatech [22]. We trained our
models from scratch without using any extra-data nor post-
processing module. We report the results using the Inter-
section over Union (IoU) metric and the global accuracy
(pixel-wise accuracy on the dataset). For a given class c,
predictions (oi) and targets (yi), the IoU is deﬁned by

IoU (c) =

(cid:80)
(cid:80)

i (oi == c ∧ yi == c)
i (oi == c ∨ yi == c)

,

(4)

where ∧ is a logical and operation, while ∨ is a logical or
operation. We compute IoU by summing over all the pixels
i of the dataset.

Layer
Batch Normalization
ReLU
3 × 3 Convolution
Dropout p = 0.2

Transition Down (TD)
Batch Normalization
ReLU
1 × 1 Convolution
Dropout p = 0.2
2 × 2 Max Pooling

Transition Up (TU)
3 × 3 Transposed Convolution
stride = 2

Table 1. Building blocks of fully convolutional DenseNets. From left to right: layer used in the model, Transition Down (TD) and Transition
Up (TU). See text for details.

4.1. Architecture and training details

4.2. CamVid dataset

We initialize our models using HeUniform [12] and train
them with RMSprop [33], with an initial learning rate of
1e − 3 and an exponential decay of 0.995 after each epoch.
All models are trained on data augmented with random
crops and vertical ﬂips. For all experiments, we ﬁnetune
our models with full size images and learning rate of 1e − 4.
We use validation set to earlystop the training and the ﬁne-
tuning. We monitor mean IoU or mean accuracy and use
patience of 100 (50 during ﬁnetuning).

We regularized our models with a weight decay of 1e−4
and a dropout rate of 0.2. For batch normalization, we use
current batch statistics at training, validation and test time.

Architecture
Input, m = 3
3 × 3 Convolution, m = 48
DB (4 layers) + TD, m = 112
DB (5 layers) + TD, m = 192
DB (7 layers) + TD, m = 304
DB (10 layers) + TD, m = 464
DB (12 layers) + TD, m = 656
DB (15 layers), m = 896
TU + DB (12 layers), m = 1088
TU + DB (10 layers), m = 816
TU + DB (7 layers), m = 578
TU + DB (5 layers), m = 384
TU + DB (4 layers), m = 256
1 × 1 Convolution, m = c
Softmax

Table 2. Architecture details of FC-DenseNet103 model used in
our experiments. This model is built from 103 convolutional lay-
ers. In the Table we use following notations: DB stands for Dense
Block, TD stands for Transition Down, TU stands for Transition
Up, BN stands for Batch Normalization and m corresponds to the
total number of feature maps at the end of a block. c stands for the
number of classes.

CamVid1 [2] is a dataset of fully segmented videos for
urban scene understanding. We used the split and image
resolution from [1], which consists of 367 frames for train-
ing, 101 frames for validation and 233 frames for test. Each
frame has a size 360 × 480 and its pixels are labeled with
11 semantic classes. Our models were trained with crops
of 224 × 224 and batch size 3. At the end, the model is
ﬁnetuned with full size images.

In Table 3, we report our results for three networks with
respectively (1) 56 layers (FC-DenseNet56), with 4 layers
per dense block and a growth rate of 12; (2) 67 layers (FC-
DenseNet67) with 5 layers per dense block and a growth
rate of 16; and (3) 103 layers (FC-DenseNet103) with a
growth rate k = 16 (see Table 2 for details). We also trained
an architecture using standard convolutions in the upsam-
pling path instead of dense blocks (Classic Upsampling). In
the latter architecture, we used 3 convolutions per resolution
level with respectively 512, 256, 128, 128 and 64 ﬁlters, as
in [27]. Results show clear superiority of the proposed up-
sampling path w.r.t.
the classic one, consistently improv-
ing the IoU signiﬁcantly for all classes. Particularly, we
observe that unrepresented classes beneﬁt notably from the
FC-DenseNet architecture, namely sign, pedestrian, fence,
cyclist experience a crucial boost in performance (ranging
from 15% to 25%).

As expected, when comparing FC-DenseNet56 or FC-
DenseNet67 to FC-DenseNet103, we see that the model
beneﬁts from having more depth as well as more parame-
ters.

When compared to other methods, we show that FC-
DenseNet architectures achieve state-of-the-art, improving
upon models with 10 times more parameters. It is worth
mentioning that our small model FC-DenseNet56 already
outperforms popular architectures with at least 100 times
more parameters.

It is worth noting that images in CamVid correspond to
video frames and, thus, the dataset contains temporal infor-
mation. Some state-of-the-art methods such as [17] incor-
porate long range spatio-temporal regularization to the out-

1http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/

put of a FCN to boost their performance. Our model is able
to outperform such state-of-the-art model, without requir-
ing any temporal smoothing. However, any post-processing
temporal regularization is complementary to our approach
and could bring additional improvements.

Unlike most of the current state-of-the-art methods, FC-
DenseNets have not been pretrained on large datasets such
as ImageNet [6] and could most likely beneﬁt from such
pretraining. More recently, it has been shown that deep net-
works can also boost their performance when pretrained on
data other than natural images, such as video games [26, 28]
or clipart [3], and this an interesting direction to explore.

Figure 3 shows some qualitative segmentation results on
the CamVid dataset. Qualitative results are well aligned
with the quantitative ones, showing sharp segmentations
that account for a lot of details. For example, trees, column
poles, sidewalk and pedestrians appear very well sketched.
Among common errors, we ﬁnd that thin details found in
trees can be confused with column poles (see ﬁfth row),
buses and trucks can be confused with buildings (fourth
row), and shop signs can be confused with road signs (sec-
ond row).

4.3. Gatech dataset

Gatech2 [23] is a geometric scene understanding dataset,
which consists of 63 videos for training/validation and 38
for testing. Each video has between 50 and 300 frames
(with an average of 190). A pixel-wise segmentation map is
provided for each frame. There are 8 classes in the dataset:
sky, ground, buildings, porous (mainly trees), humans, cars,
vertical mix and main mix. The dataset was originally built
to learn 3D geometric structure of outdoor video scenes and
the standard metric for this dataset is mean global accuracy.
We used the FC-DenseNet103 model pretrained on
CamVid, removed the softmax layer, and ﬁnetuned it for
10 epochs with crops of 224 × 224 and batch size 5. Given
the high redundancy in Gatech frames, we used only one
out of 10 frames to train the model and tested it on all full
resolution test set frames.

In Table 4, we report the obtained results. We compare
the results to the recently proposed method for video seg-
mentation of [34], which reports results of their architecture
with 2D and 3D convolutions. Frame-based 2D convolu-
tions do not have temporal information. As it can be seen
in Table 4, our method gives an impressive improvement of
23.7% in global accuracy with respect to previously pub-
lished state-of-the-art with 2D convolutions. Moreover, our
model (trained with only 2D convolutions) also achieves a
signiﬁcant improvement over state-of-the-art models based
on spatio-temporal 3D convolutions (3.4% improvement).

2http://www.cc.gatech.edu/cpl/projects/videogeometriccontext/

5. Discussion

Our fully convolutional DenseNet

implicitly inherits
the advantages of DenseNets, namely: (1) parameter ef-
ﬁciency, as our network has substantially less parame-
ters than other segmentation architectures published for the
Camvid dataset; (2) implicit deep supervision, we tried in-
cluding additional levels of supervision to different layers of
our network without noticeable change in performance; and
(3) feature reuse, as all layers can easily access their pre-
ceding layers not only due to the iterative concatenation of
feature maps in a dense block but also thanks to skip con-
nections that enforce connectivity between downsampling
and upsampling path.

Recent evidence suggest that ResNets behave like en-
semble of relatively shallow networks [35]: ”Residual net-
works avoid the vanishing gradient problem by introducing
short paths which can carry gradient throughout the extent
of very deep networks”. It would be interesting to revisit
this ﬁnding in the context of fully convolutional DenseNets.
Due to iterative feature map concatenation in the dense
block, the gradients are forced to be passed through net-
works of different depth (with different numbers of non-
linearities). Thus, thanks to the smart connectivity pat-
terns, FC-DenseNets might represent an ensemble of vari-
able depth networks. This particular ensemble behavior
would be very interesting for semantic segmentation mod-
els, where the ensemble of different paths throughout the
model would capture the multi-scale appearance of objects
in urban scene.

6. Conclusion

In this paper, we have extended DenseNets and made
them fully convolutional to tackle the problem semantic im-
age segmentation. The main idea behind DenseNets is cap-
tured in dense blocks that perform iterative concatenation
of feature maps. We designed an upsampling path mitigat-
ing the linear growth of feature maps that would appear in
a naive extension of DenseNets.

The resulting network is very deep (from 56 to 103 lay-
ers) and has very few parameters, about 10 fold reduction
w.r.t. state-of-the-art models. Moreover, it improves state-
of-the-art performance on challenging urban scene under-
standing datasets (CamVid and Gatech), without neither ad-
ditional post-processing, pretraining, nor including tempo-
ral information.

Aknowledgements

The authors would like to thank the developers of
Theano [32] and Lasagne [7]. Special thanks to Fr´ed´eric
Bastien for his work assessing the compilation issues.
Thanks to Francesco Visin for his well designed data-
loader [9], as well as Harm de Vries for his support

)

M

(

s
r
e
t
e
m
a
r
a
p

#

d
e
n
i
a
r
t
e
r
P

g
n
i
d
l
i
u
B

e
e
r
T

y
k
S

r
a
C

n
g
i

S

n
a
i
r
t
s
e
d
e
P

e
c
n
e
F

e
l
o
P

k
l
a
w
e
d
i

S

t
s
i
l
c
y
C

68.7

52.0

87.0

58.5

13.4

25.3

17.9

16.0

60.5

24.8

77.8
81.5
82.6
84.0

73.5
77.6
80.2
83.0

71.0
74.6
76.2
77.2

72.2
72.0
75.4
77.3

88.7
89.0
89.0
91.3

92.4
92.4
93.0
93.0

76.1
82.2
84.0
85.6

66.2
73.2
78.2
77.3

32.7
42.3
46.9
49.9

26.9
31.8
40.9
43.9

41.7
48.4
56.3
59.1

37.7
37.9
58.4
59.6

24.4
27.2
35.8
37.6

22.7
26.2
30.7
37.1

19.9
14.3
23.4
16.9

30.8
32.6
38.4
37.8

72.7
75.4
75.3
76.0

69.6
79.9
81.9
82.2

31.0
50.1
55.5
57.2

25.1
31.1
52.1
50.5

Table 3. Results on CamVid dataset. Note that we trained our own pretrained FCN8 model

d
a
o
R

86.2
n/a
n/a
n/a
91.2
92.2
92.2
92.5

90.0
92.8
94.7
94.5

Model

SegNet [1]

DeconvNet [21]
Visin et al. [36]
FCN8 [20]
DeepLab-LFOV [5]
Dilation8 [37]
Dilation8 + FSO [17]

! 29.5
Bayesian SegNet [15] ! 29.5
! 252
! 32.3
! 134.5
! 37.3
! 140.8
! 140.8
(cid:55)
(cid:55)
(cid:55)
(cid:55)

Classic Upsampling
FC-DenseNet56 (k=12)
FC-DenseNet67 (k=16)
FC-DenseNet103 (k=16)

20
1.5
3.5
9.4

y
c
a
r
u
c
c
a

l
a
b
o
l
G

62.5
86.9
85.9
88.7
88.0
−
79.0
88.3

86.8
88.9
90.8
91.5

U
o
I

n
a
e

M

46.4
63.1
48.9
58.8
57.0
61.6
65.3
66.1

55.2
58.9
65.8
66.9

Model

2D models (no time)
2D-V2V-from scratch [34]
FC-DenseNet103

Acc.

55.7
79.4

3D models (incorporate time)
3D-V2V-from scratch [34]
3D-V2V-pretrained [34]

66.7
76.0

Table 4. Results on Gatech dataset

in network parallelization, and Tristan Sylvain. We ac-
knowledge the support of the following agencies for re-
search funding and computing support: Imagia Inc., Span-
ish projects TRA2014-57088-C2-1-R & 2014-SGR-1506,
TECNIOspring-FP7-ACCI grant.

References

[1] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A
deep convolutional encoder-decoder architecture for image
segmentation. CoRR, abs/1511.00561, 2015.

[2] G. J. Brostow, J. Shotton, J. Fauqueur, and R. Cipolla.
Segmentation and recognition using structure from motion
point clouds. In European Conference on Computer Vision
(ECCV), 2008.

[3] L. Castrejon, Y. Aytar, C. Vondrick, H. Pirsiavash, and
A. Torralba. Learning aligned cross-modal representations
from weakly aligned data. CoRR, abs/1607.07295, 2016.
[4] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille. Deeplab: Semantic image segmentation with deep
convolutional nets, atrous convolution, and fully connected
crfs. CoRR, abs/1606.00915, 2016.

[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Semantic image segmentation with deep con-

volutional nets and fully connected crfs.
Conference of Learning Representations (ICLR), 2015.
[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2009.

In International

[7] S. Dieleman, J. Schlter, C. Raffel, E. Olson, and et al.

Lasagne: First release., Aug. 2015.

[8] M. Drozdzal, E. Vorontsov, G. Chartrand, S. Kadoury, and
C. Pal. The importance of skip connections in biomedical
image segmentation. CoRR, abs/1608.04117, 2016.

[9] A. R. F. Visin. Dataset loaders: a python library to load and
preprocess datasets. https://github.com/fvisin/
dataset_loaders, 2017.

[10] C. Gatta, A. Romero, and J. van de Weijer. Unrolling loopy
top-down semantic feedback in convolutional deep networks.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR) workshop, 2014.

[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. CoRR, abs/1512.03385, 2015.
[12] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. CoRR, abs/1502.01852, 2015.

[13] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der
Maaten. Densely connected convolutional networks. CoRR,
abs/1608.06993, 2016.

[14] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
CoRR, abs/1502.03167, 2015.

[15] A. Kendall, V. Badrinarayanan, and R. Cipolla. Bayesian
segnet: Model uncertainty in deep convolutional encoder-
CoRR,
decoder architectures for scene understanding.
abs/1511.02680, 2015.

[16] P. Kr¨ahenb¨uhl and V. Koltun. Efﬁcient inference in fully
connected crfs with gaussian edge potentials. In Advances in
Neural Information Processing Systems (NIPS). 2011.

Figure 3. Qualitative results on the CamVid test set. Pixels labeled in yellow are void class. Each row represents (from left to right):
original image, original annotation (ground truth) and prediction of our model.

[34] D. Tran, L. D. Bourdev, R. Fergus, L. Torresani, and
M. Paluri. Deep end2end voxel2voxel prediction. CoRR,
abs/1511.06681, 2015.

[35] A. Veit, M. J. Wilber, and S. J. Belongie. Residual networks
are exponential ensembles of relatively shallow networks.
CoRR, abs/1605.06431, 2016.

[36] F. Visin, M. Ciccone, A. Romero, K. Kastner, K. Cho,
Y. Bengio, M. Matteucci, and A. Courville. Reseg: A re-
current neural network-based model for semantic segmenta-
tion. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) workshop, 2016.

[37] F. Yu and V. Koltun. Multi-scale context aggregation by di-
lated convolutions. In International Conference of Learning
Representations (ICLR), 2016.

[38] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,
Z. Su, D. Du, C. Huang, and P. Torr. Conditional random
ﬁelds as recurrent neural networks. In International Confer-
ence on Computer Vision (ICCV), 2015.

[17] A. Kundu, V. Vineet, and V. Koltun. Feature space optimiza-
tion for semantic video segmentation. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2016.
[18] C. Lee, S. Xie, P. W. Gallagher, Z. Zhang, and Z. Tu. Deeply-
In International Conference on Artiﬁcial

supervised nets.
Intelligence and Statistics (AISTATS), 2015.

[19] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Dollr, and C. L. Zitnick. Microsoft coco: Common
objects in context. In European Conference on Computer Vi-
sion (ECCV), 2014.

[20] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2015.

[21] H. Noh, S. Hong, and B. Han.

tion network for semantic segmentation.
arXiv:1505.04366, 2015.

Learning deconvolu-
arXiv preprint

[22] S. H. Raza, M. Grundmann, and I. Essa. Geometric con-
text from video. IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2013.

[23] S. H. Raza, M. Grundmann, and I. Essa. Geometric context
In IEEE Conference on Computer Vision and

from video.
Pattern Recognition (CVPR), 2013.

[24] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi.
You only look once: Uniﬁed, real-time object detection.
CoRR, abs/1506.02640, 2015.

[25] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN:
towards real-time object detection with region proposal net-
works. CoRR, abs/1506.01497, 2015.

[26] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing
for data: Ground truth from computer games. In European
Conference on Computer Vision (ECCV), 2016.

[27] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-
tional networks for biomedical image segmentation. In In-
ternational Conference on Medical Image Computing and
Computer-Assisted Intervention (MICAI), 2015.

[28] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M.
Lopez. The synthia dataset: A large collection of synthetic
images for semantic segmentation of urban scenes. In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2016.

[29] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014.

[30] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: A simple way to prevent neu-
ral networks from overﬁtting. Journal of Machine Learning
Research, 15:1929–1958, 2014.

[31] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. CoRR, abs/1409.4842,
2014.

[32] Theano Development Team. Theano: A Python framework
for fast computation of mathematical expressions. arXiv e-
prints, abs/1605.02688, May 2016.

[33] T. Tieleman and G. Hinton. rmsprop adaptive learning. In
COURSERA: Neural Networks for Machine Learning, 2012.

The One Hundred Layers Tiramisu:
Fully Convolutional DenseNets for Semantic Segmentation

Simon J´egou1 Michal Drozdzal2,3 David Vazquez1,4 Adriana Romero1 Yoshua Bengio1
1Montreal Institute for Learning Algorithms 2 ´Ecole Polytechnique de Montr´eal
3Imagia Inc., Montr´eal, 4Computer Vision Center, Barcelona
simon.jegou@gmail.com, michal@imagia.com, dvazquez@cvc.uab.es,

adriana.romero.soriano@umontreal.ca, yoshua.umontreal@gmail.com

7
1
0
2
 
t
c
O
 
1
3
 
 
]

V
C
.
s
c
[
 
 
3
v
6
2
3
9
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

State-of-the-art approaches for semantic image segmen-
tation are built on Convolutional Neural Networks (CNNs).
The typical segmentation architecture is composed of (a)
a downsampling path responsible for extracting coarse se-
mantic features, followed by (b) an upsampling path trained
to recover the input image resolution at the output of the
model and, optionally, (c) a post-processing module (e.g.
Conditional Random Fields) to reﬁne the model predictions.
Recently, a new CNN architecture, Densely Connected
Convolutional Networks (DenseNets), has shown excellent
results on image classiﬁcation tasks. The idea of DenseNets
is based on the observation that if each layer is directly con-
nected to every other layer in a feed-forward fashion then
the network will be more accurate and easier to train.

In this paper, we extend DenseNets to deal with the prob-
lem of semantic segmentation. We achieve state-of-the-art
results on urban scene benchmark datasets such as CamVid
and Gatech, without any further post-processing module
nor pretraining. Moreover, due to smart construction of
the model, our approach has much less parameters than
currently published best entries for these datasets. Code
to reproduce the experiments is publicly available here :
https://github.com/SimJeg/FC-DenseNet

1. Introduction

Convolutional Neural Networks (CNNs) are driving ma-
jor advances in many computer vision tasks, such as im-
age classiﬁcation [29], object detection [25, 24] and seman-
tic image segmentation [20]. The last few years have wit-
nessed outstanding improvements on CNN-based models.
Very deep architectures [29, 11, 31] have shown impres-
sive results on standard benchmarks such as ImageNet [6]
or MSCOCO [19]. State-of-the-art CNNs heavily reduce
the input resolution through successive pooling layers and,

1

Figure 1. Diagram of our architecture for semantic segmentation.
Our architecture is built from dense blocks. The diagram is com-
posed of a downsampling path with 2 Transitions Down (TD) and
an upsampling path with 2 Transitions Up (TU). A circle repre-
sents concatenation and arrows represent connectivity patterns in
the network. Gray horizontal arrows represent skip connections,
the feature maps from the downsampling path are concatenated
with the corresponding feature maps in the upsampling path. Note
that the connectivity pattern in the upsampling and the downsam-
pling paths are different. In the downsampling path, the input to
a dense block is concatenated with its output, leading to a linear
growth of the number of feature maps, whereas in the upsampling
path, it is not.

thus, are well suited for applications where a single predic-
tion per input image is expected (e.g. image classiﬁcation
task).

Fully Convolutional Networks (FCNs) [20, 27] were in-
troduced in the literature as a natural extension of CNNs to
tackle per pixel prediction problems such as semantic im-
age segmentation. FCNs add upsampling layers to standard
CNNs to recover the spatial resolution of the input at the
output layer. As a consequence, FCNs can process images
of arbitrary size.
In order to compensate for the resolu-
tion loss induced by pooling layers, FCNs introduce skip
connections between their downsampling and upsampling
paths. Skip connections help the upsampling path recover
ﬁne-grained information from the downsampling layers.

Among CNN architectures extended as FCNs for seman-
tic segmentation purposes, Residual Networks (ResNets)
[11] make an interesting case. ResNets are designed to
ease the training of very deep networks (of hundreds of
layers) by introducing a residual block that sums two sig-
nals: a non-linear transformation of the input and its identity
mapping. The identity mapping is implemented by means
of a shortcut connection. ResNets have been extended to
work as FCNs [4, 8] yielding very good results in differ-
ent segmentation benchmarks. ResNets incorporate addi-
tional paths to FCN (shortcut paths) and, thus, increase the
number of connections within a segmentation network. This
additional shortcut paths have been shown not only to im-
prove the segmentation accuracy but also to help the net-
work optimization process, resulting in faster convergence
of the training [8].

Recently, a new CNN architecture, called DenseNet, was
introduced in [13]. DenseNets are built from dense blocks
and pooling operations, where each dense block is an itera-
tive concatenation of previous feature maps. This architec-
ture can be seen as an extension of ResNets [11], which per-
forms iterative summation of previous feature maps. How-
ever, this small modiﬁcation has some interesting implica-
tions: (1) parameter efﬁciency, DenseNets are more efﬁ-
cient in the parameter usage; (2) implicit deep supervision,
DenseNets perform deep supervision thanks to short paths
to all feature maps in the architecture (similar to Deeply
Supervised Networks [18]); and (3) feature reuse, all lay-
ers can easily access their preceding layers making it easy
to reuse the information from previously computed feature
maps. The characteristics of DenseNets make them a very
good ﬁt for semantic segmentation as they naturally induce
skip connections and multi-scale supervision.

In this paper, we extend DenseNets to work as FCNs by
adding an upsampling path to recover the full input reso-
lution. Naively building an upsampling path would result
in a computationally intractable number of feature maps
with very high resolution prior to the softmax layer. This
is because one would multiply the high resolution feature

maps with a large number of input ﬁlters (from all the lay-
ers below), resulting in both very large amount of compu-
tation and number of parameters. In order to mitigate this
effect, we only upsample the feature maps created by the
preceding dense block. Doing so allows to have a number of
dense blocks at each resolution of the upsampling path inde-
pendent of the number of pooling layers. Moreover, given
the network architecture, the upsampled dense block com-
bines the information contained in the other dense blocks
of the same resolution. The higher resolution information
is passed by means of a standard skip connection between
the downsampling and the upsampling paths. The details of
the proposed architecture are shown in Figure 1. We eval-
uate our model on two challenging benchmarks for urban
scene understanding, Camvid [2] and Gatech [22], and con-
ﬁrm the potential of DenseNets for semantic segmentation
by improving the state-of-the-art.

Thus, the contributions of the paper can be summarized

as follows:

• We carefully extend the DenseNet architecture [13] to
fully convolutional networks for semantic segmenta-
tion, while mitigating the feature map explosion.

• We highlight that the proposed upsampling path, built
from dense blocks, performs better than upsampling
path with more standard operations, such as the ones
in [27].

• We show that such a network can outperform current
state-of-the-art results on standard benchmarks for ur-
ban scene understanding without neither using pre-
trained parameters nor any further post-processing.

2. Related Work

Recent advances in semantic segmentation have been de-
voted to improve architectural designs by (1) improving
the upsampling path and increasing the connectivity within
FCNs [27, 1, 21, 8]; (2) introducing modules to account for
broader context understanding [36, 5, 37]; and/or (3) en-
dowing FCN architectures with the ability to provide struc-
tured outputs [16, 5, 38].

First, different alternatives have been proposed in the lit-
erature to address the resolution recovery in FCN’s upsam-
pling path; from simple bilinear interpolation [10, 20, 1]
to more sophisticated operators such as unpooling [1, 21]
or transposed convolutions [20]. Skip connections from
the downsampling to the upsampling path have also been
adopted to allow for a ﬁner information recovery [27]. More
recently, [8] presented a thorough analysis on the combina-
tion of identity mapping [11] and long skip connections [27]
for semantic segmentation.

Second, approaches that introduce larger context to se-
mantic segmentation networks include [10, 36, 5, 37]. In

3. Fully Convolutional DenseNets

As mentioned in Section 1, FCNs are built from a down-
sampling path, an upsampling path and skip connections.
Skip connections help the upsampling path recover spa-
tially detailed information from the downsampling path, by
reusing features maps. The goal of our model is to further
exploit the feature reuse by extending the more sophisti-
cated DenseNet architecture, while avoiding the feature ex-
plosion at the upsampling path of the network.

In this section, we detail the proposed model for seman-
tic segmentation. First, we review the recently proposed
DenseNet architecture. Second, we introduce the construc-
tion of the novel upsampling path and discuss its advantages
w.r.t. a naive DenseNet extension and more classical archi-
tectures. Finally, we wrap up with the details of the main
architecture used in Section 4.

3.1. Review of DenseNets

Let x(cid:96) be the output of the (cid:96)th layer. In a standard CNN,
x(cid:96) is computed by applying a non-linear transformation H(cid:96)
to the output of the previous layer x(cid:96)−1

x(cid:96) = H(cid:96)(x(cid:96)−1),

(1)

where H is commonly deﬁned as a convolution followed by
a rectiﬁer non-linearity (ReLU) and often dropout [30].

In order to ease the training of very deep networks,
ResNets [11] introduce a residual block that sums the iden-
tity mapping of the input to the output of a layer. The re-
sulting output x(cid:96) becomes

x(cid:96) = H(cid:96)(x(cid:96)−1) + x(cid:96)−1,

(2)

allowing for the reuse of features and permitting the gra-
dient to ﬂow directly to earlier layers. In this case, H is
deﬁned as the repetition (2 or 3 times) of a block composed
of Batch Normalization (BN) [14], followed by ReLU and
a convolution.

Pushing this idea further, DenseNets [13] design a more
sophisticated connectivity pattern that iteratively concate-
nates all feature outputs in a feedforward fashion. Thus, the
output of the (cid:96)th layer is deﬁned as

x(cid:96) = H(cid:96)([x(cid:96)−1, x(cid:96)−2, ..., x0]),

(3)

where [ ... ] represents the concatenation operation. In this
case, H is deﬁned as BN, followed by ReLU, a convolution
and dropout. Such connectivity pattern strongly encourages
the reuse of features and makes all layers in the architec-
ture receive direct supervision signal. The output dimension
of each layer (cid:96) has k feature maps where k, hereafter re-
ferred as to growth rate parameter, is typically set to a small
value (e.g. k = 12). Thus, the number of feature maps in
DenseNets grows linearly with the depth (e.g. after (cid:96) layers,
the input [x(cid:96)−1, x(cid:96)−2, ..., x0] will have (cid:96) × k feature maps).

Figure 2. Diagram of a dense block of 4 layers. A ﬁrst layer is ap-
plied to the input to create k feature maps, which are concatenated
to the input. A second layer is then applied to create another k
features maps, which are again concatenated to the previous fea-
ture maps. The operation is repeated 4 times. The output of the
block is the concatenation of the outputs of the 4 layers, and thus
contains 4 ∗ k feature maps

[10], an unsupervised global image descriptor is computed
added to the feature maps for each pixel. In [36], Recur-
rent Neural Networks (RNNs) are used to retrieve contex-
tual information by sweeping the image horizontally and
vertically in both directions.
In [5], dilated convolutions
are introduced as an alternative to late CNN pooling layers
to capture larger context without reducing the image reso-
lution. Following the same spirit, [37] propose to provide
FCNs with a context module built as a stack of dilated con-
volutional layers to enlarge the ﬁeld of view of the network.

Third, Conditional Random Fields (CRF) have long been
a popular choice to enforce structure consistency to seg-
mentation outputs. More recently, fully connected CRFs
[16] have been used to include structural properties of the
output of FCNs [5]. Interestingly, in [38], RNN have been
introduced to approximate mean-ﬁeld iterations of CRF op-
timization, allowing for an end-to-end training of both the
FCN and the RNN.

Finally, it is worth noting that current state-of-the-art
FCN architectures for semantic segmentation often rely on
pre-trained models (e.g. VGG [29] or ResNet101 [11]) to
improve their segmentation results [20, 1, 4].

A transition down is introduced to reduce the spatial di-
mensionality of the feature maps. Such transformation is
composed of a 1×1 convolution (which conserves the num-
ber of feature maps) followed by a 2 × 2 pooling operation.
In the remainder of the article, we will call dense block
the concatenation of the new feature maps created at a given
resolution. Figure 2 shows an example of dense block con-
struction. Starting from an input x0 (input image or output
of a transition down) with m feature maps, the ﬁrst layer of
the block generates an output x1 of dimension k by applying
H1(x0). These k feature maps are then stacked to the pre-
vious m feature maps by concatenation ([x1, x0]) and used
as input to the second layer. The same operation is repeated
n times, leading to a new dense block with n × k feature
maps.

3.2. From DenseNets

to Fully Convolutional

DenseNets

The DenseNet architecture described in Subsection 3.1
constitutes the downsampling path of our Fully Convolu-
tional DenseNet (FC-DenseNet). Note that, in the down-
sampling path, the linear growth in the number of features
is compensated by the reduction in spatial resolution of each
feature map after the pooling operation. The last layer of the
downsampling path is referred to as bottleneck.

In order to recover the input spatial resolution, FCNs in-
troduce an upsampling path composed of convolution, up-
sampling operations (transposed convolutions or unpooling
operations) and skip connections.
In FC-DenseNets, we
substitute the convolution operation by a dense block and
an upsampling operation referred to as transition up. Tran-
sition up modules consist of a transposed convolution that
upsamples the previous feature maps. The upsampled fea-
ture maps are then concatenated to the ones coming from
the skip connection to form the input of a new dense block.
Since the upsampling path increases the feature maps spa-
tial resolution, the linear growth in the number of features
would be too memory demanding, especially for the full
resolution features in the pre-softmax layer.

In order to overcome this limitation, the input of a dense
block is not concatenated with its output. Thus, the trans-
posed convolution is applied only to the feature maps ob-
tained by the last dense block and not to all feature maps
concatenated so far. The last dense block summarizes the
information contained in all the previous dense blocks at
the same resolution. Note that some information from ear-
lier dense blocks is lost in the transition down due to the
pooling operation. Nevertheless, this information is avail-
able in the downsampling path of the network and can be
passed via skip connections. Hence, the dense blocks of the
upsampling path are computed using all the available fea-
ture maps at a given resolution. Figure 1 illustrates this idea
in detail.

Therefore, our upsampling path approach allows us to
build very deep FC-DenseNets without a feature map explo-
sion. An alternative way of implementing the upsampling
path would be to perform consecutive transposed convolu-
tions and complement them with skip connections from the
downsampling path in a U-Net [27] or FCN-like [20] fash-
ion. This will be further discussed in Section 4

3.3. Semantic Segmentation Architecture

In this subsection, we detail the main architecture, FC-

DenseNet103, used in Section 4.

First, in Table 1, we deﬁne the dense block layer, tran-
sition down and transition up of the architecture. Dense
block layers are composed of BN, followed by ReLU, a
3 × 3 same convolution (no resolution loss) and dropout
with probability p = 0.2. The growth rate of the layer is set
to k = 16. Transition down is composed of BN, followed
by ReLU, a 1 × 1 convolution, dropout with p = 0.2 and a
non-overlapping max pooling of size 2 × 2. Transition up
is composed of a 3 × 3 transposed convolution with stride 2
to compensate for the pooling operation.

Second, in Table 2, we summarize all Dense103 layers.
This architecture is built from 103 convolutional layers : a
ﬁrst one on the input, 38 in the downsampling path, 15 in
the bottleneck and 38 in the upsampling path. We use 5
Transition Down (TD), each one containing one extra con-
volution, and 5 Transition Up (TU), each one containing a
transposed convolution. The ﬁnal layer in the network is a
1 × 1 convolution followed by a softmax non-linearity to
provide the per class distribution at each pixel.

It is worth noting that, as discussed in Subsection 3.2, the
proposed upsampling path properly mitigates the DenseNet
feature map explosion, leading to reasonable pre-softmax
feature map number of 256.

Finally, the model is trained by minimizing the pixel-

wise cross-entropy loss.

4. Experiments

We evaluate our method on two urban scene understand-
ing datasets: CamVid [2], and Gatech [22]. We trained our
models from scratch without using any extra-data nor post-
processing module. We report the results using the Inter-
section over Union (IoU) metric and the global accuracy
(pixel-wise accuracy on the dataset). For a given class c,
predictions (oi) and targets (yi), the IoU is deﬁned by

IoU (c) =

(cid:80)
(cid:80)

i (oi == c ∧ yi == c)
i (oi == c ∨ yi == c)

,

(4)

where ∧ is a logical and operation, while ∨ is a logical or
operation. We compute IoU by summing over all the pixels
i of the dataset.

Layer
Batch Normalization
ReLU
3 × 3 Convolution
Dropout p = 0.2

Transition Down (TD)
Batch Normalization
ReLU
1 × 1 Convolution
Dropout p = 0.2
2 × 2 Max Pooling

Transition Up (TU)
3 × 3 Transposed Convolution
stride = 2

Table 1. Building blocks of fully convolutional DenseNets. From left to right: layer used in the model, Transition Down (TD) and Transition
Up (TU). See text for details.

4.1. Architecture and training details

4.2. CamVid dataset

We initialize our models using HeUniform [12] and train
them with RMSprop [33], with an initial learning rate of
1e − 3 and an exponential decay of 0.995 after each epoch.
All models are trained on data augmented with random
crops and vertical ﬂips. For all experiments, we ﬁnetune
our models with full size images and learning rate of 1e − 4.
We use validation set to earlystop the training and the ﬁne-
tuning. We monitor mean IoU or mean accuracy and use
patience of 100 (50 during ﬁnetuning).

We regularized our models with a weight decay of 1e−4
and a dropout rate of 0.2. For batch normalization, we use
current batch statistics at training, validation and test time.

Architecture
Input, m = 3
3 × 3 Convolution, m = 48
DB (4 layers) + TD, m = 112
DB (5 layers) + TD, m = 192
DB (7 layers) + TD, m = 304
DB (10 layers) + TD, m = 464
DB (12 layers) + TD, m = 656
DB (15 layers), m = 896
TU + DB (12 layers), m = 1088
TU + DB (10 layers), m = 816
TU + DB (7 layers), m = 578
TU + DB (5 layers), m = 384
TU + DB (4 layers), m = 256
1 × 1 Convolution, m = c
Softmax

Table 2. Architecture details of FC-DenseNet103 model used in
our experiments. This model is built from 103 convolutional lay-
ers. In the Table we use following notations: DB stands for Dense
Block, TD stands for Transition Down, TU stands for Transition
Up, BN stands for Batch Normalization and m corresponds to the
total number of feature maps at the end of a block. c stands for the
number of classes.

CamVid1 [2] is a dataset of fully segmented videos for
urban scene understanding. We used the split and image
resolution from [1], which consists of 367 frames for train-
ing, 101 frames for validation and 233 frames for test. Each
frame has a size 360 × 480 and its pixels are labeled with
11 semantic classes. Our models were trained with crops
of 224 × 224 and batch size 3. At the end, the model is
ﬁnetuned with full size images.

In Table 3, we report our results for three networks with
respectively (1) 56 layers (FC-DenseNet56), with 4 layers
per dense block and a growth rate of 12; (2) 67 layers (FC-
DenseNet67) with 5 layers per dense block and a growth
rate of 16; and (3) 103 layers (FC-DenseNet103) with a
growth rate k = 16 (see Table 2 for details). We also trained
an architecture using standard convolutions in the upsam-
pling path instead of dense blocks (Classic Upsampling). In
the latter architecture, we used 3 convolutions per resolution
level with respectively 512, 256, 128, 128 and 64 ﬁlters, as
in [27]. Results show clear superiority of the proposed up-
sampling path w.r.t.
the classic one, consistently improv-
ing the IoU signiﬁcantly for all classes. Particularly, we
observe that unrepresented classes beneﬁt notably from the
FC-DenseNet architecture, namely sign, pedestrian, fence,
cyclist experience a crucial boost in performance (ranging
from 15% to 25%).

As expected, when comparing FC-DenseNet56 or FC-
DenseNet67 to FC-DenseNet103, we see that the model
beneﬁts from having more depth as well as more parame-
ters.

When compared to other methods, we show that FC-
DenseNet architectures achieve state-of-the-art, improving
upon models with 10 times more parameters. It is worth
mentioning that our small model FC-DenseNet56 already
outperforms popular architectures with at least 100 times
more parameters.

It is worth noting that images in CamVid correspond to
video frames and, thus, the dataset contains temporal infor-
mation. Some state-of-the-art methods such as [17] incor-
porate long range spatio-temporal regularization to the out-

1http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/

put of a FCN to boost their performance. Our model is able
to outperform such state-of-the-art model, without requir-
ing any temporal smoothing. However, any post-processing
temporal regularization is complementary to our approach
and could bring additional improvements.

Unlike most of the current state-of-the-art methods, FC-
DenseNets have not been pretrained on large datasets such
as ImageNet [6] and could most likely beneﬁt from such
pretraining. More recently, it has been shown that deep net-
works can also boost their performance when pretrained on
data other than natural images, such as video games [26, 28]
or clipart [3], and this an interesting direction to explore.

Figure 3 shows some qualitative segmentation results on
the CamVid dataset. Qualitative results are well aligned
with the quantitative ones, showing sharp segmentations
that account for a lot of details. For example, trees, column
poles, sidewalk and pedestrians appear very well sketched.
Among common errors, we ﬁnd that thin details found in
trees can be confused with column poles (see ﬁfth row),
buses and trucks can be confused with buildings (fourth
row), and shop signs can be confused with road signs (sec-
ond row).

4.3. Gatech dataset

Gatech2 [23] is a geometric scene understanding dataset,
which consists of 63 videos for training/validation and 38
for testing. Each video has between 50 and 300 frames
(with an average of 190). A pixel-wise segmentation map is
provided for each frame. There are 8 classes in the dataset:
sky, ground, buildings, porous (mainly trees), humans, cars,
vertical mix and main mix. The dataset was originally built
to learn 3D geometric structure of outdoor video scenes and
the standard metric for this dataset is mean global accuracy.
We used the FC-DenseNet103 model pretrained on
CamVid, removed the softmax layer, and ﬁnetuned it for
10 epochs with crops of 224 × 224 and batch size 5. Given
the high redundancy in Gatech frames, we used only one
out of 10 frames to train the model and tested it on all full
resolution test set frames.

In Table 4, we report the obtained results. We compare
the results to the recently proposed method for video seg-
mentation of [34], which reports results of their architecture
with 2D and 3D convolutions. Frame-based 2D convolu-
tions do not have temporal information. As it can be seen
in Table 4, our method gives an impressive improvement of
23.7% in global accuracy with respect to previously pub-
lished state-of-the-art with 2D convolutions. Moreover, our
model (trained with only 2D convolutions) also achieves a
signiﬁcant improvement over state-of-the-art models based
on spatio-temporal 3D convolutions (3.4% improvement).

2http://www.cc.gatech.edu/cpl/projects/videogeometriccontext/

5. Discussion

Our fully convolutional DenseNet

implicitly inherits
the advantages of DenseNets, namely: (1) parameter ef-
ﬁciency, as our network has substantially less parame-
ters than other segmentation architectures published for the
Camvid dataset; (2) implicit deep supervision, we tried in-
cluding additional levels of supervision to different layers of
our network without noticeable change in performance; and
(3) feature reuse, as all layers can easily access their pre-
ceding layers not only due to the iterative concatenation of
feature maps in a dense block but also thanks to skip con-
nections that enforce connectivity between downsampling
and upsampling path.

Recent evidence suggest that ResNets behave like en-
semble of relatively shallow networks [35]: ”Residual net-
works avoid the vanishing gradient problem by introducing
short paths which can carry gradient throughout the extent
of very deep networks”. It would be interesting to revisit
this ﬁnding in the context of fully convolutional DenseNets.
Due to iterative feature map concatenation in the dense
block, the gradients are forced to be passed through net-
works of different depth (with different numbers of non-
linearities). Thus, thanks to the smart connectivity pat-
terns, FC-DenseNets might represent an ensemble of vari-
able depth networks. This particular ensemble behavior
would be very interesting for semantic segmentation mod-
els, where the ensemble of different paths throughout the
model would capture the multi-scale appearance of objects
in urban scene.

6. Conclusion

In this paper, we have extended DenseNets and made
them fully convolutional to tackle the problem semantic im-
age segmentation. The main idea behind DenseNets is cap-
tured in dense blocks that perform iterative concatenation
of feature maps. We designed an upsampling path mitigat-
ing the linear growth of feature maps that would appear in
a naive extension of DenseNets.

The resulting network is very deep (from 56 to 103 lay-
ers) and has very few parameters, about 10 fold reduction
w.r.t. state-of-the-art models. Moreover, it improves state-
of-the-art performance on challenging urban scene under-
standing datasets (CamVid and Gatech), without neither ad-
ditional post-processing, pretraining, nor including tempo-
ral information.

Aknowledgements

The authors would like to thank the developers of
Theano [32] and Lasagne [7]. Special thanks to Fr´ed´eric
Bastien for his work assessing the compilation issues.
Thanks to Francesco Visin for his well designed data-
loader [9], as well as Harm de Vries for his support

)

M

(

s
r
e
t
e
m
a
r
a
p

#

d
e
n
i
a
r
t
e
r
P

g
n
i
d
l
i
u
B

e
e
r
T

y
k
S

r
a
C

n
g
i

S

n
a
i
r
t
s
e
d
e
P

e
c
n
e
F

e
l
o
P

k
l
a
w
e
d
i

S

t
s
i
l
c
y
C

68.7

52.0

87.0

58.5

13.4

25.3

17.9

16.0

60.5

24.8

77.8
81.5
82.6
84.0

73.5
77.6
80.2
83.0

71.0
74.6
76.2
77.2

72.2
72.0
75.4
77.3

88.7
89.0
89.0
91.3

92.4
92.4
93.0
93.0

76.1
82.2
84.0
85.6

66.2
73.2
78.2
77.3

32.7
42.3
46.9
49.9

26.9
31.8
40.9
43.9

41.7
48.4
56.3
59.1

37.7
37.9
58.4
59.6

24.4
27.2
35.8
37.6

22.7
26.2
30.7
37.1

19.9
14.3
23.4
16.9

30.8
32.6
38.4
37.8

72.7
75.4
75.3
76.0

69.6
79.9
81.9
82.2

31.0
50.1
55.5
57.2

25.1
31.1
52.1
50.5

Table 3. Results on CamVid dataset. Note that we trained our own pretrained FCN8 model

d
a
o
R

86.2
n/a
n/a
n/a
91.2
92.2
92.2
92.5

90.0
92.8
94.7
94.5

Model

SegNet [1]

DeconvNet [21]
Visin et al. [36]
FCN8 [20]
DeepLab-LFOV [5]
Dilation8 [37]
Dilation8 + FSO [17]

! 29.5
Bayesian SegNet [15] ! 29.5
! 252
! 32.3
! 134.5
! 37.3
! 140.8
! 140.8
(cid:55)
(cid:55)
(cid:55)
(cid:55)

Classic Upsampling
FC-DenseNet56 (k=12)
FC-DenseNet67 (k=16)
FC-DenseNet103 (k=16)

20
1.5
3.5
9.4

y
c
a
r
u
c
c
a

l
a
b
o
l
G

62.5
86.9
85.9
88.7
88.0
−
79.0
88.3

86.8
88.9
90.8
91.5

U
o
I

n
a
e

M

46.4
63.1
48.9
58.8
57.0
61.6
65.3
66.1

55.2
58.9
65.8
66.9

Model

2D models (no time)
2D-V2V-from scratch [34]
FC-DenseNet103

Acc.

55.7
79.4

3D models (incorporate time)
3D-V2V-from scratch [34]
3D-V2V-pretrained [34]

66.7
76.0

Table 4. Results on Gatech dataset

in network parallelization, and Tristan Sylvain. We ac-
knowledge the support of the following agencies for re-
search funding and computing support: Imagia Inc., Span-
ish projects TRA2014-57088-C2-1-R & 2014-SGR-1506,
TECNIOspring-FP7-ACCI grant.

References

[1] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A
deep convolutional encoder-decoder architecture for image
segmentation. CoRR, abs/1511.00561, 2015.

[2] G. J. Brostow, J. Shotton, J. Fauqueur, and R. Cipolla.
Segmentation and recognition using structure from motion
point clouds. In European Conference on Computer Vision
(ECCV), 2008.

[3] L. Castrejon, Y. Aytar, C. Vondrick, H. Pirsiavash, and
A. Torralba. Learning aligned cross-modal representations
from weakly aligned data. CoRR, abs/1607.07295, 2016.
[4] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille. Deeplab: Semantic image segmentation with deep
convolutional nets, atrous convolution, and fully connected
crfs. CoRR, abs/1606.00915, 2016.

[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Semantic image segmentation with deep con-

volutional nets and fully connected crfs.
Conference of Learning Representations (ICLR), 2015.
[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2009.

In International

[7] S. Dieleman, J. Schlter, C. Raffel, E. Olson, and et al.

Lasagne: First release., Aug. 2015.

[8] M. Drozdzal, E. Vorontsov, G. Chartrand, S. Kadoury, and
C. Pal. The importance of skip connections in biomedical
image segmentation. CoRR, abs/1608.04117, 2016.

[9] A. R. F. Visin. Dataset loaders: a python library to load and
preprocess datasets. https://github.com/fvisin/
dataset_loaders, 2017.

[10] C. Gatta, A. Romero, and J. van de Weijer. Unrolling loopy
top-down semantic feedback in convolutional deep networks.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR) workshop, 2014.

[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. CoRR, abs/1512.03385, 2015.
[12] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. CoRR, abs/1502.01852, 2015.

[13] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der
Maaten. Densely connected convolutional networks. CoRR,
abs/1608.06993, 2016.

[14] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
CoRR, abs/1502.03167, 2015.

[15] A. Kendall, V. Badrinarayanan, and R. Cipolla. Bayesian
segnet: Model uncertainty in deep convolutional encoder-
CoRR,
decoder architectures for scene understanding.
abs/1511.02680, 2015.

[16] P. Kr¨ahenb¨uhl and V. Koltun. Efﬁcient inference in fully
connected crfs with gaussian edge potentials. In Advances in
Neural Information Processing Systems (NIPS). 2011.

Figure 3. Qualitative results on the CamVid test set. Pixels labeled in yellow are void class. Each row represents (from left to right):
original image, original annotation (ground truth) and prediction of our model.

[34] D. Tran, L. D. Bourdev, R. Fergus, L. Torresani, and
M. Paluri. Deep end2end voxel2voxel prediction. CoRR,
abs/1511.06681, 2015.

[35] A. Veit, M. J. Wilber, and S. J. Belongie. Residual networks
are exponential ensembles of relatively shallow networks.
CoRR, abs/1605.06431, 2016.

[36] F. Visin, M. Ciccone, A. Romero, K. Kastner, K. Cho,
Y. Bengio, M. Matteucci, and A. Courville. Reseg: A re-
current neural network-based model for semantic segmenta-
tion. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) workshop, 2016.

[37] F. Yu and V. Koltun. Multi-scale context aggregation by di-
lated convolutions. In International Conference of Learning
Representations (ICLR), 2016.

[38] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,
Z. Su, D. Du, C. Huang, and P. Torr. Conditional random
ﬁelds as recurrent neural networks. In International Confer-
ence on Computer Vision (ICCV), 2015.

[17] A. Kundu, V. Vineet, and V. Koltun. Feature space optimiza-
tion for semantic video segmentation. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2016.
[18] C. Lee, S. Xie, P. W. Gallagher, Z. Zhang, and Z. Tu. Deeply-
In International Conference on Artiﬁcial

supervised nets.
Intelligence and Statistics (AISTATS), 2015.

[19] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Dollr, and C. L. Zitnick. Microsoft coco: Common
objects in context. In European Conference on Computer Vi-
sion (ECCV), 2014.

[20] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2015.

[21] H. Noh, S. Hong, and B. Han.

tion network for semantic segmentation.
arXiv:1505.04366, 2015.

Learning deconvolu-
arXiv preprint

[22] S. H. Raza, M. Grundmann, and I. Essa. Geometric con-
text from video. IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2013.

[23] S. H. Raza, M. Grundmann, and I. Essa. Geometric context
In IEEE Conference on Computer Vision and

from video.
Pattern Recognition (CVPR), 2013.

[24] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi.
You only look once: Uniﬁed, real-time object detection.
CoRR, abs/1506.02640, 2015.

[25] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN:
towards real-time object detection with region proposal net-
works. CoRR, abs/1506.01497, 2015.

[26] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing
for data: Ground truth from computer games. In European
Conference on Computer Vision (ECCV), 2016.

[27] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-
tional networks for biomedical image segmentation. In In-
ternational Conference on Medical Image Computing and
Computer-Assisted Intervention (MICAI), 2015.

[28] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M.
Lopez. The synthia dataset: A large collection of synthetic
images for semantic segmentation of urban scenes. In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2016.

[29] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014.

[30] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: A simple way to prevent neu-
ral networks from overﬁtting. Journal of Machine Learning
Research, 15:1929–1958, 2014.

[31] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. CoRR, abs/1409.4842,
2014.

[32] Theano Development Team. Theano: A Python framework
for fast computation of mathematical expressions. arXiv e-
prints, abs/1605.02688, May 2016.

[33] T. Tieleman and G. Hinton. rmsprop adaptive learning. In
COURSERA: Neural Networks for Machine Learning, 2012.

