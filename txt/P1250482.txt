Wasserstein Introspective Neural Networks

Kwonjoon Lee

Weijian Xu

Fan Fan

Zhuowen Tu

University of California San Diego
{kwl042, wex041, f1fan, ztu}@ucsd.edu

8
1
0
2
 
r
p
A
 
7
 
 
]

V
C
.
s
c
[
 
 
5
v
5
7
8
8
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

We present Wasserstein introspective neural networks
(WINN) that are both a generator and a discriminator
within a single model. WINN provides a signiﬁcant im-
provement over the recent introspective neural networks
(INN) method by enhancing INN’s generative modeling ca-
pability. WINN has three interesting properties: (1) A math-
ematical connection between the formulation of the INN al-
gorithm and that of Wasserstein generative adversarial net-
works (WGAN) is made. (2) The explicit adoption of the
Wasserstein distance into INN results in a large enhance-
ment to INN, achieving compelling results even with a single
classiﬁer — e.g., providing nearly a 20 times reduction in
model size over INN for unsupervised generative modeling.
(3) When applied to supervised classiﬁcation, WINN also
gives rise to improved robustness against adversarial exam-
ples in terms of the error reduction. In the experiments, we
report encouraging results on unsupervised learning prob-
lems including texture, face, and object modeling, as well as
a supervised classiﬁcation task against adversarial attacks.
Our code is available online1.

1. Introduction

Performance within the task of supervised image classi-
ﬁcation has been vastly improved in the era of deep learn-
ing using modern convolutional neural network (CNN) [30]
based discriminative classiﬁers [26, 43, 31, 42, 16, 52, 18].
On the other hand, unsupervised generative models in deep
learning were previously attained using methods under the
umbrella of graphical models — e.g., the Boltzmann ma-
chine [17] or autoencoder [4, 24] architectures. However,
the rich representational power seen within convolution-
based (discriminative) models is not being directly enjoyed
in these generative models. Later, inverting convolutional
neural networks in order to convert internal representations
into a real image was investigated in [6, 27]. Recently,
generative adversarial networks (GAN) [11] and followup

1https://github.com/kjunelee/WINN

Figure 1: Schematic illustration of Wasserstein introspective neural net-
works for unsupervised learning. The left ﬁgure shows the input examples;
the bottom ﬁgures show the pseudo-negatives (purple crosses) being pro-
gressively synthesized; the top ﬁgures show the classiﬁcation between the
given examples (positives) and synthesized pseudo-negatives (negatives).
The right ﬁgure shows the model learned to approach the target distribution
based on the given data.

works [38, 2, 14] have attracted a tremendous amount of
attention in machine learning and computer vision by pro-
ducing high quality synthesized images by training a pair
of competing models against one another in an adversar-
ial manner. While a generator tries to create “fake” images
to fool the discriminator, the discriminator attempts to dis-
cern between these “real” (given training) and “fake” im-
ages. After convergence, the generator is able to produce
images faithful to the underlying data distribution.

Before the deep learning era [17], generative model-
ing had been an area with a steady pace of development
[1, 5, 54, 36, 49, 53]. These models were guided by rigor-
ous statistical theories which, although nice in theory, did
not succeed in producing synthesized images with practical
quality.

In terms of building generative models from discrimina-
tive classiﬁers, there have been early attempts in [48, 44].

In [48], a generative model was obtained from a repeat-
edly trained boosting algorithm [8] using a weak classi-
ﬁer whereas [44] used a strong classiﬁer in order to self-
generate negative examples or “pseudo-negatives”.

To address the lack of richness in representation and ef-
ﬁciency in synthesis, convolutional neural networks were
adopted in introspective neural networks (INN) [29, 21] to
build a single model that is simultaneously generative and
discriminative. The generative modeling aspect was studied
in [29] where a sequence of CNN classiﬁers (10 − 60) were
trained, while the power within the classiﬁcation setting was
revealed in [21] in the form of introspective convolutional
networks (ICN) that used only a single CNN classiﬁer. Al-
though INN models [29, 21] point to a promising direction
to obtain a single model being both a good generator and a
strong discriminative classiﬁer, a sequence of CNNs were
needed to generate realistic synthesis. As a result, this re-
quirement may serve as a possible bottleneck with respect
to training complexity and model size.

Recently, a generic formulation [2] was developed
within the GAN model family to incorporate a Wasserstein
objective to alleviate the well-known difﬁculty in GAN
training. Motivated by introspective neural networks (INN)
[29] and this Wasserstein objective [2], we propose to adopt
the Wasserstein term into the INN formulation to enhance
the modeling capability. The resulting model, Wasserstein
introspective neural networks (WINN) shows greatly en-
hanced modeling capability over INN by having 20× re-
duction in the number of CNN classiﬁers.

2. Signiﬁcance and Related Work

We make several interesting observations for WINN:

• A mathematical connection between the WGAN formulation
[2] and the INN algorithm [29] is made to better understand the
overall objective function within INN.

• By adopting the Wasserstein distance into INN, we are able to
generate images using a single CNN in WINN with even higher
quality than those by INN that uses 20 CNNs (as seen in Figure
2, 4, 5, 6, and 7; the similar underlying CNN architectures are
used in WINN and INN). WINN achieves a signiﬁcant reduc-
tion in model complexity over INN, making the generator more
practical.

• Within texture modeling, INN and WINN are able to inherently
model the input image space, making the synthesis of large tex-
ture images realistic, whereas GAN projects a noise vector onto
the image space making the image patch stitching more difﬁ-
cult (although extensions exist), as demonstrated in Figure 2.
• To compare with the family of GAN models, we compute In-
ception scores using the standard procedure on the CIFAR-10
datasets and observed modest results. Here, we typically train
4-5 cascades to boost the numbers but WINN with one CNN is
already promising. Overall, modern GAN variants (e.g., [14])
still outperform our WINN with better quality images. Some
results are shown in Figure 7.

• To test the robustness of the discriminative abilities of WINN,
we directly make WINN into a discriminative classiﬁer by
training it on the standard MNIST and SVHN datasets. Not
only are we able to improve over the previous ICN [21] clas-
siﬁer for supervised classiﬁcation, we also observe a large
improvement in robustness against adversarial examples com-
pared with the baseline CNN, ResNet, and the competing ICN.
In terms of other related work, we brieﬂy discuss some

existing methods below.
Wasserstein GAN. A closely related work to our WINN al-
gorithm is the Wasserstein generative adversarial networks
(WGAN) method [2, 14]. While WINN adopts the Wasser-
stein distance as motivated by WGAN, our overall algo-
rithm is still within the family of introspective neural net-
works (INN) [29, 21]. WGAN on the other hand is a vari-
ant of GAN with an improvement over GAN by having an
objective that is easier to train. The level of difference be-
tween WINN and WGAN is similar to that between INN
[29, 21] and GAN [12]. The overall comparisons between
INN and GAN have been described in [29, 21].
Generative ConvNets. Recently, there has also been a clus-
ter of algorithms developed in [50, 51, 15] where Langevin
dynamics are adopted in generator CNNs. However, the
models proposed in [50, 51, 15] do not perform introspec-
tion (Figure 1) and their generator and discriminator com-
ponents are still somewhat separated; thus, their generators
are not used as effective discriminative classiﬁers to per-
form state-of-the-art classiﬁcation on standard supervised
machine learning tasks. Their training processes are also
more complex than those of INN and WINN.
Deep energy models (DEMs) [35]. DEM [35] extends the
standard density estimation by using multi-layer neural net-
works (MLNN) with a rather complex training procedure.
The probability model in DEM includes both the raw input
and the features computed by MLNN. WINN instead takes
a more general and simplistic form and is easier to train (see
Eq. (1)). In general, DEM belongs to the minimum descrip-
tion length (MDL) family models in which the maximum
likelihood is achieved. WINN, instead, has a formulation
being simultaneously discriminative and generative.

3. Introspective Neural Networks

3.1. Brief introduction of INN

We ﬁrst brieﬂy introduce the introspective neural net-
work method (INNg) [29] for generative modeling and its
companion model [21] which focuses on the classiﬁcation
aspect. The main motivation behind the INN work [29, 21]
is to make a convolutional neural network classiﬁer simulta-
neously discriminative and generative. A single CNN clas-
siﬁer is trained in an introspective manner to improve the
standard supervised classiﬁcation result [21], however, a se-
quence of CNNs (typically 10 − 60) is needed to be able to
synthesize images of good quality [29].

Figure 2: Comparison of texture synthesis algorithms. Gatys et al. [9], and TextureNet [45] results are from [45].

Figure 1 shows a brief illustration of a single introspec-
tive CNN classiﬁer [21]. We discuss our basic unsuper-
vised formulation next. Suppose we are given a set of train-
ing examples: S = {xi
| i = 1, . . . , n} where we as-
sume each xi ∈ Rm — e.g., m = 4096 for images of
size 64 × 64. These will constitute positive examples of
the patterns/targets we wish to model. The main idea of
INN is to deﬁne pseudo-negative examples that are to be
self-generated by the discriminative classiﬁer itself. We
therefore deﬁne label y for each example x, y = +1 if
x is from the given training set and y = −1 if x is self-
generated. Motivated by the generative via discriminative
learning (GDL) framework [44], one could try to learn the
generative model for the given examples, p(x|y = +1), by
a sequentially learned distribution of the pseudo-negative
samples, pt(x|y = −1; Wt) which is abbreviated as p−
(x)
Wt
where Wt includes all the model parameters learned at step
t.

1
Zt

p−
Wt

)}·p−

exp{w(1)

·φ(x; w(0)

t

t

t

t

(x) =

·φ(x; w(0)

0 (x), t = 1, . . . , T
(1)
where Zt = (cid:82) exp{w(1)
)}·p−
0 (x)dx and the ini-
tial distribution p−
0 (x) such as a Gaussian distribution over
the entire space of x ∈ Rm. The discriminative classiﬁer
is a convolutional neural network (CNN) parameterized by
Wt = (w(0)
) where w(1)
denotes the weights of the
top layer combining the features through φ(x; w(0)
) (e.g.,
softmax layer) and w(0)
parameterizing the internal repre-
sentations. The synthesis process through which pseudo-
negative samples are generated is carried out by stochastic
gradient Langevin dynamics [47] as

, w(1)
t

t

t

t

t

∆x =

∇(w(1)

· φ(x; w(0)

)) + η

t

t

(cid:15)
2

where η ∼ N (0, (cid:15)) is a Gaussian distribution and (cid:15) is the
step size that is annealed in the sampling process. Overall,
we desire

p−
Wt

(x) t=∞→ p(x|y = +1),

(2)

using the iterative reclassiﬁcation-by-synthesis process [21,
29] guided by Eq. (1).

3.2. Connection to the Wasserstein distance

overall

is carried out

training process,

reclassiﬁcation-by-
The
synthesis,
iteratively without an explicit
objective function. The generative adversarial network
(GAN) model [11] instead has an objective function
formulated in a minimax fashion with the generator and
discriminator competing against each other. The Wasser-
stein generative adversarial network (WGAN) work [2]
improves GAN [11] by replacing the Jensen-Shannon dis-
tance with an efﬁcient approximation of the Earth-Mover
distance [2]. Also, there has been further generalization of
the GAN family models in [32].

Let p+(x) ≡ p(x|y = +1) be the target distribution and
p−
W(x) ≡ p(x|y = −1; W) be the pseudo-negative distri-
bution parameterized by W. Next, we show a connection
between the INN framework and the WGAN formulation
[2], whose objective (rewritten with our notations) can be
deﬁned as

min
W

max
||f ||L≤1

Ex∼p+[f (x)] − Ex∼p−

[f (x)],

W

(3)

where ||f ||L ≤ 1 denotes the space of 1-Lipschitz func-
tions. To build the connection between Eq. (3) of WGAN
and Eq. (1) of INN, we ﬁrst present the following lemma.

Lemma 1 Considering f (x) = ln p+(x)
and assuming its
p−
W (x)
1-Lipschitz property, we have a lower bound on the Wasser-
stein distance by

max
||f ||L≤1
≥ KL(p+||p−

Ex∼p+[f (x)] − Ex∼p−

[f (x)]

W

W) + KL(p−

W||p+)

(4)

where KL(p||q) denotes the Kullback-Leibler divergence
between the two distributions p and q, and KL(p+||p−
W) +
KL(p−

W||p+) is the Jeffreys divergence.

Proof: see Appendix A.
Note that using the Bayes’ rule, the ratio of the generative
probabilities p(x|y=+1)
p(x|y=−1) in Lemma 1 can be turned into the
ratio of the discriminative probabilities p(y=+1|x)
p(y=−1|x) assuming
equal priors p(y = +1) = p(y = −1).

Theorem 1 The introspective neural network formulation
(Eq. (1)) implicitly minimizes a lower bound of the WGAN
objective [2] (Eq. (3)).
Proof: see Appendix B.

4. Wasserstein Introspective Networks

Algorithm 1 Outline of WINN-single training algorithm. We
use k = 3 and λ = 10.

while Wt has not converged do

// Classiﬁcation-step:
for k steps do

(cid:80)m

i , i = 1, . . . , m.

m} from S+.
1 ,··· ,x−

Sample m positive samples {x+
1 ,··· ,x+
m} from St
Sample m pseudo-negative samples {x−
−.
Sample m random numbers {α1,··· ,αm} from U [0, 1].
i + (1 − αi)x−
ˆxi ← αix+
Perform stochastic gradient descent:
(x−
∇Wt
end for
// Synthesis-step:
Sample r noise samples {x1, · · · , xr} from p−
Perform stochastic gradient ascent with early-stopping.
St+1
− ← St
t ← t + 1.

− ∪ {x1, · · · , xr}.

i )]+λ((cid:107)∇ˆxi

i=1{[fWt

(ˆxi)(cid:107)2−1)2}.

0 (x).

i )−fWt

(x+

fWt

1
m

end while

Here we present the formulation for WINN building
upon the formulation of the prior introspective learning
works presented in Section 3.

4.1. WINN algorithm

We denote our unlabeled input training data as S+ =
{xi|yi = +1, i = 1, . . . , n}. Also, we denote the set of
all the self-generated pseudo-negative samples up to step
t as St
− = {xi|yi = −1, i = 1, . . . , l}. In other words,
St
− consists of pseudo-negatives x sampled from our model
p−
(x) for t ≥ 1 where Wt is the model parameter vector
Wt
at step t.

Classiﬁcation-step. The classiﬁcation-step can be viewed
as training a classiﬁer to approximate the Wasserstein dis-
tance between S+ and St
− for t ≥ 1. Note that we also keep
pseudo-negatives from earlier stages – which are essentially
the mistakes of the earlier stages – to prevent the classiﬁer
forgetting what it has learned in previous stages. We use
CNNs parametrized by Wt as base classiﬁers. Let fWt(·)
denote the output of ﬁnal fully connected layer (without
passing through sigmoid nonlinearity) of the CNN. In the

previous introspective learning frameworks [29, 21], the
classiﬁer learning objective was to minimize the following
standard cross-entropy loss function on S+ ∪ St
−:

L(Wt)=−{E

x+∼p+ ln σ[+fWt (x+)]+E

ln σ[−fWt (x−)]}

x−∼p

−
Wt

where σ(·) denotes the sigmoid nonlinearity. Motivated by
Section 3.2, in WINN training we wish to minimize the fol-
lowing Wasserstein loss function by the stochastic gradient
descent algorithm via backpropagation:

L(Wt)=−(cid:2)E

x+∼p+ fWt (x+)−E

fWt (x−)(cid:3)

(5)

x−∼p

−
Wt

To enforce the function fWt to be 1-Lipschitz, we add

the following gradient penalty term [14] to L(Wt):

λEˆx∼pˆx [((cid:107)∇ˆxfWt (ˆx)(cid:107)2 − 1)2]

where ˆx = αx+ + (1 − α)x−, x+ ∼ p+, x− ∼ p−
Wt
α ∼ U [0, 1].

, and

Synthesis-step. Obtaining increasingly difﬁcult pseudo-
negative samples is an integral part of the introspective
learning framework, as it is crucial for tightening the de-
cision boundary. To this end, we develop an efﬁcient sam-
pling procedure under the Wasserstein formulation. After
the classiﬁcation-step, we obtain the following distribution
of pseudo-negatives:

p−
Wt

(x) =

exp{fWt(x)} · p−

0 (x), t = 1, . . . , T

(6)

1
Zt

where Zt = (cid:82) exp{fWt(x)} · p−
0 (x)dx; the initial distri-
bution p−
0 (x) is a Gaussian distribution G(x; 0, σ2) or the
distribution deﬁned in Appendix D. We ﬁnd that the distri-
bution of Appendix D encourages the diversity of sampled
images.
The following equivalence is shown in [29, 21]:

p(y = +1|x; Wt)
p(y = −1|x; Wt)

= exp{fWt(x)}.

(7)

The sampling strategy of [29, 21] was to carry out gradient
ascent on the term ln p(y=+1|x;Wt)
p(y=−1|x;Wt) . In Lemma 1 we chose
f (x) to be ln p+(x)
. Using Bayes’ rule, it is easy to see
p−
W (x)
p(y=−1|x;Wt) is loosely connected to ∇ ln p+(x)
that ∇ ln p(y=+1|x;Wt)
(x)
Also, [2, 14] argue that fWt(x) correlates with the quality
of the sample x. This motivates us to use the following sam-
pling strategy. After initializing x by drawing a fair sample
from p−
0 (x), we increase fWt(x) using gradient ascent on
the image x via backpropagation. Speciﬁcally, as shown in
[47], we can obtain fair samples from the distribution p−
Wt
using the following update rule:

p−
Wt

.

∆x =

∇fWt(x) + η

(cid:15)
2

where (cid:15) is a time-varying step size and η is a random vari-
able following the Gaussian distribution N (0, (cid:15)). Gaussian
noise term is added to make samples cover the full distribu-
tion. Inspired by [20], we found that injecting noise in the
image space could be substituted by applying Dropout to
the higher layers of CNN. In practice, we were able obtain
the samples of enough diversity without step size annealing
and noise injection.
As an early stopping criterion, we empirically ﬁnd that
the following is effective: (1) we measure the minimum
and maximum fWt(·) of positive examples; (2) we set the
early stopping threshold to a random number from the uni-
form distribution between these two numbers. Intuitively,
by matching the value of fWt (·) positives and pseudo-
negatives, we expect to obtain pseudo-negative samples that
match the quality of positive samples.

4.2. Expanding model capacity

In practice, we ﬁnd that the version with the single
classiﬁer – which we call WINN-single – is expressive
enough to capture the generative distribution under vari-
ety of applications. The introspective learning formulation
[44, 29, 21] allows us to model more complex distributions
by adding a sequence of cascaded classiﬁers parameterized
by (W1, . . . , WK). Then, we can model the distribution as:

1
Zt

p−
Wk (x) =

exp{fWk (x)} · p−

Wk−1(x), k = 2, . . . , K

(8)
In the next sections, we demonstrate the modeling capabil-
ity of WINN under cascaded classiﬁers, as well as its ag-
nosticy to the type of base classiﬁer.

4.3. GAN’s discriminator vs. WINN’s classiﬁer

(a) WGAN-GP discriminator

(b) WINN-single

Figure 3: Synthesized images from the discriminators of WGAN and
WINN trained on the CelebA dataset. Both use the same ResNet archi-
tecture for the discriminator as the one adopted in [14].

GAN uses the competing discriminator and the gener-
ator whereas WINN maintains a single model being both
generative and discriminative. Some general comparisons
between GAN and INN have been provided in [29, 21]. Be-
low we make a few additional observations that are worth
future exploration and discussions.

• First, the generator of GAN is a cost-effective option for image
patch synthesis, as it works in a feed-forward fashion. However
the generator of GAN is not meant to be trained as a classiﬁer
to perform the standard classiﬁcation task, while the generator

in the introspective framework is also a strong classiﬁer. Sec-
tion 5.6 shows WINN to have signiﬁcant robustness to external
adversarial examples.

• Second, the discriminator in GAN is meant to be a critic but
not a generator. To show whether or not the discriminator in
GAN can also be used as a generator, we train WGAN-GP [14]
on the CelebA face dataset. Using the same CNN architecture
(ResNet from [14]) that was used as GAN’s discriminator, we
also train a WINN-single model, making GAN’s discrimina-
tor and WINN-single to have the identical CNN architecture.
Applying the sampling strategy to WGAN-GP’s discriminator
allows us to synthesize image form WGAN-GP’s discriminator
as well and we show some samples in Figure 3 (a). These syn-
thesized images are not like faces, yet they have been classiﬁed
by the discriminator of WGAN-GP as “real” faces; this demon-
strates the separation between the generator and the discrimina-
tor in GAN. In contrast, images synthesized by WINN-single’s
CNN classiﬁer are faces like, as shown in Figure 3 (b).

• Third, the discriminator of GAN may not be used as a direct
discriminative classiﬁer for the standard supervised learning
task. As shown and discussed in ICN [21], the introspective
framework has the ability of classiﬁcation for discriminator.

5. Experiments

5.1. Implementation

Classiﬁcation-step. For training the discriminator network,
we use Adam [23] with a mini-batch size of 100. The learn-
ing rate was set to 0.0001. We set β1 = 0.0, and β2 = 0.9,
inspired by [14]. Each batch consists of 50 positive im-
ages sampled from the set of positives S+ and 50 pseudo-
negative images sampled from the set of pseudo-negatives
S−. In each iteration, we limit total number of training im-
ages to 10, 000.
Synthesis-step. For synthesizing pseudo-negative images
via back-propagation, we perform gradient ascent on the
image space. In the ﬁrst cascade, each image is initialized
with a noise sampled from the distribution described in Ap-
pendix D. In the later cascades, images are initialized with
the images sampled from the last cascade. We use Adam
with a mini-batch size of 100. The learning rate was set to
0.01. We set β1 = 0.9, and β2 = 0.99.
5.2. Texture modeling

We evaluate the texture modeling capability of WINN.
For a fair comparison, we use the same 7 texture images
presented in [9] where each texture image has a size of 256
× 256. We follow the training method of Section 5.1 ex-
cept that positive images are constructed by cropping 64
× 64 patches from the source texture image at random po-
sitions. We use network architecture of Appendix C. Af-
ter training is done on the 64×64 patch-based model, we
try to synthesize texture images of arbitrary size using the
anysize-image-generation method following [29]. During
the synthesis process, we keep a single working image of

previous generative modeling works since it contains large
pose variations and background clutters. The network archi-
tecture adopted here is described in Appendix C. In Figure
5, we show some synthesized face images using WINN-
single and WINN, as well as those by DCGAN [38], INNg-
single, and INNg [29]. WINN-single attains image quality
even higher than that of INNg (12 CNNs).
5.4. SVHN modeling

Real data

DCGAN

INNg-single

INNg

WINN-single (ours) WINN-4CNNs (ours)

Figure 6: Images generated by various models trained on SVHN. DCGAN
[38] result is from [29].

SVHN [34] consists of 32 × 32 images from Google
It contains 73, 257 training images, 26, 032
Street View.
test images, and 531, 131 extra images. We use only the
training images for the unsupervised SVHN modeling. We
use the ResNet architecture described in [14]. Generated
images by WINN-single and WINN (4 CNN classiﬁers) as
well as DCGAN and INN are shown in Figure 6. The im-
provement of WINN over INNg is evident.
5.5. CIFAR-10 modeling
Table 1: Inception score on CIFAR-10. “-L” in Improved GANs means
without labels.

Method
Real data
WGAN-GP [14]
WGAN [2]
DCGAN [38] (in [19])
ALI [7] (in [46])
Improved GANs (-L) [40]
INNg-single [29]
INNg [29]
WINN-single (ours)
WINN-5CNNs (ours)

Score
11.95 ± .20
7.86 ± .07
5.88 ± .07
6.16 ± .07
5.34 ± .05
4.36 ± .04
1.95 ± .01
3.04 ± .02
4.62 ± .05
5.58 ± .05

CIFAR-10 [25] consists of 50, 000 training images and
10, 000 test images of size 32 × 32 in 10 classes. We use
training images augmented by horizontal ﬂips [26] for un-
supervised CIFAR-10 modeling. We use the ResNet given
in [14]. Figure 7 shows generated images by various mod-
els.

To measure the semantic discriminability, we compute
the Inception scores [40] on 50, 000 generated images.

Figure 4: More texture synthesis results. Gatys et al. [9] and TextureNets
[45] results are from [45].

size 320×320. Note that we expand the image so that cen-
ter 256×256 pixels are covered with equal probability. In
each iteration, we sample 200 patches from the working im-
age, and perform gradient ascent on the chosen patches. For
the overlapping pixels between patches, we take the average
of the gradients assigned to such pixels. We show synthe-
sized texture images in Figure 2 and 4. WINN-single shows
a signiﬁcant improvement over INNg-single and compara-
ble results to INNg (using 20 CNNs).
It is worth noting
that [10, 45] leverage rich features of VGG-19 network pre-
trained on ImageNet. WINN and INNg instead train net-
works from scratch.
5.3. CelebA face modeling

Real data

DCGAN

INNg-single

INNg

WINN-single (ours) WINN-4CNNs (ours)
Figure 5: Images generated by various models trained on CelebA.

The CelebA dataset [33] consists of 202, 599 face im-
ages of celebrities. This dataset has been widely used in the

Real data

DCGAN

WINN-single (ours)

WINN-5CNNs (ours)

Figure 7: Images generated by models trained on CIFAR-10.

WINN shows its clear advantage over INN. WINN-5CNNs
produces a result close to WGAN but there is still a gap to
the state-of-the-art results by WGAN-GP.
5.6. Image classiﬁcation and adversarial examples

To demonstrate the robustness of WINN as a discrimi-
native classiﬁer, we present experiments on the supervised
classiﬁcation tasks.

Table 2: Test errors on MNIST and SVHN. When training on SVHN,
we only use training set. All results except WINN-single and baseline
ResNet-32 are from [21]. [21] adopted DCGAN [38] discriminator as their
CNN architecture. The advantage of WINN over a vanilla CNN is evident.
When applied to a stronger baseline such as ResNet-32, WINN is not los-
ing ResNet’s superior classiﬁcation capability in standard supervised clas-
siﬁcation, while attaining special generative capability and robustness to
adversarial attacks (see Table 3 and 4) that do not exist in ResNet. The
images below on the right are the generated samples by the corresponding
WINN (ResNet-32) classiﬁers reported in the table.

Method
MNIST
Baseline vanilla CNN (4 layers)
CNN + GDL [44]
CNN + DCGAN [38]
ICN [21]
WINN-single vanilla (ours)
Baseline ResNet-32
WINN-single ResNet-32 (ours)
SVHN
Baseline ResNet-32
WINN-single ResNet-32 (ours)

Error

0.89%
0.85%
0.84%
0.81%
0.67%
0.45%
0.48%

4.64%
4.50%

Training Methods. We add the Wasserstein loss term to
the ICN [21] loss function, obtaining the following:

L(Wt) =

− (cid:80)

xi∈S+

ln

(cid:80)K

(1)yi
·φ(xi;w(0)
exp{w
t
(1)k
k=1 exp{w
t
fWt(xi) − (cid:80)

)}
t
·φ(xi;w(0)

xi∈S+

t

)}

fWt(xi)

(cid:18)

(cid:80)

+α

xi∈St
−

+λEˆx∼pˆx[((cid:107)∇ˆxfWt(ˆx)(cid:107)2 − 1)2]

(cid:19)

t

, w(1)1
t

>, w(0)

, ..., w(1)K
t

where Wt =< w(0)
denotes the
t
internal parameters for the CNN, and w(1)k
denotes the top-
layer weights for the k-th class. In the experiments, we set
the weight of the WINN loss, α, to 0.01. We use the vanilla
network architecture resembling [21] as the baseline CNN,
which has less ﬁlters and parameters than the one in [21].
We also use a ResNet-32 architecture with Layer Normal-
ization [3] on MNIST and SVHN. In the classiﬁcation-step,

t

we use Adam with a ﬁxed learning rate of 0.001, β1 of 0.0.
In the synthesis-step, we use the Adam optimizer with a
learning rate of 0.02 and β1 of 0.9. Table 2 shows the errors
on MNIST and SVHN.

Table 3: Adversarial examples comparison between the baseline CNN
and WINN on MNIST. We ﬁrst generate N adversarial examples using
method A and count the number of adversarial examples misclassiﬁed by
A (= NA). Adversarial error of A is deﬁned as test error rate against
adversarial examples (= NA/N ). Then among A’s mistakes, we count
the number of adversarial examples misclassiﬁed by B (= NA∩B). Then
error correction rate by B is 1 − NA∩B/NA. ↑ means higher the better;
↓ means lower the better. The comparisons are made in two groups: the
ﬁrst group builds on top a vanilla 4 layer CNN and the second group adopts
ResNet-32. Note that the corresponding errors for these classiﬁers on the
standard MNIST supervised classiﬁcation task can be seen in Table 2.

Method
Baseline vanilla CNN
ICN [21]
WINN-single vanilla (ours)
Baseline ResNet-32
WINN-single ResNet-32 (ours)

Adversarial error Correction rate Correction rate
by Baseline ↓
by Method ↑
-
-
58.68%
52.58%
46.93%
90.00%
-
-
43.69%
89.68%

of Method ↓
32.41%
19.02%
7.99%
11.28%
2.05%

Robustness to adversarial examples. It is argued in [11]
that discriminative CNN’s vulnerability to adversarial ex-
amples primarily arises due to its linear nature. Since the
reclassiﬁcation-by-synthesis process helps tighten the de-
cision boundary (Figure 1), one might expect that CNNs
trained with the WINN algorithm are more robust to ad-
versarial examples. Note that unlike existing methods for
adversarial defenses [13, 28], our method does not train net-
works with speciﬁc types of adversarial examples. With test
images of MNIST and SVHN, we adopt “fast gradient sign
method” [11] ((cid:15) = 0.125 for MNIST and (cid:15) = 0.005 for
SVHN) to generate adversarial examples clipped to range
[−1, 1], which differs from [21]. We experiment with two
networks having the same architecture and only differing
in training method (the standard cross-entropy loss vs. the
WINN procedure). We call the former as the baseline CNN.
We summarize the results in Table 3. Compared to ICN
[21], WINN signiﬁcantly reduces the adversarial error to
7.99% and improves the correction rate to 90.00%. In ad-
dition, we have adopted the ResNet-32 architecture into
WINN. See Table 3 and 4. We still obtain the adversarial
error reduction and correction rate improvement on MNIST
and SVHN ((cid:15) = 0.005) with ResNet-32. Our observation is
that WINN is not necessarily improving over a strong base-
line for the supervised classiﬁcation task but its advantage
on adversarial attacks is evident.

Table 4: Adversarial examples comparison between the baseline ResNet-
32 [16] and WINN on SVHN. Note that the corresponding errors for these
classiﬁers on the standard supervised SVHN classiﬁcation task can be seen
in Table 2.

Method
Baseline ResNet-32
WINN-single ResNet-32 (ours)

Adversarial error Correction rate Correction rate
by Baseline ↓
by Method ↑
-
-
51.37%
74.39%

of Method
29.29%
19.53%

5.7. Agnostic to different architectures

In Figure 8, we demonstrate our algorithm being agnos-
tic to the type of classiﬁer, by varying network architectures
to ResNet [16] and DenseNet [18]. Little modiﬁcation was
required to adapt two architectures for WINN.

(1+

p

+
min
2

where JD(p+; p−
Jeffreys divergence.

)KL(p+||p−

W )≤JD(p+;p−
W) = KL(p+||p−

W )≤(1+ 2
W) + KL(p−

)KL(p+||p−
W||p+) is the

+
min

p

W ),

Proof. Based on the Pinsker’s inequality [37, 41], it is

observed that

WINN-single (ResNet-13)

WINN-single (DenseNet-20)

Figure 8: Synthesized CelebA images with varying network architectures.
We use a single CNN for each experiment.

6. Conclusion

In this paper, we have introduced Wasserstein introspec-
tive neural networks (WINN) that produce encouraging re-
sults as a generator and a discriminative classiﬁer at the
same time. WINN is able to achieve model size reduction
over the previous introspective neural networks (INN) by
a factor of 20. In most of the images shown in the paper,
we ﬁnd a single CNN classiﬁer in WINN being sufﬁcient to
produce visually appealing images as well as signiﬁcant er-
ror reduction against adversarial examples. WINN is agnos-
tic to the architecture design of CNN and we demonstrate
results on three networks including a vanilla CNN, ResNet
[16], and DenseNet [18] networks where popular CNN dis-
criminative classiﬁers are turned into generative models un-
der the WINN procedure. WINN can be adopted in a wide
range of applications in computer vision such as image clas-
siﬁcation, recognition, and generation.
Acknowledgements. This work is supported by NSF
IIS-1717431 and NSF IIS-1618477. The authors thank
Justin Lazarow, Long Jin, Hubert Le, Ying Nian Wu, Max
Welling, Richard Zemel, and Tong Zhang for valuable dis-
cussions.

7. Appendix

A. Proof of Lemma 1.

Proof. Plugging f (x) = ln p+(x)
p−
W (x)

into Eq. (3), we have

Ex∼p+ [f (x)]−E
= Ex∼p+ [ln p+ (x)

p

−
W

(x)

= (cid:82) p+(x) ln p+ (x)

p
= KL(p+||p−

−
W

(x)

[f (x)]

x∼p

−
W

]−E

x∼p

−
W
dx−(cid:82) p−

[ln p+ (x)

]

p

−
W

(x)

W (x) ln p+ (x)

p

−
W

(x)

dx

W )+KL(p−

W ||p+)

(cid:3)

B. Proof of Theorem 1.

Corollary 1 The Jeffreys divergence in Eq. (4) of lemma
1 is lower and upper bounded by KL(p+||p−
W) up to some
multiplicative constant

KL(p−

W||p+) ≥

|p−

W − p+|2 log e,

1
2

where |p−
we also have

W −p+| is total variation (TV) distance. From [41]

KL(p−

W||p+) ≤

|p−

W − p+|2,

log e
p+
min

where p+
to KL(p+||p−
|p−

W − p+| ≡ |p+ − p−
W|,

min = minx p+(x). Applying the above bounds
W) and using the symmetry of the TV distance

p+
min
2

KL(p+||p−

W) ≤ KL(p−

W||p+) ≤

KL(p+||p−

W).

2
p+
min

W) + KL(p−

W) + KL(p−
W).

Plugging the equation above into the Jeffreys divergence,
we observe that KL(p+||p−
W||p+) is upper and
(cid:3)
lower bounded by by KL(p+||p−
Now we can look at theorem 1. It was shown in [21]
that Eq. (1) reduces KL(p+||p−
W), which bounds the Jef-
freys divergence KL(p+||p−
W||p+) as shown in
corollary 1. Lemma 1 shows the connection between Jef-
freys divergence and the WGAN objective (Eq. (3)) when
f (x) = ln p+(x)
. We therefore can see that the formula-
p−
W (x)
tion of introspective neural networks (Eq. (1)) connects to
(cid:3)
a lower bound of the WGAN [2] objective (Eq. (3)).
C. Texture and CelebA Modeling Architecture. Inspired
by [22], we design a CNN architecture for 64 × 64 image
as in Table 5. We use Swish [39] non-linearity after each
convolutional layer. We add Layer Normalization [3] after
each convolution except the ﬁrst layer, following [14].
Table 5: Network architecture for the texture and face image modeling.

Textures and CelebA
Filter size/stride

Alternative Initialization

Filter size/stride

Layer
Input
Conv3-32
Conv3-64
Avg pool
Conv3-64
Conv3-128
Avg pool
Conv3-128
Conv3-256
Avg pool
Conv3-256
Conv3-512
Avg pool
FC-1

3×3/1
3×3/1
2×2/2
3×3/1
3×3/1
2×2/2
3×3/1
3×3/1
2×2/2
3×3/1
3×3/1
2×2/2

Output size
64×64×3
64×64×32
64×64×64
32×32×64
32×32×64
32×32×128
16×16×128
16×16×128
16×16×256
8×8×256
8×8×256
8×8×256
4×4×512
1×1×1

Layer
Input
Conv5-256
Upsample
Conv5-128
Upsample
Conv5-64
Upsample
Conv5-64
Upsample

5×5/1

5×5/1

5×5/1

5×5/1

Output size
4×4×512
4×4×256
8×8×256
8×8×128
16×16×128
16×16×64
32×32×64
32×32×3
64×64×3

D. Alternative Initializations. We sample an initial
pseudo-negative image by applying an operation deﬁned by
the network above to a tensor of size 4 × 4 × 512 sampled
from U [−1, 1]. The weights of the network are sampled
from G(0, 0.12). We do not apply any nonlinearities in the
network. We add Layer Normalization [3] after each con-
volution except the last layer.

References

[1] D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. A learn-
ing algorithm for boltzmann machines. Cognitive science,
9(1):147–169, 1985. 1

[2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gen-
erative adversarial networks. In ICML, 2017. 1, 2, 3, 4, 6,
8

[3] L. J. Ba, R. Kiros, and G. E. Hinton. Layer normalization.

CoRR, abs/1607.06450, 2016. 7, 8

[4] P. Baldi. Autoencoders, unsupervised learning, and deep ar-
chitectures. In ICML Workshop on Unsupervised and Trans-
fer Learning, pages 37–49, 2012. 1

[5] S. Della Pietra, V. Della Pietra, and J. Lafferty. Inducing fea-
tures of random ﬁelds. IEEE transactions on pattern analysis
and machine intelligence, 19(4):380–393, 1997. 1

[6] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning
to generate chairs with convolutional neural networks.
In
CVPR, 2015. 1
[7] V. Dumoulin,

I. Belghazi, B. Poole, O. Mastropietro,
A. Lamb, M. Arjovsky, and A. Courville. Adversarially
learned inference. In ICLR, 2017. 6

[8] Y. Freund and R. E. Schapire. A decision-theoretic general-
ization of on-line learning and an application to boosting. J.
of Comp. and Sys. Sci., 55(1), 1997. 2

[9] L. Gatys, A. S. Ecker, and M. Bethge. Texture synthesis
using convolutional neural networks. In NIPS, 2015. 3, 5, 6
[10] L. A. Gatys, A. S. Ecker, and M. Bethge. A neural algorithm

of artistic style. arXiv preprint arXiv:1508.06576, 2015. 6

[11] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In NIPS, 2014. 1, 3, 7

[12] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets.
In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger, edi-
tors, Advances in Neural Information Processing Systems 27,
pages 2672–2680. Curran Associates, Inc., 2014. 2

[13] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and

harnessing adversarial examples. In ICLR, 2015. 7

[14] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and
In

Improved training of wasserstein gans.

A. Courville.
NIPS, 2017. 1, 2, 4, 5, 6, 8

[15] T. Han, Y. Lu, S.-C. Zhu, and Y. N. Wu. Alternating back-
propagation for generator network. In AAAI, 2017. 2
[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 7, 8

[17] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learn-
ing algorithm for deep belief nets. Neural computation,
18(7):1527–1554, 2006. 1

[18] G. Huang*, Z. Liu*, L. van der Maaten, and K. Q. Wein-
berger. Densely connected convolutional networks.
In
CVPR, 2017. 1, 8

[19] X. Huang, Y. Li, O. Poursaeed, J. Hopcroft, and S. Belongie.
Stacked generative adversarial networks. In CVPR, 2017. 6
[20] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks. In CVPR,
2017. 5

[21] L. Jin, J. Lazarow, and Z. Tu.

Introspective classiﬁcation

with convolutional nets. In NIPS, 2017. 2, 3, 4, 5, 7, 8
[22] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive
growing of gans for improved quality, stability, and variation.
In ICLR, 2018. 8

[23] D. Kingma and J. Ba. Adam: A method for stochastic opti-

[24] D. P. Kingma and M. Welling. Auto-encoding variational

mization. In ICLR, 2015. 5

bayes. In ICLR, 2014. 1

[25] A. Krizhevsky, V. Nair, and G. Hinton. Cifar-10 (canadian

institute for advanced research). 6

[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

ImageNet
Classiﬁcation with Deep Convolutional Neural Networks. In
NIPS, 2012. 1, 6

[27] T. D. Kulkarni, W. F. Whitney, P. Kohli, and J. Tenenbaum.
Deep convolutional inverse graphics network. In NIPS, 2015.
1

[28] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial ma-

chine learning at scale. In ICLR, 2017. 7

[29] J. Lazarow*, L. Jin*, and Z. Tu.

Introspective neural net-
works for generative modeling. In ICCV, 2017. 2, 3, 4, 5,
6

[30] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. Howard,
W. Hubbard, and L. Jackel. Backpropagation applied to
handwritten zip code recognition. In Neural Computation,
1989. 1

[31] C.-Y. Lee*, S. Xie*, P. Gallagher, Z. Zhang, and Z. Tu.

Deeply-supervised nets. In AISTATS, 2015. 1

[32] S. Liu, O. Bousquet, and K. Chaudhuri. Approximation and
convergence properties of generative adversarial learning. In
NIPS, 2017. 3

[33] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face

attributes in the wild. In ICCV, 2015. 6

[34] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In NIPS workshop on deep learning and unsu-
pervised feature learning, volume 2011, page 5, 2011. 6
[35] J. Ngiam, Z. Chen, P. W. Koh, and A. Y. Ng. Learning deep

energy models. In ICML, 2011. 2

[36] B. A. Olshausen and D. J. Field. Sparse coding with an over-
complete basis set: A strategy employed by v1? Vision re-
search, 37(23):3311–3325, 1997. 1

[37] M. S. Pinsker. Information and information stability of ran-

dom variables and processes. 1960. 8

[38] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. In ICLR, 2016. 1, 6, 7

[39] P. Ramachandran, B. Zoph, and Q. V. Le. Searching for ac-

tivation functions. CoRR, abs/1710.05941, 2017. 8

[40] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Rad-
ford, and X. Chen. Improved techniques for training gans. In
NIPS, 2016. 6

[41] I. Sason and S. Verd´u. f -divergence inequalities.

IEEE
Transactions on Information Theory, 62(11):5973–6006,
2016. 8

[42] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
1

[43] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 1

[44] Z. Tu. Learning generative models via discriminative ap-

proaches. In CVPR, 2007. 1, 2, 3, 5, 7

[45] D. Ulyanov, V. Lebedev, A. Vedaldi, and V. Lempitsky. Tex-
ture networks: Feed-forward synthesis of textures and styl-
ized images. In ICML, 2016. 3, 6

[46] D. Warde-Farley and Y. Bengio. Improving generative adver-
sarial networks with denoising feature matching. In ICLR,
2017. 6

[47] M. Welling and Y. W. Teh. Bayesian learning via stochastic

gradient langevin dynamics. In ICML, 2011. 3, 4

[48] M. Welling, R. S. Zemel, and G. E. Hinton. Self supervised

boosting. In NIPS, 2002. 1

[49] Y. N. Wu, Z. Si, H. Gong, and S.-C. Zhu. Learning active ba-
sis model for object detection and recognition. International
journal of computer vision, 90(2):198–235, 2010. 1

[50] J. Xie, Y. Lu, R. Gao, S.-C. Zhu, and Y. N. Wu. Cooperative
learning of energy-based model and latent variable model via
mcmc teaching. In AAAI, 2018. 2

[51] J. Xie, Y. Lu, S.-C. Zhu, and Y. N. Wu. A theory of genera-

tive convnet. In ICML, 2016. 2

[52] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated
residual transformations for deep neural networks. In CVPR,
2017. 1

[53] S.-C. Zhu and D. Mumford. A stochastic grammar of im-
ages. Foundations and Trends R(cid:13) in Computer Graphics and
Vision, 2(4):259–362, 2007. 1

[54] S. C. Zhu, Y. N. Wu, and D. Mumford. Minimax entropy
principle and its application to texture modeling. Neural
Computation, 9(8):1627–1660, 1997. 1

Wasserstein Introspective Neural Networks

Kwonjoon Lee

Weijian Xu

Fan Fan

Zhuowen Tu

University of California San Diego
{kwl042, wex041, f1fan, ztu}@ucsd.edu

8
1
0
2
 
r
p
A
 
7
 
 
]

V
C
.
s
c
[
 
 
5
v
5
7
8
8
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

We present Wasserstein introspective neural networks
(WINN) that are both a generator and a discriminator
within a single model. WINN provides a signiﬁcant im-
provement over the recent introspective neural networks
(INN) method by enhancing INN’s generative modeling ca-
pability. WINN has three interesting properties: (1) A math-
ematical connection between the formulation of the INN al-
gorithm and that of Wasserstein generative adversarial net-
works (WGAN) is made. (2) The explicit adoption of the
Wasserstein distance into INN results in a large enhance-
ment to INN, achieving compelling results even with a single
classiﬁer — e.g., providing nearly a 20 times reduction in
model size over INN for unsupervised generative modeling.
(3) When applied to supervised classiﬁcation, WINN also
gives rise to improved robustness against adversarial exam-
ples in terms of the error reduction. In the experiments, we
report encouraging results on unsupervised learning prob-
lems including texture, face, and object modeling, as well as
a supervised classiﬁcation task against adversarial attacks.
Our code is available online1.

1. Introduction

Performance within the task of supervised image classi-
ﬁcation has been vastly improved in the era of deep learn-
ing using modern convolutional neural network (CNN) [30]
based discriminative classiﬁers [26, 43, 31, 42, 16, 52, 18].
On the other hand, unsupervised generative models in deep
learning were previously attained using methods under the
umbrella of graphical models — e.g., the Boltzmann ma-
chine [17] or autoencoder [4, 24] architectures. However,
the rich representational power seen within convolution-
based (discriminative) models is not being directly enjoyed
in these generative models. Later, inverting convolutional
neural networks in order to convert internal representations
into a real image was investigated in [6, 27]. Recently,
generative adversarial networks (GAN) [11] and followup

1https://github.com/kjunelee/WINN

Figure 1: Schematic illustration of Wasserstein introspective neural net-
works for unsupervised learning. The left ﬁgure shows the input examples;
the bottom ﬁgures show the pseudo-negatives (purple crosses) being pro-
gressively synthesized; the top ﬁgures show the classiﬁcation between the
given examples (positives) and synthesized pseudo-negatives (negatives).
The right ﬁgure shows the model learned to approach the target distribution
based on the given data.

works [38, 2, 14] have attracted a tremendous amount of
attention in machine learning and computer vision by pro-
ducing high quality synthesized images by training a pair
of competing models against one another in an adversar-
ial manner. While a generator tries to create “fake” images
to fool the discriminator, the discriminator attempts to dis-
cern between these “real” (given training) and “fake” im-
ages. After convergence, the generator is able to produce
images faithful to the underlying data distribution.

Before the deep learning era [17], generative model-
ing had been an area with a steady pace of development
[1, 5, 54, 36, 49, 53]. These models were guided by rigor-
ous statistical theories which, although nice in theory, did
not succeed in producing synthesized images with practical
quality.

In terms of building generative models from discrimina-
tive classiﬁers, there have been early attempts in [48, 44].

In [48], a generative model was obtained from a repeat-
edly trained boosting algorithm [8] using a weak classi-
ﬁer whereas [44] used a strong classiﬁer in order to self-
generate negative examples or “pseudo-negatives”.

To address the lack of richness in representation and ef-
ﬁciency in synthesis, convolutional neural networks were
adopted in introspective neural networks (INN) [29, 21] to
build a single model that is simultaneously generative and
discriminative. The generative modeling aspect was studied
in [29] where a sequence of CNN classiﬁers (10 − 60) were
trained, while the power within the classiﬁcation setting was
revealed in [21] in the form of introspective convolutional
networks (ICN) that used only a single CNN classiﬁer. Al-
though INN models [29, 21] point to a promising direction
to obtain a single model being both a good generator and a
strong discriminative classiﬁer, a sequence of CNNs were
needed to generate realistic synthesis. As a result, this re-
quirement may serve as a possible bottleneck with respect
to training complexity and model size.

Recently, a generic formulation [2] was developed
within the GAN model family to incorporate a Wasserstein
objective to alleviate the well-known difﬁculty in GAN
training. Motivated by introspective neural networks (INN)
[29] and this Wasserstein objective [2], we propose to adopt
the Wasserstein term into the INN formulation to enhance
the modeling capability. The resulting model, Wasserstein
introspective neural networks (WINN) shows greatly en-
hanced modeling capability over INN by having 20× re-
duction in the number of CNN classiﬁers.

2. Signiﬁcance and Related Work

We make several interesting observations for WINN:

• A mathematical connection between the WGAN formulation
[2] and the INN algorithm [29] is made to better understand the
overall objective function within INN.

• By adopting the Wasserstein distance into INN, we are able to
generate images using a single CNN in WINN with even higher
quality than those by INN that uses 20 CNNs (as seen in Figure
2, 4, 5, 6, and 7; the similar underlying CNN architectures are
used in WINN and INN). WINN achieves a signiﬁcant reduc-
tion in model complexity over INN, making the generator more
practical.

• Within texture modeling, INN and WINN are able to inherently
model the input image space, making the synthesis of large tex-
ture images realistic, whereas GAN projects a noise vector onto
the image space making the image patch stitching more difﬁ-
cult (although extensions exist), as demonstrated in Figure 2.
• To compare with the family of GAN models, we compute In-
ception scores using the standard procedure on the CIFAR-10
datasets and observed modest results. Here, we typically train
4-5 cascades to boost the numbers but WINN with one CNN is
already promising. Overall, modern GAN variants (e.g., [14])
still outperform our WINN with better quality images. Some
results are shown in Figure 7.

• To test the robustness of the discriminative abilities of WINN,
we directly make WINN into a discriminative classiﬁer by
training it on the standard MNIST and SVHN datasets. Not
only are we able to improve over the previous ICN [21] clas-
siﬁer for supervised classiﬁcation, we also observe a large
improvement in robustness against adversarial examples com-
pared with the baseline CNN, ResNet, and the competing ICN.
In terms of other related work, we brieﬂy discuss some

existing methods below.
Wasserstein GAN. A closely related work to our WINN al-
gorithm is the Wasserstein generative adversarial networks
(WGAN) method [2, 14]. While WINN adopts the Wasser-
stein distance as motivated by WGAN, our overall algo-
rithm is still within the family of introspective neural net-
works (INN) [29, 21]. WGAN on the other hand is a vari-
ant of GAN with an improvement over GAN by having an
objective that is easier to train. The level of difference be-
tween WINN and WGAN is similar to that between INN
[29, 21] and GAN [12]. The overall comparisons between
INN and GAN have been described in [29, 21].
Generative ConvNets. Recently, there has also been a clus-
ter of algorithms developed in [50, 51, 15] where Langevin
dynamics are adopted in generator CNNs. However, the
models proposed in [50, 51, 15] do not perform introspec-
tion (Figure 1) and their generator and discriminator com-
ponents are still somewhat separated; thus, their generators
are not used as effective discriminative classiﬁers to per-
form state-of-the-art classiﬁcation on standard supervised
machine learning tasks. Their training processes are also
more complex than those of INN and WINN.
Deep energy models (DEMs) [35]. DEM [35] extends the
standard density estimation by using multi-layer neural net-
works (MLNN) with a rather complex training procedure.
The probability model in DEM includes both the raw input
and the features computed by MLNN. WINN instead takes
a more general and simplistic form and is easier to train (see
Eq. (1)). In general, DEM belongs to the minimum descrip-
tion length (MDL) family models in which the maximum
likelihood is achieved. WINN, instead, has a formulation
being simultaneously discriminative and generative.

3. Introspective Neural Networks

3.1. Brief introduction of INN

We ﬁrst brieﬂy introduce the introspective neural net-
work method (INNg) [29] for generative modeling and its
companion model [21] which focuses on the classiﬁcation
aspect. The main motivation behind the INN work [29, 21]
is to make a convolutional neural network classiﬁer simulta-
neously discriminative and generative. A single CNN clas-
siﬁer is trained in an introspective manner to improve the
standard supervised classiﬁcation result [21], however, a se-
quence of CNNs (typically 10 − 60) is needed to be able to
synthesize images of good quality [29].

Figure 2: Comparison of texture synthesis algorithms. Gatys et al. [9], and TextureNet [45] results are from [45].

Figure 1 shows a brief illustration of a single introspec-
tive CNN classiﬁer [21]. We discuss our basic unsuper-
vised formulation next. Suppose we are given a set of train-
ing examples: S = {xi
| i = 1, . . . , n} where we as-
sume each xi ∈ Rm — e.g., m = 4096 for images of
size 64 × 64. These will constitute positive examples of
the patterns/targets we wish to model. The main idea of
INN is to deﬁne pseudo-negative examples that are to be
self-generated by the discriminative classiﬁer itself. We
therefore deﬁne label y for each example x, y = +1 if
x is from the given training set and y = −1 if x is self-
generated. Motivated by the generative via discriminative
learning (GDL) framework [44], one could try to learn the
generative model for the given examples, p(x|y = +1), by
a sequentially learned distribution of the pseudo-negative
samples, pt(x|y = −1; Wt) which is abbreviated as p−
(x)
Wt
where Wt includes all the model parameters learned at step
t.

1
Zt

p−
Wt

)}·p−

exp{w(1)

·φ(x; w(0)

t

t

t

t

(x) =

·φ(x; w(0)

0 (x), t = 1, . . . , T
(1)
where Zt = (cid:82) exp{w(1)
)}·p−
0 (x)dx and the ini-
tial distribution p−
0 (x) such as a Gaussian distribution over
the entire space of x ∈ Rm. The discriminative classiﬁer
is a convolutional neural network (CNN) parameterized by
Wt = (w(0)
) where w(1)
denotes the weights of the
top layer combining the features through φ(x; w(0)
) (e.g.,
softmax layer) and w(0)
parameterizing the internal repre-
sentations. The synthesis process through which pseudo-
negative samples are generated is carried out by stochastic
gradient Langevin dynamics [47] as

, w(1)
t

t

t

t

t

∆x =

∇(w(1)

· φ(x; w(0)

)) + η

t

t

(cid:15)
2

where η ∼ N (0, (cid:15)) is a Gaussian distribution and (cid:15) is the
step size that is annealed in the sampling process. Overall,
we desire

p−
Wt

(x) t=∞→ p(x|y = +1),

(2)

using the iterative reclassiﬁcation-by-synthesis process [21,
29] guided by Eq. (1).

3.2. Connection to the Wasserstein distance

overall

is carried out

training process,

reclassiﬁcation-by-
The
synthesis,
iteratively without an explicit
objective function. The generative adversarial network
(GAN) model [11] instead has an objective function
formulated in a minimax fashion with the generator and
discriminator competing against each other. The Wasser-
stein generative adversarial network (WGAN) work [2]
improves GAN [11] by replacing the Jensen-Shannon dis-
tance with an efﬁcient approximation of the Earth-Mover
distance [2]. Also, there has been further generalization of
the GAN family models in [32].

Let p+(x) ≡ p(x|y = +1) be the target distribution and
p−
W(x) ≡ p(x|y = −1; W) be the pseudo-negative distri-
bution parameterized by W. Next, we show a connection
between the INN framework and the WGAN formulation
[2], whose objective (rewritten with our notations) can be
deﬁned as

min
W

max
||f ||L≤1

Ex∼p+[f (x)] − Ex∼p−

[f (x)],

W

(3)

where ||f ||L ≤ 1 denotes the space of 1-Lipschitz func-
tions. To build the connection between Eq. (3) of WGAN
and Eq. (1) of INN, we ﬁrst present the following lemma.

Lemma 1 Considering f (x) = ln p+(x)
and assuming its
p−
W (x)
1-Lipschitz property, we have a lower bound on the Wasser-
stein distance by

max
||f ||L≤1
≥ KL(p+||p−

Ex∼p+[f (x)] − Ex∼p−

[f (x)]

W

W) + KL(p−

W||p+)

(4)

where KL(p||q) denotes the Kullback-Leibler divergence
between the two distributions p and q, and KL(p+||p−
W) +
KL(p−

W||p+) is the Jeffreys divergence.

Proof: see Appendix A.
Note that using the Bayes’ rule, the ratio of the generative
probabilities p(x|y=+1)
p(x|y=−1) in Lemma 1 can be turned into the
ratio of the discriminative probabilities p(y=+1|x)
p(y=−1|x) assuming
equal priors p(y = +1) = p(y = −1).

Theorem 1 The introspective neural network formulation
(Eq. (1)) implicitly minimizes a lower bound of the WGAN
objective [2] (Eq. (3)).
Proof: see Appendix B.

4. Wasserstein Introspective Networks

Algorithm 1 Outline of WINN-single training algorithm. We
use k = 3 and λ = 10.

while Wt has not converged do

// Classiﬁcation-step:
for k steps do

(cid:80)m

i , i = 1, . . . , m.

m} from S+.
1 ,··· ,x−

Sample m positive samples {x+
1 ,··· ,x+
m} from St
Sample m pseudo-negative samples {x−
−.
Sample m random numbers {α1,··· ,αm} from U [0, 1].
i + (1 − αi)x−
ˆxi ← αix+
Perform stochastic gradient descent:
(x−
∇Wt
end for
// Synthesis-step:
Sample r noise samples {x1, · · · , xr} from p−
Perform stochastic gradient ascent with early-stopping.
St+1
− ← St
t ← t + 1.

− ∪ {x1, · · · , xr}.

i )]+λ((cid:107)∇ˆxi

i=1{[fWt

(ˆxi)(cid:107)2−1)2}.

0 (x).

i )−fWt

(x+

fWt

1
m

end while

Here we present the formulation for WINN building
upon the formulation of the prior introspective learning
works presented in Section 3.

4.1. WINN algorithm

We denote our unlabeled input training data as S+ =
{xi|yi = +1, i = 1, . . . , n}. Also, we denote the set of
all the self-generated pseudo-negative samples up to step
t as St
− = {xi|yi = −1, i = 1, . . . , l}. In other words,
St
− consists of pseudo-negatives x sampled from our model
p−
(x) for t ≥ 1 where Wt is the model parameter vector
Wt
at step t.

Classiﬁcation-step. The classiﬁcation-step can be viewed
as training a classiﬁer to approximate the Wasserstein dis-
tance between S+ and St
− for t ≥ 1. Note that we also keep
pseudo-negatives from earlier stages – which are essentially
the mistakes of the earlier stages – to prevent the classiﬁer
forgetting what it has learned in previous stages. We use
CNNs parametrized by Wt as base classiﬁers. Let fWt(·)
denote the output of ﬁnal fully connected layer (without
passing through sigmoid nonlinearity) of the CNN. In the

previous introspective learning frameworks [29, 21], the
classiﬁer learning objective was to minimize the following
standard cross-entropy loss function on S+ ∪ St
−:

L(Wt)=−{E

x+∼p+ ln σ[+fWt (x+)]+E

ln σ[−fWt (x−)]}

x−∼p

−
Wt

where σ(·) denotes the sigmoid nonlinearity. Motivated by
Section 3.2, in WINN training we wish to minimize the fol-
lowing Wasserstein loss function by the stochastic gradient
descent algorithm via backpropagation:

L(Wt)=−(cid:2)E

x+∼p+ fWt (x+)−E

fWt (x−)(cid:3)

(5)

x−∼p

−
Wt

To enforce the function fWt to be 1-Lipschitz, we add

the following gradient penalty term [14] to L(Wt):

λEˆx∼pˆx [((cid:107)∇ˆxfWt (ˆx)(cid:107)2 − 1)2]

where ˆx = αx+ + (1 − α)x−, x+ ∼ p+, x− ∼ p−
Wt
α ∼ U [0, 1].

, and

Synthesis-step. Obtaining increasingly difﬁcult pseudo-
negative samples is an integral part of the introspective
learning framework, as it is crucial for tightening the de-
cision boundary. To this end, we develop an efﬁcient sam-
pling procedure under the Wasserstein formulation. After
the classiﬁcation-step, we obtain the following distribution
of pseudo-negatives:

p−
Wt

(x) =

exp{fWt(x)} · p−

0 (x), t = 1, . . . , T

(6)

1
Zt

where Zt = (cid:82) exp{fWt(x)} · p−
0 (x)dx; the initial distri-
bution p−
0 (x) is a Gaussian distribution G(x; 0, σ2) or the
distribution deﬁned in Appendix D. We ﬁnd that the distri-
bution of Appendix D encourages the diversity of sampled
images.
The following equivalence is shown in [29, 21]:

p(y = +1|x; Wt)
p(y = −1|x; Wt)

= exp{fWt(x)}.

(7)

The sampling strategy of [29, 21] was to carry out gradient
ascent on the term ln p(y=+1|x;Wt)
p(y=−1|x;Wt) . In Lemma 1 we chose
f (x) to be ln p+(x)
. Using Bayes’ rule, it is easy to see
p−
W (x)
p(y=−1|x;Wt) is loosely connected to ∇ ln p+(x)
that ∇ ln p(y=+1|x;Wt)
(x)
Also, [2, 14] argue that fWt(x) correlates with the quality
of the sample x. This motivates us to use the following sam-
pling strategy. After initializing x by drawing a fair sample
from p−
0 (x), we increase fWt(x) using gradient ascent on
the image x via backpropagation. Speciﬁcally, as shown in
[47], we can obtain fair samples from the distribution p−
Wt
using the following update rule:

p−
Wt

.

∆x =

∇fWt(x) + η

(cid:15)
2

where (cid:15) is a time-varying step size and η is a random vari-
able following the Gaussian distribution N (0, (cid:15)). Gaussian
noise term is added to make samples cover the full distribu-
tion. Inspired by [20], we found that injecting noise in the
image space could be substituted by applying Dropout to
the higher layers of CNN. In practice, we were able obtain
the samples of enough diversity without step size annealing
and noise injection.
As an early stopping criterion, we empirically ﬁnd that
the following is effective: (1) we measure the minimum
and maximum fWt(·) of positive examples; (2) we set the
early stopping threshold to a random number from the uni-
form distribution between these two numbers. Intuitively,
by matching the value of fWt (·) positives and pseudo-
negatives, we expect to obtain pseudo-negative samples that
match the quality of positive samples.

4.2. Expanding model capacity

In practice, we ﬁnd that the version with the single
classiﬁer – which we call WINN-single – is expressive
enough to capture the generative distribution under vari-
ety of applications. The introspective learning formulation
[44, 29, 21] allows us to model more complex distributions
by adding a sequence of cascaded classiﬁers parameterized
by (W1, . . . , WK). Then, we can model the distribution as:

1
Zt

p−
Wk (x) =

exp{fWk (x)} · p−

Wk−1(x), k = 2, . . . , K

(8)
In the next sections, we demonstrate the modeling capabil-
ity of WINN under cascaded classiﬁers, as well as its ag-
nosticy to the type of base classiﬁer.

4.3. GAN’s discriminator vs. WINN’s classiﬁer

(a) WGAN-GP discriminator

(b) WINN-single

Figure 3: Synthesized images from the discriminators of WGAN and
WINN trained on the CelebA dataset. Both use the same ResNet archi-
tecture for the discriminator as the one adopted in [14].

GAN uses the competing discriminator and the gener-
ator whereas WINN maintains a single model being both
generative and discriminative. Some general comparisons
between GAN and INN have been provided in [29, 21]. Be-
low we make a few additional observations that are worth
future exploration and discussions.

• First, the generator of GAN is a cost-effective option for image
patch synthesis, as it works in a feed-forward fashion. However
the generator of GAN is not meant to be trained as a classiﬁer
to perform the standard classiﬁcation task, while the generator

in the introspective framework is also a strong classiﬁer. Sec-
tion 5.6 shows WINN to have signiﬁcant robustness to external
adversarial examples.

• Second, the discriminator in GAN is meant to be a critic but
not a generator. To show whether or not the discriminator in
GAN can also be used as a generator, we train WGAN-GP [14]
on the CelebA face dataset. Using the same CNN architecture
(ResNet from [14]) that was used as GAN’s discriminator, we
also train a WINN-single model, making GAN’s discrimina-
tor and WINN-single to have the identical CNN architecture.
Applying the sampling strategy to WGAN-GP’s discriminator
allows us to synthesize image form WGAN-GP’s discriminator
as well and we show some samples in Figure 3 (a). These syn-
thesized images are not like faces, yet they have been classiﬁed
by the discriminator of WGAN-GP as “real” faces; this demon-
strates the separation between the generator and the discrimina-
tor in GAN. In contrast, images synthesized by WINN-single’s
CNN classiﬁer are faces like, as shown in Figure 3 (b).

• Third, the discriminator of GAN may not be used as a direct
discriminative classiﬁer for the standard supervised learning
task. As shown and discussed in ICN [21], the introspective
framework has the ability of classiﬁcation for discriminator.

5. Experiments

5.1. Implementation

Classiﬁcation-step. For training the discriminator network,
we use Adam [23] with a mini-batch size of 100. The learn-
ing rate was set to 0.0001. We set β1 = 0.0, and β2 = 0.9,
inspired by [14]. Each batch consists of 50 positive im-
ages sampled from the set of positives S+ and 50 pseudo-
negative images sampled from the set of pseudo-negatives
S−. In each iteration, we limit total number of training im-
ages to 10, 000.
Synthesis-step. For synthesizing pseudo-negative images
via back-propagation, we perform gradient ascent on the
image space. In the ﬁrst cascade, each image is initialized
with a noise sampled from the distribution described in Ap-
pendix D. In the later cascades, images are initialized with
the images sampled from the last cascade. We use Adam
with a mini-batch size of 100. The learning rate was set to
0.01. We set β1 = 0.9, and β2 = 0.99.
5.2. Texture modeling

We evaluate the texture modeling capability of WINN.
For a fair comparison, we use the same 7 texture images
presented in [9] where each texture image has a size of 256
× 256. We follow the training method of Section 5.1 ex-
cept that positive images are constructed by cropping 64
× 64 patches from the source texture image at random po-
sitions. We use network architecture of Appendix C. Af-
ter training is done on the 64×64 patch-based model, we
try to synthesize texture images of arbitrary size using the
anysize-image-generation method following [29]. During
the synthesis process, we keep a single working image of

previous generative modeling works since it contains large
pose variations and background clutters. The network archi-
tecture adopted here is described in Appendix C. In Figure
5, we show some synthesized face images using WINN-
single and WINN, as well as those by DCGAN [38], INNg-
single, and INNg [29]. WINN-single attains image quality
even higher than that of INNg (12 CNNs).
5.4. SVHN modeling

Real data

DCGAN

INNg-single

INNg

WINN-single (ours) WINN-4CNNs (ours)

Figure 6: Images generated by various models trained on SVHN. DCGAN
[38] result is from [29].

SVHN [34] consists of 32 × 32 images from Google
It contains 73, 257 training images, 26, 032
Street View.
test images, and 531, 131 extra images. We use only the
training images for the unsupervised SVHN modeling. We
use the ResNet architecture described in [14]. Generated
images by WINN-single and WINN (4 CNN classiﬁers) as
well as DCGAN and INN are shown in Figure 6. The im-
provement of WINN over INNg is evident.
5.5. CIFAR-10 modeling
Table 1: Inception score on CIFAR-10. “-L” in Improved GANs means
without labels.

Method
Real data
WGAN-GP [14]
WGAN [2]
DCGAN [38] (in [19])
ALI [7] (in [46])
Improved GANs (-L) [40]
INNg-single [29]
INNg [29]
WINN-single (ours)
WINN-5CNNs (ours)

Score
11.95 ± .20
7.86 ± .07
5.88 ± .07
6.16 ± .07
5.34 ± .05
4.36 ± .04
1.95 ± .01
3.04 ± .02
4.62 ± .05
5.58 ± .05

CIFAR-10 [25] consists of 50, 000 training images and
10, 000 test images of size 32 × 32 in 10 classes. We use
training images augmented by horizontal ﬂips [26] for un-
supervised CIFAR-10 modeling. We use the ResNet given
in [14]. Figure 7 shows generated images by various mod-
els.

To measure the semantic discriminability, we compute
the Inception scores [40] on 50, 000 generated images.

Figure 4: More texture synthesis results. Gatys et al. [9] and TextureNets
[45] results are from [45].

size 320×320. Note that we expand the image so that cen-
ter 256×256 pixels are covered with equal probability. In
each iteration, we sample 200 patches from the working im-
age, and perform gradient ascent on the chosen patches. For
the overlapping pixels between patches, we take the average
of the gradients assigned to such pixels. We show synthe-
sized texture images in Figure 2 and 4. WINN-single shows
a signiﬁcant improvement over INNg-single and compara-
ble results to INNg (using 20 CNNs).
It is worth noting
that [10, 45] leverage rich features of VGG-19 network pre-
trained on ImageNet. WINN and INNg instead train net-
works from scratch.
5.3. CelebA face modeling

Real data

DCGAN

INNg-single

INNg

WINN-single (ours) WINN-4CNNs (ours)
Figure 5: Images generated by various models trained on CelebA.

The CelebA dataset [33] consists of 202, 599 face im-
ages of celebrities. This dataset has been widely used in the

Real data

DCGAN

WINN-single (ours)

WINN-5CNNs (ours)

Figure 7: Images generated by models trained on CIFAR-10.

WINN shows its clear advantage over INN. WINN-5CNNs
produces a result close to WGAN but there is still a gap to
the state-of-the-art results by WGAN-GP.
5.6. Image classiﬁcation and adversarial examples

To demonstrate the robustness of WINN as a discrimi-
native classiﬁer, we present experiments on the supervised
classiﬁcation tasks.

Table 2: Test errors on MNIST and SVHN. When training on SVHN,
we only use training set. All results except WINN-single and baseline
ResNet-32 are from [21]. [21] adopted DCGAN [38] discriminator as their
CNN architecture. The advantage of WINN over a vanilla CNN is evident.
When applied to a stronger baseline such as ResNet-32, WINN is not los-
ing ResNet’s superior classiﬁcation capability in standard supervised clas-
siﬁcation, while attaining special generative capability and robustness to
adversarial attacks (see Table 3 and 4) that do not exist in ResNet. The
images below on the right are the generated samples by the corresponding
WINN (ResNet-32) classiﬁers reported in the table.

Method
MNIST
Baseline vanilla CNN (4 layers)
CNN + GDL [44]
CNN + DCGAN [38]
ICN [21]
WINN-single vanilla (ours)
Baseline ResNet-32
WINN-single ResNet-32 (ours)
SVHN
Baseline ResNet-32
WINN-single ResNet-32 (ours)

Error

0.89%
0.85%
0.84%
0.81%
0.67%
0.45%
0.48%

4.64%
4.50%

Training Methods. We add the Wasserstein loss term to
the ICN [21] loss function, obtaining the following:

L(Wt) =

− (cid:80)

xi∈S+

ln

(cid:80)K

(1)yi
·φ(xi;w(0)
exp{w
t
(1)k
k=1 exp{w
t
fWt(xi) − (cid:80)

)}
t
·φ(xi;w(0)

xi∈S+

t

)}

fWt(xi)

(cid:18)

(cid:80)

+α

xi∈St
−

+λEˆx∼pˆx[((cid:107)∇ˆxfWt(ˆx)(cid:107)2 − 1)2]

(cid:19)

t

, w(1)1
t

>, w(0)

, ..., w(1)K
t

where Wt =< w(0)
denotes the
t
internal parameters for the CNN, and w(1)k
denotes the top-
layer weights for the k-th class. In the experiments, we set
the weight of the WINN loss, α, to 0.01. We use the vanilla
network architecture resembling [21] as the baseline CNN,
which has less ﬁlters and parameters than the one in [21].
We also use a ResNet-32 architecture with Layer Normal-
ization [3] on MNIST and SVHN. In the classiﬁcation-step,

t

we use Adam with a ﬁxed learning rate of 0.001, β1 of 0.0.
In the synthesis-step, we use the Adam optimizer with a
learning rate of 0.02 and β1 of 0.9. Table 2 shows the errors
on MNIST and SVHN.

Table 3: Adversarial examples comparison between the baseline CNN
and WINN on MNIST. We ﬁrst generate N adversarial examples using
method A and count the number of adversarial examples misclassiﬁed by
A (= NA). Adversarial error of A is deﬁned as test error rate against
adversarial examples (= NA/N ). Then among A’s mistakes, we count
the number of adversarial examples misclassiﬁed by B (= NA∩B). Then
error correction rate by B is 1 − NA∩B/NA. ↑ means higher the better;
↓ means lower the better. The comparisons are made in two groups: the
ﬁrst group builds on top a vanilla 4 layer CNN and the second group adopts
ResNet-32. Note that the corresponding errors for these classiﬁers on the
standard MNIST supervised classiﬁcation task can be seen in Table 2.

Method
Baseline vanilla CNN
ICN [21]
WINN-single vanilla (ours)
Baseline ResNet-32
WINN-single ResNet-32 (ours)

Adversarial error Correction rate Correction rate
by Baseline ↓
by Method ↑
-
-
58.68%
52.58%
46.93%
90.00%
-
-
43.69%
89.68%

of Method ↓
32.41%
19.02%
7.99%
11.28%
2.05%

Robustness to adversarial examples. It is argued in [11]
that discriminative CNN’s vulnerability to adversarial ex-
amples primarily arises due to its linear nature. Since the
reclassiﬁcation-by-synthesis process helps tighten the de-
cision boundary (Figure 1), one might expect that CNNs
trained with the WINN algorithm are more robust to ad-
versarial examples. Note that unlike existing methods for
adversarial defenses [13, 28], our method does not train net-
works with speciﬁc types of adversarial examples. With test
images of MNIST and SVHN, we adopt “fast gradient sign
method” [11] ((cid:15) = 0.125 for MNIST and (cid:15) = 0.005 for
SVHN) to generate adversarial examples clipped to range
[−1, 1], which differs from [21]. We experiment with two
networks having the same architecture and only differing
in training method (the standard cross-entropy loss vs. the
WINN procedure). We call the former as the baseline CNN.
We summarize the results in Table 3. Compared to ICN
[21], WINN signiﬁcantly reduces the adversarial error to
7.99% and improves the correction rate to 90.00%. In ad-
dition, we have adopted the ResNet-32 architecture into
WINN. See Table 3 and 4. We still obtain the adversarial
error reduction and correction rate improvement on MNIST
and SVHN ((cid:15) = 0.005) with ResNet-32. Our observation is
that WINN is not necessarily improving over a strong base-
line for the supervised classiﬁcation task but its advantage
on adversarial attacks is evident.

Table 4: Adversarial examples comparison between the baseline ResNet-
32 [16] and WINN on SVHN. Note that the corresponding errors for these
classiﬁers on the standard supervised SVHN classiﬁcation task can be seen
in Table 2.

Method
Baseline ResNet-32
WINN-single ResNet-32 (ours)

Adversarial error Correction rate Correction rate
by Baseline ↓
by Method ↑
-
-
51.37%
74.39%

of Method
29.29%
19.53%

5.7. Agnostic to different architectures

In Figure 8, we demonstrate our algorithm being agnos-
tic to the type of classiﬁer, by varying network architectures
to ResNet [16] and DenseNet [18]. Little modiﬁcation was
required to adapt two architectures for WINN.

(1+

p

+
min
2

where JD(p+; p−
Jeffreys divergence.

)KL(p+||p−

W )≤JD(p+;p−
W) = KL(p+||p−

W )≤(1+ 2
W) + KL(p−

)KL(p+||p−
W||p+) is the

+
min

p

W ),

Proof. Based on the Pinsker’s inequality [37, 41], it is

observed that

WINN-single (ResNet-13)

WINN-single (DenseNet-20)

Figure 8: Synthesized CelebA images with varying network architectures.
We use a single CNN for each experiment.

6. Conclusion

In this paper, we have introduced Wasserstein introspec-
tive neural networks (WINN) that produce encouraging re-
sults as a generator and a discriminative classiﬁer at the
same time. WINN is able to achieve model size reduction
over the previous introspective neural networks (INN) by
a factor of 20. In most of the images shown in the paper,
we ﬁnd a single CNN classiﬁer in WINN being sufﬁcient to
produce visually appealing images as well as signiﬁcant er-
ror reduction against adversarial examples. WINN is agnos-
tic to the architecture design of CNN and we demonstrate
results on three networks including a vanilla CNN, ResNet
[16], and DenseNet [18] networks where popular CNN dis-
criminative classiﬁers are turned into generative models un-
der the WINN procedure. WINN can be adopted in a wide
range of applications in computer vision such as image clas-
siﬁcation, recognition, and generation.
Acknowledgements. This work is supported by NSF
IIS-1717431 and NSF IIS-1618477. The authors thank
Justin Lazarow, Long Jin, Hubert Le, Ying Nian Wu, Max
Welling, Richard Zemel, and Tong Zhang for valuable dis-
cussions.

7. Appendix

A. Proof of Lemma 1.

Proof. Plugging f (x) = ln p+(x)
p−
W (x)

into Eq. (3), we have

Ex∼p+ [f (x)]−E
= Ex∼p+ [ln p+ (x)

p

−
W

(x)

= (cid:82) p+(x) ln p+ (x)

p
= KL(p+||p−

−
W

(x)

[f (x)]

x∼p

−
W

]−E

x∼p

−
W
dx−(cid:82) p−

[ln p+ (x)

]

p

−
W

(x)

W (x) ln p+ (x)

p

−
W

(x)

dx

W )+KL(p−

W ||p+)

(cid:3)

B. Proof of Theorem 1.

Corollary 1 The Jeffreys divergence in Eq. (4) of lemma
1 is lower and upper bounded by KL(p+||p−
W) up to some
multiplicative constant

KL(p−

W||p+) ≥

|p−

W − p+|2 log e,

1
2

where |p−
we also have

W −p+| is total variation (TV) distance. From [41]

KL(p−

W||p+) ≤

|p−

W − p+|2,

log e
p+
min

where p+
to KL(p+||p−
|p−

W − p+| ≡ |p+ − p−
W|,

min = minx p+(x). Applying the above bounds
W) and using the symmetry of the TV distance

p+
min
2

KL(p+||p−

W) ≤ KL(p−

W||p+) ≤

KL(p+||p−

W).

2
p+
min

W) + KL(p−

W) + KL(p−
W).

Plugging the equation above into the Jeffreys divergence,
we observe that KL(p+||p−
W||p+) is upper and
(cid:3)
lower bounded by by KL(p+||p−
Now we can look at theorem 1. It was shown in [21]
that Eq. (1) reduces KL(p+||p−
W), which bounds the Jef-
freys divergence KL(p+||p−
W||p+) as shown in
corollary 1. Lemma 1 shows the connection between Jef-
freys divergence and the WGAN objective (Eq. (3)) when
f (x) = ln p+(x)
. We therefore can see that the formula-
p−
W (x)
tion of introspective neural networks (Eq. (1)) connects to
(cid:3)
a lower bound of the WGAN [2] objective (Eq. (3)).
C. Texture and CelebA Modeling Architecture. Inspired
by [22], we design a CNN architecture for 64 × 64 image
as in Table 5. We use Swish [39] non-linearity after each
convolutional layer. We add Layer Normalization [3] after
each convolution except the ﬁrst layer, following [14].
Table 5: Network architecture for the texture and face image modeling.

Textures and CelebA
Filter size/stride

Alternative Initialization

Filter size/stride

Layer
Input
Conv3-32
Conv3-64
Avg pool
Conv3-64
Conv3-128
Avg pool
Conv3-128
Conv3-256
Avg pool
Conv3-256
Conv3-512
Avg pool
FC-1

3×3/1
3×3/1
2×2/2
3×3/1
3×3/1
2×2/2
3×3/1
3×3/1
2×2/2
3×3/1
3×3/1
2×2/2

Output size
64×64×3
64×64×32
64×64×64
32×32×64
32×32×64
32×32×128
16×16×128
16×16×128
16×16×256
8×8×256
8×8×256
8×8×256
4×4×512
1×1×1

Layer
Input
Conv5-256
Upsample
Conv5-128
Upsample
Conv5-64
Upsample
Conv5-64
Upsample

5×5/1

5×5/1

5×5/1

5×5/1

Output size
4×4×512
4×4×256
8×8×256
8×8×128
16×16×128
16×16×64
32×32×64
32×32×3
64×64×3

D. Alternative Initializations. We sample an initial
pseudo-negative image by applying an operation deﬁned by
the network above to a tensor of size 4 × 4 × 512 sampled
from U [−1, 1]. The weights of the network are sampled
from G(0, 0.12). We do not apply any nonlinearities in the
network. We add Layer Normalization [3] after each con-
volution except the last layer.

References

[1] D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. A learn-
ing algorithm for boltzmann machines. Cognitive science,
9(1):147–169, 1985. 1

[2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gen-
erative adversarial networks. In ICML, 2017. 1, 2, 3, 4, 6,
8

[3] L. J. Ba, R. Kiros, and G. E. Hinton. Layer normalization.

CoRR, abs/1607.06450, 2016. 7, 8

[4] P. Baldi. Autoencoders, unsupervised learning, and deep ar-
chitectures. In ICML Workshop on Unsupervised and Trans-
fer Learning, pages 37–49, 2012. 1

[5] S. Della Pietra, V. Della Pietra, and J. Lafferty. Inducing fea-
tures of random ﬁelds. IEEE transactions on pattern analysis
and machine intelligence, 19(4):380–393, 1997. 1

[6] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning
to generate chairs with convolutional neural networks.
In
CVPR, 2015. 1
[7] V. Dumoulin,

I. Belghazi, B. Poole, O. Mastropietro,
A. Lamb, M. Arjovsky, and A. Courville. Adversarially
learned inference. In ICLR, 2017. 6

[8] Y. Freund and R. E. Schapire. A decision-theoretic general-
ization of on-line learning and an application to boosting. J.
of Comp. and Sys. Sci., 55(1), 1997. 2

[9] L. Gatys, A. S. Ecker, and M. Bethge. Texture synthesis
using convolutional neural networks. In NIPS, 2015. 3, 5, 6
[10] L. A. Gatys, A. S. Ecker, and M. Bethge. A neural algorithm

of artistic style. arXiv preprint arXiv:1508.06576, 2015. 6

[11] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In NIPS, 2014. 1, 3, 7

[12] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets.
In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger, edi-
tors, Advances in Neural Information Processing Systems 27,
pages 2672–2680. Curran Associates, Inc., 2014. 2

[13] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and

harnessing adversarial examples. In ICLR, 2015. 7

[14] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and
In

Improved training of wasserstein gans.

A. Courville.
NIPS, 2017. 1, 2, 4, 5, 6, 8

[15] T. Han, Y. Lu, S.-C. Zhu, and Y. N. Wu. Alternating back-
propagation for generator network. In AAAI, 2017. 2
[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 7, 8

[17] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learn-
ing algorithm for deep belief nets. Neural computation,
18(7):1527–1554, 2006. 1

[18] G. Huang*, Z. Liu*, L. van der Maaten, and K. Q. Wein-
berger. Densely connected convolutional networks.
In
CVPR, 2017. 1, 8

[19] X. Huang, Y. Li, O. Poursaeed, J. Hopcroft, and S. Belongie.
Stacked generative adversarial networks. In CVPR, 2017. 6
[20] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks. In CVPR,
2017. 5

[21] L. Jin, J. Lazarow, and Z. Tu.

Introspective classiﬁcation

with convolutional nets. In NIPS, 2017. 2, 3, 4, 5, 7, 8
[22] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive
growing of gans for improved quality, stability, and variation.
In ICLR, 2018. 8

[23] D. Kingma and J. Ba. Adam: A method for stochastic opti-

[24] D. P. Kingma and M. Welling. Auto-encoding variational

mization. In ICLR, 2015. 5

bayes. In ICLR, 2014. 1

[25] A. Krizhevsky, V. Nair, and G. Hinton. Cifar-10 (canadian

institute for advanced research). 6

[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

ImageNet
Classiﬁcation with Deep Convolutional Neural Networks. In
NIPS, 2012. 1, 6

[27] T. D. Kulkarni, W. F. Whitney, P. Kohli, and J. Tenenbaum.
Deep convolutional inverse graphics network. In NIPS, 2015.
1

[28] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial ma-

chine learning at scale. In ICLR, 2017. 7

[29] J. Lazarow*, L. Jin*, and Z. Tu.

Introspective neural net-
works for generative modeling. In ICCV, 2017. 2, 3, 4, 5,
6

[30] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. Howard,
W. Hubbard, and L. Jackel. Backpropagation applied to
handwritten zip code recognition. In Neural Computation,
1989. 1

[31] C.-Y. Lee*, S. Xie*, P. Gallagher, Z. Zhang, and Z. Tu.

Deeply-supervised nets. In AISTATS, 2015. 1

[32] S. Liu, O. Bousquet, and K. Chaudhuri. Approximation and
convergence properties of generative adversarial learning. In
NIPS, 2017. 3

[33] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face

attributes in the wild. In ICCV, 2015. 6

[34] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In NIPS workshop on deep learning and unsu-
pervised feature learning, volume 2011, page 5, 2011. 6
[35] J. Ngiam, Z. Chen, P. W. Koh, and A. Y. Ng. Learning deep

energy models. In ICML, 2011. 2

[36] B. A. Olshausen and D. J. Field. Sparse coding with an over-
complete basis set: A strategy employed by v1? Vision re-
search, 37(23):3311–3325, 1997. 1

[37] M. S. Pinsker. Information and information stability of ran-

dom variables and processes. 1960. 8

[38] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. In ICLR, 2016. 1, 6, 7

[39] P. Ramachandran, B. Zoph, and Q. V. Le. Searching for ac-

tivation functions. CoRR, abs/1710.05941, 2017. 8

[40] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Rad-
ford, and X. Chen. Improved techniques for training gans. In
NIPS, 2016. 6

[41] I. Sason and S. Verd´u. f -divergence inequalities.

IEEE
Transactions on Information Theory, 62(11):5973–6006,
2016. 8

[42] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
1

[43] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 1

[44] Z. Tu. Learning generative models via discriminative ap-

proaches. In CVPR, 2007. 1, 2, 3, 5, 7

[45] D. Ulyanov, V. Lebedev, A. Vedaldi, and V. Lempitsky. Tex-
ture networks: Feed-forward synthesis of textures and styl-
ized images. In ICML, 2016. 3, 6

[46] D. Warde-Farley and Y. Bengio. Improving generative adver-
sarial networks with denoising feature matching. In ICLR,
2017. 6

[47] M. Welling and Y. W. Teh. Bayesian learning via stochastic

gradient langevin dynamics. In ICML, 2011. 3, 4

[48] M. Welling, R. S. Zemel, and G. E. Hinton. Self supervised

boosting. In NIPS, 2002. 1

[49] Y. N. Wu, Z. Si, H. Gong, and S.-C. Zhu. Learning active ba-
sis model for object detection and recognition. International
journal of computer vision, 90(2):198–235, 2010. 1

[50] J. Xie, Y. Lu, R. Gao, S.-C. Zhu, and Y. N. Wu. Cooperative
learning of energy-based model and latent variable model via
mcmc teaching. In AAAI, 2018. 2

[51] J. Xie, Y. Lu, S.-C. Zhu, and Y. N. Wu. A theory of genera-

tive convnet. In ICML, 2016. 2

[52] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated
residual transformations for deep neural networks. In CVPR,
2017. 1

[53] S.-C. Zhu and D. Mumford. A stochastic grammar of im-
ages. Foundations and Trends R(cid:13) in Computer Graphics and
Vision, 2(4):259–362, 2007. 1

[54] S. C. Zhu, Y. N. Wu, and D. Mumford. Minimax entropy
principle and its application to texture modeling. Neural
Computation, 9(8):1627–1660, 1997. 1

Wasserstein Introspective Neural Networks

Kwonjoon Lee

Weijian Xu

Fan Fan

Zhuowen Tu

University of California San Diego
{kwl042, wex041, f1fan, ztu}@ucsd.edu

8
1
0
2
 
r
p
A
 
7
 
 
]

V
C
.
s
c
[
 
 
5
v
5
7
8
8
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

We present Wasserstein introspective neural networks
(WINN) that are both a generator and a discriminator
within a single model. WINN provides a signiﬁcant im-
provement over the recent introspective neural networks
(INN) method by enhancing INN’s generative modeling ca-
pability. WINN has three interesting properties: (1) A math-
ematical connection between the formulation of the INN al-
gorithm and that of Wasserstein generative adversarial net-
works (WGAN) is made. (2) The explicit adoption of the
Wasserstein distance into INN results in a large enhance-
ment to INN, achieving compelling results even with a single
classiﬁer — e.g., providing nearly a 20 times reduction in
model size over INN for unsupervised generative modeling.
(3) When applied to supervised classiﬁcation, WINN also
gives rise to improved robustness against adversarial exam-
ples in terms of the error reduction. In the experiments, we
report encouraging results on unsupervised learning prob-
lems including texture, face, and object modeling, as well as
a supervised classiﬁcation task against adversarial attacks.
Our code is available online1.

1. Introduction

Performance within the task of supervised image classi-
ﬁcation has been vastly improved in the era of deep learn-
ing using modern convolutional neural network (CNN) [30]
based discriminative classiﬁers [26, 43, 31, 42, 16, 52, 18].
On the other hand, unsupervised generative models in deep
learning were previously attained using methods under the
umbrella of graphical models — e.g., the Boltzmann ma-
chine [17] or autoencoder [4, 24] architectures. However,
the rich representational power seen within convolution-
based (discriminative) models is not being directly enjoyed
in these generative models. Later, inverting convolutional
neural networks in order to convert internal representations
into a real image was investigated in [6, 27]. Recently,
generative adversarial networks (GAN) [11] and followup

1https://github.com/kjunelee/WINN

Figure 1: Schematic illustration of Wasserstein introspective neural net-
works for unsupervised learning. The left ﬁgure shows the input examples;
the bottom ﬁgures show the pseudo-negatives (purple crosses) being pro-
gressively synthesized; the top ﬁgures show the classiﬁcation between the
given examples (positives) and synthesized pseudo-negatives (negatives).
The right ﬁgure shows the model learned to approach the target distribution
based on the given data.

works [38, 2, 14] have attracted a tremendous amount of
attention in machine learning and computer vision by pro-
ducing high quality synthesized images by training a pair
of competing models against one another in an adversar-
ial manner. While a generator tries to create “fake” images
to fool the discriminator, the discriminator attempts to dis-
cern between these “real” (given training) and “fake” im-
ages. After convergence, the generator is able to produce
images faithful to the underlying data distribution.

Before the deep learning era [17], generative model-
ing had been an area with a steady pace of development
[1, 5, 54, 36, 49, 53]. These models were guided by rigor-
ous statistical theories which, although nice in theory, did
not succeed in producing synthesized images with practical
quality.

In terms of building generative models from discrimina-
tive classiﬁers, there have been early attempts in [48, 44].

In [48], a generative model was obtained from a repeat-
edly trained boosting algorithm [8] using a weak classi-
ﬁer whereas [44] used a strong classiﬁer in order to self-
generate negative examples or “pseudo-negatives”.

To address the lack of richness in representation and ef-
ﬁciency in synthesis, convolutional neural networks were
adopted in introspective neural networks (INN) [29, 21] to
build a single model that is simultaneously generative and
discriminative. The generative modeling aspect was studied
in [29] where a sequence of CNN classiﬁers (10 − 60) were
trained, while the power within the classiﬁcation setting was
revealed in [21] in the form of introspective convolutional
networks (ICN) that used only a single CNN classiﬁer. Al-
though INN models [29, 21] point to a promising direction
to obtain a single model being both a good generator and a
strong discriminative classiﬁer, a sequence of CNNs were
needed to generate realistic synthesis. As a result, this re-
quirement may serve as a possible bottleneck with respect
to training complexity and model size.

Recently, a generic formulation [2] was developed
within the GAN model family to incorporate a Wasserstein
objective to alleviate the well-known difﬁculty in GAN
training. Motivated by introspective neural networks (INN)
[29] and this Wasserstein objective [2], we propose to adopt
the Wasserstein term into the INN formulation to enhance
the modeling capability. The resulting model, Wasserstein
introspective neural networks (WINN) shows greatly en-
hanced modeling capability over INN by having 20× re-
duction in the number of CNN classiﬁers.

2. Signiﬁcance and Related Work

We make several interesting observations for WINN:

• A mathematical connection between the WGAN formulation
[2] and the INN algorithm [29] is made to better understand the
overall objective function within INN.

• By adopting the Wasserstein distance into INN, we are able to
generate images using a single CNN in WINN with even higher
quality than those by INN that uses 20 CNNs (as seen in Figure
2, 4, 5, 6, and 7; the similar underlying CNN architectures are
used in WINN and INN). WINN achieves a signiﬁcant reduc-
tion in model complexity over INN, making the generator more
practical.

• Within texture modeling, INN and WINN are able to inherently
model the input image space, making the synthesis of large tex-
ture images realistic, whereas GAN projects a noise vector onto
the image space making the image patch stitching more difﬁ-
cult (although extensions exist), as demonstrated in Figure 2.
• To compare with the family of GAN models, we compute In-
ception scores using the standard procedure on the CIFAR-10
datasets and observed modest results. Here, we typically train
4-5 cascades to boost the numbers but WINN with one CNN is
already promising. Overall, modern GAN variants (e.g., [14])
still outperform our WINN with better quality images. Some
results are shown in Figure 7.

• To test the robustness of the discriminative abilities of WINN,
we directly make WINN into a discriminative classiﬁer by
training it on the standard MNIST and SVHN datasets. Not
only are we able to improve over the previous ICN [21] clas-
siﬁer for supervised classiﬁcation, we also observe a large
improvement in robustness against adversarial examples com-
pared with the baseline CNN, ResNet, and the competing ICN.
In terms of other related work, we brieﬂy discuss some

existing methods below.
Wasserstein GAN. A closely related work to our WINN al-
gorithm is the Wasserstein generative adversarial networks
(WGAN) method [2, 14]. While WINN adopts the Wasser-
stein distance as motivated by WGAN, our overall algo-
rithm is still within the family of introspective neural net-
works (INN) [29, 21]. WGAN on the other hand is a vari-
ant of GAN with an improvement over GAN by having an
objective that is easier to train. The level of difference be-
tween WINN and WGAN is similar to that between INN
[29, 21] and GAN [12]. The overall comparisons between
INN and GAN have been described in [29, 21].
Generative ConvNets. Recently, there has also been a clus-
ter of algorithms developed in [50, 51, 15] where Langevin
dynamics are adopted in generator CNNs. However, the
models proposed in [50, 51, 15] do not perform introspec-
tion (Figure 1) and their generator and discriminator com-
ponents are still somewhat separated; thus, their generators
are not used as effective discriminative classiﬁers to per-
form state-of-the-art classiﬁcation on standard supervised
machine learning tasks. Their training processes are also
more complex than those of INN and WINN.
Deep energy models (DEMs) [35]. DEM [35] extends the
standard density estimation by using multi-layer neural net-
works (MLNN) with a rather complex training procedure.
The probability model in DEM includes both the raw input
and the features computed by MLNN. WINN instead takes
a more general and simplistic form and is easier to train (see
Eq. (1)). In general, DEM belongs to the minimum descrip-
tion length (MDL) family models in which the maximum
likelihood is achieved. WINN, instead, has a formulation
being simultaneously discriminative and generative.

3. Introspective Neural Networks

3.1. Brief introduction of INN

We ﬁrst brieﬂy introduce the introspective neural net-
work method (INNg) [29] for generative modeling and its
companion model [21] which focuses on the classiﬁcation
aspect. The main motivation behind the INN work [29, 21]
is to make a convolutional neural network classiﬁer simulta-
neously discriminative and generative. A single CNN clas-
siﬁer is trained in an introspective manner to improve the
standard supervised classiﬁcation result [21], however, a se-
quence of CNNs (typically 10 − 60) is needed to be able to
synthesize images of good quality [29].

Figure 2: Comparison of texture synthesis algorithms. Gatys et al. [9], and TextureNet [45] results are from [45].

Figure 1 shows a brief illustration of a single introspec-
tive CNN classiﬁer [21]. We discuss our basic unsuper-
vised formulation next. Suppose we are given a set of train-
ing examples: S = {xi
| i = 1, . . . , n} where we as-
sume each xi ∈ Rm — e.g., m = 4096 for images of
size 64 × 64. These will constitute positive examples of
the patterns/targets we wish to model. The main idea of
INN is to deﬁne pseudo-negative examples that are to be
self-generated by the discriminative classiﬁer itself. We
therefore deﬁne label y for each example x, y = +1 if
x is from the given training set and y = −1 if x is self-
generated. Motivated by the generative via discriminative
learning (GDL) framework [44], one could try to learn the
generative model for the given examples, p(x|y = +1), by
a sequentially learned distribution of the pseudo-negative
samples, pt(x|y = −1; Wt) which is abbreviated as p−
(x)
Wt
where Wt includes all the model parameters learned at step
t.

1
Zt

p−
Wt

)}·p−

exp{w(1)

·φ(x; w(0)

t

t

t

t

(x) =

·φ(x; w(0)

0 (x), t = 1, . . . , T
(1)
where Zt = (cid:82) exp{w(1)
)}·p−
0 (x)dx and the ini-
tial distribution p−
0 (x) such as a Gaussian distribution over
the entire space of x ∈ Rm. The discriminative classiﬁer
is a convolutional neural network (CNN) parameterized by
Wt = (w(0)
) where w(1)
denotes the weights of the
top layer combining the features through φ(x; w(0)
) (e.g.,
softmax layer) and w(0)
parameterizing the internal repre-
sentations. The synthesis process through which pseudo-
negative samples are generated is carried out by stochastic
gradient Langevin dynamics [47] as

, w(1)
t

t

t

t

t

∆x =

∇(w(1)

· φ(x; w(0)

)) + η

t

t

(cid:15)
2

where η ∼ N (0, (cid:15)) is a Gaussian distribution and (cid:15) is the
step size that is annealed in the sampling process. Overall,
we desire

p−
Wt

(x) t=∞→ p(x|y = +1),

(2)

using the iterative reclassiﬁcation-by-synthesis process [21,
29] guided by Eq. (1).

3.2. Connection to the Wasserstein distance

overall

is carried out

training process,

reclassiﬁcation-by-
The
synthesis,
iteratively without an explicit
objective function. The generative adversarial network
(GAN) model [11] instead has an objective function
formulated in a minimax fashion with the generator and
discriminator competing against each other. The Wasser-
stein generative adversarial network (WGAN) work [2]
improves GAN [11] by replacing the Jensen-Shannon dis-
tance with an efﬁcient approximation of the Earth-Mover
distance [2]. Also, there has been further generalization of
the GAN family models in [32].

Let p+(x) ≡ p(x|y = +1) be the target distribution and
p−
W(x) ≡ p(x|y = −1; W) be the pseudo-negative distri-
bution parameterized by W. Next, we show a connection
between the INN framework and the WGAN formulation
[2], whose objective (rewritten with our notations) can be
deﬁned as

min
W

max
||f ||L≤1

Ex∼p+[f (x)] − Ex∼p−

[f (x)],

W

(3)

where ||f ||L ≤ 1 denotes the space of 1-Lipschitz func-
tions. To build the connection between Eq. (3) of WGAN
and Eq. (1) of INN, we ﬁrst present the following lemma.

Lemma 1 Considering f (x) = ln p+(x)
and assuming its
p−
W (x)
1-Lipschitz property, we have a lower bound on the Wasser-
stein distance by

max
||f ||L≤1
≥ KL(p+||p−

Ex∼p+[f (x)] − Ex∼p−

[f (x)]

W

W) + KL(p−

W||p+)

(4)

where KL(p||q) denotes the Kullback-Leibler divergence
between the two distributions p and q, and KL(p+||p−
W) +
KL(p−

W||p+) is the Jeffreys divergence.

Proof: see Appendix A.
Note that using the Bayes’ rule, the ratio of the generative
probabilities p(x|y=+1)
p(x|y=−1) in Lemma 1 can be turned into the
ratio of the discriminative probabilities p(y=+1|x)
p(y=−1|x) assuming
equal priors p(y = +1) = p(y = −1).

Theorem 1 The introspective neural network formulation
(Eq. (1)) implicitly minimizes a lower bound of the WGAN
objective [2] (Eq. (3)).
Proof: see Appendix B.

4. Wasserstein Introspective Networks

Algorithm 1 Outline of WINN-single training algorithm. We
use k = 3 and λ = 10.

while Wt has not converged do

// Classiﬁcation-step:
for k steps do

(cid:80)m

i , i = 1, . . . , m.

m} from S+.
1 ,··· ,x−

Sample m positive samples {x+
1 ,··· ,x+
m} from St
Sample m pseudo-negative samples {x−
−.
Sample m random numbers {α1,··· ,αm} from U [0, 1].
i + (1 − αi)x−
ˆxi ← αix+
Perform stochastic gradient descent:
(x−
∇Wt
end for
// Synthesis-step:
Sample r noise samples {x1, · · · , xr} from p−
Perform stochastic gradient ascent with early-stopping.
St+1
− ← St
t ← t + 1.

− ∪ {x1, · · · , xr}.

i )]+λ((cid:107)∇ˆxi

i=1{[fWt

(ˆxi)(cid:107)2−1)2}.

0 (x).

i )−fWt

(x+

fWt

1
m

end while

Here we present the formulation for WINN building
upon the formulation of the prior introspective learning
works presented in Section 3.

4.1. WINN algorithm

We denote our unlabeled input training data as S+ =
{xi|yi = +1, i = 1, . . . , n}. Also, we denote the set of
all the self-generated pseudo-negative samples up to step
t as St
− = {xi|yi = −1, i = 1, . . . , l}. In other words,
St
− consists of pseudo-negatives x sampled from our model
p−
(x) for t ≥ 1 where Wt is the model parameter vector
Wt
at step t.

Classiﬁcation-step. The classiﬁcation-step can be viewed
as training a classiﬁer to approximate the Wasserstein dis-
tance between S+ and St
− for t ≥ 1. Note that we also keep
pseudo-negatives from earlier stages – which are essentially
the mistakes of the earlier stages – to prevent the classiﬁer
forgetting what it has learned in previous stages. We use
CNNs parametrized by Wt as base classiﬁers. Let fWt(·)
denote the output of ﬁnal fully connected layer (without
passing through sigmoid nonlinearity) of the CNN. In the

previous introspective learning frameworks [29, 21], the
classiﬁer learning objective was to minimize the following
standard cross-entropy loss function on S+ ∪ St
−:

L(Wt)=−{E

x+∼p+ ln σ[+fWt (x+)]+E

ln σ[−fWt (x−)]}

x−∼p

−
Wt

where σ(·) denotes the sigmoid nonlinearity. Motivated by
Section 3.2, in WINN training we wish to minimize the fol-
lowing Wasserstein loss function by the stochastic gradient
descent algorithm via backpropagation:

L(Wt)=−(cid:2)E

x+∼p+ fWt (x+)−E

fWt (x−)(cid:3)

(5)

x−∼p

−
Wt

To enforce the function fWt to be 1-Lipschitz, we add

the following gradient penalty term [14] to L(Wt):

λEˆx∼pˆx [((cid:107)∇ˆxfWt (ˆx)(cid:107)2 − 1)2]

where ˆx = αx+ + (1 − α)x−, x+ ∼ p+, x− ∼ p−
Wt
α ∼ U [0, 1].

, and

Synthesis-step. Obtaining increasingly difﬁcult pseudo-
negative samples is an integral part of the introspective
learning framework, as it is crucial for tightening the de-
cision boundary. To this end, we develop an efﬁcient sam-
pling procedure under the Wasserstein formulation. After
the classiﬁcation-step, we obtain the following distribution
of pseudo-negatives:

p−
Wt

(x) =

exp{fWt(x)} · p−

0 (x), t = 1, . . . , T

(6)

1
Zt

where Zt = (cid:82) exp{fWt(x)} · p−
0 (x)dx; the initial distri-
bution p−
0 (x) is a Gaussian distribution G(x; 0, σ2) or the
distribution deﬁned in Appendix D. We ﬁnd that the distri-
bution of Appendix D encourages the diversity of sampled
images.
The following equivalence is shown in [29, 21]:

p(y = +1|x; Wt)
p(y = −1|x; Wt)

= exp{fWt(x)}.

(7)

The sampling strategy of [29, 21] was to carry out gradient
ascent on the term ln p(y=+1|x;Wt)
p(y=−1|x;Wt) . In Lemma 1 we chose
f (x) to be ln p+(x)
. Using Bayes’ rule, it is easy to see
p−
W (x)
p(y=−1|x;Wt) is loosely connected to ∇ ln p+(x)
that ∇ ln p(y=+1|x;Wt)
(x)
Also, [2, 14] argue that fWt(x) correlates with the quality
of the sample x. This motivates us to use the following sam-
pling strategy. After initializing x by drawing a fair sample
from p−
0 (x), we increase fWt(x) using gradient ascent on
the image x via backpropagation. Speciﬁcally, as shown in
[47], we can obtain fair samples from the distribution p−
Wt
using the following update rule:

p−
Wt

.

∆x =

∇fWt(x) + η

(cid:15)
2

where (cid:15) is a time-varying step size and η is a random vari-
able following the Gaussian distribution N (0, (cid:15)). Gaussian
noise term is added to make samples cover the full distribu-
tion. Inspired by [20], we found that injecting noise in the
image space could be substituted by applying Dropout to
the higher layers of CNN. In practice, we were able obtain
the samples of enough diversity without step size annealing
and noise injection.
As an early stopping criterion, we empirically ﬁnd that
the following is effective: (1) we measure the minimum
and maximum fWt(·) of positive examples; (2) we set the
early stopping threshold to a random number from the uni-
form distribution between these two numbers. Intuitively,
by matching the value of fWt (·) positives and pseudo-
negatives, we expect to obtain pseudo-negative samples that
match the quality of positive samples.

4.2. Expanding model capacity

In practice, we ﬁnd that the version with the single
classiﬁer – which we call WINN-single – is expressive
enough to capture the generative distribution under vari-
ety of applications. The introspective learning formulation
[44, 29, 21] allows us to model more complex distributions
by adding a sequence of cascaded classiﬁers parameterized
by (W1, . . . , WK). Then, we can model the distribution as:

1
Zt

p−
Wk (x) =

exp{fWk (x)} · p−

Wk−1(x), k = 2, . . . , K

(8)
In the next sections, we demonstrate the modeling capabil-
ity of WINN under cascaded classiﬁers, as well as its ag-
nosticy to the type of base classiﬁer.

4.3. GAN’s discriminator vs. WINN’s classiﬁer

(a) WGAN-GP discriminator

(b) WINN-single

Figure 3: Synthesized images from the discriminators of WGAN and
WINN trained on the CelebA dataset. Both use the same ResNet archi-
tecture for the discriminator as the one adopted in [14].

GAN uses the competing discriminator and the gener-
ator whereas WINN maintains a single model being both
generative and discriminative. Some general comparisons
between GAN and INN have been provided in [29, 21]. Be-
low we make a few additional observations that are worth
future exploration and discussions.

• First, the generator of GAN is a cost-effective option for image
patch synthesis, as it works in a feed-forward fashion. However
the generator of GAN is not meant to be trained as a classiﬁer
to perform the standard classiﬁcation task, while the generator

in the introspective framework is also a strong classiﬁer. Sec-
tion 5.6 shows WINN to have signiﬁcant robustness to external
adversarial examples.

• Second, the discriminator in GAN is meant to be a critic but
not a generator. To show whether or not the discriminator in
GAN can also be used as a generator, we train WGAN-GP [14]
on the CelebA face dataset. Using the same CNN architecture
(ResNet from [14]) that was used as GAN’s discriminator, we
also train a WINN-single model, making GAN’s discrimina-
tor and WINN-single to have the identical CNN architecture.
Applying the sampling strategy to WGAN-GP’s discriminator
allows us to synthesize image form WGAN-GP’s discriminator
as well and we show some samples in Figure 3 (a). These syn-
thesized images are not like faces, yet they have been classiﬁed
by the discriminator of WGAN-GP as “real” faces; this demon-
strates the separation between the generator and the discrimina-
tor in GAN. In contrast, images synthesized by WINN-single’s
CNN classiﬁer are faces like, as shown in Figure 3 (b).

• Third, the discriminator of GAN may not be used as a direct
discriminative classiﬁer for the standard supervised learning
task. As shown and discussed in ICN [21], the introspective
framework has the ability of classiﬁcation for discriminator.

5. Experiments

5.1. Implementation

Classiﬁcation-step. For training the discriminator network,
we use Adam [23] with a mini-batch size of 100. The learn-
ing rate was set to 0.0001. We set β1 = 0.0, and β2 = 0.9,
inspired by [14]. Each batch consists of 50 positive im-
ages sampled from the set of positives S+ and 50 pseudo-
negative images sampled from the set of pseudo-negatives
S−. In each iteration, we limit total number of training im-
ages to 10, 000.
Synthesis-step. For synthesizing pseudo-negative images
via back-propagation, we perform gradient ascent on the
image space. In the ﬁrst cascade, each image is initialized
with a noise sampled from the distribution described in Ap-
pendix D. In the later cascades, images are initialized with
the images sampled from the last cascade. We use Adam
with a mini-batch size of 100. The learning rate was set to
0.01. We set β1 = 0.9, and β2 = 0.99.
5.2. Texture modeling

We evaluate the texture modeling capability of WINN.
For a fair comparison, we use the same 7 texture images
presented in [9] where each texture image has a size of 256
× 256. We follow the training method of Section 5.1 ex-
cept that positive images are constructed by cropping 64
× 64 patches from the source texture image at random po-
sitions. We use network architecture of Appendix C. Af-
ter training is done on the 64×64 patch-based model, we
try to synthesize texture images of arbitrary size using the
anysize-image-generation method following [29]. During
the synthesis process, we keep a single working image of

previous generative modeling works since it contains large
pose variations and background clutters. The network archi-
tecture adopted here is described in Appendix C. In Figure
5, we show some synthesized face images using WINN-
single and WINN, as well as those by DCGAN [38], INNg-
single, and INNg [29]. WINN-single attains image quality
even higher than that of INNg (12 CNNs).
5.4. SVHN modeling

Real data

DCGAN

INNg-single

INNg

WINN-single (ours) WINN-4CNNs (ours)

Figure 6: Images generated by various models trained on SVHN. DCGAN
[38] result is from [29].

SVHN [34] consists of 32 × 32 images from Google
It contains 73, 257 training images, 26, 032
Street View.
test images, and 531, 131 extra images. We use only the
training images for the unsupervised SVHN modeling. We
use the ResNet architecture described in [14]. Generated
images by WINN-single and WINN (4 CNN classiﬁers) as
well as DCGAN and INN are shown in Figure 6. The im-
provement of WINN over INNg is evident.
5.5. CIFAR-10 modeling
Table 1: Inception score on CIFAR-10. “-L” in Improved GANs means
without labels.

Method
Real data
WGAN-GP [14]
WGAN [2]
DCGAN [38] (in [19])
ALI [7] (in [46])
Improved GANs (-L) [40]
INNg-single [29]
INNg [29]
WINN-single (ours)
WINN-5CNNs (ours)

Score
11.95 ± .20
7.86 ± .07
5.88 ± .07
6.16 ± .07
5.34 ± .05
4.36 ± .04
1.95 ± .01
3.04 ± .02
4.62 ± .05
5.58 ± .05

CIFAR-10 [25] consists of 50, 000 training images and
10, 000 test images of size 32 × 32 in 10 classes. We use
training images augmented by horizontal ﬂips [26] for un-
supervised CIFAR-10 modeling. We use the ResNet given
in [14]. Figure 7 shows generated images by various mod-
els.

To measure the semantic discriminability, we compute
the Inception scores [40] on 50, 000 generated images.

Figure 4: More texture synthesis results. Gatys et al. [9] and TextureNets
[45] results are from [45].

size 320×320. Note that we expand the image so that cen-
ter 256×256 pixels are covered with equal probability. In
each iteration, we sample 200 patches from the working im-
age, and perform gradient ascent on the chosen patches. For
the overlapping pixels between patches, we take the average
of the gradients assigned to such pixels. We show synthe-
sized texture images in Figure 2 and 4. WINN-single shows
a signiﬁcant improvement over INNg-single and compara-
ble results to INNg (using 20 CNNs).
It is worth noting
that [10, 45] leverage rich features of VGG-19 network pre-
trained on ImageNet. WINN and INNg instead train net-
works from scratch.
5.3. CelebA face modeling

Real data

DCGAN

INNg-single

INNg

WINN-single (ours) WINN-4CNNs (ours)
Figure 5: Images generated by various models trained on CelebA.

The CelebA dataset [33] consists of 202, 599 face im-
ages of celebrities. This dataset has been widely used in the

Real data

DCGAN

WINN-single (ours)

WINN-5CNNs (ours)

Figure 7: Images generated by models trained on CIFAR-10.

WINN shows its clear advantage over INN. WINN-5CNNs
produces a result close to WGAN but there is still a gap to
the state-of-the-art results by WGAN-GP.
5.6. Image classiﬁcation and adversarial examples

To demonstrate the robustness of WINN as a discrimi-
native classiﬁer, we present experiments on the supervised
classiﬁcation tasks.

Table 2: Test errors on MNIST and SVHN. When training on SVHN,
we only use training set. All results except WINN-single and baseline
ResNet-32 are from [21]. [21] adopted DCGAN [38] discriminator as their
CNN architecture. The advantage of WINN over a vanilla CNN is evident.
When applied to a stronger baseline such as ResNet-32, WINN is not los-
ing ResNet’s superior classiﬁcation capability in standard supervised clas-
siﬁcation, while attaining special generative capability and robustness to
adversarial attacks (see Table 3 and 4) that do not exist in ResNet. The
images below on the right are the generated samples by the corresponding
WINN (ResNet-32) classiﬁers reported in the table.

Method
MNIST
Baseline vanilla CNN (4 layers)
CNN + GDL [44]
CNN + DCGAN [38]
ICN [21]
WINN-single vanilla (ours)
Baseline ResNet-32
WINN-single ResNet-32 (ours)
SVHN
Baseline ResNet-32
WINN-single ResNet-32 (ours)

Error

0.89%
0.85%
0.84%
0.81%
0.67%
0.45%
0.48%

4.64%
4.50%

Training Methods. We add the Wasserstein loss term to
the ICN [21] loss function, obtaining the following:

L(Wt) =

− (cid:80)

xi∈S+

ln

(cid:80)K

(1)yi
·φ(xi;w(0)
exp{w
t
(1)k
k=1 exp{w
t
fWt(xi) − (cid:80)

)}
t
·φ(xi;w(0)

xi∈S+

t

)}

fWt(xi)

(cid:18)

(cid:80)

+α

xi∈St
−

+λEˆx∼pˆx[((cid:107)∇ˆxfWt(ˆx)(cid:107)2 − 1)2]

(cid:19)

t

, w(1)1
t

>, w(0)

, ..., w(1)K
t

where Wt =< w(0)
denotes the
t
internal parameters for the CNN, and w(1)k
denotes the top-
layer weights for the k-th class. In the experiments, we set
the weight of the WINN loss, α, to 0.01. We use the vanilla
network architecture resembling [21] as the baseline CNN,
which has less ﬁlters and parameters than the one in [21].
We also use a ResNet-32 architecture with Layer Normal-
ization [3] on MNIST and SVHN. In the classiﬁcation-step,

t

we use Adam with a ﬁxed learning rate of 0.001, β1 of 0.0.
In the synthesis-step, we use the Adam optimizer with a
learning rate of 0.02 and β1 of 0.9. Table 2 shows the errors
on MNIST and SVHN.

Table 3: Adversarial examples comparison between the baseline CNN
and WINN on MNIST. We ﬁrst generate N adversarial examples using
method A and count the number of adversarial examples misclassiﬁed by
A (= NA). Adversarial error of A is deﬁned as test error rate against
adversarial examples (= NA/N ). Then among A’s mistakes, we count
the number of adversarial examples misclassiﬁed by B (= NA∩B). Then
error correction rate by B is 1 − NA∩B/NA. ↑ means higher the better;
↓ means lower the better. The comparisons are made in two groups: the
ﬁrst group builds on top a vanilla 4 layer CNN and the second group adopts
ResNet-32. Note that the corresponding errors for these classiﬁers on the
standard MNIST supervised classiﬁcation task can be seen in Table 2.

Method
Baseline vanilla CNN
ICN [21]
WINN-single vanilla (ours)
Baseline ResNet-32
WINN-single ResNet-32 (ours)

Adversarial error Correction rate Correction rate
by Baseline ↓
by Method ↑
-
-
58.68%
52.58%
46.93%
90.00%
-
-
43.69%
89.68%

of Method ↓
32.41%
19.02%
7.99%
11.28%
2.05%

Robustness to adversarial examples. It is argued in [11]
that discriminative CNN’s vulnerability to adversarial ex-
amples primarily arises due to its linear nature. Since the
reclassiﬁcation-by-synthesis process helps tighten the de-
cision boundary (Figure 1), one might expect that CNNs
trained with the WINN algorithm are more robust to ad-
versarial examples. Note that unlike existing methods for
adversarial defenses [13, 28], our method does not train net-
works with speciﬁc types of adversarial examples. With test
images of MNIST and SVHN, we adopt “fast gradient sign
method” [11] ((cid:15) = 0.125 for MNIST and (cid:15) = 0.005 for
SVHN) to generate adversarial examples clipped to range
[−1, 1], which differs from [21]. We experiment with two
networks having the same architecture and only differing
in training method (the standard cross-entropy loss vs. the
WINN procedure). We call the former as the baseline CNN.
We summarize the results in Table 3. Compared to ICN
[21], WINN signiﬁcantly reduces the adversarial error to
7.99% and improves the correction rate to 90.00%. In ad-
dition, we have adopted the ResNet-32 architecture into
WINN. See Table 3 and 4. We still obtain the adversarial
error reduction and correction rate improvement on MNIST
and SVHN ((cid:15) = 0.005) with ResNet-32. Our observation is
that WINN is not necessarily improving over a strong base-
line for the supervised classiﬁcation task but its advantage
on adversarial attacks is evident.

Table 4: Adversarial examples comparison between the baseline ResNet-
32 [16] and WINN on SVHN. Note that the corresponding errors for these
classiﬁers on the standard supervised SVHN classiﬁcation task can be seen
in Table 2.

Method
Baseline ResNet-32
WINN-single ResNet-32 (ours)

Adversarial error Correction rate Correction rate
by Baseline ↓
by Method ↑
-
-
51.37%
74.39%

of Method
29.29%
19.53%

5.7. Agnostic to different architectures

In Figure 8, we demonstrate our algorithm being agnos-
tic to the type of classiﬁer, by varying network architectures
to ResNet [16] and DenseNet [18]. Little modiﬁcation was
required to adapt two architectures for WINN.

(1+

p

+
min
2

where JD(p+; p−
Jeffreys divergence.

)KL(p+||p−

W )≤JD(p+;p−
W) = KL(p+||p−

W )≤(1+ 2
W) + KL(p−

)KL(p+||p−
W||p+) is the

+
min

p

W ),

Proof. Based on the Pinsker’s inequality [37, 41], it is

observed that

WINN-single (ResNet-13)

WINN-single (DenseNet-20)

Figure 8: Synthesized CelebA images with varying network architectures.
We use a single CNN for each experiment.

6. Conclusion

In this paper, we have introduced Wasserstein introspec-
tive neural networks (WINN) that produce encouraging re-
sults as a generator and a discriminative classiﬁer at the
same time. WINN is able to achieve model size reduction
over the previous introspective neural networks (INN) by
a factor of 20. In most of the images shown in the paper,
we ﬁnd a single CNN classiﬁer in WINN being sufﬁcient to
produce visually appealing images as well as signiﬁcant er-
ror reduction against adversarial examples. WINN is agnos-
tic to the architecture design of CNN and we demonstrate
results on three networks including a vanilla CNN, ResNet
[16], and DenseNet [18] networks where popular CNN dis-
criminative classiﬁers are turned into generative models un-
der the WINN procedure. WINN can be adopted in a wide
range of applications in computer vision such as image clas-
siﬁcation, recognition, and generation.
Acknowledgements. This work is supported by NSF
IIS-1717431 and NSF IIS-1618477. The authors thank
Justin Lazarow, Long Jin, Hubert Le, Ying Nian Wu, Max
Welling, Richard Zemel, and Tong Zhang for valuable dis-
cussions.

7. Appendix

A. Proof of Lemma 1.

Proof. Plugging f (x) = ln p+(x)
p−
W (x)

into Eq. (3), we have

Ex∼p+ [f (x)]−E
= Ex∼p+ [ln p+ (x)

p

−
W

(x)

= (cid:82) p+(x) ln p+ (x)

p
= KL(p+||p−

−
W

(x)

[f (x)]

x∼p

−
W

]−E

x∼p

−
W
dx−(cid:82) p−

[ln p+ (x)

]

p

−
W

(x)

W (x) ln p+ (x)

p

−
W

(x)

dx

W )+KL(p−

W ||p+)

(cid:3)

B. Proof of Theorem 1.

Corollary 1 The Jeffreys divergence in Eq. (4) of lemma
1 is lower and upper bounded by KL(p+||p−
W) up to some
multiplicative constant

KL(p−

W||p+) ≥

|p−

W − p+|2 log e,

1
2

where |p−
we also have

W −p+| is total variation (TV) distance. From [41]

KL(p−

W||p+) ≤

|p−

W − p+|2,

log e
p+
min

where p+
to KL(p+||p−
|p−

W − p+| ≡ |p+ − p−
W|,

min = minx p+(x). Applying the above bounds
W) and using the symmetry of the TV distance

p+
min
2

KL(p+||p−

W) ≤ KL(p−

W||p+) ≤

KL(p+||p−

W).

2
p+
min

W) + KL(p−

W) + KL(p−
W).

Plugging the equation above into the Jeffreys divergence,
we observe that KL(p+||p−
W||p+) is upper and
(cid:3)
lower bounded by by KL(p+||p−
Now we can look at theorem 1. It was shown in [21]
that Eq. (1) reduces KL(p+||p−
W), which bounds the Jef-
freys divergence KL(p+||p−
W||p+) as shown in
corollary 1. Lemma 1 shows the connection between Jef-
freys divergence and the WGAN objective (Eq. (3)) when
f (x) = ln p+(x)
. We therefore can see that the formula-
p−
W (x)
tion of introspective neural networks (Eq. (1)) connects to
(cid:3)
a lower bound of the WGAN [2] objective (Eq. (3)).
C. Texture and CelebA Modeling Architecture. Inspired
by [22], we design a CNN architecture for 64 × 64 image
as in Table 5. We use Swish [39] non-linearity after each
convolutional layer. We add Layer Normalization [3] after
each convolution except the ﬁrst layer, following [14].
Table 5: Network architecture for the texture and face image modeling.

Textures and CelebA
Filter size/stride

Alternative Initialization

Filter size/stride

Layer
Input
Conv3-32
Conv3-64
Avg pool
Conv3-64
Conv3-128
Avg pool
Conv3-128
Conv3-256
Avg pool
Conv3-256
Conv3-512
Avg pool
FC-1

3×3/1
3×3/1
2×2/2
3×3/1
3×3/1
2×2/2
3×3/1
3×3/1
2×2/2
3×3/1
3×3/1
2×2/2

Output size
64×64×3
64×64×32
64×64×64
32×32×64
32×32×64
32×32×128
16×16×128
16×16×128
16×16×256
8×8×256
8×8×256
8×8×256
4×4×512
1×1×1

Layer
Input
Conv5-256
Upsample
Conv5-128
Upsample
Conv5-64
Upsample
Conv5-64
Upsample

5×5/1

5×5/1

5×5/1

5×5/1

Output size
4×4×512
4×4×256
8×8×256
8×8×128
16×16×128
16×16×64
32×32×64
32×32×3
64×64×3

D. Alternative Initializations. We sample an initial
pseudo-negative image by applying an operation deﬁned by
the network above to a tensor of size 4 × 4 × 512 sampled
from U [−1, 1]. The weights of the network are sampled
from G(0, 0.12). We do not apply any nonlinearities in the
network. We add Layer Normalization [3] after each con-
volution except the last layer.

References

[1] D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. A learn-
ing algorithm for boltzmann machines. Cognitive science,
9(1):147–169, 1985. 1

[2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gen-
erative adversarial networks. In ICML, 2017. 1, 2, 3, 4, 6,
8

[3] L. J. Ba, R. Kiros, and G. E. Hinton. Layer normalization.

CoRR, abs/1607.06450, 2016. 7, 8

[4] P. Baldi. Autoencoders, unsupervised learning, and deep ar-
chitectures. In ICML Workshop on Unsupervised and Trans-
fer Learning, pages 37–49, 2012. 1

[5] S. Della Pietra, V. Della Pietra, and J. Lafferty. Inducing fea-
tures of random ﬁelds. IEEE transactions on pattern analysis
and machine intelligence, 19(4):380–393, 1997. 1

[6] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning
to generate chairs with convolutional neural networks.
In
CVPR, 2015. 1
[7] V. Dumoulin,

I. Belghazi, B. Poole, O. Mastropietro,
A. Lamb, M. Arjovsky, and A. Courville. Adversarially
learned inference. In ICLR, 2017. 6

[8] Y. Freund and R. E. Schapire. A decision-theoretic general-
ization of on-line learning and an application to boosting. J.
of Comp. and Sys. Sci., 55(1), 1997. 2

[9] L. Gatys, A. S. Ecker, and M. Bethge. Texture synthesis
using convolutional neural networks. In NIPS, 2015. 3, 5, 6
[10] L. A. Gatys, A. S. Ecker, and M. Bethge. A neural algorithm

of artistic style. arXiv preprint arXiv:1508.06576, 2015. 6

[11] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In NIPS, 2014. 1, 3, 7

[12] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets.
In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger, edi-
tors, Advances in Neural Information Processing Systems 27,
pages 2672–2680. Curran Associates, Inc., 2014. 2

[13] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and

harnessing adversarial examples. In ICLR, 2015. 7

[14] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and
In

Improved training of wasserstein gans.

A. Courville.
NIPS, 2017. 1, 2, 4, 5, 6, 8

[15] T. Han, Y. Lu, S.-C. Zhu, and Y. N. Wu. Alternating back-
propagation for generator network. In AAAI, 2017. 2
[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 7, 8

[17] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learn-
ing algorithm for deep belief nets. Neural computation,
18(7):1527–1554, 2006. 1

[18] G. Huang*, Z. Liu*, L. van der Maaten, and K. Q. Wein-
berger. Densely connected convolutional networks.
In
CVPR, 2017. 1, 8

[19] X. Huang, Y. Li, O. Poursaeed, J. Hopcroft, and S. Belongie.
Stacked generative adversarial networks. In CVPR, 2017. 6
[20] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks. In CVPR,
2017. 5

[21] L. Jin, J. Lazarow, and Z. Tu.

Introspective classiﬁcation

with convolutional nets. In NIPS, 2017. 2, 3, 4, 5, 7, 8
[22] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive
growing of gans for improved quality, stability, and variation.
In ICLR, 2018. 8

[23] D. Kingma and J. Ba. Adam: A method for stochastic opti-

[24] D. P. Kingma and M. Welling. Auto-encoding variational

mization. In ICLR, 2015. 5

bayes. In ICLR, 2014. 1

[25] A. Krizhevsky, V. Nair, and G. Hinton. Cifar-10 (canadian

institute for advanced research). 6

[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

ImageNet
Classiﬁcation with Deep Convolutional Neural Networks. In
NIPS, 2012. 1, 6

[27] T. D. Kulkarni, W. F. Whitney, P. Kohli, and J. Tenenbaum.
Deep convolutional inverse graphics network. In NIPS, 2015.
1

[28] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial ma-

chine learning at scale. In ICLR, 2017. 7

[29] J. Lazarow*, L. Jin*, and Z. Tu.

Introspective neural net-
works for generative modeling. In ICCV, 2017. 2, 3, 4, 5,
6

[30] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. Howard,
W. Hubbard, and L. Jackel. Backpropagation applied to
handwritten zip code recognition. In Neural Computation,
1989. 1

[31] C.-Y. Lee*, S. Xie*, P. Gallagher, Z. Zhang, and Z. Tu.

Deeply-supervised nets. In AISTATS, 2015. 1

[32] S. Liu, O. Bousquet, and K. Chaudhuri. Approximation and
convergence properties of generative adversarial learning. In
NIPS, 2017. 3

[33] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face

attributes in the wild. In ICCV, 2015. 6

[34] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In NIPS workshop on deep learning and unsu-
pervised feature learning, volume 2011, page 5, 2011. 6
[35] J. Ngiam, Z. Chen, P. W. Koh, and A. Y. Ng. Learning deep

energy models. In ICML, 2011. 2

[36] B. A. Olshausen and D. J. Field. Sparse coding with an over-
complete basis set: A strategy employed by v1? Vision re-
search, 37(23):3311–3325, 1997. 1

[37] M. S. Pinsker. Information and information stability of ran-

dom variables and processes. 1960. 8

[38] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. In ICLR, 2016. 1, 6, 7

[39] P. Ramachandran, B. Zoph, and Q. V. Le. Searching for ac-

tivation functions. CoRR, abs/1710.05941, 2017. 8

[40] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Rad-
ford, and X. Chen. Improved techniques for training gans. In
NIPS, 2016. 6

[41] I. Sason and S. Verd´u. f -divergence inequalities.

IEEE
Transactions on Information Theory, 62(11):5973–6006,
2016. 8

[42] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
1

[43] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 1

[44] Z. Tu. Learning generative models via discriminative ap-

proaches. In CVPR, 2007. 1, 2, 3, 5, 7

[45] D. Ulyanov, V. Lebedev, A. Vedaldi, and V. Lempitsky. Tex-
ture networks: Feed-forward synthesis of textures and styl-
ized images. In ICML, 2016. 3, 6

[46] D. Warde-Farley and Y. Bengio. Improving generative adver-
sarial networks with denoising feature matching. In ICLR,
2017. 6

[47] M. Welling and Y. W. Teh. Bayesian learning via stochastic

gradient langevin dynamics. In ICML, 2011. 3, 4

[48] M. Welling, R. S. Zemel, and G. E. Hinton. Self supervised

boosting. In NIPS, 2002. 1

[49] Y. N. Wu, Z. Si, H. Gong, and S.-C. Zhu. Learning active ba-
sis model for object detection and recognition. International
journal of computer vision, 90(2):198–235, 2010. 1

[50] J. Xie, Y. Lu, R. Gao, S.-C. Zhu, and Y. N. Wu. Cooperative
learning of energy-based model and latent variable model via
mcmc teaching. In AAAI, 2018. 2

[51] J. Xie, Y. Lu, S.-C. Zhu, and Y. N. Wu. A theory of genera-

tive convnet. In ICML, 2016. 2

[52] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated
residual transformations for deep neural networks. In CVPR,
2017. 1

[53] S.-C. Zhu and D. Mumford. A stochastic grammar of im-
ages. Foundations and Trends R(cid:13) in Computer Graphics and
Vision, 2(4):259–362, 2007. 1

[54] S. C. Zhu, Y. N. Wu, and D. Mumford. Minimax entropy
principle and its application to texture modeling. Neural
Computation, 9(8):1627–1660, 1997. 1

Wasserstein Introspective Neural Networks

Kwonjoon Lee

Weijian Xu

Fan Fan

Zhuowen Tu

University of California San Diego
{kwl042, wex041, f1fan, ztu}@ucsd.edu

8
1
0
2
 
r
p
A
 
7
 
 
]

V
C
.
s
c
[
 
 
5
v
5
7
8
8
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

We present Wasserstein introspective neural networks
(WINN) that are both a generator and a discriminator
within a single model. WINN provides a signiﬁcant im-
provement over the recent introspective neural networks
(INN) method by enhancing INN’s generative modeling ca-
pability. WINN has three interesting properties: (1) A math-
ematical connection between the formulation of the INN al-
gorithm and that of Wasserstein generative adversarial net-
works (WGAN) is made. (2) The explicit adoption of the
Wasserstein distance into INN results in a large enhance-
ment to INN, achieving compelling results even with a single
classiﬁer — e.g., providing nearly a 20 times reduction in
model size over INN for unsupervised generative modeling.
(3) When applied to supervised classiﬁcation, WINN also
gives rise to improved robustness against adversarial exam-
ples in terms of the error reduction. In the experiments, we
report encouraging results on unsupervised learning prob-
lems including texture, face, and object modeling, as well as
a supervised classiﬁcation task against adversarial attacks.
Our code is available online1.

1. Introduction

Performance within the task of supervised image classi-
ﬁcation has been vastly improved in the era of deep learn-
ing using modern convolutional neural network (CNN) [30]
based discriminative classiﬁers [26, 43, 31, 42, 16, 52, 18].
On the other hand, unsupervised generative models in deep
learning were previously attained using methods under the
umbrella of graphical models — e.g., the Boltzmann ma-
chine [17] or autoencoder [4, 24] architectures. However,
the rich representational power seen within convolution-
based (discriminative) models is not being directly enjoyed
in these generative models. Later, inverting convolutional
neural networks in order to convert internal representations
into a real image was investigated in [6, 27]. Recently,
generative adversarial networks (GAN) [11] and followup

1https://github.com/kjunelee/WINN

Figure 1: Schematic illustration of Wasserstein introspective neural net-
works for unsupervised learning. The left ﬁgure shows the input examples;
the bottom ﬁgures show the pseudo-negatives (purple crosses) being pro-
gressively synthesized; the top ﬁgures show the classiﬁcation between the
given examples (positives) and synthesized pseudo-negatives (negatives).
The right ﬁgure shows the model learned to approach the target distribution
based on the given data.

works [38, 2, 14] have attracted a tremendous amount of
attention in machine learning and computer vision by pro-
ducing high quality synthesized images by training a pair
of competing models against one another in an adversar-
ial manner. While a generator tries to create “fake” images
to fool the discriminator, the discriminator attempts to dis-
cern between these “real” (given training) and “fake” im-
ages. After convergence, the generator is able to produce
images faithful to the underlying data distribution.

Before the deep learning era [17], generative model-
ing had been an area with a steady pace of development
[1, 5, 54, 36, 49, 53]. These models were guided by rigor-
ous statistical theories which, although nice in theory, did
not succeed in producing synthesized images with practical
quality.

In terms of building generative models from discrimina-
tive classiﬁers, there have been early attempts in [48, 44].

In [48], a generative model was obtained from a repeat-
edly trained boosting algorithm [8] using a weak classi-
ﬁer whereas [44] used a strong classiﬁer in order to self-
generate negative examples or “pseudo-negatives”.

To address the lack of richness in representation and ef-
ﬁciency in synthesis, convolutional neural networks were
adopted in introspective neural networks (INN) [29, 21] to
build a single model that is simultaneously generative and
discriminative. The generative modeling aspect was studied
in [29] where a sequence of CNN classiﬁers (10 − 60) were
trained, while the power within the classiﬁcation setting was
revealed in [21] in the form of introspective convolutional
networks (ICN) that used only a single CNN classiﬁer. Al-
though INN models [29, 21] point to a promising direction
to obtain a single model being both a good generator and a
strong discriminative classiﬁer, a sequence of CNNs were
needed to generate realistic synthesis. As a result, this re-
quirement may serve as a possible bottleneck with respect
to training complexity and model size.

Recently, a generic formulation [2] was developed
within the GAN model family to incorporate a Wasserstein
objective to alleviate the well-known difﬁculty in GAN
training. Motivated by introspective neural networks (INN)
[29] and this Wasserstein objective [2], we propose to adopt
the Wasserstein term into the INN formulation to enhance
the modeling capability. The resulting model, Wasserstein
introspective neural networks (WINN) shows greatly en-
hanced modeling capability over INN by having 20× re-
duction in the number of CNN classiﬁers.

2. Signiﬁcance and Related Work

We make several interesting observations for WINN:

• A mathematical connection between the WGAN formulation
[2] and the INN algorithm [29] is made to better understand the
overall objective function within INN.

• By adopting the Wasserstein distance into INN, we are able to
generate images using a single CNN in WINN with even higher
quality than those by INN that uses 20 CNNs (as seen in Figure
2, 4, 5, 6, and 7; the similar underlying CNN architectures are
used in WINN and INN). WINN achieves a signiﬁcant reduc-
tion in model complexity over INN, making the generator more
practical.

• Within texture modeling, INN and WINN are able to inherently
model the input image space, making the synthesis of large tex-
ture images realistic, whereas GAN projects a noise vector onto
the image space making the image patch stitching more difﬁ-
cult (although extensions exist), as demonstrated in Figure 2.
• To compare with the family of GAN models, we compute In-
ception scores using the standard procedure on the CIFAR-10
datasets and observed modest results. Here, we typically train
4-5 cascades to boost the numbers but WINN with one CNN is
already promising. Overall, modern GAN variants (e.g., [14])
still outperform our WINN with better quality images. Some
results are shown in Figure 7.

• To test the robustness of the discriminative abilities of WINN,
we directly make WINN into a discriminative classiﬁer by
training it on the standard MNIST and SVHN datasets. Not
only are we able to improve over the previous ICN [21] clas-
siﬁer for supervised classiﬁcation, we also observe a large
improvement in robustness against adversarial examples com-
pared with the baseline CNN, ResNet, and the competing ICN.
In terms of other related work, we brieﬂy discuss some

existing methods below.
Wasserstein GAN. A closely related work to our WINN al-
gorithm is the Wasserstein generative adversarial networks
(WGAN) method [2, 14]. While WINN adopts the Wasser-
stein distance as motivated by WGAN, our overall algo-
rithm is still within the family of introspective neural net-
works (INN) [29, 21]. WGAN on the other hand is a vari-
ant of GAN with an improvement over GAN by having an
objective that is easier to train. The level of difference be-
tween WINN and WGAN is similar to that between INN
[29, 21] and GAN [12]. The overall comparisons between
INN and GAN have been described in [29, 21].
Generative ConvNets. Recently, there has also been a clus-
ter of algorithms developed in [50, 51, 15] where Langevin
dynamics are adopted in generator CNNs. However, the
models proposed in [50, 51, 15] do not perform introspec-
tion (Figure 1) and their generator and discriminator com-
ponents are still somewhat separated; thus, their generators
are not used as effective discriminative classiﬁers to per-
form state-of-the-art classiﬁcation on standard supervised
machine learning tasks. Their training processes are also
more complex than those of INN and WINN.
Deep energy models (DEMs) [35]. DEM [35] extends the
standard density estimation by using multi-layer neural net-
works (MLNN) with a rather complex training procedure.
The probability model in DEM includes both the raw input
and the features computed by MLNN. WINN instead takes
a more general and simplistic form and is easier to train (see
Eq. (1)). In general, DEM belongs to the minimum descrip-
tion length (MDL) family models in which the maximum
likelihood is achieved. WINN, instead, has a formulation
being simultaneously discriminative and generative.

3. Introspective Neural Networks

3.1. Brief introduction of INN

We ﬁrst brieﬂy introduce the introspective neural net-
work method (INNg) [29] for generative modeling and its
companion model [21] which focuses on the classiﬁcation
aspect. The main motivation behind the INN work [29, 21]
is to make a convolutional neural network classiﬁer simulta-
neously discriminative and generative. A single CNN clas-
siﬁer is trained in an introspective manner to improve the
standard supervised classiﬁcation result [21], however, a se-
quence of CNNs (typically 10 − 60) is needed to be able to
synthesize images of good quality [29].

Figure 2: Comparison of texture synthesis algorithms. Gatys et al. [9], and TextureNet [45] results are from [45].

Figure 1 shows a brief illustration of a single introspec-
tive CNN classiﬁer [21]. We discuss our basic unsuper-
vised formulation next. Suppose we are given a set of train-
ing examples: S = {xi
| i = 1, . . . , n} where we as-
sume each xi ∈ Rm — e.g., m = 4096 for images of
size 64 × 64. These will constitute positive examples of
the patterns/targets we wish to model. The main idea of
INN is to deﬁne pseudo-negative examples that are to be
self-generated by the discriminative classiﬁer itself. We
therefore deﬁne label y for each example x, y = +1 if
x is from the given training set and y = −1 if x is self-
generated. Motivated by the generative via discriminative
learning (GDL) framework [44], one could try to learn the
generative model for the given examples, p(x|y = +1), by
a sequentially learned distribution of the pseudo-negative
samples, pt(x|y = −1; Wt) which is abbreviated as p−
(x)
Wt
where Wt includes all the model parameters learned at step
t.

1
Zt

p−
Wt

)}·p−

exp{w(1)

·φ(x; w(0)

t

t

t

t

(x) =

·φ(x; w(0)

0 (x), t = 1, . . . , T
(1)
where Zt = (cid:82) exp{w(1)
)}·p−
0 (x)dx and the ini-
tial distribution p−
0 (x) such as a Gaussian distribution over
the entire space of x ∈ Rm. The discriminative classiﬁer
is a convolutional neural network (CNN) parameterized by
Wt = (w(0)
) where w(1)
denotes the weights of the
top layer combining the features through φ(x; w(0)
) (e.g.,
softmax layer) and w(0)
parameterizing the internal repre-
sentations. The synthesis process through which pseudo-
negative samples are generated is carried out by stochastic
gradient Langevin dynamics [47] as

, w(1)
t

t

t

t

t

∆x =

∇(w(1)

· φ(x; w(0)

)) + η

t

t

(cid:15)
2

where η ∼ N (0, (cid:15)) is a Gaussian distribution and (cid:15) is the
step size that is annealed in the sampling process. Overall,
we desire

p−
Wt

(x) t=∞→ p(x|y = +1),

(2)

using the iterative reclassiﬁcation-by-synthesis process [21,
29] guided by Eq. (1).

3.2. Connection to the Wasserstein distance

overall

is carried out

training process,

reclassiﬁcation-by-
The
synthesis,
iteratively without an explicit
objective function. The generative adversarial network
(GAN) model [11] instead has an objective function
formulated in a minimax fashion with the generator and
discriminator competing against each other. The Wasser-
stein generative adversarial network (WGAN) work [2]
improves GAN [11] by replacing the Jensen-Shannon dis-
tance with an efﬁcient approximation of the Earth-Mover
distance [2]. Also, there has been further generalization of
the GAN family models in [32].

Let p+(x) ≡ p(x|y = +1) be the target distribution and
p−
W(x) ≡ p(x|y = −1; W) be the pseudo-negative distri-
bution parameterized by W. Next, we show a connection
between the INN framework and the WGAN formulation
[2], whose objective (rewritten with our notations) can be
deﬁned as

min
W

max
||f ||L≤1

Ex∼p+[f (x)] − Ex∼p−

[f (x)],

W

(3)

where ||f ||L ≤ 1 denotes the space of 1-Lipschitz func-
tions. To build the connection between Eq. (3) of WGAN
and Eq. (1) of INN, we ﬁrst present the following lemma.

Lemma 1 Considering f (x) = ln p+(x)
and assuming its
p−
W (x)
1-Lipschitz property, we have a lower bound on the Wasser-
stein distance by

max
||f ||L≤1
≥ KL(p+||p−

Ex∼p+[f (x)] − Ex∼p−

[f (x)]

W

W) + KL(p−

W||p+)

(4)

where KL(p||q) denotes the Kullback-Leibler divergence
between the two distributions p and q, and KL(p+||p−
W) +
KL(p−

W||p+) is the Jeffreys divergence.

Proof: see Appendix A.
Note that using the Bayes’ rule, the ratio of the generative
probabilities p(x|y=+1)
p(x|y=−1) in Lemma 1 can be turned into the
ratio of the discriminative probabilities p(y=+1|x)
p(y=−1|x) assuming
equal priors p(y = +1) = p(y = −1).

Theorem 1 The introspective neural network formulation
(Eq. (1)) implicitly minimizes a lower bound of the WGAN
objective [2] (Eq. (3)).
Proof: see Appendix B.

4. Wasserstein Introspective Networks

Algorithm 1 Outline of WINN-single training algorithm. We
use k = 3 and λ = 10.

while Wt has not converged do

// Classiﬁcation-step:
for k steps do

(cid:80)m

i , i = 1, . . . , m.

m} from S+.
1 ,··· ,x−

Sample m positive samples {x+
1 ,··· ,x+
m} from St
Sample m pseudo-negative samples {x−
−.
Sample m random numbers {α1,··· ,αm} from U [0, 1].
i + (1 − αi)x−
ˆxi ← αix+
Perform stochastic gradient descent:
(x−
∇Wt
end for
// Synthesis-step:
Sample r noise samples {x1, · · · , xr} from p−
Perform stochastic gradient ascent with early-stopping.
St+1
− ← St
t ← t + 1.

− ∪ {x1, · · · , xr}.

i )]+λ((cid:107)∇ˆxi

i=1{[fWt

(ˆxi)(cid:107)2−1)2}.

0 (x).

i )−fWt

(x+

fWt

1
m

end while

Here we present the formulation for WINN building
upon the formulation of the prior introspective learning
works presented in Section 3.

4.1. WINN algorithm

We denote our unlabeled input training data as S+ =
{xi|yi = +1, i = 1, . . . , n}. Also, we denote the set of
all the self-generated pseudo-negative samples up to step
t as St
− = {xi|yi = −1, i = 1, . . . , l}. In other words,
St
− consists of pseudo-negatives x sampled from our model
p−
(x) for t ≥ 1 where Wt is the model parameter vector
Wt
at step t.

Classiﬁcation-step. The classiﬁcation-step can be viewed
as training a classiﬁer to approximate the Wasserstein dis-
tance between S+ and St
− for t ≥ 1. Note that we also keep
pseudo-negatives from earlier stages – which are essentially
the mistakes of the earlier stages – to prevent the classiﬁer
forgetting what it has learned in previous stages. We use
CNNs parametrized by Wt as base classiﬁers. Let fWt(·)
denote the output of ﬁnal fully connected layer (without
passing through sigmoid nonlinearity) of the CNN. In the

previous introspective learning frameworks [29, 21], the
classiﬁer learning objective was to minimize the following
standard cross-entropy loss function on S+ ∪ St
−:

L(Wt)=−{E

x+∼p+ ln σ[+fWt (x+)]+E

ln σ[−fWt (x−)]}

x−∼p

−
Wt

where σ(·) denotes the sigmoid nonlinearity. Motivated by
Section 3.2, in WINN training we wish to minimize the fol-
lowing Wasserstein loss function by the stochastic gradient
descent algorithm via backpropagation:

L(Wt)=−(cid:2)E

x+∼p+ fWt (x+)−E

fWt (x−)(cid:3)

(5)

x−∼p

−
Wt

To enforce the function fWt to be 1-Lipschitz, we add

the following gradient penalty term [14] to L(Wt):

λEˆx∼pˆx [((cid:107)∇ˆxfWt (ˆx)(cid:107)2 − 1)2]

where ˆx = αx+ + (1 − α)x−, x+ ∼ p+, x− ∼ p−
Wt
α ∼ U [0, 1].

, and

Synthesis-step. Obtaining increasingly difﬁcult pseudo-
negative samples is an integral part of the introspective
learning framework, as it is crucial for tightening the de-
cision boundary. To this end, we develop an efﬁcient sam-
pling procedure under the Wasserstein formulation. After
the classiﬁcation-step, we obtain the following distribution
of pseudo-negatives:

p−
Wt

(x) =

exp{fWt(x)} · p−

0 (x), t = 1, . . . , T

(6)

1
Zt

where Zt = (cid:82) exp{fWt(x)} · p−
0 (x)dx; the initial distri-
bution p−
0 (x) is a Gaussian distribution G(x; 0, σ2) or the
distribution deﬁned in Appendix D. We ﬁnd that the distri-
bution of Appendix D encourages the diversity of sampled
images.
The following equivalence is shown in [29, 21]:

p(y = +1|x; Wt)
p(y = −1|x; Wt)

= exp{fWt(x)}.

(7)

The sampling strategy of [29, 21] was to carry out gradient
ascent on the term ln p(y=+1|x;Wt)
p(y=−1|x;Wt) . In Lemma 1 we chose
f (x) to be ln p+(x)
. Using Bayes’ rule, it is easy to see
p−
W (x)
p(y=−1|x;Wt) is loosely connected to ∇ ln p+(x)
that ∇ ln p(y=+1|x;Wt)
(x)
Also, [2, 14] argue that fWt(x) correlates with the quality
of the sample x. This motivates us to use the following sam-
pling strategy. After initializing x by drawing a fair sample
from p−
0 (x), we increase fWt(x) using gradient ascent on
the image x via backpropagation. Speciﬁcally, as shown in
[47], we can obtain fair samples from the distribution p−
Wt
using the following update rule:

p−
Wt

.

∆x =

∇fWt(x) + η

(cid:15)
2

where (cid:15) is a time-varying step size and η is a random vari-
able following the Gaussian distribution N (0, (cid:15)). Gaussian
noise term is added to make samples cover the full distribu-
tion. Inspired by [20], we found that injecting noise in the
image space could be substituted by applying Dropout to
the higher layers of CNN. In practice, we were able obtain
the samples of enough diversity without step size annealing
and noise injection.
As an early stopping criterion, we empirically ﬁnd that
the following is effective: (1) we measure the minimum
and maximum fWt(·) of positive examples; (2) we set the
early stopping threshold to a random number from the uni-
form distribution between these two numbers. Intuitively,
by matching the value of fWt (·) positives and pseudo-
negatives, we expect to obtain pseudo-negative samples that
match the quality of positive samples.

4.2. Expanding model capacity

In practice, we ﬁnd that the version with the single
classiﬁer – which we call WINN-single – is expressive
enough to capture the generative distribution under vari-
ety of applications. The introspective learning formulation
[44, 29, 21] allows us to model more complex distributions
by adding a sequence of cascaded classiﬁers parameterized
by (W1, . . . , WK). Then, we can model the distribution as:

1
Zt

p−
Wk (x) =

exp{fWk (x)} · p−

Wk−1(x), k = 2, . . . , K

(8)
In the next sections, we demonstrate the modeling capabil-
ity of WINN under cascaded classiﬁers, as well as its ag-
nosticy to the type of base classiﬁer.

4.3. GAN’s discriminator vs. WINN’s classiﬁer

(a) WGAN-GP discriminator

(b) WINN-single

Figure 3: Synthesized images from the discriminators of WGAN and
WINN trained on the CelebA dataset. Both use the same ResNet archi-
tecture for the discriminator as the one adopted in [14].

GAN uses the competing discriminator and the gener-
ator whereas WINN maintains a single model being both
generative and discriminative. Some general comparisons
between GAN and INN have been provided in [29, 21]. Be-
low we make a few additional observations that are worth
future exploration and discussions.

• First, the generator of GAN is a cost-effective option for image
patch synthesis, as it works in a feed-forward fashion. However
the generator of GAN is not meant to be trained as a classiﬁer
to perform the standard classiﬁcation task, while the generator

in the introspective framework is also a strong classiﬁer. Sec-
tion 5.6 shows WINN to have signiﬁcant robustness to external
adversarial examples.

• Second, the discriminator in GAN is meant to be a critic but
not a generator. To show whether or not the discriminator in
GAN can also be used as a generator, we train WGAN-GP [14]
on the CelebA face dataset. Using the same CNN architecture
(ResNet from [14]) that was used as GAN’s discriminator, we
also train a WINN-single model, making GAN’s discrimina-
tor and WINN-single to have the identical CNN architecture.
Applying the sampling strategy to WGAN-GP’s discriminator
allows us to synthesize image form WGAN-GP’s discriminator
as well and we show some samples in Figure 3 (a). These syn-
thesized images are not like faces, yet they have been classiﬁed
by the discriminator of WGAN-GP as “real” faces; this demon-
strates the separation between the generator and the discrimina-
tor in GAN. In contrast, images synthesized by WINN-single’s
CNN classiﬁer are faces like, as shown in Figure 3 (b).

• Third, the discriminator of GAN may not be used as a direct
discriminative classiﬁer for the standard supervised learning
task. As shown and discussed in ICN [21], the introspective
framework has the ability of classiﬁcation for discriminator.

5. Experiments

5.1. Implementation

Classiﬁcation-step. For training the discriminator network,
we use Adam [23] with a mini-batch size of 100. The learn-
ing rate was set to 0.0001. We set β1 = 0.0, and β2 = 0.9,
inspired by [14]. Each batch consists of 50 positive im-
ages sampled from the set of positives S+ and 50 pseudo-
negative images sampled from the set of pseudo-negatives
S−. In each iteration, we limit total number of training im-
ages to 10, 000.
Synthesis-step. For synthesizing pseudo-negative images
via back-propagation, we perform gradient ascent on the
image space. In the ﬁrst cascade, each image is initialized
with a noise sampled from the distribution described in Ap-
pendix D. In the later cascades, images are initialized with
the images sampled from the last cascade. We use Adam
with a mini-batch size of 100. The learning rate was set to
0.01. We set β1 = 0.9, and β2 = 0.99.
5.2. Texture modeling

We evaluate the texture modeling capability of WINN.
For a fair comparison, we use the same 7 texture images
presented in [9] where each texture image has a size of 256
× 256. We follow the training method of Section 5.1 ex-
cept that positive images are constructed by cropping 64
× 64 patches from the source texture image at random po-
sitions. We use network architecture of Appendix C. Af-
ter training is done on the 64×64 patch-based model, we
try to synthesize texture images of arbitrary size using the
anysize-image-generation method following [29]. During
the synthesis process, we keep a single working image of

previous generative modeling works since it contains large
pose variations and background clutters. The network archi-
tecture adopted here is described in Appendix C. In Figure
5, we show some synthesized face images using WINN-
single and WINN, as well as those by DCGAN [38], INNg-
single, and INNg [29]. WINN-single attains image quality
even higher than that of INNg (12 CNNs).
5.4. SVHN modeling

Real data

DCGAN

INNg-single

INNg

WINN-single (ours) WINN-4CNNs (ours)

Figure 6: Images generated by various models trained on SVHN. DCGAN
[38] result is from [29].

SVHN [34] consists of 32 × 32 images from Google
It contains 73, 257 training images, 26, 032
Street View.
test images, and 531, 131 extra images. We use only the
training images for the unsupervised SVHN modeling. We
use the ResNet architecture described in [14]. Generated
images by WINN-single and WINN (4 CNN classiﬁers) as
well as DCGAN and INN are shown in Figure 6. The im-
provement of WINN over INNg is evident.
5.5. CIFAR-10 modeling
Table 1: Inception score on CIFAR-10. “-L” in Improved GANs means
without labels.

Method
Real data
WGAN-GP [14]
WGAN [2]
DCGAN [38] (in [19])
ALI [7] (in [46])
Improved GANs (-L) [40]
INNg-single [29]
INNg [29]
WINN-single (ours)
WINN-5CNNs (ours)

Score
11.95 ± .20
7.86 ± .07
5.88 ± .07
6.16 ± .07
5.34 ± .05
4.36 ± .04
1.95 ± .01
3.04 ± .02
4.62 ± .05
5.58 ± .05

CIFAR-10 [25] consists of 50, 000 training images and
10, 000 test images of size 32 × 32 in 10 classes. We use
training images augmented by horizontal ﬂips [26] for un-
supervised CIFAR-10 modeling. We use the ResNet given
in [14]. Figure 7 shows generated images by various mod-
els.

To measure the semantic discriminability, we compute
the Inception scores [40] on 50, 000 generated images.

Figure 4: More texture synthesis results. Gatys et al. [9] and TextureNets
[45] results are from [45].

size 320×320. Note that we expand the image so that cen-
ter 256×256 pixels are covered with equal probability. In
each iteration, we sample 200 patches from the working im-
age, and perform gradient ascent on the chosen patches. For
the overlapping pixels between patches, we take the average
of the gradients assigned to such pixels. We show synthe-
sized texture images in Figure 2 and 4. WINN-single shows
a signiﬁcant improvement over INNg-single and compara-
ble results to INNg (using 20 CNNs).
It is worth noting
that [10, 45] leverage rich features of VGG-19 network pre-
trained on ImageNet. WINN and INNg instead train net-
works from scratch.
5.3. CelebA face modeling

Real data

DCGAN

INNg-single

INNg

WINN-single (ours) WINN-4CNNs (ours)
Figure 5: Images generated by various models trained on CelebA.

The CelebA dataset [33] consists of 202, 599 face im-
ages of celebrities. This dataset has been widely used in the

Real data

DCGAN

WINN-single (ours)

WINN-5CNNs (ours)

Figure 7: Images generated by models trained on CIFAR-10.

WINN shows its clear advantage over INN. WINN-5CNNs
produces a result close to WGAN but there is still a gap to
the state-of-the-art results by WGAN-GP.
5.6. Image classiﬁcation and adversarial examples

To demonstrate the robustness of WINN as a discrimi-
native classiﬁer, we present experiments on the supervised
classiﬁcation tasks.

Table 2: Test errors on MNIST and SVHN. When training on SVHN,
we only use training set. All results except WINN-single and baseline
ResNet-32 are from [21]. [21] adopted DCGAN [38] discriminator as their
CNN architecture. The advantage of WINN over a vanilla CNN is evident.
When applied to a stronger baseline such as ResNet-32, WINN is not los-
ing ResNet’s superior classiﬁcation capability in standard supervised clas-
siﬁcation, while attaining special generative capability and robustness to
adversarial attacks (see Table 3 and 4) that do not exist in ResNet. The
images below on the right are the generated samples by the corresponding
WINN (ResNet-32) classiﬁers reported in the table.

Method
MNIST
Baseline vanilla CNN (4 layers)
CNN + GDL [44]
CNN + DCGAN [38]
ICN [21]
WINN-single vanilla (ours)
Baseline ResNet-32
WINN-single ResNet-32 (ours)
SVHN
Baseline ResNet-32
WINN-single ResNet-32 (ours)

Error

0.89%
0.85%
0.84%
0.81%
0.67%
0.45%
0.48%

4.64%
4.50%

Training Methods. We add the Wasserstein loss term to
the ICN [21] loss function, obtaining the following:

L(Wt) =

− (cid:80)

xi∈S+

ln

(cid:80)K

(1)yi
·φ(xi;w(0)
exp{w
t
(1)k
k=1 exp{w
t
fWt(xi) − (cid:80)

)}
t
·φ(xi;w(0)

xi∈S+

t

)}

fWt(xi)

(cid:18)

(cid:80)

+α

xi∈St
−

+λEˆx∼pˆx[((cid:107)∇ˆxfWt(ˆx)(cid:107)2 − 1)2]

(cid:19)

t

, w(1)1
t

>, w(0)

, ..., w(1)K
t

where Wt =< w(0)
denotes the
t
internal parameters for the CNN, and w(1)k
denotes the top-
layer weights for the k-th class. In the experiments, we set
the weight of the WINN loss, α, to 0.01. We use the vanilla
network architecture resembling [21] as the baseline CNN,
which has less ﬁlters and parameters than the one in [21].
We also use a ResNet-32 architecture with Layer Normal-
ization [3] on MNIST and SVHN. In the classiﬁcation-step,

t

we use Adam with a ﬁxed learning rate of 0.001, β1 of 0.0.
In the synthesis-step, we use the Adam optimizer with a
learning rate of 0.02 and β1 of 0.9. Table 2 shows the errors
on MNIST and SVHN.

Table 3: Adversarial examples comparison between the baseline CNN
and WINN on MNIST. We ﬁrst generate N adversarial examples using
method A and count the number of adversarial examples misclassiﬁed by
A (= NA). Adversarial error of A is deﬁned as test error rate against
adversarial examples (= NA/N ). Then among A’s mistakes, we count
the number of adversarial examples misclassiﬁed by B (= NA∩B). Then
error correction rate by B is 1 − NA∩B/NA. ↑ means higher the better;
↓ means lower the better. The comparisons are made in two groups: the
ﬁrst group builds on top a vanilla 4 layer CNN and the second group adopts
ResNet-32. Note that the corresponding errors for these classiﬁers on the
standard MNIST supervised classiﬁcation task can be seen in Table 2.

Method
Baseline vanilla CNN
ICN [21]
WINN-single vanilla (ours)
Baseline ResNet-32
WINN-single ResNet-32 (ours)

Adversarial error Correction rate Correction rate
by Baseline ↓
by Method ↑
-
-
58.68%
52.58%
46.93%
90.00%
-
-
43.69%
89.68%

of Method ↓
32.41%
19.02%
7.99%
11.28%
2.05%

Robustness to adversarial examples. It is argued in [11]
that discriminative CNN’s vulnerability to adversarial ex-
amples primarily arises due to its linear nature. Since the
reclassiﬁcation-by-synthesis process helps tighten the de-
cision boundary (Figure 1), one might expect that CNNs
trained with the WINN algorithm are more robust to ad-
versarial examples. Note that unlike existing methods for
adversarial defenses [13, 28], our method does not train net-
works with speciﬁc types of adversarial examples. With test
images of MNIST and SVHN, we adopt “fast gradient sign
method” [11] ((cid:15) = 0.125 for MNIST and (cid:15) = 0.005 for
SVHN) to generate adversarial examples clipped to range
[−1, 1], which differs from [21]. We experiment with two
networks having the same architecture and only differing
in training method (the standard cross-entropy loss vs. the
WINN procedure). We call the former as the baseline CNN.
We summarize the results in Table 3. Compared to ICN
[21], WINN signiﬁcantly reduces the adversarial error to
7.99% and improves the correction rate to 90.00%. In ad-
dition, we have adopted the ResNet-32 architecture into
WINN. See Table 3 and 4. We still obtain the adversarial
error reduction and correction rate improvement on MNIST
and SVHN ((cid:15) = 0.005) with ResNet-32. Our observation is
that WINN is not necessarily improving over a strong base-
line for the supervised classiﬁcation task but its advantage
on adversarial attacks is evident.

Table 4: Adversarial examples comparison between the baseline ResNet-
32 [16] and WINN on SVHN. Note that the corresponding errors for these
classiﬁers on the standard supervised SVHN classiﬁcation task can be seen
in Table 2.

Method
Baseline ResNet-32
WINN-single ResNet-32 (ours)

Adversarial error Correction rate Correction rate
by Baseline ↓
by Method ↑
-
-
51.37%
74.39%

of Method
29.29%
19.53%

5.7. Agnostic to different architectures

In Figure 8, we demonstrate our algorithm being agnos-
tic to the type of classiﬁer, by varying network architectures
to ResNet [16] and DenseNet [18]. Little modiﬁcation was
required to adapt two architectures for WINN.

(1+

p

+
min
2

where JD(p+; p−
Jeffreys divergence.

)KL(p+||p−

W )≤JD(p+;p−
W) = KL(p+||p−

W )≤(1+ 2
W) + KL(p−

)KL(p+||p−
W||p+) is the

+
min

p

W ),

Proof. Based on the Pinsker’s inequality [37, 41], it is

observed that

WINN-single (ResNet-13)

WINN-single (DenseNet-20)

Figure 8: Synthesized CelebA images with varying network architectures.
We use a single CNN for each experiment.

6. Conclusion

In this paper, we have introduced Wasserstein introspec-
tive neural networks (WINN) that produce encouraging re-
sults as a generator and a discriminative classiﬁer at the
same time. WINN is able to achieve model size reduction
over the previous introspective neural networks (INN) by
a factor of 20. In most of the images shown in the paper,
we ﬁnd a single CNN classiﬁer in WINN being sufﬁcient to
produce visually appealing images as well as signiﬁcant er-
ror reduction against adversarial examples. WINN is agnos-
tic to the architecture design of CNN and we demonstrate
results on three networks including a vanilla CNN, ResNet
[16], and DenseNet [18] networks where popular CNN dis-
criminative classiﬁers are turned into generative models un-
der the WINN procedure. WINN can be adopted in a wide
range of applications in computer vision such as image clas-
siﬁcation, recognition, and generation.
Acknowledgements. This work is supported by NSF
IIS-1717431 and NSF IIS-1618477. The authors thank
Justin Lazarow, Long Jin, Hubert Le, Ying Nian Wu, Max
Welling, Richard Zemel, and Tong Zhang for valuable dis-
cussions.

7. Appendix

A. Proof of Lemma 1.

Proof. Plugging f (x) = ln p+(x)
p−
W (x)

into Eq. (3), we have

Ex∼p+ [f (x)]−E
= Ex∼p+ [ln p+ (x)

p

−
W

(x)

= (cid:82) p+(x) ln p+ (x)

p
= KL(p+||p−

−
W

(x)

[f (x)]

x∼p

−
W

]−E

x∼p

−
W
dx−(cid:82) p−

[ln p+ (x)

]

p

−
W

(x)

W (x) ln p+ (x)

p

−
W

(x)

dx

W )+KL(p−

W ||p+)

(cid:3)

B. Proof of Theorem 1.

Corollary 1 The Jeffreys divergence in Eq. (4) of lemma
1 is lower and upper bounded by KL(p+||p−
W) up to some
multiplicative constant

KL(p−

W||p+) ≥

|p−

W − p+|2 log e,

1
2

where |p−
we also have

W −p+| is total variation (TV) distance. From [41]

KL(p−

W||p+) ≤

|p−

W − p+|2,

log e
p+
min

where p+
to KL(p+||p−
|p−

W − p+| ≡ |p+ − p−
W|,

min = minx p+(x). Applying the above bounds
W) and using the symmetry of the TV distance

p+
min
2

KL(p+||p−

W) ≤ KL(p−

W||p+) ≤

KL(p+||p−

W).

2
p+
min

W) + KL(p−

W) + KL(p−
W).

Plugging the equation above into the Jeffreys divergence,
we observe that KL(p+||p−
W||p+) is upper and
(cid:3)
lower bounded by by KL(p+||p−
Now we can look at theorem 1. It was shown in [21]
that Eq. (1) reduces KL(p+||p−
W), which bounds the Jef-
freys divergence KL(p+||p−
W||p+) as shown in
corollary 1. Lemma 1 shows the connection between Jef-
freys divergence and the WGAN objective (Eq. (3)) when
f (x) = ln p+(x)
. We therefore can see that the formula-
p−
W (x)
tion of introspective neural networks (Eq. (1)) connects to
(cid:3)
a lower bound of the WGAN [2] objective (Eq. (3)).
C. Texture and CelebA Modeling Architecture. Inspired
by [22], we design a CNN architecture for 64 × 64 image
as in Table 5. We use Swish [39] non-linearity after each
convolutional layer. We add Layer Normalization [3] after
each convolution except the ﬁrst layer, following [14].
Table 5: Network architecture for the texture and face image modeling.

Textures and CelebA
Filter size/stride

Alternative Initialization

Filter size/stride

Layer
Input
Conv3-32
Conv3-64
Avg pool
Conv3-64
Conv3-128
Avg pool
Conv3-128
Conv3-256
Avg pool
Conv3-256
Conv3-512
Avg pool
FC-1

3×3/1
3×3/1
2×2/2
3×3/1
3×3/1
2×2/2
3×3/1
3×3/1
2×2/2
3×3/1
3×3/1
2×2/2

Output size
64×64×3
64×64×32
64×64×64
32×32×64
32×32×64
32×32×128
16×16×128
16×16×128
16×16×256
8×8×256
8×8×256
8×8×256
4×4×512
1×1×1

Layer
Input
Conv5-256
Upsample
Conv5-128
Upsample
Conv5-64
Upsample
Conv5-64
Upsample

5×5/1

5×5/1

5×5/1

5×5/1

Output size
4×4×512
4×4×256
8×8×256
8×8×128
16×16×128
16×16×64
32×32×64
32×32×3
64×64×3

D. Alternative Initializations. We sample an initial
pseudo-negative image by applying an operation deﬁned by
the network above to a tensor of size 4 × 4 × 512 sampled
from U [−1, 1]. The weights of the network are sampled
from G(0, 0.12). We do not apply any nonlinearities in the
network. We add Layer Normalization [3] after each con-
volution except the last layer.

References

[1] D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. A learn-
ing algorithm for boltzmann machines. Cognitive science,
9(1):147–169, 1985. 1

[2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gen-
erative adversarial networks. In ICML, 2017. 1, 2, 3, 4, 6,
8

[3] L. J. Ba, R. Kiros, and G. E. Hinton. Layer normalization.

CoRR, abs/1607.06450, 2016. 7, 8

[4] P. Baldi. Autoencoders, unsupervised learning, and deep ar-
chitectures. In ICML Workshop on Unsupervised and Trans-
fer Learning, pages 37–49, 2012. 1

[5] S. Della Pietra, V. Della Pietra, and J. Lafferty. Inducing fea-
tures of random ﬁelds. IEEE transactions on pattern analysis
and machine intelligence, 19(4):380–393, 1997. 1

[6] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning
to generate chairs with convolutional neural networks.
In
CVPR, 2015. 1
[7] V. Dumoulin,

I. Belghazi, B. Poole, O. Mastropietro,
A. Lamb, M. Arjovsky, and A. Courville. Adversarially
learned inference. In ICLR, 2017. 6

[8] Y. Freund and R. E. Schapire. A decision-theoretic general-
ization of on-line learning and an application to boosting. J.
of Comp. and Sys. Sci., 55(1), 1997. 2

[9] L. Gatys, A. S. Ecker, and M. Bethge. Texture synthesis
using convolutional neural networks. In NIPS, 2015. 3, 5, 6
[10] L. A. Gatys, A. S. Ecker, and M. Bethge. A neural algorithm

of artistic style. arXiv preprint arXiv:1508.06576, 2015. 6

[11] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In NIPS, 2014. 1, 3, 7

[12] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets.
In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger, edi-
tors, Advances in Neural Information Processing Systems 27,
pages 2672–2680. Curran Associates, Inc., 2014. 2

[13] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and

harnessing adversarial examples. In ICLR, 2015. 7

[14] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and
In

Improved training of wasserstein gans.

A. Courville.
NIPS, 2017. 1, 2, 4, 5, 6, 8

[15] T. Han, Y. Lu, S.-C. Zhu, and Y. N. Wu. Alternating back-
propagation for generator network. In AAAI, 2017. 2
[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 7, 8

[17] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learn-
ing algorithm for deep belief nets. Neural computation,
18(7):1527–1554, 2006. 1

[18] G. Huang*, Z. Liu*, L. van der Maaten, and K. Q. Wein-
berger. Densely connected convolutional networks.
In
CVPR, 2017. 1, 8

[19] X. Huang, Y. Li, O. Poursaeed, J. Hopcroft, and S. Belongie.
Stacked generative adversarial networks. In CVPR, 2017. 6
[20] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks. In CVPR,
2017. 5

[21] L. Jin, J. Lazarow, and Z. Tu.

Introspective classiﬁcation

with convolutional nets. In NIPS, 2017. 2, 3, 4, 5, 7, 8
[22] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive
growing of gans for improved quality, stability, and variation.
In ICLR, 2018. 8

[23] D. Kingma and J. Ba. Adam: A method for stochastic opti-

[24] D. P. Kingma and M. Welling. Auto-encoding variational

mization. In ICLR, 2015. 5

bayes. In ICLR, 2014. 1

[25] A. Krizhevsky, V. Nair, and G. Hinton. Cifar-10 (canadian

institute for advanced research). 6

[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

ImageNet
Classiﬁcation with Deep Convolutional Neural Networks. In
NIPS, 2012. 1, 6

[27] T. D. Kulkarni, W. F. Whitney, P. Kohli, and J. Tenenbaum.
Deep convolutional inverse graphics network. In NIPS, 2015.
1

[28] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial ma-

chine learning at scale. In ICLR, 2017. 7

[29] J. Lazarow*, L. Jin*, and Z. Tu.

Introspective neural net-
works for generative modeling. In ICCV, 2017. 2, 3, 4, 5,
6

[30] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. Howard,
W. Hubbard, and L. Jackel. Backpropagation applied to
handwritten zip code recognition. In Neural Computation,
1989. 1

[31] C.-Y. Lee*, S. Xie*, P. Gallagher, Z. Zhang, and Z. Tu.

Deeply-supervised nets. In AISTATS, 2015. 1

[32] S. Liu, O. Bousquet, and K. Chaudhuri. Approximation and
convergence properties of generative adversarial learning. In
NIPS, 2017. 3

[33] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face

attributes in the wild. In ICCV, 2015. 6

[34] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In NIPS workshop on deep learning and unsu-
pervised feature learning, volume 2011, page 5, 2011. 6
[35] J. Ngiam, Z. Chen, P. W. Koh, and A. Y. Ng. Learning deep

energy models. In ICML, 2011. 2

[36] B. A. Olshausen and D. J. Field. Sparse coding with an over-
complete basis set: A strategy employed by v1? Vision re-
search, 37(23):3311–3325, 1997. 1

[37] M. S. Pinsker. Information and information stability of ran-

dom variables and processes. 1960. 8

[38] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. In ICLR, 2016. 1, 6, 7

[39] P. Ramachandran, B. Zoph, and Q. V. Le. Searching for ac-

tivation functions. CoRR, abs/1710.05941, 2017. 8

[40] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Rad-
ford, and X. Chen. Improved techniques for training gans. In
NIPS, 2016. 6

[41] I. Sason and S. Verd´u. f -divergence inequalities.

IEEE
Transactions on Information Theory, 62(11):5973–6006,
2016. 8

[42] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
1

[43] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 1

[44] Z. Tu. Learning generative models via discriminative ap-

proaches. In CVPR, 2007. 1, 2, 3, 5, 7

[45] D. Ulyanov, V. Lebedev, A. Vedaldi, and V. Lempitsky. Tex-
ture networks: Feed-forward synthesis of textures and styl-
ized images. In ICML, 2016. 3, 6

[46] D. Warde-Farley and Y. Bengio. Improving generative adver-
sarial networks with denoising feature matching. In ICLR,
2017. 6

[47] M. Welling and Y. W. Teh. Bayesian learning via stochastic

gradient langevin dynamics. In ICML, 2011. 3, 4

[48] M. Welling, R. S. Zemel, and G. E. Hinton. Self supervised

boosting. In NIPS, 2002. 1

[49] Y. N. Wu, Z. Si, H. Gong, and S.-C. Zhu. Learning active ba-
sis model for object detection and recognition. International
journal of computer vision, 90(2):198–235, 2010. 1

[50] J. Xie, Y. Lu, R. Gao, S.-C. Zhu, and Y. N. Wu. Cooperative
learning of energy-based model and latent variable model via
mcmc teaching. In AAAI, 2018. 2

[51] J. Xie, Y. Lu, S.-C. Zhu, and Y. N. Wu. A theory of genera-

tive convnet. In ICML, 2016. 2

[52] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated
residual transformations for deep neural networks. In CVPR,
2017. 1

[53] S.-C. Zhu and D. Mumford. A stochastic grammar of im-
ages. Foundations and Trends R(cid:13) in Computer Graphics and
Vision, 2(4):259–362, 2007. 1

[54] S. C. Zhu, Y. N. Wu, and D. Mumford. Minimax entropy
principle and its application to texture modeling. Neural
Computation, 9(8):1627–1660, 1997. 1

