7
1
0
2
 
g
u
A
 
2
1
 
 
]

G
L
.
s
c
[
 
 
2
v
5
1
8
2
0
.
1
0
7
1
:
v
i
X
r
a

Stochastic Generative Hashing

∗Bo Dai1, ∗Ruiqi Guo2, Sanjiv Kumar2, Niao He3, Le Song1

1 Georgia Institute of Technology
bodai@gatech.edu, lsong@cc.gatech.edu
2 Google Research, NYC
{guorq, sanjivk}@google.com
3 University of Illinois at Urbana-Champaign
niaohe@illinois.edu

August 15, 2017

Abstract

Learning-based binary hashing has become a powerful paradigm for fast search and retrieval in massive
databases. However, due to the requirement of discrete outputs for the hash functions, learning such
functions is known to be very challenging.
In addition, the objective functions adopted by existing
hashing techniques are mostly chosen heuristically. In this paper, we propose a novel generative approach
to learn hash functions through Minimum Description Length principle such that the learned hash codes
maximally compress the dataset and can also be used to regenerate the inputs. We also develop an
eﬃcient learning algorithm based on the stochastic distributional gradient, which avoids the notorious
diﬃculty caused by binary output constraints, to jointly optimize the parameters of the hash function
and the associated generative model. Extensive experiments on a variety of large-scale datasets show
that the proposed method achieves better retrieval results than the existing state-of-the-art methods.

1 Introduction

Search for similar items in web-scale datasets is a fundamental step in a number of applications, especially
i=1 with x ∈ X ⊂ Rd, we
in image and document retrieval. Formally, given a reference dataset X = {xi}N
want to retrieve similar items from X for a given query y according to some similarity measure sim(x, y).
When the negative Euclidean distance is used, i.e., sim(x, y) = −(cid:107)x − y(cid:107)2, this corresponds to L2 Nearest
Neighbor Search (L2NNS) problem; when the inner product is used, i.e., sim(x, y) = x(cid:62)y, it becomes a
Maximum Inner Product Search (MIPS) problem. In this work, we focus on L2NNS for simplicity, however
our method handles MIPS problems as well, as shown in the supplementary material D. Brute-force linear
search is expensive for large datasets. To alleviate the time and storage bottlenecks, two research directions
have been studied extensively: (1) partition the dataset so that only a subset of data points is searched; (2)
represent the data as codes so that similarity computation can be carried out more eﬃciently. The former
often resorts to search-tree or bucket-based lookup; while the latter relies on binary hashing or quantization.
These two groups of techniques are orthogonal and are typically employed together in practice.

In this work, we focus on speeding up search via binary hashing. Hashing for similarity search was
popularized by inﬂuential works such as Locality Sensitive Hashing (Indyk and Motwani, 1998; Gionis et al.,
1999; Charikar , 2002). The crux of binary hashing is to utilize a hash function, f (·) : X → {0, 1}l,
which maps the original samples in X ∈ Rd to l-bit binary vectors h ∈ {0, 1}l while preserving the similarity

∗Authors are equally contributed.

1

measure, e.g., Euclidean distance or inner product. Search with such binary representations can be eﬃciently
conducted using Hamming distance computation, which is supported via POPCNT on modern CPUs and
GPUs. Quantization based techniques (Babenko and Lempitsky, 2014; Jegou et al., 2011; Zhang et al.,
2014b) have been shown to give stronger empirical results but tend to be less eﬃcient than Hamming search
over binary codes (Douze et al., 2015; He et al., 2013).

Data-dependent hash functions are well-known to perform better than randomized ones (Wang et al.,
2014). Learning hash functions or binary codes has been discussed in several papers, including spectral
hashing (Weiss et al., 2009), semi-supervised hashing (Wang et al., 2010), iterative quantization (Gong and
Lazebnik, 2011), and others (Liu et al., 2011; Gong et al., 2013; Yu et al., 2014; Shen et al., 2015; Guo et al.,
2016). The main idea behind these works is to optimize some objective function that captures the preferred
properties of the hash function in a supervised or unsupervised fashion.

Even though these methods have shown promising performance in several applications, they suﬀer from
two main drawbacks: (1) the objective functions are often heuristically constructed without a principled
characterization of goodness of hash codes, and (2) when optimizing, the binary constraints are crudely
handled through some relaxation, leading to inferior results (Liu et al., 2014). In this work, we introduce
Stochastic Generative Hashing (SGH) to address these two key issues. We propose a generative model which
captures both the encoding of binary codes h from input x and the decoding of input x from h. This provides
a principled hash learning framework, where the hash function is learned by Minimum Description Length
(MDL) principle. Therefore, its generated codes can compress the dataset maximally. Such a generative
model also enables us to optimize distributions over discrete hash codes without the necessity to handle
discrete variables. Furthermore, we introduce a novel distributional stochastic gradient descent method
which exploits distributional derivatives and generates higher quality hash codes. Prior work on binary
autoencoders (Carreira-Perpin´an and Raziperchikolaei, 2015) also takes a generative view of hashing but
still uses relaxation of binary constraints when optimizing the parameters, leading to inferior performance
as shown in the experiment section. We also show that binary autoencoders can be seen as a special case of
our formulation. In this work, we mainly focus on the unsupervised setting1.

2 Stochastic Generative Hashing (SGH)

We start by ﬁrst formalizing the two key issues that motivate the development of the proposed algorithm.
Generative view. Given an input x ∈ Rd, most hashing works in the literature emphasize modeling
the forward process of generating binary codes from input, i.e., h(x) ∈ {0, 1}l, to ensure that the generated
hash codes preserve the local neighborhood structure in the original space. Few works focus on modeling the
reverse process of generating input from binary codes, so that the reconstructed input has small reconstruction
error. In fact, the generative view provides a natural learning objective for hashing. Following this intuition,
we model the process of generating x from h, p(x|h) and derive the corresponding hash function q(h|x)
from the generative process. Our approach is not tied to any speciﬁc choice of p(x|h) but can adapt to any
generative model appropriate for the domain. In this work, we show that even using a simple generative
model (Section 2.1) already achieves the state-of-the-art performance.

Binary constraints. The other issue arises from dealing with binary constraints. One popular approach
is to relax the constraints from {0, 1} (Weiss et al., 2009), but this often leads to a large optimality gap
between the relaxed and non-relaxed objectives. Another approach is to enforce the model parameterization
to have a particular structure so that when applying alternating optimization, the algorithm can alternate
between updating the parameters and binarization eﬃciently. For example, (Gong and Lazebnik, 2011; Gong
et al., 2012) imposed an orthogonality constraint on the projection matrix, while (Yu et al., 2014) proposed
to use circulant constraints, and (Zhang et al., 2014a) introduced Kronecker Product structure. Although
such constraints alleviate the diﬃculty with optimization, they substantially reduce the model ﬂexibility.
In contrast, we avoid such constraints and propose to optimize the distributions over the binary variables
to avoid directly working with binary variables. This is attained by resorting to the stochastic neuron

1The proposed algorithm can be extended to supervised/semi-supervised setting easily as described in the supplementary

material E.

2

reparametrization (Section 2.4), which allows us to back-propagate through the layers of weights using the
stochsastic gradient estimator.

Unlike (Carreira-Perpin´an and Raziperchikolaei, 2015) which relies on solving expensive integer programs,
our model is end-to-end trainable using distributional stochastic gradient descent (Section 3). Our algorithm
requires no iterative steps unlike iterative quantization (ITQ) (Gong and Lazebnik, 2011). The training
procedure is much more eﬃcient with guaranteed convergence compared to alternating optimization for
ITQ.

In the following sections, we ﬁrst introduce the generative hashing model p(x|h) in Section 2.1. Then,
we describe the corresponding process of generating hash codes given input x, q(h|x) in Section 2.2. Finally,
we describe the training procedure based on the Minimum Description Length (MDL) principle and the
stochastic neuron reparametrization in Sections 2.3 and 2.4. We also introduce the distributional stochastic
gradient descent algorithm in Section 3.

2.1 Generative Model p(x|h)

Unlike most works which start with the hash function h(x), we ﬁrst introduce a generative model that
deﬁnes the likelihood of generating input x given its binary code h, i.e., p(x|h).
It is also referred as a
decoding function. The corresponding hash codes are derived from an encoding function q(h|x), described
in Section 2.2.

We use a simple Gaussian distribution to model the generation of x given h:

p(x, h) = p(x|h)p(h), where p(x|h) = N (U h,ρ2I)

i=1, ui ∈ Rd is a codebook with l codewords. The prior p(h) ∼ B(θ) = (cid:81)l

(1)
and U = {ui}l
i (1 − θi)1−hi is
modeled as the multivariate Bernoulli distribution on the hash codes, where θ = [θi]l
i=1 ∈ [0, 1]l. Intuitively,
this is an additive model which reconstructs x by summing the selected columns of U given h, with a Bernoulli
prior on the distribution of hash codes. The joint distribution can be written as:

i=1 θhi

p(x, h) ∝ exp



1
2ρ2





(cid:0)x(cid:62)x + h(cid:62)U (cid:62)U h − 2x(cid:62)U h(cid:1)
(cid:123)(cid:122)
(cid:125)
(cid:124)
(cid:107)x−U (cid:62)h(cid:107)2
2

−(log

(cid:62)
)

h

θ
1 − θ







(2)

This generative model can be seen as a restricted form of general Markov Random Fields in the sense that
the parameters for modeling correlation between latent variables h and correlation between x and h are
shared. However, it is more ﬂexible compared to Gaussian Restricted Boltzmann machines (Krizhevsky,
2009; Marc’Aurelio and Geoﬀrey, 2010) due to an extra quadratic term for modeling correlation between
latent variables. We ﬁrst show that this generative model preserves local neighborhood structure of the x
when the Frobenius norm of U is bounded.

Proposition 1 If (cid:107)U (cid:107)F is bounded, then the Gaussian reconstruction error, (cid:107)x − U hx(cid:107)2 is a surrogate for
Euclidean neighborhood preservation.

Proof Given two points x, y ∈ Rd, their Euclidean distance is bounded by
(cid:107)x − y(cid:107)2

= (cid:107)(x − U (cid:62)hx) − (y − U (cid:62)hy) + (U (cid:62)hx − U (cid:62)hy)(cid:107)2
(cid:54) (cid:107)x − U (cid:62)hx(cid:107)2 + (cid:107)y − U (cid:62)hy(cid:107)2 + (cid:107)U (cid:62)(hx − hy)(cid:107)2
(cid:54) (cid:107)x − U (cid:62)hx(cid:107)2 + (cid:107)y − U (cid:62)hy(cid:107)2 + (cid:107)U (cid:107)F (cid:107)hx − hy(cid:107)2
where hx and hy denote the binary latent variables corresponding to x and y, respectively. Therefore, we
have

(cid:107)x − y(cid:107)2 − (cid:107)U (cid:107)F (cid:107)hx − hy(cid:107)2 (cid:54) (cid:107)x − U (cid:62)hx(cid:107)2 + (cid:107)y − U (cid:62)hy(cid:107)2
which means minimizing the Gaussian reconstruction error, i.e., − log p(x|h), will lead to Euclidean neigh-
borhood preservation.

3

A similar argument can be made with respect to MIPS neighborhood preservation as shown in the sup-
plementary material D. Note that the choice of p(x|h) is not unique, and any generative model that leads
to neighborhood preservation can be used here. In fact, one can even use more sophisticated models with
multiple layers and nonlinear functions.
In our experiments, we ﬁnd complex generative models tend to
perform similarly to the Gaussian model on datasets such as SIFT-1M and GIST-1M. Therefore, we use the
Gaussian model for simplicity.

2.2 Encoding Model q(h|x)

Even with the simple Gaussian model (1), computing the posterior p(h|x) = p(x,h)
is not tractable, and
p(x)
ﬁnding the MAP solution of the posterior involves solving an expensive integer programming subproblem.
Inspired by the recent work on variational auto-encoder (Kingma and Welling, 2013; Mnih and Gregor, 2014;
Gregor et al., 2014), we propose to bypass these diﬃculties by parameterizing the encoding function as

q(h|x) =

q(hk = 1|x)hk q(hk = 0|x)1−hk ,

(3)

l
(cid:89)

k=1

k=1 ∼ B(σ(W (cid:62)x)) with
to approximate the exact posterior p(h|x). With the linear parametrization, h = [hk]l
W = [wk]l
k=1. At the training step, a hash code is obtained by sampling from B(σ(W (cid:62)x)). At the inference
step, it is still possible to sample h. More directly, the MAP solution of the encoding function (3) is readily
given by

h(x) = argmax

q(h|x) =

h

sign(W (cid:62)x) + 1
2

This involves only a linear projection followed by a sign operation, which is common in the hashing literature.
Computing h(x) in our model thus has the same amount of computation as ITQ (Gong and Lazebnik, 2011),
except without the orthogonality constraints.

2.3 Training Objective

Since our goal is to reconstruct x using the least information in binary codes, we train the variational
auto-encoder using the Minimal Description Length (MDL) principle, which ﬁnds the best parameters that
maximally compress the training data. The MDL principle seeks to minimize the expected amount of
information to communicate x:

L(x) =

q(h|x)(L(h) + L(x|h))

(cid:88)

h

where L(h) = − log p(h) + log q(h|x) is the description length of the hashed representation h and L(x|h) =
− log p(x|h) is the description length of x having already communicated h in (Hinton and Van Camp, 1993;
Hinton and Zemel, 1994; Mnih and Gregor, 2014). By summing over all training examples x, we obtain the
following training objective, which we wish to minimize with respect to the parameters of p(x|h) and q(h|x):

min
Θ={W,U,β,ρ}

(cid:88)

x

(cid:88)

(cid:88)

x

h

H(Θ) :=

L(x; Θ) = −

q(h|x)(log p(x, h) − log q(h|x)),

(4)

where U, ρ and β := log θ
1−θ are parameters of the generative model p(x, h) as deﬁned in (1), and W comes
from the encoding function q(h|x) deﬁned in (3). This objective is sometimes called Helmholtz (variational)
free energy (Williams, 1980; Zellner, 1988; Dai et al., 2016). When the true posterior p(h|x) falls into the
family of (3), q(h|x) becomes the true posterior p(h|x), which leads to the shortest description length to
represent x.

We emphasize that this objective no longer includes binary variables h as parameters and therefore avoids
optimizing with discrete variables directly. This paves the way for continuous optimization methods such as
stochastic gradient descent (SGD) to be applied in training. As far as we are aware, this is the ﬁrst time
such a procedure has been used in the problem of unsupervised learning to hash. Our methodology serves
as a viable alternative to the relaxation-based approaches commonly used in the past.

4

2.4 Reparametrization via Stochastic Neuron

Using the training objective of (4), we can directly compute the gradients w.r.t. parameters of p(x|h).
However, we cannot compute the stochastic gradients w.r.t. W because it depends on the stochastic bi-
nary variables h.
In order to back-propagate through stochastic nodes of h, two possible solutions have
been proposed. First, the reparametrization trick (Kingma and Welling, 2013) which works by introducing
auxiliary noise variables in the model. However, it is diﬃcult to apply when the stochastic variables are
discrete, as is the case for h in our model. On the other hand, the gradient estimators based on REINFORCE
trick (Bengio et al., 2013) suﬀer from high variance. Although some variance reduction remedies have been
proposed (Mnih and Gregor, 2014; Gu et al., 2015), they are either biased or require complicated extra
computation in practice.

In next section, we ﬁrst provide an unbiased estimator of the gradient w.r.t. W derived based on
distributional derivative, and then, we derive a simple and eﬃcient approximator. Before we derive the
estimator, we ﬁrst introduce the stochastic neuron for reparametrizing Bernoulli distribution. A stochastic
neuron reparameterizes each Bernoulli variable hk(z) with z ∈ (0, 1).
Introducing random variables ξ ∼
U(0, 1), the stochastic neuron is deﬁned as

˜h(z, ξ) :=

(cid:40)

1
0

if z (cid:62) ξ
if z < ξ

.

(5)

Because P(˜h(z, ξ) = 1) = z, we have ˜h(z, ξ) ∼ B(z). We use the stochastic neuron (5) to reparameterize our
k=1. Note that ˜h now behaves
binary variables h by replacing [hk]l
deterministically given ξ. This gives us the reparameterized version of our original training objective (4):
˜H(Θ; x) :=

k x)) with [˜hk(σ(w(cid:62)

k=1(x) ∼ B(σ(w(cid:62)

(cid:105)
(cid:104)
(cid:96)(˜h, x)

k x), ξk)]l

˜H(Θ) =

(cid:88)

(cid:88)

(6)

,

Eξ

where (cid:96)(˜h, x) := − log p(x, ˜h(σ(W (cid:62)x), ξ)) + log q(˜h(σ(W (cid:62)x), ξ)|x) with ξ ∼ U(0, 1). With such a reformu-
lation, the new objective can now be optimized by exploiting the distributional stochastic gradient descent,
which will be explained in the next section.

x

x

3 Distributional Stochastic Gradient Descent

i=1, the stochastic gradient (cid:98)∇U,β,ρ ˜H(Θ; x)
For the objective in (6), given a point x randomly sampled from {xi}N
can be easily computed in the standard way. However, with the reparameterization, the function ˜H(Θ; x) is
no longer diﬀerentiable with respect to W due to the discontinuity of the stochastic neuron ˜h(z, ξ). Namely,
the SGD algorithm is not readily applicable. To overcome this diﬃculty, we will adopt the notion of distri-
butional derivative for generalized functions or distributions (Grubb, 2008).

3.1 Distributional derivative of Stochastic Neuron

Let Ω ⊂ Rd be an open set. Denote C∞
0 (Ω) as the space of the functions that are inﬁnitely diﬀerentiable
with compact support in Ω. Let D(cid:48)(Ω) be the space of continuous linear functionals on C∞
0 (Ω), which can
be considered as the dual space. The elements in space D(cid:48)(Ω) are often called general distributions. We
emphasize this deﬁnition of distributions is more general than that of traditional probability distributions.

Deﬁnition 2 (Distributional derivative) (Grubb, 2008) Let u ∈ D(cid:48)(Ω), then a distribution v is called
the distributional derivative of u, denoted as v = Du, if it satisﬁes
(cid:90)

(cid:90)

vφdx = −

u∂φdx,

∀φ ∈ C∞

0 (Ω).

Ω

Ω
It is straightforward to verify that for given ξ, the function ˜h(z, ξ) ∈ D(cid:48)(Ω) and moreover, Dz
˜h(z, ξ) = δξ(z),
which is exactly the Dirac-δ function. Based on the deﬁnition of distributional derivatives and chain rules, we
are able to compute the distributional derivative of the function ˜H(Θ; x), which is provided in the following
lemma.

5

i=1

Algorithm 1 Distributional-SGD
Input: {xi}N
1: Initialize Θ0 = {W, U, β, ρ} randomly.
2: for i = 1, . . . , t do
3:
4:

5:
6: Update parameters as

7: end for

Sample xi uniformly from {xi}N
Sample ξi ∼ U([0, 1]l).
Compute stochastic gradients (cid:98)∇Θ ˜H(Θi; xi) or (cid:98)˜∇Θ ˜H(Θi; xi), deﬁned in (8) and (10), respectively.

i=1.

Θi+1 = Θi − γi (cid:98)∇Θ ˜H(Θi; xi), or

Θi+1 = Θi − γi (cid:98)˜∇Θ ˜H(Θi; xi), respectively.

Lemma 3 For a given sample x, the distributional derivative of function ˜H(Θ; x) w.r.t. W is given by

DW ˜H(Θ; x) = Eξ

∆˜h(cid:96)(˜h(σ(W (cid:62)x), ξ))σ(W (cid:62)x) • (1 − σ(W (cid:62)x))x(cid:62)(cid:105)
where • denotes point-wise product and ∆˜h(cid:96)(˜h) denotes the ﬁnite diﬀerence deﬁned as
(cid:96)(˜h0

k]l = ˜hl if k (cid:54)= l, otherwise [˜hi

k]l = i, i ∈ {0, 1}.

k), where [˜hi

(cid:104)

,

(7)

(cid:104)
∆˜h(cid:96)(˜h)

(cid:105)

k

= (cid:96)(˜h1

k) −

We can therefore combine distributional derivative estimators (7) with stochastic gradient descent algorithm
(see e.g., (Nemirovski et al., 2009) and its variants (Kingma and Ba, 2014; Bottou et al., 2016)), which we
designate as Distributional SGD. The detail is presented in Algorithm 1, where we denote
(cid:104)

(cid:105)

(cid:98)∇Θ ˜H(Θi; xi) =

(cid:98)DW ˜H(Θi; xi), (cid:98)∇U,β,ρ ˜H(Θi; xi)

(8)

as the unbiased stochastic estimator of the gradient at Θi constructed by sample xi, ξi. Compared to the
existing algorithms for learning to hash which require substantial eﬀort on optimizing over binary variables,
the proposed distributional SGD is much simpler and also amenable to online settings (Huang et al., 2013;
Leng et al., 2015).

In general, the distributional derivative estimator (7) requires two forward passes of the model for each
dimension. To further accelerate the computation, we approximate the distributional derivative DW ˜H(Θ; x)
by exploiting the mean value theorem and Taylor expansion by

˜DW ˜H(Θ; x) := Eξ

(cid:104)
∇˜h(cid:96)(˜h(σ(W (cid:62)x), ξ))σ(W (cid:62)x) • (1 − σ(W (cid:62)x))x(cid:62)(cid:105)

,

(9)

which can be computed for each dimension in one pass. Then, we can exploit this estimator
(cid:104)
(cid:98)˜DW ˜H(Θi; xi), (cid:98)∇U,β,ρ ˜H(Θi; xi)
in Algorithm 1.
Interestingly, the approximate stochastic gradient estimator of the stochastic neuron
we established through the distributional derivative coincides with the heuristic “pseudo-gradient” con-
structed (Raiko et al., 2014). Please refer to the supplementary material A for details for the derivation of
the approximate gradient estimator (9).

(cid:98)˜∇Θ ˜H(Θi; xi) =

(10)

(cid:105)

3.2 Convergence of Distributional SGD

One caveat here is that due to the potential discrepancy of the distributional derivative and the traditional
gradient, whether the distributional derivative is still a descent direction and whether the SGD algorithm
integrated with distributional derivative converges or not remains unclear in general. However, for our
learning to hash problem, one can easily show that the distributional derivative in (7) is indeed the true
gradient.

Proposition 4 The distributional derivative DW ˜H(Θ; x) is equivalent to the traditional gradient ∇W H(Θ; x).

6

Proof First of all, by deﬁnition, we have ˜H(Θ; x) = H(Θ; x). One can easily verify that under mild
condition, both DW ˜H(Θ; x) and ∇W H(Θ; x) are continuous and 1-norm bounded. Hence, it suﬃces to show
that for any distribution u ∈ C1(Ω) and Du, ∇u ∈ L1(Ω), Du = ∇u. For any φ ∈ C∞
0 (Ω), by deﬁnition
of the distributional derivative, we have (cid:82)
Ω u∂φdx. On the other hand, we always have
Ω ∇uφdx = − (cid:82) u∂φdx. Hence, (cid:82)
(cid:82)
0 (Ω). By the Du Bois-Reymond’s lemma
(see Lemma 3.2 in (Grubb, 2008)), we have Du = ∇u.

Ω Duφdx = − (cid:82)
Ω(Du − ∇u)φdx = 0 for all φ ∈ C∞

Consequently, the distributional SGD algorithm enjoys the same convergence property as the traditional
SGD algorithm. Applying theorem 2.1 in (Ghadimi and Lan, 2013), we arrive at

Theorem 5 Under the assumption that H is L-Lipschitz smooth and the variance of the stochastic dis-
tributional gradient (8) is bounded by σ2 in the distributional SGD, for the solution ΘR sampled from the
trajectory {Θi}t

where γi ∼ O (cid:0)1/

t(cid:1), we have

√

i=1 with probability P (R = i) = 2γi−Lγ2
2(cid:21)

(cid:80)t

i=1 2γi−Lγ2
i

i

E

(cid:20)(cid:13)
(cid:13)∇Θ ˜H(ΘR)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

∼ O

(cid:19)

.

(cid:18) 1
√
t

In fact, with the approximate gradient estimators (9), the proposed algorithm is also converging in terms of
ﬁrst-order conditions, i.e.,

Theorem 6 Under the assumption that the variance of the approximate stochastic distributional gradi-
ent (10) is bounded by σ2, for the solution ΘR sampled from the trajectory {Θi}t
i=1 with probability P (R =
i) = γi

t(cid:1), we have

where γi ∼ O (cid:0)1/

√

(cid:80)t

i=1 γi

(cid:104)

E

(ΘR − Θ∗)(cid:62) ˜∇Θ ˜H(ΘR)

∼ O

(cid:105)

(cid:19)

,

(cid:18) 1
√
t

where Θ∗ denotes the optimal solution.

For the detailed proof of theorem 5 and 6, please refer to the supplementary material B.

4 Connections

The proposed stochastic generative hashing is a general framework. In this section, we reveal the connection
to several existing algorithms.
Iterative Quantization (ITQ). If we ﬁx some ρ > 0, and U = W R where W is formed by eigenvectors of
the covariance matrix and R is an orthogonal matrix, we have U (cid:62)U = I. If we assume the joint distribution
as

p(x, h) ∝ N (W Rh, ρ2I)B(θ),
and parametrize q(h|xi) = δbi(h), then from the objective in (4) and ignoring the irrelevant terms, we obtain
the optimization

min
R,b

N
(cid:88)

i=1

(cid:107)xi − W Rbi(cid:107)2,

which is exactly the objective of iterative quantization (Gong and Lazebnik, 2011).
Binary Autoencoder (BA). If we use the deterministic linear encoding function, i.e., q(h|x) = δ 1+sign(W (cid:62) x)

(h),

and preﬁx some ρ > 0, and ignore the irrelevant terms, the optimization (4) reduces to

min
U,W

N
(cid:88)

i=1

(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)xi − U h
(cid:13)

, s.t. h =

1 + sign(W (cid:62)x)
2

,

which is the objective of a binary autoencoder (Carreira-Perpin´an and Raziperchikolaei, 2015).

In BA, the encoding procedure is deterministic, therefore, the entropy term Eq(h|x) [log q(h|x)] = 0. In
fact, the entropy term, if non-zero, performs like a regularization and helps to avoid wasting bits. Moreover,

(11)

2

(12)

7

(a) Reconstruction Error

(b) Training Time

Figure 1: (a) Convergence of reconstruction error with number of samples seen by SGD, and (b) training
time comparison of BA and SGH on SIFT-1M over the course of training with varying number of bits.

without the stochasticity, the optimization (12) becomes extremely diﬃcult due to the binary constraints.
While for the proposed algorithm, we exploit the stochasticity to bypass such diﬃculty in optimization. The
stochasticity enables us to accelerate the optimization as shown in section 5.2.

5 Experiments

In this section, we evaluate the performance of the proposed distributional SGD on commonly used datasets
in hashing. Due to the eﬃciency consideration, we conduct the experiments mainly with the approximate
gradient estimator (9). We evaluate the model and algorithm from several aspects to demonstrate the power
of the proposed SGH: (1) Reconstruction loss. To demonstrate the ﬂexibility of generative modeling, we
compare the L2 reconstruction error to that of ITQ (Gong and Lazebnik, 2011), showing the beneﬁts of mod-
eling without the orthogonality constraints. (2) Convergence of the distributional SGD. We evaluate
the reconstruction error showing that the proposed algorithm indeed converges, verifying the theorems. (3)
Training time. The existing generative works require a signiﬁcant amount of time for training the model.
In contrast, our SGD algorithm is very fast to train both in terms of number of examples needed and the
wall time. (4) Nearest neighbor retrieval. We show Recall K@N plots on standard large scale nearest
neighbor search benchmark datasets of MNIST, SIFT-1M, GIST-1M and SIFT-1B, for all of which we achieve
state-of-the-art among binary hashing methods. (5) Reconstruction visualization. Due to the generative
nature of our model, we can regenerate the original input with very few bits. On MNIST and CIFAR10, we
qualitatively illustrate the templates that correspond to each bit and the resulting reconstruction.

We used several benchmarks datasets, i.e., (1) MNIST which contains 60,000 digit images of size 28 × 28
pixels, (2) CIFAR-10 which contains 60,000 32 × 32 pixel color images in 10 classes, (3) SIFT-1M and (4)
SIFT-1B which contain 106 and 109 samples, each of which is a 128 dimensional vector, and (5) GIST-1M
which contains 106 samples, each of which is a 960 dimensional vector.

5.1 Reconstruction loss

Because our method has a generative model p(x|h), we can easily compute the regenerated input ˜x =
argmax p(x|h), and then compute the L2 loss of the regenerated input and the original x, i.e., (cid:107)x − ˜x(cid:107)2
2.
ITQ also trains by minimizing the binary quantization loss, as described in Equation (2) in (Gong and
Lazebnik, 2011), which is essentially L2 reconstruction loss when the magnitude of the feature vectors is
compatible with the radius of the binary cube. We plotted the L2 reconstruction loss of our method and

8

Table 1: Training time on SIFT-1M in second.

Method
SGH
ITQ

8 bits
28.32
92.82

16 bits
29.38
121.73

32 bits
37.28
173.65

64 bits
55.03
259.13

ITQ on SIFT-1M in Figure 1(a) and on MNIST and GIST-1M in Figure 4, where the x-axis indicates the number
of examples seen by the training algorithm and the y-axis shows the average L2 reconstruction loss. The
training time comparison is listed in Table 1. Our method (SGH) arrives at a better reconstruction loss with
comparable or even less time compared to ITQ. The lower reconstruction loss demonstrates our claim that
the ﬂexibility of the proposed model aﬀorded by removing the orthogonality constraints indeed brings extra
modeling ability. Note that ITQ is generally regarded as a technique with fast training among the existing
binary hashing algorithms, and most other algorithms (He et al., 2013; Heo et al., 2012; Carreira-Perpin´an
and Raziperchikolaei, 2015) take much more time to train.

5.2 Empirical study of Distributional SGD

We demonstrate the convergence of the distributional derivative with Adam (Kingma and Ba, 2014) numer-
ically on SIFT-1M, GIST-1M and MINST from 8 bits to 64 bits. The convergence curves on SIFT-1M are shown
in Figure 1 (a). The results on GIST-1M and MNIST are similar and shown in Figure 4 in supplementary
material C. Obviously, the proposed algorithm, even with a biased gradient estimator, converges quickly, no
matter how many bits are used. It is reasonable that with more bits, the model ﬁts the data better and the
reconstruction error can be reduced further.

In line with the expectation, our distributional SGD trains much faster since it bypasses integer pro-
gramming. We benchmark the actual time taken to train our method to convergence and compare that
to binary autoencoder hashing (BA) (Carreira-Perpin´an and Raziperchikolaei, 2015) on SIFT-1M, GIST-1M
and MINST. We illustrate the performance on SIFT-1M in Figure 1(b) . The results on GIST-1M and MNIST
datasets follow a similar trend as shown in the supplementary material C. Empirically, BA takes signiﬁcantly
more time to train on all bit settings due to the expensive cost for solving integer programming subproblem.
Our experiments were run on AMD 2.4GHz Opteron CPUs×4 and 32G memory. Our implementation of the
stochastic neuron as well as the whole training procedure was done in TensorFlow. We have released our
code on GitHub2. For the competing methods, we directly used the code released by the authors.

5.3 Large scale nearest neighbor retrieval

We compared the stochastic generative hashing on an L2NNS task with several state-of-the-art unsupervised
algorithms, including K-means hashing (KMH) (He et al., 2013), iterative quantization (ITQ) (Gong and
Lazebnik, 2011), spectral hashing (SH) (Weiss et al., 2009), spherical hashing (SpH) (Heo et al., 2012), binary
autoencoder (BA) (Carreira-Perpin´an and Raziperchikolaei, 2015), and scalable graph hashing (GH) (Jiang
and Li, 2015). We demonstrate the performance of our binary codes by doing standard benchmark experi-
ments of Approximate Nearest Neighbor (ANN) search by comparing the retrieval recall. In particular, we
compare with other unsupervised techniques that also generate binary codes. For each query, linear search
in Hamming space is conducted to ﬁnd the approximate neighbors.

Following the experimental setting of (He et al., 2013), we plot the Recall10@N curve for MNIST, SIFT-1M,
GIST-1M, and SIFT-1B datasets under varying number of bits (16, 32 and 64) in Figure 2. On the SIFT-1B
datasets, we only compared with ITQ since the training cost of the other competitors is prohibitive. The
recall is deﬁned as the fraction of retrieved true nearest neighbors to the total number of true nearest
neighbors. The Recall10@N is the recall of 10 ground truth neighbors in the N retrieved samples. Note
that Recall10@N is generally a more challenging criteria than Recall@N (which is essentially Recall1@N),
and better characterizes the retrieval results. For completeness, results of various Recall K@N curves can be
found in the supplementary material which show similar trend as the Recall10@N curves.

2https://github.com/doubling/Stochastic Generative Hashing

9

Figure 2: L2NNS comparison on MNIST, SIFT-1M, and GIST-1M and SIFT-1B with the length of binary codes
varying from 16 to 64 bits. We evaluate the performance with Recall 10@M (fraction of top 10 ground truth
neighbors in retrieved M), where M increases up to 1000.

Figure 2 shows that the proposed SGH consistently performs the best across all bit settings and all
datasets. The searching time is the same for the same number of bits, because all algorithms use the same
optimized implementation of POPCNT based Hamming distance computation and priority queue. We point
out that many of the baselines need signiﬁcant parameter tuning for each experiment to achieve a reasonable
recall, except for ITQ and our method, where we ﬁx hyperparameters for all our experiments and used a batch
size of 500 and learning rate of 0.01 with stepsize decay. Our method is less sensitive to hyperparameters.

5.4 Visualization of reconstruction

One important aspect of utilizing a generative model for a hash function is that one can generate the input
from its hash code. When the inputs are images, this corresponds to image generation, which allows us to
visually inspect what the hash bits encode, as well as the diﬀerences in the original and generated images.
In our experiments on MNIST and CIFAR-10, we ﬁrst visualize the “template” which corresponds to each
hash bit, i.e., each column of the decoding dictionary U . This gives an interesting insight into what each
hash bit represents. Unlike PCA components, where the top few look like averaged images and the rest
are high frequency noise, each of our image template encodes distinct information and looks much like ﬁlter
banks of convolution neural networks. Empirically, each template also looks quite diﬀerent and encodes
somewhat meaningful information, indicating that no bits are wasted or duplicated. Note that we obtain
this representation as a by-product, without explicitly setting up the model with supervised information,
similar to the case in convolution neural nets.

We also compare the reconstruction ability of SGH with the that of ITQ and real valued PCA in Figure 3.

10

(a) Templates and re-generated images on MNIST

(b) Templates and re-generated images on CIFAR-10

Figure 3: Illustration of MNIST and CIFAR-10 templates (left) and regenerated images (right) from diﬀerent
methods with 64 hidden binary variables. In MNIST, the four rows and their number of bits used to encode
them are, from the top: (1) original image, 28×28×8 = 6272 bits; (2) PCA with 64 components 64×32 = 2048
bits; (3) SGH, 64 bits; (4) ITQ, 64 bits. In CIFAR : (1) original image, 30 × 30 × 24 = 21600 bits; (2) PCA
with 64 components 64 × 32 = 2048 bits; (3) SGH, 64 bits; (4) ITQ, 64 bits. The SGH reconstruction tends
to be much better than that of ITQ, and is on par with PCA which uses 32 times more bits!

For ITQ and SGH, we use a 64-bit hash code. For PCA, we kept 64 components, which amounts to
64 × 32 = 2048 bits. Visually comparing with SGH, ITQ reconstructed images look much less recognizable
on MNIST and much more blurry on CIFAR-10. Compared to PCA, SGH achieves similar visual quality
while using a signiﬁcantly lower (32× less) number of bits!

6 Conclusion

In this paper, we have proposed a novel generative approach to learn binary hash functions. We have justiﬁed
from a theoretical angle that the proposed algorithm is able to provide a good hash function that preserves
Euclidean neighborhoods, while achieving fast learning and retrieval. Extensive experimental results justify
the ﬂexibility of our model, especially in reconstructing the input from the hash codes. Comparisons with
approximate nearest neighbor search over several benchmarks demonstrate the advantage of the proposed
algorithm empirically. We emphasize that the proposed generative hashing is a general framework which
can be extended to semi-supervised settings and other learning to hash scenarios as detailed in the supple-
mentary material. Moreover, the proposed distributional SGD with the unbiased gradient estimator and
its approximator can be applied to general integer programming problems, which may be of independent
interest.

LS is supported in part by NSF IIS-1218749, NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983,
NSF IIS-1639792 EAGER, ONR N00014-15-1-2340, NVIDIA, Intel and Amazon AWS.

Acknowledgements

References

Babenko, Artem and Lempitsky, Victor. Additive quantization for extreme vector compression. In roceedings

of the IEEE Conference on Computer Vision and Pattern Recognition, 2014.

11

Yoshua Bengio, Nicholas L´eonard, and Aaron Courville. Estimating or propagating gradients through

stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.

L´eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning.

arXiv preprint arXiv:1606.04838, 2016.

Miguel A Carreira-Perpin´an and Ramin Raziperchikolaei. Hashing with binary autoencoders. In Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition, pages 557–566, 2015.

Charikar, Moses S. Similarity estimation techniques from rounding algorithms. Proceedings of the thiry-fourth

annual ACM symposium on Theory of computing, pages 380–388, 2002. ‘’

Bo Dai, Niao He, Hanjun Dai, and Le Song. Provable bayesian inference via particle mirror descent. In
Proceedings of the 19th International Conference on Artiﬁcial Intelligence and Statistics, pages 985–994,
2016.

Matthijs Douze, Herv´e J´egou, and Florent Perronnin. Polysemous codes.

In European Conference on

Computer Vision, 2016.

Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic

programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013.

Aristides Gionis, Piotr Indyk, Rajeev Motwani, et al. Similarity search in high dimensions via hashing. In

VLDB, volume 99, pages 518–529, 1999.

Yunchao Gong and Svetlana Lazebnik. Iterative quantization: A procrustean approach to learning binary
codes. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 817–824.
IEEE, 2011.

Yunchao Gong, Sanjiv Kumar, Vishal Verma, and Svetlana Lazebnik. Angular quantization-based binary

codes for fast similarity search. In Advances in neural information processing systems, 2012.

Yunchao Gong, Sanjiv Kumar, Henry A Rowley, and Svetlana Lazebnik. Learning binary codes for high-
dimensional data using bilinear projections. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 484–491, 2013.

Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep autoregressive
networks. In Proceedings of The 31st International Conference on Machine Learning, pages 1242–1250,
2014.

Gerd Grubb. Distributions and operators, volume 252. Springer Science & Business Media, 2008.

Shixiang Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih. Muprop: Unbiased backpropagation for

stochastic neural networks. arXiv preprint arXiv:1511.05176, 2015.

Ruiqi Guo, Sanjiv Kumar, Krzysztof Choromanski, and David Simcha. Quantization based fast inner product

search. 19th International Conference on Artiﬁcial Intelligence and Statistics, 2016.

Kaiming He, Fang Wen, and Jian Sun. K-means hashing: An aﬃnity-preserving quantization method for
learning binary compact codes. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 2938–2945, 2013.

Jae-Pil Heo, Youngwoon Lee, Junfeng He, Shih-Fu Chang, and Sung-Eui Yoon. Spherical hashing.

In
Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 2957–2964. IEEE,
2012.

12

Geoﬀrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description
length of the weights. In Proceedings of the sixth annual conference on Computational learning theory,
pages 5–13. ACM, 1993.

Geoﬀrey E Hinton and Richard S Zemel. Autoencoders, minimum description length and helmholtz free

energy. In Advances in Neural Information Processing Systems, pages 3–10, 1994.

Long-Kai Huang, Qiang Yang, and Wei-Shi Zheng. Online hashing. In Proceedings of the Twenty-Third

international joint conference on Artiﬁcial Intelligence, pages 1422–1428. AAAI Press, 2013.

Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of dimen-
sionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604–613.
ACM, 1998.

Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE

transactions on pattern analysis and machine intelligence, 33(1):117–128, 2011.

Qing-Yuan Jiang and Wu-Jun Li. Scalable Graph Hashing with Feature Transformation. In Twenty-Fourth

International Joint Conference on Artiﬁcial Intelligence, 2015.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization.

arXiv preprint

Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,

arXiv:1412.6980, 2014.

2013.

Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.

Cong Leng, Jiaxiang Wu, Jian Cheng, Xiao Bai, and Hanqing Lu. Online sketching hashing. In 2015 IEEE

Conference on Computer Vision and Pattern Recognition (CVPR), pages 2503–2511. IEEE, 2015.

Wei Liu, Jun Wang, Sanjiv Kumar, and Shih-Fu Chang. Hashing with graphs. In Proceedings of the 28th

international conference on machine learning (ICML-11), pages 1–8, 2011.

Wei Liu , Cun Mu, Sanjiv Kumar and Shih-Fu Chang. Discrete graph hashing.

In Advances in Neural

Information Processing Systems (NIPS), 2014.

Ranzato Marc’Aurelio and E Hinton Geoﬀrey. Modeling pixel means and covariances using factorized third-
order boltzmann machines. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference
on, pages 2551–2558. IEEE, 2010.

Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. arXiv preprint

arXiv:1402.0030, 2014.

Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approxima-

tion approach to stochastic programming. SIAM Journal on optimization, 19(4):1574–1609, 2009.

Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh. Techniques for learning binary

stochastic feedforward neural networks. arXiv preprint arXiv:1406.2989, 2014.

Fumin Shen, Wei Liu, Shaoting Zhang, Yang Yang, and Heng Tao Shen. Learning binary codes for maximum
inner product search. In 2015 IEEE International Conference on Computer Vision (ICCV), pages 4148–
4156. IEEE, 2015.

Jun Wang, Sanjiv Kumar, and Shih-Fu Chang. Semi-supervised hashing for scalable image retrieval. In

Computer Vision and Pattern Recognition (CVPR), 2010.

13

Jingdong Wang, Heng Tao Shen, Jingkuan Song, and Jianqiu Ji. Hashing for similarity search: A survey.

arXiv preprint arXiv:1408.2927, 2014.

Yair Weiss, Antonio Torralba, and Rob Fergus. Spectral hashing.

In Advances in neural information

processing systems, pages 1753–1760, 2009.

P. M. Williams. Bayesian conditionalisation and the principle of minimum information. British Journal for

the Philosophy of Science, 31(2):131–144, 1980.

Felix X Yu, Sanjiv Kumar, Yunchao Gong, and Shih-Fu Chang. Circulant binary embedding. In International

conference on machine learning, volume 6, page 7, 2014.

Arnold Zellner. Optimal Information Processing and Bayes’s Theorem. The American Statistician, 42(4),

November 1988.

Peichao Zhang, Wei Zhang, Wu-Jun Li, and Minyi Guo. Supervised hashing with latent factor models. In
Proceedings of the 37th international ACM SIGIR conference on Research & development in information
retrieval, pages 173–182. ACM, 2014a.

Ting Zhang, Chao Du, and Jingdong Wang. Composite quantization for approximate nearest neighbor
search. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 838–
846, 2014b.

Han Zhu, Mingsheng Long, Jianmin Wang, and Yue Cao. Deep hashing network for eﬃcient similarity

retrieval. In Thirtieth AAAI Conference on Artiﬁcial Intelligence, 2016.

14

Supplementary Material

A Distributional Derivative of Stochastic Neuron

Before we prove the lemma 3, we ﬁrst introduce the chain rule of distributional derivative.

Lemma 7 (Grubb, 2008) Let u ∈ D(cid:48)(Ω), we have

1. (Chain Rule I) The distribution derivative of v = u ◦ f for any f (x) ∈ C1 : Ω(cid:48) → Ω is given by

Dv = Du ∂f
∂x .

given by Dv = f (cid:48)(u)Du.

2. (Chain Rule II) The distribution derivative of v = f ◦ u for any f (x) ∈ C1(R) with f (cid:48) bounded is

Proof of Lemma 3. Without loss of generality, we ﬁrst consider 1-dimension case. Given (cid:96)(˜h) : R → R,
ξ ∼ U(0, 1), ˜h : Ω → {0, 1}. For ∀φ ∈ C∞

(cid:90)

0 (Ω), we have
D(cid:96)(˜h(x))

φ(x)

(cid:16)

(cid:17)

(cid:90)

dx = −

φ(cid:48)(x)(cid:96)(x)dx

= −

φ(cid:48)(x)(cid:96)(0)dx +

φ(cid:48)(x)(cid:96)(1)dx

(cid:19)

(cid:18)(cid:90) 0

(cid:32)

−∞

0

(cid:12)
(cid:12)
φ(x)
(cid:12)
(cid:12)

−∞

(cid:90) ∞

0
(cid:12)
∞
(cid:12)
(cid:12)
(cid:12)
0

(cid:33)

= −

(cid:96)(0) + φ(x)

(cid:96)(1)

= ((cid:96)(1) − (cid:96)(0)) φ(0)

where the last equation comes from φ ∈ C∞

0 (Ω). We obtain

D(cid:96)(˜h) = ((cid:96)(1) − (cid:96)(0))δ(h) := ∆(cid:96)(h).

We generalize the conclusion to l-dimension case with expectation over ξ, i.e., ˜h(·, ξ) : Ω → {0, 1}l, we

have the partial distributional derivative for k-th coordinate as
(cid:105)
Dk(cid:96)(˜h(z, ξ))

(cid:104)
(cid:96)(˜h(z, ξ))

= E

DkE

(cid:104)

(cid:105)

{ξi}l

i=1

{ξi}l

i=1

Therefore, we have the distributional derivative w.r.t. W as
(cid:96)(˜h(σ(W (cid:62)x), ξ))

= E

DE

(cid:105)

(cid:104)

{ξi}l

i=1

{ξi}l

i=1

= E

{ξi}l

i=1,i(cid:54)=k

(cid:104)
((cid:96)(˜h1

k) − (cid:96)(˜h0

k))

(cid:105)

.

(cid:105)

(cid:104)
Dk(cid:96)(˜h(σ(W (cid:62)x), ξ))
(cid:104)
D˜hk

chain rule I = E

(cid:105)
(cid:96)(˜h(σ(W (cid:62)x), ξ))∇W σ(W (cid:62)x)
{ξi}l
∆˜h(cid:96)(˜h(σ(W (cid:62)x), ξ))σ(W (cid:62)x) • (cid:0)1 − σ(W (cid:62)x)(cid:1) x(cid:62)(cid:105)
(cid:104)

i=1

.

= Eξ

To derive the approximation of the distributional derivative, we exploit the mean value theorem and

Taylor expansion. Speciﬁcally, for a continuous and diﬀerential loss function (cid:96)(·), there exists (cid:15) ∈ (0, 1)

Moreover, for general smooth functions, we rewrite the ∂˜hi

(cid:96)(˜h)|˜hi=(cid:15) by Taylor expansion, i.e.,

∂˜hk

(cid:96)(˜h)|˜hk=(cid:15) =

(cid:104)

(cid:105)
∆˜h(cid:96)(˜h)

.

k

∂˜hk
∂˜hk

(cid:96)(˜h)|˜hi=(cid:15) = ∂˜hk
(cid:96)(˜h)|˜hi=(cid:15) = ∂˜hk

(cid:96)(˜h)|˜hi=1 + O((cid:15))
(cid:96)(˜h)|˜hi=0 + O((cid:15)).

we have an approximator as

(cid:96)(˜h)|˜hk=1 + (1 − σ(w(cid:62)
Plugging into the distributional derivative estimator (7), we obtain a simple biased gradient estimator,

(cid:96)(˜h)|˜hk=(cid:15) ≈ σ(w(cid:62)

(cid:96)(˜h)|˜hk=0 = Eξ

k x))∂˜hk

k x)∂˜hk

∂˜hk

.

(13)

(cid:104)
∇˜h(cid:96)(˜h, ξ)

(cid:105)

DW ˜H(Θ; x) ≈ ˜DW ˜H(Θ; x) := Eξ

(cid:104)
∇˜h(cid:96)(˜h(σ(W (cid:62)x), ξ))σ(W (cid:62)x) • (1 − σ(W (cid:62)x))x(cid:62)(cid:105)

.

(14)

15

B Convergence of Distributional SGD

Lemma 8 (Ghadimi and Lan, 2013) Under the assumption that H is L-Lipschitz smooth and the variance of
the stochastic distributional gradient (8) is bounded by σ2, the proposed distributional SGD outputs {Θi}t
i=1,
2(cid:21)

(cid:18)

(cid:19)

t
(cid:88)

i=1

γi −

L
2

γ2
i

E

(cid:20)(cid:13)
(cid:13)∇Θ ˜H(Θi)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:54) ˜H(Θ0) − ˜H(Θ∗) +

Lσ2
2

t
(cid:88)

i=1

γ2
i ,

where Θt = {Wt, Ut, βt, ρt}.

Proof of Theorem 5. Lemma 8 implies that by randomly sampling a search point ΘR with probability
P (R = i) = 2γi−Lγ2

√

i

i=1, we have

(cid:80)t

i=1 2γi−Lγ2
i

where γi ∼ O (cid:0)1/
(cid:20)(cid:13)
(cid:13)∇Θ ˜H(ΘR)
(cid:13)

E

t(cid:1) from trajectory {Θi}t
(cid:19)
2(cid:21)

∼ O

(cid:13)
(cid:13)
(cid:13)

(cid:18) 1
√
t

.

Lemma 9 Under the assumption that the variance of the approximate stochastic distributional gradient (10)
is bounded by σ2, the proposed distributional SGD outputs {Θi}t

i=1 such that

t
(cid:88)

i=1

γiE

(cid:105)
(cid:104)
(Θi − Θ∗)(cid:62) ˜∇Θ ˜H(Θi)

E

(cid:107)Θ0 − Θ∗(cid:107)2(cid:105)
(cid:104)

+

(cid:32)

(cid:54) 1
2

(cid:33)

i σ2
γ2

,

t
(cid:88)

i=1

where Θ∗ denotes the optimal solution.

Proof Denote the optimal solution as Θ∗, we have

(cid:107)Θi+1 − Θ∗(cid:107)2 =

(cid:13)
(cid:13)
2
(cid:13)Θi − γi (cid:98)˜∇Θ ˜H(Θi, xi) − Θ∗)
(cid:13)
(cid:13)
(cid:13)

= (cid:107)Θi − Θ∗(cid:107)2 + γ2
i
Taking expectation on both sides and denoting aj = (cid:107)Θj − Θ∗(cid:107)2, we have

(cid:13)
(cid:13)
2
(cid:98)˜∇Θ ˜H(Θi, xi)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

− 2γi (Θi − Θ∗)(cid:62) (cid:98)˜∇Θ ˜H(Θi, xi).

E [ai+1] (cid:54) E [ai] − 2γiE

(cid:104)

(cid:105)
(Θi − Θ∗)(cid:62) ˜∇Θ ˜H(Θi)

+ γ2

i σ2.

Therefore,

γiE

(cid:104)

(cid:105)
(Θi − Θ∗)(cid:62) ˜∇Θ ˜H(Θi)

t
(cid:88)

i=1

(cid:32)

(cid:54) 1
2

(cid:33)

E [a0] +

i σ2
γ2

.

t
(cid:88)

i=1

Proof of Theorem 6. The lemma 9 implies by randomly sampling a search point ΘR with probability
P (R = i) = γi

where γi ∼ O (cid:0)1/

√

(cid:80)t

i=1 γi

(cid:104)

E

(ΘR − Θ∗)(cid:62) ˜∇Θ ˜H(ΘR)

t(cid:1) from trajectory {Θi}t
(cid:107)Θ0 − Θ∗(cid:107)2(cid:105)
(cid:104)
2 (cid:80)t

(cid:54)

E

(cid:105)

i=1 γi

i=1, we have
+ (cid:80)t

i=1 γ2

i σ2

∼ O

(cid:19)

.

(cid:18) 1
√
t

C More Experiments

C.1 Convergence of Distributional SGD and Reconstruction Error Comparison

We shows the reconstruction error comparison between ITQ and SGH on MNIST and GIST-1M in Figure 4.
The results are similar to the performance on SIFT-1M. Because SGH optimizes a more expressive objective
than ITQ (without orthogonality) and do not use alternating optimization, it ﬁnd better solution with lower
reconstruction error.

16

(a) MNIST

(b) GIST-1M

Figure 4: L2 reconstruction error convergence on MNIST and GIST-1M of ITQ and SGH over the course of
training with varying of the length of the bits (8, 16, 32, 64, respectively). The x-axis represents the number
of examples seen by the training algorithm. For ITQ, it sees the training dataset once in one iteration.

C.2 Training Time Comparison

(a) MNIST

(b) GIST-1M

Figure 5: Training time comparison between BA and SGH on MNIST and GIST-1M.

We shows the training time comparison between BA and SGH on MNIST and GIST-1M in Figure 5. The
results are similar to the performance on SIFT-1M. The proposed distributional SGD learns the model much
faster.

C.3 More Evaluation on L2NNS Retrieval Tasks

We also use diﬀerent RecallK@N to evaluate the performances of our algorithm and the competitors. We ﬁrst
evaluated the performance of the algorithms with Recall 1@N in Figure 6. This is an easier task comparing
to K = 10. Under such measure, the proposed SGH still achieves the state-of-the-art performance.

In Figure 7, we set K, N = 100 and plot the recall by varying the length of the bits on MNIST, SIFT-1M,
and GIST-1M. This is to show the eﬀects of length of bits in diﬀerent baselines. Similar to the Recall10@N,
the proposed algorithm still consistently achieves the state-of-the-art performance under such evaluation
measure.

17

D Stochastic Generative Hashing For Maximum Inner Product

Search

In Maximum Inner Product Search (MIPS) problem, we evaluate the similarity in terms of inner product
which can avoid the scaling issue, i.e., the length of the samples in reference dataset and the queries may
vary. The proposed model can also be applied to the MIPS problem. In fact, the Gaussian reconstruction
model also preserve the inner product neighborhoods. Denote the asymmetric inner product as x(cid:62)U hy, we
claim
Proposition 10 The Gaussian reconstruction error is a surrogate for asymmetric inner product preserva-
tion.
Proof We evaluate the diﬀerence between inner product and the asymmetric inner product,

(cid:107)x(cid:62)y − x(cid:62)U (cid:62)hy(cid:107)2 = (cid:107)x(cid:62) (cid:0)y − U (cid:62)hy

(cid:1) (cid:107)2 (cid:54) (cid:107)x(cid:107)2(cid:107)y − U (cid:62)hy(cid:107)2,

which means minimizing the Gaussian reconstruction, i.e., − log p(x|h), error will also lead to asymmetric
inner product preservation.
We emphasize that our method is designed for hashing problems primarily. Although it can be used for
MIPS problem, it is diﬀerent from the product quantization and its variants whose distance are calculated
based on lookup table. The proposed distributional SGD can be extended to quantization. This is out of
the scope of this paper, and we will leave it as the future work.

D.1 MIPS Retrieval Comparison

To evaluate the performance of the proposed SGH on MIPS problem, we tested the algorithm on WORD2VEC
dataset for MIPS task. Besides the hashing baselines, since KMH is the Hamming distance generalization of
PQ, we replace the KMH with product quantization (Jegou et al., 2011). We trained the SGH with 71,291
samples and evaluated the performance with 10,000 query. Similarly, we vary the length of binary codes from
16, 32 to 64, and evaluate the performance by Recall 10@N. We calculated the ground-truth via retrieval
through the original inner product. The performances are illustrated in Figure 8. The proposed algorithm
outperforms the competitors signiﬁcantly, demonstrating the proposed SGH is also applicable to MIPS task.

E Generalization

We generalize the basic model to translation and scale invariant extension, semi-supervised extension, as
well as coding with h ∈ {−1, 1}l.

E.1 Translation and Scale Invariant Reduced-MRFs

As we known, the data may not zero-mean, and the scale of each sample in dataset can be totally diﬀerent.
To eliminate the translation and scale eﬀects, we extend the basic model to translation and scale invariant
reduced-MRFs by introducing parameter α to separate the translation eﬀect and the latent variable z to
model the scale eﬀect in each sample x, therefore, the potential function becomes
1
2ρ2 (x − α − U (cid:62)(z · h))(cid:62)(x − α − U (cid:62)(z · h)),

E(x, h, z) = −β(cid:62)h +

(15)

where · denotes element-wise product, α ∈ Rd and z ∈ Rl. Comparing to (2), we replace U (cid:62)h with
U (cid:62)(z · h) + α so that the translation and scale eﬀects in both dimension and sample are modeled explicitly.
We treat the α as parameters and z as latent variable. Assume the independence in posterior for
computational eﬃciency, we approximate the posterior p(z, h|x) with q(h|x; Wh)q(z|x; Wz), where Wh, Wz
denotes the parameters in the posterior approximation. With similar derivation, we obtain the learning

18

Obviously, the proposed distributional SGD is still applicable to this optimization.

Eq(h|xi)q(z|xi) [−E(x, h, z) − log q(h|xi) − log q(z|xi)] .

(16)

objective as

max
U,α,β,ρ;Wh,Wz

1
N

N
(cid:88)

i=1

E.2 Semi-supervised Extension

Although we only focus on learning the hash function in unsupervised setting, the proposed model can be
easily extended to exploit the supervision information by introducing pairwise model, e.g., (Zhang et al.,
2014a; Zhu et al., 2016). Speciﬁcally, we are provided the (partial) supervision information for some pairs
of data, i.e., S = {xi, xi, yij}M

i,j, where
(cid:40)

yij =

1
0

if xi ∈ N N (xj) or xj ∈ N N (xi)
o.w.

,

and N N (x) stands for the set of nearest neighbors of x. Besides the original Gaussian reconstruction model
in the basic model in (2), we introduce the pairwise model p(yij|hi, hj) = B(σ(h(cid:62)
i hj)) into the framework,
which results the joint distribution over x, y, h as

p(xi, xj, hi, hj, yij) = p(xi|hi)p(xj|hj)p(hi)p(hj)p(yij|hi, hj)1S (ij),

where 1S (ij) is an indicator that outputs 1 when (xi, xj) ∈ S, otherwise 0. Plug the extended model into
the Helmholtz free energy, we have the learning objective as,

max
U,β,ρ;W

1
N 2

N 2
(cid:88)

i,j=1

(cid:16)

Eq(hi|xi)q(hj |xj ) [log p(xi, xj, hi, hj)] + Eq(hi|xi)q(hj |xj ) [1S (ij) log p(yij|hi, hj)]

−Eq(hi|xi)q(hj |xi) [log q(hj|xj)q(hj|xi)]

(cid:17)

,

Obviously, the proposed distributional SGD is still applicable to the semi-supervised extension.

E.3 {±1}-Binary Coding

In the main text, we mainly focus on coding with {0, 1}. In fact, the proposed model is applicable to coding
with {−1, 1} with minor modiﬁcation. Moreover, the proposed distributional SGD is still applicable. We
only discuss the basic model here, the model can also be extended to scale-invariant and semi-supervised
variants.

If we set h ∈ {−1, 1}l, the potential function of basic reduced-MRFs (2) does not have any change, i.e.,

E(x, h) = −β(cid:62)h +

(cid:0)x(cid:62)x + h(cid:62)U (cid:62)U h − 2x(cid:62)U h(cid:1) .

1
2ρ2

We need to modify the parametrization of q(h|x) as

l
(cid:89)

q(h|x) =

σ(w(cid:62)

i x)

1+hi
2

(cid:0)1 − σ(w(cid:62)

i x)(cid:1) 1−hi

2

.

i=1
Therefore, the stochastic neuron becomes

With similar derivation, we have the distributional derivative of the objective w.r.t. W as

where [∆f (cid:96)(f (z, ξ))]k = (cid:96)(f 1

k ) − (cid:96)(f −1

k ). Furthermore, we have a similar biased gradient estimator as

Plug these modiﬁcation into the model and algorithm, we can learn a {−1, 1}-encoding function.

f (z, ξ) :=

(cid:40)
1
−1

if σ(z) (cid:62) ξ
if σ(z) < ξ

.

∇W Lsn = Eξ

(cid:2)∆f (cid:96)(f (z, ξ))∇zσ(z)x(cid:62)(cid:3) ,

˜∇W Lsn = Eξ

(cid:2)∇f (cid:96)(f (z, ξ))∇zσ(z)x(cid:62)(cid:3) .

19

(17)

(18)

(19)

(20)

Figure 6: L2NNS comparison on MNIST, SIFT-1M, SIFT-1B, and GIST-1M with the length of binary bits from
16 to 64. We evaluate the performance with Recall 1@M , where M increasing to 1000.

20

(a) L2NNS on MNIST

(b) L2NNS on SIFT-1M

(c) L2NNS on GIST-1M

Figure 7: L2NNS comparison on MNIST, SIFT-1M, and GIST-1M with Recall 100@100 for the length of bits
from 8 to 64.

Figure 8: MIPS comparison on WORD2VEC with the length of binary bits from 16 to 64. We evaluate the
performance with Recall 10@M , where M increasing to 1000.

21

7
1
0
2
 
g
u
A
 
2
1
 
 
]

G
L
.
s
c
[
 
 
2
v
5
1
8
2
0
.
1
0
7
1
:
v
i
X
r
a

Stochastic Generative Hashing

∗Bo Dai1, ∗Ruiqi Guo2, Sanjiv Kumar2, Niao He3, Le Song1

1 Georgia Institute of Technology
bodai@gatech.edu, lsong@cc.gatech.edu
2 Google Research, NYC
{guorq, sanjivk}@google.com
3 University of Illinois at Urbana-Champaign
niaohe@illinois.edu

August 15, 2017

Abstract

Learning-based binary hashing has become a powerful paradigm for fast search and retrieval in massive
databases. However, due to the requirement of discrete outputs for the hash functions, learning such
functions is known to be very challenging.
In addition, the objective functions adopted by existing
hashing techniques are mostly chosen heuristically. In this paper, we propose a novel generative approach
to learn hash functions through Minimum Description Length principle such that the learned hash codes
maximally compress the dataset and can also be used to regenerate the inputs. We also develop an
eﬃcient learning algorithm based on the stochastic distributional gradient, which avoids the notorious
diﬃculty caused by binary output constraints, to jointly optimize the parameters of the hash function
and the associated generative model. Extensive experiments on a variety of large-scale datasets show
that the proposed method achieves better retrieval results than the existing state-of-the-art methods.

1 Introduction

Search for similar items in web-scale datasets is a fundamental step in a number of applications, especially
i=1 with x ∈ X ⊂ Rd, we
in image and document retrieval. Formally, given a reference dataset X = {xi}N
want to retrieve similar items from X for a given query y according to some similarity measure sim(x, y).
When the negative Euclidean distance is used, i.e., sim(x, y) = −(cid:107)x − y(cid:107)2, this corresponds to L2 Nearest
Neighbor Search (L2NNS) problem; when the inner product is used, i.e., sim(x, y) = x(cid:62)y, it becomes a
Maximum Inner Product Search (MIPS) problem. In this work, we focus on L2NNS for simplicity, however
our method handles MIPS problems as well, as shown in the supplementary material D. Brute-force linear
search is expensive for large datasets. To alleviate the time and storage bottlenecks, two research directions
have been studied extensively: (1) partition the dataset so that only a subset of data points is searched; (2)
represent the data as codes so that similarity computation can be carried out more eﬃciently. The former
often resorts to search-tree or bucket-based lookup; while the latter relies on binary hashing or quantization.
These two groups of techniques are orthogonal and are typically employed together in practice.

In this work, we focus on speeding up search via binary hashing. Hashing for similarity search was
popularized by inﬂuential works such as Locality Sensitive Hashing (Indyk and Motwani, 1998; Gionis et al.,
1999; Charikar , 2002). The crux of binary hashing is to utilize a hash function, f (·) : X → {0, 1}l,
which maps the original samples in X ∈ Rd to l-bit binary vectors h ∈ {0, 1}l while preserving the similarity

∗Authors are equally contributed.

1

measure, e.g., Euclidean distance or inner product. Search with such binary representations can be eﬃciently
conducted using Hamming distance computation, which is supported via POPCNT on modern CPUs and
GPUs. Quantization based techniques (Babenko and Lempitsky, 2014; Jegou et al., 2011; Zhang et al.,
2014b) have been shown to give stronger empirical results but tend to be less eﬃcient than Hamming search
over binary codes (Douze et al., 2015; He et al., 2013).

Data-dependent hash functions are well-known to perform better than randomized ones (Wang et al.,
2014). Learning hash functions or binary codes has been discussed in several papers, including spectral
hashing (Weiss et al., 2009), semi-supervised hashing (Wang et al., 2010), iterative quantization (Gong and
Lazebnik, 2011), and others (Liu et al., 2011; Gong et al., 2013; Yu et al., 2014; Shen et al., 2015; Guo et al.,
2016). The main idea behind these works is to optimize some objective function that captures the preferred
properties of the hash function in a supervised or unsupervised fashion.

Even though these methods have shown promising performance in several applications, they suﬀer from
two main drawbacks: (1) the objective functions are often heuristically constructed without a principled
characterization of goodness of hash codes, and (2) when optimizing, the binary constraints are crudely
handled through some relaxation, leading to inferior results (Liu et al., 2014). In this work, we introduce
Stochastic Generative Hashing (SGH) to address these two key issues. We propose a generative model which
captures both the encoding of binary codes h from input x and the decoding of input x from h. This provides
a principled hash learning framework, where the hash function is learned by Minimum Description Length
(MDL) principle. Therefore, its generated codes can compress the dataset maximally. Such a generative
model also enables us to optimize distributions over discrete hash codes without the necessity to handle
discrete variables. Furthermore, we introduce a novel distributional stochastic gradient descent method
which exploits distributional derivatives and generates higher quality hash codes. Prior work on binary
autoencoders (Carreira-Perpin´an and Raziperchikolaei, 2015) also takes a generative view of hashing but
still uses relaxation of binary constraints when optimizing the parameters, leading to inferior performance
as shown in the experiment section. We also show that binary autoencoders can be seen as a special case of
our formulation. In this work, we mainly focus on the unsupervised setting1.

2 Stochastic Generative Hashing (SGH)

We start by ﬁrst formalizing the two key issues that motivate the development of the proposed algorithm.
Generative view. Given an input x ∈ Rd, most hashing works in the literature emphasize modeling
the forward process of generating binary codes from input, i.e., h(x) ∈ {0, 1}l, to ensure that the generated
hash codes preserve the local neighborhood structure in the original space. Few works focus on modeling the
reverse process of generating input from binary codes, so that the reconstructed input has small reconstruction
error. In fact, the generative view provides a natural learning objective for hashing. Following this intuition,
we model the process of generating x from h, p(x|h) and derive the corresponding hash function q(h|x)
from the generative process. Our approach is not tied to any speciﬁc choice of p(x|h) but can adapt to any
generative model appropriate for the domain. In this work, we show that even using a simple generative
model (Section 2.1) already achieves the state-of-the-art performance.

Binary constraints. The other issue arises from dealing with binary constraints. One popular approach
is to relax the constraints from {0, 1} (Weiss et al., 2009), but this often leads to a large optimality gap
between the relaxed and non-relaxed objectives. Another approach is to enforce the model parameterization
to have a particular structure so that when applying alternating optimization, the algorithm can alternate
between updating the parameters and binarization eﬃciently. For example, (Gong and Lazebnik, 2011; Gong
et al., 2012) imposed an orthogonality constraint on the projection matrix, while (Yu et al., 2014) proposed
to use circulant constraints, and (Zhang et al., 2014a) introduced Kronecker Product structure. Although
such constraints alleviate the diﬃculty with optimization, they substantially reduce the model ﬂexibility.
In contrast, we avoid such constraints and propose to optimize the distributions over the binary variables
to avoid directly working with binary variables. This is attained by resorting to the stochastic neuron

1The proposed algorithm can be extended to supervised/semi-supervised setting easily as described in the supplementary

material E.

2

reparametrization (Section 2.4), which allows us to back-propagate through the layers of weights using the
stochsastic gradient estimator.

Unlike (Carreira-Perpin´an and Raziperchikolaei, 2015) which relies on solving expensive integer programs,
our model is end-to-end trainable using distributional stochastic gradient descent (Section 3). Our algorithm
requires no iterative steps unlike iterative quantization (ITQ) (Gong and Lazebnik, 2011). The training
procedure is much more eﬃcient with guaranteed convergence compared to alternating optimization for
ITQ.

In the following sections, we ﬁrst introduce the generative hashing model p(x|h) in Section 2.1. Then,
we describe the corresponding process of generating hash codes given input x, q(h|x) in Section 2.2. Finally,
we describe the training procedure based on the Minimum Description Length (MDL) principle and the
stochastic neuron reparametrization in Sections 2.3 and 2.4. We also introduce the distributional stochastic
gradient descent algorithm in Section 3.

2.1 Generative Model p(x|h)

Unlike most works which start with the hash function h(x), we ﬁrst introduce a generative model that
deﬁnes the likelihood of generating input x given its binary code h, i.e., p(x|h).
It is also referred as a
decoding function. The corresponding hash codes are derived from an encoding function q(h|x), described
in Section 2.2.

We use a simple Gaussian distribution to model the generation of x given h:

p(x, h) = p(x|h)p(h), where p(x|h) = N (U h,ρ2I)

i=1, ui ∈ Rd is a codebook with l codewords. The prior p(h) ∼ B(θ) = (cid:81)l

(1)
and U = {ui}l
i (1 − θi)1−hi is
modeled as the multivariate Bernoulli distribution on the hash codes, where θ = [θi]l
i=1 ∈ [0, 1]l. Intuitively,
this is an additive model which reconstructs x by summing the selected columns of U given h, with a Bernoulli
prior on the distribution of hash codes. The joint distribution can be written as:

i=1 θhi

p(x, h) ∝ exp



1
2ρ2





(cid:0)x(cid:62)x + h(cid:62)U (cid:62)U h − 2x(cid:62)U h(cid:1)
(cid:123)(cid:122)
(cid:125)
(cid:124)
(cid:107)x−U (cid:62)h(cid:107)2
2

−(log

(cid:62)
)

h

θ
1 − θ







(2)

This generative model can be seen as a restricted form of general Markov Random Fields in the sense that
the parameters for modeling correlation between latent variables h and correlation between x and h are
shared. However, it is more ﬂexible compared to Gaussian Restricted Boltzmann machines (Krizhevsky,
2009; Marc’Aurelio and Geoﬀrey, 2010) due to an extra quadratic term for modeling correlation between
latent variables. We ﬁrst show that this generative model preserves local neighborhood structure of the x
when the Frobenius norm of U is bounded.

Proposition 1 If (cid:107)U (cid:107)F is bounded, then the Gaussian reconstruction error, (cid:107)x − U hx(cid:107)2 is a surrogate for
Euclidean neighborhood preservation.

Proof Given two points x, y ∈ Rd, their Euclidean distance is bounded by
(cid:107)x − y(cid:107)2

= (cid:107)(x − U (cid:62)hx) − (y − U (cid:62)hy) + (U (cid:62)hx − U (cid:62)hy)(cid:107)2
(cid:54) (cid:107)x − U (cid:62)hx(cid:107)2 + (cid:107)y − U (cid:62)hy(cid:107)2 + (cid:107)U (cid:62)(hx − hy)(cid:107)2
(cid:54) (cid:107)x − U (cid:62)hx(cid:107)2 + (cid:107)y − U (cid:62)hy(cid:107)2 + (cid:107)U (cid:107)F (cid:107)hx − hy(cid:107)2
where hx and hy denote the binary latent variables corresponding to x and y, respectively. Therefore, we
have

(cid:107)x − y(cid:107)2 − (cid:107)U (cid:107)F (cid:107)hx − hy(cid:107)2 (cid:54) (cid:107)x − U (cid:62)hx(cid:107)2 + (cid:107)y − U (cid:62)hy(cid:107)2
which means minimizing the Gaussian reconstruction error, i.e., − log p(x|h), will lead to Euclidean neigh-
borhood preservation.

3

A similar argument can be made with respect to MIPS neighborhood preservation as shown in the sup-
plementary material D. Note that the choice of p(x|h) is not unique, and any generative model that leads
to neighborhood preservation can be used here. In fact, one can even use more sophisticated models with
multiple layers and nonlinear functions.
In our experiments, we ﬁnd complex generative models tend to
perform similarly to the Gaussian model on datasets such as SIFT-1M and GIST-1M. Therefore, we use the
Gaussian model for simplicity.

2.2 Encoding Model q(h|x)

Even with the simple Gaussian model (1), computing the posterior p(h|x) = p(x,h)
is not tractable, and
p(x)
ﬁnding the MAP solution of the posterior involves solving an expensive integer programming subproblem.
Inspired by the recent work on variational auto-encoder (Kingma and Welling, 2013; Mnih and Gregor, 2014;
Gregor et al., 2014), we propose to bypass these diﬃculties by parameterizing the encoding function as

q(h|x) =

q(hk = 1|x)hk q(hk = 0|x)1−hk ,

(3)

l
(cid:89)

k=1

k=1 ∼ B(σ(W (cid:62)x)) with
to approximate the exact posterior p(h|x). With the linear parametrization, h = [hk]l
W = [wk]l
k=1. At the training step, a hash code is obtained by sampling from B(σ(W (cid:62)x)). At the inference
step, it is still possible to sample h. More directly, the MAP solution of the encoding function (3) is readily
given by

h(x) = argmax

q(h|x) =

h

sign(W (cid:62)x) + 1
2

This involves only a linear projection followed by a sign operation, which is common in the hashing literature.
Computing h(x) in our model thus has the same amount of computation as ITQ (Gong and Lazebnik, 2011),
except without the orthogonality constraints.

2.3 Training Objective

Since our goal is to reconstruct x using the least information in binary codes, we train the variational
auto-encoder using the Minimal Description Length (MDL) principle, which ﬁnds the best parameters that
maximally compress the training data. The MDL principle seeks to minimize the expected amount of
information to communicate x:

L(x) =

q(h|x)(L(h) + L(x|h))

(cid:88)

h

where L(h) = − log p(h) + log q(h|x) is the description length of the hashed representation h and L(x|h) =
− log p(x|h) is the description length of x having already communicated h in (Hinton and Van Camp, 1993;
Hinton and Zemel, 1994; Mnih and Gregor, 2014). By summing over all training examples x, we obtain the
following training objective, which we wish to minimize with respect to the parameters of p(x|h) and q(h|x):

min
Θ={W,U,β,ρ}

(cid:88)

x

(cid:88)

(cid:88)

x

h

H(Θ) :=

L(x; Θ) = −

q(h|x)(log p(x, h) − log q(h|x)),

(4)

where U, ρ and β := log θ
1−θ are parameters of the generative model p(x, h) as deﬁned in (1), and W comes
from the encoding function q(h|x) deﬁned in (3). This objective is sometimes called Helmholtz (variational)
free energy (Williams, 1980; Zellner, 1988; Dai et al., 2016). When the true posterior p(h|x) falls into the
family of (3), q(h|x) becomes the true posterior p(h|x), which leads to the shortest description length to
represent x.

We emphasize that this objective no longer includes binary variables h as parameters and therefore avoids
optimizing with discrete variables directly. This paves the way for continuous optimization methods such as
stochastic gradient descent (SGD) to be applied in training. As far as we are aware, this is the ﬁrst time
such a procedure has been used in the problem of unsupervised learning to hash. Our methodology serves
as a viable alternative to the relaxation-based approaches commonly used in the past.

4

2.4 Reparametrization via Stochastic Neuron

Using the training objective of (4), we can directly compute the gradients w.r.t. parameters of p(x|h).
However, we cannot compute the stochastic gradients w.r.t. W because it depends on the stochastic bi-
nary variables h.
In order to back-propagate through stochastic nodes of h, two possible solutions have
been proposed. First, the reparametrization trick (Kingma and Welling, 2013) which works by introducing
auxiliary noise variables in the model. However, it is diﬃcult to apply when the stochastic variables are
discrete, as is the case for h in our model. On the other hand, the gradient estimators based on REINFORCE
trick (Bengio et al., 2013) suﬀer from high variance. Although some variance reduction remedies have been
proposed (Mnih and Gregor, 2014; Gu et al., 2015), they are either biased or require complicated extra
computation in practice.

In next section, we ﬁrst provide an unbiased estimator of the gradient w.r.t. W derived based on
distributional derivative, and then, we derive a simple and eﬃcient approximator. Before we derive the
estimator, we ﬁrst introduce the stochastic neuron for reparametrizing Bernoulli distribution. A stochastic
neuron reparameterizes each Bernoulli variable hk(z) with z ∈ (0, 1).
Introducing random variables ξ ∼
U(0, 1), the stochastic neuron is deﬁned as

˜h(z, ξ) :=

(cid:40)

1
0

if z (cid:62) ξ
if z < ξ

.

(5)

Because P(˜h(z, ξ) = 1) = z, we have ˜h(z, ξ) ∼ B(z). We use the stochastic neuron (5) to reparameterize our
k=1. Note that ˜h now behaves
binary variables h by replacing [hk]l
deterministically given ξ. This gives us the reparameterized version of our original training objective (4):
˜H(Θ; x) :=

k x)) with [˜hk(σ(w(cid:62)

k=1(x) ∼ B(σ(w(cid:62)

(cid:105)
(cid:104)
(cid:96)(˜h, x)

k x), ξk)]l

˜H(Θ) =

(cid:88)

(cid:88)

(6)

,

Eξ

where (cid:96)(˜h, x) := − log p(x, ˜h(σ(W (cid:62)x), ξ)) + log q(˜h(σ(W (cid:62)x), ξ)|x) with ξ ∼ U(0, 1). With such a reformu-
lation, the new objective can now be optimized by exploiting the distributional stochastic gradient descent,
which will be explained in the next section.

x

x

3 Distributional Stochastic Gradient Descent

i=1, the stochastic gradient (cid:98)∇U,β,ρ ˜H(Θ; x)
For the objective in (6), given a point x randomly sampled from {xi}N
can be easily computed in the standard way. However, with the reparameterization, the function ˜H(Θ; x) is
no longer diﬀerentiable with respect to W due to the discontinuity of the stochastic neuron ˜h(z, ξ). Namely,
the SGD algorithm is not readily applicable. To overcome this diﬃculty, we will adopt the notion of distri-
butional derivative for generalized functions or distributions (Grubb, 2008).

3.1 Distributional derivative of Stochastic Neuron

Let Ω ⊂ Rd be an open set. Denote C∞
0 (Ω) as the space of the functions that are inﬁnitely diﬀerentiable
with compact support in Ω. Let D(cid:48)(Ω) be the space of continuous linear functionals on C∞
0 (Ω), which can
be considered as the dual space. The elements in space D(cid:48)(Ω) are often called general distributions. We
emphasize this deﬁnition of distributions is more general than that of traditional probability distributions.

Deﬁnition 2 (Distributional derivative) (Grubb, 2008) Let u ∈ D(cid:48)(Ω), then a distribution v is called
the distributional derivative of u, denoted as v = Du, if it satisﬁes
(cid:90)

(cid:90)

vφdx = −

u∂φdx,

∀φ ∈ C∞

0 (Ω).

Ω

Ω
It is straightforward to verify that for given ξ, the function ˜h(z, ξ) ∈ D(cid:48)(Ω) and moreover, Dz
˜h(z, ξ) = δξ(z),
which is exactly the Dirac-δ function. Based on the deﬁnition of distributional derivatives and chain rules, we
are able to compute the distributional derivative of the function ˜H(Θ; x), which is provided in the following
lemma.

5

i=1

Algorithm 1 Distributional-SGD
Input: {xi}N
1: Initialize Θ0 = {W, U, β, ρ} randomly.
2: for i = 1, . . . , t do
3:
4:

5:
6: Update parameters as

7: end for

Sample xi uniformly from {xi}N
Sample ξi ∼ U([0, 1]l).
Compute stochastic gradients (cid:98)∇Θ ˜H(Θi; xi) or (cid:98)˜∇Θ ˜H(Θi; xi), deﬁned in (8) and (10), respectively.

i=1.

Θi+1 = Θi − γi (cid:98)∇Θ ˜H(Θi; xi), or

Θi+1 = Θi − γi (cid:98)˜∇Θ ˜H(Θi; xi), respectively.

Lemma 3 For a given sample x, the distributional derivative of function ˜H(Θ; x) w.r.t. W is given by

DW ˜H(Θ; x) = Eξ

∆˜h(cid:96)(˜h(σ(W (cid:62)x), ξ))σ(W (cid:62)x) • (1 − σ(W (cid:62)x))x(cid:62)(cid:105)
where • denotes point-wise product and ∆˜h(cid:96)(˜h) denotes the ﬁnite diﬀerence deﬁned as
(cid:96)(˜h0

k]l = ˜hl if k (cid:54)= l, otherwise [˜hi

k]l = i, i ∈ {0, 1}.

k), where [˜hi

(cid:104)

,

(7)

(cid:104)
∆˜h(cid:96)(˜h)

(cid:105)

k

= (cid:96)(˜h1

k) −

We can therefore combine distributional derivative estimators (7) with stochastic gradient descent algorithm
(see e.g., (Nemirovski et al., 2009) and its variants (Kingma and Ba, 2014; Bottou et al., 2016)), which we
designate as Distributional SGD. The detail is presented in Algorithm 1, where we denote
(cid:104)

(cid:105)

(cid:98)∇Θ ˜H(Θi; xi) =

(cid:98)DW ˜H(Θi; xi), (cid:98)∇U,β,ρ ˜H(Θi; xi)

(8)

as the unbiased stochastic estimator of the gradient at Θi constructed by sample xi, ξi. Compared to the
existing algorithms for learning to hash which require substantial eﬀort on optimizing over binary variables,
the proposed distributional SGD is much simpler and also amenable to online settings (Huang et al., 2013;
Leng et al., 2015).

In general, the distributional derivative estimator (7) requires two forward passes of the model for each
dimension. To further accelerate the computation, we approximate the distributional derivative DW ˜H(Θ; x)
by exploiting the mean value theorem and Taylor expansion by

˜DW ˜H(Θ; x) := Eξ

(cid:104)
∇˜h(cid:96)(˜h(σ(W (cid:62)x), ξ))σ(W (cid:62)x) • (1 − σ(W (cid:62)x))x(cid:62)(cid:105)

,

(9)

which can be computed for each dimension in one pass. Then, we can exploit this estimator
(cid:104)
(cid:98)˜DW ˜H(Θi; xi), (cid:98)∇U,β,ρ ˜H(Θi; xi)
in Algorithm 1.
Interestingly, the approximate stochastic gradient estimator of the stochastic neuron
we established through the distributional derivative coincides with the heuristic “pseudo-gradient” con-
structed (Raiko et al., 2014). Please refer to the supplementary material A for details for the derivation of
the approximate gradient estimator (9).

(cid:98)˜∇Θ ˜H(Θi; xi) =

(10)

(cid:105)

3.2 Convergence of Distributional SGD

One caveat here is that due to the potential discrepancy of the distributional derivative and the traditional
gradient, whether the distributional derivative is still a descent direction and whether the SGD algorithm
integrated with distributional derivative converges or not remains unclear in general. However, for our
learning to hash problem, one can easily show that the distributional derivative in (7) is indeed the true
gradient.

Proposition 4 The distributional derivative DW ˜H(Θ; x) is equivalent to the traditional gradient ∇W H(Θ; x).

6

Proof First of all, by deﬁnition, we have ˜H(Θ; x) = H(Θ; x). One can easily verify that under mild
condition, both DW ˜H(Θ; x) and ∇W H(Θ; x) are continuous and 1-norm bounded. Hence, it suﬃces to show
that for any distribution u ∈ C1(Ω) and Du, ∇u ∈ L1(Ω), Du = ∇u. For any φ ∈ C∞
0 (Ω), by deﬁnition
of the distributional derivative, we have (cid:82)
Ω u∂φdx. On the other hand, we always have
Ω ∇uφdx = − (cid:82) u∂φdx. Hence, (cid:82)
(cid:82)
0 (Ω). By the Du Bois-Reymond’s lemma
(see Lemma 3.2 in (Grubb, 2008)), we have Du = ∇u.

Ω Duφdx = − (cid:82)
Ω(Du − ∇u)φdx = 0 for all φ ∈ C∞

Consequently, the distributional SGD algorithm enjoys the same convergence property as the traditional
SGD algorithm. Applying theorem 2.1 in (Ghadimi and Lan, 2013), we arrive at

Theorem 5 Under the assumption that H is L-Lipschitz smooth and the variance of the stochastic dis-
tributional gradient (8) is bounded by σ2 in the distributional SGD, for the solution ΘR sampled from the
trajectory {Θi}t

where γi ∼ O (cid:0)1/

t(cid:1), we have

√

i=1 with probability P (R = i) = 2γi−Lγ2
2(cid:21)

(cid:80)t

i=1 2γi−Lγ2
i

i

E

(cid:20)(cid:13)
(cid:13)∇Θ ˜H(ΘR)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

∼ O

(cid:19)

.

(cid:18) 1
√
t

In fact, with the approximate gradient estimators (9), the proposed algorithm is also converging in terms of
ﬁrst-order conditions, i.e.,

Theorem 6 Under the assumption that the variance of the approximate stochastic distributional gradi-
ent (10) is bounded by σ2, for the solution ΘR sampled from the trajectory {Θi}t
i=1 with probability P (R =
i) = γi

t(cid:1), we have

where γi ∼ O (cid:0)1/

√

(cid:80)t

i=1 γi

(cid:104)

E

(ΘR − Θ∗)(cid:62) ˜∇Θ ˜H(ΘR)

∼ O

(cid:105)

(cid:19)

,

(cid:18) 1
√
t

where Θ∗ denotes the optimal solution.

For the detailed proof of theorem 5 and 6, please refer to the supplementary material B.

4 Connections

The proposed stochastic generative hashing is a general framework. In this section, we reveal the connection
to several existing algorithms.
Iterative Quantization (ITQ). If we ﬁx some ρ > 0, and U = W R where W is formed by eigenvectors of
the covariance matrix and R is an orthogonal matrix, we have U (cid:62)U = I. If we assume the joint distribution
as

p(x, h) ∝ N (W Rh, ρ2I)B(θ),
and parametrize q(h|xi) = δbi(h), then from the objective in (4) and ignoring the irrelevant terms, we obtain
the optimization

min
R,b

N
(cid:88)

i=1

(cid:107)xi − W Rbi(cid:107)2,

which is exactly the objective of iterative quantization (Gong and Lazebnik, 2011).
Binary Autoencoder (BA). If we use the deterministic linear encoding function, i.e., q(h|x) = δ 1+sign(W (cid:62) x)

(h),

and preﬁx some ρ > 0, and ignore the irrelevant terms, the optimization (4) reduces to

min
U,W

N
(cid:88)

i=1

(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)xi − U h
(cid:13)

, s.t. h =

1 + sign(W (cid:62)x)
2

,

which is the objective of a binary autoencoder (Carreira-Perpin´an and Raziperchikolaei, 2015).

In BA, the encoding procedure is deterministic, therefore, the entropy term Eq(h|x) [log q(h|x)] = 0. In
fact, the entropy term, if non-zero, performs like a regularization and helps to avoid wasting bits. Moreover,

(11)

2

(12)

7

(a) Reconstruction Error

(b) Training Time

Figure 1: (a) Convergence of reconstruction error with number of samples seen by SGD, and (b) training
time comparison of BA and SGH on SIFT-1M over the course of training with varying number of bits.

without the stochasticity, the optimization (12) becomes extremely diﬃcult due to the binary constraints.
While for the proposed algorithm, we exploit the stochasticity to bypass such diﬃculty in optimization. The
stochasticity enables us to accelerate the optimization as shown in section 5.2.

5 Experiments

In this section, we evaluate the performance of the proposed distributional SGD on commonly used datasets
in hashing. Due to the eﬃciency consideration, we conduct the experiments mainly with the approximate
gradient estimator (9). We evaluate the model and algorithm from several aspects to demonstrate the power
of the proposed SGH: (1) Reconstruction loss. To demonstrate the ﬂexibility of generative modeling, we
compare the L2 reconstruction error to that of ITQ (Gong and Lazebnik, 2011), showing the beneﬁts of mod-
eling without the orthogonality constraints. (2) Convergence of the distributional SGD. We evaluate
the reconstruction error showing that the proposed algorithm indeed converges, verifying the theorems. (3)
Training time. The existing generative works require a signiﬁcant amount of time for training the model.
In contrast, our SGD algorithm is very fast to train both in terms of number of examples needed and the
wall time. (4) Nearest neighbor retrieval. We show Recall K@N plots on standard large scale nearest
neighbor search benchmark datasets of MNIST, SIFT-1M, GIST-1M and SIFT-1B, for all of which we achieve
state-of-the-art among binary hashing methods. (5) Reconstruction visualization. Due to the generative
nature of our model, we can regenerate the original input with very few bits. On MNIST and CIFAR10, we
qualitatively illustrate the templates that correspond to each bit and the resulting reconstruction.

We used several benchmarks datasets, i.e., (1) MNIST which contains 60,000 digit images of size 28 × 28
pixels, (2) CIFAR-10 which contains 60,000 32 × 32 pixel color images in 10 classes, (3) SIFT-1M and (4)
SIFT-1B which contain 106 and 109 samples, each of which is a 128 dimensional vector, and (5) GIST-1M
which contains 106 samples, each of which is a 960 dimensional vector.

5.1 Reconstruction loss

Because our method has a generative model p(x|h), we can easily compute the regenerated input ˜x =
argmax p(x|h), and then compute the L2 loss of the regenerated input and the original x, i.e., (cid:107)x − ˜x(cid:107)2
2.
ITQ also trains by minimizing the binary quantization loss, as described in Equation (2) in (Gong and
Lazebnik, 2011), which is essentially L2 reconstruction loss when the magnitude of the feature vectors is
compatible with the radius of the binary cube. We plotted the L2 reconstruction loss of our method and

8

Table 1: Training time on SIFT-1M in second.

Method
SGH
ITQ

8 bits
28.32
92.82

16 bits
29.38
121.73

32 bits
37.28
173.65

64 bits
55.03
259.13

ITQ on SIFT-1M in Figure 1(a) and on MNIST and GIST-1M in Figure 4, where the x-axis indicates the number
of examples seen by the training algorithm and the y-axis shows the average L2 reconstruction loss. The
training time comparison is listed in Table 1. Our method (SGH) arrives at a better reconstruction loss with
comparable or even less time compared to ITQ. The lower reconstruction loss demonstrates our claim that
the ﬂexibility of the proposed model aﬀorded by removing the orthogonality constraints indeed brings extra
modeling ability. Note that ITQ is generally regarded as a technique with fast training among the existing
binary hashing algorithms, and most other algorithms (He et al., 2013; Heo et al., 2012; Carreira-Perpin´an
and Raziperchikolaei, 2015) take much more time to train.

5.2 Empirical study of Distributional SGD

We demonstrate the convergence of the distributional derivative with Adam (Kingma and Ba, 2014) numer-
ically on SIFT-1M, GIST-1M and MINST from 8 bits to 64 bits. The convergence curves on SIFT-1M are shown
in Figure 1 (a). The results on GIST-1M and MNIST are similar and shown in Figure 4 in supplementary
material C. Obviously, the proposed algorithm, even with a biased gradient estimator, converges quickly, no
matter how many bits are used. It is reasonable that with more bits, the model ﬁts the data better and the
reconstruction error can be reduced further.

In line with the expectation, our distributional SGD trains much faster since it bypasses integer pro-
gramming. We benchmark the actual time taken to train our method to convergence and compare that
to binary autoencoder hashing (BA) (Carreira-Perpin´an and Raziperchikolaei, 2015) on SIFT-1M, GIST-1M
and MINST. We illustrate the performance on SIFT-1M in Figure 1(b) . The results on GIST-1M and MNIST
datasets follow a similar trend as shown in the supplementary material C. Empirically, BA takes signiﬁcantly
more time to train on all bit settings due to the expensive cost for solving integer programming subproblem.
Our experiments were run on AMD 2.4GHz Opteron CPUs×4 and 32G memory. Our implementation of the
stochastic neuron as well as the whole training procedure was done in TensorFlow. We have released our
code on GitHub2. For the competing methods, we directly used the code released by the authors.

5.3 Large scale nearest neighbor retrieval

We compared the stochastic generative hashing on an L2NNS task with several state-of-the-art unsupervised
algorithms, including K-means hashing (KMH) (He et al., 2013), iterative quantization (ITQ) (Gong and
Lazebnik, 2011), spectral hashing (SH) (Weiss et al., 2009), spherical hashing (SpH) (Heo et al., 2012), binary
autoencoder (BA) (Carreira-Perpin´an and Raziperchikolaei, 2015), and scalable graph hashing (GH) (Jiang
and Li, 2015). We demonstrate the performance of our binary codes by doing standard benchmark experi-
ments of Approximate Nearest Neighbor (ANN) search by comparing the retrieval recall. In particular, we
compare with other unsupervised techniques that also generate binary codes. For each query, linear search
in Hamming space is conducted to ﬁnd the approximate neighbors.

Following the experimental setting of (He et al., 2013), we plot the Recall10@N curve for MNIST, SIFT-1M,
GIST-1M, and SIFT-1B datasets under varying number of bits (16, 32 and 64) in Figure 2. On the SIFT-1B
datasets, we only compared with ITQ since the training cost of the other competitors is prohibitive. The
recall is deﬁned as the fraction of retrieved true nearest neighbors to the total number of true nearest
neighbors. The Recall10@N is the recall of 10 ground truth neighbors in the N retrieved samples. Note
that Recall10@N is generally a more challenging criteria than Recall@N (which is essentially Recall1@N),
and better characterizes the retrieval results. For completeness, results of various Recall K@N curves can be
found in the supplementary material which show similar trend as the Recall10@N curves.

2https://github.com/doubling/Stochastic Generative Hashing

9

Figure 2: L2NNS comparison on MNIST, SIFT-1M, and GIST-1M and SIFT-1B with the length of binary codes
varying from 16 to 64 bits. We evaluate the performance with Recall 10@M (fraction of top 10 ground truth
neighbors in retrieved M), where M increases up to 1000.

Figure 2 shows that the proposed SGH consistently performs the best across all bit settings and all
datasets. The searching time is the same for the same number of bits, because all algorithms use the same
optimized implementation of POPCNT based Hamming distance computation and priority queue. We point
out that many of the baselines need signiﬁcant parameter tuning for each experiment to achieve a reasonable
recall, except for ITQ and our method, where we ﬁx hyperparameters for all our experiments and used a batch
size of 500 and learning rate of 0.01 with stepsize decay. Our method is less sensitive to hyperparameters.

5.4 Visualization of reconstruction

One important aspect of utilizing a generative model for a hash function is that one can generate the input
from its hash code. When the inputs are images, this corresponds to image generation, which allows us to
visually inspect what the hash bits encode, as well as the diﬀerences in the original and generated images.
In our experiments on MNIST and CIFAR-10, we ﬁrst visualize the “template” which corresponds to each
hash bit, i.e., each column of the decoding dictionary U . This gives an interesting insight into what each
hash bit represents. Unlike PCA components, where the top few look like averaged images and the rest
are high frequency noise, each of our image template encodes distinct information and looks much like ﬁlter
banks of convolution neural networks. Empirically, each template also looks quite diﬀerent and encodes
somewhat meaningful information, indicating that no bits are wasted or duplicated. Note that we obtain
this representation as a by-product, without explicitly setting up the model with supervised information,
similar to the case in convolution neural nets.

We also compare the reconstruction ability of SGH with the that of ITQ and real valued PCA in Figure 3.

10

(a) Templates and re-generated images on MNIST

(b) Templates and re-generated images on CIFAR-10

Figure 3: Illustration of MNIST and CIFAR-10 templates (left) and regenerated images (right) from diﬀerent
methods with 64 hidden binary variables. In MNIST, the four rows and their number of bits used to encode
them are, from the top: (1) original image, 28×28×8 = 6272 bits; (2) PCA with 64 components 64×32 = 2048
bits; (3) SGH, 64 bits; (4) ITQ, 64 bits. In CIFAR : (1) original image, 30 × 30 × 24 = 21600 bits; (2) PCA
with 64 components 64 × 32 = 2048 bits; (3) SGH, 64 bits; (4) ITQ, 64 bits. The SGH reconstruction tends
to be much better than that of ITQ, and is on par with PCA which uses 32 times more bits!

For ITQ and SGH, we use a 64-bit hash code. For PCA, we kept 64 components, which amounts to
64 × 32 = 2048 bits. Visually comparing with SGH, ITQ reconstructed images look much less recognizable
on MNIST and much more blurry on CIFAR-10. Compared to PCA, SGH achieves similar visual quality
while using a signiﬁcantly lower (32× less) number of bits!

6 Conclusion

In this paper, we have proposed a novel generative approach to learn binary hash functions. We have justiﬁed
from a theoretical angle that the proposed algorithm is able to provide a good hash function that preserves
Euclidean neighborhoods, while achieving fast learning and retrieval. Extensive experimental results justify
the ﬂexibility of our model, especially in reconstructing the input from the hash codes. Comparisons with
approximate nearest neighbor search over several benchmarks demonstrate the advantage of the proposed
algorithm empirically. We emphasize that the proposed generative hashing is a general framework which
can be extended to semi-supervised settings and other learning to hash scenarios as detailed in the supple-
mentary material. Moreover, the proposed distributional SGD with the unbiased gradient estimator and
its approximator can be applied to general integer programming problems, which may be of independent
interest.

LS is supported in part by NSF IIS-1218749, NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983,
NSF IIS-1639792 EAGER, ONR N00014-15-1-2340, NVIDIA, Intel and Amazon AWS.

Acknowledgements

References

Babenko, Artem and Lempitsky, Victor. Additive quantization for extreme vector compression. In roceedings

of the IEEE Conference on Computer Vision and Pattern Recognition, 2014.

11

Yoshua Bengio, Nicholas L´eonard, and Aaron Courville. Estimating or propagating gradients through

stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.

L´eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning.

arXiv preprint arXiv:1606.04838, 2016.

Miguel A Carreira-Perpin´an and Ramin Raziperchikolaei. Hashing with binary autoencoders. In Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition, pages 557–566, 2015.

Charikar, Moses S. Similarity estimation techniques from rounding algorithms. Proceedings of the thiry-fourth

annual ACM symposium on Theory of computing, pages 380–388, 2002. ‘’

Bo Dai, Niao He, Hanjun Dai, and Le Song. Provable bayesian inference via particle mirror descent. In
Proceedings of the 19th International Conference on Artiﬁcial Intelligence and Statistics, pages 985–994,
2016.

Matthijs Douze, Herv´e J´egou, and Florent Perronnin. Polysemous codes.

In European Conference on

Computer Vision, 2016.

Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic

programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013.

Aristides Gionis, Piotr Indyk, Rajeev Motwani, et al. Similarity search in high dimensions via hashing. In

VLDB, volume 99, pages 518–529, 1999.

Yunchao Gong and Svetlana Lazebnik. Iterative quantization: A procrustean approach to learning binary
codes. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 817–824.
IEEE, 2011.

Yunchao Gong, Sanjiv Kumar, Vishal Verma, and Svetlana Lazebnik. Angular quantization-based binary

codes for fast similarity search. In Advances in neural information processing systems, 2012.

Yunchao Gong, Sanjiv Kumar, Henry A Rowley, and Svetlana Lazebnik. Learning binary codes for high-
dimensional data using bilinear projections. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 484–491, 2013.

Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep autoregressive
networks. In Proceedings of The 31st International Conference on Machine Learning, pages 1242–1250,
2014.

Gerd Grubb. Distributions and operators, volume 252. Springer Science & Business Media, 2008.

Shixiang Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih. Muprop: Unbiased backpropagation for

stochastic neural networks. arXiv preprint arXiv:1511.05176, 2015.

Ruiqi Guo, Sanjiv Kumar, Krzysztof Choromanski, and David Simcha. Quantization based fast inner product

search. 19th International Conference on Artiﬁcial Intelligence and Statistics, 2016.

Kaiming He, Fang Wen, and Jian Sun. K-means hashing: An aﬃnity-preserving quantization method for
learning binary compact codes. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 2938–2945, 2013.

Jae-Pil Heo, Youngwoon Lee, Junfeng He, Shih-Fu Chang, and Sung-Eui Yoon. Spherical hashing.

In
Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 2957–2964. IEEE,
2012.

12

Geoﬀrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description
length of the weights. In Proceedings of the sixth annual conference on Computational learning theory,
pages 5–13. ACM, 1993.

Geoﬀrey E Hinton and Richard S Zemel. Autoencoders, minimum description length and helmholtz free

energy. In Advances in Neural Information Processing Systems, pages 3–10, 1994.

Long-Kai Huang, Qiang Yang, and Wei-Shi Zheng. Online hashing. In Proceedings of the Twenty-Third

international joint conference on Artiﬁcial Intelligence, pages 1422–1428. AAAI Press, 2013.

Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of dimen-
sionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604–613.
ACM, 1998.

Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE

transactions on pattern analysis and machine intelligence, 33(1):117–128, 2011.

Qing-Yuan Jiang and Wu-Jun Li. Scalable Graph Hashing with Feature Transformation. In Twenty-Fourth

International Joint Conference on Artiﬁcial Intelligence, 2015.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization.

arXiv preprint

Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,

arXiv:1412.6980, 2014.

2013.

Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.

Cong Leng, Jiaxiang Wu, Jian Cheng, Xiao Bai, and Hanqing Lu. Online sketching hashing. In 2015 IEEE

Conference on Computer Vision and Pattern Recognition (CVPR), pages 2503–2511. IEEE, 2015.

Wei Liu, Jun Wang, Sanjiv Kumar, and Shih-Fu Chang. Hashing with graphs. In Proceedings of the 28th

international conference on machine learning (ICML-11), pages 1–8, 2011.

Wei Liu , Cun Mu, Sanjiv Kumar and Shih-Fu Chang. Discrete graph hashing.

In Advances in Neural

Information Processing Systems (NIPS), 2014.

Ranzato Marc’Aurelio and E Hinton Geoﬀrey. Modeling pixel means and covariances using factorized third-
order boltzmann machines. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference
on, pages 2551–2558. IEEE, 2010.

Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. arXiv preprint

arXiv:1402.0030, 2014.

Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approxima-

tion approach to stochastic programming. SIAM Journal on optimization, 19(4):1574–1609, 2009.

Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh. Techniques for learning binary

stochastic feedforward neural networks. arXiv preprint arXiv:1406.2989, 2014.

Fumin Shen, Wei Liu, Shaoting Zhang, Yang Yang, and Heng Tao Shen. Learning binary codes for maximum
inner product search. In 2015 IEEE International Conference on Computer Vision (ICCV), pages 4148–
4156. IEEE, 2015.

Jun Wang, Sanjiv Kumar, and Shih-Fu Chang. Semi-supervised hashing for scalable image retrieval. In

Computer Vision and Pattern Recognition (CVPR), 2010.

13

Jingdong Wang, Heng Tao Shen, Jingkuan Song, and Jianqiu Ji. Hashing for similarity search: A survey.

arXiv preprint arXiv:1408.2927, 2014.

Yair Weiss, Antonio Torralba, and Rob Fergus. Spectral hashing.

In Advances in neural information

processing systems, pages 1753–1760, 2009.

P. M. Williams. Bayesian conditionalisation and the principle of minimum information. British Journal for

the Philosophy of Science, 31(2):131–144, 1980.

Felix X Yu, Sanjiv Kumar, Yunchao Gong, and Shih-Fu Chang. Circulant binary embedding. In International

conference on machine learning, volume 6, page 7, 2014.

Arnold Zellner. Optimal Information Processing and Bayes’s Theorem. The American Statistician, 42(4),

November 1988.

Peichao Zhang, Wei Zhang, Wu-Jun Li, and Minyi Guo. Supervised hashing with latent factor models. In
Proceedings of the 37th international ACM SIGIR conference on Research & development in information
retrieval, pages 173–182. ACM, 2014a.

Ting Zhang, Chao Du, and Jingdong Wang. Composite quantization for approximate nearest neighbor
search. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 838–
846, 2014b.

Han Zhu, Mingsheng Long, Jianmin Wang, and Yue Cao. Deep hashing network for eﬃcient similarity

retrieval. In Thirtieth AAAI Conference on Artiﬁcial Intelligence, 2016.

14

Supplementary Material

A Distributional Derivative of Stochastic Neuron

Before we prove the lemma 3, we ﬁrst introduce the chain rule of distributional derivative.

Lemma 7 (Grubb, 2008) Let u ∈ D(cid:48)(Ω), we have

1. (Chain Rule I) The distribution derivative of v = u ◦ f for any f (x) ∈ C1 : Ω(cid:48) → Ω is given by

Dv = Du ∂f
∂x .

given by Dv = f (cid:48)(u)Du.

2. (Chain Rule II) The distribution derivative of v = f ◦ u for any f (x) ∈ C1(R) with f (cid:48) bounded is

Proof of Lemma 3. Without loss of generality, we ﬁrst consider 1-dimension case. Given (cid:96)(˜h) : R → R,
ξ ∼ U(0, 1), ˜h : Ω → {0, 1}. For ∀φ ∈ C∞

(cid:90)

0 (Ω), we have
D(cid:96)(˜h(x))

φ(x)

(cid:16)

(cid:17)

(cid:90)

dx = −

φ(cid:48)(x)(cid:96)(x)dx

= −

φ(cid:48)(x)(cid:96)(0)dx +

φ(cid:48)(x)(cid:96)(1)dx

(cid:19)

(cid:18)(cid:90) 0

(cid:32)

−∞

0

(cid:12)
(cid:12)
φ(x)
(cid:12)
(cid:12)

−∞

(cid:90) ∞

0
(cid:12)
∞
(cid:12)
(cid:12)
(cid:12)
0

(cid:33)

= −

(cid:96)(0) + φ(x)

(cid:96)(1)

= ((cid:96)(1) − (cid:96)(0)) φ(0)

where the last equation comes from φ ∈ C∞

0 (Ω). We obtain

D(cid:96)(˜h) = ((cid:96)(1) − (cid:96)(0))δ(h) := ∆(cid:96)(h).

We generalize the conclusion to l-dimension case with expectation over ξ, i.e., ˜h(·, ξ) : Ω → {0, 1}l, we

have the partial distributional derivative for k-th coordinate as
(cid:105)
Dk(cid:96)(˜h(z, ξ))

(cid:104)
(cid:96)(˜h(z, ξ))

= E

DkE

(cid:105)

(cid:104)

{ξi}l

i=1

{ξi}l

i=1

Therefore, we have the distributional derivative w.r.t. W as
(cid:96)(˜h(σ(W (cid:62)x), ξ))

= E

DE

(cid:105)

(cid:104)

{ξi}l

i=1

{ξi}l

i=1

= E

{ξi}l

i=1,i(cid:54)=k

(cid:104)
((cid:96)(˜h1

k) − (cid:96)(˜h0

k))

(cid:105)

.

(cid:105)

(cid:104)
Dk(cid:96)(˜h(σ(W (cid:62)x), ξ))
(cid:104)
D˜hk

chain rule I = E

(cid:105)
(cid:96)(˜h(σ(W (cid:62)x), ξ))∇W σ(W (cid:62)x)
{ξi}l
∆˜h(cid:96)(˜h(σ(W (cid:62)x), ξ))σ(W (cid:62)x) • (cid:0)1 − σ(W (cid:62)x)(cid:1) x(cid:62)(cid:105)
(cid:104)

i=1

.

= Eξ

To derive the approximation of the distributional derivative, we exploit the mean value theorem and

Taylor expansion. Speciﬁcally, for a continuous and diﬀerential loss function (cid:96)(·), there exists (cid:15) ∈ (0, 1)

Moreover, for general smooth functions, we rewrite the ∂˜hi

(cid:96)(˜h)|˜hi=(cid:15) by Taylor expansion, i.e.,

∂˜hk

(cid:96)(˜h)|˜hk=(cid:15) =

(cid:104)

(cid:105)
∆˜h(cid:96)(˜h)

.

k

∂˜hk
∂˜hk

(cid:96)(˜h)|˜hi=(cid:15) = ∂˜hk
(cid:96)(˜h)|˜hi=(cid:15) = ∂˜hk

(cid:96)(˜h)|˜hi=1 + O((cid:15))
(cid:96)(˜h)|˜hi=0 + O((cid:15)).

we have an approximator as

(cid:96)(˜h)|˜hk=1 + (1 − σ(w(cid:62)
Plugging into the distributional derivative estimator (7), we obtain a simple biased gradient estimator,

(cid:96)(˜h)|˜hk=(cid:15) ≈ σ(w(cid:62)

(cid:96)(˜h)|˜hk=0 = Eξ

k x))∂˜hk

k x)∂˜hk

∂˜hk

.

(13)

(cid:104)
∇˜h(cid:96)(˜h, ξ)

(cid:105)

DW ˜H(Θ; x) ≈ ˜DW ˜H(Θ; x) := Eξ

(cid:104)
∇˜h(cid:96)(˜h(σ(W (cid:62)x), ξ))σ(W (cid:62)x) • (1 − σ(W (cid:62)x))x(cid:62)(cid:105)

.

(14)

15

B Convergence of Distributional SGD

Lemma 8 (Ghadimi and Lan, 2013) Under the assumption that H is L-Lipschitz smooth and the variance of
the stochastic distributional gradient (8) is bounded by σ2, the proposed distributional SGD outputs {Θi}t
i=1,
2(cid:21)

(cid:18)

(cid:19)

t
(cid:88)

i=1

γi −

L
2

γ2
i

E

(cid:20)(cid:13)
(cid:13)∇Θ ˜H(Θi)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:54) ˜H(Θ0) − ˜H(Θ∗) +

Lσ2
2

t
(cid:88)

i=1

γ2
i ,

where Θt = {Wt, Ut, βt, ρt}.

Proof of Theorem 5. Lemma 8 implies that by randomly sampling a search point ΘR with probability
P (R = i) = 2γi−Lγ2

√

i

i=1, we have

(cid:80)t

i=1 2γi−Lγ2
i

where γi ∼ O (cid:0)1/
(cid:20)(cid:13)
(cid:13)∇Θ ˜H(ΘR)
(cid:13)

E

t(cid:1) from trajectory {Θi}t
(cid:19)
2(cid:21)

∼ O

(cid:13)
(cid:13)
(cid:13)

(cid:18) 1
√
t

.

Lemma 9 Under the assumption that the variance of the approximate stochastic distributional gradient (10)
is bounded by σ2, the proposed distributional SGD outputs {Θi}t

i=1 such that

t
(cid:88)

i=1

γiE

(cid:105)
(cid:104)
(Θi − Θ∗)(cid:62) ˜∇Θ ˜H(Θi)

E

(cid:107)Θ0 − Θ∗(cid:107)2(cid:105)
(cid:104)

+

(cid:32)

(cid:54) 1
2

(cid:33)

i σ2
γ2

,

t
(cid:88)

i=1

where Θ∗ denotes the optimal solution.

Proof Denote the optimal solution as Θ∗, we have

(cid:107)Θi+1 − Θ∗(cid:107)2 =

(cid:13)
(cid:13)
2
(cid:13)Θi − γi (cid:98)˜∇Θ ˜H(Θi, xi) − Θ∗)
(cid:13)
(cid:13)
(cid:13)

= (cid:107)Θi − Θ∗(cid:107)2 + γ2
i
Taking expectation on both sides and denoting aj = (cid:107)Θj − Θ∗(cid:107)2, we have

(cid:13)
(cid:13)
2
(cid:98)˜∇Θ ˜H(Θi, xi)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

− 2γi (Θi − Θ∗)(cid:62) (cid:98)˜∇Θ ˜H(Θi, xi).

E [ai+1] (cid:54) E [ai] − 2γiE

(cid:104)

(cid:105)
(Θi − Θ∗)(cid:62) ˜∇Θ ˜H(Θi)

+ γ2

i σ2.

Therefore,

γiE

(cid:104)

(cid:105)
(Θi − Θ∗)(cid:62) ˜∇Θ ˜H(Θi)

t
(cid:88)

i=1

(cid:32)

(cid:54) 1
2

(cid:33)

E [a0] +

i σ2
γ2

.

t
(cid:88)

i=1

Proof of Theorem 6. The lemma 9 implies by randomly sampling a search point ΘR with probability
P (R = i) = γi

where γi ∼ O (cid:0)1/

√

(cid:80)t

i=1 γi

(cid:104)

E

(ΘR − Θ∗)(cid:62) ˜∇Θ ˜H(ΘR)

t(cid:1) from trajectory {Θi}t
(cid:107)Θ0 − Θ∗(cid:107)2(cid:105)
(cid:104)
2 (cid:80)t

(cid:54)

E

(cid:105)

i=1 γi

i=1, we have
+ (cid:80)t

i=1 γ2

i σ2

∼ O

(cid:19)

.

(cid:18) 1
√
t

C More Experiments

C.1 Convergence of Distributional SGD and Reconstruction Error Comparison

We shows the reconstruction error comparison between ITQ and SGH on MNIST and GIST-1M in Figure 4.
The results are similar to the performance on SIFT-1M. Because SGH optimizes a more expressive objective
than ITQ (without orthogonality) and do not use alternating optimization, it ﬁnd better solution with lower
reconstruction error.

16

(a) MNIST

(b) GIST-1M

Figure 4: L2 reconstruction error convergence on MNIST and GIST-1M of ITQ and SGH over the course of
training with varying of the length of the bits (8, 16, 32, 64, respectively). The x-axis represents the number
of examples seen by the training algorithm. For ITQ, it sees the training dataset once in one iteration.

C.2 Training Time Comparison

(a) MNIST

(b) GIST-1M

Figure 5: Training time comparison between BA and SGH on MNIST and GIST-1M.

We shows the training time comparison between BA and SGH on MNIST and GIST-1M in Figure 5. The
results are similar to the performance on SIFT-1M. The proposed distributional SGD learns the model much
faster.

C.3 More Evaluation on L2NNS Retrieval Tasks

We also use diﬀerent RecallK@N to evaluate the performances of our algorithm and the competitors. We ﬁrst
evaluated the performance of the algorithms with Recall 1@N in Figure 6. This is an easier task comparing
to K = 10. Under such measure, the proposed SGH still achieves the state-of-the-art performance.

In Figure 7, we set K, N = 100 and plot the recall by varying the length of the bits on MNIST, SIFT-1M,
and GIST-1M. This is to show the eﬀects of length of bits in diﬀerent baselines. Similar to the Recall10@N,
the proposed algorithm still consistently achieves the state-of-the-art performance under such evaluation
measure.

17

D Stochastic Generative Hashing For Maximum Inner Product

Search

In Maximum Inner Product Search (MIPS) problem, we evaluate the similarity in terms of inner product
which can avoid the scaling issue, i.e., the length of the samples in reference dataset and the queries may
vary. The proposed model can also be applied to the MIPS problem. In fact, the Gaussian reconstruction
model also preserve the inner product neighborhoods. Denote the asymmetric inner product as x(cid:62)U hy, we
claim
Proposition 10 The Gaussian reconstruction error is a surrogate for asymmetric inner product preserva-
tion.
Proof We evaluate the diﬀerence between inner product and the asymmetric inner product,

(cid:107)x(cid:62)y − x(cid:62)U (cid:62)hy(cid:107)2 = (cid:107)x(cid:62) (cid:0)y − U (cid:62)hy

(cid:1) (cid:107)2 (cid:54) (cid:107)x(cid:107)2(cid:107)y − U (cid:62)hy(cid:107)2,

which means minimizing the Gaussian reconstruction, i.e., − log p(x|h), error will also lead to asymmetric
inner product preservation.
We emphasize that our method is designed for hashing problems primarily. Although it can be used for
MIPS problem, it is diﬀerent from the product quantization and its variants whose distance are calculated
based on lookup table. The proposed distributional SGD can be extended to quantization. This is out of
the scope of this paper, and we will leave it as the future work.

D.1 MIPS Retrieval Comparison

To evaluate the performance of the proposed SGH on MIPS problem, we tested the algorithm on WORD2VEC
dataset for MIPS task. Besides the hashing baselines, since KMH is the Hamming distance generalization of
PQ, we replace the KMH with product quantization (Jegou et al., 2011). We trained the SGH with 71,291
samples and evaluated the performance with 10,000 query. Similarly, we vary the length of binary codes from
16, 32 to 64, and evaluate the performance by Recall 10@N. We calculated the ground-truth via retrieval
through the original inner product. The performances are illustrated in Figure 8. The proposed algorithm
outperforms the competitors signiﬁcantly, demonstrating the proposed SGH is also applicable to MIPS task.

E Generalization

We generalize the basic model to translation and scale invariant extension, semi-supervised extension, as
well as coding with h ∈ {−1, 1}l.

E.1 Translation and Scale Invariant Reduced-MRFs

As we known, the data may not zero-mean, and the scale of each sample in dataset can be totally diﬀerent.
To eliminate the translation and scale eﬀects, we extend the basic model to translation and scale invariant
reduced-MRFs by introducing parameter α to separate the translation eﬀect and the latent variable z to
model the scale eﬀect in each sample x, therefore, the potential function becomes
1
2ρ2 (x − α − U (cid:62)(z · h))(cid:62)(x − α − U (cid:62)(z · h)),

E(x, h, z) = −β(cid:62)h +

(15)

where · denotes element-wise product, α ∈ Rd and z ∈ Rl. Comparing to (2), we replace U (cid:62)h with
U (cid:62)(z · h) + α so that the translation and scale eﬀects in both dimension and sample are modeled explicitly.
We treat the α as parameters and z as latent variable. Assume the independence in posterior for
computational eﬃciency, we approximate the posterior p(z, h|x) with q(h|x; Wh)q(z|x; Wz), where Wh, Wz
denotes the parameters in the posterior approximation. With similar derivation, we obtain the learning

18

Obviously, the proposed distributional SGD is still applicable to this optimization.

Eq(h|xi)q(z|xi) [−E(x, h, z) − log q(h|xi) − log q(z|xi)] .

(16)

objective as

max
U,α,β,ρ;Wh,Wz

1
N

N
(cid:88)

i=1

E.2 Semi-supervised Extension

Although we only focus on learning the hash function in unsupervised setting, the proposed model can be
easily extended to exploit the supervision information by introducing pairwise model, e.g., (Zhang et al.,
2014a; Zhu et al., 2016). Speciﬁcally, we are provided the (partial) supervision information for some pairs
of data, i.e., S = {xi, xi, yij}M

i,j, where
(cid:40)

yij =

1
0

if xi ∈ N N (xj) or xj ∈ N N (xi)
o.w.

,

and N N (x) stands for the set of nearest neighbors of x. Besides the original Gaussian reconstruction model
in the basic model in (2), we introduce the pairwise model p(yij|hi, hj) = B(σ(h(cid:62)
i hj)) into the framework,
which results the joint distribution over x, y, h as

p(xi, xj, hi, hj, yij) = p(xi|hi)p(xj|hj)p(hi)p(hj)p(yij|hi, hj)1S (ij),

where 1S (ij) is an indicator that outputs 1 when (xi, xj) ∈ S, otherwise 0. Plug the extended model into
the Helmholtz free energy, we have the learning objective as,

max
U,β,ρ;W

1
N 2

N 2
(cid:88)

i,j=1

(cid:16)

Eq(hi|xi)q(hj |xj ) [log p(xi, xj, hi, hj)] + Eq(hi|xi)q(hj |xj ) [1S (ij) log p(yij|hi, hj)]

−Eq(hi|xi)q(hj |xi) [log q(hj|xj)q(hj|xi)]

(cid:17)

,

Obviously, the proposed distributional SGD is still applicable to the semi-supervised extension.

E.3 {±1}-Binary Coding

In the main text, we mainly focus on coding with {0, 1}. In fact, the proposed model is applicable to coding
with {−1, 1} with minor modiﬁcation. Moreover, the proposed distributional SGD is still applicable. We
only discuss the basic model here, the model can also be extended to scale-invariant and semi-supervised
variants.

If we set h ∈ {−1, 1}l, the potential function of basic reduced-MRFs (2) does not have any change, i.e.,

E(x, h) = −β(cid:62)h +

(cid:0)x(cid:62)x + h(cid:62)U (cid:62)U h − 2x(cid:62)U h(cid:1) .

1
2ρ2

We need to modify the parametrization of q(h|x) as

l
(cid:89)

q(h|x) =

σ(w(cid:62)

i x)

1+hi
2

(cid:0)1 − σ(w(cid:62)

i x)(cid:1) 1−hi

2

.

i=1
Therefore, the stochastic neuron becomes

With similar derivation, we have the distributional derivative of the objective w.r.t. W as

where [∆f (cid:96)(f (z, ξ))]k = (cid:96)(f 1

k ) − (cid:96)(f −1

k ). Furthermore, we have a similar biased gradient estimator as

Plug these modiﬁcation into the model and algorithm, we can learn a {−1, 1}-encoding function.

f (z, ξ) :=

(cid:40)
1
−1

if σ(z) (cid:62) ξ
if σ(z) < ξ

.

∇W Lsn = Eξ

(cid:2)∆f (cid:96)(f (z, ξ))∇zσ(z)x(cid:62)(cid:3) ,

˜∇W Lsn = Eξ

(cid:2)∇f (cid:96)(f (z, ξ))∇zσ(z)x(cid:62)(cid:3) .

19

(17)

(18)

(19)

(20)

Figure 6: L2NNS comparison on MNIST, SIFT-1M, SIFT-1B, and GIST-1M with the length of binary bits from
16 to 64. We evaluate the performance with Recall 1@M , where M increasing to 1000.

20

(a) L2NNS on MNIST

(b) L2NNS on SIFT-1M

(c) L2NNS on GIST-1M

Figure 7: L2NNS comparison on MNIST, SIFT-1M, and GIST-1M with Recall 100@100 for the length of bits
from 8 to 64.

Figure 8: MIPS comparison on WORD2VEC with the length of binary bits from 16 to 64. We evaluate the
performance with Recall 10@M , where M increasing to 1000.

21

