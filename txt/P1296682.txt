6
1
0
2
 
v
o
N
 
5
 
 
]

V
C
.
s
c
[
 
 
1
v
2
4
6
1
0
.
1
1
6
1
:
v
i
X
r
a

This space is reserved for the Procedia header, do not use it

GPU-based pedestrian detection for autonomous driving

V. Campmany1,2, S. Silva1,2, A. Espinosa1, J.C. Moure1
, D. V´azquez2, and A. M. L´opez2

1 Universitat Autonoma de Barcelona, Spain.
2 Computer Vision Centre (CVC), Spain.

Abstract
We propose a real-time pedestrian detection system for the embedded Nvidia Tegra X1 GPU-
CPU hybrid platform. The pipeline is composed by the following state-of-the-art algorithms:
Histogram of Local Binary Patterns (LBP) and Histograms of Oriented Gradients (HOG) fea-
tures extracted from the input image; Pyramidal Sliding Window technique for candidate gen-
eration; and Support Vector Machine (SVM) for classiﬁcation. Results show a 8x speedup
in the target Tegra X1 platform and a better perf ormance/watt ratio than desktop CUDA
platforms in study.

Keywords: Autonomous Driving, Pedestrian detection, Computer Vision, CUDA, Massive Parallelism

Introduction

1
Autonomous driving will improve safety, reduce pollution and congestion, and enable handi-
capped people, elderly persons and kids to have more freedom. It requires perceiving and un-
derstanding the vehicle environment (road, traﬃc signs, pedestrians, vehicles ...) using sensors
(cameras, LIDARs, sonars, and radar), providing self-localization (with GPS, inertial sensors
and visual localization in precise maps), controlling the vehicle, and planning the routes. The
underlying algorithms are high computationally demanding and require real-time response.

A pedestrian detector that locates humans on a digital image is a key module for robotics
application and autonomous vehicles. The wide variation by which humans appear with diﬀer-
ent poses, clothes, illuminations and backgrounds makes the problem one of the hardest within
the computer vision ﬁeld, and the source of an active research during the last twenty years
[1, 2, 3, 4].

The real-time constraints of most applications of pedestrian detection are tight, and cur-
rently not attainable by general-purpose processors. Attaching a desktop GPU for performance
acceleration is expensive in terms of space and power consumption. The recent appearance
of embedded GPU-accelerated systems based on the Nvidia’s Tegra X1 ARM processor, like
the Jetson TX1 and DrivePX platforms, represents a promising approach for low-cost and
low-consumption real-time pedestrian detection.

We have designed a complete GPU-accelerated pedestrian detection pipeline1 based on [4].
We have analyzed the alternative parallelization schemes and data layouts of the underlying

1https://github.com/vcampmany/CudaVisionSysDeploy

1

GPU-based pedestrian detection for autonomous driving

Victor Campmany et. al.

algorithms and selected general and scalable solutions that do not sacriﬁce performance nor
detection accuracy. We have evaluated three pedestrian detection versions on desktop and
embedded platforms and proved that: (1) real-time (20 images of 1242×375 pixels per sec-
ond) can be reached on an embedded GPU-accelerated system with state-of-the-art accuracy;
(2) GPU-acceleration provides between 8x and 60x performance speedup with respect to a
baseline multi-core CPU implementation; and (3) the Tegra X1 processor at least doubles the
performance per watt of the system accelerated by a GTX 960 GPU.

The rest of the paper is organized as follows. Section 2 introduces the related work. Section
3 describes the baseline pedestrian detector. In section 4 we analyze each algorithm and discuss
diﬀerent parallelization schemes. Finally, section 5 provides the obtained results and section 6
summarizes the work.
2 Related work
Computer Uniﬁed Device Architecture (CUDA) is a platform created by Nvidia to develop
general purpose applications for the GPUs. Nvidia GPUs are composed by tens of processing
units called Streaming Multiprocessor (SMs). The SMs share a L2 cache and an external global
memory. Each SM has a shared memory that is managed explicitly and a L1 cache. A CUDA
kernel is composed by thousands of threads executing the same program with diﬀerent data.
The threads are divided into groups of up to 1024 threads called Cooperative Thread Arrays
(CTAs). The threads in a CTA collaborate using the on-chip shared memory. Each CTA is
divided into batches of 32 threads called warps. Finally, individual threads have a reserved
memory region in each layer of the memory hierarchy called local memory.

A critical performance issue is the memory access pattern of the algorithm. GPUs achieve
full memory performance when the memory accesses are coalesced. Coalesced memory access
refers to combining multiple memory operations into a single memory transactions. To achieve
coalescing, the 32 threads of the warp must access consecutive memory addresses. The pre-
viously described constrains data layout, memory transfers and work distribution become key
factors in order to achieve the best performance.

Since the appearance of GPGPU computational platforms, several object detection algo-
rithms have been ported to the GPU. There are also examples of using Field Programmable
Gate Array (FPGA) designs, obtaining outstanding results
[5]. In comparison, the reduced
development costs of the CUDA programming environment and the aﬀordable desktop CUDA
enabled GPU cards make them more suitable for testing new algorithms.

Works such as [6] assert that exploiting the massively parallel paradigm for object detection
algorithms outperforms a highly tuned CPU version [7]. Previously related researches like
[8, 9, 10] developed a GPU object detector using the well-known HOG-SVM approach obtaining
a performance boost.

In the previous work, the evaluations use desktop GPUs to evaluate the algorithms proposed
as a ﬁrst step to evaluate the GPU as a suitable target platform. In this work, we propose a
real-time pedestrian detector running on a low-consumption GPU devices like the Tegra X1
platform. We also present for the ﬁrst time a GPU implementation of the HOGLBP-SVM
detection pipeline [11].
3 Pedestrian detection
Any computer vision-based pedestrian detector is mainly composed by four core modules: the
candidate generation, the feature extraction, the classiﬁcation and the reﬁnement. The can-
didate generation provides rectangular image windows which eventually contain pedestrians.
These windows are described using distinctive patches in the feature extraction stage. During
the classiﬁcation stage the windows are labeled using a learned model accordingly to its fea-

2

GPU-based pedestrian detection for autonomous driving

Victor Campmany et. al.

tures. Finally, as a pedestrian could be detected by several windows, they are ﬁnally managed
in the reﬁnement stage.

For the candidate generation we use the Pyramidal Sliding Window (SW) method. To
extract the features of the images we use Local Binary Patterns (LBP) and Histograms of
Oriented Gradients (HOG). A Support Vector Machine (SVM) is taken as classiﬁer, and, ﬁnally
the reﬁnement is performed by the Non-maximum Suppression algorithm [12].

4 Design and Analysis of Massively-Parallel Algorithms
We have implemented three diﬀerent detection pipelines combining the basic algorithms men-
tioned in section 3 . In this section we present the algorithms and the decisions behind their
massively-parallel implementations on a CUDA architecture. We start describing the general
detection pipelines and the design methodology, and then delve into the details of each algo-
rithm.
4.1 Overview of the Detection Pipelines
The three detection pipelines considered in this work, ordered from lower to higher accuracy
and computational complexity, are LBP-SVM, HOG-SVM and HOGLBP-SVM. They represent
three realistic options for an actual detection system, where one of them has to trade oﬀ
functionality with processing rate. The pipelines follow the same scheme; diﬀerences appear on
the feature extraction stage: LBP-SVM uses LBP, while HOG-SVM takes advantage of HOG.
The HOGLBP-SVM pipeline combines both algorithms. As shown in previous research [11],
concatenating HOG and LBP feature vectors gives an signiﬁcant increase of performance. The
hybrid processing pipeline is designed to use the CPU (Host) and the GPU (Device): (1) the
captured images are copied from the Host memory space to the Device; (2) the scaled-pyramid
of images is created; (3) features are extracted from each pyramid layer; (4) every layer is
segmented into windows to be classiﬁed; (5) detection results are copied to the CPU memory
to execute the Non-maximum Suppression algorithm that reﬁnes the results.

4.2 Histograms of Local Binary Patterns (LBP)
LBP is a feature extraction method that gives information of the texture on a chunk of the
image. The process can be divided into two steps: the LBP Map computation and the LBP
Histograms computation.

The LBP Map [13] is computationally classiﬁed as a 2-dimensional Stencil pattern algo-
rithm. The central pixel is compared with each of its nearest neighbors; if the value is lower
than the center a 0 is stored, otherwise, a 1 is stored. Then, this binary code is converted to
decimal to generate the output pixel value.

Algorithm 1: Massively parallel computation of the LBP map

input : I[H][W]
output: LBP[H][W]

1 parallel for y=0 to H and x=0 to W do
2

LBP[y][x] = LBPf(y, x);

Finally, we extract the image features by computing the LBP Histograms. Histograms of
blocks of 16×16 pixels are computed over the LBP image. The histograms have a 50% overlap in
the X and Y axis meaning that each region will be redundantly computed 4 times. We avoided
the redundant computing of the overlapped descriptors by splitting the Block Histograms into
smaller Cell Histograms of 8 × 8 pixels. Then, these partial histograms are reduced in groups
of four (histogram reduction) to generate the output block histograms. Figure 1 shows the
previously described sequence of operations to compute the LBP.

3

GPU-based pedestrian detection for autonomous driving

Victor Campmany et. al.

Figure 1: LBP: (1) compute LBP value for each pixel (Algorithm 1), (2) compute 8 × 8 pixels
Cell Histograms (Algorithm 2), (3) compute 16 × 16 pixels block histograms (Algorithm 3).

4.2.1 Analysis of parallelism & CUDA mapping for computing LBP
We implemented the 2-dimensional Stencil pattern by mapping each thread to one output pixel
(Algorithm 1). With this work distribution there are no data dependences among threads.
Each thread performs 9 reads (the central value and the eight neighbors) and 1 store and, the
memory accesses are coalesced.

We designed two diﬀerent solutions to compute the LBP histograms; the ﬁrst, is a straight-
forward parallelization without thread collaboration (Na¨ıve scheme); the second, with thread
collaboration, is designed to be more scalable (Scalable scheme).

The Na¨ıve scheme follows a Map pattern: each thread generates a Cell Histogram. The
histogram reduction is performed in the same way: each thread is mapped to a Block Histogram
and the thread performs the histogram reduction.

The Scalable scheme solution used by our system aims for an eﬃcient memory access and
data reutilization. Each thread is mapped to an input pixel of the image, and using atomic
operations each thread adds to its corresponding Cell Histogram (Scatter pattern, see Algorithm
2). To generate the Block Histograms every histogram reduction is performed by a warp (see
Algorithm 3). With this design we attain coalesced memory access which leads to an scalable
algorithm for diﬀerent image sizes.

Algorithm 2: Scalable computation scheme of the Cell Histograms. Each thread reads a
pixel and updates the corresponding cell. We use atomic operations (Read-Add-Store) to
avoid data races. S ⇐ histogram.

input : LBP[H][W]
output: CH[H/8][W/8][S]

1 parallel for y=0 to H and x=0 to W do
2

bin = LBP[y][x];
atomicAdd (CH[y/8][x/8][bin], 1) ;

3

4.3 Histogram of Oriented Gradients (HOG)
The method of Histograms of Oriented Gradients [14] counts the occurrence of gradient ori-
entation on a chunk of the image. The process could be divided into two steps: Gradient
computation and the Histograms computation.

Gradient computation is used to measure the directional change of color in an image. The
algorithm follows a 2 dimensional Stencil pattern. The gradient of a pixel has two components,
the magnitude (ω) and the orientation (θ). The orientation is the directional change of color
and the magnitude gives us information of the intensity of the change.

4

GPU-based pedestrian detection for autonomous driving

Victor Campmany et. al.

Algorithm 3: Scalable computation of the histogram reduction to generate the Block
Histograms (Fa) hReduction: Hb ⇐ H/8 − 1; W b ⇐ W/8 − 1.

input : CH[H/8][W/8][S]
output: Fa[Hb][Wb][S]

1 parallel for y=0 to Hb and x=0 to Wb do
2

SIMD parallel for lane=0 to WarpSize do

3

4

5

6

t = lane;
while t<S do

Fa[y][x][t] = hReduction(t);
t = t + WarpSize;

Figure 2: Steps to compute the HOG features: (1) given a grayscale image, compute the gradient
(Algorithm 4); (2) compute the Block Histograms with trilinear interpolation (Algorithm 5).

Histograms are computed by splitting the Gradient image into blocks of 16 × 16 pixels with
50% overlap in X and Y axis (the same conﬁguration as the LBP Histograms). In this case,
because of histograms trilinear interpolation we can not compute 8×8 pixels Cell Histograms and
then carry out the histogram reduction. Trilinear interpolation is used to avoid sudden changes
in the Block Histograms vector (aliasing eﬀect) [14]. Each Block Histogram is composed by
four concatenated 8 × 8 pixels Cell Histograms. Diﬀerent bins of the Block Histogram receive a
weighted value of the orientation (θ) multiplied by the magnitude of the gradient (ω). Depending
on the pixel coordinates, each input value could be binned into two, four or eight bins of the
Block Histogram. The sequence of steps to compute the HOG features is described in Figure
2.

4.3.1 Analysis of parallelism & CUDA mapping for computing HOG

The gradient computation kernel follows a Stencil pattern: individual threads are mapped to
each output pixel (Algorithm 4). Single threads perform 4 reads and 1 store, and coalesced
memory accesses are achieved.

The Histograms computation has been parallelized assigning each thread to the computation
of one histogram (Large-grain task parallelism, see Algorithm 5). With this structure there is
no collaboration among threads and memory accesses are not coalesced, though, the mapping
avoids the use of the costly atomic memory operations.

We implemented three diﬀerent kernels following the scheme in Algorithm 5. The ﬁrst
stores the data in global memory (HOG Global ). To reduce the latency of the global memory
we designed two more kernels: one stores the histograms in local memory, taking advantage of
the L1 cache (HOG Local ) and the other uses the on-chip shared memory (HOG Shared ). In
section 5.3 we will discuss the results of the implementations.

5

GPU-based pedestrian detection for autonomous driving

Victor Campmany et. al.

Algorithm 4: Massively parallel computation of the Gradient

input : I[H][W]
output: M[H][W], O[H][W]

1 parallel for y=0 to H and x=0 to W do
2

dx = I[y][x-1] - I[y][x+1];
dy = I[y-1][x] - I[y+1][x];
M[y][x] = sqrt(dx ∗ dx, dy ∗ dy);
O[y][x] = arctan(dx, dy);

3

4

5

Algorithm 5: Massively parallel computation of the HOG Histograms.

input : M[H][W], O[H][W]
output: Fb[Hb][Wb][S]

1 parallel for y=0 to Hb and x=0 to Wb do
2

for i=0 to 16 do

3

4

for j=0 to 16 do

updateBlockHistogram(Fb[y][x], i, j);

4.4 Pyramidal Sliding Window & Support Vector Machine
Pyramidal Sliding Window creates multiple downscaled copies of the input images to detect
pedestrians of various sizes and at diﬀerent distances. Then, every copy is split into highly
overlapped regions of 128 × 64 pixels, called windows. Each window is described with a feature
vector ((cid:126)x). The vector is composed by the concatenation of the Block Histograms (computed
with HOG and LBP) enclosed in the given region. Then, every vector is evaluated to predict if
the region contains a pedestrian or not.

Support Vector Machine (SVM) is a supervised learning method that is able to discriminate
two categories, in our case pedestrians from background [15]. After training the SVM, we
obtain a model that performs as an n-dimensional plane that distinguishes pedestrians from
background. The SVM training is done oﬄine. However, the SVM inference is done online.
SVM gets as input a feature vector ((cid:126)x) and computes its distance to the model hyper-plane
((cid:126)ω). This distance is computed with the dot product operation. Then, the window is classiﬁed
as pedestrian if the distance is greater than a given threshold and as background otherwise.
Figure 3 shows the steps needed to evaluate each window taking HOG and LBP features as the
image descriptors.
4.4.1 Analysis of parallelism & CUDA mapping for SW-SVM
We ﬁrst implemented a na¨ıve version (Na¨ıve SVM ) with a Large-grain task parallelism and
no thread collaboration. Each thread is responsible of the computation of the dot product
between the candidate window ((cid:126)x) and the model ((cid:126)ω). The approach was not scalable and
became critical as it is the kernel with the largest workload of the pipeline.

To eﬃciently compute the dot product we designed a CUDA kernel where each warp is
assigned to a window ((cid:126)x) of the transformed image (Warp-level SVM ). The computation of the
dot product is divided between the threads in the warp. Once intra-warp threads have computed
the partial results, these are communicated among threads using register shuﬄe instructions
and then reduced. Algorithm 6 shows the mapping of the Sliding Window and SVM inference
to the massive parallel architecture.

We decided to use a warp-level approach to avoid the overhead of thread synchronization

6

GPU-based pedestrian detection for autonomous driving

Victor Campmany et. al.

Figure 3: (1) Sliding Window and SVM inference of the HOG and LBP features. Each window
is evaluated with Algorithm 6 computing the distance of the HOG+LBP features from the SVM

as warps have implicit hardware synchronization. This conﬁguration allows the full utilization
of the memory bandwidth as the memory accesses are coalesced.

Algorithm 6: Massively parallel computation of the Sliding Window and the SVM in-
ference. N is the SVM trained model, Hn and Wn are the number of Block Histograms
ﬁtting in a window and S is the histogram size. Hb ⇐ H/8 − 1; W b ⇐ W/8 − 1.

input : Fa[Hb][Wb][S], Fb[Hb][Wb][S] N[Hn][Wn][S]
output: scores[Y][X]

1 parallel for y=0 to Y and x=0 to X do
2

SIMD parallel for lane=0 to WarpSize do

3

4

5

6

7

8

9

10

11

12

t = lane;
for i=0 to Hm do

for j=0 to Wm do
while t<S do

res += Fa[i+y][j+x][t] ∗ N[i][j][t];
res += Fb[i+y][j+x][t] ∗ N[i][j][t];
t = t + WarpSize;

res = SIMDreductionSum(res);
if lane == 0 then

scores[y][x] = res;

5 Experiments & Results
In this section we present the obtained performance results for the algorithms and pipelines. All
the experiments are run with an Intel i7-5930K processor, and both Nvidia GPUs: GTX 960
and a Tegra X1. We start by showing the whole pipeline results. We measure the performance
of each of the algorithms by measuring the number of processed pixels per nanosecond (px/ns);
we will also refer to it as Performance.
5.1 Detection Pipelines performance
To evaluate the P erf ormance we use a video sequence with an image size of 1242 × 375 pixels.
Figure 5 presents the performance results of the LBP-SVM, HOG-SVM and HOGLBP-SVM
pipelines, measured in processed frames per second (FPS). Results show the achieved FPS for
the multithreaded CPU baseline application [4] and the GPU accelerated versions. Results
prove that we have accomplished the objective of running the application in real-time under
the low consumption ARM platform.

7

GPU-based pedestrian detection for autonomous driving

Victor Campmany et. al.

Figure 4: Legend values are the area below the
curve. Must be minimized in reliable detectors

Performance of

Figure 5:
the detection
pipelines measured in processed frames per
second (FPS) in the diﬀerent architectures.

Figure 4 illustrates the miss rate depending on the false positive per image (FPPI). FPPI
is the number of candidate windows wrongly classiﬁed as pedestrians, it can be understood as
the tolerance of the system. As the FPPI increases, the miss rate decreases leading to a more
tolerant system.

The system is able to achieve state of the art accuracy with the HOGLBP-SVM detection
pipeline. The rest have slightly lower accuracy; nonetheless, they demand less computational
power to achieve real-time performance which make them suitable for less powerful GPUs.

5.2 Detection Pipeline eﬃciency
Besides needing real time processing, the application demands low power consumption. To
compare the power eﬃciency of the two GPUs we introduce a new metric: F P S/W att. To assess
the Watt consumption we use the Thermal Design Power (TDP ). TDP is the amount of heat
generated by the processor in typical use cases; the attribute is provided by the manufacturer
company.

Table 1 shows the TDP of each platform and the obtained eﬃciency. We expose the results
of the HOGLBP-SVM pipeline as it is the pipeline that reaches state of the art accuracy. The
Tegra X1 platform outperforms the CPU and GTX 960 implementations. It gives an increment
of 200 points compared to the CPU application. It also doubles the eﬃciency of the GTX 960
desktop GPU.

HOGLBP-SVM TDP F P S/W att
i7-5930K
GTX 960
Tegra X1

0.01
0.99
2

140
120
10

Table 1: TDP of each platform and eﬃciency of the HOGLBP-SVM pipeline measured in
F P S/W att.

5.3 Feature extraction & classiﬁcation algorithms
In this section we present the performance results of the individual algorithms that form the
three pipelines. Figures 6, 7, 8 show the obtained performance results measured in px/ns with
various image sizes.

8

GPU-based pedestrian detection for autonomous driving

Victor Campmany et. al.

For the LBP algorithm we experimented with the two developed designs (Na¨ıve and Scalable
schemes). As the image size increases, the Na¨ıve scheme suﬀers a decrease of performance caused
by the poor memory management. However, the Scalable scheme performance is more regular
because of the coalesced memory access (see Figure 6). For the two biggest images it attains a
2.5x speed-up in both GPUs compared to the Na¨ıve kernel. Although, for the smaller images
the boost is low; the non-coalesced memory access is attenuated by the high utilization of the
L2 cache which hides the latency of the memory operations (Figure 6).

Figure 7 expose the attained results for the HOG algorithm. In this case, we analize the
three developed solutions (HOG Global, HOG Local and HOG Shared). Results prove that
the ﬁrst massively parallel implementation was not competitive with a CPU version; for most
of the experiments in Figure 7, the CPU outperforms the performance of the HOG Global
kernel on both GPUs. However, if we consider the best implementation (HOG Shared) we can
conclude that the performance the GPUs surpass the CPU; the GTX 960 has a speed-up of
4x and the Tegra X1 18x. The diﬀerence in speed-up between devices is caused by the lower
memory bandwidth of the Tegra X1. The HOG Shared kernel reduces the usage of the global
memory as it stores the data in the shared memory leading to a less stressed memory in the
Tegra ARM system.

The Pyramidal Sliding Window and Support Vector Machine is the most time consuming
part of the pipeline it takes up to 55% of the time to process an image. Figure 8 shows the
results for the Na¨ıve and Warp-level approaches. The Na¨ıve kernel suﬀers from non-coalesced
memory access. It becomes a critical issue in the Tegra X1 as performance is equivalent to the
CPU one. The Warp-level design the memory bandwidth is used at peak performance because
of coalesced memory access. We obtain a boost of 10x compared to the Na¨ıve kernel on the
GTX960 card and 8x speed-up in the Tegra X1 system. Despite, the Warp-level kernel suﬀers a
decrease of performance for the bigger images, in this cases, the data needs to be fetched from
the global memory because it does not ﬁt in the L2 cache.

Figure 6: Performance of the LBP algorithm.

Figure 7: Performance of HOG algorithm.

6 Conclusions
Our experiments conﬁrm the importance of adapting the problem to the GPU architecture.
Smart work distribution and thread collaboration are key factors to attain signiﬁcant speed-ups
compared to a high end CPU. The stated facts become even more critical when the development
is done under a low consumption platform like the Tegra X1 processor. For the developed
algorithms the P erf ormance/W att of the Tegra X1 doubles the GTX 960. The evidence
determines that the Tegra ARM platform is an energy eﬃcient system able to challenge the
desktop GPU performance when running massively parallel applications.

9

GPU-based pedestrian detection for autonomous driving

Victor Campmany et. al.

Figure 8: Performance of the SW-SVM method.

In this work we show a massively parallel implementation of a pedestrian detector that uses
LBP and HOG as features and SVM for classiﬁcation. Our implementation is able to achieve
the real-time requirements on the autonomous driving platform, the Nvidia DrivePX.

7 Acknowledgements
This research has been supported by the MICINN under contract number TIN2014-53234-
C2-1-R. By the MEC under contract number TRA2014-57088-C2-1-R, the spanish DGT
project SPIP2014-01352, and the Generalitat de Catalunya projects 2014-SGR-1506 and 2014-
SGR1562. We thank Nvidia for the donation of the systems used in this work.

References

[1] D. Geronimo, A. M. Lopez, A. D. Sappa, and T. Graf. Survey of pedestrian detection for advanced

driver assistance systems. In PAMI, 2010.

[2] D. M. Gavrila. The visual analysis of human movement: A survey. In CVIU, 1999.
[3] Dollar, Wojek, Schiele, and Perona. Pedestrian detection: An evaluation of the state of the art.

[4] J. Marin, D. Vazquez, A. M. Lopez, J. Amores, and B. Leibe. Random forests of local experts for

In PAMI, 2012.

pedestrian detection. In ICCV, 2013.

[5] M. Hahnle, F. Saxen, M. Hisung, U. Brunsmann, and K. Doll. FPGA-based real-time pedestrian

detection on high-resolution images. In CVPR, 2013.

[6] R. Benenson, M. Mathias, R. Timofte, and L. Van Gool. Pedestrian detection at 100 frames per

second. In CVPR, 2012.

[7] P. Dollar, Belongie, and Perona. The fastest pedestrian detector in the west. In BMVC, 2010.
[8] C. Wojek, G. Dork, A. Schulz, and B. Schiele. Sliding-windows for rapid object class localization:

A parallel technique. In Pattern Recognition, 2008.

[9] Zhang and Nevatia. Eﬃcient scan-window based object detection using GPGPU. In CVPR, 2008.
[10] V. A. Prisacariu and I. Reid. fastHOG - a real-time GPU implementation of HOG. In Technical

[11] X. Wang, T. X. Han, and S. Yan. An HOG-LBP human detector with partial occlusion handling.

Report, 2009.

In ICCV, 2009.

[12] Laptev. Improving object detection with boosted histograms. In Image and Vision Comp., 2009.
[13] T. Ojala, M. Pietikainen, and T. Maenpaa. Multiresolution gray-scale and rotation invariant

texture classiﬁcation with local binary patterns. In PAMI, 2002.

[14] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005.
[15] C. Cortes and V. Vapnik. Support-vector networks. In Machine learning, 1995.

10

6
1
0
2
 
v
o
N
 
5
 
 
]

V
C
.
s
c
[
 
 
1
v
2
4
6
1
0
.
1
1
6
1
:
v
i
X
r
a

This space is reserved for the Procedia header, do not use it

GPU-based pedestrian detection for autonomous driving

V. Campmany1,2, S. Silva1,2, A. Espinosa1, J.C. Moure1
, D. V´azquez2, and A. M. L´opez2

1 Universitat Autonoma de Barcelona, Spain.
2 Computer Vision Centre (CVC), Spain.

Abstract
We propose a real-time pedestrian detection system for the embedded Nvidia Tegra X1 GPU-
CPU hybrid platform. The pipeline is composed by the following state-of-the-art algorithms:
Histogram of Local Binary Patterns (LBP) and Histograms of Oriented Gradients (HOG) fea-
tures extracted from the input image; Pyramidal Sliding Window technique for candidate gen-
eration; and Support Vector Machine (SVM) for classiﬁcation. Results show a 8x speedup
in the target Tegra X1 platform and a better perf ormance/watt ratio than desktop CUDA
platforms in study.

Keywords: Autonomous Driving, Pedestrian detection, Computer Vision, CUDA, Massive Parallelism

Introduction

1
Autonomous driving will improve safety, reduce pollution and congestion, and enable handi-
capped people, elderly persons and kids to have more freedom. It requires perceiving and un-
derstanding the vehicle environment (road, traﬃc signs, pedestrians, vehicles ...) using sensors
(cameras, LIDARs, sonars, and radar), providing self-localization (with GPS, inertial sensors
and visual localization in precise maps), controlling the vehicle, and planning the routes. The
underlying algorithms are high computationally demanding and require real-time response.

A pedestrian detector that locates humans on a digital image is a key module for robotics
application and autonomous vehicles. The wide variation by which humans appear with diﬀer-
ent poses, clothes, illuminations and backgrounds makes the problem one of the hardest within
the computer vision ﬁeld, and the source of an active research during the last twenty years
[1, 2, 3, 4].

The real-time constraints of most applications of pedestrian detection are tight, and cur-
rently not attainable by general-purpose processors. Attaching a desktop GPU for performance
acceleration is expensive in terms of space and power consumption. The recent appearance
of embedded GPU-accelerated systems based on the Nvidia’s Tegra X1 ARM processor, like
the Jetson TX1 and DrivePX platforms, represents a promising approach for low-cost and
low-consumption real-time pedestrian detection.

We have designed a complete GPU-accelerated pedestrian detection pipeline1 based on [4].
We have analyzed the alternative parallelization schemes and data layouts of the underlying

1https://github.com/vcampmany/CudaVisionSysDeploy

1

GPU-based pedestrian detection for autonomous driving

Victor Campmany et. al.

algorithms and selected general and scalable solutions that do not sacriﬁce performance nor
detection accuracy. We have evaluated three pedestrian detection versions on desktop and
embedded platforms and proved that: (1) real-time (20 images of 1242×375 pixels per sec-
ond) can be reached on an embedded GPU-accelerated system with state-of-the-art accuracy;
(2) GPU-acceleration provides between 8x and 60x performance speedup with respect to a
baseline multi-core CPU implementation; and (3) the Tegra X1 processor at least doubles the
performance per watt of the system accelerated by a GTX 960 GPU.

The rest of the paper is organized as follows. Section 2 introduces the related work. Section
3 describes the baseline pedestrian detector. In section 4 we analyze each algorithm and discuss
diﬀerent parallelization schemes. Finally, section 5 provides the obtained results and section 6
summarizes the work.
2 Related work
Computer Uniﬁed Device Architecture (CUDA) is a platform created by Nvidia to develop
general purpose applications for the GPUs. Nvidia GPUs are composed by tens of processing
units called Streaming Multiprocessor (SMs). The SMs share a L2 cache and an external global
memory. Each SM has a shared memory that is managed explicitly and a L1 cache. A CUDA
kernel is composed by thousands of threads executing the same program with diﬀerent data.
The threads are divided into groups of up to 1024 threads called Cooperative Thread Arrays
(CTAs). The threads in a CTA collaborate using the on-chip shared memory. Each CTA is
divided into batches of 32 threads called warps. Finally, individual threads have a reserved
memory region in each layer of the memory hierarchy called local memory.

A critical performance issue is the memory access pattern of the algorithm. GPUs achieve
full memory performance when the memory accesses are coalesced. Coalesced memory access
refers to combining multiple memory operations into a single memory transactions. To achieve
coalescing, the 32 threads of the warp must access consecutive memory addresses. The pre-
viously described constrains data layout, memory transfers and work distribution become key
factors in order to achieve the best performance.

Since the appearance of GPGPU computational platforms, several object detection algo-
rithms have been ported to the GPU. There are also examples of using Field Programmable
Gate Array (FPGA) designs, obtaining outstanding results
[5]. In comparison, the reduced
development costs of the CUDA programming environment and the aﬀordable desktop CUDA
enabled GPU cards make them more suitable for testing new algorithms.

Works such as [6] assert that exploiting the massively parallel paradigm for object detection
algorithms outperforms a highly tuned CPU version [7]. Previously related researches like
[8, 9, 10] developed a GPU object detector using the well-known HOG-SVM approach obtaining
a performance boost.

In the previous work, the evaluations use desktop GPUs to evaluate the algorithms proposed
as a ﬁrst step to evaluate the GPU as a suitable target platform. In this work, we propose a
real-time pedestrian detector running on a low-consumption GPU devices like the Tegra X1
platform. We also present for the ﬁrst time a GPU implementation of the HOGLBP-SVM
detection pipeline [11].
3 Pedestrian detection
Any computer vision-based pedestrian detector is mainly composed by four core modules: the
candidate generation, the feature extraction, the classiﬁcation and the reﬁnement. The can-
didate generation provides rectangular image windows which eventually contain pedestrians.
These windows are described using distinctive patches in the feature extraction stage. During
the classiﬁcation stage the windows are labeled using a learned model accordingly to its fea-

2

GPU-based pedestrian detection for autonomous driving

Victor Campmany et. al.

tures. Finally, as a pedestrian could be detected by several windows, they are ﬁnally managed
in the reﬁnement stage.

For the candidate generation we use the Pyramidal Sliding Window (SW) method. To
extract the features of the images we use Local Binary Patterns (LBP) and Histograms of
Oriented Gradients (HOG). A Support Vector Machine (SVM) is taken as classiﬁer, and, ﬁnally
the reﬁnement is performed by the Non-maximum Suppression algorithm [12].

4 Design and Analysis of Massively-Parallel Algorithms
We have implemented three diﬀerent detection pipelines combining the basic algorithms men-
tioned in section 3 . In this section we present the algorithms and the decisions behind their
massively-parallel implementations on a CUDA architecture. We start describing the general
detection pipelines and the design methodology, and then delve into the details of each algo-
rithm.
4.1 Overview of the Detection Pipelines
The three detection pipelines considered in this work, ordered from lower to higher accuracy
and computational complexity, are LBP-SVM, HOG-SVM and HOGLBP-SVM. They represent
three realistic options for an actual detection system, where one of them has to trade oﬀ
functionality with processing rate. The pipelines follow the same scheme; diﬀerences appear on
the feature extraction stage: LBP-SVM uses LBP, while HOG-SVM takes advantage of HOG.
The HOGLBP-SVM pipeline combines both algorithms. As shown in previous research [11],
concatenating HOG and LBP feature vectors gives an signiﬁcant increase of performance. The
hybrid processing pipeline is designed to use the CPU (Host) and the GPU (Device): (1) the
captured images are copied from the Host memory space to the Device; (2) the scaled-pyramid
of images is created; (3) features are extracted from each pyramid layer; (4) every layer is
segmented into windows to be classiﬁed; (5) detection results are copied to the CPU memory
to execute the Non-maximum Suppression algorithm that reﬁnes the results.

4.2 Histograms of Local Binary Patterns (LBP)
LBP is a feature extraction method that gives information of the texture on a chunk of the
image. The process can be divided into two steps: the LBP Map computation and the LBP
Histograms computation.

The LBP Map [13] is computationally classiﬁed as a 2-dimensional Stencil pattern algo-
rithm. The central pixel is compared with each of its nearest neighbors; if the value is lower
than the center a 0 is stored, otherwise, a 1 is stored. Then, this binary code is converted to
decimal to generate the output pixel value.

Algorithm 1: Massively parallel computation of the LBP map

input : I[H][W]
output: LBP[H][W]

1 parallel for y=0 to H and x=0 to W do
2

LBP[y][x] = LBPf(y, x);

Finally, we extract the image features by computing the LBP Histograms. Histograms of
blocks of 16×16 pixels are computed over the LBP image. The histograms have a 50% overlap in
the X and Y axis meaning that each region will be redundantly computed 4 times. We avoided
the redundant computing of the overlapped descriptors by splitting the Block Histograms into
smaller Cell Histograms of 8 × 8 pixels. Then, these partial histograms are reduced in groups
of four (histogram reduction) to generate the output block histograms. Figure 1 shows the
previously described sequence of operations to compute the LBP.

3

GPU-based pedestrian detection for autonomous driving

Victor Campmany et. al.

Figure 1: LBP: (1) compute LBP value for each pixel (Algorithm 1), (2) compute 8 × 8 pixels
Cell Histograms (Algorithm 2), (3) compute 16 × 16 pixels block histograms (Algorithm 3).

4.2.1 Analysis of parallelism & CUDA mapping for computing LBP
We implemented the 2-dimensional Stencil pattern by mapping each thread to one output pixel
(Algorithm 1). With this work distribution there are no data dependences among threads.
Each thread performs 9 reads (the central value and the eight neighbors) and 1 store and, the
memory accesses are coalesced.

We designed two diﬀerent solutions to compute the LBP histograms; the ﬁrst, is a straight-
forward parallelization without thread collaboration (Na¨ıve scheme); the second, with thread
collaboration, is designed to be more scalable (Scalable scheme).

The Na¨ıve scheme follows a Map pattern: each thread generates a Cell Histogram. The
histogram reduction is performed in the same way: each thread is mapped to a Block Histogram
and the thread performs the histogram reduction.

The Scalable scheme solution used by our system aims for an eﬃcient memory access and
data reutilization. Each thread is mapped to an input pixel of the image, and using atomic
operations each thread adds to its corresponding Cell Histogram (Scatter pattern, see Algorithm
2). To generate the Block Histograms every histogram reduction is performed by a warp (see
Algorithm 3). With this design we attain coalesced memory access which leads to an scalable
algorithm for diﬀerent image sizes.

Algorithm 2: Scalable computation scheme of the Cell Histograms. Each thread reads a
pixel and updates the corresponding cell. We use atomic operations (Read-Add-Store) to
avoid data races. S ⇐ histogram.

input : LBP[H][W]
output: CH[H/8][W/8][S]

1 parallel for y=0 to H and x=0 to W do
2

bin = LBP[y][x];
atomicAdd (CH[y/8][x/8][bin], 1) ;

3

4.3 Histogram of Oriented Gradients (HOG)
The method of Histograms of Oriented Gradients [14] counts the occurrence of gradient ori-
entation on a chunk of the image. The process could be divided into two steps: Gradient
computation and the Histograms computation.

Gradient computation is used to measure the directional change of color in an image. The
algorithm follows a 2 dimensional Stencil pattern. The gradient of a pixel has two components,
the magnitude (ω) and the orientation (θ). The orientation is the directional change of color
and the magnitude gives us information of the intensity of the change.

4

GPU-based pedestrian detection for autonomous driving

Victor Campmany et. al.

Algorithm 3: Scalable computation of the histogram reduction to generate the Block
Histograms (Fa) hReduction: Hb ⇐ H/8 − 1; W b ⇐ W/8 − 1.

input : CH[H/8][W/8][S]
output: Fa[Hb][Wb][S]

1 parallel for y=0 to Hb and x=0 to Wb do
2

SIMD parallel for lane=0 to WarpSize do

3

4

5

6

t = lane;
while t<S do

Fa[y][x][t] = hReduction(t);
t = t + WarpSize;

Figure 2: Steps to compute the HOG features: (1) given a grayscale image, compute the gradient
(Algorithm 4); (2) compute the Block Histograms with trilinear interpolation (Algorithm 5).

Histograms are computed by splitting the Gradient image into blocks of 16 × 16 pixels with
50% overlap in X and Y axis (the same conﬁguration as the LBP Histograms). In this case,
because of histograms trilinear interpolation we can not compute 8×8 pixels Cell Histograms and
then carry out the histogram reduction. Trilinear interpolation is used to avoid sudden changes
in the Block Histograms vector (aliasing eﬀect) [14]. Each Block Histogram is composed by
four concatenated 8 × 8 pixels Cell Histograms. Diﬀerent bins of the Block Histogram receive a
weighted value of the orientation (θ) multiplied by the magnitude of the gradient (ω). Depending
on the pixel coordinates, each input value could be binned into two, four or eight bins of the
Block Histogram. The sequence of steps to compute the HOG features is described in Figure
2.

4.3.1 Analysis of parallelism & CUDA mapping for computing HOG

The gradient computation kernel follows a Stencil pattern: individual threads are mapped to
each output pixel (Algorithm 4). Single threads perform 4 reads and 1 store, and coalesced
memory accesses are achieved.

The Histograms computation has been parallelized assigning each thread to the computation
of one histogram (Large-grain task parallelism, see Algorithm 5). With this structure there is
no collaboration among threads and memory accesses are not coalesced, though, the mapping
avoids the use of the costly atomic memory operations.

We implemented three diﬀerent kernels following the scheme in Algorithm 5. The ﬁrst
stores the data in global memory (HOG Global ). To reduce the latency of the global memory
we designed two more kernels: one stores the histograms in local memory, taking advantage of
the L1 cache (HOG Local ) and the other uses the on-chip shared memory (HOG Shared ). In
section 5.3 we will discuss the results of the implementations.

5

GPU-based pedestrian detection for autonomous driving

Victor Campmany et. al.

Algorithm 4: Massively parallel computation of the Gradient

input : I[H][W]
output: M[H][W], O[H][W]

1 parallel for y=0 to H and x=0 to W do
2

dx = I[y][x-1] - I[y][x+1];
dy = I[y-1][x] - I[y+1][x];
M[y][x] = sqrt(dx ∗ dx, dy ∗ dy);
O[y][x] = arctan(dx, dy);

3

4

5

Algorithm 5: Massively parallel computation of the HOG Histograms.

input : M[H][W], O[H][W]
output: Fb[Hb][Wb][S]

1 parallel for y=0 to Hb and x=0 to Wb do
2

for i=0 to 16 do

3

4

for j=0 to 16 do

updateBlockHistogram(Fb[y][x], i, j);

4.4 Pyramidal Sliding Window & Support Vector Machine
Pyramidal Sliding Window creates multiple downscaled copies of the input images to detect
pedestrians of various sizes and at diﬀerent distances. Then, every copy is split into highly
overlapped regions of 128 × 64 pixels, called windows. Each window is described with a feature
vector ((cid:126)x). The vector is composed by the concatenation of the Block Histograms (computed
with HOG and LBP) enclosed in the given region. Then, every vector is evaluated to predict if
the region contains a pedestrian or not.

Support Vector Machine (SVM) is a supervised learning method that is able to discriminate
two categories, in our case pedestrians from background [15]. After training the SVM, we
obtain a model that performs as an n-dimensional plane that distinguishes pedestrians from
background. The SVM training is done oﬄine. However, the SVM inference is done online.
SVM gets as input a feature vector ((cid:126)x) and computes its distance to the model hyper-plane
((cid:126)ω). This distance is computed with the dot product operation. Then, the window is classiﬁed
as pedestrian if the distance is greater than a given threshold and as background otherwise.
Figure 3 shows the steps needed to evaluate each window taking HOG and LBP features as the
image descriptors.
4.4.1 Analysis of parallelism & CUDA mapping for SW-SVM
We ﬁrst implemented a na¨ıve version (Na¨ıve SVM ) with a Large-grain task parallelism and
no thread collaboration. Each thread is responsible of the computation of the dot product
between the candidate window ((cid:126)x) and the model ((cid:126)ω). The approach was not scalable and
became critical as it is the kernel with the largest workload of the pipeline.

To eﬃciently compute the dot product we designed a CUDA kernel where each warp is
assigned to a window ((cid:126)x) of the transformed image (Warp-level SVM ). The computation of the
dot product is divided between the threads in the warp. Once intra-warp threads have computed
the partial results, these are communicated among threads using register shuﬄe instructions
and then reduced. Algorithm 6 shows the mapping of the Sliding Window and SVM inference
to the massive parallel architecture.

We decided to use a warp-level approach to avoid the overhead of thread synchronization

6

GPU-based pedestrian detection for autonomous driving

Victor Campmany et. al.

Figure 3: (1) Sliding Window and SVM inference of the HOG and LBP features. Each window
is evaluated with Algorithm 6 computing the distance of the HOG+LBP features from the SVM

as warps have implicit hardware synchronization. This conﬁguration allows the full utilization
of the memory bandwidth as the memory accesses are coalesced.

Algorithm 6: Massively parallel computation of the Sliding Window and the SVM in-
ference. N is the SVM trained model, Hn and Wn are the number of Block Histograms
ﬁtting in a window and S is the histogram size. Hb ⇐ H/8 − 1; W b ⇐ W/8 − 1.

input : Fa[Hb][Wb][S], Fb[Hb][Wb][S] N[Hn][Wn][S]
output: scores[Y][X]

1 parallel for y=0 to Y and x=0 to X do
2

SIMD parallel for lane=0 to WarpSize do

3

4

5

6

7

8

9

10

11

12

t = lane;
for i=0 to Hm do

for j=0 to Wm do
while t<S do

res += Fa[i+y][j+x][t] ∗ N[i][j][t];
res += Fb[i+y][j+x][t] ∗ N[i][j][t];
t = t + WarpSize;

res = SIMDreductionSum(res);
if lane == 0 then

scores[y][x] = res;

5 Experiments & Results
In this section we present the obtained performance results for the algorithms and pipelines. All
the experiments are run with an Intel i7-5930K processor, and both Nvidia GPUs: GTX 960
and a Tegra X1. We start by showing the whole pipeline results. We measure the performance
of each of the algorithms by measuring the number of processed pixels per nanosecond (px/ns);
we will also refer to it as Performance.
5.1 Detection Pipelines performance
To evaluate the P erf ormance we use a video sequence with an image size of 1242 × 375 pixels.
Figure 5 presents the performance results of the LBP-SVM, HOG-SVM and HOGLBP-SVM
pipelines, measured in processed frames per second (FPS). Results show the achieved FPS for
the multithreaded CPU baseline application [4] and the GPU accelerated versions. Results
prove that we have accomplished the objective of running the application in real-time under
the low consumption ARM platform.

7

GPU-based pedestrian detection for autonomous driving

Victor Campmany et. al.

Figure 4: Legend values are the area below the
curve. Must be minimized in reliable detectors

Performance of

Figure 5:
the detection
pipelines measured in processed frames per
second (FPS) in the diﬀerent architectures.

Figure 4 illustrates the miss rate depending on the false positive per image (FPPI). FPPI
is the number of candidate windows wrongly classiﬁed as pedestrians, it can be understood as
the tolerance of the system. As the FPPI increases, the miss rate decreases leading to a more
tolerant system.

The system is able to achieve state of the art accuracy with the HOGLBP-SVM detection
pipeline. The rest have slightly lower accuracy; nonetheless, they demand less computational
power to achieve real-time performance which make them suitable for less powerful GPUs.

5.2 Detection Pipeline eﬃciency
Besides needing real time processing, the application demands low power consumption. To
compare the power eﬃciency of the two GPUs we introduce a new metric: F P S/W att. To assess
the Watt consumption we use the Thermal Design Power (TDP ). TDP is the amount of heat
generated by the processor in typical use cases; the attribute is provided by the manufacturer
company.

Table 1 shows the TDP of each platform and the obtained eﬃciency. We expose the results
of the HOGLBP-SVM pipeline as it is the pipeline that reaches state of the art accuracy. The
Tegra X1 platform outperforms the CPU and GTX 960 implementations. It gives an increment
of 200 points compared to the CPU application. It also doubles the eﬃciency of the GTX 960
desktop GPU.

HOGLBP-SVM TDP F P S/W att
i7-5930K
GTX 960
Tegra X1

0.01
0.99
2

140
120
10

Table 1: TDP of each platform and eﬃciency of the HOGLBP-SVM pipeline measured in
F P S/W att.

5.3 Feature extraction & classiﬁcation algorithms
In this section we present the performance results of the individual algorithms that form the
three pipelines. Figures 6, 7, 8 show the obtained performance results measured in px/ns with
various image sizes.

8

GPU-based pedestrian detection for autonomous driving

Victor Campmany et. al.

For the LBP algorithm we experimented with the two developed designs (Na¨ıve and Scalable
schemes). As the image size increases, the Na¨ıve scheme suﬀers a decrease of performance caused
by the poor memory management. However, the Scalable scheme performance is more regular
because of the coalesced memory access (see Figure 6). For the two biggest images it attains a
2.5x speed-up in both GPUs compared to the Na¨ıve kernel. Although, for the smaller images
the boost is low; the non-coalesced memory access is attenuated by the high utilization of the
L2 cache which hides the latency of the memory operations (Figure 6).

Figure 7 expose the attained results for the HOG algorithm. In this case, we analize the
three developed solutions (HOG Global, HOG Local and HOG Shared). Results prove that
the ﬁrst massively parallel implementation was not competitive with a CPU version; for most
of the experiments in Figure 7, the CPU outperforms the performance of the HOG Global
kernel on both GPUs. However, if we consider the best implementation (HOG Shared) we can
conclude that the performance the GPUs surpass the CPU; the GTX 960 has a speed-up of
4x and the Tegra X1 18x. The diﬀerence in speed-up between devices is caused by the lower
memory bandwidth of the Tegra X1. The HOG Shared kernel reduces the usage of the global
memory as it stores the data in the shared memory leading to a less stressed memory in the
Tegra ARM system.

The Pyramidal Sliding Window and Support Vector Machine is the most time consuming
part of the pipeline it takes up to 55% of the time to process an image. Figure 8 shows the
results for the Na¨ıve and Warp-level approaches. The Na¨ıve kernel suﬀers from non-coalesced
memory access. It becomes a critical issue in the Tegra X1 as performance is equivalent to the
CPU one. The Warp-level design the memory bandwidth is used at peak performance because
of coalesced memory access. We obtain a boost of 10x compared to the Na¨ıve kernel on the
GTX960 card and 8x speed-up in the Tegra X1 system. Despite, the Warp-level kernel suﬀers a
decrease of performance for the bigger images, in this cases, the data needs to be fetched from
the global memory because it does not ﬁt in the L2 cache.

Figure 6: Performance of the LBP algorithm.

Figure 7: Performance of HOG algorithm.

6 Conclusions
Our experiments conﬁrm the importance of adapting the problem to the GPU architecture.
Smart work distribution and thread collaboration are key factors to attain signiﬁcant speed-ups
compared to a high end CPU. The stated facts become even more critical when the development
is done under a low consumption platform like the Tegra X1 processor. For the developed
algorithms the P erf ormance/W att of the Tegra X1 doubles the GTX 960. The evidence
determines that the Tegra ARM platform is an energy eﬃcient system able to challenge the
desktop GPU performance when running massively parallel applications.

9

GPU-based pedestrian detection for autonomous driving

Victor Campmany et. al.

Figure 8: Performance of the SW-SVM method.

In this work we show a massively parallel implementation of a pedestrian detector that uses
LBP and HOG as features and SVM for classiﬁcation. Our implementation is able to achieve
the real-time requirements on the autonomous driving platform, the Nvidia DrivePX.

7 Acknowledgements
This research has been supported by the MICINN under contract number TIN2014-53234-
C2-1-R. By the MEC under contract number TRA2014-57088-C2-1-R, the spanish DGT
project SPIP2014-01352, and the Generalitat de Catalunya projects 2014-SGR-1506 and 2014-
SGR1562. We thank Nvidia for the donation of the systems used in this work.

References

[1] D. Geronimo, A. M. Lopez, A. D. Sappa, and T. Graf. Survey of pedestrian detection for advanced

driver assistance systems. In PAMI, 2010.

[2] D. M. Gavrila. The visual analysis of human movement: A survey. In CVIU, 1999.
[3] Dollar, Wojek, Schiele, and Perona. Pedestrian detection: An evaluation of the state of the art.

[4] J. Marin, D. Vazquez, A. M. Lopez, J. Amores, and B. Leibe. Random forests of local experts for

In PAMI, 2012.

pedestrian detection. In ICCV, 2013.

[5] M. Hahnle, F. Saxen, M. Hisung, U. Brunsmann, and K. Doll. FPGA-based real-time pedestrian

detection on high-resolution images. In CVPR, 2013.

[6] R. Benenson, M. Mathias, R. Timofte, and L. Van Gool. Pedestrian detection at 100 frames per

second. In CVPR, 2012.

[7] P. Dollar, Belongie, and Perona. The fastest pedestrian detector in the west. In BMVC, 2010.
[8] C. Wojek, G. Dork, A. Schulz, and B. Schiele. Sliding-windows for rapid object class localization:

A parallel technique. In Pattern Recognition, 2008.

[9] Zhang and Nevatia. Eﬃcient scan-window based object detection using GPGPU. In CVPR, 2008.
[10] V. A. Prisacariu and I. Reid. fastHOG - a real-time GPU implementation of HOG. In Technical

[11] X. Wang, T. X. Han, and S. Yan. An HOG-LBP human detector with partial occlusion handling.

Report, 2009.

In ICCV, 2009.

[12] Laptev. Improving object detection with boosted histograms. In Image and Vision Comp., 2009.
[13] T. Ojala, M. Pietikainen, and T. Maenpaa. Multiresolution gray-scale and rotation invariant

texture classiﬁcation with local binary patterns. In PAMI, 2002.

[14] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005.
[15] C. Cortes and V. Vapnik. Support-vector networks. In Machine learning, 1995.

10

