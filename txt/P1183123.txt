DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks

Andrey Ignatov, Nikolay Kobyshev, Kenneth Vanhoey, Radu Timofte, Luc Van Gool

andrey.ignatoff@gmail.com, {nk, vanhoey, timofter, vangool}@vision.ee.ethz.ch

ETH Zurich

7
1
0
2
 
p
e
S
 
5
 
 
]

V
C
.
s
c
[
 
 
2
v
0
7
4
2
0
.
4
0
7
1
:
v
i
X
r
a

Figure 1: iPhone 3GS photo enhanced to DSLR-quality by our method. Best zoomed on screen.

Abstract

Despite a rapid rise in the quality of built-in smartphone cam-
eras, their physical limitations — small sensor size, compact
lenses and the lack of speciﬁc hardware, — impede them to
achieve the quality results of DSLR cameras. In this work we
present an end-to-end deep learning approach that bridges this
gap by translating ordinary photos into DSLR-quality images.
We propose learning the translation function using a residual
convolutional neural network that improves both color rendition
and image sharpness. Since the standard mean squared loss is
not well suited for measuring perceptual image quality, we intro-
duce a composite perceptual error function that combines con-
tent, color and texture losses. The ﬁrst two losses are deﬁned
analytically, while the texture loss is learned in an adversarial
fashion. We also present DPED, a large-scale dataset that con-
sists of real photos captured from three different phones and one
high-end reﬂex camera. Our quantitative and qualitative assess-
ments reveal that the enhanced image quality is comparable to
that of DSLR-taken photos, while the methodology is general-
ized to any type of digital camera.

1

Introduction

During the last several years there has been a signiﬁcant
improvement in compact camera sensors quality, which has
brought mobile photography to a substantially new level. Even

low-end devices are now able to take reasonably good photos
in appropriate lighting conditions, thanks to their advanced soft-
ware and hardware tools for post-processing. However, when it
comes to artistic quality, mobile devices still fall behind their
DSLR counterparts. Larger sensors and high-aperture optics
yield better photo resolution, color rendition and less noise,
whereas their additional sensors help to ﬁne-tune shooting pa-
rameters. These physical differences result in strong obstacles,
making DSLR camera quality unattainable for compact mobile
devices.

While a number of photographer tools for automatic image
enhancement exist, they are usually focused on adjusting only
global parameters such as contrast or brightness, without im-
proving texture quality or taking image semantics into account.
Besides that, they are usually based on a pre-deﬁned set of rules
that do not always consider the speciﬁcs of a particular device.
Therefore, the dominant approach to photo post-processing is
still based on manual image correction using specialized re-
touching software.

1.1 Related work

The problem of automatic image quality enhancement has not
been addressed in its entirety in the area of computer vision,
though a number of sub-tasks and related problems have been al-
ready successfully solved using deep learning techniques. Such
tasks are usually dealing with image-to-image translation prob-
lems, and their common property is that they are targeted at re-

1

moving artiﬁcially added artifacts to the original images. Among
the related problems are the following:

Image super-resolution aims at restoring the original im-
age from its downscaled version.
In [4] a CNN architecture
and MSE loss are used for directly learning low to high reso-
lution mapping.
It is the ﬁrst CNN-based solution to achieve
top performance in single image super-resolution, comparable
with non-CNN methods [20]. The subsequent works devel-
oped deeper and more complex CNN architectures (e.g., [10, 18,
16]). Currently, the best photo-realistic results on this task are
achieved using a VGG-based loss function [9] and adversarial
networks [12] that turned out to be efﬁcient at recovering plau-
sible high-frequency components.

Image deblurring/dehazing tries to remove artiﬁcially added
haze or blur from the images. Usually, MSE is used as a target
loss function and the proposed CNN architectures consist of 3 to
15 convolutional layers [14, 2, 6] or are bi-channel CNNs [17].
Image denoising/sparse inpainting similarly targets removal
of noise and artifacts from the pictures. In [28] the authors pro-
posed weighted MSE together with a 3-layer CNN, while in [19]
it was shown that an 8-layer residual CNN performs better when
using a standard mean square error. Among other solutions are
a bi-channel CNN [29], a 17-layer CNN [26] and a recurrent
CNN [24] that was reapplied several times to the produced re-
sults.

Image colorization. Here the goal is to recover colors that
were removed from the original image. The baseline approach
for this problem is to predict new values for each pixel based
on its local description that consists of various hand-crafted fea-
tures [3]. Considerably better performance on this task was ob-
tained using generative adversarial networks [8] or a 16-layer
CNN with a multinomial cross-entropy loss function [27].

Image adjustment. A few works considered the problem of
image color/contrast/exposure adjustment. In [25] the authors
proposed an algorithm for automatic exposure correction using
In [23], a more
hand-designed features and predeﬁned rules.
general algorithm was proposed that – similarly to [3] – uses
local description of image pixels for reproducing various pho-
tographic styles. A different approach was considered in [13],
where images with similar content are retrieved from a database
and their styles are applied to the target picture. All of these
adjustments are implicitly included in our end-to-end transfor-
mation learning approach by design.

1.2 Contributions

The key challenge we face is dealing with all the aforementioned
enhancements at once. Even advanced tools cannot notably im-
prove image sharpness, texture details or small color variations
that were lost by the camera sensor, thus we can not generate tar-
get enhanced photos from the existing ones. Corrupting DSLR
photos and training an algorithm on the corrupted images does
not work either: the solution would not generalize to real-world
and very complex artifacts unless they are modeled and applied
as corruptions, which is infeasible. To tackle this problem, we
present a different approach: we propose to learn the transfor-
mation that modiﬁes photos taken by a given camera to DSLR-

Table 1: DPED camera characteristics.

Camera

iPhone 3GS
BlackBerry Passport
Sony Xperia Z
Canon 70D DSLR

Sensor

3 MP
13 MP
13 MP
20 MP

Image size

Photo quality

Poor

2048 × 1536
4160 × 3120 Mediocre
2592 × 1944 Average
3648 × 2432 Excellent

Figure 2: The rig with the four DPED cameras from Table 1.

quality ones. Thus, the goal is to learn a cross-distribution trans-
lation function, where the input distribution is deﬁned by a given
mobile camera sensor, and the target distribution by a DSLR sen-
sor. To supervise the learning process, we create and leverage a
dataset of images capturing the same scene with different cam-
eras. Once the function is learned, it can be further applied to
unseen photos at will.

Our main contributions are:

• A novel approach1 for the photo enhancement task based on
learning a mapping function between photos from mobile
devices and a DSLR camera. The target model is trained in
an end-to-end fashion without using any additional super-
vision or handcrafted features.

• A new large-scale dataset of over 6K photos taken syn-
chronously by a DSLR camera and 3 low-end cameras of
smartphones in a wide variety of conditions.

• A multi-term loss function composed of color, texture and
content terms, allowing an efﬁcient image quality estima-
tion.

• Experiments measuring objective and subjective quality
demonstrating the advantage of the enhanced photos over
the originals and, at the same time, their comparable qual-
ity with the DSLR counterparts.

The remainder of the paper is structured as follows.
In
Section 2 we describe the new DPED dataset. Section 3 presents
our architecture and the chosen loss functions. Section 4 shows
and analyzes the experimental results. Finally, Section 5 con-
cludes the paper.

1https://github.com/aiff22/DPED

2

iPhone

BlackBerry

Sony

Canon

Figure 4: Matching algorithm: an overlapping region is deter-
mined by SIFT descriptor matching, followed by a non-linear
transform and a crop resulting in two images of the same resolu-
tion representing the same scene. Here: Canon and BlackBerry
images, respectively.

Training CNN on the aligned high-resolution images is infea-
sible, thus patches of size 100×100px were extracted from these
photos. Our preliminary experiments revealed that larger patch
sizes do not lead to better performance, while requiring consid-
erably more computational resources. We extracted patches us-
ing a non-overlapping sliding window. The window was mov-
ing in parallel along both images from each phone-DSLR im-
age pair, and its position on the phone image was additionally
adjusted by shifts and rotations based on the cross-correlation
metrics. To avoid signiﬁcant displacements, only patches with
cross-correlation greater than 0.9 were included in the dataset.
Around 100 original images were reserved for testing, the rest
of the photos were used for training and validation. This proce-
dure resulted in 139K, 160K and 162K training and 2.4-4.3K test
patches for BlackBerry-Canon, iPhone-Canon and Sony-Canon
pairs, respectively. It should be emphasized that both training
and test patches are precisely matched, the potential shifts do not
exceed 5 pixels. In the following we assume that these patches
of size 3×100×100 constitute the input data to our CNNs.

3 Method

Given a low-quality photo Is (source image), the goal of the con-
sidered enhancement task is to reproduce the image It (target
image) taken by a DSLR camera. A deep residual CNN FW pa-
rameterized by weights W is used to learn the underlying trans-
lation function. Given the training set {I j
j=1 consisting of
N image pairs, it is trained to minimize:

s , I j

t }N

W∗ = arg min
W

1
N

N
(cid:88)

j=1

L(cid:0)FW(I j

s ), I j

t

(cid:1),

(1)

where L denotes a multi-term loss function we detail in sec-
tion 3.1. We then deﬁne the system architecture of our solution
in Section 3.2.

Figure 3: Example quadruplets of images taken synchronously
by the DPED four cameras.

2 DSLR Photo Enhancement Dataset

In order to tackle the problem of image translation from poor
quality images captured by smartphone cameras to superior
quality images achieved by a professional DSLR camera, we
introduce a large-scale real-world dataset, namely the “DSLR
Photo Enhancement Dataset” (DPED)2, that can be used for the
general photo quality enhancement task. DPED consists of pho-
tos taken in the wild synchronously by three smartphones and
one DSLR camera. The devices used to collect the data are de-
scribed in Table 1 and example quadruplets can be seen in Fig-
ure 3.

To ensure that all cameras were capturing photos simultane-
ously, the devices were mounted on a tripod and activated re-
motely by a wireless control system (see Figure 2). In total, over
22K photos were collected during 3 weeks, including 4549 pho-
tos from Sony smartphone, 5727 from iPhone and 6015 photos
from each Canon and BlackBerry cameras. The photos were
taken during the daytime in a wide variety of places and in vari-
ous illumination and weather conditions. The photos were cap-
tured in automatic mode, and we used default settings for all
cameras throughout the whole collection procedure.

Matching algorithm. The synchronously captured images are
not perfectly aligned since the cameras have different viewing
angles and positions as can be seen in Figure 3. To address this,
we performed additional non-linear transformations resulting in
a ﬁxed-resolution image that our network takes as an input. The
algorithm goes as follows (see Fig. 4). First, for each (phone-
DSLR) image pair, we compute and match SIFT keypoints [15]
across the images. These are used to estimate a homography
using RANSAC [21]. We then crop both images to the intersec-
tion part and downscale the DSLR image crop to the size of the
phone crop.

2http://dped-photos.vision.ee.ethz.ch

3

Figure 5: Fragments from the original and blurred images taken
by the phone (two left-most) and DSLR (two right-most) cam-
era. Blurring removes high-frequencies and makes color com-
parison easier.

3.1 Loss function

The main difﬁculty of the image enhancement task is that in-
put and target photos cannot be matched densely (i.e., pixel-
to-pixel): different optics and sensors cause speciﬁc local non-
linear distortions and aberrations, leading to a non-constant shift
of pixels between each image pair even after precise alignment.
Hence, the standard per-pixel losses, besides being doubtful as
a perceptual quality metric, are not applicable in our case. We
build our loss function under the assumption that the overall per-
ceptual image quality can be decomposed into three independent
parts: i) color quality, ii) texture quality and iii) content quality.
We now deﬁne loss functions for each component, and ensure
invariance to local shifts by design.

3.1.1 Color loss

To measure the color difference between the enhanced and tar-
get images, we propose applying a Gaussian blur (see Figure 5)
and computing Euclidean distance between the obtained repre-
sentations. In the context of CNNs, this is equivalent to using
one additional convolutional layer with a ﬁxed Gaussian kernel
followed by the mean squared error (MSE) function. Color loss
can be written as:

Lcolor(X, Y ) = (cid:107)Xb − Yb(cid:107)2
2,

(2)

where Xb and Yb are the blurred images of X and Y , resp.:

Xb(i, j) =

X(i + k, j + l) · G(k, l),

(3)

(cid:88)

k,l

and the 2D Gaussian blur operator is given by

G(k, l) = A exp

−

(cid:18)

(k − µx)2
2σx

−

(l − µy)2
2σy

(cid:19)

(4)

where we deﬁned A = 0.053, µx,y = 0, and σx,y = 3.

The idea behind this loss is to evaluate the difference in bright-
ness, contrast and major colors between the images while elim-
inating texture and content comparison. Hence, we ﬁxed a con-
stant σ by visual inspection as the smallest value that ensures that
texture and content are dropped. The crucial property of this loss
is its invariance to small distortions. Figure 6 demonstrates the
MSE and Color losses for image pairs (X, Y), where Y equals X
shifted in a random direction by n pixels. As one can see, color
loss is nearly insensitive to small distortions ((cid:54) 2 pixels). For

Figure 6: Comparison between MSE and color loss as a function
of the magnitude of shift between images. Results were averaged
over 50K images.

higher shifts (3-5px), it is still about 5-10 times smaller com-
pared to the MSE, whereas for larger displacements it demon-
strates similar magnitude and behavior. As a result, color loss
forces the enhanced image to have the same color distribution as
the target one, while being tolerant to small mismatches.

3.1.2 Texture loss

Instead of using a pre-deﬁned loss function, we build upon gen-
erative adversarial networks (GANs) [5] to directly learn a suit-
able metric for measuring texture quality. The discriminator
CNN is applied to grayscale images so that it is targeted speciﬁ-
cally on texture processing. It observes both fake (improved) and
real (target) images, and its goal is to predict whether the input
image is real or not. It is trained to minimize the cross-entropy
loss function, and the texture loss is deﬁned as a standard gener-
ator objective:

Ltexture = −

log D(FW(Is), It),

(5)

(cid:88)

i

where FW and D denote the generator and discriminator net-
works, respectively. The discriminator is pre-trained on the
{phone, DSLR} image pairs, and then trained jointly with the
It should be
proposed network as is conventional for GANs.
noted that this loss is shift-invariant by deﬁnition since no align-
ment is required in this case.

3.1.3 Content loss

Inspired by [9, 12], we deﬁne our content loss based on the ac-
tivation maps produced by the ReLU layers of the pre-trained
VGG-19 network. Instead of measuring per-pixel difference be-
tween the images, this loss encourages them to have similar fea-
ture representation that comprises various aspects of their con-
tent and perceptual quality.
In our case it is used to preserve
image semantics since other losses don’t consider it. Let ψj() be
the feature map obtained after the j-th convolutional layer of the

4

VGG-19 CNN, then our content loss is deﬁned as Euclidean dis-
tance between feature representations of the enhanced and target
images:

Lcontent =

1
CjHjWj

(cid:107)ψj

(cid:0)FW(Is)(cid:1) − ψj

(cid:0)It

(cid:1)(cid:107),

(6)

where Cj, Hj and Wj denotes the number, height and width of
the feature maps, and FW(Is) the enhanced image.

3.1.4 Total variation loss

In addition to previous losses, we add total variation (TV)
loss [1] to enforce spatial smoothness of the produced images:

Ltv =

(cid:107)∇xFW(Is) + ∇yFW(Is)(cid:107),

(7)

1
CHW

where C, H and W are the dimensions of the generated image
FW(Is). As it is relatively lowly weighted (see Eqn. 8), it does
not harm high-frequency components while it is quite effective
at removing salt-and-pepper noise.

3.1.5 Total loss

Our ﬁnal loss is deﬁned as a weighted sum of previous losses
with the following coefﬁcients:

Figure 7: The overall architecture of the proposed system.

were optimized using Adam [11] modiﬁcation of stochastic gra-
dient descent with a learning rate of 5e-4. The whole pipeline
and experimental setup was identical for all cameras.

Ltotal = Lcontent + 0.4 · Ltexture + 0.1 · Lcolor + 400 · Ltv,

(8)

4 Experiments

where the content loss is based on the features produced by the
relu 5 4 layer of the VGG-19 network. The coefﬁcients were
chosen based on preliminary experiments on the DPED training
data.

3.2 Generator and Discriminator CNNs

the pro-
Figure 7 illustrates the overall architecture of
posed CNNs. Our image transformation network is fully-
convolutional, and starts with a 9×9 layer followed by four
residual blocks. Each residual block consists of two 3×3 layers
alternated with batch-normalization layers. We use two addi-
tional layers with kernels of size 3×3 and one with 9×9 kernels
after the residual blocks. All layers in the transformation net-
work have 64 channels and are followed by a ReLU activation
function, except for the last one, where a scaled tanh is applied
to the outputs.

The discriminator CNN consists of ﬁve convolutional lay-
ers each followed by a LeakyReLU nonlinearity and batch nor-
malization. The ﬁrst, second and ﬁfth convolutional layers are
strided with a step size of 4, 2 and 2, respectively. A sigmoidal
activation function is applied to the outputs of the last fully-
connected layer containing 1024 neurons and produces a proba-
bility that the input image was taken by the target DSLR camera.

3.3 Training details

The network was trained on a NVidia Titan X GPU for 20K iter-
ations using a batch size of 50. The parameters of the network

Our general goal to “improve image quality” is subjective and
hard to evaluate quantitatively. We suggest a set of tools and
methods from the literature that are most relevant to our prob-
lem. We use them, as well as our proposed method, on a set of
test images taken by mobile devices and compare how close the
results are to the DSRL shots.

In section 4.1, we present the methods we compare to. Then
we present both objective and subjective evaluations: the former
w.r.t. the ground truth reference (i.e., the DSLR images) in sec-
tion 4.2, the latter with no-reference subjective quality scores in
section 4.3. Finally, section 4.4 analyzes the limitations of the
proposed solution.

4.1 Benchmark methods

In addition to our proposed photo enhancement solution, we
compare with the following tools and methods.

Apple Photo Enhancer (APE) is a commercial product
known to generate among the best visual results, while the al-
gorithm is unpublished. We trigger the method using the auto-
matic Enhance function from the Photos app. It performs image
improvement without taking any parameters.

Dong et al. [4] is a fundamental baseline super-resolution
method, thus addredding a task related to end-to-end image-to-
image mapping. Hence we chose it to apply on our task and
compare with. The method relies on a standard 3-layer CNN
and MSE loss function and maps from low resolution / corrupted
image to the restored image.

5

Figure 8: From left to right, top to bottom: original iPhone photo and the same image after applying, respectively: APE, Dong et
al. [4], Johnson et al. [9], our generator network, and the corresponding DSLR image.

Table 2: Average PSNR/SSIM results on DPED test images.

Phone

APE

Dong et al. [4]

Johnson et al. [9]

Ours

iPhone
BlackBerry
Sony

PSNR
17.28
18.91
19.45

SSIM PSNR
19.27
0.8631
18.89
0.8922
21.21
0.9168

SSIM PSNR
0.8992
20.32
0.9134
20.11
21.33
0.9382

SSIM PSNR
20.08
0.9161
20.07
0.9298
0.9434
21.81

SSIM
0.9201
0.9328
0.9437

Johnson et al. [9] is one of the latest state of the art in
photo-realistic super-resolution and style transferring tasks. The
method is based on a deep residual network (with four resid-
ual blocks, each consisting of two convolutional layers) that is
trained to minimize a VGG-based loss function.

Manual enhancement. We asked a graphical artist to en-
hance color, sharpness and general look-and-feel of 9 images
using professional software (Adobe Photoshop CS6). A time
limit of one workday was given, so as to simulate a realistic sce-
nario. Figure 8 illustrates the ensemble of enhancement methods
we consider for comparison in our experiments. Dong et al. [4]
and Johnson et al. [9] are trained using the same train image pairs
as for our solution for each of the smartphones from the DPED
dataset.

(the ground truth DSLR image). We use classical distance met-
rics, namely PSNR and SSIM scores: the former measures sig-
nal distortion w.r.t. the reference, the latter measures structural
similarity which is known to be a strong cue for perceived qual-
ity [22]. First, one can note that our method is the best in terms
of SSIM, at the same time producing images that are cleaner and
sharper, thus perceptually performs the best. On PSNR terms,
our method competes with the state of the art:
it slightly im-
proves or worsens depending on the dataset, i.e., on the actual
phone used. Alignment issues could be responsible for these mi-
nor variations, and thus we consider Johnson et al.’s method [4]
and ours equivalent here, while outperforming other methods.
In Fig. 8 we show visual results comparing to the source photo
(iPhone) and the target DSLR photo (Canon). More results are
in the supplementary material.

4.2 Quantitative evaluation

We ﬁrst quantitatively compare APE, Dong et al. [4], Johnson et
al. [9] and our method on the task of mapping photos from three
low-end cameras to the high-quality DSLR (Canon) images and
report the results in Table 3. As such, we do not evaluate global
image quality but, rather, we measure resemblance to a reference

Our goal is to produce DSLR-quality images for the end user of
smartphone cameras. To measure overall quality we designed
a no-reference user study where subjects are repeatedly asked
to choose the better looking picture out of a displayed pair.

4.3 User study

6

BlackBerry

BlackBerry

Sony

Sony

Figure 9: Four examples of original (top) vs. enhanced (bottom) images captured by BlackBerry and Sony cameras.

Users were instructed to ignore precise picture composition er-
rors (e.g., ﬁeld of view, perspective variation, etc.). There was
no time limit given to the participants, images were shown in full
resolution and the users were allowed to zoom in and out at will.
In this setting, we did the following pairwise comparisons (ev-
ery group of experiments contains 3 classes of pictures, the users
were shown all possible pairwise combinations of these classes):
(i) Comparison between:

• original low-end phone photos,
• DSLR photos,
• photos enhanced by our proposed method.

At every question, the user is shown two pictures from differ-
ent categories (original, DSLR or enhanced). 9 scenes were used
for each phone (e.g., see Fig. 11). In total, there are 27 questions
for every phone, thus 81 in total.
(ii) Additionally, we compared (iPhone images only):

• photos enhanced by the proposed method,
• photos enhanced manually (by a professional),
• photos enhanced by APE.

We again considered 9 images that resulted in 27 binary se-
lection questions. Thus, in total the study consists of 108 binary
questions. All pairs are shufﬂed randomly for every subject, as
is the sequence of displayed images. 42 subjects unaware of the
goal of this work participated. They are mostly young scientists
with a computer science background.

Figure 10 shows results: for every experiment the ﬁrst 3 bars
show the results of the pairwise comparison averaged over the
9 images shown, while the last bar shows the fraction of cases
when the selected method was chosen over all experiments.

The subﬁgures 10a-c show the results of enhancing photos
from 3 different mobile devices. It can be seen that in all cases
both pictures taken with a DSLR as well as pictures enhanced by
the proposed CNN are picked much more often than the original
ones taken with the mobile devices. When subjects are asked

to select the better picture among the DSLR-picture and our en-
hanced picture, the choice is almost random (see the third bar
in subﬁgures 10a-c). This means that the quality difference is
inexistent or indistinguishable, and users resort to chance.

Subﬁgure 10d shows user choices among our method, human
artist work, and APE. Although human enhancement turns out to
be slightly preferred to the automatic APE, the images enhanced
by our method are picked more often, outperforming even man-
ual retouching.

We can conclude that our results are of on pair quality com-
pared to DSLR images, while starting from low quality phone
cameras. The human subjects are unable to distinguish between
them – the preferences are equally distributed.

4.4 Limitations

Since the proposed enhancement process is fully-automated,
Two typical artifacts that can
some ﬂaws are inevitable.
appear on the processed images are color deviations (see
ground/mountains in ﬁrst image of Fig. 12) and too high contrast
levels (second image). Although they often cause rather plau-
sible visual effects, in some situations this can lead to content
changes that may look artiﬁcial, i.e. greenish asphalt in the sec-
ond image of Fig. 12. Another notable problem is noise ampliﬁ-
cation – due to the nature of GANs, they can effectively restore
high frequency-components. However, high-frequency noise is
emphasized too. Fig. 12 (2nd and 3rd images) shows that a high
noise in the original image is ampliﬁed in the enhanced image.
Note that this noise issue occurs mostly on the lowest-quality
photos (i.e., from the iPhone), not on the better phone cameras.
the need of a strong supervision in the form of
matched source/target training image pairs makes the process
tedious to repeat for other cameras. To overcome this, we pro-
pose a weakly-supervised approach in [7] that does not require
the mentioned correspondence.

Finally,

7

(a) BlackBerry phone

(b) iPhone

(c) Sony phone

(d) Enhanced iPhone pictures

Figure 10: User study: results of pairwise comparisons. In every subﬁgure, the ﬁrst three bars show the result of the pairwise
experiments, while the last bar shows the distribution of the aggregated scores.

Figure 11: The 9 scenes shown to the participants of the user study. Here: BlackBerry images enhanced using our technique.

Figure 12: Typical artifacts generated by our method (2nd row) compared with original iPhone images (1st row)

5 Conclusions

References

We proposed a photo enhancement solution to effectively
transform cameras from common smartphones into high quality
DSLR cameras. Our end-to-end deep learning approach uses
a composite perceptual error function that combines content,
color and texture losses. To train and evaluate our method we
introduced DPED – a large-scale dataset that consists of real
photos captured from three different phones and one high-end
reﬂex camera, and suggested an efﬁcient way of calibrating the
images so that they are suitable for image-to-image learning.
Our quantitative and qualitative assessments reveal that the
enhanced images demonstrate a quality comparable to DSLR-
taken photos, and the method itself can be applied to cameras of
various quality levels.

[1] H. A. Aly and E. Dubois. Image up-sampling using total-
variation regularization with a new observation model.
IEEE Transactions on Image Processing, 14(10):1647–
1659, Oct 2005. 5

[2] B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao. Dehazenet:
An end-to-end system for single image haze removal.
IEEE Transactions on Image Processing, 25(11):5187–
5198, Nov 2016. 2

[3] Z. Cheng, Q. Yang, and B. Sheng. Deep colorization. In
The IEEE International Conference on Computer Vision
(ICCV), December 2015. 2

[4] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a Deep
Convolutional Network for Image Super-Resolution, pages
184–199. Springer International Publishing, Cham, 2014.
2, 5, 6

Acknowledgments. Work supported by the ETH Zurich Gen-
eral Fund (OK), Toyota via the project TRACE-Zurich, the ERC
grant VarCity, and an NVidia GPU grant.

[5] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.
Generative adversarial nets. In Z. Ghahramani, M. Welling,

8

image and video super-resolution using an efﬁcient sub-
pixel convolutional neural network. In The IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
June 2016. 2

[19] P. Svoboda, M. Hradis, D. Barina, and P. Zemc´ık. Com-
pression artifacts removal using convolutional neural net-
works. CoRR, abs/1605.00366, 2016. 2

[20] R. Timofte, V. De Smet, and L. Van Gool. A+: Ad-
justed Anchored Neighborhood Regression for Fast Super-
Resolution, pages 111–126. Springer International Pub-
lishing, Cham, 2015. 2

[21] A. Vedaldi and B. Fulkerson. VLFeat: An open and
portable library of computer vision algorithms, 2008. 3

[22] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.
Image quality assessment: from error visibility to struc-
tural similarity. IEEE Transactions on Image Processing,
13(4):600–612, April 2004. 6

[23] Z. Yan, H. Zhang, B. Wang, S. Paris, and Y. Yu. Automatic
photo adjustment using deep neural networks. ACM Trans.
Graph., 35(2):11:1–11:15, Feb. 2016. 2

[24] W. Yang, R. T. Tan, J. Feng, J. Liu, Z. Guo, and S. Yan.
Joint rain detection and removal via iterative region depen-
dent multi-task learning. CoRR, abs/1609.07769, 2016. 2
[25] L. Yuan and J. Sun. Automatic Exposure Correction of
Consumer Photographs, pages 771–785. Springer Berlin
Heidelberg, Berlin, Heidelberg, 2012. 2

[26] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Be-
yond a gaussian denoiser: Residual learning of deep CNN
for image denoising. IEEE Transactions on Image Process-
ing, 2017. 2

[27] R. Zhang, P. Isola, and A. A. Efros. Colorful image col-

orization. ECCV, 2016. 2

[28] X. Zhang and R. Wu. Fast depth image denoising and en-
In 2016
hancement using a deep convolutional network.
IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pages 2499–2503, March
2016. 2

[29] E. Zhou, H. Fan, Z. Cao, Y. Jiang, and Q. Yin. Learn-
In Proceedings of the
ing face hallucination in the wild.
Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence,
AAAI’15, pages 3871–3877. AAAI Press, 2015. 2

C. Cortes, N. D. Lawrence, and K. Q. Weinberger, edi-
tors, Advances in Neural Information Processing Systems
27, pages 2672–2680. Curran Associates, Inc., 2014. 4
[6] M. Hradiˇs, J. Kotera, P. Zemˇc´ık, and F. ˇSroubek. Con-
volutional neural networks for direct text deblurring.
In
Proceedings of BMVC 2015. The British Machine Vision
Association and Society for Pattern Recognition, 2015. 2

[7] A. Ignatov, N. Kobyshev, R. Timofte, K. Vanhoey, and
L. Van Gool. Wespe: Weakly supervised photo enhancer
for digital cameras. 2017. 7

[8] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros.

Image-to-
image translation with conditional adversarial networks.
arxiv, 2016. 2

[9] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual Losses
for Real-Time Style Transfer and Super-Resolution, pages
694–711. Springer International Publishing, Cham, 2016.
2, 4, 6

[10] J. Kim, J. K. Lee, and K. M. Lee. Accurate image super-
resolution using very deep convolutional networks. In 2016
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 1646–1654, June 2016. 2

[11] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. CoRR, abs/1412.6980, 2014. 5

[12] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunning-
ham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and
W. Shi. Photo-realistic single image super-resolution using
a generative adversarial network. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July
2017. 2, 4

[13] J.-Y. Lee, K. Sunkavalli, Z. Lin, X. Shen, and I. So Kweon.
Automatic content-aware color and tone stylization. In The
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), June 2016. 2

[14] Z. Ling, G. Fan, Y. Wang, and X. Lu. Learning deep trans-
mission network for single image dehazing. In 2016 IEEE
International Conference on Image Processing (ICIP),
pages 2296–2300, Sept 2016. 2

[15] D. G. Lowe. Distinctive image features from scale-
invariant keypoints. International Journal of Computer Vi-
sion, 60(2):91–110, 2004. 3

[16] X. Mao, C. Shen, and Y.-B. Yang. Image restoration using
very deep convolutional encoder-decoder networks with
symmetric skip connections. In D. D. Lee, M. Sugiyama,
U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances
in Neural Information Processing Systems 29, pages 2802–
2810. Curran Associates, Inc., 2016. 2

[17] W. Ren, S. Liu, H. Zhang, J. Pan, X. Cao, and M.-H. Yang.
Single Image Dehazing via Multi-scale Convolutional Neu-
ral Networks, pages 154–169. Springer International Pub-
lishing, Cham, 2016. 2

[18] W. Shi, J. Caballero, F. Huszar, J. Totz, A. P. Aitken,
R. Bishop, D. Rueckert, and Z. Wang. Real-time single

9

6 Appendix. Results of the proposed method: iPhone3

iPhone original

Enhanced with our method

Figure 13: Image results of our method for iPhone DPED test images.

3All visual results for iPhone are available at http://people.ee.ethz.ch/˜ihnatova/dped_iphone.html

10

iPhone original

Enhanced with our method

Figure 14: Image results of our method for iPhone DPED test images.

11

7 Appendix. Results of the proposed method: BlackBerry4

BlackBerry original

Enhanced with our method

Figure 15: Image results of our method for BlackBerry DPED test images.

4All visual results for BlackBerry are available at http://people.ee.ethz.ch/˜ihnatova/dped_blackberry.html

12

BlackBerry original

Enhanced with our method

Figure 16: Image results of our method for BlackBerry DPED test images.

13

8 Appendix. Results of the proposed method: Sony5

Sony original

Enhanced with our method

Figure 17: Image results of our method for Sony DPED test images.

5All visual results for Sony are available at http://people.ee.ethz.ch/˜ihnatova/dped_sony.html

14

Sony original

Enhanced with our method

Figure 18: Image results of our method for Sony DPED test images.

15

9 Appendix. Loss analysis

In this section, we study the contribution of different terms of the proposed perceptual loss function. For this purpose, we consider
four different loss combinations: 1) the proposed one [color + content + texture], 2) [content + texture] loss, 3) [MSE + texture]
loss and 4) [MSE] loss. For each of these target loss combinations, a CNN was trained on the DPED dataset and validated on its test
subset. The results of this experiment are provided in Table 3 and visual results are shown in Fig. 19. As one can see, the adversarial
network that stands behind the texture loss can cause signiﬁcant color deviations, and the additional MSE term cannot effectively
suppress them since it is not precise in this task (images are not perfectly aligned). Content loss shows better results in this case
since it is less sensitive to image mismatches. Adding an extra color term further improves the resulting images, making the colors
more saturated and closer to the target. Single MSE demonstrates high PSNR and SSIM values and natural color rendition while
causing strong artifacts and slightly degrading image sharpness. Overall our proposed [color + content + texture] loss leads to the
best visual results while at the same time achieves top SSIM scores.

Table 3: PSNR/SSIM scores for different loss functions.

Phone

iPhone
BlackBerry
Sony

Color + Content + Texture Content + Texture MSE + Texture
PSNR
PSNR
19.05
20.08
19.64
20.07
21.81
21.59

SSIM
0.9201
0.9328
0.9437

SSIM
0.9166
0.9312
0.9426

PSNR
20.11
20.13
21.72

SSIM PSNR
20.56
0.9125
20.15
0.9241
21.35
0.9416

MSE

SSIM
0.9198
0.9292
0.9453

Color + Content + Texture

Content + Texture

MSE + Texture

MSE

Figure 19: Result images for iPhone camera for 4 different target loss functions.

16

