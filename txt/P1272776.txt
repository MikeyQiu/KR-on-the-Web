Learning with Structured Representations for Negation Scope Extraction

Hao Li and Wei Lu
Singapore University of Technology and Design
8 Somapah Road, Singapore, 487372
hao li@mymail.sutd.edu.sg, luwei@sutd.edu.sg

Abstract

We report an empirical study on the task of
negation scope extraction given the nega-
tion cue. Our key observation is that cer-
tain useful information such as features re-
lated to negation cue, long distance de-
pendencies as well as some latent struc-
tural information can be exploited for such
a task. We design approaches based on
conditional random ﬁelds (CRF), semi-
Markov CRF, as well as latent-variable
CRF models to capture such information.
Extensive experiments on several standard
datasets demonstrate that our approaches
are able to achieve better results than exist-
ing approaches reported in the literature.

1

Introduction

Negation is an important linguistic phenomenon
(Morante and Sporleder, 2012), which reverts the
assertion associated with a proposition. Broadly
speaking, the part of the sentence being negated
is called negation scope (Huddleston et al., 2002).
Automatic negation scope detection is a vital but
challenging task that has various applications in
areas such as text mining (Szarvas et al., 2008),
and sentiment analysis (Wiegand et al., 2010;
Councill et al., 2010). Negation scope detection
task commonly involves a negation cue which can
be one of the following 3 types – either a single
word (e.g., not), afﬁxes (e.g., im-, -less) or multi-
ple words (e.g., no longer) expressing negation.
Figure 1 presents two real examples for such a
task, where the ﬁrst example involves discontin-
uous negation scope of an afﬁx cue. The second
example shows a discontinuous negation cue and
its corresponding discontinuous negation scope.

Most existing approaches tackled the negation
scope detection problem from a boundary detec-

He declares that he heard cries but is unable
to state from what direction they came .

There is neither money nor credit in it , and
yet one would wish to tidy it up .

Figure 1: Two examples with negation cues in
bold blue and negation scope in red.

tion perspective, aiming to identify whether each
word token in the sentence belongs to the nega-
tion scope or not. To perform sequence labeling,
various approaches have been proposed based on
models such as support vector machines (SVMs)
(with heuristic rules) (Read et al., 2012; de Al-
bornoz et al., 2012; Packard et al., 2014), condi-
tional random ﬁelds (CRF) (Lapponi et al., 2012;
Chowdhury and Mahbub, 2012; White, 2012; Zou
et al., 2015) and neural networks (Fancellu et al.,
2016; Qian et al., 2016). These models typically
either make use of external resources for extract-
ing complex syntax and grammar features, or are
based on neural architectures such as long short-
term memory networks (LSTM) and convolutional
neural networks (CNNs) to extract automatic fea-
tures.

We observe that there are some useful fea-
tures that can be explicitly and implicitly captured
and modelled in the learning process for negation
scope extraction. We use the term partial scope to
refer to a continuous text span that is part of dis-
continuous scope, and use the term gap to refer to
the text span between two pieces of partial scope.
From the ﬁrst example in Figure 1 we can ob-
serve that, with the negation cue as a preﬁx in a
word, the partial scope before, after and in the
middle of the negation cue differ in terms of com-
position of words and their associated syntactic
roles in the sentence. Furthermore, the type of cue

He

declares

that

heard

cries

unable

state

from

what

direction

they

came

Linear
Semi o
Semi i
Semi io

Latent i

Latent o

Latent io

I

I

I

I

I0

I1

I

I0

I1

O

O

O

O

O

O0

O1

O0

O1

O

O

O

O0

O1

O0

O1

he

O

O

O

O0

O1

O0

O1

O

O

O

O0

O1

O0

O1

O

O

O

O0

O1

O0

O1

but

O

O

O

O0

O1

O0

O1

is

I

I

I

I

I0

I1

I

I0

I1

to

I

I

I0

I1

I

I0

I1

I

I

I0

I1

I

I0

I1

I

I

I0

I1

I

I0

I1

I

I

I0

I1

I

I0

I1

I

I

I0

I1

I

I0

I1

I

I

I0

I1

I

I0

I1

I

I

I0

I1

I

I0

I1

I

I

I0

I1

I

I0

I1

.

O

O

O

O

O

O0

O1

O0

O1

Figure 2: Label assignments of model variants for the ﬁrst example mentioned in Figure 1.

as we mentioned earlier may also reveal crucial
information for this task. Moreover, two pieces of
partial scope separated by a gap might have some
long distance dependencies. For instance, in the
ﬁrst sentence of Figure 1, “He” as the ﬁrst partial
scope is the subject phrase of the token “is” which
is the ﬁrst word of the second partial scope with a
long gap in between.

Similarly, the second example shows that a dis-
continuous negation cue involves multiple words,
neither and nor, which shows the importance of
cue features and some long distance dependencies
among text spans.

Furthermore, besides explicit features that we
are able to deﬁne, we believe there exist some
implicit linguistic patterns within the scope for a
given negation cue. While it is possible to manu-
ally design linguistic features to extract such pat-
terns, approaches that can automatically capture
such implicit patterns in a domain and language
independent manner can be more attractive. How
to design models that can effectively capture such
features mentioned above remains a research ques-
tion to be answered.

In the paper, we design different models to cap-
ture such useful features based on the above moti-
vations, and report our empirical ﬁndings through
extensive experiments. We release our code at
http://statnlp.org/research/st.

2 Approaches

Based on the observations described earlier, we
aim to capture three types of features by design-
ing different models based on CRF.

Negation Cue

The linear CRF (Lafferty et al., 2001) model
(which we refer to as Linear in this paper), as il-
lustrated in Figure 2 , is used to capture negation
cue related features. The probability of predicting

a possible output y, the label sequence capturing
negation scope, given an input sentence x is:

p(y|x) =

exp (wT f (x, y))
y(cid:48)∈Y(x) exp(wT f (x, y(cid:48)))

(cid:80)

where f (x, y) is a feature function deﬁned over
the (x, y) pair. The negation cue related features
mainly involve cue type, position of the cue, as
well as relative positions of each partial scope. For
example, the cue type refers to the string form of
the cue, which could be a single word, an afﬁx
(preﬁx or sufﬁx), or a multi-word expression.

We follow a standard approach to assign tags to
words. Speciﬁcally, O and I are used to indicate
whether a word appears outside or inside the nega-
tion scope respectively.

Long Distance Dependencies

We use the semi-CRF (Sarawagi and Cohen, 2004)
model (referred to as Semi) to capture long dis-
tance dependencies. The semi-CRF is an exten-
sion to the linear-CRF. The difference is that the
output y may not be a sequence of individual
words. Rather, it is now a sequence of spans,
where each span consists of one or more words.
The semi-CRF model is more expressive than the
linear-CRF model as such a model is able to cap-
ture features that are deﬁned at the span level, al-
lowing longer-range dependencies to be captured.
Since the Semi approach is capable of model-
ing a span (which can be a gap or partial scope),
we are able to model the features between two sep-
arate text spans. We propose three variants for the
Semi model, as illustrated in Figure 2, to capture
different types of long distance features. The Semi
i model regards a piece of partial scope as a span
in order to capture long distance dependencies be-
tween two gaps. The Semi o model treats a gap
as a span to capture long distance dependencies
between two pieces of partial scope. The Semi io

model regards both partial scope and gaps as spans
to capture both types of long distance dependen-
cies mentioned above.

Implicit Patterns

The latent variable CRF model, denoted as La-
tent, is used to model implicit patterns. The prob-
ability of predicting a possible output y, which is
the label sequence capturing negation scope infor-
mation, given an input sentence x is deﬁned as:

p(y|x) =

(cid:80)

(cid:80)

h exp (wT f (x, y, h))

y(cid:48)∈Y(x)

(cid:80)

h(cid:48) exp(wT f (x, y(cid:48), h(cid:48)))

where h is a latent variable encoding the implicit
pattern.

We believe such latent pattern information can
be learned from data without any linguistic guid-
ance. The Latent model is capable of capturing
this type of implicit signals. For example, as illus-
trated as Latent io in Figure 2, each position has
O0, O1, I0 and I1 as latent tags. This way, we
can construct features of forms such as “He/Oi-
declares/Ij” that capture the underlying interac-
tions between the words and the latent tag patterns.
In order to investigate the relation between la-
tent variables and tags, we proposed another two
latent models. The Latent i only considers latent
variables on I tags (for partial scope), while La-
tent o only takes latent variables on O tags (for
gaps).

3 Experimental Setup

CDS-CO
#Sentence
#Instance

Train Dev
144
847
173
983

Test
235
264

Table 1: Statistics of the CDS-CO corpus.

We mainly conducted our experiments on the
CDS-CO corpus released from the *SEM2012
shared task (Morante and Blanco, 2012). The
negation cue and corresponding negation scope
are annotated. For each word token, the corre-
sponding POS tag and the syntax tree informa-
tion are provided.
If the sentence contains mul-
tiple negation cues, each of them is annotated
separately. The corpus statistics is listed in Ta-
ble 1. During training and testing, following prior
works (Fancellu et al., 2016), only instances with
at least one negation cue will be selected. For
the sentence containing multiple negation cues, we
create as many copies as the number of instances,

System

Read et al. (2012)

Packard et al. (2014)

Fancellu et al. (2016)
Linear (-c -r)
Linear (-r)
Linear (-c)
Linear
Semi i
Semi o
Semi io
Latent i
Latent o
Latent io

Token-Level
R.
-
90.4
85.1
73.9
78.4
78.9
82.6
84.1
85.3
84.1
83.4
83.9
83.2

F1
-
88.2
88.7
78.6
84.1
84.5
88.1
89.2
89.4
89.0
88.6
87.1
88.6

P.
-
86.1
92.6
84.7
90.6
91.0
94.4
95.0
94.0
94.5
94.4
90.4
94.8

Scope-Level (Exact Scope Match)
PA. RA. F1A PB. RB. F1B
98.8
98.8
99.4
99.2
100
99.3
100
100
100
100
99.4
99.4
100

-
-
-
51.5
61.4
60.0
69.3
69.4
71.1
70.3
69.6
66.3
70.6

-
-
-
49.4
60.6
56.6
67.9
67.5
69.1
68.3
67.9
65.5
69.5

64.3
65.5
63.9
49.4
60.6
56.6
67.9
67.5
69.1
68.3
67.9
65.5
69.5

77.9
78.7
77.8
65.6
75.5
72.1
80.9
80.6
81.7
81.2
80.7
78.9
82.0

-
-
-
50.4
61.0
58.3
68.6
68.4
70.1
69.3
68.7
65.9
70.0

Table 2: Main results on CDS-CO data

each of which has only one negation cue and its
corresponding negation scope.

The L2 regularization hyper-parameter λ is set
to 0.1 based on the development set. We conduct
evaluations of negation scope extraction based on
metrics at token-level evaluations and scope-level
evaluations. There are two versions of evalua-
tion metrics, referred to as version A and version
B1, deﬁned at the scope-level that can be used to
measure the performance according to *SEM2012
shared task (Morante and Blanco, 2012).

Moreover, to understand the model robustness,
we also conducted additional experiments on Bio-
Scope (Szarvas et al., 2008) and CNeSP (Zou
et al., 2015).

4 Results and Discussion

4.1 Main Results

The main results on the CDS-CO corpus are shown
in Table 2. PA., RA. and F1A. are precision, recall
and F1 measure under version A, while PB., RB.,
F1B. are for version B. Note that none of the prior
works reported results under version B. Moreover,
c refers to the cue type features, r refers to relative
position of partial scope with respect to the cue.

We focus on Linear models ﬁrst, where Linear
(-c -r) is the baseline without features c and r for
comparisons. By adding c, the Linear (-r) model
improves the performance by 9.9 and 10.6 in terms
of F1 scores for both versions of evaluation meth-
ods at the scope level respectively. By adding r
solely, the Linear (-c) model increases the perfor-
mance by 6.5 and 7.9 on F1 scores of both ver-
sions. By adding both c and r, the Linear model
increases the performance by 15.3 and 18.2 on F1
scores at the scope level, outperforming previous

1The ofﬁcial evaluation contains both two versions. We
explain the differences between two versions of evaluation in
the supplementary material.

This person is alone and can not be approached by
letter without a breach of that absolute secrecy .

He has been there for ten days, and neither Mr. War-
ren , nor I , nor the girl has once set eyes upon him.

Figure 3: Examples showing incorrect instances
in the Linear model but correct in Semi o model
(The incorrect predictions by Linear model are un-
derlined).

works. These improvements demonstrate the im-
portance of the negation cue features.

Compared with the Linear model, Semi models
achieve better results. Speciﬁcally, Semi o model
achieves the best result on F1B at the scope level
among all the models and achieves the highest re-
sult on F1 at the token level.

The Latent io model outperforms all the other
models in terms of F1A at scope level and a com-
petitive result in terms of F1B.

4.2 Analysis

By analyzing predictions that are incorrect in Lin-
ear model that are correct in the Semi models,
we have some interesting observations explaining
why Semi models work better. The ﬁrst type of
observation is that the Semi models tend to pre-
dict more correct scope tokens, which improves
results at scope level and token level. The second
type is that Semi models recover some missing re-
mote partial scope, which shows the importance
of capturing long distance dependencies. For in-
stance, in the ﬁrst example of Figure 3, the Semi
models recover the subject phrase “This person” as
the ﬁrst partial scope. The third type happens on
discontinuous cues as well as multiple short gaps
as shown in the second example in Figure 3. The
Linear model fails to predict “Mr. Waren ,” and “I
,’’ as two pieces of partial scope between three cue
words which are also gaps. These observations in-
dicate that Semi models are capable of capturing
long distance features and can correct some wrong
predictions made by the Linear model.

Similarly, by analyzing predictions that are in-
correct in Linear model that are correct in the La-
tent models, we observe that Latent models tend
to make more accurate predictions. We found that
there is only 1 incorrect prediction from the La-
tent io that is corrected by the Linear model. This
indicates that the Latent io model is able to ﬁx er-

System

Li et al. (2010)

Velldal et al. (2012)

Zou et al. (2013)

Qian et al. (2016)
Linear
Semi io
Latent io

Clinical

Abstract

Full Paper
F1T F1A P CS F1T F1A P CS F1T F1A P CS
-
89.8
-
74.4 -
-
85.3
-
-
89.9 -
89.7
90.3 90.3 82.3 80.8 74.0 58.8 96.4 96.6 93.3
92.1 91.3 84.1 83.1 75.1 60.1 97.5 97.1 94.4
91.5 90.8 83.2 79.5 71.0 55.1 97.3 97.0 94.1

64.0 -
-
90.7 -
-
61.2 -
-
55.3 94.4 -

-
81.8 -
70.2 -
-
76.9 -
-
77.1 83.5 -

Table 3: Results on BioScope datasets.

rors for the Linear model without producing other
wrong predictions. This analysis implies that the
Latent models are able to capture some latent pat-
terns to some extent. The performance of the La-
tent o model is lower than the performance of La-
tent io and Latent i, indicating that latent vari-
ables on tag I captures more information.

Let us focus on the token-level performance of
our model. We obtained satisfactory precision
scores, but comparatively low recall scores. Mean-
while, at the scope level, our precision scores are
comparable to the previous works, but our recall
scores are consistently better, indicating our mod-
els are capable of successfully recovering more
gold scope information from the test data. Our
further analysis shows that our models tend to
predict negation scope that is signiﬁcantly shorter
than the gold scope for those instances that involve
some long negation scope. We ﬁnd that around
1/3 of the word tokens appearing inside any nega-
tion scope come from such instances. These facts
make token-level recall of our models compara-
tively low.

In addition, we inspect the top 200 features with
highest feature weights, and we ﬁnd that around
45% of them are related to POS tags with label
transition (the string form concatenating current
tag and next tag), indicating POS tag features play
an important role in the learning process for our
models.

4.3 Experiments on Model Robustness

To understand the robustness of our model, we ad-
ditionally conducted two sets of experiments.

BioScope

The BioScope corpus (Szarvas et al., 2008) con-
tains three data collections from medical domains:
Abstract, Full Paper and Clinical. NLTK (Bird
and Loper, 2004) is used to perform tokeniza-
tion and POS tagging for preprocessing. Follow-
ing (Morante and Daelemans, 2009; Qian et al.,
2016), we perform 10-fold cross validation on Ab-

System

(Zou et al., 2015)
Linear
Semi io
Latent io

Product Review

F1T
-
89.60
90.78
90.60

F1A
-
81.86
83.49
83.95

F1B
-
69.39
71.69
72.43

P CS
60.93
69.39
71.69
72.43

Table 4: Results on Product Review from CNeSP.

stract, whereas the results on Full Paper and Clin-
ical are obtained by training on the full Abstract
dataset and testing on Full Paper and Clinical re-
spectively. The latter experiment can help us un-
derstand the robustness of the model by applying
the learned model to different types of text within
the same domain.

The Semi io model mostly outperforms the
other models. Comparing against all the prior
works, our models are able to achieve better results
on Abstract under both token-level and scope-level
F1 as well as P CS 2. Moreover, we also ob-
tain signiﬁcantly higher results in terms of scope-
level F1 on Full Paper and Clinical, indicating
the good robustness of our approaches. Note that
the P CS score on Full Paper is not as satisfac-
tory as on Clinical. This is largely because the
model is trained on Abstract, but Full Paper con-
tains much longer sentences with longer negation
scope, which presents a challenge for our model
as discussed in the previous sections. On the other
hand, the baseline systems (Li et al., 2010; Velldal
et al., 2012) adopt features from syntactic trees,
which allow them to capture long-distance syntac-
tic dependencies.

CNeSP

To understand how well our model works on
another language other than English, we also
conducted an experiment on the Product Review
collection from the CNeSP corpus (Zou et al.,
2015). We used Jieba (Sun, 2012) and Stanford
tagger (Toutanova and Manning, 2000) to per-
form Chinese word segmentation and POS tag-
ging. Following the data splitting scheme de-
scribed in (Zou et al., 2015), we performed 10-
fold cross-validation and the results are shown in
Table 4. Our model obtains a signiﬁcantly higher
P CS score than the model reported in (Zou et al.,
2015). The results further conﬁrm the robustness
of our model, showing it is language independent.

2P CS is deﬁned as percentage of correct scope which is

the same as the recall score.

5 Related Work

The negation scope extraction task has been stud-
ied within the NLP community through the Bio-
Scope corpus (Szarvas et al., 2008) in biomed-
ical domain, usually together with the negation
cue detection task. The negation scope detection
task has mostly been regarded as a boundary de-
tection task. Morante et al. (2008) and Morante
and Daelemans (2009) tackled the task by build-
ing classiﬁers based on k-nearest neighbors algo-
rithm (Cover and Hart, 1967), SVM (Cortes and
Vapnik, 1995) as well as CRF (Lafferty et al.,
2001) on each token to determine if it is inside
the scope.
Li et al. (2010) incorporated more
syntactic features such as parse tree information
by adopting shallow semantic parsing (Gildea and
Palmer, 2002; Punyakanok et al., 2005) for build-
ing an SVM classiﬁer. With similar motivation,
Apostolova et al. (2011) proposed a rule-based
method to extract lexico-syntactic patterns to iden-
tify the scope boundaries. To further investigate
the syntactic features, Zou et al. (2013) extracted
more syntactic information from constituency and
dependency trees obtained from parsers to feed
into the SVM classiﬁer.
Qian et al. (2016)
adopted a convolutional neural network based ap-
proach (LeCun et al., 1989) to extract position fea-
tures and syntactic path features encoding the path
from the cue to the candidate token along the con-
stituency trees. They also captured relative posi-
tion information between the words in the scope
and the cue as features in their model.

In order to resolve the corpus scarcity issue in
different languages for the negation scope extrac-
tion task, Zou et al. (2015) constructed a Chinese
corpus CNeSP analogous to the BioScope corpus.
They again tackled the negation scope extraction
task using CRF with rich syntactic features ex-
tracted from constituency and dependency trees.

6 Conclusion

We explored several approaches based on CRF
to capture some useful features for solving the
task of extracting negation scope based on a given
negation cue in a sentence. We conducted exten-
sive experiments on a standard dataset, and the re-
sults show that our models are able to achieve sig-
niﬁcantly better results than various previous ap-
proaches. We also demonstrated the robustness of
our approaches through extensive analysis as well
as additional experiments on other datasets.

Acknowledgments

We would like to thank the anonymous reviewers
for their constructive comments on this work. We
would also like to thank Bowei Zou for answer-
ing our questions. This work is supported by Sin-
gapore Ministry of Education Academic Research
Fund (AcRF) Tier 2 Project MOE2017-T2-1-156.

References

Emilia Apostolova, Noriko Tomuro,

and Dina
Demner-Fushman. 2011.
Automatic extraction
of lexico-syntactic patterns for detection of nega-
In Proc. of ACL.
tion and speculation scopes.
http://www.aclweb.org/anthology/P11-2049.

Steven Bird and Edward Loper. 2004. Nltk: The
In Proc. of ACL.

natural
https://doi.org/10.3115/1219044.1219075.

language toolkit.

Md Chowdhury and Faisal Mahbub. 2012.

Fbk:
Exploiting phrasal and contextual clues for nega-
In Proc. of *SEM2012.
tion scope detection.
http://www.aclweb.org/anthology/S12-1045.

Corinna Cortes
Support-vector
ing
/10.1007/BF00994018.pdf.

and Vladimir Vapnik.
Machine
networks.

1995.
learn-
https://link.springer.com/content/pdf//

Isaac G Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What’s great and what’s not:
learning to classify the scope of negation for im-
In Proc. of NeSp-
proved sentiment analysis.
NLP.
https://aclanthology.info/pdf/W/W10/W10-
3110.pdf.

Thomas Cover and Peter Hart. 1967.
est neighbor pattern classiﬁcation.
transactions
information
on
https://ieeexplore.ieee.org/document/1053964.

Near-
IEEE
theory

Jorge Carrillo de Albornoz, Laura Plaza, Alberto
D´ıaz, and Miguel Ballesteros. 2012.
Ucm-
i: A rule-based syntactic approach for resolving
In Proc. of *SEM2012.
the scope of negation.
http://www.aclweb.org/anthology/S12-1037.

Federico

Fancellu, Adam Lopez,

nie L Webber. 2016.
negation scope detection.
https://doi.org/10.18653/v1/p16-1047.

and Bon-
Neural networks for
In Proc. of ACL.

Daniel Gildea

and Martha

Palmer.

The necessity of parsing for predicate
gument
https://doi.org/10.3115/1073083.1073124.

recognition.

In Proc.

2002.
ar-
of ACL.

John Lafferty, Andrew McCallum,

and Fer-
Conditional random
nando CN Pereira. 2001.
ﬁelds: Probabilistic models for segmenting and
ICML.
In Proc. of
labeling sequence data.
http://repository.upenn.edu/cis papers/159.

Emanuele Lapponi, Erik Velldal, Lilja Øvrelid, and
Jonathon Read. 2012. Uio 2: sequence-labeling
In Proc. of
negation using dependency features.
*SEM2012. http://www.aclweb.org/anthology/S12-
1042.

Henderson,

Yann LeCun, Bernhard Boser,

John S Denker,
E Howard,
and Lawrence D Jackel.
Backpropagation applied to handwrit-
Neural Computation

Donnie
Wayne Hubbard,
1989.
ten zip code recognition.
https://doi.org/10.1162/neco.1989.1.4.541.

Richard

Junhui Li, Guodong Zhou, Hongling Wang, and
Qiaoming Zhu. 2010. Learning the scope of nega-
tion via shallow semantic parsing. In Proc. of COL-
ING. http://www.aclweb.org/anthology/C10-1076.

Roser Morante and Eduardo Blanco. 2012.

*sem
2012 shared task: Resolving the scope and fo-
In Proc. of *SEM 2012.
cus of negation.
http://www.aclweb.org/anthology/S12-1035.

Roser Morante

and Walter Daelemans.

2009.
A metalearning approach to processing the
In Proc. of CoNLL.
scope of negation.
http://www.aclweb.org/anthology/W09-1105.

Roser Morante, Anthony Liekens, and Walter Daele-
Learning the scope of nega-
In Proc. of ACL.

mans. 2008.
tion in biomedical
https://doi.org/10.3115/1613715.1613805.

texts.

Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special
issue. Computational linguistics 38(2):223–260.
https://doi.org/10.1162/COLI a 00095.

Woodley Packard, Emily M Bender, Jonathon Read,
Stephan Oepen, and Rebecca Dridan. 2014. Sim-
ple negation scope resolution through deep parsing:
A semantic solution to a semantic problem. In Proc.
of ACL. https://doi.org/10.3115/v1/p14-1007.

Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
The necessity of syntactic parsing for
IJCAI.

2005.
semantic role labeling.
http://www.ijcai.org/Proceedings/05/Papers/1672.pdf.

In Proc. of

Zhong Qian, Peifeng Li, Qiaoming Zhu, Guodong
Zhou, Zhunchen Luo, and Wei Luo. 2016. Spec-
ulation and negation scope detection via convo-
In Proc. of EMNLP.
lutional neural networks.
https://doi.org/10.18653/v1/d16-1078.

Rodney Huddleston, Geoffrey K Pullum,

et al.
2002. The cambridge grammar of english. Lan-
guage. Cambridge: Cambridge University Press
http://www.academia.edu/download/37907813//
/2001025630.pdf.

Jonathon Read, Erik Velldal, Lilja Øvrelid, and
Uio 1: Constituent-
nega-
for
ranking
In Proc. of *SEM2012.

Stephan Oepen. 2012.
based
discriminative
tion resolution.
http://www.aclweb.org/anthology/S12-1041.

random ﬁelds for

Sunita Sarawagi and William W Cohen. 2004.
in-
Semi-markov conditional
In Advances in neural
formation extraction.
information processing systems. pages 1185–
1192.
http://papers.nips.cc/paper/2648-semi-
markov-conditional-random-ﬁelds-for-information-
extraction.pdf.

J Sun. 2012.

Jieba chinese word segmentation tool.

https://github.com/fxsjy/jieba.

Gy¨orgy

Szarvas,

Veronika Vincze,

and J´anos Csirik. 2008.

Rich´ard
The bio-
Farkas,
un-
scope corpus:
annotation for negation,
texts.
certainty and their scope in biomedical
the Workshop on Current Trends
In Proc. of
in Biomedical Natural Language Processing.
https://aclanthology.info/pdf/W/W08/W08-
0606.pdf.

2000.

Kristina Toutanova

and Christopher D Man-
ning.
knowledge
the
Enriching
sources used in a maximum entropy part-
of EMNLP.
of-speech
https://doi.org/10.3115/1117794.1117802.

In Proc.

tagger.

Erik Velldal, Lilja Øvrelid,
Stephan Oepen. 2012.

Jonathon Read, and
Speculation and nega-

Rules,
Computational

tion:
tax.
https://doi.org/10.1162/COLI a 00126.

and the role of

syn-
linguistics 38(2):369–410.

rankers,

James Paul White. 2012. Uwashington: Negation res-
olution using machine learning methods. In Proc. of
*SEM2012. http://www.aclweb.org/anthology/S12-
1044.

Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andr´es Montoyo. 2010.
A survey on the role of negation in sentiment
the workshop on nega-
analysis.
tion and speculation in natural language process-
ing.
https://aclanthology.info/pdf/W/W10/W10-
3111.pdf.

In Proc. of

Bowei Zou, Guodong Zhou, and Qiaoming Zhu.
2013.
Tree kernel-based negation and spec-
ulation scope detection with structured syn-
In Proc. of EMNLP.
tactic parse features.
http://www.aclweb.org/anthology/D13-1099.

Bowei Zou, Qiaoming Zhu, and Guodong Zhou.
Negation and speculation identiﬁca-
In Proc. of ACL.

2015.
tion in chinese language.
https://doi.org/10.3115/v1/p15-1064.

