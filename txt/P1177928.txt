8
1
0
2
 
r
a

 

M
1
3
 
 
]

C
O
.
h
t
a
m

[
 
 
3
v
2
6
4
7
0
.
0
1
7
1
:
v
i
X
r
a

Tracking the gradients using the Hessian:
A new look at variance reducing stochastic methods

Robert M. Gower
LTCI, T´el´ecom-Paristech, Universit Paris-Saclay∗
robert.gower@telecom-paristech.fr

Nicolas Le Roux
Google Brain
nlr@google.com

Francis Bach
INRIA, D´epartement d’Informatique de l’ENS
francis.bach@inria.fr

April 3, 2018

Abstract

Our goal is to improve variance reducing stochastic methods through better control
variates. We ﬁrst propose a modiﬁcation of SVRG which uses the Hessian to track gradients
over time, rather than to recondition, increasing the correlation of the control variates and
leading to faster theoretical convergence close to the optimum. We then propose accurate
and computationally eﬃcient approximations to the Hessian, both using a diagonal and a
low-rank matrix. Finally, we demonstrate the eﬀectiveness of our method on a wide range
of problems.

1

Introduction

Many machine learning problems can be written as the minimization of a loss over a set of
samples. Though this set can be inﬁnite, it is in most cases ﬁnite, in which case the problem
becomes the minimization of the average loss over N samples, i.e.

θ∗ = arg min

θ∈Rd

1
N

N
(cid:88)

i=1

fi(θ) = arg min

F (θ),

θ∈Rd

(1)

where fi(θ) is the loss incurred by parameters θ for the i-th sample. In this work, we make the
assumption that each function fi is strongly convex.

A common way of solving Eq. (1) is through a stochastic gradient method (Robbins and
Monro 1951). Starting from θ0, the standard stochastic method samples i uniformly at random
in {1, . . . , N } then computes θt+1 = θt − γt
|θ=θt where γt can be a scalar or a full matrix.
To unclutter notation, we will use g to denote the gradient, i.e. gi(θt) = ∂fi(θ)

∂fi(θ)
∂θ

|θ=θt.

Stochastic gradient updates are not convergent for constant stepsizes γt since the variance
of gi(θt) does not converge to 0 (Schmidt 2014). Achieving convergence requires a decreasing

∂θ

∗Robert M. Gower carried out this work while at INRIA in the Sierra team, funded by the Fondation de

Sciences Math´ematiques de Paris (FSMP)

1

schedule which impacts the convergence speed. By reducing the variance of these updates, one
can hope to achieve convergence with larger or even constant stepsizes and thus obtain a faster
rate. We now review the most popular stochastic variance reducing methods.

SAG (Le Roux et al. 2012; Schmidt et al. 2013) was the ﬁrst stochastic method to achieve a
linear convergence rate for strongly convex problems over a ﬁnite dataset. It required a storage
linear in N and used biased parameter updates. SDCA (Zhang et al. 2013; Shalev-Shwartz and
Zhang 2013) is a similar method solving a dual minimization problem for an important subclass
of such problems. Compared to SAG which is adaptive, SDCA requires the knowledge of the
strong convexity constant.

SVRG (Johnson and Zhang 2013) achieves a similar rate with a storage requirement indepen-
dent of N , albeit at the expense of a constant factor increase in the computational cost. SVRG
also requires an extra parameter besides the stepsize: the frequency at which the true gradient
is recomputed. This is also the ﬁrst work to provide the variance reduction interpretation. In
this paper, we will mostly consider extensions of SVRG.

SAGA (Defazio et al. 2014) modiﬁes SAG to get unbiased updates, leading to better
convergence rates. Using a proximal algorithm, SAGA also works for non-smooth regularizers.
MISO (Mairal 2013) is a majorization-minimization algorithm achieving a linear convergence
rate on strongly convex problems but it can also be used to solve nonconvex problems under
some conditions.

All of these methods reduce variance by incorporating knowledge about gradients on all
datapoints at every timestep rather than only using the gradient for the sampled datapoint.
Where they diﬀer is in which gradients they use and how they are incorporated.

This paper builds up on all these methods and makes the following contributions:

• We recall in Section 2 that most of these methods can be seen as using control variates, an
observation made by Defazio et al. (2014), then highlight that none of the control variates
currently used track the gradients.

• We propose in Section 3 a new control variate that tracks the gradient by building a linear
model which is based on the Hessian matrix. This second-order information is used to
further reduce the variance of the parameter updates, and not as a preconditioner.

• Since calculating Hessians typically scales as O(d2), we propose several new approximations
in Section 4 based on diagonal or low-rank matrices. Such techniques are usually only
dedicated to preconditioning.

• We present in Section 5 a faster theoretical convergence rate than that of SVRG within a

neighborhood of the solution.

• Finally, in our experiments in Section 6, we show a signiﬁcant and consistent improvement

over SVRG.

Recently new work based on tracking the gradient using the Hessians of the fi functions has
come to our attention (Wai et al. 2017). In this work the authors use incremental updates of
the Hessian with the 2nd order Taylor expansion to maintain a rolling average estimate of the
gradient. Our work also makes use of incremental Hessian updates together with the 2nd order
Taylor expansion to estimate the gradient. Though diﬀerently from Wai et al. 2017 we focus on
improving the SVRG method, as opposed to maintaining a rolling average. Furthermore, Wai
et al. 2017 use the full Hessian of the fi functions, which results in an iteration cost of d2 while
we focus on using diagonal and low rank Hessian approximations to keep the iteration cost
linear in d.

2

2 Control variates

We now recall that most variance reduced methods use control variates, a well-known technique
in Monte-Carlo simulation (see, e.g., Rubinstein and Kroese (2016)) designed to reduce the
variance of the estimate of the expectation of a random variable. Let us start by describing this
technique.

We try to estimate the unknown expectation ¯x of a random variable x and that we have access
to another random variable, z, whose expectation ¯z is known. Then the quantity xz = x − z + ¯z
has expectation ¯x and variance V [xz] = V [x] + V [z] − 2Cov(x, z) where V [] is the variance and
Cov[] the covariance. V [xz] is lower than V [x] whenever z is suﬃciently positively correlated
with x and the variance reduction is larger when the control variate is more correlated with the
random variable.

Using the true gradient g(θt) = 1
N

i gi(θt), batch methods achieve a linear convergence
rate, albeit with an update computational cost linear in N . Stochastic methods estimate the
true gradient through a single1 sample gi(θt), leading to a slower convergence rate but cheaper
updates. The goal is thus to estimate the true gradient as accurately as possible while still
maintaining update costs independent of N . As the true gradient is the expectation of the
individual gradients, control variates can be applied.

(cid:80)

More speciﬁcally, using a control variate to reduce the variance of the gradient estimation

means replacing the quantity gi(θt) with

˜gi(θt) = gi(θt) − zi(θt) +

zj(θt) ,

1
N

(cid:88)

j

transforming the stochastic gradient updates into

θt+1 = θt − γt

gi(θt) − zi(θt) +

(cid:16)

(cid:88)

(cid:17)

zj(θt)

.

1
N

j

(2)

(3)

The new gradient estimate ˜gi(θt) is an unbiased estimator of the true gradient g(θt) with lower
variance than gi(θt) when zi(θt) is positively correlated with gi(θt).

The main diﬀerence between existing variance reducing gradient methods lies in the chosen
zi(θt). SAGA, for instance, chooses zi(θt) = gi(¯θi) the last gradient computed for sample i.
Using such a control variate requires a storage linear in N . SVRG uses zi(θt) = gi(¯θt), the
gradient computed at a ﬁxed ¯θt for all samples. This allows for the computation of the control
variate while only having to store ¯θt. As it is the same for all datapoints, the storage requirement
is independent of N .

One commonality to all existing methods is that each control variate remains constant until
a new gradient has been computed for that datapoint. In particular, this means that control
variates can easily become outdated if the gradients change quickly, leading to lower correlation
and thus higher variance of the estimator. Since the variance of the estimator is directly linked to
the convergence speed, improving correlation will lead to faster convergence. In the next section,
we will show how to modify SVRG to make use of second-order approximations, maintaining
highly correlated control variates longer than in SVRG.

3 Tracking gradients with second-order control variates

We now show how we can modify all stored gradients with a cost independent of N by using
second-order information about each individual function.

1Or a small number in the case of minibatch gradient methods.

3

All algorithms of the previous section used for zi(θt) the gradient computed for the same
sample at a previous parameter ¯θ. In particular, zi(θt) does not change when the ith datapoint is
not sampled. While this is eﬃcient close to convergence when θ does not move much, correlation
might be low early on, leading to slower convergence.

In particular, the convergence rate of SAGA is proportional to (cid:0)1 −

(cid:1), with µ the
strong-convexity constant and Lmax the maximum smoothness constant of the individual fi’s.2
It is crucial to note that Lmax is not the same as L, the smoothness constant of the full function
F . More precisely, we have Lmax/d ≤ L ≤ Lmax. When all individual gradients are equal we
have that L = Lmax and SAGA is exactly N times faster than full gradient descent. On the
other hand, when the individual gradients are uniformly distributed over the whole space, we
have L = Lmax/d and the speed improvement of SAGA depends on N/d.

µ
2(µN +Lmax)

When µ/Lmax is much smaller than 1/N , the poor condition number means that θ does not
move much and SAGA should be close to batch gradient descent.3 Indeed, the rate is close
to (1 − µ/Lmax) which is similar to (1 − µ/L), the convergence rate of batch gradient descent.
However, the diﬀerence between L and Lmax means that, even in that regime, the rate of batch
gradient descent can be much better. This is because batch gradient descent can make use of a
stepsize of 1/L, potentially much larger than the stepsize of

1

2(µN +Lmax) of SAGA.

When µ/Lmax is larger than 1/N , the convergence rate of SAGA is close to 1 − 1/2N , which
is much smaller than the rate of convergence of batch gradient descent. This is because, in the
well-conditioned case, θ changes quickly, leading to poorly correlated control variates and slower
convergence relative to batch gradient descent.

Thus, ﬁnding control variates that have a greater correlation with the gradient would
allow these algorithms to maintain a fast convergence for larger values of N and potentially
use larger stepsizes. Additionally, SVRG requires regular recomputation of ¯θt to maintain
its convergence rate. Maintaining a high correlation for longer could decrease this required
frequency of recomputation.

Algorithm 1 describes the original SVRG method (Johnson and Zhang 2013).

Algorithm 1 SVRG

Parameter: Functions fi for i = 1, . . . , N
Choose ¯θ0 ∈ Rd and stepsize γ > 0
for k = 0, . . . , K − 1 do
Calculate g(¯θk) = 1
N
for t = 0, 1, 2, . . . , T − 1 do

j=1 gj(¯θk), θ0 = ¯θk

(cid:80)N

i ∼ U[1, N ]
θt+1 = θt − γ

(cid:16)

gi(θt) − gi(¯θk) + g(¯θk)

(cid:17)

¯θk+1 = θT

Output ¯θK

We now modify Algorithm 1 by using control variates based on the ﬁrst order Taylor

expansion of the stochastic gradient. Rather than zj(θt) = gj(¯θ), we will use:

zj(θt) = gj(¯θ) + Hj(¯θ)(θt − ¯θ),
(4)
where Hj(¯θ) is the Hessian of fj taken at ¯θ. The new control variate zj(θt) now depends on θt,
which is not the case for the control variate used in SVRG.

2For linear regression, we have Lmax = maxi (cid:107)xi(cid:107)2 = R2 the radius of the input data.
3The resulting algorithm would be N times faster since an update only requires looking at a single datapoint

compared to the full dataset for batch gradient descent.

4

Algorithm 2 SVRG2 (SVRG with tracking)
Parameter: Functions fi for i = 1, . . . , N
Choose ¯θ0 ∈ Rd and stepsize γ > 0
for k = 0, . . . , K − 1 do
Calculate g(¯θk) = 1
N
for t = 0, 1, 2, . . . , T − 1 do

(cid:80)N

j=1 gj(¯θk), H(¯θk) = 1

N

(cid:80)N

j=1 Hj(¯θk), θ0 = ¯θk

(cid:16)

(cid:17)
gi(θt) − gi(¯θk)−Hi(¯θk)(θt − ¯θk) + g(¯θk) + H(¯θk)(θt − ¯θk)

i ∼ U[1, N ]
θt+1 = θt − γ

¯θk+1 = θT

Output ¯θK

Plugging Eq. (4) into Eq. (3) yields the following update:

θt+1 = θt − γt

gi(θt) − gi(¯θ) − Hi(¯θ)(θt − ¯θ) +

(cid:16)

(cid:88)

gj(¯θ) +

(cid:88)

(cid:17)
Hj(¯θ)(θt − ¯θ)

.

(5)

1
N

j

1
N

j

The resulting method, SVRG2, is presented in Algorithm 2. The only changes compared to
Algorithm 1 are the computation of H(¯θ) and the terms involving the Hessian in the update
(underlined). The theoretical analysis of Algorithm 2 for generalized linear models is carried out
in Section 5.

It is apparent that, if the fi’s are exactly quadratic, then gi(¯θ) + Hi(¯θ)(θt − ¯θ) = gi(θt) and
the update is the same as batch gradient descent, albeit with a computational cost independent
of N . The goal is thus to yield an update as close as possible to that of batch gradient descent
without paying the price of recomputing all gradients every time.

Algorithm 2 requires the computation of the full Hessian every time a new ¯θ is chosen and
does two matrix-vector products for each parameter update. The cost of computing the full
Hessian is O(N d2) and the cost of each update is O(d2). Since T , the number of updates before
recomputing the full gradient, is typically a multiple of N , the average cost per update is O(d2),
which is d times bigger than all ﬁrst-order stochastic methods. As this cost is prohibitive for
everything but low-dimensional problems, we need to resort to using approximations of the
Hessian.

4 Approximations of the Hessian

Algorithm 2 uses the following quantities:

• Hi(¯θ) needed for multiplying with the current θt;
• (cid:80)

j Hj(¯θ) needed for multiplying with the current θt;

• Hi(¯θ)θi, (cid:80)

j gj(¯θ) and (cid:80)

j Hj(¯θ)¯θ which are all vectors.
The ﬁrst and second terms cannot be stored for large dimensional problems and, even in that
case, the computational cost will be prohibitive. We thus need to resort to approximations
ˆHj(¯θ) of each Hessian Hj(¯θ) such that their sum ˆH(¯θ) is also easy to store.

This restriction makes us focus on linear projections of the Hessians, and two in particular:
diagonal approximations and projections onto a low-rank subspace. Note that both of these
classes of approximations include the special case Hi = 0, which is the original SVRG.

5

4.1 Diagonal approximation

We propose here a diagonal approximation of each Hessian leading to a storage cost of O(d).

Diagonal of the Hessian: By minimizing the Frobenius distance with the true Hessian,
the diagonal of the Hessian tries to approximate the true Hessian in all directions of the space.
This is in general very conservative as we care mostly about approximating the Hessian in the
directions θt − ¯θ.

Using the secant equation: Since the approximation is used in the term ˆHi(¯θ)(θt − ¯θ),
we want ˆHi(¯θ)(θt − ¯θ) to approximate well Hi(¯θ)(θt − ¯θ). One can use the secant equation and
set

ˆHi(¯θ) =

Hi(¯θ)(θt − ¯θ)
θt − ¯θ

≈

gi(θt) − gi(¯θ)
θt − ¯θ

,

where the division between two vectors is to be taken elementwise. However, this leads to
unstable updates as no guarantee is given outside of one direction.

Robust secant equation: We can robustify the secant equation by minimizing the average
squared-(cid:96)2 distance within a small ball around the previous direction. In other words, we seek

ˆH = arg min

(cid:13)Z(θt − ¯θ + ∆) − Hi(¯θ)(θt − ¯θ + ∆)(cid:13)
(cid:13)
2
(cid:13)

p(∆) d∆.

(cid:90)

Z

∆

Assuming ∆ ∼ N (0, σ2I), we get

ˆH =

(θt − ¯θ) (cid:12) Hi(¯θ)(θt − ¯θ) + σ2diag(Hi(¯θ))
(θt − ¯θ) (cid:12) (θt − ¯θ) + σ2

≈

(θt − ¯θ) (cid:12) (gi(θt) − gi(¯θ)) + σ2diag(Hi(¯θ))
(θt − ¯θ) (cid:12) (θt − ¯θ) + σ2

.

This is an interpolation between the two previous approximations. In particular, σ2 = +∞
recovers the diagonal of the Hessian while σ2 = 0 recovers the secant equation. For a ﬁxed σ2,
larger parameters updates at the beginning of optimization will make this approximation close
to the secant equation while, close to convergence, this approximation will converge towards the
diagonal of the Hessian due to smaller parameter updates.

4.2 Low-rank approximation: Curvature Matching

Another possibility is to build a low rank approximation of the Hessian by using a low dimensional
embedding. For brevity let Hi := Hi(¯θ) and H := H(¯θ).

Consider a matrix S ∈ Rd×k where k (cid:28) d, N . The matrix S may be random. Calculating
the embedded Hessian S(cid:62)HiS gives us a low rank approximation of Hi using the following
model:

(cid:107)X(cid:107)2

ˆHi = arg min
X∈Rd×d
subject to S(cid:62)XS = S(cid:62)HiS,

F (H)

where (cid:107) ˆH(cid:107)2
is a rank k matrix given by

F (H) := Tr

(cid:16) ˆH (cid:62)H ˆHH

(cid:17)

is the weighted Frobenius norm. The solution to the above

ˆHi = HS(S(cid:62)HS)†S(cid:62)HiS(S(cid:62)HS)†S(cid:62)H ∈ Rd×d,

(6)

(7)

(8)

6

where M † is the pseudoinverse of M . We refer to methods based on the Hessian approximation
presented in Eq. (8) as the Curvature Matching methods since the approximation has the same
curvature as the true Hessian over the subspace spanned by S.

Our use of a Hessian weighted Frobenius norm in (7) is largely inspired on quasi-Newton
methods.
Indeed, during the development of quasi-Newton methods it was observed that
using the Hessian as a weighting matrix in the Frobenius norm gave rise to a more eﬃcient
method (Goldfarb 1970), namely the BFGS method. Our ﬁndings mirror that of the quasi-
Newton literature in that our empirical tests showed that using the Hessian weighted Frobenius
norm is more eﬃcient than using the standard Frobenius norm.

Note that, to compute Eq. (8), we only need to store the d × k matrix HS and calculate
S(cid:62)HiS. The advantages of this approach are numerous and include: (a) Reduced memory
storage, (b) Hessian-vector products ˆHiv can be computed eﬃciently at a cost of O(k(2d + 3k))
which is linear in d, (c) the approximation ˆHi is the result of applying a linear operator on Hi
and consequently the expectation of ˆHi can be computed eﬃciently.

This last item is of particular importance for maintaining an unbiased estimator. Indeed,

taking the expectation over (8), we have that

(cid:105)

(cid:104) ˆHi

Ei

(8)
= HS(S(cid:62)HS)†S(cid:62)Ei [Hi] S(S(cid:62)HS)†S(cid:62)H

= HS(S(cid:62)HS)†S(cid:62)HS(S(cid:62)HS)†S(cid:62)H
= HS(S(cid:62)HS)†S(cid:62)H,

(9)

where in the last step we used that M †M M † = M † for all matrices M . From Eq. (9) we see
that the expectation of ˆHi is equal to the resulting approximation were we to use H in place
of Hi in Eq. (8). Furthermore (9) is a low rank matrix which can be stored and computed
eﬃciently since we only need to calculate the action of the true Hessian HS.4

(cid:80)N

j=1 Hj(¯θ)S and the k × k Hessian curvature matrix C = (S(cid:62) 1

Speciﬁcally, in the outer iterations, we can compute and store the d × k Hessian action matrix
j=1 Hj(¯θ)S)†/2 then
A = 1
N
use A and C to perform the necessary matrix vector products. See Algorithm 3 for our
implementation. Again we underline the computations that need to be added to the SVRG
algorithm to produce our new algorithm.

(cid:80)N

N

Algorithm 3 CM: Curvature Matching

Parameter: Functions fi for i = 1, . . . , N
Choose ¯θ0 ∈ Rd and stepsize γ > 0
for k = 0, . . . , K − 1 do
Calculate g(¯θk) = 1
N
(cid:80)N
Calculate A = 1
N
Generate S ∈ Rd×k, calculate ¯S = SC.
Normalize the Hessian action ¯A = AC.
for t = 0, 1, 2, . . . , T − 1 do

(cid:80)N
j=1 gj(¯θk), θ0 = ¯θk
j=1 Hj(¯θ)S, C = (S(cid:62)A)†/2.

i ∼ U[1, N ]
dt = gi(θt) − gi(¯θk) + g(¯θk) − ¯A ¯S(cid:62)Hi(¯θk) ¯S ¯A(cid:62)(θt − ¯θk) + ¯A ¯A(cid:62)(θt − ¯θk)
θt+1 = θt − γdt

¯θk+1 = θT
Output ¯θK

4With automatic diﬀerentiation, it costs only k times the cost of evaluating the function F (θ) to compute HS.

7

While S can be any matrix, including a random one drawn from some distribution, we found
the method performed best when the construction of S was based on the step directions dt
for t = 0, . . . , T − 1 taken from the inner loop. The reasoning behind this choice is that, since
ultimately we only require the action of the Hessian approximation along the directions θt − ¯θ,
the matrix S should be chosen in such a way that our Hessian approximation is accurate along
these directions. Thus we will construct the embedding matrix S by again taking inspiration
from the BFGS (Goldfarb 1970) and L-BFGS (Nocedal 1980) methods and column concatenate
previous step directions. Speciﬁcally, since there are many step directions and sequential step
directions are often correlated, we arranged the T step directions into k sets. We used the
average over each of these k sets as a column of S. That is

S = [ ¯d0, . . . , ¯dk−1] where

¯di =

(10)

k
T

T
k (i+1)−1
(cid:88)

dj,

j= T

k i

where we assume, for simplicity, that k divides T.

4.2.1 Computational complexity

Algorithm 3 can be applied to large-scale problems due to its reduced complexity. Indeed, using
automatic diﬀerentiation (Walther 2008; Christianson 1992) the cost of computing each one of
the matrix-matrix products Hj(¯θ)S is O(k · eval(fj)), where eval(fj) is the cost of evaluating
the function fj.5 Furthermore, evaluating a single gradient gj also costs O(eval(fj)). The matrix
product S(cid:62)A costs O(k2d) to compute, while the products AC and SC cost dk2. Computing
the square and pseudoinverse in C = (S(cid:62)A)†/2 costs O(k3), leading to a total for the outer
iteration of

O(k ·

eval(fj) + k2d + k3) = O(k · eval(f ) + k2d + k3).

n
(cid:88)

j=1

The main cost in the inner iteration is computing Hi(¯θk)S and a handful of matrix-vector
products where the matrices have dimensions k × d or d × k. The total cost of each inner
iteration is O(k · eval(fi) + dk2). Overall, the cost of computing an iteration (inner or outer) of
Algorithm 3 is linear in d and comes at a k-fold increase of the cost of computing an SVRG
iteration. In our experiments, we used k = 10.

4.3 Low-rank approximation: Action Matching

In the previous section we developed a low rank update based on curvature matching, though
ultimately we required the action of the true Hessian HS to compute the resulting Hessian
approximation (8). With the action of the Hessian we can also build an approximation of the
Hessian using an action matching model:
ˆHi = arg min
X∈Rd×d
subject to XS = HiS, X = X (cid:62).

(cid:107)X(cid:107)2

(11)

F (H)

In other words, ˆHi is the symmetric matrix with the smallest norm which matches the action of
the true Hessian. The solution to the above is a rank 2k matrix given by

ˆHi = HS(ST HS)−1S(cid:62)Hi

(cid:0)I − S(ST HS)−1S(cid:62)H(cid:1) + HiS(ST HS)−1S(cid:62)H.

(12)

5In fact, these matrix-matrix products are computed using directional derivatives, and it is also easy enough

to handcode these directional derivatives to achieve this complexity.

8

Diﬀerently from the curvature matching model (7), here we have had to explicitly enforce the
symmetry of the Hessian approximation as a constraint (11) as the solution of the unconstrained
model might not be symmetric.

This approximation is a generalization of the Davidson-Fletcher-Powell (Fletcher and Powell
1963; Davidon 1968) update. This update, and a regularized inverse of this update, have already
been used in an optimization context (Gower and Gondzio 2014) but only as a preconditioner.
Again, we have that the approximation (12) is the result of applying a linear function to Hi.

Thus, taking the expectation, we have that
(cid:104) ˆHi

Ei

(cid:105)

= HS(ST HS)−1S(cid:62)Ei [Hi] + Ei [Hi] S(ST HS)−1S(cid:62)H

− HS(ST HS)−1S(cid:62)Ei [Hi] S(ST HS)−1S(cid:62)H

= HS(ST HS)−1S(cid:62)H,

which is the same as (9) and thus can be eﬃciently computed.

The pseudo-code of our implementation of the method based on (11) is in Algorithm 4. The
total complexity of the Algorithm 4 is the same as the complexity of Algorithm 3, though the
hidden constant in the complexity is a bit larger as there is an additional matrix-vector product.

Algorithm 4 AM: Action Matching

Parameter: Functions fi for i = 1, . . . , N
Choose ¯θ0 ∈ Rd and stepsize γ > 0
for k = 0, . . . , K − 1 do
Calculate g(¯θk) = 1
N
Calculate and store A = 1
N
Generate S ∈ Rd×k, calculate and store ¯S = SC.
Normalize the Hessian action ¯A = AC.
for t = 0, 1, 2, . . . , T − 1 do

j=1 gj(¯θk), θ0 = ¯θk

(cid:80)N

(cid:80)N

i ∼ U[1, N ]
θt+1 = θt − γ

(cid:16)

gi(θt) − gi(¯θk) + g(¯θk)

j=1 Hj(¯θ)S and C = (S(cid:62)A)†/2.

−(cid:0) ¯A ¯S(cid:62)Hi

(cid:0)I − ¯S ¯A(cid:62)(cid:1) + Hi ¯S ¯A(cid:62)(cid:1) (θt − ¯θk) + ¯A ¯A(cid:62)(θt − ¯θk)

(cid:17)

¯θk+1 = θT
Output ¯θK

A similar approximation to (11) has been developed before (Gower et al. 2016; Gower and
Richt´arik 2016) but focused on obtaining an approximation to the inverse Hessian to then be
applied as a quasi-Newton update.

4.4 Comparing approximations

Both the diagonal and the low-rank approximations have a free parameter: σ2 for the diagonal
approximation and the rank k for the low-rank approximation. These two parameters make
diﬀerent tradeoﬀs. σ2 controls the robustness of the approximation: a larger value is more stable
but likely to decrease the convergence rate. k controls the complexity of the approximation: a
larger value leads to a better convergence rate in terms of updates but with a larger computational
cost per update. Though this was not explored in this work, it seems natural to combine these
two approximations.

9

5 Theoretical Analysis and Discussion

Convergence rates: We now provide a convergence analysis for generalized linear models
when an approximation of the Hessian is used. The full proofs of the results below may be
found in the Appendix. We make the following assumptions:

• We consider a compact set Θ included in the ball of center θ∗ and radius D, where θ∗ is

the minimizer of F (θ) = 1
N

(cid:80)N

i=1 fi(θ).

• F is µ-strongly convex and L-smooth on Θ.

• Each fi is of the form fi(θ) = ϕi(x(cid:62)

i θ) with (cid:107)xi(cid:107)2 (cid:54) R2, and ϕ(cid:48)(cid:48)

i (x(cid:62)

i θ) ∈ [α, 1], ϕ(cid:48)(cid:48)(cid:48)

i (x(cid:62)

i θ) (cid:54)

β for all θ ∈ Θ. In particular, this means that we can use Lmax = R2.

• We consider the control variate zi(θ) = f (cid:48)

i (¯θ) + Hi(θ − ¯θ), where Hi is an approximation of
i (¯θ).
the Hessian f (cid:48)(cid:48)
Note that η = 1 if we take Hi = 0 (plain SVRG) and η = 0 if we use the exact Hessian.

i (¯θ), with a relative error so that 1

i (¯θ) − Hi)2 (cid:52) R2η 1

i=1(f (cid:48)(cid:48)

i=1 f (cid:48)(cid:48)

(cid:80)N

(cid:80)n

N

N

We obtained the following proposition which shows that we get the actual condition number
L/µ rather than Lmax/µ (which is the one for SVRG), if we are close enough to the global
optimum and the Hessians are suﬃciently well approximated:

Proposition 1. Assume D2 (cid:54) Lα
SVRG2 with T (cid:62) 16L

µ , the error is reduced by a factor of 3/4.

8β2R2 , γ = 1/(4L) with η (cid:54) Lα

8β2 . Then, after a single epoch of

The result above implies that, in order to reach a precistion ε, we need K = O(log 1
ε )
epochs of SVRG2, with an overall number of accesses to gradient and Hessian oracles less than
O(N + L
ε ), once we are close enough to the optimum (but note that the bound on D does
not depend on µ). In the Appendix, we also show that our algorithm is robust even far away
from optimum with a step-size γ ∝ 1/R2.

µ log 1

Diﬀerence with reconditioning: To our knowledge, this is the ﬁrst use of the Hessian to
track gradients over time as it is usually used for reconditioning. While using the Hessian to
track the gradients does not remove the dependency on the condition number, it has several
advantages. First, since there is no matrix inversion, it is much more robust to approximations of
the true Hessian, as shown in the theoretical analysis above. Second, while classical second-order
methods only enjoy superlinear convergence close to the optimum, tracking gradients improves
convergence in a larger ball around the optimum.

This does not preclude the use of reconditioning and we believe the Hessian could be used
It remains to be seen, however, if the best Hessian

for both purposes at the same time.
approximation is the same for both uses.

(cid:80)

Diﬀerence to non-uniform sampling Non-uniform sampling aims to close the gap
between Lmax and 1
i Li (Kern and Gyorgy (2016) and Schmidt et al. (2015)). Our method
n
closes the gap between Lmax and L. These two quantities, 1
i Li and L, can be very diﬀerent.
n
For instance, for a linear least squares objective f (w) = 1/(2n)||Xw − b||2
2 we have that
1
n λmax(XX (cid:62)), thus L can be upto n times smaller than
n
1
n

i Li = (1/n)Tr (cid:0)XX (cid:62)(cid:1) while L = 1
i Li.

(cid:80)
(cid:80)

(cid:80)

6 Experiments

Algorithms. We empirically tested the impact of using second-order information for the
diﬀerent Hessian approximations. All the code for our experiments was written in the julia

10

(a) a9a

(b) covtype

(c) mushrooms

(d) phishing

(e) w8a

(f) rcv1-train

(g) gisette

(h) madelon

Figure 1: Performance of various SVRG-based methods on multiple LIBSVM problems:
(a) a9a (b) covtype (c) mushrooms (d) phishing (e) w8a (f) rcv1-train (g) gisette (h)
11
madelon.

SVRG
2D
2Dsec
CMgauss
CMprev
AMgauss
AMprev
SVRG2

a9a
28
28
29
28
28
28
29
210

covtype
29
210
210
29
28
29
29
29

gisette
28
28
28
28
28
29
29
29

madelon
27
27
27
27
27
27
28
28

mushrooms
28
28
28
28
27
27
28
29

phishing
26
26
26
26
26
27
26
26

w8a
28
29
29
29
29
29
29
29

rcv1
210
214
213
29
210
29
210
210

Table 1: Best empirical stepsize for each (problem, method) pair. In general, incorporating a
Hessian approximation allows for larger stepsizes.

progamming language and can be found at https://github.com/gowerrobert/StochOpt. In
particular, we tried SVRG, the original SVRG algorithm, SVRG2, which tracks the gradients
using the full Hessian, 2D, which tracks the gradients using the diagonal of the Hessian, 2Dsec,
which tracks the gradients using the robust secant equation (6), CM, which tracks the gradients
using the low-rank curvature matching approximation of the Hessian (8), and AM which uses the
low-rank action matching approximation of the Hessian (12) to track the gradients. We also
tested two variants of each of the low rank approximations, namely AMgauss, CMgauss and
AMprev, CMprev. For the AMgauss and CMgauss methods, the matrix S ∈ Rn×k in (8) and (12)
is a randomly generated Gaussian matrix. For the AMprev and CMprev methods, we construct
S using previous search directions according to (10).

All the data we use are binary classiﬁcation problems taken from LIBSVM, and we use
a logistic loss with a squared-(cid:96)2 regularizer. We set the regularization parameter to λ =
maxi=1,...,N (cid:107)xi(cid:107)2
2/4N in all experiments. In each ﬁgure, the y-axis is the relative suboptimality
gap given by [f (θ) − f (θ∗)]/[f (θ0) − f (θ∗)] on a log scale.

Convergence rate was observed both in terms of passes through the data and in terms of
wall clock time, to account for the higher computational cost of our proposed methods. Due to
the high computational cost of SVRG2, we did not perform experiments on SVRG2 for problems
with dimension (number of features) over 5000.

Choice of the stepsize. Since convergence is guaranteed for batch gradient descent with a
larger stepsize than for variance reducing stochastic methods, our hope is that using second-order
information will allow the use of larger stepsizes while maintaining convergence. Thus, we ran a
grid search over the stepsize for each method, trying values of 2a/ Lmax for a = 10, 7, . . . , −7, −9,
where Lmax := maxi=1,...,N (cid:107)xi(cid:107)2

2 + λ.

We expected our proposed covariates to provide better approximations to the true gradient
and thus allow for greater stepsizes. This is the case in general, albeit not by a large amount, as
can be seen in Table 1. Our proposed methods often allowed larger stepsizes, with the SVRG2
method typically allowing the largest stepsize.

Results. Figure 1 shows the results on multiple datasets. We see that, in terms of passes
through the data, all tracking methods consistently outperform SVRG, often by a large margin.
In particular the SVRG2 method often has the best performance in terms of datapasses with the
exception of the covtype problem where the two methods based on diagonal approximations 2D
and 2Dsec have the best performance. In terms of time taken, the AMgauss and AMprev are the
most eﬃcient methods, which indicates the superiority of the Hessian approximations based on
action matching (11). For all these experiments, we chose a rank k = 10.

12

(a) a9a

(b) covtype

(c) phishing

(d) mushrooms

Figure 2: Performance of the SVRG2sec with diﬀerent choices of σ on: (a) a9a (b) covtype (c)
phishing (d) mushrooms.

Robustness of the diagonal approximation. Our robust secant equation has a hyper-
parameter, σ2. Since the popularity of an optimization method depends as much of its ease
of use as of its convergence rate, we tested the impact of σ2 on the convergence speed. The
results are presented in Fig. 2. We can see that the impact is generally very limited and that
our method is robust to the choice of σ2. In all other experiments we set σ2 = 0.1.

7 Conclusion

Control variates are at the core of stochastic variance-reducing methods. We proposed the
ﬁrst dynamic method which updates these control variates at every timestep, leading to faster
empirical and theoretical convergence as well as increased robustness. While we applied it to
SVRG, we hope it can be extended to other methods such as SAGA.

We also showed that the Hessian could be used beyond reconditioning and that eﬃcient
approximations existed. Though reconditioning in stochastic variance-reducing methods has
been explored, it remains to be seen whether using the Hessian for both reconditioning and
gradient tracking simultaneously is beneﬁcial.

Finally, we believe there is still more to do to bridge the gap between stochastic and batch
methods and we hope this work will open the way to further analyses of the importance of
control variates in optimization.

The authors would like to acknowledge support from the Fondation de Sciences Math´ematiques
de Paris (FSMP) and from the European Research Council (ERC SEQUOIA).

Acknowledgements

References

[1] S´ebastien Bubeck et al. “Convex optimization: Algorithms and complexity”. In: Founda-

tions and Trends R(cid:13) in Machine Learning 8.3-4 (2015), pp. 231–357.

[2] Bruce Christianson. “Automatic Hessians by reverse accumulation”. In: IMA Journal of

Numerical Analysis 12.2 (1992), pp. 135–150.

[3] W. C. Davidon. “Variance algorithms for minimization”. In: Computer Journal 10 (1968),

pp. 406–410.

[4] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. “SAGA: A fast incremental
gradient method with support for non-strongly convex composite objectives”. In: Advances
in Neural Information Processing Systems. 2014, pp. 1646–1654.

13

[5] By R Fletcher and M J D Powell. “A Rapidly Convergent Descent Method for Minimiza-

tion”. In: The Computer Journal 6.2 (1963), pp. 163–168.

[6] Donald Goldfarb. “A Family of Variable-Metric Methods Derived by Variational Means”.

In: Mathematics of Computation 24.109 (1970), pp. 23–26.

[7] Robert M. Gower and Jacek Gondzio. “Action constrained quasi-Newton methods”. In:

arXiv:1412.8045v1 (2014).

[8] Robert M. Gower, Donald Goldfarb, and Peter Richt´arik. “Stochastic Block BFGS: Squeez-
ing More Curvature out of Data”. In: Proceedings of the 33rd International Conference on
Machine Learning (2016).

[9] Robert Mansel Gower and Peter Richt´arik. “Randomized Quasi-Newton Updates are

Linearly Convergent Matrix Inversion Algorithms”. In: arXiv:1602.01768 (2016).

[10] Rie Johnson and Tong Zhang. “Accelerating stochastic gradient descent using predictive
variance reduction”. In: Advances in Neural Information Processing Systems. 2013, pp. 315–
323.

[11] T Kern and A Gyorgy. “SVRG++ with non-uniform sampling”. In: Neural Information

Processing Systems Foundation, Inc., 2016.

[12] Nicolas Le Roux, Mark Schmidt, and Francis Bach. “A stochastic gradient method with an
exponential convergence rate for ﬁnite training sets”. In: Advances in Neural Information
Processing Systems. 2012, pp. 2663–2671.

[13] Julien Mairal. “Optimization with First-Order Surrogate Functions”. In: Proceedings of

The 30th International Conference on Machine Learning. 2013, pp. 783–791.

[14] Jorge Nocedal. “Updating Quasi-Newton Matrices with Limited Storage”. In: Mathematics

of Computation 35 (July 1980), pp. 773–782.

[15] Herbert Robbins and Sutton Monro. “A stochastic approximation method”. In: Annals of

Mathematical Statistics 22.3 (1951), pp. 400–407.

[16] Reuven Y Rubinstein and Dirk P Kroese. Simulation and the Monte Carlo method. John

Wiley & Sons, 2016.

(2014).

[17] Mark Schmidt. “Convergence rate of stochastic gradient with constant step size”. In:

[18] Mark Schmidt, Nicolas Le Roux, and Francis Bach. “Minimizing Finite Sums with the

Stochastic Average Gradient”. In: arXiv preprint arXiv:1309.2388 (2013).

[19] Mark W. Schmidt et al. “Non-Uniform Stochastic Average Gradient Method for Training
Conditional Random Fields”. In: Proceedings of the Eighteenth International Conference
on Artiﬁcial Intelligence and Statistics, AISTATS 2015, San Diego, California, USA, May
9-12, 2015. 2015.

[20] Shai Shalev-Shwartz and Tong Zhang. “Stochastic Dual Coordinate Ascent Methods for
Regularized Loss”. In: Journal of Machine Learning Research 14.1 (Feb. 2013), pp. 567–599.
issn: 1532-4435.

[21] Hoi-To Wai, Wei Shi, Angelia Nedic, and Anna Scaglione. “Curvature-aided incremental

aggregated gradient method”. In: Allerton. IEEE, 2017, pp. 526–532.

[22] Andrea Walther. “Computing sparse Hessians with automatic diﬀerentiation”. In: ACM

Trans. Math. Software 34.1 (Jan. 2008), Art. 3, 15.

[23] Lijun Zhang, Mehrdad Mahdavi, and Rong Jin. “Linear convergence with condition number
independent access of full gradients”. In: Advances in Neural Information Processing
Systems. 2013, pp. 980–988.

14

A Theoretical Analysis: Proofs

Following Bubeck (2015), we consider a single epoch of SVRG and its extensions, that is, ¯θ ∈ Θ,
and the iteration, started from θ0 = ¯θ:

θt = ΠΘ

θt−1 − γ

(θt−1) − zit(θt−1) +

(cid:18)

(cid:104)

f (cid:48)
it

zj(θt−1)

(cid:105)(cid:19)
,

1
N

N
(cid:88)

j=1

with it uniformly at random in {1, . . . , N }.

We also recall that, while the results are given using R the radius of the data, they can be

readily transposed to Lmax using Lmax = R2.

We have, with Ft−1 representing the information up to time t:

E (cid:2)(cid:107)θt − θ∗(cid:107)2|Ft−1

(cid:3) (cid:54) E

(cid:13)
(cid:13)θt−1 − θ∗ − γ

(cid:104)

f (cid:48)
it

(θt−1) − zit(θt−1) +







zj(θt−1)

|Ft−1



(cid:105)(cid:13)
2
(cid:13)

1
N

N
(cid:88)

j=1

by contractivity of projections,

(cid:54) (cid:107)θt−1 − θ∗(cid:107)2 − 2γF (cid:48)(θt−1)(cid:62)(θt−1 − θ∗) + γ2(cid:107)F (cid:48)(θt−1)(cid:107)2
N
(cid:88)

N
(cid:88)

N
(cid:88)

i (θt−1) − zi(θt−1) −

1
N

f (cid:48)
j(θt−1) +

1
N

zj(θt−1)

(cid:13)
2
(cid:13)
(cid:13)

+

γ2
N

(cid:13)
(cid:13)f (cid:48)
(cid:13)

i=1

j=1
(cid:54) (cid:107)θt−1 − θ∗(cid:107)2 − 2γF (cid:48)(θt−1)(cid:62)(θt−1 − θ∗) + γ2(cid:107)F (cid:48)(θt−1)(cid:107)2

j=1

+

γ2
N

N
(cid:88)

i=1

(cid:13)
(cid:13)f (cid:48)

i (θt−1) − zi(θt−1)(cid:13)
2
(cid:13)

, by bounding the variance by the second moment.

In the following sections, we provide proofs for several algorithms we consider in this paper.

A.1 SVRG

For regular SVRG (we provide the proof for completeness and because we need it later), we
have: zi(θ) = f (cid:48)

i (¯θ) and we consider the bound

γ2
N

N
(cid:88)

i=1

(cid:13)
(cid:13)f (cid:48)

2 (cid:54) 2γ2
i (θt−1) − zi(θt−1)(cid:13)
(cid:13)
N

N
(cid:88)

i=1

(cid:13)
(cid:13)f (cid:48)

i (θt−1) − f (cid:48)

i (θ∗)(cid:13)
2
(cid:13)

+

(cid:13)
(cid:13)f (cid:48)

i (¯θ) − f (cid:48)

i (θ∗)(cid:13)
2
(cid:13)

2γ2
N

N
(cid:88)

i=1

(cid:54) 2γ2R2F (cid:48)(θt−1)(cid:62)(θt−1 − θ∗) + 2γ2R2[F (¯θ) − F (θ∗)]

leading to

E (cid:2)(cid:107)θt − θ∗(cid:107)2|Ft−1

(cid:3) (cid:54) (cid:107)θt−1 − θ∗(cid:107)2 − 2γF (cid:48)(θt−1)(cid:62)(θt−1 − θ∗) + γ2(cid:107)F (cid:48)(θt−1)(cid:107)2

+2γ2R2F (cid:48)(θt−1)(cid:62)(θt−1 − θ∗) + 2γ2R2[F (¯θ) − F (θ∗)].

Thus if γ (cid:54) 1/(2R2 + L), we get

E (cid:2)(cid:107)θt − θ∗(cid:107)2|Ft−1

(cid:3) (cid:54) (cid:107)θt−1 − θ∗(cid:107)2 − γ[F (¯θt−1) − F (θ∗)] + 2γ2R2[F (¯θ) − F (θ∗)].

15

This implies that

T
(cid:88)

t=1

1
T

(cid:34)

E

F

(cid:16) 1
T

T
(cid:88)

t=1

E (cid:2)F (¯θt−1) − F (θ∗)(cid:3) (cid:54) 1
γT

(cid:107)¯θ − θ∗(cid:107)2 + 2γR2[F (¯θ) − F (θ∗)]

(cid:17)

¯θt−1

− F (θ∗)

(cid:54)

(cid:35)

(cid:16) 2

µγT

+ 2γR2(cid:17)

[F (¯θ) − F (θ∗)].

This implies that if γ = 1

4R2 and T (cid:62) 8/(γµ) = 32R2

µ , then

(cid:34)

E

F

(cid:16) 1
T

T
(cid:88)

t=1

(cid:17)

¯θt−1

− F (θ∗)

(cid:35)

(cid:54) 3
4

[F (¯θ) − F (θ∗)].

Thus, after K = O(log 1
an overall access to gradients of KN + KT = (cid:0)N + R2

ε ) epochs of SVRG we have attained the required precision, which makes
(cid:1) log 1
ε .

µ

A.2 SVRG-2
We assume that 4β2R4

α D2 (cid:54) L and γ = 1/(4L).
In this situation, with no approximation, we have zi(θ) = f (cid:48)

i (¯θ) + f (cid:48)(cid:48)

i (¯θ)(θ − ¯θ) and:

(cid:13)
(cid:13)f (cid:48)

i (θt−1) − zi(θt−1)(cid:13)
2
(cid:13)

(cid:54) γ2
N

R2(cid:2) β
2

(x(cid:62)

i θt−1 − x(cid:62)
i

¯θ)2(cid:3)2

=

γ2β2R2
4N

N
(cid:88)

i=1

(x(cid:62)

i (θt−1 − ¯θ))4 using the bound on ϕ(cid:48)(cid:48)(cid:48),

(cid:2)2(x(cid:62)

i (θt−1 − θ∗))4 + 2(x(cid:62)

i (θ∗ − ¯θ))4(cid:3)

(cid:2)2R2(cid:107)θt−1 − θ∗(cid:107)2(x(cid:62)

i (θt−1 − θ∗))2 + 2R2(cid:107)¯θ − θ∗(cid:107)2(x(cid:62)

i (¯θ − θ∗))2(cid:3) using (cid:107)xi(cid:107) (cid:54) R,

(cid:107)θt−1 − θ∗(cid:107)2

(x(cid:62)

i (θt−1 − θ∗))2 +

(cid:107)¯θ − θ∗(cid:107)2

(x(cid:62)

i (¯θ − θ∗))2

N
(cid:88)

i=1

2γ2β2R4
N

4γ2β2R4
α

N
(cid:88)

i=1

(cid:107)θt−1 − θ∗(cid:107)2[F (θt−1) − F (θ∗)] +

(cid:107)¯θ − θ∗(cid:107)2[F (¯θ) − F (θ∗)] using ϕ(cid:48)(cid:48) (cid:62) α,

D2[F (θt−1) − F (θ∗)] +

D2[F (¯θ) − F (θ∗)], using the compactness of Θ.

4γ2β2R4
α

With our assumptions, we have γ

(cid:18)

L + 4β2R4

α D2

(cid:19)

(cid:54) 1, and we get that

E (cid:2)(cid:107)θt − θ∗(cid:107)2|Ft−1

(cid:3) (cid:54) (cid:107)θt−1 − θ∗(cid:107)2 − γF (cid:48)(θt−1)(cid:62)(θt−1 − θ∗) +

D2[F (¯θ) − F (θ∗)]

(cid:54) (cid:107)θt−1 − θ∗(cid:107)2 − γ[F (θt−1) − F (θ∗)] +

D2[F (¯θ) − F (θ∗)].

4γ2β2R4
α
4γ2β2R4
α

γ2
N

N
(cid:88)

i=1

N
(cid:88)

i=1

(cid:54) γ2β2R2

(cid:54) γ2β2R2

N
(cid:88)

i=1

N
(cid:88)

i=1

(cid:54) 2γ2β2R4

(cid:54) 4γ2β2R4

(cid:54) 4γ2β2R4

N

N

N

α

α

16

This leads to, with T (cid:62) 4/(µγ) =

and using γ

16L
µ

(cid:35)

(cid:18)

4β2R4

α D2

(cid:54) 1/2,

(cid:19)

(cid:19)

(cid:34)

E

F

(cid:16) 1
T

T
(cid:88)

t=1

(cid:17)

¯θt−1

− F (θ∗)

(cid:54)

(cid:18) 2
µγT

+

4γβ2R4
α

D2

[F (¯θ) − F (θ∗)]

(cid:54) 3
4

[F (¯θ) − F (θ∗)].

Thus, after K = O(log 1
an overall access to gradients of KN + KT = (cid:0)N + L

ε ) epochs of SVRG we have attained the required precision, which makes
(cid:1) log 1
ε .

µ

A.3 Stability of SVRG-2

If we make no compactness assumption on Θ, then we have:

(cid:13)
(cid:13)f (cid:48)

i (θt−1) − zi(θt−1)(cid:13)
2
(cid:13)

γ2
N

N
(cid:88)

i=1

N
(cid:88)

(cid:54) 2γ2
N

(cid:13)
(cid:13)f (cid:48)

i (θt−1) − fi(¯θ)(cid:13)
2
(cid:13)

+

(cid:13)
(cid:13)f (cid:48)(cid:48)

i(cid:48) (¯θ)(θt−1 − ¯θ)(cid:13)
2
(cid:13)

N
(cid:88)

2γ2
N

i=1

i=1
(cid:54) 2γ2R2F (cid:48)(θt−1)(cid:62)(θt−1 − θ∗) + 2γ2R2[F (¯θ) − F (θ∗)] from the SVRG proof ,

+

2γ2
N

N
(cid:88)

i=1

R2(cid:13)

(cid:13)x(cid:62)

i (θt−1 − ¯θ)(cid:13)
2
(cid:13)

(cid:54) 2γ2R2F (cid:48)(θt−1)(cid:62)(θt−1 − θ∗) + 2γ2R2[F (¯θ) − F (θ∗)]

2γ2R2
α

(cid:54) 4γ2R2

α

F (cid:48)(θt−1)(cid:62)(θt−1 − θ∗) +

[F (¯θ) − F (θ∗)]

F (cid:48)(θt−1)(cid:62)(θt−1 − θ∗) +

[F (¯θ) − F (θ∗)]

2γ2R2
α
4γ2R2
α

Thus, if we take the smaller step-size γ = α

8R2 and T = 64R2

αµ , we get the same convergence.

17

