Path-Based Attention Neural Model for Fine-Grained Entity Typing

Denghui Zhang1, Manling Li1, Pengshan Cai2, Yantao Jia1, Yuanzhuo Wang1, Xueqi Cheng1
1Institute of Computing Technology, Chinese Academy of Sciences, Beijing, 100190, China
2School of Computer Science, University of Massachusetts Amherst, MA 01003
{zhangdenghui, wangyuanzhuo, cxq}@ict.ac.cn, {limanlingcs, jamaths.h}@gmail.com, pengshancai@cs.umass.edu

8
1
0
2
 
n
a
J
 
9
 
 
]
L
C
.
s
c
[
 
 
2
v
5
8
5
0
1
.
0
1
7
1
:
v
i
X
r
a

Abstract

Fine-grained entity typing aims to assign entity mentions
in the free text with types arranged in a hierarchical struc-
ture. Traditional distant supervision based methods employ a
structured data source as a weak supervision and do not need
hand-labeled data, but they neglect the label noise in the au-
tomatically labeled training corpus. Although recent studies
use many features to prune wrong data ahead of training, they
suffer from error propagation and bring much complexity. In
this paper, we propose an end-to-end typing model, called the
path-based attention neural model (PAN), to learn a noise-
robust performance by leveraging the hierarchical structure
of types. Experiments demonstrate its effectiveness.

Introduction
Fine-grained entity typing aims to assign types (e.g., “per-
son”, “politician”, etc.) to entity mentions in the local con-
text (a single sentence), and the type set constitutes a tree-
structured hierarchy (i.e., type hierarchy). Recent years wit-
ness the boost of neural models in this task, e.g., (Shimaoka
et al. 2016) employs an attention based LSTM to attain
sentence representations and achieves state-of-the-art per-
formance. However, it still suffers from noise in training
data, which is a main challenge in this task. The training
data is generated by distant supervision, which assumes that
if an entity has a type in knowledge bases (KBs), then all
sentences containing this entity will express this type. This
method inevitably introduces irrelevant types to the context.
For example, the entity “Donald Trump” has types “person”,
“businessman” and “politician” in KBs, thus all three types
are annotated for its mentions in the training corpora. But
in sentence “Donald Trump announced his candidacy for
President of US.”, only “person” and “politician” are cor-
rect types, while “businessman” can not be deduced from
the sentence, thus serves as noise. To alleviate this issue, a
few systems try to denoise training data by ﬁltering irrele-
vant types ahead of training. For instance, (Ren et al. 2016)
proposes PLE to identify correct types by jointly embedding
mentions, context and type hierarchy, and then use clean
data to train classiﬁers. However, the denoising and training
process are not uniﬁed, which may cause error propagation
and bring much additional complexity.

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Motivated by this, we propose an end-to-end typing
model, called the path-based attention neural model (PAN),
to select relevant sentences to each type, which can dynam-
ically reduce the weights of wrong labeled sentences for
each type during training. This idea is inspired by some
successful attempts to reduce noise in relation extraction,
e.g.,(Lin et al. 2016). However, these methods fail to formu-
late type hierarchy, which is distinct in ﬁne-grained entity
typing. Speciﬁcally, if a sentence indicates a type, its parent
type can be also deduced from the sentence. Like the ex-
ample above, “politician” is the subtype of “person”. Since
the sentence indicates that “Donald Trump” is “politician”,
“person” should also be assigned. Thus, we build path-based
attention for each type by utilizing its path to its coarsest par-
ent type (e.g., person, businessman) in the type hierarchy.
Compared to the simple attention in relation extraction, it
enables parameter sharing for types in the same path. With
the support of hierarchical information of types, it can re-
duce noise effectively and yields a better typing classiﬁer.
Experiments on two data sets validate effectiveness of PAN.

Path-Based Attention Neural Model
The architecture of PAN is illustrated in Figure1. Sup-
posing that there are n sentences containing entity e, i.e.,
Se = {s1, s2, ..., sn}, and Te is the automatically labeled
types based on KBs. Firstly PAN employs LSTM to gen-
erate representations of sentences si following (Shimaoka
et al. 2016), where si ∈ Rd is the semantic representation
of si, i ∈ {1, 2, ..., n}. Afterwards, we build path-based at-
tention αi,t over sentences si for each type t ∈ Te, which
is expected to focus on relevant sentences to type t. Then,
the representation of sentence set Se for type t, denoted by
se,t ∈ Rd, is calculated through weighted sum of vectors
of sentences. Finally, we obtain predicted types through a
classiﬁcation layer.

Figure 1: The architecture of PAN for given entity e, type t

More precisely, given e, an attention αi,t is learned to

score how well sentence si matches type t, i.e.,

αi,t =

exp(siApt)
j=1 exp(sjApt)

,

(cid:80)n

where A ∈ Rd×d is a weighted diagonal matrix. pt ∈ Rd is
the representation of path pt for type t. Speciﬁcally, for each
type, we deﬁne one path as a sequence of types starting from
its coarsest parent type, and ending with it. More formally,
for type tl, ptl = t1→t2→...→tl, where t1 is its coarsest
parent type, and ti+1 is the subtype of ti. For example, for
type tl = politician, its path is ptl = person→politician.
We represent the path ptl as a semantic composition of all
the types on the path, i.e., ptl = t1 ◦ t2 ◦ ... ◦ tl, where
ti ∈ Rd is the representation of type ti, which is a parameter
to learn. ◦ is a composition operator. In this paper, we con-
sider two types of operators: (1) Addition (PAN-A), where
ptl equals the sum of type vectors. (2) Multiplication (PAN-
M), where ptl equals the cumulative product of type vectors.
In this way, path-based attention enables the model to share
parameters between types in the same path. For example, the
attention learned for “person” could assist the learning of the
attention for “politician”. It makes learning easier especially
for infrequent subtypes, which suffer from dearth of training
data, since the attentions for these subtypes can get support
from the attention for parent type.

Then, the representation of sentence set Se for type t, i.e.,
se,t, is calculated through weighted sum of sentence vectors,
n
(cid:88)

se,t =

αi,tsi.

i=1
Since one mention can have multiple types, we employ a
classiﬁcation layer consisting of N logistic classiﬁers, where
N is the total number of types. Each classiﬁer outputs the
probability of respective type, i.e.,
exp(wT
1 + exp(wT
where wt, bt ∈ Rd are the logistic regression parameters.
To optimize the model, a multi-type loss is deﬁned according
to the cross entropy as follows,
J = −

[It ln P (t|se,t) + (1 − It) ln(1 − P (t|se,t))],

t se,t + bt)

t se,t + bt)

P (t|se,t) =

(cid:88)

(cid:88)

,

e

t

where It is indicator function to indicate whether t is the
annotated type of entity e, i.e., t ∈ Te.

Experiments and Conclusion
Experiments are carried on two widely used datasets
OntoNotes and FIGER(GOLD), and the training dataset of
OntoNotes is noisy compared to FIGER(GOLD) (Shimaoka
et al. 2016). The statistics of the datasets are listed in Table1.
Table 1: Statistics of the datasets.

#Type
89
113

#Layer #Context #Train
143K
3
223K
1.51M 2.69M 563
2

Datasets
OntoNotes
FIGER(GOLD)
We employ Strict Accuracy (Acc), Loose Macro F1 (Ma-
F1), and Loose Micro F1 (Mi-F1) as evaluation measures
following (Shimaoka et al. 2016). Speciﬁcally, “Strict” eval-
uates on the type set of each entity mention, while “Loose”

#Test
8,963

on each type. “Marco” is the geometric average over all
mentions, while “Micro” is the arithmetic average. The base-
lines are chosen from two aspects: (1) Predicting types in
a uniﬁed process using raw noisy data, i.e., TLSTM (Shi-
maoka et al. 2016), and other methods shown in Table2. (2)
Predicting types using clean data by denoising ahead, i.e.,
H PLE and F PLE (Ren et al. 2016). To prove the superi-
ority of path-based attention, we also directly apply the at-
tention neural model in relation extraction (Lin et al. 2016)
without using type hierarchy (AN). The results of baselines
are the best results reported in their papers.

Table 2: Performance on FIGER(GOLD) and OntoNotes

Metric

OntoNotes
Acc Ma-F1 Mi-F1

HYENA 24.9
FIGER
36.9
TLSTM 50.8
52.3
AN
54.9
PAN-A
PAN-M 53.0

49.7
57.8
70.1
71.7
72.8
71.9

44.6
51.6
64.9
65.2
66.5
65.3

FIGER(GOLD)
Acc Ma-F1 Mi-F1
28.8
47.4
59.7
60.0
60.2
60.0

50.6
65.5
75.4
75.9
76.2
76.0

52.8
69.2
79.0
79.5
79.9
79.4

We can observed that: (1) When using the same raw
noisy data, PAN outperforms all methods on both data sets,
which proves the anti-noise ability of PAN. (2) PAN per-
forms better than AN, since the attention learned in PAN
utilizes the hierarchical structure to enable parameter shar-
ing. (3) The improvements on OntoNotes are higher than
FIGER(GOLD), because OntoNotes is more noisy, and the
hierarchical structure in OntoNotes is more complex with
more layers, which further demonstrates that path-based at-
tention does well with type hierarchy, and proves the supe-
riority of PAN in reducing noise. (4) PAN-A achieves better
performance than PAN-M, which shows that addition oper-
ator can better capture type hierarchy.

Table 3: Performance on FIGER(GOLD) and OntoNotes

Metric

H PLE
F PLE
PAN-A

OntoNotes
Acc Ma-F1 Mi-F1
54.6
57.2
54.9

62.5
66.1
66.5

69.2
71.5
72.8

FIGER(GOLD)
Acc Ma-F1 Mi-F1
54.3
59.9
60.2

68.1
74.9
76.2

69.5
76.3
79.9

As shown in Table3, PAN using raw noisy data outper-
forms H PLE and F PLE using denoised data on Ma-F1
and Mi-F1. It makes sense that F PLE has higher Acc on
OntoNotes since the noise is reduced before training, but it
needs to learn additional parameters about mentions, con-
text and types, while PAN only needs to learn parameters of
attention. Thus, PAN is more efﬁcient to reduce noise.

In conclusion, PAN can reduce noise effectively through
an end-to-end process, and achieves better typing perfor-
mance on datasets with more noise.

References

[Lin et al. 2016] Lin, Y.; Shen, S.; Liu, Z.; and et al. 2016.
Neural relation extraction with selective attention over in-
stances. In ACL.
[Ren et al. 2016] Ren, X.; He, W.; Qu, M.; Voss, C. R.; Ji,
H.; and Han, J. 2016. Label noise reduction in entity typing
by heterogeneous partial-label embedding. In KDD.

[Shimaoka et al. 2016] Shimaoka, S.; Stenetorp, P.; Inui, K.;
and Riedel, S. 2016. Neural architectures for ﬁne-grained
entity type classiﬁcation. In EACL.

