Wronging a Right: Generating Better Errors to Improve
Grammatical Error Detection

Sudhanshu Kasewa and Pontus Stenetorp and Sebastian Riedel
{sudhanshu.kasewa.16, p.stenetorp, s.riedel}@ucl.ac.uk
University College London

Abstract

Grammatical error correction, like other ma-
chine learning tasks, greatly beneﬁts from
large quantities of high quality training data,
which is typically expensive to produce. While
writing a program to automatically generate
realistic grammatical errors would be difﬁcult,
one could learn the distribution of naturally-
occurring errors and attempt to introduce them
into other datasets.
Initial work on induc-
ing errors in this way using statistical machine
translation has shown promise; we investigate
cheaply constructing synthetic samples, given
a small corpus of human-annotated data, using
an off-the-rack attentive sequence-to-sequence
model and a straight-forward post-processing
procedure. Our approach yields error-ﬁlled ar-
tiﬁcial data that helps a vanilla bi-directional
LSTM to outperform the previous state of the
art at grammatical error detection, and a pre-
viously introduced model to gain further im-
provements of over 5% F0.5 score. When at-
tempting to determine if a given sentence is
synthetic, a human annotator at best achieves
39.39 F1 score, indicating that our model gen-
erates mostly human-like instances.

1

Introduction

There is an ever-growing number of people learn-
ing English as a second language; providing them
with quick feedback to facilitate their learning is
a crucial, labour-intensive endeavour. Part of this
process is identifying and correcting grammati-
cal errors, and several computational techniques
have been developed to automate it (Rozovskaya
and Roth, 2014; Junczys-Dowmunt and Grund-
kiewicz, 2016). For example, given an erroneous
sentence “I wanted to goes to the beach”, the
grammatical error correction task is to output the
valid sentence “I wanted to go to the beach”.
The task can be cast as a two-stage process, de-
tection and correction, which can either be per-

formed sequentially (Yannakoudakis et al., 2017),
or jointly (Napoles and Callison-Burch, 2017).

Automated error correction performance is ar-
guably still
too low for practical considera-
tion, perhaps limited by the amount of training
data (Rei et al., 2017). High quality annotations
are expensive to procure, and foreign language
learners and commercial entities may feel uncom-
fortable granting access to their data. Instead, one
could attempt to supplement existing manual an-
notations with synthetic instances. Such artiﬁcial
samples are beneﬁcial only when they share struc-
ture with the true distribution from which human
errors are generated. Generative Adversarial Net-
works (Goodfellow et al., 2014) could be used for
this purpose, but they are difﬁcult to train, and re-
quire a large collection of sentences that are incor-
rect. One might attempt self-training (McClosky
et al., 2006), where new instances are generated
by applying a trained model to unannotated data,
using high-conﬁdence predictions as ground truth
labels. However, in such a scheme, the expectation
is that the unlabelled text already contains errors,
which is not usually the case for most freely avail-
able text such as Wikipedia articles as they strive
towards correctness.

In place of using machine translation (MT)
to correct grammatical mistakes (Yuan and Fe-
lice, 2013; Junczys-Dowmunt and Grundkiewicz,
2014; Yuan and Briscoe, 2016), one might con-
sider swapping the input and output streams, and
instead learn to induce errors into error-free text,
for the purpose of creating a synthetic training
dataset (Felice and Yuan, 2014). Recently, Rei
et al. (2017) used a statistical MT (SMT) sys-
tem to induce errors into error-free text. Build-
ing on this work, and leveraging recent advances
in neural MT (NMT), we used an off-the-shelf at-
tentive sequence-to-sequence model (Britz et al.,
2017), eliminating the need of specialised soft-

ware such as a phrase-table generator, decoder,
and part-of-speech tagger. We created multi-
ple synthetic datasets from in-domain and out-
of-domain sources, and found that stochastic to-
ken sampling, and pruning redundant and low-
likelihood sentences, were helpful in generating
meaningful corruptions. Using the artiﬁcial sam-
ples thus generated, we improved upon detec-
tion results with simply a vanilla bi-directional
LSTM (Hochreiter and Schmidhuber, 1997). Us-
ing a more powerful model, we established new
state-of-the-art results, that improve on previously
published F0.5 scores by over 5%. Addition-
ally, we conﬁrm that our generated instances are
human-like, as an annotator identifying generated
sentences achieved a maximum F1 score of 39.39.

2 Related work

In computer vision, images are blurred, rotated, or
otherwise deformed inexpensively to create new
training instances (Wang and Perez, 2017), be-
cause such manipulation does not signiﬁcantly
alter the image semantics. Similar coarse pro-
cesses do not work in NLP since mutating even
a single letter or a word can change a sentence’s
meaning, or render it nonsensical. Nonetheless,
Vinyals et al. (2015) employed a kind of self-
training where they use noisy predictions for un-
labelled instances output by existing state-of-the-
art parsers as ground-truth labels, and improved
syntactic parsing performance. Sennrich et al.
(2016) synthesised training instances by round-
trip-translating a monolingual corpus with weaker
versions of an NMT learner, and used them to im-
prove the translation. Bouchard et al. (2016) de-
veloped an efﬁcient algorithm to blend generated
and true data for improving generalisation.

Grammar correction is a well-studied task in
NLP, and early systems were rule-based pattern
recognisers (Macdonald, 1983) and dictionary-
based linguistic analysis engines (Richardson and
Braden-Harder, 1988). Later systems used sta-
tistical approaches, addressing speciﬁc kinds of
errors such as article insertion (Knight et al.,
1994) and spelling correction (Golding and Roth,
1996). Most recently, architectural innovations in
neural sequence labelling (Rei et al., 2016; Rei,
2017) raised error detection performance through
improved ability to process unknown words and
jointly learning a language model.

Early efforts for artiﬁcial error generation in-

cluded generating speciﬁc types of errors, such as
mass noun errors (Brockett et al., 2006) and arti-
cle errors (Rozovskaya and Roth, 2010), and lever-
aging linguistic information to identify error pat-
terns and transfer them onto grammatically correct
text (Foster and Andersen, 2009; Yuan and Felice,
2013). Imamura et al. (2012) investigated meth-
ods to generate pseudo-erroneous sentences for er-
ror correction in Japanese. Recently, Rei et al.
(2017) corrupted error-free text using SMT to cre-
ate training instances for error detection.

3 Neural error generation

To learn to introduce errors, we use an off-the-
shelf attentive sequence-to-sequence neural net-
work (Bahdanau et al., 2014). Given an input se-
quence, the encoder generates context vectors for
each token. Then, the attention mechanism and
the decoder work in tandem to emit a distribution
over the target vocabulary. At every decoder time-
step, the encoder context vectors are scored by the
attention mechanism, and a weighted sum is sup-
plied to the decoder, along with its propagated in-
ternal state and last output symbol.

Corruption: Tokens from this distribution are
sampled at every decoder time-step, either by
argmax (AM), which emits the most likely word,
or by a stochastic alternative such as temperature
sampling (TS) as argmax cannot be relied on
to generate rare words. A temperature parameter
τ > 0 sharpens or softens the distribution:

˜pi = fτ (p)i =

p

1
τ
i
j p

1
τ
j

(cid:80)

where i are the components of the probability dis-
tribution corresponding to words in the vocabu-
lary. As one interpolates τ from 0 to 1, the be-
haviour of ˜p transitions from argmax to p, control-
ling the diversity of the generated tokens.

The sentence generated by TS might be a low
probability sequence from the joint conditional
distribution P (v|u), where u is the input sentence
and v is the output sentence. One way around this
is to use beam search (BS), which checks the like-
lihood of every possible continuation of a sentence
fragment, and maintains a list of the n best trans-
lations generated up to the current time-step. AM,
TS, and BS are indicative of the trade-off between
increasing levels of model ﬂexibility at the cost of
computation; we compare them to assess whether

Original

Corruption

She promised to turn over a new leaf.
At the moment I’m in Spain.

She promissed to turn over a new leaf.
During the moment I’m in Spain.

Table 1: Example sentences generated by our NMT pipeline.

Data augmentation strategy

Model

FCE (dev)

FCE CoNLL1 CoNLL2

Rei et al. (2017) FCEP AT + EVPP AT
Rei et al. (2017) FCESM T + EVPSM T
Rei et al. (2017) FCESM T +P AT + EVPSM T +P AT

SL
SL
SL

None
FCET S
EVPBS
SWT S
FCEAM +T S+EVPAM +T S

None
FCET S
EVPBS
SWT S
FCEAM +T S+EVPAM +T S
FCEAM +T S+EVPAM +T S+SWAM +T S

BiLSTM
BiLSTM
BiLSTM
BiLSTM
BiLSTM

SL
SL
SL
SL
SL
SL

–
–
–

47.9
51.2
52.1
51.5
52.3

52.5
54.8
55.2
53.8
56.9
56.5

47.8
48.4
49.1

43.6
47.1
50.1
50.6
50.4

48.2
49.9
54.6
52.7
54.6
55.6

19.5
19.7
21.9

16.6
19.7
20.8
24.2
22.1

17.4
20.9
23.3
26.8
25.1
28.3

28.5
28.4
30.1

24.3
28.9
29.0
31.7
30.8

25.5
29.2
31.4
34.3
33.0
35.5

Table 2: F0.5 scores on various tests contrasted with published results and unaugmented baseline models.

the additional computations were helpful in creat-
ing high-quality synthetic instances.

Post-processing: Original and corrupted sen-
tences are aligned at a word-level using Leven-
shtein distance. Using the minimal alignment,
words in the corrupted sentence are labelled cor-
rect, ‘c’, or incorrect, ‘i’, as follows:

If the word is not aligned with itself, then ‘i’.
Else, if following a gap, then ‘i’, as at this point
a human reader would notice that there is a word
missing in the sentence. Else, if it is the last word,
but it is not aligned to the last word of the source
sentence, then ‘i’, as a human would realise that
this sentence ends abruptly, Else, ‘c’.

These token-labelled corrupted sentences now
form an artiﬁcial dataset for training an error de-
tector. Duplicate instances and corrupted sen-
tences with more than 5 errors were dropped to
remove noise from the downstream training.

4 Experiments

We evaluated our approach on the First Certiﬁcate
of English (FCE) error detection dataset (Rei and
Yannakoudakis, 2016), as well as on two human-
annotated test sets (CoNLL1, CoNLL2) from the
CoNLL 2014 shared task (Ng et al., 2014). The
CoNLL data sets pose a unique challenge; as they
are different in style and domain from FCE, we

have no matching training data. We compared
the effect of different neural generation proce-
dures (AM, TS, BS) and contrasted the down-
stream performance of a bidirectional LSTM with
an elaborate sequence labeller.

4.1

Implementation details

NMT training and corruption: We minimally
modiﬁed the open source implementation1 of Britz
et al. (2017) to implement TS and BS.2 We trained
our NMT with a single-layered encoder and de-
coder with cell size 256, on the parallel cor-
pus version of FCE (Yannakoudakis et al., 2011),
with early stopping after the FCE development
set score dropped consistently for 20 epochs. We
introduced errors into three datasets: FCE it-
self (450K tokens), the English Vocabulary Pro-
ﬁle or EVP (270K tokens) and a subset of Simple
Wikipedia or SW (8.4M tokens); of these, FCE
and EVP were both used in artiﬁcial error genera-
tion via SMT and pattern extraction (PAT) by Rei
et al. (2017), enabling us to make a fair experi-
mental comparison. Ten corrupted versions using
each of AM, TS (τ = 0.05) and BS were sam-
pled for FCE and EVP corruptions, while one suf-
ﬁced for SW. The theoretical time complexity of
BS is O(bn) for each sentence, where b is num-

1 https://github.com/google/seq2seq
2 https://github.com/skasewa/wronging

Baseline
EVP+AM
EVP+TS
EVP+BS

10

5

0

e
r
o
c
s

5
.
0
F
n
i

e
s
a
e
r
c
n
i

e
t
u
l
o
s
b
A

e
r
o
c
s

5
.
0
F

35

30

25

20

15

FCE

SW

CoNLL1
CoNLL2

1x 2x 3x 4x 5x 6x 7x 8x 9x 10x
Amount of augmented data

FCE (dev)

FCE

CoNLL1

CoNLL2

Figure 1: Improvements using three different meth-
ods of generation.

Figure 2: Training with increasing amounts of cor-
rupted data from FCE and SW.

ber of candidates, and n is the maximum length
of a sentence. Empirically, BS with b = 11 took
a factor of 11.3 more time than AM. Examples of
generated errors are provided in Table 1.

Error detection: We compare two error detec-
tion models: a vanilla bi-directional LSTM (BiL-
STM) (Schuster and Paliwal, 1997), and the state-
of-the-art sequence labeller (SL) neural network
used by Rei et al. (2017). These models were
trained on the binary-labelled FCE training set
augmented with the corrupted instances. Wher-
ever no model is explicitly stated, the SL model
was used. During training, we alternate between
the annotated FCE dataset and the synthetic col-
lection. This alternating protocol prevents over-
ﬁtting on FCE; once it shifts back, it reinforces
connections made from the helpful synthetic cor-
ruptions while forgetting about the noisy ones.

4.2 Results

The results for our baselines and data augmenta-
tion strategies can be found in Table 2. Augmented
with our NMT generated data, even our vanilla
downstream BiLSTM outperforms the SMT+PAT
artiﬁcial error augmentation approach of Rei et al.
(2017), indicating that our process better gener-
alises the error information in the source dataset.
Using the more powerful SL network bests the
previous state of the art by over 5% on the FCE
test. Most intriguingly, we note a signiﬁcant im-
provement for the CoNLL tests using corruptions
from out-of-domain SW. Figure 2 illustrates how
we gain performance on these tests with increas-
ing amounts of corrupted SW, which does not hold
true for corrupted FCE. This shows that we were

able to induce useful errors into a corpus with a
large unseen vocabulary and different syntactic bi-
ases, and this in turn proved valuable for detect-
ing errors in a third domain, suggesting that our
method can transfer learned distributions across
stylistic genres.

Using EVP as a standard source, Figure 1 illus-
trates the variance of the different sampling meth-
ods. All generation methods yield corruptions that
signiﬁcantly improve test performance, with in-
stances sampled by beam-search consistently out-
performing the alternatives.

5 Discussion

5.1 Error distribution

The original FCE dataset was annotated using the
error taxonomy speciﬁed in Nicholls (2003), and
contains 75 unique error codes. We annotated
samples of EVP corrupted by all three sampling
methods, at a reduced resolution, to compare the
distribution of errors across FCE and the synthetic
corpora. These are presented in Table 3.

At a high level, NMT generates errors more of-
ten among more common parts-of-speech, favour-
ing errors in verbs and nouns, rather than in ad-
verbs and conjunctions. It did not make spelling
errors as often as in the source dataset;
this
is likely because it only observed the speciﬁc
spelling errors present in FCE, and as the vocab-
ulary is restricted to that dataset, it does not en-
counter those words as frequently in EVP, and thus
rarely makes the same spelling mistakes.

Additionally, the differences in these distribu-
tions can partially be attributed to the implicit dif-
ferences between us and the annotators of FCE.

Spelling

FCE AM TS BS

Spelling errors

11

1

1

4

Part-of-speech FCE AM TS BS

AM

TS

BS

Precision
Recall
F1

81.25
26.00
39.39

63.63
28.00
38.89

50.00
14.00
22.22

Verb
Preposition
Determiner
Noun
Pronoun
Adverb
Adjective
Conjunction
Quantiﬁer

Replacement
Inclusion
Removal
Word form
Word order

34
18
16
13
7
5
3
2
1

49
23
14
9
5

16
16
7
36
3
5
15
2
0

35
30
33
2
0

26
10
6
35
3
3
16
2
0

34
27
36
2
0

16
14
10
43
1
5
12
1
0

32
35
32
1
0

Remedy Type

FCE AM TS BS

Table 3: Error distribution across FCE and manu-
ally annotated samples of artiﬁcial data. Spelling
errors are a % of all errors, while Part-of-speech,
and Remedy Type are compared within their own
categories to sum to 100%.

5.2 Comparison with human errors

To check if the synthetic instances passed for
human-like, we mixed 50 generated sentences
among an equal number of actual ungrammatical
instances from FCE-dev and tasked a human eval-
uator to identify the artiﬁcial statements, in a sim-
ple Turing-style test. We created three such sets,
one for each of our sampling techniques, and the
test subject aimed to identify synthetic samples
with high conﬁdence. Results of this test are pre-
sented in Table 4.

The high precision but low recall scores suggest
that while it is still possible to spot some corrup-
tions that are quite clearly artiﬁcial, the bulk of our
samples do not betray their synthetic nature and
are indistinguishable from naturally occurring er-
roneous sentences. In order to fairly compare our
work with earlier results, we intended to conduct
such a test for sentences generated by the SMT
of Rei et al. (2017). Unfortunately, we were only
able to source corruptions of FCE-train via this
method; therefore, we decided not to perform this
test as its results cannot be compared to ours.

Table 4: Results of a Turing-style test, where a sub-
ject was asked to distinguish between real and fake
sentences, sampled from each of the different gen-
erated corpora.

6 Conclusions and future work

We presented a novel data augmentation tech-
nique for grammatical error detection using neu-
ral machine translation to learn the distribution
of language-learner errors, and induce such er-
rors into grammatically correct text. We explored
several different variants of sampling to improve
the quality of our synthetic errors. After creat-
ing artiﬁcial training instances with an off-the-
shelf NMT, we bettered previous state-of-the-art
results on the canonical test with even a basic BiL-
STM, and established a new state of the art using
a stronger model. Additionally, we demonstrated
that we were able to leverage corruptions of an
out-of-domain dataset to set new benchmarks on
separate, also out-of-domain tests, without specif-
ically optimising for either.

Our work indicates that neural error genera-
tion warrants further investigation with different
datasets and architectures, both for error detec-
tion and error correction. Among possible fu-
ture work is using generative adversarial networks
as corruption engines, and developing better se-
quence alignment methods. Some preliminary re-
sults with simple corruptions using word substitu-
tion and word dropout (Iyyer et al., 2015) appear
to be promising, and may feature as components of
a future corruption system. Finally, one could use
such artiﬁcial error-prone corpora as source text
for self-training an error detection system.

Acknowledgements

We thank Marek Rei and Mariano Felice for grant-
ing access to their data and code. Also, we would
like to thank the anonymous reviewers and Jo-
hannes Welbl for valuable feedback and discus-
sions. This work was supported by an Allen Dis-
tinguished Investigator Award.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Neural machine translation by
CoRR,

Bengio. 2014.
jointly learning to align and translate.
abs/1409.0473.

Guillaume Bouchard, Pontus Stenetorp, and Sebastian
Riedel. 2016. Learning to generate textual data.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1608–1616, Austin, Texas. Association for Compu-
tational Linguistics.

Denny Britz, Anna Goldie, Thang Luong, and Quoc
Le. 2017. Massive Exploration of Neural Machine
Translation Architectures. ArXiv e-prints.

Chris Brockett, William B. Dolan, and Michael Ga-
mon. 2006. Correcting esl errors using phrasal smt
In Proceedings of the 21st Interna-
techniques.
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 249–256, Sydney, Aus-
tralia. Association for Computational Linguistics.

Mariano Felice and Zheng Yuan. 2014. Generat-
ing artiﬁcial errors for grammatical error correction.
In Proceedings of the Student Research Workshop
at the 14th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 116–126, Gothenburg, Sweden. Association
for Computational Linguistics.

Jennifer Foster and Øistein E Andersen. 2009. Gen-
errate: generating errors for use in grammatical error
detection. In Proceedings of the fourth workshop on
innovative use of nlp for building educational ap-
plications, pages 82–90. Association for Computa-
tional Linguistics.

Andrew R Golding and Dan Roth. 1996. Apply-
ing winnow to context-sensitive spelling correction.
arXiv preprint cmp-lg/9607024.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
In Advances in neural information
versarial nets.
processing systems, pages 2672–2680.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8):1735–
1780.

Kenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, and
Hitoshi Nishikawa. 2012. Grammar error correc-
tion using pseudo-error sentences and domain adap-
tation. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Short Papers-Volume 2, pages 388–392. Association
for Computational Linguistics.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daum´e III. 2015. Deep unordered com-
position rivals syntactic methods for text classiﬁca-
tion. In Proceedings of the 53rd Annual Meeting of

the Association for Computational Linguistics and
the 7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), vol-
ume 1, pages 1681–1691.

Marcin Junczys-Dowmunt and Roman Grundkiewicz.
2014. The amu system in the conll-2014 shared
task: Grammatical error correction by data-intensive
and feature-rich statistical machine translation.
In
Proceedings of the Eighteenth Conference on Com-
Shared
putational Natural Language Learning:
Task, pages 25–33, Baltimore, Maryland. Associa-
tion for Computational Linguistics.

Marcin Junczys-Dowmunt and Roman Grundkiewicz.
2016. Phrase-based machine translation is state-of-
the-art for automatic grammatical error correction.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1546–1556, Austin, Texas. Association for Compu-
tational Linguistics.

Kevin Knight,

Ishwar Chander, Matthew Haines,
Vasileios Hatzivassiloglou, Eduard Hovy, Masayo
Iida, Steve K Luk, Akitoshi Okumura, Richard
Whitney, and Kenji Yamada. 1994.
Integrating
knowledge bases and statistics in mt. arXiv preprint
cmp-lg/9409001.

Nina H Macdonald. 1983. Human factors and behav-
ioral science: The unix writer’s workbench soft-
ware: Rationale and design. Bell Labs Technical
Journal, 62(6):1891–1908.

David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing.
In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152–159, New York City, USA. Association for
Computational Linguistics.

Courtney Napoles and Chris Callison-Burch. 2017.
Systematically adapting machine translation for
In Proceedings of
grammatical error correction.
the 12th Workshop on Innovative Use of NLP for
Building Educational Applications, pages 345–356,
Copenhagen, Denmark. Association for Computa-
tional Linguistics.

Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The conll-2014 shared task on
grammatical error correction. In Proceedings of the
Eighteenth Conference on Computational Natural
Language Learning: Shared Task, pages 1–14, Bal-
timore, Maryland. Association for Computational
Linguistics.

Diane Nicholls. 2003. The cambridge learner corpus:
Error coding and analysis for lexicography and elt.
In Proceedings of the Corpus Linguistics 2003 con-
ference, volume 16, pages 572–581.

Marek Rei. 2017. Semi-supervised multitask learn-
In Proceedings of the

ing for sequence labeling.

Jason Wang and Luis Perez. 2017. The effectiveness of
data augmentation in image classiﬁcation using deep
learning. Technical report, Technical report.

Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading esol texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
180–189, Portland, Oregon, USA. Association for
Computational Linguistics.

Helen Yannakoudakis, Marek Rei, Øistein E. Ander-
sen, and Zheng Yuan. 2017. Neural sequence-
labelling models for grammatical error correction.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2795–2806, Copenhagen, Denmark. Association for
Computational Linguistics.

Zheng Yuan and Ted Briscoe. 2016. Grammatical er-
ror correction using neural machine translation. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 380–386, San Diego, California. Association
for Computational Linguistics.

Zheng Yuan and Mariano Felice. 2013. Constrained
grammatical error correction using statistical ma-
In Proceedings of the Seven-
chine translation.
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 52–61, Soﬁa,
Bulgaria. Association for Computational Linguis-
tics.

55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 2121–2130, Vancouver, Canada. Association
for Computational Linguistics.

Marek Rei, Gamal Crichton, and Sampo Pyysalo. 2016.
Attending to characters in neural sequence label-
In Proceedings of COLING 2016,
ing models.
the 26th International Conference on Computational
Linguistics: Technical Papers, pages 309–318, Os-
aka, Japan. The COLING 2016 Organizing Commit-
tee.

Marek Rei, Mariano Felice, Zheng Yuan, and Ted
Briscoe. 2017. Artiﬁcial error generation with ma-
chine translation and syntactic patterns. In Proceed-
ings of the 12th Workshop on Innovative Use of NLP
for Building Educational Applications, pages 287–
292, Copenhagen, Denmark. Association for Com-
putational Linguistics.

Marek Rei and Helen Yannakoudakis. 2016. Composi-
tional sequence labeling models for error detection
In Proceedings of the 54th An-
in learner writing.
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1181–
1191, Berlin, Germany. Association for Computa-
tional Linguistics.

Stephen D. Richardson and Lisa C. Braden-Harder.
1988. The experience of developing a large-scale
natural language text processing system: Critique.
In Second Conference on Applied Natural Language
Processing.

Alla Rozovskaya and Dan Roth. 2010.

Training
paradigms for correcting errors in grammar and us-
In Human language technologies: The 2010
age.
annual conference of the north american chapter of
the association for computational linguistics, pages
154–162. Association for Computational Linguis-
tics.

Alla Rozovskaya and Dan Roth. 2014. Building a
state-of-the-art grammatical error correction system.
Transactions of the Association for Computational
Linguistics, 2:419–434.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673–2681.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
Improving neural machine translation mod-
2016.
In Proceedings of the
els with monolingual data.
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
86–96, Berlin, Germany. Association for Computa-
tional Linguistics.

Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-
mar as a foreign language. In Advances in Neural
Information Processing Systems, pages 2773–2781.

Wronging a Right: Generating Better Errors to Improve
Grammatical Error Detection

Sudhanshu Kasewa and Pontus Stenetorp and Sebastian Riedel
{sudhanshu.kasewa.16, p.stenetorp, s.riedel}@ucl.ac.uk
University College London

Abstract

Grammatical error correction, like other ma-
chine learning tasks, greatly beneﬁts from
large quantities of high quality training data,
which is typically expensive to produce. While
writing a program to automatically generate
realistic grammatical errors would be difﬁcult,
one could learn the distribution of naturally-
occurring errors and attempt to introduce them
into other datasets.
Initial work on induc-
ing errors in this way using statistical machine
translation has shown promise; we investigate
cheaply constructing synthetic samples, given
a small corpus of human-annotated data, using
an off-the-rack attentive sequence-to-sequence
model and a straight-forward post-processing
procedure. Our approach yields error-ﬁlled ar-
tiﬁcial data that helps a vanilla bi-directional
LSTM to outperform the previous state of the
art at grammatical error detection, and a pre-
viously introduced model to gain further im-
provements of over 5% F0.5 score. When at-
tempting to determine if a given sentence is
synthetic, a human annotator at best achieves
39.39 F1 score, indicating that our model gen-
erates mostly human-like instances.

1

Introduction

There is an ever-growing number of people learn-
ing English as a second language; providing them
with quick feedback to facilitate their learning is
a crucial, labour-intensive endeavour. Part of this
process is identifying and correcting grammati-
cal errors, and several computational techniques
have been developed to automate it (Rozovskaya
and Roth, 2014; Junczys-Dowmunt and Grund-
kiewicz, 2016). For example, given an erroneous
sentence “I wanted to goes to the beach”, the
grammatical error correction task is to output the
valid sentence “I wanted to go to the beach”.
The task can be cast as a two-stage process, de-
tection and correction, which can either be per-

formed sequentially (Yannakoudakis et al., 2017),
or jointly (Napoles and Callison-Burch, 2017).

Automated error correction performance is ar-
guably still
too low for practical considera-
tion, perhaps limited by the amount of training
data (Rei et al., 2017). High quality annotations
are expensive to procure, and foreign language
learners and commercial entities may feel uncom-
fortable granting access to their data. Instead, one
could attempt to supplement existing manual an-
notations with synthetic instances. Such artiﬁcial
samples are beneﬁcial only when they share struc-
ture with the true distribution from which human
errors are generated. Generative Adversarial Net-
works (Goodfellow et al., 2014) could be used for
this purpose, but they are difﬁcult to train, and re-
quire a large collection of sentences that are incor-
rect. One might attempt self-training (McClosky
et al., 2006), where new instances are generated
by applying a trained model to unannotated data,
using high-conﬁdence predictions as ground truth
labels. However, in such a scheme, the expectation
is that the unlabelled text already contains errors,
which is not usually the case for most freely avail-
able text such as Wikipedia articles as they strive
towards correctness.

In place of using machine translation (MT)
to correct grammatical mistakes (Yuan and Fe-
lice, 2013; Junczys-Dowmunt and Grundkiewicz,
2014; Yuan and Briscoe, 2016), one might con-
sider swapping the input and output streams, and
instead learn to induce errors into error-free text,
for the purpose of creating a synthetic training
dataset (Felice and Yuan, 2014). Recently, Rei
et al. (2017) used a statistical MT (SMT) sys-
tem to induce errors into error-free text. Build-
ing on this work, and leveraging recent advances
in neural MT (NMT), we used an off-the-shelf at-
tentive sequence-to-sequence model (Britz et al.,
2017), eliminating the need of specialised soft-

ware such as a phrase-table generator, decoder,
and part-of-speech tagger. We created multi-
ple synthetic datasets from in-domain and out-
of-domain sources, and found that stochastic to-
ken sampling, and pruning redundant and low-
likelihood sentences, were helpful in generating
meaningful corruptions. Using the artiﬁcial sam-
ples thus generated, we improved upon detec-
tion results with simply a vanilla bi-directional
LSTM (Hochreiter and Schmidhuber, 1997). Us-
ing a more powerful model, we established new
state-of-the-art results, that improve on previously
published F0.5 scores by over 5%. Addition-
ally, we conﬁrm that our generated instances are
human-like, as an annotator identifying generated
sentences achieved a maximum F1 score of 39.39.

2 Related work

In computer vision, images are blurred, rotated, or
otherwise deformed inexpensively to create new
training instances (Wang and Perez, 2017), be-
cause such manipulation does not signiﬁcantly
alter the image semantics. Similar coarse pro-
cesses do not work in NLP since mutating even
a single letter or a word can change a sentence’s
meaning, or render it nonsensical. Nonetheless,
Vinyals et al. (2015) employed a kind of self-
training where they use noisy predictions for un-
labelled instances output by existing state-of-the-
art parsers as ground-truth labels, and improved
syntactic parsing performance. Sennrich et al.
(2016) synthesised training instances by round-
trip-translating a monolingual corpus with weaker
versions of an NMT learner, and used them to im-
prove the translation. Bouchard et al. (2016) de-
veloped an efﬁcient algorithm to blend generated
and true data for improving generalisation.

Grammar correction is a well-studied task in
NLP, and early systems were rule-based pattern
recognisers (Macdonald, 1983) and dictionary-
based linguistic analysis engines (Richardson and
Braden-Harder, 1988). Later systems used sta-
tistical approaches, addressing speciﬁc kinds of
errors such as article insertion (Knight et al.,
1994) and spelling correction (Golding and Roth,
1996). Most recently, architectural innovations in
neural sequence labelling (Rei et al., 2016; Rei,
2017) raised error detection performance through
improved ability to process unknown words and
jointly learning a language model.

Early efforts for artiﬁcial error generation in-

cluded generating speciﬁc types of errors, such as
mass noun errors (Brockett et al., 2006) and arti-
cle errors (Rozovskaya and Roth, 2010), and lever-
aging linguistic information to identify error pat-
terns and transfer them onto grammatically correct
text (Foster and Andersen, 2009; Yuan and Felice,
2013). Imamura et al. (2012) investigated meth-
ods to generate pseudo-erroneous sentences for er-
ror correction in Japanese. Recently, Rei et al.
(2017) corrupted error-free text using SMT to cre-
ate training instances for error detection.

3 Neural error generation

To learn to introduce errors, we use an off-the-
shelf attentive sequence-to-sequence neural net-
work (Bahdanau et al., 2014). Given an input se-
quence, the encoder generates context vectors for
each token. Then, the attention mechanism and
the decoder work in tandem to emit a distribution
over the target vocabulary. At every decoder time-
step, the encoder context vectors are scored by the
attention mechanism, and a weighted sum is sup-
plied to the decoder, along with its propagated in-
ternal state and last output symbol.

Corruption: Tokens from this distribution are
sampled at every decoder time-step, either by
argmax (AM), which emits the most likely word,
or by a stochastic alternative such as temperature
sampling (TS) as argmax cannot be relied on
to generate rare words. A temperature parameter
τ > 0 sharpens or softens the distribution:

˜pi = fτ (p)i =

p

1
τ
i
j p

1
τ
j

(cid:80)

where i are the components of the probability dis-
tribution corresponding to words in the vocabu-
lary. As one interpolates τ from 0 to 1, the be-
haviour of ˜p transitions from argmax to p, control-
ling the diversity of the generated tokens.

The sentence generated by TS might be a low
probability sequence from the joint conditional
distribution P (v|u), where u is the input sentence
and v is the output sentence. One way around this
is to use beam search (BS), which checks the like-
lihood of every possible continuation of a sentence
fragment, and maintains a list of the n best trans-
lations generated up to the current time-step. AM,
TS, and BS are indicative of the trade-off between
increasing levels of model ﬂexibility at the cost of
computation; we compare them to assess whether

Original

Corruption

She promised to turn over a new leaf.
At the moment I’m in Spain.

She promissed to turn over a new leaf.
During the moment I’m in Spain.

Table 1: Example sentences generated by our NMT pipeline.

Data augmentation strategy

Model

FCE (dev)

FCE CoNLL1 CoNLL2

Rei et al. (2017) FCEP AT + EVPP AT
Rei et al. (2017) FCESM T + EVPSM T
Rei et al. (2017) FCESM T +P AT + EVPSM T +P AT

SL
SL
SL

None
FCET S
EVPBS
SWT S
FCEAM +T S+EVPAM +T S

None
FCET S
EVPBS
SWT S
FCEAM +T S+EVPAM +T S
FCEAM +T S+EVPAM +T S+SWAM +T S

BiLSTM
BiLSTM
BiLSTM
BiLSTM
BiLSTM

SL
SL
SL
SL
SL
SL

–
–
–

47.9
51.2
52.1
51.5
52.3

52.5
54.8
55.2
53.8
56.9
56.5

47.8
48.4
49.1

43.6
47.1
50.1
50.6
50.4

48.2
49.9
54.6
52.7
54.6
55.6

19.5
19.7
21.9

16.6
19.7
20.8
24.2
22.1

17.4
20.9
23.3
26.8
25.1
28.3

28.5
28.4
30.1

24.3
28.9
29.0
31.7
30.8

25.5
29.2
31.4
34.3
33.0
35.5

Table 2: F0.5 scores on various tests contrasted with published results and unaugmented baseline models.

the additional computations were helpful in creat-
ing high-quality synthetic instances.

Post-processing: Original and corrupted sen-
tences are aligned at a word-level using Leven-
shtein distance. Using the minimal alignment,
words in the corrupted sentence are labelled cor-
rect, ‘c’, or incorrect, ‘i’, as follows:

If the word is not aligned with itself, then ‘i’.
Else, if following a gap, then ‘i’, as at this point
a human reader would notice that there is a word
missing in the sentence. Else, if it is the last word,
but it is not aligned to the last word of the source
sentence, then ‘i’, as a human would realise that
this sentence ends abruptly, Else, ‘c’.

These token-labelled corrupted sentences now
form an artiﬁcial dataset for training an error de-
tector. Duplicate instances and corrupted sen-
tences with more than 5 errors were dropped to
remove noise from the downstream training.

4 Experiments

We evaluated our approach on the First Certiﬁcate
of English (FCE) error detection dataset (Rei and
Yannakoudakis, 2016), as well as on two human-
annotated test sets (CoNLL1, CoNLL2) from the
CoNLL 2014 shared task (Ng et al., 2014). The
CoNLL data sets pose a unique challenge; as they
are different in style and domain from FCE, we

have no matching training data. We compared
the effect of different neural generation proce-
dures (AM, TS, BS) and contrasted the down-
stream performance of a bidirectional LSTM with
an elaborate sequence labeller.

4.1

Implementation details

NMT training and corruption: We minimally
modiﬁed the open source implementation1 of Britz
et al. (2017) to implement TS and BS.2 We trained
our NMT with a single-layered encoder and de-
coder with cell size 256, on the parallel cor-
pus version of FCE (Yannakoudakis et al., 2011),
with early stopping after the FCE development
set score dropped consistently for 20 epochs. We
introduced errors into three datasets: FCE it-
self (450K tokens), the English Vocabulary Pro-
ﬁle or EVP (270K tokens) and a subset of Simple
Wikipedia or SW (8.4M tokens); of these, FCE
and EVP were both used in artiﬁcial error genera-
tion via SMT and pattern extraction (PAT) by Rei
et al. (2017), enabling us to make a fair experi-
mental comparison. Ten corrupted versions using
each of AM, TS (τ = 0.05) and BS were sam-
pled for FCE and EVP corruptions, while one suf-
ﬁced for SW. The theoretical time complexity of
BS is O(bn) for each sentence, where b is num-

1 https://github.com/google/seq2seq
2 https://github.com/skasewa/wronging

Baseline
EVP+AM
EVP+TS
EVP+BS

10

5

0

e
r
o
c
s

5
.
0
F
n
i

e
s
a
e
r
c
n
i

e
t
u
l
o
s
b
A

e
r
o
c
s

5
.
0
F

35

30

25

20

15

FCE

SW

CoNLL1
CoNLL2

1x 2x 3x 4x 5x 6x 7x 8x 9x 10x
Amount of augmented data

FCE (dev)

FCE

CoNLL1

CoNLL2

Figure 1: Improvements using three different meth-
ods of generation.

Figure 2: Training with increasing amounts of cor-
rupted data from FCE and SW.

ber of candidates, and n is the maximum length
of a sentence. Empirically, BS with b = 11 took
a factor of 11.3 more time than AM. Examples of
generated errors are provided in Table 1.

Error detection: We compare two error detec-
tion models: a vanilla bi-directional LSTM (BiL-
STM) (Schuster and Paliwal, 1997), and the state-
of-the-art sequence labeller (SL) neural network
used by Rei et al. (2017). These models were
trained on the binary-labelled FCE training set
augmented with the corrupted instances. Wher-
ever no model is explicitly stated, the SL model
was used. During training, we alternate between
the annotated FCE dataset and the synthetic col-
lection. This alternating protocol prevents over-
ﬁtting on FCE; once it shifts back, it reinforces
connections made from the helpful synthetic cor-
ruptions while forgetting about the noisy ones.

4.2 Results

The results for our baselines and data augmenta-
tion strategies can be found in Table 2. Augmented
with our NMT generated data, even our vanilla
downstream BiLSTM outperforms the SMT+PAT
artiﬁcial error augmentation approach of Rei et al.
(2017), indicating that our process better gener-
alises the error information in the source dataset.
Using the more powerful SL network bests the
previous state of the art by over 5% on the FCE
test. Most intriguingly, we note a signiﬁcant im-
provement for the CoNLL tests using corruptions
from out-of-domain SW. Figure 2 illustrates how
we gain performance on these tests with increas-
ing amounts of corrupted SW, which does not hold
true for corrupted FCE. This shows that we were

able to induce useful errors into a corpus with a
large unseen vocabulary and different syntactic bi-
ases, and this in turn proved valuable for detect-
ing errors in a third domain, suggesting that our
method can transfer learned distributions across
stylistic genres.

Using EVP as a standard source, Figure 1 illus-
trates the variance of the different sampling meth-
ods. All generation methods yield corruptions that
signiﬁcantly improve test performance, with in-
stances sampled by beam-search consistently out-
performing the alternatives.

5 Discussion

5.1 Error distribution

The original FCE dataset was annotated using the
error taxonomy speciﬁed in Nicholls (2003), and
contains 75 unique error codes. We annotated
samples of EVP corrupted by all three sampling
methods, at a reduced resolution, to compare the
distribution of errors across FCE and the synthetic
corpora. These are presented in Table 3.

At a high level, NMT generates errors more of-
ten among more common parts-of-speech, favour-
ing errors in verbs and nouns, rather than in ad-
verbs and conjunctions. It did not make spelling
errors as often as in the source dataset;
this
is likely because it only observed the speciﬁc
spelling errors present in FCE, and as the vocab-
ulary is restricted to that dataset, it does not en-
counter those words as frequently in EVP, and thus
rarely makes the same spelling mistakes.

Additionally, the differences in these distribu-
tions can partially be attributed to the implicit dif-
ferences between us and the annotators of FCE.

Spelling

FCE AM TS BS

Spelling errors

11

1

1

4

Part-of-speech FCE AM TS BS

AM

TS

BS

Precision
Recall
F1

81.25
26.00
39.39

63.63
28.00
38.89

50.00
14.00
22.22

Verb
Preposition
Determiner
Noun
Pronoun
Adverb
Adjective
Conjunction
Quantiﬁer

Replacement
Inclusion
Removal
Word form
Word order

34
18
16
13
7
5
3
2
1

49
23
14
9
5

16
16
7
36
3
5
15
2
0

35
30
33
2
0

26
10
6
35
3
3
16
2
0

34
27
36
2
0

16
14
10
43
1
5
12
1
0

32
35
32
1
0

Remedy Type

FCE AM TS BS

Table 3: Error distribution across FCE and manu-
ally annotated samples of artiﬁcial data. Spelling
errors are a % of all errors, while Part-of-speech,
and Remedy Type are compared within their own
categories to sum to 100%.

5.2 Comparison with human errors

To check if the synthetic instances passed for
human-like, we mixed 50 generated sentences
among an equal number of actual ungrammatical
instances from FCE-dev and tasked a human eval-
uator to identify the artiﬁcial statements, in a sim-
ple Turing-style test. We created three such sets,
one for each of our sampling techniques, and the
test subject aimed to identify synthetic samples
with high conﬁdence. Results of this test are pre-
sented in Table 4.

The high precision but low recall scores suggest
that while it is still possible to spot some corrup-
tions that are quite clearly artiﬁcial, the bulk of our
samples do not betray their synthetic nature and
are indistinguishable from naturally occurring er-
roneous sentences. In order to fairly compare our
work with earlier results, we intended to conduct
such a test for sentences generated by the SMT
of Rei et al. (2017). Unfortunately, we were only
able to source corruptions of FCE-train via this
method; therefore, we decided not to perform this
test as its results cannot be compared to ours.

Table 4: Results of a Turing-style test, where a sub-
ject was asked to distinguish between real and fake
sentences, sampled from each of the different gen-
erated corpora.

6 Conclusions and future work

We presented a novel data augmentation tech-
nique for grammatical error detection using neu-
ral machine translation to learn the distribution
of language-learner errors, and induce such er-
rors into grammatically correct text. We explored
several different variants of sampling to improve
the quality of our synthetic errors. After creat-
ing artiﬁcial training instances with an off-the-
shelf NMT, we bettered previous state-of-the-art
results on the canonical test with even a basic BiL-
STM, and established a new state of the art using
a stronger model. Additionally, we demonstrated
that we were able to leverage corruptions of an
out-of-domain dataset to set new benchmarks on
separate, also out-of-domain tests, without specif-
ically optimising for either.

Our work indicates that neural error genera-
tion warrants further investigation with different
datasets and architectures, both for error detec-
tion and error correction. Among possible fu-
ture work is using generative adversarial networks
as corruption engines, and developing better se-
quence alignment methods. Some preliminary re-
sults with simple corruptions using word substitu-
tion and word dropout (Iyyer et al., 2015) appear
to be promising, and may feature as components of
a future corruption system. Finally, one could use
such artiﬁcial error-prone corpora as source text
for self-training an error detection system.

Acknowledgements

We thank Marek Rei and Mariano Felice for grant-
ing access to their data and code. Also, we would
like to thank the anonymous reviewers and Jo-
hannes Welbl for valuable feedback and discus-
sions. This work was supported by an Allen Dis-
tinguished Investigator Award.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Neural machine translation by
CoRR,

Bengio. 2014.
jointly learning to align and translate.
abs/1409.0473.

Guillaume Bouchard, Pontus Stenetorp, and Sebastian
Riedel. 2016. Learning to generate textual data.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1608–1616, Austin, Texas. Association for Compu-
tational Linguistics.

Denny Britz, Anna Goldie, Thang Luong, and Quoc
Le. 2017. Massive Exploration of Neural Machine
Translation Architectures. ArXiv e-prints.

Chris Brockett, William B. Dolan, and Michael Ga-
mon. 2006. Correcting esl errors using phrasal smt
In Proceedings of the 21st Interna-
techniques.
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 249–256, Sydney, Aus-
tralia. Association for Computational Linguistics.

Mariano Felice and Zheng Yuan. 2014. Generat-
ing artiﬁcial errors for grammatical error correction.
In Proceedings of the Student Research Workshop
at the 14th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 116–126, Gothenburg, Sweden. Association
for Computational Linguistics.

Jennifer Foster and Øistein E Andersen. 2009. Gen-
errate: generating errors for use in grammatical error
detection. In Proceedings of the fourth workshop on
innovative use of nlp for building educational ap-
plications, pages 82–90. Association for Computa-
tional Linguistics.

Andrew R Golding and Dan Roth. 1996. Apply-
ing winnow to context-sensitive spelling correction.
arXiv preprint cmp-lg/9607024.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
In Advances in neural information
versarial nets.
processing systems, pages 2672–2680.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8):1735–
1780.

Kenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, and
Hitoshi Nishikawa. 2012. Grammar error correc-
tion using pseudo-error sentences and domain adap-
tation. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Short Papers-Volume 2, pages 388–392. Association
for Computational Linguistics.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daum´e III. 2015. Deep unordered com-
position rivals syntactic methods for text classiﬁca-
tion. In Proceedings of the 53rd Annual Meeting of

the Association for Computational Linguistics and
the 7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), vol-
ume 1, pages 1681–1691.

Marcin Junczys-Dowmunt and Roman Grundkiewicz.
2014. The amu system in the conll-2014 shared
task: Grammatical error correction by data-intensive
and feature-rich statistical machine translation.
In
Proceedings of the Eighteenth Conference on Com-
Shared
putational Natural Language Learning:
Task, pages 25–33, Baltimore, Maryland. Associa-
tion for Computational Linguistics.

Marcin Junczys-Dowmunt and Roman Grundkiewicz.
2016. Phrase-based machine translation is state-of-
the-art for automatic grammatical error correction.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1546–1556, Austin, Texas. Association for Compu-
tational Linguistics.

Kevin Knight,

Ishwar Chander, Matthew Haines,
Vasileios Hatzivassiloglou, Eduard Hovy, Masayo
Iida, Steve K Luk, Akitoshi Okumura, Richard
Whitney, and Kenji Yamada. 1994.
Integrating
knowledge bases and statistics in mt. arXiv preprint
cmp-lg/9409001.

Nina H Macdonald. 1983. Human factors and behav-
ioral science: The unix writer’s workbench soft-
ware: Rationale and design. Bell Labs Technical
Journal, 62(6):1891–1908.

David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing.
In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152–159, New York City, USA. Association for
Computational Linguistics.

Courtney Napoles and Chris Callison-Burch. 2017.
Systematically adapting machine translation for
In Proceedings of
grammatical error correction.
the 12th Workshop on Innovative Use of NLP for
Building Educational Applications, pages 345–356,
Copenhagen, Denmark. Association for Computa-
tional Linguistics.

Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The conll-2014 shared task on
grammatical error correction. In Proceedings of the
Eighteenth Conference on Computational Natural
Language Learning: Shared Task, pages 1–14, Bal-
timore, Maryland. Association for Computational
Linguistics.

Diane Nicholls. 2003. The cambridge learner corpus:
Error coding and analysis for lexicography and elt.
In Proceedings of the Corpus Linguistics 2003 con-
ference, volume 16, pages 572–581.

Marek Rei. 2017. Semi-supervised multitask learn-
In Proceedings of the

ing for sequence labeling.

Jason Wang and Luis Perez. 2017. The effectiveness of
data augmentation in image classiﬁcation using deep
learning. Technical report, Technical report.

Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading esol texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
180–189, Portland, Oregon, USA. Association for
Computational Linguistics.

Helen Yannakoudakis, Marek Rei, Øistein E. Ander-
sen, and Zheng Yuan. 2017. Neural sequence-
labelling models for grammatical error correction.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2795–2806, Copenhagen, Denmark. Association for
Computational Linguistics.

Zheng Yuan and Ted Briscoe. 2016. Grammatical er-
ror correction using neural machine translation. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 380–386, San Diego, California. Association
for Computational Linguistics.

Zheng Yuan and Mariano Felice. 2013. Constrained
grammatical error correction using statistical ma-
In Proceedings of the Seven-
chine translation.
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 52–61, Soﬁa,
Bulgaria. Association for Computational Linguis-
tics.

55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 2121–2130, Vancouver, Canada. Association
for Computational Linguistics.

Marek Rei, Gamal Crichton, and Sampo Pyysalo. 2016.
Attending to characters in neural sequence label-
In Proceedings of COLING 2016,
ing models.
the 26th International Conference on Computational
Linguistics: Technical Papers, pages 309–318, Os-
aka, Japan. The COLING 2016 Organizing Commit-
tee.

Marek Rei, Mariano Felice, Zheng Yuan, and Ted
Briscoe. 2017. Artiﬁcial error generation with ma-
chine translation and syntactic patterns. In Proceed-
ings of the 12th Workshop on Innovative Use of NLP
for Building Educational Applications, pages 287–
292, Copenhagen, Denmark. Association for Com-
putational Linguistics.

Marek Rei and Helen Yannakoudakis. 2016. Composi-
tional sequence labeling models for error detection
In Proceedings of the 54th An-
in learner writing.
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1181–
1191, Berlin, Germany. Association for Computa-
tional Linguistics.

Stephen D. Richardson and Lisa C. Braden-Harder.
1988. The experience of developing a large-scale
natural language text processing system: Critique.
In Second Conference on Applied Natural Language
Processing.

Alla Rozovskaya and Dan Roth. 2010.

Training
paradigms for correcting errors in grammar and us-
In Human language technologies: The 2010
age.
annual conference of the north american chapter of
the association for computational linguistics, pages
154–162. Association for Computational Linguis-
tics.

Alla Rozovskaya and Dan Roth. 2014. Building a
state-of-the-art grammatical error correction system.
Transactions of the Association for Computational
Linguistics, 2:419–434.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673–2681.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
Improving neural machine translation mod-
2016.
In Proceedings of the
els with monolingual data.
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
86–96, Berlin, Germany. Association for Computa-
tional Linguistics.

Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-
mar as a foreign language. In Advances in Neural
Information Processing Systems, pages 2773–2781.

