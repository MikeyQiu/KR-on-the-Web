7
1
0
2
 
b
e
F
 
5
 
 
]
L
C
.
s
c
[
 
 
2
v
9
5
7
3
0
.
0
1
6
1
:
v
i
X
r
a

Language Models with Pre-Trained (GloVe)
Word Embeddings

Victor Makarenkov, Lior Rokach, Bracha Shapira
makarenk@post.bgu.ac.il, liorrk@bgu.ac.il, bshapira@bgu.ac.il

Department of Software and Information Systems Engineering
Ben-Gurion University of the Negev

1

Introduction

In this work we present a step-by-step implementation of training a Language
Model (LM) , using Recurrent Neural Network (RNN) and pre-trained GloVe
word embeddings, introduced by Pennigton et al. in [1]. The implementation is
following the general idea of training RNNs for LM tasks presented in [2] , but
is rather using Gated Recurrent Unit (GRU) [3] for a memory cell, and not the
more commonly used LSTM [4]. The implementation presented is based on using
keras1 [5].

2 Motivation

Language Modeling is an important task in many Natural Language Processing
(NLP) applications. These application include clustering, information retrieval,
machine translation, spelling and grammatical errors correction. In general, a
language model deﬁned as a function that puts a probability measure over strings
drawn from some vocabulary. In this work we consider a RNN based language
model task, which aims at predicting the next n-th word in a sequence, given
the previous n − 1 words. Put otherwise, ﬁnding the word with maximum value
for P (wn|w1, ..., wn−1) . The n parameter is the ContextW indowSize argument
in the algorithm described further.
To maximize the eﬀectiveness and performance of the model we use word em-
beddings into a continuous vector space. The model of embedding we use is
the GloVe [1] model, with dimensionality size equal to 300 or 50. We use both
pre-trained2 on 42 billion tokens and 1.9 million vocabulary size, and speciﬁ-
cally trained for this work vector models, which we trained on SIGIR-2015 and
ICTIR-2015 conferences’ proceedings.
The model itself is trained as a RNN, with internal GRU for memorizing the prior
sequence of words. It was shown lately, that RNNs outperform most language
modeling based tasks [2, 3] when tuned and trained correctly.

1 https://keras.io/
2 downloaded from: http://nlp.stanford.edu/projects/glove/

3 Short Description

In this work we use 300-dimensional and 50-dimensional, GloVe word embed-
dings. In order to embed the words in a vector space, GloVe model is trained
by examining word co-occurrence matrix Xij within a huge text corpus. Despite
the huge size of the Common Crawl corpus, some words may not exist with the
embeddings, so we set these words to random vectors, and use the same embed-
dings consistently if we encounter the same unseen word again in the text. The
RNN is further trained to predict the next word in its embedding form, that is,
predicts the next n-dimensional vector, given the ContextW indowSize previ-
ous words. We divide the T extF ile into 70% and 30% for training and testing
purposes.

4 Pseudo Code

input : Input: glove-vectors : Pre-Trained-Word-Embeddings, Text-File,

ContextWindowSize=10, hidden-unites=300

output: A Language Model trained on Text-File with word-embeddings

representation
for w ∈ Text-File do

if w ∈ OOV-ﬁle then

end
if w ∈ glove-vectors then

tokenized-ﬁle.append(OOV-ﬁle.get-vector(w))

tokenized-ﬁle.append(glove-vectors.get-vector(w))

end
else

end

vector ← random-vector()
tokenized-ﬁle.append(vector)
OOV-ﬁle.append(vector)

end
NN ← CreateSingleHiddenLayerDenseRNN(unit=GRU, inputs=300,
outputs=300, hidden-unites)
NN.setDropout(0.8)
NN.setActivationFunction(Linear)
NN.setLossFunction(MSE)
NN.setOptimizer(rmsprop)
Xtrain ← tokenized-ﬁle.getInterval(0.0,0.7)
Xtest ← tokenized-ﬁle.getInterval(0.7,1.0)
Ytrain ← Xtrain.Shif t(ContextW indowSize)
Ytest ← Xtest.Shif t(ContextW indowSize)
NN.Fit(Xtrain, Ytrain)
NN.Predict(Xtest, Ytest)

Algorithm 1: Training a language model on word embeddings

5 Detailed Explanation

As stated earlier, GloVe model is trained by examining word co-occurrence
matrix of two words i and j: Xij within a huge text corpus. While training
T wj + bi + bj = log(Xij) where wi and wj
the main idea is stating that wi
are the trained vectors, bi and bj are the scalar bias terms associated with
words i and j. The most important parts of the training process in GloVe
are: 1) A weighting function f for elimination of very common words (like
stop words) which add noise and not overweighted, 2) rare words are not over-
weighted 3) the co-occurrence strength, when modeled as a distance, should be
smoothed with a log function. Thus, the ﬁnal loss function for a GloVe model is
T wj +bi +bj −log(Xij))2 where V is a complete vocabulary,
J = Σi,j∈V f (Xij)(wi
and f (x) = (x/xmax)α if x < xmax, and f (x) = 1 otherwise. The model that is
used in this work was trained with xmax = 100 and α = 0.75.

Fig. 1. A general architecture of training an unfolded RNN with 1-sized window based
shifted labeled data

Training of the RNN is somewhat blurred between supervised and unsuper-
vised techniques. That is, no extra labeled data is given, but part of the input is
used as labels. In this unfolded training paradigm, which is illustrated on Figure
1, the output is shifted in a way to create a labels for the input train dataset.
In this way the RNN can actually learn to predict a next word (vector) in a
sequence.

6 Evaluation

6.1 Pre-trained Vector Models

In order to evaluate our implementation of the language model, we train several
diﬀerent language models and compare the predicted error distribution with a

random word prediction. The error is measured with a cosine distance3 between
two vectors: 1 − xxx·yyy
||xxx||·||yyy|| . On ﬁgure 2 we see the error distribution of the RNN
with 30 hidden units. The training was performed on 5000 tokens long text ﬁle,
the ﬁrst entry at English wikipedia, Anarchism.

Fig. 2. Two distributions of the predicted next word vector errors. The left is the
result of predicted by RNN errors, and the righ is predicted by random. The RNN in
the model was trained with 30 hidden GRU units. It took 300 iterations (epochs) on
the data to achieve these results.

The machine that was used to run the evaluation had the following charac-

teristics: 1.7 GHz, Core i7 with 8 GB memory, OS X version 10.11.13.

The time it took to train the model with 30 epochs was 125 seconds . The

time took to make the predictions on a test set is 0.5 seconds.

On ﬁgure 3 we see the error distribution of the RNN with 300 hidden units
The time it took to train the model with 300 epochs was 1298 seconds . The
time took to make the predictions on a test set is 0.49 seconds.

6.2 Self Trained Vector Model

In addition, in order to further evaluate the current approach, we speciﬁcally
trained a narrow domain-speciﬁc, vector model. We used ICTIR-2015 and SIGIR-
2015 conferences proceedings as a corpora, and produced 50-dimensional vectors.
The vector model is based on 1,500,000 tokens total, and resulted in 17,000 long
vocabulary. The language model for the evaluation was built on a paper pub-
lished in the ICTIR-2015 proceedings [7]. Consider ﬁgure 4. The predicted words’
errors distribution diﬀers even more than in the general case, where the vectors
were trained on the general Common-Crawl corpora. That is, the performance
of the language model, for the task of word prediction is higher.

The time took to train this model is 10 seconds. The time took to compute

the predictions for evaluations is 0.04 seconds.

3 implemented on python, with scipy package

Fig. 3. Two distributions of the predicted next word vector errors. The left is the
result of predicted by RNN errors, and the righ is predicted by random. The RNN in
the model was trained with 300 hidden GRU units. It took 300 iterations (epochs) on
the data to achieve these results.

7

Instructions for running the code

The implementation of the model training was written in this work in the Python
language, version 2.7. The library that was used is Keras, which in the course
of this implementation was based on Theano framework. Instead of Theano, the
Google’s Tensorﬂow can be also used behind the scenes of the Keras in this
implementation. In order to train the model yourself, you need to follow the
next steps:

1. Download pre-trained GloVe vectors from http://nlp.stanford.edu/projects/glove/
2. Obtain a text to train the model on. In our example we use a Wikipdia

Anarchism entry.

3. Open and adjust the LM_RNN_GloVe.py ﬁle parameters inside the main

function:
(a) ﬁle_2_tokenize_name (example = "/Users/macbook/corpora/text2tokenize.txt")
(b) tokenized_ﬁle_name (example = "/Users/macbook/corpora/tokenized2vectors.txt")
(c) glove_vectors_ﬁle_name (example = "/Users/macbook/corpora/glove.42B.300d.txt")
(d) extra_vocab_ﬁlename (example = "/Users/macbook/corpora/extra_vocab.txt").

This argument has also a default value in the get_vector function

4. Run the following methods:

(a) tokenize_ﬁle_to_vectors(glove_vectors_ﬁle_name, ﬁle_2_tokenize_name,

tokenized_ﬁle_name)

(b) run_experiment(tokenized_ﬁle_name)

8 Discussion

In this work we implemented and tested the training of a LM based on RNN.
To emphasize the strength of such an approach, we have chosen one of the most

Fig. 4. Two distributions of the predicted next word vector errors. The left is the
result of predicted by RNN errors, and the righ is predicted by random. The RNN in
the model was trained with 50 hidden GRU units. It took 50 iterations (epochs) on
the data to achieve these results.

powerful and prominent techniques for word embeddings - the GloVe embed-
dings. Although there are other approaches, such as the popular word2vec [8]
technique, the GloVe embeddings was shown to outperform it on several tasks
[1], partially because of the reasons described in section 5. By training the model
with two diﬀerent settings, one of which is order of magnitude more complex than
the other we show the power of such LM. The distributions shown on ﬁgures 2
and 3 clearly indicate much smaller error on the task of next word prediction.

The main limitation of this implementation is the ﬁxed window size, of the
preﬁx in the LM. This approach does not fully show the full power of RNN-
based LM. For dynamic size preﬁx LM please consider the DyNet [6] package for
example. DyNet supports a dynamic computation graph and shares the learned
parameters across multiple variable length instances during the training.

9 The source at GitHub

The code was submitted publicly to the GitHub repository of the author, and is
available under vicmak username, proofseer project4.

4 https://github.com/vicmak/ProofSeer

References

[1] Pennington, J., Socher, R., Manning, C.D.: Glove: Global vectors for word
In: Empirical Methods in Natural Language Processing

representation.
(EMNLP). (2014) 1532–1543

[2] Zaremba, W., Sutskever, I., Vinyals, O.: Recurrent neural network regular-

ization. CoRR abs/1409.2329 (2014)

[3] Cho, K., van Merrienboer, B., Gülçehre, Ç., Bougares, F., Schwenk, H., Ben-
gio, Y.: Learning phrase representations using RNN encoder-decoder for
statistical machine translation. CoRR abs/1406.1078 (2014)

[4] Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput.

9 (1997) 1735–1780

[5] FranÃ§ois Chollet : keras GitHub repository. 2015.
[6] Graham Neubig and Chris Dyer and Yoav Goldberg and Austin Matthews
and Waleed Ammar and Antonios Anastasopoulos and Miguel Ballesteros
and David Chiang and Daniel Clothiaux and Trevor Cohn and Kevin Duh
and Manaal Faruqui and Cynthia Gan and Dan Garrette and Yangfeng Ji and
Lingpeng Kong and Adhiguna Kuncoro and Gaurav Kumar and Chaitanya
Malaviya and Paul Michel and Yusuke Oda and Matthew Richardson and
Naomi Saphra and Swabha Swayamdipta and Pengcheng Yin: DyNet: The
Dynamic Neural Network Toolkit arXiv preprint arXiv:1701.03980. 2017.
[7] Makarenkov, V., Shapira, B., Rokach, L.: Theoretical categorization of query
performance predictors. In: Proceedings of the 2015 International Conference
on The Theory of Information Retrieval. ICTIR ’15, New York, NY, USA,
ACM (2015) 369–372

[8] Mikolov, T., Chen, K., Corrado, G., Dean, J.: Eﬃcient estimation of word

representations in vector space. CoRR abs/1301.3781 (2013)

