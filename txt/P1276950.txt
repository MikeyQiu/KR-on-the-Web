Adversarial Multi-Criteria Learning for Chinese Word Segmentation

Xinchi Chen, Zhan Shi, Xipeng Qiu∗, Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, China
{xinchichen13,zshi16,xpqiu,xjhuang}@fudan.edu.cn

Abstract

Different
linguistic perspectives causes
many diverse segmentation criteria for
Chinese word segmentation (CWS). Most
existing methods focus on improve the
performance for each single criterion.
However, it is interesting to exploit these
different criteria and mining their com-
In this pa-
mon underlying knowledge.
per, we propose adversarial multi-criteria
learning for CWS by integrating shared
knowledge from multiple heterogeneous
Experiments on
segmentation criteria.
eight corpora with heterogeneous segmen-
tation criteria show that the performance
of each corpus obtains a signiﬁcant im-
provement, compared to single-criterion
learning. Source codes of this paper are
available on Github1.

1

Introduction

Chinese word segmentation (CWS) is a prelimi-
nary and important task for Chinese natural lan-
guage processing (NLP). Currently, the state-of-
the-art methods are based on statistical supervised
learning algorithms, and rely on a large-scale an-
notated corpus whose cost is extremely expen-
sive. Although there have been great achieve-
ments in building CWS corpora, they are some-
what incompatible due to different segmentation
criteria. As shown in Table 1, given a sentence
“YaoMing reaches the ﬁnal”, the two commonly-
used corpora, PKU’s People’s Daily (PKU) (Yu
et al.(2001)Yu, Lu, Zhu, Duan, Kang, Sun, Wang,
Zhao, and Zhan) and Penn Chinese Treebank
(CTB) (Fei(2000)), use different segmentation cri-
teria. In a sense, it is a waste of resources if we fail
to fully exploit these corpora.

∗Corresponding author.
1https://github.com/FudanNLP

Figure 1: Illustration of the different segmentation
criteria.

Recently, some efforts have been made to
exploit heterogeneous annotation data for Chi-
nese word segmentation or part-of-speech tag-
ging (Jiang et al.(2009)Jiang, Huang, and Liu;
Sun and Wan(2012); Qiu et al.(2013)Qiu, Zhao,
and Huang; Li et al.(2015)Li, Chao, Zhang,
and Chen; Li et al.(2016)Li, Chao, Zhang, and
Yang).
These methods adopted stacking or
multi-task architectures and showed that hetero-
geneous corpora can help each other. How-
ever, most of these model adopt the shallow lin-
ear classiﬁer with discrete features, which makes
it difﬁcult to design the shared feature spaces,
usually resulting in a complex model.
Fortu-
nately, recent deep neural models provide a con-
venient way to share information among multi-
ple tasks (Collobert and Weston(2008); Luong
et al.(2015)Luong, Le, Sutskever, Vinyals, and
Kaiser; Chen et al.(2016)Chen, Zhang, and Liu).

In this paper, we propose an adversarial multi-
criteria learning for CWS by integrating shared
knowledge from multiple segmentation criteria.
Speciﬁcally, we regard each segmentation cri-
terion as a single task and propose three dif-
ferent shared-private models under the frame-
work of multi-task learning (Caruana(1997); Ben-
David and Schuller(2003)), where a shared layer
is used to extract the criteria-invariant features,
and a private layer is used to extract the criteria-
speciﬁc features.
Inspired by the success of ad-
versarial strategy on domain adaption (Ajakan
et al.(2014)Ajakan, Germain, Larochelle, Lavi-
olette, and Marchand; Ganin et al.(2016)Ganin,
Ustinova, Ajakan, Germain, Larochelle, Lavi-

7
1
0
2
 
r
p
A
 
5
2
 
 
]
L
C
.
s
c
[
 
 
1
v
6
5
5
7
0
.
4
0
7
1
:
v
i
X
r
a

olette, Marchand, and Lempitsky; Bousmalis
et al.(2016)Bousmalis, Trigeorgis, Silberman, Kr-
ishnan, and Erhan), we further utilize adversarial
strategy to make sure the shared layer can extract
the common underlying and criteria-invariant fea-
tures, which are suitable for all the criteria. Fi-
nally, we exploit the eight segmentation criteria
on the ﬁve simpliﬁed Chinese and three traditional
Chinese corpora. Experiments show that our mod-
els are effective to improve the performance for
CWS. We also observe that traditional Chinese
could beneﬁt from incorporating knowledge from
simpliﬁed Chinese.

The contributions of this paper could be sum-

marized as follows.

• Multi-criteria learning is ﬁrst introduced for
CWS, in which we propose three shared-
private models to integrate multiple segmen-
tation criteria.

• An adversarial strategy is used to force the
shared layer to learn criteria-invariant fea-
tures, in which an new objective function is
also proposed instead of the original cross-
entropy loss.

• We conduct extensive experiments on eight
CWS corpora with different segmentation
criteria, which is by far the largest number
of datasets used simultaneously.

2 General Neural Model for Chinese

Word Segmentation

Chinese word segmentation task is usually re-
garded as a character based sequence labeling
problem. Speciﬁcally, each character in a sen-
tence is labeled as one of L = {B, M, E, S},
indicating the begin, middle, end of a word, or
a word with single character. There are lots
of prevalent methods to solve sequence labeling
problem such as maximum entropy Markov model
(MEMM), conditional random ﬁelds (CRF), etc.
Recently, neural networks are widely applied to
Chinese word segmentation task for their abil-
ity to minimize the effort in feature engineer-
ing (Zheng et al.(2013)Zheng, Chen, and Xu;
Pei et al.(2014)Pei, Ge, and Baobao; Chen
et al.(2015a)Chen, Qiu, Zhu, and Huang; Chen
et al.(2015b)Chen, Qiu, Zhu, Liu, and Huang).

Speciﬁcally, given a sequence with n charac-
ters X = {x1, . . . , xn}, the aim of CWS task
is to ﬁgure out the ground truth of labels Y ∗ =

Figure 2: General neural architecture for Chinese
word segmentation.

{y∗

1, . . . , y∗

n}:

Y ∗ = arg max

p(Y |X),

(1)

Y ∈Ln

where L = {B, M, E, S}.

The general architecture of neural CWS could
be characterized by three components: (1) a char-
acter embedding layer; (2) feature layers consist-
ing of several classical neural networks and (3) a
tag inference layer. The role of feature layers is to
extract features, which could be either convolution
neural network or recurrent neural network. In this
paper, we adopt the bi-directional long short-term
memory neural networks followed by CRF as the
tag inference layer. Figure 2 illustrates the general
architecture of CWS.

2.1 Embedding layer

In neural models, the ﬁrst step usually is to map
discrete language symbols to distributed embed-
ding vectors. Formally, we lookup embedding
vector from embedding matrix for each character
xi as exi ∈ Rde, where de is a hyper-parameter
indicating the size of character embedding.

2.2 Feature layers

feature layers.

long short-term mem-
We adopt bi-directional
While
ory (Bi-LSTM) as
there are numerous LSTM variants, here we
use the LSTM architecture used by (Joze-
fowicz et al.(2015)Jozefowicz, Zaremba, and
Sutskever), which is similar to the architecture
of (Graves(2013)) but without peep-hole connec-
tions.

LSTM LSTM introduces gate mechanism and
memory cell to maintain long dependency infor-
mation and avoid gradient vanishing. Formally,
LSTM, with input gate i, output gate o, forget gate
f and memory cell c, could be expressed as:













ii
oi
fi
˜ci

=













σ
σ
σ
φ

(cid:18)

(cid:124)

Wg

(cid:21)

(cid:20) exi
hi−1

(cid:19)

+ bg

,

ci = ci−1 (cid:12) fi + ˜ci (cid:12) ii,
hi = oi (cid:12) φ(ci),

where Wg ∈ R(de+dh)×4dh and bg ∈ R4dh are
trainable parameters. dh is a hyper-parameter, in-
dicating the hidden state size. Function σ(·) and
φ(·) are sigmoid and tanh functions respectively.

Bi-LSTM In order to incorporate information
from both sides of sequence, we use bi-directional
LSTM (Bi-LSTM) with forward and backward di-
rections. The update of each Bi-LSTM unit can be
written precisely as follows:

hi =

−→
h i ⊕

←−
h i,

−→
h i−1,

←−
h i+1, θ),

−→
h i and

= Bi-LSTM(exi,
←−
h i are the hidden states at posi-
where
tion i of the forward and backward LSTMs respec-
tively; ⊕ is concatenation operation; θ denotes all
parameters in Bi-LSTM model.

(6)

2.3

Inference Layer

random ﬁelds

After extracting features, we employ con-
ditional
(Lafferty
et al.(2001)Lafferty, McCallum, and Pereira)
layer to inference tags. In CRF layer, p(Y |X) in
Eq (1) could be formalized as:

(CRF)

p(Y |X) =

Ψ(Y |X)
Y (cid:48)∈Ln Ψ(Y (cid:48)|X)

.

(cid:80)

Here, Ψ(Y |X) is the potential function, and we
only consider interactions between two successive
labels (ﬁrst order linear chain CRFs):

n
(cid:89)

i=2

Ψ(Y |X) =

ψ(X, i, yi−1, yi),

ψ(x, i, y(cid:48), y) = exp(s(X, i)y + by(cid:48)y),

(2)

(3)

(4)

(5)

(7)

(8)

(9)

assigns score for each label on tagging the i-th
character:

s(X, i) = W(cid:62)

s hi + bs,

(10)

where hi is the hidden state of Bi-LSTM at posi-
tion i; Ws ∈ Rdh×|L| and bs ∈ R|L| are trainable
parameters.

3 Multi-Criteria Learning for Chinese

Word Segmentation

Although neural models are widely used on CWS,
most of them cannot deal with incompatible crite-
ria with heterogonous segmentation criteria simul-
taneously.

Inspired by the success of multi-task learning
(Caruana(1997); Ben-David and Schuller(2003);
Liu et al.(2016a)Liu, Qiu, and Huang; Liu
et al.(2016b)Liu, Qiu, and Huang), we regard the
heterogenous criteria as multiple “related” tasks,
which could improve the performance of each
other simultaneously with shared information.

Formally, assume that there are M corpora with
heterogeneous segmentation criteria. We refer Dm
as corpus m with Nm samples:
, Y (m)
i

Dm = {(X (m)

)}Nm
i=1,

(11)

i

i and Y m
where X m
i
the corresponding label in corpus m.

denote the i-th sentence and

To exploit the shared information between these
different criteria, we propose three sharing models
for CWS task as shown in Figure 3. The feature
layers of these three models consist of a private
(criterion-speciﬁc) layer and a shared (criterion-
invariant) layer. The difference between three
models is the information ﬂow between the task
layer and the shared layer. Besides, all of these
three models also share the embedding layer.

3.1 Model-I: Parallel Shared-Private Model

In the feature layer of Model-I, we regard the pri-
vate layer and shared layer as two parallel layers.
For corpus m, the hidden states of shared layer and
private layer are:

h(s)
i =Bi-LSTM(exi,
h(m)
i =Bi-LSTM(exi,

−→
h (s)
i−1,
−→
h (m)
i−1,

←−
h (s)
←−
h (m)

i+1, θs),
i+1, θm),

(12)

(13)

and the score function in the CRF layer is com-
puted as:

where by(cid:48)y ∈ R is trainable parameters respective
to label pair (y(cid:48), y). Score function s(X, i) ∈ R|L|

s(m)(X, i) = W(m)

s

+ b(m)
s

,

(14)

(cid:34)

(cid:62)

(cid:35)

h(s)
i
h(m)
i

(a) Model-I

(b) Model-II

(c) Model-III

Figure 3: Three shared-private models for multi-criteria learning. The yellow blocks are the shared Bi-
LSTM layer, while the gray block are the private Bi-LSTM layer. The yellow circles denote the shared
embedding layer. The red information ﬂow indicates the difference between three models.

where W(m)
criterion-speciﬁc parameters for corpus m.

∈ R2dh×|L| and b(m)

s

s

∈ R|L| are

3.2 Model-II: Stacked Shared-Private Model
In the feature layer of Model-II, we arrange the
shared layer and private layer in stacked manner.
The private layer takes output of shared layer as
input. For corpus m, the hidden states of shared
layer and private layer are:

h(s)

i = Bi-LSTM(exi ,
(cid:20) exi
h(s)
i

i = Bi-LSTM(

h(m)

−→
h (s)
i−1,
(cid:21)
−→
h (m)
i−1,

←−
h (s)
i+1, θs),
←−
h (m)

,

i+1, θm)

(15)

(16)

and the score function in the CRF layer is com-
puted as:

s(m)(X, i) = W(m)

s

(cid:62)

h(m)
i + b(m)

s

,

(17)

Figure 4: Architecture of Model-III with adversar-
ial training strategy for shared layer. The discrim-
inator ﬁrstly averages the hidden states of shared
layer, then derives probability over all possible cri-
teria by applying softmax operation after a linear
transformation.

where W(m)
criterion-speciﬁc parameters for corpus m.

∈ R2dh×|L| and b(m)

s

s

∈ R|L| are

on all the corpora. The objective function Jseg can
be computed as:

3.3 Model-III: Skip-Layer Shared-Private

Model

In the feature layer of Model-III, the shared layer
and private layer are in stacked manner as Model-
II. Additionally, we send the outputs of shared
layer to CRF layer directly.

The Model III can be regarded as a combination
of Model-I and Model-II. For corpus m, the hid-
den states of shared layer and private layer are the
same with Eq (15) and (16), and the score function
in CRF layer is computed as the same as Eq (14).

3.4 Objective function

The parameters of the network are trained to max-
imize the log conditional likelihood of true labels

M
(cid:88)

Nm(cid:88)

m=1

i=1

Jseg(Θm, Θs) =

log p(Y (m)

i

|X (m)
i

; Θm, Θs),

(18)
where Θm and Θs denote all the parameters in pri-
vate and shared layers respectively.

4

Incorporating Adversarial Training for
Shared Layer

Although the shared-private model separates the
feature space into shared and private spaces, there
is no guarantee that sharable features do not ex-
ist in private feature space, or vice versa.
In-
spired by the work on domain adaptation (Ajakan
et al.(2014)Ajakan, Germain, Larochelle, Lavi-
olette, and Marchand; Ganin et al.(2016)Ganin,

Ustinova, Ajakan, Germain, Larochelle, Lavi-
olette, Marchand, and Lempitsky; Bousmalis
et al.(2016)Bousmalis, Trigeorgis, Silberman, Kr-
ishnan, and Erhan), we hope that the features ex-
tracted by shared layer is invariant across the het-
erogonous segmentation criteria. Therefore, we
jointly optimize the shared layer via adversar-
ial training (Goodfellow et al.(2014)Goodfellow,
Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair,
Courville, and Bengio).

Therefore, besides the task loss for CWS, we
additionally introduce an adversarial loss to pre-
vent criterion-speciﬁc feature from creeping into
shared space as shown in Figure 4. We use a crite-
rion discriminator which aims to recognize which
criterion the sentence is annotated by using the
shared features.

Speciﬁcally, given a sentence X with length n,
we refer to h(s)
X as shared features for X in one
of the sharing models. Here, we compute h(s)
X by
simply averaging the hidden states of shared layer
h(s)
X = 1
xi . The criterion discriminator
n
computes the probability p(·|X) over all criteria
as:

i h(s)

(cid:80)n

p(·|X; Θd, Θs) = softmax(W(cid:62)

d h(s)

X + bd),

(19)

where Θd indicates the parameters of criterion dis-
criminator Wd ∈ Rdh×M and bd ∈ RM ; Θs de-
notes the parameters of shared layers.

4.1 Adversarial loss function

The criterion discriminator maximizes the cross
entropy of predicted criterion distribution p(·|X)
and true criterion.

J 1

adv(Θd) =

max
Θd

M
(cid:88)

Nm(cid:88)

m=1

i=1

log p(m|X (m)

; Θd, Θs).

(20)

i

An adversarial loss aims to produce shared fea-
tures, such that a criterion discriminator cannot re-
liably predict the criterion by using these shared
features. Therefore, we maximize the entropy
of predicted criterion distribution when training
shared parameters.

max
Θs

J 2

adv(Θs) =

M
(cid:88)

Nm(cid:88)

m=1

i=1

(cid:16)
p(m|X (m)

i

H

; Θd, Θs)

(cid:17)

,

where H(p) = − (cid:80)
tribution p.

(21)
i pi log pi is an entropy of dis-

Unlike (Ganin et al.(2016)Ganin, Ustinova,
Ajakan, Germain, Larochelle, Laviolette, Marc-
hand, and Lempitsky), we use entropy term in-
stead of negative cross-entropy.

Algorithm 1 Adversarial multi-criteria learning
for CWS task.

1: for i = 1; i <= n epoch; i + + do
# Train tag predictor for CWS
2:
for m = 1; m <= M ; m + + do

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

# Randomly pick data from corpus m
B = {X, Y }bm
1 ∈ Dm
Θs += α∇ΘsJ (Θ; B)
Θm += α∇ΘmJ (Θ; B)

end for
# Train criterion discriminator
for m = 1; m <= M ; m + + do

B = {X, Y }bm
1 ∈ Dm
Θd += α∇ΘdJ (Θ; B)

end for

13:
14: end for

5 Training

Finally, we combine the task and adversarial ob-
jective functions.

J (Θ; D) = Jseg(Θm, Θs) + J 1

adv(Θd) + λJ 2

adv(Θs),

(22)
where λ is the weight that controls the interaction
of the loss terms and D is the training corpora.

The training procedure is to optimize two dis-
criminative classiﬁers alternately as shown in Al-
gorithm 1. We use Adam (Kingma and Ba(2014))
with minibatchs to maximize the objectives.

Notably, when using adversarial strategy, we
ﬁrstly train 2400 epochs (each epoch only trains
on eight batches from different corpora), then we
only optimize Jseg(Θm, Θs) with Θs ﬁxed until
convergence (early stop strategy).

6 Experiments

6.1 Datasets

(Emerson(2005))

To evaluate our proposed architecture, we
experiment on eight prevalent CWS datasets
from SIGHAN2005
and
SIGHAN2008 (Jin and Chen(2008)). Table 1
gives the details of the eight datasets. Among
these datasets, AS, CITYU and CKIP are tradi-
tional Chinese, while the remains, MSRA, PKU,
CTB, NCC and SXU, are simpliﬁed Chinese. We
use 10% data of shufﬂed train set as development
set for all datasets.

6.2 Experimental Conﬁgurations

For hyper-parameter conﬁgurations, we set both
the character embedding size de and the dimen-

5
0
n
a
h
g
i
S

8
0
n
a
h
g
i
S

Datasets

MSRA

AS

PKU

CTB

CKIP

CITYU

NCC

SXU

Train
Test
Train
Test
Train
Test
Train
Test
Train
Test
Train
Test
Train
Test
Train
Test

Words
2.4M
0.1M
5.4M
0.1M
1.1M
0.2M
0.6M
0.1M
0.7M
0.1M
1.1M
0.2M
0.5M
0.1M
0.5M
0.1M

Chars
4.1M
0.2M
8.4M
0.2M
1.8M
0.3M
1.1M
0.1M
1.1M
0.1M
1.8M
0.3M
0.8M
0.2M
0.9M
0.2M

Word Types
88.1K
12.9K
141.3K
18.8K
55.2K
17.6K
42.2K
9.8K
48.1K
15.3K
43.6K
17.8K
45.2K
17.5K
32.5K
12.4K

Char Types
5.2K
2.8K
6.1K
3.7K
4.7K
3.4K
4.2K
2.6K
4.7K
3.5K
4.4K
3.4K
5.0K
3.6K
4.2K
2.8K

Sents
86.9K
4.0K
709.0K
14.4K
47.3K
6.4K
23.4K
2.1K
94.2K
10.9K
36.2K
6.7K
18.9K
3.6K
17.1K
3.7K

OOV Rate
-
2.60%
-
4.30%
-
-
-
5.55%
-
7.41%
-
8.23%
-
4.74%
-
5.12%

Table 1: Details of the eight datasets.

sionality of LSTM hidden states dh to 100. The
initial learning rate α is set to 0.01. The loss
weight coefﬁcient λ is set to 0.05. Since the
scale of each dataset varies, we use different train-
ing batch sizes for datasets. Speciﬁcally, we set
batch sizes of AS and MSR datasets as 512 and
256 respectively, and 128 for remains. We em-
ploy dropout strategy on embedding layer, keep-
ing 80% inputs (20% dropout rate).
initialization, we

all pa-
randomize
For
at
uniform distribution
rameters
(−0.05, 0.05).
We simply map traditional
Chinese characters to simpliﬁed Chinese, and
optimize on the same character embedding matrix
across datasets, which is pre-trained on Chi-
nese Wikipedia corpus, using word2vec toolkit
(Mikolov et al.(2013)Mikolov, Chen, Corrado,
and Dean).
Following previous work (Chen
et al.(2015b)Chen, Qiu, Zhu, Liu, and Huang; Pei
et al.(2014)Pei, Ge, and Baobao), all experiments
including baseline results are using pre-trained
character embedding with bigram feature.

following

6.3 Overall Results

Table 2 shows the experiment results of the pro-
posed models on test sets of eight CWS datasets,
which has three blocks.

(1) In the ﬁrst block, we can see that the per-
formance is boosted by using Bi-LSTM, and the
performance of Bi-LSTM cannot be improved
by merely increasing the depth of networks.
In
addition, although the F value of LSTM model
in (Chen et al.(2015b)Chen, Qiu, Zhu, Liu, and
Huang) is 97.4%, they additionally incorporate an
external idiom dictionary.

(2) In the second block, our proposed three
models based on multi-criteria learning boost per-
formance. Model-I gains 0.75% improvement
on averaging F-measure score compared with Bi-
LSTM result (94.14%). Only the performance
on MSRA drops slightly. Compared to the base-
line results (Bi-LSTM and stacked Bi-LSTM), the
proposed models boost the performance with the
help of exploiting information across these het-
erogeneous segmentation criteria. Although var-
ious criteria have different segmentation granular-
ities, there are still some underlying information
shared. For instance, MSRA and CTB treat fam-
ily name and last name as one token “ (NingZe-
Tao)”, whereas some other datasets, like PKU, re-
gard them as two tokens, “ (Ning)” and “ (ZeTao)”.
The partial boundaries (before “ (Ning)” or after “
(Tao)”) can be shared.

(3) In the third block, we introduce adversarial
training. By introducing adversarial training, the
performances are further boosted, and Model-I is
slightly better than Model-II and Model-III. The
adversarial training tries to make shared layer keep
criteria-invariant features. For instance, as shown
in Table 2, when we use shared information, the
performance on MSRA drops (worse than base-
line result). The reason may be that the shared
parameters bias to other segmentation criteria and
introduce noisy features into shared parameters.
When we additionally incorporate the adversar-
ial strategy, we observe that the performance on
MSRA is improved and outperforms the baseline
results. We could also observe the improvements
on other datasets. However, the boost from the ad-
versarial strategy is not signiﬁcant. The main rea-

Models

LSTM

Bi-LSTM

MSRA
95.13
95.55
95.34
63.60
95.70
95.99
95.84
66.28
95.69
95.81
95.75
65.55

P
R
F
OOV
P
R
F
OOV
P
R
F
OOV

Stacked Bi-LSTM

Multi-Criteria Learning

Model-I

Model-II

Model-III

P
R
F
OOV
P
R
F
OOV
P
R
F
OOV

95.67
95.82
95.74
69.89
95.74
95.74
95.74
69.67
95.76
95.89
95.82
70.72
Adversarial Multi-Criteria Learning
95.95
96.14
96.04
71.60
96.02
95.86
95.94
72.76
95.92
95.83
95.87
70.86

P
R
F
OOV
P
R
F
OOV
P
R
F
OOV

Model-III+ADV

Model-II+ADV

Model-I+ADV

AS
93.66
94.71
94.18
69.83
93.64
94.77
94.20
70.07
93.89
94.54
94.22
71.50

94.44
95.09
94.76
74.13
94.60
95.20
94.90
74.87
93.99
95.07
94.53
72.59

94.17
95.11
94.64
73.50
94.52
94.98
94.75
75.37
94.25
95.11
94.68
72.89

PKU
93.96
92.65
93.30
66.34
93.67
92.93
93.30
66.09
94.10
92.66
93.37
67.92

94.93
93.73
94.33
72.96
94.82
93.76
94.28
72.28
94.95
93.48
94.21
73.12

94.86
93.78
94.32
72.67
94.65
93.61
94.13
73.13
94.68
93.82
94.25
72.20

CTB
95.36
85.52
95.44
76.34
95.19
95.42
95.30
76.47
95.20
95.40
95.30
75.44

95.95
96.00
95.97
81.12
95.90
95.94
95.92
79.94
95.85
96.11
95.98
81.21

96.02
96.33
96.18
82.48
96.09
95.90
96.00
82.19
95.86
96.10
95.98
81.65

CKIP
91.85
93.34
92.59
68.67
92.44
93.69
93.06
72.12
92.40
93.39
92.89
70.50

93.99
94.52
94.26
77.58
93.51
94.56
94.03
76.67
93.50
94.58
94.04
76.56

93.82
94.70
94.26
77.59
93.80
94.69
94.24
77.71
93.67
94.48
94.07
76.13

CITYU
94.01
94.00
94.00
65.48
94.00
94.15
94.07
65.79
94.13
93.99
94.06
66.35

95.10
95.60
95.35
80.00
95.30
95.50
95.40
81.05
95.56
95.62
95.59
82.14

95.39
95.70
95.55
81.40
95.37
95.63
95.50
81.05
95.24
95.60
95.42
80.71

NCC
91.45
92.22
91.83
56.28
91.86
92.47
92.17
59.11
91.81
92.62
92.21
57.39

92.54
92.69
92.61
64.14
92.26
92.84
92.55
61.51
92.17
92.96
92.57
60.83

92.46
93.19
92.83
63.31
92.42
93.20
92.81
62.16
92.47
92.73
92.60
63.22

SXU
95.02
95.05
95.04
69.46
95.11
95.23
95.17
71.27
94.99
95.37
95.18
69.69

96.07
96.08
96.07
77.05
96.17
95.95
96.06
77.96
96.10
96.13
96.12
77.56

96.07
96.01
96.04
77.10
95.85
96.07
95.96
76.88
96.24
96.04
96.14
77.88

Avg.
93.81
92.88
93.97
67.00
93.95
94.33
94.14
68.40
94.03
94.22
94.12
68.04

94.84
94.94
94.89
74.61
94.79
94.94
94.86
74.24
94.74
94.98
94.86
74.34

94.84
95.12
94.98
74.96
94.84
94.99
94.92
75.16
94.79
94.96
94.88
74.44

Table 2: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The ﬁrst
block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of
our proposed three models without adversarial training. The third block consists of our proposed three
models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV
recall rate respectively. The maximum F values in each block are highlighted for each dataset.

son might be that the proposed three sharing mod-
els implicitly attempt to keep invariant features by
shared parameters and learn discrepancies by the
task layer.

6.4 Speed

To further explore the convergence speed, we plot
the results on development sets through epochs.
Figure 5 shows the learning curve of Model-I
without incorporating adversarial strategy. As
shown in Figure 5, the proposed model makes
progress gradually on all datasets. After about
1000 epochs, the performance becomes stable and
convergent.

We also test the decoding speed, and our mod-

els process 441.38 sentences per second averagely.
As the proposed models and the baseline mod-
els (Bi-LSTM and stacked Bi-LSTM) are nearly
in the same complexity, all models are nearly the
same efﬁcient. However, the time consumption of
training process varies from model to model. For
the models without adversarial training, it costs
about 10 hours for training (the same for stacked
Bi-LSTM to train eight datasets), whereas it takes
about 16 hours for the models with adversarial
training. All the experiments are conducted on the
hardware with Intel(R) Xeon(R) CPU E5-2643 v3
@ 3.40GHz and NVIDIA GeForce GTX TITAN
X.

)

%
(
e
u
l
a
v
-
F

96

94

92

90

MSRA

PKU

CKIP

NCC

AS

CTB

CITYU

SXU

0

500 1,000 1,500 2,000 2,500

epoches

Figure 5: Convergence speed of Model-I without
adversarial training on development sets of eight
datasets.

(a)

(b)

Figure 6: F-measure scores on test set of CITYU
dataset. Each point denotes a sentence, with the
(x, y) values of each point denoting the F-measure
scores of the two models, respectively.
(a) is
comparison between Bi-LSTM and Model-I. (b) is
comparison between Bi-LSTM and Model-I with
adversarial training.

6.5 Error Analysis

We further investigate the beneﬁts of the pro-
posed models by comparing the error distribu-
tions between the single-criterion learning (base-
line model Bi-LSTM) and multi-criteria learning
(Model-I and Model-I with adversarial training) as
shown in Figure 6. According to the results, we
could observe that a large proportion of points lie
above diagonal lines in Figure 6a and Figure 6b,
which implies that performance beneﬁt from in-
tegrating knowledge and complementary informa-
tion from other corpora. As shown in Table 2, on
the test set of CITYU, the performance of Model-I
and its adversarial version (Model-I+ADV) boost
from 92.17% to 95.59% and 95.42% respectively.
In addition, we observe that adversarial strategy
is effective to prevent criterion speciﬁc features

Figure 7: Segmentation cases of personal names.

Models
Baseline(Bi-LSTM)
Model-I∗

AS
94.20
94.12

CKIP
93.06
93.24

CITYU
94.07
95.20

Avg.
93.78
94.19

Table 3: Performance on 3 traditional Chinese
datasets. Model-I∗ means that the shared param-
eters are trained on 5 simpliﬁed Chinese datasets
and are ﬁxed for traditional Chinese datasets.
Here, we conduct Model-I without incorporating
adversarial training strategy.

from creeping into shared space. For instance, the
segmentation granularity of personal name is often
different according to heterogenous criteria. With
the help of adversarial strategy, our models could
correct a large proportion of mistakes on personal
name. Table 7 lists the examples from 2333-th and
89-th sentences in test sets of PKU and MSRA
datasets respectively.

7 Knowledge Transfer

We also conduct experiments of whether the
shared layers can be transferred to the other related
tasks or domains. In this section, we investigate
the ability of knowledge transfer on two experi-
ments: (1) simpliﬁed Chinese to traditional Chi-
nese and (2) formal texts to informal texts.

7.1 Simpliﬁed Chinese to Traditional Chinese

Traditional Chinese and simpliﬁed Chinese are
two similar languages with slightly difference on
character forms (e.g. multiple traditional charac-
ters might map to one simpliﬁed character). We
investigate that if datasets in traditional Chinese
and simpliﬁed Chinese could help each other. Ta-
ble 3 gives the results of Model-I on 3 tradi-
tional Chinese datasets under the help of 5 sim-
pliﬁed Chinese datasets. Speciﬁcally, we ﬁrstly
train the model on simpliﬁed Chinese datasets,
then we train traditional Chinese datasets indepen-
dently with shared parameters ﬁxed.

As we can see,

the average performance is
boosted by 0.41% on F-measure score (from
93.78% to 94.19%), which indicates that shared

Dataset
Train
Dev
Test

Words
421,166
43,697
187,877

Chars Word Types Char Types
4,502
43,331
2,879
11,187
3,911
27,804

688,743
73,246
315,865

Sents OOV Rate
-
6.82%
6.98%

20,135
2,052
8,592

Table 4: Statistical information of NLPCC 2016 dataset.

Models
Baseline(Bi-LSTM)
Model-I∗

P
93.56
93.65

R
94.33
94.83

F
93.94
94.24

OOV
70.75
74.72

Table 5: Performances on the test set of NLPCC
2016 dataset. Model-I∗ means that the shared pa-
rameters are trained on 8 Chinese datasets (Table
1) and are ﬁxed for NLPCC dataset. Here, we
conduct Model-I without incorporating adversar-
ial training strategy.

features learned from simpliﬁed Chinese segmen-
tation criteria can help to improve performance on
traditional Chinese. Like MSRA, as AS dataset is
relatively large (train set of 5.4M tokens), the fea-
tures learned by shared parameters might bias to
other datasets and thus hurt performance on such
large dataset AS.

7.2 Formal Texts to Informal Texts

7.2.1 Dataset

the NLPCC 2016 dataset2

(Qiu
We use
et al.(2016)Qiu, Qian, and Shi)
to evaluate
our model on micro-blog texts. The NLPCC 2016
data are provided by the shared task in the 5th
CCF Conference on Natural Language Processing
& Chinese Computing (NLPCC 2016): Chinese
Word Segmentation and POS Tagging for micro-
blog Text. Unlike the popular used newswire
dataset, the NLPCC 2016 dataset is collected from
Sina Weibo3, which consists of the informal texts
from micro-blog with the various topics, such as
ﬁnance, sports, entertainment, and so on. The
information of the dataset is shown in Table 4.

7.2.2 Results

Formal documents (like the eight datasets in Table
1) and micro-blog texts are dissimilar in many as-
pects. Thus, we further investigate that if the for-
mal texts could help to improve the performance
of micro-blog texts. Table 5 gives the results of
Model-I on the NLPCC 2016 dataset under the
help of the eight datasets in Table 1. Speciﬁcally,

2https://github.com/FudanNLP/

NLPCC-WordSeg-Weibo

3http://www.weibo.com/

we ﬁrstly train the model on the eight datasets,
then we train on the NLPCC 2016 dataset alone
with shared parameters ﬁxed. The baseline model
is Bi-LSTM which is trained on the NLPCC 2016
dataset alone.

As we can see,

the performance is boosted
by 0.30% on F-measure score (from 93.94% to
94.24%), and we could also observe that the OOV
recall rate is boosted by 3.97%.
It shows that
the shared features learned from formal texts can
help to improve the performance on of micro-blog
texts.

8 Related Works

There are many works on exploiting heteroge-
neous annotation data to improve various NLP
tasks. Jiang et al.(2009)Jiang, Huang, and Liu)
proposed a stacking-based model which could
train a model for one speciﬁc desired annota-
tion criterion by utilizing knowledge from corpora
with other heterogeneous annotations. Sun and
Wan(2012)) proposed a structure-based stacking
model to reduce the approximation error, which
makes use of structured features such as sub-
words. These models are unidirectional aid and
also suffer from error propagation problem.

Qiu et al.(2013)Qiu, Zhao, and Huang) used
multi-tasks learning framework to improve the
performance of POS tagging on two heteroge-
neous datasets. Li et al.(2015)Li, Chao, Zhang,
and Chen) proposed a coupled sequence la-
beling model which could directly learn and
infer
Chao
et al.(2015)Chao, Li, Chen, and Zhang) also uti-
lize multiple corpora using coupled sequence la-
beling model. These methods adopt the shallow
classiﬁers, therefore suffering from the problem of
deﬁning shared features.

two heterogeneous annotations.

Our proposed models use deep neural networks,
which can easily share information with hidden
shared layers. Chen et al.(2016)Chen, Zhang, and
Liu) also adopted neural network models for ex-
ploiting heterogeneous annotations based on neu-
ral multi-view model, which can be regarded as a
simpliﬁed version of our proposed models by re-

moving private hidden layers.

Unlike the above models, we design three
sharing-private architectures and keep shared layer
to extract criterion-invariance features by intro-
ducing adversarial training. Moreover, we fully
exploit eight corpora with heterogeneous segmen-
tation criteria to model the underlying shared in-
formation.

9 Conclusions & Future Works

In this paper, we propose adversarial multi-criteria
learning for CWS by fully exploiting the under-
lying shared knowledge across multiple heteroge-
neous criteria. Experiments show that our pro-
posed three shared-private models are effective to
extract the shared information, and achieve signiﬁ-
cant improvements over the single-criterion meth-
ods.

Acknowledgments

We appreciate the contribution from Jingjing
Gong and Jiacheng Xu. Besides, we would like
to thank the anonymous reviewers for their valu-
able comments. This work is partially funded
by National Natural Science Foundation of China
(No. 61532011 and 61672162), Shanghai Munici-
pal Science and Technology Commission on (No.
16JC1420401).

References

Hana Ajakan, Pascal Germain, Hugo Larochelle,
Franc¸ois Laviolette, and Mario Marchand. 2014.
Domain-adversarial neural networks. arXiv preprint
arXiv:1412.4446 .

S. Ben-David and R. Schuller. 2003. Exploiting task
Learning

relatedness for multiple task learning.
Theory and Kernel Machines pages 567–580.

Konstantinos Bousmalis, George Trigeorgis, Nathan
Silberman, Dilip Krishnan, and Dumitru Erhan.
2016. Domain separation networks. In Advances in
Neural Information Processing Systems. pages 343–
351.

Rich Caruana. 1997. Multitask learning. Machine

learning 28(1):41–75.

Jiayuan Chao, Zhenghua Li, Wenliang Chen, and Min
Zhang. 2015. Exploiting heterogeneous annota-
tions for weibo word segmentation and pos tagging.
In National CCF Conference on Natural Language
Processing and Chinese Computing. Springer, pages
495–506.

Hongshen Chen, Yue Zhang, and Qun Liu. 2016. Neu-
ral network for heterogeneous annotations. Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing .

Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing
Huang. 2015a. Gated recursive neural network for
chinese word segmentation. In Proceedings of An-
nual Meeting of the Association for Computational
Linguistics..

Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu,
and Xuanjing Huang. 2015b. Long short-term mem-
ory neural networks for chinese word segmentation.
In EMNLP. pages 1197–1206.

Ronan Collobert and Jason Weston. 2008. A uniﬁed
architecture for natural language processing: Deep
In Pro-
neural networks with multitask learning.
ceedings of ICML.

Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of
the fourth SIGHAN workshop on Chinese language
Processing. volume 133.

XIA Fei. 2000. The part-of-speech tagging guide-
lines for the penn chinese treebank (3.0). URL:
http://www. cis. upenn. edu/˜ chinese/segguide. 3rd.
ch. pdf .

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, Franc¸ois Lavi-
olette, Mario Marchand, and Victor Lempitsky.
2016. Domain-adversarial training of neural net-
Journal of Machine Learning Research
works.
17(59):1–35.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
In Advances in Neural Information
versarial nets.
Processing Systems. pages 2672–2680.

Alex Graves. 2013.

recurrent neural networks.
arXiv:1308.0850 .

Generating sequences with
arXiv preprint

W. Jiang, L. Huang, and Q. Liu. 2009. Automatic adap-
tation of annotation standards: Chinese word seg-
mentation and POS tagging: a case study. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing. pages
522–530.

G. Jin and X. Chen. 2008. The fourth international
chinese language processing bakeoff: Chinese word
segmentation, named entity recognition and chinese
pos tagging. In Sixth SIGHAN Workshop on Chinese
Language Processing. page 69.

Rafal

Jozefowicz, Wojciech Zaremba,

and Ilya
Sutskever. 2015. An empirical exploration of recur-
In Proceedings of The
rent network architectures.
32nd International Conference on Machine Learn-
ing.

S. Yu, J. Lu, X. Zhu, H. Duan, S. Kang, H. Sun,
H. Wang, Q. Zhao, and W. Zhan. 2001. Processing
norms of modern Chinese corpus. Technical report,
Technical report.

Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013.
Deep learning for chinese word segmentation and
pos tagging. In EMNLP. pages 647–657.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random ﬁelds:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning.

Zhenghua Li, Jiayuan Chao, Min Zhang, and Wenliang
Chen. 2015. Coupled sequence labeling on hetero-
geneous annotations: Pos tagging as a case study.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing.

Zhenghua Li, Jiayuan Chao, Min Zhang, and Jiwen
Yang. 2016. Fast coupled sequence labeling on het-
erogeneous annotations via context-aware pruning.
In Proceedings of EMNLP.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016a.
In
Deep multi-task learning with shared memory.
Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016b.
Recurrent neural network for text classiﬁcation with
multi-task learning. In Proceedings of International
Joint Conference on Artiﬁcial Intelligence.

Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2015. Multi-task
arXiv preprint
sequence to sequence learning.
arXiv:1511.06114 .

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
Efﬁcient estimation of word
arXiv preprint

frey Dean. 2013.
representations in vector space.
arXiv:1301.3781 .

Wenzhe Pei, Tao Ge, and Chang Baobao. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Proceedings of ACL.

Xipeng Qiu, Peng Qian, and Zhan Shi. 2016. Overview
of the NLPCC-ICCPOL 2016 shared task: Chinese
word segmentation for micro-blog texts. In Interna-
tional Conference on Computer Processing of Ori-
ental Languages. Springer, pages 901–906.

Xipeng Qiu, Jiayi Zhao, and Xuanjing Huang. 2013.
Joint chinese word segmentation and pos tagging on
heterogeneous annotated corpora with multiple task
learning. In EMNLP. pages 658–668.

Weiwei Sun and Xiaojun Wan. 2012. Reducing ap-
proximation and estimation errors for chinese lexical
processing with heterogeneous annotations. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics: Long Papers-
Volume 1. pages 232–241.

