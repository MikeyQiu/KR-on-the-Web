Learning to Fuse Music Genres with Generative Adversarial Dual Learning

Zhiqian Chen∗, Chih-Wei Wu†, Yen-Cheng Lu∗, Alexander Lerch† and Chang-Tien Lu∗
∗Computer Science Department, Virginia Tech
Email:{czq,kevinlu,ctlu}@vt.edu
†Center for Music Technology, Georgia Institute of Technology
Email:{cwu307, alexander.lerch}@gatech.edu

7
1
0
2
 
c
e
D
 
5
 
 
]

G
L
.
s
c
[
 
 
1
v
6
5
4
1
0
.
2
1
7
1
:
v
i
X
r
a

Abstract—FusionGAN is a novel genre fusion framework
for music generation that integrates the strengths of gener-
ative adversarial networks and dual learning. In particular,
learning extension that
the proposed method offers a dual
can effectively integrate the styles of the given domains. To
efﬁciently quantify the difference among diverse domains and
avoid the vanishing gradient issue, FusionGAN provides a
Wasserstein based metric to approximate the distance between
the target domain and the existing domains. Adopting the
Wasserstein distance, a new domain is created by combining
the patterns of the existing domains using adversarial learning.
Experimental results on public music datasets demonstrated
that our approach could effectively merge two genres.

I. INTRODUCTION

Computational creativity is a lively research area that
focuses on understanding and facilitating human creativity
through the implementation of software programs [2]. With
the rapid advances in data-driven algorithms such as deep
learning [7], the exploration into computational creativity
via machine learning approaches has become increasingly
popular. Examples showing the potential of deep learning for
creative approaches are the artistic style transfer on images
[4] and videos [12]. Music, with its complex hierarchical and
sequential structure and its inherent emotional and aesthetic
subjectivity, is an intriguing research subject at the core
of human creativity. While there has been some work on
generative models for music, research that investigates the
capabilities of deep learning for creative applications such as
style transfer is limited. This paper aims to ﬁll this space by
exploring the idea of style fusion in music with generative
adversarial dual learning.

In the ﬁeld of unsupervised generative learning, genera-
tive adversarial networks (GAN) [6] have recently gained
considerable attention. It is important to note, however, that
GANs are designed to learn from a single domain and cannot
discover cross-domain knowledge. Inspired by dual learning
in machine translation, several recent publications propose
methods to pair unlabeled data points in different domains
by optimizing bi-directional reconstruction errors [14], [8],
[16]. Although these methods are designed to address the
challenge of multimodal learning, they do not address the
problem of domain fusion.

The goal of this paper is to provide a solution for
fusing two or more groups of sequential patterns using
unsupervised methods. Speciﬁcally, we focus on the problem
of generating music with a fused music genre. To combine
genres using generative adversarial learning, we propose a
framework for integrating multiple GANs. Unlike previous

work, our study targets the creation of an unknown domain
by mixing two given domains; a multi-way GAN-based
model is proposed to absorb existing patterns and yield a
new mixture. With the utilization of the Wasserstein measure
[1], we pose a distance constraint on the new domain that
automatically balances its relation to the given domains. As
a result, our model is capable of generating a mixed pattern
after convergence. The main contributions of this paper are:
• A novel framework for unsupervised music fusion:
The dual learning scheme is extended to involve multi-
ple domains by leveraging mutual regularization. In this
way, the proposed framework enables the new domain
to keep equal similarities with the given domains and
produce a mixture of existing sequential patterns.

• A sequence fusion method using GANs: To apply
unsupervised fusion learning, we extend the generative
adversarial model so that multiple generators and dis-
criminators can be updated by one another.

• Formulation of an objective function indicating fu-
sion progress: An objective function based on Wasser-
stein distance is designed to integrate the information
from multiple domains and represent learning progress.

II. RELATED WORK
Music Genre Fusion: Fusion is mostly known as the
sub-genre of jazz that emerged in the late ’60s, and that
combines several musical styles such as funk, rock, and
blues with the jazz harmony and improvisation [5]. The
term can be generalized, however, to any combinations of
music genres. The creative combination of two or more
genres obviously requires not only intimate knowledge of
the stylistic characteristics of the involved genres but also
extensive compositional experience and skill to combine
genres in a meaningful and satisfying way. Recently, Engel
et al. [3] proposed a new approach of generating new
musical sounds through interpolating the latent space learned
by WaveNet [11] autoencoders. Additionally, Gatys et al.
demonstrated the Deep Neural Networks’ (DNNs) capabili-
ties of learning the artistic styles by blending the style of the
training materials to the testing images using ConvNet [4].
Both examples show the great potential of using DNNs for
merging two abstract concepts without the need for domain
knowledge and explicitly deﬁned rules.

Cross-domain GAN: GAN [6], which has quickly risen
to one of the most popular generative approaches, learns
patterns without requiring adversarial examples or labels. In
CGAN [10], the generator G has a conditional parameter
c and will learn the conditional distribution of the data.

However, those conditions need to be provided manually,
somewhat similar to supervised learning. CoGAN [9] is
proposed to learn a joint distribution with only samples
drawn from the marginal distributions. DiscoGAN [8] solves
the cross-domain pairing by minimizing bi-directional recon-
struction loss, while DualGAN [14] takes advantage of the
dual learning paradigm in machine translation. CycleGAN
[16] presents a method whose mapping functions are cycle-
consistent and proposes a cycle consistency loss function to
further reduce the space of possible mapping functions.

Different from the previous studies, our work raises a new
problem, i.e., merging two sequential patterns into one. We
propose a GAN-based framework that merges two domains
without manually tuning the distance between the given
domains and the target domain.

III. LEARNING TO BLEND MUSIC WITH FUSIONGAN

A. Problem Deﬁnition

This paper aims to characterize music fusion across
different genres in an unsupervised manner. Based on the
data XA and XB from existing music domains DA, DB
respectively, our goal is to create a new domain DF which
resembles both DA and DB. In the ﬁrst phase, the new
domain DF is expected to learn from DA, DB and keep
an equal distance from both. Speciﬁcally, the generator GF
of DF obtains feedback from the discriminators DA, DB
of existing domains DA, DB, while the discriminator DF
collects data from GF and DA, DB for iterative updating.
The GF from the new domain minimizes the distance from
the domains DA, DB, respectively, and keeps the same
distance from DA and DB.

B. FusionGAN

First, the framework initializes individual GAN models
for existing domains. Maximum likelihood estimation using
Long Short Term Memory (LSTM) is applied to initialize the
sequence generators GA(GB). Then DA(DB) are trained
given the domain data and sequence sampled from GA(GB).
Then GAN is employed to iteratively enhance the GA(GB)
and DA(DB). After pre-training, we build a GAN model for
the new domain using the feedback from the models con-
structed in the pre-training procedure. First, a new domain
DF is initialized randomly and trained by both DA and DB.
Following the dual learning strategy, DA is enhanced by DB
and DF in the second phase. Similarly, DB is improved by
DA and DF . Such learning proceeds until convergence. The
framework overview is shown in Figure 1.

[1]. To avoid the intractable inﬁmum in Wasserstein-1 dis-
tance, we resort to its Kantorovich-Rubinstein duality [13]:
Ex∼Pr [f (x)] − Ex∼Pθ [f (x)].
W (Pr, Pθ) = sup(cid:107)f (cid:107)L≤1
For the vanilla GAN,
is to ﬁnd the optimal
the goal
conﬁguration of the parameters φ of discriminator (f =
D). When the discriminator is optimized, the maximized
Wasserstein distance can be used as reward in the policy
gradient process of the generator. Similarly, we deﬁne a
three-way Wasserstein distance as the objective function:
W (PXA, PXB , Pθ) = L to measure the integrated distance
between Pθ and PXA , PXB. This measure should consider
the relationship between every pair of domains. The frame-
work consists of three pairs of generators and discriminators,
i.e., GA − DA, GB − DB, GF − DF, and two input data,
i.e., XA, XB. For each discriminator, there are ﬁve possible
inputs, i.e., XA, XB, GA, GB, GF. Collecting all possible
inputs w.r.t. the three discriminators, the objective function
to maximize is deﬁned as:
(cid:1)
L = W (cid:0)PXA , PXB , Pθ
Ex∼PXA
[DA (x)] − Ex∼PXB
Ez∼p(z) [DA (GA (z))] − Ez∼p(z) [DA (GB (z))] −
Ez∼p(z) [DA (GF (z))] −
Ex∼PXA
Ez∼p(z) [DB (GA (z))] − Ez∼p(z) [DB (GB (z))] −
Ez∼p(z) [DB (GF (z))] +
Ex∼PXA
Ez∼p(z) [DF (GA (z))] + Ez∼p(z) [DF (GB (z))] −
Ez∼p(z) [DF (GF (z))] ,

[DB (x)] + Ex∼PXB

[DF (x)] + Ex∼PXB

[DA (x)] −

[DB (x)] −

[DF (x)] +

(1)

where the ﬁrst ﬁve terms are for DA, the second ﬁve terms
are for DB, and the last ﬁve terms are for DF. This loss
is used to update all the generators and discriminators. The
following two subsections, III-C and III-D, will present the
update rules using Eq. 1.

C. DF Update of FusionGAN

Removing the terms that are unrelated to DF in Eq. 1
(zero derivative w.r.t. DF and GF), we have the objective
function for updating DF and GF:
[DF (x)] + Ex∼PXB

LF = Ex∼PXA

[DF (x)] +

Ez∼p(z) [DF (GA (z))] + Ez∼p(z) [DF (GB (z))] −
Ez∼p(z) [DF (GF (z))] −
Ez∼p(z) [DA (GF (z))] −
Ez∼p(z) [DB (GF (z))] .

(2)

First, LF is calculated w.r.t. GF so as to update GF. Consid-
ering differentiating ∇θ LF , the optimal GF is approximated
as its convergence. The proof is provided below to show that
this derivative is principled under the optimality assumption.
Theorem III.1 (FusionGAN Optimality Theorem). Let PXA
and PXB be any distribution, Let Pθ be the distribution
of GFθ(Z) with Z, a random variable with density p and
GFθ a function satisfying Ez∼p[L(θ, z)] < +∞, where L
is Lipschitz constants. All functions are well-deﬁned. Then
there is a solution DF : X → R to the problem:

W (cid:0)PXA , PXB , Pθ

(cid:1) = max

LF .

(cid:107)DF(cid:107)L≤1

(3)

(4)

Figure 1: FusionGAN framework

and we have:

In FusionGAN, we employ the Wasserstein-1 distance for
all discriminators because it is sensible when the learning
distributions are supported by low dimensional manifolds

∇θ LGF =∇θW (cid:0)PXA , PXB , Pθ
(cid:88)
= − Ez∼p(z) ∇θ

(cid:1)

Di (GF (z))

i∈{A,B,F }

W (cid:0)PXA , PXB , Pθ

Proof of Theorem III.1: Let us deﬁne LF = V ( ˜DF, θ)
˜DF lies in F = { ˜DF : X → R, ˜DF ∈
where
Cb(X ), (cid:107)DF(cid:107)L ≤ 1} and θ ∈ Rd. By the Kantorovich-
Rubinstein duality, there is an f ∈ F that satisﬁes:
V ( ˜DF, θ) = V (DF, θ).

(cid:1) = sup
˜DF∈F
Let us deﬁne X ∗(θ) = {DF ∈ F : V (DF, θ) =
W (PXA, PXB , Pθ)}, which shows that X ∗(θ) is non-empty.
According to Theorem 1 in [1]: (1) if GF is continuous in
θ, so is W (PXA, PXB , Pθ), (2) If GF is locally Lipschitz
and satisﬁes Ez∼p[L(θ, z)] < +∞, then W (PXA , PXB, Pθ)
is continuous and differentiable for any DF ∈ X ∗(θ) when
both terms are well-deﬁned. Let DF ∈ X ∗(θ), which we
knows exists since X ∗(θ)is non-empty for all θ. Then:

(cid:1)

∇θW (cid:0)PXA , PXB , Pθ
=∇θV (DF, θ) = ∇θ LF
= − ∇θ Ez∼p(z)[DF (GF (z)) + DA (GF (z)) + DB (GF (z))],

(5)
since DF is 1-Lipschitz. Furthermore, GFθ(z) is locally Lip-
schitz as a function of (θ, z). Therefore, GFθ(z) is locally
Lipschitz on (θ, z) with constants L(θ, z). By Radamach-
ers Theorem, DF(GFθ(z)) has to be differentiable almost
everywhere for (θ, z) jointly. Rewriting this, the set A =
{(θ, z) : DF ◦ GF is not differentiable} has measure 0.
By Fubinis Theorem, this implies that for almost every θ
the section Aθ = {z : (θ, z) ∈ A} has measure 0. Let’s
ﬁx a θ0 such that the measure of Aθ0 is null. For this θ0
we have ∇θ DF(GFθ(z))|θ0 is well-deﬁned for almost any
z, and since p(z) has a density, it is deﬁned p(z) almost
everywhere. Given the condition Ez∼p[L(θ, z)] < +∞, we
have

Ez∼p(z)[(cid:107)∇θ DF(GFθ(z))(cid:107)] ≤ Ez∼p(z)[L(θ0, z)] < +∞,
(6)
so Ez∼p(z)[∇θf (gθ(z))] is well-deﬁned for almost every θ0.
To keep the notation simple, we leave it implicit in the
following equation that the E subjects to z ∼ p(z). :

(cid:20)

(cid:88)

1
(cid:107)θ − θ0(cid:107)

E[Di(GFθ(z))] − E[Di(GFθ0 (z))]−

i

i

i

(cid:21)
(cid:104)(θ − θ0), E[∇θ[Di(GFθ(z))]]|θ0 (cid:105)
(cid:20)

=

1
(cid:107)θ − θ0(cid:107)

(cid:88)

E

(cid:21)
(cid:104)(θ − θ0), ∇θ[Di(GFθ(z))]|θ0 (cid:105)

1
(cid:107)θ − θ0(cid:107)
(cid:88)

≤

≤

i

2L(θ0, z) = 6L(θ0, z),

(cid:88)

(cid:13) (cid:107)θ − θ0(cid:107) L(θ0, z) + (cid:107)θ − θ0(cid:107) · (cid:107)∇θ[Di(GFθ(z))](cid:13)
(cid:13)
(cid:13)

(7)
where i ∈ {A, B, F }. By differentiability, the term inside
the integral converges p(z) to 0 as θ → θ0. Furthermore,
and since Ez∼p(z)[6L(θ0, z)] < +∞, we get by dominated
convergence that Eq. 5 converges to 0 as θ → θ0. So the
result of Eq. 5 equals to:
− Ez∼p(z)[∇θ DF (GF (z)) + ∇θ DA (GF (z)) + ∇θ DB (GF (z))].

Therefore, the update rules for the parameters θ of GF is

Eq. 4.

Figure 2: FusionGAN: update GF

The physical meaning is that all three discriminators can
offer feedback to generator GF as shown in Fig. 2. The left
cyan rectangle indicates DA, the middle pink one is DF ,
and the right yellow one DB. Similarly, the gradient w.r.t.
the parameters φ of DF is:

∇φ LF = ∇φ

(cid:2) Ex∼PXA

[DF (x)] + Ex∼PXB

[DF (x)] +

Ez∼p(z) [DF (GA (z))] + Ez∼p(z) [DF (GB (z))] −
Ez∼p(z) [DF (GF (z))] (cid:3).

(8)

Figure 3: FusionGAN: update DF

As shown in Fig. 3, the update for DF actually implicitly
balances itself between DA and DB. However, this update
does not cover the degree of blending which may lead to an
unbalanced mixture. Speciﬁcally, a good learning process
for the discriminator DF should consider two factors: (1)
minimizing the distance with DA, DB and DF simultane-
ously and (2) maintaining a equal distance from DA and
DB so as to satisfy both domains. To satisfy (2) and push
the discriminator DF to have a well-proportioned blending,
an additional constraint is employed to further balance the
ratio of existing domains, i.e., DA, DB:
(cid:13)Ez∼p(z) [DF (GA (z))] − Ez∼p(z) [DF (GB (z))](cid:13)
LF −bal = (cid:13)
(cid:13)
Ex∼PXA
(cid:13)
(cid:13)

[DF (x)] − Ex∼PXB

[DF (x)]

(cid:13)
(cid:13)
(cid:13) .

(cid:13) +

(9)

After updating DF , FusionGAN will improve DA and
DB. Since DA and DB are symmetric in the framework,
we only discuss DA as shown in Fig. 9. Keeping related
terms in Eq. 1, we get the loss function for DA:

LA = Ex∼PXA

[DA (x)] − Ex∼PXB

[DA (x)] −

Ez∼p(z) [DA (GA (z))] − Ez∼p(z) [DA (GB (z))] −
Ez∼p(z) [DA (GF (z))] −
Ez∼p(z) [DB (GA (z))] +
Ez∼p(z) [DF (GA (z))] .

(10)

Taking the derivative of LA w.r.t. DA, we have

∇DA LA = ∇DA

(cid:2) Ex∼PXA

[DA (x)] − Ex∼PXB
Ez∼p(z) [DA (GA (z))] − Ez∼p(z) [DA (GB (z))] −
Ez∼p(z) [DA (GF (z))] (cid:3).

[DA (x)] −

(11)

Di(GFθ(z)) − Di(GFθ0 (z))−

D. DA and DB Update of FusionGAN

Figure 4: FusionGAN: update DA

There are four negative terms and one positive term in Eq.
11 w.r.t. DA. The training implicitly increases the positive
terms and decreases the value of the negative terms. How-
ever, the inequalities in the two groups are not controlled.
To further control the distance among the three domains, we
propose an inequality constraint for DA:

DA (XA) ≥ DA (GF (z)) ≥ DA (XB)

(12)

Then combining Eq. 12 into loss function Eq. 11, it can

be rewritten as:

LA−bal = (cid:107) DA (XA) − DA (GF (z)) (cid:107) + (cid:107) DA (GF (z)) − DA (XB) (cid:107).

(13)

end
for DA training do

Figure 5: FusionGAN: update GA

Some inequalities that have been implicitly included in
GAN training are not covered in Eq. 12, such as DA (XA) ≥
DA (GA). Similar to the GF update in Fig. 5, the derivative
of LA w.r.t. GA is:

∇GA LA = Ez∼p(z) ∇GA

(cid:2) − DA (GA (z)) −

DB (GA (z)) + DF (GA (z)) (cid:3).

(14)

E. Algorithm Description

The Algorithm 11 initializes the parameters of the discrim-
inators and generators (line 1). TextCNN [15] and LSTM
are employed to build the discriminators and generators
respectively. Given XA, the generator is updated by maxi-
mum likelihood estimation (line 3). From lines 4 to 5, DA
receives the sequences generated by GA as negative data,
and real data as positive samples. Between lines 6 and 16,
a GAN procedure is applied to optimize DA and GA. DA
returns regression scores that are treated as rewards for the
policy gradient of GA. Similarly, DA improves itself by the
real world data XA and the data generated by GA. Our
method proceeds to update DB and GB by using the same
process from lines 3 to 16. DF repeats the same pre-training
procedure from 3 to 16 with DF, GF, XF = XA + XB.
From line 21 to 28, the framework iteratively updates the
generators and discriminators from DF , DA, DB. Lines 21
to 24 are for DF , while lines 25 to 28 are for DA and
DB. The update rules for the parameters of DF and GF
have been covered in subsection (III-C), while those for
the parameters of GA and DA are described in subsection

1https://github.com/aquastar/fusion gan

Algorithm 1: FusionGAN
Input: Input data XA, XB from DA, DB
Output: a generator and a discriminator: GF and DF

1 Randomly initialize GA, DA, GB, DB, GF, DF,

generators/discriminators employ LSTM/TextCNN;

2 // Pre-training
3 Train GA using maximum likelihood estimation on

real data XA;

4 Generate negative samples NA using GA;
5 Train binary classiﬁer DA with NA and XA;
6 repeat
7

for GA training do

Sample a sequence S 1:T = (s1, ..., sT ) ∼ GA;
Derive Q value as rewards R from DA;
Update parameters θ of GA by policy gradient:
θ ← θ + α∇θ R ;

Generate negative examples ∼ GA;
Train DA with negative samples and XA;

end

15
16 until DA converges;
17 Repeat line 3 to 16 with DB, i.e., DB, GB, XB;
18 Repeat line 3 to 5 with DF , i.e.,
DF, GF, XF = XA + XB;
19 // FusionGAN training
20 repeat
21

for DF training do

Update parameters of DF using the sum of Eq.
8 and Eq. 9 :
θDF ← θDF + α∇DF (LF + LF −bal) ;
Update parameters of GF using Eq. 4
θGF ← θGF + α∇GF LF ;

end
for DA training do

Update parameters of DA using the sum of Eq.
11 and Eq. 13 :
θDA ← θDA + α∇DA(LA + LA−bal);
Update parameters of GA using Eq. 14:
θGA ← θGA + α∇GA LA;

end
Repeat line 25 to 28 with DB, i.e., DB, GB, XB;

29
30 until DA, DB, DF converges;

8

9

10

11

12

13

14

22

23

24

25

26

27

28

(III-D). Again, DB follows the same process as that of DA.
The algorithm stops when the discriminators converge.

IV. EVALUATION
Training data includes two datasets called the Weimar
jazz dataset2 and the Essen Associative Code (EsAC) folk
dataset3. Both of these datasets contain symbolic data of
single-voiced tracks, which support our objective of gener-
ating solo melodies. The baseline methods include:

1) Random Mixing (RM): RM randomly selects notes
and duration from the pitch and duration distribution

2http://jazzomat.hfm-weimar.de
3http://www.esac-data.org

jazz

folk

GAN

MLE

RL

MC

RM

Fusion

0

5 10 15

0

5 10 15

0

5 10 15

0

5 10 15

0

5 10 15

0

5 10 15

0

5 10 15

0

5 10 15

C D E F# G# A#

C D E F# G# A#

C D E F# G# A#

C D E F# G# A#

C D E F# G# A#

C D E F# G# A#

C D E F# G# A#

C D E F# G# A#

Figure 6: upper line: DD (x-axis: duration length;y-axis: percentage); lower line: NPD (x-axis: pitch class; y-axis: percentage)

on the combination of XA and XB.

2) Monte Carlo Sampling (MC): After calculating the
note pitch and duration distributions on the combina-
tion of XA and XB, MC samples notes and duration
subject to these distributions.

3) Maximum Likelihood Estimation (MLE): Employ-
ing LSTM, MLE directly learns a pattern from the
mixture of XA and XB by predicting the next element.
4) GAN: The model directly learns a pattern from the

mixture of XA and XB using GAN.

5) Reinforcement Learning (RL): After applying GAN
on DA − GA, DB − GB, we exchange the discrimina-
tors DA and DB which are treated as rewards function
to GB and GA respectively.

A. Quantitative Study

Due to the aesthetic nature of the task, a quantitative
evaluation can only give a ﬁrst impression of the power
of the generative system. Here, we investigate the distance
between pitch and note duration distributions, knowingly
discarding all sequential information of the melodies.

The ideal output should keep equally close to the given
domains and minimize this distance. Two distributions of
properties are selected for the study.

1) Duration Distribution (DD): Notes are categorized

by their durations.

2) Normalized Pitch Distribution (NPD): All tokens are
categorized into C/C#/D/D#/E/F/F#/G/G#/A/A#/B.

To quantify the similarity between distributions,

two
symmetric metrics, the Euclidean distance (EUD) and the
Wasserstein-1 metric (Earth Mover distance (EM)), are
employed to calculate the distance between the distributions
of generated sequences and those of domains XA and XB.
the distance between XA and XB, between
Intuitively,
XA, DF , and between XB, DF should satisfy the triangle
inequality, and the optimal conﬁguration should minimize
the inequality, i.e.,

min Dif f = (cid:107) XA − GF(z)(cid:107) + (cid:107) XB − GF(z)(cid:107) − (cid:107) XA − XB (cid:107),

Ratio =

(cid:107) XA − GF(z)(cid:107) + (cid:107) XB − GF(z)(cid:107)
(cid:107) XA − XB (cid:107)

.

Figure 6 (top) shows the note duration distributions (DD)
and Table I (top) gives the corresponding distance measures.
The note duration distribution of jazz and folk have shapes
that resemble the power law distribution. The shape of RM
is very different from jazz or folk. Visually, RL’s duration
distribution does not match that of jazz and folk because
the portion of notes at the length of 0 and 1 (the peak) were

Table I: Distance between Distributions (the lower the better)

EUD
Diff

39742.2
24546.4
24765.2
37971.2
13988.7
19452.6

19586.6
15921.4
16807.1
16927.1
11175.0
11564.6

RM
MLE
GAN
RL
MC
Fusion

RM
MLE
GAN
RL
MC
Fusion

DD

NPD

Ratio

1.375
1.231
1.233
1.358
1.132
1.183

1.647
1.526
1.555
1.559
1.369
1.382

EM
Diff

2757.6
2005.2
2064.3
2629.0
1289.2
1831.9

5231.0
4147.5
4098.0
4399.2
2660.2
3182.5

Ratio

1.461
1.335
1.345
1.439
1.215
1.306

1.643
1.510
1.504
1.541
1.327
1.391

relatively higher than that of jazz or folk. The other baselines
(GAN, MLE, MC) appear similar to the given domains
because they conform to the power law distributions. Fusion
the same level of MC. Note that, given
is roughly at
our evaluation metrics, MC has the best performance in
this experiment since it directly samples notes from the
true distribution. This disregard of sequential information,
however, results in unnatural rhythmic qualities of the music
clips generated by MC.

Figure 6 (bottom) shows the pitch distributions (NPD) and
Table I (bottom) gives the corresponding distance measures.
RL’s pitch distribution is —due to overﬁtting— different
from that of jazz and folk, which shows its low capacity
in modeling pitch distribution. The single peak at D# as
generated by GAN does not match either the jazz or folk
distributions. The distributions of the other baselines are
difﬁcult to categorize as a good or bad match. Since MC is
directly sampled from real distributions, its conﬁgurations
should be exactly the same as that of the optimal fusion.
MC shows two valleys at C# and F#, and two peaks at C
and G, which should be expected. RM satisﬁes one of them
(F# is a valley), while RM meets only one standard (C is the
peak). GAN fails in all the characteristic pitch classes, and
both MLE and Fusion meet all of them. Quantitatively, the
ratio of RM in NPD is the highest which means that RM is
the worst in modeling pitch. GAN and RL improve RM by
around 10%. As one of the two distributions that look most
similar to the original distributions, MLE outperforms RM,
RL, and GAN. Our method Fusion has the second lowest
ratio after MC. However, as mentioned above, the MC does
not consider the sequential consistence, resulting in bad

evaluations in the listening test described in Sect. IV-B.

B. User Study via Listening Test

To evaluate the subjective quality of the generated se-
quences, a user study was conducted via Amazon Mechan-
ical Turk. The listening test was divided into three steps:

1) Qualiﬁcation test: “Listen to the music, and choose
one genre below that most matches the music”. This is
to test the evaluators’ qualiﬁcation level. The provided
choices are (A) jazz, (B) folk, (C) neither.

2) Fusion recognition: “Listen to the attached music,
and then choose the choice that most matches the
music” Possible answers are (A) pure jazz, (B) pure
folk, (C) mixture of jazz and folk, (D) neither. Each of
the pieces of music generated by our proposed method
and the baselines are assigned to one such question.
3) Musicality: “Listen to the attached music in 2nd step,
who do you think its composer”, the allowed answers
are (A) expert, (B) newbie, (C) robot. Similarly, each
generated sample is associated with one such question.
We randomly drew from the training data and prepared
800 sets for the ﬁrst question. In the ﬁrst step, 66.5% of
candidates chose the correct answer, which is an acceptable
rate given the inherent difﬁculties of genre identiﬁcation for
average listeners. The statistics resulting from the valid an-
swers are shown in Table II. Each of the methods generated
500 samples, which were randomly selected for the second
and third questions.

As shown in Table II,

the best system based on the
percentage of mixture is MLE. A closer look at MLE,
however, reveals the unbalanced distribution between jazz
and folk, which implies its bias towards the jazz genre.
On the other hand, RM is the best system based on the
balance between jazz and folk, but the high percentage of
neither suggests the confusion of the listeners. It is clear
that a single criterion is insufﬁcient for determining the best
system. Therefore, we design a metric to summarize the
results and represent the fusion level (FL) of the evaluated
systems with the following equation:

maximize FL = 1 −

(cid:107)Cjazz − Cf olk(cid:107) + Cneither
Cjazz + Cf olk + Cmixture + Cneither

,

where Ci indicates the count of i. (cid:107)Cjazz − Cf olk(cid:107) means
the unbalanced error, and Cneither can be treated as another
type of error. A higher FL value implies better fusion.
In the M usicality test, MLE (45.5%), GAN (42.0%),
and Fusion (43.8%) were voted by the majority as expert.
Overall, Fusion shows a balanced performance in both
F usionrecognition and M usicality tests, which demon-
strates the effectiveness of our proposed method.

V. CONCLUSION
In this paper, we proposed a three-way GAN-based learn-
ing framework to integrate multiple domains. A Wasser-
stein distance based metric is introduced to indicate the
blending progress. The evaluation investigated objective
metrics looking at the pitch and note duration distributions
of the generated data. A user study explicitly illustrates
the validity of the proposed method as compared to the
baselines, as the melodies generated by our model were
preferred by the majority of users. While the listening
test results are encouraging, future work will beneﬁt from

Table II: Listening test results

Fusion recognition

RM
MLE
GAN
RL
MC
Fusion

RM
MLE
GAN
RL
MC
Fusion

jazz
25.0%
43.6%
34.0%
20.1%
32.0%
35.9%

expert
22.5%
45.5%
42.0%
32.1%
30.0%
43.8%

folk
22.5%
9.1%
17.0%
28.3%
2.0%
25.0%

mixture
12.5%
30.9%
26.0%
20.8%
14.0%
20.0%
Musicality
newbie
50.0%
21.8%
32.0%
37.7%
36.0%
28.1%

neither
40%
16.4%
14%
30.8%
52%
19.1%

FL
57.5%
49.1%
69%
61%
16%
70%

robot
27.5%
32.7%
26.0%
30.2%
34.0%
28.1%

removing key dependence of the training data, a careful look
into perceptually meaningful evaluation metrics that take
into account the sequential nature of music, and a careful
listening test design, including human-composed melodies
for comparison.

REFERENCES

[1] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,”

arXiv:1701.07875, 2017.

[2] S. Colton, R. L. de M´antaras, and O. Stock, “Computational
creativity: Coming of age,” AI Magazine, vol. 30, no. 3, p. 11,
2009.

[3] J. Engel, C. Resnick, A. Roberts, S. Dieleman, D. Eck, K. Si-
monyan, and M. Norouzi, “Neural audio synthesis of musical
notes with wavenet autoencoders,” arXiv:1704.01279, 2017.
[4] L. A. Gatys, A. S. Ecker, and M. Bethge, “A Neural Algo-
rithm of Artistic Style,” arXiv preprint, pp. 3–7, 2015.
[5] J. Gerald David, Encyclopedia of African American Society.

London: SAGE Publications, Inc., 2005.

[6] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-
Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative
adversarial nets,” in NIPS, 2014, pp. 2672–2680.

[7] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning
algorithm for deep belief nets,” Neural computation, vol. 18,
no. 7, pp. 1527–1554, 2006.

[8] T. Kim, M. Cha, H. Kim, J. Lee, and J. Kim, “Learning to
discover cross-domain relations with generative adversarial
networks,” arXiv:1703.05192, 2017.

[9] M.-Y. Liu and O. Tuzel, “Coupled generative adversarial

networks,” in NIPS, 2016, pp. 469–477.

[10] M. Mirza and S. Osindero, “Conditional Generative Adver-

sarial Nets,” arXiv preprint arXiv:1411.1784, 2014.

[11] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan,
O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and
K. Kavukcuoglu, “Wavenet: A generative model for raw
audio,” arXiv:1609.03499, 2016.

[12] M. Ruder, A. Dosovitskiy, and T. Brox, “Artistic style transfer
for videos,” in German Conference on Pattern Recognition.
Springer, 2016, pp. 26–36.

[13] C. Villani, Optimal transport: old and new. Springer Science

& Business Media, 2008, vol. 338.

[14] Z. Yi, H. Zhang, P. Tan, and M. Gong, “Dualgan: Unsu-
pervised dual learning for image-to-image translation,” arXiv
preprint arXiv:1704.02510, 2017.

[15] X. Zhang and Y. LeCun, “Text understanding from scratch,”

arXiv preprint arXiv:1502.01710, 2015.

[16] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired
image-to-image translation using cycle-consistent adversarial
networks,” arXiv:1703.10593, 2017.

Learning to Fuse Music Genres with Generative Adversarial Dual Learning

Zhiqian Chen∗, Chih-Wei Wu†, Yen-Cheng Lu∗, Alexander Lerch† and Chang-Tien Lu∗
∗Computer Science Department, Virginia Tech
Email:{czq,kevinlu,ctlu}@vt.edu
†Center for Music Technology, Georgia Institute of Technology
Email:{cwu307, alexander.lerch}@gatech.edu

7
1
0
2
 
c
e
D
 
5
 
 
]

G
L
.
s
c
[
 
 
1
v
6
5
4
1
0
.
2
1
7
1
:
v
i
X
r
a

Abstract—FusionGAN is a novel genre fusion framework
for music generation that integrates the strengths of gener-
ative adversarial networks and dual learning. In particular,
learning extension that
the proposed method offers a dual
can effectively integrate the styles of the given domains. To
efﬁciently quantify the difference among diverse domains and
avoid the vanishing gradient issue, FusionGAN provides a
Wasserstein based metric to approximate the distance between
the target domain and the existing domains. Adopting the
Wasserstein distance, a new domain is created by combining
the patterns of the existing domains using adversarial learning.
Experimental results on public music datasets demonstrated
that our approach could effectively merge two genres.

I. INTRODUCTION

Computational creativity is a lively research area that
focuses on understanding and facilitating human creativity
through the implementation of software programs [2]. With
the rapid advances in data-driven algorithms such as deep
learning [7], the exploration into computational creativity
via machine learning approaches has become increasingly
popular. Examples showing the potential of deep learning for
creative approaches are the artistic style transfer on images
[4] and videos [12]. Music, with its complex hierarchical and
sequential structure and its inherent emotional and aesthetic
subjectivity, is an intriguing research subject at the core
of human creativity. While there has been some work on
generative models for music, research that investigates the
capabilities of deep learning for creative applications such as
style transfer is limited. This paper aims to ﬁll this space by
exploring the idea of style fusion in music with generative
adversarial dual learning.

In the ﬁeld of unsupervised generative learning, genera-
tive adversarial networks (GAN) [6] have recently gained
considerable attention. It is important to note, however, that
GANs are designed to learn from a single domain and cannot
discover cross-domain knowledge. Inspired by dual learning
in machine translation, several recent publications propose
methods to pair unlabeled data points in different domains
by optimizing bi-directional reconstruction errors [14], [8],
[16]. Although these methods are designed to address the
challenge of multimodal learning, they do not address the
problem of domain fusion.

The goal of this paper is to provide a solution for
fusing two or more groups of sequential patterns using
unsupervised methods. Speciﬁcally, we focus on the problem
of generating music with a fused music genre. To combine
genres using generative adversarial learning, we propose a
framework for integrating multiple GANs. Unlike previous

work, our study targets the creation of an unknown domain
by mixing two given domains; a multi-way GAN-based
model is proposed to absorb existing patterns and yield a
new mixture. With the utilization of the Wasserstein measure
[1], we pose a distance constraint on the new domain that
automatically balances its relation to the given domains. As
a result, our model is capable of generating a mixed pattern
after convergence. The main contributions of this paper are:
• A novel framework for unsupervised music fusion:
The dual learning scheme is extended to involve multi-
ple domains by leveraging mutual regularization. In this
way, the proposed framework enables the new domain
to keep equal similarities with the given domains and
produce a mixture of existing sequential patterns.

• A sequence fusion method using GANs: To apply
unsupervised fusion learning, we extend the generative
adversarial model so that multiple generators and dis-
criminators can be updated by one another.

• Formulation of an objective function indicating fu-
sion progress: An objective function based on Wasser-
stein distance is designed to integrate the information
from multiple domains and represent learning progress.

II. RELATED WORK
Music Genre Fusion: Fusion is mostly known as the
sub-genre of jazz that emerged in the late ’60s, and that
combines several musical styles such as funk, rock, and
blues with the jazz harmony and improvisation [5]. The
term can be generalized, however, to any combinations of
music genres. The creative combination of two or more
genres obviously requires not only intimate knowledge of
the stylistic characteristics of the involved genres but also
extensive compositional experience and skill to combine
genres in a meaningful and satisfying way. Recently, Engel
et al. [3] proposed a new approach of generating new
musical sounds through interpolating the latent space learned
by WaveNet [11] autoencoders. Additionally, Gatys et al.
demonstrated the Deep Neural Networks’ (DNNs) capabili-
ties of learning the artistic styles by blending the style of the
training materials to the testing images using ConvNet [4].
Both examples show the great potential of using DNNs for
merging two abstract concepts without the need for domain
knowledge and explicitly deﬁned rules.

Cross-domain GAN: GAN [6], which has quickly risen
to one of the most popular generative approaches, learns
patterns without requiring adversarial examples or labels. In
CGAN [10], the generator G has a conditional parameter
c and will learn the conditional distribution of the data.

However, those conditions need to be provided manually,
somewhat similar to supervised learning. CoGAN [9] is
proposed to learn a joint distribution with only samples
drawn from the marginal distributions. DiscoGAN [8] solves
the cross-domain pairing by minimizing bi-directional recon-
struction loss, while DualGAN [14] takes advantage of the
dual learning paradigm in machine translation. CycleGAN
[16] presents a method whose mapping functions are cycle-
consistent and proposes a cycle consistency loss function to
further reduce the space of possible mapping functions.

Different from the previous studies, our work raises a new
problem, i.e., merging two sequential patterns into one. We
propose a GAN-based framework that merges two domains
without manually tuning the distance between the given
domains and the target domain.

III. LEARNING TO BLEND MUSIC WITH FUSIONGAN

A. Problem Deﬁnition

This paper aims to characterize music fusion across
different genres in an unsupervised manner. Based on the
data XA and XB from existing music domains DA, DB
respectively, our goal is to create a new domain DF which
resembles both DA and DB. In the ﬁrst phase, the new
domain DF is expected to learn from DA, DB and keep
an equal distance from both. Speciﬁcally, the generator GF
of DF obtains feedback from the discriminators DA, DB
of existing domains DA, DB, while the discriminator DF
collects data from GF and DA, DB for iterative updating.
The GF from the new domain minimizes the distance from
the domains DA, DB, respectively, and keeps the same
distance from DA and DB.

B. FusionGAN

First, the framework initializes individual GAN models
for existing domains. Maximum likelihood estimation using
Long Short Term Memory (LSTM) is applied to initialize the
sequence generators GA(GB). Then DA(DB) are trained
given the domain data and sequence sampled from GA(GB).
Then GAN is employed to iteratively enhance the GA(GB)
and DA(DB). After pre-training, we build a GAN model for
the new domain using the feedback from the models con-
structed in the pre-training procedure. First, a new domain
DF is initialized randomly and trained by both DA and DB.
Following the dual learning strategy, DA is enhanced by DB
and DF in the second phase. Similarly, DB is improved by
DA and DF . Such learning proceeds until convergence. The
framework overview is shown in Figure 1.

[1]. To avoid the intractable inﬁmum in Wasserstein-1 dis-
tance, we resort to its Kantorovich-Rubinstein duality [13]:
Ex∼Pr [f (x)] − Ex∼Pθ [f (x)].
W (Pr, Pθ) = sup(cid:107)f (cid:107)L≤1
For the vanilla GAN,
is to ﬁnd the optimal
the goal
conﬁguration of the parameters φ of discriminator (f =
D). When the discriminator is optimized, the maximized
Wasserstein distance can be used as reward in the policy
gradient process of the generator. Similarly, we deﬁne a
three-way Wasserstein distance as the objective function:
W (PXA, PXB , Pθ) = L to measure the integrated distance
between Pθ and PXA , PXB. This measure should consider
the relationship between every pair of domains. The frame-
work consists of three pairs of generators and discriminators,
i.e., GA − DA, GB − DB, GF − DF, and two input data,
i.e., XA, XB. For each discriminator, there are ﬁve possible
inputs, i.e., XA, XB, GA, GB, GF. Collecting all possible
inputs w.r.t. the three discriminators, the objective function
to maximize is deﬁned as:
(cid:1)
L = W (cid:0)PXA , PXB , Pθ
Ex∼PXA
[DA (x)] − Ex∼PXB
Ez∼p(z) [DA (GA (z))] − Ez∼p(z) [DA (GB (z))] −
Ez∼p(z) [DA (GF (z))] −
Ex∼PXA
Ez∼p(z) [DB (GA (z))] − Ez∼p(z) [DB (GB (z))] −
Ez∼p(z) [DB (GF (z))] +
Ex∼PXA
Ez∼p(z) [DF (GA (z))] + Ez∼p(z) [DF (GB (z))] −
Ez∼p(z) [DF (GF (z))] ,

[DB (x)] + Ex∼PXB

[DF (x)] + Ex∼PXB

[DA (x)] −

[DB (x)] −

[DF (x)] +

(1)

where the ﬁrst ﬁve terms are for DA, the second ﬁve terms
are for DB, and the last ﬁve terms are for DF. This loss
is used to update all the generators and discriminators. The
following two subsections, III-C and III-D, will present the
update rules using Eq. 1.

C. DF Update of FusionGAN

Removing the terms that are unrelated to DF in Eq. 1
(zero derivative w.r.t. DF and GF), we have the objective
function for updating DF and GF:
[DF (x)] + Ex∼PXB

LF = Ex∼PXA

[DF (x)] +

Ez∼p(z) [DF (GA (z))] + Ez∼p(z) [DF (GB (z))] −
Ez∼p(z) [DF (GF (z))] −
Ez∼p(z) [DA (GF (z))] −
Ez∼p(z) [DB (GF (z))] .

(2)

First, LF is calculated w.r.t. GF so as to update GF. Consid-
ering differentiating ∇θ LF , the optimal GF is approximated
as its convergence. The proof is provided below to show that
this derivative is principled under the optimality assumption.
Theorem III.1 (FusionGAN Optimality Theorem). Let PXA
and PXB be any distribution, Let Pθ be the distribution
of GFθ(Z) with Z, a random variable with density p and
GFθ a function satisfying Ez∼p[L(θ, z)] < +∞, where L
is Lipschitz constants. All functions are well-deﬁned. Then
there is a solution DF : X → R to the problem:

W (cid:0)PXA , PXB , Pθ

(cid:1) = max

LF .

(cid:107)DF(cid:107)L≤1

(3)

(4)

Figure 1: FusionGAN framework

and we have:

In FusionGAN, we employ the Wasserstein-1 distance for
all discriminators because it is sensible when the learning
distributions are supported by low dimensional manifolds

∇θ LGF =∇θW (cid:0)PXA , PXB , Pθ
(cid:88)
= − Ez∼p(z) ∇θ

(cid:1)

Di (GF (z))

i∈{A,B,F }

W (cid:0)PXA , PXB , Pθ

Proof of Theorem III.1: Let us deﬁne LF = V ( ˜DF, θ)
˜DF lies in F = { ˜DF : X → R, ˜DF ∈
where
Cb(X ), (cid:107)DF(cid:107)L ≤ 1} and θ ∈ Rd. By the Kantorovich-
Rubinstein duality, there is an f ∈ F that satisﬁes:
V ( ˜DF, θ) = V (DF, θ).

(cid:1) = sup
˜DF∈F
Let us deﬁne X ∗(θ) = {DF ∈ F : V (DF, θ) =
W (PXA, PXB , Pθ)}, which shows that X ∗(θ) is non-empty.
According to Theorem 1 in [1]: (1) if GF is continuous in
θ, so is W (PXA, PXB , Pθ), (2) If GF is locally Lipschitz
and satisﬁes Ez∼p[L(θ, z)] < +∞, then W (PXA , PXB, Pθ)
is continuous and differentiable for any DF ∈ X ∗(θ) when
both terms are well-deﬁned. Let DF ∈ X ∗(θ), which we
knows exists since X ∗(θ)is non-empty for all θ. Then:

(cid:1)

∇θW (cid:0)PXA , PXB , Pθ
=∇θV (DF, θ) = ∇θ LF
= − ∇θ Ez∼p(z)[DF (GF (z)) + DA (GF (z)) + DB (GF (z))],

(5)
since DF is 1-Lipschitz. Furthermore, GFθ(z) is locally Lip-
schitz as a function of (θ, z). Therefore, GFθ(z) is locally
Lipschitz on (θ, z) with constants L(θ, z). By Radamach-
ers Theorem, DF(GFθ(z)) has to be differentiable almost
everywhere for (θ, z) jointly. Rewriting this, the set A =
{(θ, z) : DF ◦ GF is not differentiable} has measure 0.
By Fubinis Theorem, this implies that for almost every θ
the section Aθ = {z : (θ, z) ∈ A} has measure 0. Let’s
ﬁx a θ0 such that the measure of Aθ0 is null. For this θ0
we have ∇θ DF(GFθ(z))|θ0 is well-deﬁned for almost any
z, and since p(z) has a density, it is deﬁned p(z) almost
everywhere. Given the condition Ez∼p[L(θ, z)] < +∞, we
have

Ez∼p(z)[(cid:107)∇θ DF(GFθ(z))(cid:107)] ≤ Ez∼p(z)[L(θ0, z)] < +∞,
(6)
so Ez∼p(z)[∇θf (gθ(z))] is well-deﬁned for almost every θ0.
To keep the notation simple, we leave it implicit in the
following equation that the E subjects to z ∼ p(z). :

(cid:20)

(cid:88)

1
(cid:107)θ − θ0(cid:107)

E[Di(GFθ(z))] − E[Di(GFθ0 (z))]−

i

i

i

(cid:21)
(cid:104)(θ − θ0), E[∇θ[Di(GFθ(z))]]|θ0 (cid:105)
(cid:20)

=

1
(cid:107)θ − θ0(cid:107)

(cid:88)

E

(cid:21)
(cid:104)(θ − θ0), ∇θ[Di(GFθ(z))]|θ0 (cid:105)

1
(cid:107)θ − θ0(cid:107)
(cid:88)

≤

≤

i

2L(θ0, z) = 6L(θ0, z),

(cid:88)

(cid:13) (cid:107)θ − θ0(cid:107) L(θ0, z) + (cid:107)θ − θ0(cid:107) · (cid:107)∇θ[Di(GFθ(z))](cid:13)
(cid:13)
(cid:13)

(7)
where i ∈ {A, B, F }. By differentiability, the term inside
the integral converges p(z) to 0 as θ → θ0. Furthermore,
and since Ez∼p(z)[6L(θ0, z)] < +∞, we get by dominated
convergence that Eq. 5 converges to 0 as θ → θ0. So the
result of Eq. 5 equals to:
− Ez∼p(z)[∇θ DF (GF (z)) + ∇θ DA (GF (z)) + ∇θ DB (GF (z))].

Therefore, the update rules for the parameters θ of GF is

Eq. 4.

Figure 2: FusionGAN: update GF

The physical meaning is that all three discriminators can
offer feedback to generator GF as shown in Fig. 2. The left
cyan rectangle indicates DA, the middle pink one is DF ,
and the right yellow one DB. Similarly, the gradient w.r.t.
the parameters φ of DF is:

∇φ LF = ∇φ

(cid:2) Ex∼PXA

[DF (x)] + Ex∼PXB

[DF (x)] +

Ez∼p(z) [DF (GA (z))] + Ez∼p(z) [DF (GB (z))] −
Ez∼p(z) [DF (GF (z))] (cid:3).

(8)

Figure 3: FusionGAN: update DF

As shown in Fig. 3, the update for DF actually implicitly
balances itself between DA and DB. However, this update
does not cover the degree of blending which may lead to an
unbalanced mixture. Speciﬁcally, a good learning process
for the discriminator DF should consider two factors: (1)
minimizing the distance with DA, DB and DF simultane-
ously and (2) maintaining a equal distance from DA and
DB so as to satisfy both domains. To satisfy (2) and push
the discriminator DF to have a well-proportioned blending,
an additional constraint is employed to further balance the
ratio of existing domains, i.e., DA, DB:
(cid:13)Ez∼p(z) [DF (GA (z))] − Ez∼p(z) [DF (GB (z))](cid:13)
LF −bal = (cid:13)
(cid:13)
Ex∼PXA
(cid:13)
(cid:13)

[DF (x)] − Ex∼PXB

[DF (x)]

(cid:13)
(cid:13)
(cid:13) .

(cid:13) +

(9)

After updating DF , FusionGAN will improve DA and
DB. Since DA and DB are symmetric in the framework,
we only discuss DA as shown in Fig. 9. Keeping related
terms in Eq. 1, we get the loss function for DA:

LA = Ex∼PXA

[DA (x)] − Ex∼PXB

[DA (x)] −

Ez∼p(z) [DA (GA (z))] − Ez∼p(z) [DA (GB (z))] −
Ez∼p(z) [DA (GF (z))] −
Ez∼p(z) [DB (GA (z))] +
Ez∼p(z) [DF (GA (z))] .

(10)

Taking the derivative of LA w.r.t. DA, we have

∇DA LA = ∇DA

(cid:2) Ex∼PXA

[DA (x)] − Ex∼PXB
Ez∼p(z) [DA (GA (z))] − Ez∼p(z) [DA (GB (z))] −
Ez∼p(z) [DA (GF (z))] (cid:3).

[DA (x)] −

(11)

Di(GFθ(z)) − Di(GFθ0 (z))−

D. DA and DB Update of FusionGAN

Figure 4: FusionGAN: update DA

There are four negative terms and one positive term in Eq.
11 w.r.t. DA. The training implicitly increases the positive
terms and decreases the value of the negative terms. How-
ever, the inequalities in the two groups are not controlled.
To further control the distance among the three domains, we
propose an inequality constraint for DA:

DA (XA) ≥ DA (GF (z)) ≥ DA (XB)

(12)

Then combining Eq. 12 into loss function Eq. 11, it can

be rewritten as:

LA−bal = (cid:107) DA (XA) − DA (GF (z)) (cid:107) + (cid:107) DA (GF (z)) − DA (XB) (cid:107).

(13)

end
for DA training do

Figure 5: FusionGAN: update GA

Some inequalities that have been implicitly included in
GAN training are not covered in Eq. 12, such as DA (XA) ≥
DA (GA). Similar to the GF update in Fig. 5, the derivative
of LA w.r.t. GA is:

∇GA LA = Ez∼p(z) ∇GA

(cid:2) − DA (GA (z)) −

DB (GA (z)) + DF (GA (z)) (cid:3).

(14)

E. Algorithm Description

The Algorithm 11 initializes the parameters of the discrim-
inators and generators (line 1). TextCNN [15] and LSTM
are employed to build the discriminators and generators
respectively. Given XA, the generator is updated by maxi-
mum likelihood estimation (line 3). From lines 4 to 5, DA
receives the sequences generated by GA as negative data,
and real data as positive samples. Between lines 6 and 16,
a GAN procedure is applied to optimize DA and GA. DA
returns regression scores that are treated as rewards for the
policy gradient of GA. Similarly, DA improves itself by the
real world data XA and the data generated by GA. Our
method proceeds to update DB and GB by using the same
process from lines 3 to 16. DF repeats the same pre-training
procedure from 3 to 16 with DF, GF, XF = XA + XB.
From line 21 to 28, the framework iteratively updates the
generators and discriminators from DF , DA, DB. Lines 21
to 24 are for DF , while lines 25 to 28 are for DA and
DB. The update rules for the parameters of DF and GF
have been covered in subsection (III-C), while those for
the parameters of GA and DA are described in subsection

1https://github.com/aquastar/fusion gan

Algorithm 1: FusionGAN
Input: Input data XA, XB from DA, DB
Output: a generator and a discriminator: GF and DF

1 Randomly initialize GA, DA, GB, DB, GF, DF,

generators/discriminators employ LSTM/TextCNN;

2 // Pre-training
3 Train GA using maximum likelihood estimation on

real data XA;

4 Generate negative samples NA using GA;
5 Train binary classiﬁer DA with NA and XA;
6 repeat
7

for GA training do

Sample a sequence S 1:T = (s1, ..., sT ) ∼ GA;
Derive Q value as rewards R from DA;
Update parameters θ of GA by policy gradient:
θ ← θ + α∇θ R ;

Generate negative examples ∼ GA;
Train DA with negative samples and XA;

end

15
16 until DA converges;
17 Repeat line 3 to 16 with DB, i.e., DB, GB, XB;
18 Repeat line 3 to 5 with DF , i.e.,
DF, GF, XF = XA + XB;
19 // FusionGAN training
20 repeat
21

for DF training do

Update parameters of DF using the sum of Eq.
8 and Eq. 9 :
θDF ← θDF + α∇DF (LF + LF −bal) ;
Update parameters of GF using Eq. 4
θGF ← θGF + α∇GF LF ;

end
for DA training do

Update parameters of DA using the sum of Eq.
11 and Eq. 13 :
θDA ← θDA + α∇DA(LA + LA−bal);
Update parameters of GA using Eq. 14:
θGA ← θGA + α∇GA LA;

end
Repeat line 25 to 28 with DB, i.e., DB, GB, XB;

29
30 until DA, DB, DF converges;

8

9

10

11

12

13

14

22

23

24

25

26

27

28

(III-D). Again, DB follows the same process as that of DA.
The algorithm stops when the discriminators converge.

IV. EVALUATION
Training data includes two datasets called the Weimar
jazz dataset2 and the Essen Associative Code (EsAC) folk
dataset3. Both of these datasets contain symbolic data of
single-voiced tracks, which support our objective of gener-
ating solo melodies. The baseline methods include:

1) Random Mixing (RM): RM randomly selects notes
and duration from the pitch and duration distribution

2http://jazzomat.hfm-weimar.de
3http://www.esac-data.org

jazz

folk

GAN

MLE

RL

MC

RM

Fusion

0

5 10 15

0

5 10 15

0

5 10 15

0

5 10 15

0

5 10 15

0

5 10 15

0

5 10 15

0

5 10 15

C D E F# G# A#

C D E F# G# A#

C D E F# G# A#

C D E F# G# A#

C D E F# G# A#

C D E F# G# A#

C D E F# G# A#

C D E F# G# A#

Figure 6: upper line: DD (x-axis: duration length;y-axis: percentage); lower line: NPD (x-axis: pitch class; y-axis: percentage)

on the combination of XA and XB.

2) Monte Carlo Sampling (MC): After calculating the
note pitch and duration distributions on the combina-
tion of XA and XB, MC samples notes and duration
subject to these distributions.

3) Maximum Likelihood Estimation (MLE): Employ-
ing LSTM, MLE directly learns a pattern from the
mixture of XA and XB by predicting the next element.
4) GAN: The model directly learns a pattern from the

mixture of XA and XB using GAN.

5) Reinforcement Learning (RL): After applying GAN
on DA − GA, DB − GB, we exchange the discrimina-
tors DA and DB which are treated as rewards function
to GB and GA respectively.

A. Quantitative Study

Due to the aesthetic nature of the task, a quantitative
evaluation can only give a ﬁrst impression of the power
of the generative system. Here, we investigate the distance
between pitch and note duration distributions, knowingly
discarding all sequential information of the melodies.

The ideal output should keep equally close to the given
domains and minimize this distance. Two distributions of
properties are selected for the study.

1) Duration Distribution (DD): Notes are categorized

by their durations.

2) Normalized Pitch Distribution (NPD): All tokens are
categorized into C/C#/D/D#/E/F/F#/G/G#/A/A#/B.

To quantify the similarity between distributions,

two
symmetric metrics, the Euclidean distance (EUD) and the
Wasserstein-1 metric (Earth Mover distance (EM)), are
employed to calculate the distance between the distributions
of generated sequences and those of domains XA and XB.
the distance between XA and XB, between
Intuitively,
XA, DF , and between XB, DF should satisfy the triangle
inequality, and the optimal conﬁguration should minimize
the inequality, i.e.,

min Dif f = (cid:107) XA − GF(z)(cid:107) + (cid:107) XB − GF(z)(cid:107) − (cid:107) XA − XB (cid:107),

Ratio =

(cid:107) XA − GF(z)(cid:107) + (cid:107) XB − GF(z)(cid:107)
(cid:107) XA − XB (cid:107)

.

Figure 6 (top) shows the note duration distributions (DD)
and Table I (top) gives the corresponding distance measures.
The note duration distribution of jazz and folk have shapes
that resemble the power law distribution. The shape of RM
is very different from jazz or folk. Visually, RL’s duration
distribution does not match that of jazz and folk because
the portion of notes at the length of 0 and 1 (the peak) were

Table I: Distance between Distributions (the lower the better)

EUD
Diff

39742.2
24546.4
24765.2
37971.2
13988.7
19452.6

19586.6
15921.4
16807.1
16927.1
11175.0
11564.6

RM
MLE
GAN
RL
MC
Fusion

RM
MLE
GAN
RL
MC
Fusion

DD

NPD

Ratio

1.375
1.231
1.233
1.358
1.132
1.183

1.647
1.526
1.555
1.559
1.369
1.382

EM
Diff

2757.6
2005.2
2064.3
2629.0
1289.2
1831.9

5231.0
4147.5
4098.0
4399.2
2660.2
3182.5

Ratio

1.461
1.335
1.345
1.439
1.215
1.306

1.643
1.510
1.504
1.541
1.327
1.391

relatively higher than that of jazz or folk. The other baselines
(GAN, MLE, MC) appear similar to the given domains
because they conform to the power law distributions. Fusion
the same level of MC. Note that, given
is roughly at
our evaluation metrics, MC has the best performance in
this experiment since it directly samples notes from the
true distribution. This disregard of sequential information,
however, results in unnatural rhythmic qualities of the music
clips generated by MC.

Figure 6 (bottom) shows the pitch distributions (NPD) and
Table I (bottom) gives the corresponding distance measures.
RL’s pitch distribution is —due to overﬁtting— different
from that of jazz and folk, which shows its low capacity
in modeling pitch distribution. The single peak at D# as
generated by GAN does not match either the jazz or folk
distributions. The distributions of the other baselines are
difﬁcult to categorize as a good or bad match. Since MC is
directly sampled from real distributions, its conﬁgurations
should be exactly the same as that of the optimal fusion.
MC shows two valleys at C# and F#, and two peaks at C
and G, which should be expected. RM satisﬁes one of them
(F# is a valley), while RM meets only one standard (C is the
peak). GAN fails in all the characteristic pitch classes, and
both MLE and Fusion meet all of them. Quantitatively, the
ratio of RM in NPD is the highest which means that RM is
the worst in modeling pitch. GAN and RL improve RM by
around 10%. As one of the two distributions that look most
similar to the original distributions, MLE outperforms RM,
RL, and GAN. Our method Fusion has the second lowest
ratio after MC. However, as mentioned above, the MC does
not consider the sequential consistence, resulting in bad

evaluations in the listening test described in Sect. IV-B.

B. User Study via Listening Test

To evaluate the subjective quality of the generated se-
quences, a user study was conducted via Amazon Mechan-
ical Turk. The listening test was divided into three steps:

1) Qualiﬁcation test: “Listen to the music, and choose
one genre below that most matches the music”. This is
to test the evaluators’ qualiﬁcation level. The provided
choices are (A) jazz, (B) folk, (C) neither.

2) Fusion recognition: “Listen to the attached music,
and then choose the choice that most matches the
music” Possible answers are (A) pure jazz, (B) pure
folk, (C) mixture of jazz and folk, (D) neither. Each of
the pieces of music generated by our proposed method
and the baselines are assigned to one such question.
3) Musicality: “Listen to the attached music in 2nd step,
who do you think its composer”, the allowed answers
are (A) expert, (B) newbie, (C) robot. Similarly, each
generated sample is associated with one such question.
We randomly drew from the training data and prepared
800 sets for the ﬁrst question. In the ﬁrst step, 66.5% of
candidates chose the correct answer, which is an acceptable
rate given the inherent difﬁculties of genre identiﬁcation for
average listeners. The statistics resulting from the valid an-
swers are shown in Table II. Each of the methods generated
500 samples, which were randomly selected for the second
and third questions.

As shown in Table II,

the best system based on the
percentage of mixture is MLE. A closer look at MLE,
however, reveals the unbalanced distribution between jazz
and folk, which implies its bias towards the jazz genre.
On the other hand, RM is the best system based on the
balance between jazz and folk, but the high percentage of
neither suggests the confusion of the listeners. It is clear
that a single criterion is insufﬁcient for determining the best
system. Therefore, we design a metric to summarize the
results and represent the fusion level (FL) of the evaluated
systems with the following equation:

maximize FL = 1 −

(cid:107)Cjazz − Cf olk(cid:107) + Cneither
Cjazz + Cf olk + Cmixture + Cneither

,

where Ci indicates the count of i. (cid:107)Cjazz − Cf olk(cid:107) means
the unbalanced error, and Cneither can be treated as another
type of error. A higher FL value implies better fusion.
In the M usicality test, MLE (45.5%), GAN (42.0%),
and Fusion (43.8%) were voted by the majority as expert.
Overall, Fusion shows a balanced performance in both
F usionrecognition and M usicality tests, which demon-
strates the effectiveness of our proposed method.

V. CONCLUSION
In this paper, we proposed a three-way GAN-based learn-
ing framework to integrate multiple domains. A Wasser-
stein distance based metric is introduced to indicate the
blending progress. The evaluation investigated objective
metrics looking at the pitch and note duration distributions
of the generated data. A user study explicitly illustrates
the validity of the proposed method as compared to the
baselines, as the melodies generated by our model were
preferred by the majority of users. While the listening
test results are encouraging, future work will beneﬁt from

Table II: Listening test results

Fusion recognition

RM
MLE
GAN
RL
MC
Fusion

RM
MLE
GAN
RL
MC
Fusion

jazz
25.0%
43.6%
34.0%
20.1%
32.0%
35.9%

expert
22.5%
45.5%
42.0%
32.1%
30.0%
43.8%

folk
22.5%
9.1%
17.0%
28.3%
2.0%
25.0%

mixture
12.5%
30.9%
26.0%
20.8%
14.0%
20.0%
Musicality
newbie
50.0%
21.8%
32.0%
37.7%
36.0%
28.1%

neither
40%
16.4%
14%
30.8%
52%
19.1%

FL
57.5%
49.1%
69%
61%
16%
70%

robot
27.5%
32.7%
26.0%
30.2%
34.0%
28.1%

removing key dependence of the training data, a careful look
into perceptually meaningful evaluation metrics that take
into account the sequential nature of music, and a careful
listening test design, including human-composed melodies
for comparison.

REFERENCES

[1] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,”

arXiv:1701.07875, 2017.

[2] S. Colton, R. L. de M´antaras, and O. Stock, “Computational
creativity: Coming of age,” AI Magazine, vol. 30, no. 3, p. 11,
2009.

[3] J. Engel, C. Resnick, A. Roberts, S. Dieleman, D. Eck, K. Si-
monyan, and M. Norouzi, “Neural audio synthesis of musical
notes with wavenet autoencoders,” arXiv:1704.01279, 2017.
[4] L. A. Gatys, A. S. Ecker, and M. Bethge, “A Neural Algo-
rithm of Artistic Style,” arXiv preprint, pp. 3–7, 2015.
[5] J. Gerald David, Encyclopedia of African American Society.

London: SAGE Publications, Inc., 2005.

[6] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-
Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative
adversarial nets,” in NIPS, 2014, pp. 2672–2680.

[7] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning
algorithm for deep belief nets,” Neural computation, vol. 18,
no. 7, pp. 1527–1554, 2006.

[8] T. Kim, M. Cha, H. Kim, J. Lee, and J. Kim, “Learning to
discover cross-domain relations with generative adversarial
networks,” arXiv:1703.05192, 2017.

[9] M.-Y. Liu and O. Tuzel, “Coupled generative adversarial

networks,” in NIPS, 2016, pp. 469–477.

[10] M. Mirza and S. Osindero, “Conditional Generative Adver-

sarial Nets,” arXiv preprint arXiv:1411.1784, 2014.

[11] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan,
O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and
K. Kavukcuoglu, “Wavenet: A generative model for raw
audio,” arXiv:1609.03499, 2016.

[12] M. Ruder, A. Dosovitskiy, and T. Brox, “Artistic style transfer
for videos,” in German Conference on Pattern Recognition.
Springer, 2016, pp. 26–36.

[13] C. Villani, Optimal transport: old and new. Springer Science

& Business Media, 2008, vol. 338.

[14] Z. Yi, H. Zhang, P. Tan, and M. Gong, “Dualgan: Unsu-
pervised dual learning for image-to-image translation,” arXiv
preprint arXiv:1704.02510, 2017.

[15] X. Zhang and Y. LeCun, “Text understanding from scratch,”

arXiv preprint arXiv:1502.01710, 2015.

[16] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired
image-to-image translation using cycle-consistent adversarial
networks,” arXiv:1703.10593, 2017.

