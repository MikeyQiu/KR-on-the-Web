Dynamic Image Networks for Action Recognition

Hakan Bilen†∗ Basura Fernando‡∗ Efstratios Gavves§ Andrea Vedaldi† Stephen Gould‡
†University of Oxford ‡The Australian National University §QUVA Lab, University of Amsterdam

Abstract

We introduce the concept of dynamic image, a novel
compact representation of videos useful for video analy-
sis especially when convolutional neural networks (CNNs)
are used. The dynamic image is based on the rank pool-
ing concept and is obtained through the parameters of a
ranking machine that encodes the temporal evolution of the
frames of the video. Dynamic images are obtained by di-
rectly applying rank pooling on the raw image pixels of a
video producing a single RGB image per video. This idea
is simple but powerful as it enables the use of existing CNN
models directly on video data with ﬁne-tuning. We present
an efﬁcient and effective approximate rank pooling opera-
tor, speeding it up orders of magnitude compared to rank
pooling. Our new approximate rank pooling CNN layer al-
lows us to generalize dynamic images to dynamic feature
maps and we demonstrate the power of our new representa-
tions on standard benchmarks in action recognition achiev-
ing state-of-the-art performance.

1. Introduction

Videos comprise a large majority of the visual data in
existence, surpassing by a wide margin still images. There-
fore understanding the content of videos accurately and
on a large scale is of paramount importance. The advent
of modern learnable representations such as deep convolu-
tional neural networks (CNNs) has improved dramatically
the performance in many image understanding tasks. Since
videos are composed of a sequence of still images, some of
these improvements have been shown to transfer to videos
directly. However, it remains unclear how videos could be
optimally represented. For example, one can look at a video
as a sequence of still images, perhaps enjoying some form
of temporal smoothness, or as a subspace of images or im-
age features, or as the output of a neural network encoder.
Which one among these and other possibilities results in the
best representation of videos is not well understood.

∗Equal contribution
1From left to right and top to bottom: “blowing hair dry”, “band march-
ing”, “balancing on beam”, “golf swing”, “fencing”, “playing the cello”,

Figure 1: Dynamic images summarizing the actions and
motions that happen in images in standard 2d image for-
mat. Can you guess what actions are visualized just from
their dynamic image motion signature? 1

In this paper, we explore a new powerful and yet simple
representation of videos in the context of deep learning. As
a representative goal we consider the standard problem of
recognizing human actions in short video sequences. Re-
cent works such as [5, 6, 7, 9, 23] pointed out that long
term dynamics and temporal patterns are a very important
cues for the recognition of actions. However, representing
complex long term dynamics is difﬁcult, particularly if one
seeks compact representations that can be processed efﬁ-
ciently. Several efﬁcient representations of long term dy-
namics have been obtained by temporal pooling of image
features in a video. Temporal pooling has been done using
temporal templates [1] or using ranking functions for video
frames [6] or subvideos [9] or by more traditional pooling
operators [23].

In this paper we propose a new long-term pooling opera-
tor which is simple, efﬁcient, compact, and very powerful in
a neural network context. Since a CNN provides a whole hi-
erarchy of image representations, one for each intermediate
layer, the ﬁrst question is where temporal pooling should
take place. For example, one could use a method such as

“horse racing”, “doing push-ups”, “drumming”.

13034

rank pooling [6] to pool the output of the fully-connected
layers of a standard CNN architecture pre-trained on still
images and applied to individual frames. A downside of
this solution is that the CNN itself is unaware of the lower
level dynamics of the video. Alternatively, one can model
the dynamics of the response of some intermediate network
layer. In this case, the lower layers are still computed from
individual frames, but the upper layers can reason about the
overall dynamics in the video. An extreme version of this
idea is to capture the video dynamics directly at the level of
the image pixels, considering those as the ﬁrst layer of the
architecture.

Here we build on the latter intuition and build dynamics
directly at the level of the input image. To this end, our ﬁrst
contribution (section 2) is to introduce the notion of a dy-
namic image, i.e. a RGB still image that summarizes, in a
compressed format, the gist of a whole video (sub)sequence
(ﬁg. 1). The dynamic image is obtained as a ranking classi-
ﬁer that, similarly to [6, 7], sorts video frames temporally;
the difference is that we compute this classiﬁer directly at
the level of the image pixels instead of using an intermedi-
ate feature representation.

There are three keys advantages to this idea. First, the
new RGB image can be processed by a CNN architecture
which is structurally nearly identical to architectures used
for still images, while still capturing the long-term dynam-
ics in a video that relate to the long term dynamics therein.
It is then possible to use a standard CNN architecture to
learn suitable “dynamic” features from the videos. The sec-
ond advantage of this method is its remarkable efﬁciency:
the extraction of the dynamic image is extremely simple and
efﬁcient and it allows to reduce video classiﬁcation to clas-
siﬁcation of a single image by a standard CNN architecture.
The third advantage is the compression factor, as a whole
video is summarized by an amount of data equivalent to a
single frame.

Our second contribution is to provide a fast approxima-
tion of the ranker in the construction of the dynamic image.
We replace learning the ranker by simple linear operations
over the images, which is extremely efﬁcient. We also show
that, in this manner, it is possible to apply the concept of dy-
namic image to the intermediate layers of a CNN represen-
tation by constructing an efﬁcient rank pooling layer. This
layer can be incorporated into end-to-end training of a CNN
for video data.

Our third contribution is to use these ideas to propose a
novel static/dynamic neural network architecture (section 3)
which can perform end-to-end training from videos com-
bining both static appearance information from still frames,
as well as short and long term dynamics from the whole
video. We show that these technique result in efﬁcient and
accurate classiﬁcation of actions in videos, outperforming
the state-of-the-art in standard benchmarks in the area (sec-

tion 4). Our ﬁndings are summarized in section 5.

1.1. Related Work

Existing video representations can be roughly broken
into two categories. The ﬁrst category, which comprises the
majority of the literature on video processing, action and
event classiﬁcation, be it with shallow [31, 6, 20] or deep
architectures [21, 24], has viewed videos either as a stream
of still images [21] or as a short and smooth transition be-
tween similar frames [24]. Although obviously suboptimal,
considering the video as bag of static frames performs rea-
sonably well [21], as the surroundings of an action strongly
correlate with the action itself (e.g., “playing basketball”
takes place usually in a basketball court). The second cat-
egory extends CNNs to a third, temporal dimension [12]
replacing 2D ﬁlters with 3D ones. So far, this approach has
produced little beneﬁts, probably due to the lack of anno-
tated video data. Increasing the amount of annotated videos
would probably help as shown by recent 3D convolution
methods [29], although what seems especially important is
spatial consistency between frames. More speciﬁcally, a
pixel to pixel registration [24] based on the video’s opti-
cal ﬂow brings considerable improvements. Similarly, [8]
uses action tubes to to ﬁt a double stream appearance and
motion based neural network that captures the movement
of the actor.

We can also distinguish two architectural choices in the
construction of video CNNs. The ﬁrst choice is to provide
as input to the CNN a sub-video of ﬁxed length, packing a
short sequence of video frames into an array of images. The
advantage of this technique is that they allow using simple
modiﬁcations of standard CNN architectures (e.g. [15]) by
swapping 3D ﬁlters for the 2D ones. Examples of such tech-
niques include [12] and [24].

Although the aforementioned methods successfully cap-
ture the local changes within a small time window, they
cannot capture longer-term motion patterns associated with
certain actions. An alternative solution is to consider a sec-
ond family of architectures based on recurrent neural net-
works (RNNs) [4, 28]. RNNs typically consider memory
cells [11], which are sensitive to both short as well as longer
term patterns. RNNs parse the video frames sequentially
and encode the frame-level information in their memory.
In [4] LSTMs are used together with convolutional neu-
ral network activations to either output an action label or
a video description. In [28] an autoencoder-like LSTM ar-
chitecture is proposed such that either the current frame or
the next frame is accurately reconstructed. Finally, the au-
thors of [33] propose an LSTM with a temporal attention
model for densely labelling video frames.

Many of the ideas in video CNNs originated in earlier
architectures that used hand-crafted features. For exam-
ple, the authors of [18, 30, 31] have shown that local mo-

3035

tion patterns in short frame sequences can capture very well
the short temporal structures in actions. The rank pooling
idea, on which our dynamic images are based, was proposed
in [6, 7] using hand-crafted representation of the frames,
while in [5] authors increase the capacity of rank pooling
using a hierarchical approach.

Our static/dynamic CNN uses a multi-stream architec-
ture. Multiple streams have been used in a variety of dif-
ferent contexts. Examples include Siamese architectures
for learning metrics for face identiﬁcation [2] of for unsu-
pervised training of CNNs [3]. Simonyan et al. [24] use
two streams to encode respectively static frames and optical
ﬂow frames in action recognition. The authors of [19] pro-
pose a dual loss neural network was proposed, where coarse
and ﬁne outputs are jointly optimized. A difference of our
model compared to these is that we branch off two streams
at arbitrary location in the network, either at the input, at
the level of the convolutional layers, or at the level of the
fully-connected layers.

2. Dynamic Images

In this section we introduce the concept of dynamic im-
age, which is a standard RGB image that summarizes the
appearance and dynamics of a whole video sequence (sec-
tion 2.1). Then, we show how dynamic images can be
used to train dynamic-aware CNNs for action recognition
in videos (section 2.2). Finally, we propose a fast approx-
imation to accelerate the computation of dynamic images
(section 2.3).

2.1. Constructing dynamic images

While CNNs can learn automatically powerful data rep-
resentations, they can only operate within the conﬁnes of a
speciﬁc hand-crafted architecture. In designing a CNN for
video data, in particular, it is necessary to think of how the
video information should be presented to the CNN. As dis-
cussed in section 1.1, standard solutions include encoding
sub-videos of a ﬁxed duration as multi-dimensional arrays
or using recurrent architectures. Here we propose an al-
ternative and more efﬁcient approach in which the video
content is summarized by a single still image which can
then be processed by a standard CNN architecture such as
AlexNet [15].

Summarizing the video content in a single still image
may seem difﬁcult. In particular, it is not clear how image
pixels, which already contain appearance information in the
video frames, could be overloaded to reﬂect dynamic infor-
mation as well, and in particular the long-term dynamics
that are important in action recognition.

We show here that the construction of Fernando et al. [6]
can be used to obtain exactly such an image. The idea of
their paper is to represent a video as a ranking function for
Rd be
its frames I1, . . . , IT . In more detail, let ψ(It)

∈

a representation or feature vector extracted from each indi-
t
vidual frame It in the video. Let Vt = 1
τ =1 ψ(Iτ ) be
t P
time average of these features up to time t. The ranking
function associates to each time t a score S(t
,
i
Rd is a vector of parameters. The function pa-
where d
rameters d are learned so that the scores reﬂect the rank of
the frames in the video. Therefore, later times are associ-
d).
ated with larger scores, i.e. q > t =
Learning d is posed as a convex optimization problem using
the RankSVM [26] formulation:

d) > S(t

d) =

d, Vt

S(q

⇒

∈

h

|

|

|

d∗

= ρ(I1, . . . , IT ; ψ) = argmin

E(d),

d

E(d) =

d

2+
k

λ
2 k
2

T (T

1) × X
q>t

−

(1)

max

0, 1
{

−

S(q

d) + S(t

d)

.
}

|

|

The ﬁrst
term in this objective function is the usual
quadratic regularizer used in SVMs. The second term is
a hinge-loss soft-counting how many pairs q > t are incor-
rectly ranked by the scoring function. Note in particular that
a pair is considered correctly ranked only if scores are sep-
d) + 1.
arated by at least a unit margin, i.e. S(q
Optimizing eq. (1) deﬁnes a function ρ(I1, . . . , IT ; ψ)
that maps a sequence of T video frames to a single vector
d∗. Since this vector contains enough information to rank
all the frames in the video, it aggregates information from
all of them and can be used as a video descriptor. In the rest
of the paper we refer to the process of constructing d∗ from
a sequence of video frames as rank pooling.

d) > S(t

|

|

·

In [6] the map ψ(

) used in this construction is set to
be the Fisher Vector coding of a number of local features
(HOG, HOF, MBH, TRJ) extracted from individual video
frames. Here, we propose to apply rank pooling directly to
the RGB image pixels instead. While this idea is simple, in
the next several sections we will show that it has remarkable
advantages.

The ψ(It) is now an operator that stacks the RGB com-
ponents of each pixel in image It on a large vector. Alter-
natively, ψ(It) may incorporate a simple component-wise
non-linearity, such as the square root function √
(which
corresponds to using the Hellinger’s kernel in the SVM).
In all cases, the descriptor d∗ is a real vector that has the
same number of elements as a single video frame. There-
fore, d∗ can be interpreted as standard RGB image. Further-
more, since this image is obtained by rank pooling the video
frames, it summarizes information from the whole video se-
quence.

·

A few examples of dynamic images are shown in ﬁg. 1.
Several observations can be made. First, it is interesting
to note that the dynamic images tend to focus mainly on
the acting objects, such as humans or other animals such
as horses in the “horse racing” action, or objects such as

3036

training a CNN on top of such dynamic images, we implic-
itly capture the temporal patterns contained in the video.
However, since the CNN is still applied to images, we can
start from a CNN pre-trained for still image recognition,
such as AlexNet pre-trained on the ImageNet ILSVRC data,
and ﬁne-tune it on a dataset of dynamic images. Fine-tuning
allows the CNN to learn features that capture the video
dynamics without the need to train the architecture from
scratch. This is an important beneﬁt of our method because
training large CNNs require millions of data samples which
may be difﬁcult to obtain for videos.

Multiple Dynamic Images (MDI). While ﬁne-tuning
does not require as much annotated data as training a CNN
from scratch,
the domain gap between natural and dy-
namic images is sufﬁciently large that an adequately large
ﬁne-tuning dataset of dynamic images may be appropriate.
However, as noted above, in most cases there are only a few
videos available for training.

In order to address this potential limitation, in the second
scenario we propose to generate multiple dynamic images
from each video by breaking it into segments. In particu-
lar, for each video we extract multiple partially-overlapping
segments of duration τ and with stride s. In this manner, we
create multiple video segments per video, essentially mul-
tiplying the dataset size by a factor of approximately T /s,
where T is the average number of frames per video. This
can also be seen as a data augmentation step, where instead
of mirroring, cropping, or shearing images we simply take
a subset of the video frames. From each of the new video
segments, we can then compute a dynamic image to train
the CNN, using as ground truth class information of each
subsequence the class of the original video.

2.3. Fast dynamic image computation

Computing a dynamic image entails solving the opti-
mization problem of eq. (1). While this is not particularly
slow with modern solvers, in this section we propose an ap-
proximation to rank pooling which is much faster and works
as well in practice. Later, this technique, which we call
approximate rank pooling, will be critical in incorporating
rank pooling in intermediate layers of a deep CNN and to
allow back-prop training through it.

The derivation of approximate rank pooling is based on
the idea of considering the ﬁrst step in a gradient-based op-
timization of eq. (1). Starting with d = ~0, the ﬁrst ap-
proximated solution obtained by gradient descent is d∗ =
~0

E(d)

E(d)

η

−

∇
E(~0)

∇

|d=~0 ∝ −∇
max

∝ X

q>t ∇

0, 1

{

|d=~0 for any η > 0, where
d) + S(t
}|d=~0

S(q

d)

−

|

|

Figure 2: Left column: dynamic images. Right col-
umn: motion blur. Although fundamentally different both
methodologically, as well as in terms of applications, they
both seem to capture time in a similar manner.

drums in the “drumming” action. On the contrary, back-
ground pixels and background motion patterns tend to be
averaged away. Hence, the pixels in the dynamic image ap-
pear to focus on the identity and motion of the salient actors
in videos, indicating that they may contain the information
necessary to perform action recognition.

Second, we observe that dynamic images behave dif-
ferently for actions of different speeds. For slow actions,
like “blowing hair dry” in the ﬁrst row of ﬁg. 1, the motion
seems to be dragged over many frames. For faster actions,
such as “golf swing” in the second row of ﬁg. 1, the dynamic
image reﬂects key steps in the action such as preparing to
swing and stopping after swinging. For longer term actions
such as “horse riding” in the third row of ﬁg. 1, the dynamic
image reﬂects different parts of the video; for instance, the
rails that appear as a secondary motion contributor are su-
perimposed on top of the horses and the jockeys who are
the main actors. Such observations were also made in [7].

Last, it is interesting to note that dynamic images are
reminiscent of some other imaging effects that convey mo-
tion and time, such as motion blur or panning, an analogy is
illustrated in ﬁg. 2. While motion blur captures the time and
motion by integrating over subsequent pixel intensities de-
ﬁned by the camera shutter speed, dynamic images capture
the time by integrating and reordering the pixel intensities
over time within a video.

2.2. Using dynamic images

Given that the dynamic images are in the format of stan-
dard RGB images, they can be used to apply any method
for still image analysis, and in particular any state-of-the-art
CNN, to the analysis of video. In particular, we experiment
with two usage scenarios.

Single Dynamic Image (SDI).
In the ﬁrst scenario, a dy-
namic image summarizes an entire video sequence. By

=

X
q>t ∇h

d, Vt

Vq

=

−

i

Vt

Vq.

−

X
q>t

3037

,

 using time-averaged vectors V

t

t

t

,

 using A

 directly

t

t

,

15

10

5

0

-5

-10

-15

2

4

8

10

6
time

Figure 3: The graph compares the approximated rank pool-
ing weighting functions αt (for T = 11 samples) of eq. (2)
using time-averaged feature frames Vt to the variant eq. (4)
that ranks directly the feature frames ψt as is.

We can further expand d∗ as follows

d∗

Vq

Vt =

−

∝ X
q>t

1
q





X
q>t

q

ψi

−

X
i=1

1
t

t

X
j=1





T

X
t=1

ψj

=

αtψt

where the coefﬁcients αt are given by

αt = 2(T

t + 1)

(T + 1)(HT

Ht−1),

−
t
i=1 1/t is the t-th Harmonic number and

where Ht =
H0 = 0. Hence the rank pooling operator reduces to

P

−

−

(2)

ˆρ(I1, . . . , IT ; ψ) =

αtψ(It).

(3)

T

X
t=1

In
which is a weighted combination of the data points.
particular, the dynamic image computation reduces to ac-
cumulating the video frames after pre-multiplying them by
αt. The function αt, however, is non-trivial, as illustrated
in ﬁg. 3.

An alternative construction of the rank pooler does
not compute the intermediate average features Vt =
T
q=1 ψ(Iq), but uses directly individual video fea-
(1/t)
tures ψ(It) in the deﬁnition of the ranking scores (1). In
this case, the derivation above results in a weighting func-
tion of the type

P

αt = 2t

T

1

(4)

−
which is linear in t. The two scoring functions eq. (2) and
eq. (4) are compared in ﬁg. 3 and in the experiments.

−

3. Dynamic Maps Networks

In the previous section we have introduced the concept
of dynamic image as a method to pool the information con-
tained in a number of video frames in a single RGB image.

Figure 4: Dynamic image and dynamic map networks on
the left and the right pictures respectively, after applying a
rank pooling operation on top of the previous layer activa-
tions.

Here, we notice that every layer of a CNN produces as out-
put a feature map which, having a spatial structure similar
to an image, can be used in place of video frames in this
construction. We call the result of applying rank pooling
to such features a dynamic feature map, or dynamic map in
short. In the rest of the section we explain how to incor-
porate this construction as a rank-pooling layer in a CNN
(section 3.1) and how to accelerate it signiﬁcantly and per-
form back-propagation by using approximate rank pooling
(section 3.2).

3.1. Dynamic maps

The structure of a dynamic map network is illustrated
in ﬁg. 4. In the case seen so far (left in ﬁg. 4), rank pool-
ing is applied at the level of the input RGB video frames,
which we could think of as layer zero in the architecture.
We call the latter a dynamic image network. By contrast,
a dynamic map network moves rank pooling higher in the
hierarchy, by applying one or more layers of feature com-
putations to the individual feature frames and applying the
same construction to the resulting feature maps.

, . . . , a(l−1)

In particular, let a(l−1)

denote the feature
1
maps computed at the l
1 layers of the architecture, one for
each of the T video frames. Then, we use the rank pooling
equation (1) to aggregate these maps into a single dynamic
map,

−

T

a(l) = ρ(a(l−1)

, . . . , a(l−1)

).

T

1

(5)

Note that, compared to eq. (1), we dropped the term ψ;
since networks are already learning feature maps, we set
this term to the identity function. The dynamic image
network is obtained by setting l = 1 in this construction.

Rank pooling layer (RankPool) & backpropagation. In
order to train a CNN with rank pooling as an intermediate
layer, it is necessary to compute the derivatives of eq. (5) for
the backpropagation step. We can rewrite eq. (5) as a linear

3038

combination of the input data V1, . . . , VT , namely

a(l) =

βt(V1, . . . , VT )Vt

(6)

T

X
t=1

In turn, Vt is the temporal average of the input features and
is therefore a linear function Vt(a(l−1)
). Sub-
stituting, we can rewrite a(l) as

, . . . , a(l−1)

1

t

a(l) =

αt(a(l−1)

1

, . . . , a(l−1)

)a(l−1)

.

t

T

(7)

T

X
t=1

Unfortunately, we observe that due to the non-linear nature
of the optimization problem of equation (1), the coefﬁcients
βt, αt depend on the data a(l−1)
themselves. Computing
t
the gradient of a(l) with respect to the per frame data points
a(l−1)
is a challenging derivation. Hence, using dynamic
t
maps and rank pooling directly as a layer in a CNN is not
straightforward.

We note that the rank pooling layer (RankPool) consti-
tutes a new type of portable convolutional network layer,
just like a max-pooling or a ReLU layer.
It can be used
whenever dynamic information must be pooled across time.

3.2. Approximate dynamic maps.

Constructing the precise dynamic maps, or images, is in
theory optimal, but not necessarily practical. On one hand
computing the precise dynamic maps via an optimization
is computationally inefﬁcient. This is especially important
in the context of CNNs, where efﬁcient computations are
extremely important for training on large datasets, and the
optimization of eq. (5) would be slow compared to other
components of the network. On the other hand, computing
the gradients would be non trivial.

To this end we replace once again rank pooling with ap-
proximate rank pooling. With the approximate rank pool-
ing we signiﬁcantly accelerate the computations, even by
a factor of 45 as we show later in the experiments. Sec-
ondly, and more importantly, the approximate rank pooling
is also a linear combination of frames, where the per frame
coefﬁcients are given by eq. (2). These coefﬁcients are in-
dependent of the frame features Vt and ψ(It). Hence, the
derivative of the approximate rank pooling is much simpler
and can be easily computed as the vectorized coefﬁcients of
eq. (2), namely

We conclude that using approximate rank pooling in the
context of CNNs is not only practical, but also necessary for
the optimization through backpropagation.

4. Experiments

4.1. Datasets

We explore the proposed models on two state-of-the-
art datasets used for evaluating neural network based
models for action recognition, namely UCF101 [27] and
HMDB51 [16].

UCF101. The UCF101 dataset [27] comprises of 101
human action categories, like “Apply Eye Makeup” and
“Rock Climbing” and spans over 13, 320 videos. The
videos are realistic and relatively clean. They contain
little background clutter and contain a single action.
thus almost all frames
Also the videos are trimmed,
relate to the action in the video. The standard evaluation
is average accuracy over three parts provided by the authors.

HMDB51. The HMDB51 dataset [16] comprises of 51 hu-
man action categories , such as “backhand ﬂip” and “swing
baseball bat” and spans over 6, 766 videos. The videos are
realistic, downloaded from Youtube contain a single action.
The dataset is split in three parts and accuracy is averaged
over all three parts, similar to UCF101.

4.2. Implementation details

·

·

To maintain the same function domain and range we se-
) the square rooting kernel
lect for non-linear operations ψ(
and time varying mean vectors [6]. We generate
maps √
dynamic images for each color channel separately and then
merge them so that they can be directly used directly as in-
put to a CNN. As the initial dynamic images are not in the
natural range of [0, 255] for RGB data, we apply minmax
normalization. We use BVLC reference CaffeNet model
[13] trained on ImageNet images as a starting point to train
our dynamic image networks. We ﬁne-tune all the layers
with the learning rate to be 10−3 and gradually decrease it
per epoch. We use a maximum of 20 epoch during training.
Sharing code, data, models. We share our code, mod-
els and data 2. Furthermore, we have computed the dy-
namic images of the Sports1M dataset [14], and share the
Alexnet and VGGnet dynamic image networks trained on
the Sports1M.

∂ vec a(l)
∂(vec a(l−1)

t

)⊤

= αtI

where I is the identity matrix. Interestingly, we would ob-
tain the same expression for the derivative if αt in eq. (7)
would be constant and did not depend on the video frames.

4.3. Mean, max and dynamic images

(8)

First, we compare “single image per video”, namely the
proposed Single Dynamic Image (SDI) with the per video
sequence mean and max image. For all methods we ﬁrst

2https://github.com/hbilen/dynamic-image-nets

3039

Method
Mean Image
Max Image
SDI

SPLIT1
52.6
48.0
57.2

SPLIT2
53.4
46.0
58.7

51.7
42.3
57.7

52.6
45.4
57.9

SPLIT3 AVERAGE

Table 1: Comparing several video representative image
models using UCF101

Method
Appr. Rank Pooling
Rank Pooling

Speed
5920 fps
131 fps

Accuracy
96.5 ± 0.9
99.5 ± 0.1

Table 2: Approximate rank pooling vs rank pooling.

compute the single images per video ofﬂine, and for SDI
speciﬁcally we use SVR [26]. Then we train and test on
action recognition using CaffeNet network. Results are re-
ported in Table 1.

From all representations we observe that SDI achieves
the highest accuracy. We conclude that SDI model is a bet-
ter single image model than the mean and max image mod-
els.

4.4. Approximate Rank Pooling vs Rank Pooling

Next, we compare the approximate rank pooling and
rank pooling in terms of speed (frames per second) and pair-
wise ranking accuracy, which is the common measure for
evaluating learning-to-rank methods. We train on a subset
of 10 videos that contain different actions and evaluate on a
new set of 10 videos with the same type of actions respec-
tively. We report results with the mean and the standard
deviations in Table 2.

×

We observe that approximate rank pooling is 45

faster
than rank pooling, while obtaining similar ranking perfor-
mance. Further, in Figure 5 we plot the score distributions
for rank pooling and approximate rank pooling. We observe
that their score proﬁles are also similar. We conclude that
approximate rank pooling is a good approximation to rank
pooling, while being two magnitudes faster as it involves no
optimization.

4.5. Evaluating the effect of end-to-end training

Next, we evaluate in Table 3 rank pooling dynamic im-
ages with and without end-to-end training. We also eval-
uate rank pooling with dynamic maps. The ﬁrst method
generates multiple dynamic images on the RGB pixels as
earlier. These dynamic images can be computed ofﬂine,
then we train a network from end to end. The second
method passes these dynamic images through the network,
computes the fc6 activations using a pre-trained Alexnet
and aggregates them with max pooling, then trains SVM
classiﬁers per action class. The third method considers a
RankPool layer after the conv1 to generate multiple dy-

Appr. rank pooling
Rank pooling

100

200

300

400

500

600

700

800

900

50

100

150

200

250

300

350

400

450

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0

0

50

100

150

200

250

300

350

400

450

50

100

150

200

250

300

350

400

450

Figure 5: Comparison between score proﬁle of ranking
functions for approximate rank pooling and rank pooling.
Generally the approximate rank pooling follows the trend
of rank pooling.

Method RankPool Layer

Training

HMDB51 UCF101

MDI
MDI
MDM

After images
fc6+max pool
After conv1

End-to-end
SVM
End-to-end

35.8
32.3
–

70.9
68.6
67.1

Table 3: Evaluating the effect of end-to-end training for
multiple dynamic images and multiple dynamic maps after
the convolutional layer 1.

namic maps (MDM) based on approximate rank pooling.
To generate the multiple dynamic images or maps we use a
window size of 25 and a stride of 20 which allows for 80%
overlap.

We observe that compared to a uniﬁed, end-to-end train-
ing is beneﬁcial, bringing 2-3% accuracy improvements
depending on the dataset. Furthermore, approximate dy-
namic maps computed after the convolutional layer 1 per-
form slighly below the dynamic images (dynamic maps
computed after layer 0). We conclude that multiple dy-
namic images are better to be pooled on top of the static
RGB frames. Furthermore, multiple dynamic images per-
form better when employed in end-to-end training.

4.6. Combining dynamic images with static images

Next, we evaluate how complementary dynamic image
networks and static RGB frame networks are. For both net-
works we apply max pooling on the per video activations at
pool5 layer. Results are reported in Table 4.

As expected, we observe that static appearance informa-
tion appears to be equally important to the dynamic appear-
ance in the context of convolutional neural networks. A
combination of the two, however, brings a noticeable 6%
increase in accuracy. We conclude that the two representa-
tions are complementary to each other.

3040

HMDB51 UCF101

Method
This paper
Zha et al. [34]
Simonyan et al. [24]
Yue-Hei-Ng et al. [21]
Wu et al. [32]
Fernando et al. [6]
Hoai et al. [10]
Lan et al. [17]
Peng et al. [22]

p
e
e
d

w
o
l
l
a
h
s

65.2
–
59.4
–
56.4
63.7
60.8
65.4
66.8

89.1
89.6
88.0
88.6
84.2
–
–
89.1
–

Table 7: Comparison with the state-of-the-art. Despite be-
ing a relatively simple representation, the proposed method
is able to obtain results on par with the state state-of-the-art.

tain their accuracies after combining their methods with im-
proved trajectories [31] for optimal results.

Considering deep learning methods, our method per-
forms on par and is only outperformed from [34].
[34]
makes use of the very deep VGGnet [25], which is a more
competitive network than that the Alexnet architecture we
rely on. Hence a direct comparison is not possible. Com-
pared to the shallow methods the proposed method is also
competitive. We anticipate that combining the proposed
dynamic images with sophisticated encodings [17, 22] will
beneﬁt the accuracies further.

We conclude that while being in the context of CNNs a
simple and efﬁcient video representation, dynamic images
allow for state-of-the-art accuracy in action recognition.

5. Conclusion

We present dynamic images, a powerful and new, yet
simple video representation in the context of deep learn-
ing that summarizes videos into single images. As such,
dynamic images are directly compative to existing CNN
architectures allowing for end-to-end action recognition
learning. Extending dynamic images to the hierarchical
CNN feature maps, we introduce a novel temporal pooling
layer, Approximate-RankPool directly. Experiments
on state-of-the-art action recognition datasets demonstrate
the descriptive power of dynamic images, despite their con-
ceptual simplicity. A visual inspection outlines the richness
of dynamic images in describing complex motion patterns
as simple 2d images.

Acknowledgments: This work acknowledges the support of
the EPSRC grant EP/L024683/1, the ERC Starting Grant IDIU and
the Australian Research Council Centre of Excellence for Robotic
Vision (project number CE140100016).

Method
Static RGB
MDI-end-to-end
MDI-end-to-end + static-rgb

HMDB51 UCF101

36.7
35.8
42.8

70.1
70.9
76.9

Table 4: Evaluating complementarity of dynamic images
with static images.

Classes
SoccerJuggling
CleanAndJerk
PullUps
PushUps
PizzaTossing

Classes

Diff.
+38.5 CricketShot
+36.4 Drumming
+32.1
+26.7
+25.0

PlayingPiano
PlayingFlute
Fencing

Diff.
-47.9
-25.6
-22.0
-21.4
-18.2

Table 5: Class by class comparison between RGB and MDI
networks, where the difference in scores using MDI and
RGB are reported. A positive difference is better for MDI,
a negative difference better for RGB

Method
Trajectories [31]
MDI-end-to-end + static-rgb+trj

HMDB51 UCF101

60.0
65.2

86.0
89.1

Table 6: Combining with trajectory features brings a notice-
able increase in accuracy.

4.7. Further analysis

We, furthermore, perform a per analysis between static
rgb networks and MDI based networks. We list in Ta-
ble 5 the top 5 classes based on the relative performances
for each method. MDI performs better for “PullUps” and
“PushUps”, where motion is dominant and discriminating
between motion patterns is important. RGB static models
seems to work better on classes such as “CricketShot” and
“Drumming”, where context is already quite revealing. We
conclude that dynamic images are useful for actions where
there exist characteristic motion patterns and dynamics.

Furthermore, we investigate whether dynamic images
are complementary to state-of-the-art features,
like im-
proved trajectories [31], relying on late fusion. Results are
reported in Table 6. We obtain a signiﬁcant improvement
of 5.2% over trajectory features alone on HMDB51 dataset
and 3.1% on UCF101 dataset.

Due to the lack of space we refer to the supplementary
material for a more in depth analysis of dynamic images and
dynamic maps and their learning behavior.

4.8. State-of-the-art comparisons

Last, we compare with the state-of-the-art techniques in
UCF101 and HMDB51 in Table 7, where we make a dis-
tinction between deep and shallow architectures. Note that
similar to us, almost all methods, be it shallow or deep, ob-

3041

[20] M. Mazloom, E. Gavves, and C. G. M. Snoek. Conceptlets:
Selective semantics for classifying video events. IEEE TMM,
December 2014.

[21] J. Y. Ng, M. J. Hausknecht, S. Vijayanarasimhan, O. Vinyals,
R. Monga, and G. Toderici. Beyond short snippets: Deep
networks for video classiﬁcation. In CVPR, 2015.

[22] X. Peng, C. Zou, Y. Qiao, and Q. Peng. Action recognition

with stacked ﬁsher vectors. In ECCV, 2014.

[23] M. S. Ryoo, B. Rothrock, and L. Matthies. Pooled motion

features for ﬁrst-person videos. In CVPR, 2015.

[24] K. Simonyan and A. Zisserman.

Two-stream convolu-
tional networks for action recognition in videos. CoRR,
abs/1406.2199:1–8, 2014.

[25] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[26] A. J. Smola and B. Sch¨olkopf. A tutorial on support vector
regression. Statistics and computing, 14:199–222, 2004.
[27] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset of
101 human actions classes from videos in the wild. CoRR,
2012.

[28] N. Srivastava, E. Mansimov, and R. Salakhudinov. Unsuper-
vised learning of video representations using lstms. In ICML,
2015.

[29] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. arXiv preprint arXiv:1412.0767, 2014.

[30] H. Wang, A. Kl¨aser, C. Schmid, and C.-L. Liu. Dense tra-
jectories and motion boundary descriptors for action recog-
nition. IJCV, 103:60–79, 2013.

[31] H. Wang and C. Schmid. Action recognition with improved

trajectories. In ICCV, 2013.

[32] J. Wu, Y. Zhang, and W. Lin. Towards good practices for

action video encoding. In CVPR, 2014.

[33] S. Yeung, O. Russakovsky, N. Jin, M. Andriluka, G. Mori,
and L. Fei-Fei. Every moment counts: Dense detailed label-
ing of actions in complex videos. ArXiv e-prints, 2015.
[34] S. Zha, F. Luisier, W. Andrews, N. Srivastava, and
R. Salakhutdinov. Exploiting Image-trained CNN Archi-
tectures for Unconstrained Video Classiﬁcation. In BMVC,
2015.

References

[1] A. F. Bobick and J. W. Davis. The recognition of hu-
man movement using temporal templates. Pattern Analysis
and Machine Intelligence, IEEE Transactions on, 23(3):257–
267, 2001.

[2] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity
metric discriminatively, with application to face veriﬁcation.
In CVPR, 2005.

[3] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised vi-
sual representation learning by context prediction. In CVPR,
2015.

[4] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach,
S. Venugopalan, K. Saenko, and T. Darrell. Long-term recur-
rent convolutional networks for visual recognition and de-
scription. In arXiv preprint arXiv:1411.4389, 2014.

[5] B. Fernando, P. Anderson, M. Hutter, and S. Gould. Discrim-
inative hierarchical rank pooling for activity recognition. In
CVPR, 2016.

[6] B. Fernando, E. Gavves, J. Oramas, A. Ghodrati, and
T. Tuytelaars. Modeling video evolution for action recog-
nition. In CVPR, 2015.

[7] B. Fernando, E. Gavves, J. Oramas, A. Ghodrati, and
T. Tuytelaars. Rank pooling for action recognition. TPAMI,
2016.

[8] G. Gkioxari and J. Malik. Finding action tubes. In CVPR,

June 2015.

[9] M. Hoai and A. Zisserman. Improving human action recog-
nition using score distribution and ranking. In Proceedings
of Asian Conference on Computer Vision, 2014.

[10] M. Hoai and A. Zisserman. Improving human action recog-
nition using score distribution and ranking. In ACCV, 2014.
[11] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997.

[12] S. Ji, W. Xu, M. Yang, and K. Yu. 3d convolutional neural
networks for human action recognition. TPAMI, 2013.
[13] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. arXiv preprint
arXiv:1408.5093, 2014.

[14] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In CVPR, 2014.
[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012.

Imagenet
In

[16] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre.
Hmdb: a large video database for human motion recognition.
In ICCV, 2011.

[17] Z.-Z. Lan, M. Lin, X. Li, A. G. Hauptmann, and B. Raj.
Beyond gaussian pyramid: Multi-skip feature stacking for
action recognition. In CVPR, 2015.

[18] I. Laptev. On space-time interest points. IJCV, 64:107–123,

2005.

[19] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional

networks for semantic segmentation. CVPR, 2015.

3042

Dynamic Image Networks for Action Recognition

Hakan Bilen†∗ Basura Fernando‡∗ Efstratios Gavves§ Andrea Vedaldi† Stephen Gould‡
†University of Oxford ‡The Australian National University §QUVA Lab, University of Amsterdam

Abstract

We introduce the concept of dynamic image, a novel
compact representation of videos useful for video analy-
sis especially when convolutional neural networks (CNNs)
are used. The dynamic image is based on the rank pool-
ing concept and is obtained through the parameters of a
ranking machine that encodes the temporal evolution of the
frames of the video. Dynamic images are obtained by di-
rectly applying rank pooling on the raw image pixels of a
video producing a single RGB image per video. This idea
is simple but powerful as it enables the use of existing CNN
models directly on video data with ﬁne-tuning. We present
an efﬁcient and effective approximate rank pooling opera-
tor, speeding it up orders of magnitude compared to rank
pooling. Our new approximate rank pooling CNN layer al-
lows us to generalize dynamic images to dynamic feature
maps and we demonstrate the power of our new representa-
tions on standard benchmarks in action recognition achiev-
ing state-of-the-art performance.

1. Introduction

Videos comprise a large majority of the visual data in
existence, surpassing by a wide margin still images. There-
fore understanding the content of videos accurately and
on a large scale is of paramount importance. The advent
of modern learnable representations such as deep convolu-
tional neural networks (CNNs) has improved dramatically
the performance in many image understanding tasks. Since
videos are composed of a sequence of still images, some of
these improvements have been shown to transfer to videos
directly. However, it remains unclear how videos could be
optimally represented. For example, one can look at a video
as a sequence of still images, perhaps enjoying some form
of temporal smoothness, or as a subspace of images or im-
age features, or as the output of a neural network encoder.
Which one among these and other possibilities results in the
best representation of videos is not well understood.

∗Equal contribution
1From left to right and top to bottom: “blowing hair dry”, “band march-
ing”, “balancing on beam”, “golf swing”, “fencing”, “playing the cello”,

Figure 1: Dynamic images summarizing the actions and
motions that happen in images in standard 2d image for-
mat. Can you guess what actions are visualized just from
their dynamic image motion signature? 1

In this paper, we explore a new powerful and yet simple
representation of videos in the context of deep learning. As
a representative goal we consider the standard problem of
recognizing human actions in short video sequences. Re-
cent works such as [5, 6, 7, 9, 23] pointed out that long
term dynamics and temporal patterns are a very important
cues for the recognition of actions. However, representing
complex long term dynamics is difﬁcult, particularly if one
seeks compact representations that can be processed efﬁ-
ciently. Several efﬁcient representations of long term dy-
namics have been obtained by temporal pooling of image
features in a video. Temporal pooling has been done using
temporal templates [1] or using ranking functions for video
frames [6] or subvideos [9] or by more traditional pooling
operators [23].

In this paper we propose a new long-term pooling opera-
tor which is simple, efﬁcient, compact, and very powerful in
a neural network context. Since a CNN provides a whole hi-
erarchy of image representations, one for each intermediate
layer, the ﬁrst question is where temporal pooling should
take place. For example, one could use a method such as

“horse racing”, “doing push-ups”, “drumming”.

13034

rank pooling [6] to pool the output of the fully-connected
layers of a standard CNN architecture pre-trained on still
images and applied to individual frames. A downside of
this solution is that the CNN itself is unaware of the lower
level dynamics of the video. Alternatively, one can model
the dynamics of the response of some intermediate network
layer. In this case, the lower layers are still computed from
individual frames, but the upper layers can reason about the
overall dynamics in the video. An extreme version of this
idea is to capture the video dynamics directly at the level of
the image pixels, considering those as the ﬁrst layer of the
architecture.

Here we build on the latter intuition and build dynamics
directly at the level of the input image. To this end, our ﬁrst
contribution (section 2) is to introduce the notion of a dy-
namic image, i.e. a RGB still image that summarizes, in a
compressed format, the gist of a whole video (sub)sequence
(ﬁg. 1). The dynamic image is obtained as a ranking classi-
ﬁer that, similarly to [6, 7], sorts video frames temporally;
the difference is that we compute this classiﬁer directly at
the level of the image pixels instead of using an intermedi-
ate feature representation.

There are three keys advantages to this idea. First, the
new RGB image can be processed by a CNN architecture
which is structurally nearly identical to architectures used
for still images, while still capturing the long-term dynam-
ics in a video that relate to the long term dynamics therein.
It is then possible to use a standard CNN architecture to
learn suitable “dynamic” features from the videos. The sec-
ond advantage of this method is its remarkable efﬁciency:
the extraction of the dynamic image is extremely simple and
efﬁcient and it allows to reduce video classiﬁcation to clas-
siﬁcation of a single image by a standard CNN architecture.
The third advantage is the compression factor, as a whole
video is summarized by an amount of data equivalent to a
single frame.

Our second contribution is to provide a fast approxima-
tion of the ranker in the construction of the dynamic image.
We replace learning the ranker by simple linear operations
over the images, which is extremely efﬁcient. We also show
that, in this manner, it is possible to apply the concept of dy-
namic image to the intermediate layers of a CNN represen-
tation by constructing an efﬁcient rank pooling layer. This
layer can be incorporated into end-to-end training of a CNN
for video data.

Our third contribution is to use these ideas to propose a
novel static/dynamic neural network architecture (section 3)
which can perform end-to-end training from videos com-
bining both static appearance information from still frames,
as well as short and long term dynamics from the whole
video. We show that these technique result in efﬁcient and
accurate classiﬁcation of actions in videos, outperforming
the state-of-the-art in standard benchmarks in the area (sec-

tion 4). Our ﬁndings are summarized in section 5.

1.1. Related Work

Existing video representations can be roughly broken
into two categories. The ﬁrst category, which comprises the
majority of the literature on video processing, action and
event classiﬁcation, be it with shallow [31, 6, 20] or deep
architectures [21, 24], has viewed videos either as a stream
of still images [21] or as a short and smooth transition be-
tween similar frames [24]. Although obviously suboptimal,
considering the video as bag of static frames performs rea-
sonably well [21], as the surroundings of an action strongly
correlate with the action itself (e.g., “playing basketball”
takes place usually in a basketball court). The second cat-
egory extends CNNs to a third, temporal dimension [12]
replacing 2D ﬁlters with 3D ones. So far, this approach has
produced little beneﬁts, probably due to the lack of anno-
tated video data. Increasing the amount of annotated videos
would probably help as shown by recent 3D convolution
methods [29], although what seems especially important is
spatial consistency between frames. More speciﬁcally, a
pixel to pixel registration [24] based on the video’s opti-
cal ﬂow brings considerable improvements. Similarly, [8]
uses action tubes to to ﬁt a double stream appearance and
motion based neural network that captures the movement
of the actor.

We can also distinguish two architectural choices in the
construction of video CNNs. The ﬁrst choice is to provide
as input to the CNN a sub-video of ﬁxed length, packing a
short sequence of video frames into an array of images. The
advantage of this technique is that they allow using simple
modiﬁcations of standard CNN architectures (e.g. [15]) by
swapping 3D ﬁlters for the 2D ones. Examples of such tech-
niques include [12] and [24].

Although the aforementioned methods successfully cap-
ture the local changes within a small time window, they
cannot capture longer-term motion patterns associated with
certain actions. An alternative solution is to consider a sec-
ond family of architectures based on recurrent neural net-
works (RNNs) [4, 28]. RNNs typically consider memory
cells [11], which are sensitive to both short as well as longer
term patterns. RNNs parse the video frames sequentially
and encode the frame-level information in their memory.
In [4] LSTMs are used together with convolutional neu-
ral network activations to either output an action label or
a video description. In [28] an autoencoder-like LSTM ar-
chitecture is proposed such that either the current frame or
the next frame is accurately reconstructed. Finally, the au-
thors of [33] propose an LSTM with a temporal attention
model for densely labelling video frames.

Many of the ideas in video CNNs originated in earlier
architectures that used hand-crafted features. For exam-
ple, the authors of [18, 30, 31] have shown that local mo-

3035

tion patterns in short frame sequences can capture very well
the short temporal structures in actions. The rank pooling
idea, on which our dynamic images are based, was proposed
in [6, 7] using hand-crafted representation of the frames,
while in [5] authors increase the capacity of rank pooling
using a hierarchical approach.

Our static/dynamic CNN uses a multi-stream architec-
ture. Multiple streams have been used in a variety of dif-
ferent contexts. Examples include Siamese architectures
for learning metrics for face identiﬁcation [2] of for unsu-
pervised training of CNNs [3]. Simonyan et al. [24] use
two streams to encode respectively static frames and optical
ﬂow frames in action recognition. The authors of [19] pro-
pose a dual loss neural network was proposed, where coarse
and ﬁne outputs are jointly optimized. A difference of our
model compared to these is that we branch off two streams
at arbitrary location in the network, either at the input, at
the level of the convolutional layers, or at the level of the
fully-connected layers.

2. Dynamic Images

In this section we introduce the concept of dynamic im-
age, which is a standard RGB image that summarizes the
appearance and dynamics of a whole video sequence (sec-
tion 2.1). Then, we show how dynamic images can be
used to train dynamic-aware CNNs for action recognition
in videos (section 2.2). Finally, we propose a fast approx-
imation to accelerate the computation of dynamic images
(section 2.3).

2.1. Constructing dynamic images

While CNNs can learn automatically powerful data rep-
resentations, they can only operate within the conﬁnes of a
speciﬁc hand-crafted architecture. In designing a CNN for
video data, in particular, it is necessary to think of how the
video information should be presented to the CNN. As dis-
cussed in section 1.1, standard solutions include encoding
sub-videos of a ﬁxed duration as multi-dimensional arrays
or using recurrent architectures. Here we propose an al-
ternative and more efﬁcient approach in which the video
content is summarized by a single still image which can
then be processed by a standard CNN architecture such as
AlexNet [15].

Summarizing the video content in a single still image
may seem difﬁcult. In particular, it is not clear how image
pixels, which already contain appearance information in the
video frames, could be overloaded to reﬂect dynamic infor-
mation as well, and in particular the long-term dynamics
that are important in action recognition.

We show here that the construction of Fernando et al. [6]
can be used to obtain exactly such an image. The idea of
their paper is to represent a video as a ranking function for
Rd be
its frames I1, . . . , IT . In more detail, let ψ(It)

∈

a representation or feature vector extracted from each indi-
t
vidual frame It in the video. Let Vt = 1
τ =1 ψ(Iτ ) be
t P
time average of these features up to time t. The ranking
function associates to each time t a score S(t
,
i
Rd is a vector of parameters. The function pa-
where d
rameters d are learned so that the scores reﬂect the rank of
the frames in the video. Therefore, later times are associ-
d).
ated with larger scores, i.e. q > t =
Learning d is posed as a convex optimization problem using
the RankSVM [26] formulation:

d) > S(t

d) =

d, Vt

S(q

⇒

∈

h

|

|

|

d∗

= ρ(I1, . . . , IT ; ψ) = argmin

E(d),

d

E(d) =

d

2+
k

λ
2 k
2

T (T

1) × X
q>t

−

(1)

max

0, 1
{

−

S(q

d) + S(t

d)

.
}

|

|

The ﬁrst
term in this objective function is the usual
quadratic regularizer used in SVMs. The second term is
a hinge-loss soft-counting how many pairs q > t are incor-
rectly ranked by the scoring function. Note in particular that
a pair is considered correctly ranked only if scores are sep-
d) + 1.
arated by at least a unit margin, i.e. S(q
Optimizing eq. (1) deﬁnes a function ρ(I1, . . . , IT ; ψ)
that maps a sequence of T video frames to a single vector
d∗. Since this vector contains enough information to rank
all the frames in the video, it aggregates information from
all of them and can be used as a video descriptor. In the rest
of the paper we refer to the process of constructing d∗ from
a sequence of video frames as rank pooling.

d) > S(t

|

|

·

In [6] the map ψ(

) used in this construction is set to
be the Fisher Vector coding of a number of local features
(HOG, HOF, MBH, TRJ) extracted from individual video
frames. Here, we propose to apply rank pooling directly to
the RGB image pixels instead. While this idea is simple, in
the next several sections we will show that it has remarkable
advantages.

The ψ(It) is now an operator that stacks the RGB com-
ponents of each pixel in image It on a large vector. Alter-
natively, ψ(It) may incorporate a simple component-wise
non-linearity, such as the square root function √
(which
corresponds to using the Hellinger’s kernel in the SVM).
In all cases, the descriptor d∗ is a real vector that has the
same number of elements as a single video frame. There-
fore, d∗ can be interpreted as standard RGB image. Further-
more, since this image is obtained by rank pooling the video
frames, it summarizes information from the whole video se-
quence.

·

A few examples of dynamic images are shown in ﬁg. 1.
Several observations can be made. First, it is interesting
to note that the dynamic images tend to focus mainly on
the acting objects, such as humans or other animals such
as horses in the “horse racing” action, or objects such as

3036

training a CNN on top of such dynamic images, we implic-
itly capture the temporal patterns contained in the video.
However, since the CNN is still applied to images, we can
start from a CNN pre-trained for still image recognition,
such as AlexNet pre-trained on the ImageNet ILSVRC data,
and ﬁne-tune it on a dataset of dynamic images. Fine-tuning
allows the CNN to learn features that capture the video
dynamics without the need to train the architecture from
scratch. This is an important beneﬁt of our method because
training large CNNs require millions of data samples which
may be difﬁcult to obtain for videos.

Multiple Dynamic Images (MDI). While ﬁne-tuning
does not require as much annotated data as training a CNN
from scratch,
the domain gap between natural and dy-
namic images is sufﬁciently large that an adequately large
ﬁne-tuning dataset of dynamic images may be appropriate.
However, as noted above, in most cases there are only a few
videos available for training.

In order to address this potential limitation, in the second
scenario we propose to generate multiple dynamic images
from each video by breaking it into segments. In particu-
lar, for each video we extract multiple partially-overlapping
segments of duration τ and with stride s. In this manner, we
create multiple video segments per video, essentially mul-
tiplying the dataset size by a factor of approximately T /s,
where T is the average number of frames per video. This
can also be seen as a data augmentation step, where instead
of mirroring, cropping, or shearing images we simply take
a subset of the video frames. From each of the new video
segments, we can then compute a dynamic image to train
the CNN, using as ground truth class information of each
subsequence the class of the original video.

2.3. Fast dynamic image computation

Computing a dynamic image entails solving the opti-
mization problem of eq. (1). While this is not particularly
slow with modern solvers, in this section we propose an ap-
proximation to rank pooling which is much faster and works
as well in practice. Later, this technique, which we call
approximate rank pooling, will be critical in incorporating
rank pooling in intermediate layers of a deep CNN and to
allow back-prop training through it.

The derivation of approximate rank pooling is based on
the idea of considering the ﬁrst step in a gradient-based op-
timization of eq. (1). Starting with d = ~0, the ﬁrst ap-
proximated solution obtained by gradient descent is d∗ =
~0

E(d)

E(d)

η

−

∇
E(~0)

∇

|d=~0 ∝ −∇
max

∝ X

q>t ∇

0, 1

{

|d=~0 for any η > 0, where
d) + S(t
}|d=~0

S(q

d)

−

|

|

Figure 2: Left column: dynamic images. Right col-
umn: motion blur. Although fundamentally different both
methodologically, as well as in terms of applications, they
both seem to capture time in a similar manner.

drums in the “drumming” action. On the contrary, back-
ground pixels and background motion patterns tend to be
averaged away. Hence, the pixels in the dynamic image ap-
pear to focus on the identity and motion of the salient actors
in videos, indicating that they may contain the information
necessary to perform action recognition.

Second, we observe that dynamic images behave dif-
ferently for actions of different speeds. For slow actions,
like “blowing hair dry” in the ﬁrst row of ﬁg. 1, the motion
seems to be dragged over many frames. For faster actions,
such as “golf swing” in the second row of ﬁg. 1, the dynamic
image reﬂects key steps in the action such as preparing to
swing and stopping after swinging. For longer term actions
such as “horse riding” in the third row of ﬁg. 1, the dynamic
image reﬂects different parts of the video; for instance, the
rails that appear as a secondary motion contributor are su-
perimposed on top of the horses and the jockeys who are
the main actors. Such observations were also made in [7].

Last, it is interesting to note that dynamic images are
reminiscent of some other imaging effects that convey mo-
tion and time, such as motion blur or panning, an analogy is
illustrated in ﬁg. 2. While motion blur captures the time and
motion by integrating over subsequent pixel intensities de-
ﬁned by the camera shutter speed, dynamic images capture
the time by integrating and reordering the pixel intensities
over time within a video.

2.2. Using dynamic images

Given that the dynamic images are in the format of stan-
dard RGB images, they can be used to apply any method
for still image analysis, and in particular any state-of-the-art
CNN, to the analysis of video. In particular, we experiment
with two usage scenarios.

Single Dynamic Image (SDI).
In the ﬁrst scenario, a dy-
namic image summarizes an entire video sequence. By

=

X
q>t ∇h

d, Vt

Vq

=

−

i

Vt

Vq.

−

X
q>t

3037

,

 using time-averaged vectors V

t

t

t

,

 using A

 directly

t

t

,

15

10

5

0

-5

-10

-15

2

4

8

10

6
time

Figure 3: The graph compares the approximated rank pool-
ing weighting functions αt (for T = 11 samples) of eq. (2)
using time-averaged feature frames Vt to the variant eq. (4)
that ranks directly the feature frames ψt as is.

We can further expand d∗ as follows

d∗

Vq

Vt =

−

∝ X
q>t

1
q





X
q>t

q

ψi

−

X
i=1

1
t

t

X
j=1





T

X
t=1

ψj

=

αtψt

where the coefﬁcients αt are given by

αt = 2(T

t + 1)

(T + 1)(HT

Ht−1),

−
t
i=1 1/t is the t-th Harmonic number and

where Ht =
H0 = 0. Hence the rank pooling operator reduces to

P

−

−

(2)

ˆρ(I1, . . . , IT ; ψ) =

αtψ(It).

(3)

T

X
t=1

In
which is a weighted combination of the data points.
particular, the dynamic image computation reduces to ac-
cumulating the video frames after pre-multiplying them by
αt. The function αt, however, is non-trivial, as illustrated
in ﬁg. 3.

An alternative construction of the rank pooler does
not compute the intermediate average features Vt =
T
q=1 ψ(Iq), but uses directly individual video fea-
(1/t)
tures ψ(It) in the deﬁnition of the ranking scores (1). In
this case, the derivation above results in a weighting func-
tion of the type

P

αt = 2t

T

1

(4)

−
which is linear in t. The two scoring functions eq. (2) and
eq. (4) are compared in ﬁg. 3 and in the experiments.

−

3. Dynamic Maps Networks

In the previous section we have introduced the concept
of dynamic image as a method to pool the information con-
tained in a number of video frames in a single RGB image.

Figure 4: Dynamic image and dynamic map networks on
the left and the right pictures respectively, after applying a
rank pooling operation on top of the previous layer activa-
tions.

Here, we notice that every layer of a CNN produces as out-
put a feature map which, having a spatial structure similar
to an image, can be used in place of video frames in this
construction. We call the result of applying rank pooling
to such features a dynamic feature map, or dynamic map in
short. In the rest of the section we explain how to incor-
porate this construction as a rank-pooling layer in a CNN
(section 3.1) and how to accelerate it signiﬁcantly and per-
form back-propagation by using approximate rank pooling
(section 3.2).

3.1. Dynamic maps

The structure of a dynamic map network is illustrated
in ﬁg. 4. In the case seen so far (left in ﬁg. 4), rank pool-
ing is applied at the level of the input RGB video frames,
which we could think of as layer zero in the architecture.
We call the latter a dynamic image network. By contrast,
a dynamic map network moves rank pooling higher in the
hierarchy, by applying one or more layers of feature com-
putations to the individual feature frames and applying the
same construction to the resulting feature maps.

, . . . , a(l−1)

In particular, let a(l−1)

denote the feature
1
maps computed at the l
1 layers of the architecture, one for
each of the T video frames. Then, we use the rank pooling
equation (1) to aggregate these maps into a single dynamic
map,

−

T

a(l) = ρ(a(l−1)

, . . . , a(l−1)

).

T

1

(5)

Note that, compared to eq. (1), we dropped the term ψ;
since networks are already learning feature maps, we set
this term to the identity function. The dynamic image
network is obtained by setting l = 1 in this construction.

Rank pooling layer (RankPool) & backpropagation. In
order to train a CNN with rank pooling as an intermediate
layer, it is necessary to compute the derivatives of eq. (5) for
the backpropagation step. We can rewrite eq. (5) as a linear

3038

combination of the input data V1, . . . , VT , namely

a(l) =

βt(V1, . . . , VT )Vt

(6)

T

X
t=1

In turn, Vt is the temporal average of the input features and
is therefore a linear function Vt(a(l−1)
). Sub-
stituting, we can rewrite a(l) as

, . . . , a(l−1)

1

t

a(l) =

αt(a(l−1)

1

, . . . , a(l−1)

)a(l−1)

.

t

T

(7)

T

X
t=1

Unfortunately, we observe that due to the non-linear nature
of the optimization problem of equation (1), the coefﬁcients
βt, αt depend on the data a(l−1)
themselves. Computing
t
the gradient of a(l) with respect to the per frame data points
a(l−1)
is a challenging derivation. Hence, using dynamic
t
maps and rank pooling directly as a layer in a CNN is not
straightforward.

We note that the rank pooling layer (RankPool) consti-
tutes a new type of portable convolutional network layer,
just like a max-pooling or a ReLU layer.
It can be used
whenever dynamic information must be pooled across time.

3.2. Approximate dynamic maps.

Constructing the precise dynamic maps, or images, is in
theory optimal, but not necessarily practical. On one hand
computing the precise dynamic maps via an optimization
is computationally inefﬁcient. This is especially important
in the context of CNNs, where efﬁcient computations are
extremely important for training on large datasets, and the
optimization of eq. (5) would be slow compared to other
components of the network. On the other hand, computing
the gradients would be non trivial.

To this end we replace once again rank pooling with ap-
proximate rank pooling. With the approximate rank pool-
ing we signiﬁcantly accelerate the computations, even by
a factor of 45 as we show later in the experiments. Sec-
ondly, and more importantly, the approximate rank pooling
is also a linear combination of frames, where the per frame
coefﬁcients are given by eq. (2). These coefﬁcients are in-
dependent of the frame features Vt and ψ(It). Hence, the
derivative of the approximate rank pooling is much simpler
and can be easily computed as the vectorized coefﬁcients of
eq. (2), namely

We conclude that using approximate rank pooling in the
context of CNNs is not only practical, but also necessary for
the optimization through backpropagation.

4. Experiments

4.1. Datasets

We explore the proposed models on two state-of-the-
art datasets used for evaluating neural network based
models for action recognition, namely UCF101 [27] and
HMDB51 [16].

UCF101. The UCF101 dataset [27] comprises of 101
human action categories, like “Apply Eye Makeup” and
“Rock Climbing” and spans over 13, 320 videos. The
videos are realistic and relatively clean. They contain
little background clutter and contain a single action.
thus almost all frames
Also the videos are trimmed,
relate to the action in the video. The standard evaluation
is average accuracy over three parts provided by the authors.

HMDB51. The HMDB51 dataset [16] comprises of 51 hu-
man action categories , such as “backhand ﬂip” and “swing
baseball bat” and spans over 6, 766 videos. The videos are
realistic, downloaded from Youtube contain a single action.
The dataset is split in three parts and accuracy is averaged
over all three parts, similar to UCF101.

4.2. Implementation details

·

·

To maintain the same function domain and range we se-
) the square rooting kernel
lect for non-linear operations ψ(
and time varying mean vectors [6]. We generate
maps √
dynamic images for each color channel separately and then
merge them so that they can be directly used directly as in-
put to a CNN. As the initial dynamic images are not in the
natural range of [0, 255] for RGB data, we apply minmax
normalization. We use BVLC reference CaffeNet model
[13] trained on ImageNet images as a starting point to train
our dynamic image networks. We ﬁne-tune all the layers
with the learning rate to be 10−3 and gradually decrease it
per epoch. We use a maximum of 20 epoch during training.
Sharing code, data, models. We share our code, mod-
els and data 2. Furthermore, we have computed the dy-
namic images of the Sports1M dataset [14], and share the
Alexnet and VGGnet dynamic image networks trained on
the Sports1M.

∂ vec a(l)
∂(vec a(l−1)

t

)⊤

= αtI

where I is the identity matrix. Interestingly, we would ob-
tain the same expression for the derivative if αt in eq. (7)
would be constant and did not depend on the video frames.

4.3. Mean, max and dynamic images

(8)

First, we compare “single image per video”, namely the
proposed Single Dynamic Image (SDI) with the per video
sequence mean and max image. For all methods we ﬁrst

2https://github.com/hbilen/dynamic-image-nets

3039

Method
Mean Image
Max Image
SDI

SPLIT1
52.6
48.0
57.2

SPLIT2
53.4
46.0
58.7

51.7
42.3
57.7

52.6
45.4
57.9

SPLIT3 AVERAGE

Table 1: Comparing several video representative image
models using UCF101

Method
Appr. Rank Pooling
Rank Pooling

Speed
5920 fps
131 fps

Accuracy
96.5 ± 0.9
99.5 ± 0.1

Table 2: Approximate rank pooling vs rank pooling.

compute the single images per video ofﬂine, and for SDI
speciﬁcally we use SVR [26]. Then we train and test on
action recognition using CaffeNet network. Results are re-
ported in Table 1.

From all representations we observe that SDI achieves
the highest accuracy. We conclude that SDI model is a bet-
ter single image model than the mean and max image mod-
els.

4.4. Approximate Rank Pooling vs Rank Pooling

Next, we compare the approximate rank pooling and
rank pooling in terms of speed (frames per second) and pair-
wise ranking accuracy, which is the common measure for
evaluating learning-to-rank methods. We train on a subset
of 10 videos that contain different actions and evaluate on a
new set of 10 videos with the same type of actions respec-
tively. We report results with the mean and the standard
deviations in Table 2.

×

We observe that approximate rank pooling is 45

faster
than rank pooling, while obtaining similar ranking perfor-
mance. Further, in Figure 5 we plot the score distributions
for rank pooling and approximate rank pooling. We observe
that their score proﬁles are also similar. We conclude that
approximate rank pooling is a good approximation to rank
pooling, while being two magnitudes faster as it involves no
optimization.

4.5. Evaluating the effect of end-to-end training

Next, we evaluate in Table 3 rank pooling dynamic im-
ages with and without end-to-end training. We also eval-
uate rank pooling with dynamic maps. The ﬁrst method
generates multiple dynamic images on the RGB pixels as
earlier. These dynamic images can be computed ofﬂine,
then we train a network from end to end. The second
method passes these dynamic images through the network,
computes the fc6 activations using a pre-trained Alexnet
and aggregates them with max pooling, then trains SVM
classiﬁers per action class. The third method considers a
RankPool layer after the conv1 to generate multiple dy-

Appr. rank pooling
Rank pooling

100

200

300

400

500

600

700

800

900

50

100

150

200

250

300

350

400

450

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0

0

50

100

150

200

250

300

350

400

450

50

100

150

200

250

300

350

400

450

Figure 5: Comparison between score proﬁle of ranking
functions for approximate rank pooling and rank pooling.
Generally the approximate rank pooling follows the trend
of rank pooling.

Method RankPool Layer

Training

HMDB51 UCF101

MDI
MDI
MDM

After images
fc6+max pool
After conv1

End-to-end
SVM
End-to-end

35.8
32.3
–

70.9
68.6
67.1

Table 3: Evaluating the effect of end-to-end training for
multiple dynamic images and multiple dynamic maps after
the convolutional layer 1.

namic maps (MDM) based on approximate rank pooling.
To generate the multiple dynamic images or maps we use a
window size of 25 and a stride of 20 which allows for 80%
overlap.

We observe that compared to a uniﬁed, end-to-end train-
ing is beneﬁcial, bringing 2-3% accuracy improvements
depending on the dataset. Furthermore, approximate dy-
namic maps computed after the convolutional layer 1 per-
form slighly below the dynamic images (dynamic maps
computed after layer 0). We conclude that multiple dy-
namic images are better to be pooled on top of the static
RGB frames. Furthermore, multiple dynamic images per-
form better when employed in end-to-end training.

4.6. Combining dynamic images with static images

Next, we evaluate how complementary dynamic image
networks and static RGB frame networks are. For both net-
works we apply max pooling on the per video activations at
pool5 layer. Results are reported in Table 4.

As expected, we observe that static appearance informa-
tion appears to be equally important to the dynamic appear-
ance in the context of convolutional neural networks. A
combination of the two, however, brings a noticeable 6%
increase in accuracy. We conclude that the two representa-
tions are complementary to each other.

3040

HMDB51 UCF101

Method
This paper
Zha et al. [34]
Simonyan et al. [24]
Yue-Hei-Ng et al. [21]
Wu et al. [32]
Fernando et al. [6]
Hoai et al. [10]
Lan et al. [17]
Peng et al. [22]

p
e
e
d

w
o
l
l
a
h
s

65.2
–
59.4
–
56.4
63.7
60.8
65.4
66.8

89.1
89.6
88.0
88.6
84.2
–
–
89.1
–

Table 7: Comparison with the state-of-the-art. Despite be-
ing a relatively simple representation, the proposed method
is able to obtain results on par with the state state-of-the-art.

tain their accuracies after combining their methods with im-
proved trajectories [31] for optimal results.

Considering deep learning methods, our method per-
forms on par and is only outperformed from [34].
[34]
makes use of the very deep VGGnet [25], which is a more
competitive network than that the Alexnet architecture we
rely on. Hence a direct comparison is not possible. Com-
pared to the shallow methods the proposed method is also
competitive. We anticipate that combining the proposed
dynamic images with sophisticated encodings [17, 22] will
beneﬁt the accuracies further.

We conclude that while being in the context of CNNs a
simple and efﬁcient video representation, dynamic images
allow for state-of-the-art accuracy in action recognition.

5. Conclusion

We present dynamic images, a powerful and new, yet
simple video representation in the context of deep learn-
ing that summarizes videos into single images. As such,
dynamic images are directly compative to existing CNN
architectures allowing for end-to-end action recognition
learning. Extending dynamic images to the hierarchical
CNN feature maps, we introduce a novel temporal pooling
layer, Approximate-RankPool directly. Experiments
on state-of-the-art action recognition datasets demonstrate
the descriptive power of dynamic images, despite their con-
ceptual simplicity. A visual inspection outlines the richness
of dynamic images in describing complex motion patterns
as simple 2d images.

Acknowledgments: This work acknowledges the support of
the EPSRC grant EP/L024683/1, the ERC Starting Grant IDIU and
the Australian Research Council Centre of Excellence for Robotic
Vision (project number CE140100016).

Method
Static RGB
MDI-end-to-end
MDI-end-to-end + static-rgb

HMDB51 UCF101

36.7
35.8
42.8

70.1
70.9
76.9

Table 4: Evaluating complementarity of dynamic images
with static images.

Classes
SoccerJuggling
CleanAndJerk
PullUps
PushUps
PizzaTossing

Classes

Diff.
+38.5 CricketShot
+36.4 Drumming
+32.1
+26.7
+25.0

PlayingPiano
PlayingFlute
Fencing

Diff.
-47.9
-25.6
-22.0
-21.4
-18.2

Table 5: Class by class comparison between RGB and MDI
networks, where the difference in scores using MDI and
RGB are reported. A positive difference is better for MDI,
a negative difference better for RGB

Method
Trajectories [31]
MDI-end-to-end + static-rgb+trj

HMDB51 UCF101

60.0
65.2

86.0
89.1

Table 6: Combining with trajectory features brings a notice-
able increase in accuracy.

4.7. Further analysis

We, furthermore, perform a per analysis between static
rgb networks and MDI based networks. We list in Ta-
ble 5 the top 5 classes based on the relative performances
for each method. MDI performs better for “PullUps” and
“PushUps”, where motion is dominant and discriminating
between motion patterns is important. RGB static models
seems to work better on classes such as “CricketShot” and
“Drumming”, where context is already quite revealing. We
conclude that dynamic images are useful for actions where
there exist characteristic motion patterns and dynamics.

Furthermore, we investigate whether dynamic images
are complementary to state-of-the-art features,
like im-
proved trajectories [31], relying on late fusion. Results are
reported in Table 6. We obtain a signiﬁcant improvement
of 5.2% over trajectory features alone on HMDB51 dataset
and 3.1% on UCF101 dataset.

Due to the lack of space we refer to the supplementary
material for a more in depth analysis of dynamic images and
dynamic maps and their learning behavior.

4.8. State-of-the-art comparisons

Last, we compare with the state-of-the-art techniques in
UCF101 and HMDB51 in Table 7, where we make a dis-
tinction between deep and shallow architectures. Note that
similar to us, almost all methods, be it shallow or deep, ob-

3041

[20] M. Mazloom, E. Gavves, and C. G. M. Snoek. Conceptlets:
Selective semantics for classifying video events. IEEE TMM,
December 2014.

[21] J. Y. Ng, M. J. Hausknecht, S. Vijayanarasimhan, O. Vinyals,
R. Monga, and G. Toderici. Beyond short snippets: Deep
networks for video classiﬁcation. In CVPR, 2015.

[22] X. Peng, C. Zou, Y. Qiao, and Q. Peng. Action recognition

with stacked ﬁsher vectors. In ECCV, 2014.

[23] M. S. Ryoo, B. Rothrock, and L. Matthies. Pooled motion

features for ﬁrst-person videos. In CVPR, 2015.

[24] K. Simonyan and A. Zisserman.

Two-stream convolu-
tional networks for action recognition in videos. CoRR,
abs/1406.2199:1–8, 2014.

[25] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[26] A. J. Smola and B. Sch¨olkopf. A tutorial on support vector
regression. Statistics and computing, 14:199–222, 2004.
[27] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset of
101 human actions classes from videos in the wild. CoRR,
2012.

[28] N. Srivastava, E. Mansimov, and R. Salakhudinov. Unsuper-
vised learning of video representations using lstms. In ICML,
2015.

[29] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. arXiv preprint arXiv:1412.0767, 2014.

[30] H. Wang, A. Kl¨aser, C. Schmid, and C.-L. Liu. Dense tra-
jectories and motion boundary descriptors for action recog-
nition. IJCV, 103:60–79, 2013.

[31] H. Wang and C. Schmid. Action recognition with improved

trajectories. In ICCV, 2013.

[32] J. Wu, Y. Zhang, and W. Lin. Towards good practices for

action video encoding. In CVPR, 2014.

[33] S. Yeung, O. Russakovsky, N. Jin, M. Andriluka, G. Mori,
and L. Fei-Fei. Every moment counts: Dense detailed label-
ing of actions in complex videos. ArXiv e-prints, 2015.
[34] S. Zha, F. Luisier, W. Andrews, N. Srivastava, and
R. Salakhutdinov. Exploiting Image-trained CNN Archi-
tectures for Unconstrained Video Classiﬁcation. In BMVC,
2015.

References

[1] A. F. Bobick and J. W. Davis. The recognition of hu-
man movement using temporal templates. Pattern Analysis
and Machine Intelligence, IEEE Transactions on, 23(3):257–
267, 2001.

[2] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity
metric discriminatively, with application to face veriﬁcation.
In CVPR, 2005.

[3] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised vi-
sual representation learning by context prediction. In CVPR,
2015.

[4] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach,
S. Venugopalan, K. Saenko, and T. Darrell. Long-term recur-
rent convolutional networks for visual recognition and de-
scription. In arXiv preprint arXiv:1411.4389, 2014.

[5] B. Fernando, P. Anderson, M. Hutter, and S. Gould. Discrim-
inative hierarchical rank pooling for activity recognition. In
CVPR, 2016.

[6] B. Fernando, E. Gavves, J. Oramas, A. Ghodrati, and
T. Tuytelaars. Modeling video evolution for action recog-
nition. In CVPR, 2015.

[7] B. Fernando, E. Gavves, J. Oramas, A. Ghodrati, and
T. Tuytelaars. Rank pooling for action recognition. TPAMI,
2016.

[8] G. Gkioxari and J. Malik. Finding action tubes. In CVPR,

June 2015.

[9] M. Hoai and A. Zisserman. Improving human action recog-
nition using score distribution and ranking. In Proceedings
of Asian Conference on Computer Vision, 2014.

[10] M. Hoai and A. Zisserman. Improving human action recog-
nition using score distribution and ranking. In ACCV, 2014.
[11] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997.

[12] S. Ji, W. Xu, M. Yang, and K. Yu. 3d convolutional neural
networks for human action recognition. TPAMI, 2013.
[13] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. arXiv preprint
arXiv:1408.5093, 2014.

[14] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In CVPR, 2014.
[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012.

Imagenet
In

[16] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre.
Hmdb: a large video database for human motion recognition.
In ICCV, 2011.

[17] Z.-Z. Lan, M. Lin, X. Li, A. G. Hauptmann, and B. Raj.
Beyond gaussian pyramid: Multi-skip feature stacking for
action recognition. In CVPR, 2015.

[18] I. Laptev. On space-time interest points. IJCV, 64:107–123,

2005.

[19] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional

networks for semantic segmentation. CVPR, 2015.

3042

Dynamic Image Networks for Action Recognition

Hakan Bilen†∗ Basura Fernando‡∗ Efstratios Gavves§ Andrea Vedaldi† Stephen Gould‡
†University of Oxford ‡The Australian National University §QUVA Lab, University of Amsterdam

Abstract

We introduce the concept of dynamic image, a novel
compact representation of videos useful for video analy-
sis especially when convolutional neural networks (CNNs)
are used. The dynamic image is based on the rank pool-
ing concept and is obtained through the parameters of a
ranking machine that encodes the temporal evolution of the
frames of the video. Dynamic images are obtained by di-
rectly applying rank pooling on the raw image pixels of a
video producing a single RGB image per video. This idea
is simple but powerful as it enables the use of existing CNN
models directly on video data with ﬁne-tuning. We present
an efﬁcient and effective approximate rank pooling opera-
tor, speeding it up orders of magnitude compared to rank
pooling. Our new approximate rank pooling CNN layer al-
lows us to generalize dynamic images to dynamic feature
maps and we demonstrate the power of our new representa-
tions on standard benchmarks in action recognition achiev-
ing state-of-the-art performance.

1. Introduction

Videos comprise a large majority of the visual data in
existence, surpassing by a wide margin still images. There-
fore understanding the content of videos accurately and
on a large scale is of paramount importance. The advent
of modern learnable representations such as deep convolu-
tional neural networks (CNNs) has improved dramatically
the performance in many image understanding tasks. Since
videos are composed of a sequence of still images, some of
these improvements have been shown to transfer to videos
directly. However, it remains unclear how videos could be
optimally represented. For example, one can look at a video
as a sequence of still images, perhaps enjoying some form
of temporal smoothness, or as a subspace of images or im-
age features, or as the output of a neural network encoder.
Which one among these and other possibilities results in the
best representation of videos is not well understood.

∗Equal contribution
1From left to right and top to bottom: “blowing hair dry”, “band march-
ing”, “balancing on beam”, “golf swing”, “fencing”, “playing the cello”,

Figure 1: Dynamic images summarizing the actions and
motions that happen in images in standard 2d image for-
mat. Can you guess what actions are visualized just from
their dynamic image motion signature? 1

In this paper, we explore a new powerful and yet simple
representation of videos in the context of deep learning. As
a representative goal we consider the standard problem of
recognizing human actions in short video sequences. Re-
cent works such as [5, 6, 7, 9, 23] pointed out that long
term dynamics and temporal patterns are a very important
cues for the recognition of actions. However, representing
complex long term dynamics is difﬁcult, particularly if one
seeks compact representations that can be processed efﬁ-
ciently. Several efﬁcient representations of long term dy-
namics have been obtained by temporal pooling of image
features in a video. Temporal pooling has been done using
temporal templates [1] or using ranking functions for video
frames [6] or subvideos [9] or by more traditional pooling
operators [23].

In this paper we propose a new long-term pooling opera-
tor which is simple, efﬁcient, compact, and very powerful in
a neural network context. Since a CNN provides a whole hi-
erarchy of image representations, one for each intermediate
layer, the ﬁrst question is where temporal pooling should
take place. For example, one could use a method such as

“horse racing”, “doing push-ups”, “drumming”.

13034

rank pooling [6] to pool the output of the fully-connected
layers of a standard CNN architecture pre-trained on still
images and applied to individual frames. A downside of
this solution is that the CNN itself is unaware of the lower
level dynamics of the video. Alternatively, one can model
the dynamics of the response of some intermediate network
layer. In this case, the lower layers are still computed from
individual frames, but the upper layers can reason about the
overall dynamics in the video. An extreme version of this
idea is to capture the video dynamics directly at the level of
the image pixels, considering those as the ﬁrst layer of the
architecture.

Here we build on the latter intuition and build dynamics
directly at the level of the input image. To this end, our ﬁrst
contribution (section 2) is to introduce the notion of a dy-
namic image, i.e. a RGB still image that summarizes, in a
compressed format, the gist of a whole video (sub)sequence
(ﬁg. 1). The dynamic image is obtained as a ranking classi-
ﬁer that, similarly to [6, 7], sorts video frames temporally;
the difference is that we compute this classiﬁer directly at
the level of the image pixels instead of using an intermedi-
ate feature representation.

There are three keys advantages to this idea. First, the
new RGB image can be processed by a CNN architecture
which is structurally nearly identical to architectures used
for still images, while still capturing the long-term dynam-
ics in a video that relate to the long term dynamics therein.
It is then possible to use a standard CNN architecture to
learn suitable “dynamic” features from the videos. The sec-
ond advantage of this method is its remarkable efﬁciency:
the extraction of the dynamic image is extremely simple and
efﬁcient and it allows to reduce video classiﬁcation to clas-
siﬁcation of a single image by a standard CNN architecture.
The third advantage is the compression factor, as a whole
video is summarized by an amount of data equivalent to a
single frame.

Our second contribution is to provide a fast approxima-
tion of the ranker in the construction of the dynamic image.
We replace learning the ranker by simple linear operations
over the images, which is extremely efﬁcient. We also show
that, in this manner, it is possible to apply the concept of dy-
namic image to the intermediate layers of a CNN represen-
tation by constructing an efﬁcient rank pooling layer. This
layer can be incorporated into end-to-end training of a CNN
for video data.

Our third contribution is to use these ideas to propose a
novel static/dynamic neural network architecture (section 3)
which can perform end-to-end training from videos com-
bining both static appearance information from still frames,
as well as short and long term dynamics from the whole
video. We show that these technique result in efﬁcient and
accurate classiﬁcation of actions in videos, outperforming
the state-of-the-art in standard benchmarks in the area (sec-

tion 4). Our ﬁndings are summarized in section 5.

1.1. Related Work

Existing video representations can be roughly broken
into two categories. The ﬁrst category, which comprises the
majority of the literature on video processing, action and
event classiﬁcation, be it with shallow [31, 6, 20] or deep
architectures [21, 24], has viewed videos either as a stream
of still images [21] or as a short and smooth transition be-
tween similar frames [24]. Although obviously suboptimal,
considering the video as bag of static frames performs rea-
sonably well [21], as the surroundings of an action strongly
correlate with the action itself (e.g., “playing basketball”
takes place usually in a basketball court). The second cat-
egory extends CNNs to a third, temporal dimension [12]
replacing 2D ﬁlters with 3D ones. So far, this approach has
produced little beneﬁts, probably due to the lack of anno-
tated video data. Increasing the amount of annotated videos
would probably help as shown by recent 3D convolution
methods [29], although what seems especially important is
spatial consistency between frames. More speciﬁcally, a
pixel to pixel registration [24] based on the video’s opti-
cal ﬂow brings considerable improvements. Similarly, [8]
uses action tubes to to ﬁt a double stream appearance and
motion based neural network that captures the movement
of the actor.

We can also distinguish two architectural choices in the
construction of video CNNs. The ﬁrst choice is to provide
as input to the CNN a sub-video of ﬁxed length, packing a
short sequence of video frames into an array of images. The
advantage of this technique is that they allow using simple
modiﬁcations of standard CNN architectures (e.g. [15]) by
swapping 3D ﬁlters for the 2D ones. Examples of such tech-
niques include [12] and [24].

Although the aforementioned methods successfully cap-
ture the local changes within a small time window, they
cannot capture longer-term motion patterns associated with
certain actions. An alternative solution is to consider a sec-
ond family of architectures based on recurrent neural net-
works (RNNs) [4, 28]. RNNs typically consider memory
cells [11], which are sensitive to both short as well as longer
term patterns. RNNs parse the video frames sequentially
and encode the frame-level information in their memory.
In [4] LSTMs are used together with convolutional neu-
ral network activations to either output an action label or
a video description. In [28] an autoencoder-like LSTM ar-
chitecture is proposed such that either the current frame or
the next frame is accurately reconstructed. Finally, the au-
thors of [33] propose an LSTM with a temporal attention
model for densely labelling video frames.

Many of the ideas in video CNNs originated in earlier
architectures that used hand-crafted features. For exam-
ple, the authors of [18, 30, 31] have shown that local mo-

3035

tion patterns in short frame sequences can capture very well
the short temporal structures in actions. The rank pooling
idea, on which our dynamic images are based, was proposed
in [6, 7] using hand-crafted representation of the frames,
while in [5] authors increase the capacity of rank pooling
using a hierarchical approach.

Our static/dynamic CNN uses a multi-stream architec-
ture. Multiple streams have been used in a variety of dif-
ferent contexts. Examples include Siamese architectures
for learning metrics for face identiﬁcation [2] of for unsu-
pervised training of CNNs [3]. Simonyan et al. [24] use
two streams to encode respectively static frames and optical
ﬂow frames in action recognition. The authors of [19] pro-
pose a dual loss neural network was proposed, where coarse
and ﬁne outputs are jointly optimized. A difference of our
model compared to these is that we branch off two streams
at arbitrary location in the network, either at the input, at
the level of the convolutional layers, or at the level of the
fully-connected layers.

2. Dynamic Images

In this section we introduce the concept of dynamic im-
age, which is a standard RGB image that summarizes the
appearance and dynamics of a whole video sequence (sec-
tion 2.1). Then, we show how dynamic images can be
used to train dynamic-aware CNNs for action recognition
in videos (section 2.2). Finally, we propose a fast approx-
imation to accelerate the computation of dynamic images
(section 2.3).

2.1. Constructing dynamic images

While CNNs can learn automatically powerful data rep-
resentations, they can only operate within the conﬁnes of a
speciﬁc hand-crafted architecture. In designing a CNN for
video data, in particular, it is necessary to think of how the
video information should be presented to the CNN. As dis-
cussed in section 1.1, standard solutions include encoding
sub-videos of a ﬁxed duration as multi-dimensional arrays
or using recurrent architectures. Here we propose an al-
ternative and more efﬁcient approach in which the video
content is summarized by a single still image which can
then be processed by a standard CNN architecture such as
AlexNet [15].

Summarizing the video content in a single still image
may seem difﬁcult. In particular, it is not clear how image
pixels, which already contain appearance information in the
video frames, could be overloaded to reﬂect dynamic infor-
mation as well, and in particular the long-term dynamics
that are important in action recognition.

We show here that the construction of Fernando et al. [6]
can be used to obtain exactly such an image. The idea of
their paper is to represent a video as a ranking function for
Rd be
its frames I1, . . . , IT . In more detail, let ψ(It)

∈

a representation or feature vector extracted from each indi-
t
vidual frame It in the video. Let Vt = 1
τ =1 ψ(Iτ ) be
t P
time average of these features up to time t. The ranking
function associates to each time t a score S(t
,
i
Rd is a vector of parameters. The function pa-
where d
rameters d are learned so that the scores reﬂect the rank of
the frames in the video. Therefore, later times are associ-
d).
ated with larger scores, i.e. q > t =
Learning d is posed as a convex optimization problem using
the RankSVM [26] formulation:

d) > S(t

d) =

d, Vt

S(q

⇒

∈

h

|

|

|

d∗

= ρ(I1, . . . , IT ; ψ) = argmin

E(d),

d

E(d) =

d

2+
k

λ
2 k
2

T (T

1) × X
q>t

−

(1)

max

0, 1
{

−

S(q

d) + S(t

d)

.
}

|

|

The ﬁrst
term in this objective function is the usual
quadratic regularizer used in SVMs. The second term is
a hinge-loss soft-counting how many pairs q > t are incor-
rectly ranked by the scoring function. Note in particular that
a pair is considered correctly ranked only if scores are sep-
d) + 1.
arated by at least a unit margin, i.e. S(q
Optimizing eq. (1) deﬁnes a function ρ(I1, . . . , IT ; ψ)
that maps a sequence of T video frames to a single vector
d∗. Since this vector contains enough information to rank
all the frames in the video, it aggregates information from
all of them and can be used as a video descriptor. In the rest
of the paper we refer to the process of constructing d∗ from
a sequence of video frames as rank pooling.

d) > S(t

|

|

·

In [6] the map ψ(

) used in this construction is set to
be the Fisher Vector coding of a number of local features
(HOG, HOF, MBH, TRJ) extracted from individual video
frames. Here, we propose to apply rank pooling directly to
the RGB image pixels instead. While this idea is simple, in
the next several sections we will show that it has remarkable
advantages.

The ψ(It) is now an operator that stacks the RGB com-
ponents of each pixel in image It on a large vector. Alter-
natively, ψ(It) may incorporate a simple component-wise
non-linearity, such as the square root function √
(which
corresponds to using the Hellinger’s kernel in the SVM).
In all cases, the descriptor d∗ is a real vector that has the
same number of elements as a single video frame. There-
fore, d∗ can be interpreted as standard RGB image. Further-
more, since this image is obtained by rank pooling the video
frames, it summarizes information from the whole video se-
quence.

·

A few examples of dynamic images are shown in ﬁg. 1.
Several observations can be made. First, it is interesting
to note that the dynamic images tend to focus mainly on
the acting objects, such as humans or other animals such
as horses in the “horse racing” action, or objects such as

3036

training a CNN on top of such dynamic images, we implic-
itly capture the temporal patterns contained in the video.
However, since the CNN is still applied to images, we can
start from a CNN pre-trained for still image recognition,
such as AlexNet pre-trained on the ImageNet ILSVRC data,
and ﬁne-tune it on a dataset of dynamic images. Fine-tuning
allows the CNN to learn features that capture the video
dynamics without the need to train the architecture from
scratch. This is an important beneﬁt of our method because
training large CNNs require millions of data samples which
may be difﬁcult to obtain for videos.

Multiple Dynamic Images (MDI). While ﬁne-tuning
does not require as much annotated data as training a CNN
from scratch,
the domain gap between natural and dy-
namic images is sufﬁciently large that an adequately large
ﬁne-tuning dataset of dynamic images may be appropriate.
However, as noted above, in most cases there are only a few
videos available for training.

In order to address this potential limitation, in the second
scenario we propose to generate multiple dynamic images
from each video by breaking it into segments. In particu-
lar, for each video we extract multiple partially-overlapping
segments of duration τ and with stride s. In this manner, we
create multiple video segments per video, essentially mul-
tiplying the dataset size by a factor of approximately T /s,
where T is the average number of frames per video. This
can also be seen as a data augmentation step, where instead
of mirroring, cropping, or shearing images we simply take
a subset of the video frames. From each of the new video
segments, we can then compute a dynamic image to train
the CNN, using as ground truth class information of each
subsequence the class of the original video.

2.3. Fast dynamic image computation

Computing a dynamic image entails solving the opti-
mization problem of eq. (1). While this is not particularly
slow with modern solvers, in this section we propose an ap-
proximation to rank pooling which is much faster and works
as well in practice. Later, this technique, which we call
approximate rank pooling, will be critical in incorporating
rank pooling in intermediate layers of a deep CNN and to
allow back-prop training through it.

The derivation of approximate rank pooling is based on
the idea of considering the ﬁrst step in a gradient-based op-
timization of eq. (1). Starting with d = ~0, the ﬁrst ap-
proximated solution obtained by gradient descent is d∗ =
~0

E(d)

E(d)

η

−

∇
E(~0)

∇

|d=~0 ∝ −∇
max

∝ X

q>t ∇

0, 1

{

|d=~0 for any η > 0, where
d) + S(t
}|d=~0

S(q

d)

−

|

|

Figure 2: Left column: dynamic images. Right col-
umn: motion blur. Although fundamentally different both
methodologically, as well as in terms of applications, they
both seem to capture time in a similar manner.

drums in the “drumming” action. On the contrary, back-
ground pixels and background motion patterns tend to be
averaged away. Hence, the pixels in the dynamic image ap-
pear to focus on the identity and motion of the salient actors
in videos, indicating that they may contain the information
necessary to perform action recognition.

Second, we observe that dynamic images behave dif-
ferently for actions of different speeds. For slow actions,
like “blowing hair dry” in the ﬁrst row of ﬁg. 1, the motion
seems to be dragged over many frames. For faster actions,
such as “golf swing” in the second row of ﬁg. 1, the dynamic
image reﬂects key steps in the action such as preparing to
swing and stopping after swinging. For longer term actions
such as “horse riding” in the third row of ﬁg. 1, the dynamic
image reﬂects different parts of the video; for instance, the
rails that appear as a secondary motion contributor are su-
perimposed on top of the horses and the jockeys who are
the main actors. Such observations were also made in [7].

Last, it is interesting to note that dynamic images are
reminiscent of some other imaging effects that convey mo-
tion and time, such as motion blur or panning, an analogy is
illustrated in ﬁg. 2. While motion blur captures the time and
motion by integrating over subsequent pixel intensities de-
ﬁned by the camera shutter speed, dynamic images capture
the time by integrating and reordering the pixel intensities
over time within a video.

2.2. Using dynamic images

Given that the dynamic images are in the format of stan-
dard RGB images, they can be used to apply any method
for still image analysis, and in particular any state-of-the-art
CNN, to the analysis of video. In particular, we experiment
with two usage scenarios.

Single Dynamic Image (SDI).
In the ﬁrst scenario, a dy-
namic image summarizes an entire video sequence. By

=

X
q>t ∇h

d, Vt

Vq

=

−

i

Vt

Vq.

−

X
q>t

3037

,

 using time-averaged vectors V

t

t

t

,

 using A

 directly

t

t

,

15

10

5

0

-5

-10

-15

2

4

8

10

6
time

Figure 3: The graph compares the approximated rank pool-
ing weighting functions αt (for T = 11 samples) of eq. (2)
using time-averaged feature frames Vt to the variant eq. (4)
that ranks directly the feature frames ψt as is.

We can further expand d∗ as follows

d∗

Vq

Vt =

−

∝ X
q>t

1
q





X
q>t

q

ψi

−

X
i=1

1
t

t

X
j=1





T

X
t=1

ψj

=

αtψt

where the coefﬁcients αt are given by

αt = 2(T

t + 1)

(T + 1)(HT

Ht−1),

−
t
i=1 1/t is the t-th Harmonic number and

where Ht =
H0 = 0. Hence the rank pooling operator reduces to

P

−

−

(2)

ˆρ(I1, . . . , IT ; ψ) =

αtψ(It).

(3)

T

X
t=1

In
which is a weighted combination of the data points.
particular, the dynamic image computation reduces to ac-
cumulating the video frames after pre-multiplying them by
αt. The function αt, however, is non-trivial, as illustrated
in ﬁg. 3.

An alternative construction of the rank pooler does
not compute the intermediate average features Vt =
T
q=1 ψ(Iq), but uses directly individual video fea-
(1/t)
tures ψ(It) in the deﬁnition of the ranking scores (1). In
this case, the derivation above results in a weighting func-
tion of the type

P

αt = 2t

T

1

(4)

−
which is linear in t. The two scoring functions eq. (2) and
eq. (4) are compared in ﬁg. 3 and in the experiments.

−

3. Dynamic Maps Networks

In the previous section we have introduced the concept
of dynamic image as a method to pool the information con-
tained in a number of video frames in a single RGB image.

Figure 4: Dynamic image and dynamic map networks on
the left and the right pictures respectively, after applying a
rank pooling operation on top of the previous layer activa-
tions.

Here, we notice that every layer of a CNN produces as out-
put a feature map which, having a spatial structure similar
to an image, can be used in place of video frames in this
construction. We call the result of applying rank pooling
to such features a dynamic feature map, or dynamic map in
short. In the rest of the section we explain how to incor-
porate this construction as a rank-pooling layer in a CNN
(section 3.1) and how to accelerate it signiﬁcantly and per-
form back-propagation by using approximate rank pooling
(section 3.2).

3.1. Dynamic maps

The structure of a dynamic map network is illustrated
in ﬁg. 4. In the case seen so far (left in ﬁg. 4), rank pool-
ing is applied at the level of the input RGB video frames,
which we could think of as layer zero in the architecture.
We call the latter a dynamic image network. By contrast,
a dynamic map network moves rank pooling higher in the
hierarchy, by applying one or more layers of feature com-
putations to the individual feature frames and applying the
same construction to the resulting feature maps.

, . . . , a(l−1)

In particular, let a(l−1)

denote the feature
1
maps computed at the l
1 layers of the architecture, one for
each of the T video frames. Then, we use the rank pooling
equation (1) to aggregate these maps into a single dynamic
map,

−

T

a(l) = ρ(a(l−1)

, . . . , a(l−1)

).

T

1

(5)

Note that, compared to eq. (1), we dropped the term ψ;
since networks are already learning feature maps, we set
this term to the identity function. The dynamic image
network is obtained by setting l = 1 in this construction.

Rank pooling layer (RankPool) & backpropagation. In
order to train a CNN with rank pooling as an intermediate
layer, it is necessary to compute the derivatives of eq. (5) for
the backpropagation step. We can rewrite eq. (5) as a linear

3038

combination of the input data V1, . . . , VT , namely

a(l) =

βt(V1, . . . , VT )Vt

(6)

T

X
t=1

In turn, Vt is the temporal average of the input features and
is therefore a linear function Vt(a(l−1)
). Sub-
stituting, we can rewrite a(l) as

, . . . , a(l−1)

1

t

a(l) =

αt(a(l−1)

1

, . . . , a(l−1)

)a(l−1)

.

t

T

(7)

T

X
t=1

Unfortunately, we observe that due to the non-linear nature
of the optimization problem of equation (1), the coefﬁcients
βt, αt depend on the data a(l−1)
themselves. Computing
t
the gradient of a(l) with respect to the per frame data points
a(l−1)
is a challenging derivation. Hence, using dynamic
t
maps and rank pooling directly as a layer in a CNN is not
straightforward.

We note that the rank pooling layer (RankPool) consti-
tutes a new type of portable convolutional network layer,
just like a max-pooling or a ReLU layer.
It can be used
whenever dynamic information must be pooled across time.

3.2. Approximate dynamic maps.

Constructing the precise dynamic maps, or images, is in
theory optimal, but not necessarily practical. On one hand
computing the precise dynamic maps via an optimization
is computationally inefﬁcient. This is especially important
in the context of CNNs, where efﬁcient computations are
extremely important for training on large datasets, and the
optimization of eq. (5) would be slow compared to other
components of the network. On the other hand, computing
the gradients would be non trivial.

To this end we replace once again rank pooling with ap-
proximate rank pooling. With the approximate rank pool-
ing we signiﬁcantly accelerate the computations, even by
a factor of 45 as we show later in the experiments. Sec-
ondly, and more importantly, the approximate rank pooling
is also a linear combination of frames, where the per frame
coefﬁcients are given by eq. (2). These coefﬁcients are in-
dependent of the frame features Vt and ψ(It). Hence, the
derivative of the approximate rank pooling is much simpler
and can be easily computed as the vectorized coefﬁcients of
eq. (2), namely

We conclude that using approximate rank pooling in the
context of CNNs is not only practical, but also necessary for
the optimization through backpropagation.

4. Experiments

4.1. Datasets

We explore the proposed models on two state-of-the-
art datasets used for evaluating neural network based
models for action recognition, namely UCF101 [27] and
HMDB51 [16].

UCF101. The UCF101 dataset [27] comprises of 101
human action categories, like “Apply Eye Makeup” and
“Rock Climbing” and spans over 13, 320 videos. The
videos are realistic and relatively clean. They contain
little background clutter and contain a single action.
thus almost all frames
Also the videos are trimmed,
relate to the action in the video. The standard evaluation
is average accuracy over three parts provided by the authors.

HMDB51. The HMDB51 dataset [16] comprises of 51 hu-
man action categories , such as “backhand ﬂip” and “swing
baseball bat” and spans over 6, 766 videos. The videos are
realistic, downloaded from Youtube contain a single action.
The dataset is split in three parts and accuracy is averaged
over all three parts, similar to UCF101.

4.2. Implementation details

·

·

To maintain the same function domain and range we se-
) the square rooting kernel
lect for non-linear operations ψ(
and time varying mean vectors [6]. We generate
maps √
dynamic images for each color channel separately and then
merge them so that they can be directly used directly as in-
put to a CNN. As the initial dynamic images are not in the
natural range of [0, 255] for RGB data, we apply minmax
normalization. We use BVLC reference CaffeNet model
[13] trained on ImageNet images as a starting point to train
our dynamic image networks. We ﬁne-tune all the layers
with the learning rate to be 10−3 and gradually decrease it
per epoch. We use a maximum of 20 epoch during training.
Sharing code, data, models. We share our code, mod-
els and data 2. Furthermore, we have computed the dy-
namic images of the Sports1M dataset [14], and share the
Alexnet and VGGnet dynamic image networks trained on
the Sports1M.

∂ vec a(l)
∂(vec a(l−1)

t

)⊤

= αtI

where I is the identity matrix. Interestingly, we would ob-
tain the same expression for the derivative if αt in eq. (7)
would be constant and did not depend on the video frames.

4.3. Mean, max and dynamic images

(8)

First, we compare “single image per video”, namely the
proposed Single Dynamic Image (SDI) with the per video
sequence mean and max image. For all methods we ﬁrst

2https://github.com/hbilen/dynamic-image-nets

3039

Method
Mean Image
Max Image
SDI

SPLIT1
52.6
48.0
57.2

SPLIT2
53.4
46.0
58.7

51.7
42.3
57.7

52.6
45.4
57.9

SPLIT3 AVERAGE

Table 1: Comparing several video representative image
models using UCF101

Method
Appr. Rank Pooling
Rank Pooling

Speed
5920 fps
131 fps

Accuracy
96.5 ± 0.9
99.5 ± 0.1

Table 2: Approximate rank pooling vs rank pooling.

compute the single images per video ofﬂine, and for SDI
speciﬁcally we use SVR [26]. Then we train and test on
action recognition using CaffeNet network. Results are re-
ported in Table 1.

From all representations we observe that SDI achieves
the highest accuracy. We conclude that SDI model is a bet-
ter single image model than the mean and max image mod-
els.

4.4. Approximate Rank Pooling vs Rank Pooling

Next, we compare the approximate rank pooling and
rank pooling in terms of speed (frames per second) and pair-
wise ranking accuracy, which is the common measure for
evaluating learning-to-rank methods. We train on a subset
of 10 videos that contain different actions and evaluate on a
new set of 10 videos with the same type of actions respec-
tively. We report results with the mean and the standard
deviations in Table 2.

×

We observe that approximate rank pooling is 45

faster
than rank pooling, while obtaining similar ranking perfor-
mance. Further, in Figure 5 we plot the score distributions
for rank pooling and approximate rank pooling. We observe
that their score proﬁles are also similar. We conclude that
approximate rank pooling is a good approximation to rank
pooling, while being two magnitudes faster as it involves no
optimization.

4.5. Evaluating the effect of end-to-end training

Next, we evaluate in Table 3 rank pooling dynamic im-
ages with and without end-to-end training. We also eval-
uate rank pooling with dynamic maps. The ﬁrst method
generates multiple dynamic images on the RGB pixels as
earlier. These dynamic images can be computed ofﬂine,
then we train a network from end to end. The second
method passes these dynamic images through the network,
computes the fc6 activations using a pre-trained Alexnet
and aggregates them with max pooling, then trains SVM
classiﬁers per action class. The third method considers a
RankPool layer after the conv1 to generate multiple dy-

Appr. rank pooling
Rank pooling

100

200

300

400

500

600

700

800

900

50

100

150

200

250

300

350

400

450

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0

0

50

100

150

200

250

300

350

400

450

50

100

150

200

250

300

350

400

450

Figure 5: Comparison between score proﬁle of ranking
functions for approximate rank pooling and rank pooling.
Generally the approximate rank pooling follows the trend
of rank pooling.

Method RankPool Layer

Training

HMDB51 UCF101

MDI
MDI
MDM

After images
fc6+max pool
After conv1

End-to-end
SVM
End-to-end

35.8
32.3
–

70.9
68.6
67.1

Table 3: Evaluating the effect of end-to-end training for
multiple dynamic images and multiple dynamic maps after
the convolutional layer 1.

namic maps (MDM) based on approximate rank pooling.
To generate the multiple dynamic images or maps we use a
window size of 25 and a stride of 20 which allows for 80%
overlap.

We observe that compared to a uniﬁed, end-to-end train-
ing is beneﬁcial, bringing 2-3% accuracy improvements
depending on the dataset. Furthermore, approximate dy-
namic maps computed after the convolutional layer 1 per-
form slighly below the dynamic images (dynamic maps
computed after layer 0). We conclude that multiple dy-
namic images are better to be pooled on top of the static
RGB frames. Furthermore, multiple dynamic images per-
form better when employed in end-to-end training.

4.6. Combining dynamic images with static images

Next, we evaluate how complementary dynamic image
networks and static RGB frame networks are. For both net-
works we apply max pooling on the per video activations at
pool5 layer. Results are reported in Table 4.

As expected, we observe that static appearance informa-
tion appears to be equally important to the dynamic appear-
ance in the context of convolutional neural networks. A
combination of the two, however, brings a noticeable 6%
increase in accuracy. We conclude that the two representa-
tions are complementary to each other.

3040

HMDB51 UCF101

Method
This paper
Zha et al. [34]
Simonyan et al. [24]
Yue-Hei-Ng et al. [21]
Wu et al. [32]
Fernando et al. [6]
Hoai et al. [10]
Lan et al. [17]
Peng et al. [22]

p
e
e
d

w
o
l
l
a
h
s

65.2
–
59.4
–
56.4
63.7
60.8
65.4
66.8

89.1
89.6
88.0
88.6
84.2
–
–
89.1
–

Table 7: Comparison with the state-of-the-art. Despite be-
ing a relatively simple representation, the proposed method
is able to obtain results on par with the state state-of-the-art.

tain their accuracies after combining their methods with im-
proved trajectories [31] for optimal results.

Considering deep learning methods, our method per-
forms on par and is only outperformed from [34].
[34]
makes use of the very deep VGGnet [25], which is a more
competitive network than that the Alexnet architecture we
rely on. Hence a direct comparison is not possible. Com-
pared to the shallow methods the proposed method is also
competitive. We anticipate that combining the proposed
dynamic images with sophisticated encodings [17, 22] will
beneﬁt the accuracies further.

We conclude that while being in the context of CNNs a
simple and efﬁcient video representation, dynamic images
allow for state-of-the-art accuracy in action recognition.

5. Conclusion

We present dynamic images, a powerful and new, yet
simple video representation in the context of deep learn-
ing that summarizes videos into single images. As such,
dynamic images are directly compative to existing CNN
architectures allowing for end-to-end action recognition
learning. Extending dynamic images to the hierarchical
CNN feature maps, we introduce a novel temporal pooling
layer, Approximate-RankPool directly. Experiments
on state-of-the-art action recognition datasets demonstrate
the descriptive power of dynamic images, despite their con-
ceptual simplicity. A visual inspection outlines the richness
of dynamic images in describing complex motion patterns
as simple 2d images.

Acknowledgments: This work acknowledges the support of
the EPSRC grant EP/L024683/1, the ERC Starting Grant IDIU and
the Australian Research Council Centre of Excellence for Robotic
Vision (project number CE140100016).

Method
Static RGB
MDI-end-to-end
MDI-end-to-end + static-rgb

HMDB51 UCF101

36.7
35.8
42.8

70.1
70.9
76.9

Table 4: Evaluating complementarity of dynamic images
with static images.

Classes
SoccerJuggling
CleanAndJerk
PullUps
PushUps
PizzaTossing

Classes

Diff.
+38.5 CricketShot
+36.4 Drumming
+32.1
+26.7
+25.0

PlayingPiano
PlayingFlute
Fencing

Diff.
-47.9
-25.6
-22.0
-21.4
-18.2

Table 5: Class by class comparison between RGB and MDI
networks, where the difference in scores using MDI and
RGB are reported. A positive difference is better for MDI,
a negative difference better for RGB

Method
Trajectories [31]
MDI-end-to-end + static-rgb+trj

HMDB51 UCF101

60.0
65.2

86.0
89.1

Table 6: Combining with trajectory features brings a notice-
able increase in accuracy.

4.7. Further analysis

We, furthermore, perform a per analysis between static
rgb networks and MDI based networks. We list in Ta-
ble 5 the top 5 classes based on the relative performances
for each method. MDI performs better for “PullUps” and
“PushUps”, where motion is dominant and discriminating
between motion patterns is important. RGB static models
seems to work better on classes such as “CricketShot” and
“Drumming”, where context is already quite revealing. We
conclude that dynamic images are useful for actions where
there exist characteristic motion patterns and dynamics.

Furthermore, we investigate whether dynamic images
are complementary to state-of-the-art features,
like im-
proved trajectories [31], relying on late fusion. Results are
reported in Table 6. We obtain a signiﬁcant improvement
of 5.2% over trajectory features alone on HMDB51 dataset
and 3.1% on UCF101 dataset.

Due to the lack of space we refer to the supplementary
material for a more in depth analysis of dynamic images and
dynamic maps and their learning behavior.

4.8. State-of-the-art comparisons

Last, we compare with the state-of-the-art techniques in
UCF101 and HMDB51 in Table 7, where we make a dis-
tinction between deep and shallow architectures. Note that
similar to us, almost all methods, be it shallow or deep, ob-

3041

[20] M. Mazloom, E. Gavves, and C. G. M. Snoek. Conceptlets:
Selective semantics for classifying video events. IEEE TMM,
December 2014.

[21] J. Y. Ng, M. J. Hausknecht, S. Vijayanarasimhan, O. Vinyals,
R. Monga, and G. Toderici. Beyond short snippets: Deep
networks for video classiﬁcation. In CVPR, 2015.

[22] X. Peng, C. Zou, Y. Qiao, and Q. Peng. Action recognition

with stacked ﬁsher vectors. In ECCV, 2014.

[23] M. S. Ryoo, B. Rothrock, and L. Matthies. Pooled motion

features for ﬁrst-person videos. In CVPR, 2015.

[24] K. Simonyan and A. Zisserman.

Two-stream convolu-
tional networks for action recognition in videos. CoRR,
abs/1406.2199:1–8, 2014.

[25] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[26] A. J. Smola and B. Sch¨olkopf. A tutorial on support vector
regression. Statistics and computing, 14:199–222, 2004.
[27] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset of
101 human actions classes from videos in the wild. CoRR,
2012.

[28] N. Srivastava, E. Mansimov, and R. Salakhudinov. Unsuper-
vised learning of video representations using lstms. In ICML,
2015.

[29] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. arXiv preprint arXiv:1412.0767, 2014.

[30] H. Wang, A. Kl¨aser, C. Schmid, and C.-L. Liu. Dense tra-
jectories and motion boundary descriptors for action recog-
nition. IJCV, 103:60–79, 2013.

[31] H. Wang and C. Schmid. Action recognition with improved

trajectories. In ICCV, 2013.

[32] J. Wu, Y. Zhang, and W. Lin. Towards good practices for

action video encoding. In CVPR, 2014.

[33] S. Yeung, O. Russakovsky, N. Jin, M. Andriluka, G. Mori,
and L. Fei-Fei. Every moment counts: Dense detailed label-
ing of actions in complex videos. ArXiv e-prints, 2015.
[34] S. Zha, F. Luisier, W. Andrews, N. Srivastava, and
R. Salakhutdinov. Exploiting Image-trained CNN Archi-
tectures for Unconstrained Video Classiﬁcation. In BMVC,
2015.

References

[1] A. F. Bobick and J. W. Davis. The recognition of hu-
man movement using temporal templates. Pattern Analysis
and Machine Intelligence, IEEE Transactions on, 23(3):257–
267, 2001.

[2] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity
metric discriminatively, with application to face veriﬁcation.
In CVPR, 2005.

[3] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised vi-
sual representation learning by context prediction. In CVPR,
2015.

[4] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach,
S. Venugopalan, K. Saenko, and T. Darrell. Long-term recur-
rent convolutional networks for visual recognition and de-
scription. In arXiv preprint arXiv:1411.4389, 2014.

[5] B. Fernando, P. Anderson, M. Hutter, and S. Gould. Discrim-
inative hierarchical rank pooling for activity recognition. In
CVPR, 2016.

[6] B. Fernando, E. Gavves, J. Oramas, A. Ghodrati, and
T. Tuytelaars. Modeling video evolution for action recog-
nition. In CVPR, 2015.

[7] B. Fernando, E. Gavves, J. Oramas, A. Ghodrati, and
T. Tuytelaars. Rank pooling for action recognition. TPAMI,
2016.

[8] G. Gkioxari and J. Malik. Finding action tubes. In CVPR,

June 2015.

[9] M. Hoai and A. Zisserman. Improving human action recog-
nition using score distribution and ranking. In Proceedings
of Asian Conference on Computer Vision, 2014.

[10] M. Hoai and A. Zisserman. Improving human action recog-
nition using score distribution and ranking. In ACCV, 2014.
[11] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997.

[12] S. Ji, W. Xu, M. Yang, and K. Yu. 3d convolutional neural
networks for human action recognition. TPAMI, 2013.
[13] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. arXiv preprint
arXiv:1408.5093, 2014.

[14] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In CVPR, 2014.
[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012.

Imagenet
In

[16] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre.
Hmdb: a large video database for human motion recognition.
In ICCV, 2011.

[17] Z.-Z. Lan, M. Lin, X. Li, A. G. Hauptmann, and B. Raj.
Beyond gaussian pyramid: Multi-skip feature stacking for
action recognition. In CVPR, 2015.

[18] I. Laptev. On space-time interest points. IJCV, 64:107–123,

2005.

[19] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional

networks for semantic segmentation. CVPR, 2015.

3042

Dynamic Image Networks for Action Recognition

Hakan Bilen†∗ Basura Fernando‡∗ Efstratios Gavves§ Andrea Vedaldi† Stephen Gould‡
†University of Oxford ‡The Australian National University §QUVA Lab, University of Amsterdam

Abstract

We introduce the concept of dynamic image, a novel
compact representation of videos useful for video analy-
sis especially when convolutional neural networks (CNNs)
are used. The dynamic image is based on the rank pool-
ing concept and is obtained through the parameters of a
ranking machine that encodes the temporal evolution of the
frames of the video. Dynamic images are obtained by di-
rectly applying rank pooling on the raw image pixels of a
video producing a single RGB image per video. This idea
is simple but powerful as it enables the use of existing CNN
models directly on video data with ﬁne-tuning. We present
an efﬁcient and effective approximate rank pooling opera-
tor, speeding it up orders of magnitude compared to rank
pooling. Our new approximate rank pooling CNN layer al-
lows us to generalize dynamic images to dynamic feature
maps and we demonstrate the power of our new representa-
tions on standard benchmarks in action recognition achiev-
ing state-of-the-art performance.

1. Introduction

Videos comprise a large majority of the visual data in
existence, surpassing by a wide margin still images. There-
fore understanding the content of videos accurately and
on a large scale is of paramount importance. The advent
of modern learnable representations such as deep convolu-
tional neural networks (CNNs) has improved dramatically
the performance in many image understanding tasks. Since
videos are composed of a sequence of still images, some of
these improvements have been shown to transfer to videos
directly. However, it remains unclear how videos could be
optimally represented. For example, one can look at a video
as a sequence of still images, perhaps enjoying some form
of temporal smoothness, or as a subspace of images or im-
age features, or as the output of a neural network encoder.
Which one among these and other possibilities results in the
best representation of videos is not well understood.

∗Equal contribution
1From left to right and top to bottom: “blowing hair dry”, “band march-
ing”, “balancing on beam”, “golf swing”, “fencing”, “playing the cello”,

Figure 1: Dynamic images summarizing the actions and
motions that happen in images in standard 2d image for-
mat. Can you guess what actions are visualized just from
their dynamic image motion signature? 1

In this paper, we explore a new powerful and yet simple
representation of videos in the context of deep learning. As
a representative goal we consider the standard problem of
recognizing human actions in short video sequences. Re-
cent works such as [5, 6, 7, 9, 23] pointed out that long
term dynamics and temporal patterns are a very important
cues for the recognition of actions. However, representing
complex long term dynamics is difﬁcult, particularly if one
seeks compact representations that can be processed efﬁ-
ciently. Several efﬁcient representations of long term dy-
namics have been obtained by temporal pooling of image
features in a video. Temporal pooling has been done using
temporal templates [1] or using ranking functions for video
frames [6] or subvideos [9] or by more traditional pooling
operators [23].

In this paper we propose a new long-term pooling opera-
tor which is simple, efﬁcient, compact, and very powerful in
a neural network context. Since a CNN provides a whole hi-
erarchy of image representations, one for each intermediate
layer, the ﬁrst question is where temporal pooling should
take place. For example, one could use a method such as

“horse racing”, “doing push-ups”, “drumming”.

13034

rank pooling [6] to pool the output of the fully-connected
layers of a standard CNN architecture pre-trained on still
images and applied to individual frames. A downside of
this solution is that the CNN itself is unaware of the lower
level dynamics of the video. Alternatively, one can model
the dynamics of the response of some intermediate network
layer. In this case, the lower layers are still computed from
individual frames, but the upper layers can reason about the
overall dynamics in the video. An extreme version of this
idea is to capture the video dynamics directly at the level of
the image pixels, considering those as the ﬁrst layer of the
architecture.

Here we build on the latter intuition and build dynamics
directly at the level of the input image. To this end, our ﬁrst
contribution (section 2) is to introduce the notion of a dy-
namic image, i.e. a RGB still image that summarizes, in a
compressed format, the gist of a whole video (sub)sequence
(ﬁg. 1). The dynamic image is obtained as a ranking classi-
ﬁer that, similarly to [6, 7], sorts video frames temporally;
the difference is that we compute this classiﬁer directly at
the level of the image pixels instead of using an intermedi-
ate feature representation.

There are three keys advantages to this idea. First, the
new RGB image can be processed by a CNN architecture
which is structurally nearly identical to architectures used
for still images, while still capturing the long-term dynam-
ics in a video that relate to the long term dynamics therein.
It is then possible to use a standard CNN architecture to
learn suitable “dynamic” features from the videos. The sec-
ond advantage of this method is its remarkable efﬁciency:
the extraction of the dynamic image is extremely simple and
efﬁcient and it allows to reduce video classiﬁcation to clas-
siﬁcation of a single image by a standard CNN architecture.
The third advantage is the compression factor, as a whole
video is summarized by an amount of data equivalent to a
single frame.

Our second contribution is to provide a fast approxima-
tion of the ranker in the construction of the dynamic image.
We replace learning the ranker by simple linear operations
over the images, which is extremely efﬁcient. We also show
that, in this manner, it is possible to apply the concept of dy-
namic image to the intermediate layers of a CNN represen-
tation by constructing an efﬁcient rank pooling layer. This
layer can be incorporated into end-to-end training of a CNN
for video data.

Our third contribution is to use these ideas to propose a
novel static/dynamic neural network architecture (section 3)
which can perform end-to-end training from videos com-
bining both static appearance information from still frames,
as well as short and long term dynamics from the whole
video. We show that these technique result in efﬁcient and
accurate classiﬁcation of actions in videos, outperforming
the state-of-the-art in standard benchmarks in the area (sec-

tion 4). Our ﬁndings are summarized in section 5.

1.1. Related Work

Existing video representations can be roughly broken
into two categories. The ﬁrst category, which comprises the
majority of the literature on video processing, action and
event classiﬁcation, be it with shallow [31, 6, 20] or deep
architectures [21, 24], has viewed videos either as a stream
of still images [21] or as a short and smooth transition be-
tween similar frames [24]. Although obviously suboptimal,
considering the video as bag of static frames performs rea-
sonably well [21], as the surroundings of an action strongly
correlate with the action itself (e.g., “playing basketball”
takes place usually in a basketball court). The second cat-
egory extends CNNs to a third, temporal dimension [12]
replacing 2D ﬁlters with 3D ones. So far, this approach has
produced little beneﬁts, probably due to the lack of anno-
tated video data. Increasing the amount of annotated videos
would probably help as shown by recent 3D convolution
methods [29], although what seems especially important is
spatial consistency between frames. More speciﬁcally, a
pixel to pixel registration [24] based on the video’s opti-
cal ﬂow brings considerable improvements. Similarly, [8]
uses action tubes to to ﬁt a double stream appearance and
motion based neural network that captures the movement
of the actor.

We can also distinguish two architectural choices in the
construction of video CNNs. The ﬁrst choice is to provide
as input to the CNN a sub-video of ﬁxed length, packing a
short sequence of video frames into an array of images. The
advantage of this technique is that they allow using simple
modiﬁcations of standard CNN architectures (e.g. [15]) by
swapping 3D ﬁlters for the 2D ones. Examples of such tech-
niques include [12] and [24].

Although the aforementioned methods successfully cap-
ture the local changes within a small time window, they
cannot capture longer-term motion patterns associated with
certain actions. An alternative solution is to consider a sec-
ond family of architectures based on recurrent neural net-
works (RNNs) [4, 28]. RNNs typically consider memory
cells [11], which are sensitive to both short as well as longer
term patterns. RNNs parse the video frames sequentially
and encode the frame-level information in their memory.
In [4] LSTMs are used together with convolutional neu-
ral network activations to either output an action label or
a video description. In [28] an autoencoder-like LSTM ar-
chitecture is proposed such that either the current frame or
the next frame is accurately reconstructed. Finally, the au-
thors of [33] propose an LSTM with a temporal attention
model for densely labelling video frames.

Many of the ideas in video CNNs originated in earlier
architectures that used hand-crafted features. For exam-
ple, the authors of [18, 30, 31] have shown that local mo-

3035

tion patterns in short frame sequences can capture very well
the short temporal structures in actions. The rank pooling
idea, on which our dynamic images are based, was proposed
in [6, 7] using hand-crafted representation of the frames,
while in [5] authors increase the capacity of rank pooling
using a hierarchical approach.

Our static/dynamic CNN uses a multi-stream architec-
ture. Multiple streams have been used in a variety of dif-
ferent contexts. Examples include Siamese architectures
for learning metrics for face identiﬁcation [2] of for unsu-
pervised training of CNNs [3]. Simonyan et al. [24] use
two streams to encode respectively static frames and optical
ﬂow frames in action recognition. The authors of [19] pro-
pose a dual loss neural network was proposed, where coarse
and ﬁne outputs are jointly optimized. A difference of our
model compared to these is that we branch off two streams
at arbitrary location in the network, either at the input, at
the level of the convolutional layers, or at the level of the
fully-connected layers.

2. Dynamic Images

In this section we introduce the concept of dynamic im-
age, which is a standard RGB image that summarizes the
appearance and dynamics of a whole video sequence (sec-
tion 2.1). Then, we show how dynamic images can be
used to train dynamic-aware CNNs for action recognition
in videos (section 2.2). Finally, we propose a fast approx-
imation to accelerate the computation of dynamic images
(section 2.3).

2.1. Constructing dynamic images

While CNNs can learn automatically powerful data rep-
resentations, they can only operate within the conﬁnes of a
speciﬁc hand-crafted architecture. In designing a CNN for
video data, in particular, it is necessary to think of how the
video information should be presented to the CNN. As dis-
cussed in section 1.1, standard solutions include encoding
sub-videos of a ﬁxed duration as multi-dimensional arrays
or using recurrent architectures. Here we propose an al-
ternative and more efﬁcient approach in which the video
content is summarized by a single still image which can
then be processed by a standard CNN architecture such as
AlexNet [15].

Summarizing the video content in a single still image
may seem difﬁcult. In particular, it is not clear how image
pixels, which already contain appearance information in the
video frames, could be overloaded to reﬂect dynamic infor-
mation as well, and in particular the long-term dynamics
that are important in action recognition.

We show here that the construction of Fernando et al. [6]
can be used to obtain exactly such an image. The idea of
their paper is to represent a video as a ranking function for
Rd be
its frames I1, . . . , IT . In more detail, let ψ(It)

∈

a representation or feature vector extracted from each indi-
t
vidual frame It in the video. Let Vt = 1
τ =1 ψ(Iτ ) be
t P
time average of these features up to time t. The ranking
function associates to each time t a score S(t
,
i
Rd is a vector of parameters. The function pa-
where d
rameters d are learned so that the scores reﬂect the rank of
the frames in the video. Therefore, later times are associ-
d).
ated with larger scores, i.e. q > t =
Learning d is posed as a convex optimization problem using
the RankSVM [26] formulation:

d) > S(t

d) =

d, Vt

S(q

⇒

∈

h

|

|

|

d∗

= ρ(I1, . . . , IT ; ψ) = argmin

E(d),

d

E(d) =

d

2+
k

λ
2 k
2

T (T

1) × X
q>t

−

(1)

max

0, 1
{

−

S(q

d) + S(t

d)

.
}

|

|

The ﬁrst
term in this objective function is the usual
quadratic regularizer used in SVMs. The second term is
a hinge-loss soft-counting how many pairs q > t are incor-
rectly ranked by the scoring function. Note in particular that
a pair is considered correctly ranked only if scores are sep-
d) + 1.
arated by at least a unit margin, i.e. S(q
Optimizing eq. (1) deﬁnes a function ρ(I1, . . . , IT ; ψ)
that maps a sequence of T video frames to a single vector
d∗. Since this vector contains enough information to rank
all the frames in the video, it aggregates information from
all of them and can be used as a video descriptor. In the rest
of the paper we refer to the process of constructing d∗ from
a sequence of video frames as rank pooling.

d) > S(t

|

|

·

In [6] the map ψ(

) used in this construction is set to
be the Fisher Vector coding of a number of local features
(HOG, HOF, MBH, TRJ) extracted from individual video
frames. Here, we propose to apply rank pooling directly to
the RGB image pixels instead. While this idea is simple, in
the next several sections we will show that it has remarkable
advantages.

The ψ(It) is now an operator that stacks the RGB com-
ponents of each pixel in image It on a large vector. Alter-
natively, ψ(It) may incorporate a simple component-wise
non-linearity, such as the square root function √
(which
corresponds to using the Hellinger’s kernel in the SVM).
In all cases, the descriptor d∗ is a real vector that has the
same number of elements as a single video frame. There-
fore, d∗ can be interpreted as standard RGB image. Further-
more, since this image is obtained by rank pooling the video
frames, it summarizes information from the whole video se-
quence.

·

A few examples of dynamic images are shown in ﬁg. 1.
Several observations can be made. First, it is interesting
to note that the dynamic images tend to focus mainly on
the acting objects, such as humans or other animals such
as horses in the “horse racing” action, or objects such as

3036

training a CNN on top of such dynamic images, we implic-
itly capture the temporal patterns contained in the video.
However, since the CNN is still applied to images, we can
start from a CNN pre-trained for still image recognition,
such as AlexNet pre-trained on the ImageNet ILSVRC data,
and ﬁne-tune it on a dataset of dynamic images. Fine-tuning
allows the CNN to learn features that capture the video
dynamics without the need to train the architecture from
scratch. This is an important beneﬁt of our method because
training large CNNs require millions of data samples which
may be difﬁcult to obtain for videos.

Multiple Dynamic Images (MDI). While ﬁne-tuning
does not require as much annotated data as training a CNN
from scratch,
the domain gap between natural and dy-
namic images is sufﬁciently large that an adequately large
ﬁne-tuning dataset of dynamic images may be appropriate.
However, as noted above, in most cases there are only a few
videos available for training.

In order to address this potential limitation, in the second
scenario we propose to generate multiple dynamic images
from each video by breaking it into segments. In particu-
lar, for each video we extract multiple partially-overlapping
segments of duration τ and with stride s. In this manner, we
create multiple video segments per video, essentially mul-
tiplying the dataset size by a factor of approximately T /s,
where T is the average number of frames per video. This
can also be seen as a data augmentation step, where instead
of mirroring, cropping, or shearing images we simply take
a subset of the video frames. From each of the new video
segments, we can then compute a dynamic image to train
the CNN, using as ground truth class information of each
subsequence the class of the original video.

2.3. Fast dynamic image computation

Computing a dynamic image entails solving the opti-
mization problem of eq. (1). While this is not particularly
slow with modern solvers, in this section we propose an ap-
proximation to rank pooling which is much faster and works
as well in practice. Later, this technique, which we call
approximate rank pooling, will be critical in incorporating
rank pooling in intermediate layers of a deep CNN and to
allow back-prop training through it.

The derivation of approximate rank pooling is based on
the idea of considering the ﬁrst step in a gradient-based op-
timization of eq. (1). Starting with d = ~0, the ﬁrst ap-
proximated solution obtained by gradient descent is d∗ =
~0

E(d)

E(d)

η

−

∇
E(~0)

∇

|d=~0 ∝ −∇
max

∝ X

q>t ∇

0, 1

{

|d=~0 for any η > 0, where
d) + S(t
}|d=~0

S(q

d)

−

|

|

Figure 2: Left column: dynamic images. Right col-
umn: motion blur. Although fundamentally different both
methodologically, as well as in terms of applications, they
both seem to capture time in a similar manner.

drums in the “drumming” action. On the contrary, back-
ground pixels and background motion patterns tend to be
averaged away. Hence, the pixels in the dynamic image ap-
pear to focus on the identity and motion of the salient actors
in videos, indicating that they may contain the information
necessary to perform action recognition.

Second, we observe that dynamic images behave dif-
ferently for actions of different speeds. For slow actions,
like “blowing hair dry” in the ﬁrst row of ﬁg. 1, the motion
seems to be dragged over many frames. For faster actions,
such as “golf swing” in the second row of ﬁg. 1, the dynamic
image reﬂects key steps in the action such as preparing to
swing and stopping after swinging. For longer term actions
such as “horse riding” in the third row of ﬁg. 1, the dynamic
image reﬂects different parts of the video; for instance, the
rails that appear as a secondary motion contributor are su-
perimposed on top of the horses and the jockeys who are
the main actors. Such observations were also made in [7].

Last, it is interesting to note that dynamic images are
reminiscent of some other imaging effects that convey mo-
tion and time, such as motion blur or panning, an analogy is
illustrated in ﬁg. 2. While motion blur captures the time and
motion by integrating over subsequent pixel intensities de-
ﬁned by the camera shutter speed, dynamic images capture
the time by integrating and reordering the pixel intensities
over time within a video.

2.2. Using dynamic images

Given that the dynamic images are in the format of stan-
dard RGB images, they can be used to apply any method
for still image analysis, and in particular any state-of-the-art
CNN, to the analysis of video. In particular, we experiment
with two usage scenarios.

Single Dynamic Image (SDI).
In the ﬁrst scenario, a dy-
namic image summarizes an entire video sequence. By

=

X
q>t ∇h

d, Vt

Vq

=

−

i

Vt

Vq.

−

X
q>t

3037

,

 using time-averaged vectors V

t

t

t

,

 using A

 directly

t

t

,

15

10

5

0

-5

-10

-15

2

4

8

10

6
time

Figure 3: The graph compares the approximated rank pool-
ing weighting functions αt (for T = 11 samples) of eq. (2)
using time-averaged feature frames Vt to the variant eq. (4)
that ranks directly the feature frames ψt as is.

We can further expand d∗ as follows

d∗

Vq

Vt =

−

∝ X
q>t

1
q





X
q>t

q

ψi

−

X
i=1

1
t

t

X
j=1





T

X
t=1

ψj

=

αtψt

where the coefﬁcients αt are given by

αt = 2(T

t + 1)

(T + 1)(HT

Ht−1),

−
t
i=1 1/t is the t-th Harmonic number and

where Ht =
H0 = 0. Hence the rank pooling operator reduces to

P

−

−

(2)

ˆρ(I1, . . . , IT ; ψ) =

αtψ(It).

(3)

T

X
t=1

In
which is a weighted combination of the data points.
particular, the dynamic image computation reduces to ac-
cumulating the video frames after pre-multiplying them by
αt. The function αt, however, is non-trivial, as illustrated
in ﬁg. 3.

An alternative construction of the rank pooler does
not compute the intermediate average features Vt =
T
q=1 ψ(Iq), but uses directly individual video fea-
(1/t)
tures ψ(It) in the deﬁnition of the ranking scores (1). In
this case, the derivation above results in a weighting func-
tion of the type

P

αt = 2t

T

1

(4)

−
which is linear in t. The two scoring functions eq. (2) and
eq. (4) are compared in ﬁg. 3 and in the experiments.

−

3. Dynamic Maps Networks

In the previous section we have introduced the concept
of dynamic image as a method to pool the information con-
tained in a number of video frames in a single RGB image.

Figure 4: Dynamic image and dynamic map networks on
the left and the right pictures respectively, after applying a
rank pooling operation on top of the previous layer activa-
tions.

Here, we notice that every layer of a CNN produces as out-
put a feature map which, having a spatial structure similar
to an image, can be used in place of video frames in this
construction. We call the result of applying rank pooling
to such features a dynamic feature map, or dynamic map in
short. In the rest of the section we explain how to incor-
porate this construction as a rank-pooling layer in a CNN
(section 3.1) and how to accelerate it signiﬁcantly and per-
form back-propagation by using approximate rank pooling
(section 3.2).

3.1. Dynamic maps

The structure of a dynamic map network is illustrated
in ﬁg. 4. In the case seen so far (left in ﬁg. 4), rank pool-
ing is applied at the level of the input RGB video frames,
which we could think of as layer zero in the architecture.
We call the latter a dynamic image network. By contrast,
a dynamic map network moves rank pooling higher in the
hierarchy, by applying one or more layers of feature com-
putations to the individual feature frames and applying the
same construction to the resulting feature maps.

, . . . , a(l−1)

In particular, let a(l−1)

denote the feature
1
maps computed at the l
1 layers of the architecture, one for
each of the T video frames. Then, we use the rank pooling
equation (1) to aggregate these maps into a single dynamic
map,

−

T

a(l) = ρ(a(l−1)

, . . . , a(l−1)

).

T

1

(5)

Note that, compared to eq. (1), we dropped the term ψ;
since networks are already learning feature maps, we set
this term to the identity function. The dynamic image
network is obtained by setting l = 1 in this construction.

Rank pooling layer (RankPool) & backpropagation. In
order to train a CNN with rank pooling as an intermediate
layer, it is necessary to compute the derivatives of eq. (5) for
the backpropagation step. We can rewrite eq. (5) as a linear

3038

combination of the input data V1, . . . , VT , namely

a(l) =

βt(V1, . . . , VT )Vt

(6)

T

X
t=1

In turn, Vt is the temporal average of the input features and
is therefore a linear function Vt(a(l−1)
). Sub-
stituting, we can rewrite a(l) as

, . . . , a(l−1)

1

t

a(l) =

αt(a(l−1)

1

, . . . , a(l−1)

)a(l−1)

.

t

T

(7)

T

X
t=1

Unfortunately, we observe that due to the non-linear nature
of the optimization problem of equation (1), the coefﬁcients
βt, αt depend on the data a(l−1)
themselves. Computing
t
the gradient of a(l) with respect to the per frame data points
a(l−1)
is a challenging derivation. Hence, using dynamic
t
maps and rank pooling directly as a layer in a CNN is not
straightforward.

We note that the rank pooling layer (RankPool) consti-
tutes a new type of portable convolutional network layer,
just like a max-pooling or a ReLU layer.
It can be used
whenever dynamic information must be pooled across time.

3.2. Approximate dynamic maps.

Constructing the precise dynamic maps, or images, is in
theory optimal, but not necessarily practical. On one hand
computing the precise dynamic maps via an optimization
is computationally inefﬁcient. This is especially important
in the context of CNNs, where efﬁcient computations are
extremely important for training on large datasets, and the
optimization of eq. (5) would be slow compared to other
components of the network. On the other hand, computing
the gradients would be non trivial.

To this end we replace once again rank pooling with ap-
proximate rank pooling. With the approximate rank pool-
ing we signiﬁcantly accelerate the computations, even by
a factor of 45 as we show later in the experiments. Sec-
ondly, and more importantly, the approximate rank pooling
is also a linear combination of frames, where the per frame
coefﬁcients are given by eq. (2). These coefﬁcients are in-
dependent of the frame features Vt and ψ(It). Hence, the
derivative of the approximate rank pooling is much simpler
and can be easily computed as the vectorized coefﬁcients of
eq. (2), namely

We conclude that using approximate rank pooling in the
context of CNNs is not only practical, but also necessary for
the optimization through backpropagation.

4. Experiments

4.1. Datasets

We explore the proposed models on two state-of-the-
art datasets used for evaluating neural network based
models for action recognition, namely UCF101 [27] and
HMDB51 [16].

UCF101. The UCF101 dataset [27] comprises of 101
human action categories, like “Apply Eye Makeup” and
“Rock Climbing” and spans over 13, 320 videos. The
videos are realistic and relatively clean. They contain
little background clutter and contain a single action.
thus almost all frames
Also the videos are trimmed,
relate to the action in the video. The standard evaluation
is average accuracy over three parts provided by the authors.

HMDB51. The HMDB51 dataset [16] comprises of 51 hu-
man action categories , such as “backhand ﬂip” and “swing
baseball bat” and spans over 6, 766 videos. The videos are
realistic, downloaded from Youtube contain a single action.
The dataset is split in three parts and accuracy is averaged
over all three parts, similar to UCF101.

4.2. Implementation details

·

·

To maintain the same function domain and range we se-
) the square rooting kernel
lect for non-linear operations ψ(
and time varying mean vectors [6]. We generate
maps √
dynamic images for each color channel separately and then
merge them so that they can be directly used directly as in-
put to a CNN. As the initial dynamic images are not in the
natural range of [0, 255] for RGB data, we apply minmax
normalization. We use BVLC reference CaffeNet model
[13] trained on ImageNet images as a starting point to train
our dynamic image networks. We ﬁne-tune all the layers
with the learning rate to be 10−3 and gradually decrease it
per epoch. We use a maximum of 20 epoch during training.
Sharing code, data, models. We share our code, mod-
els and data 2. Furthermore, we have computed the dy-
namic images of the Sports1M dataset [14], and share the
Alexnet and VGGnet dynamic image networks trained on
the Sports1M.

∂ vec a(l)
∂(vec a(l−1)

t

)⊤

= αtI

where I is the identity matrix. Interestingly, we would ob-
tain the same expression for the derivative if αt in eq. (7)
would be constant and did not depend on the video frames.

4.3. Mean, max and dynamic images

(8)

First, we compare “single image per video”, namely the
proposed Single Dynamic Image (SDI) with the per video
sequence mean and max image. For all methods we ﬁrst

2https://github.com/hbilen/dynamic-image-nets

3039

Method
Mean Image
Max Image
SDI

SPLIT1
52.6
48.0
57.2

SPLIT2
53.4
46.0
58.7

51.7
42.3
57.7

52.6
45.4
57.9

SPLIT3 AVERAGE

Table 1: Comparing several video representative image
models using UCF101

Method
Appr. Rank Pooling
Rank Pooling

Speed
5920 fps
131 fps

Accuracy
96.5 ± 0.9
99.5 ± 0.1

Table 2: Approximate rank pooling vs rank pooling.

compute the single images per video ofﬂine, and for SDI
speciﬁcally we use SVR [26]. Then we train and test on
action recognition using CaffeNet network. Results are re-
ported in Table 1.

From all representations we observe that SDI achieves
the highest accuracy. We conclude that SDI model is a bet-
ter single image model than the mean and max image mod-
els.

4.4. Approximate Rank Pooling vs Rank Pooling

Next, we compare the approximate rank pooling and
rank pooling in terms of speed (frames per second) and pair-
wise ranking accuracy, which is the common measure for
evaluating learning-to-rank methods. We train on a subset
of 10 videos that contain different actions and evaluate on a
new set of 10 videos with the same type of actions respec-
tively. We report results with the mean and the standard
deviations in Table 2.

×

We observe that approximate rank pooling is 45

faster
than rank pooling, while obtaining similar ranking perfor-
mance. Further, in Figure 5 we plot the score distributions
for rank pooling and approximate rank pooling. We observe
that their score proﬁles are also similar. We conclude that
approximate rank pooling is a good approximation to rank
pooling, while being two magnitudes faster as it involves no
optimization.

4.5. Evaluating the effect of end-to-end training

Next, we evaluate in Table 3 rank pooling dynamic im-
ages with and without end-to-end training. We also eval-
uate rank pooling with dynamic maps. The ﬁrst method
generates multiple dynamic images on the RGB pixels as
earlier. These dynamic images can be computed ofﬂine,
then we train a network from end to end. The second
method passes these dynamic images through the network,
computes the fc6 activations using a pre-trained Alexnet
and aggregates them with max pooling, then trains SVM
classiﬁers per action class. The third method considers a
RankPool layer after the conv1 to generate multiple dy-

Appr. rank pooling
Rank pooling

100

200

300

400

500

600

700

800

900

50

100

150

200

250

300

350

400

450

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0

0

50

100

150

200

250

300

350

400

450

50

100

150

200

250

300

350

400

450

Figure 5: Comparison between score proﬁle of ranking
functions for approximate rank pooling and rank pooling.
Generally the approximate rank pooling follows the trend
of rank pooling.

Method RankPool Layer

Training

HMDB51 UCF101

MDI
MDI
MDM

After images
fc6+max pool
After conv1

End-to-end
SVM
End-to-end

35.8
32.3
–

70.9
68.6
67.1

Table 3: Evaluating the effect of end-to-end training for
multiple dynamic images and multiple dynamic maps after
the convolutional layer 1.

namic maps (MDM) based on approximate rank pooling.
To generate the multiple dynamic images or maps we use a
window size of 25 and a stride of 20 which allows for 80%
overlap.

We observe that compared to a uniﬁed, end-to-end train-
ing is beneﬁcial, bringing 2-3% accuracy improvements
depending on the dataset. Furthermore, approximate dy-
namic maps computed after the convolutional layer 1 per-
form slighly below the dynamic images (dynamic maps
computed after layer 0). We conclude that multiple dy-
namic images are better to be pooled on top of the static
RGB frames. Furthermore, multiple dynamic images per-
form better when employed in end-to-end training.

4.6. Combining dynamic images with static images

Next, we evaluate how complementary dynamic image
networks and static RGB frame networks are. For both net-
works we apply max pooling on the per video activations at
pool5 layer. Results are reported in Table 4.

As expected, we observe that static appearance informa-
tion appears to be equally important to the dynamic appear-
ance in the context of convolutional neural networks. A
combination of the two, however, brings a noticeable 6%
increase in accuracy. We conclude that the two representa-
tions are complementary to each other.

3040

HMDB51 UCF101

Method
This paper
Zha et al. [34]
Simonyan et al. [24]
Yue-Hei-Ng et al. [21]
Wu et al. [32]
Fernando et al. [6]
Hoai et al. [10]
Lan et al. [17]
Peng et al. [22]

p
e
e
d

w
o
l
l
a
h
s

65.2
–
59.4
–
56.4
63.7
60.8
65.4
66.8

89.1
89.6
88.0
88.6
84.2
–
–
89.1
–

Table 7: Comparison with the state-of-the-art. Despite be-
ing a relatively simple representation, the proposed method
is able to obtain results on par with the state state-of-the-art.

tain their accuracies after combining their methods with im-
proved trajectories [31] for optimal results.

Considering deep learning methods, our method per-
forms on par and is only outperformed from [34].
[34]
makes use of the very deep VGGnet [25], which is a more
competitive network than that the Alexnet architecture we
rely on. Hence a direct comparison is not possible. Com-
pared to the shallow methods the proposed method is also
competitive. We anticipate that combining the proposed
dynamic images with sophisticated encodings [17, 22] will
beneﬁt the accuracies further.

We conclude that while being in the context of CNNs a
simple and efﬁcient video representation, dynamic images
allow for state-of-the-art accuracy in action recognition.

5. Conclusion

We present dynamic images, a powerful and new, yet
simple video representation in the context of deep learn-
ing that summarizes videos into single images. As such,
dynamic images are directly compative to existing CNN
architectures allowing for end-to-end action recognition
learning. Extending dynamic images to the hierarchical
CNN feature maps, we introduce a novel temporal pooling
layer, Approximate-RankPool directly. Experiments
on state-of-the-art action recognition datasets demonstrate
the descriptive power of dynamic images, despite their con-
ceptual simplicity. A visual inspection outlines the richness
of dynamic images in describing complex motion patterns
as simple 2d images.

Acknowledgments: This work acknowledges the support of
the EPSRC grant EP/L024683/1, the ERC Starting Grant IDIU and
the Australian Research Council Centre of Excellence for Robotic
Vision (project number CE140100016).

Method
Static RGB
MDI-end-to-end
MDI-end-to-end + static-rgb

HMDB51 UCF101

36.7
35.8
42.8

70.1
70.9
76.9

Table 4: Evaluating complementarity of dynamic images
with static images.

Classes
SoccerJuggling
CleanAndJerk
PullUps
PushUps
PizzaTossing

Classes

Diff.
+38.5 CricketShot
+36.4 Drumming
+32.1
+26.7
+25.0

PlayingPiano
PlayingFlute
Fencing

Diff.
-47.9
-25.6
-22.0
-21.4
-18.2

Table 5: Class by class comparison between RGB and MDI
networks, where the difference in scores using MDI and
RGB are reported. A positive difference is better for MDI,
a negative difference better for RGB

Method
Trajectories [31]
MDI-end-to-end + static-rgb+trj

HMDB51 UCF101

60.0
65.2

86.0
89.1

Table 6: Combining with trajectory features brings a notice-
able increase in accuracy.

4.7. Further analysis

We, furthermore, perform a per analysis between static
rgb networks and MDI based networks. We list in Ta-
ble 5 the top 5 classes based on the relative performances
for each method. MDI performs better for “PullUps” and
“PushUps”, where motion is dominant and discriminating
between motion patterns is important. RGB static models
seems to work better on classes such as “CricketShot” and
“Drumming”, where context is already quite revealing. We
conclude that dynamic images are useful for actions where
there exist characteristic motion patterns and dynamics.

Furthermore, we investigate whether dynamic images
are complementary to state-of-the-art features,
like im-
proved trajectories [31], relying on late fusion. Results are
reported in Table 6. We obtain a signiﬁcant improvement
of 5.2% over trajectory features alone on HMDB51 dataset
and 3.1% on UCF101 dataset.

Due to the lack of space we refer to the supplementary
material for a more in depth analysis of dynamic images and
dynamic maps and their learning behavior.

4.8. State-of-the-art comparisons

Last, we compare with the state-of-the-art techniques in
UCF101 and HMDB51 in Table 7, where we make a dis-
tinction between deep and shallow architectures. Note that
similar to us, almost all methods, be it shallow or deep, ob-

3041

[20] M. Mazloom, E. Gavves, and C. G. M. Snoek. Conceptlets:
Selective semantics for classifying video events. IEEE TMM,
December 2014.

[21] J. Y. Ng, M. J. Hausknecht, S. Vijayanarasimhan, O. Vinyals,
R. Monga, and G. Toderici. Beyond short snippets: Deep
networks for video classiﬁcation. In CVPR, 2015.

[22] X. Peng, C. Zou, Y. Qiao, and Q. Peng. Action recognition

with stacked ﬁsher vectors. In ECCV, 2014.

[23] M. S. Ryoo, B. Rothrock, and L. Matthies. Pooled motion

features for ﬁrst-person videos. In CVPR, 2015.

[24] K. Simonyan and A. Zisserman.

Two-stream convolu-
tional networks for action recognition in videos. CoRR,
abs/1406.2199:1–8, 2014.

[25] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[26] A. J. Smola and B. Sch¨olkopf. A tutorial on support vector
regression. Statistics and computing, 14:199–222, 2004.
[27] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset of
101 human actions classes from videos in the wild. CoRR,
2012.

[28] N. Srivastava, E. Mansimov, and R. Salakhudinov. Unsuper-
vised learning of video representations using lstms. In ICML,
2015.

[29] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. arXiv preprint arXiv:1412.0767, 2014.

[30] H. Wang, A. Kl¨aser, C. Schmid, and C.-L. Liu. Dense tra-
jectories and motion boundary descriptors for action recog-
nition. IJCV, 103:60–79, 2013.

[31] H. Wang and C. Schmid. Action recognition with improved

trajectories. In ICCV, 2013.

[32] J. Wu, Y. Zhang, and W. Lin. Towards good practices for

action video encoding. In CVPR, 2014.

[33] S. Yeung, O. Russakovsky, N. Jin, M. Andriluka, G. Mori,
and L. Fei-Fei. Every moment counts: Dense detailed label-
ing of actions in complex videos. ArXiv e-prints, 2015.
[34] S. Zha, F. Luisier, W. Andrews, N. Srivastava, and
R. Salakhutdinov. Exploiting Image-trained CNN Archi-
tectures for Unconstrained Video Classiﬁcation. In BMVC,
2015.

References

[1] A. F. Bobick and J. W. Davis. The recognition of hu-
man movement using temporal templates. Pattern Analysis
and Machine Intelligence, IEEE Transactions on, 23(3):257–
267, 2001.

[2] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity
metric discriminatively, with application to face veriﬁcation.
In CVPR, 2005.

[3] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised vi-
sual representation learning by context prediction. In CVPR,
2015.

[4] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach,
S. Venugopalan, K. Saenko, and T. Darrell. Long-term recur-
rent convolutional networks for visual recognition and de-
scription. In arXiv preprint arXiv:1411.4389, 2014.

[5] B. Fernando, P. Anderson, M. Hutter, and S. Gould. Discrim-
inative hierarchical rank pooling for activity recognition. In
CVPR, 2016.

[6] B. Fernando, E. Gavves, J. Oramas, A. Ghodrati, and
T. Tuytelaars. Modeling video evolution for action recog-
nition. In CVPR, 2015.

[7] B. Fernando, E. Gavves, J. Oramas, A. Ghodrati, and
T. Tuytelaars. Rank pooling for action recognition. TPAMI,
2016.

[8] G. Gkioxari and J. Malik. Finding action tubes. In CVPR,

June 2015.

[9] M. Hoai and A. Zisserman. Improving human action recog-
nition using score distribution and ranking. In Proceedings
of Asian Conference on Computer Vision, 2014.

[10] M. Hoai and A. Zisserman. Improving human action recog-
nition using score distribution and ranking. In ACCV, 2014.
[11] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997.

[12] S. Ji, W. Xu, M. Yang, and K. Yu. 3d convolutional neural
networks for human action recognition. TPAMI, 2013.
[13] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. arXiv preprint
arXiv:1408.5093, 2014.

[14] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In CVPR, 2014.
[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012.

Imagenet
In

[16] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre.
Hmdb: a large video database for human motion recognition.
In ICCV, 2011.

[17] Z.-Z. Lan, M. Lin, X. Li, A. G. Hauptmann, and B. Raj.
Beyond gaussian pyramid: Multi-skip feature stacking for
action recognition. In CVPR, 2015.

[18] I. Laptev. On space-time interest points. IJCV, 64:107–123,

2005.

[19] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional

networks for semantic segmentation. CVPR, 2015.

3042

