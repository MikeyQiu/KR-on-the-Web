Investigating Capsule Networks with Dynamic Routing for
Text Classiﬁcation

∗
Wei Zhao1,2, Jianbo Ye3, Min Yang1
, Zeyang Lei4, Soufei Zhang5, Zhou Zhao6

1 Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences

2 Tencent
3 Pennsylvania State University
4 Graduate School at Shenzhen, Tsinghua University

5 Nanjing University of Posts and Telecommunications

6 Zhejiang University

Abstract

In this study, we explore capsule networks
with dynamic routing for text classiﬁca-
tion. We propose three strategies to sta-
bilize the dynamic routing process to al-
leviate the disturbance of some noise cap-
sules which may contain “background” in-
formation or have not been successfully
trained. A series of experiments are con-
ducted with capsule networks on six text
classiﬁcation benchmarks. Capsule net-
works achieve competitive results over the
compared baseline methods on 4 out of
6 datasets, which shows the effectiveness
of capsule networks for text classiﬁca-
tion. We additionally show that capsule
networks exhibit signiﬁcant improvement
when transfer single-label to multi-label
text classiﬁcation over the competitors. To
the best of our knowledge, this is the ﬁrst
work that capsule networks have been em-
pirically investigated for text modeling1.

1

Introduction

Modeling articles or sentences computationally is
a fundamental topic in natural language process-
ing.
It could be as simple as a keyword/phrase
matching problem, but it could also be a nontrivial
problem if compositions, hierarchies, and struc-
tures of texts are considered. For example, a news
article which mentions a single phrase “US elec-
tion” may be categorized into the political news
with high probability. But it could be very difﬁ-
cult for a computer to predict which presidential
candidate is favored by its author, or whether the

∗ Corresponding author (min.yang@siat.ac.cn)
1Codes

available

publicly

are

at:

https:

//github.com/andyweizhao/capsule_text_
classification.

author’s view in the article is more liberal or more
conservative.

Earlier efforts in modeling texts have achieved
limited success on text categorization using a sim-
ple bag-of-words classiﬁer (Joachims, 1998; Mc-
Callum et al., 1998), implying understanding the
meaning of the individual word or n-gram is a
necessary step towards more sophisticated mod-
els. It is therefore not a surprise that distributed
representations of words, a.k.a. word embeddings,
have received great attention from NLP commu-
nity addressing the question “what” to be modeled
at the basic level (Mikolov et al., 2013; Penning-
ton et al., 2014). In order to model higher level
concepts and facts in texts, an NLP researcher has
to think cautiously the so-called “what” question:
what is actually modeled beyond word meanings.
A common approach to the question is to treat
the texts as sequences and focus on their spatial
patterns, whose representatives include convolu-
tional neural networks (CNNs) (Kim, 2014; Zhang
et al., 2015; Conneau et al., 2017) and long short-
term memory networks (LSTMs) (Tai et al., 2015;
Mousa and Schuller, 2017). Another common ap-
proach is to completely ignore the order of words
but focus on their compositions as a collection,
whose representatives include probabilistic topic
modeling (Blei et al., 2003; Mcauliffe and Blei,
2008) and Earth Mover’s Distance based model-
ing (Kusner et al., 2015; Ye et al., 2017).

Those two approaches, albeit quite different
from the computational perspective, actually fol-
low a common measure to be diagnosed regarding
In neural
their answers to the “what” question.
network approaches, spatial patterns aggregated at
lower levels contribute to representing higher level
concepts. Here, they form a recursive process to
articulate what to be modeled. For example, CNN
builds convolutional feature detectors to extract lo-
cal patterns from a window of vector sequences

8
1
0
2
 
p
e
S
 
3
 
 
]
L
C
.
s
c
[
 
 
4
v
8
3
5
0
0
.
4
0
8
1
:
v
i
X
r
a

and uses max-pooling to select the most promi-
nent ones. It then hierarchically builds such pat-
tern extraction pipelines at multiple levels. Being
a spatially sensitive model, CNN pays a price for
the inefﬁciency of replicating feature detectors on
a grid. As argued in (Sabour et al., 2017), one
has to choose between replicating detectors whose
size grows exponentially with the number of di-
mensions, or increasing the volume of the labeled
training set in a similar exponential way. On the
other hand, methods that are spatially insensitive
are perfectly efﬁcient at the inference time regard-
less of any order of words or local patterns. How-
ever, they are unavoidably more restricted to en-
code rich structures presented in a sequence. Im-
proving the efﬁciency to encode spatial patterns
while keeping the ﬂexibility of their representation
capability is thus a central issue.

A recent method called capsule network intro-
duced by Sabour et al. (2017) possesses this at-
tractive potential to address the aforementioned is-
sue. They introduce an iterative routing process to
decide the credit attribution between nodes from
lower and higher layers. A metaphor (also as an
argument) they made is that human visual system
intelligently assigns parts to wholes at the infer-
ence time without hard-coding patterns to be per-
spective relevant. As an outcome, their model
could encode the intrinsic spatial relationship be-
tween a part and a whole constituting viewpoint
invariant knowledge that automatically general-
izes to novel viewpoints. In our work, we follow
a similar spirit to use this technique in modeling
texts. Three strategies are proposed to stabilize
the dynamic routing process to alleviate the distur-
bance of some noise capsules which may contain
“background” information such as stop words and
the words that are unrelated to speciﬁc categories.
We conduct a series of experiments with capsule
networks on top of the pre-trained word vectors
for six text classiﬁcation benchmarks. More im-
portantly, we show that capsule networks achieves
signiﬁcant improvement when transferring single-
label to multi-label text classiﬁcations over strong
baseline methods.

2 Our Model

Our capsule network, depicted in Figure 1,
is
a variant of the capsule networks proposed in
Sabour et al. (2017). It consists of four layers: n-
gram convolutional layer, primary capsule layer,

convolutional capsule layer, and fully connected
capsule layer. In addition, we explore two capsule
frameworks to integrate these four components in
different ways. In the rest of this section, we elab-
orate the key components in detail.

2.1 N -gram Convolutional Layer

This layer is a standard convolutional layer which
extracts n-gram features at different positions of a
sentence through various convolutional ﬁlters.

Suppose x ∈ RL×V denotes the input sentence
representation where L is the length of the sen-
tence and V is the embedding size of words. Let
xi ∈ RV be the V -dimensional word vector cor-
responding to the i-th word in the sentence. Let
W a ∈ RK1×V be the ﬁlter for the convolution op-
eration, where K1 is the N -gram size while sliding
over a sentence for the purpose of detecting fea-
tures at different positions. A ﬁlter W a convolves
with the word-window xi:i+K1−1 at each possible
position (with stride of 1) to produce a column fea-
ture map ma ∈ RL−K1+1, each element ma
i ∈ R
of the feature map is produced by

ma

i = f (xi:i+K1−1 ◦ W a + b0)

(1)

where ◦ is element-wise multiplication, b0 is a
bias term, and f is a nonlinear activate function
(i.e., ReLU). We have described the process by
which one feature is extracted from one ﬁlter.
Hence, for a = 1, . . . , B, totally B ﬁlters with
the same N -gram size, one can generate B feature
maps which can be rearranged as

M = [m1, m2, ..., mB] ∈ R(L−K1+1)×B

(2)

2.2 Primary Capsule Layer

This is the ﬁrst capsule layer in which the cap-
sules replace the scalar-output feature detectors of
CNNs with vector-output capsules to preserve the
instantiated parameters such as the local order of
words and semantic representations of words.

Suppose pi ∈ Rd denotes the instantiated pa-
rameters of a capsule, where d is the dimension of
the capsule. Let W b ∈ RB×d be the ﬁlter shared in
different sliding windows. For each matrix multi-
plication, we have a window sliding over each N -
gram vector denoted as Mi ∈ RB, then the corre-
sponding N -gram phrases in the form of capsule
are produced with pi = (W b)T Mi.

The ﬁlter W b multiplies each N -gram vector
with stride of 1 to produce a

in {Mi}L−K1+1

i=1

Figure 1: The Architecture of Capsule network for text classiﬁcation. The processes of dynamic routing
between consecutive layers are shown in the bottom.

column-list of capsules p ∈ R(L−K1+1)×d, each
capsule pi ∈ Rd in the column-list is computed as

pi = g(W bMi + b1)

(3)

where g is nonlinear squash function through the
entire vector, b1 is the capsule bias term. For all
C ﬁlters, the generated capsule feature maps can
be rearranged as

P = [p1, p2, ..., pC] ∈ R(L−K1+1)×C×d,

(4)

where totally (L − K1 + 1) × C d-dimensional
vectors are collected as capsules in P.

2.2.1 Child-Parent Relationships
As argued in (Sabour et al., 2017), capsule net-
work tries to address the representational limita-
tion and exponential inefﬁciencies of convolutions
It allows the net-
with transformation matrices.
works to automatically learn child-parent (or part-
whole) relationships. In text classiﬁcation tasks,
different sentences with the same category are sup-
posed to have the similar topic but with different
viewpoints.

In this paper, we explore two different types
of transformation matrices to generate prediction
vector (vote) ˆuj|i ∈ Rd from its child capsule i to
the parent capsule j. The ﬁrst one shares weights
W t1 ∈ RN ×d×d across child capsules in the layer
below, where N is the number of parent capsules
in the layer above. Formally, each corresponding
vote can be computed by:

ˆuj|i = W t1

j ui + ˆbj|i ∈ Rd

(5)

where ui is a child-capsule in the layer below and
ˆbj|i is the capsule bias term.

In the second design, we replace the shared
weight matrix W t1
j with non-shared weight ma-
trix W t2
i,j, where the weight matrices W t2 ∈
RH×N ×d×d and H is the number of child capsules
in the layer below.

2.3 Dynamic Routing

The basic idea of dynamic routing is to construct
a non-linear map in an iterative manner ensuring
that the output of each capsule gets sent to an ap-
propriate parent in the subsequent layer:

ˆuj|i ∈ Rd(cid:111)
(cid:110)

i=1,...,H,j=1...,N

vj ∈ Rd(cid:111)N
(cid:110)

(cid:55)→

.

j=1

For each potential parent, the capsule network can
increase or decrease the connection strength by
dynamic routing, which is more effective than the
primitive routing strategies such as max-pooling in
CNN that essentially detects whether a feature is
present in any position of the text, but loses spatial
information about the feature. We explore three
strategies to boost the accuracy of routing process
by alleviating the disturbance of some noisy cap-
sules:

Orphan Category
Inspired by Sabour et al.
(2017), an additional “orphan” category is added
to the network, which can capture the “back-
ground” information of the text such as stop words
and the words that are unrelated to speciﬁc cat-
egories, helping the capsule network model the
child-parent relationship more efﬁciently. Adding
“orphan” category in the text is more effective than
in image since there is no single consistent “back-
ground” object in images, while the stop words are

consistent in texts such as predicate “s”, “am” and
pronouns “his”, “she”.

Leaky-Softmax We
explore Leaky-Softmax
Sabour et al. (2017) in the place of standard soft-
max while updating connection strength between
the children capsules and their parents. Despite
the orphan category in the last capsule layer, we
also need a light-weight method between two
consecutive layers to route the noise child cap-
sules to extra dimension without any additional
parameters and computation consuming.

Coefﬁcients Amendment We also attempt to
use the probability of existence of child capsules
in the layer below to iteratively amend the con-
nection strength as Eq.6.

Algorithm 1: Dynamic Routing Algorithm
1 procedure ROUTING(ˆuj|i, ˆaj|i, r, l)
2 Initialize the logits of coupling coefﬁcients

bj|i = 0

3 for r iterations do

4

5

6

for all capsule i in layer l and capsule j in
layer l + 1:
cj|i = ˆaj|i · leaky-softmax(bj|i)
for all capsule j in layer l + 1:
vj = g((cid:80)
i cj|i ˆuj|i), aj = |vj|
for all capsule i in layer l and capsule j in
layer l + 1: bj|i = bj|i + ˆuj|i · vj

7 return vj,aj

Given each prediction vector ˆuj|i and its prob-
ability of existence ˆaj|i, where ˆaj|i = ˆai, each it-
erative coupling coefﬁcient of connection strength
cj|i is updated by

where bj|i is the logits of coupling coefﬁcients.
Each parent capsule vj in the layer above is a
weighted sum over all prediction vectors ˆuj|i:

vj = g(

cj|i ˆuj|i), aj = |vj|

(7)

(cid:88)

i

where aj is the probabilities of parent capsules, g
is nonlinear squash function Sabour et al. (2017)
through the entire vector. Once all of the parent
capsules are produced, each coupling coefﬁcient
bj|i is updated by:

bj|i = bj|i + ˆuj|i · vj

For simplicity of notation, the parent capsules and
their probabilities in the layer above are denoted
as

v, a = Routing(ˆu)

(9)

where ˆu denotes all of the child capsules in the
layer below, v denotes all of the parent-capsules
and their probabilities a.

Our dynamic routing algorithm is summarized

in Algorithm 1.

2.4 Convolutional Capsule Layer

In this layer, each capsule is connected only to
a local region K2 × C spatially in the layer be-
low. Those capsules in the region multiply trans-
formation matrices to learn child-parent relation-
ships followed by routing by agreement to produce
parent capsules in the layer above.

Suppose W c1 ∈ RD×d×d and W c2 ∈
RK2×C×D×d×d denote shared and non-shared
weights, respectively, where K2 · C is the number
of child capsules in a local region in the layer be-
low, D is the number of parent capsules which the
child capsules are sent to. When the transforma-
tion matrices are shared across the child capsules,
each potential parent-capsule ˆuj|i is produced by

ˆuj|i = W c1

j ui + ˆbj|i

(10)

where ˆbj|i is the capsule bias term, ui is a child
capsule in a local region K2 × C and W c1
is the
j
jth matrix in tensor W c1. Then, we use routing-
by-agreement to produce parent capsules feature
maps totally (L−K1 −K2 +2)×D d-dimensional
capsules in this layer. When using the non-shared
weights across the child capsules, we replace the
transformation matrix W c1
in Eq. (10) with W c2
j .
j

The capsules in the layer below are ﬂattened into
a list of capsules and fed into fully connected
capsule layer in which capsules are multiplied by
transformation matrix W d1 ∈ RE×d×d or W d2 ∈
RH×E×d×d followed by routing-by-agreement to
produce ﬁnal capsule vj ∈ Rd and its probability
aj ∈ R for each category. Here, H is the number
of child capsules in the layer below, E is the num-
ber of categories plus an extra orphan category.

2.6 The Architectures of Capsule Network

We explore two capsule architectures (denoted as
Capsule-A and Capsule-B) to integrate these four

(8)

cj|i = ˆaj|i · leaky-softmax(bj|i)

(6)

2.5 Fully Connected Capsule Layer

marks including: movie reviews (MR) (Pang and
Lee, 2005), Stanford Sentiment Treebankan exten-
sion of MR (SST-2) (Socher et al., 2013), Subjec-
tivity dataset (Subj) (Pang and Lee, 2004), TREC
question dataset (TREC) (Li and Roth, 2002), cus-
tomer review (CR) (Hu and Liu, 2004), and AG’s
news corpus (Conneau et al., 2017). These bench-
marks cover several text classiﬁcation tasks such
as sentiment classiﬁcation, question categoriza-
tion, news categorization. The detailed statistics
are presented in Table 1.

Dataset

Train Dev

Classes

Classiﬁcation Task

MR
SST-2
Subj
TREC
CR
AG’s news

8.6k
8.6k
8.1k
5.4k
3.1k
108k

0.9k
0.9k
0.9k
0.5k
0.3k
12.0k

Test

1.1k
1.8k
1.0k
0.5k
0.4k
7.6k

2
2
2
6
2
4

review classiﬁcation
sentiment analysis
opinion classiﬁcation
question categorization
review classiﬁcation
news categorization

Table 1: Characteristics of the datasets.

3.2

Implementation Details

In the experiments, we use 300-dimensional
word2vec (Mikolov et al., 2013) vectors to ini-
tialize embedding vectors. We conduct mini-batch
with size 50 for AG’s news and size 25 for other
datasets. We use Adam optimization algorithm
with 1e-3 learning rate to train the model. We use
3 iteration of routing for all datasets since it opti-
mizes the loss faster and converges to a lower loss
at the end.

3.3 Baseline methods

In the experiments, we evaluate and compare
our model with several strong baseline methods
including: LSTM/Bi-LSTM (Cho et al., 2014),
tree-structured LSTM (Tree-LSTM) (Tai et al.,
2015), LSTM regularized by linguistic knowl-
edge (LR-LSTM) (Qian et al., 2016), CNN-
(Kim, 2014),
rand/CNN-static/CNN-non-static
very deep convolutional network (VD-CNN)
(Conneau et al., 2017), and character-level convo-
lutional network (CL-CNN) (Zhang et al., 2015).

4 Experimental Results

4.1 Quantitative Evaluation

In our experiments, the evaluation metric is classi-
ﬁcation accuracy. We summarize the experimental
results in Table 2. From the results, we observe
that the capsule networks achieve best results on
4 out of 6 benchmarks, which veriﬁes the effec-
tiveness of the capsule networks. In particular, our
model substantially and consistently outperforms

Capsule-A

Capsule-B

Figure 2: Two architectures of capsule networks.

components in different ways, as depicted in Fig-
ure 2.

Capsule-A starts with an embedding layer
which transforms each word in the corpus to a
300-dimensional (V = 300) word vector, fol-
lowed by a 3-gram (K1 = 3) convolutional layer
with 32 ﬁlters (B = 32) and a stride of 1 with
ReLU non-linearity. All the other layers are cap-
sule layers starting with a B × d primary cap-
sule layer with 32 ﬁlters (C = 32), followed by
a 3 × C × d × d (K2 = 3) convolutional capsule
layer with 16 ﬁlters (D = 16) and a fully con-
nected capsule layer in sequence.

Each capsule has 16-dimensional (d = 16) in-
stantiated parameters and their length (norm) can
describe the probability of the existence of cap-
sules. The capsule layers are connected by the
transformation matrices, and each connection is
also multiplied by a routing coefﬁcient that is
dynamically computed by routing by agreement
mechanism.

The basic structure of Capsule-B is similar to
Capsule-A except that we adopt three parallel net-
works with ﬁlter windows (N ) of 3, 4, 5 in the
N -gram convolutional layer (see Figure 2). The
ﬁnal output of the fully connected capsule layer is
fed into the average pooling to produce the ﬁnal re-
sults. In this way, Capsule-B can learn more mean-
ingful and comprehensive text representation.

3 Experimental Setup

3.1 Experimental Datasets

In order to evaluate the effectiveness of our model,
we conduct a series of experiments on six bench-

MR SST2

Subj TREC CR AG’s

LSTM
BiLSTM
Tree-LSTM
LR-LSTM

CNN-rand
CNN-static
CNN-non-static
CL-CNN
VD-CNN

Capsule-A
Capsule-B

75.9
79.3
80.7
81.5

76.1
81.0
81.5
-
-

81.3
82.3

80.6
83.2
85.7
87.5

82.7
86.8
87.2
-
-

86.4
86.8

89.3
90.5
91.3
89.9

89.6
93.0
93.4
88.4
88.2

93.3
93.8

86.8
89.6
91.8
-

91.2
92.8
93.6
85.7
85.4

91.8
92.8

78.4
82.1
83.2
82.5

79.8
84.7
84.3
-
-

83.8
85.1

86.1
88.2
90.1
-

92.2
91.4
92.3
92.3
91.3

92.1
92.6

Table 2: Comparisons of our capsule networks and
baselines on six text classiﬁcation benchmarks.
Dataset

Test Description

Train Dev

Reuters-Multi-label
Reuters-Full

5.8k
5.8k

0.6k
0.6k

0.3k
3.4k

only multi-label data in test
full data in test

Table 3: Characteristics of Reuters-21578 corpus.

the simple deep neural networks such as LSTM,
Bi-LSTM and CNN-rand by a noticeable margin
on all the experimental datasets. Capsule net-
work also achieves competitive results against the
more sophisticated deep learning models such as
LR-LSTM, Tree-LSTM, VC-CNN and CL-CNN.
Note that Capsule-B consistently performs better
than Capsule-A since Capsule-B allows to learn
more meaningful and comprehensive text repre-
sentation. For example, a combination of N-gram
convolutional layer with ﬁlter windows of {3,4,5}
can capture the 3/4/5-gram features of the text
which play a crucial role in text modeling.

4.2 Ablation Study

To analyze the effect of varying different compo-
nents of our capsule architecture for text classiﬁca-
tion, we also report the ablation test of the capsule-
B model in terms of using different setups of the
capsule network. The experimental results are
summarized in Table 5. Generally, all three pro-
posed dynamic routing strategies contribute to the
effectiveness of Capsule-B by alleviating the dis-
turbance of some noise capsules which may con-
tain “background” information such as stop words
and the words that are unrelated to speciﬁc cate-
gories. More comprehensive comparison results
are demonstrated in Table A.4 in Supplementary
Material.

5 Single-Label to Multi-Label Text

Classiﬁcation

Capsule network demonstrates promising perfor-
mance in single-label text classiﬁcation which as-

signs a label from a predeﬁned set to a text (see Ta-
ble 2). Multi-label text classiﬁcation is, however, a
more challenging practical problem. From single-
label to multi-label (with n category labels) text
classiﬁcation, the label space is expanded from n
to 2n, thus more training is required to cover the
whole label space. For single-label texts, it is prac-
tically easy to collect and annotate the samples.
However, the burden of collection and annotation
for a large scale multi-label text dataset is gener-
ally extremely high. How deep neural networks
(e.g., CNN and LSTM) best cope with multi-label
text classiﬁcation still remains a problem since ob-
taining large scale of multi-label dataset is a time-
consuming and expensive process. In this section,
we investigate the capability of capsule network
on multi-label text classiﬁcation by using only the
single-label samples as training data. With fea-
ture property as part of the information extracted
by capsules, we may generalize the model better
to multi-label text classiﬁcation without an over
extensive amount of labeled data.

The evaluation is carried on the Reuters-21578
This dataset consists
dataset (Lewis, 1992).
of 10,788 documents from the Reuters ﬁnancial
newswire service, where each document contains
either multiple labels or a single label. We re-
process the corpus to evaluate the capability of
capsule networks of transferring from single-label
to multi-label text classiﬁcation. For dev and train-
ing, we only use the single-label documents in the
Reuters dev and training sets. For testing, Reuters-
Multi-label only uses the multi-label documents
in testing dataset, while Reuters-Full includes all
documents in test set. The characteristics of these
two datasets are described in Table 3.

Following (Sorower, 2010), we adopt Micro
Averaged Precision (Precision), Micro Averaged
Recall (Recall) and Micro Averaged F1 scores
(F1) as the evaluation metrics for multi-label text
classiﬁcation. Any of these scores are ﬁrstly com-
puted on individual class labels and then averaged
over all classes, called label-based measures. In
addition, we also measure the Exact Match Ratio
(ER) which considers partially correct prediction
as incorrect and only counts fully correct samples.

The experimental results are summarized in Ta-
ble 4. From the results, we can observe that
the capsule networks have substantial and signif-
icant improvement in terms of all four evaluation
metrics over the strong baseline methods on the

Reuters-Multi-label

Reuters-Full

ER

Precision

Recall

F1

ER

Precision

Recall

F1

LSTM
BiLSTM

CNN-rand
CNN-static
CNN-non-static

Capsule-A
Capsule-B

23.3
26.4

22.5
27.1
27.4

57.2
60.3

86.7
82.3

88.6
91.1
92.0

88.2
95.4

54.7
55.9

56.4
59.1
59.7

80.1
82.0

63.5
64.6

67.1
69.7
70.4

82.0
85.8

62.5
65.8

63.4
63.3
64.1

66.0
67.7

78.6
83.7

78.7
78.5
80.6

83.9
86.4

72.6
75.4

71.5
71.2
72.7

80.5
80.1

74.0
77.8

73.6
73.3
75.0

80.2
81.4

Table 4: Comparisons of the capability for transferring from single-label to multi-label text classiﬁcation
on Reuters-Multi-label and Reuters-Full datasets. For fair comparison, we use margin-loss for our model
and other baselines.

Iteration Accuracy

Capsule-B + Sabour’s routing

Capsule-B + our routing
Capsule-B + our routing
Capsule-B + our routing
w/o Leaky-softmax
w/o Orphan Category
w/o Amendent Coefﬁent

3

1
3
5
3
3
3

81.4

81.4
82.3
81.6
81.7
81.9
82.1

Table 5: Ablation study of Capsule-B on MR
dataset.
The standard routing is routing-by-
agreement algorithm without leaky-softmax and
orphan category in the last capsule layer. More
ablations are discussed in Appendix.

test sets in both Reuters-Multi-label and Reuters-
Full datasets.
In particular, larger improvement
is achieved on Reuters-Multi-label dataset which
only contains the multi-label documents in the test
set. This is within our expectation since the cap-
sule network is capable of preserving the instanti-
ated parameters of the categories trained by single-
label documents. The capsule network has much
stronger transferring capability than the conven-
tional deep neural networks. In addition, the good
results on Reuters-Full also indicate that the cap-
sule network has robust superiority over competi-
tors on single-label documents.

5.1 Connection Strength Visualization

attention mechanism. This should allow the cap-
sule networks to recognize multiple categories in
the text even though the model is trained on single-
label documents.

Due to space reasons, we choose a multi-
label document from Reuters-Multi-label test set
whose category labels (i.e., Interest Rates and
Money/Foreign Exchange) are correctly predicted
(fully correct) by our model with high conﬁdence
(p > 0.8) to report in Table 6. The category-
speciﬁc phrases such as “interest rates” and “for-
eign exchange” are highlighted with red color.
We use the tag cloud to visualize the 3-gram
phrases for Interest Rates and Money/Foreign Ex-
change categories. The stronger the connection
strength, the bigger the font size. From the re-
sults, we observe that capsule networks can cor-
rectly recognize and cluster the important phrases
with respect
The his-
to the text categories.
tograms are used to show the intensity of con-
nection strengths between primary capsules and
the fully connected capsules, as shown in Table
6 (bottom line). Due to space reasons, ﬁve his-
tograms are demonstrated. The routing procedure
correctly routes the votes into the Interest Rates
and Money/Foreign Exchange categories. More
examples can be found in Table A.2-A.3 in Sup-
plementary Material.

To visualize the connection strength between cap-
sule layers clearly, we remove the convolutional
capsule layer and make the primary capsule layer
followed by the fully connected capsule layer di-
rectly, where the primary capsules denote N-gram
phrases in the form of capsules. The connection
strength shows the importance of each primary
capsule for text categories, acting like a parallel

To experimentally verify the convergence of the
routing algorithm, we also plot learning curve to
show the training loss over time with different it-
erations of routing. From Figure 3, we observe
that the Capsule-B with 3 or 5 iterations of routing
optimizes the loss faster and converges to a lower
loss at the end than the capsule network with 1 it-
eration.

U.K. MONEY RATES FIRM ON LAWSON STERLING TARGETS

Interest Rates

Money/Foreign Exchange

Interest rates on the London money market were slightly ﬁrmer on news U.K.
Chancellor of the Exchequer Nigel Lawson had stated target rates for sterling
against the dollar and mark, dealers said. They said this had come as a surprise
and expected the targets, 2.90 marks and 1.60 dlrs, to be promptly tested in the
foreign exchange markets. Sterling opened 0.3 points lower in trade weighted
terms at 71.3. Dealers noted the chancellor said he would achieve his goals
on sterling by a combination of intervention in currency markets and interest
rates. Operators feel the foreign exchanges are likely to test sterling on the
downside and that this seems to make a fall in U.K. Base lending rates even
less likely in the near term, dealers said. The feeling remains in the market,
however, that fundamental factors have not really changed and that a rise in
U.K. Interest rates is not very likely. The market is expected to continue at
around these levels, reﬂecting the current 10 pct base rate level, for some time.
The key three months interbank rate was 1/16 point ﬁrmer at 10 9-7/8 pct.

Table 6: Visualization of connection strength between primary capsules and the FC capsules by 3-gram
phrases cloud and histogram of the their intensities. x axis denotes primary capsules (3-gram phrases)
selected for demonstration, y axis denotes intensity of connection strength. The results are retrieved from
Capsule-B trained with 3 routing iterations. The category-speciﬁc key-phrases in red color in raw text
(ﬁrst column) are annotated manually for reference.

works, in particular LSTMs and CNNs.
(Kim,
2014) reported on a series of experiments with
CNNs trained on top of pre-trained word vectors
for sentence-level classiﬁcation tasks. The CNN
models improved upon the state of the art on 4
(Zhang et al., 2015) offered an
out of 7 tasks.
empirical exploration on the use of character-level
convolutional networks (Convnets) for text classi-
ﬁcation and the experiments showed that Convnets
outperformed the traditional models. (Joulin et al.,
2016) proposed a simple and efﬁcient text classi-
ﬁcation method fastText, which could be trained
on a billion words within ten minutes. (Conneau
et al., 2017) proposed a very deep convolutional
networks (with 29 convolutional layers) for text
classiﬁcation.
(Tai et al., 2015) generalized the
LSTM to the tree-structured network topologies
(Tree-LSTM) that achieved best results on two text
classiﬁcation tasks.

Recently, a novel type of neural network is pro-
posed using the concept of capsules to improve
the representational limitations of CNN and RNN.
Hinton et al. (2011) ﬁrstly introduced the con-
cept of “capsules” to address the representational
limitations of CNNs and RNNs. Capsules with
transformation matrices allowed networks to au-
tomatically learn part-whole relationships. Conse-
quently, Sabour et al. (2017) proposed capsule net-
works that replaced the scalar-output feature de-
tectors of CNNs with vector-output capsules and

Figure 3: Training loss of Capsule-B on Reuters-
Multi-label dataset.

6 Related Work

Early methods for text classiﬁcation adopted the
typical features such as bag-of-words, n-grams,
and their TF-IDF features (Zhang et al., 2008) as
input of machine learning algorithms such as sup-
port vector machine (SVM) (Joachims, 1998), lo-
gistic regression (Genkin et al., 2007), naive Bayes
(NB) (McCallum et al., 1998) for classiﬁcation.
However, these models usually heavily relied on
laborious feature engineering or massive extra lin-
guistic resources.

Recent advances in deep neural networks and
representation learning have substantially im-
proved the performance of text classiﬁcation tasks.
The dominant approaches are recurrent neural net-

max-pooling with routing-by-agreement. The cap-
sule network has shown its potential by achiev-
ing a state-of-the-art result on MNIST data. Un-
like max-pooling in CNN, however, Capsule net-
work do not throw away information about the
precise position of the entity within the region. For
lowlevel capsules, location information is place-
coded by which capsule is active. (Xi et al., 2017)
further tested out the application of capsule net-
works on CIFAR data with higher dimensionality.
(Hinton et al., 2018) proposed a new iterative rout-
ing procedure between capsule layers based on the
EM algorithm, which achieves signiﬁcantly bet-
ter accuracy on the smallNORB data set. (Zhang
et al., 2018) generalized existing routing methods
within the framework of weighted kernel density
estimation. To date, no work investigates the per-
formance of capsule networks in NLP tasks. This
study herein takes the lead in this topic.

7 Conclusion

In this paper, we investigated capsule networks
with dynamic routing for text classiﬁcation. Three
strategies were proposed to boost the performance
of the dynamic routing process to alleviate the dis-
turbance of noisy capsules. Extensive experiments
on six text classiﬁcation benchmarks show the ef-
fectiveness of capsule networks in text classiﬁ-
cation. More importantly, capsule networks also
show signiﬁcant improvement when transferring
single-label to multi-label text classiﬁcations over
strong baseline methods.

References

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of ma-
chine Learning research, 3(Jan):993–1022.

Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014.
Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

Alexis Conneau, Holger Schwenk, Lo¨ıc Barrault, and
Yann Lecun. 2017. Very deep convolutional net-
works for text classiﬁcation. In Proceedings of the
15th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Volume 1,
Long Papers, volume 1, pages 1107–1116.

Alexander Genkin, David D Lewis, and David Madi-
gan. 2007. Large-scale bayesian logistic regression

for text categorization. Technometrics, 49(3):291–
304.

Geoffrey Hinton,

and Nicholas
Sara Sabour,
Frosst. 2018. Matrix capsules with em routing.
https://openreview.net/forum?id=HJWLfGWRb.

Geoffrey E Hinton, Alex Krizhevsky, and Sida D
Wang. 2011. Transforming auto-encoders. In Inter-
national Conference on Artiﬁcial Neural Networks,
pages 44–51. Springer.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177.
ACM.

Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many rel-
evant features. In European conference on machine
learning, pages 137–142. Springer.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2016. Bag of tricks for efﬁcient text
classiﬁcation. In Annual meeting on Association for
Computational Linguistics, pages 427–431.

Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. In Proceedings of the 42nd
annual meeting on Association for Computational
Linguistics, pages 1746–1751. Association for Com-
putational Linguistics.

Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian
Weinberger. 2015. From word embeddings to docu-
ment distances. In International Conference on Ma-
chine Learning, pages 957–966.

David D. Lewis. 1992. An evaluation of phrasal and
clustered representations on a text categorization
task. In Fifteenth Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 37–50.

Xin Li and Dan Roth. 2002. Learning question clas-
In Proceedings of the 19th international
siﬁers.
conference on Computational linguistics-Volume 1,
pages 1–7. Association for Computational Linguis-
tics.

Jon D Mcauliffe and David M Blei. 2008. Supervised
In Advances in neural information

topic models.
processing systems, pages 121–128.

Andrew McCallum, Kamal Nigam, et al. 1998. A com-
parison of event models for naive bayes text classi-
ﬁcation. In AAAI-98 workshop on learning for text
categorization, volume 752, pages 41–48. Citeseer.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
In Advances in neural information processing
ity.
systems, pages 3111–3119.

the 55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
volume 1, pages 1847–1856.

Suofei Zhang, Wei Zhao, Xiaofu Wu, and Quan
Zhou. 2018.
Fast dynamic routing based on
weighted kernel density estimation. arXiv preprint
arXiv:1805.10807.

Wen Zhang, Taketoshi Yoshida, and Xijin Tang. 2008.
Tﬁdf, lsi and multi-word in information retrieval and
In IEEE International Confer-
text categorization.
ence on Systems, Man and Cybernetics, pages 108–
113. IEEE.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
In Advances in neural information pro-
siﬁcation.
cessing systems, pages 649–657.

Amr Mousa and Bj¨orn Schuller. 2017. Contextual bidi-
rectional long short-term memory recurrent neural
network language models: A generative approach to
sentiment analysis. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics: Volume 1, Long Pa-
pers, volume 1, pages 1023–1032.

Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd annual meeting on Association for Compu-
tational Linguistics, page 271. Association for Com-
putational Linguistics.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the
43rd annual meeting on association for computa-
tional linguistics, pages 115–124. Association for
Computational Linguistics.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Qiao Qian, Minlie Huang,

Jinhao Lei, and Xi-
Linguistically regularized
aoyan Zhu. 2016.
lstms for sentiment classiﬁcation. arXiv preprint
arXiv:1611.03949.

Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton.
In Ad-
2017. Dynamic routing between capsules.
vances in Neural Information Processing Systems,
pages 3859–3869.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
In Proceedings of the 2013 conference on
bank.
empirical methods in natural language processing,
pages 1631–1642.

Mohammad S Sorower. 2010. A literature survey on
algorithms for multi-label learning. Oregon State
University, Corvallis, 18.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
In Proceedings of the 53rd Annual Meet-
works.
ing of the Association for Computational Linguis-
tics, pages 1556–1566.

Edgar Xi, Selina Bing, and Yang Jin. 2017. Cap-
sule network performance on complex data. arXiv
preprint arXiv:1712.03480.

Jianbo Ye, Yanran Li, Zhaohui Wu, James Z Wang,
Wenjie Li, and Jia Li. 2017. Determining gains ac-
quired from word embedding quantitatively using
In Proceedings of
discrete distribution clustering.

Supplementary Material

To better demonstrate the orphan and other categories with top unigrams, we remove the convolutional
capsule layer and make the primary capsule layer followed by the fully connected capsule layer directly,
similar to the settings in section 5.1. Here, the primary capsules denote uni-grams in the form of capsules.
We picked top-20 uni-gram (words) from four categories (i.e., Orphan category, Trade category, Money
Exchange category and Interest Rates category) sorted by their connection strengths.

Index Orphan

Trade

Money Exchange

Interest Rates

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

the
and
said
for
its
U.S.
that
from
mln(Millon)
was
with
billion
gulf
not
today
will
they
had
were
would

trade
money
Fed (Federal Reserve) market
market
rate
deﬁcit
surplus
pct (Percent of Total)
minister
customer
export
mln(Millon)
bank
imports
money
oil
agreements
repurchase
goods
bills
shipping

bank
currency
STG (Sterling)
rate
repurchase
reserves
dollar
customer
bills
funds
exchange
liquidity
dealers
monetary
treasury
sterling
Bundesbank
deposits

Fed (Federal Reserve)
rate
pct (Percent of Total)
bank
market
repurchase
customer
federal
dealers
reserve
economists
Bundesbank
interest
discount
trading
money
lending
treasury
bankers
agreements

Table A.1: Top-20 words picked from four categories (i.e., Orphan category, Trade category, Money
Exchange category and Interest Rates category) sorted by their connection strengths.

Trade

Money/Foreign Exchange

G-7 ISSUES STATEMENT AFTER MEETING
Following is the text of a statement by the Group of Seven – the U.S., Japan, West Germany,
France, Britain, Italy and Canada – issued after a Washington meeting yesterday. 1. The ﬁnance
ministers and central bank governors of seven major industrial countries met today. They contin-
ued the process of multilateral surveillance of their economies pursuant to the arrangements for
strengthened economic policy coordination agreed at the 1986 Tokyo summit of their heads of
state or government. 2. The ministers and governors reafﬁrmed the commitment to the coopera-
tive approach agreed at the recent Paris meeting, and noted the progress achieved in implementing
the undertakings embodied in the Louvre Agreement. In this connection they welcomed the pro-
posals just announced by the governing Liberal Democratic Party in Japan for extraordinary and
urgent measures to stimulate Japan’s economy through early implementation of a large supple-
mentary budget exceeding those of previous years, as well as unprecedented front-end loading of
public works expenditures. They concluded that present and prospective progress in implementing
the policy undertakings at the Louvre and in this statement provided a basis for continuing close
cooperation to foster the stability of exchange rates.

Table A.2: Fully Corrected Case with weakly conﬁdence (0.4 < p < 0.6). Although category-speciﬁc
phrases are mentioned only once, category labels are correctly predicted.

PAPERS SAY VENEZUELAN CENTRAL BANK CHIEF TO RESIGN

Interest Rates

Money/Foreign Exchange

Venezuelan Central Bank President Hernan Anzola has submitted his resignation and asked Pres-
ident Jaime Lusinchi to transfer him to a post in the oil industry, two leading Venezuelan newspa-
pers reported. The El Universal and El Nacional papers said Anzola would leave his position soon.
Lusinchi already has decided on his successor, the El Nacional reported. Central Bank ofﬁcials
were not available for comment. Banking sources said Anzola differed with the Finance Ministry
over economic policy, particularly over the direction of interest rates. He favoured raising the
rates, which are currently well below the annual inﬂation rate of 33.2 pct. But the sources said
he ran into opposition from Finance Ministry and government ofﬁcials who thought an interest
increase would fuel inﬂation.

The label of ”Interest Rates” is correctly predicted but
Table A.3: Partially Corrected Case.
”Money/Foreign Exchange” is confused with ”Mergers/Acquisitions” since the category-speciﬁc phrases
are subtle and not be mentioned directly.

Index Routing Leaky

Shared OrphanCap

Loss

Squash Coefﬁcient Accuracy

1
2
3
4
5
6
7
8
9
10
11
12
13
14

1
5
2
3
3
3
3
3
3
3
3
3
3
3

Yes
Yes
Yes
Yes
Yes
Yes
Yes
No
Yes
Yes
Yes
Yes
Yes
Yes

Yes
Yes
Yes
No
Yes
No
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes

No
No
No
Yes
Yes
No
No
No
No
No
No
No
No
No

Margin
Margin
Margin
Margin
Margin
Margin
Margin
Margin
Margin
Spread
CrossEnt
Margin
Margin
Margin

|x|/(1 + |x|)
|x|/(1 + |x|)
|x|/(1 + |x|)
|x|/(1 + |x|)
|x|/(1 + |x|)
|x|/(1 + |x|)
|x|/(1 + |x|)
|x|/(1 + |x|)
|x|/(1 + |x|)
|x|/(1 + |x|)
|x|/(1 + |x|)
1 − exp(−|x|)
tanh(|x|)
None

80.4
81.1
80.5
82.3
81.8
81.9
81.2
80.9
81.6
81.1
80.3
80.5
80.8
80.6

Table A.4: The effect of varying different components of Capsule-B on MR dataset. “Routing”: represent
the number of the routing iteration. “Leaky”: use leaky softmax or not. “Shared”: use shared weights
between child-parent relationships or not. “OrphanCap”: use orphan category or not.

