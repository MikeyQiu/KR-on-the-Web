Recurrent Neural Network Based Loanwords Identiﬁcation in Uyghur

Chenggang Mi1,2, Yating Yang1,2,3, Xi Zhou1,2, Lei Wang1,2, Xiao Li1,2 and Tonghai Jiang1,2
1Xinjiang Technical Institute of Physics and Chemistry of Chinese Academy of Sciences,
Urumqi, 830011, China
2Xinjiang Laboratory of Minority Speech and Language Information Processing,
Xinjiang Technical Institute of Physics and Chemistry of Chinese Academy of Sciences,
Urumqi, 830011, China
3Institute of Acoustics of the Chinese Academy of Sciences, Beijing, 100190, China
{micg,yangyt,zhouxi,wanglei,xiaoli,jth}@ms.xjb.ac.cn

Abstract

Comparable corpus is the most important re-
source in several NLP tasks. However, it is
very expensive to collect manually. Lexical
borrowing happened in almost all languages.
We can use the loanwords to detect useful
bilingual knowledge and expand the size of
donor-recipient / recipient-donor comparable
corpora.
In this paper, we propose a recur-
rent neural network (RNN) based framework
to identify loanwords in Uyghur. Addition-
inverse lan-
ally, we suggest two features:
guage model feature and collocation feature
to improve the performance of our model. Ex-
perimental results show that our approach out-
performs several sequence labeling baselines.

1

Introduction

Most natural language processing (NLP) tools rely
on large scale language resources, but many lan-
guages in the world are resource-poor. To make
these NLP tools widely used, some researchers
have focused on techniques that obtain resources
of resource-poor languages from resource-rich lan-
guages using parallel data for NLP applications such
as syntactic parsing, word sense tagging, machine
translation, semantic role labeling, and some cross-
lingual NLP tasks. However, high quality parallel
corpora are expensive and difﬁcult to obtain, es-
pecially for resource-poor languages like Uyghur.
Lexical borrowing is very common between lan-
guages. It is a phenomenon of cross-linguistic in-
ﬂuence (Tsvetkov et al., 2015a).
If loanwords in
resource-poor languages (e.g. Uyghur) can be iden-
tiﬁed effectively, we can use the bilingual word pairs

as an important factor in comparable corpora build-
ing. And comparable corpora are vital resources in
parallel corpus detection (Munteanu et al., 2006).
Additionally, loanwords can be integrated into bilin-
gual dictionaries directly. Therefore, loanwords are
valuable to study in several NLP tasks such as ma-
chine translation, information extraction and infor-
mation retrieval.

In this paper, we design a novel model to iden-
tify loanwords (Chinese, Russian and Arabic) from
Uyghur texts. Our model based on a RNN Encoder-
Decoder framework (Cho et al., 2014). The En-
coder processes a variable length input (Uyghur sen-
tence) and builds a ﬁxed-length vector representa-
tion. Based on the encoded representation, the de-
coder generates a variable-length sequence (Labeled
sequence). To optimize the output of decoder, we
also propose two important features:
inverse lan-
guage model feature and collocation feature. We
conduct three groups of experiments; experimental
results show that, our model outperforms other ap-
proaches.

This paper makes the following contributions to

this area:

• We introduce a novel approach to loanwords
identiﬁcation in Uyghur. This approach in-
creases F1 score by 12% relative to traditional
approach on the task of loanwords detection.

• We conduct experiments to evaluate the per-
formance of off-the-shelf loanwords detection
tools trained on news corpus when applied to
loanwords detection. By utilizing in-domain
and out-of-domain data.

• For integrate these crucial information for bet-
ter loanwords prediction, we combine two fea-
tures into the loanwords identiﬁcation model,
so that we can use more important information
to select the better loanword candidate.

The rest of this paper is organized as follows:
Section 2 presents the background of loanwords in
Uyghur; Section 3 interprets the framework used
in our model; Section 4 introduces our method in
detail. Section 5 describes the experimental setup
and the analysis of experimental results. Section 6
discusses the related work. Conclusion and future
work are presented in Section 7.

in both social and ofﬁcial spheres, as well as in
print, radio and television, and is mostly used as a
lingua franca by other ethnic minorities in Xinjiang.
Uyghur belongs to the Turkic language family,
which also includes languages such as the more
In addition to inﬂuence
distantly related Uzbek.
of other Turkic languages, Uyghur has historically
been inﬂuenced strongly by Person and Arabic and
more recently by Mandarin Chinese and Russian
(Table 1).

Loanwords in Uyghur not only include named
entities such as person and location names, but also
some daily used words.

2 Background

Before we present our loanwords detection model,
we provide a brief introduction of Uyghur and loan-
words identiﬁcation in this section. This will help
build relevant background knowledge.

2.1 Introduction of Loanwords

A loanword is a word adopted from one language
(the donor language) and incorporated into a differ-
ent, recipient language without translation.
It can
be distinguished from a calque, or loan translation,
where a meaning or idiom from another language
is translated into existing words or roots of the host
language. When borrowing, the words may have
several changes to adopt the recipient language:

• Changes in meaning. Words are occasionally
improved with a different meaning than that in
the donor language

• Changes in spelling. Words taken into differ-
ent recipient languages are something spelled
as in the donor language. Sometimes borrowed
words retain original (or near-original) pronun-
ciation, but undergo a spelling change to repre-
sent the orthography of the recipient language.

• Changes in pronunciation.

In cases where a
new loanword has a very unusual sound, the
pronunciation of the word is radically changed.

2.2 Loanwords in Uyghur

Uyghur is an ofﬁcial
language of the Xinjiang
Uyghur Autonomous Region, and is widely used

2.3 Challenges in Loanwords Identiﬁcation in

Uyghur

Spelling Change When Borrowed From Donor
Languages

To adopt the pronunciation and grammar in Uyghur,
spelling of words (loanwords) may change when
borrowed from donor languages.
Changes of
spelling have a great
impact on the loanwords
identiﬁcation task.
Russian loanwords in Uyghur:
“radiyo”1-“radio”(“radio”)
Chinese loanwords in Uyghur:
“koi”-“(cid:2)” (“kuai”)

Sufﬁxes of Uyghur Words Affect
Identiﬁcation

the Loanwords

A Uygur word is composed of a stem and several
sufﬁxes, which can be formally described as:

If we just use the traditional approaches such
as edit distance, in some cases, these algorithms
cannot give us sure results, for example, the length
of sufﬁxes equal even greater than the original
word’s length.

Data Sparsity Degrades
Loanwords Identiﬁcation Model

the Performance of

1In this paper, we use Uyghur Latin Alphabet.

W ord = stem + suf f ix0 + suf f ix1 + ... + suf f ixN (1)

Chinese loan words in Uyghur [in English] Russian loan words in Uyghur [in English]
shinjang((cid:2)(cid:3)) [Xinjiang]
laza((cid:2)(cid:2)) [hot pepper]
shuji((cid:2)(cid:2)) [secretary]
koi((cid:2)) [Yuan]
lengpung((cid:3)(cid:2)) [agar-agar jelly]
dufu((cid:2)(cid:2)) [bean curd]

tElEfon(telefon) [telephone]
uniwErsitEt(universitet) [university]
radiyo(radio) [radio]
pochta(poqta) [post ofﬁce]
wElsipit(velosiped) [bicycle]
oblast(oblast(cid:2)) [region]

Table 1: Examples of Chinese and Russian Loanwords in Uyghur.

Loanwords detection can be reformulated as a se-
quence labeling problem. Most sequence labeling
tools (such as CRFs-based, HMM-based etc.) are
built on large scales labeled data, lack of available
labeled language resource makes decrease of perfor-
mance on loanwords identiﬁcation in Uyghur using
above ”off-the-shelf” tools.

3.2 RNN Encoder-Decoder Framework

In this section, we give a brief introduction of the
framework, which was
RNN Encoder-Decoder
proposed by (Cho et al., 2014a) and (Sutskever et
al., 2014). We build a novel architecture that learns
to identify loanwords in Uyghur texts based on this
framework.

3 Methodology

Recent development of deep learning (representa-
tion learning) has a strong impact in the area of
NLP (natural language processing). According to
traditional approaches, extraction of features often
requires expensive human labor and often relies on
expert knowledge, and these features usually cannot
be expended in other situations. The most exciting
thing of deep learning is that features used in most
traditional machine learning models can be learned
automatically.

In this section, we ﬁrst introduced the most popu-
lar deep learning models used in this paper, then, we
involved in the details of this model.

3.1 Recurrent Neural Network

RNNs (Recurrent Neural Networks) are artiﬁcial
neural network models where connections between
units form a directed cycle (Jaeger, 2002). This cre-
ates an internal state of the network which allows it
to exhibit dynamic temporal behavior. RNNs can
use their internal memory to process arbitrary se-
quences of inputs. This makes them show great
promise in many NLP tasks.

The most important feature of a RNN model is
that the network contains at least one feed-back con-
nection, so the activations can ﬂow round in a loop.
That makes the networks very suited for tasks like
temporal processing and sequence labeling.

(cid:4670)(cid:1841)(cid:882)(cid:481) (cid:1841)(cid:883)(cid:481) (cid:1841)(cid:884)(cid:481) (cid:485) (cid:481) (cid:1841)(cid:4666)(cid:1864)(cid:3398)(cid:883)(cid:4667)(cid:481) (cid:1841)(cid:1864)(cid:4671) 

Decode

(cid:4670)(cid:1878)(cid:1874)(cid:882)(cid:481) (cid:1878)(cid:1874)(cid:883)(cid:481) (cid:1878)(cid:1874)(cid:884)(cid:481) (cid:485) (cid:481) (cid:1878)(cid:1874)(cid:4666)(cid:1864)(cid:3398)(cid:883)(cid:4667)(cid:481) (cid:1878)(cid:1874)(cid:1864) (cid:4671) 

Encode

(cid:4670)(cid:1847)(cid:882)(cid:481) (cid:1847)(cid:883)(cid:481) (cid:1847)(cid:884)(cid:481) (cid:485) (cid:481) (cid:1847)(cid:4666)(cid:1864)(cid:3398)(cid:883)(cid:4667)(cid:481) (cid:1847)(cid:1864)(cid:4671) 

Figure 1: The Encoder-Decoder Framework Used in
Loanword Identiﬁcation Model.

a

is

labels

sequence

(loanword or not),

U0, U1, U2, ..., Ul−1, Ul

of
Uyghur words, O0, O1, O2, ..., Ol−1, Ol
a
and
sequence of
Zv0, Zv1, Zv2, ..., Zv(l−1), Zvl
is a sequence of
vector representation of Uyghur words. The bold
face ”Encode” and ”Decode” are two processes of
encoder and decoder in our loanword identiﬁcation
model, respectively (Figure 1).

is

Encoder

In the RNN Encoder-Decoder
a
sentence is ﬁrstly transformed into a sequence of
vectors x = (x1, x2, ..., xl), then the encoder reads
x as a vector (cid:2)c. The most common approach is to

framework,

use an RNN such that

ht = f (xt, ht−1)

And

c = q(h1, h2, ..., hl−1, hl)

where ht ∈ γ is a hidden state at time t, and (cid:2)c is a
vector generated from the sequence of the hidden
states. f and q are some nonlinear functions. For
instance, the Long-Short Term Memory (LSTM) is
used as f , and q(h1, h2, ..., hl−1, hl) as ht.

(2)

(3)

p(e|f ) indicates the translation model, which
means the probability that the source string e is
the translation of the target string f ; p(e) is the
language model, which indicates the probability
that the string e appeared in target language.

Usually, there are different pronunciation systems
between donor language and recipient language.
Different pronunciation rules can be represented by
different character-based language models.
In our
paper, we propose an inverse language model (ILM)
to constraint the output of loanword identiﬁcation
system.

Decoder

N-gram Model

the decoder is often used
In RNN framework,
the next word yt given the context
to predict
vector (cid:2)c and all the previously predicted words
y1, y2, ..., yt−1.
In other words, the decoder de-
ﬁnes a probability over the identiﬁcation yt by
decomposing the joint probability into the ordered
conditionals:

p(y) =

p(yt| {y1, y2, ..., yt−1} , (cid:2)c)

(4)

l(cid:4)

t=1

4 Loanwords Identiﬁcation

loanwords

identiﬁcation model
To make our
stronger, we also proposed several features as the ad-
ditional knowledge of RNN. These features can be
derived from monolingual corpus.
In this section,
we present two features ﬁrstly, then, we introduce
the decoding part of our model.

4.1 Features

Inverse Language Model Feature

A language model
is a probability distribution
over sequences of words in NLP. Traditionally,
language models are widely used in applications
such as machine translation, speech recognition,
part-of-speech tagging, parsing, and information
retrieval.
in statistical machine
translation, language models are used to improve
the ﬂuency of generated texts
(Lembersky et al.,
2012)

For example,

N-gram is a contiguous sequence of n items
from a given text. An n-gram model models natural
language sequences using the statistical properties
of n-grams. Practically, an n-gram model predicts xi
based on xi−(n−1), ..., xi−1. This can be indicated
by probability terms as

p(xi|xi−(n−1), ..., xi−1)

(6)

When used in language modeling,
independent
assumptions are made so that each item word
replies on its last n − 1 words.

Inverse Language Model

As mentioned above, we can use a character-
based language model to indicate a pronunciation
system. Although a loanword may adapt the pro-
nunciation of recipient language when borrowing,
there are still some differences exist, and these dif-
ferences usually reﬂect features of donor language
pronunciation system. Accordingly, we computed
the inverse language model feature as following

pilm = (1 − λ1puyg) + λ2pdnr

(7)

Where puyg is the language model probability of
a given character sequence in Uyghur, pdnr is the
language model probability of above sequence in
donor languages. λ1 and λ2 are weights which can
be obtained during model optimization. Language
model probabilities are all based on n-gram models.

p(e|f ) ∝ p(f |e)p(e)

(5)

Collocation Feature

Unlike Chinese, some words (e.g. person names)
are written separately. For detect loanwords effec-
tively, we proposed a collocation feature, which
measures the co-occurrence probability of
two
words.

The frequency of words is a simple but effective
metric in NLP. In this paper, we use a probability
of words co-occurrence to measure the composition
of several parts of a possible loanword. Similar to
language model, we also use smooth mechanism to
alleviate the data sparseness.

Another metric used in our collocation extraction
is the skip-gram language model (SGLM), which
has the ability to model semantic relations between
words, and capture a form of compositionality.
We represent words by distributed representation
encoded in the hidden layers of neural networks.
Given a word, the context can be learned by the
model. In our model, we can apply the SGLM to
predict a loanword (such as Chinese person names
in Uyghur texts) based on one part of it.

pclc1 = max

log p(wt+j|wt)

(8)

l(cid:3)

−k≤j−1,j≤k

For example, if we indicate v as a function that maps
one part of a loanword w to its n-dimensional vector
representation, then

v(“jinping”) − v(“shi”) + v(“zEmin”)
≈ v(“jyang”)

(9)

The symbol ≈ means its right hand side must be the
nearest neighbor of the value of the left hand side.

4.2 Decoding

In this paper, we use a beam search decoder as the
basic framework, a neural network and two features
are also integrated into it. Two features used in this
model are as two re-rankers to ﬁlter out incorrect
outputs.

s(o|w) = log prnn(o|w)

+ μ1 log pilm(wc1...ck)
+ μ2 log pclc(w)

(10)

several characteristics are captured when loanwords
identiﬁcation. prnn(o|w) is the most important part
in our framework, some information that difﬁcult to
deﬁne by human can be learned automatically by
the RNN model. Different language has a differ-
In our loanwords iden-
ent pronunciation system.
tiﬁcation task, we use the inverse language model of
characters to highlight the probability of a loanword
pilm(wc1...ck). For loanwords such as person names
which have been separated by blank spaces, we pre-
dict their contexts according to themselves. In our
method, we integrated both inner word information
and information between words into the decoder.

5 Experiments and Results

We conduct experiments to evaluate our loanword
identiﬁcation model. According to the tasks de-
ﬁned in this paper, these experiments can be divided
into two types: 1) in-domain experiments; 2) cross-
domain experiments. We train the loanword identi-
ﬁcation model using a small set of training data, and
evaluate the performance of our model with three
held out test sets for each language.

5.1 Setup and Datasets

We evaluate our approach on three donor languages:
Chinese, Russian and Arabic. In our approach, loan-
word identiﬁcation models are trained on Uyghur
news corpora. Test sets used in our experiments in-
clude in-domain test sets and cross-domain test sets.
Since we are very familiar with Chinese and Rus-
sian, we labeled several types of Chinese (Chn) and
Russian (Rus) loanwords in Uyghur test sets, such as
person names, locations, and other daily used words.
For Arabic loanwords (Arab), we labeled them in
test set manually. Because we have limited knowl-
edge about Arabic, we just labeled some person
names and locations. We collected some relatively
regular corpora from news websites in Chinese2 ,
Russian3 and Arabic4 to train the language models,
respectively.

We built the recurrent neural network which used
in our loanword identiﬁcation model on the open
source deep learning software Deeplearning4j5. For

Where μ1 and μ2 are parameters which deter-
mine how much inverse language model and collo-
cation model are weighted. According to our model,

2http://www.people.com.cn/
3http://sputniknews.ru/
4http://arabic-media.com/arabicnews.htm
5http://deeplearning4j.org

Languages
Uyghur
Chinese
Russian
Arabic

TR-Set
10,000 * 3
\
\
\

DE-Set
1,000 * 3
1,000
1,000
1,000

TE-Set
1,000 * 3
\
\
\

Table 2: Statistic of Corpora.

the inverse language model, we used a java ver-
sion language model tool which was implemented
by ourselves. For the collocation extraction feature,
we trained a model based on the word2vec, which
was proposed by Tomas Mikolov, and a java ver-
sion is also implemented in Deeplearning4j toolkits.
To evaluate the performance of loanword identiﬁ-
cation models, several metrics are used in our exper-
iments:

R =

, P =

, F 1 =

A
A + C

A
A + B

2 (cid:5) R (cid:5) P
P + R

(11)

P (P recision) indicates the percentage of loan-
words found that match exactly the spans found in
the evaluations data (test set);

R(Recall) means the percentage of loanwords
deﬁned in the corpus that were found in the same
location;

F 1 can be interpreted as the harmonic mean of P

and R.

5.2 Experiments and Analysis

For validate the effectiveness of our loanword
identiﬁcation model, we ﬁrst compare our model
(RNN-based model) with other loanword detection
models,
(CRFs)
including CRFs-based model
(Lafferty et al., 2001),
the identiﬁcation model
based on string similarity (SSIM) (Mi et al., 2013),
classiﬁcation-based identiﬁcation model (CBIM)
(Mi et al., 2014). We suggest two important features
in this paper to optimize the output of our loanword
identiﬁcation model, affection of these features are
evaluated on identiﬁcation performance. Loanwords
can exist in any domains in a language; therefore,
we also conduct experiments on texts in several
domains.

Evaluation on Loanword Identiﬁcation Mod-
els

In this part, we introduce the experiment
sults on four models, then we analysis the reasons.

re-

From the Table 3 we can found that the perfor-
mance of RNN based model outperforms other three
approaches, we summarized possible reasons as
follow: 1) CRFs model rely heavily on labeled data,
because we only have limited training examples, the
CRFs model achieved lowest performance among
four models; 2) SSIM model based on two string
similarity algorithms: edit distance and the common
substring, compare with the RNN model, SSIM
has a limited ability of generalization, and cannot
capture semantic information in Uyghur texts, so
the SSIM achieved a relative low performance;
3) Several information including above two algo-
rithms are integrated into the CIBM model, and
consider the loanwords identiﬁcation as a classiﬁ-
cation problem, the performance of CIBM model
outperforms the CRFs model and SSIM model.
However, like the SSIM model, there is almost no
semantic information and limited generalization
ability, therefore, the performance of CIBM model
cannot achieve or outperform the RNN based model.

Evaluation on Features Used in RNN-based
Model

Features used in our model optimized the out-
put of loanword identiﬁcation. We show the
experimental results on combination of features:
RN N +f 0 (no additional feature used), RN N +f 1
(inverse language model feature used), RN N + f 2
(collocation feature used) and RN N + f 1 + f 2
(both inverse language model feature and colloca-
tion feature are used).

In Table 4, RN N + f 1 combines the in-
verse language model information into loanword
identiﬁcation model, which apply the local feature
in our task, so the performance of RN N + f 1
outperforms the basic RNN model and RN N + f 2.
RN N + f 2 integrated the collocation information
into the model, and the generation ability of the
model only rely on RNN, therefore, the performance
of RN N + f 2 only outperform the basic RNN
model. The RN N + f 1 + f 2 not only combine
the generalization ability into the model, but also
the local feature (f 1) and global feature (f 2). So

Languages P-Chn R-Chn F1-Chn P-Rus R-Rus F1-Rus P-Arab R-Arab F1-Arab
CRFs
SSIM
CIBM
RNNs

66.35
71.38
73.18
79.08

69.78
66.32
78.82
78.97

62.33
77.28
68.30
79.20

63.25
70.02
73.22
75.93

71.64
75.39
81.03
82.55

67.18
72.61
76.93
79.10

72.50
73.76
75.22
83.26

65.32
67.51
70.71
77.58

68.72
70.50
72.90
80.32

Table 3: Experimental Results on Loanword Identiﬁcation Models.

Languages
RNN+f0
RNN+f1
CIRNN+f2
RNN+f1+f2

P-Chn R-Chn F1-Chn P-Rus R-Rus F1-Rus P-Arab R-Arab F1-Arab
77.65
78.86
78.79
78.97

67.89
70.32
69.54
79.20

72.44
74.35
73.88
79.08

78.02
81.94
81.35
82.55

72.85
75.88
75.98
79.10

68.33
70.65
71.28
75.93

78.38
81.12
80.76
83.26

70.96
71.52
70.20
77.58

74.49
76.02
75.11
80.32

Table 4: Evaluation on Features Used in RNN-based Model.

the RN N + f 1 + f 2 model achieved the best
performance.

system with Arabic. These two reasons contribute to
Arabic loanwords identiﬁcation in Uyghur.

Evaluation on Cross-domain Corpora

We evaluate our model in two domains on dif-
sets: RN N LIS + N EW S and
ferent
RN N LIS + ORAL.

test

In Table 5,

the experimental results on news
(RN N LIS + N EW S) which is similar with our
training examples are outperform the results on
oral test set (RN N LIS + ORAL). This may be
no doubt. Amazingly, we found that performance
of RN N LIS + ORAL is just a little worse
compared with RN N LIS + N EW S. A possible
reason is that our model can learn representation of
knowledge beyond given training examples.

5.3 Discussion

In our experiments, we try to identify Chinese, Rus-
sian and Arabic loanwords in Uyghur texts. We
found that results on Arabic loanwords identiﬁcation
achieved the best performance. There are two possi-
ble reasons. First, most of loanwords labeled in the
training examples for Arabic loanwords identiﬁca-
tion are person names; therefore, it is relatively easy
to ﬁnd them out. Second, Persian has exerted some
inﬂuence on Arabic, and borrowing much vocabu-
lary from it. Meanwhile, Uyghur has historically
been inﬂuenced strongly by Persian, so Arabic loan-
words in Uyghur may have the similar pronunciation

We have limited number of labeled corpus, so a
competitive identiﬁcation result cannot be expected
if a traditional approach is used (such as the CRF).
Our proposed RNN Encoder-Decoder framework
can learn features automatically and use its inter-
nal memory to process arbitrary sequences of inputs.
Additionally, two features inverse language model
and collocation can constraint the output of identi-
ﬁcation model. Therefore, our model achieved the
best performance.

Loanword identiﬁcation models are all trained on
news corpora, so in cross-domain (news and oral)
experiments, results in news are outperform results
in oral. We analysis the results, and found that sev-
eral errors including spelling error are exist in oral
corpora, and these errors may affect the performance
of our model.

6 Related work

There has been relatively few previous works on
loanwords identiﬁcation in Uyghur. Our work is in-
spired by two lines of research: (1) recurrent neural
network; (2) loanwords detection.

6.1 Recurrent Neural Network

In recent years, the Recurrent Neural Network has
proven to be highly successful in capturing semantic
information in text and has improved the results of
several tasks in NLP area. (Socher et al., 2013) uses

Languages
RNNLIS+NEWS
RNNLIS+ORAL

P-Chn R-Chn F1-Chn P-Rus R-Rus F1-Rus P-Arab R-Arab F1-Arab
78.97
75.23

79.20
76.44

79.08
75.83

82.55
78.11

75.93
70.59

79.10
74.16

83.26
80.03

77.58
76.42

80.32
78.18

Table 5: Evaluation on Cross-Domain Corpora.

a recursive neural network to predict sentence senti-
ment. (Luong et al., 2013) generates better word rep-
resentation with recursive neural network. (Cho et
al., 2014a) proposed a RNN encoder-decoder model
to learn phrase representations in SMT.
(Irsoy et
al., 2014) introduce a deep recursive neural network,
and evaluate this model on the task of ﬁne-grained
sentiment classiﬁcation. (Liu et al., 2014) propose a
recursive recurrent neural network to model the end-
to-end decoding process for SMT; experiments show
that this approach can outperform the state-of-the-
art baseline. (Yao et al., 2013) optimized the recur-
rent neural network language model to perform lan-
guage understanding. (Graves , 2012) apply a RNN
based system in probabilistic sequence transduction.

6.2 Loanwords Detection

In general, word borrowing is often concerned by
linguists (Chen, 2011; Chen et al., 2011a). There
are relatively few researches about loanwords in
NLP area. (Tsvetkov et al., 2015a) and (Tsvetkov et
al., 2016) proposed a morph-phonological transfor-
mation model, features used in this model are based
on optimality theory; experiment has been proved
that with a few training examples, this model can
obtain good performance at predicting donor forms
from borrowed forms. (Tsvetkov et al., 2015) sug-
gest an approach that uses the lexical borrowing as a
model in SMT framework to translate OOV words in
a low-resource language. For loanwords detection in
Uyghur, string similarity based methods were often
used at the early stage (Mi et al., 2013). (Mi et al.,
2014) propose a loanword detection method based
on the perceptron model, several features are used
in model training.

7 Conclusion

We have presented an approach to identify loan-
words (Chinese, Russian and Arabic loanwords)
in Uyghur texts, our model based on the RNN
Encoder-Decoder framework. We also suggested

two important features: inverse language model and
collocation feature to optimize the output of our
loanword identiﬁcation model. Experimental results
show that our model achieves signiﬁcant improve-
ments in loanwords detection tasks. In the future, we
plan to further validate the effectiveness of our ap-
proach on more languages, especially on languages
with rich morphology.

Acknowledgments

We sincerely thank the anonymous reviewers for
their
thorough reviewing and valuable sugges-
tions. This work is supported by the West Light
Foundation of The Chinese Academy of Sciences
the Xinjiang
under Grant No.2015-XBQN-B-10,
Key Laboratory Fund under Grant No.2015KL031
and the Strategic Priority Research Program of
the Chinese Academy of Sciences under Grant
No.XDA06030400.

References

Shiming Chen. 2011. New Research on Chinese Loan-
words in the Uyghur Language. N.W.Journal of Eth-
nology, pages 176-180, 28(1).

Yan Chen and Ping Chen. 2011. A Comparison on the
methods of Uyghur and Chinese Loan Words. Journal
of Kashgar Teachers College pages 51-55, 32(2).
Kyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bah-
danau and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. In Proceedings of the Eighth Workshop on
Syntax, Semantics and Structure in Statistical Trans-
lation (SSST-8), pages 103-111, October 25, Doha,
Qatar. Association for Computational Linguistics.
Kyunghyun Cho, Bart van Merri¨enboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk
and Yoshua Bengio. 2014. Learning Phrase Repre-
sentations using RNN Encoder(cid:2)Decoder for Statisti-
cal Machine Translation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2014), pages 1724-1734,
October 25-29, Doha, Qatar. Association for Compu-
tational Linguistics.

2006), pages 81-88, July 17-21, Sydney, Australia. As-
sociation for Computational Linguistics.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
Christopher D. Manning, Andrew Ng and Christopher
Potts. 2013. Recursive Deep Models for Semantic
Compositionality Over a Sentiment Treebank. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP 2013),
pages 1631-1642, October 18-21, Seattle, Washington,
USA. Association for Computational Linguistics.
Ilya Sutskever, Oriol Vinyals and Quoc V. Le. 2014. Se-
quence to Sequence Learning with Neural Networks.
In Proceedings of the 2014 Conference on Advances in
Neural Information Processing Systems (NIPS 2014),
pages 3104-3112, December 8-13, Montr´eal, Canada.
Yulia Tsvetkov, Waleed Ammar and Chris Dyer. 2015.
Constraint-Based Models of Lexical Borrowing.
In
Proceedings of the 2015 Conference on Human Lan-
guage Technologies: The 2015 Annual Conference of
the North American Chapter of the ACL (NAACL-HLT
2015), pages 598-608, May 31-June 5, Denver, Col-
orado.

Yulia Tsvetkov and Chris Dyer. 2016. Cross-Lingual
Jour-
Bridges with Models of Lexical Borrowing.
nal of Artiﬁcial Intelligence Research pages 63-93,
55(2016).

Yulia Tsvetkov and Chris Dyer. 2015. Lexicon Stratiﬁ-
In
cation for Translating Out-of-Vocabulary Words.
Proceedings of the 53rd Annual Meeting of the As-
sociation for Computational Linguistics and the 7th
International Joint Conference on Natural Language
Processing (Short Papers)(ACL-IJCNLP 2015), pages
125(cid:2)131, July 26-31, Beijing, China.

Kaisheng Yao, Geoffrey Zweig, Mei-Yuh Hwang,
Yangyang Shi and Dong Yu. 2013. Recurrent Neural
Networks for Language Understanding.
In Proceed-
ings of 14th Annual Conference of the Inter national
Speech Communication Association (INTERSPEECH
2013), pages 2524-2528, August 25-29, Lyon, France.

Alex Graves. 2012. Sequence Transduction with Recur-
rent Neural Networks. In Proceedings ICML Repre-
sentation Learning Workshop, Edinburgh, Scotland.

Ozan Irsoy and Claire Cardie.

2012. Deep Recur-
sive Neural Networks for Compositionality in Lan-
guage.
In Proceedings of the 2014 Conference
on Advances in Neural Information Processing Sys-
tems (NIPS 2014), pages 2096-2104, December 8-13,
Montr´eal, Canada.

Herbert Jaeger.

2002. A tutorial on training recur-
rent neural networks, covering BPPT, RTRL, EKF
and the ”echo state network” approach.
GMD-
Forschungszentrum Informationstechnik.

John Lafferty, Andrew McCallum and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data.
In
Proceedings of the Eighteenth International Confer-
ence on Machine Learning (ICML 2001), pages 282-
289, June 28-July 2, Bellevue, Washington, USA.
Gennadi Lembersky, Noam Ordan and Shuly Wintner.
2012. Language Models for Machine Translation:
Original vs. Translated Texts. Computational Linguis-
tics, pages 799-825, 38(4). Association for Computa-
tional Linguistics.

Shujie Liu, Nan Yang, Mu Li and Ming Zhou. 2014.
A Recursive Recurrent Neural Network for Statisti-
cal Machine Translation. In Proceedings of the 52nd
Annual Meeting of the Association for Computational
Linguistics (ACL 2014), pages 1491-1500, June 23-25,
Baltimore, Maryland, USA. Association for Computa-
tional Linguistics.

Thang Luong, Richard Socher and Christopher Manning.
2013. Better Word Representations with Recursive
Neural Networks for Morphology. In Proceedings of
the Seventeenth Conference on Computational Natural
Language Learning (CoNLL 2013), pages 104-113,
August 8-9, Soﬁa, Bulgaria. Association for Compu-
tational Linguistics.

Chenggang Mi, Yating Yang, Xi Zhou, Xiao Li and
Mingzhong Yang.
2013. Recognition of Chinese
Loan Words in Uyghur Based on String Similarity.
Journal of Chinese Information Processing, pages
173-179, 27(5).

Chenggang Mi, Yating Yang, Lei Wang, Xiao Li and Ka-
mali Dalielihan. 2014. Detection of Loan Words in
Uyghur Texts. In Proceedings of the 3rd International
Conference on Natural Language Processing and Chi-
nese Computing (NLPCC 2014), pages 103-112, De-
cember 5-9, Shen Zhen, China.

Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting Parallel Sub-Sentential Fragments from Non-
Parallel Corpora.
In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the ACL (COLING-ACL

