6
1
0
2
 
y
a
M
 
6
2
 
 
]
I

A
.
s
c
[
 
 
2
v
9
6
9
7
0
.
5
0
6
1
:
v
i
X
r
a

Adaptive Neural Compilation

Rudy Bunel∗
University of Oxford
rudy@robots.ox.ac.uk

Alban Desmaison∗
University of Oxford
alban@robots.ox.ac.uk

Pushmeet Kohli
Microsoft Research
pkohli@microsoft.com

Philip H.S. Torr
University of Oxford
philip.torr@eng.ox.ac.uk

M. Pawan Kumar
University of Oxford
pawan@robots.ox.ac.uk

Abstract

This paper proposes an adaptive neural-compilation framework to address
the problem of eﬃcient program learning. Traditional code optimisation
strategies used in compilers are based on applying pre-speciﬁed set of
transformations that make the code faster to execute without changing
its semantics. In contrast, our work involves adapting programs to make
them more eﬃcient while considering correctness only on a target input
distribution. Our approach is inspired by the recent works on diﬀerentiable
representations of programs. We show that it is possible to compile programs
written in a low-level language to a diﬀerentiable representation. We also
show how programs in this representation can be optimised to make them
eﬃcient on a target distribution of inputs. Experimental results demonstrate
that our approach enables learning speciﬁcally-tuned algorithms for given
data distributions with a high success rate.

1

Introduction

Algorithm design often requires making simplifying assumptions about the input data.
Consider, for instance, the computational problem of accessing an element in a linked list.
Without the knowledge of the input data distribution, one can only specify an algorithm
that runs in a time linear in the number of elements of the list. However, suppose all the
linked lists that we encountered in practice were ordered in memory. Then it would be
advantageous to design an algorithm speciﬁcally for this task as it can lead to a constant
running time. Unfortunately, the input data distribution of a real world problem cannot be
easily speciﬁed as in the above simple example. The best that one can hope for is to obtain
samples drawn from the distribution. A natural question that arises from these observations:
“How can we adapt a generic algorithm for a computational task using samples from an
unknown input data distribution?”

The process of ﬁnding the most eﬃcient implementation of an algorithm has received
considerable attention in the theoretical computer science and code optimisation community.
Recently, Conditionally Correct Superoptimization [14] was proposed as a method for
leveraging samples of the input data distribution to go beyond semantically equivalent
optimisation and towards data-speciﬁc performance improvements. The underlying procedure
is based on a stochastic search over the space of all possible programs. Additionally, they
restrict their applications to reasonably small, loop-free programs, thereby limiting their
impact in practice.

In this work, we take inspiration from the recent wave of machine-learning frameworks for
estimating programs. Using recurrent models, Graves et al. [2] introduced a fully diﬀerentiable

∗The ﬁrst two authors contributed equally.

representation of a program, enabling the use of gradient-based methods to learn a program
from examples. Many other models have been published recently [3, 5, 6, 8] that build and
improve on the early work by Graves et al. [2]. Unfortunately, these models are usually
complex to train and need to rely on methods such as curriculum learning or gradient noise
to reach good solutions as shown by Neelakantan et al. [10]. Moreover, their interpretability
is limited. The learnt model is too complex for the underlying algorithm to be recovered
and transformed into a regular computer program.

The main focus of the machine-learning community has thus far been on learning programs
from scratch, with little emphasis on running time. However, for nearly all computational
problems, it is feasible to design generic algorithms for the worst-case. We argue that a
more pragmatic goal for the machine learning community is to design methods for adapting
existing programs for speciﬁc input data distributions. To this end, we propose the Adaptive
Neural Compiler (ANC). We design a compiler capable of mechanically converting algorithms
to a diﬀerentiable representation, thereby providing adequate initialisation to the diﬃcult
problem of optimal program learning. We then present a method to improve this compiled
program using data-driven optimisation, alleviating the need to perform a wide search over
the set of all possible programs. We show experimentally that this framework is capable of
adapting simple generic algorithms to perform better on given datasets.

2 Related Works

The idea of compiling programs to neural networks has previously been explored in the
literature. Siegelmann [15] described how to build a Neural Network that would perform
the same operations as a given program. A compiler has been designed by Gruau et al. [4]
targeting an extended version of Pascal. A complete implementation was achieved when
Neto et al. [11] wrote a compiler for NETDEF, a language based on the Occam programming
language. While these methods allow us to obtain an exact representation of a program as a
neural network, they do not lend themselves to optimisation to improve the original program.
Indeed, in their formulation, each elementary step of a program is expressed as a group of
neurons with a precise topology, set of weights and biases, thereby rendering learning via
gradient descent infeasible. Performing gradient descent in this parameter space would result
in invalid operations and thus is unlikely to lead to any improvement. The recent work by
Reed and de Freitas [12] on Neural Programmer-Interpreters (NPI) can also be seen as a way
to compile any program into a neural network by learning a model that mimic the program.
While more ﬂexible than the previous approaches, the NPI is unable to improve on a learned
program due to its dependency on a non-diﬀerentiable environment.

Another approach to this learning problem is the one taken by the code optimisation
community. By exploring the space of all possible programs, either exhaustively [9] or in a
stochastic manner [13], they search for programs having the same results but being more
eﬃcient. The work of Sharma et al. [14] broadens the space of acceptable improvements
to data-speciﬁc optimisations as opposed to the provably equivalent transformations that
were previously the only ones considered. However, this method is still reliant on non-
gradient-based methods for eﬃcient exploration of the space. By representing everything in
a diﬀerentiable manner, we aim to obtain gradients to guide the exploration.

Recently, Graves et al. [2] introduced a learnable representation of programs, called the
Neural Turing Machine (NTM). The NTM uses an LSTM as a Controller, which outputs
commands to be executed by a deterministic diﬀerentiable Machine. From examples of
input/output sequences, they manage to learn a Controller such that the model becomes
capable of performing simple algorithmic tasks. Extensions of this model have been proposed
in [3, 5] where the memory tape was replaced by diﬀerentiable versions of stacks or lists.
Kurach et al. [8] modiﬁed the NTM to introduce a notion of pointers making it more
amenable to represent traditional programs. Parallel works have been using Reinforcement
Learning techniques such as the REINFORCE algorithm [1, 16, 17] or Q-learning [18] to
be able to work with non diﬀerentiable versions of the above mentioned models. All these
models are trained only with a loss based on the diﬀerence between the output of the model
and the expected output. This weak supervision leads to a complex training. For instance

2

IR0

R0

Controller

Controller

Machine

Machine

IR1

R1

M1

stop

...

...

...

IR2

stop

R2

M2

M0

Memory

Memory

MT

Inst

arg1

arg2

output

side eﬀect

STOP
ZERO
INC
DEC
ADD
SUB
MIN
MAX
READ
WRITE

JEZ

-
-
a
a
a
a
a
a
a
a

a

-
-
-
-
b
b
b
b
-
b

b

0
0
a+1
a-1
a+b
a-b
min(a,b)
max(a,b)
mt
a
0

0

stop = 1
-
-
-
-
-
-
-
Memory access
mt
a = b
IRt = b
if a = 0

(a) General view of the whole Model.

(b) Machine instructions.

Figure 1: Model components.

the Neural RAM [8] requires a high number of random restarts before converging to a correct
solution [10], even when using the best hyperparameters obtained through a large grid search.

In our work, we will ﬁrst show that we can design a new neural compiler whose target will
be a Controller-Machine model. This makes the compiled model amenable to learning from
examples. Moreover, we can use it as initialisation for the learning procedure, allowing us to
aim for the more complex task of ﬁnding an eﬃcient algorithm.

3 Model

Our model is composed of two parts: (i) a Controller, in charge of specifying what should
be executed; and (ii) a Machine, following the commands of the Controller. We start by
describing the global architecture of the model. For the sake of simplicity, the general
description will present a non-diﬀerentiable version of the model. Section 3.2 will then
explain the modiﬁcations required to make this model completely diﬀerentiable. A more
detailed description of the model is provided in appendix A.

3.1 General Model

1, rt

1, mt

2, . . . , rt

2, . . . , mt

We ﬁrst deﬁne for each timestep t the memory tape that contains M integer values
Mt = {mt
M }, the registers that contain R values Rt = {rt
R} and the
instruction register that contain a single value IRt. We also deﬁne a set of instructions
that can be executed, whose main role is to perform computations using the registers. For
example, add the values contained in two registers. We also deﬁne as a side eﬀect any action
that involves elements other than the input and output values of the instruction. Interaction
with the memory is an example of such side eﬀect. All the instructions, their computations
and side eﬀects are detailed in Figure 1b.
As can be seen in Figure 1a the execution model takes as input an initial memory tape M0
and outputs a ﬁnal memory tape MT after T steps. At each step t, the Controller uses
the instruction register IRt to compute the command for the Machine. The command is
a 4-tuple e, a, b, o. The ﬁrst element e is the instruction that should be executed by the
Machine, enumerated as an integer. The elements a and b specify which registers should be
used as arguments for the given instruction. The last element o speciﬁes in which register
the output of the instruction should be written. For example, the command {ADD, 2, 3, 1}
means that only the value of the ﬁrst register should change, following rt+1
2, rt
3).
Then the Machine will execute this command, updating the values of the memory, the
registers and the instruction register. The Machine always performs two other operations
apart from the required instruction. It outputs a stop ﬂag that allows the model to decide
when to stop the execution. It also increments the instruction register IRt by one at each
iteration.

1 = ADD(rt

3.2 Diﬀerentiability

The model presented above is a simple execution machine but it is not diﬀerentiable. In
order to be able to train this model end-to-end from a loss deﬁned over the ﬁnal memory
tape, we need to make every intermediate operation diﬀerentiable.

3

To achieve this, we replace every discrete value in our model by a multinomial distribution
over all the possible values that could have been taken. Moreover, each hard choice that
would have been non-diﬀerentiable is replaced by a continuous soft choice. We will henceforth
use bold letters to indicate the probabilistic version of a value.
First, the memory tape Mt is replaced by an M × M matrix Mt, where Mt
i,j corresponds
i taking the value j. The same change is applied to the registers Rt,
to the probability of mt
replacing them with an R × M matrix Rt, where Rt
i taking
the value j. Finally, the instruction register is also transformed from a single value IRt to a
vector of size M noted IRt, where the i-th element represents its probability to take the
value i.

i,j represents the probability of rt

The Machine does not contain any learnable parameter and will just execute a given command.
To make it diﬀerentiable, the Machine now takes as input four probability distributions et,
at, bt and ot, where et is a distribution over instructions, and at, bt and ot are distributions
t as convex combinations
over the registers. We compute the argument values arg1
of the diﬀerent registers:

t and arg2

arg1

t =

irt
at
i

arg2

t =

irt
bt
i,

(1)

R
X

i=1

i and bt

where at
compute the output value of each instruction k using the following formula:
X

i are the i-th values of the vectors at and bt. Using these values, we can

arg1

t
i · arg2

j · 1[gk(i, j) = c mod M ],
t

(2)

∀0 ≤ c ≤ M outt

k,c =

R
X

i=1

0≤i,j≤M

where gk is the function associated to the k-th instruction as presented in Table 1b. Since the
executed instruction is controlled by the probability e, the output written to the register will
also be a convex combination: outt = PN
k=1 et
k, where N is the number of instructions.
This value is then stored into the registers by performing a soft-write parametrised by ot.

koutt

A special case is associated with the stop signal. When executing the model, we keep track of
the probability that the program should have terminated before this iteration based on the
probability associated at each iteration with the speciﬁc instruction that controls this ﬂag.
Once this probability goes over a threshold ηstop ∈ (0, 1], the execution is halted. We applied
the same techniques to make the side-eﬀects diﬀerentiable, this is presented in appendix A.1.

et = We ∗ IRt,

at = Wa ∗ IRt,

The Controller is the only learnable part of our model. The ﬁrst learnable part is the initial
values for the registers R0 and for the instruction register IR0. The second learnable part
is the parameters of the Controller which computes the required distributions using:
ot = Wo ∗ IRt

(3)
where We is an N × M matrix and Wa, Wb and Wo are R × M matrices. A representation
of these matrices can be found in Figure 4c. The Controller as deﬁned above is composed of
four independent, fully-connected layers. In Section 4.3 we will see that this complexity is
suﬃcient for our model to be able to represent any program.
Henceforth, we will denote by θ = {R0, IR0, We, Wa, Wb, Wo} the set of all the learnable
parameters of this model.

bt = Wb ∗ IRt,

4 Adaptative Neural Compiler

We will now present the Adaptive Neural Compiler.
Its goal is to ﬁnd the best set of
weights θ∗ for a given dataset such that our model will perform the correct input/output
mapping as eﬃciently as it can. We begin by describing our learning objective in details.
The two subsequent sections will focus on making the optimisation of our learning objective
computationally feasible.

4.1 Objective function

Our goal is to solve a given algorithmic problem eﬃciently. The algorithmic problem is
deﬁned as a set of input/output pairs. We also have access to a generic program that is able

4

to perform the required mapping. In our example of accessing elements in a linked list, the
transformation would consist in writing down the desired value at the speciﬁed position in
the tape. The program given to us would iteratively go through the elements of the linked
list, ﬁnd the desired value and write it down at the desired position. If there exists some
bias that would allow this traversal to be faster, we expect the program to exploit it.

Our approach to this problem is to construct a diﬀerentiable objective function, mapping
controller parameters to a loss. We deﬁne this loss based on the states of the memory tape
and outputs of the Controller at each step of the execution. The precise mathematical
formulation for each term of the loss is given in appendix B. Here we present the motivation
behind each of them.

Correctness For a given input, we have the expected output. We compare the values of
the expected output with the ﬁnal memory tape provided by the execution.

Halting To prevent programs to take an inﬁnite amount of time without stopping, we
deﬁned a maximum number of iterations Tmax after which the execution is halted. Moreover,
we add a penalty in the loss if the Controller didn’t halt before this limit.

Eﬃciency We penalise each iteration taken by the program where it does not stop.

Conﬁdence We add a term which will penalise probability of stopping if the current state
of the memory is not the expected one.

If only the correctness term was considered, nothing would encourage the learnt algorithm to
halt as soon as it ﬁnished. If only correctness and halting were considered, then the program
may not halt as early as possible. Conﬁdence enables the algorithm to evaluate better when
to stop.

The loss is a weighted sum of the four above-mentioned terms. We denote the loss of the i-th
training sample, given parameters θ, as Li(θ). Our learning objective is then speciﬁed as:

min
θ

X

i

Li(θ)

s.t. θ ∈ Θ,

(4)

where Θ is a set over the parameters such that the outputs of the Controller, the initial
values of each register and of the instruction register are all probability distributions.

The above optimisation is a highly non-convex problem. To be able to solve it using standard
gradient descent based methods, we will ﬁrst need to transform it to an unconstrained
problem. We also know that the result of the optimisation of a non-convex objective function
is strongly dependent on the initialisation point. In the rest of this section, we will ﬁrst
present a small modiﬁcation to the model that will remove the constraints. We will then
present our Neural Compiler that will provide a good initialisation to solve this problem.

4.2 Reformulation

In order to use gradient descent methods without having to project the parameters on Θ,
we alter the formulation of the controller. We add a softmax layer after each linear layer
ensuring that the constraints on the Controller’s output will be respected. We also apply a
softmax to the initial values of the registers and the instructions register, ensuring they will
also respect the original constraints. This way, we transform the constrained-optimisation
problem into an unconstrained one, allowing us to use standard gradient descent methods.
As discussed in other works [10], this kind of model is hard to train and requires a high
number of random restarts before converging to a good solution. We will now present a
Neural Compiler that will provide good initialisations to help with this problem.

4.3 Neural Compiler

The goal for the Neural Compiler is to convert an algorithm, written as an unambiguous
program, to a set of parameters. These parameters, when put into the controller, will
reproduce the exact steps of the algorithm. This is very similar to the problem framed by
Reed and de Freitas [12], but we show here a way to accomplish it without any learning.

5

var head = 0;
var nb_jump = 1;
var out_write = 2;

nb_jump = READ(nb_jump);
out_write = READ(out_write);

loop : head = READ(head);

nb_jump = DEC(nb_jump);
JEZ(nb_jump, end);
JEZ(0, loop);
end : head = INC(head);

head = READ(head);
WRITE(out_write, head);
STOP();

Initial Registers:

R1 = 6; R2 = 2; R3 = 0;
R4 = 2; R5 = 1; R6 = 0;
R7 = 0;

Program:

0 : R5 = READ (R5, R7)
1 : R4 = READ (R4, R7)
2 : R6 = READ (R6, R7)
3 : R5 = DEC (R5, R7)
(R5, R1)
4 : R7 = JEZ
(R3, R2)
5 : R3 = JEZ
6 : R6 = INC
(R6, R7)
7 : R6 = READ (R6, R7)
8 : R7 = WRITE(R4, R6)
9 : R7 = STOP (R7, R7)

(i) Instr.

(ii) Arg1

(iii) Arg2

(iv) Out

(a) Input program

(a) Intermediary representation

(c) Weights

Figure 4: Example of the compilation process. (2a) Program written to perform the ListK task.
Given a pointer to the head of a linked list, an integer k, a target cell and a linked list, write in
the target cell the k-th element of the list. (3a) Intermediary representation of the program. This
corresponds to the instruction that a Random Access Machine would need to perform to execute the
program. (4c) Representation of the weights that encodes the intermediary representation. Each
row of the matrix correspond to one state/line. Initial value of the registers are also parameters of
the model, omitted here.

The diﬀerent steps of the compilation are illustrated in Figure 4. The ﬁrst step is to go from
the written version of the program to the equivalent list of low level instruction. This step can
be seen as going from Figure 2a to Figure 3a. The illustrative example uses a fairly low-level
language but traditional features of programming languages such as loops or if-statements
can be supported using the JEZ instruction. The use of constants as arguments or as values
is handled by introducing new registers that hold these values. The value required to be
passed as target position to the JEZ instruction can be resolved at compile time.

Having obtained this intermediate representation, generating the parameters is straight-
forward. As can be seen in Figure 3a, each line contains one instruction, the two input
registers and the output register, and corresponds to a command that the Controller will
have to output. If we ensure that IR is a Dirac-delta distribution on a given value, then
the matrix-vector product is equivalent to selecting a row of the weight matrix. As IR is
incremented at each iteration, the Controller outputs the rows of the matrix in order. We
thus have a one-to-one mapping between the lines of the intermediate representation and
the rows of the weight matrix. An example of these matrices can be found in Figure 4c. The
weight matrix has 10 rows, corresponding to the number of lines of code of our intermediate
representation. On the ﬁrst line of the matrix corresponding to the ﬁrst argument (4cii), the
ﬁfth element has value 1, and is linked to the ﬁrst line of code where the ﬁrst argument to
the READ operation is the ﬁfth register.

The number of rows of the weight matrix is linear in the number of lines of code in the original
program. To output a command, we must be able to index its line with the instruction
register IR, which means that the largest representable number in our Machine needs to be
greater than the number of lines in our program.

Moreover, any program written in a regular assembly language can be rewritten to use only
our restricted set of instructions. This can be done ﬁrst because all the conditionals of the
the assembly language can be expressed as a combination of arithmetic and JEZ instructions.
Secondly because all the arithmetic operations can be represented as a combination of our
simple arithmetic operations, loops and ifs statements. This means that any program that
can run on a regular computer, can be ﬁrst rewritten to use our restricted set of instructions
and then compiled down to a set of weights for our model. Even though other models use
LSTM as controller, we showed here that a Controller composed of simple linear functions is
expressive enough. The advantage of this simpler model is that we can now easily interpret

6

the weights of our model in a way that would not have be possible if we had a recurrent
network as a controller.

The most straightforward way to leverage the results of the compilation is to initialise the
Controller with the weights obtained through compilation of the generic algorithm. To
account for the extra softmax layer, we need to multiply the weights produced by the compiler
by a large constant to output Dirac-delta distributions. Some results associated with this
technique can be found in Section 5.1. However, if we initialise with exactly this sharp set of
parameters, the training procedure is not able to move away from the initialisation as the
gradients associated with the softmax in this region are very small. Instead, we initialise
the controller with a non-ideal version of the generic algorithm. This means that the choice
with the highest probability in the output of the Controller is correct, but the probability of
other choices is not zero. As can be seen in Section 5.2, this allows the Controller to learn
by gradient descent a new algorithm, diﬀerent from the original one, that has a lower loss
than the ideal version of the compiled program.

5 Experiments

We performed two sets of experiments. The ﬁrst shows the capability of the Neural Compiler
to perfectly reproduce any given program. The second shows that our Neural Compiler
can adapt and improve the performance of programs. We present results of data-speciﬁc
optimisation being carried out and show decreases in runtime for all the algorithms and
additionally, for some algorithms, show that the runtime is a diﬀerent computational-
complexity class altogether. All the code required to reproduce these experiments is available
online 1.

5.1 Compilation

The compiler described in section 4.3 allows us to go from a program written using our
instruction set to a set of weights θ for our Controller.

To illustrate this point, we implemented simple programs that can solve the tasks introduced
by Kurach et al. [8] and a shortest path problem. One of these implementations can be
found in Figure 2a, while the others are available in appendix F. These programs are written
in a speciﬁc language, and are transformed by the Neural Compiler into parameters for the
model. As expected, the resulting models solve the original tasks exactly and can generalise
to any input sequence.

5.2 ANC experiments

In addition to being able to reproduce any given program as was done by Reed and de Freitas
[12], we have the possibility of optimising the resulting program further. We exhibit this
by compiling program down to our model and optimising their performance. The eﬃciency
gains for these tasks come either from ﬁnding simpler, equivalent algorithms or by exploiting
some bias in the data to either remove instructions or change the underlying algorithm.

We identify three diﬀerent levels of interpretability for our model: The ﬁrst type corresponds
to weights containing only Dirac-delta distributions, there is an exact one-to-one mapping
between lines in the weight matrices and lines of assembly code. In the second type where
all probabilities are Dirac-delta except the ones associated with the execution of the JEZ
instruction, we can recover an exact algorithm that will use if statements to enumerate the
diﬀerent cases arising from this conditional jump. In the third type where any operation
other than JEZ is executed in a soft way or use a soft argument, it is not possible to recover
a program that will be as eﬃcient as the learned one.

We present here brieﬂy the considered tasks and biases, and report the reader to appendix F
for a detailed encoding of the input/output tape.

1https://github.com/albanD/adaptive-neural-compilation

7

Table 1: Average numbers of iterations required to solve instances of the problems for the original
program, the best learned program and the ideal algorithm for the biased dataset. We also include
the success rate of reaching a more eﬃcient algorithm across multiple random restarts.

Access

Increment Swap ListK Addition Sort

Generic
Learned
Ideal

6
4
4

40
16
34

10
6
6

18
11
10

20
9
6

38
18
9.5

Success Rate

37 %

84%

27%

19%

12%

74%

1. Access: Given a value k and an array A, return A[k]. In the biased version, the
value of k is always be the same, so the address of the required element can be stored
in a constant. This is similar to the optimisation known as constant folding.

2. Swap: Given an array A and two pointers p and q, swap the elements A[p] and A[q].
In the biased version, p and q are always the same so reading them can be avoided.
3. Increment: Given an array, increment all its element by 1. In the biased version,
the array is of ﬁxed size and the elements of the array have the same value so you
don’t need to read all of them when going through the array.

4. Listk: Given a pointer to the head of a linked list, a number k and a linked list,
ﬁnd the value of the k-th element. In the biased version, the linked list is organised
in order in memory, as would be an array, so the address of the k-th value can be
computed in constant time. This is the example developed in Figure 4.

5. Addition: Two values are written on the tape and should be summed. No data bias
is introduced but the starting algorithm is non-eﬃcient: it performs the addition as
a series of increment operation. The more eﬃcient operation would be to add the
two numbers.

6. Sort: Given an array A, sort it. In the biased version, only the start of the array
might be unsorted. Once the start has been arranged, the end of the array can be
safely ignored.

For each of these tasks, we perform a grid search on the loss parameters and on our hyper-
parameters. Training is performed using Adam [7]. We choose the best set of hyperparameters
and run the optimisation with 100 diﬀerent random seeds. We consider that a program has
been successfully optimised when two conditions are fulﬁlled. First, it needs to output the
correct solution for all test cases presenting the same bias. Second, the average number of
iterations taken to solve a problem must be lower than the algorithm used for initialisation.
Note that if we cared only about the ﬁrst criterion, the methods presented in Section 5.1
would already provide a success rate of 100%, without requiring any training.

The results are presented in Table 1. For each of these tasks, we manage to ﬁnd faster
algorithms. In the simple cases of Access and Swap, the optimal algorithm for the presented
datasets are obtained. Exploiting the bias of the data, successful heuristics are incorporated
in the algorithm and appropriate constants get stored in the initial value of registers. The
learned programs for these tasks are always in the ﬁrst case of interpretability, this means
that we can recover the most eﬃcient algorithm from the learned weights.

While ListK and Addition have lower success rates, the improvements between the original
and learned algorithms are still signiﬁcant. Both were initialised with iterative algorithms
with O(n) complexities. They managed to ﬁnd constant time O(1) algorithms to solve the
given problems, making the runtime independent of the input. Achieving this means that
the equivalence between the two approaches has been identiﬁed, similar to how optimising
compilers operate. Moreover, on the ListK task, some learned programs corresponds to
the second type of interpretability. Indeed these programs use soft jumps to condition the
execution on the value of k. Even though these program would not generalise to other values
of k, some learned programs for this task achieve a type one interpretability and a study of
the learned algorithm reveal that they can generalise to any value of k.

8

Finally, the Increment task achieves an unexpected result. Indeed, it is able to outperform
our best possible algorithm. By looking at the learned program, we can see that it is actually
leveraging the possibility to perform soft writes over multiple elements of the memory at
the same time to reduce its runtime. This is the only case where we see a learned program
associated with the third type of interpretability. While our ideal algorithm would give a
conﬁdence of 1 on the output, this algorithm is unable to do so, but it has a high enough
conﬁdence of 0.9 to be considered a correct algorithm.

In practice, for all but the most simple tasks, we observe that further optimisation is possible,
as some useless instructions remain present. Some transformations of the controller are indeed
diﬃcult to achieve through the local changes operated by the gradient descent algorithm.
An analysis of these failure modes of our algorithm can be found in appendix G.4. This
motivates us to envision the use of approaches other than gradient descent to address these
issues.

6 Discussion

The work presented here is a ﬁrst step towards adaptive learning of programs. It opens
up several interesting directions of future research. For exemple, the deﬁnition of eﬃciency
that we considered in this paper is ﬂexible. We chose to only look at the average number
of operations executed to generate the output from the input. We leave the study of other
potential measures such as Kolmogorov Complexity and sloc, to name a few, for future
works.

As shown in the experiment section, our current method is very good at ﬁnding eﬃcient
solutions for simple programs. For more complex programs, only a solution close to the
initialisation can be found. Even though training heuristics could help with the tasks
considered here, they would likely not scale up to real applications. Indeed, the main problem
we identiﬁed is that the gradient-descent based optimisation is unable to explore the space
of programs eﬀectively, by performing only local transformations. In future work, we want
to explore diﬀerent optimisation methods. One approach would be to mix global and local
exploration to improve the quality of the solutions. A more ambitious plan would be to
leverage the structure of the problem and use techniques from combinatorial optimisation to
try and solve the original discrete problem.

References

[1] Marcin Andrychowicz and Karol Kurach. Learning eﬃcient algorithms with hierarchical

attentive memory. CoRR, 2016.

[2] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, 2014.

[3] Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom.

Learning to transduce with unbounded memory. In NIPS, 2015.

[4] Frédéric Gruau, Jean-Yves Ratajszczak, and Gilles Wiber. A neural compiler. Theoretical

Computer Science, 1995.

recurrent nets. In NIPS, 2015.

[5] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented

[6] Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. In ICLR, 2016.

[7] Diederik Kingma and Jimmy Adam. A method for stochastic optimization. In ICLR,

2015.

chines. In ICLR, 2016.

[8] Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access ma-

[9] Henry Massalin. Superoptimizer: a look at the smallest program. In ACM SIGPLAN

Notices, volume 22, pages 122–126. IEEE Computer Society Press, 1987.

9

[10] Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol
Kurach, and James Martens. Adding gradient noise improves learning for very deep
networks. In ICLR, 2016.

[11] João Pedro Neto, Hava Siegelmann, and Félix Costa. Symbolic processing in neural

networks. Journal of the Brazilian Computer Society, 2003.

[12] Scott Reed and Nando de Freitas. Neural programmer-interpreters. In ICLR, 2016.

[13] Eric Schkufza, Rahul Sharma, and Alex Aiken. Stochastic superoptimization. In ACM

SIGARCH Computer Architecture News, 2013.

[14] Rahul Sharma, Eric Schkufza, Berkeley Churchill, and Alex Aiken. Conditionally correct

superoptimization. In OOPSLA, 2015.

[15] Hava Siegelmann. Neural programming language. In AAAI, 1994.

[16] Ronald Williams. Simple statistical gradient-following algorithms for connectionist

reinforcement learning. Machine learning, 1992.

[17] Wojciech Zaremba and Ilya Sutskever. Reinforcement learning neural turing machines.

arXiv preprint arXiv:1505.00521, 2015.

[18] Wojciech Zaremba, Tomas Mikolov, Armand Joulin, and Rob Fergus. Learning simple

algorithms from examples. CoRR, 2015.

7 Appendix

A Detailed Model Description

In this section, we are going to precisely deﬁne the non diﬀerentiable model used above. This
model can be seen as a recurrent network. Indeed, it takes as input an initial memory tape,
performs a certain number of iterations and outputs a ﬁnal memory tape. The memory tape
is an array of M cells, where a cell is an element holding a single integer value. The internal
state of this recurrent model are the memory, the registers and the instruction register.
The registers are another set of R cells that are internal to the model. The instruction
register is a single cell used in a speciﬁc way described later. These internal states are noted
R} and IRt for the memory, the registers and
Mt = {mt
M }, Rt = {rt
2, . . . , mt
the instruction register respectively.

2, . . . , rt

1, mt

1, rt

Figure 1 describes in more detail how the diﬀerent elements interact with each other. At
each iteration, the Controller takes as input the value of the instruction register IRt and
outputs four values:

et, at, bt, ot = Controller(IRt).
(5)
The ﬁrst value et is used to select one of the instruction of the Machine to execute at this
iteration. The second and third values at and bt will identify which registers to use as the
ﬁrst and second argument for the selected instruction. The fourth value ot identity the
output register where to write the result of the executed instruction. The Machine then
takes as input these four values and the internal state and computes the updated value of
the internal state and a stop ﬂag:

Mt+1, Rt+1, IRt+1, stop = Machine(Mt, Rt, IRt, et, at, bt, ot).

(6)

The stop ﬂag is a binary ﬂag. When its value is 1, it means that the model will stop the
execution and the current memory state will be returned.

10

The Machine The machine is a deterministic function that increments the instruction
register and executes the command given by the Controller to update the current internal
state. The set of instructions that can be executed by the Machine can be found in Table 1b.
Each instruction takes two values as arguments and returns a value. Additionally, some of
these instructions have side eﬀects. This mean that they do not just output a value, they
perform another task. This other task can be for example to modify the content of the
memory. All the considered side eﬀects can be found in Table 1b. By convention, instructions
that don’t have a value to return and that are used only for their side-eﬀect will return a
value of 0.

The Controller The Controller is a function that takes as input a single value and outputs
four diﬀerent values. The Controller’s internal parameters, the initial values for the registers
and the initial value of the instruction register deﬁne uniquely a given Controller.

The usual choice in the literature is to use an LSTM network[2, 3, 8] as controller. Our
choice was to instead use a simpler model. Indeed, our Controller associates a command
to each possible value of the instruction register. Since the instruction register’s value will
increase by one at each iteration, this will enforce the Controller to encode in its weights
what to do at each iteration. If we were using a recurrent controller the same instruction
register could potentially be associated to diﬀerent sets of outputs and we would lose this
one to one mapping.

To make this clearer, we ﬁrst rewrite the instruction register as an indicator vector with a 1
at the position of its value:

Ii =

(cid:26)1
0

if i = IRt
otherwise

.

(7)

In this case, we can write a single output at of the Controller as the result of a linear function
of I:

at = Wa ∗ I ,
(8)
where Wa is the 1xM matrix containing the value that need to be chosen as ﬁrst arguments for
each possible value of the instruction register and ∗ represent a matrix vector multiplication.

A.1 Mathematical details of the diﬀerentiable model

In order to make the model diﬀerentiable, every value and every choice are replaced by
probability distributions over the possible choices. Using convex combinations of probability,
the execution of the Machine is made diﬀerentiable. We present here the mathematical
formulation of this procedure for the case of the side-eﬀects.

STOP In the discrete model, the execution is halted when the STOP instruction is
executed. However, in the diﬀerentiable model, the STOP instruction may be executed
with a probability smaller than 1. To take this into account, when executing the model, we
keep track of the probability that the program should have terminated before this iteration
based on the probability associated to the STOP instruction at each iteration. Once this
probability goes over a threshold ηstop ∈]0, 1], the execution is halted.

READ The mechanism is entirely the same as the one used to compute the arguments
based on the registers and a probability distribution over the registers.

JEZ We note IRt+1
or not the JEZ instruction. We also have et
iteration t. The new value of the instruction register is:

njez the new value of IRt if we had respectively executed
jez the probability of executing this instruction at

jez and IRt+1

IRt+1 = IRt+1

njez · (1 − et

jez) + IRt+1

jez · et

jez

IRt+1
jez is himself computed based on several probability distribution. If we consider that
the instruction JEZ is executed with probabilistic arguments cond and label, its value is
given by

IRt+1

jez = label · cond0 + INC(IRt) · (1 − cond0)

(9)

(10)

11

With a probability equals to the one that the ﬁrst argument is equal to zero, the new value
of IRt is label. With the complement, it is equal to the incremented version of its current
value, as the machine automatically increments the instruction register.

WRITE The mechanism is fairly similar to the one of the JEZ instruction.
We note Mt+1
W RIT E and Mt+1
not the WRITE instruction. We also have et
at iteration t. The new value of the memory matrix register is:

nW RIT E the new value of Mt if we had respectively executed or
write the probability of executing this instruction

Mt+1 = Mt+1

nW RIT E · (1 − et

write) + Mt+1

W RIT E · et

W RIT E

As with the JEZ instruction, the value of Mt+1
W RIT E is dependent on the two probability
distribution given as input: addr and val. The probability that the i-th cell of the memory
tape contains the value j after the update is:

M t+1

i,j = addri · valj + (1 − addri) · M t
i,j

Note that this can done using linear algebra operations so as to update everything in one
global operation.

Mt+1 = (cid:0)((1 − addr)1T ) ⊗ Mt(cid:1) + (addr valT )

(11)

(12)

(13)

B Speciﬁcation of the loss

This loss contains four terms that will balance the correctness of the learnt algorithm, proper
usage of the stop signal and speed of the algorithms. The parameters deﬁning the models are
the weight of the Controller’s function and the initial value of the registers. When running
the model with the parameters θ, we consider that the execution ran for T time steps. We
consider the memory to have a size M and that each number can be an integer between
0 and M − 1. Mt was the state of the memory at the t-th step. T and C are the target
memory and the 0-1 mask of the elements we want to consider. All these elements are
matrices where for example Mt
i,j is the probability of the i-th entry of the memory to take
the value j at the step t. We also note pstop,t the probability outputted by the Machine that
it should have stopped before iteration t.

Correctness The ﬁrst term corresponds to the correctness of the given algorithm. For a
given input, we have the expected output and a mask. The mask allows us to know which
elements in the memory we should consider when comparing the solutions. For the given
input, we will compare the values speciﬁed by the mask of the expected output with the
ﬁnal memory tape provided by the execution. We compare them with the L2 distance in the
probability space. Using the notations from above, we can write this term as:
X

Lc(θ) =

Ci,j(MT

i,j(θ) − Ti,j)2.

(14)

i,j

If we optimised only this ﬁrst term, nothing would encourage the learnt algorithm to use the
STOP instruction and halt as soon as it ﬁnished.

Halting To prevent programs to take an inﬁnite amount of time without stopping, we
deﬁned a maximum number of iterations Tmax after which the execution is halted. During
training, we also add a penalty if the Controller didn’t halt before this limit:

LsTmax(θ) = (1 − pstop−T (θ)) · [T == Tmax]

Eﬃciency If we consider only the above mentioned losses, the program will make sure to
halt by itself but won’t do it as early as possible. We incentivise this behaviour by penalising
each iteration taken by the program where it does not stop:

(15)

(16)

Lt(θ) =

(1 − pstop,t(θ)).

X

t∈[1,T −1]

12

Conﬁdence Moreover, we want the algorithm to have a good conﬁdence to stop when
it has found the correct output. To do so, we add the following term which will penalise
probability of stopping if the current state of the memory is not the expected one:

Lst(θ) =

X

X

t∈[2,T ]

i,j

(pstop,t(θ) − pstop,t−1(θ))Ci,j(Mt

i,j(θ) − Ti,j)2.

(17)

The increase in probability (pstop,t − pstop,t−1) corresponds to the probability of stopping
exactly at iteration t. So, this is equivalent to the expected error made.

Total loss The complete loss that we use is then the following:

L(θ) = αLc(θ) + βLsTmax (θ) + γLst(θ) + δLt(θ).

(18)

C Distributed representation of the program

For the most of out experiments, the learned weights are fully interpretable as they ﬁt in the
ﬁrst type of interpretability. However, in some speciﬁc cases, under the pressure of our loss
encouraging a smaller number of iterations, an interesting behavior emerges.

It is interesting to note that the decompiled version is not straightforward to
Remarks
interpret. Indeed when we reach a program that has non Dirac-delta distributions in its
weights, we cannot perform the inverse of the one-to-one mapping performed by the compiler.
In fact, it relies on this blurriness to be able to execute the program with a smaller number
of instruction. Notably, by having some blurriness on the JEZ instruction, the program can
hide additional instructions, by creating a distributed state. We now explain the mechanism
used to achieve this.

Creating a distributed state Consider the following program and assume that the initial
value of IR is 0:

Initial Registers:
R1 = 0; R2 = 1; R3 = 4, R4 = 0

Program:
0 : R1 = READ (R1, R4)
1 : R4 = JEZ (R1, R3)
2 : R4 = WRITE(R1, R1)
3 : R4 = WRITE(R1, R3)

If you take this program and execute it for three iterations, it will: read the ﬁrst value of
the tape into R1. Then, if this value is zero, it will jump to State 4, otherwise it will just
increment IR. This means that depending on the value that was in R1, the next instruction
that will be executed will be diﬀerent (in this case, the diﬀerence between State 3 and State
4 is which registers they will be writing from). This is our standard way of implementing
conditionals.

Imagine that, after learning, the second instruction in our example program has 0.5 probability
of being a JEZ and 0.5 probability of being a ZERO. If the content of R1 is a zero, according
to the JEZ, we should jump to State 4, but this instruction is executed with a probability
of 0.5. We also have 0.5 probability of executing the ZERO instruction, which would lead to
State 3.

Therefore, IR is not a Dirac-delta distribution anymore but points to State 3 with probability
0.5 and State 4 with probability 0.5.

Exploiting a distributed state To illustrate, we will discuss how the Controller computes
a for a model with 3 registers. The Table 2 show an example of some weights for such a
controller.

If we are in State 1, the output of the controller is going to be

out = softmax([20, 5, −20]) = [0.9999..., 3e−7, 4e−18]

(19)

13

State 1
State 2

R1 R2 R3
-20
5
20
20
5
-20

Table 2: Controller Weights

If we are in State 2, the output of the controller is going to be

out = softmax([−20, 5, 20]) = [4e−18, 3e−7, 0.9999...]

(20)

In both cases, the output of the controller is therefore going to be almost discrete. In State
1, R1 would be chosen and in State 2, R3 would be chosen.

However, in the case where we have a distributed state with probability 0.5 over State 1 and
0.5 over State 2, the output would be:

out = softmax(0.5 ∗ [−20, 5, 20] + 0.5[20, 5, −20])

= softmax([0, 10, 0])
= [4e−5, 0.999, 4e−5].

(21)

Note that the result of the distributed state is actually diﬀerent from the result of the discrete
states. Moreover it is still a discrete choice of the second register.

Because this program contains distributed elements, it is not possible to perform the one-to-
one mapping between the weights and the lines of code. Though every instruction executed
by the program, except for the JEZ, are binary. This means that this model can be translated
to a regular program that will take exactly the same runtime, but will require more lines of
codes than the number of lines in the matrix.

D Alternative Learning Strategies

A critique that can be made to this method is that we will still initialise close to a local
minimum. Another approach might be to start from a random initialisation but adding a
penalty on the value of the weights such that they are encourage to be close to the generic
algorithm. This can be seen as L2 regularisation but instead of pushing the weights to 0, we
push then with the value corresponding to the generic algorithm. If we start with a very
high value of this penalty but use an annealing schedule where its importance is very quickly
reduced, this is going to be equivalent to the previous method.

E Possible Extension

E.1 Making objective function diﬀerentiable

These experiments showed that we can transform any program that perform a mapping
between an input memory tape to an output memory tape to a set of parameters and execute
it using our model. The ﬁrst point we want to make here is that this means that we take
any program and transform it into a diﬀerentiable function easily. For example, if we want
to learn a model that given a graph and two nodes a and b, will output the list of nodes to
go through to go from a to b in the shortest amount of time. We can easily deﬁne the loss of
the length of the path outputted by the model. Unfortunately, the function that computes
this length from the set of nodes is not diﬀerentiable. Here we could implement this function
in our model and use it between the prediction of the model and the loss function to get an
end to end trainable system.

E.2 Beyond mimicking and towards open problems

It would even be possible to generalise our learning procedure to more complex problems
for which we don’t have a ground truth output. For example, we could consider problems
where the exact answer for a given input is not computable or not unique. If the goodness
of a solution can be computed easily, this value could be used as training objective. Any
program giving a solution could be used as initialisation and our framework would improve
it, making it generate better solutions.

14

This section will present the programs that we use as initialisation for the experiment section.

F Example tasks

F.1 Access

In this task, the ﬁrst element in the memory is a value k. Starting from the second element,
the memory contains a zero-terminated list. The goal is to access the k-th element in the
list that is zero-indexed. The program associated with this task can be found in Listing 1.

1

2

3

4

5

6

(cid:7)
var k = 0
k = READ (0)
k = INC ( k )
k = READ ( k )
WRITE (0 , k )
STOP ()
(cid:6)
Listing 1: Access Task

(cid:4)

(cid:5)

Example input:
Output:

6
1

9
9

1
1

2
2

7
7

9
9

8
8

1
1

3
3

5
5

F.2 Copy

In this task, the ﬁrst element in the memory is a pointer p. Starting from the second element,
the memory contains a zero-terminated list. The goal is to copy this list at the given pointer.
The program associated with this task can be found in Listing 2.

1

2

3

4

5

6

7

8

9

10

11

12

13

(cid:7)
var read_addr = 0
var read_value = 0
var write_addr = 0

write_addr = READ (0)
l_loop : read_value = READ ( read_addr )
JEZ ( read_value , l_stop )
WRITE ( write_addr , read_value )
read_addr = INC ( read_addr )
write_addr = INC ( write_addr )
JEZ (0 , l_loop )

l_stop : STOP ()
(cid:6)

Listing 2: Copy Task

(cid:4)

(cid:5)

Example input:
Output:

9
9

11
11

3
3

1
1

5
5

14
14

0
0

0
0

0
0

0
11

0
3

0
1

0
5

0
14

0
0

F.3

Increment

In this task, the memory contains a zero-terminated list. The goal is to increment each value
in the list by 1. The program associated with this task can be found in Listing 3.

15

1

2

3

4

5

6

7

8

9

10

11

(cid:7)
var read_addr = 0
var read_value = 0

l_loop : read_value = READ ( read_addr )
JEZ ( read_value , l_stop )
read_value = INC ( read_value )
WRITE ( read_addr , read_value )
read_addr = INC ( read_addr )
JEZ (0 , l_loop )

l_stop : STOP ()
(cid:6)

Listing 3: Increment Task

(cid:4)

(cid:5)

Example input:
Output:

1
2

2
3

2
3

3
4

0
0

0
0

0
0

F.4 Reverse

In this task, the ﬁrst element in the memory is a pointer p. Starting from the second element,
the memory contains a zero-terminated list. The goal is to copy this list at the given pointer
in the reverse order. The program associated with this task can be found in Listing 4.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

(cid:7)
var read_addr = 0
var read_value = 0
var write_addr = 0

write_addr = READ ( write_addr )
l_count_phase : read_value = READ ( read_addr )
JEZ ( read_value , l_copy_phase )
read_addr = INC ( read_addr )
JEZ (0 , l_count_phase )

l_copy_phase : read_addr = DEC ( read_addr )
JEZ ( read_addr , l_stop )
read_value = READ ( read_addr )
WRITE ( write_addr , read_value )
write_addr = INC ( write_addr )
JEZ (0 , l_copy_phase )

l_stop : STOP ()
(cid:6)

Listing 4: Reverse Task

(cid:4)

(cid:5)

Example input:
Output:

5
5

7
7

2
2

13
13

14
14

0
14

0
13

0
2

0
7

0
0

0
0

0
0

0
0

0
0

0
0

F.5 Permutation

In this task, the memory contains two zero-terminated list one after the other. The ﬁrst
contains a set of indices. the second contains a set of values. The goal is to ﬁll the ﬁrst list
with the values in the second list at the given index. The program associated with this task
can be found in Listing 5.

16

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

(cid:7)
var read_addr = 0
var read_value = 0
var write_offset = 0

l_count_phase : read_value = READ ( write_offset )
write_offset = INC ( write_offset )
JEZ ( read_value , l_copy_phase )
JEZ (0 , l_count_phase )

l_copy_phase : read_value = DEC ( read_addr )
JEZ ( read_value , l_stop )
read_value = ADD ( write_offset , read_value )
read_value = READ ( read_value )
WRITE ( read_addr , read_value )
read_addr = INC ( read_addr )
JEZ (0 , l_copy_phase )
l_stop : STOP ()
(cid:6)

Listing 5: Permutation Task

(cid:4)

(cid:5)

Example input:
Output:

2
4

1
13

3
6

0
0

13
13

4
4

6
6

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

F.6 Swap

In this task, the ﬁrst two elements in the memory are pointers p and q. Starting from the
third element, the memory contains a zero-terminated list. The goal is to swap the elements
pointed by p and q in the list that is zero-indexed. The program associated with this task
can be found in Listing 6.

(cid:7)
var p = 0
var p_val = 0
var q = 0
var q_val = 0

p = READ (0)
q = READ (1)
p_val = READ ( p )
q_val = READ ( q )
WRITE (q , p_val )
WRITE (p , q_val )
STOP ()
(cid:6)

1

2

3

4

5

6

7

8

9

10

11

12

(cid:4)

(cid:5)

Listing 6: Swap Task

Example input:
Output:

1
1

3
3

7
7

6
5

7
7

5
6

2
2

0
0

0
0

0
0

F.7 ListSearch

In this task, the ﬁrst three elements in the memory are a pointer to the head of the linked
list, the value we are looking for v and a pointer to a place in memory where to store the
result. The rest of the memory contains the linked list. Each element in the linked list is two
values, the ﬁrst one is the pointer to the next element, the second is the value contained in
this element. By convention, the last element in the list points to the address 0. The goal is
to return the pointer to the ﬁrst element whose value is equal to v. The program associated
with this task can be found in Listing 7.

17

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

(cid:7)
var p_out = 0
var p_current = 0
var val_current = 0
var val_searched = 0

val_searched = READ (1)
p_out = READ (2)
l_loop : p_current = READ ( p_current )
val_current = INC ( p_current )
val_current = READ ( val_current )
val_current = SUB ( val_current , val_searched )
JEZ ( val_current , l_stop )
JEZ (0 , l_loop )
l_stop : WRITE ( p_out , p_current )
STOP ()
(cid:6)

Listing 7: ListSearch Task

(cid:4)

(cid:5)

Example input:
Output:

11
11

10
10

2
5

9
9

4
4

3
3

10
10

0
0

6
6

7
7

13
13

5
5

12
12

0
0

0
0

F.8 ListK

In this task, the ﬁrst three elements in the memory are a pointer to the head of the linked
list, the number of hops we want to do k in the list and a pointer to a place in memory
where to store the result. The rest of the memory contains the linked list. Each element in
the linked list is two values, the ﬁrst one is the pointer to the next element, the second is
the value contained in this element. By convention, the last element in the list points to
the address 0. The goal is to return the value of the k-th element of the linked list. The
program associated with this task can be found in Listing 8.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

(cid:7)
var p_out = 0
var p_current = 0
var val_current = 0
var k = 0

k = READ (1)
p_out = READ (2)
l_loop : p_current = READ ( p_current )
k = DEC ( k )
JEZ (k , l_stop )
JEZ (0 , l_loop )
l_stop : p_current = INC ( p_current )
p_current = READ ( p_current )
WRITE ( p_out , p_current )
STOP ()
(cid:6)

Listing 8: ListK Task

(cid:4)

(cid:5)

3
3

2
2

2
17

9
9

15
15

0
0

0
0

0
0

1
1

15
15

17
17

7
7

13
13

0
0

0
0

11
11

Example input:
Output:
0
0
10
0
0
10

0
0

F.9 Walk BST

In this task, the ﬁrst two elements in the memory are a pointer to the head of the BST and a
pointer to a place in memory where to store the result. Starting at the third element, there

18

is a zero-terminated list containing the instructions on how to traverse in the BST. The rest
of the memory contains the BST. Each element in the BST has three values, the ﬁrst one is
the value of this node, the second is the pointer to the left node and the third is the pointer
to the right element. By convention, the leafs points to the address 0. The goal is to return
the value of the node we get at after following the instructions. The instructions are 1 or
2 to go respectively to the left or the right. The program associated with this task can be
found in Listing 9.
(cid:7)
var p_out = 0
var p_current = 0
var p_instr = 0
var instr = 0

(cid:4)

2

3

1

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

p_current = READ (0)
p_out = READ (1)
instr = READ (2)

l_loop : JEZ ( instr , l_stop )
p_current = ADD ( p_current , instr )
p_current = READ ( p_current )
p_instr = INC ( p_instr )
JEZ (0 , l_loop )

l_stop : p_current = READ ( p_current )
WRITE ( p_out , p_current )
STOP ()
(cid:6)

(cid:5)

Listing 9: WalkBST Task

Example input:
Output:
24
0
8
24
0
8

0
0

0
0

12
12
0
0

1
10
0
0

0
0

1
1

2
2

0
0
10
10

0
0

0
0
0
0

15
15
0
0

0
0

0
0

0
0

0
0

9
9

23
23

0
0

0
0

11
11

15
15

6
6

F.10 Merge

In this task, the ﬁrst three elements in the memory are pointers to respectively, the ﬁrst list,
the second list and the output. The two lists are zero-terminated sorted lists. The goal is to
merge the two lists into a single sorted zero-terminated list that starts at the output pointer.
The program associated with this task can be found in Listing 10.

19

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

(cid:4)

(cid:5)

(cid:7)
var p_first_list = 0
var val_first_list = 0
var p_second_list = 0
var val_second_list = 0
var p_output_list = 0
var min = 0

p_first_list = READ (0)
p_second_list = READ (1)
p_output_list = READ (2)

l_loop : val_first_list = READ ( p_first_list )
val_second_list = READ ( p_second_list )
JEZ ( val_first_list , l_first_finished )
JEZ ( val_second_list , l_second_finished )
min = MIN ( val_first_list , val_second_list )
min = SUB ( val_first_list , min )
JEZ ( min , l_first_smaller )

WRITE ( p_output_list , val_first_list )
p_output_list = INC ( p_output_list )
p_first_list = INC ( p_first_list )
JEZ (0 , l_loop )

l_first_smaller : WRITE ( p_output_list , val_second_list )
p_output_list = INC ( p_output_list )
p_second_list = INC ( p_second_list )
JEZ (0 , l_loop )

l_first_finished : p_first_list = ADD ( p_second_list , 0)
val_first_list = ADD ( val_second_list , 0)

l_second_finished : WRITE ( p_output_list , val_first_list )
p_first_list = INC ( p_first_list )
p_output_list = INC ( p_output_list )
val_first_list = READ ( p_first_list )
JEZ ( val_first_list , l_stop )
JEZ (0 , l_second_finished )

l_stop : STOP ()
(cid:6)

Listing 10: Merge Task

Example input:
Output:
0
0
0
0
1
16

0
0

0
0

3
3
0
0

8
8
0
0

11
11
0
0

27
27
0
0

0
0

17
17
0
0

16
16
0
0

1
1
0
0

0
0
0
0

29
29
0
0

26
26

0
0

0
29

0
27

0
26

0
17

F.11 Dijkstra

In this task, we are provided with a graph represented in the input memory as follow. The
ﬁrst element is a pointer pout indicating where to write the results. The following elements
contain a zero-terminated array with one entry for each vertex in the graph. Each entry is a
pointer to a zero-terminated list that contains a pair of values for each outgoing edge of the
considered node. Each pair of value contains ﬁrst the index in the ﬁrst array of the child node
and the second value contains the cost of this edge. The goal is to write a zero-terminated
list at the address provided by pout that will contain the value of the shortest path from the
ﬁrst node in the list to this node. The program associated with this task can be found in
Listings 11 and 12.

20

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

(cid:7)
var min = 0
var argmin = 0

var p_out = 0
var p_out_temp = 0
var p_in = 1
var p_in_temp = 1

var nnodes = 0

var zero = 0
var big = 99

var tmp_node = 0
var tmp_weight = 0
var tmp_current = 0
var tmp = 0

var didsmth = 0

p_out = READ ( p_out )
p_out_temp = ADD ( p_out , zero )

tmp_current = INC ( zero )
l_loop_nnodes : tmp = READ ( p_in_temp )
JEZ ( tmp , l_found_nnodes )
WRITE ( p_out_temp , big )
p_out_temp = INC ( p_out_temp )
WRITE ( p_out_temp , tmp_current )
p_out_temp = INC ( p_out_temp )
p_in_temp = INC ( p_in_temp )
nnodes = INC ( nnodes )
JEZ ( zero , l_loop_nnodes )

l_found_nnodes : WRITE ( p_out , zero )
JEZ ( zero , l_find_min )
l_min_return : p_in_temp = ADD ( p_in , argmin )
p_in_temp = READ ( p_in_temp )

l_loop_sons : tmp_node = READ ( p_in_temp )
JEZ ( tmp_node , l_find_min )
tmp_node = DEC ( tmp_node )
p_in_temp = INC ( p_in_temp )
tmp_weight = READ ( p_in_temp )
p_in_temp = INC ( p_in_temp )

p_out_temp = ADD ( p_out , tmp_node )
p_out_temp = ADD ( p_out_temp , tmp_node )
tmp_current = READ ( p_out_temp )
tmp_weight = ADD ( min , tmp_weight )

tmp = MIN ( tmp_current , tmp_weight )
tmp = SUB ( tmp_current , tmp )
JEZ ( tmp , l_loop_sons )
WRITE ( p_out_temp , tmp_weight )
JEZ ( zero , l_loop_sons )
(cid:6)

Listing 11: Dijkstra Algorithm (Part 1)

21

(cid:4)

(cid:5)

57

58

59

60

61

62

63

64

65

66

67

68

69

70

71

72

73

74

75

76

77

78

79

80

81

82

83

84

85

86

87

88

(cid:7)
l_find_min : p_out_temp = DEC ( p_out )
tmp_node = DEC ( zero )
min = ADD ( big , zero )
argmin = DEC ( zero )

l_loop_min : p_out_temp = INC ( p_out_temp )
tmp_node = INC ( tmp_node )
tmp = SUB ( tmp_node , nnodes )
JEZ ( tmp , l_min_found )

tmp_weight = READ ( p_out_temp )

p_out_temp = INC ( p_out_temp )
tmp = READ ( p_out_temp )
JEZ ( tmp , l_loop_min )

tmp = MAX ( min , tmp_weight )
tmp = SUB ( tmp , tmp_weight )
JEZ ( tmp , l_loop_min )
min = ADD ( tmp_weight , zero )
argmin = ADD ( tmp_node , zero )
JEZ ( zero , l_loop_min )

l_min_found : tmp = SUB ( min , big )
JEZ ( tmp , l_stop )
p_out_temp = ADD ( p_out , argmin )
p_out_temp = ADD ( p_out_temp , argmin )
p_out_temp = INC ( p_out_temp )
WRITE ( p_out_temp , zero )
JEZ ( zero , l_min_return )

l_stop : STOP ()
(cid:6)

Listing 12: Dijkstra Algorithm (Part 2)

(cid:4)

(cid:5)

Example omitted for space reasons

G Learned optimisation: Case study

Here we present an analysis of the optimisation achieved by the ANC. We take the example
of the ListK task and study the diﬀerence between the learned program and the initialisation
used.

G.1 Representation

The representation chosen is under the form of the intermediary representation described in
Figure (2b). Based on the parameters of the Controller, we can recover the approximate
representation described in Figure (2b): 1For each possible "discrete state" of the instruction
register, we can compute the commands outputted by the controller. We report the most
probable value for each distribution, as well as the probability that the compiler would assign
to this value. If no value has a probability higher than 0.5, we only report a neutral token
(R-, -, NOP).

G.2 Biased ListK

Figure 5 represents the program that was used as initialisation to the optimisation problem.
This is the direct result from the compilation performed by the Neural Compiler of the
program described in Listing 8. A version with a probability of 1 for all necessary instructions
would have been easily obtained but not amenable to learning.

22

Figure 6 similarly describes the program that was obtained after learning.

As a remainder, the bias introduced in the ListK task is that the linked list is well organised
in memory. In the general case, the element could be in any order. An input memory tape to
the problem of asking for the third element in the linked list containing {4, 5, 6, 7} would be:

or

9

5

3

3

2

2

0

0

0

0

11

5

0

7

4

15

7

5

5

0

4

7

7

0

6

0

0

0

0

0

0

9

0

6

0

0

0

0

0

0

In the biased version of the task, all the elements are arranged in order and contiguously
positioned on the tape. The only valid representation of this problems is:

3

3

2

5

4

7

5

9

6

0

7

0

0

0

0

0

0

0

0

0

G.3 Solutions

Because of the additional structure of the problem, the bias in the data, a more eﬃcient
algorithm to ﬁnd the solution exists. Let us dive into the comparison of the two diﬀerent
solutions.

Both use their ﬁrst two states to read the parameters of the given instance of the task.
Which element of the list should be returned is read at line (0:) and where to write the
returned value is read at line (1:). Step (2:) to (6:) are dedicated to putting the address of
the k-th value of the linked list into the registers R1. Step (7:) to (9:) perform the same
task in both solution: reading the value at the address contained in R1, writing it at the
desired position and stopping.

The diﬀerence between the two programs lies in how they put the address of the k-th value
into R1.

Generic The initialisation program, used for initialisation, works in the most general case
so needs to perform a loop where it put the address of the next element in the linked list in
R1 (2:), decrement the number of jumps remaining to be made (3:), checking whether the
wanted element has been reached (4:) and going back to the start of the loop if not (5:).
Once the desired element is reached, R1 is incremented so as to point on the value of the
linked list element.

Speciﬁc On the other hand, in the biased version of the problem, the position of the
desired value can be analytically determined. The function parameters occupy the ﬁrst three
cells of the tape. After those, each element of the linked list will occupy two cells (one for
the pointer to the next address and one for the value). Therefore, the address of the desired
value is given by

R1 = 3 + (2 ∗ (k − 1) + 1) − 1 + 1

= 3 + 2 ∗ k − 1

(the -1 comes from the fact that the address are 0-indexed and the ﬁnal +1 from the fact
that we are interested in the position of the value and not of the pointer.)

The way this is computed is as follows:

- R1 = 3 + k by adding the constant 3 to the registers R2 containing K.
- R2 = k − 1
- R1 = 3 + 2 ∗ k − 1 by adding the now reduced value of R2.

The algorithm implemented by the learned version is therefore much more eﬃcient for the
biased dataset, due to its capability to ignore the loop.

(22)

(2:)
(3:)
(6:)

23

G.4 Failure analysis

An observation that can be made is that in the learned version of the program, Step (4:)
and (5:) are not contributing to the algorithms. They execute instructions that have no
side eﬀect and store the results into the registers R7 that is never used later in the execution.

The learned algorithm could easily be more eﬃcient by not performing these two operations.
However, such an optimisation, while perhaps trivial for a standard compiler, capable of
detecting unused values, is fairly hard for our optimisers to discover. Because we are only
doing gradient descent, the action of "moving some instructions earlier in the program" which
would be needed here to make the useless instructions disappear, is fairly hard, as it involves
modifying several rows of the program at once in a coherent manner.

R1 = 0 (0.88)
R2 = 1 (0.88)
R3 = 2 (0.88)
R4 = 6 (0.88)
R5 = 0 (0.88)
R6 = 2 (0.88)
R7 = - (0.05)

Initial State: 0 (0.88)

R2 (0.96)
0:
R3 (0.96)
1:
R1 (0.96)
2:
R2 (0.96)
3:
R7 (0.96)
4:
R7 (0.96)
5:
R1 (0.96)
6:
R1 (0.96)
7:
R7 (0.96)
8:
9:
R7 (0.96)
10: R- (0.16)
11: R- (0.16)
12: R- (0.17)
13: R- (0.16)
14: R- (0.17)
15: R- (0.16)
16: R- (0.17)
17: R- (0.18)
18: R- (0.17)
19: R- (0.15)

= READ (0.93)
= READ (0.93)
= READ (0.93)
(0.93)
= DEC
(0.93)
= JEZ
(0.93)
= JEZ
(0.93)
= INC
= READ (0.93)
= WRIT (0.93)
= STOP (0.93)
(0.11)
= NOP
(0.11)
= NOP
(0.1)
= NOP
(0.11)
= NOP
(0.11)
= NOP
(0.1)
= NOP
(0.11)
= NOP
(0.1)
= NOP
(0.1)
= NOP
(0.11)
= NOP

[ R2 (0.96)
[ R3 (0.96)
[ R1 (0.96)
[ R2 (0.96)
[ R2 (0.96)
[ R5 (0.96)
[ R1 (0.96)
[ R1 (0.96)
[ R3 (0.96)
[ R- (0.14)
[ R- (0.16)
[ R- (0.18)

[ R- (0.17)

[ R- (0.17)
[ R- (0.16)

, R- (0.14)
, R- (0.14)
, R- (0.14)
, R- (0.14)
, R4 (0.96)
, R6 (0.96)
, R- (0.14)
, R- (0.14)
, R1 (0.96)
, R- (0.14)
, R- (0.16)
, R- (0.16)

, R- (0.17)

, R- (0.16)
, R- (0.16)

[ R- (0.16)

, R- (0.17)

[ R- (0.16)

, R- (0.18)

[ R- (0.18)
[ R- (0.16)

, R- (0.17)
, R- (0.16)

[ R- (0.16)

, R- (0.17)

]
]
]
]
]
]
]
]
]
]
]
]

]

]
]

]

]

]

]
]

Figure 5: Initialisation used for the learning of the ListK task.

24

Initial State: 0 (0.99)

R1 = 3 (0.99)
R2 = 1 (0.99)
R3 = 2 (0.99)
R4 = 10 (0.53)
R5 = 0 (0.99)
R6 = 2 (0.99)
R7 = 7 (0.99)

R2 (0.99)
R6 (0.99)
R1 (1)
R2 (1)
R7 (0.99)
R7 (0.99)
R1 (1)
R1 (0.99)
R7 (0.99)
R7 (0.9)

0:
1:
2:
3:
4:
5:
6:
7:
8:
9:
10: R2 (0.99)
11: R1 (0.98)
12: R3 (0.98)
13: R3 (0.87)
14: R3 (0.89)
15: R3 (0.99)
16: R3 (0.99)
17: R2 (0.99)
18: R3 (0.99)
19: R3 (0.98)

= READ (0.99)
= READ (0.99)
(0.99)
(0.99)

= ADD
= DEC

= ADD

= MAX
= INC

(0.99)
(0.99)

(0.99)
= READ (0.99)
= WRIT (0.99)

= STOP (0.99)

[ R2 (1)
[ R3 (0.99)

, R1 (0.97)

]

, R6 (0.5)
]
]

, R2 (1)
, R1 (0.99)

]

[ R1 (0.99)
[ R2 (1)

[ R2 (0.99)
[ R6 (0.7)

, R1 (0.51)
, R1 (0.89)

]
]

[ R1 (0.99)

, R2 (1)

]

[ R1 (0.99)
[ R6 (1)
[ R6 (0.98)

, R1 (0.53)

]

, R1 (0.99)

]

, R1 (0.99)

]

= STOP (0.96)
(0.73)
= ADD
= ADD
(0.64)
= STOP (0.65)
= STOP (0.62)
= STOP (0.65)
(0.45)
= NOP
= INC
(0.56)
= STOP (0.65)
= STOP (0.98)

[ R1 (0.52)
[ R4 (0.99)
[ R6 (0.99)
[ R3 (0.52)
[ R6 (0.99)
[ R3 (0.99)
[ R6 (0.99)
[ R6 (0.7)
[ R3 (0.99)
[ R2 (0.62)

, R1 (0.99)
, R2 (0.99)
, R1 (0.99)
, R1 (0.99)
, R2 (0.62)
, R2 (0.71)
, R1 (0.99)

, R1 (0.98)

]

, R1 (0.99)
, R1 (0.79)

]
]
]
]
]
]
]

]
]

Figure 6: Learnt program for the listK task

25

6
1
0
2
 
y
a
M
 
6
2
 
 
]
I

A
.
s
c
[
 
 
2
v
9
6
9
7
0
.
5
0
6
1
:
v
i
X
r
a

Adaptive Neural Compilation

Rudy Bunel∗
University of Oxford
rudy@robots.ox.ac.uk

Alban Desmaison∗
University of Oxford
alban@robots.ox.ac.uk

Pushmeet Kohli
Microsoft Research
pkohli@microsoft.com

Philip H.S. Torr
University of Oxford
philip.torr@eng.ox.ac.uk

M. Pawan Kumar
University of Oxford
pawan@robots.ox.ac.uk

Abstract

This paper proposes an adaptive neural-compilation framework to address
the problem of eﬃcient program learning. Traditional code optimisation
strategies used in compilers are based on applying pre-speciﬁed set of
transformations that make the code faster to execute without changing
its semantics. In contrast, our work involves adapting programs to make
them more eﬃcient while considering correctness only on a target input
distribution. Our approach is inspired by the recent works on diﬀerentiable
representations of programs. We show that it is possible to compile programs
written in a low-level language to a diﬀerentiable representation. We also
show how programs in this representation can be optimised to make them
eﬃcient on a target distribution of inputs. Experimental results demonstrate
that our approach enables learning speciﬁcally-tuned algorithms for given
data distributions with a high success rate.

1

Introduction

Algorithm design often requires making simplifying assumptions about the input data.
Consider, for instance, the computational problem of accessing an element in a linked list.
Without the knowledge of the input data distribution, one can only specify an algorithm
that runs in a time linear in the number of elements of the list. However, suppose all the
linked lists that we encountered in practice were ordered in memory. Then it would be
advantageous to design an algorithm speciﬁcally for this task as it can lead to a constant
running time. Unfortunately, the input data distribution of a real world problem cannot be
easily speciﬁed as in the above simple example. The best that one can hope for is to obtain
samples drawn from the distribution. A natural question that arises from these observations:
“How can we adapt a generic algorithm for a computational task using samples from an
unknown input data distribution?”

The process of ﬁnding the most eﬃcient implementation of an algorithm has received
considerable attention in the theoretical computer science and code optimisation community.
Recently, Conditionally Correct Superoptimization [14] was proposed as a method for
leveraging samples of the input data distribution to go beyond semantically equivalent
optimisation and towards data-speciﬁc performance improvements. The underlying procedure
is based on a stochastic search over the space of all possible programs. Additionally, they
restrict their applications to reasonably small, loop-free programs, thereby limiting their
impact in practice.

In this work, we take inspiration from the recent wave of machine-learning frameworks for
estimating programs. Using recurrent models, Graves et al. [2] introduced a fully diﬀerentiable

∗The ﬁrst two authors contributed equally.

representation of a program, enabling the use of gradient-based methods to learn a program
from examples. Many other models have been published recently [3, 5, 6, 8] that build and
improve on the early work by Graves et al. [2]. Unfortunately, these models are usually
complex to train and need to rely on methods such as curriculum learning or gradient noise
to reach good solutions as shown by Neelakantan et al. [10]. Moreover, their interpretability
is limited. The learnt model is too complex for the underlying algorithm to be recovered
and transformed into a regular computer program.

The main focus of the machine-learning community has thus far been on learning programs
from scratch, with little emphasis on running time. However, for nearly all computational
problems, it is feasible to design generic algorithms for the worst-case. We argue that a
more pragmatic goal for the machine learning community is to design methods for adapting
existing programs for speciﬁc input data distributions. To this end, we propose the Adaptive
Neural Compiler (ANC). We design a compiler capable of mechanically converting algorithms
to a diﬀerentiable representation, thereby providing adequate initialisation to the diﬃcult
problem of optimal program learning. We then present a method to improve this compiled
program using data-driven optimisation, alleviating the need to perform a wide search over
the set of all possible programs. We show experimentally that this framework is capable of
adapting simple generic algorithms to perform better on given datasets.

2 Related Works

The idea of compiling programs to neural networks has previously been explored in the
literature. Siegelmann [15] described how to build a Neural Network that would perform
the same operations as a given program. A compiler has been designed by Gruau et al. [4]
targeting an extended version of Pascal. A complete implementation was achieved when
Neto et al. [11] wrote a compiler for NETDEF, a language based on the Occam programming
language. While these methods allow us to obtain an exact representation of a program as a
neural network, they do not lend themselves to optimisation to improve the original program.
Indeed, in their formulation, each elementary step of a program is expressed as a group of
neurons with a precise topology, set of weights and biases, thereby rendering learning via
gradient descent infeasible. Performing gradient descent in this parameter space would result
in invalid operations and thus is unlikely to lead to any improvement. The recent work by
Reed and de Freitas [12] on Neural Programmer-Interpreters (NPI) can also be seen as a way
to compile any program into a neural network by learning a model that mimic the program.
While more ﬂexible than the previous approaches, the NPI is unable to improve on a learned
program due to its dependency on a non-diﬀerentiable environment.

Another approach to this learning problem is the one taken by the code optimisation
community. By exploring the space of all possible programs, either exhaustively [9] or in a
stochastic manner [13], they search for programs having the same results but being more
eﬃcient. The work of Sharma et al. [14] broadens the space of acceptable improvements
to data-speciﬁc optimisations as opposed to the provably equivalent transformations that
were previously the only ones considered. However, this method is still reliant on non-
gradient-based methods for eﬃcient exploration of the space. By representing everything in
a diﬀerentiable manner, we aim to obtain gradients to guide the exploration.

Recently, Graves et al. [2] introduced a learnable representation of programs, called the
Neural Turing Machine (NTM). The NTM uses an LSTM as a Controller, which outputs
commands to be executed by a deterministic diﬀerentiable Machine. From examples of
input/output sequences, they manage to learn a Controller such that the model becomes
capable of performing simple algorithmic tasks. Extensions of this model have been proposed
in [3, 5] where the memory tape was replaced by diﬀerentiable versions of stacks or lists.
Kurach et al. [8] modiﬁed the NTM to introduce a notion of pointers making it more
amenable to represent traditional programs. Parallel works have been using Reinforcement
Learning techniques such as the REINFORCE algorithm [1, 16, 17] or Q-learning [18] to
be able to work with non diﬀerentiable versions of the above mentioned models. All these
models are trained only with a loss based on the diﬀerence between the output of the model
and the expected output. This weak supervision leads to a complex training. For instance

2

IR0

R0

Controller

Controller

Machine

Machine

IR1

R1

M1

stop

...

...

...

IR2

stop

R2

M2

M0

Memory

Memory

MT

Inst

arg1

arg2

output

side eﬀect

STOP
ZERO
INC
DEC
ADD
SUB
MIN
MAX
READ
WRITE

JEZ

-
-
a
a
a
a
a
a
a
a

a

-
-
-
-
b
b
b
b
-
b

b

0
0
a+1
a-1
a+b
a-b
min(a,b)
max(a,b)
mt
a
0

0

stop = 1
-
-
-
-
-
-
-
Memory access
mt
a = b
IRt = b
if a = 0

(a) General view of the whole Model.

(b) Machine instructions.

Figure 1: Model components.

the Neural RAM [8] requires a high number of random restarts before converging to a correct
solution [10], even when using the best hyperparameters obtained through a large grid search.

In our work, we will ﬁrst show that we can design a new neural compiler whose target will
be a Controller-Machine model. This makes the compiled model amenable to learning from
examples. Moreover, we can use it as initialisation for the learning procedure, allowing us to
aim for the more complex task of ﬁnding an eﬃcient algorithm.

3 Model

Our model is composed of two parts: (i) a Controller, in charge of specifying what should
be executed; and (ii) a Machine, following the commands of the Controller. We start by
describing the global architecture of the model. For the sake of simplicity, the general
description will present a non-diﬀerentiable version of the model. Section 3.2 will then
explain the modiﬁcations required to make this model completely diﬀerentiable. A more
detailed description of the model is provided in appendix A.

3.1 General Model

1, rt

1, mt

2, . . . , rt

2, . . . , mt

We ﬁrst deﬁne for each timestep t the memory tape that contains M integer values
Mt = {mt
M }, the registers that contain R values Rt = {rt
R} and the
instruction register that contain a single value IRt. We also deﬁne a set of instructions
that can be executed, whose main role is to perform computations using the registers. For
example, add the values contained in two registers. We also deﬁne as a side eﬀect any action
that involves elements other than the input and output values of the instruction. Interaction
with the memory is an example of such side eﬀect. All the instructions, their computations
and side eﬀects are detailed in Figure 1b.
As can be seen in Figure 1a the execution model takes as input an initial memory tape M0
and outputs a ﬁnal memory tape MT after T steps. At each step t, the Controller uses
the instruction register IRt to compute the command for the Machine. The command is
a 4-tuple e, a, b, o. The ﬁrst element e is the instruction that should be executed by the
Machine, enumerated as an integer. The elements a and b specify which registers should be
used as arguments for the given instruction. The last element o speciﬁes in which register
the output of the instruction should be written. For example, the command {ADD, 2, 3, 1}
means that only the value of the ﬁrst register should change, following rt+1
2, rt
3).
Then the Machine will execute this command, updating the values of the memory, the
registers and the instruction register. The Machine always performs two other operations
apart from the required instruction. It outputs a stop ﬂag that allows the model to decide
when to stop the execution. It also increments the instruction register IRt by one at each
iteration.

1 = ADD(rt

3.2 Diﬀerentiability

The model presented above is a simple execution machine but it is not diﬀerentiable. In
order to be able to train this model end-to-end from a loss deﬁned over the ﬁnal memory
tape, we need to make every intermediate operation diﬀerentiable.

3

To achieve this, we replace every discrete value in our model by a multinomial distribution
over all the possible values that could have been taken. Moreover, each hard choice that
would have been non-diﬀerentiable is replaced by a continuous soft choice. We will henceforth
use bold letters to indicate the probabilistic version of a value.
First, the memory tape Mt is replaced by an M × M matrix Mt, where Mt
i,j corresponds
i taking the value j. The same change is applied to the registers Rt,
to the probability of mt
replacing them with an R × M matrix Rt, where Rt
i taking
the value j. Finally, the instruction register is also transformed from a single value IRt to a
vector of size M noted IRt, where the i-th element represents its probability to take the
value i.

i,j represents the probability of rt

The Machine does not contain any learnable parameter and will just execute a given command.
To make it diﬀerentiable, the Machine now takes as input four probability distributions et,
at, bt and ot, where et is a distribution over instructions, and at, bt and ot are distributions
t as convex combinations
over the registers. We compute the argument values arg1
of the diﬀerent registers:

t and arg2

arg1

t =

irt
at
i

arg2

t =

irt
bt
i,

(1)

R
X

i=1

i and bt

where at
compute the output value of each instruction k using the following formula:
X

i are the i-th values of the vectors at and bt. Using these values, we can

arg1

t
i · arg2

j · 1[gk(i, j) = c mod M ],
t

(2)

∀0 ≤ c ≤ M outt

k,c =

R
X

i=1

0≤i,j≤M

where gk is the function associated to the k-th instruction as presented in Table 1b. Since the
executed instruction is controlled by the probability e, the output written to the register will
also be a convex combination: outt = PN
k=1 et
k, where N is the number of instructions.
This value is then stored into the registers by performing a soft-write parametrised by ot.

koutt

A special case is associated with the stop signal. When executing the model, we keep track of
the probability that the program should have terminated before this iteration based on the
probability associated at each iteration with the speciﬁc instruction that controls this ﬂag.
Once this probability goes over a threshold ηstop ∈ (0, 1], the execution is halted. We applied
the same techniques to make the side-eﬀects diﬀerentiable, this is presented in appendix A.1.

et = We ∗ IRt,

at = Wa ∗ IRt,

The Controller is the only learnable part of our model. The ﬁrst learnable part is the initial
values for the registers R0 and for the instruction register IR0. The second learnable part
is the parameters of the Controller which computes the required distributions using:
ot = Wo ∗ IRt

(3)
where We is an N × M matrix and Wa, Wb and Wo are R × M matrices. A representation
of these matrices can be found in Figure 4c. The Controller as deﬁned above is composed of
four independent, fully-connected layers. In Section 4.3 we will see that this complexity is
suﬃcient for our model to be able to represent any program.
Henceforth, we will denote by θ = {R0, IR0, We, Wa, Wb, Wo} the set of all the learnable
parameters of this model.

bt = Wb ∗ IRt,

4 Adaptative Neural Compiler

We will now present the Adaptive Neural Compiler.
Its goal is to ﬁnd the best set of
weights θ∗ for a given dataset such that our model will perform the correct input/output
mapping as eﬃciently as it can. We begin by describing our learning objective in details.
The two subsequent sections will focus on making the optimisation of our learning objective
computationally feasible.

4.1 Objective function

Our goal is to solve a given algorithmic problem eﬃciently. The algorithmic problem is
deﬁned as a set of input/output pairs. We also have access to a generic program that is able

4

to perform the required mapping. In our example of accessing elements in a linked list, the
transformation would consist in writing down the desired value at the speciﬁed position in
the tape. The program given to us would iteratively go through the elements of the linked
list, ﬁnd the desired value and write it down at the desired position. If there exists some
bias that would allow this traversal to be faster, we expect the program to exploit it.

Our approach to this problem is to construct a diﬀerentiable objective function, mapping
controller parameters to a loss. We deﬁne this loss based on the states of the memory tape
and outputs of the Controller at each step of the execution. The precise mathematical
formulation for each term of the loss is given in appendix B. Here we present the motivation
behind each of them.

Correctness For a given input, we have the expected output. We compare the values of
the expected output with the ﬁnal memory tape provided by the execution.

Halting To prevent programs to take an inﬁnite amount of time without stopping, we
deﬁned a maximum number of iterations Tmax after which the execution is halted. Moreover,
we add a penalty in the loss if the Controller didn’t halt before this limit.

Eﬃciency We penalise each iteration taken by the program where it does not stop.

Conﬁdence We add a term which will penalise probability of stopping if the current state
of the memory is not the expected one.

If only the correctness term was considered, nothing would encourage the learnt algorithm to
halt as soon as it ﬁnished. If only correctness and halting were considered, then the program
may not halt as early as possible. Conﬁdence enables the algorithm to evaluate better when
to stop.

The loss is a weighted sum of the four above-mentioned terms. We denote the loss of the i-th
training sample, given parameters θ, as Li(θ). Our learning objective is then speciﬁed as:

min
θ

X

i

Li(θ)

s.t. θ ∈ Θ,

(4)

where Θ is a set over the parameters such that the outputs of the Controller, the initial
values of each register and of the instruction register are all probability distributions.

The above optimisation is a highly non-convex problem. To be able to solve it using standard
gradient descent based methods, we will ﬁrst need to transform it to an unconstrained
problem. We also know that the result of the optimisation of a non-convex objective function
is strongly dependent on the initialisation point. In the rest of this section, we will ﬁrst
present a small modiﬁcation to the model that will remove the constraints. We will then
present our Neural Compiler that will provide a good initialisation to solve this problem.

4.2 Reformulation

In order to use gradient descent methods without having to project the parameters on Θ,
we alter the formulation of the controller. We add a softmax layer after each linear layer
ensuring that the constraints on the Controller’s output will be respected. We also apply a
softmax to the initial values of the registers and the instructions register, ensuring they will
also respect the original constraints. This way, we transform the constrained-optimisation
problem into an unconstrained one, allowing us to use standard gradient descent methods.
As discussed in other works [10], this kind of model is hard to train and requires a high
number of random restarts before converging to a good solution. We will now present a
Neural Compiler that will provide good initialisations to help with this problem.

4.3 Neural Compiler

The goal for the Neural Compiler is to convert an algorithm, written as an unambiguous
program, to a set of parameters. These parameters, when put into the controller, will
reproduce the exact steps of the algorithm. This is very similar to the problem framed by
Reed and de Freitas [12], but we show here a way to accomplish it without any learning.

5

var head = 0;
var nb_jump = 1;
var out_write = 2;

nb_jump = READ(nb_jump);
out_write = READ(out_write);

loop : head = READ(head);

nb_jump = DEC(nb_jump);
JEZ(nb_jump, end);
JEZ(0, loop);
end : head = INC(head);

head = READ(head);
WRITE(out_write, head);
STOP();

Initial Registers:

R1 = 6; R2 = 2; R3 = 0;
R4 = 2; R5 = 1; R6 = 0;
R7 = 0;

Program:

0 : R5 = READ (R5, R7)
1 : R4 = READ (R4, R7)
2 : R6 = READ (R6, R7)
3 : R5 = DEC (R5, R7)
(R5, R1)
4 : R7 = JEZ
(R3, R2)
5 : R3 = JEZ
6 : R6 = INC
(R6, R7)
7 : R6 = READ (R6, R7)
8 : R7 = WRITE(R4, R6)
9 : R7 = STOP (R7, R7)

(i) Instr.

(ii) Arg1

(iii) Arg2

(iv) Out

(a) Input program

(a) Intermediary representation

(c) Weights

Figure 4: Example of the compilation process. (2a) Program written to perform the ListK task.
Given a pointer to the head of a linked list, an integer k, a target cell and a linked list, write in
the target cell the k-th element of the list. (3a) Intermediary representation of the program. This
corresponds to the instruction that a Random Access Machine would need to perform to execute the
program. (4c) Representation of the weights that encodes the intermediary representation. Each
row of the matrix correspond to one state/line. Initial value of the registers are also parameters of
the model, omitted here.

The diﬀerent steps of the compilation are illustrated in Figure 4. The ﬁrst step is to go from
the written version of the program to the equivalent list of low level instruction. This step can
be seen as going from Figure 2a to Figure 3a. The illustrative example uses a fairly low-level
language but traditional features of programming languages such as loops or if-statements
can be supported using the JEZ instruction. The use of constants as arguments or as values
is handled by introducing new registers that hold these values. The value required to be
passed as target position to the JEZ instruction can be resolved at compile time.

Having obtained this intermediate representation, generating the parameters is straight-
forward. As can be seen in Figure 3a, each line contains one instruction, the two input
registers and the output register, and corresponds to a command that the Controller will
have to output. If we ensure that IR is a Dirac-delta distribution on a given value, then
the matrix-vector product is equivalent to selecting a row of the weight matrix. As IR is
incremented at each iteration, the Controller outputs the rows of the matrix in order. We
thus have a one-to-one mapping between the lines of the intermediate representation and
the rows of the weight matrix. An example of these matrices can be found in Figure 4c. The
weight matrix has 10 rows, corresponding to the number of lines of code of our intermediate
representation. On the ﬁrst line of the matrix corresponding to the ﬁrst argument (4cii), the
ﬁfth element has value 1, and is linked to the ﬁrst line of code where the ﬁrst argument to
the READ operation is the ﬁfth register.

The number of rows of the weight matrix is linear in the number of lines of code in the original
program. To output a command, we must be able to index its line with the instruction
register IR, which means that the largest representable number in our Machine needs to be
greater than the number of lines in our program.

Moreover, any program written in a regular assembly language can be rewritten to use only
our restricted set of instructions. This can be done ﬁrst because all the conditionals of the
the assembly language can be expressed as a combination of arithmetic and JEZ instructions.
Secondly because all the arithmetic operations can be represented as a combination of our
simple arithmetic operations, loops and ifs statements. This means that any program that
can run on a regular computer, can be ﬁrst rewritten to use our restricted set of instructions
and then compiled down to a set of weights for our model. Even though other models use
LSTM as controller, we showed here that a Controller composed of simple linear functions is
expressive enough. The advantage of this simpler model is that we can now easily interpret

6

the weights of our model in a way that would not have be possible if we had a recurrent
network as a controller.

The most straightforward way to leverage the results of the compilation is to initialise the
Controller with the weights obtained through compilation of the generic algorithm. To
account for the extra softmax layer, we need to multiply the weights produced by the compiler
by a large constant to output Dirac-delta distributions. Some results associated with this
technique can be found in Section 5.1. However, if we initialise with exactly this sharp set of
parameters, the training procedure is not able to move away from the initialisation as the
gradients associated with the softmax in this region are very small. Instead, we initialise
the controller with a non-ideal version of the generic algorithm. This means that the choice
with the highest probability in the output of the Controller is correct, but the probability of
other choices is not zero. As can be seen in Section 5.2, this allows the Controller to learn
by gradient descent a new algorithm, diﬀerent from the original one, that has a lower loss
than the ideal version of the compiled program.

5 Experiments

We performed two sets of experiments. The ﬁrst shows the capability of the Neural Compiler
to perfectly reproduce any given program. The second shows that our Neural Compiler
can adapt and improve the performance of programs. We present results of data-speciﬁc
optimisation being carried out and show decreases in runtime for all the algorithms and
additionally, for some algorithms, show that the runtime is a diﬀerent computational-
complexity class altogether. All the code required to reproduce these experiments is available
online 1.

5.1 Compilation

The compiler described in section 4.3 allows us to go from a program written using our
instruction set to a set of weights θ for our Controller.

To illustrate this point, we implemented simple programs that can solve the tasks introduced
by Kurach et al. [8] and a shortest path problem. One of these implementations can be
found in Figure 2a, while the others are available in appendix F. These programs are written
in a speciﬁc language, and are transformed by the Neural Compiler into parameters for the
model. As expected, the resulting models solve the original tasks exactly and can generalise
to any input sequence.

5.2 ANC experiments

In addition to being able to reproduce any given program as was done by Reed and de Freitas
[12], we have the possibility of optimising the resulting program further. We exhibit this
by compiling program down to our model and optimising their performance. The eﬃciency
gains for these tasks come either from ﬁnding simpler, equivalent algorithms or by exploiting
some bias in the data to either remove instructions or change the underlying algorithm.

We identify three diﬀerent levels of interpretability for our model: The ﬁrst type corresponds
to weights containing only Dirac-delta distributions, there is an exact one-to-one mapping
between lines in the weight matrices and lines of assembly code. In the second type where
all probabilities are Dirac-delta except the ones associated with the execution of the JEZ
instruction, we can recover an exact algorithm that will use if statements to enumerate the
diﬀerent cases arising from this conditional jump. In the third type where any operation
other than JEZ is executed in a soft way or use a soft argument, it is not possible to recover
a program that will be as eﬃcient as the learned one.

We present here brieﬂy the considered tasks and biases, and report the reader to appendix F
for a detailed encoding of the input/output tape.

1https://github.com/albanD/adaptive-neural-compilation

7

Table 1: Average numbers of iterations required to solve instances of the problems for the original
program, the best learned program and the ideal algorithm for the biased dataset. We also include
the success rate of reaching a more eﬃcient algorithm across multiple random restarts.

Access

Increment Swap ListK Addition Sort

Generic
Learned
Ideal

6
4
4

40
16
34

10
6
6

18
11
10

20
9
6

38
18
9.5

Success Rate

37 %

84%

27%

19%

12%

74%

1. Access: Given a value k and an array A, return A[k]. In the biased version, the
value of k is always be the same, so the address of the required element can be stored
in a constant. This is similar to the optimisation known as constant folding.

2. Swap: Given an array A and two pointers p and q, swap the elements A[p] and A[q].
In the biased version, p and q are always the same so reading them can be avoided.
3. Increment: Given an array, increment all its element by 1. In the biased version,
the array is of ﬁxed size and the elements of the array have the same value so you
don’t need to read all of them when going through the array.

4. Listk: Given a pointer to the head of a linked list, a number k and a linked list,
ﬁnd the value of the k-th element. In the biased version, the linked list is organised
in order in memory, as would be an array, so the address of the k-th value can be
computed in constant time. This is the example developed in Figure 4.

5. Addition: Two values are written on the tape and should be summed. No data bias
is introduced but the starting algorithm is non-eﬃcient: it performs the addition as
a series of increment operation. The more eﬃcient operation would be to add the
two numbers.

6. Sort: Given an array A, sort it. In the biased version, only the start of the array
might be unsorted. Once the start has been arranged, the end of the array can be
safely ignored.

For each of these tasks, we perform a grid search on the loss parameters and on our hyper-
parameters. Training is performed using Adam [7]. We choose the best set of hyperparameters
and run the optimisation with 100 diﬀerent random seeds. We consider that a program has
been successfully optimised when two conditions are fulﬁlled. First, it needs to output the
correct solution for all test cases presenting the same bias. Second, the average number of
iterations taken to solve a problem must be lower than the algorithm used for initialisation.
Note that if we cared only about the ﬁrst criterion, the methods presented in Section 5.1
would already provide a success rate of 100%, without requiring any training.

The results are presented in Table 1. For each of these tasks, we manage to ﬁnd faster
algorithms. In the simple cases of Access and Swap, the optimal algorithm for the presented
datasets are obtained. Exploiting the bias of the data, successful heuristics are incorporated
in the algorithm and appropriate constants get stored in the initial value of registers. The
learned programs for these tasks are always in the ﬁrst case of interpretability, this means
that we can recover the most eﬃcient algorithm from the learned weights.

While ListK and Addition have lower success rates, the improvements between the original
and learned algorithms are still signiﬁcant. Both were initialised with iterative algorithms
with O(n) complexities. They managed to ﬁnd constant time O(1) algorithms to solve the
given problems, making the runtime independent of the input. Achieving this means that
the equivalence between the two approaches has been identiﬁed, similar to how optimising
compilers operate. Moreover, on the ListK task, some learned programs corresponds to
the second type of interpretability. Indeed these programs use soft jumps to condition the
execution on the value of k. Even though these program would not generalise to other values
of k, some learned programs for this task achieve a type one interpretability and a study of
the learned algorithm reveal that they can generalise to any value of k.

8

Finally, the Increment task achieves an unexpected result. Indeed, it is able to outperform
our best possible algorithm. By looking at the learned program, we can see that it is actually
leveraging the possibility to perform soft writes over multiple elements of the memory at
the same time to reduce its runtime. This is the only case where we see a learned program
associated with the third type of interpretability. While our ideal algorithm would give a
conﬁdence of 1 on the output, this algorithm is unable to do so, but it has a high enough
conﬁdence of 0.9 to be considered a correct algorithm.

In practice, for all but the most simple tasks, we observe that further optimisation is possible,
as some useless instructions remain present. Some transformations of the controller are indeed
diﬃcult to achieve through the local changes operated by the gradient descent algorithm.
An analysis of these failure modes of our algorithm can be found in appendix G.4. This
motivates us to envision the use of approaches other than gradient descent to address these
issues.

6 Discussion

The work presented here is a ﬁrst step towards adaptive learning of programs. It opens
up several interesting directions of future research. For exemple, the deﬁnition of eﬃciency
that we considered in this paper is ﬂexible. We chose to only look at the average number
of operations executed to generate the output from the input. We leave the study of other
potential measures such as Kolmogorov Complexity and sloc, to name a few, for future
works.

As shown in the experiment section, our current method is very good at ﬁnding eﬃcient
solutions for simple programs. For more complex programs, only a solution close to the
initialisation can be found. Even though training heuristics could help with the tasks
considered here, they would likely not scale up to real applications. Indeed, the main problem
we identiﬁed is that the gradient-descent based optimisation is unable to explore the space
of programs eﬀectively, by performing only local transformations. In future work, we want
to explore diﬀerent optimisation methods. One approach would be to mix global and local
exploration to improve the quality of the solutions. A more ambitious plan would be to
leverage the structure of the problem and use techniques from combinatorial optimisation to
try and solve the original discrete problem.

References

[1] Marcin Andrychowicz and Karol Kurach. Learning eﬃcient algorithms with hierarchical

attentive memory. CoRR, 2016.

[2] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, 2014.

[3] Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom.

Learning to transduce with unbounded memory. In NIPS, 2015.

[4] Frédéric Gruau, Jean-Yves Ratajszczak, and Gilles Wiber. A neural compiler. Theoretical

Computer Science, 1995.

recurrent nets. In NIPS, 2015.

[5] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented

[6] Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. In ICLR, 2016.

[7] Diederik Kingma and Jimmy Adam. A method for stochastic optimization. In ICLR,

2015.

chines. In ICLR, 2016.

[8] Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access ma-

[9] Henry Massalin. Superoptimizer: a look at the smallest program. In ACM SIGPLAN

Notices, volume 22, pages 122–126. IEEE Computer Society Press, 1987.

9

[10] Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol
Kurach, and James Martens. Adding gradient noise improves learning for very deep
networks. In ICLR, 2016.

[11] João Pedro Neto, Hava Siegelmann, and Félix Costa. Symbolic processing in neural

networks. Journal of the Brazilian Computer Society, 2003.

[12] Scott Reed and Nando de Freitas. Neural programmer-interpreters. In ICLR, 2016.

[13] Eric Schkufza, Rahul Sharma, and Alex Aiken. Stochastic superoptimization. In ACM

SIGARCH Computer Architecture News, 2013.

[14] Rahul Sharma, Eric Schkufza, Berkeley Churchill, and Alex Aiken. Conditionally correct

superoptimization. In OOPSLA, 2015.

[15] Hava Siegelmann. Neural programming language. In AAAI, 1994.

[16] Ronald Williams. Simple statistical gradient-following algorithms for connectionist

reinforcement learning. Machine learning, 1992.

[17] Wojciech Zaremba and Ilya Sutskever. Reinforcement learning neural turing machines.

arXiv preprint arXiv:1505.00521, 2015.

[18] Wojciech Zaremba, Tomas Mikolov, Armand Joulin, and Rob Fergus. Learning simple

algorithms from examples. CoRR, 2015.

7 Appendix

A Detailed Model Description

In this section, we are going to precisely deﬁne the non diﬀerentiable model used above. This
model can be seen as a recurrent network. Indeed, it takes as input an initial memory tape,
performs a certain number of iterations and outputs a ﬁnal memory tape. The memory tape
is an array of M cells, where a cell is an element holding a single integer value. The internal
state of this recurrent model are the memory, the registers and the instruction register.
The registers are another set of R cells that are internal to the model. The instruction
register is a single cell used in a speciﬁc way described later. These internal states are noted
R} and IRt for the memory, the registers and
Mt = {mt
M }, Rt = {rt
2, . . . , mt
the instruction register respectively.

2, . . . , rt

1, mt

1, rt

Figure 1 describes in more detail how the diﬀerent elements interact with each other. At
each iteration, the Controller takes as input the value of the instruction register IRt and
outputs four values:

et, at, bt, ot = Controller(IRt).
(5)
The ﬁrst value et is used to select one of the instruction of the Machine to execute at this
iteration. The second and third values at and bt will identify which registers to use as the
ﬁrst and second argument for the selected instruction. The fourth value ot identity the
output register where to write the result of the executed instruction. The Machine then
takes as input these four values and the internal state and computes the updated value of
the internal state and a stop ﬂag:

Mt+1, Rt+1, IRt+1, stop = Machine(Mt, Rt, IRt, et, at, bt, ot).

(6)

The stop ﬂag is a binary ﬂag. When its value is 1, it means that the model will stop the
execution and the current memory state will be returned.

10

The Machine The machine is a deterministic function that increments the instruction
register and executes the command given by the Controller to update the current internal
state. The set of instructions that can be executed by the Machine can be found in Table 1b.
Each instruction takes two values as arguments and returns a value. Additionally, some of
these instructions have side eﬀects. This mean that they do not just output a value, they
perform another task. This other task can be for example to modify the content of the
memory. All the considered side eﬀects can be found in Table 1b. By convention, instructions
that don’t have a value to return and that are used only for their side-eﬀect will return a
value of 0.

The Controller The Controller is a function that takes as input a single value and outputs
four diﬀerent values. The Controller’s internal parameters, the initial values for the registers
and the initial value of the instruction register deﬁne uniquely a given Controller.

The usual choice in the literature is to use an LSTM network[2, 3, 8] as controller. Our
choice was to instead use a simpler model. Indeed, our Controller associates a command
to each possible value of the instruction register. Since the instruction register’s value will
increase by one at each iteration, this will enforce the Controller to encode in its weights
what to do at each iteration. If we were using a recurrent controller the same instruction
register could potentially be associated to diﬀerent sets of outputs and we would lose this
one to one mapping.

To make this clearer, we ﬁrst rewrite the instruction register as an indicator vector with a 1
at the position of its value:

Ii =

(cid:26)1
0

if i = IRt
otherwise

.

(7)

In this case, we can write a single output at of the Controller as the result of a linear function
of I:

at = Wa ∗ I ,
(8)
where Wa is the 1xM matrix containing the value that need to be chosen as ﬁrst arguments for
each possible value of the instruction register and ∗ represent a matrix vector multiplication.

A.1 Mathematical details of the diﬀerentiable model

In order to make the model diﬀerentiable, every value and every choice are replaced by
probability distributions over the possible choices. Using convex combinations of probability,
the execution of the Machine is made diﬀerentiable. We present here the mathematical
formulation of this procedure for the case of the side-eﬀects.

STOP In the discrete model, the execution is halted when the STOP instruction is
executed. However, in the diﬀerentiable model, the STOP instruction may be executed
with a probability smaller than 1. To take this into account, when executing the model, we
keep track of the probability that the program should have terminated before this iteration
based on the probability associated to the STOP instruction at each iteration. Once this
probability goes over a threshold ηstop ∈]0, 1], the execution is halted.

READ The mechanism is entirely the same as the one used to compute the arguments
based on the registers and a probability distribution over the registers.

JEZ We note IRt+1
or not the JEZ instruction. We also have et
iteration t. The new value of the instruction register is:

njez the new value of IRt if we had respectively executed
jez the probability of executing this instruction at

jez and IRt+1

IRt+1 = IRt+1

njez · (1 − et

jez) + IRt+1

jez · et

jez

IRt+1
jez is himself computed based on several probability distribution. If we consider that
the instruction JEZ is executed with probabilistic arguments cond and label, its value is
given by

IRt+1

jez = label · cond0 + INC(IRt) · (1 − cond0)

(9)

(10)

11

With a probability equals to the one that the ﬁrst argument is equal to zero, the new value
of IRt is label. With the complement, it is equal to the incremented version of its current
value, as the machine automatically increments the instruction register.

WRITE The mechanism is fairly similar to the one of the JEZ instruction.
We note Mt+1
W RIT E and Mt+1
not the WRITE instruction. We also have et
at iteration t. The new value of the memory matrix register is:

nW RIT E the new value of Mt if we had respectively executed or
write the probability of executing this instruction

Mt+1 = Mt+1

nW RIT E · (1 − et

write) + Mt+1

W RIT E · et

W RIT E

As with the JEZ instruction, the value of Mt+1
W RIT E is dependent on the two probability
distribution given as input: addr and val. The probability that the i-th cell of the memory
tape contains the value j after the update is:

M t+1

i,j = addri · valj + (1 − addri) · M t
i,j

Note that this can done using linear algebra operations so as to update everything in one
global operation.

Mt+1 = (cid:0)((1 − addr)1T ) ⊗ Mt(cid:1) + (addr valT )

(11)

(12)

(13)

B Speciﬁcation of the loss

This loss contains four terms that will balance the correctness of the learnt algorithm, proper
usage of the stop signal and speed of the algorithms. The parameters deﬁning the models are
the weight of the Controller’s function and the initial value of the registers. When running
the model with the parameters θ, we consider that the execution ran for T time steps. We
consider the memory to have a size M and that each number can be an integer between
0 and M − 1. Mt was the state of the memory at the t-th step. T and C are the target
memory and the 0-1 mask of the elements we want to consider. All these elements are
matrices where for example Mt
i,j is the probability of the i-th entry of the memory to take
the value j at the step t. We also note pstop,t the probability outputted by the Machine that
it should have stopped before iteration t.

Correctness The ﬁrst term corresponds to the correctness of the given algorithm. For a
given input, we have the expected output and a mask. The mask allows us to know which
elements in the memory we should consider when comparing the solutions. For the given
input, we will compare the values speciﬁed by the mask of the expected output with the
ﬁnal memory tape provided by the execution. We compare them with the L2 distance in the
probability space. Using the notations from above, we can write this term as:
X

Lc(θ) =

Ci,j(MT

i,j(θ) − Ti,j)2.

(14)

i,j

If we optimised only this ﬁrst term, nothing would encourage the learnt algorithm to use the
STOP instruction and halt as soon as it ﬁnished.

Halting To prevent programs to take an inﬁnite amount of time without stopping, we
deﬁned a maximum number of iterations Tmax after which the execution is halted. During
training, we also add a penalty if the Controller didn’t halt before this limit:

LsTmax(θ) = (1 − pstop−T (θ)) · [T == Tmax]

Eﬃciency If we consider only the above mentioned losses, the program will make sure to
halt by itself but won’t do it as early as possible. We incentivise this behaviour by penalising
each iteration taken by the program where it does not stop:

(15)

(16)

Lt(θ) =

(1 − pstop,t(θ)).

X

t∈[1,T −1]

12

Conﬁdence Moreover, we want the algorithm to have a good conﬁdence to stop when
it has found the correct output. To do so, we add the following term which will penalise
probability of stopping if the current state of the memory is not the expected one:

Lst(θ) =

X

X

t∈[2,T ]

i,j

(pstop,t(θ) − pstop,t−1(θ))Ci,j(Mt

i,j(θ) − Ti,j)2.

(17)

The increase in probability (pstop,t − pstop,t−1) corresponds to the probability of stopping
exactly at iteration t. So, this is equivalent to the expected error made.

Total loss The complete loss that we use is then the following:

L(θ) = αLc(θ) + βLsTmax (θ) + γLst(θ) + δLt(θ).

(18)

C Distributed representation of the program

For the most of out experiments, the learned weights are fully interpretable as they ﬁt in the
ﬁrst type of interpretability. However, in some speciﬁc cases, under the pressure of our loss
encouraging a smaller number of iterations, an interesting behavior emerges.

It is interesting to note that the decompiled version is not straightforward to
Remarks
interpret. Indeed when we reach a program that has non Dirac-delta distributions in its
weights, we cannot perform the inverse of the one-to-one mapping performed by the compiler.
In fact, it relies on this blurriness to be able to execute the program with a smaller number
of instruction. Notably, by having some blurriness on the JEZ instruction, the program can
hide additional instructions, by creating a distributed state. We now explain the mechanism
used to achieve this.

Creating a distributed state Consider the following program and assume that the initial
value of IR is 0:

Initial Registers:
R1 = 0; R2 = 1; R3 = 4, R4 = 0

Program:
0 : R1 = READ (R1, R4)
1 : R4 = JEZ (R1, R3)
2 : R4 = WRITE(R1, R1)
3 : R4 = WRITE(R1, R3)

If you take this program and execute it for three iterations, it will: read the ﬁrst value of
the tape into R1. Then, if this value is zero, it will jump to State 4, otherwise it will just
increment IR. This means that depending on the value that was in R1, the next instruction
that will be executed will be diﬀerent (in this case, the diﬀerence between State 3 and State
4 is which registers they will be writing from). This is our standard way of implementing
conditionals.

Imagine that, after learning, the second instruction in our example program has 0.5 probability
of being a JEZ and 0.5 probability of being a ZERO. If the content of R1 is a zero, according
to the JEZ, we should jump to State 4, but this instruction is executed with a probability
of 0.5. We also have 0.5 probability of executing the ZERO instruction, which would lead to
State 3.

Therefore, IR is not a Dirac-delta distribution anymore but points to State 3 with probability
0.5 and State 4 with probability 0.5.

Exploiting a distributed state To illustrate, we will discuss how the Controller computes
a for a model with 3 registers. The Table 2 show an example of some weights for such a
controller.

If we are in State 1, the output of the controller is going to be

out = softmax([20, 5, −20]) = [0.9999..., 3e−7, 4e−18]

(19)

13

State 1
State 2

R1 R2 R3
-20
5
20
20
5
-20

Table 2: Controller Weights

If we are in State 2, the output of the controller is going to be

out = softmax([−20, 5, 20]) = [4e−18, 3e−7, 0.9999...]

(20)

In both cases, the output of the controller is therefore going to be almost discrete. In State
1, R1 would be chosen and in State 2, R3 would be chosen.

However, in the case where we have a distributed state with probability 0.5 over State 1 and
0.5 over State 2, the output would be:

out = softmax(0.5 ∗ [−20, 5, 20] + 0.5[20, 5, −20])

= softmax([0, 10, 0])
= [4e−5, 0.999, 4e−5].

(21)

Note that the result of the distributed state is actually diﬀerent from the result of the discrete
states. Moreover it is still a discrete choice of the second register.

Because this program contains distributed elements, it is not possible to perform the one-to-
one mapping between the weights and the lines of code. Though every instruction executed
by the program, except for the JEZ, are binary. This means that this model can be translated
to a regular program that will take exactly the same runtime, but will require more lines of
codes than the number of lines in the matrix.

D Alternative Learning Strategies

A critique that can be made to this method is that we will still initialise close to a local
minimum. Another approach might be to start from a random initialisation but adding a
penalty on the value of the weights such that they are encourage to be close to the generic
algorithm. This can be seen as L2 regularisation but instead of pushing the weights to 0, we
push then with the value corresponding to the generic algorithm. If we start with a very
high value of this penalty but use an annealing schedule where its importance is very quickly
reduced, this is going to be equivalent to the previous method.

E Possible Extension

E.1 Making objective function diﬀerentiable

These experiments showed that we can transform any program that perform a mapping
between an input memory tape to an output memory tape to a set of parameters and execute
it using our model. The ﬁrst point we want to make here is that this means that we take
any program and transform it into a diﬀerentiable function easily. For example, if we want
to learn a model that given a graph and two nodes a and b, will output the list of nodes to
go through to go from a to b in the shortest amount of time. We can easily deﬁne the loss of
the length of the path outputted by the model. Unfortunately, the function that computes
this length from the set of nodes is not diﬀerentiable. Here we could implement this function
in our model and use it between the prediction of the model and the loss function to get an
end to end trainable system.

E.2 Beyond mimicking and towards open problems

It would even be possible to generalise our learning procedure to more complex problems
for which we don’t have a ground truth output. For example, we could consider problems
where the exact answer for a given input is not computable or not unique. If the goodness
of a solution can be computed easily, this value could be used as training objective. Any
program giving a solution could be used as initialisation and our framework would improve
it, making it generate better solutions.

14

This section will present the programs that we use as initialisation for the experiment section.

F Example tasks

F.1 Access

In this task, the ﬁrst element in the memory is a value k. Starting from the second element,
the memory contains a zero-terminated list. The goal is to access the k-th element in the
list that is zero-indexed. The program associated with this task can be found in Listing 1.

1

2

3

4

5

6

(cid:7)
var k = 0
k = READ (0)
k = INC ( k )
k = READ ( k )
WRITE (0 , k )
STOP ()
(cid:6)
Listing 1: Access Task

(cid:4)

(cid:5)

Example input:
Output:

6
1

9
9

1
1

2
2

7
7

9
9

8
8

1
1

3
3

5
5

F.2 Copy

In this task, the ﬁrst element in the memory is a pointer p. Starting from the second element,
the memory contains a zero-terminated list. The goal is to copy this list at the given pointer.
The program associated with this task can be found in Listing 2.

1

2

3

4

5

6

7

8

9

10

11

12

13

(cid:7)
var read_addr = 0
var read_value = 0
var write_addr = 0

write_addr = READ (0)
l_loop : read_value = READ ( read_addr )
JEZ ( read_value , l_stop )
WRITE ( write_addr , read_value )
read_addr = INC ( read_addr )
write_addr = INC ( write_addr )
JEZ (0 , l_loop )

l_stop : STOP ()
(cid:6)

Listing 2: Copy Task

(cid:4)

(cid:5)

Example input:
Output:

9
9

11
11

3
3

1
1

5
5

14
14

0
0

0
0

0
0

0
11

0
3

0
1

0
5

0
14

0
0

F.3

Increment

In this task, the memory contains a zero-terminated list. The goal is to increment each value
in the list by 1. The program associated with this task can be found in Listing 3.

15

1

2

3

4

5

6

7

8

9

10

11

(cid:7)
var read_addr = 0
var read_value = 0

l_loop : read_value = READ ( read_addr )
JEZ ( read_value , l_stop )
read_value = INC ( read_value )
WRITE ( read_addr , read_value )
read_addr = INC ( read_addr )
JEZ (0 , l_loop )

l_stop : STOP ()
(cid:6)

Listing 3: Increment Task

(cid:4)

(cid:5)

Example input:
Output:

1
2

2
3

2
3

3
4

0
0

0
0

0
0

F.4 Reverse

In this task, the ﬁrst element in the memory is a pointer p. Starting from the second element,
the memory contains a zero-terminated list. The goal is to copy this list at the given pointer
in the reverse order. The program associated with this task can be found in Listing 4.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

(cid:7)
var read_addr = 0
var read_value = 0
var write_addr = 0

write_addr = READ ( write_addr )
l_count_phase : read_value = READ ( read_addr )
JEZ ( read_value , l_copy_phase )
read_addr = INC ( read_addr )
JEZ (0 , l_count_phase )

l_copy_phase : read_addr = DEC ( read_addr )
JEZ ( read_addr , l_stop )
read_value = READ ( read_addr )
WRITE ( write_addr , read_value )
write_addr = INC ( write_addr )
JEZ (0 , l_copy_phase )

l_stop : STOP ()
(cid:6)

Listing 4: Reverse Task

(cid:4)

(cid:5)

Example input:
Output:

5
5

7
7

2
2

13
13

14
14

0
14

0
13

0
2

0
7

0
0

0
0

0
0

0
0

0
0

0
0

F.5 Permutation

In this task, the memory contains two zero-terminated list one after the other. The ﬁrst
contains a set of indices. the second contains a set of values. The goal is to ﬁll the ﬁrst list
with the values in the second list at the given index. The program associated with this task
can be found in Listing 5.

16

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

(cid:7)
var read_addr = 0
var read_value = 0
var write_offset = 0

l_count_phase : read_value = READ ( write_offset )
write_offset = INC ( write_offset )
JEZ ( read_value , l_copy_phase )
JEZ (0 , l_count_phase )

l_copy_phase : read_value = DEC ( read_addr )
JEZ ( read_value , l_stop )
read_value = ADD ( write_offset , read_value )
read_value = READ ( read_value )
WRITE ( read_addr , read_value )
read_addr = INC ( read_addr )
JEZ (0 , l_copy_phase )
l_stop : STOP ()
(cid:6)

Listing 5: Permutation Task

(cid:4)

(cid:5)

Example input:
Output:

2
4

1
13

3
6

0
0

13
13

4
4

6
6

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

F.6 Swap

In this task, the ﬁrst two elements in the memory are pointers p and q. Starting from the
third element, the memory contains a zero-terminated list. The goal is to swap the elements
pointed by p and q in the list that is zero-indexed. The program associated with this task
can be found in Listing 6.

(cid:7)
var p = 0
var p_val = 0
var q = 0
var q_val = 0

p = READ (0)
q = READ (1)
p_val = READ ( p )
q_val = READ ( q )
WRITE (q , p_val )
WRITE (p , q_val )
STOP ()
(cid:6)

1

2

3

4

5

6

7

8

9

10

11

12

(cid:4)

(cid:5)

Listing 6: Swap Task

Example input:
Output:

1
1

3
3

7
7

6
5

7
7

5
6

2
2

0
0

0
0

0
0

F.7 ListSearch

In this task, the ﬁrst three elements in the memory are a pointer to the head of the linked
list, the value we are looking for v and a pointer to a place in memory where to store the
result. The rest of the memory contains the linked list. Each element in the linked list is two
values, the ﬁrst one is the pointer to the next element, the second is the value contained in
this element. By convention, the last element in the list points to the address 0. The goal is
to return the pointer to the ﬁrst element whose value is equal to v. The program associated
with this task can be found in Listing 7.

17

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

(cid:7)
var p_out = 0
var p_current = 0
var val_current = 0
var val_searched = 0

val_searched = READ (1)
p_out = READ (2)
l_loop : p_current = READ ( p_current )
val_current = INC ( p_current )
val_current = READ ( val_current )
val_current = SUB ( val_current , val_searched )
JEZ ( val_current , l_stop )
JEZ (0 , l_loop )
l_stop : WRITE ( p_out , p_current )
STOP ()
(cid:6)

Listing 7: ListSearch Task

(cid:4)

(cid:5)

Example input:
Output:

11
11

10
10

2
5

9
9

4
4

3
3

10
10

0
0

6
6

7
7

13
13

5
5

12
12

0
0

0
0

F.8 ListK

In this task, the ﬁrst three elements in the memory are a pointer to the head of the linked
list, the number of hops we want to do k in the list and a pointer to a place in memory
where to store the result. The rest of the memory contains the linked list. Each element in
the linked list is two values, the ﬁrst one is the pointer to the next element, the second is
the value contained in this element. By convention, the last element in the list points to
the address 0. The goal is to return the value of the k-th element of the linked list. The
program associated with this task can be found in Listing 8.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

(cid:7)
var p_out = 0
var p_current = 0
var val_current = 0
var k = 0

k = READ (1)
p_out = READ (2)
l_loop : p_current = READ ( p_current )
k = DEC ( k )
JEZ (k , l_stop )
JEZ (0 , l_loop )
l_stop : p_current = INC ( p_current )
p_current = READ ( p_current )
WRITE ( p_out , p_current )
STOP ()
(cid:6)

Listing 8: ListK Task

(cid:4)

(cid:5)

3
3

2
2

2
17

9
9

15
15

0
0

0
0

0
0

1
1

15
15

17
17

7
7

13
13

0
0

0
0

11
11

Example input:
Output:
0
0
10
0
0
10

0
0

F.9 Walk BST

In this task, the ﬁrst two elements in the memory are a pointer to the head of the BST and a
pointer to a place in memory where to store the result. Starting at the third element, there

18

is a zero-terminated list containing the instructions on how to traverse in the BST. The rest
of the memory contains the BST. Each element in the BST has three values, the ﬁrst one is
the value of this node, the second is the pointer to the left node and the third is the pointer
to the right element. By convention, the leafs points to the address 0. The goal is to return
the value of the node we get at after following the instructions. The instructions are 1 or
2 to go respectively to the left or the right. The program associated with this task can be
found in Listing 9.
(cid:7)
var p_out = 0
var p_current = 0
var p_instr = 0
var instr = 0

(cid:4)

2

1

4

3

5

6

7

8

9

10

11

12

13

14

15

16

17

18

p_current = READ (0)
p_out = READ (1)
instr = READ (2)

l_loop : JEZ ( instr , l_stop )
p_current = ADD ( p_current , instr )
p_current = READ ( p_current )
p_instr = INC ( p_instr )
JEZ (0 , l_loop )

l_stop : p_current = READ ( p_current )
WRITE ( p_out , p_current )
STOP ()
(cid:6)

(cid:5)

Listing 9: WalkBST Task

Example input:
Output:
24
0
8
24
0
8

0
0

0
0

12
12
0
0

1
10
0
0

0
0

1
1

2
2

0
0
10
10

0
0

0
0
0
0

15
15
0
0

0
0

0
0

0
0

0
0

9
9

23
23

0
0

0
0

11
11

15
15

6
6

F.10 Merge

In this task, the ﬁrst three elements in the memory are pointers to respectively, the ﬁrst list,
the second list and the output. The two lists are zero-terminated sorted lists. The goal is to
merge the two lists into a single sorted zero-terminated list that starts at the output pointer.
The program associated with this task can be found in Listing 10.

19

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

(cid:4)

(cid:5)

(cid:7)
var p_first_list = 0
var val_first_list = 0
var p_second_list = 0
var val_second_list = 0
var p_output_list = 0
var min = 0

p_first_list = READ (0)
p_second_list = READ (1)
p_output_list = READ (2)

l_loop : val_first_list = READ ( p_first_list )
val_second_list = READ ( p_second_list )
JEZ ( val_first_list , l_first_finished )
JEZ ( val_second_list , l_second_finished )
min = MIN ( val_first_list , val_second_list )
min = SUB ( val_first_list , min )
JEZ ( min , l_first_smaller )

WRITE ( p_output_list , val_first_list )
p_output_list = INC ( p_output_list )
p_first_list = INC ( p_first_list )
JEZ (0 , l_loop )

l_first_smaller : WRITE ( p_output_list , val_second_list )
p_output_list = INC ( p_output_list )
p_second_list = INC ( p_second_list )
JEZ (0 , l_loop )

l_first_finished : p_first_list = ADD ( p_second_list , 0)
val_first_list = ADD ( val_second_list , 0)

l_second_finished : WRITE ( p_output_list , val_first_list )
p_first_list = INC ( p_first_list )
p_output_list = INC ( p_output_list )
val_first_list = READ ( p_first_list )
JEZ ( val_first_list , l_stop )
JEZ (0 , l_second_finished )

l_stop : STOP ()
(cid:6)

Listing 10: Merge Task

Example input:
Output:
0
0
0
0
1
16

0
0

0
0

3
3
0
0

8
8
0
0

11
11
0
0

27
27
0
0

0
0

17
17
0
0

16
16
0
0

1
1
0
0

0
0
0
0

29
29
0
0

26
26

0
0

0
29

0
27

0
26

0
17

F.11 Dijkstra

In this task, we are provided with a graph represented in the input memory as follow. The
ﬁrst element is a pointer pout indicating where to write the results. The following elements
contain a zero-terminated array with one entry for each vertex in the graph. Each entry is a
pointer to a zero-terminated list that contains a pair of values for each outgoing edge of the
considered node. Each pair of value contains ﬁrst the index in the ﬁrst array of the child node
and the second value contains the cost of this edge. The goal is to write a zero-terminated
list at the address provided by pout that will contain the value of the shortest path from the
ﬁrst node in the list to this node. The program associated with this task can be found in
Listings 11 and 12.

20

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

(cid:7)
var min = 0
var argmin = 0

var p_out = 0
var p_out_temp = 0
var p_in = 1
var p_in_temp = 1

var nnodes = 0

var zero = 0
var big = 99

var tmp_node = 0
var tmp_weight = 0
var tmp_current = 0
var tmp = 0

var didsmth = 0

p_out = READ ( p_out )
p_out_temp = ADD ( p_out , zero )

tmp_current = INC ( zero )
l_loop_nnodes : tmp = READ ( p_in_temp )
JEZ ( tmp , l_found_nnodes )
WRITE ( p_out_temp , big )
p_out_temp = INC ( p_out_temp )
WRITE ( p_out_temp , tmp_current )
p_out_temp = INC ( p_out_temp )
p_in_temp = INC ( p_in_temp )
nnodes = INC ( nnodes )
JEZ ( zero , l_loop_nnodes )

l_found_nnodes : WRITE ( p_out , zero )
JEZ ( zero , l_find_min )
l_min_return : p_in_temp = ADD ( p_in , argmin )
p_in_temp = READ ( p_in_temp )

l_loop_sons : tmp_node = READ ( p_in_temp )
JEZ ( tmp_node , l_find_min )
tmp_node = DEC ( tmp_node )
p_in_temp = INC ( p_in_temp )
tmp_weight = READ ( p_in_temp )
p_in_temp = INC ( p_in_temp )

p_out_temp = ADD ( p_out , tmp_node )
p_out_temp = ADD ( p_out_temp , tmp_node )
tmp_current = READ ( p_out_temp )
tmp_weight = ADD ( min , tmp_weight )

tmp = MIN ( tmp_current , tmp_weight )
tmp = SUB ( tmp_current , tmp )
JEZ ( tmp , l_loop_sons )
WRITE ( p_out_temp , tmp_weight )
JEZ ( zero , l_loop_sons )
(cid:6)

Listing 11: Dijkstra Algorithm (Part 1)

21

(cid:4)

(cid:5)

57

58

59

60

61

62

63

64

65

66

67

68

69

70

71

72

73

74

75

76

77

78

79

80

81

82

83

84

85

86

87

88

(cid:7)
l_find_min : p_out_temp = DEC ( p_out )
tmp_node = DEC ( zero )
min = ADD ( big , zero )
argmin = DEC ( zero )

l_loop_min : p_out_temp = INC ( p_out_temp )
tmp_node = INC ( tmp_node )
tmp = SUB ( tmp_node , nnodes )
JEZ ( tmp , l_min_found )

tmp_weight = READ ( p_out_temp )

p_out_temp = INC ( p_out_temp )
tmp = READ ( p_out_temp )
JEZ ( tmp , l_loop_min )

tmp = MAX ( min , tmp_weight )
tmp = SUB ( tmp , tmp_weight )
JEZ ( tmp , l_loop_min )
min = ADD ( tmp_weight , zero )
argmin = ADD ( tmp_node , zero )
JEZ ( zero , l_loop_min )

l_min_found : tmp = SUB ( min , big )
JEZ ( tmp , l_stop )
p_out_temp = ADD ( p_out , argmin )
p_out_temp = ADD ( p_out_temp , argmin )
p_out_temp = INC ( p_out_temp )
WRITE ( p_out_temp , zero )
JEZ ( zero , l_min_return )

l_stop : STOP ()
(cid:6)

Listing 12: Dijkstra Algorithm (Part 2)

(cid:4)

(cid:5)

Example omitted for space reasons

G Learned optimisation: Case study

Here we present an analysis of the optimisation achieved by the ANC. We take the example
of the ListK task and study the diﬀerence between the learned program and the initialisation
used.

G.1 Representation

The representation chosen is under the form of the intermediary representation described in
Figure (2b). Based on the parameters of the Controller, we can recover the approximate
representation described in Figure (2b): 1For each possible "discrete state" of the instruction
register, we can compute the commands outputted by the controller. We report the most
probable value for each distribution, as well as the probability that the compiler would assign
to this value. If no value has a probability higher than 0.5, we only report a neutral token
(R-, -, NOP).

G.2 Biased ListK

Figure 5 represents the program that was used as initialisation to the optimisation problem.
This is the direct result from the compilation performed by the Neural Compiler of the
program described in Listing 8. A version with a probability of 1 for all necessary instructions
would have been easily obtained but not amenable to learning.

22

Figure 6 similarly describes the program that was obtained after learning.

As a remainder, the bias introduced in the ListK task is that the linked list is well organised
in memory. In the general case, the element could be in any order. An input memory tape to
the problem of asking for the third element in the linked list containing {4, 5, 6, 7} would be:

or

9

5

3

3

2

2

0

0

0

0

11

5

0

7

4

15

7

5

5

0

4

7

7

0

6

0

0

0

0

0

0

9

0

6

0

0

0

0

0

0

In the biased version of the task, all the elements are arranged in order and contiguously
positioned on the tape. The only valid representation of this problems is:

3

3

2

5

4

7

5

9

6

0

7

0

0

0

0

0

0

0

0

0

G.3 Solutions

Because of the additional structure of the problem, the bias in the data, a more eﬃcient
algorithm to ﬁnd the solution exists. Let us dive into the comparison of the two diﬀerent
solutions.

Both use their ﬁrst two states to read the parameters of the given instance of the task.
Which element of the list should be returned is read at line (0:) and where to write the
returned value is read at line (1:). Step (2:) to (6:) are dedicated to putting the address of
the k-th value of the linked list into the registers R1. Step (7:) to (9:) perform the same
task in both solution: reading the value at the address contained in R1, writing it at the
desired position and stopping.

The diﬀerence between the two programs lies in how they put the address of the k-th value
into R1.

Generic The initialisation program, used for initialisation, works in the most general case
so needs to perform a loop where it put the address of the next element in the linked list in
R1 (2:), decrement the number of jumps remaining to be made (3:), checking whether the
wanted element has been reached (4:) and going back to the start of the loop if not (5:).
Once the desired element is reached, R1 is incremented so as to point on the value of the
linked list element.

Speciﬁc On the other hand, in the biased version of the problem, the position of the
desired value can be analytically determined. The function parameters occupy the ﬁrst three
cells of the tape. After those, each element of the linked list will occupy two cells (one for
the pointer to the next address and one for the value). Therefore, the address of the desired
value is given by

R1 = 3 + (2 ∗ (k − 1) + 1) − 1 + 1

= 3 + 2 ∗ k − 1

(the -1 comes from the fact that the address are 0-indexed and the ﬁnal +1 from the fact
that we are interested in the position of the value and not of the pointer.)

The way this is computed is as follows:

- R1 = 3 + k by adding the constant 3 to the registers R2 containing K.
- R2 = k − 1
- R1 = 3 + 2 ∗ k − 1 by adding the now reduced value of R2.

The algorithm implemented by the learned version is therefore much more eﬃcient for the
biased dataset, due to its capability to ignore the loop.

(22)

(2:)
(3:)
(6:)

23

G.4 Failure analysis

An observation that can be made is that in the learned version of the program, Step (4:)
and (5:) are not contributing to the algorithms. They execute instructions that have no
side eﬀect and store the results into the registers R7 that is never used later in the execution.

The learned algorithm could easily be more eﬃcient by not performing these two operations.
However, such an optimisation, while perhaps trivial for a standard compiler, capable of
detecting unused values, is fairly hard for our optimisers to discover. Because we are only
doing gradient descent, the action of "moving some instructions earlier in the program" which
would be needed here to make the useless instructions disappear, is fairly hard, as it involves
modifying several rows of the program at once in a coherent manner.

R1 = 0 (0.88)
R2 = 1 (0.88)
R3 = 2 (0.88)
R4 = 6 (0.88)
R5 = 0 (0.88)
R6 = 2 (0.88)
R7 = - (0.05)

Initial State: 0 (0.88)

R2 (0.96)
0:
R3 (0.96)
1:
R1 (0.96)
2:
R2 (0.96)
3:
R7 (0.96)
4:
R7 (0.96)
5:
R1 (0.96)
6:
R1 (0.96)
7:
R7 (0.96)
8:
9:
R7 (0.96)
10: R- (0.16)
11: R- (0.16)
12: R- (0.17)
13: R- (0.16)
14: R- (0.17)
15: R- (0.16)
16: R- (0.17)
17: R- (0.18)
18: R- (0.17)
19: R- (0.15)

= READ (0.93)
= READ (0.93)
= READ (0.93)
(0.93)
= DEC
(0.93)
= JEZ
(0.93)
= JEZ
(0.93)
= INC
= READ (0.93)
= WRIT (0.93)
= STOP (0.93)
(0.11)
= NOP
(0.11)
= NOP
(0.1)
= NOP
(0.11)
= NOP
(0.11)
= NOP
(0.1)
= NOP
(0.11)
= NOP
(0.1)
= NOP
(0.1)
= NOP
(0.11)
= NOP

[ R2 (0.96)
[ R3 (0.96)
[ R1 (0.96)
[ R2 (0.96)
[ R2 (0.96)
[ R5 (0.96)
[ R1 (0.96)
[ R1 (0.96)
[ R3 (0.96)
[ R- (0.14)
[ R- (0.16)
[ R- (0.18)

[ R- (0.17)

[ R- (0.17)
[ R- (0.16)

, R- (0.14)
, R- (0.14)
, R- (0.14)
, R- (0.14)
, R4 (0.96)
, R6 (0.96)
, R- (0.14)
, R- (0.14)
, R1 (0.96)
, R- (0.14)
, R- (0.16)
, R- (0.16)

, R- (0.17)

, R- (0.16)
, R- (0.16)

[ R- (0.16)

, R- (0.17)

[ R- (0.16)

, R- (0.18)

[ R- (0.18)
[ R- (0.16)

, R- (0.17)
, R- (0.16)

[ R- (0.16)

, R- (0.17)

]
]
]
]
]
]
]
]
]
]
]
]

]

]
]

]

]

]

]
]

Figure 5: Initialisation used for the learning of the ListK task.

24

Initial State: 0 (0.99)

R1 = 3 (0.99)
R2 = 1 (0.99)
R3 = 2 (0.99)
R4 = 10 (0.53)
R5 = 0 (0.99)
R6 = 2 (0.99)
R7 = 7 (0.99)

R2 (0.99)
R6 (0.99)
R1 (1)
R2 (1)
R7 (0.99)
R7 (0.99)
R1 (1)
R1 (0.99)
R7 (0.99)
R7 (0.9)

0:
1:
2:
3:
4:
5:
6:
7:
8:
9:
10: R2 (0.99)
11: R1 (0.98)
12: R3 (0.98)
13: R3 (0.87)
14: R3 (0.89)
15: R3 (0.99)
16: R3 (0.99)
17: R2 (0.99)
18: R3 (0.99)
19: R3 (0.98)

= READ (0.99)
= READ (0.99)
(0.99)
(0.99)

= ADD
= DEC

= ADD

= MAX
= INC

(0.99)
(0.99)

(0.99)
= READ (0.99)
= WRIT (0.99)

= STOP (0.99)

[ R2 (1)
[ R3 (0.99)

, R1 (0.97)

]

, R6 (0.5)
]
]

, R2 (1)
, R1 (0.99)

]

[ R1 (0.99)
[ R2 (1)

[ R2 (0.99)
[ R6 (0.7)

, R1 (0.51)
, R1 (0.89)

]
]

[ R1 (0.99)

, R2 (1)

]

[ R1 (0.99)
[ R6 (1)
[ R6 (0.98)

, R1 (0.53)

]

, R1 (0.99)

]

, R1 (0.99)

]

= STOP (0.96)
(0.73)
= ADD
= ADD
(0.64)
= STOP (0.65)
= STOP (0.62)
= STOP (0.65)
(0.45)
= NOP
= INC
(0.56)
= STOP (0.65)
= STOP (0.98)

[ R1 (0.52)
[ R4 (0.99)
[ R6 (0.99)
[ R3 (0.52)
[ R6 (0.99)
[ R3 (0.99)
[ R6 (0.99)
[ R6 (0.7)
[ R3 (0.99)
[ R2 (0.62)

, R1 (0.99)
, R2 (0.99)
, R1 (0.99)
, R1 (0.99)
, R2 (0.62)
, R2 (0.71)
, R1 (0.99)

, R1 (0.98)

]

, R1 (0.99)
, R1 (0.79)

]
]
]
]
]
]
]

]
]

Figure 6: Learnt program for the listK task

25

6
1
0
2
 
y
a
M
 
6
2
 
 
]
I

A
.
s
c
[
 
 
2
v
9
6
9
7
0
.
5
0
6
1
:
v
i
X
r
a

Adaptive Neural Compilation

Rudy Bunel∗
University of Oxford
rudy@robots.ox.ac.uk

Alban Desmaison∗
University of Oxford
alban@robots.ox.ac.uk

Pushmeet Kohli
Microsoft Research
pkohli@microsoft.com

Philip H.S. Torr
University of Oxford
philip.torr@eng.ox.ac.uk

M. Pawan Kumar
University of Oxford
pawan@robots.ox.ac.uk

Abstract

This paper proposes an adaptive neural-compilation framework to address
the problem of eﬃcient program learning. Traditional code optimisation
strategies used in compilers are based on applying pre-speciﬁed set of
transformations that make the code faster to execute without changing
its semantics. In contrast, our work involves adapting programs to make
them more eﬃcient while considering correctness only on a target input
distribution. Our approach is inspired by the recent works on diﬀerentiable
representations of programs. We show that it is possible to compile programs
written in a low-level language to a diﬀerentiable representation. We also
show how programs in this representation can be optimised to make them
eﬃcient on a target distribution of inputs. Experimental results demonstrate
that our approach enables learning speciﬁcally-tuned algorithms for given
data distributions with a high success rate.

1

Introduction

Algorithm design often requires making simplifying assumptions about the input data.
Consider, for instance, the computational problem of accessing an element in a linked list.
Without the knowledge of the input data distribution, one can only specify an algorithm
that runs in a time linear in the number of elements of the list. However, suppose all the
linked lists that we encountered in practice were ordered in memory. Then it would be
advantageous to design an algorithm speciﬁcally for this task as it can lead to a constant
running time. Unfortunately, the input data distribution of a real world problem cannot be
easily speciﬁed as in the above simple example. The best that one can hope for is to obtain
samples drawn from the distribution. A natural question that arises from these observations:
“How can we adapt a generic algorithm for a computational task using samples from an
unknown input data distribution?”

The process of ﬁnding the most eﬃcient implementation of an algorithm has received
considerable attention in the theoretical computer science and code optimisation community.
Recently, Conditionally Correct Superoptimization [14] was proposed as a method for
leveraging samples of the input data distribution to go beyond semantically equivalent
optimisation and towards data-speciﬁc performance improvements. The underlying procedure
is based on a stochastic search over the space of all possible programs. Additionally, they
restrict their applications to reasonably small, loop-free programs, thereby limiting their
impact in practice.

In this work, we take inspiration from the recent wave of machine-learning frameworks for
estimating programs. Using recurrent models, Graves et al. [2] introduced a fully diﬀerentiable

∗The ﬁrst two authors contributed equally.

representation of a program, enabling the use of gradient-based methods to learn a program
from examples. Many other models have been published recently [3, 5, 6, 8] that build and
improve on the early work by Graves et al. [2]. Unfortunately, these models are usually
complex to train and need to rely on methods such as curriculum learning or gradient noise
to reach good solutions as shown by Neelakantan et al. [10]. Moreover, their interpretability
is limited. The learnt model is too complex for the underlying algorithm to be recovered
and transformed into a regular computer program.

The main focus of the machine-learning community has thus far been on learning programs
from scratch, with little emphasis on running time. However, for nearly all computational
problems, it is feasible to design generic algorithms for the worst-case. We argue that a
more pragmatic goal for the machine learning community is to design methods for adapting
existing programs for speciﬁc input data distributions. To this end, we propose the Adaptive
Neural Compiler (ANC). We design a compiler capable of mechanically converting algorithms
to a diﬀerentiable representation, thereby providing adequate initialisation to the diﬃcult
problem of optimal program learning. We then present a method to improve this compiled
program using data-driven optimisation, alleviating the need to perform a wide search over
the set of all possible programs. We show experimentally that this framework is capable of
adapting simple generic algorithms to perform better on given datasets.

2 Related Works

The idea of compiling programs to neural networks has previously been explored in the
literature. Siegelmann [15] described how to build a Neural Network that would perform
the same operations as a given program. A compiler has been designed by Gruau et al. [4]
targeting an extended version of Pascal. A complete implementation was achieved when
Neto et al. [11] wrote a compiler for NETDEF, a language based on the Occam programming
language. While these methods allow us to obtain an exact representation of a program as a
neural network, they do not lend themselves to optimisation to improve the original program.
Indeed, in their formulation, each elementary step of a program is expressed as a group of
neurons with a precise topology, set of weights and biases, thereby rendering learning via
gradient descent infeasible. Performing gradient descent in this parameter space would result
in invalid operations and thus is unlikely to lead to any improvement. The recent work by
Reed and de Freitas [12] on Neural Programmer-Interpreters (NPI) can also be seen as a way
to compile any program into a neural network by learning a model that mimic the program.
While more ﬂexible than the previous approaches, the NPI is unable to improve on a learned
program due to its dependency on a non-diﬀerentiable environment.

Another approach to this learning problem is the one taken by the code optimisation
community. By exploring the space of all possible programs, either exhaustively [9] or in a
stochastic manner [13], they search for programs having the same results but being more
eﬃcient. The work of Sharma et al. [14] broadens the space of acceptable improvements
to data-speciﬁc optimisations as opposed to the provably equivalent transformations that
were previously the only ones considered. However, this method is still reliant on non-
gradient-based methods for eﬃcient exploration of the space. By representing everything in
a diﬀerentiable manner, we aim to obtain gradients to guide the exploration.

Recently, Graves et al. [2] introduced a learnable representation of programs, called the
Neural Turing Machine (NTM). The NTM uses an LSTM as a Controller, which outputs
commands to be executed by a deterministic diﬀerentiable Machine. From examples of
input/output sequences, they manage to learn a Controller such that the model becomes
capable of performing simple algorithmic tasks. Extensions of this model have been proposed
in [3, 5] where the memory tape was replaced by diﬀerentiable versions of stacks or lists.
Kurach et al. [8] modiﬁed the NTM to introduce a notion of pointers making it more
amenable to represent traditional programs. Parallel works have been using Reinforcement
Learning techniques such as the REINFORCE algorithm [1, 16, 17] or Q-learning [18] to
be able to work with non diﬀerentiable versions of the above mentioned models. All these
models are trained only with a loss based on the diﬀerence between the output of the model
and the expected output. This weak supervision leads to a complex training. For instance

2

IR0

R0

Controller

Controller

Machine

Machine

IR1

R1

M1

stop

...

...

...

IR2

stop

R2

M2

M0

Memory

Memory

MT

Inst

arg1

arg2

output

side eﬀect

STOP
ZERO
INC
DEC
ADD
SUB
MIN
MAX
READ
WRITE

JEZ

-
-
a
a
a
a
a
a
a
a

a

-
-
-
-
b
b
b
b
-
b

b

0
0
a+1
a-1
a+b
a-b
min(a,b)
max(a,b)
mt
a
0

0

stop = 1
-
-
-
-
-
-
-
Memory access
mt
a = b
IRt = b
if a = 0

(a) General view of the whole Model.

(b) Machine instructions.

Figure 1: Model components.

the Neural RAM [8] requires a high number of random restarts before converging to a correct
solution [10], even when using the best hyperparameters obtained through a large grid search.

In our work, we will ﬁrst show that we can design a new neural compiler whose target will
be a Controller-Machine model. This makes the compiled model amenable to learning from
examples. Moreover, we can use it as initialisation for the learning procedure, allowing us to
aim for the more complex task of ﬁnding an eﬃcient algorithm.

3 Model

Our model is composed of two parts: (i) a Controller, in charge of specifying what should
be executed; and (ii) a Machine, following the commands of the Controller. We start by
describing the global architecture of the model. For the sake of simplicity, the general
description will present a non-diﬀerentiable version of the model. Section 3.2 will then
explain the modiﬁcations required to make this model completely diﬀerentiable. A more
detailed description of the model is provided in appendix A.

3.1 General Model

1, rt

1, mt

2, . . . , rt

2, . . . , mt

We ﬁrst deﬁne for each timestep t the memory tape that contains M integer values
Mt = {mt
M }, the registers that contain R values Rt = {rt
R} and the
instruction register that contain a single value IRt. We also deﬁne a set of instructions
that can be executed, whose main role is to perform computations using the registers. For
example, add the values contained in two registers. We also deﬁne as a side eﬀect any action
that involves elements other than the input and output values of the instruction. Interaction
with the memory is an example of such side eﬀect. All the instructions, their computations
and side eﬀects are detailed in Figure 1b.
As can be seen in Figure 1a the execution model takes as input an initial memory tape M0
and outputs a ﬁnal memory tape MT after T steps. At each step t, the Controller uses
the instruction register IRt to compute the command for the Machine. The command is
a 4-tuple e, a, b, o. The ﬁrst element e is the instruction that should be executed by the
Machine, enumerated as an integer. The elements a and b specify which registers should be
used as arguments for the given instruction. The last element o speciﬁes in which register
the output of the instruction should be written. For example, the command {ADD, 2, 3, 1}
means that only the value of the ﬁrst register should change, following rt+1
2, rt
3).
Then the Machine will execute this command, updating the values of the memory, the
registers and the instruction register. The Machine always performs two other operations
apart from the required instruction. It outputs a stop ﬂag that allows the model to decide
when to stop the execution. It also increments the instruction register IRt by one at each
iteration.

1 = ADD(rt

3.2 Diﬀerentiability

The model presented above is a simple execution machine but it is not diﬀerentiable. In
order to be able to train this model end-to-end from a loss deﬁned over the ﬁnal memory
tape, we need to make every intermediate operation diﬀerentiable.

3

To achieve this, we replace every discrete value in our model by a multinomial distribution
over all the possible values that could have been taken. Moreover, each hard choice that
would have been non-diﬀerentiable is replaced by a continuous soft choice. We will henceforth
use bold letters to indicate the probabilistic version of a value.
First, the memory tape Mt is replaced by an M × M matrix Mt, where Mt
i,j corresponds
i taking the value j. The same change is applied to the registers Rt,
to the probability of mt
replacing them with an R × M matrix Rt, where Rt
i taking
the value j. Finally, the instruction register is also transformed from a single value IRt to a
vector of size M noted IRt, where the i-th element represents its probability to take the
value i.

i,j represents the probability of rt

The Machine does not contain any learnable parameter and will just execute a given command.
To make it diﬀerentiable, the Machine now takes as input four probability distributions et,
at, bt and ot, where et is a distribution over instructions, and at, bt and ot are distributions
t as convex combinations
over the registers. We compute the argument values arg1
of the diﬀerent registers:

t and arg2

arg1

t =

irt
at
i

arg2

t =

irt
bt
i,

(1)

R
X

i=1

i and bt

where at
compute the output value of each instruction k using the following formula:
X

i are the i-th values of the vectors at and bt. Using these values, we can

arg1

t
i · arg2

j · 1[gk(i, j) = c mod M ],
t

(2)

∀0 ≤ c ≤ M outt

k,c =

R
X

i=1

0≤i,j≤M

where gk is the function associated to the k-th instruction as presented in Table 1b. Since the
executed instruction is controlled by the probability e, the output written to the register will
also be a convex combination: outt = PN
k=1 et
k, where N is the number of instructions.
This value is then stored into the registers by performing a soft-write parametrised by ot.

koutt

A special case is associated with the stop signal. When executing the model, we keep track of
the probability that the program should have terminated before this iteration based on the
probability associated at each iteration with the speciﬁc instruction that controls this ﬂag.
Once this probability goes over a threshold ηstop ∈ (0, 1], the execution is halted. We applied
the same techniques to make the side-eﬀects diﬀerentiable, this is presented in appendix A.1.

et = We ∗ IRt,

at = Wa ∗ IRt,

The Controller is the only learnable part of our model. The ﬁrst learnable part is the initial
values for the registers R0 and for the instruction register IR0. The second learnable part
is the parameters of the Controller which computes the required distributions using:
ot = Wo ∗ IRt

(3)
where We is an N × M matrix and Wa, Wb and Wo are R × M matrices. A representation
of these matrices can be found in Figure 4c. The Controller as deﬁned above is composed of
four independent, fully-connected layers. In Section 4.3 we will see that this complexity is
suﬃcient for our model to be able to represent any program.
Henceforth, we will denote by θ = {R0, IR0, We, Wa, Wb, Wo} the set of all the learnable
parameters of this model.

bt = Wb ∗ IRt,

4 Adaptative Neural Compiler

We will now present the Adaptive Neural Compiler.
Its goal is to ﬁnd the best set of
weights θ∗ for a given dataset such that our model will perform the correct input/output
mapping as eﬃciently as it can. We begin by describing our learning objective in details.
The two subsequent sections will focus on making the optimisation of our learning objective
computationally feasible.

4.1 Objective function

Our goal is to solve a given algorithmic problem eﬃciently. The algorithmic problem is
deﬁned as a set of input/output pairs. We also have access to a generic program that is able

4

to perform the required mapping. In our example of accessing elements in a linked list, the
transformation would consist in writing down the desired value at the speciﬁed position in
the tape. The program given to us would iteratively go through the elements of the linked
list, ﬁnd the desired value and write it down at the desired position. If there exists some
bias that would allow this traversal to be faster, we expect the program to exploit it.

Our approach to this problem is to construct a diﬀerentiable objective function, mapping
controller parameters to a loss. We deﬁne this loss based on the states of the memory tape
and outputs of the Controller at each step of the execution. The precise mathematical
formulation for each term of the loss is given in appendix B. Here we present the motivation
behind each of them.

Correctness For a given input, we have the expected output. We compare the values of
the expected output with the ﬁnal memory tape provided by the execution.

Halting To prevent programs to take an inﬁnite amount of time without stopping, we
deﬁned a maximum number of iterations Tmax after which the execution is halted. Moreover,
we add a penalty in the loss if the Controller didn’t halt before this limit.

Eﬃciency We penalise each iteration taken by the program where it does not stop.

Conﬁdence We add a term which will penalise probability of stopping if the current state
of the memory is not the expected one.

If only the correctness term was considered, nothing would encourage the learnt algorithm to
halt as soon as it ﬁnished. If only correctness and halting were considered, then the program
may not halt as early as possible. Conﬁdence enables the algorithm to evaluate better when
to stop.

The loss is a weighted sum of the four above-mentioned terms. We denote the loss of the i-th
training sample, given parameters θ, as Li(θ). Our learning objective is then speciﬁed as:

min
θ

X

i

Li(θ)

s.t. θ ∈ Θ,

(4)

where Θ is a set over the parameters such that the outputs of the Controller, the initial
values of each register and of the instruction register are all probability distributions.

The above optimisation is a highly non-convex problem. To be able to solve it using standard
gradient descent based methods, we will ﬁrst need to transform it to an unconstrained
problem. We also know that the result of the optimisation of a non-convex objective function
is strongly dependent on the initialisation point. In the rest of this section, we will ﬁrst
present a small modiﬁcation to the model that will remove the constraints. We will then
present our Neural Compiler that will provide a good initialisation to solve this problem.

4.2 Reformulation

In order to use gradient descent methods without having to project the parameters on Θ,
we alter the formulation of the controller. We add a softmax layer after each linear layer
ensuring that the constraints on the Controller’s output will be respected. We also apply a
softmax to the initial values of the registers and the instructions register, ensuring they will
also respect the original constraints. This way, we transform the constrained-optimisation
problem into an unconstrained one, allowing us to use standard gradient descent methods.
As discussed in other works [10], this kind of model is hard to train and requires a high
number of random restarts before converging to a good solution. We will now present a
Neural Compiler that will provide good initialisations to help with this problem.

4.3 Neural Compiler

The goal for the Neural Compiler is to convert an algorithm, written as an unambiguous
program, to a set of parameters. These parameters, when put into the controller, will
reproduce the exact steps of the algorithm. This is very similar to the problem framed by
Reed and de Freitas [12], but we show here a way to accomplish it without any learning.

5

var head = 0;
var nb_jump = 1;
var out_write = 2;

nb_jump = READ(nb_jump);
out_write = READ(out_write);

loop : head = READ(head);

nb_jump = DEC(nb_jump);
JEZ(nb_jump, end);
JEZ(0, loop);
end : head = INC(head);

head = READ(head);
WRITE(out_write, head);
STOP();

Initial Registers:

R1 = 6; R2 = 2; R3 = 0;
R4 = 2; R5 = 1; R6 = 0;
R7 = 0;

Program:

0 : R5 = READ (R5, R7)
1 : R4 = READ (R4, R7)
2 : R6 = READ (R6, R7)
3 : R5 = DEC (R5, R7)
(R5, R1)
4 : R7 = JEZ
(R3, R2)
5 : R3 = JEZ
6 : R6 = INC
(R6, R7)
7 : R6 = READ (R6, R7)
8 : R7 = WRITE(R4, R6)
9 : R7 = STOP (R7, R7)

(i) Instr.

(ii) Arg1

(iii) Arg2

(iv) Out

(a) Input program

(a) Intermediary representation

(c) Weights

Figure 4: Example of the compilation process. (2a) Program written to perform the ListK task.
Given a pointer to the head of a linked list, an integer k, a target cell and a linked list, write in
the target cell the k-th element of the list. (3a) Intermediary representation of the program. This
corresponds to the instruction that a Random Access Machine would need to perform to execute the
program. (4c) Representation of the weights that encodes the intermediary representation. Each
row of the matrix correspond to one state/line. Initial value of the registers are also parameters of
the model, omitted here.

The diﬀerent steps of the compilation are illustrated in Figure 4. The ﬁrst step is to go from
the written version of the program to the equivalent list of low level instruction. This step can
be seen as going from Figure 2a to Figure 3a. The illustrative example uses a fairly low-level
language but traditional features of programming languages such as loops or if-statements
can be supported using the JEZ instruction. The use of constants as arguments or as values
is handled by introducing new registers that hold these values. The value required to be
passed as target position to the JEZ instruction can be resolved at compile time.

Having obtained this intermediate representation, generating the parameters is straight-
forward. As can be seen in Figure 3a, each line contains one instruction, the two input
registers and the output register, and corresponds to a command that the Controller will
have to output. If we ensure that IR is a Dirac-delta distribution on a given value, then
the matrix-vector product is equivalent to selecting a row of the weight matrix. As IR is
incremented at each iteration, the Controller outputs the rows of the matrix in order. We
thus have a one-to-one mapping between the lines of the intermediate representation and
the rows of the weight matrix. An example of these matrices can be found in Figure 4c. The
weight matrix has 10 rows, corresponding to the number of lines of code of our intermediate
representation. On the ﬁrst line of the matrix corresponding to the ﬁrst argument (4cii), the
ﬁfth element has value 1, and is linked to the ﬁrst line of code where the ﬁrst argument to
the READ operation is the ﬁfth register.

The number of rows of the weight matrix is linear in the number of lines of code in the original
program. To output a command, we must be able to index its line with the instruction
register IR, which means that the largest representable number in our Machine needs to be
greater than the number of lines in our program.

Moreover, any program written in a regular assembly language can be rewritten to use only
our restricted set of instructions. This can be done ﬁrst because all the conditionals of the
the assembly language can be expressed as a combination of arithmetic and JEZ instructions.
Secondly because all the arithmetic operations can be represented as a combination of our
simple arithmetic operations, loops and ifs statements. This means that any program that
can run on a regular computer, can be ﬁrst rewritten to use our restricted set of instructions
and then compiled down to a set of weights for our model. Even though other models use
LSTM as controller, we showed here that a Controller composed of simple linear functions is
expressive enough. The advantage of this simpler model is that we can now easily interpret

6

the weights of our model in a way that would not have be possible if we had a recurrent
network as a controller.

The most straightforward way to leverage the results of the compilation is to initialise the
Controller with the weights obtained through compilation of the generic algorithm. To
account for the extra softmax layer, we need to multiply the weights produced by the compiler
by a large constant to output Dirac-delta distributions. Some results associated with this
technique can be found in Section 5.1. However, if we initialise with exactly this sharp set of
parameters, the training procedure is not able to move away from the initialisation as the
gradients associated with the softmax in this region are very small. Instead, we initialise
the controller with a non-ideal version of the generic algorithm. This means that the choice
with the highest probability in the output of the Controller is correct, but the probability of
other choices is not zero. As can be seen in Section 5.2, this allows the Controller to learn
by gradient descent a new algorithm, diﬀerent from the original one, that has a lower loss
than the ideal version of the compiled program.

5 Experiments

We performed two sets of experiments. The ﬁrst shows the capability of the Neural Compiler
to perfectly reproduce any given program. The second shows that our Neural Compiler
can adapt and improve the performance of programs. We present results of data-speciﬁc
optimisation being carried out and show decreases in runtime for all the algorithms and
additionally, for some algorithms, show that the runtime is a diﬀerent computational-
complexity class altogether. All the code required to reproduce these experiments is available
online 1.

5.1 Compilation

The compiler described in section 4.3 allows us to go from a program written using our
instruction set to a set of weights θ for our Controller.

To illustrate this point, we implemented simple programs that can solve the tasks introduced
by Kurach et al. [8] and a shortest path problem. One of these implementations can be
found in Figure 2a, while the others are available in appendix F. These programs are written
in a speciﬁc language, and are transformed by the Neural Compiler into parameters for the
model. As expected, the resulting models solve the original tasks exactly and can generalise
to any input sequence.

5.2 ANC experiments

In addition to being able to reproduce any given program as was done by Reed and de Freitas
[12], we have the possibility of optimising the resulting program further. We exhibit this
by compiling program down to our model and optimising their performance. The eﬃciency
gains for these tasks come either from ﬁnding simpler, equivalent algorithms or by exploiting
some bias in the data to either remove instructions or change the underlying algorithm.

We identify three diﬀerent levels of interpretability for our model: The ﬁrst type corresponds
to weights containing only Dirac-delta distributions, there is an exact one-to-one mapping
between lines in the weight matrices and lines of assembly code. In the second type where
all probabilities are Dirac-delta except the ones associated with the execution of the JEZ
instruction, we can recover an exact algorithm that will use if statements to enumerate the
diﬀerent cases arising from this conditional jump. In the third type where any operation
other than JEZ is executed in a soft way or use a soft argument, it is not possible to recover
a program that will be as eﬃcient as the learned one.

We present here brieﬂy the considered tasks and biases, and report the reader to appendix F
for a detailed encoding of the input/output tape.

1https://github.com/albanD/adaptive-neural-compilation

7

Table 1: Average numbers of iterations required to solve instances of the problems for the original
program, the best learned program and the ideal algorithm for the biased dataset. We also include
the success rate of reaching a more eﬃcient algorithm across multiple random restarts.

Access

Increment Swap ListK Addition Sort

Generic
Learned
Ideal

6
4
4

40
16
34

10
6
6

18
11
10

20
9
6

38
18
9.5

Success Rate

37 %

84%

27%

19%

12%

74%

1. Access: Given a value k and an array A, return A[k]. In the biased version, the
value of k is always be the same, so the address of the required element can be stored
in a constant. This is similar to the optimisation known as constant folding.

2. Swap: Given an array A and two pointers p and q, swap the elements A[p] and A[q].
In the biased version, p and q are always the same so reading them can be avoided.
3. Increment: Given an array, increment all its element by 1. In the biased version,
the array is of ﬁxed size and the elements of the array have the same value so you
don’t need to read all of them when going through the array.

4. Listk: Given a pointer to the head of a linked list, a number k and a linked list,
ﬁnd the value of the k-th element. In the biased version, the linked list is organised
in order in memory, as would be an array, so the address of the k-th value can be
computed in constant time. This is the example developed in Figure 4.

5. Addition: Two values are written on the tape and should be summed. No data bias
is introduced but the starting algorithm is non-eﬃcient: it performs the addition as
a series of increment operation. The more eﬃcient operation would be to add the
two numbers.

6. Sort: Given an array A, sort it. In the biased version, only the start of the array
might be unsorted. Once the start has been arranged, the end of the array can be
safely ignored.

For each of these tasks, we perform a grid search on the loss parameters and on our hyper-
parameters. Training is performed using Adam [7]. We choose the best set of hyperparameters
and run the optimisation with 100 diﬀerent random seeds. We consider that a program has
been successfully optimised when two conditions are fulﬁlled. First, it needs to output the
correct solution for all test cases presenting the same bias. Second, the average number of
iterations taken to solve a problem must be lower than the algorithm used for initialisation.
Note that if we cared only about the ﬁrst criterion, the methods presented in Section 5.1
would already provide a success rate of 100%, without requiring any training.

The results are presented in Table 1. For each of these tasks, we manage to ﬁnd faster
algorithms. In the simple cases of Access and Swap, the optimal algorithm for the presented
datasets are obtained. Exploiting the bias of the data, successful heuristics are incorporated
in the algorithm and appropriate constants get stored in the initial value of registers. The
learned programs for these tasks are always in the ﬁrst case of interpretability, this means
that we can recover the most eﬃcient algorithm from the learned weights.

While ListK and Addition have lower success rates, the improvements between the original
and learned algorithms are still signiﬁcant. Both were initialised with iterative algorithms
with O(n) complexities. They managed to ﬁnd constant time O(1) algorithms to solve the
given problems, making the runtime independent of the input. Achieving this means that
the equivalence between the two approaches has been identiﬁed, similar to how optimising
compilers operate. Moreover, on the ListK task, some learned programs corresponds to
the second type of interpretability. Indeed these programs use soft jumps to condition the
execution on the value of k. Even though these program would not generalise to other values
of k, some learned programs for this task achieve a type one interpretability and a study of
the learned algorithm reveal that they can generalise to any value of k.

8

Finally, the Increment task achieves an unexpected result. Indeed, it is able to outperform
our best possible algorithm. By looking at the learned program, we can see that it is actually
leveraging the possibility to perform soft writes over multiple elements of the memory at
the same time to reduce its runtime. This is the only case where we see a learned program
associated with the third type of interpretability. While our ideal algorithm would give a
conﬁdence of 1 on the output, this algorithm is unable to do so, but it has a high enough
conﬁdence of 0.9 to be considered a correct algorithm.

In practice, for all but the most simple tasks, we observe that further optimisation is possible,
as some useless instructions remain present. Some transformations of the controller are indeed
diﬃcult to achieve through the local changes operated by the gradient descent algorithm.
An analysis of these failure modes of our algorithm can be found in appendix G.4. This
motivates us to envision the use of approaches other than gradient descent to address these
issues.

6 Discussion

The work presented here is a ﬁrst step towards adaptive learning of programs. It opens
up several interesting directions of future research. For exemple, the deﬁnition of eﬃciency
that we considered in this paper is ﬂexible. We chose to only look at the average number
of operations executed to generate the output from the input. We leave the study of other
potential measures such as Kolmogorov Complexity and sloc, to name a few, for future
works.

As shown in the experiment section, our current method is very good at ﬁnding eﬃcient
solutions for simple programs. For more complex programs, only a solution close to the
initialisation can be found. Even though training heuristics could help with the tasks
considered here, they would likely not scale up to real applications. Indeed, the main problem
we identiﬁed is that the gradient-descent based optimisation is unable to explore the space
of programs eﬀectively, by performing only local transformations. In future work, we want
to explore diﬀerent optimisation methods. One approach would be to mix global and local
exploration to improve the quality of the solutions. A more ambitious plan would be to
leverage the structure of the problem and use techniques from combinatorial optimisation to
try and solve the original discrete problem.

References

[1] Marcin Andrychowicz and Karol Kurach. Learning eﬃcient algorithms with hierarchical

attentive memory. CoRR, 2016.

[2] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, 2014.

[3] Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom.

Learning to transduce with unbounded memory. In NIPS, 2015.

[4] Frédéric Gruau, Jean-Yves Ratajszczak, and Gilles Wiber. A neural compiler. Theoretical

Computer Science, 1995.

recurrent nets. In NIPS, 2015.

[5] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented

[6] Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. In ICLR, 2016.

[7] Diederik Kingma and Jimmy Adam. A method for stochastic optimization. In ICLR,

2015.

chines. In ICLR, 2016.

[8] Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access ma-

[9] Henry Massalin. Superoptimizer: a look at the smallest program. In ACM SIGPLAN

Notices, volume 22, pages 122–126. IEEE Computer Society Press, 1987.

9

[10] Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol
Kurach, and James Martens. Adding gradient noise improves learning for very deep
networks. In ICLR, 2016.

[11] João Pedro Neto, Hava Siegelmann, and Félix Costa. Symbolic processing in neural

networks. Journal of the Brazilian Computer Society, 2003.

[12] Scott Reed and Nando de Freitas. Neural programmer-interpreters. In ICLR, 2016.

[13] Eric Schkufza, Rahul Sharma, and Alex Aiken. Stochastic superoptimization. In ACM

SIGARCH Computer Architecture News, 2013.

[14] Rahul Sharma, Eric Schkufza, Berkeley Churchill, and Alex Aiken. Conditionally correct

superoptimization. In OOPSLA, 2015.

[15] Hava Siegelmann. Neural programming language. In AAAI, 1994.

[16] Ronald Williams. Simple statistical gradient-following algorithms for connectionist

reinforcement learning. Machine learning, 1992.

[17] Wojciech Zaremba and Ilya Sutskever. Reinforcement learning neural turing machines.

arXiv preprint arXiv:1505.00521, 2015.

[18] Wojciech Zaremba, Tomas Mikolov, Armand Joulin, and Rob Fergus. Learning simple

algorithms from examples. CoRR, 2015.

7 Appendix

A Detailed Model Description

In this section, we are going to precisely deﬁne the non diﬀerentiable model used above. This
model can be seen as a recurrent network. Indeed, it takes as input an initial memory tape,
performs a certain number of iterations and outputs a ﬁnal memory tape. The memory tape
is an array of M cells, where a cell is an element holding a single integer value. The internal
state of this recurrent model are the memory, the registers and the instruction register.
The registers are another set of R cells that are internal to the model. The instruction
register is a single cell used in a speciﬁc way described later. These internal states are noted
R} and IRt for the memory, the registers and
Mt = {mt
M }, Rt = {rt
2, . . . , mt
the instruction register respectively.

2, . . . , rt

1, mt

1, rt

Figure 1 describes in more detail how the diﬀerent elements interact with each other. At
each iteration, the Controller takes as input the value of the instruction register IRt and
outputs four values:

et, at, bt, ot = Controller(IRt).
(5)
The ﬁrst value et is used to select one of the instruction of the Machine to execute at this
iteration. The second and third values at and bt will identify which registers to use as the
ﬁrst and second argument for the selected instruction. The fourth value ot identity the
output register where to write the result of the executed instruction. The Machine then
takes as input these four values and the internal state and computes the updated value of
the internal state and a stop ﬂag:

Mt+1, Rt+1, IRt+1, stop = Machine(Mt, Rt, IRt, et, at, bt, ot).

(6)

The stop ﬂag is a binary ﬂag. When its value is 1, it means that the model will stop the
execution and the current memory state will be returned.

10

The Machine The machine is a deterministic function that increments the instruction
register and executes the command given by the Controller to update the current internal
state. The set of instructions that can be executed by the Machine can be found in Table 1b.
Each instruction takes two values as arguments and returns a value. Additionally, some of
these instructions have side eﬀects. This mean that they do not just output a value, they
perform another task. This other task can be for example to modify the content of the
memory. All the considered side eﬀects can be found in Table 1b. By convention, instructions
that don’t have a value to return and that are used only for their side-eﬀect will return a
value of 0.

The Controller The Controller is a function that takes as input a single value and outputs
four diﬀerent values. The Controller’s internal parameters, the initial values for the registers
and the initial value of the instruction register deﬁne uniquely a given Controller.

The usual choice in the literature is to use an LSTM network[2, 3, 8] as controller. Our
choice was to instead use a simpler model. Indeed, our Controller associates a command
to each possible value of the instruction register. Since the instruction register’s value will
increase by one at each iteration, this will enforce the Controller to encode in its weights
what to do at each iteration. If we were using a recurrent controller the same instruction
register could potentially be associated to diﬀerent sets of outputs and we would lose this
one to one mapping.

To make this clearer, we ﬁrst rewrite the instruction register as an indicator vector with a 1
at the position of its value:

Ii =

(cid:26)1
0

if i = IRt
otherwise

.

(7)

In this case, we can write a single output at of the Controller as the result of a linear function
of I:

at = Wa ∗ I ,
(8)
where Wa is the 1xM matrix containing the value that need to be chosen as ﬁrst arguments for
each possible value of the instruction register and ∗ represent a matrix vector multiplication.

A.1 Mathematical details of the diﬀerentiable model

In order to make the model diﬀerentiable, every value and every choice are replaced by
probability distributions over the possible choices. Using convex combinations of probability,
the execution of the Machine is made diﬀerentiable. We present here the mathematical
formulation of this procedure for the case of the side-eﬀects.

STOP In the discrete model, the execution is halted when the STOP instruction is
executed. However, in the diﬀerentiable model, the STOP instruction may be executed
with a probability smaller than 1. To take this into account, when executing the model, we
keep track of the probability that the program should have terminated before this iteration
based on the probability associated to the STOP instruction at each iteration. Once this
probability goes over a threshold ηstop ∈]0, 1], the execution is halted.

READ The mechanism is entirely the same as the one used to compute the arguments
based on the registers and a probability distribution over the registers.

JEZ We note IRt+1
or not the JEZ instruction. We also have et
iteration t. The new value of the instruction register is:

njez the new value of IRt if we had respectively executed
jez the probability of executing this instruction at

jez and IRt+1

IRt+1 = IRt+1

njez · (1 − et

jez) + IRt+1

jez · et

jez

IRt+1
jez is himself computed based on several probability distribution. If we consider that
the instruction JEZ is executed with probabilistic arguments cond and label, its value is
given by

IRt+1

jez = label · cond0 + INC(IRt) · (1 − cond0)

(9)

(10)

11

With a probability equals to the one that the ﬁrst argument is equal to zero, the new value
of IRt is label. With the complement, it is equal to the incremented version of its current
value, as the machine automatically increments the instruction register.

WRITE The mechanism is fairly similar to the one of the JEZ instruction.
We note Mt+1
W RIT E and Mt+1
not the WRITE instruction. We also have et
at iteration t. The new value of the memory matrix register is:

nW RIT E the new value of Mt if we had respectively executed or
write the probability of executing this instruction

Mt+1 = Mt+1

nW RIT E · (1 − et

write) + Mt+1

W RIT E · et

W RIT E

As with the JEZ instruction, the value of Mt+1
W RIT E is dependent on the two probability
distribution given as input: addr and val. The probability that the i-th cell of the memory
tape contains the value j after the update is:

M t+1

i,j = addri · valj + (1 − addri) · M t
i,j

Note that this can done using linear algebra operations so as to update everything in one
global operation.

Mt+1 = (cid:0)((1 − addr)1T ) ⊗ Mt(cid:1) + (addr valT )

(11)

(12)

(13)

B Speciﬁcation of the loss

This loss contains four terms that will balance the correctness of the learnt algorithm, proper
usage of the stop signal and speed of the algorithms. The parameters deﬁning the models are
the weight of the Controller’s function and the initial value of the registers. When running
the model with the parameters θ, we consider that the execution ran for T time steps. We
consider the memory to have a size M and that each number can be an integer between
0 and M − 1. Mt was the state of the memory at the t-th step. T and C are the target
memory and the 0-1 mask of the elements we want to consider. All these elements are
matrices where for example Mt
i,j is the probability of the i-th entry of the memory to take
the value j at the step t. We also note pstop,t the probability outputted by the Machine that
it should have stopped before iteration t.

Correctness The ﬁrst term corresponds to the correctness of the given algorithm. For a
given input, we have the expected output and a mask. The mask allows us to know which
elements in the memory we should consider when comparing the solutions. For the given
input, we will compare the values speciﬁed by the mask of the expected output with the
ﬁnal memory tape provided by the execution. We compare them with the L2 distance in the
probability space. Using the notations from above, we can write this term as:
X

Lc(θ) =

Ci,j(MT

i,j(θ) − Ti,j)2.

(14)

i,j

If we optimised only this ﬁrst term, nothing would encourage the learnt algorithm to use the
STOP instruction and halt as soon as it ﬁnished.

Halting To prevent programs to take an inﬁnite amount of time without stopping, we
deﬁned a maximum number of iterations Tmax after which the execution is halted. During
training, we also add a penalty if the Controller didn’t halt before this limit:

LsTmax(θ) = (1 − pstop−T (θ)) · [T == Tmax]

Eﬃciency If we consider only the above mentioned losses, the program will make sure to
halt by itself but won’t do it as early as possible. We incentivise this behaviour by penalising
each iteration taken by the program where it does not stop:

(15)

(16)

Lt(θ) =

(1 − pstop,t(θ)).

X

t∈[1,T −1]

12

Conﬁdence Moreover, we want the algorithm to have a good conﬁdence to stop when
it has found the correct output. To do so, we add the following term which will penalise
probability of stopping if the current state of the memory is not the expected one:

Lst(θ) =

X

X

t∈[2,T ]

i,j

(pstop,t(θ) − pstop,t−1(θ))Ci,j(Mt

i,j(θ) − Ti,j)2.

(17)

The increase in probability (pstop,t − pstop,t−1) corresponds to the probability of stopping
exactly at iteration t. So, this is equivalent to the expected error made.

Total loss The complete loss that we use is then the following:

L(θ) = αLc(θ) + βLsTmax (θ) + γLst(θ) + δLt(θ).

(18)

C Distributed representation of the program

For the most of out experiments, the learned weights are fully interpretable as they ﬁt in the
ﬁrst type of interpretability. However, in some speciﬁc cases, under the pressure of our loss
encouraging a smaller number of iterations, an interesting behavior emerges.

It is interesting to note that the decompiled version is not straightforward to
Remarks
interpret. Indeed when we reach a program that has non Dirac-delta distributions in its
weights, we cannot perform the inverse of the one-to-one mapping performed by the compiler.
In fact, it relies on this blurriness to be able to execute the program with a smaller number
of instruction. Notably, by having some blurriness on the JEZ instruction, the program can
hide additional instructions, by creating a distributed state. We now explain the mechanism
used to achieve this.

Creating a distributed state Consider the following program and assume that the initial
value of IR is 0:

Initial Registers:
R1 = 0; R2 = 1; R3 = 4, R4 = 0

Program:
0 : R1 = READ (R1, R4)
1 : R4 = JEZ (R1, R3)
2 : R4 = WRITE(R1, R1)
3 : R4 = WRITE(R1, R3)

If you take this program and execute it for three iterations, it will: read the ﬁrst value of
the tape into R1. Then, if this value is zero, it will jump to State 4, otherwise it will just
increment IR. This means that depending on the value that was in R1, the next instruction
that will be executed will be diﬀerent (in this case, the diﬀerence between State 3 and State
4 is which registers they will be writing from). This is our standard way of implementing
conditionals.

Imagine that, after learning, the second instruction in our example program has 0.5 probability
of being a JEZ and 0.5 probability of being a ZERO. If the content of R1 is a zero, according
to the JEZ, we should jump to State 4, but this instruction is executed with a probability
of 0.5. We also have 0.5 probability of executing the ZERO instruction, which would lead to
State 3.

Therefore, IR is not a Dirac-delta distribution anymore but points to State 3 with probability
0.5 and State 4 with probability 0.5.

Exploiting a distributed state To illustrate, we will discuss how the Controller computes
a for a model with 3 registers. The Table 2 show an example of some weights for such a
controller.

If we are in State 1, the output of the controller is going to be

out = softmax([20, 5, −20]) = [0.9999..., 3e−7, 4e−18]

(19)

13

State 1
State 2

R1 R2 R3
-20
5
20
20
5
-20

Table 2: Controller Weights

If we are in State 2, the output of the controller is going to be

out = softmax([−20, 5, 20]) = [4e−18, 3e−7, 0.9999...]

(20)

In both cases, the output of the controller is therefore going to be almost discrete. In State
1, R1 would be chosen and in State 2, R3 would be chosen.

However, in the case where we have a distributed state with probability 0.5 over State 1 and
0.5 over State 2, the output would be:

out = softmax(0.5 ∗ [−20, 5, 20] + 0.5[20, 5, −20])

= softmax([0, 10, 0])
= [4e−5, 0.999, 4e−5].

(21)

Note that the result of the distributed state is actually diﬀerent from the result of the discrete
states. Moreover it is still a discrete choice of the second register.

Because this program contains distributed elements, it is not possible to perform the one-to-
one mapping between the weights and the lines of code. Though every instruction executed
by the program, except for the JEZ, are binary. This means that this model can be translated
to a regular program that will take exactly the same runtime, but will require more lines of
codes than the number of lines in the matrix.

D Alternative Learning Strategies

A critique that can be made to this method is that we will still initialise close to a local
minimum. Another approach might be to start from a random initialisation but adding a
penalty on the value of the weights such that they are encourage to be close to the generic
algorithm. This can be seen as L2 regularisation but instead of pushing the weights to 0, we
push then with the value corresponding to the generic algorithm. If we start with a very
high value of this penalty but use an annealing schedule where its importance is very quickly
reduced, this is going to be equivalent to the previous method.

E Possible Extension

E.1 Making objective function diﬀerentiable

These experiments showed that we can transform any program that perform a mapping
between an input memory tape to an output memory tape to a set of parameters and execute
it using our model. The ﬁrst point we want to make here is that this means that we take
any program and transform it into a diﬀerentiable function easily. For example, if we want
to learn a model that given a graph and two nodes a and b, will output the list of nodes to
go through to go from a to b in the shortest amount of time. We can easily deﬁne the loss of
the length of the path outputted by the model. Unfortunately, the function that computes
this length from the set of nodes is not diﬀerentiable. Here we could implement this function
in our model and use it between the prediction of the model and the loss function to get an
end to end trainable system.

E.2 Beyond mimicking and towards open problems

It would even be possible to generalise our learning procedure to more complex problems
for which we don’t have a ground truth output. For example, we could consider problems
where the exact answer for a given input is not computable or not unique. If the goodness
of a solution can be computed easily, this value could be used as training objective. Any
program giving a solution could be used as initialisation and our framework would improve
it, making it generate better solutions.

14

This section will present the programs that we use as initialisation for the experiment section.

F Example tasks

F.1 Access

In this task, the ﬁrst element in the memory is a value k. Starting from the second element,
the memory contains a zero-terminated list. The goal is to access the k-th element in the
list that is zero-indexed. The program associated with this task can be found in Listing 1.

1

2

3

4

5

6

(cid:7)
var k = 0
k = READ (0)
k = INC ( k )
k = READ ( k )
WRITE (0 , k )
STOP ()
(cid:6)
Listing 1: Access Task

(cid:4)

(cid:5)

Example input:
Output:

6
1

9
9

1
1

2
2

7
7

9
9

8
8

1
1

3
3

5
5

F.2 Copy

In this task, the ﬁrst element in the memory is a pointer p. Starting from the second element,
the memory contains a zero-terminated list. The goal is to copy this list at the given pointer.
The program associated with this task can be found in Listing 2.

1

2

3

4

5

6

7

8

9

10

11

12

13

(cid:7)
var read_addr = 0
var read_value = 0
var write_addr = 0

write_addr = READ (0)
l_loop : read_value = READ ( read_addr )
JEZ ( read_value , l_stop )
WRITE ( write_addr , read_value )
read_addr = INC ( read_addr )
write_addr = INC ( write_addr )
JEZ (0 , l_loop )

l_stop : STOP ()
(cid:6)

Listing 2: Copy Task

(cid:4)

(cid:5)

Example input:
Output:

9
9

11
11

3
3

1
1

5
5

14
14

0
0

0
0

0
0

0
11

0
3

0
1

0
5

0
14

0
0

F.3

Increment

In this task, the memory contains a zero-terminated list. The goal is to increment each value
in the list by 1. The program associated with this task can be found in Listing 3.

15

1

2

3

4

5

6

7

8

9

10

11

(cid:7)
var read_addr = 0
var read_value = 0

l_loop : read_value = READ ( read_addr )
JEZ ( read_value , l_stop )
read_value = INC ( read_value )
WRITE ( read_addr , read_value )
read_addr = INC ( read_addr )
JEZ (0 , l_loop )

l_stop : STOP ()
(cid:6)

Listing 3: Increment Task

(cid:4)

(cid:5)

Example input:
Output:

1
2

2
3

2
3

3
4

0
0

0
0

0
0

F.4 Reverse

In this task, the ﬁrst element in the memory is a pointer p. Starting from the second element,
the memory contains a zero-terminated list. The goal is to copy this list at the given pointer
in the reverse order. The program associated with this task can be found in Listing 4.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

(cid:7)
var read_addr = 0
var read_value = 0
var write_addr = 0

write_addr = READ ( write_addr )
l_count_phase : read_value = READ ( read_addr )
JEZ ( read_value , l_copy_phase )
read_addr = INC ( read_addr )
JEZ (0 , l_count_phase )

l_copy_phase : read_addr = DEC ( read_addr )
JEZ ( read_addr , l_stop )
read_value = READ ( read_addr )
WRITE ( write_addr , read_value )
write_addr = INC ( write_addr )
JEZ (0 , l_copy_phase )

l_stop : STOP ()
(cid:6)

Listing 4: Reverse Task

(cid:4)

(cid:5)

Example input:
Output:

5
5

7
7

2
2

13
13

14
14

0
14

0
13

0
2

0
7

0
0

0
0

0
0

0
0

0
0

0
0

F.5 Permutation

In this task, the memory contains two zero-terminated list one after the other. The ﬁrst
contains a set of indices. the second contains a set of values. The goal is to ﬁll the ﬁrst list
with the values in the second list at the given index. The program associated with this task
can be found in Listing 5.

16

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

(cid:7)
var read_addr = 0
var read_value = 0
var write_offset = 0

l_count_phase : read_value = READ ( write_offset )
write_offset = INC ( write_offset )
JEZ ( read_value , l_copy_phase )
JEZ (0 , l_count_phase )

l_copy_phase : read_value = DEC ( read_addr )
JEZ ( read_value , l_stop )
read_value = ADD ( write_offset , read_value )
read_value = READ ( read_value )
WRITE ( read_addr , read_value )
read_addr = INC ( read_addr )
JEZ (0 , l_copy_phase )
l_stop : STOP ()
(cid:6)

Listing 5: Permutation Task

(cid:4)

(cid:5)

Example input:
Output:

2
4

1
13

3
6

0
0

13
13

4
4

6
6

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

F.6 Swap

In this task, the ﬁrst two elements in the memory are pointers p and q. Starting from the
third element, the memory contains a zero-terminated list. The goal is to swap the elements
pointed by p and q in the list that is zero-indexed. The program associated with this task
can be found in Listing 6.

(cid:7)
var p = 0
var p_val = 0
var q = 0
var q_val = 0

p = READ (0)
q = READ (1)
p_val = READ ( p )
q_val = READ ( q )
WRITE (q , p_val )
WRITE (p , q_val )
STOP ()
(cid:6)

1

2

3

4

5

6

7

8

9

10

11

12

(cid:4)

(cid:5)

Listing 6: Swap Task

Example input:
Output:

1
1

3
3

7
7

6
5

7
7

5
6

2
2

0
0

0
0

0
0

F.7 ListSearch

In this task, the ﬁrst three elements in the memory are a pointer to the head of the linked
list, the value we are looking for v and a pointer to a place in memory where to store the
result. The rest of the memory contains the linked list. Each element in the linked list is two
values, the ﬁrst one is the pointer to the next element, the second is the value contained in
this element. By convention, the last element in the list points to the address 0. The goal is
to return the pointer to the ﬁrst element whose value is equal to v. The program associated
with this task can be found in Listing 7.

17

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

(cid:7)
var p_out = 0
var p_current = 0
var val_current = 0
var val_searched = 0

val_searched = READ (1)
p_out = READ (2)
l_loop : p_current = READ ( p_current )
val_current = INC ( p_current )
val_current = READ ( val_current )
val_current = SUB ( val_current , val_searched )
JEZ ( val_current , l_stop )
JEZ (0 , l_loop )
l_stop : WRITE ( p_out , p_current )
STOP ()
(cid:6)

Listing 7: ListSearch Task

(cid:4)

(cid:5)

Example input:
Output:

11
11

10
10

2
5

9
9

4
4

3
3

10
10

0
0

6
6

7
7

13
13

5
5

12
12

0
0

0
0

F.8 ListK

In this task, the ﬁrst three elements in the memory are a pointer to the head of the linked
list, the number of hops we want to do k in the list and a pointer to a place in memory
where to store the result. The rest of the memory contains the linked list. Each element in
the linked list is two values, the ﬁrst one is the pointer to the next element, the second is
the value contained in this element. By convention, the last element in the list points to
the address 0. The goal is to return the value of the k-th element of the linked list. The
program associated with this task can be found in Listing 8.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

(cid:7)
var p_out = 0
var p_current = 0
var val_current = 0
var k = 0

k = READ (1)
p_out = READ (2)
l_loop : p_current = READ ( p_current )
k = DEC ( k )
JEZ (k , l_stop )
JEZ (0 , l_loop )
l_stop : p_current = INC ( p_current )
p_current = READ ( p_current )
WRITE ( p_out , p_current )
STOP ()
(cid:6)

Listing 8: ListK Task

(cid:4)

(cid:5)

3
3

2
2

2
17

9
9

15
15

0
0

0
0

0
0

1
1

15
15

17
17

7
7

13
13

0
0

0
0

11
11

Example input:
Output:
0
0
10
0
0
10

0
0

F.9 Walk BST

In this task, the ﬁrst two elements in the memory are a pointer to the head of the BST and a
pointer to a place in memory where to store the result. Starting at the third element, there

18

is a zero-terminated list containing the instructions on how to traverse in the BST. The rest
of the memory contains the BST. Each element in the BST has three values, the ﬁrst one is
the value of this node, the second is the pointer to the left node and the third is the pointer
to the right element. By convention, the leafs points to the address 0. The goal is to return
the value of the node we get at after following the instructions. The instructions are 1 or
2 to go respectively to the left or the right. The program associated with this task can be
found in Listing 9.
(cid:7)
var p_out = 0
var p_current = 0
var p_instr = 0
var instr = 0

(cid:4)

1

3

4

2

5

6

7

8

9

10

11

12

13

14

15

16

17

18

p_current = READ (0)
p_out = READ (1)
instr = READ (2)

l_loop : JEZ ( instr , l_stop )
p_current = ADD ( p_current , instr )
p_current = READ ( p_current )
p_instr = INC ( p_instr )
JEZ (0 , l_loop )

l_stop : p_current = READ ( p_current )
WRITE ( p_out , p_current )
STOP ()
(cid:6)

(cid:5)

Listing 9: WalkBST Task

Example input:
Output:
24
0
8
24
0
8

0
0

0
0

12
12
0
0

1
10
0
0

0
0

1
1

2
2

0
0
10
10

0
0

0
0
0
0

15
15
0
0

0
0

0
0

0
0

0
0

9
9

23
23

0
0

0
0

11
11

15
15

6
6

F.10 Merge

In this task, the ﬁrst three elements in the memory are pointers to respectively, the ﬁrst list,
the second list and the output. The two lists are zero-terminated sorted lists. The goal is to
merge the two lists into a single sorted zero-terminated list that starts at the output pointer.
The program associated with this task can be found in Listing 10.

19

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

(cid:4)

(cid:5)

(cid:7)
var p_first_list = 0
var val_first_list = 0
var p_second_list = 0
var val_second_list = 0
var p_output_list = 0
var min = 0

p_first_list = READ (0)
p_second_list = READ (1)
p_output_list = READ (2)

l_loop : val_first_list = READ ( p_first_list )
val_second_list = READ ( p_second_list )
JEZ ( val_first_list , l_first_finished )
JEZ ( val_second_list , l_second_finished )
min = MIN ( val_first_list , val_second_list )
min = SUB ( val_first_list , min )
JEZ ( min , l_first_smaller )

WRITE ( p_output_list , val_first_list )
p_output_list = INC ( p_output_list )
p_first_list = INC ( p_first_list )
JEZ (0 , l_loop )

l_first_smaller : WRITE ( p_output_list , val_second_list )
p_output_list = INC ( p_output_list )
p_second_list = INC ( p_second_list )
JEZ (0 , l_loop )

l_first_finished : p_first_list = ADD ( p_second_list , 0)
val_first_list = ADD ( val_second_list , 0)

l_second_finished : WRITE ( p_output_list , val_first_list )
p_first_list = INC ( p_first_list )
p_output_list = INC ( p_output_list )
val_first_list = READ ( p_first_list )
JEZ ( val_first_list , l_stop )
JEZ (0 , l_second_finished )

l_stop : STOP ()
(cid:6)

Listing 10: Merge Task

Example input:
Output:
0
0
0
0
1
16

0
0

0
0

3
3
0
0

8
8
0
0

11
11
0
0

27
27
0
0

0
0

17
17
0
0

16
16
0
0

1
1
0
0

0
0
0
0

29
29
0
0

26
26

0
0

0
29

0
27

0
26

0
17

F.11 Dijkstra

In this task, we are provided with a graph represented in the input memory as follow. The
ﬁrst element is a pointer pout indicating where to write the results. The following elements
contain a zero-terminated array with one entry for each vertex in the graph. Each entry is a
pointer to a zero-terminated list that contains a pair of values for each outgoing edge of the
considered node. Each pair of value contains ﬁrst the index in the ﬁrst array of the child node
and the second value contains the cost of this edge. The goal is to write a zero-terminated
list at the address provided by pout that will contain the value of the shortest path from the
ﬁrst node in the list to this node. The program associated with this task can be found in
Listings 11 and 12.

20

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

(cid:7)
var min = 0
var argmin = 0

var p_out = 0
var p_out_temp = 0
var p_in = 1
var p_in_temp = 1

var nnodes = 0

var zero = 0
var big = 99

var tmp_node = 0
var tmp_weight = 0
var tmp_current = 0
var tmp = 0

var didsmth = 0

p_out = READ ( p_out )
p_out_temp = ADD ( p_out , zero )

tmp_current = INC ( zero )
l_loop_nnodes : tmp = READ ( p_in_temp )
JEZ ( tmp , l_found_nnodes )
WRITE ( p_out_temp , big )
p_out_temp = INC ( p_out_temp )
WRITE ( p_out_temp , tmp_current )
p_out_temp = INC ( p_out_temp )
p_in_temp = INC ( p_in_temp )
nnodes = INC ( nnodes )
JEZ ( zero , l_loop_nnodes )

l_found_nnodes : WRITE ( p_out , zero )
JEZ ( zero , l_find_min )
l_min_return : p_in_temp = ADD ( p_in , argmin )
p_in_temp = READ ( p_in_temp )

l_loop_sons : tmp_node = READ ( p_in_temp )
JEZ ( tmp_node , l_find_min )
tmp_node = DEC ( tmp_node )
p_in_temp = INC ( p_in_temp )
tmp_weight = READ ( p_in_temp )
p_in_temp = INC ( p_in_temp )

p_out_temp = ADD ( p_out , tmp_node )
p_out_temp = ADD ( p_out_temp , tmp_node )
tmp_current = READ ( p_out_temp )
tmp_weight = ADD ( min , tmp_weight )

tmp = MIN ( tmp_current , tmp_weight )
tmp = SUB ( tmp_current , tmp )
JEZ ( tmp , l_loop_sons )
WRITE ( p_out_temp , tmp_weight )
JEZ ( zero , l_loop_sons )
(cid:6)

Listing 11: Dijkstra Algorithm (Part 1)

21

(cid:4)

(cid:5)

57

58

59

60

61

62

63

64

65

66

67

68

69

70

71

72

73

74

75

76

77

78

79

80

81

82

83

84

85

86

87

88

(cid:7)
l_find_min : p_out_temp = DEC ( p_out )
tmp_node = DEC ( zero )
min = ADD ( big , zero )
argmin = DEC ( zero )

l_loop_min : p_out_temp = INC ( p_out_temp )
tmp_node = INC ( tmp_node )
tmp = SUB ( tmp_node , nnodes )
JEZ ( tmp , l_min_found )

tmp_weight = READ ( p_out_temp )

p_out_temp = INC ( p_out_temp )
tmp = READ ( p_out_temp )
JEZ ( tmp , l_loop_min )

tmp = MAX ( min , tmp_weight )
tmp = SUB ( tmp , tmp_weight )
JEZ ( tmp , l_loop_min )
min = ADD ( tmp_weight , zero )
argmin = ADD ( tmp_node , zero )
JEZ ( zero , l_loop_min )

l_min_found : tmp = SUB ( min , big )
JEZ ( tmp , l_stop )
p_out_temp = ADD ( p_out , argmin )
p_out_temp = ADD ( p_out_temp , argmin )
p_out_temp = INC ( p_out_temp )
WRITE ( p_out_temp , zero )
JEZ ( zero , l_min_return )

l_stop : STOP ()
(cid:6)

Listing 12: Dijkstra Algorithm (Part 2)

(cid:4)

(cid:5)

Example omitted for space reasons

G Learned optimisation: Case study

Here we present an analysis of the optimisation achieved by the ANC. We take the example
of the ListK task and study the diﬀerence between the learned program and the initialisation
used.

G.1 Representation

The representation chosen is under the form of the intermediary representation described in
Figure (2b). Based on the parameters of the Controller, we can recover the approximate
representation described in Figure (2b): 1For each possible "discrete state" of the instruction
register, we can compute the commands outputted by the controller. We report the most
probable value for each distribution, as well as the probability that the compiler would assign
to this value. If no value has a probability higher than 0.5, we only report a neutral token
(R-, -, NOP).

G.2 Biased ListK

Figure 5 represents the program that was used as initialisation to the optimisation problem.
This is the direct result from the compilation performed by the Neural Compiler of the
program described in Listing 8. A version with a probability of 1 for all necessary instructions
would have been easily obtained but not amenable to learning.

22

Figure 6 similarly describes the program that was obtained after learning.

As a remainder, the bias introduced in the ListK task is that the linked list is well organised
in memory. In the general case, the element could be in any order. An input memory tape to
the problem of asking for the third element in the linked list containing {4, 5, 6, 7} would be:

or

9

5

3

3

2

2

0

0

0

0

11

5

0

7

4

15

7

5

5

0

4

7

7

0

6

0

0

0

0

0

0

9

0

6

0

0

0

0

0

0

In the biased version of the task, all the elements are arranged in order and contiguously
positioned on the tape. The only valid representation of this problems is:

3

3

2

5

4

7

5

9

6

0

7

0

0

0

0

0

0

0

0

0

G.3 Solutions

Because of the additional structure of the problem, the bias in the data, a more eﬃcient
algorithm to ﬁnd the solution exists. Let us dive into the comparison of the two diﬀerent
solutions.

Both use their ﬁrst two states to read the parameters of the given instance of the task.
Which element of the list should be returned is read at line (0:) and where to write the
returned value is read at line (1:). Step (2:) to (6:) are dedicated to putting the address of
the k-th value of the linked list into the registers R1. Step (7:) to (9:) perform the same
task in both solution: reading the value at the address contained in R1, writing it at the
desired position and stopping.

The diﬀerence between the two programs lies in how they put the address of the k-th value
into R1.

Generic The initialisation program, used for initialisation, works in the most general case
so needs to perform a loop where it put the address of the next element in the linked list in
R1 (2:), decrement the number of jumps remaining to be made (3:), checking whether the
wanted element has been reached (4:) and going back to the start of the loop if not (5:).
Once the desired element is reached, R1 is incremented so as to point on the value of the
linked list element.

Speciﬁc On the other hand, in the biased version of the problem, the position of the
desired value can be analytically determined. The function parameters occupy the ﬁrst three
cells of the tape. After those, each element of the linked list will occupy two cells (one for
the pointer to the next address and one for the value). Therefore, the address of the desired
value is given by

R1 = 3 + (2 ∗ (k − 1) + 1) − 1 + 1

= 3 + 2 ∗ k − 1

(the -1 comes from the fact that the address are 0-indexed and the ﬁnal +1 from the fact
that we are interested in the position of the value and not of the pointer.)

The way this is computed is as follows:

- R1 = 3 + k by adding the constant 3 to the registers R2 containing K.
- R2 = k − 1
- R1 = 3 + 2 ∗ k − 1 by adding the now reduced value of R2.

The algorithm implemented by the learned version is therefore much more eﬃcient for the
biased dataset, due to its capability to ignore the loop.

(22)

(2:)
(3:)
(6:)

23

G.4 Failure analysis

An observation that can be made is that in the learned version of the program, Step (4:)
and (5:) are not contributing to the algorithms. They execute instructions that have no
side eﬀect and store the results into the registers R7 that is never used later in the execution.

The learned algorithm could easily be more eﬃcient by not performing these two operations.
However, such an optimisation, while perhaps trivial for a standard compiler, capable of
detecting unused values, is fairly hard for our optimisers to discover. Because we are only
doing gradient descent, the action of "moving some instructions earlier in the program" which
would be needed here to make the useless instructions disappear, is fairly hard, as it involves
modifying several rows of the program at once in a coherent manner.

R1 = 0 (0.88)
R2 = 1 (0.88)
R3 = 2 (0.88)
R4 = 6 (0.88)
R5 = 0 (0.88)
R6 = 2 (0.88)
R7 = - (0.05)

Initial State: 0 (0.88)

R2 (0.96)
0:
R3 (0.96)
1:
R1 (0.96)
2:
R2 (0.96)
3:
R7 (0.96)
4:
R7 (0.96)
5:
R1 (0.96)
6:
R1 (0.96)
7:
R7 (0.96)
8:
9:
R7 (0.96)
10: R- (0.16)
11: R- (0.16)
12: R- (0.17)
13: R- (0.16)
14: R- (0.17)
15: R- (0.16)
16: R- (0.17)
17: R- (0.18)
18: R- (0.17)
19: R- (0.15)

= READ (0.93)
= READ (0.93)
= READ (0.93)
(0.93)
= DEC
(0.93)
= JEZ
(0.93)
= JEZ
(0.93)
= INC
= READ (0.93)
= WRIT (0.93)
= STOP (0.93)
(0.11)
= NOP
(0.11)
= NOP
(0.1)
= NOP
(0.11)
= NOP
(0.11)
= NOP
(0.1)
= NOP
(0.11)
= NOP
(0.1)
= NOP
(0.1)
= NOP
(0.11)
= NOP

[ R2 (0.96)
[ R3 (0.96)
[ R1 (0.96)
[ R2 (0.96)
[ R2 (0.96)
[ R5 (0.96)
[ R1 (0.96)
[ R1 (0.96)
[ R3 (0.96)
[ R- (0.14)
[ R- (0.16)
[ R- (0.18)

[ R- (0.17)

[ R- (0.17)
[ R- (0.16)

, R- (0.14)
, R- (0.14)
, R- (0.14)
, R- (0.14)
, R4 (0.96)
, R6 (0.96)
, R- (0.14)
, R- (0.14)
, R1 (0.96)
, R- (0.14)
, R- (0.16)
, R- (0.16)

, R- (0.17)

, R- (0.16)
, R- (0.16)

[ R- (0.16)

, R- (0.17)

[ R- (0.16)

, R- (0.18)

[ R- (0.18)
[ R- (0.16)

, R- (0.17)
, R- (0.16)

[ R- (0.16)

, R- (0.17)

]
]
]
]
]
]
]
]
]
]
]
]

]

]
]

]

]

]

]
]

Figure 5: Initialisation used for the learning of the ListK task.

24

Initial State: 0 (0.99)

R1 = 3 (0.99)
R2 = 1 (0.99)
R3 = 2 (0.99)
R4 = 10 (0.53)
R5 = 0 (0.99)
R6 = 2 (0.99)
R7 = 7 (0.99)

R2 (0.99)
R6 (0.99)
R1 (1)
R2 (1)
R7 (0.99)
R7 (0.99)
R1 (1)
R1 (0.99)
R7 (0.99)
R7 (0.9)

0:
1:
2:
3:
4:
5:
6:
7:
8:
9:
10: R2 (0.99)
11: R1 (0.98)
12: R3 (0.98)
13: R3 (0.87)
14: R3 (0.89)
15: R3 (0.99)
16: R3 (0.99)
17: R2 (0.99)
18: R3 (0.99)
19: R3 (0.98)

= READ (0.99)
= READ (0.99)
(0.99)
(0.99)

= ADD
= DEC

= ADD

= MAX
= INC

(0.99)
(0.99)

(0.99)
= READ (0.99)
= WRIT (0.99)

= STOP (0.99)

[ R2 (1)
[ R3 (0.99)

, R1 (0.97)

]

, R6 (0.5)
]
]

, R2 (1)
, R1 (0.99)

]

[ R1 (0.99)
[ R2 (1)

[ R2 (0.99)
[ R6 (0.7)

, R1 (0.51)
, R1 (0.89)

]
]

[ R1 (0.99)

, R2 (1)

]

[ R1 (0.99)
[ R6 (1)
[ R6 (0.98)

, R1 (0.53)

]

, R1 (0.99)

]

, R1 (0.99)

]

= STOP (0.96)
(0.73)
= ADD
= ADD
(0.64)
= STOP (0.65)
= STOP (0.62)
= STOP (0.65)
(0.45)
= NOP
= INC
(0.56)
= STOP (0.65)
= STOP (0.98)

[ R1 (0.52)
[ R4 (0.99)
[ R6 (0.99)
[ R3 (0.52)
[ R6 (0.99)
[ R3 (0.99)
[ R6 (0.99)
[ R6 (0.7)
[ R3 (0.99)
[ R2 (0.62)

, R1 (0.99)
, R2 (0.99)
, R1 (0.99)
, R1 (0.99)
, R2 (0.62)
, R2 (0.71)
, R1 (0.99)

, R1 (0.98)

]

, R1 (0.99)
, R1 (0.79)

]
]
]
]
]
]
]

]
]

Figure 6: Learnt program for the listK task

25

6
1
0
2
 
y
a
M
 
6
2
 
 
]
I

A
.
s
c
[
 
 
2
v
9
6
9
7
0
.
5
0
6
1
:
v
i
X
r
a

Adaptive Neural Compilation

Rudy Bunel∗
University of Oxford
rudy@robots.ox.ac.uk

Alban Desmaison∗
University of Oxford
alban@robots.ox.ac.uk

Pushmeet Kohli
Microsoft Research
pkohli@microsoft.com

Philip H.S. Torr
University of Oxford
philip.torr@eng.ox.ac.uk

M. Pawan Kumar
University of Oxford
pawan@robots.ox.ac.uk

Abstract

This paper proposes an adaptive neural-compilation framework to address
the problem of eﬃcient program learning. Traditional code optimisation
strategies used in compilers are based on applying pre-speciﬁed set of
transformations that make the code faster to execute without changing
its semantics. In contrast, our work involves adapting programs to make
them more eﬃcient while considering correctness only on a target input
distribution. Our approach is inspired by the recent works on diﬀerentiable
representations of programs. We show that it is possible to compile programs
written in a low-level language to a diﬀerentiable representation. We also
show how programs in this representation can be optimised to make them
eﬃcient on a target distribution of inputs. Experimental results demonstrate
that our approach enables learning speciﬁcally-tuned algorithms for given
data distributions with a high success rate.

1

Introduction

Algorithm design often requires making simplifying assumptions about the input data.
Consider, for instance, the computational problem of accessing an element in a linked list.
Without the knowledge of the input data distribution, one can only specify an algorithm
that runs in a time linear in the number of elements of the list. However, suppose all the
linked lists that we encountered in practice were ordered in memory. Then it would be
advantageous to design an algorithm speciﬁcally for this task as it can lead to a constant
running time. Unfortunately, the input data distribution of a real world problem cannot be
easily speciﬁed as in the above simple example. The best that one can hope for is to obtain
samples drawn from the distribution. A natural question that arises from these observations:
“How can we adapt a generic algorithm for a computational task using samples from an
unknown input data distribution?”

The process of ﬁnding the most eﬃcient implementation of an algorithm has received
considerable attention in the theoretical computer science and code optimisation community.
Recently, Conditionally Correct Superoptimization [14] was proposed as a method for
leveraging samples of the input data distribution to go beyond semantically equivalent
optimisation and towards data-speciﬁc performance improvements. The underlying procedure
is based on a stochastic search over the space of all possible programs. Additionally, they
restrict their applications to reasonably small, loop-free programs, thereby limiting their
impact in practice.

In this work, we take inspiration from the recent wave of machine-learning frameworks for
estimating programs. Using recurrent models, Graves et al. [2] introduced a fully diﬀerentiable

∗The ﬁrst two authors contributed equally.

representation of a program, enabling the use of gradient-based methods to learn a program
from examples. Many other models have been published recently [3, 5, 6, 8] that build and
improve on the early work by Graves et al. [2]. Unfortunately, these models are usually
complex to train and need to rely on methods such as curriculum learning or gradient noise
to reach good solutions as shown by Neelakantan et al. [10]. Moreover, their interpretability
is limited. The learnt model is too complex for the underlying algorithm to be recovered
and transformed into a regular computer program.

The main focus of the machine-learning community has thus far been on learning programs
from scratch, with little emphasis on running time. However, for nearly all computational
problems, it is feasible to design generic algorithms for the worst-case. We argue that a
more pragmatic goal for the machine learning community is to design methods for adapting
existing programs for speciﬁc input data distributions. To this end, we propose the Adaptive
Neural Compiler (ANC). We design a compiler capable of mechanically converting algorithms
to a diﬀerentiable representation, thereby providing adequate initialisation to the diﬃcult
problem of optimal program learning. We then present a method to improve this compiled
program using data-driven optimisation, alleviating the need to perform a wide search over
the set of all possible programs. We show experimentally that this framework is capable of
adapting simple generic algorithms to perform better on given datasets.

2 Related Works

The idea of compiling programs to neural networks has previously been explored in the
literature. Siegelmann [15] described how to build a Neural Network that would perform
the same operations as a given program. A compiler has been designed by Gruau et al. [4]
targeting an extended version of Pascal. A complete implementation was achieved when
Neto et al. [11] wrote a compiler for NETDEF, a language based on the Occam programming
language. While these methods allow us to obtain an exact representation of a program as a
neural network, they do not lend themselves to optimisation to improve the original program.
Indeed, in their formulation, each elementary step of a program is expressed as a group of
neurons with a precise topology, set of weights and biases, thereby rendering learning via
gradient descent infeasible. Performing gradient descent in this parameter space would result
in invalid operations and thus is unlikely to lead to any improvement. The recent work by
Reed and de Freitas [12] on Neural Programmer-Interpreters (NPI) can also be seen as a way
to compile any program into a neural network by learning a model that mimic the program.
While more ﬂexible than the previous approaches, the NPI is unable to improve on a learned
program due to its dependency on a non-diﬀerentiable environment.

Another approach to this learning problem is the one taken by the code optimisation
community. By exploring the space of all possible programs, either exhaustively [9] or in a
stochastic manner [13], they search for programs having the same results but being more
eﬃcient. The work of Sharma et al. [14] broadens the space of acceptable improvements
to data-speciﬁc optimisations as opposed to the provably equivalent transformations that
were previously the only ones considered. However, this method is still reliant on non-
gradient-based methods for eﬃcient exploration of the space. By representing everything in
a diﬀerentiable manner, we aim to obtain gradients to guide the exploration.

Recently, Graves et al. [2] introduced a learnable representation of programs, called the
Neural Turing Machine (NTM). The NTM uses an LSTM as a Controller, which outputs
commands to be executed by a deterministic diﬀerentiable Machine. From examples of
input/output sequences, they manage to learn a Controller such that the model becomes
capable of performing simple algorithmic tasks. Extensions of this model have been proposed
in [3, 5] where the memory tape was replaced by diﬀerentiable versions of stacks or lists.
Kurach et al. [8] modiﬁed the NTM to introduce a notion of pointers making it more
amenable to represent traditional programs. Parallel works have been using Reinforcement
Learning techniques such as the REINFORCE algorithm [1, 16, 17] or Q-learning [18] to
be able to work with non diﬀerentiable versions of the above mentioned models. All these
models are trained only with a loss based on the diﬀerence between the output of the model
and the expected output. This weak supervision leads to a complex training. For instance

2

IR0

R0

Controller

Controller

Machine

Machine

IR1

R1

M1

stop

...

...

...

IR2

stop

R2

M2

M0

Memory

Memory

MT

Inst

arg1

arg2

output

side eﬀect

STOP
ZERO
INC
DEC
ADD
SUB
MIN
MAX
READ
WRITE

JEZ

-
-
a
a
a
a
a
a
a
a

a

-
-
-
-
b
b
b
b
-
b

b

0
0
a+1
a-1
a+b
a-b
min(a,b)
max(a,b)
mt
a
0

0

stop = 1
-
-
-
-
-
-
-
Memory access
mt
a = b
IRt = b
if a = 0

(a) General view of the whole Model.

(b) Machine instructions.

Figure 1: Model components.

the Neural RAM [8] requires a high number of random restarts before converging to a correct
solution [10], even when using the best hyperparameters obtained through a large grid search.

In our work, we will ﬁrst show that we can design a new neural compiler whose target will
be a Controller-Machine model. This makes the compiled model amenable to learning from
examples. Moreover, we can use it as initialisation for the learning procedure, allowing us to
aim for the more complex task of ﬁnding an eﬃcient algorithm.

3 Model

Our model is composed of two parts: (i) a Controller, in charge of specifying what should
be executed; and (ii) a Machine, following the commands of the Controller. We start by
describing the global architecture of the model. For the sake of simplicity, the general
description will present a non-diﬀerentiable version of the model. Section 3.2 will then
explain the modiﬁcations required to make this model completely diﬀerentiable. A more
detailed description of the model is provided in appendix A.

3.1 General Model

1, rt

1, mt

2, . . . , rt

2, . . . , mt

We ﬁrst deﬁne for each timestep t the memory tape that contains M integer values
Mt = {mt
M }, the registers that contain R values Rt = {rt
R} and the
instruction register that contain a single value IRt. We also deﬁne a set of instructions
that can be executed, whose main role is to perform computations using the registers. For
example, add the values contained in two registers. We also deﬁne as a side eﬀect any action
that involves elements other than the input and output values of the instruction. Interaction
with the memory is an example of such side eﬀect. All the instructions, their computations
and side eﬀects are detailed in Figure 1b.
As can be seen in Figure 1a the execution model takes as input an initial memory tape M0
and outputs a ﬁnal memory tape MT after T steps. At each step t, the Controller uses
the instruction register IRt to compute the command for the Machine. The command is
a 4-tuple e, a, b, o. The ﬁrst element e is the instruction that should be executed by the
Machine, enumerated as an integer. The elements a and b specify which registers should be
used as arguments for the given instruction. The last element o speciﬁes in which register
the output of the instruction should be written. For example, the command {ADD, 2, 3, 1}
means that only the value of the ﬁrst register should change, following rt+1
2, rt
3).
Then the Machine will execute this command, updating the values of the memory, the
registers and the instruction register. The Machine always performs two other operations
apart from the required instruction. It outputs a stop ﬂag that allows the model to decide
when to stop the execution. It also increments the instruction register IRt by one at each
iteration.

1 = ADD(rt

3.2 Diﬀerentiability

The model presented above is a simple execution machine but it is not diﬀerentiable. In
order to be able to train this model end-to-end from a loss deﬁned over the ﬁnal memory
tape, we need to make every intermediate operation diﬀerentiable.

3

To achieve this, we replace every discrete value in our model by a multinomial distribution
over all the possible values that could have been taken. Moreover, each hard choice that
would have been non-diﬀerentiable is replaced by a continuous soft choice. We will henceforth
use bold letters to indicate the probabilistic version of a value.
First, the memory tape Mt is replaced by an M × M matrix Mt, where Mt
i,j corresponds
i taking the value j. The same change is applied to the registers Rt,
to the probability of mt
replacing them with an R × M matrix Rt, where Rt
i taking
the value j. Finally, the instruction register is also transformed from a single value IRt to a
vector of size M noted IRt, where the i-th element represents its probability to take the
value i.

i,j represents the probability of rt

The Machine does not contain any learnable parameter and will just execute a given command.
To make it diﬀerentiable, the Machine now takes as input four probability distributions et,
at, bt and ot, where et is a distribution over instructions, and at, bt and ot are distributions
t as convex combinations
over the registers. We compute the argument values arg1
of the diﬀerent registers:

t and arg2

arg1

t =

irt
at
i

arg2

t =

irt
bt
i,

(1)

R
X

i=1

i and bt

where at
compute the output value of each instruction k using the following formula:
X

i are the i-th values of the vectors at and bt. Using these values, we can

arg1

t
i · arg2

j · 1[gk(i, j) = c mod M ],
t

(2)

∀0 ≤ c ≤ M outt

k,c =

R
X

i=1

0≤i,j≤M

where gk is the function associated to the k-th instruction as presented in Table 1b. Since the
executed instruction is controlled by the probability e, the output written to the register will
also be a convex combination: outt = PN
k=1 et
k, where N is the number of instructions.
This value is then stored into the registers by performing a soft-write parametrised by ot.

koutt

A special case is associated with the stop signal. When executing the model, we keep track of
the probability that the program should have terminated before this iteration based on the
probability associated at each iteration with the speciﬁc instruction that controls this ﬂag.
Once this probability goes over a threshold ηstop ∈ (0, 1], the execution is halted. We applied
the same techniques to make the side-eﬀects diﬀerentiable, this is presented in appendix A.1.

et = We ∗ IRt,

at = Wa ∗ IRt,

The Controller is the only learnable part of our model. The ﬁrst learnable part is the initial
values for the registers R0 and for the instruction register IR0. The second learnable part
is the parameters of the Controller which computes the required distributions using:
ot = Wo ∗ IRt

(3)
where We is an N × M matrix and Wa, Wb and Wo are R × M matrices. A representation
of these matrices can be found in Figure 4c. The Controller as deﬁned above is composed of
four independent, fully-connected layers. In Section 4.3 we will see that this complexity is
suﬃcient for our model to be able to represent any program.
Henceforth, we will denote by θ = {R0, IR0, We, Wa, Wb, Wo} the set of all the learnable
parameters of this model.

bt = Wb ∗ IRt,

4 Adaptative Neural Compiler

We will now present the Adaptive Neural Compiler.
Its goal is to ﬁnd the best set of
weights θ∗ for a given dataset such that our model will perform the correct input/output
mapping as eﬃciently as it can. We begin by describing our learning objective in details.
The two subsequent sections will focus on making the optimisation of our learning objective
computationally feasible.

4.1 Objective function

Our goal is to solve a given algorithmic problem eﬃciently. The algorithmic problem is
deﬁned as a set of input/output pairs. We also have access to a generic program that is able

4

to perform the required mapping. In our example of accessing elements in a linked list, the
transformation would consist in writing down the desired value at the speciﬁed position in
the tape. The program given to us would iteratively go through the elements of the linked
list, ﬁnd the desired value and write it down at the desired position. If there exists some
bias that would allow this traversal to be faster, we expect the program to exploit it.

Our approach to this problem is to construct a diﬀerentiable objective function, mapping
controller parameters to a loss. We deﬁne this loss based on the states of the memory tape
and outputs of the Controller at each step of the execution. The precise mathematical
formulation for each term of the loss is given in appendix B. Here we present the motivation
behind each of them.

Correctness For a given input, we have the expected output. We compare the values of
the expected output with the ﬁnal memory tape provided by the execution.

Halting To prevent programs to take an inﬁnite amount of time without stopping, we
deﬁned a maximum number of iterations Tmax after which the execution is halted. Moreover,
we add a penalty in the loss if the Controller didn’t halt before this limit.

Eﬃciency We penalise each iteration taken by the program where it does not stop.

Conﬁdence We add a term which will penalise probability of stopping if the current state
of the memory is not the expected one.

If only the correctness term was considered, nothing would encourage the learnt algorithm to
halt as soon as it ﬁnished. If only correctness and halting were considered, then the program
may not halt as early as possible. Conﬁdence enables the algorithm to evaluate better when
to stop.

The loss is a weighted sum of the four above-mentioned terms. We denote the loss of the i-th
training sample, given parameters θ, as Li(θ). Our learning objective is then speciﬁed as:

min
θ

X

i

Li(θ)

s.t. θ ∈ Θ,

(4)

where Θ is a set over the parameters such that the outputs of the Controller, the initial
values of each register and of the instruction register are all probability distributions.

The above optimisation is a highly non-convex problem. To be able to solve it using standard
gradient descent based methods, we will ﬁrst need to transform it to an unconstrained
problem. We also know that the result of the optimisation of a non-convex objective function
is strongly dependent on the initialisation point. In the rest of this section, we will ﬁrst
present a small modiﬁcation to the model that will remove the constraints. We will then
present our Neural Compiler that will provide a good initialisation to solve this problem.

4.2 Reformulation

In order to use gradient descent methods without having to project the parameters on Θ,
we alter the formulation of the controller. We add a softmax layer after each linear layer
ensuring that the constraints on the Controller’s output will be respected. We also apply a
softmax to the initial values of the registers and the instructions register, ensuring they will
also respect the original constraints. This way, we transform the constrained-optimisation
problem into an unconstrained one, allowing us to use standard gradient descent methods.
As discussed in other works [10], this kind of model is hard to train and requires a high
number of random restarts before converging to a good solution. We will now present a
Neural Compiler that will provide good initialisations to help with this problem.

4.3 Neural Compiler

The goal for the Neural Compiler is to convert an algorithm, written as an unambiguous
program, to a set of parameters. These parameters, when put into the controller, will
reproduce the exact steps of the algorithm. This is very similar to the problem framed by
Reed and de Freitas [12], but we show here a way to accomplish it without any learning.

5

var head = 0;
var nb_jump = 1;
var out_write = 2;

nb_jump = READ(nb_jump);
out_write = READ(out_write);

loop : head = READ(head);

nb_jump = DEC(nb_jump);
JEZ(nb_jump, end);
JEZ(0, loop);
end : head = INC(head);

head = READ(head);
WRITE(out_write, head);
STOP();

Initial Registers:

R1 = 6; R2 = 2; R3 = 0;
R4 = 2; R5 = 1; R6 = 0;
R7 = 0;

Program:

0 : R5 = READ (R5, R7)
1 : R4 = READ (R4, R7)
2 : R6 = READ (R6, R7)
3 : R5 = DEC (R5, R7)
(R5, R1)
4 : R7 = JEZ
(R3, R2)
5 : R3 = JEZ
6 : R6 = INC
(R6, R7)
7 : R6 = READ (R6, R7)
8 : R7 = WRITE(R4, R6)
9 : R7 = STOP (R7, R7)

(i) Instr.

(ii) Arg1

(iii) Arg2

(iv) Out

(a) Input program

(a) Intermediary representation

(c) Weights

Figure 4: Example of the compilation process. (2a) Program written to perform the ListK task.
Given a pointer to the head of a linked list, an integer k, a target cell and a linked list, write in
the target cell the k-th element of the list. (3a) Intermediary representation of the program. This
corresponds to the instruction that a Random Access Machine would need to perform to execute the
program. (4c) Representation of the weights that encodes the intermediary representation. Each
row of the matrix correspond to one state/line. Initial value of the registers are also parameters of
the model, omitted here.

The diﬀerent steps of the compilation are illustrated in Figure 4. The ﬁrst step is to go from
the written version of the program to the equivalent list of low level instruction. This step can
be seen as going from Figure 2a to Figure 3a. The illustrative example uses a fairly low-level
language but traditional features of programming languages such as loops or if-statements
can be supported using the JEZ instruction. The use of constants as arguments or as values
is handled by introducing new registers that hold these values. The value required to be
passed as target position to the JEZ instruction can be resolved at compile time.

Having obtained this intermediate representation, generating the parameters is straight-
forward. As can be seen in Figure 3a, each line contains one instruction, the two input
registers and the output register, and corresponds to a command that the Controller will
have to output. If we ensure that IR is a Dirac-delta distribution on a given value, then
the matrix-vector product is equivalent to selecting a row of the weight matrix. As IR is
incremented at each iteration, the Controller outputs the rows of the matrix in order. We
thus have a one-to-one mapping between the lines of the intermediate representation and
the rows of the weight matrix. An example of these matrices can be found in Figure 4c. The
weight matrix has 10 rows, corresponding to the number of lines of code of our intermediate
representation. On the ﬁrst line of the matrix corresponding to the ﬁrst argument (4cii), the
ﬁfth element has value 1, and is linked to the ﬁrst line of code where the ﬁrst argument to
the READ operation is the ﬁfth register.

The number of rows of the weight matrix is linear in the number of lines of code in the original
program. To output a command, we must be able to index its line with the instruction
register IR, which means that the largest representable number in our Machine needs to be
greater than the number of lines in our program.

Moreover, any program written in a regular assembly language can be rewritten to use only
our restricted set of instructions. This can be done ﬁrst because all the conditionals of the
the assembly language can be expressed as a combination of arithmetic and JEZ instructions.
Secondly because all the arithmetic operations can be represented as a combination of our
simple arithmetic operations, loops and ifs statements. This means that any program that
can run on a regular computer, can be ﬁrst rewritten to use our restricted set of instructions
and then compiled down to a set of weights for our model. Even though other models use
LSTM as controller, we showed here that a Controller composed of simple linear functions is
expressive enough. The advantage of this simpler model is that we can now easily interpret

6

the weights of our model in a way that would not have be possible if we had a recurrent
network as a controller.

The most straightforward way to leverage the results of the compilation is to initialise the
Controller with the weights obtained through compilation of the generic algorithm. To
account for the extra softmax layer, we need to multiply the weights produced by the compiler
by a large constant to output Dirac-delta distributions. Some results associated with this
technique can be found in Section 5.1. However, if we initialise with exactly this sharp set of
parameters, the training procedure is not able to move away from the initialisation as the
gradients associated with the softmax in this region are very small. Instead, we initialise
the controller with a non-ideal version of the generic algorithm. This means that the choice
with the highest probability in the output of the Controller is correct, but the probability of
other choices is not zero. As can be seen in Section 5.2, this allows the Controller to learn
by gradient descent a new algorithm, diﬀerent from the original one, that has a lower loss
than the ideal version of the compiled program.

5 Experiments

We performed two sets of experiments. The ﬁrst shows the capability of the Neural Compiler
to perfectly reproduce any given program. The second shows that our Neural Compiler
can adapt and improve the performance of programs. We present results of data-speciﬁc
optimisation being carried out and show decreases in runtime for all the algorithms and
additionally, for some algorithms, show that the runtime is a diﬀerent computational-
complexity class altogether. All the code required to reproduce these experiments is available
online 1.

5.1 Compilation

The compiler described in section 4.3 allows us to go from a program written using our
instruction set to a set of weights θ for our Controller.

To illustrate this point, we implemented simple programs that can solve the tasks introduced
by Kurach et al. [8] and a shortest path problem. One of these implementations can be
found in Figure 2a, while the others are available in appendix F. These programs are written
in a speciﬁc language, and are transformed by the Neural Compiler into parameters for the
model. As expected, the resulting models solve the original tasks exactly and can generalise
to any input sequence.

5.2 ANC experiments

In addition to being able to reproduce any given program as was done by Reed and de Freitas
[12], we have the possibility of optimising the resulting program further. We exhibit this
by compiling program down to our model and optimising their performance. The eﬃciency
gains for these tasks come either from ﬁnding simpler, equivalent algorithms or by exploiting
some bias in the data to either remove instructions or change the underlying algorithm.

We identify three diﬀerent levels of interpretability for our model: The ﬁrst type corresponds
to weights containing only Dirac-delta distributions, there is an exact one-to-one mapping
between lines in the weight matrices and lines of assembly code. In the second type where
all probabilities are Dirac-delta except the ones associated with the execution of the JEZ
instruction, we can recover an exact algorithm that will use if statements to enumerate the
diﬀerent cases arising from this conditional jump. In the third type where any operation
other than JEZ is executed in a soft way or use a soft argument, it is not possible to recover
a program that will be as eﬃcient as the learned one.

We present here brieﬂy the considered tasks and biases, and report the reader to appendix F
for a detailed encoding of the input/output tape.

1https://github.com/albanD/adaptive-neural-compilation

7

Table 1: Average numbers of iterations required to solve instances of the problems for the original
program, the best learned program and the ideal algorithm for the biased dataset. We also include
the success rate of reaching a more eﬃcient algorithm across multiple random restarts.

Access

Increment Swap ListK Addition Sort

Generic
Learned
Ideal

6
4
4

40
16
34

10
6
6

18
11
10

20
9
6

38
18
9.5

Success Rate

37 %

84%

27%

19%

12%

74%

1. Access: Given a value k and an array A, return A[k]. In the biased version, the
value of k is always be the same, so the address of the required element can be stored
in a constant. This is similar to the optimisation known as constant folding.

2. Swap: Given an array A and two pointers p and q, swap the elements A[p] and A[q].
In the biased version, p and q are always the same so reading them can be avoided.
3. Increment: Given an array, increment all its element by 1. In the biased version,
the array is of ﬁxed size and the elements of the array have the same value so you
don’t need to read all of them when going through the array.

4. Listk: Given a pointer to the head of a linked list, a number k and a linked list,
ﬁnd the value of the k-th element. In the biased version, the linked list is organised
in order in memory, as would be an array, so the address of the k-th value can be
computed in constant time. This is the example developed in Figure 4.

5. Addition: Two values are written on the tape and should be summed. No data bias
is introduced but the starting algorithm is non-eﬃcient: it performs the addition as
a series of increment operation. The more eﬃcient operation would be to add the
two numbers.

6. Sort: Given an array A, sort it. In the biased version, only the start of the array
might be unsorted. Once the start has been arranged, the end of the array can be
safely ignored.

For each of these tasks, we perform a grid search on the loss parameters and on our hyper-
parameters. Training is performed using Adam [7]. We choose the best set of hyperparameters
and run the optimisation with 100 diﬀerent random seeds. We consider that a program has
been successfully optimised when two conditions are fulﬁlled. First, it needs to output the
correct solution for all test cases presenting the same bias. Second, the average number of
iterations taken to solve a problem must be lower than the algorithm used for initialisation.
Note that if we cared only about the ﬁrst criterion, the methods presented in Section 5.1
would already provide a success rate of 100%, without requiring any training.

The results are presented in Table 1. For each of these tasks, we manage to ﬁnd faster
algorithms. In the simple cases of Access and Swap, the optimal algorithm for the presented
datasets are obtained. Exploiting the bias of the data, successful heuristics are incorporated
in the algorithm and appropriate constants get stored in the initial value of registers. The
learned programs for these tasks are always in the ﬁrst case of interpretability, this means
that we can recover the most eﬃcient algorithm from the learned weights.

While ListK and Addition have lower success rates, the improvements between the original
and learned algorithms are still signiﬁcant. Both were initialised with iterative algorithms
with O(n) complexities. They managed to ﬁnd constant time O(1) algorithms to solve the
given problems, making the runtime independent of the input. Achieving this means that
the equivalence between the two approaches has been identiﬁed, similar to how optimising
compilers operate. Moreover, on the ListK task, some learned programs corresponds to
the second type of interpretability. Indeed these programs use soft jumps to condition the
execution on the value of k. Even though these program would not generalise to other values
of k, some learned programs for this task achieve a type one interpretability and a study of
the learned algorithm reveal that they can generalise to any value of k.

8

Finally, the Increment task achieves an unexpected result. Indeed, it is able to outperform
our best possible algorithm. By looking at the learned program, we can see that it is actually
leveraging the possibility to perform soft writes over multiple elements of the memory at
the same time to reduce its runtime. This is the only case where we see a learned program
associated with the third type of interpretability. While our ideal algorithm would give a
conﬁdence of 1 on the output, this algorithm is unable to do so, but it has a high enough
conﬁdence of 0.9 to be considered a correct algorithm.

In practice, for all but the most simple tasks, we observe that further optimisation is possible,
as some useless instructions remain present. Some transformations of the controller are indeed
diﬃcult to achieve through the local changes operated by the gradient descent algorithm.
An analysis of these failure modes of our algorithm can be found in appendix G.4. This
motivates us to envision the use of approaches other than gradient descent to address these
issues.

6 Discussion

The work presented here is a ﬁrst step towards adaptive learning of programs. It opens
up several interesting directions of future research. For exemple, the deﬁnition of eﬃciency
that we considered in this paper is ﬂexible. We chose to only look at the average number
of operations executed to generate the output from the input. We leave the study of other
potential measures such as Kolmogorov Complexity and sloc, to name a few, for future
works.

As shown in the experiment section, our current method is very good at ﬁnding eﬃcient
solutions for simple programs. For more complex programs, only a solution close to the
initialisation can be found. Even though training heuristics could help with the tasks
considered here, they would likely not scale up to real applications. Indeed, the main problem
we identiﬁed is that the gradient-descent based optimisation is unable to explore the space
of programs eﬀectively, by performing only local transformations. In future work, we want
to explore diﬀerent optimisation methods. One approach would be to mix global and local
exploration to improve the quality of the solutions. A more ambitious plan would be to
leverage the structure of the problem and use techniques from combinatorial optimisation to
try and solve the original discrete problem.

References

[1] Marcin Andrychowicz and Karol Kurach. Learning eﬃcient algorithms with hierarchical

attentive memory. CoRR, 2016.

[2] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, 2014.

[3] Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom.

Learning to transduce with unbounded memory. In NIPS, 2015.

[4] Frédéric Gruau, Jean-Yves Ratajszczak, and Gilles Wiber. A neural compiler. Theoretical

Computer Science, 1995.

recurrent nets. In NIPS, 2015.

[5] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented

[6] Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. In ICLR, 2016.

[7] Diederik Kingma and Jimmy Adam. A method for stochastic optimization. In ICLR,

2015.

chines. In ICLR, 2016.

[8] Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access ma-

[9] Henry Massalin. Superoptimizer: a look at the smallest program. In ACM SIGPLAN

Notices, volume 22, pages 122–126. IEEE Computer Society Press, 1987.

9

[10] Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol
Kurach, and James Martens. Adding gradient noise improves learning for very deep
networks. In ICLR, 2016.

[11] João Pedro Neto, Hava Siegelmann, and Félix Costa. Symbolic processing in neural

networks. Journal of the Brazilian Computer Society, 2003.

[12] Scott Reed and Nando de Freitas. Neural programmer-interpreters. In ICLR, 2016.

[13] Eric Schkufza, Rahul Sharma, and Alex Aiken. Stochastic superoptimization. In ACM

SIGARCH Computer Architecture News, 2013.

[14] Rahul Sharma, Eric Schkufza, Berkeley Churchill, and Alex Aiken. Conditionally correct

superoptimization. In OOPSLA, 2015.

[15] Hava Siegelmann. Neural programming language. In AAAI, 1994.

[16] Ronald Williams. Simple statistical gradient-following algorithms for connectionist

reinforcement learning. Machine learning, 1992.

[17] Wojciech Zaremba and Ilya Sutskever. Reinforcement learning neural turing machines.

arXiv preprint arXiv:1505.00521, 2015.

[18] Wojciech Zaremba, Tomas Mikolov, Armand Joulin, and Rob Fergus. Learning simple

algorithms from examples. CoRR, 2015.

7 Appendix

A Detailed Model Description

In this section, we are going to precisely deﬁne the non diﬀerentiable model used above. This
model can be seen as a recurrent network. Indeed, it takes as input an initial memory tape,
performs a certain number of iterations and outputs a ﬁnal memory tape. The memory tape
is an array of M cells, where a cell is an element holding a single integer value. The internal
state of this recurrent model are the memory, the registers and the instruction register.
The registers are another set of R cells that are internal to the model. The instruction
register is a single cell used in a speciﬁc way described later. These internal states are noted
R} and IRt for the memory, the registers and
Mt = {mt
M }, Rt = {rt
2, . . . , mt
the instruction register respectively.

2, . . . , rt

1, mt

1, rt

Figure 1 describes in more detail how the diﬀerent elements interact with each other. At
each iteration, the Controller takes as input the value of the instruction register IRt and
outputs four values:

et, at, bt, ot = Controller(IRt).
(5)
The ﬁrst value et is used to select one of the instruction of the Machine to execute at this
iteration. The second and third values at and bt will identify which registers to use as the
ﬁrst and second argument for the selected instruction. The fourth value ot identity the
output register where to write the result of the executed instruction. The Machine then
takes as input these four values and the internal state and computes the updated value of
the internal state and a stop ﬂag:

Mt+1, Rt+1, IRt+1, stop = Machine(Mt, Rt, IRt, et, at, bt, ot).

(6)

The stop ﬂag is a binary ﬂag. When its value is 1, it means that the model will stop the
execution and the current memory state will be returned.

10

The Machine The machine is a deterministic function that increments the instruction
register and executes the command given by the Controller to update the current internal
state. The set of instructions that can be executed by the Machine can be found in Table 1b.
Each instruction takes two values as arguments and returns a value. Additionally, some of
these instructions have side eﬀects. This mean that they do not just output a value, they
perform another task. This other task can be for example to modify the content of the
memory. All the considered side eﬀects can be found in Table 1b. By convention, instructions
that don’t have a value to return and that are used only for their side-eﬀect will return a
value of 0.

The Controller The Controller is a function that takes as input a single value and outputs
four diﬀerent values. The Controller’s internal parameters, the initial values for the registers
and the initial value of the instruction register deﬁne uniquely a given Controller.

The usual choice in the literature is to use an LSTM network[2, 3, 8] as controller. Our
choice was to instead use a simpler model. Indeed, our Controller associates a command
to each possible value of the instruction register. Since the instruction register’s value will
increase by one at each iteration, this will enforce the Controller to encode in its weights
what to do at each iteration. If we were using a recurrent controller the same instruction
register could potentially be associated to diﬀerent sets of outputs and we would lose this
one to one mapping.

To make this clearer, we ﬁrst rewrite the instruction register as an indicator vector with a 1
at the position of its value:

Ii =

(cid:26)1
0

if i = IRt
otherwise

.

(7)

In this case, we can write a single output at of the Controller as the result of a linear function
of I:

at = Wa ∗ I ,
(8)
where Wa is the 1xM matrix containing the value that need to be chosen as ﬁrst arguments for
each possible value of the instruction register and ∗ represent a matrix vector multiplication.

A.1 Mathematical details of the diﬀerentiable model

In order to make the model diﬀerentiable, every value and every choice are replaced by
probability distributions over the possible choices. Using convex combinations of probability,
the execution of the Machine is made diﬀerentiable. We present here the mathematical
formulation of this procedure for the case of the side-eﬀects.

STOP In the discrete model, the execution is halted when the STOP instruction is
executed. However, in the diﬀerentiable model, the STOP instruction may be executed
with a probability smaller than 1. To take this into account, when executing the model, we
keep track of the probability that the program should have terminated before this iteration
based on the probability associated to the STOP instruction at each iteration. Once this
probability goes over a threshold ηstop ∈]0, 1], the execution is halted.

READ The mechanism is entirely the same as the one used to compute the arguments
based on the registers and a probability distribution over the registers.

JEZ We note IRt+1
or not the JEZ instruction. We also have et
iteration t. The new value of the instruction register is:

njez the new value of IRt if we had respectively executed
jez the probability of executing this instruction at

jez and IRt+1

IRt+1 = IRt+1

njez · (1 − et

jez) + IRt+1

jez · et

jez

IRt+1
jez is himself computed based on several probability distribution. If we consider that
the instruction JEZ is executed with probabilistic arguments cond and label, its value is
given by

IRt+1

jez = label · cond0 + INC(IRt) · (1 − cond0)

(9)

(10)

11

With a probability equals to the one that the ﬁrst argument is equal to zero, the new value
of IRt is label. With the complement, it is equal to the incremented version of its current
value, as the machine automatically increments the instruction register.

WRITE The mechanism is fairly similar to the one of the JEZ instruction.
We note Mt+1
W RIT E and Mt+1
not the WRITE instruction. We also have et
at iteration t. The new value of the memory matrix register is:

nW RIT E the new value of Mt if we had respectively executed or
write the probability of executing this instruction

Mt+1 = Mt+1

nW RIT E · (1 − et

write) + Mt+1

W RIT E · et

W RIT E

As with the JEZ instruction, the value of Mt+1
W RIT E is dependent on the two probability
distribution given as input: addr and val. The probability that the i-th cell of the memory
tape contains the value j after the update is:

M t+1

i,j = addri · valj + (1 − addri) · M t
i,j

Note that this can done using linear algebra operations so as to update everything in one
global operation.

Mt+1 = (cid:0)((1 − addr)1T ) ⊗ Mt(cid:1) + (addr valT )

(11)

(12)

(13)

B Speciﬁcation of the loss

This loss contains four terms that will balance the correctness of the learnt algorithm, proper
usage of the stop signal and speed of the algorithms. The parameters deﬁning the models are
the weight of the Controller’s function and the initial value of the registers. When running
the model with the parameters θ, we consider that the execution ran for T time steps. We
consider the memory to have a size M and that each number can be an integer between
0 and M − 1. Mt was the state of the memory at the t-th step. T and C are the target
memory and the 0-1 mask of the elements we want to consider. All these elements are
matrices where for example Mt
i,j is the probability of the i-th entry of the memory to take
the value j at the step t. We also note pstop,t the probability outputted by the Machine that
it should have stopped before iteration t.

Correctness The ﬁrst term corresponds to the correctness of the given algorithm. For a
given input, we have the expected output and a mask. The mask allows us to know which
elements in the memory we should consider when comparing the solutions. For the given
input, we will compare the values speciﬁed by the mask of the expected output with the
ﬁnal memory tape provided by the execution. We compare them with the L2 distance in the
probability space. Using the notations from above, we can write this term as:
X

Lc(θ) =

Ci,j(MT

i,j(θ) − Ti,j)2.

(14)

i,j

If we optimised only this ﬁrst term, nothing would encourage the learnt algorithm to use the
STOP instruction and halt as soon as it ﬁnished.

Halting To prevent programs to take an inﬁnite amount of time without stopping, we
deﬁned a maximum number of iterations Tmax after which the execution is halted. During
training, we also add a penalty if the Controller didn’t halt before this limit:

LsTmax(θ) = (1 − pstop−T (θ)) · [T == Tmax]

Eﬃciency If we consider only the above mentioned losses, the program will make sure to
halt by itself but won’t do it as early as possible. We incentivise this behaviour by penalising
each iteration taken by the program where it does not stop:

(15)

(16)

Lt(θ) =

(1 − pstop,t(θ)).

X

t∈[1,T −1]

12

Conﬁdence Moreover, we want the algorithm to have a good conﬁdence to stop when
it has found the correct output. To do so, we add the following term which will penalise
probability of stopping if the current state of the memory is not the expected one:

Lst(θ) =

X

X

t∈[2,T ]

i,j

(pstop,t(θ) − pstop,t−1(θ))Ci,j(Mt

i,j(θ) − Ti,j)2.

(17)

The increase in probability (pstop,t − pstop,t−1) corresponds to the probability of stopping
exactly at iteration t. So, this is equivalent to the expected error made.

Total loss The complete loss that we use is then the following:

L(θ) = αLc(θ) + βLsTmax (θ) + γLst(θ) + δLt(θ).

(18)

C Distributed representation of the program

For the most of out experiments, the learned weights are fully interpretable as they ﬁt in the
ﬁrst type of interpretability. However, in some speciﬁc cases, under the pressure of our loss
encouraging a smaller number of iterations, an interesting behavior emerges.

It is interesting to note that the decompiled version is not straightforward to
Remarks
interpret. Indeed when we reach a program that has non Dirac-delta distributions in its
weights, we cannot perform the inverse of the one-to-one mapping performed by the compiler.
In fact, it relies on this blurriness to be able to execute the program with a smaller number
of instruction. Notably, by having some blurriness on the JEZ instruction, the program can
hide additional instructions, by creating a distributed state. We now explain the mechanism
used to achieve this.

Creating a distributed state Consider the following program and assume that the initial
value of IR is 0:

Initial Registers:
R1 = 0; R2 = 1; R3 = 4, R4 = 0

Program:
0 : R1 = READ (R1, R4)
1 : R4 = JEZ (R1, R3)
2 : R4 = WRITE(R1, R1)
3 : R4 = WRITE(R1, R3)

If you take this program and execute it for three iterations, it will: read the ﬁrst value of
the tape into R1. Then, if this value is zero, it will jump to State 4, otherwise it will just
increment IR. This means that depending on the value that was in R1, the next instruction
that will be executed will be diﬀerent (in this case, the diﬀerence between State 3 and State
4 is which registers they will be writing from). This is our standard way of implementing
conditionals.

Imagine that, after learning, the second instruction in our example program has 0.5 probability
of being a JEZ and 0.5 probability of being a ZERO. If the content of R1 is a zero, according
to the JEZ, we should jump to State 4, but this instruction is executed with a probability
of 0.5. We also have 0.5 probability of executing the ZERO instruction, which would lead to
State 3.

Therefore, IR is not a Dirac-delta distribution anymore but points to State 3 with probability
0.5 and State 4 with probability 0.5.

Exploiting a distributed state To illustrate, we will discuss how the Controller computes
a for a model with 3 registers. The Table 2 show an example of some weights for such a
controller.

If we are in State 1, the output of the controller is going to be

out = softmax([20, 5, −20]) = [0.9999..., 3e−7, 4e−18]

(19)

13

State 1
State 2

R1 R2 R3
-20
5
20
20
5
-20

Table 2: Controller Weights

If we are in State 2, the output of the controller is going to be

out = softmax([−20, 5, 20]) = [4e−18, 3e−7, 0.9999...]

(20)

In both cases, the output of the controller is therefore going to be almost discrete. In State
1, R1 would be chosen and in State 2, R3 would be chosen.

However, in the case where we have a distributed state with probability 0.5 over State 1 and
0.5 over State 2, the output would be:

out = softmax(0.5 ∗ [−20, 5, 20] + 0.5[20, 5, −20])

= softmax([0, 10, 0])
= [4e−5, 0.999, 4e−5].

(21)

Note that the result of the distributed state is actually diﬀerent from the result of the discrete
states. Moreover it is still a discrete choice of the second register.

Because this program contains distributed elements, it is not possible to perform the one-to-
one mapping between the weights and the lines of code. Though every instruction executed
by the program, except for the JEZ, are binary. This means that this model can be translated
to a regular program that will take exactly the same runtime, but will require more lines of
codes than the number of lines in the matrix.

D Alternative Learning Strategies

A critique that can be made to this method is that we will still initialise close to a local
minimum. Another approach might be to start from a random initialisation but adding a
penalty on the value of the weights such that they are encourage to be close to the generic
algorithm. This can be seen as L2 regularisation but instead of pushing the weights to 0, we
push then with the value corresponding to the generic algorithm. If we start with a very
high value of this penalty but use an annealing schedule where its importance is very quickly
reduced, this is going to be equivalent to the previous method.

E Possible Extension

E.1 Making objective function diﬀerentiable

These experiments showed that we can transform any program that perform a mapping
between an input memory tape to an output memory tape to a set of parameters and execute
it using our model. The ﬁrst point we want to make here is that this means that we take
any program and transform it into a diﬀerentiable function easily. For example, if we want
to learn a model that given a graph and two nodes a and b, will output the list of nodes to
go through to go from a to b in the shortest amount of time. We can easily deﬁne the loss of
the length of the path outputted by the model. Unfortunately, the function that computes
this length from the set of nodes is not diﬀerentiable. Here we could implement this function
in our model and use it between the prediction of the model and the loss function to get an
end to end trainable system.

E.2 Beyond mimicking and towards open problems

It would even be possible to generalise our learning procedure to more complex problems
for which we don’t have a ground truth output. For example, we could consider problems
where the exact answer for a given input is not computable or not unique. If the goodness
of a solution can be computed easily, this value could be used as training objective. Any
program giving a solution could be used as initialisation and our framework would improve
it, making it generate better solutions.

14

This section will present the programs that we use as initialisation for the experiment section.

F Example tasks

F.1 Access

In this task, the ﬁrst element in the memory is a value k. Starting from the second element,
the memory contains a zero-terminated list. The goal is to access the k-th element in the
list that is zero-indexed. The program associated with this task can be found in Listing 1.

1

2

3

4

5

6

(cid:7)
var k = 0
k = READ (0)
k = INC ( k )
k = READ ( k )
WRITE (0 , k )
STOP ()
(cid:6)
Listing 1: Access Task

(cid:4)

(cid:5)

Example input:
Output:

6
1

9
9

1
1

2
2

7
7

9
9

8
8

1
1

3
3

5
5

F.2 Copy

In this task, the ﬁrst element in the memory is a pointer p. Starting from the second element,
the memory contains a zero-terminated list. The goal is to copy this list at the given pointer.
The program associated with this task can be found in Listing 2.

1

2

3

4

5

6

7

8

9

10

11

12

13

(cid:7)
var read_addr = 0
var read_value = 0
var write_addr = 0

write_addr = READ (0)
l_loop : read_value = READ ( read_addr )
JEZ ( read_value , l_stop )
WRITE ( write_addr , read_value )
read_addr = INC ( read_addr )
write_addr = INC ( write_addr )
JEZ (0 , l_loop )

l_stop : STOP ()
(cid:6)

Listing 2: Copy Task

(cid:4)

(cid:5)

Example input:
Output:

9
9

11
11

3
3

1
1

5
5

14
14

0
0

0
0

0
0

0
11

0
3

0
1

0
5

0
14

0
0

F.3

Increment

In this task, the memory contains a zero-terminated list. The goal is to increment each value
in the list by 1. The program associated with this task can be found in Listing 3.

15

1

2

3

4

5

6

7

8

9

10

11

(cid:7)
var read_addr = 0
var read_value = 0

l_loop : read_value = READ ( read_addr )
JEZ ( read_value , l_stop )
read_value = INC ( read_value )
WRITE ( read_addr , read_value )
read_addr = INC ( read_addr )
JEZ (0 , l_loop )

l_stop : STOP ()
(cid:6)

Listing 3: Increment Task

(cid:4)

(cid:5)

Example input:
Output:

1
2

2
3

2
3

3
4

0
0

0
0

0
0

F.4 Reverse

In this task, the ﬁrst element in the memory is a pointer p. Starting from the second element,
the memory contains a zero-terminated list. The goal is to copy this list at the given pointer
in the reverse order. The program associated with this task can be found in Listing 4.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

(cid:7)
var read_addr = 0
var read_value = 0
var write_addr = 0

write_addr = READ ( write_addr )
l_count_phase : read_value = READ ( read_addr )
JEZ ( read_value , l_copy_phase )
read_addr = INC ( read_addr )
JEZ (0 , l_count_phase )

l_copy_phase : read_addr = DEC ( read_addr )
JEZ ( read_addr , l_stop )
read_value = READ ( read_addr )
WRITE ( write_addr , read_value )
write_addr = INC ( write_addr )
JEZ (0 , l_copy_phase )

l_stop : STOP ()
(cid:6)

Listing 4: Reverse Task

(cid:4)

(cid:5)

Example input:
Output:

5
5

7
7

2
2

13
13

14
14

0
14

0
13

0
2

0
7

0
0

0
0

0
0

0
0

0
0

0
0

F.5 Permutation

In this task, the memory contains two zero-terminated list one after the other. The ﬁrst
contains a set of indices. the second contains a set of values. The goal is to ﬁll the ﬁrst list
with the values in the second list at the given index. The program associated with this task
can be found in Listing 5.

16

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

(cid:7)
var read_addr = 0
var read_value = 0
var write_offset = 0

l_count_phase : read_value = READ ( write_offset )
write_offset = INC ( write_offset )
JEZ ( read_value , l_copy_phase )
JEZ (0 , l_count_phase )

l_copy_phase : read_value = DEC ( read_addr )
JEZ ( read_value , l_stop )
read_value = ADD ( write_offset , read_value )
read_value = READ ( read_value )
WRITE ( read_addr , read_value )
read_addr = INC ( read_addr )
JEZ (0 , l_copy_phase )
l_stop : STOP ()
(cid:6)

Listing 5: Permutation Task

(cid:4)

(cid:5)

Example input:
Output:

2
4

1
13

3
6

0
0

13
13

4
4

6
6

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

F.6 Swap

In this task, the ﬁrst two elements in the memory are pointers p and q. Starting from the
third element, the memory contains a zero-terminated list. The goal is to swap the elements
pointed by p and q in the list that is zero-indexed. The program associated with this task
can be found in Listing 6.

(cid:7)
var p = 0
var p_val = 0
var q = 0
var q_val = 0

p = READ (0)
q = READ (1)
p_val = READ ( p )
q_val = READ ( q )
WRITE (q , p_val )
WRITE (p , q_val )
STOP ()
(cid:6)

1

2

3

4

5

6

7

8

9

10

11

12

(cid:4)

(cid:5)

Listing 6: Swap Task

Example input:
Output:

1
1

3
3

7
7

6
5

7
7

5
6

2
2

0
0

0
0

0
0

F.7 ListSearch

In this task, the ﬁrst three elements in the memory are a pointer to the head of the linked
list, the value we are looking for v and a pointer to a place in memory where to store the
result. The rest of the memory contains the linked list. Each element in the linked list is two
values, the ﬁrst one is the pointer to the next element, the second is the value contained in
this element. By convention, the last element in the list points to the address 0. The goal is
to return the pointer to the ﬁrst element whose value is equal to v. The program associated
with this task can be found in Listing 7.

17

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

(cid:7)
var p_out = 0
var p_current = 0
var val_current = 0
var val_searched = 0

val_searched = READ (1)
p_out = READ (2)
l_loop : p_current = READ ( p_current )
val_current = INC ( p_current )
val_current = READ ( val_current )
val_current = SUB ( val_current , val_searched )
JEZ ( val_current , l_stop )
JEZ (0 , l_loop )
l_stop : WRITE ( p_out , p_current )
STOP ()
(cid:6)

Listing 7: ListSearch Task

(cid:4)

(cid:5)

Example input:
Output:

11
11

10
10

2
5

9
9

4
4

3
3

10
10

0
0

6
6

7
7

13
13

5
5

12
12

0
0

0
0

F.8 ListK

In this task, the ﬁrst three elements in the memory are a pointer to the head of the linked
list, the number of hops we want to do k in the list and a pointer to a place in memory
where to store the result. The rest of the memory contains the linked list. Each element in
the linked list is two values, the ﬁrst one is the pointer to the next element, the second is
the value contained in this element. By convention, the last element in the list points to
the address 0. The goal is to return the value of the k-th element of the linked list. The
program associated with this task can be found in Listing 8.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

(cid:7)
var p_out = 0
var p_current = 0
var val_current = 0
var k = 0

k = READ (1)
p_out = READ (2)
l_loop : p_current = READ ( p_current )
k = DEC ( k )
JEZ (k , l_stop )
JEZ (0 , l_loop )
l_stop : p_current = INC ( p_current )
p_current = READ ( p_current )
WRITE ( p_out , p_current )
STOP ()
(cid:6)

Listing 8: ListK Task

(cid:4)

(cid:5)

3
3

2
2

2
17

9
9

15
15

0
0

0
0

0
0

1
1

15
15

17
17

7
7

13
13

0
0

0
0

11
11

Example input:
Output:
0
0
10
0
0
10

0
0

F.9 Walk BST

In this task, the ﬁrst two elements in the memory are a pointer to the head of the BST and a
pointer to a place in memory where to store the result. Starting at the third element, there

18

is a zero-terminated list containing the instructions on how to traverse in the BST. The rest
of the memory contains the BST. Each element in the BST has three values, the ﬁrst one is
the value of this node, the second is the pointer to the left node and the third is the pointer
to the right element. By convention, the leafs points to the address 0. The goal is to return
the value of the node we get at after following the instructions. The instructions are 1 or
2 to go respectively to the left or the right. The program associated with this task can be
found in Listing 9.
(cid:7)
var p_out = 0
var p_current = 0
var p_instr = 0
var instr = 0

(cid:4)

1

2

4

3

5

6

7

8

9

10

11

12

13

14

15

16

17

18

p_current = READ (0)
p_out = READ (1)
instr = READ (2)

l_loop : JEZ ( instr , l_stop )
p_current = ADD ( p_current , instr )
p_current = READ ( p_current )
p_instr = INC ( p_instr )
JEZ (0 , l_loop )

l_stop : p_current = READ ( p_current )
WRITE ( p_out , p_current )
STOP ()
(cid:6)

(cid:5)

Listing 9: WalkBST Task

Example input:
Output:
24
0
8
24
0
8

0
0

0
0

12
12
0
0

1
10
0
0

0
0

1
1

2
2

0
0
10
10

0
0

0
0
0
0

15
15
0
0

0
0

0
0

0
0

0
0

9
9

23
23

0
0

0
0

11
11

15
15

6
6

F.10 Merge

In this task, the ﬁrst three elements in the memory are pointers to respectively, the ﬁrst list,
the second list and the output. The two lists are zero-terminated sorted lists. The goal is to
merge the two lists into a single sorted zero-terminated list that starts at the output pointer.
The program associated with this task can be found in Listing 10.

19

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

(cid:4)

(cid:5)

(cid:7)
var p_first_list = 0
var val_first_list = 0
var p_second_list = 0
var val_second_list = 0
var p_output_list = 0
var min = 0

p_first_list = READ (0)
p_second_list = READ (1)
p_output_list = READ (2)

l_loop : val_first_list = READ ( p_first_list )
val_second_list = READ ( p_second_list )
JEZ ( val_first_list , l_first_finished )
JEZ ( val_second_list , l_second_finished )
min = MIN ( val_first_list , val_second_list )
min = SUB ( val_first_list , min )
JEZ ( min , l_first_smaller )

WRITE ( p_output_list , val_first_list )
p_output_list = INC ( p_output_list )
p_first_list = INC ( p_first_list )
JEZ (0 , l_loop )

l_first_smaller : WRITE ( p_output_list , val_second_list )
p_output_list = INC ( p_output_list )
p_second_list = INC ( p_second_list )
JEZ (0 , l_loop )

l_first_finished : p_first_list = ADD ( p_second_list , 0)
val_first_list = ADD ( val_second_list , 0)

l_second_finished : WRITE ( p_output_list , val_first_list )
p_first_list = INC ( p_first_list )
p_output_list = INC ( p_output_list )
val_first_list = READ ( p_first_list )
JEZ ( val_first_list , l_stop )
JEZ (0 , l_second_finished )

l_stop : STOP ()
(cid:6)

Listing 10: Merge Task

Example input:
Output:
0
0
0
0
1
16

0
0

0
0

3
3
0
0

8
8
0
0

11
11
0
0

27
27
0
0

0
0

17
17
0
0

16
16
0
0

1
1
0
0

0
0
0
0

29
29
0
0

26
26

0
0

0
29

0
27

0
26

0
17

F.11 Dijkstra

In this task, we are provided with a graph represented in the input memory as follow. The
ﬁrst element is a pointer pout indicating where to write the results. The following elements
contain a zero-terminated array with one entry for each vertex in the graph. Each entry is a
pointer to a zero-terminated list that contains a pair of values for each outgoing edge of the
considered node. Each pair of value contains ﬁrst the index in the ﬁrst array of the child node
and the second value contains the cost of this edge. The goal is to write a zero-terminated
list at the address provided by pout that will contain the value of the shortest path from the
ﬁrst node in the list to this node. The program associated with this task can be found in
Listings 11 and 12.

20

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

(cid:7)
var min = 0
var argmin = 0

var p_out = 0
var p_out_temp = 0
var p_in = 1
var p_in_temp = 1

var nnodes = 0

var zero = 0
var big = 99

var tmp_node = 0
var tmp_weight = 0
var tmp_current = 0
var tmp = 0

var didsmth = 0

p_out = READ ( p_out )
p_out_temp = ADD ( p_out , zero )

tmp_current = INC ( zero )
l_loop_nnodes : tmp = READ ( p_in_temp )
JEZ ( tmp , l_found_nnodes )
WRITE ( p_out_temp , big )
p_out_temp = INC ( p_out_temp )
WRITE ( p_out_temp , tmp_current )
p_out_temp = INC ( p_out_temp )
p_in_temp = INC ( p_in_temp )
nnodes = INC ( nnodes )
JEZ ( zero , l_loop_nnodes )

l_found_nnodes : WRITE ( p_out , zero )
JEZ ( zero , l_find_min )
l_min_return : p_in_temp = ADD ( p_in , argmin )
p_in_temp = READ ( p_in_temp )

l_loop_sons : tmp_node = READ ( p_in_temp )
JEZ ( tmp_node , l_find_min )
tmp_node = DEC ( tmp_node )
p_in_temp = INC ( p_in_temp )
tmp_weight = READ ( p_in_temp )
p_in_temp = INC ( p_in_temp )

p_out_temp = ADD ( p_out , tmp_node )
p_out_temp = ADD ( p_out_temp , tmp_node )
tmp_current = READ ( p_out_temp )
tmp_weight = ADD ( min , tmp_weight )

tmp = MIN ( tmp_current , tmp_weight )
tmp = SUB ( tmp_current , tmp )
JEZ ( tmp , l_loop_sons )
WRITE ( p_out_temp , tmp_weight )
JEZ ( zero , l_loop_sons )
(cid:6)

Listing 11: Dijkstra Algorithm (Part 1)

21

(cid:4)

(cid:5)

57

58

59

60

61

62

63

64

65

66

67

68

69

70

71

72

73

74

75

76

77

78

79

80

81

82

83

84

85

86

87

88

(cid:7)
l_find_min : p_out_temp = DEC ( p_out )
tmp_node = DEC ( zero )
min = ADD ( big , zero )
argmin = DEC ( zero )

l_loop_min : p_out_temp = INC ( p_out_temp )
tmp_node = INC ( tmp_node )
tmp = SUB ( tmp_node , nnodes )
JEZ ( tmp , l_min_found )

tmp_weight = READ ( p_out_temp )

p_out_temp = INC ( p_out_temp )
tmp = READ ( p_out_temp )
JEZ ( tmp , l_loop_min )

tmp = MAX ( min , tmp_weight )
tmp = SUB ( tmp , tmp_weight )
JEZ ( tmp , l_loop_min )
min = ADD ( tmp_weight , zero )
argmin = ADD ( tmp_node , zero )
JEZ ( zero , l_loop_min )

l_min_found : tmp = SUB ( min , big )
JEZ ( tmp , l_stop )
p_out_temp = ADD ( p_out , argmin )
p_out_temp = ADD ( p_out_temp , argmin )
p_out_temp = INC ( p_out_temp )
WRITE ( p_out_temp , zero )
JEZ ( zero , l_min_return )

l_stop : STOP ()
(cid:6)

Listing 12: Dijkstra Algorithm (Part 2)

(cid:4)

(cid:5)

Example omitted for space reasons

G Learned optimisation: Case study

Here we present an analysis of the optimisation achieved by the ANC. We take the example
of the ListK task and study the diﬀerence between the learned program and the initialisation
used.

G.1 Representation

The representation chosen is under the form of the intermediary representation described in
Figure (2b). Based on the parameters of the Controller, we can recover the approximate
representation described in Figure (2b): 1For each possible "discrete state" of the instruction
register, we can compute the commands outputted by the controller. We report the most
probable value for each distribution, as well as the probability that the compiler would assign
to this value. If no value has a probability higher than 0.5, we only report a neutral token
(R-, -, NOP).

G.2 Biased ListK

Figure 5 represents the program that was used as initialisation to the optimisation problem.
This is the direct result from the compilation performed by the Neural Compiler of the
program described in Listing 8. A version with a probability of 1 for all necessary instructions
would have been easily obtained but not amenable to learning.

22

Figure 6 similarly describes the program that was obtained after learning.

As a remainder, the bias introduced in the ListK task is that the linked list is well organised
in memory. In the general case, the element could be in any order. An input memory tape to
the problem of asking for the third element in the linked list containing {4, 5, 6, 7} would be:

or

9

5

3

3

2

2

0

0

0

0

11

5

0

7

4

15

7

5

5

0

4

7

7

0

6

0

0

0

0

0

0

9

0

6

0

0

0

0

0

0

In the biased version of the task, all the elements are arranged in order and contiguously
positioned on the tape. The only valid representation of this problems is:

3

3

2

5

4

7

5

9

6

0

7

0

0

0

0

0

0

0

0

0

G.3 Solutions

Because of the additional structure of the problem, the bias in the data, a more eﬃcient
algorithm to ﬁnd the solution exists. Let us dive into the comparison of the two diﬀerent
solutions.

Both use their ﬁrst two states to read the parameters of the given instance of the task.
Which element of the list should be returned is read at line (0:) and where to write the
returned value is read at line (1:). Step (2:) to (6:) are dedicated to putting the address of
the k-th value of the linked list into the registers R1. Step (7:) to (9:) perform the same
task in both solution: reading the value at the address contained in R1, writing it at the
desired position and stopping.

The diﬀerence between the two programs lies in how they put the address of the k-th value
into R1.

Generic The initialisation program, used for initialisation, works in the most general case
so needs to perform a loop where it put the address of the next element in the linked list in
R1 (2:), decrement the number of jumps remaining to be made (3:), checking whether the
wanted element has been reached (4:) and going back to the start of the loop if not (5:).
Once the desired element is reached, R1 is incremented so as to point on the value of the
linked list element.

Speciﬁc On the other hand, in the biased version of the problem, the position of the
desired value can be analytically determined. The function parameters occupy the ﬁrst three
cells of the tape. After those, each element of the linked list will occupy two cells (one for
the pointer to the next address and one for the value). Therefore, the address of the desired
value is given by

R1 = 3 + (2 ∗ (k − 1) + 1) − 1 + 1

= 3 + 2 ∗ k − 1

(the -1 comes from the fact that the address are 0-indexed and the ﬁnal +1 from the fact
that we are interested in the position of the value and not of the pointer.)

The way this is computed is as follows:

- R1 = 3 + k by adding the constant 3 to the registers R2 containing K.
- R2 = k − 1
- R1 = 3 + 2 ∗ k − 1 by adding the now reduced value of R2.

The algorithm implemented by the learned version is therefore much more eﬃcient for the
biased dataset, due to its capability to ignore the loop.

(22)

(2:)
(3:)
(6:)

23

G.4 Failure analysis

An observation that can be made is that in the learned version of the program, Step (4:)
and (5:) are not contributing to the algorithms. They execute instructions that have no
side eﬀect and store the results into the registers R7 that is never used later in the execution.

The learned algorithm could easily be more eﬃcient by not performing these two operations.
However, such an optimisation, while perhaps trivial for a standard compiler, capable of
detecting unused values, is fairly hard for our optimisers to discover. Because we are only
doing gradient descent, the action of "moving some instructions earlier in the program" which
would be needed here to make the useless instructions disappear, is fairly hard, as it involves
modifying several rows of the program at once in a coherent manner.

R1 = 0 (0.88)
R2 = 1 (0.88)
R3 = 2 (0.88)
R4 = 6 (0.88)
R5 = 0 (0.88)
R6 = 2 (0.88)
R7 = - (0.05)

Initial State: 0 (0.88)

R2 (0.96)
0:
R3 (0.96)
1:
R1 (0.96)
2:
R2 (0.96)
3:
R7 (0.96)
4:
R7 (0.96)
5:
R1 (0.96)
6:
R1 (0.96)
7:
R7 (0.96)
8:
9:
R7 (0.96)
10: R- (0.16)
11: R- (0.16)
12: R- (0.17)
13: R- (0.16)
14: R- (0.17)
15: R- (0.16)
16: R- (0.17)
17: R- (0.18)
18: R- (0.17)
19: R- (0.15)

= READ (0.93)
= READ (0.93)
= READ (0.93)
(0.93)
= DEC
(0.93)
= JEZ
(0.93)
= JEZ
(0.93)
= INC
= READ (0.93)
= WRIT (0.93)
= STOP (0.93)
(0.11)
= NOP
(0.11)
= NOP
(0.1)
= NOP
(0.11)
= NOP
(0.11)
= NOP
(0.1)
= NOP
(0.11)
= NOP
(0.1)
= NOP
(0.1)
= NOP
(0.11)
= NOP

[ R2 (0.96)
[ R3 (0.96)
[ R1 (0.96)
[ R2 (0.96)
[ R2 (0.96)
[ R5 (0.96)
[ R1 (0.96)
[ R1 (0.96)
[ R3 (0.96)
[ R- (0.14)
[ R- (0.16)
[ R- (0.18)

[ R- (0.17)

[ R- (0.17)
[ R- (0.16)

, R- (0.14)
, R- (0.14)
, R- (0.14)
, R- (0.14)
, R4 (0.96)
, R6 (0.96)
, R- (0.14)
, R- (0.14)
, R1 (0.96)
, R- (0.14)
, R- (0.16)
, R- (0.16)

, R- (0.17)

, R- (0.16)
, R- (0.16)

[ R- (0.16)

, R- (0.17)

[ R- (0.16)

, R- (0.18)

[ R- (0.18)
[ R- (0.16)

, R- (0.17)
, R- (0.16)

[ R- (0.16)

, R- (0.17)

]
]
]
]
]
]
]
]
]
]
]
]

]

]
]

]

]

]

]
]

Figure 5: Initialisation used for the learning of the ListK task.

24

Initial State: 0 (0.99)

R1 = 3 (0.99)
R2 = 1 (0.99)
R3 = 2 (0.99)
R4 = 10 (0.53)
R5 = 0 (0.99)
R6 = 2 (0.99)
R7 = 7 (0.99)

R2 (0.99)
R6 (0.99)
R1 (1)
R2 (1)
R7 (0.99)
R7 (0.99)
R1 (1)
R1 (0.99)
R7 (0.99)
R7 (0.9)

0:
1:
2:
3:
4:
5:
6:
7:
8:
9:
10: R2 (0.99)
11: R1 (0.98)
12: R3 (0.98)
13: R3 (0.87)
14: R3 (0.89)
15: R3 (0.99)
16: R3 (0.99)
17: R2 (0.99)
18: R3 (0.99)
19: R3 (0.98)

= READ (0.99)
= READ (0.99)
(0.99)
(0.99)

= ADD
= DEC

= ADD

= MAX
= INC

(0.99)
(0.99)

(0.99)
= READ (0.99)
= WRIT (0.99)

= STOP (0.99)

[ R2 (1)
[ R3 (0.99)

, R1 (0.97)

]

, R6 (0.5)
]
]

, R2 (1)
, R1 (0.99)

]

[ R1 (0.99)
[ R2 (1)

[ R2 (0.99)
[ R6 (0.7)

, R1 (0.51)
, R1 (0.89)

]
]

[ R1 (0.99)

, R2 (1)

]

[ R1 (0.99)
[ R6 (1)
[ R6 (0.98)

, R1 (0.53)

]

, R1 (0.99)

]

, R1 (0.99)

]

= STOP (0.96)
(0.73)
= ADD
= ADD
(0.64)
= STOP (0.65)
= STOP (0.62)
= STOP (0.65)
(0.45)
= NOP
= INC
(0.56)
= STOP (0.65)
= STOP (0.98)

[ R1 (0.52)
[ R4 (0.99)
[ R6 (0.99)
[ R3 (0.52)
[ R6 (0.99)
[ R3 (0.99)
[ R6 (0.99)
[ R6 (0.7)
[ R3 (0.99)
[ R2 (0.62)

, R1 (0.99)
, R2 (0.99)
, R1 (0.99)
, R1 (0.99)
, R2 (0.62)
, R2 (0.71)
, R1 (0.99)

, R1 (0.98)

]

, R1 (0.99)
, R1 (0.79)

]
]
]
]
]
]
]

]
]

Figure 6: Learnt program for the listK task

25

