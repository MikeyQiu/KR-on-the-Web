Cold-Start Aware User and Product Attention
for Sentiment Classiﬁcation

Reinald Kim Amplayo and Jihyeok Kim and Sua Sung and Seung-won Hwang
Yonsei University
Seoul, South Korea
{rktamplayo, zizi1532, dormouse, seungwonh}@yonsei.ac.kr

8
1
0
2
 
n
u
J
 
4
1
 
 
]
L
C
.
s
c
[
 
 
1
v
7
0
5
5
0
.
6
0
8
1
:
v
i
X
r
a

Abstract

The use of user/product information in
sentiment analysis is important, especially
for cold-start users/products, whose num-
ber of reviews are very limited. How-
ever, current models do not deal with the
cold-start problem which is typical in re-
view websites. In this paper, we present
Hybrid Contextualized Sentiment Classi-
ﬁer (HCSC), which contains two mod-
ules: (1) a fast word encoder that returns
word vectors embedded with short and
long range dependency features; and (2)
Cold-Start Aware Attention (CSAA), an
attention mechanism that considers the ex-
istence of cold-start problem when atten-
tively pooling the encoded word vectors.
HCSC introduces shared vectors that are
constructed from similar users/products,
and are used when the original distinct
vectors do not have sufﬁcient informa-
cold-start). This is decided
tion (i.e.
by a frequency-guided selective gate vec-
tor. Our experiments show that in terms
of RMSE, HCSC performs signiﬁcantly
better when compared with on famous
datasets, despite having less complexity,
and thus can be trained much faster. More
importantly, our model performs signiﬁ-
cantly better than previous models when
the training data is sparse and has cold-
start problems.

1

Introduction

Sentiment classiﬁcation is the fundamental task of
sentiment analysis (Pang et al., 2002), where we
are to classify the sentiment of a given text. It is
widely used on online review websites as they con-
tain huge amounts of review data that can be clas-

Figure 1: Conceptual schema of HCSC applied to
users. The same idea can be applied to products.

siﬁed a sentiment. In these websites, a sentiment is
usually represented as an intensity (e.g. 4 out of 5).
The reviews are written by users who have bought
a product. Recently, sentiment analysis research
has focused on personalization (Zhang, 2015) to
recommend product to users, and vise versa.

To this end, many have used user and prod-
uct information not only to develop personaliza-
tion but also to improve the performance of the
classiﬁcation model (Tang et al., 2015). Indeed,
these information are important in two ways. First,
some expressions are user-speciﬁc for a certain
sentiment intensity. For example, the phrase “very
salty” may have different sentiments for a person
who likes salty food and a person who likes oth-
erwise. This is also apparent in terms of prod-
ucts. Second, these additional contexts help mit-
igate data sparsity and cold-start problems. Cold-
start is a problem when the model cannot draw
useful information from users/products where data
is insufﬁcient. User and product information can
help by introducing a frequent user/product with
similar attributes to the cold-start user/product.

Thanks to the promising results of deep neu-
ral networks to the sentiment classiﬁcation task

(Glorot et al., 2011; Tang et al., 2014), more re-
cent models incorporate user and product informa-
tion to convolutional neural networks (Tang et al.,
2015) and deep memory networks (Dou, 2017),
and have shown signiﬁcant improvements. The
current state-of-the-art model, NSC (Chen et al.,
2016a), introduced an attention mechanism called
UPA which is based on user and product infor-
mation and applied this to a hierarchical LSTM.
The main problem with current models is that they
use user and product information naively as an
ordinary additional context, not considering the
possible existence of cold-start problems. This
makes NSC more problematic than helpful in re-
ality since majority of the users in review websites
have very few number of reviews.

To this end, we propose the idea shown in Fig-
ure 1.
If the
It can be described as follows:
model does not have enough information to cre-
ate a user/product vector, then we use a vector
computed from other user/product vectors that are
similar. We introduce a new model called Hy-
brid Contextualized Sentiment Classiﬁer (HCSC),
which consists of two modules. First, we build a
fast yet effective word encoder that accepts word
vectors and outputs new encoded vectors that are
contextualized with short- and long-range con-
texts. Second, we combine these vectors into one
pooled vector through a novel attention mecha-
nism called Cold-Start Aware Attention (CSAA).
The CSAA mechanism has three components:
(a) a user/product-speciﬁc distinct vector derived
from the original user/product information of the
review, (b) a user/product-speciﬁc shared vec-
tor derived from other users/products, and (c)
a frequency-guided selective gate which decides
which vector to use. Multiple experiments are
conducted with the following results: In the orig-
inal non-sparse datasets, our model performs sig-
niﬁcantly better than the previous state-of-the-art,
NSC, in terms of RMSE, despite being less com-
plex. In the sparse datasets, HCSC performs sig-
niﬁcantly better than previous competing models.

2 Related work

Previous studies have shown that using additional
contexts for sentiment classiﬁcation helps improve
the performance of the classiﬁer. We survey sev-
eral competing baseline models that use user and
product information and other models using other
kinds of additional context.

Baselines: Models with user and product infor-
mation User and product information are help-
ful to improve the performance of a sentiment
classiﬁer. This argument was veriﬁed by Tang
et al. (2015) through the observation at the con-
sistency between user/product
information and
the sentiments and expressions found in the text.
Listed below are the following models that employ
user and product information:

• JMARS (Diao et al., 2014) jointly models
the aspects, ratings, and sentiments of a re-
view while considering the user and product
information using collaborative ﬁltering and
topic modeling techniques.

• UPNN (Tang et al., 2015) uses a CNN-based
classiﬁer and extends it to incorporate user-
and product-speciﬁc text preference matrix
in the word level which modiﬁes the word
meaning.

• TLFM+PRC (Song et al., 2017) is a text-
driven latent factor model that uniﬁes user-
and product-speciﬁc latent factor models rep-
resented using the consistency assumption by
Tang et al. (2015).

• UPDMN (Dou, 2017) uses an LSTM clas-
siﬁer as the document encoder and modi-
ﬁes the encoded vector using a deep mem-
ory network with other documents of the
user/product as the memory.

• TUPCNN (Chen et al., 2016b) extends the
CNN-based classiﬁer by adding temporal
user and product embeddings, which are ob-
tained from a sequential model and learned
through the temporal order of reviews.

• NSC (Chen et al., 2016a) is the current state-
of-the-art model that utilizes a hierarchical
LSTM model (Yang et al., 2016) and incor-
porates user and product information in the
attention mechanism.

Models with other additional contexts Other
additional contexts used previously are spatial
(Yang et al., 2017) and temporal (Fukuhara et al.,
2007) features which help contextualize the sen-
timent based on the location where and the time
Inferred contexts were
when the text is written.
also used as additional contexts for sentiment clas-
siﬁers, such as latent topics (Lin and He, 2009)
and aspects (Jo and Oh, 2011) from a topic model,
argumentation features (Wachsmuth et al., 2015),
and more recently, latent review clusters (Am-
playo and Hwang, 2017). These additional con-

Figure 2: Full architecture of HCSC, which consists of the Hybrid Contextualized Word Encoder (middle), and
user-speciﬁc (left) and product-speciﬁc (right) Cold-Start Aware Attention (CSAA).

texts were especially useful when data is sparse,
i.e. number of instances is small or there exists
cold-start entities.

Our model differs from the baseline models
mainly because we consider the possible exis-
tence of the data sparsity problem. Through this,
we are able to construct more effective models
that are comparably powerful yet more efﬁcient
complexity-wise than the state-of-the-art, and are
better when the training data is sparse. Ultimately,
our goal is to demonstrate that, similar to other
additional contexts, user and product information
can be used to effectively mitigate the problem
caused by cold-start users and products.

3 Our model

In this section, we present our model, Hybrid
Contextualized Sentiment Classiﬁer (HCSC)1
which consists of a fast hybrid contextualized
word encoder and an attention mechanism called
Cold-Start Aware Attention (CSAA). The word
encoder returns word vectors with both local and
global contexts to cover both short and long range
dependency relationship between words.
The
CSAA then incorporates user and product infor-
mation to the contextualized words through an at-
tention mechanism that considers the possible ex-
istence of cold-start problems. The full architec-
ture of the model is presented in Figure 2. We

1The data and code used in this paper are available here:

https://github.com/rktamplayo/HCSC.

describe the subparts of the model below.

3.1 Hybrid contextualized word encoder

The base model is a word encoder that transforms
vectors of words {wi} in the text to new word vec-
tors. In this paper, we present a fast yet very effec-
tive word encoder based on two different off-the-
shelf classiﬁers.

The ﬁrst part of HCWE is based on a CNN
model which is widely used in text classiﬁcation
(Kim, 2014). This encoder contextualizes words
based on local context words to capture short
range relationships between words. Speciﬁcally,
we do the convolution operation using ﬁlter matri-
ces Wf ∈ Rh×d with ﬁlter size h to a window of
h words. We do this for different sizes of h. This
produces new feature vectors ci,h as shown below,
where f (.) is a non-linear function:

ci,h = f ([wi−(h−1)/2; ...; wi+(h−1)/2](cid:62)Wf + bf )

The convolution operation reduces the number
of words differently depending on the ﬁlter size
h. To prevent loss of information and to pro-
duce the same amount of feature vectors ci,h, we
pad the texts dynamically such that when the ﬁl-
ter size is h, the number of paddings on each side
is (h − 1)/2. This requires the ﬁlter sizes to be
odd numbers. Finally, we concatenate all feature
vectors of different h’s for each i as the new word
vector:

wcnni = [ci,h1; ci,h2; ...]

The second part of HCWE is based on an RNN
model which is used when texts are longer and in-
clude word dependencies that may not be captured
by the CNN model. Speciﬁcally, we use a bidi-
rectional LSTM and concatenate the forward and
backward hidden state vectors as the new word
vector, as shown below:

−→
h i = LST M (wi,
←−
h i = LST M (wi,
−→
h i;

←−
h i]

wrnni = [

−→
h i−1)
←−
h i+1)

The answer to the question whether to use lo-
cal or global context to encode words for senti-
ment classiﬁcation is still unclear, and both CNN
and RNN models have previous empirical evi-
dence that they perform better than the other (Kim,
2014; McCann et al., 2017). We believe that both
short and long range relationships, captured by
CNN and RNN respectively, are useful for senti-
ment classiﬁcation. There are already previous at-
tempts to intricately combine both CNN and RNN
(Zhou et al., 2016), resulting to a slower model.
On the other hand, HCWE resorts to combine
them by simply concatenating the word vectors
encoded from both CNN and RNN encoders, i.e.
wi = [wcnni; wrnni]. This straightforward yet
fast alternative outputs a word vector with seman-
tics contextualized from both local and global con-
texts. Moreover, they perform as well as complex
hierarchical structured models (Yang et al., 2016;
Chen et al., 2016a) which train very slow.

3.2 Cold-start aware attention

Incorporating the user and product information
of the text as context vectors u and p to atten-
tively pool the word vectors, i.e. e(wi, u, p) =
v(cid:62)tanh(Wwwi + Wuu + Wpp + b), has been
proven to improve the performance of sentiment
classiﬁers (Chen et al., 2016a). However, this
method assumes that the user and product vectors
are always present. This is not the case in real
world settings where a user/product may be new
and has just got its ﬁrst review. In this case, the
vectors u and p are rendered useless and may also
contain noisy signals that decrease the overall per-
formance of the models.

To this end, we present an attention mecha-
nism called Cold-Start Aware Attention (CSAA).
CSAA operates on the idea that a cold-start
user/product can use the information of other sim-

ilar users/products with sufﬁcient number of re-
views. CSAA separates the construction of pooled
vectors for user and for product, unlike previ-
ous methods that use both user/product informa-
tion to create a single pooled vector. Construct-
ing a user/product-speciﬁc pooled vector consists
of three parts: the distinct pooled vector created
using the original user/product, the shared pooled
vector created using similar users/products, and
the selective gate to select between the distinct
and shared vectors. Finally, the user- and product-
speciﬁc pooled vectors are combined into one ﬁnal
pooled vector.

In the following paragraphs, we discuss the
step-by-step process on how the user-speciﬁc
pooled vector is constructed. A similar process is
done to construct the product-speciﬁc pooled vec-
tor, but is not presented here for conciseness.

The user-speciﬁc distinct pooled vector vd
u is
created using a method similar to the additive at-
tention mechanism (Bahdanau et al., 2014), i.e.
vd
u = att({wi}, u), where the context vector is the
distinct vector of user u, as shown in the equation
below. An equivalent method is used to create the
distinct product-speciﬁc pooled vector vd
p.

u(wi, u) = vd(cid:62)tanh(W d
ed

wwi + W d

u u + bd)

ad
ui =

vd
u =

u(wi, u))

u(wj, u))

exp(ed
j exp(ed
ad
ui × wi

(cid:80)

(cid:88)

i

The user-speciﬁc shared pooled vector vs
u is cre-
ated using the same method above, but using a
shared context vector u(cid:48). The shared context vec-
tor u(cid:48) is constructed using the vectors of other
users and weighted based on a similarity weight.
Similarity is deﬁned as how similar the word us-
ages of two users are. This means that if a user
uk uses words similarly to the word usage of the
original user u, then uk receives a high similarity
weight. The similarity weight as
uk is calculated as
the softmax of the product of µ({wi}) and uk with
a project matrix in the middle, where µ({wi}) is
the average of the word vectors. The similarity
weights are used to create u(cid:48), as shown below.
Similar method is used for the shared product-
speciﬁc pooled vector vs
p.

u(µ({wi}), uk) = µ({wi})W s
es

uuk
u(wi, uk))

u(wi, uj))

exp(es
j exp(es
as
uk

× uk

as
uk

=

(cid:80)

(cid:88)

u(cid:48) =

k

u = att({wi}, u(cid:48))
vs

u and vs

We select between the user-speciﬁc distinct and
shared pooled vector, vd
u, into one user-
speciﬁc pooled vector vu through a gate vector gu.
The vector gu should put more weight to the dis-
tinct vector when user u is not cold-start and to
the shared vector when u is otherwise. We use a
frequency-guided selective gate that utilizes the
frequency, i.e. the number of reviews user u has
written. The challenge is that we do not know how
many reviews should be considered cold-start or
not. This is automatically learned through a two-
parameter Weibull cumulative distribution where
given the review frequency of the user f (u), a
learned shape vector ku and a learned scale vector
λu, a probability vector is sampled and is used as
the gate vector gu to create vu, according to the
equation below. We normalized f (u) by divid-
ing it to the average user review frequency. The
relu function ensures that both ku and λu are non-
negative vectors. The ﬁnal product-speciﬁc pooled
vector vp is created in a similar manner.

(cid:18)

(cid:16) f (u)

(cid:17)relu(ku)(cid:19)

gu = 1 − exp

−

vu = gu × vd

relu(λu)
u + (1 − gu) × vs
u

Finally, we combine both the user- and product-
speciﬁc pooled vector, vu and vp, into one pooled
vector vup. This is done by using a gate vector
gup created using a sigmoidal transformation of
the concatenation of vu and vp, as illustrated in
the equation below.

gup = σ(Wg[vu; vp] + bg)
vup = gup × vu + (1 − gup) × vp

We note that our attention mechanism can be
applied to any word encoders, including the basic
bag of words (BoW) to more recent models such
as CNN and RNN. Later (in Section 4.2), we show
that CSAA improves the performance of simpler
models greatly.

3.3 Training objective

Normally, a sentiment classiﬁer transforms the ﬁ-
nal vector vup, usually in a linear fashion, into a
vector with a dimension equivalent to the num-
ber of classes C. A softmax layer is then used to
obtain a probability distribution y(cid:48) over the senti-
ment classes. Finally, the full model uses a cross-
entropy over all training documents D as objec-
tive function L during training, where y is the gold
probability distribution:

y(cid:48) = sof tmax(W vup + b)
(cid:88)

(cid:88)

L = −

y(d)
c

· log(y(cid:48)(d)

)

c

d∈D

c∈C

u, vs

u, vd

However, HCSC has a nice architecture
which can be used to improve the train-
It contains seven pooled vectors V =
ing.
{vd
p, vs
p, vu, vp, vup} that are essentially in
the same vector space. This is because these
vectors are created using weighted sums of ei-
ther the encoded word vectors through attention
or the parent pooled vectors through the selective
gates. Therefore, we can train separate classiﬁers
for each pooled vectors using the same parame-
ters W and b. Speciﬁcally, for each v ∈ V, we
calculate the loss Lv using the above formulas.
The ﬁnal loss is then the sum of all the losses, i.e.
L = (cid:80)

v∈V Lv.

4 Experiments

In this section, we present our experiments and
the corresponding results. We use the models de-
scribed in Section 2 as baseline models: JMARS
(Diao et al., 2014), UPNN (Tang et al., 2015),
TLFM+PRC (Song et al., 2017), UPDMN (Dou,
2017), TUPCNN (Chen et al., 2016b), and NSC
(Chen et al., 2016a), where NSC is the model with
state-of-the-art results.

4.1 Experimental settings

Implementation We set the size of the word,
user, and product vectors to 300 dimensions. We
use pre-trained GloVe embeddings2 (Pennington
et al., 2014) to initialize our word vectors. We
simply set the parameters for both BiLSTMs and
CNN to produce an output with 300 dimensions:
For the BiLSTMs, we set the state sizes of the
LSTMs to 75 dimensions, for a total of 150 dimen-
sions. For CNN, we set h = 3, 5, 7, each with 50

2https://nlp.stanford.edu/projects/

glove/

Datasets

Classes

IMDB
Yelp 2013

IMDB
Yelp 2013

10
5

10
5

Datasets

Classes

#docs
67426
62522

#docs
44261
38687

Train
#users
1310
1631
Sparse20
#users
1042
1301

#prods
1635
1633

#prods
1323
1288

#docs
8381
7773

#docs
17963
16058

Dev
#users
1310
1631
Sparse50
#users
659
818

#prods
1574
1559

#prods
840
823

#docs
9112
8671

#docs
2450
2406

Test
#users
1310
1631
Sparse80
#users
250
352

#prods
1578
1577

#prods
312
304

Table 1: Dataset statistics

feature maps, for a total of 150 dimensions. These
two are concatenated to create a 300-dimension
encoded word vectors. We use dropout (Srivastava
et al., 2014) on all non-linear connections with a
dropout rate of 0.5. We set the batch size to 32.
Training is done via stochastic gradient descent
over shufﬂed mini-batches with the Adadelta up-
date rule (Zeiler, 2012), with l2 constraint (Hinton
et al., 2012) of 3. We perform early stopping using
a subset of the given development dataset. Train-
ing and experiments are all done using a NVIDIA
GeForce GTX 1080 Ti graphics card.

Additionally, we also implement two versions
of our model where the word encoder is a sub-
(a) the CNN-based model
part of HCSC, i.e.
(CNN+CSAA) and (b) the RNN-based model
(RNN+CSAA). For the CNN-based model, we
use 100 feature maps for each of the ﬁlter sizes
h = 3, 5, 7, for a total of 300 dimensions. For the
RNN-based model, we set the state sizes of the
LSTMs to 150, for a total of 300 dimensions.

Datasets and evaluation We evaluate and com-
pare our models with other competing models
using two widely used sentiment classiﬁcation
datasets with available user and product informa-
tion:
IMDB and Yelp 2013. Both datasets are
curated by Tang et al. (2015), where they are di-
vided into train, dev, and test sets using a 8:1:1 ra-
tio, and are tokenized and sentence-splitted using
Stanford CoreNLP (Manning et al., 2014). In ad-
dition, we create three subsets of the train dataset
to test the robustness of the models on sparse
datasets. To create these datasets, we randomly re-
move all the reviews of x% of all users and prod-
ucts, where x = 20, 50, 80. These datasets are
not only more sparse than the original datasets,
but also have smaller number of users and prod-
ucts, introducing cold-start users and products. All
datasets are summarized in Table 1. Evaluation is
done using two metrics: the Accuracy which mea-
sures the overall sentiment classiﬁcation perfor-
mance and the RMSE which measures the diver-

IMDB

Yelp 2013

Models

JMARS
UPNN
TLFM+PRC
UPDMN
TUPCNN
NSC

Acc.
-
0.435∗
-
0.465∗
0.488∗
0.533
CNN+CSAA 0.522∗
RNN+CSAA 0.527∗
0.542

HCSC

RMSE
1.773∗
1.602∗
1.352∗
1.351∗
1.451∗
1.281∗
1.256∗
1.237∗
1.213

Acc.
-
0.596∗
-
0.639∗
0.639∗
0.650
0.654
0.654
0.657

RMSE
0.985∗
0.784∗
0.716∗
0.662
0.694∗
0.692∗
0.665
0.667
0.660

Table 2: Accuracy and RMSE values of competing
models on the original non-sparse datasets. An aster-
isk indicates that HCSC is signiﬁcantly better than the
model (p < 0.01).

gence between predicted and ground truth classes.
We notice very minimal differences among perfor-
mances of different runs.

4.2 Comparisons on original datasets

We report
the results on the original datasets
in Table 2. On both datasets, HCSC outper-
forms all previous models based on both accuracy
and RMSE. Based on accuracy, HCSC performs
signiﬁcantly better than all previous models ex-
cept NSC, where it performs slightly better with
0.9% and 0.7% increase on IMDB and Yelp 2013
datasets. Based on RMSE, HCSC performs sig-
niﬁcantly better than all previous models, except
when compared with UPDMN on the Yelp 2013
datasets, where it performs slightly better. We note
that RMSE is a better metric because it measures
how close the wrongly predicted sentiment and the
ground truth sentiment are. Although NSC per-
forms as well as HCSC based on accuracy, it per-
forms worse based on RMSE, which means that its
predictions deviate far from the original sentiment.
It is also interesting to note that when CSAA
is used as attentive pooling, both simple CNN
and RNN models perform just as well as NSC,
despite NSC being very complex and model-
ing the documents with compositionality (Chen
et al., 2016a). This is especially true when com-

Models
NSC(LA)
NSC
CNN+CSAA
RNN+CSAA
HCSC

Sparse20
0.469
0.497
0.497
0.505
0.505

Sparse50
0.428
0.408
0.444
0.455
0.456

Models
NSC(LA)
NSC
CNN+CSAA
RNN+CSAA
HCSC

(a) IMDB Datasets
Sparse20
0.624
0.626
0.626
0.633
0.636

Sparse50
0.590
0.592
0.605
0.603
0.608

(b) Yelp 2013 Datasets

Sparse80
0.309
0.292
0.343
0.364
0.368

Sparse80
0.523
0.511
0.522
0.527
0.538

Table 3: Accuracy values of competing models when
the training data used is sparse. Bold-faced values are
the best accuracies in the column, while red values are
accuracies worse than NSC(LA).

pared using RMSE, where both CNN+CSAA and
RNN+CSAA perform signiﬁcantly better (p <
0.01) than NSC. This proves that CSAA is an ef-
fective use of the user and product information for
sentiment classiﬁcation.

4.3 Comparisons on sparse datasets

Table 3 shows the accuracy of NSC (Chen
et al., 2016a) and our models CNN+CSAA,
RNN+CSAA, and HCSC on the sparse datasets.
As shown in the table, on all datasets with dif-
ferent levels of sparsity, HCSC performs the best
among the competing models. The difference be-
tween the accuracy of HCSC and NSC increases as
the level of sparsity intensiﬁes: While the HCSC
only gains 0.8% and 1.0% over NSC on the less
sparse Sparse20 IMDB and Yelp 2013 datasets, it
improves over NSC signiﬁcantly with 7.6% and
2.7% increase on the more sparse Sparse80 IMDB
and Yelp 2013 datasets, respectively.

We also run our experiments using NSC with-
out user and product information, i.e. NSC(LA)
which reduces the model into a hierarchical LSTM
model (Yang et al., 2016). Results show that
although the use of user and product informa-
tion in NSC improves the model on less sparse
datasets (as also shown in the original paper (Chen
et al., 2016a)), it decreases the performance of the
model on more sparse datasets: It performs 2.0%,
1.7%, and 1.2% worse than NSC(LA) on Sparse50
IMDB, Sparse80 IMDB, and Sparse80 Yelp 2013
datasets. We argue that this is because NSC does
not consider the existence of cold-start problems,
which makes the additional user and product in-

Figure 3: Accuracy per user/product review frequency
on both datasets. The review frequency value f repre-
sents the frequencies in the range [f, f + 10), except
when f = 100, which represents the frequencies in the
range [f, ∞).

formation more noisy than helpful.

5 Analysis

In this section, we show further interesting anal-
yses of the properties of HCSC. We use the
Sparse50 datasets and the corresponding results of
several models as the experimental data.

Performance per review frequency We in-
vestigate the performance of the model over
users/products with different number of reviews.
Figure 3 shows plots of accuracy of both NSC and
HCSC over (a) different user review frequency on
IMDB dataset and (b) different product review fre-
quency on Yelp 2013 dataset. On both plots, we
observe that when the review frequency is small,
the performance gain of HCSC over NSC is very
large. However, as the review frequency becomes
larger, the performance gain of HCSC over NSC
decreases to a very marginal increase. This means
that HCSC ﬁnds its improvements over NSC from
cold-start users and products, in which NSC does
not consider explicitly.

How few is cold-start? One intriguing question
is when do we say that a user/product is cold-
start or not. Obviously, users/products with no
previous reviews at all should be considered cold-
start, however the cut-off point between cold-start
and non-cold-start entities is vague. Although we

Figure 4: Visualization of attention and gate values of two examples from the Yelp 2013 dataset. Example 2 is
truncated, leaving only the important parts. Gate values g’s are the average of the values in the original gate vector.

Models
NSC

IMDB
7331

CNN+CSAA 256 (28.6x)
RNN+CSAA
968 (7.6x)
1110 (6.6x)
HCSC

Yelp 2013
6569
146 (45.0x)
561 (11.7x)
615 (10.7x)

Table 4: Time (in seconds) to process the ﬁrst 100
batches of competing models for each dataset. The
numbers in the parenthesis are the speedup of time
when compared to NSC.

user/product vectors, and distinct/shared vectors
work.
In the ﬁrst example, both user and prod-
uct are cold-start. The user distinct vector focuses
its attention to wrong words, since it is not able
to use any useful information from the user at all.
In this case, HCSC uses the user shared vector by
using a gate vector gu = 0. The user shared vec-
tor correctly attends to important words such as
fresh, baked, soft, and pretzels. In the second ex-
ample, both user and product are not cold-start. In
this case, the distinct vectors are used almost en-
tirely by setting the gates close to 1. Still, the cor-
responding shared vectors are similar to the dis-
tinct vectors, proving that HCSC is able to create
useful user/product-speciﬁc context from similar
users/products. Finally, we look at the differing at-
tention values of users and products. We observe
that user vectors focus on words that describe the
product or express their emotions (e.g. fresh and
enjoyed). On the other hand, product vectors focus
more on words pertaining to the products/services
(e.g. pretzels and waitress).

Figure 5: Graph of the user/product-speciﬁc Weibull
cumulative distribution on both datasets.

cannot provide an exact answer to this question,
HCSC is able to provide a nice visualization by re-
ducing the shape and scale vectors, k and λ, of the
frequency-guided selective gate into their averages
and draw a Weibull cumulative distribution graph,
as shown in Figure 5. The ﬁgure provides us these
observations: First, users have a more lenient cold-
start cut-off point compared to products; in the
IMDB dataset, a user only needs approximately at
least ﬁve reviews to use at least 80% of its own in-
formation (i.e. distinct vector). On the other hand,
products tend to need more reviews to be consid-
ered sufﬁcient and not cold start; in the IMDB
dataset, a product needs approximately 40 reviews
to use at least 80% of its own information. This
explains the marginal increase in performance of
previous models when only product information is
used as additional context, as reported by previous
papers (Tang et al., 2015; Chen et al., 2016a).

On the different pooled vectors We visualize
the attention and gate values of two example re-
sults from HCSC in Figure 4 to investigate on how

On the time complexity of models Finally,
we report
the time in seconds to run 100
batches of data of the models NSC, CNN+CSAA,

RNN+CSAA, and HCSC in Figure 4. NSC takes
too long to train, needing at least 6500 seconds
to process 100 batches of data. This is because it
uses two non-parallelizable LSTMs on top of each
other. Our models, on the other hand, only use
one (or none in the case of CNN+CSAA) level of
BiLSTM. This results to at least 6.6x speedup on
the IMDB datasets, and at least 10.7x speedup on
the Yelp 2013 datasets. This means that HCSC
does not sacriﬁce a lot of time complexity to ob-
tain better results.

6 Conclusion

We propose Hybrid Contextualized Sentiment
Classiﬁer (HCSC) with a fast word encoder which
contextualizes words to contain both short and
long range word dependency features, and an at-
tention mechanism called Cold-start Aware Atten-
tion (CSAA) which considers the existence of the
cold-start problem among users and products by
using a shared vector and a frequency-guided se-
lective gate, in addition to the original distinct vec-
tor. Our experimental results show that our model
performs signiﬁcantly better than previous mod-
els. These improvements increase when the level
of sparsity in data increases, which conﬁrm that
HCSC is able to deal with the cold-start problem.

Acknowledgements

This work was supported by Microsoft Re-
search Asia and the ICT R&D program of
MSIT/IITP. [2017-0-01778, Development of Ex-
plainable Human-level Deep Machine Learning
Inference Framework]

References

Reinald Kim Amplayo and Seung-won Hwang. 2017.
Aspect sentiment model for micro reviews.
In
2017 IEEE International Conference on Data Min-
ing (ICDM). IEEE, pages 727–732.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Neural machine translation by
CoRR

Bengio. 2014.
jointly learning to align and translate.
abs/1409.0473.

Huimin Chen, Maosong Sun, Cunchao Tu, Yankai Lin,
and Zhiyuan Liu. 2016a. Neural sentiment classiﬁ-
cation with user and product attention. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing. pages 1650–1659.

Tao Chen, Ruifeng Xu, Yulan He, Yunqing Xia, and
Xuan Wang. 2016b. Learning user and product

distributed representations using a sequence model
for sentiment analysis. IEEE Computational Intelli-
gence Magazine 11(3):34–44.

Qiming Diao, Minghui Qiu, Chao-Yuan Wu, Alexan-
der J Smola, Jing Jiang, and Chong Wang. 2014.
Jointly modeling aspects, ratings and sentiments for
movie recommendation (jmars). In Proceedings of
the 20th ACM SIGKDD international conference on
Knowledge discovery and data mining. ACM, pages
193–202.

Zi-Yi Dou. 2017. Capturing user and product informa-
tion for document level sentiment analysis with deep
memory network. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing. pages 521–526.

Tomohiro Fukuhara, Hiroshi Nakagawa, and Toyoaki
Nishida. 2007. Understanding sentiment of people
from news articles: Temporal sentiment analysis of
social events. In ICWSM.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
In Pro-
classiﬁcation: A deep learning approach.
ceedings of the 28th international conference on ma-
chine learning (ICML-11). pages 513–520.

Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
dinov. 2012.
Improving neural networks by
preventing co-adaptation of feature detectors. CoRR
abs/1207.0580.

Yohan Jo and Alice H Oh. 2011. Aspect and sentiment
uniﬁcation model for online review analysis. In Pro-
ceedings of the fourth ACM international conference
on Web search and data mining. ACM, pages 815–
824.

Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. In Proceedings of the 2014
conference on empirical methods in natural lan-
guage processing (EMNLP).

Chenghua Lin and Yulan He. 2009.

Joint senti-
In Pro-
ment/topic model for sentiment analysis.
ceedings of the 18th ACM conference on Informa-
tion and knowledge management. ACM, pages 375–
384.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The stanford corenlp natural language pro-
In Proceedings of 52nd annual
cessing toolkit.
meeting of the association for computational lin-
guistics: system demonstrations. pages 55–60.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In Advances in Neural In-
formation Processing Systems. pages 6297–6308.

Yongfeng Zhang. 2015.

Incorporating phrase-level
sentiment analysis on textual reviews for personal-
ized recommendation. In Proceedings of the eighth
ACM international conference on web search and
data mining. ACM, pages 435–440.

Peng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu,
Hongyun Bao, and Bo Xu. 2016. Text classiﬁcation
improved by integrating bidirectional lstm with two-
dimensional max pooling. In Proceedings of COL-
ING 2016, the 26th International Conference on
Computational Linguistics: Technical Papers. pages
3485–3495.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classiﬁcation using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natu-
ral language processing-Volume 10. Association for
Computational Linguistics, pages 79–86.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP). pages 1532–1543.

Kaisong Song, Wei Gao, Shi Feng, Daling Wang, Kam-
Fai Wong, and Chengqi Zhang. 2017. Recommen-
dation vs sentiment analysis: a text-driven latent fac-
tor model for rating prediction with cold-start aware-
ness. In Proceedings of the 26th International Joint
Conference on Artiﬁcial Intelligence. AAAI Press,
pages 2744–2750.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. The Journal of Machine Learning
Research 15(1):1929–1958.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Learning
semantic representations of users and products for
document level sentiment classiﬁcation. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers). volume 1, pages
1014–1023.

Duyu Tang, Furu Wei, Bing Qin, Ting Liu, and Ming
Zhou. 2014. Coooolll: A deep learning system for
In Proceedings of
twitter sentiment classiﬁcation.
the 8th International Workshop on Semantic Evalu-
ation (SemEval 2014). pages 208–212.

Henning Wachsmuth, Johannes Kiesel, and Benno
Stein. 2015. Sentiment ﬂow-a general model of web
review argumentation. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing. pages 601–611.

Min Yang, Jincheng Mei, Heng Ji, Zhou Zhao, Xiao-
jun Chen, et al. 2017. Identifying and tracking sen-
timents and topics from social media texts during
natural disasters. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing. pages 527–533.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classiﬁcation.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
pages 1480–1489.

Matthew D. Zeiler. 2012. Adadelta: An adaptive learn-

ing rate method. CoRR abs/1212.5701.

