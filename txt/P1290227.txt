Recurrent Calibration Network for Irregular Text Recognition

Yunze Gao1
Zhen Lei1

Yingying Chen1
Xiao-Yu Zhang2

Jinqiao Wang1
Hanqing Lu1

1National Laboratory of Pattern Recognition, Institute of Automation,
Chinese Academy of Sciences, Beijing, China
2Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China
{yunze.gao, yingying.chen, jqwang, zlei, luhq}@nlpr.ia.ac.cn
zhangxiaoyu@iie.ac.cn

8
1
0
2
 
c
e
D
 
8
1
 
 
]

V
C
.
s
c
[
 
 
1
v
5
4
1
7
0
.
2
1
8
1
:
v
i
X
r
a

Abstract

Scene text recognition has received increased attention
in the research community. Text in the wild often pos-
sesses irregular arrangements, typically including perspec-
tive text, curved text, oriented text. Most existing meth-
ods are hard to work well for irregular text, especially for
severely distorted text. In this paper, we propose a novel
Recurrent Calibration Network (RCN) for irregular scene
text recognition. The RCN progressively calibrates the ir-
regular text to boost the recognition performance. By de-
composing the calibration process into multiple steps, the
irregular text can be calibrated to normal one step by step.
Besides, in order to avoid the accumulation of lost informa-
tion caused by inaccurate transformation, we further design
a ﬁducial-point reﬁnement structure to keep the integrity of
text during the recurrent process. Instead of the calibrated
images, the coordinates of ﬁducial points are tracked and
reﬁned, which implicitly models the transformation infor-
mation. Based on the reﬁned ﬁducial points, we estimate
the transformation parameters and sample from the origi-
nal image at each step. In this way, the original charac-
ter information is preserved until the ﬁnal transformation.
Such designs lead to optimal calibration results to boost the
performance of succeeding recognition. Extensive experi-
ments on challenging datasets demonstrate the superiority
of our method, especially on irregular benchmarks.

1. Introduction

Scene text recognition has drawn remarkable attention in
computer vision due to its importance in various real world
applications, such as scene understanding, card information
entry, street sign reading and so on. Beneﬁting from recent
advancements of deep learning, reading text in natural im-
ages has experienced a rapid evolution during the past few

Figure 1. Examples of irregular text in natural images.

years. Despite signiﬁcant advances, scene text recognition
in unconstrained conditions still remains as a challenging
problem due to the complex situations such as blurring, dis-
tortion, orientation and uneven lighting.

Irregular text frequently appears in natural scenes, ow-
ing to curved character placement, perspective distortion,
etc., as shown in Figure 1. Recognizing text with arbi-
trary shapes is an extremely difﬁcult task because of unpre-
dictable changeful text layouts. Most existing approaches
mainly focus on regular text recognition, which are difﬁcult
to be generalized to distorted text. Recently, some attempts
have been made towards irregular text recognition. Yang et
al. [33] utilized a 2D attention mechanism to focus on each
character and introduced an auxiliary dense character de-
tection task to encourage the learning of text-speciﬁc visual
representations. However, this method used an exhausting
multi-task learning strategy and the inaccurate attention re-
gions will cause recognition errors. Cheng et al. [5] pro-
posed the arbitrary orientation network (AON) to extract
scene text features in four directions and adopted a weight-
ing mechanism to combine the four feature sequences of
different directions.
In order to extract features with the
same dimension in four directions, AON has to resize the
text image into a square shape. However, scene text gen-
erally has various aspect ratios, the strategy of scaling to
a square will severely destroy the aspect ratio of the text
line, especially for long text. Shi et al.
[29] applied a

1

Figure 2. Overview of the Recurrent Calibration Network (RCN) for irregular text recognition. Our RCN recurrently reﬁnes the coordinates
of ﬁducial points and transforms the original input image. The dotted lines represent the coordinates delivery.

spatial transformation prior to recognition to transform the
input image and rectify the text in it. The Spatial Trans-
former Network (STN) framework with Thin-Plate Spline
(TPS) transformation is utilized to perform text rectiﬁca-
tion. Although [29] has shown impressive results on ir-
regular benchmarks, we observe that the rectiﬁed images
may still have distortions or lose some character informa-
tion, especially for severely distorted text, which results in
mistaken recognition results.

In this paper, we design a novel Recurrent Calibration
Network (RCN) to progressively calibrate the irregular text
to boost the recognition performance. The recurrent struc-
ture iteratively reﬁnes the geometric transformation of ir-
In each
regular text under the same parametric capacity.
iteration, the residual between the previous and current geo-
metric transformation ﬁelds is estimated based on the previ-
ously calibrated image to get one step closer to the optimal
one. In this way, the difﬁculty of each step is intrinsically
relieved and the severe distortions can be eliminated in a
progressive manner. Therefore, such design is capable of
effectively improving the robustness of the model to large
variations of text. Besides, we observe that spatial transfor-
mation on output image of the previous step cannot restore
the missing character information, and the incomplete ap-
pearances will cause recognition errors as well. Therefore,
we elaborate a ﬁducial-point reﬁnement structure to keep
the integrity of text during the recurrent process. Instead of
the calibrated images, the coordinates of ﬁducial points are
tracked and transmitted during multiple iterations. At each
step, the localization network predicts the coordinate offsets
with respect to the previous positions, which implicitly re-
ﬂects the residual spatial transformation. Furthermore, we
map the coordinates of ﬁducial points back to the original
input image and sample from the original. In this way, while
the coordinates fall outside the calibrated image, mapping
back to the original means compensating some missing in-
formation. Our method can effectively calibrate the irregu-

lar text while preserving the original character information
in multiple calibrations. The calibration network is jointly
optimized with the recognition network under the same ob-
jective in an end-to-end scheme. Therefore, our RCN can
automatically learn the optimal transformation for the fol-
lowing recognition task.

The main contributions are summarized as follows:
(1) We propose a Recurrent Calibration Network (RCN)
to progressively calibrate the irregular text to boost the
recognition performance.

(2) We design a ﬁducial-point reﬁnement structure to
keep the integrity of text during the recurrent process, which
avoids the accumulation of missing information in the sce-
nario of iterative calibrations.

(3) Our RCN achieves superior performance com-
pared with the state-of-the-art methods on the challenging
datasets, especially on irregular benchmarks.

2. Related Work

Scene text recognition has been widely researched and
numerous methods are proposed in recent years. Tradi-
tional methods recognized scene text in a character-level
manner, which ﬁrst performed detection to generate mul-
tiple candidates of character locations, then applied a char-
acter classiﬁer for recognition. Wang et al. [31] detected
each character by sliding window, and recognized it with
a character classiﬁer trained on the HOG descriptors. Bis-
sacco et al. [3] designed a fully connected network to ex-
tract character feature representations, then used a language
model to recognize characters. However, the performance
of these methods is limited due to the inaccurate charac-
ter detector. To be free from this problem, some methods
directly learned the mapping between the entire word im-
ages and target strings. For example, Jaderberg et al. [13]
assigned a class label to each word in a pre-deﬁned lex-
icon and performed a 90k-class classiﬁcation with CNN.

Rodriguez-Serrano et al.
[26] formulated the scene text
recognition as a retrieval problem, which embedded word
labels and word images into a common Euclidean space and
found the closest word label in this space.

With the successful application of recurrent neural net-
work (RNN) in sequence recognition, some researchers
[27, 10, 28, 19] developed sequence-based methods and
combined convolutional neural network (CNN) and RNN
to encode the feature representations of word images. Shi
et al. [27] and He et al. [10] both used the Connectionist
Temporal Classiﬁcation (CTC) [8] loss to calculate the con-
ditional probabilities between the outputs of RNN and the
target sequences. After that, Shi et al. [28] and Li et al. [19]
introduced an attention mechanism to adaptively weight the
features and select the most relevant feature representations
in RNN-based decoder. In order to eliminate attention drift
problem, Cheng et al.
[4] employed a focusing attention
mechanism to automatically adjust the attention weights.
Bai et al. [2] proposed the edit probability to estimate the
probability of generating a string while considering possible
occurrences of missing or superﬂuous characters. Although
these approaches have shown promising results, they cannot
effectively handle with the irregular text. The main reason
is that word images are encoded into 1D feature sequences,
but the irregular text is not horizontally arranged.

The researches on irregular text recognition are relatively
sparse. Yang et al. [33] adopted a 2D soft attention mech-
anism to focus on individual character at each step and in-
troduced an auxiliary character detection task to learn text-
speciﬁc features. Although this method recognized text in
a 2D space, it used the multi-task learning framework and
needed character-level annotations. Liu et al.
[20] pre-
sented to remove the distortion of scene text by detecting
and rectifying individual character. However, the detection
errors will affect the performance of subsequent rectiﬁca-
tion and recognition. Cheng et al. [5] proposed that the vi-
sual representation of irregular text can be described as the
combination of features in four directions. This approach
is able to effectively capture the deep features of irregular
text, but the strategy of scaling word images to square will
severely destroy the aspect ratio of text lines, especially for
long text. Shi et al. [29] introduced a spatial transforma-
tion network with thin-plate spline (TPS) transformation to
calibrate irregular text into regular one, then recognized the
calibrated text with attention-based framework. Although
considerably improving the performance for irregular text
recognition, it is still difﬁcult to precisely locate the ﬁdu-
cial points which tightly bound the text region, especially
for severely distorted text. This leads to errors in param-
eters estimation of the TPS transformation, and hence the
deformation of scene text.

Different from the existing methods, we design a novel
Recurrent Calibration Network (RCN) to calibrate irregular

text in a progressive manner. The calibration process is de-
composed into multiple steps and the calibration results are
iteratively reﬁned. Different steps work together to elimi-
nate text distortion for better recognition, thus the difﬁculty
of each step is greatly relieved and large distortion also can
be effectively removed.

3. The proposed Approach

The overview of our Recurrent Calibration Network
(RCN) for irregular text recognition is shown in Figure 2.
The irregular text is progressively calibrated to regular one,
which serves as the input of subsequent recognition net-
work. During calibration process, the distortions are elim-
inated step by step and the recurrent framework maintains
the same parametric capacity. Besides, the ﬁducial-point re-
ﬁnement structure transmits and reﬁnes the coordinates of
ﬁducial points during the recurrent process. At each step,
the network ﬁrst predicts the coordinate offsets, then gets
the updated coordinates and projects them into the original
input image. After that, we can estimate the TPS parame-
ters and sample from the original image, which effectively
keeps the integrity of text. RCN is able to deal with dis-
torted text with various orientations and shapes, including
severely distorted text.

3.1. Recurrent Calibration

The spatial transformer [14] is a learnable module which
explicitly allows spatial manipulations. This can be mathe-
matically written as

Iout = Iin(T ), where T = f (Iin)

(1)

The function f is parameterized as a learnable localization
network, which predicts the transform parameters from the
input image. Usually, it is common to calibrate scene text
with spatial transform network.

However, the single-step calibration often fails to fully
remove geometrical distortions, and could lead to text
content loss, making unfavorable effects on the following
recognition. Therefore, we design a recurrent structure to
decompose the calibration process into multiple progressive
steps, which mitigates the difﬁculty of each step. Differ-
ent steps work together to calibrate irregular text for better
recognition. In each iteration, the calibrated result is fur-
ther reﬁned by feeding it into the pipeline again, forming
a recurrent structure. Therefore, we can reﬁne the calibra-
tion results iteratively under the same parametric capacity.
Moreover, we note that the grid generator and the sampler
in STN can be combined to be a single transform function.
Denote the transformation parameters estimation as E, and
the spatial transformation operation as R, we have the fol-
lowing structure:

Tt = E(It)

(2)

Figure 3. The comparison of two recurrent methods, the top and
bottom are the direct iterations structure and ﬁducial-point reﬁne-
ment structure, respectively. (a) The original input images. (b)
The calibrated images in the ﬁrst iteration. (c) The calibrated im-
ages in the second iteration. (d) The calibrated images in the last
iteration. Direct iterations structure discards the information out-
side the calibrated images, while the missing information can be
recovered in the ﬁducial-point reﬁnement structure.

It+1 = R(It, Tt)

(3)

where t represent the t-th iteration, and I0 is the original in-
put image. In this way, large variations can be progressively
handled to match with the succeeding recognition task.

For each transformation, we predict a set of ﬁducial
points and calculate the TPS transformation parameters
based on them. Speciﬁcally, the base ﬁducial points C on
target image Iout are deﬁned to evenly distribute along the
top and bottom image borders, which are always constant.
In the feed-forward process, the localization network re-
gresses the coordinates of ﬁducial points C (cid:48) on input image
Iin. Assuming K ﬁducial points on both Iin and Iout, the
parameters of the TPS transformation is represented by a
2 × (K + 3) matrix:

T = [C (cid:48)

02×3]∆−1
C

where ∆C is a (K + 3) × (K + 3) matrix calculated from
C:

∆C =





11×K
C
ˆC

0
0

0
0
1K×1 C (cid:62)





where ˆC is a K × K matrix comprising ˆca,b = φ((cid:107)ca −
cb(cid:107)) and φ(r) = r2 log(r). Given a point p = [xp
yp](cid:62)
on Iout, TPS ﬁnds its corresponding point p(cid:48) by linearly
projection:

(4)

(5)

(6)

p(cid:48) = T







1
p
φ((cid:107)p − c1(cid:107))
φ((cid:107)p − cK(cid:107))







In the transform module, a grid P on Iin is obtained by
iterating over all points on Iout, and the calibrated image is
generated by bilinear interpolation based on P.

3.2. Fiducial-point Reﬁnement

As observed in Figure 3, the iterative reﬁnements can
gradually calibrate the irregular text to be more beneﬁcial

to recognition, however, the missing information caused by
inaccurate transformation cannot be restored in the direct
iterations structure. This leads to incomplete character ap-
pearances and thus results in the recognition errors. We ana-
lyze that the output image samples only from the calibrated
image and pixel information outside the region is discarded.
In the top row of Figure 3, such effect is visible while requir-
ing pixel information outside the calibrated images. In the
scenario of iterative calibrations, the effects of missing in-
formation are accumulated during multiple transformations.
To remedy this issue, we design a ﬁducial-point reﬁne-
ment structure in the network to keep the integrity of text
in the recurrent process. It is worth noting that the transfor-
mation parameters are only deﬁned by the ﬁducial points.
Therefore, we advocate that the transformation information
is transmitted through the ﬁducial points, rather than being
discarded after transformation. During multiple iterations,
the coordinates of ﬁducial points are kept track of and re-
ﬁned. Based on the reﬁned ﬁducial points, we estimate the
transformation parameters and sample from the original im-
age at each step. Therefore, the original character informa-
tion is preserved until the ﬁnal transformation. In order to
ease the training of networks, the localization network just
predicts the coordinates offsets. It is easier to optimize the
residual coordinates than to optimize the original coordi-
nates. Furthermore, the offsets and the previous coordinates
are composed to describe the current positions of ﬁducial
points. However, based on the calibrated images, the gener-
ated offsets and coordinates are both on the calibrated im-
ages. In order to sample from the original image, we need
to map the coordinates back to the original image. Espe-
cially, at the ﬁrst step, the input is the original image, so the
mapping is an identity transformation and can be omitted. It
also should be noted that the ﬁducial points on original im-
age are mapped again to the top and bottom image borders
on calibrated image after each transformation. Therefore,
the previous ﬁducial points are always the canonical forms
(see green points in Fig 2). In t-th recursion, assuming that
the offsets of ﬁducial points are denoted as Ot, and the co-
ordinates of ﬁducial points on original image and calibrated
image are F ori
separately. The current coordi-
nates of ﬁducial points on calibrated images are generated
as the sum of the offsets and the previous coordinates:
t = F cal
F cal

t−1 + Ot

and F cal

(7)

t

t

where

Ot = L(It−1)
(8)
It−1 is the output calibrated image of the previous step, and
L is the localization network. Then the current coordinates
are mapped back to the original image, which serve as the
updated coordinates. The mapping operation M is the same
as the deﬁnition in Eq. 6, and the formulation is shown as

t = M (F cal
F ori

t

, Tt−1)

(9)

Based on the coordinates of the updated ﬁducial points
F ori
, the transformation parameters Tt can be estimated as
t
in Eq. 4 and the next calibrated image can be sampled from
the original image Iori:

It = R(Iori, Tt)

(10)

In this way, the integrity of text is effectively kept be-
cause pixel information outside the calibrated image is also
preserved until the ﬁnal transformation. Therefore, our net-
work can progressively calibrate the irregular text with-
out any character information loss. Besides, all modules
of the calibration process are differentiable, allowing for
backpropagation within an end-to-end learning framework.
Moreover, the calibration process focuses on the text region,
which implicitly models the attention mechanism. Localiz-
ing the text region accurately not only can achieve satis-
factory calibration, but also effectively removes the back-
ground noises.

3.3. Recognition Network

First,

For the recognition network, we use the attention-
based encoder-decoder pipeline as in [29].
the
encoder extracts a sequence of feature vectors H =
(h1, . . . , hn) through CNN-LSTM structure. Then atten-
tion decoder recurrently generates the character sequence
y = (y1, . . . , ym). At step i, the decoder dynamically
weights the image feature and selects the most relevant con-
tents to generate the probability distribution p(yi). Given
the last RNN hidden state si−1 and feature sequence H, the
attention weights can be obtained by scoring each element
in feature sequence separately:

ei,j = vT tanh(W si−1 + U hj + b)

αi,j =

exp(ei,j)
j=1 exp(ei,j)

(cid:80)n

(11)

(12)

Then we can obtain the weighted sum of sequential feature
vectors, which focuses on the most relevant features:

gi =

αi,jhj

(13)

n
(cid:88)

j=1

After that, the RNN hidden state is updated and the proba-
bility distribution p(yi) is estimated as follows:

si = RN N (yi−1, si−1, gi)

p(yi) = sof tmax(V T si)

(14)

(15)

3.4. Model Training

Given the input image I and corresponding ground truth
ˆy = ( ˆy1, ˆy2, · · · , ˆym), the objective function is formulated
by considering both the left-to-right decoder and the right-
to-left decoder as follows:

L(θ) = −

(logpl2r( ˆyi|I) + log pr2l( ˆyi|I))

(16)

1
2

m
(cid:88)

i=1

in which, θ denotes the parameters of both calibration
network and recognition network, pl2r and pr2l are the
output probability distributions of decoders in left-to-right
and right-to-left order, respectively. Through the recurrent
structure, we can perform multiple calibrations under the
same parametric capacity. Furthermore, the calibration net-
work and recognition network are optimized together under
the same recognition loss, so the calibration network is en-
couraged to transform the irregular text to best match the
succeeding recognition network.

4. Experiments

In this section, we describe the details of experimental
settings and evaluate the effectiveness of our method. We
compare the performance of our RCN with the other ap-
proaches on both regular and irregular datasets.

4.1. Datasets

• Street View Text Perspective (SVT-P) [24] contains
639 cropped word images which are captured from the
side-view angles in Google Street View. Most of them
suffer from severely perspective distortion. Each im-
age is speciﬁed with a 50 words lexicon and a full lex-
icon.

• CUTE80 [25] contains 288 cropped word images for
testing, which is specially collected for evaluating the
performance of curved text recognition. No lexicon is
provided.

• ICDAR 2015 [16] contains 2077 word images in-
cluding plenty of irregular text, which are taken from
Google Glasses. For fair comparison, we discard the
images that contain non-alphanumeric characters. No
lexicon is speciﬁed.

• Street View Text [31] contains 647 word images
which are collected from Google Street View. Each
image is associated with a 50 words lexicon deﬁned
by [31]. Many images suffer from low resolution, blur
and noise.

Above, W , U , V , v, b are the learnable parameters. Fol-
lowing [29], we exploited a bidirectional decoder, which
consists of two decoders in opposite directions.

• IIIT5K [23] contains 3000 cropped word images col-
lected from the Internet. Each image has a 50 words
lexicon and a 1000 words lexicon.

Table 1. Scene text recognition accuracies on irregular datasets. “50” and “Full” represent the size of lexicon used for lexicon-based
recognition, and “None” represents lexicon-free recognition.

Methods

SVT-Perspective

CUTE80

IC15

ABBYY[31]
Mishra et al.[23]
Phan et al. [24]
Shi et al.[27]
Yang et al.[33]
Shi et al.[28]
Liu et al. [20]
Cheng et al. [4]
Cheng et al. [5]
Shi et al. [29]
baseline
RCN(ours)

50
40.5
45.7
75.6
92.6
93.0
91.2
-
92.6
94.0
-
93.4
95.0

Full None
26.1
24.7
67.0
72.6
80.2
77.4
-
81.6
83.7
-
89.7
91.2

-
-
-
66.8
75.8
71.8
73.5
71.5
73.0
78.5
77.0
80.6

None
-
-
-
54.9
69.3
59.2
-
63.9
76.8
79.5
82.3
88.5

None
-
-
-
-
-
-
-
66.2
68.2
76.1
72.0
77.1

Total-Text
(multi-oriented)
None
-
-
-
-
-
-
-
-
-
-
72.3
76.3

Total-Text
(curved)
None
-
-
-
-
-
-
-
-
-
-
56.4
66.7

• ICDAR 2003 [22] contains 860 cropped word images
for testing. Following the evaluation protocol in [31],
we recognize the images containing only alphanumeric
characters with at least three characters. Each image is
speciﬁed with a 50 words lexicon deﬁned by [31]. And
a full lexicon consists of all the words that appear in the
test set.

• ICDAR 2013 [17] derives from the ICDAR 2003.
Following [29], we remove the images that contain
non-alphanumeric characters, which results in 1015
cropped word images without any pre-deﬁned lexicon.

• ToTal-Text [6] has annotated word images with three
different text orientations including horizontal, multi-
oriented, and curved text. We select the multi-oriented
and curved text collections, which contain 480 and 971
images separately.

We use the synthetic dataset as the training data, includ-
ing Synth90k released by Jaderberg et al. [12] and Synth-
Text released by Gupta et al. [9] . Our model is evaluated
on all other real-world test datasets without any ﬁnetuning.

4.2. Implementation Details

To validate the effectiveness of our method, we use the
same network architecture and experimental settings with
[29] to ensure fair comparison. We just replace the spatial
transform process with our proposed recurrent calibration
framework. When the number of iterations is one, the RCN
degenerates to the base model [29]. Notice that the calibra-
tion network uses its own shared weight matrixes, so our
RCN has the same parametric capacity as the base model.
Moreover, we also explore the effect of the number of it-
erations. The input images are resized to 64×256, and the

32×64 downsampled images serve as the input of the local-
ization network. After performing spatial transformation,
the calibrated images have the size of 64×256 in the mid-
dle steps and 32×100 in the last step. Besides, we do not
use any data augmentation. The model is trained with the
ADADELTA [35] optimization method.

During the process of testing, we report the results
In
for both lexicon-free and lexicon-based recognition.
lexicon-free setting, we directly select the most probable
character at each decoding step. Then the bidirectional de-
coder generates two results and we choose the result with
the higher probability. In the lexicon-based setting, we pick
the nearest lexicon word with the generated string under the
metric of edit distance.

Our network is implemented under the Pytorch [18]
framework. Most parts of our model are GPU-accelerated
due to the CUDA backend. All the experiments are car-
ried out on a workstation which has one Inter(R) Xeon(R)
E5-2630 2.20Ghz CPU, an NVIDIA TITAN X GPU and
256GB RAM.

4.3. Depth of Recurrence

The recurrent structure progressively calibrates the irreg-
ular text and thus generates better calibrated images. We in-
vestigate the effect of the number of iterations and report the
results in Table 3. As the number of iterations increases, the
recognition results also gradually perform better. In partic-
ular, the performance improvements on curved text bench-
marks are remarkable, which suggests the signiﬁcance of
iterative calibrations in recognizing severely distorted text.
Moreover, our network degenerates to [29] when the num-
ber of iterations is one. Compared with [29], we do not
reproduce their reported results and fall behind them. How-
ever, we still achieve better performance than [29], which
demonstrates the effectiveness of our designs. More than

Table 2. Scene text recognition accuracies on regular datasets. “50”, “1000” and “Full” represent the size of lexicon used for lexicon-based
recognition, and “None” represents lexicon-free recognition.

Methods

SVT

Wang et al.[31]
Mishra et al.[23]
Wang et al.[32]
Bissacco et al.[3]
Almaz´an et al.[1]
Yao et al.[34]
Rodriguez-Serrano et al.[26]
Jaderberg et al.[15]
Gordo [7]
Jaderberg et al.[13]
Jaderberg et al.[11]
Shi et al.[27]
Lee et al.[19]
He et al.[10]
Wang and Hu[30]
Cheng et al. [4]
Bai et al. [2]
Liu et al. [21]
Yang et al.[33]
Shi et al.[28]
Liu et al. [20]
Cheng et al. [5]
Shi et al. [29]
baseline
RCN(ours)

50
57.0
73.2
70.0
90.4
89.2
75.9
70.0
86.1
91.8
95.4
93.2
97.5
96.3
92.0
96.3
97.1
96.6
96.1
95.2
95.5
-
96.0
97.4
96.4
97.7

None
-
-
-
78.0
-
-
-
-
-
80.7
71.7
82.7
80.7
-
81.5
85.9
87.5
-
-
81.9
84.4
82.8
89.5
86.2
88.6

IIIT5k
1k
-
57.5
-
-
82.1
69.3
57.4
-
86.6
92.7
89.6
95.0
94.4
91.6
95.6
97.5
97.9
94.3
96.1
93.8
-
98.1
98.8
98.9
98.9

None
-
-
-
-
-
-
-
-
-
-
-
81.2
78.4
-
80.8
87.4
88.3
86.6
-
81.9
83.6
87.0
93.4
92.6
94.0

50
-
64.1
-
-
91.2
80.2
76.1
-
93.3
97.1
95.5
97.8
96.8
94.0
98.0
99.3
99.5
96.9
97.8
96.2
-
99.6
99.6
99.6
99.6

IC03
IC13
Full None None
-
62.0
-
67.8
-
84.0
-
-
-
-
-
80.3
-
-
-
91.5
-
-
98.6
93.1
89.6
97.0
91.9
98.0
88.7
97.0
-
94.4
91.2
97.8
94.2
97.3
94.6
97.9
93.1
97.9
-
-
90.1
96.2
91.5
-
91.5
97.1
94.5
98.0
92.9
97.6
93.6
97.9

-
-
-
87.6
-
-
-
-
-
90.8
81.8
89.6
90.0
-
-
93.3
94.4
92.7
-
88.6
90.8
-
91.8
90.7
93.2

50
76.0
81.8
90.0
-
-
88.5
-
96.2
-
98.7
97.8
98.7
97.9
97.0
98.8
99.2
98.7
98.4
97.7
98.3
-
98.5
98.8
98.6
99.0

three iterations result in negligible effects and the number
of iterations is set as three in the following experiments.

In addition, by comparing the RCN-3 and RCN-3 (w/o
FP-R), we can see that the ﬁducial-point reﬁnement struc-
ture leads to signiﬁcant performance promotion under the
same number of iterations. The main reason is that the orig-
inal character information is preserved and thus the missing
information can be recovered. And the succeeding recogni-
tion network beneﬁts from the integrity of text.

Furthermore, some examples are presented in Figure 4.
As observed, the text becomes more regular with the num-
ber of iterations increases. Besides, the lost character in-
formation in the previous step can be recovered in the sub-
sequent processes. Therefore, the integrity of text is effec-
tively preserved during the iterative calibrations. Our net-
work not only transforms the text in the direction that is
more beneﬁcial to recognition, but also gradually removes
the background noises.

4.4. Performance on Irregular Benchmarks

Recognizing irregular text is very challenging, due to the
various character placements. To validate the effectiveness
of our method, we evaluate RCN on several irregular bench-

marks and summarize the results in Table 1. The network
with a single calibration is taken as the baseline method.
As observed, our method achieves signiﬁcant improvements
than the baseline, and consistently outperforms the other
approaches by a large margin. It is worth noting that we
do not reproduce the reported results in [29], by comparing
the baseline and [29]. Nevertheless, we still achieve better
performance on all benchmarks, which suggests the signif-
icance of our method. Especially, we outperform [29] by
a margin of 9 percentages on CUTE80. Besides, we ﬁnd
that the performance gains on the curved text benchmarks
are more signiﬁcant than the perspective text benchmarks.
The distortions of curved text are more serious and hard to
model, so existing methods perform worse on curved text.
By contrast, our approach can effectively calibrate severely
distorted text, and hence obtains much promotion on curved
text benchmarks. It also should be pointed out that the RCN
not only achieves much better calibrations, but also has no
any extra parameter. As shown in Figure 4, our RCN is ca-
pable of calibrating the irregular text with various degrees
of deformation, including the nearly vertical text. Com-
pared with [5], we do not destroy the aspect ratio of text,
and thus the characters have no deformation. We also re-

Table 3. Lexicon-free results on several benchmarks with the different number of iterations. FP-R represents the proposed ﬁducial-point
reﬁnement structure.

Method

SVT

IIIT5k

IC03

IC13

SVT-P CUTE80

IC15

RCN-3(w/o FP-R)
RCN-1
RCN-2
RCN-3

87.5
86.2
85.6
88.6

92.4
92.6
92.7
94.0

93.5
92.9
93.0
93.6

91.5
90.7
91.6
93.2

78.1
77.0
77.9
80.6

86.1
82.3
83.0
88.5

74.1
72.0
74.6
77.1

Total-Text
(multi-oriented)
73.8
72.3
75.2
76.3

Total-Text
(curved)
65.2
56.4
57.6
66.7

probability to train their networks, while we only use the
traditional frame-wise loss. Besides, [13] beneﬁted from a
pre-deﬁned 90k lexicon and only recognized the words in its
dictionary. It also should be remarked that [4] and [33] used
the extra character bounding box annotations. By contrast,
our method just requires the textual labels, which saves a lot
of resources.

5. Conclusion

In this paper, we propose a Recurrent Calibration Net-
work (RCN) for irregular text recognition. We divide the
calibration process into multiple progressive steps to relieve
the calibration difﬁculty of each step. Besides, the recurrent
structure makes our network has the same parametric ca-
pacity as the network with a single spatial transformation.
Moreover, we design a ﬁducial-point reﬁnement structure
to track and transmit the coordinates of ﬁducial points, in-
stead of propagating the calibrated images. Therefore, our
network is able to effectively keep the integrity of text dur-
ing iterative calibrations. The lost information caused by
inaccurate transformation can be recovered in subsequent
processes. Furthermore, the calibration network and recog-
nition network are jointly trained under the same objective
for text recognition. Thus, the text is gradually calibrated
in the direction that is more beneﬁcial to recognition. Ex-
tensive experiments conducted on challenging benchmarks
verify the effectiveness of our method, especially on irreg-
ular datasets.

References

[1] J. Almaz´an, A. Gordo, A. Forn´es, and E. Valveny. Word
IEEE
spotting and recognition with embedded attributes.
transactions on pattern analysis and machine intelligence,
36(12):2552–2566, 2014. 7

[2] F. Bai, Z. Cheng, Y. Niu, S. Pu, and S. Zhou. Edit probability
for scene text recognition. arXiv preprint arXiv:1805.03384,
2018. 3, 7, 8

[3] A. Bissacco, M. Cummins, Y. Netzer, and H. Neven. Pho-
toocr: Reading text in uncontrolled conditions. In 2013 IEEE
International Conference on Computer Vision, pages 785–
792. IEEE, 2013. 2, 7

[4] Z. Cheng, F. Bai, Y. Xu, G. Zheng, S. Pu, and S. Zhou. Fo-
cusing attention: Towards accurate text recognition in natu-
ral images. In Computer Vision (ICCV), 2017 IEEE Interna-

Figure 4. Visualizations of the calibrated images during iterative
reﬁnements.

port the recognition performance on Total-Text that has not
been recorded in previous literature. Our method achieves
promising results on both multi-oriented and curved text
collections. Furthermore, with the calibrated text from the
last iteration, any kind of recognition network can be ex-
ploited. If using the focusing attention mechanism in [4]
and the edit probability in [2], the performance can be fur-
ther improved.

4.5. Performance on Regular Benchmarks

We also conduct experiments on regular benchmarks.
Most samples in these datasets are regular text, but irreg-
ular text also exists. We report our results in Table 2. Shi et
al. [29] corrected the results on SVT in their released code
and we record the updated results. The baseline is the net-
work with a single calibration as in [29]. Compared with the
baseline method, the RCN signiﬁcantly improves the recog-
nition performance. We can see that the RCN performs best
on IIIT5k in lexicon-free setting. It is observed that IIIT5k
contains many curved text, which demonstrates the advan-
tage of RCN in dealing with severely distorted text. In the
lexicon-based scenario, we achieve the best results on SVT
and IIIT5k, but slightly fall behind [4, 13] on IC03 and [2]
on IC13. However, [2] applied the specially designed edit

tional Conference on, pages 5086–5094. IEEE, 2017. 3, 6,
7, 8

[5] Z. Cheng, X. Liu, F. Bai, Y. Niu, S. Pu, and S. Zhou.
arXiv preprint

recognition.

Arbitrarily-oriented text
arXiv:1711.04226, 2017. 1, 3, 6, 7

[6] C. K. Ch’ng and C. S. Chan. Total-text: A comprehensive
In Docu-
dataset for scene text detection and recognition.
ment Analysis and Recognition (ICDAR), 2017 14th IAPR In-
ternational Conference on, volume 1, pages 935–942. IEEE,
2017. 6

[7] A. Gordo. Supervised mid-level features for word image
In Proceedings of the IEEE conference on
representation.
computer vision and pattern recognition, pages 2956–2964,
2015. 7

[8] A. Graves, S. Fern´andez, F. Gomez, and J. Schmidhu-
ber. Connectionist temporal classiﬁcation: labelling unseg-
mented sequence data with recurrent neural networks.
In
Proceedings of the 23rd international conference on Ma-
chine learning, pages 369–376. ACM, 2006. 3

[9] A. Gupta, A. Vedaldi, and A. Zisserman. Synthetic data for
In Proceedings of the
text localisation in natural images.
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 2315–2324, 2016. 6

[10] P. He, W. Huang, Y. Qiao, C. C. Loy, and X. Tang. Reading
In AAAI, vol-

scene text in deep convolutional sequences.
ume 16, pages 3501–3508, 2016. 3, 7

[11] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman.
Deep structured output learning for unconstrained text recog-
nition. arXiv preprint arXiv:1412.5903, 2014. 7

[12] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman.
Synthetic data and artiﬁcial neural networks for natural scene
text recognition. arXiv preprint arXiv:1406.2227, 2014. 6

[13] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisser-
man. Reading text in the wild with convolutional neural net-
works. International Journal of Computer Vision, 116(1):1–
20, 2016. 2, 7, 8

[14] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial
In Advances in neural information

transformer networks.
processing systems, pages 2017–2025, 2015. 3

[15] M. Jaderberg, A. Vedaldi, and A. Zisserman. Deep features
In European conference on computer vi-

for text spotting.
sion, pages 512–528. Springer, 2014. 7

[16] D. Karatzas, L. Gomez-Bigorda, A. Nicolaou, S. Ghosh,
A. Bagdanov, M. Iwamura, J. Matas, L. Neumann, V. R.
Chandrasekhar, S. Lu, et al. Icdar 2015 competition on ro-
In Document Analysis and Recognition (IC-
bust reading.
DAR), 2015 13th International Conference on, pages 1156–
1160. IEEE, 2015. 5

[17] D. Karatzas, F. Shafait, S. Uchida, M. Iwamura, L. G. i Big-
orda, S. R. Mestre, J. Mas, D. F. Mota, J. A. Almazan, and
L. P. De Las Heras. Icdar 2013 robust reading competition.
In Document Analysis and Recognition (ICDAR), 2013 12th
International Conference on, pages 1484–1493. IEEE, 2013.
6

[18] N. Ketkar. Introduction to pytorch. In Deep Learning with

Python, pages 195–208. Springer, 2017. 6

[19] C.-Y. Lee and S. Osindero. Recursive recurrent nets with at-
tention modeling for ocr in the wild. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 2231–2239, 2016. 3, 7

[20] W. Liu, C. Chen, and K.-Y. K. Wong. Char-net: A character-
aware neural network for distorted scene text recognition. In
AAAI, 2018. 3, 6, 7

[21] Z. Liu, Y. Li, F. Ren, W. L. Goh, and H. Yu. Squeezedtext:
A real-time scene text recognition by binary convolutional
encoder-decoder network. In AAAI, 2018. 7

[22] S. M. Lucas, A. Panaretos, L. Sosa, A. Tang, S. Wong,
R. Young, K. Ashida, H. Nagai, M. Okamoto, H. Yamamoto,
Icdar 2003 robust reading competitions: entries, re-
et al.
International Journal of Doc-
sults, and future directions.
ument Analysis and Recognition (IJDAR), 7(2-3):105–122,
2005. 6

[23] A. Mishra, K. Alahari, and C. Jawahar. Top-down and
bottom-up cues for scene text recognition. In CVPR-IEEE
Conference on Computer Vision and Pattern Recognition.
IEEE, 2012. 5, 6, 7

[24] T. Quy Phan, P. Shivakumara, S. Tian, and C. Lim Tan. Rec-
ognizing text with perspective distortion in natural scenes. In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 569–576, 2013. 5, 6

[25] A. Risnumawan, P. Shivakumara, C. S. Chan, and C. L. Tan.
A robust arbitrary text detection system for natural scene im-
ages. Expert Systems with Applications, 41(18):8027–8048,
2014. 5

[26] J. A. Rodriguez-Serrano, A. Gordo, and F. Perronnin. Label
embedding: A frugal baseline for text recognition. Interna-
tional Journal of Computer Vision, 113(3):193–207, 2015. 2,
7

[27] B. Shi, X. Bai, and C. Yao. An end-to-end trainable neural
network for image-based sequence recognition and its appli-
cation to scene text recognition. IEEE transactions on pat-
tern analysis and machine intelligence, 39(11):2298–2304,
2017. 3, 6, 7

[28] B. Shi, X. Wang, P. Lyu, C. Yao, and X. Bai. Robust scene
In Proceed-
text recognition with automatic rectiﬁcation.
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 4168–4176, 2016. 3, 6, 7

[29] B. Shi, M. Yang, X. Wang, P. Lyu, C. Yao, and X. Bai. Aster:
an attentional scene text recognizer with ﬂexible rectiﬁca-
IEEE transactions on pattern analysis and machine
tion.
intelligence, 2018. 1, 2, 3, 5, 6, 7, 8

[30] J. Wang and X. Hu. Gated recurrent convolution neural net-
work for ocr. In Advances in Neural Information Processing
Systems, pages 335–344, 2017. 7

[31] K. Wang, B. Babenko, and S. Belongie. End-to-end scene
In Computer Vision (ICCV), 2011 IEEE
text recognition.
International Conference on, pages 1457–1464. IEEE, 2011.
2, 5, 6, 7

[32] T. Wang, D. J. Wu, A. Coates, and A. Y. Ng. End-to-end text
recognition with convolutional neural networks. In Pattern
Recognition (ICPR), 2012 21st International Conference on,
pages 3304–3308. IEEE, 2012. 7

[33] X. Yang, D. He, Z. Zhou, D. Kifer, and C. L. Giles. Learning
In Pro-
to read irregular text with attention mechanisms.
ceedings of the Twenty-Sixth International Joint Conference
on Artiﬁcial Intelligence, IJCAI-17, pages 3280–3286, 2017.
1, 3, 6, 7, 8

[34] C. Yao, X. Bai, B. Shi, and W. Liu. Strokelets: A learned
multi-scale representation for scene text recognition. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 4042–4049, 2014. 7

[35] M. D. Zeiler. Adadelta: an adaptive learning rate method.

arXiv preprint arXiv:1212.5701, 2012. 6

