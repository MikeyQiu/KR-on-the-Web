Multi-View Dynamic Facial Action Unit Detection

Andr´es Romero
rv.andres10@uniandes.edu.co

Juan Le´on
jc.leon@uniandes.edu.co

Pablo Arbel´aez
pa.arbelaez@uniandes.edu.co

Universidad de los Andes

8
1
0
2
 
g
u
A
 
0
2
 
 
]

V
C
.
s
c
[
 
 
2
v
3
6
8
7
0
.
4
0
7
1
:
v
i
X
r
a

Abstract

We propose a novel convolutional neural network ap-
proach to address the ﬁne-grained recognition problem
of multi-view dynamic facial action unit detection. We
leverage recent gains in large-scale object recognition by
formulating the task of predicting the presence or ab-
sence of a speciﬁc action unit in a still image of a hu-
man face as holistic classiﬁcation. We then explore the
design space of our approach by considering both shared
and independent representations for separate action
units, and also diﬀerent CNN architectures for combin-
ing color and motion information. We then move to the
novel setup of the FERA 2017 Challenge, in which we
propose a multi-view extension of our approach that op-
erates by ﬁrst predicting the viewpoint from which the
video was taken, and then evaluating an ensemble of
action unit detectors that were trained for that speciﬁc
viewpoint. Our approach is holistic, eﬃcient, and mod-
ular, since new action units can be easily included in
the overall system. Our approach signiﬁcantly outper-
forms the baseline of the FERA 2017 Challenge, with
an absolute improvement of 14% on the F1-metric.
Additionally, it compares favorably against the winner
of the FERA 2017 challenge. Code source is available
at https: // github. com/ BCV-Uniandes/ AUNets .

1. Introduction

The ﬁeld of human facial expression interpretation
has beneﬁted from seminal contributions by renowned
psychologists such as P. Ekman, who characterized
and studied the manifestation of prototypical emotions
through changes in facial features [12]. From the com-
puter vision perspective, the problem of automated fa-
cial expression analysis is a cornerstone towards high-
level human computer interaction, and its study has
a long tradition within the community. Initially, the
problem was approached by focusing on its most basic
version, and classifying static images or short sequences

Figure 1: Results of our approach. Given a video
of a human face that was taken from an unknown view-
point, our system predicts the presence or absence of
multiple action units in each frame.

of faces into a handful of prototypical emotions (e.g.,
happiness, sadness, fear, etc.). Recent methods[25, 18]
have achieved signiﬁcant progress in the study of this
task, leading to the near saturation of standard bench-
marks such as the CK+ dataset [36].

In order to systematize the study of facial expres-
sions, Ekman and his collaborators designed the Facial
Action Coding System (FACS) [13]. FACS relies on
identifying visible local appearance variations in the
human face, called Action Units (AUs), that are pro-
duced by the individual activation of facial muscles.
Thus, a raised eyebrow is coded as the activation of
the outer brow raiser, and noted AU02. Since any fa-
cial expression can be represented as a combination
of action units, they constitute a natural physiological
basis for face analysis. The existence of such a ba-
sis is a rare boon for a computer vision domain, as
it allows focusing on the essential atoms of the prob-
lem and, by virtue of their exponentially large possible
combinations, opens the door for studying a wide range
of applications beyond prototypical emotion classiﬁca-

1

Figure 2: Overview of AUNets. Our system takes as input a video of a human head and computes its optical
ﬂow ﬁeld. It predicts the viewpoint from which the video was taken, and uses this information to select and evaluate
an ensemble of holistic action unit detectors that were trained for that speciﬁc view. Final AUNets predictions are
then temporally smoothed.

tion. Consequently, in the last years, the main focus of
the community has shifted towards the detection of ac-
tion units, and recent datasets such as BP4D [54] and
the FERA 2017 challenge [46] include annotations by
trained psychologists for multiple action units in indi-
vidual video frames.

When compared to global emotion classiﬁcation, ac-
tion unit detection is a much challenging and ﬁne-
grained recognition task, as shown by the local and
delicate appearance variations in Fig. 1. The expres-
sion of action units is typically brief and unconscious,
and their detection requires analyzing subtle appear-
ance changes in the human face. Furthermore, action
units do not appear in isolation, but as elemental units
of facial expressions, and hence some AUs co-occur
frequently while others are mutually exclusive. In re-
sponse to these challenges, the literature has converged
to a dominant approach for the study of action unit
detection [57, 56, 53, 23, 14, 1, 26, 17, 34] that uses
the localization of facial landmarks as starting point.
Recent methods implement this paradigm in two diﬀer-
ent ways, either using facial landmarks as anchors and
analyzing the appearance of patches centered in those
keypoints as well as their combinations, or using land-
marks to perform face alignment prior to AU detec-
tion. Focusing on ﬁxed facial regions has the advantage
of constraining appearance variations. However, facial
landmark localization in the wild is still an open prob-
lem, its study requires also very expensive annotations,
and its solution is as challenging as the AU detection
itself. Furthermore, while such an approach is suitable
for a near frontal face setup, as is the case in the BP4D
dataset, it ﬁnds its limitations in a multi-view setting
such as FERA 2017, in which large variations in head

pose imply occlusion of multiple facial landmarks and
signiﬁcant appearance changes.

In this paper, we depart from the mainstream ap-
proach for AU detection and propose a system that di-
rectly analyzes the information on the whole human
face in order to predict the presence of speciﬁc ac-
tion units, bypassing thus landmark localization on the
standard benchmarks for this task. For this purpose,
we leverage recent insights on designing Convolutional
Neural Networks (CNNs) for recognition applications.
It is important to mention that [56, 57, 17, 34] can
avoid keypoints, however, they are required to use a
diﬀerent intermediate step in order to align faces.

First, we observe that large-capacity CNN architec-
tures [28, 42, 43, 20] that were originally designed for
object recognition on datasets such as Imagenet [10]
analyze a whole raw image and are capable of making
subtle category distinctions (e.g. between dog breeds).
Moreover, they can be successfully specialized for other
ﬁne-grained recognition problems [50, 5]. Therefore,
we formulate the problem of predicting the presence or
absence of an speciﬁc AU in a single face image as holis-
tic binary classiﬁcation. We explore the design space
of our approach and, in particular, the trade-oﬀ be-
tween eﬃciency and accuracy when considering shared
or independent convolutional representations for diﬀer-
ent action units.

Second,

since action units appear dynamically
within the spatio-temporal continuum of human facial
expressions, we model explicitly temporal information
in two diﬀerent ways. At frame level, we compute an
optical ﬂow ﬁeld and use it as an additional cue for
improving AU detection.Then, we enforce short-term
consistency across the sequence by a smoothing opera-

2

tor that improves our predictions over a small temporal
window.

Third, in order to address multi-view detection, the
main focus of the recent FERA 2017 challenge, we take
inspiration from the work of Dai et al . [9], who showed
that training CNNs for a hard recognition task such as
object instance segmentation on MS-COCO [30] ben-
eﬁts from decomposing learning into a cascade of dif-
ferent sub-tasks of increasing complexity. Thus, our
multi-view system starts by predicting the overall view
of an input sequence before proceeding to the ﬁner
grained task of action unit detection. Furthermore,
as the experiments will show, our system beneﬁts from
a gradual domain adaptation to the ﬁnal multi-view
setup. Fig. 2 presents an overview of our approach,
which we call AUNets.

We perform an extensive empirical validation of
our approach. We develop our system on the BP4D
dataset, the largest and most varied available bench-
mark for frontal action unit detection, where we re-
port an absolute improvement of 7% over the pre-
vious state-of-the-art by Li et al . [29]. We then turn
to the FERA2017 challenge to evaluate our multi-view
system, and report an absolute improvement of 14%
on the F1-metric over the challenge baseline of Valstar
et al . [46], while comparing favorably against state-of-
the-art method by Tang et al . [44]. In order to ensure
reproducibility of our results and to promote future re-
search on action unit detection, all the resources of this
project –source code, benchmarks and results– will be
made publicly available.

2. Related Work

Before deep learning techniques became mainstream
within the ﬁeld of computer vision [28], most meth-
ods relied on the classical two-stage approach of de-
signing ﬁxed handcrafted features such as SIFT [35]
or LBP [39], and then training unrelated classiﬁers for
recognition [47, 33, 1, 53, 14, 56, 26]. However, simi-
larly to AUNets, the best performing techniques cur-
rently available [23, 17, 57, 29] rely on the power of deep
convolutional neural networks for joint representation
and classiﬁcation.

Most recent methods [32, 1, 53, 14, 56, 23, 17, 57, 26]
follow the paradigm of ﬁrst detecting facial landmarks
using external approaches such as Active Appearance
Models [8], either to treat these keypoints as anchors
for extracting rectangular regions for further analysis,
to perform face alignment, or both. The recent method
of Li et al . [29] does not require robust facial keypoint
alignment as it is trained taking this issue into ac-
count; however, facial alignment is recommended and
the method is developed and tested only in a frontal

face setup. In contrast, AUNets operate on the whole
face and do not require any particular alignment in
existing AU detection multi-view benchmarks.

Pioneering methods for this task used the whole
face image as input [47, 31, 33]. However, the trend
reversed towards analyzing patches in more local ap-
proaches such as [32, 1, 53, 14, 56, 23]. State-of-the-art
techniques [17, 57, 29] join AUNets in returning to a
holistic face analysis, in particular Li et al . [29] shares
ideas with Zhao et al . [57] in forcing a CNN-based ap-
proach to speciﬁcally focusing on speciﬁc regions of the
face by using a map saliency. Our method is far from
these approaches since we train speciﬁc networks for
each AU which avoids the need of building attention
maps.

Given that groups of action units can co-occur or
be mutually exclusive in the human face, several meth-
ods [57, 47, 56, 29] have approached the task as multi-
label learning. However, as AUs databases are becom-
ing larger and better annotated [55, 37], these methods
need to be completely retrained to integrate new action
units. In contrast, AUNets are modular by design, and
can naturally evolve towards more general datasets and
new action units. Furthermore, by analyzing the whole
face, AUNets learn naturally the relevant local face re-
gions for predicting each action unit.

While most methods operate on single RGB images
both at training and testing [33, 1, 17, 31], some tech-
niques focus on exploiting the temporal information
for improved AU detection [53, 26, 32]. In particular,
Jaiswal et al . [23] and Chu et al . [6] use CNN’s and
Bidirectional Long Short-Term Memory to model time
dependencies. Similarly, Liu et al . [34] tackle the tem-
poral information by taking advantage of the Optical
Flow [21]; for this purpose, starting from the OF they
extract hand-crafted features as histogram oriented [4]
in order to train a classiﬁer. However, our method
takes advantage of the full resolution of the OF by
feeding it into a CNN. Moreover, on behalf of the Op-
tical Flow, we perform simple statistical operations to
smooth the temporal ﬂow as a post-processing step.

The FERA17 Challenge [46] introduces a new exper-
imental framework for the facial expression recognition
problem. It does not only consider frontal images, but
also multi-view rendered facial images [46]. Our in-
sight comes from realizing that each view should be
detected ﬁrst and then treated independently as well
as each AU. Instead of automatically frontalizing each
view by using facial alignment [46], AUNets, similar
to other FERA17 participants [44, 19], learn indepen-
dent AUs regardless the multi-label setup, and avoiding
intermediate steps such as keypoint detection. How-
ever, AUNets core lies on the simplicity of using in-

3

dependent AUs for each view; thus, by taking advan-
tage of the temporal information we compare favorably
against state-of-the-art methods [44, 19] in this chal-
lenging problem.

3. Approach

3.1. Facial Expression Representation

We formulate individual action unit detection as
holistic binary classiﬁcation in order to build on re-
cent high-capacity CNN architectures [28, 42, 43, 20]
that were originally designed for object classiﬁcation
in large-scale datasets such as Imagenet [10]. Recent
works have shown that these networks can learn useful
representations for diﬀerent object-level applications
such as contour detection [48, 49], semantic segmenta-
tion [38], instance segmentation [9], and action recog-
nition [3].

However, relative to ImageNet, the domain adapta-
tion we require for action unit detection is much larger
than the one for the above mentioned problems, which
are typically studied in VOC PASCAL [15]. There-
fore, we start by learning a high-level representation
on a related but simpler facial expression analysis task.
We achieve this goal by converting the last fully con-
nected layers of a popular CNN architecture such (e.g.,
VGG [42]) into 22 diﬀerent outputs instead of the 1000
of ImageNet, and then re-train it for the task of emo-
tion classiﬁcation.
In order to generalize across eth-
nicity, gender and age, we train the encoder on a non-
traditional dataset [11] labeled beyond the prototypi-
cal 8 basic emotions, instead it uses 22 diﬀerent emo-
tions (e.g., happily surprised, sadly fearful, angrily dis-
gusted, etc) with diﬀerent characteristics, see Figure 3
for details about this dataset. After ”EmoNet”(ENet)
learning is completed, we remove the output layer of
the network and use the learned weights as convolu-
tional encoder for facial expressions in all subsequent
experiments. Our results show that this initial domain
adaptation is beneﬁcial for good performance, indicat-
ing that the convolutional encoder is indeed learning
features that are speciﬁc to faces.

3.2. Single-Image Action Unit Detection

Starting from the learned convolutional encoder,
training a specialized detector for a given action unit in
single RGB images is straightforward. We extend the
encoder with a binary output softmax layer predict-
ing the presence or absence of the AU in the image,
we initialize it with random weights, and then resume
training in a database that provides AU annotations,
such as BP4D. However, when considering multiple ac-
tion units, two architectural options emerge, depending

Figure 3: Dataset for domain adaptation. This
dataset contains images from 230 diﬀerent subjects
(Caucasian, Asian, African American and Hispanic)
and 21 diﬀerent compound emotions (22 including neu-
tral) for a total of 5060 images (See [11] for more de-
tails about the acquisition). Once we extract the face,
we perform data augmentation by randomly cropping
from the center and tuning the saturation, brightness,
contrast and hue.

on whether or not we share features at the lower layers
of the network.

3.2.1 HydraNet

The ﬁrst option is to preserve a shared representa-
tion and to train individual modules for diﬀerent AUs.
This strategy is implemented by freezing the lower lay-
ers of the convolutional encoder and learning separate
weights at the upper layers sequentially for each action
unit. We call this network “Hydra-Net” in reference to
the fabulous Lernaean Hydra from the Greek mythol-
ogy, a dragon with a single body and multiple heads
that would multiply when severed. Hydra-Net is eﬃ-
cient at test time and modular by design, as it can be
easily extended to new action units by “growing” ad-
ditional heads. Furthermore, as the experiments will
show, this baseline system already outperforms the
state-of-the-art on BP4D for single-image AU detec-
tion.

3.2.2 AUNets

The elegance of a shared representation in Hydra-Net
comes at the price of a reduction in absolute perfor-

4

mance, because only a fraction of the original encoder
weights are being specialized for each action unit. In
order to quantify this trade-oﬀ, we train a second vari-
ant of our system in which we relearn all the weights
of the encoder for each action unit. This strategy re-
sults in a battery of independent AU detectors, which
we call “AU-Nets”. When compared to Hydra-Net,
the larger-capacity ensemble of AU-Nets requires more
data at training time and is less eﬃcient at test time.
However, both approaches are modular in the presence
of new action units and, as the experiments will show,
the increased ﬂexibility of AU-Nets allows them to fo-
cus better on speciﬁc regions of the face associated to
diﬀerent action units, leveraging thus local information
for improved performance.

3.3. Dynamic Action Unit Detection

Although trained human observers can annotate ac-
tion units on single images, AUs are continuous and
precisely localized events that occur within the spatio-
temporal ﬂow of facial expressions, and therefore an
explicit analysis of motion information should facili-
tate AU detection when video data is available. We
model motion by computing a dense optical ﬂow ﬁeld
and embedding it into a three dimensional space in
which the ﬁrst two dimensions are normalized x and
y pixel motion and the third dimension is the optical
ﬂow magnitude. This embedding is common in video
analysis applications [16, 41] and has the advantage of
providing a motion signal that has the same scale and
dimensions as the original RGB frame. We considered
three architectures for combining color and motion in-
formation, as illustrated in Fig. 4, b, c, and d.

3.3.1 Additional input Channels

A ﬁrst option is to consider the optical ﬂow embedding
as three additional channels of the input RGB image,
ending up with and input of 224x224x6. This architec-
ture is economical, as it adds only additional weights
for the ﬁrst layer of the network, which are initialized
as copies of the RGB weights. This ﬁrst layer is then
learning to combine all channels for each spatial loca-
tion.

3.3.2 Concatenation

A larger architecture is obtained by concatenating color
and motion images in one of the two spatial dimensions.
In this case, the network analyzes the two signals with
the same ﬁrst-layer ﬁlters, but its ﬁrst fully convolu-
tional architecture is expanded accordingly in order to
combine their information (replicating the pre-trained
weights); resulting also in larger capacity models. The

concatenation is performed in horizontal position, re-
sulting therefore in an input of 224x448x3.

3.3.3 Two streams

A third option is to consider two diﬀerent streams
to represent color and motion information, and to
merge them in higher layers. In this case, we extract
the weights of the RGB encoder and copy them to
initialize the motion encoder. We are thus adapting
the RGB representation to the motion signal, while
simultaneously learning its optimal combination with
the color representation in the deeper layers. This
strategy is also known as ’π Network’ and it is carried
out whether after the last convolutional layer (conv5 3
of VGG), after the ﬁrst FC layer (fc6 of VGG), or
after the second fully connected layer (fc7 of vgg), also
known as π/conv, π/fc6 and π/fc7 respectively.

The experimental section will show that explicit
modeling of motion between successive frames is ben-
eﬁcial for AU detection. Nevertheless, we note that
our dynamic AU detectors above still operate indepen-
dently in each frame, and hence cannot capture the
temporal smoothness of action unit activation. In or-
der to enforce such consistency, we perform a simple
post-processing by applying a temporal sliding median
ﬁlter over predictions of each action unit across the
video. We found this simple smoothing to be empiri-
cally beneﬁcial for all the variants of our approach.

3.4. Multi-View System

Our ﬁnal system is evaluated on the FERA 2017
challenge, which focuses on multi-view AU detection.
Starting from a video of a face that was taken from
an unknown viewpoint among nine options, the task
consists in predicting labels for twelve common action
units on all its frames. We extend our system in two
ways to address this multi-view version of the problem.
First, we train a view classiﬁer, which takes the video as
input and predicts its viewpoint. Once the viewpoint
has been estimated, our cascaded approach proceeds to
evaluate an ensemble of temporally consistent dynamic
AU-Nets that were trained for that speciﬁc view. In
order to perform domain adaptation, we ﬁrst retrain
our system developed in BP4D in the frontal view of
FERA 2017, and then use those weights to initialize
learning for the other views.

4. Experimental Validation

In this section, we provide the empirical evidence
to support our approach. We ﬁrst conduct extensive
experiments in the BP4D dataset analyzing diﬀerent

5

Figure 4: We consider three architectures for combining color and motion information: (a) RGB/OF alone archi-
tecture, (b) additional input channels, (c) an extended image (horizontal concatenation in the input), and (d)
two separate CNN streams (independent networks that fuse in the fully connected layer). Layers whose weights
are altered by the input size are displayed with a red star. Yellow circle depicts the binary output. See text for
more details.

aspects of our method for frontal faces. We then turn to
the FERA 2017 dataset in order to extend our system
to a challenging multi-view setting.

4.1. Results on BP4D-Spontaneous

4.1.1 Experimental Setup

The BP4D-Spontaneous database [54] is currently one
of the largest and most challenging benchmarks avail-
able for the task of action unit detection [54]. BP4D
contains 2D and 3D videos of spontaneous facial ex-
pressions in young people. All of them were recorded
in non-acted scenarios, as the emotions were always
elicited by experts. This dataset contains around
150.000 individual frames from 41 subjects annotated
by trained psychologists.

The BP4D dataset has a signiﬁcant class imbalance
for most AUs; for instance, there are 6 times more neg-
atives samples than positive samples for AU23, while
AU12 is almost balanced. We follow the common prac-
tice in the literature [57, 56, 6, 29], and consider the 12
most frequent AUs [54] for the BP4D dataset. In order
to compare our approach directly with the state-of-the-
art, we report results using the standard performance
metric from the literature, the image-based F1-score
(f1-frame) [45], also known as F-measure and deﬁned
by the harmonic mean of precision and recall, which
considers a threshold of 0.5 in the operating regime
of the detector. Since state-of-the-art weights are not
publicly available, for all the BP4D experiments, we
strictly follow the experimental framework of Zhao et
al . [57] and Chu et al . [6] and perform three-fold cross

validation (so called three fold testing) using their same
splits, in order to ensure that our results are directly
comparable.

It is important to mention that we split the training
set into training and validation. We randomly extract
all the images from one single subject of the full train-
ing set to validation, for the three folds. All hyper-
parameters and training decisions were tuned based on
the performance of the validation set.

4.1.2 Implementation

All models are trained using Adam [27] optimizer with
β1=0.5, β2=0.999. We train with an initial learning
rate of 0.0001 linearly decaying to 0 over the next 12
epochs, unless early stop due to validation convergence
i.e., stop only if F1-val reaches plateau region after 3
consecutive epochs. All experiments were performed
on a single GPU TITAN X. Training details regarding
time consumption are discussed in §5.

To overcome the BP4D dataset’s AU skew, we aug-
ment the positives training samples using the well
known strategy of image jittering [28], that is shift-
ing the bounding box to the right, left, up, down and
any possible combination of those until we reach an
approximate balance with the negative samples.

4.1.3 Domain Adaptation

We ﬁrst quantify the importance of learning a high-
level representation for faces prior to action unit detec-
tion. We select three popular CNN architectures that

6

AU

HydraNets
AlexNet GoogLeNet

VGG

INet ENet INet ENet INet ENet
24.6 39.9
19.8 40.3 27.0 34.1
1
23.6 35.0
16.4 23.3 17.2 28.0
2
40.9 48.9
31.4 43.3 44.9 46.0
4
72.8 75.2
49.4 65.3 71.6 76.3
6
72.0 73.6
51.3 71.3 64.8 70.5
7
81.0 83.8
47.2 83.7 70.4 75.0
10
81.7 86.2
53.7 79.5 75.1 84.1
12
60.0 64.8
23.4 49.1 51.1 58.4
14
19.1 35.8
22.7 40.0 31.3 32.3
15
54.3 58.0
21.4 27.3 51.2 54.9
17
28.6 30.6
20.4 34.2 28.1 29.6
23
35.3 38.8
24
21.0 43.1 37.5 39.7
49.5 55.9
Av. 31.5 49.3 47.5 52.2

AUNets
VGG
ENet
48.4
42.3
55.6
77.8
78.9
81.6
88.4
65.7
50.6
60.4
42.9
47.4
61.6

Table 1: Control experiments for convolutional
encoder. Results are reported over BP4D by using
3-fold cross validation and the F1-metric. It compares
several base networks as starting point for training Hy-
draNets and AUNets, using either Imagenet (INet) or
EmoNet (ENet) pre-trained weights.

were originally designed and trained on ImageNet, and
that have been applied to multiple vision problems:
AlexNet [28], VGG-16 [42], and GoogLeNet [43].
In
order to expose the network to a large variety of faces,
we ﬁrst train an Facial Expression Classiﬁcation net-
work using a non-traditional dataset created by Du et
al . [11] for facial expressions recognition. We chose
this dataset because of its subject variability and not
being grounded to the basic emotions (happiness, sad-
ness, etc). Thus, we modify the output layer for 22-way
emotion classiﬁcation and train following §4.1.2. At the
end, we use the best model trained on EmoNet and it
is called ENet. To this point, we have three diﬀerent
models (AlexNet, GoogLeNet, and VGG) trained on
facial expression classiﬁcation (ENet) and three mod-
els originally trained on ImageNet (INet).

We then use these six convolutional encoders as dif-
ferent initialization to the Hydra-Net architecture in
BP4D and train them until validation convergence.

Results are presented in the left panel of Table 1. We
ﬁrst observe that all three architectures produce rea-
sonable AU detection results when initialized with Im-
ageNet weights, and that deeper architectures produce
improved results, in accordance to their original perfor-
mance in ImageNet. This behavior contrasts with ran-
dom initialization, which causes the training process
to diverge for the three CNNs (results not included).
Furthermore, we observe a signiﬁcant improvement in
AU detection for all base CNN architectures when
initializing with our face expression convolutional en-
coder instead of the original ImageNet weights. Among
the three CNNs considered, VGG-16 generalizes better

to our application domain, obtaining a +6% absolute
improvement in performance over ImageNet weights.
These results highlight the relevance of domain adap-
tation for facial expression analysis. We use the con-
volutional encoder based on the VGG-16 architecture
for all subsequent experiments.

4.1.4 Shared vs. Independent Representation

We next explore the trade-oﬀ between eﬃciency and
accuracy when considering shared or independent rep-
resentations for diﬀerent action units. For this purpose,
we start from our shared representation Hydra-Net and
train with the above mentioned parameters a new fully
independent detector for each AU. Learning is carried
out by unfreezing all layers in the convolutional en-
coder. The right column of Table 1 presents the results
for independent detectors, which we call in the sequel
AUNets. In comparison to HydraNet, under an inde-
pendent representation 11 out of the 12 action units
show improvements. Moreover, we observe an overall
better performance under this strategy as, on average,
the F1 measure increases by 6 over the whole set.

4.1.5 Optical Flow

To incorporate short term motion patterns into our
experimental setup, we estimate the optical ﬂow (OF)
with the variational model proposed by Brox et al . [2].
Given the large scale of the BP4D and FERA17
datasets, we resort to a GPU implementation of this
algorithm [22] as it provides a fair trade-oﬀ between
accuracy and computation time. In practice, we calcu-
late the OF for the whole training set of FERA17 in
under 60 hours with a single Titan X GPU.

To adapt the resulting dense vector map into the
AU-Nets, it is transformed into an RGB image as pro-
posed by Gkioxari et al . [16] and Peng et al . [41], by
independently normalizing the l2 norm of the x and y
dimension over the OF ﬁeld, extracting the face from
the RGB bounding box, and then re-scaling to 224 to
conform the images R and G channels, whereas the B
channel is calculated as the norm of the original vector.
Initially, we consider motion information in isola-
tion and train our AUNets ensemble under the same
experimental set up as the original RGB frames. Col-
umn ’Alone’ in Table 2 summarizes the results for this
experiment. Even without any color information, the
motion signal alone achieves a performance of 50.4%,
which is competitive with the state-of-the-art in BP4D.
We then explore the strategies outlined in Sec-
tion 3.3 to fuse information from the OF and RGB
domains. Table 2-right shows the results for the ﬁve
proposed architectures, including three options for the

7

AU
1
2
4
6
7
10
12
14
15
17
23
24
Av.

AUNets
ENet
48.4
42.3
55.6
77.8
78.9
81.6
88.4
65.7
50.6
60.4
42.9
47.4
61.6

Alone
35.0
26.5
37.6
67.0
69.3
74.0
77.6
61.0
29.2
54.2
36.8
37.0
50.4

Horizontal
52.8
44.0
55.3
78.9
77.5
82.8
88.0
66.2
47.7
61.6
46.9
49.4
62.6

AUNets+OF

Channels
52.0
44.5
54.6
77.4
78.5
83.2
87.8
66.4
47.9
61.2
41.9
45.5
61.7

π/conv
46.8
42.1
55.9
77.6
78.0
83.3
88.7
65.9
50.1
60.1
44.0
50.3
61.9

π/fc6
48.0
41.2
56.8
78.2
79.3
83.2
88.8
65.8
50.8
60.8
45.8
49.8
62.4

π/fc7
53.4
49.8
56.0
77.4
78.4
83.2
88.0
67.3
44.2
61.5
41.7
49.5
62.5

Table 2: Control experiments for Optical Flow. It presents experiments on AUNets and diﬀerent combination
of Optical Flow and Color. Alone means solely training with the OF as input (Figure 4a). Channel corresponds to
the channel embedding (Figure 4b). Horizontal means horizontal concatenation in the input (Figure 4c). π/conv,
π/fc6 and π/fc7 are the diﬀerent two CNN streams that fuses before the FC6, FC7, FC8 layer of vgg, respectively
(Figure 4d). See more details at §3.3.3.

Two Stream strategy. We observe that the motion sig-
nal is complementary to the color information, as the
inclusion of OF always brings an improvement over the
RGB results. The Two Stream and the Concatenation
strategies are both superior to the Additional Chan-
nels, and provide comparable performance.

We conclude that, overall, the best approach is to
concatenate the OF and RGB information at the very
input stage;
it has the very same performance as a
π network joint at the FC7 layer, yet, the number of
parameters is 12% smaller, this means that further do-
main adaptation will be faster, and overﬁt will be less
likely.

4.1.6 Temporal Smoothing

Since action units appear in smooth, short-term du-
ration intervals, we add a post-processing step which
aims at removing instantaneous AU activation and pos-
sible misclassiﬁcation inside a continuous sequence. For
this goal we use a median operator in a sliding win-
dow fashion over the AU detection the sequences. This
hyper-parameter was tuned based on the validation set.
Our approach uses small window sizes as we want
to avoid the suppression of properly detected but short
temporal series of AU activation. Therefore, we test
with window sizes: {3,5,7,9,10,11}, these remain ﬁxed
for all AUs over the entire train set. Empirically we
conclude that the best window sizes are {3,5,7} with
7 being the optimal, longer temporal windows do not
improve the base performance. Table 4 shows the result
(+median) of performing this temporal smoothing to
the best OF arrangement (Horizontal concatenation).

4.1.7 Comparison against the state-of-the-art

We now compare our results against the state-of-the-
art approaches [29, 6, 57, 56]. Table 5 summarizes the
average of three-fold cross validation for each action
unit. We observe that our method consistently outper-
forms all methods for 10 out of 12 action units, with
an average improvement over the F1 measure of 7%,
over EAC.

4.2. Results on the FERA 2017 Challenge

4.2.1 Experimental Setup

The new version of the FERA17 challenge [46] intro-
duces a novel way to approach the problem of facial
expression analysis. This dataset contains a 3D video
database rendered mainly from BP4D [54]. Moreover,
for the test set it includes subjects from a diﬀerent
dataset [55], which do not overlap with BP4D subjects.
The FERA17 dataset includes 9 diﬀerent camera an-
gles for non-symmetric facial views. It contains videos
from 41 subjects and around 1.500.000 frames for the
whole training split. An additional set of videos from
20 diﬀerent subjects with about 750.000 frames inte-
grate the validation split. A ﬁnal set of 1080 videos
are included in the withheld test split.

4.2.2 Viewpoint Classiﬁcation

Our AU-Net ensemble is view speciﬁc, and thus relies
on a proper view selection for optimal results. We ap-
proach this problem by performing a view recognition
sub-task, prior to the AU-Net ensemble.

For this sub-task, we build a view classiﬁer on top of
another deep convolutional encoder. Since we require

8

Params

HydraNets

AUNets

Total
Learnable
12 models

134m
119m
1443m

134m
134m
1608m

Channel
134m
134m
1608m

Horizontal
237m
237m
2844m

AUNets+OF
pi/conv
251m
237m
3012m

pi/fc7
pi/fc6
268m
268m
151m
134m
3216m 3216m

Table 3: Number of parameters per architecture per AU. Total number of parameters and total learn-
able neurons for HydraNet, AUNets and AUNets with Optical Flow with additional input channels, horizontal
concatenation, streaming fusion after last convolutional layer, after FC6 layer and after FC7 layer, respectively

AU
F1

1
53.4

2
44.7

4
55.8

6
79.2

7
78.1

10
83.1

12
88.4

14
66.6

15
47.5

17
62.0

23
47.3

24
49.7

Av.
63.0

Table 4: Results of our best approach (RGB+OF Horizontal concatenation). These results are reported
with median temporal window of 7.

AU JPML[56] DRML[57] MSTC[6] EAC[29] Ours
53.4
1
44.7
2
55.8
4
79.2
6
78.2
7
83.1
10
88.4
12
66.6
14
47.5
15
62.0
17
47.3
23
49.7
24
63.0
Av.

31.4
31.1
71.4
63.3
77.1
45.0
82.6
72.9
34.0
53.9
38.5
37.0
53.2

36.4
41.8
43.0
55.0
67.0
66.3
65.8
54.1
33.2
48.0
31.7
30.0
48.3

39.0
35.2
48.6
76.1
72.9
81.9
86.2
58.8
37.5
59.1
35.9
35.8
55.9

32.6
25.6
37.4
42.3
50.5
72.2
74.1
65.7
38.1
40.0
30.4
42.3
45.9

Table 5: 3-fold cross validation for each Ac-
tion Unit over BP4D dataset using F1-metric.
Comparisons against the state-of-the-art methods:
JPML [56], DRML [57], MSTC [6], and EAC [29].

a simple architecture to train, we start from the Caﬀe-
GoogLeNet [24] reference model, and proceed to learn a
suitable representation for the view classiﬁcation prob-
lem using the full training set. To avoid overﬁtting, we
initially freeze the ﬁrst 6 Inception modules and opti-
mize over the ﬁnal 3 modules (4f,5b,5c), we also drop
the two deep supervision branches as they remain con-
nected to frozen segments of the encoder. We use the
default ImageNet weights for the viewpoint network,
but learn from scratch the weights for the ﬁnal fully
connected layer. After this modiﬁcations the encoder
produces a 9-dimensional output, its soft-max normal-
ized output approximates the probability for each view.
The viewpoint-network is trained for two epochs, with
an initial learning rate of 5−5, gamma parameter of
0.1, and a weight decay of 5−5, using Stochastic Gradi-
ent Descend, we reduce the learning rate after the ﬁrst
epoch.

As ﬁnal step we get a single prediction for any video

as maxv(p), where p is the per frame prediction and v is
the full set of frames for a video. Evaluation results in
Table 6 suggest that the viewpoint classiﬁer is almost
perfect in the FERA 2017 setup. This result indicates
that AU detection will not be aﬀected in any signiﬁcant
way by prior view classiﬁcation.

4.2.3 Multi-View System

For our ﬁrst approach to the FERA17 Challenge, we
want to assess the generalization capability of our sys-
tem, so we proceed to evaluate our system trained in
BP4D over the entire validation partition of FERA17
without any retraining. The evaluation procedure re-
sults in an performance of 46.5%, which shows that
our system trained on BP4D with no previous training
on FERA17 already surpasses the baseline approach for
validation partition in FERA17 Challenge [46] that is
41.6%. We observe that, when the view is close to the
frontal one, our model improves performance, which is
51.9% over the frontal view, and 41.4% over an upper-
left view.

We use the BP4D models as starting-point to train
on the frontal view of FERA17 dataset. We follow
the same strategy for each view, in order to learn the
entire 9 views for 10 AUs, resulting in a total of 90
models. For each view, we learn our models with the
same setup as in BP4D, as outlined in Section 3.4. We
train each view and each AU until validation conver-
gence with learning rate of 10−4 and weight decay of
5−3, the whole system is trained for a total of 50h in a
single Titan X GPU. AUNets+OF trained on FERA17
and evaluated in the validation set obtain 64.4% on
average, which widely outperforms the 40.4% of the
baseline [46], and the FERA17-winner[44], which re-
ports 58.0%.

9

Per Frame Precision
Per Frame Recall
Per Video Precision
Per Video Recall

V1
0.96
0.99
0.97
0.99

V2
0.96
0.95
0.96
0.96

V3
1.0
0.95
1.0
0.95

V4
0.96
1.0
0.96
1.0

V5
0.97
0.96
0.97
0.96

V6
0.99
0.98
0.99
0.98

V7
0.98
0.95
0.98
0.95

V8
0.91
0.93
0.92
0.94

V9
0.94
0.96
0.94
0.96

Av.
0.96
0.96
0.97
0.97

Table 6: Evaluation of Multi-view Classiﬁer, Per frame and per video results for the view recognition sub-task
in the validation set of FERA17.

AU

1
4
6
7
10
12
14
15
17
23
Av.

[46]
57.0
52.0
67.6
64.2
63.8
66.0
62.2
30.7
48.5
37.3
54.9

Validation

Test

ACC
[44] Ours
93.3
78.2
92.7
80.8
83.2
79.9
75.5
73.7
82.3
82.9
85.9
86.0
64.8
66.7
88.1
80.6
82.2
82.2
85.7
86.2
83.4
79.7

[46]
15.4
17.2
56.4
72.7
69.2
64.7
62.2
14.6
22.4
20.7
41.6

F1
[44] Ours
48.7
30.4
55.6
36.2
76.1
71.2
81.8
77.9
83.6
83.6
85.3
84.0
73.4
69.7
43.1
35.3
46.6
44.2
49.6
47.5
64.4
58.0

[46]
53.0
55.7
66.2
66.4
67.1
65.1
61.5
31.0
52.2
43.2
56.1

ACC
[44] Ours
89.8
76.5
93.5
85.7
81.0
79.4
77.7
76.3
81.1
83.2
82.4
82.9
69.2
68.3
83.6
73.6
79.0
76.3
74.3
75.4
81.8
77.8

[46]
14.7
4.40
63.0
75.5
75.8
68.7
66.8
22.0
27.4
34.2
45.2

F1
[44] Ours
30.9
26.3
16.6
11.8
79.9
77.6
83.6
80.8
84.6
86.5
82.7
84.3
78.2
75.7
28.0
36.2
44.3
42.4
48.6
51.9
57.7
57.4

Table 7: Results over FERA17 Validation and Test, using F1-score and Accuracy (ACC). Comparison
with the baseline approach[46] and the challenge winner[44].

4.2.4 Evaluation on Test Set

4.2.5 Eﬃciency

It is important to mention that the test set of
FERA17 Challenge [46] includes subjects from BP4D+
dataset [55], which are diﬀerent subjects that the BP4D
dataset [54]. We include the results obtained during the
FERA17 Challenge, where the test server was made
available from February 8 to March 1 2017 with a limit
of 5 submissions per group. The challenge rules re-
quired participants to upload their source code to the
FERA17 server, and the organizers ran the models on
an undisclosed test set. Following the challenge setup,
we report F1-score and ACC in Table 7, 8 and 9, for
Val and Test sets. Our ﬁnal submitted system is rep-
resented in Fig. 2. As can be observed, our model sets
a new benchmark in the problem of multi-view action
unit detection by improving with an 14% in F1-score
over the baseline approach in the Test set, and it is on-
par with the challenge winner [44], under that metric.
Furthermore, on the ACC metric, we outperform both
the baseline and the challenge winner with an absolute
improvement of 15% and 4% respectively.

10

We measure the average execution time for the ﬁnal
multi-view system as follows: For a video with 500
frames, it takes on average 12 seconds to transform
it into frames. The Optical Flow takes about 2 min-
utes producing the whole set of OF frames. Once the
RGB and OF frames are complete, the view detector
takes around 2 seconds predicting the corresponding
view of the video from a frame sub-sampling. For the
ﬁnal step, it takes roughly 8 more minutes to forward
every frame of the video through the 10 AUs detectors.
In summary, it takes less than 1 second to process each
frame once the OF is computed.

4.3. Qualitative Results

Similar to Zhao et al . [57] and Chu et al . [6], we
present qualitative results over BP4D dataset using the
visualization techniques of Zeiler and Fergus [52], and
Yosinski [51].

One of Zeiler’s insights is to occlude parts of the
input image in order to look for the classiﬁcation prob-
ability this region is supplying to the full image. Fig. 5
show this approach over several images for diﬀerent
AUs. We can observe that our approach emphasizes
very speciﬁc regions of the face (third row) using thus

AU

v1
v2
1
45.1
47.3
4
53.4
57.7
6
75.4
73.6
7
81.6
81.9
10
82.4
83.9
12
86.2
86.6
14
72.3
73.6
15
40.4
45.5
17
52.0
52.4
54.2
52.0
23
Av. 64.1 65.7

AU
v1
91.2
1
92.1
4
81.4
6
75.3
7
80.1
10
87.1
12
62.5
14
86.8
15
83.4
17
23
87.5
Av. 82.7

v2
92.9
94.4
81.6
75.9
83.1
87.8
64.6
90.4
86.5
88.5
84.6

F1-score
Ours
v3
v6
v9 Global
45.2
54.8
47.1
46.0
55.3
43.1
77.6
79.0
74.4
81.4
82.6
80.8
82.6
84.8
82.0
84.6
85.2
82.9
71.7
76.4
68.2
40.2
47.7
28.6
51.0
48.4
36.5
34.9
54.2
53.6
63.4 65.8 68.6 66.8 64.3 62.7 57.9

48.7
55.6
76.1
81.8
83.6
85.3
73.4
43.1
46.6
49.6
64.4

v4
54.5
53.3
77.2
81.9
83.1
86.7
73.8
53.0
45.2
49.4

v7
45.0
59.1
77.3
80.7
85.0
85.2
72.5
45.0
43.7
49.4

v8
49.6
51.1
72.3
81.7
83.7
84.9
74.9
39.9
44.8
44.4

v5
49.5
81.5
77.7
83.3
84.5
85.3
77.0
47.4
45.7
53.9

v7
v5
v3
92.3
93.4
92.2
92.6
97.3
90.0
84.2
84.1
84.5
73.8
79.0
73.9
83.5
83.4
81.4
83.9
86.2
86.0
63.4
71.1
62.4
90.6
91.0
84.7
79.8
85.8
85.0
87.3
86.4
87.8
82.7 83.9 85.9 85.9 83.1

ACC
v6
95.2
94.0
85.9
77.6
83.6
86.2
72.7
91.4
84.9
87.9

v4
94.6
93.2
84.5
76.3
81.9
87.5
65.0
92.4
76.1
87.3

v9 Global
v8
94
93.7
88.1
92.7
82.7
80.1
72.4
74.9
80.6
82.8
83.4
85.3
54.9
66.6
75.9
89.8
76.4
82.0
82.3
76.7
83.0 78.5

93.3
92.7
83.2
75.5
82.3
85.9
64.8
88.1
82.2
85.7
83.4

Table 8: Results over FERA17 Validation, using F1-score and Accuracy (ACC).

AU

v1
33.7
22.6
79.8
82.0
80.7
82.3
77.4
23.2
50.1
46.5

v9 Global
28.7
1
19.0
4
79.7
6
83.3
7
84.3
10
81.9
12
77.3
14
29.1
15
37.8
17
23
44.7
Av. 57.8 59.1 59.4 58.5 58.7 56.9 56.5 56.7 56.6

30.9
16.6
79.9
83.6
84.6
82.7
78.2
28.0
44.3
48.6
57.7

v2
30.5
29.2
80.3
85.2
83.8
81.8
78.9
21.1
48.4
51.4

v3
28.8
13.2
80.4
85.0
81.4
83.7
78.0
38.8
52.5
51.7

v7
34.0
14.7
79.7
83.2
86.1
79.3
76.8
26.4
42.3
42.4

v4
35.0
18.9
79.8
83.3
85.9
84.0
78.1
26.6
43.2
50.6

v8
27.5
19.3
80.4
82.8
84.7
82.1
77.8
30.2
37.9
44.3

v5
32.3
14.9
79.2
83.7
87.8
84.5
79.1
28.0
43.5
54.3

F1-score
Ours
v6
28.1
2.90
80.1
83.5
85.8
84.1
80.3
24.1
48.7
51.3

v1
92.4
97.3
81.5
76.4
76.8
81.8
66.5
82.4
83.0
72.2

AU
v7
89.2
1
92.7
4
80.9
6
75.9
7
82.4
10
79.0
12
67.7
14
82.9
15
74.0
17
23
72.9
Av. 81.0 83.3 82.3 81.1 82.2 82.3 79.8

v2
91.3
97.6
81.9
80.5
81.5
82.6
70.4
86.1
84.3
77.0

v3
90.6
93.8
81.5
80.3
79.2
83.6
68.7
84.2
85.1
75.9

v4
90.3
94.7
80.1
77.1
81.9
82.7
68.7
84.6
75.4
75.5

v5
87.7
87.9
79.0
78.4
84.3
83.5
72.6
86.7
82.5
79.1

ACC
v6
89.9
94.9
81.6
78.5
81.9
83.1
73.2
85.1
82.3
72.6

v9 Global
v8
88.7
88.0
91.2
91.5
80.8
81.5
76.6
75.8
80.9
81.4
82.4
82.5
67.1
68.2
79.7
81.2
72.1
73.1
71.4
72.1
79.5 79.2

89.8
93.5
81.0
77.7
81.1
82.4
69.2
83.6
79.0
74.3
81.2

Table 9: Results over FERA17 Test set, using F1-score and Accuracy .

local information for each action unit despite analyzing
the face hollistically and not using facial alignment.

Additionally, Yosinski’s method creates synthetic
images as input in order to maximize the output of

one speciﬁc neuron; for instance, we maximize the out-
put of the binary one. Similar to Zeiler’s, Fig. 6 shows
that AU1 and AU2 models are focused on the eyes,
thus AU12 and AU15 representations are looking for

11

patterns over the mouth region, conﬁrming thus the
speciﬁcity of these models.

5. Limitations

Despite our approach radically pushes forward the
state-of-the-art in this problem, it has two main short-
comings regarding the huge number of parameters (12
models for each fold in BP4D, and 90 diﬀerent models
for FERA17) and the time required to train all of them.
Table 3 summarizes the number of parameters. On the
other hand, in order to train 12 models in 3 fold cross
validation over a 12GB GPU Titan X it takes around
one week per variant architecture.

6. Conclusions

We have presented HydraNets and AUNets for rec-
ognizing 12 diﬀerent facial action units. Our method
takes advantage of the power of CNNs for large-scale
classiﬁcation problems, and is capable of detecting mul-
tiple action units simultaneously. Our method uses
the temporal information OF to enhance performance.
There is a trade-oﬀ between eﬃciency and performance
with HydraNets and AUNets, yet both approaches ob-
tain competitive results when compared against state-
of-the-art methods, and our ﬁnal multi-view system
compares favorably in performance against state-of-
the-arts approaches in a challenging benchmark. At
the core of our approach lies a ﬂexible and modu-
lar architecture that can easily incorporate new action
units. In order to promote further research in action
unit detection, our source code, models, and results
can be found at https://github.com/BCV-Uniandes/
AUNets1.

7. Acknowledgements

This work was partially supported by a Google
Research Award Latin America. We are grateful to
NVIDIA Corporation for donating the GPUs used in
this work.

References

[1] T. Almaev, B. Martinez, and M. Valstar. Learning
to transfer: Transferring latent task structures and its
application to person-speciﬁc facial action unit detec-
tion. In ICCV, 2015. 2, 3

[2] T. Brox and J. Malik. Large displacement optical ﬂow:
descriptor matching in variational motion estimation.
TPAMI, 2011. 7

[3] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Car-
los Niebles. Activitynet: A large-scale video bench-

1Written in PyTorch framework [40].

mark for human activity understanding.
2015. 4

In CVPR,

[4] R. Chaudhry, A. Ravichandran, G. Hager, and R. Vi-
dal. Histograms of oriented optical ﬂow and binet-
cauchy kernels on nonlinear dynamical systems for the
recognition of human actions. In CVPR, 2009. 3
[5] G. Ch´eron, I. Laptev, and C. Schmid. P-cnn: Pose-
In ICCV,

based cnn features for action recognition.
2015. 2

[6] W.-S. Chu, F. De la Torre, and J. F. Cohn. Modeling
spatial and temporal cues for multi-label facial action
unit detection. arXiv preprint arXiv:1608.00911, 2016.
3, 6, 8, 9, 10

[7] CMU. Facial action coding system, 2002. 13
[8] T. F. Cootes, G. J. Edwards, and C. J. Taylor. Active

appearance models. In ECCV. 1998. 3

[9] J. Dai, K. He, and J. Sun. Instance-aware semantic seg-
mentation via multi-task network cascades. In ICCV,
2016. 3, 4

[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and
L. Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, 2009. 2, 4

[11] S. Du, Y. Tao, and A. M. Martinez. Compound facial
expressions of emotion. Proceedings of the National
Academy of Sciences, 111(15):E1454–E1462, 2014. 4,
7

[12] P. Ekman and W. V. Friesen. Facial action coding

system. 1977. 1

[13] P. Ekman and E. L. Rosenberg. What the face reveals:
Basic and applied studies of spontaneous expression us-
ing the Facial Action Coding System (FACS). Oxford
University Press, USA, 1997. 1

[14] S. Eleftheriadis, O. Rudovic, and M. Pantic. Multi-
conditional latent variable model for joint facial action
unit detection. In ICCV, 2015. 2, 3

[15] M. Everingham, L. Van Gool, C. K. Williams, J. Winn,
and A. Zisserman. The pascal visual object classes
(voc) challenge. International journal of computer vi-
sion, 88(2):303–338, 2010. 4

[16] G. Gkioxari and J. Malik. Finding action tubes. In

CVPR, 2015. 5, 7

[17] A. Gudi, H. E. Tasli, T. M. den Uyl, and A. Maroulis.
Deep learning based facs action unit occurrence and
intensity estimation. In 11th IEEE International Con-
ference and Workshops on Automatic Face and Ges-
ture Recognition, 2015. 2, 3

[18] S. Happy and A. Routray. Automatic facial expres-
sion recognition using features of salient facial patches.
IEEE transactions on Aﬀective Computing, 6(1):1–12,
2015. 1

[19] J. He, D. Li, B. Yang, S. Cao, B. Sun, and L. Yu.
Multi view facial action unit detection based on cnn
and blstm-rnn. In Automatic Face & Gesture Recog-
nition (FG 2017), 2017 12th IEEE International Con-
ference on, pages 848–853. IEEE, 2017. 3, 4

[20] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual

learning for image recognition. In CVPR, 2016. 2, 4

12

Inner Brow Raiser Outer Brow Raiser Upper Lip Raiser

Lip Corner Puller

Lip Corner
Depressor

AU01

AU02

AU10

AU12

AU15

AU01

AU02

AU10

AU12

AU15

AU01

AU10
Figure 5: Zeiler’s method [52] for network visualization. The top row presents 5 diﬀerent Action Units [7]. The heat
maps highlight the most important regions in the human face for each speciﬁc Action Unit (blue: less important,
red: more important).

AU15

AU12

AU02

AU01

AU10
Figure 6: Yosinski’s method [51] for network visualization. This approach shows synthetic hallucinations that
maximize the output of the network for AU 1,2,10,12,15 i.e., generate the best image that entirely maximize the
output neuron.

AU12

AU15

AU02

[21] B. K. Horn and B. G. Schunck. Determining optical

2016. 2, 3

ﬂow. Artiﬁcial intelligence, 17(1-3):185–203, 1981. 3

[22] Itseez. Open source computer vision library. https:

//github.com/itseez/opencv, 2015. 7

[23] S. Jaiswal and M. Valstar. Deep learning the dynamic
appearance and shape of facial action units. In WACV,

[24] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long,
R. Girshick, S. Guadarrama, and T. Darrell. Caﬀe:
Convolutional architecture for fast feature embedding.
In ACM, 2014. 9

[25] S. E. Kahou, P. Froumenty, and C. Pal. Facial expres-

13

sion analysis based on high dimensional binary fea-
tures. In ECCV, 2014. 1

[26] S. Kaltwang, S. Todorovic, and M. Pantic. Latent
trees for estimating intensity of facial action units. In
CVPR, 2015. 2, 3

[27] D. P. Kingma and J. Ba. Adam: A method for stochas-
arXiv preprint arXiv:1412.6980,

tic optimization.
2014. 6

[28] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Ima-
genet classiﬁcation with deep convolutional neural net-
works. In NIPS, 2012. 2, 3, 4, 6

[29] W. Li, F. Abtahi, Z. Zhu, and L. Yin. Eac-net:
A region-based deep enhancing and cropping ap-
proach for facial action unit detection. arXiv preprint
arXiv:1702.02925, 2017. 3, 6, 8, 9

[30] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona,
D. Ramanan, P. Doll´ar, and C. L. Zitnick. Microsoft
coco: Common objects in context. In ECCV, 2014. 3
[31] M. Liu, S. Li, S. Shan, and X. Chen. Au-aware deep
In Auto-

networks for facial expression recognition.
matic Face and Gesture Recognition (FG), 2013. 3
[32] M. Liu, S. Li, S. Shan, R. Wang, and X. Chen. Deeply
learning deformable facial action parts model for dy-
namic expression analysis. In ACCV, 2014. 3

[33] P. Liu, S. Han, Z. Meng, and Y. Tong. Facial expres-
sion recognition via a boosted deep belief network. In
CVPR, 2014. 3

[34] Y.-J. Liu, J.-K. Zhang, W.-J. Yan, S.-J. Wang,
G. Zhao, and X. Fu. A main directional mean optical
ﬂow feature for spontaneous micro-expression recog-
nition.
IEEE Transactions on Aﬀective Computing,
7(4):299–310, 2016. 2, 3

[35] D. G. Lowe. Object recognition from local scale-
invariant features. In Computer vision, 1999. The pro-
ceedings of the seventh IEEE international conference
on, volume 2, pages 1150–1157. Ieee, 1999. 3

[36] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Am-
badar, and I. Matthews. The extended cohn-kanade
dataset (ck+): A complete dataset for action unit and
emotion-speciﬁed expression. In CVPR, 2010. 1
[37] M. Mavadati, P. Sanger, and M. H. Mahoor. Extended
disfa dataset: Investigating posed and spontaneous fa-
cial expressions. In CVPR Workshops, 2016. 3
[38] H. Noh, S. Hong, and B. Han. Learning deconvolution
network for semantic segmentation. In ICCV, 2015. 4
[39] T. Ojala, M. Pietikainen, and D. Harwood. Perfor-
mance evaluation of texture measures with classiﬁca-
tion based on kullback discrimination of distributions.
In Pattern Recognition, 1994. Vol. 1-Conference A:
Computer Vision & Image Processing., Proceedings of
the 12th IAPR International Conference on, volume 1,
pages 582–585. IEEE, 1994. 3

[40] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang,
Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and
A. Lerer. Automatic diﬀerentiation in pytorch. 2017.
12

[41] X. Peng and C. Schmid. Multi-region two-stream r-cnn

for action detection. In ECCV, 2016. 5, 7

[42] K. Simonyan and A. Zisserman. Very deep convo-
lutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014. 2, 4, 6

[43] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Ra-
binovich. Going deeper with convolutions. In CVPR,
2015. 2, 4, 6

[44] C. Tang, W. Zheng, J. Yan, Q. Li, Y. Li, T. Zhang, and
Z. Cui. View-independent facial action unit detection.
In Automatic Face & Gesture Recognition (FG 2017),
2017 12th IEEE International Conference on, pages
878–882. IEEE, 2017. 3, 4, 9, 10

[45] M. F. Valstar, M. Mehu, B. Jiang, M. Pantic, and
K. Scherer. Meta-analysis of the ﬁrst facial expres-
sion recognition challenge. Systems, Man, and Cyber-
netics, Part B: Cybernetics, IEEE Transactions on,
42(4):966–979, 2012. 6

[46] M. F. Valstar, E. S´anchez-Lozano, J. F. Cohn, L. A.
Jeni, J. M. Girard, Z. Zhang, L. Yin, and M. Pan-
tic. Fera 2017-addressing head pose in the third facial
expression recognition and analysis challenge. arXiv
preprint arXiv:1702.04174, 2017. 2, 3, 8, 9, 10

[47] Z. Wang, Y. Li, S. Wang, and Q. Ji. Capturing global
semantic relationships for facial action unit recogni-
tion. In ICCV, 2013. 3

[48] S. ”Xie and Z. Tu. Holistically-nested edge detection.

In ICCV, 2015. 4

[49] J. Yang, B. Price, S. Cohen, H. Lee, and M.-H. Yang.
Object contour detection with a fully convolutional
encoder-decoder network. In CVPR, 2016. 4

[50] L. Yang, P. Luo, C. Change Loy, and X. Tang. A
large-scale car dataset for ﬁne-grained categorization
and veriﬁcation. In CVPR, 2015. 2

[51] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lip-
son. Understanding neural networks through deep vi-
sualization. In ICML, 2015. 10, 13

[52] M. D. Zeiler and R. Fergus. Visualizing and under-
standing convolutional networks. In ECCV, 2014. 10,
13

[53] J. Zeng, W.-S. Chu, F. De la Torre, J. F. Cohn, and
Z. Xiong. Conﬁdence preserving machine for facial
action unit detection. In ICCV, 2015. 2, 3

[54] X. Zhang, L. Yin, J. F. Cohn, S. Canavan, M. Reale,
A. Horowitz, P. Liu, and J. M. Girard.
Bp4d-
spontaneous: a high-resolution spontaneous 3d dy-
namic facial expression database. Image and Vision
Computing, 32(10):692–706, 2014. 2, 6, 8, 10

[55] Z. Zhang, J. M. Girard, Y. Wu, X. Zhang, P. Liu,
U. Ciftci, S. Canavan, M. Reale, A. Horowitz, H. Yang,
et al. Multimodal spontaneous emotion corpus for hu-
man behavior analysis. In CVPR, 2016. 3, 8, 10
[56] K. Zhao, W.-S. Chu, F. De la Torre, J. F. Cohn, and
H. Zhang. Joint patch and multi-label learning for
facial action unit detection. In CVPR, 2015. 2, 3, 6,
8, 9

[57] K. Zhao, W.-S. Chu, and H. Zhang. Deep region and
multi-label learning for facial action unit detection. In
CVPR, 2016. 2, 3, 6, 8, 9, 10

14

Multi-View Dynamic Facial Action Unit Detection

Andr´es Romero
rv.andres10@uniandes.edu.co

Juan Le´on
jc.leon@uniandes.edu.co

Pablo Arbel´aez
pa.arbelaez@uniandes.edu.co

Universidad de los Andes

8
1
0
2
 
g
u
A
 
0
2
 
 
]

V
C
.
s
c
[
 
 
2
v
3
6
8
7
0
.
4
0
7
1
:
v
i
X
r
a

Abstract

We propose a novel convolutional neural network ap-
proach to address the ﬁne-grained recognition problem
of multi-view dynamic facial action unit detection. We
leverage recent gains in large-scale object recognition by
formulating the task of predicting the presence or ab-
sence of a speciﬁc action unit in a still image of a hu-
man face as holistic classiﬁcation. We then explore the
design space of our approach by considering both shared
and independent representations for separate action
units, and also diﬀerent CNN architectures for combin-
ing color and motion information. We then move to the
novel setup of the FERA 2017 Challenge, in which we
propose a multi-view extension of our approach that op-
erates by ﬁrst predicting the viewpoint from which the
video was taken, and then evaluating an ensemble of
action unit detectors that were trained for that speciﬁc
viewpoint. Our approach is holistic, eﬃcient, and mod-
ular, since new action units can be easily included in
the overall system. Our approach signiﬁcantly outper-
forms the baseline of the FERA 2017 Challenge, with
an absolute improvement of 14% on the F1-metric.
Additionally, it compares favorably against the winner
of the FERA 2017 challenge. Code source is available
at https: // github. com/ BCV-Uniandes/ AUNets .

1. Introduction

The ﬁeld of human facial expression interpretation
has beneﬁted from seminal contributions by renowned
psychologists such as P. Ekman, who characterized
and studied the manifestation of prototypical emotions
through changes in facial features [12]. From the com-
puter vision perspective, the problem of automated fa-
cial expression analysis is a cornerstone towards high-
level human computer interaction, and its study has
a long tradition within the community. Initially, the
problem was approached by focusing on its most basic
version, and classifying static images or short sequences

Figure 1: Results of our approach. Given a video
of a human face that was taken from an unknown view-
point, our system predicts the presence or absence of
multiple action units in each frame.

of faces into a handful of prototypical emotions (e.g.,
happiness, sadness, fear, etc.). Recent methods[25, 18]
have achieved signiﬁcant progress in the study of this
task, leading to the near saturation of standard bench-
marks such as the CK+ dataset [36].

In order to systematize the study of facial expres-
sions, Ekman and his collaborators designed the Facial
Action Coding System (FACS) [13]. FACS relies on
identifying visible local appearance variations in the
human face, called Action Units (AUs), that are pro-
duced by the individual activation of facial muscles.
Thus, a raised eyebrow is coded as the activation of
the outer brow raiser, and noted AU02. Since any fa-
cial expression can be represented as a combination
of action units, they constitute a natural physiological
basis for face analysis. The existence of such a ba-
sis is a rare boon for a computer vision domain, as
it allows focusing on the essential atoms of the prob-
lem and, by virtue of their exponentially large possible
combinations, opens the door for studying a wide range
of applications beyond prototypical emotion classiﬁca-

1

Figure 2: Overview of AUNets. Our system takes as input a video of a human head and computes its optical
ﬂow ﬁeld. It predicts the viewpoint from which the video was taken, and uses this information to select and evaluate
an ensemble of holistic action unit detectors that were trained for that speciﬁc view. Final AUNets predictions are
then temporally smoothed.

tion. Consequently, in the last years, the main focus of
the community has shifted towards the detection of ac-
tion units, and recent datasets such as BP4D [54] and
the FERA 2017 challenge [46] include annotations by
trained psychologists for multiple action units in indi-
vidual video frames.

When compared to global emotion classiﬁcation, ac-
tion unit detection is a much challenging and ﬁne-
grained recognition task, as shown by the local and
delicate appearance variations in Fig. 1. The expres-
sion of action units is typically brief and unconscious,
and their detection requires analyzing subtle appear-
ance changes in the human face. Furthermore, action
units do not appear in isolation, but as elemental units
of facial expressions, and hence some AUs co-occur
frequently while others are mutually exclusive. In re-
sponse to these challenges, the literature has converged
to a dominant approach for the study of action unit
detection [57, 56, 53, 23, 14, 1, 26, 17, 34] that uses
the localization of facial landmarks as starting point.
Recent methods implement this paradigm in two diﬀer-
ent ways, either using facial landmarks as anchors and
analyzing the appearance of patches centered in those
keypoints as well as their combinations, or using land-
marks to perform face alignment prior to AU detec-
tion. Focusing on ﬁxed facial regions has the advantage
of constraining appearance variations. However, facial
landmark localization in the wild is still an open prob-
lem, its study requires also very expensive annotations,
and its solution is as challenging as the AU detection
itself. Furthermore, while such an approach is suitable
for a near frontal face setup, as is the case in the BP4D
dataset, it ﬁnds its limitations in a multi-view setting
such as FERA 2017, in which large variations in head

pose imply occlusion of multiple facial landmarks and
signiﬁcant appearance changes.

In this paper, we depart from the mainstream ap-
proach for AU detection and propose a system that di-
rectly analyzes the information on the whole human
face in order to predict the presence of speciﬁc ac-
tion units, bypassing thus landmark localization on the
standard benchmarks for this task. For this purpose,
we leverage recent insights on designing Convolutional
Neural Networks (CNNs) for recognition applications.
It is important to mention that [56, 57, 17, 34] can
avoid keypoints, however, they are required to use a
diﬀerent intermediate step in order to align faces.

First, we observe that large-capacity CNN architec-
tures [28, 42, 43, 20] that were originally designed for
object recognition on datasets such as Imagenet [10]
analyze a whole raw image and are capable of making
subtle category distinctions (e.g. between dog breeds).
Moreover, they can be successfully specialized for other
ﬁne-grained recognition problems [50, 5]. Therefore,
we formulate the problem of predicting the presence or
absence of an speciﬁc AU in a single face image as holis-
tic binary classiﬁcation. We explore the design space
of our approach and, in particular, the trade-oﬀ be-
tween eﬃciency and accuracy when considering shared
or independent convolutional representations for diﬀer-
ent action units.

Second,

since action units appear dynamically
within the spatio-temporal continuum of human facial
expressions, we model explicitly temporal information
in two diﬀerent ways. At frame level, we compute an
optical ﬂow ﬁeld and use it as an additional cue for
improving AU detection.Then, we enforce short-term
consistency across the sequence by a smoothing opera-

2

tor that improves our predictions over a small temporal
window.

Third, in order to address multi-view detection, the
main focus of the recent FERA 2017 challenge, we take
inspiration from the work of Dai et al . [9], who showed
that training CNNs for a hard recognition task such as
object instance segmentation on MS-COCO [30] ben-
eﬁts from decomposing learning into a cascade of dif-
ferent sub-tasks of increasing complexity. Thus, our
multi-view system starts by predicting the overall view
of an input sequence before proceeding to the ﬁner
grained task of action unit detection. Furthermore,
as the experiments will show, our system beneﬁts from
a gradual domain adaptation to the ﬁnal multi-view
setup. Fig. 2 presents an overview of our approach,
which we call AUNets.

We perform an extensive empirical validation of
our approach. We develop our system on the BP4D
dataset, the largest and most varied available bench-
mark for frontal action unit detection, where we re-
port an absolute improvement of 7% over the pre-
vious state-of-the-art by Li et al . [29]. We then turn
to the FERA2017 challenge to evaluate our multi-view
system, and report an absolute improvement of 14%
on the F1-metric over the challenge baseline of Valstar
et al . [46], while comparing favorably against state-of-
the-art method by Tang et al . [44]. In order to ensure
reproducibility of our results and to promote future re-
search on action unit detection, all the resources of this
project –source code, benchmarks and results– will be
made publicly available.

2. Related Work

Before deep learning techniques became mainstream
within the ﬁeld of computer vision [28], most meth-
ods relied on the classical two-stage approach of de-
signing ﬁxed handcrafted features such as SIFT [35]
or LBP [39], and then training unrelated classiﬁers for
recognition [47, 33, 1, 53, 14, 56, 26]. However, simi-
larly to AUNets, the best performing techniques cur-
rently available [23, 17, 57, 29] rely on the power of deep
convolutional neural networks for joint representation
and classiﬁcation.

Most recent methods [32, 1, 53, 14, 56, 23, 17, 57, 26]
follow the paradigm of ﬁrst detecting facial landmarks
using external approaches such as Active Appearance
Models [8], either to treat these keypoints as anchors
for extracting rectangular regions for further analysis,
to perform face alignment, or both. The recent method
of Li et al . [29] does not require robust facial keypoint
alignment as it is trained taking this issue into ac-
count; however, facial alignment is recommended and
the method is developed and tested only in a frontal

face setup. In contrast, AUNets operate on the whole
face and do not require any particular alignment in
existing AU detection multi-view benchmarks.

Pioneering methods for this task used the whole
face image as input [47, 31, 33]. However, the trend
reversed towards analyzing patches in more local ap-
proaches such as [32, 1, 53, 14, 56, 23]. State-of-the-art
techniques [17, 57, 29] join AUNets in returning to a
holistic face analysis, in particular Li et al . [29] shares
ideas with Zhao et al . [57] in forcing a CNN-based ap-
proach to speciﬁcally focusing on speciﬁc regions of the
face by using a map saliency. Our method is far from
these approaches since we train speciﬁc networks for
each AU which avoids the need of building attention
maps.

Given that groups of action units can co-occur or
be mutually exclusive in the human face, several meth-
ods [57, 47, 56, 29] have approached the task as multi-
label learning. However, as AUs databases are becom-
ing larger and better annotated [55, 37], these methods
need to be completely retrained to integrate new action
units. In contrast, AUNets are modular by design, and
can naturally evolve towards more general datasets and
new action units. Furthermore, by analyzing the whole
face, AUNets learn naturally the relevant local face re-
gions for predicting each action unit.

While most methods operate on single RGB images
both at training and testing [33, 1, 17, 31], some tech-
niques focus on exploiting the temporal information
for improved AU detection [53, 26, 32]. In particular,
Jaiswal et al . [23] and Chu et al . [6] use CNN’s and
Bidirectional Long Short-Term Memory to model time
dependencies. Similarly, Liu et al . [34] tackle the tem-
poral information by taking advantage of the Optical
Flow [21]; for this purpose, starting from the OF they
extract hand-crafted features as histogram oriented [4]
in order to train a classiﬁer. However, our method
takes advantage of the full resolution of the OF by
feeding it into a CNN. Moreover, on behalf of the Op-
tical Flow, we perform simple statistical operations to
smooth the temporal ﬂow as a post-processing step.

The FERA17 Challenge [46] introduces a new exper-
imental framework for the facial expression recognition
problem. It does not only consider frontal images, but
also multi-view rendered facial images [46]. Our in-
sight comes from realizing that each view should be
detected ﬁrst and then treated independently as well
as each AU. Instead of automatically frontalizing each
view by using facial alignment [46], AUNets, similar
to other FERA17 participants [44, 19], learn indepen-
dent AUs regardless the multi-label setup, and avoiding
intermediate steps such as keypoint detection. How-
ever, AUNets core lies on the simplicity of using in-

3

dependent AUs for each view; thus, by taking advan-
tage of the temporal information we compare favorably
against state-of-the-art methods [44, 19] in this chal-
lenging problem.

3. Approach

3.1. Facial Expression Representation

We formulate individual action unit detection as
holistic binary classiﬁcation in order to build on re-
cent high-capacity CNN architectures [28, 42, 43, 20]
that were originally designed for object classiﬁcation
in large-scale datasets such as Imagenet [10]. Recent
works have shown that these networks can learn useful
representations for diﬀerent object-level applications
such as contour detection [48, 49], semantic segmenta-
tion [38], instance segmentation [9], and action recog-
nition [3].

However, relative to ImageNet, the domain adapta-
tion we require for action unit detection is much larger
than the one for the above mentioned problems, which
are typically studied in VOC PASCAL [15]. There-
fore, we start by learning a high-level representation
on a related but simpler facial expression analysis task.
We achieve this goal by converting the last fully con-
nected layers of a popular CNN architecture such (e.g.,
VGG [42]) into 22 diﬀerent outputs instead of the 1000
of ImageNet, and then re-train it for the task of emo-
tion classiﬁcation.
In order to generalize across eth-
nicity, gender and age, we train the encoder on a non-
traditional dataset [11] labeled beyond the prototypi-
cal 8 basic emotions, instead it uses 22 diﬀerent emo-
tions (e.g., happily surprised, sadly fearful, angrily dis-
gusted, etc) with diﬀerent characteristics, see Figure 3
for details about this dataset. After ”EmoNet”(ENet)
learning is completed, we remove the output layer of
the network and use the learned weights as convolu-
tional encoder for facial expressions in all subsequent
experiments. Our results show that this initial domain
adaptation is beneﬁcial for good performance, indicat-
ing that the convolutional encoder is indeed learning
features that are speciﬁc to faces.

3.2. Single-Image Action Unit Detection

Starting from the learned convolutional encoder,
training a specialized detector for a given action unit in
single RGB images is straightforward. We extend the
encoder with a binary output softmax layer predict-
ing the presence or absence of the AU in the image,
we initialize it with random weights, and then resume
training in a database that provides AU annotations,
such as BP4D. However, when considering multiple ac-
tion units, two architectural options emerge, depending

Figure 3: Dataset for domain adaptation. This
dataset contains images from 230 diﬀerent subjects
(Caucasian, Asian, African American and Hispanic)
and 21 diﬀerent compound emotions (22 including neu-
tral) for a total of 5060 images (See [11] for more de-
tails about the acquisition). Once we extract the face,
we perform data augmentation by randomly cropping
from the center and tuning the saturation, brightness,
contrast and hue.

on whether or not we share features at the lower layers
of the network.

3.2.1 HydraNet

The ﬁrst option is to preserve a shared representa-
tion and to train individual modules for diﬀerent AUs.
This strategy is implemented by freezing the lower lay-
ers of the convolutional encoder and learning separate
weights at the upper layers sequentially for each action
unit. We call this network “Hydra-Net” in reference to
the fabulous Lernaean Hydra from the Greek mythol-
ogy, a dragon with a single body and multiple heads
that would multiply when severed. Hydra-Net is eﬃ-
cient at test time and modular by design, as it can be
easily extended to new action units by “growing” ad-
ditional heads. Furthermore, as the experiments will
show, this baseline system already outperforms the
state-of-the-art on BP4D for single-image AU detec-
tion.

3.2.2 AUNets

The elegance of a shared representation in Hydra-Net
comes at the price of a reduction in absolute perfor-

4

mance, because only a fraction of the original encoder
weights are being specialized for each action unit. In
order to quantify this trade-oﬀ, we train a second vari-
ant of our system in which we relearn all the weights
of the encoder for each action unit. This strategy re-
sults in a battery of independent AU detectors, which
we call “AU-Nets”. When compared to Hydra-Net,
the larger-capacity ensemble of AU-Nets requires more
data at training time and is less eﬃcient at test time.
However, both approaches are modular in the presence
of new action units and, as the experiments will show,
the increased ﬂexibility of AU-Nets allows them to fo-
cus better on speciﬁc regions of the face associated to
diﬀerent action units, leveraging thus local information
for improved performance.

3.3. Dynamic Action Unit Detection

Although trained human observers can annotate ac-
tion units on single images, AUs are continuous and
precisely localized events that occur within the spatio-
temporal ﬂow of facial expressions, and therefore an
explicit analysis of motion information should facili-
tate AU detection when video data is available. We
model motion by computing a dense optical ﬂow ﬁeld
and embedding it into a three dimensional space in
which the ﬁrst two dimensions are normalized x and
y pixel motion and the third dimension is the optical
ﬂow magnitude. This embedding is common in video
analysis applications [16, 41] and has the advantage of
providing a motion signal that has the same scale and
dimensions as the original RGB frame. We considered
three architectures for combining color and motion in-
formation, as illustrated in Fig. 4, b, c, and d.

3.3.1 Additional input Channels

A ﬁrst option is to consider the optical ﬂow embedding
as three additional channels of the input RGB image,
ending up with and input of 224x224x6. This architec-
ture is economical, as it adds only additional weights
for the ﬁrst layer of the network, which are initialized
as copies of the RGB weights. This ﬁrst layer is then
learning to combine all channels for each spatial loca-
tion.

3.3.2 Concatenation

A larger architecture is obtained by concatenating color
and motion images in one of the two spatial dimensions.
In this case, the network analyzes the two signals with
the same ﬁrst-layer ﬁlters, but its ﬁrst fully convolu-
tional architecture is expanded accordingly in order to
combine their information (replicating the pre-trained
weights); resulting also in larger capacity models. The

concatenation is performed in horizontal position, re-
sulting therefore in an input of 224x448x3.

3.3.3 Two streams

A third option is to consider two diﬀerent streams
to represent color and motion information, and to
merge them in higher layers. In this case, we extract
the weights of the RGB encoder and copy them to
initialize the motion encoder. We are thus adapting
the RGB representation to the motion signal, while
simultaneously learning its optimal combination with
the color representation in the deeper layers. This
strategy is also known as ’π Network’ and it is carried
out whether after the last convolutional layer (conv5 3
of VGG), after the ﬁrst FC layer (fc6 of VGG), or
after the second fully connected layer (fc7 of vgg), also
known as π/conv, π/fc6 and π/fc7 respectively.

The experimental section will show that explicit
modeling of motion between successive frames is ben-
eﬁcial for AU detection. Nevertheless, we note that
our dynamic AU detectors above still operate indepen-
dently in each frame, and hence cannot capture the
temporal smoothness of action unit activation. In or-
der to enforce such consistency, we perform a simple
post-processing by applying a temporal sliding median
ﬁlter over predictions of each action unit across the
video. We found this simple smoothing to be empiri-
cally beneﬁcial for all the variants of our approach.

3.4. Multi-View System

Our ﬁnal system is evaluated on the FERA 2017
challenge, which focuses on multi-view AU detection.
Starting from a video of a face that was taken from
an unknown viewpoint among nine options, the task
consists in predicting labels for twelve common action
units on all its frames. We extend our system in two
ways to address this multi-view version of the problem.
First, we train a view classiﬁer, which takes the video as
input and predicts its viewpoint. Once the viewpoint
has been estimated, our cascaded approach proceeds to
evaluate an ensemble of temporally consistent dynamic
AU-Nets that were trained for that speciﬁc view. In
order to perform domain adaptation, we ﬁrst retrain
our system developed in BP4D in the frontal view of
FERA 2017, and then use those weights to initialize
learning for the other views.

4. Experimental Validation

In this section, we provide the empirical evidence
to support our approach. We ﬁrst conduct extensive
experiments in the BP4D dataset analyzing diﬀerent

5

Figure 4: We consider three architectures for combining color and motion information: (a) RGB/OF alone archi-
tecture, (b) additional input channels, (c) an extended image (horizontal concatenation in the input), and (d)
two separate CNN streams (independent networks that fuse in the fully connected layer). Layers whose weights
are altered by the input size are displayed with a red star. Yellow circle depicts the binary output. See text for
more details.

aspects of our method for frontal faces. We then turn to
the FERA 2017 dataset in order to extend our system
to a challenging multi-view setting.

4.1. Results on BP4D-Spontaneous

4.1.1 Experimental Setup

The BP4D-Spontaneous database [54] is currently one
of the largest and most challenging benchmarks avail-
able for the task of action unit detection [54]. BP4D
contains 2D and 3D videos of spontaneous facial ex-
pressions in young people. All of them were recorded
in non-acted scenarios, as the emotions were always
elicited by experts. This dataset contains around
150.000 individual frames from 41 subjects annotated
by trained psychologists.

The BP4D dataset has a signiﬁcant class imbalance
for most AUs; for instance, there are 6 times more neg-
atives samples than positive samples for AU23, while
AU12 is almost balanced. We follow the common prac-
tice in the literature [57, 56, 6, 29], and consider the 12
most frequent AUs [54] for the BP4D dataset. In order
to compare our approach directly with the state-of-the-
art, we report results using the standard performance
metric from the literature, the image-based F1-score
(f1-frame) [45], also known as F-measure and deﬁned
by the harmonic mean of precision and recall, which
considers a threshold of 0.5 in the operating regime
of the detector. Since state-of-the-art weights are not
publicly available, for all the BP4D experiments, we
strictly follow the experimental framework of Zhao et
al . [57] and Chu et al . [6] and perform three-fold cross

validation (so called three fold testing) using their same
splits, in order to ensure that our results are directly
comparable.

It is important to mention that we split the training
set into training and validation. We randomly extract
all the images from one single subject of the full train-
ing set to validation, for the three folds. All hyper-
parameters and training decisions were tuned based on
the performance of the validation set.

4.1.2 Implementation

All models are trained using Adam [27] optimizer with
β1=0.5, β2=0.999. We train with an initial learning
rate of 0.0001 linearly decaying to 0 over the next 12
epochs, unless early stop due to validation convergence
i.e., stop only if F1-val reaches plateau region after 3
consecutive epochs. All experiments were performed
on a single GPU TITAN X. Training details regarding
time consumption are discussed in §5.

To overcome the BP4D dataset’s AU skew, we aug-
ment the positives training samples using the well
known strategy of image jittering [28], that is shift-
ing the bounding box to the right, left, up, down and
any possible combination of those until we reach an
approximate balance with the negative samples.

4.1.3 Domain Adaptation

We ﬁrst quantify the importance of learning a high-
level representation for faces prior to action unit detec-
tion. We select three popular CNN architectures that

6

AU

HydraNets
AlexNet GoogLeNet

VGG

INet ENet INet ENet INet ENet
24.6 39.9
19.8 40.3 27.0 34.1
1
23.6 35.0
16.4 23.3 17.2 28.0
2
40.9 48.9
31.4 43.3 44.9 46.0
4
72.8 75.2
49.4 65.3 71.6 76.3
6
72.0 73.6
51.3 71.3 64.8 70.5
7
81.0 83.8
47.2 83.7 70.4 75.0
10
81.7 86.2
53.7 79.5 75.1 84.1
12
60.0 64.8
23.4 49.1 51.1 58.4
14
19.1 35.8
22.7 40.0 31.3 32.3
15
54.3 58.0
21.4 27.3 51.2 54.9
17
28.6 30.6
20.4 34.2 28.1 29.6
23
35.3 38.8
24
21.0 43.1 37.5 39.7
49.5 55.9
Av. 31.5 49.3 47.5 52.2

AUNets
VGG
ENet
48.4
42.3
55.6
77.8
78.9
81.6
88.4
65.7
50.6
60.4
42.9
47.4
61.6

Table 1: Control experiments for convolutional
encoder. Results are reported over BP4D by using
3-fold cross validation and the F1-metric. It compares
several base networks as starting point for training Hy-
draNets and AUNets, using either Imagenet (INet) or
EmoNet (ENet) pre-trained weights.

were originally designed and trained on ImageNet, and
that have been applied to multiple vision problems:
AlexNet [28], VGG-16 [42], and GoogLeNet [43].
In
order to expose the network to a large variety of faces,
we ﬁrst train an Facial Expression Classiﬁcation net-
work using a non-traditional dataset created by Du et
al . [11] for facial expressions recognition. We chose
this dataset because of its subject variability and not
being grounded to the basic emotions (happiness, sad-
ness, etc). Thus, we modify the output layer for 22-way
emotion classiﬁcation and train following §4.1.2. At the
end, we use the best model trained on EmoNet and it
is called ENet. To this point, we have three diﬀerent
models (AlexNet, GoogLeNet, and VGG) trained on
facial expression classiﬁcation (ENet) and three mod-
els originally trained on ImageNet (INet).

We then use these six convolutional encoders as dif-
ferent initialization to the Hydra-Net architecture in
BP4D and train them until validation convergence.

Results are presented in the left panel of Table 1. We
ﬁrst observe that all three architectures produce rea-
sonable AU detection results when initialized with Im-
ageNet weights, and that deeper architectures produce
improved results, in accordance to their original perfor-
mance in ImageNet. This behavior contrasts with ran-
dom initialization, which causes the training process
to diverge for the three CNNs (results not included).
Furthermore, we observe a signiﬁcant improvement in
AU detection for all base CNN architectures when
initializing with our face expression convolutional en-
coder instead of the original ImageNet weights. Among
the three CNNs considered, VGG-16 generalizes better

to our application domain, obtaining a +6% absolute
improvement in performance over ImageNet weights.
These results highlight the relevance of domain adap-
tation for facial expression analysis. We use the con-
volutional encoder based on the VGG-16 architecture
for all subsequent experiments.

4.1.4 Shared vs. Independent Representation

We next explore the trade-oﬀ between eﬃciency and
accuracy when considering shared or independent rep-
resentations for diﬀerent action units. For this purpose,
we start from our shared representation Hydra-Net and
train with the above mentioned parameters a new fully
independent detector for each AU. Learning is carried
out by unfreezing all layers in the convolutional en-
coder. The right column of Table 1 presents the results
for independent detectors, which we call in the sequel
AUNets. In comparison to HydraNet, under an inde-
pendent representation 11 out of the 12 action units
show improvements. Moreover, we observe an overall
better performance under this strategy as, on average,
the F1 measure increases by 6 over the whole set.

4.1.5 Optical Flow

To incorporate short term motion patterns into our
experimental setup, we estimate the optical ﬂow (OF)
with the variational model proposed by Brox et al . [2].
Given the large scale of the BP4D and FERA17
datasets, we resort to a GPU implementation of this
algorithm [22] as it provides a fair trade-oﬀ between
accuracy and computation time. In practice, we calcu-
late the OF for the whole training set of FERA17 in
under 60 hours with a single Titan X GPU.

To adapt the resulting dense vector map into the
AU-Nets, it is transformed into an RGB image as pro-
posed by Gkioxari et al . [16] and Peng et al . [41], by
independently normalizing the l2 norm of the x and y
dimension over the OF ﬁeld, extracting the face from
the RGB bounding box, and then re-scaling to 224 to
conform the images R and G channels, whereas the B
channel is calculated as the norm of the original vector.
Initially, we consider motion information in isola-
tion and train our AUNets ensemble under the same
experimental set up as the original RGB frames. Col-
umn ’Alone’ in Table 2 summarizes the results for this
experiment. Even without any color information, the
motion signal alone achieves a performance of 50.4%,
which is competitive with the state-of-the-art in BP4D.
We then explore the strategies outlined in Sec-
tion 3.3 to fuse information from the OF and RGB
domains. Table 2-right shows the results for the ﬁve
proposed architectures, including three options for the

7

AU
1
2
4
6
7
10
12
14
15
17
23
24
Av.

AUNets
ENet
48.4
42.3
55.6
77.8
78.9
81.6
88.4
65.7
50.6
60.4
42.9
47.4
61.6

Alone
35.0
26.5
37.6
67.0
69.3
74.0
77.6
61.0
29.2
54.2
36.8
37.0
50.4

Horizontal
52.8
44.0
55.3
78.9
77.5
82.8
88.0
66.2
47.7
61.6
46.9
49.4
62.6

AUNets+OF

Channels
52.0
44.5
54.6
77.4
78.5
83.2
87.8
66.4
47.9
61.2
41.9
45.5
61.7

π/conv
46.8
42.1
55.9
77.6
78.0
83.3
88.7
65.9
50.1
60.1
44.0
50.3
61.9

π/fc6
48.0
41.2
56.8
78.2
79.3
83.2
88.8
65.8
50.8
60.8
45.8
49.8
62.4

π/fc7
53.4
49.8
56.0
77.4
78.4
83.2
88.0
67.3
44.2
61.5
41.7
49.5
62.5

Table 2: Control experiments for Optical Flow. It presents experiments on AUNets and diﬀerent combination
of Optical Flow and Color. Alone means solely training with the OF as input (Figure 4a). Channel corresponds to
the channel embedding (Figure 4b). Horizontal means horizontal concatenation in the input (Figure 4c). π/conv,
π/fc6 and π/fc7 are the diﬀerent two CNN streams that fuses before the FC6, FC7, FC8 layer of vgg, respectively
(Figure 4d). See more details at §3.3.3.

Two Stream strategy. We observe that the motion sig-
nal is complementary to the color information, as the
inclusion of OF always brings an improvement over the
RGB results. The Two Stream and the Concatenation
strategies are both superior to the Additional Chan-
nels, and provide comparable performance.

We conclude that, overall, the best approach is to
concatenate the OF and RGB information at the very
input stage;
it has the very same performance as a
π network joint at the FC7 layer, yet, the number of
parameters is 12% smaller, this means that further do-
main adaptation will be faster, and overﬁt will be less
likely.

4.1.6 Temporal Smoothing

Since action units appear in smooth, short-term du-
ration intervals, we add a post-processing step which
aims at removing instantaneous AU activation and pos-
sible misclassiﬁcation inside a continuous sequence. For
this goal we use a median operator in a sliding win-
dow fashion over the AU detection the sequences. This
hyper-parameter was tuned based on the validation set.
Our approach uses small window sizes as we want
to avoid the suppression of properly detected but short
temporal series of AU activation. Therefore, we test
with window sizes: {3,5,7,9,10,11}, these remain ﬁxed
for all AUs over the entire train set. Empirically we
conclude that the best window sizes are {3,5,7} with
7 being the optimal, longer temporal windows do not
improve the base performance. Table 4 shows the result
(+median) of performing this temporal smoothing to
the best OF arrangement (Horizontal concatenation).

4.1.7 Comparison against the state-of-the-art

We now compare our results against the state-of-the-
art approaches [29, 6, 57, 56]. Table 5 summarizes the
average of three-fold cross validation for each action
unit. We observe that our method consistently outper-
forms all methods for 10 out of 12 action units, with
an average improvement over the F1 measure of 7%,
over EAC.

4.2. Results on the FERA 2017 Challenge

4.2.1 Experimental Setup

The new version of the FERA17 challenge [46] intro-
duces a novel way to approach the problem of facial
expression analysis. This dataset contains a 3D video
database rendered mainly from BP4D [54]. Moreover,
for the test set it includes subjects from a diﬀerent
dataset [55], which do not overlap with BP4D subjects.
The FERA17 dataset includes 9 diﬀerent camera an-
gles for non-symmetric facial views. It contains videos
from 41 subjects and around 1.500.000 frames for the
whole training split. An additional set of videos from
20 diﬀerent subjects with about 750.000 frames inte-
grate the validation split. A ﬁnal set of 1080 videos
are included in the withheld test split.

4.2.2 Viewpoint Classiﬁcation

Our AU-Net ensemble is view speciﬁc, and thus relies
on a proper view selection for optimal results. We ap-
proach this problem by performing a view recognition
sub-task, prior to the AU-Net ensemble.

For this sub-task, we build a view classiﬁer on top of
another deep convolutional encoder. Since we require

8

Params

HydraNets

AUNets

Total
Learnable
12 models

134m
119m
1443m

134m
134m
1608m

Channel
134m
134m
1608m

Horizontal
237m
237m
2844m

AUNets+OF
pi/conv
251m
237m
3012m

pi/fc7
pi/fc6
268m
268m
151m
134m
3216m 3216m

Table 3: Number of parameters per architecture per AU. Total number of parameters and total learn-
able neurons for HydraNet, AUNets and AUNets with Optical Flow with additional input channels, horizontal
concatenation, streaming fusion after last convolutional layer, after FC6 layer and after FC7 layer, respectively

AU
F1

1
53.4

2
44.7

4
55.8

6
79.2

7
78.1

10
83.1

12
88.4

14
66.6

15
47.5

17
62.0

23
47.3

24
49.7

Av.
63.0

Table 4: Results of our best approach (RGB+OF Horizontal concatenation). These results are reported
with median temporal window of 7.

AU JPML[56] DRML[57] MSTC[6] EAC[29] Ours
53.4
1
44.7
2
55.8
4
79.2
6
78.2
7
83.1
10
88.4
12
66.6
14
47.5
15
62.0
17
47.3
23
49.7
24
63.0
Av.

31.4
31.1
71.4
63.3
77.1
45.0
82.6
72.9
34.0
53.9
38.5
37.0
53.2

36.4
41.8
43.0
55.0
67.0
66.3
65.8
54.1
33.2
48.0
31.7
30.0
48.3

39.0
35.2
48.6
76.1
72.9
81.9
86.2
58.8
37.5
59.1
35.9
35.8
55.9

32.6
25.6
37.4
42.3
50.5
72.2
74.1
65.7
38.1
40.0
30.4
42.3
45.9

Table 5: 3-fold cross validation for each Ac-
tion Unit over BP4D dataset using F1-metric.
Comparisons against the state-of-the-art methods:
JPML [56], DRML [57], MSTC [6], and EAC [29].

a simple architecture to train, we start from the Caﬀe-
GoogLeNet [24] reference model, and proceed to learn a
suitable representation for the view classiﬁcation prob-
lem using the full training set. To avoid overﬁtting, we
initially freeze the ﬁrst 6 Inception modules and opti-
mize over the ﬁnal 3 modules (4f,5b,5c), we also drop
the two deep supervision branches as they remain con-
nected to frozen segments of the encoder. We use the
default ImageNet weights for the viewpoint network,
but learn from scratch the weights for the ﬁnal fully
connected layer. After this modiﬁcations the encoder
produces a 9-dimensional output, its soft-max normal-
ized output approximates the probability for each view.
The viewpoint-network is trained for two epochs, with
an initial learning rate of 5−5, gamma parameter of
0.1, and a weight decay of 5−5, using Stochastic Gradi-
ent Descend, we reduce the learning rate after the ﬁrst
epoch.

As ﬁnal step we get a single prediction for any video

as maxv(p), where p is the per frame prediction and v is
the full set of frames for a video. Evaluation results in
Table 6 suggest that the viewpoint classiﬁer is almost
perfect in the FERA 2017 setup. This result indicates
that AU detection will not be aﬀected in any signiﬁcant
way by prior view classiﬁcation.

4.2.3 Multi-View System

For our ﬁrst approach to the FERA17 Challenge, we
want to assess the generalization capability of our sys-
tem, so we proceed to evaluate our system trained in
BP4D over the entire validation partition of FERA17
without any retraining. The evaluation procedure re-
sults in an performance of 46.5%, which shows that
our system trained on BP4D with no previous training
on FERA17 already surpasses the baseline approach for
validation partition in FERA17 Challenge [46] that is
41.6%. We observe that, when the view is close to the
frontal one, our model improves performance, which is
51.9% over the frontal view, and 41.4% over an upper-
left view.

We use the BP4D models as starting-point to train
on the frontal view of FERA17 dataset. We follow
the same strategy for each view, in order to learn the
entire 9 views for 10 AUs, resulting in a total of 90
models. For each view, we learn our models with the
same setup as in BP4D, as outlined in Section 3.4. We
train each view and each AU until validation conver-
gence with learning rate of 10−4 and weight decay of
5−3, the whole system is trained for a total of 50h in a
single Titan X GPU. AUNets+OF trained on FERA17
and evaluated in the validation set obtain 64.4% on
average, which widely outperforms the 40.4% of the
baseline [46], and the FERA17-winner[44], which re-
ports 58.0%.

9

Per Frame Precision
Per Frame Recall
Per Video Precision
Per Video Recall

V1
0.96
0.99
0.97
0.99

V2
0.96
0.95
0.96
0.96

V3
1.0
0.95
1.0
0.95

V4
0.96
1.0
0.96
1.0

V5
0.97
0.96
0.97
0.96

V6
0.99
0.98
0.99
0.98

V7
0.98
0.95
0.98
0.95

V8
0.91
0.93
0.92
0.94

V9
0.94
0.96
0.94
0.96

Av.
0.96
0.96
0.97
0.97

Table 6: Evaluation of Multi-view Classiﬁer, Per frame and per video results for the view recognition sub-task
in the validation set of FERA17.

AU

1
4
6
7
10
12
14
15
17
23
Av.

[46]
57.0
52.0
67.6
64.2
63.8
66.0
62.2
30.7
48.5
37.3
54.9

Validation

Test

ACC
[44] Ours
93.3
78.2
92.7
80.8
83.2
79.9
75.5
73.7
82.3
82.9
85.9
86.0
64.8
66.7
88.1
80.6
82.2
82.2
85.7
86.2
83.4
79.7

[46]
15.4
17.2
56.4
72.7
69.2
64.7
62.2
14.6
22.4
20.7
41.6

F1
[44] Ours
48.7
30.4
55.6
36.2
76.1
71.2
81.8
77.9
83.6
83.6
85.3
84.0
73.4
69.7
43.1
35.3
46.6
44.2
49.6
47.5
64.4
58.0

[46]
53.0
55.7
66.2
66.4
67.1
65.1
61.5
31.0
52.2
43.2
56.1

ACC
[44] Ours
89.8
76.5
93.5
85.7
81.0
79.4
77.7
76.3
81.1
83.2
82.4
82.9
69.2
68.3
83.6
73.6
79.0
76.3
74.3
75.4
81.8
77.8

[46]
14.7
4.40
63.0
75.5
75.8
68.7
66.8
22.0
27.4
34.2
45.2

F1
[44] Ours
30.9
26.3
16.6
11.8
79.9
77.6
83.6
80.8
84.6
86.5
82.7
84.3
78.2
75.7
28.0
36.2
44.3
42.4
48.6
51.9
57.7
57.4

Table 7: Results over FERA17 Validation and Test, using F1-score and Accuracy (ACC). Comparison
with the baseline approach[46] and the challenge winner[44].

4.2.4 Evaluation on Test Set

4.2.5 Eﬃciency

It is important to mention that the test set of
FERA17 Challenge [46] includes subjects from BP4D+
dataset [55], which are diﬀerent subjects that the BP4D
dataset [54]. We include the results obtained during the
FERA17 Challenge, where the test server was made
available from February 8 to March 1 2017 with a limit
of 5 submissions per group. The challenge rules re-
quired participants to upload their source code to the
FERA17 server, and the organizers ran the models on
an undisclosed test set. Following the challenge setup,
we report F1-score and ACC in Table 7, 8 and 9, for
Val and Test sets. Our ﬁnal submitted system is rep-
resented in Fig. 2. As can be observed, our model sets
a new benchmark in the problem of multi-view action
unit detection by improving with an 14% in F1-score
over the baseline approach in the Test set, and it is on-
par with the challenge winner [44], under that metric.
Furthermore, on the ACC metric, we outperform both
the baseline and the challenge winner with an absolute
improvement of 15% and 4% respectively.

10

We measure the average execution time for the ﬁnal
multi-view system as follows: For a video with 500
frames, it takes on average 12 seconds to transform
it into frames. The Optical Flow takes about 2 min-
utes producing the whole set of OF frames. Once the
RGB and OF frames are complete, the view detector
takes around 2 seconds predicting the corresponding
view of the video from a frame sub-sampling. For the
ﬁnal step, it takes roughly 8 more minutes to forward
every frame of the video through the 10 AUs detectors.
In summary, it takes less than 1 second to process each
frame once the OF is computed.

4.3. Qualitative Results

Similar to Zhao et al . [57] and Chu et al . [6], we
present qualitative results over BP4D dataset using the
visualization techniques of Zeiler and Fergus [52], and
Yosinski [51].

One of Zeiler’s insights is to occlude parts of the
input image in order to look for the classiﬁcation prob-
ability this region is supplying to the full image. Fig. 5
show this approach over several images for diﬀerent
AUs. We can observe that our approach emphasizes
very speciﬁc regions of the face (third row) using thus

AU

v1
v2
1
45.1
47.3
4
53.4
57.7
6
75.4
73.6
7
81.6
81.9
10
82.4
83.9
12
86.2
86.6
14
72.3
73.6
15
40.4
45.5
17
52.0
52.4
54.2
52.0
23
Av. 64.1 65.7

AU
v1
91.2
1
92.1
4
81.4
6
75.3
7
80.1
10
87.1
12
62.5
14
86.8
15
83.4
17
23
87.5
Av. 82.7

v2
92.9
94.4
81.6
75.9
83.1
87.8
64.6
90.4
86.5
88.5
84.6

F1-score
Ours
v3
v6
v9 Global
45.2
54.8
47.1
46.0
55.3
43.1
77.6
79.0
74.4
81.4
82.6
80.8
82.6
84.8
82.0
84.6
85.2
82.9
71.7
76.4
68.2
40.2
47.7
28.6
51.0
48.4
36.5
34.9
54.2
53.6
63.4 65.8 68.6 66.8 64.3 62.7 57.9

48.7
55.6
76.1
81.8
83.6
85.3
73.4
43.1
46.6
49.6
64.4

v4
54.5
53.3
77.2
81.9
83.1
86.7
73.8
53.0
45.2
49.4

v8
49.6
51.1
72.3
81.7
83.7
84.9
74.9
39.9
44.8
44.4

v5
49.5
81.5
77.7
83.3
84.5
85.3
77.0
47.4
45.7
53.9

v7
45.0
59.1
77.3
80.7
85.0
85.2
72.5
45.0
43.7
49.4

v7
v5
v3
92.3
93.4
92.2
92.6
97.3
90.0
84.2
84.1
84.5
73.8
79.0
73.9
83.5
83.4
81.4
83.9
86.2
86.0
63.4
71.1
62.4
90.6
91.0
84.7
79.8
85.8
85.0
87.3
86.4
87.8
82.7 83.9 85.9 85.9 83.1

ACC
v6
95.2
94.0
85.9
77.6
83.6
86.2
72.7
91.4
84.9
87.9

v4
94.6
93.2
84.5
76.3
81.9
87.5
65.0
92.4
76.1
87.3

v9 Global
v8
94
93.7
88.1
92.7
82.7
80.1
72.4
74.9
80.6
82.8
83.4
85.3
54.9
66.6
75.9
89.8
76.4
82.0
82.3
76.7
83.0 78.5

93.3
92.7
83.2
75.5
82.3
85.9
64.8
88.1
82.2
85.7
83.4

Table 8: Results over FERA17 Validation, using F1-score and Accuracy (ACC).

AU

v1
33.7
22.6
79.8
82.0
80.7
82.3
77.4
23.2
50.1
46.5

v9 Global
28.7
1
19.0
4
79.7
6
83.3
7
84.3
10
81.9
12
77.3
14
29.1
15
37.8
17
23
44.7
Av. 57.8 59.1 59.4 58.5 58.7 56.9 56.5 56.7 56.6

30.9
16.6
79.9
83.6
84.6
82.7
78.2
28.0
44.3
48.6
57.7

v2
30.5
29.2
80.3
85.2
83.8
81.8
78.9
21.1
48.4
51.4

v3
28.8
13.2
80.4
85.0
81.4
83.7
78.0
38.8
52.5
51.7

v5
32.3
14.9
79.2
83.7
87.8
84.5
79.1
28.0
43.5
54.3

v8
27.5
19.3
80.4
82.8
84.7
82.1
77.8
30.2
37.9
44.3

v4
35.0
18.9
79.8
83.3
85.9
84.0
78.1
26.6
43.2
50.6

v7
34.0
14.7
79.7
83.2
86.1
79.3
76.8
26.4
42.3
42.4

F1-score
Ours
v6
28.1
2.90
80.1
83.5
85.8
84.1
80.3
24.1
48.7
51.3

v1
92.4
97.3
81.5
76.4
76.8
81.8
66.5
82.4
83.0
72.2

AU
v7
89.2
1
92.7
4
80.9
6
75.9
7
82.4
10
79.0
12
67.7
14
82.9
15
74.0
17
23
72.9
Av. 81.0 83.3 82.3 81.1 82.2 82.3 79.8

v3
90.6
93.8
81.5
80.3
79.2
83.6
68.7
84.2
85.1
75.9

v2
91.3
97.6
81.9
80.5
81.5
82.6
70.4
86.1
84.3
77.0

v4
90.3
94.7
80.1
77.1
81.9
82.7
68.7
84.6
75.4
75.5

v5
87.7
87.9
79.0
78.4
84.3
83.5
72.6
86.7
82.5
79.1

ACC
v6
89.9
94.9
81.6
78.5
81.9
83.1
73.2
85.1
82.3
72.6

v9 Global
v8
88.7
88.0
91.2
91.5
80.8
81.5
76.6
75.8
80.9
81.4
82.4
82.5
67.1
68.2
79.7
81.2
72.1
73.1
71.4
72.1
79.5 79.2

89.8
93.5
81.0
77.7
81.1
82.4
69.2
83.6
79.0
74.3
81.2

Table 9: Results over FERA17 Test set, using F1-score and Accuracy .

local information for each action unit despite analyzing
the face hollistically and not using facial alignment.

Additionally, Yosinski’s method creates synthetic
images as input in order to maximize the output of

one speciﬁc neuron; for instance, we maximize the out-
put of the binary one. Similar to Zeiler’s, Fig. 6 shows
that AU1 and AU2 models are focused on the eyes,
thus AU12 and AU15 representations are looking for

11

patterns over the mouth region, conﬁrming thus the
speciﬁcity of these models.

5. Limitations

Despite our approach radically pushes forward the
state-of-the-art in this problem, it has two main short-
comings regarding the huge number of parameters (12
models for each fold in BP4D, and 90 diﬀerent models
for FERA17) and the time required to train all of them.
Table 3 summarizes the number of parameters. On the
other hand, in order to train 12 models in 3 fold cross
validation over a 12GB GPU Titan X it takes around
one week per variant architecture.

6. Conclusions

We have presented HydraNets and AUNets for rec-
ognizing 12 diﬀerent facial action units. Our method
takes advantage of the power of CNNs for large-scale
classiﬁcation problems, and is capable of detecting mul-
tiple action units simultaneously. Our method uses
the temporal information OF to enhance performance.
There is a trade-oﬀ between eﬃciency and performance
with HydraNets and AUNets, yet both approaches ob-
tain competitive results when compared against state-
of-the-art methods, and our ﬁnal multi-view system
compares favorably in performance against state-of-
the-arts approaches in a challenging benchmark. At
the core of our approach lies a ﬂexible and modu-
lar architecture that can easily incorporate new action
units. In order to promote further research in action
unit detection, our source code, models, and results
can be found at https://github.com/BCV-Uniandes/
AUNets1.

7. Acknowledgements

This work was partially supported by a Google
Research Award Latin America. We are grateful to
NVIDIA Corporation for donating the GPUs used in
this work.

References

[1] T. Almaev, B. Martinez, and M. Valstar. Learning
to transfer: Transferring latent task structures and its
application to person-speciﬁc facial action unit detec-
tion. In ICCV, 2015. 2, 3

[2] T. Brox and J. Malik. Large displacement optical ﬂow:
descriptor matching in variational motion estimation.
TPAMI, 2011. 7

[3] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Car-
los Niebles. Activitynet: A large-scale video bench-

1Written in PyTorch framework [40].

mark for human activity understanding.
2015. 4

In CVPR,

[4] R. Chaudhry, A. Ravichandran, G. Hager, and R. Vi-
dal. Histograms of oriented optical ﬂow and binet-
cauchy kernels on nonlinear dynamical systems for the
recognition of human actions. In CVPR, 2009. 3
[5] G. Ch´eron, I. Laptev, and C. Schmid. P-cnn: Pose-
In ICCV,

based cnn features for action recognition.
2015. 2

[6] W.-S. Chu, F. De la Torre, and J. F. Cohn. Modeling
spatial and temporal cues for multi-label facial action
unit detection. arXiv preprint arXiv:1608.00911, 2016.
3, 6, 8, 9, 10

[7] CMU. Facial action coding system, 2002. 13
[8] T. F. Cootes, G. J. Edwards, and C. J. Taylor. Active

appearance models. In ECCV. 1998. 3

[9] J. Dai, K. He, and J. Sun. Instance-aware semantic seg-
mentation via multi-task network cascades. In ICCV,
2016. 3, 4

[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and
L. Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, 2009. 2, 4

[11] S. Du, Y. Tao, and A. M. Martinez. Compound facial
expressions of emotion. Proceedings of the National
Academy of Sciences, 111(15):E1454–E1462, 2014. 4,
7

[12] P. Ekman and W. V. Friesen. Facial action coding

system. 1977. 1

[13] P. Ekman and E. L. Rosenberg. What the face reveals:
Basic and applied studies of spontaneous expression us-
ing the Facial Action Coding System (FACS). Oxford
University Press, USA, 1997. 1

[14] S. Eleftheriadis, O. Rudovic, and M. Pantic. Multi-
conditional latent variable model for joint facial action
unit detection. In ICCV, 2015. 2, 3

[15] M. Everingham, L. Van Gool, C. K. Williams, J. Winn,
and A. Zisserman. The pascal visual object classes
(voc) challenge. International journal of computer vi-
sion, 88(2):303–338, 2010. 4

[16] G. Gkioxari and J. Malik. Finding action tubes. In

CVPR, 2015. 5, 7

[17] A. Gudi, H. E. Tasli, T. M. den Uyl, and A. Maroulis.
Deep learning based facs action unit occurrence and
intensity estimation. In 11th IEEE International Con-
ference and Workshops on Automatic Face and Ges-
ture Recognition, 2015. 2, 3

[18] S. Happy and A. Routray. Automatic facial expres-
sion recognition using features of salient facial patches.
IEEE transactions on Aﬀective Computing, 6(1):1–12,
2015. 1

[19] J. He, D. Li, B. Yang, S. Cao, B. Sun, and L. Yu.
Multi view facial action unit detection based on cnn
and blstm-rnn. In Automatic Face & Gesture Recog-
nition (FG 2017), 2017 12th IEEE International Con-
ference on, pages 848–853. IEEE, 2017. 3, 4

[20] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual

learning for image recognition. In CVPR, 2016. 2, 4

12

Inner Brow Raiser Outer Brow Raiser Upper Lip Raiser

Lip Corner Puller

Lip Corner
Depressor

AU01

AU02

AU10

AU12

AU15

AU01

AU02

AU10

AU12

AU15

AU01

AU10
Figure 5: Zeiler’s method [52] for network visualization. The top row presents 5 diﬀerent Action Units [7]. The heat
maps highlight the most important regions in the human face for each speciﬁc Action Unit (blue: less important,
red: more important).

AU12

AU02

AU15

AU01

AU10
Figure 6: Yosinski’s method [51] for network visualization. This approach shows synthetic hallucinations that
maximize the output of the network for AU 1,2,10,12,15 i.e., generate the best image that entirely maximize the
output neuron.

AU02

AU12

AU15

[21] B. K. Horn and B. G. Schunck. Determining optical

2016. 2, 3

ﬂow. Artiﬁcial intelligence, 17(1-3):185–203, 1981. 3

[22] Itseez. Open source computer vision library. https:

//github.com/itseez/opencv, 2015. 7

[23] S. Jaiswal and M. Valstar. Deep learning the dynamic
appearance and shape of facial action units. In WACV,

[24] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long,
R. Girshick, S. Guadarrama, and T. Darrell. Caﬀe:
Convolutional architecture for fast feature embedding.
In ACM, 2014. 9

[25] S. E. Kahou, P. Froumenty, and C. Pal. Facial expres-

13

sion analysis based on high dimensional binary fea-
tures. In ECCV, 2014. 1

[26] S. Kaltwang, S. Todorovic, and M. Pantic. Latent
trees for estimating intensity of facial action units. In
CVPR, 2015. 2, 3

[27] D. P. Kingma and J. Ba. Adam: A method for stochas-
arXiv preprint arXiv:1412.6980,

tic optimization.
2014. 6

[28] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Ima-
genet classiﬁcation with deep convolutional neural net-
works. In NIPS, 2012. 2, 3, 4, 6

[29] W. Li, F. Abtahi, Z. Zhu, and L. Yin. Eac-net:
A region-based deep enhancing and cropping ap-
proach for facial action unit detection. arXiv preprint
arXiv:1702.02925, 2017. 3, 6, 8, 9

[30] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona,
D. Ramanan, P. Doll´ar, and C. L. Zitnick. Microsoft
coco: Common objects in context. In ECCV, 2014. 3
[31] M. Liu, S. Li, S. Shan, and X. Chen. Au-aware deep
In Auto-

networks for facial expression recognition.
matic Face and Gesture Recognition (FG), 2013. 3
[32] M. Liu, S. Li, S. Shan, R. Wang, and X. Chen. Deeply
learning deformable facial action parts model for dy-
namic expression analysis. In ACCV, 2014. 3

[33] P. Liu, S. Han, Z. Meng, and Y. Tong. Facial expres-
sion recognition via a boosted deep belief network. In
CVPR, 2014. 3

[34] Y.-J. Liu, J.-K. Zhang, W.-J. Yan, S.-J. Wang,
G. Zhao, and X. Fu. A main directional mean optical
ﬂow feature for spontaneous micro-expression recog-
nition.
IEEE Transactions on Aﬀective Computing,
7(4):299–310, 2016. 2, 3

[35] D. G. Lowe. Object recognition from local scale-
invariant features. In Computer vision, 1999. The pro-
ceedings of the seventh IEEE international conference
on, volume 2, pages 1150–1157. Ieee, 1999. 3

[36] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Am-
badar, and I. Matthews. The extended cohn-kanade
dataset (ck+): A complete dataset for action unit and
emotion-speciﬁed expression. In CVPR, 2010. 1
[37] M. Mavadati, P. Sanger, and M. H. Mahoor. Extended
disfa dataset: Investigating posed and spontaneous fa-
cial expressions. In CVPR Workshops, 2016. 3
[38] H. Noh, S. Hong, and B. Han. Learning deconvolution
network for semantic segmentation. In ICCV, 2015. 4
[39] T. Ojala, M. Pietikainen, and D. Harwood. Perfor-
mance evaluation of texture measures with classiﬁca-
tion based on kullback discrimination of distributions.
In Pattern Recognition, 1994. Vol. 1-Conference A:
Computer Vision & Image Processing., Proceedings of
the 12th IAPR International Conference on, volume 1,
pages 582–585. IEEE, 1994. 3

[40] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang,
Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and
A. Lerer. Automatic diﬀerentiation in pytorch. 2017.
12

[41] X. Peng and C. Schmid. Multi-region two-stream r-cnn

for action detection. In ECCV, 2016. 5, 7

[42] K. Simonyan and A. Zisserman. Very deep convo-
lutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014. 2, 4, 6

[43] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Ra-
binovich. Going deeper with convolutions. In CVPR,
2015. 2, 4, 6

[44] C. Tang, W. Zheng, J. Yan, Q. Li, Y. Li, T. Zhang, and
Z. Cui. View-independent facial action unit detection.
In Automatic Face & Gesture Recognition (FG 2017),
2017 12th IEEE International Conference on, pages
878–882. IEEE, 2017. 3, 4, 9, 10

[45] M. F. Valstar, M. Mehu, B. Jiang, M. Pantic, and
K. Scherer. Meta-analysis of the ﬁrst facial expres-
sion recognition challenge. Systems, Man, and Cyber-
netics, Part B: Cybernetics, IEEE Transactions on,
42(4):966–979, 2012. 6

[46] M. F. Valstar, E. S´anchez-Lozano, J. F. Cohn, L. A.
Jeni, J. M. Girard, Z. Zhang, L. Yin, and M. Pan-
tic. Fera 2017-addressing head pose in the third facial
expression recognition and analysis challenge. arXiv
preprint arXiv:1702.04174, 2017. 2, 3, 8, 9, 10

[47] Z. Wang, Y. Li, S. Wang, and Q. Ji. Capturing global
semantic relationships for facial action unit recogni-
tion. In ICCV, 2013. 3

[48] S. ”Xie and Z. Tu. Holistically-nested edge detection.

In ICCV, 2015. 4

[49] J. Yang, B. Price, S. Cohen, H. Lee, and M.-H. Yang.
Object contour detection with a fully convolutional
encoder-decoder network. In CVPR, 2016. 4

[50] L. Yang, P. Luo, C. Change Loy, and X. Tang. A
large-scale car dataset for ﬁne-grained categorization
and veriﬁcation. In CVPR, 2015. 2

[51] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lip-
son. Understanding neural networks through deep vi-
sualization. In ICML, 2015. 10, 13

[52] M. D. Zeiler and R. Fergus. Visualizing and under-
standing convolutional networks. In ECCV, 2014. 10,
13

[53] J. Zeng, W.-S. Chu, F. De la Torre, J. F. Cohn, and
Z. Xiong. Conﬁdence preserving machine for facial
action unit detection. In ICCV, 2015. 2, 3

[54] X. Zhang, L. Yin, J. F. Cohn, S. Canavan, M. Reale,
A. Horowitz, P. Liu, and J. M. Girard.
Bp4d-
spontaneous: a high-resolution spontaneous 3d dy-
namic facial expression database. Image and Vision
Computing, 32(10):692–706, 2014. 2, 6, 8, 10

[55] Z. Zhang, J. M. Girard, Y. Wu, X. Zhang, P. Liu,
U. Ciftci, S. Canavan, M. Reale, A. Horowitz, H. Yang,
et al. Multimodal spontaneous emotion corpus for hu-
man behavior analysis. In CVPR, 2016. 3, 8, 10
[56] K. Zhao, W.-S. Chu, F. De la Torre, J. F. Cohn, and
H. Zhang. Joint patch and multi-label learning for
facial action unit detection. In CVPR, 2015. 2, 3, 6,
8, 9

[57] K. Zhao, W.-S. Chu, and H. Zhang. Deep region and
multi-label learning for facial action unit detection. In
CVPR, 2016. 2, 3, 6, 8, 9, 10

14

