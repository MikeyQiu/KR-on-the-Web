Handwritten Chinese Font Generation with Collaborative Stroke Reﬁnement

Chuan Wen1

Jie Chang1 Ya Zhang1

Siheng Chen2 Yanfeng Wang1 Mei Han3 Qi Tian4

1Cooperative Madianet Innovation Center, Shanghai Jiao Tong University
2Mitsubishi Electric Research Laboratories
3PingAn Technology US Research Lab
4Huawei Technologies Noah’s Ark Lab

{alvinwen, j chang, ya zhang, wangyanfeng}@sjtu.edu.cn

sihengc@andrew.cmu.edu

hanmei613@pingan.com.cn

tian.qi1@huawei.com

9
1
0
2
 
y
a
M
 
6
 
 
]

V
C
.
s
c
[
 
 
3
v
8
6
2
3
1
.
4
0
9
1
:
v
i
X
r
a

Abstract

Automatic character generation is an appealing solu-
tion for new typeface design, especially for Chinese type-
faces including over 3700 most commonly-used characters.
This task has two main pain points: (i) handwritten char-
acters are usually associated with thin strokes of few infor-
mation and complex structure which are error prone dur-
ing deformation; (ii) thousands of characters with various
shapes are needed to synthesize based on a few manually
designed characters. To solve those issues, we propose a
novel convolutional-neural-network-based model with three
main techniques: collaborative stroke reﬁnement, using col-
laborative training strategy to recover the missing or broken
strokes; online zoom-augmentation, taking the advantage of
the content-reuse phenomenon to reduce the size of train-
ing set; and adaptive pre-deformation, standardizing and
aligning the characters. The proposed model needs only
750 paired training samples; no pre-trained network, extra
dataset resource or labels is needed. Experimental results
show that the proposed method signiﬁcantly outperforms
the state-of-the-art methods under the practical restriction
on handwritten font synthesis.

Figure 1. Collaborative stroke reﬁnement handles the thin issue.
Handwritten characters usually have various stroke weights. Thin
strokes have lower fault tolerance in synthesis, leading to distor-
tions. Collaborate stroke reﬁnement adopts collaborative train-
ing. An auxiliary branch is introduced to capture various stroke
weights, guiding the dominating branch to solve the thin issue.

1. Introduction

Chinese language consists of more than 8000 characters,
among which about 3700 are frequently used. Designing
a new Chinese font involves considerable tedious manual
efforts which are often prohibitive. A desirable solution is
to automatically complete the rest of vocabulary given an
initial set of manually designed samples, i.e. Chinese font
synthesis.
Inspired by the recent progress of neural style
transfer [6, 9, 10, 18, 21, 26], several attempts have been re-
cently made to model font synthesis as an image-to-image
translation problem [7, 19, 24]. Unlike neural style transfer,

font transfer is a low fault-tolerant task because any mis-
placement of strokes may change the semantics of the char-
acters [7]. So far, two main streams of approaches have
been explored. One addresses the problem with a bottle-
neck CNN structure [1, 2, 7, 15, 12] and the other proposes
to disentangle representation of characters into content and
style [19, 24]. While promising results have been shown in
font synthesis, most existing methods require an impracti-
cable large-size initial set to train a model. For example,
[1, 2, 7, 15] require 3000 paired characters for training su-
pervision, which requires huge labor resources. Several re-

1

mation to standardize characters. This allows the proposed
networks focus on learning high-level style deformation.

Combining the above methods, we proposed an end-
to-end model (as shown in Fig.3), generating high-quality
characters with only 750 training samples. No pre-trained
network, or labels is needed. We verify the performance
of the proposed model on several Chinese fonts including
both handwritten and printed fonts. The results demonstrate
the signiﬁcant superiority of our proposed method over the
state-of-the-art Chinese font generation models.

The main contributions are summarized as follows.

• We propose an end-to-end model to synthesize hand-
written Chinese font given only 750 training samples;

• We propose collaborative stroke reﬁnement, handling
the thin issue; online zoom-augmentation, enabling
learning with fewer training samples; and adaptive pre-
deformation, standardizing and aligning the charac-
ters;

• Experiments show that the proposed networks are far
superior to baselines in respect of visual effects, RMSE
metric and user study.

2. Related Work

Most image-to-image translation tasks such as art-style
transfer [8, 13], coloring [5, 23], super-resolution [14], de-
hazing [3] have achieved the progressive improvement since
the raise of CNNs. Recently, several works model font
glyph synthesis as a supervised image-to-image translation
task mapping the character from one typeface to another
by CNN-framework analogous to an encoder/decoder back-
bone [1, 2, 4, 7, 12, 15]. Specially, generative adversarial
networks (GANs) [11, 16, 17] are widely adopted by them
for obtaining promising results. Differently, EMD [24] and
SA-VAE [19] are proposed to disentangle the style/content
representation of Chinese fonts for a more ﬂexible transfer.
Preserving the content consistency is important in font
synthesis task. “From A to Z” assigned each Latin alpha-
bet a one-hot label into the network to constrain the con-
tent [20]. Similarly, SA-VAE [19] embedded a pre-trained
Chinese character recognition network [22, 25] into the
framework to provide a correct content label. SA-VAE also
embedded 133-bits coding denoting the structure/content-
reuse knowledge of inputs, which must rely on the extra
In
labeling related to structure and content beforehand.
order to synthesize multiple font, zi2zi [2] utilized a one-
hot category embedding. Similarly, DCFont [12] involved
a pre-trained 100-class font-classiﬁer in the framework to
provide a better style representation. And recently, MC-
GAN [4] can synthesize ornamented glyphs from a few ex-
amples. However, the generation of Chinese characters are

Figure 2. Online zoom-augmentation exploits the content-reuse
phenomenon in Chinese characters. A same radical can be
reused in various characters at various locations. Online zoom-
augmentation implicitly model the variety of locations and sizes
from elementary characters without decomposition, signiﬁcantly
reducing the number of training samples.

cent methods [12, 19, 24] explore learning with less paired
characters; however, these methods heavily depend on ex-
tra labels or other pre-trained networks. Table 1 provides a
comparative summary of the above methods.

Most of the above methods focus on printed font synthe-
sis. Compared with printed typeface synthesis, handwritten
font synthesis is a much more challenging task. First, its
strokes are thinner especially in the joints of strokes. Hand-
writing fonts are also associated with irregular structures
and are hard to be modeled. So far there is no satisfying
solution for handwritten font synthesis.

This paper aims to synthesize more realistic handwrit-
ten fonts with fewer training samples. There are two main
challenges: (i) handwritten characters are usually associ-
ated with thin strokes of few information and complex struc-
ture which are error prone during deformation; (see Fig-
ure 1) and (ii) thousands of characters with various shapes
are needed to synthesize based on a few manually de-
signed characters (see Figure 2). To solve the ﬁrst issue,
we propose collaborative stroke reﬁnement that using col-
laborative training strategy to recover the missing or bro-
ken strokes. Collaborative stroke reﬁnement uses an aux-
iliary branch to generate the syntheses with various stroke
weights, guiding the dominating branch to capture charac-
ters with various stroke weights. To solve the second issue,
we fully exploit the content-reuse phenomenon in Chinese
characters; that is, the same radicals may present in various
characters at various locations. Based on this observation,
we propose online zoom-augmentation, which synthesizes
a complicated character by a series of elementary compo-
nents. The proposed networks can focus on learning va-
rieties of locations and ratios of those elementary compo-
nents. This signiﬁcantly reduces the size of training sam-
ples. To make the proposed networks easy to capture the
deformation from source to target, we further propose adap-
tive pre-deformation, which learns the size and scale defor-

2

Methods
Rewrite [1]
AEGN [15]
HAN [7]
zi2zi [2]
DCFont [12]
SA-VAE [19]
EMD [24]
Ours

Table 1. Comparison of Our with existing Chinese font transfer methods based on CNN.
Requirements of data resources

Dependence of extra label?

Dependence of pre-trained network?

3000 paired samples.

None

Hundreds of fonts (each containing 3000 samples).
Hundreds of fonts (each containing 755 samples).
Hundreds of fonts (each containing
3000 samples).

Only 755 paired samples

Class-label
for each font.
133-bit structural code for each character.

14

None

None

None

Pre-trained font-classiﬁer based on Vgg16
Pre-trained character recognition network

Figure 3. Schematic of proposed model including coarse generator and collaborative stroke reﬁnement. In coarse generator and source
image x is transformed to 32 × 32 × 128 feature map after adaptive pre-deformation, online zoom-augmentation A. In collaborative stroke
reﬁnement, auxiliary branch generates ˆb(y), which uses b(y) as ground-truth. Simultaneously, dominating branch generates ˆy guided
by our desired target y. And in reﬁnement branch, ˆb(y) is eroded to a thin version g( ˆb(y)) and feature maps extracted from it ﬂow to
dominating branch as compensation. Besides, skip-connections are used between coarse generator and the reﬁne branch.

very different from English alphabet no matter in complex-
ity or considerations. EMD [24] is the ﬁrst model which
can achieve good performance on new Chinese font transfer
with a few samples, but it does work poorly on handwritten
fonts.

3. Methodology

The task of handwritten Chinese font generation is that
given a source character, we aim to generate the same char-
acter in the target style. Mathematically, let C = {ci}m
i=0
be a image set of m standardized Chinese characters, where
each element ci represents a single Chinese character. Let
the source set X = {xi = dsource(ci)|ci ∈ C}i and the tar-
get set Y = {yi = dtarget(ci)|ci ∈ C}i be two training im-
age sets representing the characters C in two styles, where
dsource(·) denotes the deformation function in the source
style and dtarget(·) denotes the deformation function in the
target style. In the training phase, we train with the source-
target pairs, X and Y. In the testing phase, given an input
image, x = dsource(c), we should produce a synthesis im-
age, y = dtarget(c). Since that we are blind to both defor-
mation functions, the key of this task is to learn a mapping
from dsource(·) to dtarget(·) based on training sets X and Y.
To make it practical, we want the size of the training set m

as small as possible. Here we use deep neural networks to
learn the mapping from dsource(·) to dtarget(·)

The proposed networks are illustrated in Fig. 3. The
proposed networks consist of a coarse generator, which is
a shared encoder-decoder module to generate low resolu-
tion feature maps, collaborative stroke reﬁnement, which
follows an encoder-decoder framework and generates the
results. We further use adaptive pre-deformation and online
zooming to augment the training data, which carefully ex-
ploit the speciﬁc properties of Chinese letters, enabling the
proposed network to train with fewer training samples.

3.1. Collaborative Stroke Reﬁnement

As illustrated in Fig.1, we see that generating bold
strokes of handwritten fonts are usually easier than gener-
ating thin strokes of handwritten fonts, because thin strokes
have fewer information and have lower fault tolerance in
the reconstruction process. In practice, a single handwritten
font may involve various stroke weights, making the syn-
thesis challenging; see thin issue in Fig. 1.

To solve this issue, we propose collaborative stroke re-
ﬁnement, which uses multitask learning and collaborative
training strategy. Collaborative stroke reﬁnement includes
three branches: dominating branch, auxiliary branch and

3

Figure 4. Dilation may cause the strokes overlapping issue.

3.2. Online Zoom-Augmentation

reﬁne branch; see Fig. 3. The auxiliary branch learns stroke
deformations between the source x and a bolded target b(y),
where b(·) is a bolding function; the reﬁne branch merges
information from the auxiliary branch to the dominating
branch; and ﬁnally, the dominating branch learns the stroke
deformation between the source x and the original target y
with auxiliary information from the reﬁne branch. We train
three branches simultaneously.

Auxiliary branch. The auxiliary branch synthesizes ˆb(y)
to approximate the bolded target b(y) based on the source
x. The bolded target is obtained from original target by
applying the morphological dilation operation:
b(y) = y ⊕ e = {z|(ˆe)z ∩ y (cid:54)= ∅},
where e is the structuring element, ˆ∗ denotes reﬂecting all
elements of ∗ about the origin of this set, (∗)z denotes trans-
lating the origin of ∗ to point z. The synthesis ˆb(y) has bold
strokes and are robust to potential distortion. Since hand-
written fonts’ strokes are usually highly connected, simply
using dilation may cause overlapping of strokes; see Fig. 4.
Instead of using ˆb(y) as the ﬁnal synthesis, we use it as aux-
iliary information and output to the reﬁne branch.

Reﬁne branch. Reﬁne branch merges information from
the auxiliary branch to the dominating branch. To allow
the dominating branch to capture various stroke weights,
we aim to make ˆb(y) thinner; however, directly using the
erosion operation, which is the inverse of dilation, blocks
the end-to-end training due to erosion is not differentiable.
To overcome this, we use max-pooling to mimic erosion,
which is differentiable. We use max-pooling twice to pro-
duce two stroke-weight syntheses, g( ˆb(y)) and g(g( ˆb(y))),
where g(·) denote the max-pooling with stride 2. We in-
put each of two syntheses, g( ˆb(y)) and g(g( ˆb(y))), to the
convolution layers separately and obtain feature maps that
reﬂect the same style with various stroke weights.

Dominating branch. The dominating branch collects
auxiliary information from the reﬁne branch and produces
the ﬁnal synthesis. The network structure is based on HAN
with a generator and a hierarchical discriminator [7] (a dis-
criminator to distinguish not only between the generated
image and ground truth, but also between the feature maps,
obtained from the ﬁrst few layers before the output layer,
and the feature maps of ground truth by conducting the
same convolution), making the model converge faster and
the results more smooth. The input of the generator is
the output feature maps of the coarse generator. We pro-
cess it with a convolution layer and then concatenate it with

4

g( ˆb(y)), which is the auxiliary information from the reﬁne
branch. We next process the concatenation with another
convolution layer and concatenate it with g(g( ˆb(y))). Twice
concatenations allow the dominating branch to be aware of
various stroke weights. We put the concatenation to the
CNN to reconstruct the ﬁnal synthesis ˆy. The hierarchi-
cal discriminator tries to distinguish the synthesis ˆy and its
feather maps from the ground truth y and y’s feature maps,
promoting the generator to converge faster. The dominating
branch fuses auxiliary information from the reﬁne branch
twice to capture various stroke weights, making the ﬁnal
synthesis robust to distortion and missing strokes.

The above three branches are trained simultaneously.
ˆb(y) to approximate the
The auxiliary branch generates
skeleton of the bolded target; the reﬁne branch extracts fea-
tures from various stroke-weight syntheses and pass them
to the dominating branch; the dominating branch ﬁnally
combines the auxiliary information from the reﬁne branch
and produces the ﬁnal syntheses. This three-branch design
follows similar favor of multitask learning and collabora-
tive training strategy. It pushes the networks to learn the
deformation from one style to another with various stroke
weights and handles the thin-stroke issue. Furthermore,
the idea of replacing the erosion operation by max-pooling
makes the gradient ﬂow through all the three branches; we
thus call the method collaborative stroke reﬁnement.

Chinese characters have the content-reuse phenomenon;
that is, the same radicals may present in various characters
at various locations; see Fig. 2).
In other words, a Chi-
nese character can be decomposed into a series of elemen-
tary characters. Mathematically, the standardized character
c can be modeled as

(cid:88)

c =

d(cid:96)(b(cid:96)),

(cid:96)

(1)

where b(cid:96) is an elementary characters and d(cid:96)(·) is the defor-
mation function associated with each elementary character.
The shape of an arbitrary character c could have huge
variations and it is thus hard to learn deformation directly
based on c; on the other hand, the shapes of the elemen-
tary character b(cid:96) are limited and it is thus much easier to
learn deformation directly based on b(cid:96). The functionality
of online zoom-augmentation is to explore this decomposi-
tion (1) adaptively. It has two main advantages: (i) it lever-
ages the repetitiveness of radicals in Chinese characters and
guides the networks to learn a few elementary structures of
Chinese characters, leading to a signiﬁcantly reduction of
the training size; and (ii) it guides the networks learn char-
acters at a variety of locations and sizes, making the net-
works robust. To our best knowledge, no CNN-based meth-
ods leverage this domain knowledge except SA-VAE [19].
However, SA-VAE explicitly models this as a pre-labeling

133-bits code embedded into the model. Instead,we care-
fully select 750 training samples as the elementary charac-
ter b(cid:96); see details in Section 4.1

We further use various positions and scales to train
the element-wise deformation operator d(cid:96)(·). Speciﬁcally,
when a paired image x, y is fed into the model, we zoom
the centered character region to change the aspect ratio.
We then translate the zoomed character region horizontally
or vertically. Assuming the ratio of each character region
is h : w, the zoomed result with h
2 : w will be verti-
cally translated in the image (mostly translated to the up-
per/middle/bottom position), while the zoomed result with
h : w
2 will be horizontally translated (mostly translated to
the left/middle/right position). Additionally, the zoomed re-
sult with h
2 is translated to any possible position. Essen-
tially, d(cid:96)(·) captures the mapping between b(cid:96) and radicals of
arbitrary character. Augmented training samples guide the
networks to learn a variety of location patterns.

2 : w

3.3. Adaptive Pre-deformation

The overall

task is to learn a mapping from x =
dsource(c) to y = dtarget(c) for arbitrary character c. In Sec-
tion 3.2, we decompose the character c. We can further de-
compose the deformations, dsource(·) and dtarget(·), to ease
the learning process. The main intuition is that some basic
deformations, such as resize and rescale, are easy to learn.
We can use a separate component to focus on learning those
basic deformations and the main networks can then focus on
learning complicated deformations. Mathematically, a gen-
eral deformation function can be decomposed as

d(·) = dcomplex(drescale(dreszie(·))).

The functionality of adaptive pre-deformation is to learn
drescale(·) and dreszie(·), such that the main networks can
focus on learning dcomplex(·).

All paired xi, yi from SD construct the source image
set X and the target image set Y. We calculate the aver-
age character-region proportion r1 and average character’s
height-width ration r2 for Y. First we ﬁnd the minimum
character bounding box bi of each yi, the height and width
of bi is respectively hi and wi. So,

r1 =

1
N

N
(cid:88)

i

hi · wi
64 × 64

, r2 =

1
N

N
(cid:88)

i

hi
wi

;

where N = 750. According to the above two statistics, we
then pre-deformed each xi to align its character region with
yi. The deformed result ˆxi is:

ˆxi = drescale(dresize(xi)),
where dresize(·) and drescale(·) denote the size-deformation
and scale-deformation, respectively. The character skele-
ton of xi is then roughly aligned with yi. Here by pre-
deformation, the skeleton of source character is roughly

aligned with the corresponding target character, which re-
duces the transfer-difﬁculty. Speciﬁcally, the model does
not ﬁt the size and can focus on learning stroke distortions.

3.4. Losses

We next describe loss functions optimized in our model.
pixel +

There are 4 loss terms divided into two groups, L1
L1
cGAN .

cGAN and L2

pixel + L2

pixel(G1) = E(x,y)[−y · (log(ˆy)) − (1 − y) · log(1 − ˆy)],
L1

L1

cGAN (G1, D1) =

E(x,y)[log D1(x, y)]
+E[1 − log D1(x, G1(x)],

(2)

where y is the character with target
(x, y) is
pair-wise samples, x ∼ psource domain(x) and y ∼
ptarget domain(y). D1 is the discriminator 1. G1 includes
coarse generator and the reﬁne branch.

font,

pixel(G2) = E(x,b(y))[−b(y) · (log( ˆb(y)))
L2
−(1 − b(y)) · log(1 − ˆb(y))],

cGAN (G2, D2) = E(x,b(y))[log D2(x, b(y))]
L2
+E[1 − log D2(x, G2(x)],

(3)

where b(y) is the bold version character of target font.
D2 is the discriminator 2. G2 only contains the reﬁne
branch. The whole network is jointly optimized by L1
pixel +
L1
cGAN . It is notable that all x, y and b(y)
here are augmented by d(cid:96)(·) and pre-deformation function
dresize(·), drescale(·). We just write this for simplicity.

cGAN +L2

pixel+L2

4. Experiments

We conduct extensive experiments to validate the pro-
posed method and compare it with two state-of-the-art
methods, HAN[7] and EMD[24]

Figure 5. Examples of “compound” and “single-element” charac-
ters. Some radicals are marked in red.

5

demonstrate our model generally outperforms baselines.
According to Fig. 7, our model slightly outperforms base-
line methods on printed-style fonts (1st row) since printed-
style fonts are always featured with regular structure and
wide strokes, which makes the model take no advantages
of proposed collaborative stroke reﬁnement. However, our
method achieves impressive results on handwritten fonts
featured with thin strokes(2nd row) or even irregular struc-
ture (3rd rows). Compared with baselines, our model syn-
thesizes more details without missing or overlapped strokes.
While these defections happen in baseline methods, we
can barely recognize some synthesized characters of them.
More experimental results are displayed in appendix.

4.2.2 Performance on Different Training-set Size

We further change the size of training set to 1550 and 2550
by randomly adding additional samples to SD. we compare
our model with baselines under various sizes of training set
on handwritten font synthesis tasks (see Fig. 9).

Besides RMSE, to rationally measure the ﬁdelity of syn-
thesized characters, we conduct a user Study (see Fig. 6).
100 volunteers are invited to subjectively rate the synthe-
sized characters from score 1 to 5, where 1 is the worst and 5
is the best. The user study results show that the performance
of all methods is improved with the increase of the training
set. However, when the size is larger than 1550, the increas-
ing trend of our method stops and the score begins to ﬂoat
up and down. Thus we conclude that 1550 training samples
have completely covered radicals/single-element characters
with different shapes so that more training samples will not
bring more improvement. Additionally, User Study result
demonstrates that when the size of training set is 750, our
method achieves equal even higher subjective score com-
pared with HAN trained by 2550 characters.
4.3. Ablation Study

Effect of Adaptive Pre-deformation We conduct exper-
iments by removing adaptive pre-deformation or not. As
shown in the second row of Fig. 10, some strokes are miss-
ing compared with the ground truth, while the results of
ours are signiﬁcantly better, which means pre-deformation
guarantees that the generated strokes are complete. When
absolute locations of a certain stroke between the source
character and the target character are seriously discrepant,
the transfer network may be confused about whether this
stroke should be abandoned or be mapped to another po-
sition. Our adaptive pre-deformation roughly align the
strokes essentially relieves the transfer network of learning
the mapping of stroke location.

Effect of online zoom-augmentation The results after
removing online augmentation module are shown in the
third row of Fig. 10 from which we can see the gener-
ated strokes are so disordered that we even cannot recog-
nize the characters. Without zoom-augmentation, the model

Figure 6. In the user study, our method achieves the close subjec-
tive score with much less training set compared with baselines.

4.1. Training Sample Selection

For Chinese characters, “compound” characters appear
as obvious layout structure (e.g. left-right, top-bottom, sur-
rounding, left-middle-right or top-middle-bottom), while
“single-element” characters cannot be structural decom-
posed. Both radicals and “single-element” characters are
known as basic units to construct all characters (see Fig. 5).
Many compound Chinese characters share the same basic
units in themselves, which means though over 8000 char-
acters in Chinese language, there are only rather limited
basic units (including 150 radicals and about 450 “single-
element” characters). Based on this prior knowledge, a
small set of characters are selected as training samples.
We select 450 “single-element” characters and 150×2 com-
pound characters covering all 150 radicals, to create a small
dataset SD totally including 750 training samples.
4.2. Comparison with Baseline Methods

Two state-of-the-art Chinese font synthesis methods we
compared with are HAN and EMD. HAN[7] is especially
proposed for handwritten fonts synthesis.
It proposes hi-
erarchical adversarial discriminator to improve the gener-
ated performance. Experiment shows it relies on about
2500 paired training samples to achieve good performance;
EMD[24] can achieve style transfer from a few samples
by disentangling the style/content representation of fonts,
while it relies on a large font library for training and per-
forms not well on handwritten fonts.

4.2.1 Performance on Small Dataset SD

We ﬁrst experimentally compare our method with HAN[7]
and EMD[24] under the selected small dataset SD(see Fig.
7). Specially, EMD (few-shot) denotes we generate char-
acters using a few reference inputs, just like what the pa-
per has done. However, for fair comparison, EMD (ﬁne-
tune) denotes we further ﬁne tune EMD with SD. For
HAN, we directly train it on SD. We specially choose
both printed-style and handwritten-style fonts to fairly

6

RMSE

4.888

4.964

4.844

4.767

4.965

4.999

4.886

4.643

5.095

5.525

5.113

5.023

Source

EMD
few-shot
EMD
ﬁnetune

HAN

Ours

Target

EMD
few-shot
EMD
ﬁnetune

HAN

Ours

Target

EMD
few-shot
EMD
ﬁnetune

HAN

Ours

Target

source

result

target

Figure 7. Performance of transferring Fang Song font (source) to other fonts (target) including printed (1st row) and handwritten fonts
(2-3 rows). Characters in red boxes are some failure samples. Both qualitative results and quantitative evaluation (RMSE) demonstrate our
model performs better than baselines.

Font1

Font2

Font3

Figure 8. The generated results of Korean fonts prove that our method can be transplanted to any languages with “content reuse” phe-
nomenon.

is actually trained with only 750 paired samples, which
may lead to serious over-ﬁtting. Zoom-augmentation boosts
the model implicitly learn the shape diversity and location
diversity of one character. Besides, it also models com-
mon structure information by these vertically or horizon-
tally translated characters. So our method can reconstruct

correct topological structure of characters.

Effect of Stroke Reﬁnement We disconnect the data
ﬂow from reﬁne branch to dominating branch for analyzing
the effect of stroke reﬁnement. Comparison results illus-
trated in Fig. 10 demonstrate that characters generated with
stroke reﬁnement strategy have more continuous structures

7

Source

EMD ﬁnetune
750

HAN 750

Ours 750

EMD ﬁnetune
1550

HAN 1550

Ours 1550

EMD ﬁnetune
2550

HAN 2550

Ours 2550

Target

source

w/o pre deformation

w/o augmentation

w/o reﬁnement

w/o GAN

all

target

Figure 9. Performance comparison on three handwritten font synthesis tasks with increasing size of training set. The comparison between
Ours 750 and HAN 2550 demonstrate that our method achieve the equal even better performance with much less training set.

Font1

Font2

Font3

Figure 10. Ablation comparison: for each example font, we show predictions without adaptive pre-deformation, online zoom-augmentation,
stroke reﬁnement, inﬂuence of GAN-related losses predictions and last with all the methods. The results are most similar to the target while
the performance under other ablation setting shows various defections.

without missing or broken strokes, while characters gener-
ated without the participation of stroke reﬁnement present
seriously deviant and even broken strokes, especially on
cursive handwritten fonts (Font 1 and Font 2). These phe-
nomena proves the effectiveness of stroke reﬁnement.

Effect of GAN We last remove the inﬂuence of two dis-
criminators in the model by setting GAN-related loss terms,
Eq. 2 and Eq. 3, to zero. The results in Fig. 10 generally
shows a little blurry.

4.4. Extension Study

To show the portability of our approach, we experiment
Korean fonts. Fig. 8 shows that the syntheses are impres-
sively similar to the targets, validating our model can be
applied to other font generation tasks, once they have com-
plicated handwriting style and content reuse phenomenon.

8

Conference on Computer Vision, pages 694–711. Springer,
2016.

[14] C. Ledig, L. Theis, F. Husz´ar, J. Caballero, A. Cunningham,
A. Acosta, A. P. Aitken, A. Tejani, J. Totz, Z. Wang, et al.
Photo-realistic single image super-resolution using a genera-
tive adversarial network. In CVPR, volume 2, page 4, 2017.
[15] P. Lyu, X. Bai, C. Yao, Z. Zhu, T. Huang, and W. Liu.
Auto-encoder guided gan for chinese calligraphy synthesis.
In Document Analysis and Recognition (ICDAR), 2017 14th
IAPR International Conference on, volume 1, pages 1095–
1100. IEEE, 2017.

[16] M. Mirza and S. Osindero. Conditional generative adversar-

ial nets. arXiv preprint arXiv:1411.1784, 2014.

[17] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. arXiv preprint arXiv:1511.06434, 2015.
[18] A. Sanakoyeu, D. Kotovenko, S. Lang, and B. Ommer. A
style-aware content loss for real-time hd style transfer. In The
European Conference on Computer Vision (ECCV), Septem-
ber 2018.

[19] D. Sun, T. Ren, C. Li, J. Zhu, and H. Su. Learning to write
stylized chinese characters by reading a handful of examples.
arXiv preprint arXiv:1712.06424, 2017.

[20] P. Upchurch, N. Snavely, and K. Bala. From a to z: super-
vised transfer of style and content using deep neural network
generators. arXiv preprint arXiv:1603.02003, 2016.

[21] J. Wang and A. Cherian. Learning discriminative video rep-
resentations using adversarial perturbations. In Proceedings
of the European Conference on Computer Vision (ECCV),
pages 685–701, 2018.

[22] X. Xiao, L. Jin, Y. Yang, W. Yang, J. Sun, and T. Chang.
Building fast and compact convolutional neural networks for
ofﬂine handwritten chinese character recognition. Pattern
Recognition, 72:72–81, 2017.

[23] R. Zhang, P. Isola, and A. A. Efros. Colorful image coloriza-
In European Conference on Computer Vision, pages

tion.
649–666. Springer, 2016.

[24] Y. Zhang, Y. Zhang, and W. Cai. Separating style and con-
In Proceedings of the
tent for generalized style transfer.
IEEE Conference on Computer Vision and Pattern Recog-
nition, volume 1, 2018.

[25] Z. Zhong, X.-Y. Zhang, F. Yin, and C.-L. Liu. Handwritten
chinese character recognition with spatial transformer and
deep residual networks. In Pattern Recognition (ICPR), 2016
23rd International Conference on, pages 3440–3445. IEEE,
2016.

[26] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. In IEEE International Conference on Computer Vi-
sion, 2017.

5. Conclusions

We propose an end-to-end model to synthesize hand-
written Chinese font with only 750 training samples. The
whole model includes three main novelties: collaborative
stroke reﬁnement, handling the thin issue, online zoom-
augmentation, exloiting the content-reuse phenomenon, and
adaptive pre-deformation, augmenting training data. We
perceptually and quantitatively evaluate our model and the
experimental results validate the effectiveness of our model.

References

2016.

2017.

[1] https://github.com/kaonashi-tyc/Rewrite,

[2] https://github.com/kaonashi-tyc/zi2zi,

[3] C. Ancuti, C. O. Ancuti, R. Timofte, L. Van Gool, L. Zhang,
M.-H. Yang, V. M. Patel, H. Zhang, V. A. Sindagi, R. Zhao,
et al. Ntire 2018 challenge on image dehazing: Methods and
In The IEEE Conference on Computer Vision and
results.
Pattern Recognition (CVPR) Workshops, volume 1, 2018.
[4] S. Azadi, M. Fisher, V. G. Kim, Z. Wang, E. Shechtman, and
T. Darrell. Multi-content gan for few-shot font style transfer.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.

[5] Y. Cao, Z. Zhou, W. Zhang, and Y. Yu. Unsupervised diverse
In Joint
colorization via generative adversarial networks.
European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 151–166. Springer, 2017.
[6] H. Chang, J. Lu, F. Yu, and A. Finkelstein. Pairedcycle-
gan: Asymmetric style transfer for applying and removing
makeup. In 2018 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2018.

[7] J. Chang, Y. Gu, Y. Zhang, and Y. Wang. Chinese handwrit-
ing imitation with hierarchical generative adversarial net-
work. 2018.

[8] D. Chen, L. Yuan, J. Liao, N. Yu, and G. Hua. Stylebank:
An explicit representation for neural image style transfer. In
Proc. CVPR, volume 1, page 4, 2017.

[9] Y. Chen, Y.-K. Lai, and Y.-J. Liu. Cartoongan: Generative
adversarial networks for photo cartoonization. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 9465–9474, 2018.

[10] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer
using convolutional neural networks. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 2414–2423, 2016.

[11] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, pages 2672–2680, 2014.

[12] Y. Jiang, Z. Lian, Y. Tang, and J. Xiao. Dcfont: an end-
to-end deep chinese font generation system. In SIGGRAPH
Asia 2017 Technical Briefs, page 22. ACM, 2017.

[13] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
In European

real-time style transfer and super-resolution.

9

As shown in Figure 17, Font 4 is a typical handwritten
font. The results are realistic because our model learns the
stroke deformation well. The previous models hardly per-
form well on such irregular fonts.

6.4. Appendix IV

As shown in Figure 18, we display the small training set
SD used in all the Chinese font generation experiments in
this paper. According to the selection principle, we select
450 single-element characters and 150×2 compound char-
acters covering all 150 radicals, to create the small dataset
SD totally including 750 training samples. By utilizing the
content-reuse phenomenon, we reduce the training set from
3000 to 750, which makes the model practical in font de-
sign.

6. Appendix

6.1. Appendix I

As shown in Figure 11, we get the intermediate results
(cid:100)b(y) and g((cid:100)b(y)) during the training process to analyze the
pipeline of our model. Besides, we also conduct an exper-
iment by removing the collaborative stroke reﬁnement to
show the importance of this module. The results of our
model are much more similar to targets than the model
without stroke reﬁnement. We mark the distorted charac-
ters with red bounding boxes. From the number of bound-
ing boxes, we can learn that the ﬁrst row performs worst
because its strokes are thinnest. Removing the collabora-
tive stroke reﬁnement, many strokes of generated charac-
ters are broken because of the low fault tolerance. But our
model improve the fault tolerance by generating and reﬁn-
ing (cid:100)b(y). Figure 11 illustrates the whole process of collab-
orative stroke reﬁnement and shows its importance.

6.2. Appendix II

As shown in Figure 12, we conduct extensive experi-
ments on ancient Chinese characters and calligraphy. Font
1 is a type of ancient font named Xiao Zhuan and Font 2 is
a calligraphy duplicated from rubbings written by a famous
calligrapher in Tang Dynasty. The results reconstruct the
overall glyph and structure of targets. The results can be
applied to history and art research area.

As shown in Figure 13, we display results of Korean
fonts. The 1st row is printed font and the 2nd and 3rd rows
are handwritten ones. We randomly select 750 characters
for each font to train the model and performance is pretty
good in ﬁrst two rows. It is because the training samples are
selected randomly, the 3rd row is not as good as expected.
In Chinese font generation task, we build the small training
set SD by selecting the single-element characters. How-
ever, For Korean, we do not use such prior knowledge, so
the performance is impacted seriously by the randomness of
choice.

6.3. Appendix III

As shown in Figure 14, Font 1 is a printed style and the
generated characters are similar to targets except a little blur
in details. Our model performs sufﬁciently well to be used
in practical font design.

As shown in Figure 15, the characteristics of Font 2 is
that the strokes are distorted, so the results are not as accu-
rate as Font 1. Even so, the quality of generated characters
is still impressive.

As shown in Figure 16, the strokes of Font 3 is so thin
that the outputs of previous methods are usually broken. But
our model solves this problem efﬁciently by Collaborative
Stroke Reﬁnement.

10

6.5. Appendix I

(cid:98)y w/o reﬁnement

(cid:98)y w/o reﬁnement

Source

(cid:100)b(y)

g((cid:100)b(y))

(cid:98)y our
target

(cid:100)b(y)

g((cid:100)b(y))

(cid:100)b(y) our

target

(cid:100)b(y)

g((cid:100)b(y))

(cid:98)y our
target

(cid:100)b(y)

g((cid:100)b(y))

(cid:98)y our
target

(cid:100)b(y)

g((cid:100)b(y))

(cid:98)y our
target

(cid:98)y w/o reﬁnement

(cid:98)y w/o reﬁnement

(cid:98)y w/o reﬁnement

Figure 11. Intermediate results of the model and the results of model without stroke reﬁnement. The characters in red boxes are seri-
ously distorted because of removing reﬁnement module. It proves that collaborative stroke reﬁnement strategy makes the strokes more
continuous, smooth and sharp.

11

6.6. Appendix II

Source1

font1

target

Source2

font2

target

Source

font1

target

font2

target

font3

target

Figure 12. Results of ancient Chinese characters and calligraphy. Font 1 is a type of ancient Chinese characters named Xiao Zhuan, which
has more distorted strokes than modern characters. Font 2 is a calligraphy duplicated from Rubbings written by a famous calligrapher in
Tang Dynasty. The performance of our model on these two fonts are pretty good, which means our model can be applied in history and art
research.

Figure 13. More results of Korean fonts. We randomly select 750 characters for each font to train the model. The 1st row is printed
font and the 2nd and 3rd rows are handwritten ones. The results are impressive and they prove that the our model can be used for other
hieroglyphics.

12

6.7. Appendix III

Figure 14. The display of a large number of results for Font 1. The characters in the left of each column are ground truths while those on
the right are generated characters.

13

Figure 15. The display of a large number of results for Font2. The characters in the left of each column are ground truths while those on
the right are generated characters.

14

Figure 16. The display of a large number of results for Font 3. The characters in the left of each column are ground truths while those on
the right are generated characters.

15

Figure 17. The display of a large number of results for Font4. The characters in the left of each column are ground truths while those on
the right are generated characters.

16

6.8. Appendix IV

Figure 18. The manually selected training set with 750 characters.

17

