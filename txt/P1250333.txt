6
1
0
2
 
b
e
F
 
9
2
 
 
]

G
L
.
s
c
[
 
 
2
v
3
9
7
2
0
.
1
1
5
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2016

GENERATING IMAGES FROM CAPTIONS
WITH ATTENTION

Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba & Ruslan Salakhutdinov
Department of Computer Science
University of Toronto
Toronto, Ontario, Canada
{emansim,eparisotto,rsalakhu}@cs.toronto.edu, jimmy@psi.utoronto.ca

ABSTRACT

Motivated by the recent progress in generative models, we introduce a model that
generates images from natural language descriptions. The proposed model itera-
tively draws patches on a canvas, while attending to the relevant words in the de-
scription. After training on Microsoft COCO, we compare our model with several
baseline generative models on image generation and retrieval tasks. We demon-
strate that our model produces higher quality samples than other approaches and
generates images with novel scene compositions corresponding to previously un-
seen captions in the dataset.

1

INTRODUCTION

Statistical natural image modelling remains a fundamental problem in computer vision and image
understanding. The challenging nature of this task has motivated recent approaches to exploit the
inference and generative capabilities of deep neural networks. Previously studied deep generative
models of images often deﬁned distributions that were restricted to being either unconditioned or
conditioned on classiﬁcation labels. In real world applications, however, images rarely appear in
isolation as they are often accompanied by unstructured textual descriptions, such as on web pages
and in books. The additional information from these descriptions could be used to simplify the
image modelling task. Moreover, learning generative models conditioned on text also allows a better
understanding of the generalization performance of the model, as we can create textual descriptions
of completely new scenes not seen at training time.

There are numerous ways to learn a generative model over both image and text modalities. One
approach is to learn a generative model of text conditioned on images, known as caption generation
(Kiros et al., 2014a; Karpathy & Li, 2015; Vinyals et al., 2015; Xu et al., 2015). These models take
an image descriptor and generate unstructured texts using a recurrent decoder. In contrast, in this
taking textual descriptions
paper we explore models that condition in the opposite direction, i.e.
as input and using them to generate relevant images. Generating high dimensional realistic images
from their descriptions combines the two challenging components of language modelling and image
generation, and can be considered to be more difﬁcult than caption generation.

In this paper, we illustrate how sequential deep learning techniques can be used to build a conditional
probabilistic model over natural image space effectively. By extending the Deep Recurrent Atten-
tion Writer (DRAW) (Gregor et al., 2015), our model iteratively draws patches on a canvas, while
attending to the relevant words in the description. Overall, the main contributions of this work are
the following: we introduce a conditional alignDRAW model, a generative model of images from
captions using a soft attention mechanism. The images generated by our alignDRAW model are
reﬁned in a post-processing step by a deterministic Laplacian pyramid adversarial network (Denton
et al., 2015). We further illustrate how our method, learned on Microsoft COCO (Lin et al., 2014),
generalizes to captions describing novel scenes that are not seen in the dataset, such as “A stop sign
is ﬂying in blue skies” (see Fig. 1).

1

Published as a conference paper at ICLR 2016

A stop sign is ﬂying in
blue skies.

A herd of elephants ﬂy-
ing in the blue skies.

A toilet seat sits open in
the grass ﬁeld.

A person skiing on sand
clad vast desert.

Figure 1: Examples of generated images based on captions that describe novel scene compositions that are
highly unlikely to occur in real life. The captions describe a common object doing unusual things or set in a
strange location.

2 RELATED WORK

Deep Neural Networks have achieved signiﬁcant success in various tasks such as image recognition
(Krizhevsky et al., 2012), speech transcription (Graves et al., 2013), and machine translation (Bah-
danau et al., 2015). While most of the recent success has been achieved by discriminative models,
generative models have not yet enjoyed the same level of success. Most of the previous work in
generative models has focused on variants of Boltzmann Machines (Smolensky, 1986; Salakhutdi-
nov & Hinton, 2009) and Deep Belief Networks (Hinton et al., 2006). While these models are very
powerful, each iteration of training requires a computationally costly step of MCMC to approximate
derivatives of an intractable partition function (normalization constant), making it difﬁcult to scale
them to large datasets.

Kingma & Welling (2014), Rezende et al. (2014) have introduced the Variational Auto-Encoder
(VAE) which can be seen as a neural network with continuous latent variables. The encoder is used
to approximate a posterior distribution and the decoder is used to stochastically reconstruct the data
from latent variables. Gregor et al. (2015) further introduced the Deep Recurrent Attention Writer
(DRAW), extending the VAE approach by incorporating a novel differentiable attention mechanism.

Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are another type of generative
models that use noise-contrastive estimation (Gutmann & Hyv¨arinen, 2010) to avoid calculating
an intractable partition function. The model consists of a generator that generates samples using
a uniform distribution and a discriminator that discriminates between real and generated images.
Recently, Denton et al. (2015) have scaled those models by training conditional GANs at each level
of a Laplacian pyramid of images.

While many of the previous approaches have focused on unconditional models or models condi-
tioned on labels, in this paper we develop a generative model of images conditioned on captions.

3 MODEL

Our proposed model deﬁnes a generative process of images conditioned on captions. In particular,
captions are represented as a sequence of consecutive words and images are represented as a se-
quence of patches drawn on a canvas ct over time t = 1, ..., T . The model can be viewed as a part
of the sequence-to-sequence framework (Sutskever et al., 2014; Cho et al., 2014; Srivastava et al.,
2015).

3.1 LANGUAGE MODEL: THE BIDIRECTIONAL ATTENTION RNN

Let y be the input caption, represented as a sequence of 1-of-K encoded words y = (y1, y2, ..., yN ),
where K is the size of the vocabulary and N is the length of the sequence. We obtain the caption sen-
tence representation by ﬁrst transforming each word yi to an m-dimensional vector representation
hlang
, i = 1, .., N using the Bidirectional RNN. In a Bidirectional RNN, the two LSTMs (Hochre-
i
iter & Schmidhuber, 1997) with forget gates (Gers et al., 2000) process the input sequence from both
forward and backward directions. The Forward LSTM computes the sequence of forward hidden
−→
−→
−→
h lang
h lang
h lang
N ] , whereas the Backward LSTM computes the sequence of back-
, ...,
states [
2
1
←−
←−
h lang
h lang
N ]. These hidden states are then concatenated together
ward hidden states [
,
2
1
−→
h lang
into the sequence hlang = [hlang
i

←−
h lang
, ..., hlang

N ], with hlang

, ...,
, hlang
2

], 1 ≤ i ≤ N .

←−
h lang
i

= [

1

,

,

i

2

Published as a conference paper at ICLR 2016

Figure 2: AlignDRAW model for generating images by learning an alignment between the input captions and
generating canvas. The caption is encoded using the Bidirectional RNN (left). The generative RNN takes a
latent sequence z1:T sampled from the prior along with the dynamic caption representation s1:T to generate
the canvas matrix cT , which is then used to generate the ﬁnal image x (right). The inference RNN is used to
compute approximate posterior Q over the latent sequence.

3.2

IMAGE MODEL: THE CONDITIONAL DRAW NETWORK

To generate an image x conditioned on the caption information y, we extended the DRAW net-
work (Gregor et al., 2015) to include caption representation hlang at each step, as shown in Fig. 2.
The conditional DRAW network is a stochastic recurrent neural network that consists of a sequence
of latent variables Zt ∈ RD, t = 1, .., T , where the output is accumulated over all T time-steps. For
simplicity in notation, the images x ∈ Rh×w are assumed to have size h-by-w and only one color
channel.

Unlike the original DRAW network where latent variables are independent spherical Gaussians
N (0, I), the latent variables in the proposed alignDRAW model have their mean and variance de-
pend on the previous hidden states of the generative LSTM hgen
t−1, except for P (Z1) = N (0, I).
Namely, the mean and variance of the prior distribution over Zt are parameterized by:

P (Zt|Z1:t−1) = N

µ(hgen

t−1), σ(hgen
t−1)

(cid:18)

(cid:19)
,

µ(hgen
σ(hgen

t−1) = tanh(Wµhgen
t−1) = exp (cid:0) tanh(Wσhgen

t−1),

t−1)(cid:1),

where Wµ ∈ RD×n, Wσ ∈ RD×n are the learned model parameters, and n is the dimensional-
ity of hgen
, the hidden state of the generative LSTM. Similar to (Bachman & Precup, 2015), we
have observed that the model performance is improved by including dependencies between latent
variables.

t

Formally, an image is generated by iteratively computing the following set of equations for t =
1, ..., T (see Fig. 2), with hgen
and c0 initialized to learned biases:
(cid:19)
,

zt ∼ P (Zt|Z1:t−1) = N

µ(hgen

t−1), σ(hgen
t−1)

(1)

(cid:18)

0

t−1, hlang),

st = align(hgen
t = LSTM gen(hgen
hgen
ct = ct−1 + write(hgen

t−1, [zt, st]),
),
(cid:89)

t

˜x ∼ P (x | y, Z1:T ) =

P (xi | y, Z1:T ) =

Bern(σ(cT,i)).

i

(cid:89)

i

The align function is used to compute the alignment between the input caption and intermediate
image generative steps (Bahdanau et al., 2015). Given the caption representation from the language
model, hlang = [hlang
N ], the align operator outputs a dynamic sentence representa-
tion st at each step through a weighted sum using alignment probabilities αt

, ..., hlang

, hlang
2

1

st = align(hgen

t−1, hlang) = αt

1hlang

1 + αt

2hlang

2 + ... + αt

1...N :
N hlang
N .

3

(2)

(3)

(4)

(5)

(6)

Published as a conference paper at ICLR 2016

The corresponding alignment probability αt
caption representation hlang and the current hidden state of the generative model hgen
t−1:
v(cid:62) tanh(U hlang

k for the kth word in the caption is obtained using the

exp

k + W hgen

t−1 + b)

(cid:16)

(cid:17)

αt

k =

(cid:80)N

i=1 exp

(cid:16)

v(cid:62) tanh(U hlang

i

+ W hgen

t−1 + b)

(cid:17) ,

(7)

where v ∈ Rl, U ∈ Rl×m, W ∈ Rl×n and b ∈ Rl are the learned model parameters of the alignment
model.
The LSTM gen function of Eq. 3 is deﬁned by the LSTM network with forget gates (Gers et al.,
2000) at a single time-step. To generate the next hidden state hgen
, the LSTM gen takes the previous
hidden state hgen
t−1 and combines it with the input from both the latent sample zt and the sentence
representation st.
The output of the LSTM gen function hgen
is then passed through the write operator which is added
to a cumulative canvas matrix ct ∈ Rh×w (Eq. 4). The write operator produces two arrays of 1D
Gaussian ﬁlter banks Fx(hgen
) ∈ Rw×p whose ﬁlter locations and scales are
t
t
computed from the generative LSTM hidden state hgen
(same as deﬁned in Gregor et al. (2015)).
t
The Gaussian ﬁlter banks are then applied to the generated p-by-p image patch K(hgen
) ∈ Rp×p,
placing it onto the canvas:

) ∈ Rh×p and Fy(hgen

t

t

t

∆ct = ct − ct−1 = write(hgen

) = Fx(hgen

)K(hgen

)Fy(hgen

t

)(cid:62).

t

t

t

(8)

Finally, each entry cT,i from the ﬁnal canvas matrix cT is transformed using a sigmoid function σ to
produce a conditional Bernoulli distribution with mean vector σ(cT ) over the h × w image pixels x
given the latent variables Z1:T and the input caption y1. In practice, when generating an image x,
instead of sampling from the conditional Bernoulli distribution, we simply use the conditional mean
x = σ(cT ).

3.3 LEARNING

(cid:88)

Z

The model is trained to maximize a variational lower bound L on the marginal likelihood of the
correct image x given the input caption y:

L =

Q(Z | x, y) log P (x | y, Z) − DKL (Q(Z | x, y) (cid:107) P (Z | y)) ≤ log P (x | y).

(9)

Similar to the DRAW model, the inference recurrent network produces an approximate posterior
Q(Z1:T | x, y) via a read operator, which reads a patch from an input image x using two arrays of
1D Gaussian ﬁlters (inverse of write from section 3.2) at each time-step t. Speciﬁcally,

ˆxt = x − σ(ct−1),
rt = read (xt, ˆxt, hgen

t−1),
t−1 , [rt, hgen
= LSTM inf er(hinf er
), σ(hinf er
,
t

µ(hinf er
t

t−1]),
(cid:17)
)

(cid:16)

hinf er
t

Q(Zt|x, y, Z1:t−1) = N

0

where ˆx is the error image and hinf er
is initialized to the learned bias b. Note that the inference
LSTM inf er takes as its input both the output of the read operator rt ∈ Rp×p, which depends on
the original input image x, and the previous state of the generative decoder hgen
t−1, which depends
on the latent sample history z1:t−1 and dynamic sentence representation st−1 (see Eq. 3). Hence,
the approximate posterior Q will depend on the input image x, the corresponding caption y, and the
latent history Z1:t−1, except for the ﬁrst step Q(Z1|x), which depends only on x.

The terms in the variational lower bound Eq. 9 can be rearranged using the law of total expectation.
Therefore, the variational bound L is calculated as follows:

log p(x | y, Z1:T ) −

DKL (Q(Zt | Z1:t−1, y, x) (cid:107) P (Zt | Z1:t−1, y))

(cid:34)

L =EQ(Z1:T | y,x)

− DKL (Q(Z1 | x) (cid:107) P (Z1)) .

1We also experimented with a conditional Gaussian observation model, but it worked worse compared to

the Bernoulli model.

T
(cid:88)

t=2

4

(10)

(11)

(12)

(13)

(cid:35)

(14)

Published as a conference paper at ICLR 2016

A yellow school bus
parked in a parking lot.

A red school bus parked
in a parking lot.

A green
parked in a parking lot.

school

bus

A blue school bus parked
in a parking lot.

The decadent chocolate
desert is on the table.

A bowl of bananas is on
the table.

A vintage photo of a cat. A vintage photo of a dog.

Figure 3: Top: Examples of changing the color while keeping the caption ﬁxed. Bottom: Examples of changing
the object while keeping the caption ﬁxed. The shown images are the probabilities σ(cT ). Best viewed in
colour.

The expectation can be approximated by L Monte Carlo samples ˜z1:T from Q(Z1:T | y, x):
(cid:35)

(cid:34)

log p(x | y, ˜zl

1:T ) −

DKL

(cid:0)Q(Zt | ˜zl

1:t−1, y, x) (cid:107) P (Zt | ˜zl

1:t−1, y)(cid:1)

L ≈

1
L

L
(cid:88)

l=1

T
(cid:88)

t=2

− DKL (Q(Z1 | x) (cid:107) P (Z1)) .

(15)

The model can be trained using stochastic gradient descent. In all of our experiments, we used
only a single sample from Q(Z1:T | y, x) for parameter learning. Training details, hyperparameter
settings, and the overall model architecture are speciﬁed in Appendix B. The code is available at
https://github.com/emansim/text2image.

3.4 GENERATING IMAGES FROM CAPTIONS

During the image generation step, we discard the inference network and instead sample from the
prior distribution. Due to the blurriness of samples generated by the DRAW model, we perform an
additional post processing step where we use an adversarial network trained on residuals of a Lapla-
cian pyramid conditioned on the skipthought representation (Kiros et al., 2015) of the captions to
sharpen the generated images, similar to (Denton et al., 2015). By ﬁxing the prior of the adversarial
generator to its mean, it gets treated as a deterministic neural network that allows us to deﬁne the
conditional data term in Eq. 14 on the sharpened images and estimate the variational lower bound
accordingly.

4 EXPERIMENTS

4.1 MICROSOFT COCO

Microsoft COCO (Lin et al., 2014) is a large dataset containing 82,783 images, each annotated with
at least 5 captions. The rich collection of images with a wide variety of styles, backgrounds and
objects makes the task of learning a good generative model very challenging. For consistency with
related work on caption generation, we used only the ﬁrst ﬁve captions when training and evaluating
our model. The images were resized to 32 × 32 pixels for consistency with other tiny image datasets
(Krizhevsky, 2009). In the following subsections, we analyzed both the qualitative and quantitative
aspects of our model as well as compared its performance with that of other, related generative
models.2 Appendix A further reports some additional experiments using the MNIST dataset.

4.1.1 ANALYSIS OF GENERATED IMAGES

The main goal of this work is to learn a model that can understand the semantic meaning expressed
in the textual descriptions of images, such as the properties of objects, the relationships between
them, and then use that knowledge to generate relevant images. To examine the understanding of

2To see more generated images, visit http://www.cs.toronto.edu/˜emansim/cap2im.html

5

Published as a conference paper at ICLR 2016

A very large commercial
plane ﬂying in blue skies.

A very large commer-
cial plane ﬂying in rainy
skies.

A herd of elephants walk-
ing across a dry grass
ﬁeld.

A herd of elephants walk-
ing across a green grass
ﬁeld.

Figure 4: Bottom: Examples of changing the background while keeping the caption ﬁxed. Top: The respective
nearest training images based on pixel-wise L2 distance. The nearest images from the training set also indicate
that the model was not simply copying the patterns it observed during the learning phase.

our model, we wrote a set of captions inspired by the COCO dataset and changed some words in the
captions to see whether the model made the relevant changes in the generated samples.

First, we explored whether the model understood one of the most basic properties of any object, the
color. In Fig. 3, we generated images of school buses with four different colors: yellow, red, green
and blue. Although, there are images of buses with different colors in the training set, all mentioned
school buses are speciﬁcally colored yellow. Despite that, the model managed to generate images of
an object that is visually reminiscent of a school bus that is painted with the speciﬁed color.

Apart from changing the colors of objects, we next examined whether changing the background of
the scene described in a caption would result in the appropriate changes in the generated samples.
The task of changing the background of an image is somewhat harder than just changing the color of
an object because the model will have to make alterations over a wider visual area. Nevertheless, as
shown in Fig. 4 changing the skies from blue to rainy in a caption as well as changing the grass type
from dry to green in another caption resulted in the appropriate changes in the generated image.

Despite a large number of ways of changing colors and backgrounds in descriptions, in general
we found that the model made appropriate changes as long as some similar pattern was present in
the training set. However, the model struggled when the visual difference between objects was very
small, such as when the objects have the same general shape and color. In Fig. 3, we demonstrate that
when we swap two objects that are both visually similar, for example cats and dogs, it is difﬁcult
to discriminate solely from the generated samples whether it is an image of a cat or dog, even
though we might notice an animal-like shape. This highlights a limitation of the model in that it has
difﬁculty modelling the ﬁne-grained details of objects.

As a test of model generalization, we tried generating images corresponding to captions that describe
scenarios that are highly unlikely to occur in real life. These captions describe a common object
doing unusual things or set in a strange location, for example “A toilet seat sits open in the grass
ﬁeld”. Even though some of these scenarios may never occur in real life, it is very easy for humans
to imagine the corresponding scene. Nevertheless, as you can see in Fig. 1, the model managed to
generate reasonable images.

4.1.2 ANALYSIS OF ATTENTION

After ﬂipping sets of words in the captions, we further explored which words the model attended
to when generating images. It turned out that during the generation step, the model mostly focused
on the speciﬁc words (or nearby words) that carried the main semantic meaning expressed in the
sentences. The attention values of words in sentences helped us interpret the reasons why the model
made the changes it did when we ﬂipped certain words. For example, in Fig. 5, top row, we can see
that when we ﬂipped the word “desert” to “forest”, the attention over words in the sentence did not
change drastically. This suggests that, in their respective sentences, the model looked at “desert”
and “forest” with relatively equal probability, and thus made the correct changes. In contrast, when
we swap words “beach” and “sun”, we can see a drastic change between sentences in the probability
distribution over words. By noting that the model completely ignores the word “sun” in the second

6

Published as a conference paper at ICLR 2016

A rider on a blue motor-
cycle in the desert.

A rider on a blue motor-
cycle in the forest.

A surfer, a woman, and a
child walk on the beach.

A surfer, a woman, and a
child walk on the sun.

alignDRAW

LAPGAN

Conv-Deconv VAE

Fully-Conn VAE

Figure 5: Top: Examples of most attended words while changing the background in the caption. Bottom: Four
different models displaying results from sampling caption A group of people walk on a beach with surf boards.

sentence, we can therefore gain a more thorough understanding of why we see no visual differences
between the images generated by each caption.

We also tried to analyze the way the model generated images. Unfortunately, we found that there
was no signiﬁcant connection between the patches drawn on canvas and the most attended words at
particular time-steps.

4.1.3 COMPARISON WITH OTHER MODELS

Quantitatively evaluating generative models remains a challenging task in itself as each method of
evaluation suffers from its own speciﬁc drawbacks. Compared to reporting classiﬁcation accuracies
in discriminative models, the measures deﬁning generative models are intractable most of the times
and might not correctly deﬁne the real quality of the model. To get a better comparison between
performances of different generative models, we report results on two different metrics as well as a
qualitative comparison of different generative models.

We compared the performance of the proposed model to the DRAW model conditioned on cap-
tions without the align function (noalignDRAW) as well as the DRAW model conditioned on
the skipthought vectors of (Kiros et al., 2015) (skipthoughtDRAW). All of the conditional DRAW
models were trained with a binary cross-entropy cost function, i.e. they had Bernoulli conditional
likelihoods. We also compared our model with Fully-Connected (Fully-Conn) and Convolutional-
Deconvolutional (Conv-Deconv) Variational Autoencoders which were trained with the least squares
cost function. The LAPGAN model of (Denton et al., 2015) was trained on a two level Lapla-
cian Pyramid with a GAN as a top layer generator and all stages were conditioned on the same
skipthought vector.

In Fig. 5, bottom row, we generated several samples from the prior of each of the current state-of-
the-art generative models, conditioned on the caption “A group of people walk on a beach with surf
boards”. While all of the samples look sharp, the images generated by LAPGAN look more noisy
and it is harder to make out deﬁnite objects, whereas the images generated by variational models
trained with least squares cost function have a watercolor effect on the images. We found that the
quality of generated samples was similar among different variants of conditional DRAW models.

As for the quantitative comparison of different models, we ﬁrst compare the performances of the
model trained with variational methods. We rank the images in the test set conditioned on the
captions based on the variational lower bound of the log-probabilities and then report the Precision-
Recall metric as an evaluation of the quality of the generative model (see Table 1.). Perhaps un-
surprisingly, generative models did not perform well on the image retrieval task. To deal with the
large computational complexity involved in looping through each test image, we create a shortlist
of one hundred images including the correct one, based on the images having the closest Euclidean
distance in the last fully-connected feature space of a VGG-like model (Simonyan & Zisserman,
2015) trained on the CIFAR dataset3 (Krizhevsky, 2009). Since there are “easy” images for which

3The architecture of the model is described here http://torch.ch/blog/2015/07/30/cifar.
html. The shortlist of test images used for evaluation can be downloaded from http://www.cs.
toronto.edu/˜emansim/cap2im/test-nns.pkl.

7

Published as a conference paper at ICLR 2016

Model
LAPGAN
Fully-Conn VAE
Conv-Deconv VAE
skipthoughtDRAW
noalignDRAW
alignDRAW

Microsoft COCO (prior to sharpening)
Image Retrieval
R@1 R@5 R@10 R@50 Med r
-
12.0
12.0
18.9
23.1
22.9

-
53.4
52.9
63.3
68.0
68.5

-
6.6
6.5
11.2
14.1
14.0

-
1.0
1.0
2.0
2.8
3.0

-
47
48
36
31
31

Image Similarity
SSI
0.08 ± 0.07
0.156 ± 0.10
0.164 ± 0.10
0.157 ± 0.11
0.155 ± 0.11
0.156 ± 0.11

Table 1: Retrieval results of different models. R@K is Recall@K (higher is better). Med r is the median rank
(lower is better). SSI is Structural Similarity Index, which is between −1 and 1 (higher is better).

the model assigns high log-probabilities independent of the query caption, we instead look at the
ratio of the likelihood of the image conditioned on the sentence to the likelihood of the image con-
ditioned on the mean sentence representation in the training set, following the retrieval protocol
of (Kiros et al., 2014b). We also found that the lower bound on the test log-probabilities decreased
for sharpened images, and that sharpening considerably hurt the retrieval results. Since sharpening
changes the statistics of images, the estimated log-probabilities of image pixels is not necessarily
a good metric. Some examples of generated images before and after sharpening are shown in Ap-
pendix C.

Instead of calculating error per pixel, we turn to a smarter metric, the Structural Similarity Index
(SSI) (Wang et al., 2004), which incorporates luminance and contrast masking into the error cal-
culation. Strong inter-dependencies of closer pixels are also taken into account and the metric is
calculated on small windows of the images. Due to independence property of test captions, we sam-
pled ﬁfty images from the prior of each generative model for every caption in the test set in order
to calculate SSI. As you can see on Table 1, SSI scores achieved by variational models were higher
compared to SSI score achieved by LAPGAN.

5 DISCUSSION

In this paper, we demonstrated that the alignDRAW model, a combination of a recurrent variational
autoencoder with an alignment model over words, succeeded in generating images that correspond
to a given input caption. By extensively using attentional mechanisms, our model gained several
advantages. Namely, the use of the visual attention mechanism allowed us to decompose the problem
of image generation into a series of steps instead of a single forward pass, while the attention over
words provided us an insight whenever our model failed to generate a relevant image. Additionally,
our model generated images corresponding to captions which generalized beyond the training set,
such as sentences describing novel scenarios which are highly unlikely to occur in real life.

Because the alignDRAW model tends to output slightly blurry samples, we augmented the model
with a sharpening post-processing step in which GAN generated edges which were added to the
alignDRAW samples. Unfortunately, this is not an ideal solution due to the fact that the whole
model was not trained in an end-to-end fashion. Therefore a direction of future work would be to
ﬁnd methods that can bypass the separate post-processing step and output sharp images directly in
an end-to-end manner.

Acknowledgments: This work was supported by Samsung and IARPA, Raytheon BBN Contract No.
D11PC20071. We would like to thank developers of Theano (Bastien et al., 2012), the authors of (Denton
et al., 2015) for open sourcing their code, and Ryan Kiros and Nitish Srivastava for helpful discussions.

REFERENCES

ICLR, 2015.

Bachman, Philip and Precup, Doina. Data generation as sequential decision making. In NIPS, 2015.

Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. In

Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian J., Bergeron, Arnaud,
Bouchard, Nicolas, Warde-Farley, David, and Bengio, Yoshua. Theano: new features and speed improve-
ments. CoRR, abs/1211.5590, 2012.

8

Published as a conference paper at ICLR 2016

Cho, K., van Merrienboer, B., G¨ulc¸ehre, C¸ ., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. Learning
phrase representations using RNN encoder-decoder for statistical machine translation. In EMNLP, 2014.

Denton, Emily L., Chintala, Soumith, Szlam, Arthur, and Fergus, Robert. Deep generative image models using

a laplacian pyramid of adversarial networks. In NIPS, 2015.

Gers, Felix, Schmidhuber, J¨urgen, and Cummins, Fred. Learning to forget: Continual prediction with lstm.

Neural Computation, 2000.

Goodfellow, Ian J., Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil,

Courville, Aaron C., and Bengio, Yoshua. Generative adversarial nets. In NIPS, 2014.

Graves, A., Jaitly, N., and Mohamed, A.-r. Hybrid speech recognition with deep bidirectional LSTM. In IEEE

Workshop on Automatic Speech Recognition and Understanding, 2013.

Gregor, Karol, Danihelka, Ivo, Graves, Alex, and Wierstra, Daan. DRAW: A recurrent neural network for

image generation. In ICML, 2015.

Gutmann, Michael and Hyv¨arinen, Aapo. Noise-contrastive estimation: A new estimation principle for unnor-

malized statistical models. In AISTATS, 2010.

Hinton, Geoffrey E., Osindero, Simon, and Teh, Yee Whye. A fast learning algorithm for deep belief nets.

Neural Computation, 2006.

Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural Computation, 1997.

Karpathy, Andrej and Li, Fei-Fei. Deep visual-semantic alignments for generating image descriptions.

In

CVPR, 2015.

Kingma, Diederik P. and Welling, Max. Auto-encoding variational bayes. In ICLR, 2014.

Kiros, R., Salakhutdinov, R., and Zemel, R. Multimodal neural language models. In ICML, 2014a.

Kiros, Ryan, Salakhutdinov, Ruslan, and Zemel, Richard S. Unifying visual-semantic embeddings with multi-

modal neural language models. CoRR, abs/1411.2539, 2014b.

Kiros, Ryan, Zhu, Yukun, Salakhutdinov, Ruslan, Zemel, Richard S., Torralba, Antonio, Urtasun, Raquel, and

Fidler, Sanja. Skip-thought vectors. In NIPS, 2015.

Krizhevsky, Alex. Learning multiple layers of features from tiny images. Master’s Thesis, Department of

Computer Science, University of Toronto, 2009.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convolutional

neural networks. In NIPS, 2012.

Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P., and Zitnick, C. L. Microsoft

COCO: Common objects in context. In ECCV, 2014.

Rezende, Danilo J., Mohamed, Shakir, and Wierstra, Daan. Stochastic backpropagation and variational infer-

ence in deep latent gaussian models. In ICML, 2014.

Salakhutdinov, Ruslan and Hinton, Geoffrey E. Deep boltzmann machines. In AISTATS, 2009.

Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image recognition.

In ICLR, 2015.

Smolensky, Paul.

Information processing in dynamical systems: foundations of harmony theory. Parallel

Distributed Processing: Explorations in the Microstructure of Cognition, 1986.

Srivastava, Nitish, Mansimov, Elman, and Salakhutdinov, Ruslan. Unsupervised learning of video representa-

tions using LSTMs. In ICML, 2015.

Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence to sequence learning with neural networks. In NIPS,

Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural image caption

2014.

generator. In CVPR, 2015.

Wang, Zhou, Bovik, Alan C., Sheikh, Hamid R., and Simoncelli, Eero P. Image quality assessment: from error

visibility to structural similarity. IEEE Transactions on Image Processing, 2004.

Xu, Kelvin, Ba, Jimmy, Kiros, Ryan, Cho, Kyunghyun, Courville, Aaron C., Salakhutdinov, Ruslan, Zemel,
Richard S., and Bengio, Yoshua. Show, attend and tell: Neural image caption generation with visual atten-
tion. In ICML, 2015.

9

Published as a conference paper at ICLR 2016

Figure 6: Examples of generating 60 × 60 MNIST images corresponding to respective captions. The captions
on the left column were part of the training set. The digits described in the captions on the right column were
hidden during training for the respective conﬁgurations.

APPENDIX A: MNIST WITH CAPTIONS

As an additional experiment, we trained our model on the MNIST dataset with artiﬁcial captions.
Either one or two digits from the MNIST training dataset were placed on a 60 × 60 blank image.
One digit was placed in one of the four (top-left, top-right, bottom-left or bottom-right) corners
of the image. Two digits were either placed horizontally or vertically in non-overlapping fashion.
The corresponding artiﬁcial captions speciﬁed the identity of each digit along with their relative
positions, e.g. “The digit three is at the top of the digit one”, or “The digit seven is at the bottom left
of the image”.

The generated images together with the attention alignments are displayed in Figure 6. The model
correctly displayed the speciﬁed digits at the described positions and even managed to generalize
reasonably well to the conﬁgurations that were never present during training. In the case of gener-
ating two digits, the model would dynamically attend to the digit in the caption it was drawing at
that particular time-step. Similarly, in the setting where the caption speciﬁed only a single digit, the
model would correctly attend to the digit in the caption during the whole generation process. In both
cases, the model placed small attention values on the words describing the position of digits in the
images.

APPENDIX B: TRAINING DETAILS

HYPERPARAMETERS

Each parameter in alignDRAW was initialized by sampling from a Gaussian distribution with mean
0 and standard deviation 0.01. The model was trained using RMSprop with an initial learning rate
of 0.001. For the Microsoft COCO task, we trained our model for 18 epochs. The learning rate
was reduced to 0.0001 after 11 epochs. For the MNIST with Captions task, the model was trained
for 150 epochs and the learning rate was reduced to 0.0001 after 110 epochs. During each epoch,
randomly created 10, 000 training samples were used for learning. The norm of the gradients was
clipped at 10 during training to avoid the exploding gradients problem.

We used a vocabulary size of K = 25323 and K = 22 for the Microsoft COCO and MNIST with
Captions datasets respectively. All capital letters in the words were converted to small letters as a
in the language model had
preprocessing step. For all tasks, the hidden states
128 units. Hence the dimensionality of the concatenated state of the Bidirectional LSTM hlang
=
−→
h lang
] was 256. The parameters in the align operator (Eq. 7) had a dimensionality of
[
i
l = 512, so that v ∈ R512, U ∈ R512×256, W ∈ R512×ngen
and b ∈ R512. The architectural
conﬁgurations of the alignDRAW models are shown in Table 2.

−→
h lang
i

←−
h lang
i

←−
h lang
i

and

,

i

10

Published as a conference paper at ICLR 2016

alignDRAW Model

Task

MS COCO
MNIST

#glimpses
T
32
32

Inference

Generative

Latent

Read Size Write Size

Dim of hinf er Dim of hgen Dim of Z

550
300

550
300

275
150

p
9
8

p
9
8

Table 2: The architectural conﬁgurations of alignDRAW models.

The GAN model used for sharpening had the same conﬁguration as the 28 × 28 model trained
by Denton et al. (2015) on the edge residuals of the CIFAR dataset. The conﬁguration can be
found at https://gist.github.com/soumith/e3f722173ea16c1ea0d9. The model
was trained for 6 epochs.

EVALUATION

Table 3 shows the estimated variational lower bounds on the average train/validation/test log-
probabilities. Note that the alignDRAW model does not suffer much from overﬁtting. The results
substantially worsen after sharpening test images.

Model

Train

skipthoughtDRAW -1794.29
-1792.14
-1792.15

noalignDRAW
alignDRAW

Validation
-1797.41
-1796.94
-1797.24

Test
-1791.37
-1791.15
-1791.53

Test (after sharpening)
-2045.84
-2051.07
-2042.31

Table 3: The lower bound on the average test log-probabilities of conditional DRAW models, trained on the
Microsoft COCO dataset.

11

Published as a conference paper at ICLR 2016

APPENDIX C: EFFECT OF SHARPENING IMAGES.

Some examples of generated images before (top row) and after (bottom row) sharpening im-
ages using an adversarial network trained on residuals of a Laplacian pyramid conditioned on the
skipthought vectors of the captions.

Figure 7: Effect of sharpening images.

12

6
1
0
2
 
b
e
F
 
9
2
 
 
]

G
L
.
s
c
[
 
 
2
v
3
9
7
2
0
.
1
1
5
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2016

GENERATING IMAGES FROM CAPTIONS
WITH ATTENTION

Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba & Ruslan Salakhutdinov
Department of Computer Science
University of Toronto
Toronto, Ontario, Canada
{emansim,eparisotto,rsalakhu}@cs.toronto.edu, jimmy@psi.utoronto.ca

ABSTRACT

Motivated by the recent progress in generative models, we introduce a model that
generates images from natural language descriptions. The proposed model itera-
tively draws patches on a canvas, while attending to the relevant words in the de-
scription. After training on Microsoft COCO, we compare our model with several
baseline generative models on image generation and retrieval tasks. We demon-
strate that our model produces higher quality samples than other approaches and
generates images with novel scene compositions corresponding to previously un-
seen captions in the dataset.

1

INTRODUCTION

Statistical natural image modelling remains a fundamental problem in computer vision and image
understanding. The challenging nature of this task has motivated recent approaches to exploit the
inference and generative capabilities of deep neural networks. Previously studied deep generative
models of images often deﬁned distributions that were restricted to being either unconditioned or
conditioned on classiﬁcation labels. In real world applications, however, images rarely appear in
isolation as they are often accompanied by unstructured textual descriptions, such as on web pages
and in books. The additional information from these descriptions could be used to simplify the
image modelling task. Moreover, learning generative models conditioned on text also allows a better
understanding of the generalization performance of the model, as we can create textual descriptions
of completely new scenes not seen at training time.

There are numerous ways to learn a generative model over both image and text modalities. One
approach is to learn a generative model of text conditioned on images, known as caption generation
(Kiros et al., 2014a; Karpathy & Li, 2015; Vinyals et al., 2015; Xu et al., 2015). These models take
an image descriptor and generate unstructured texts using a recurrent decoder. In contrast, in this
taking textual descriptions
paper we explore models that condition in the opposite direction, i.e.
as input and using them to generate relevant images. Generating high dimensional realistic images
from their descriptions combines the two challenging components of language modelling and image
generation, and can be considered to be more difﬁcult than caption generation.

In this paper, we illustrate how sequential deep learning techniques can be used to build a conditional
probabilistic model over natural image space effectively. By extending the Deep Recurrent Atten-
tion Writer (DRAW) (Gregor et al., 2015), our model iteratively draws patches on a canvas, while
attending to the relevant words in the description. Overall, the main contributions of this work are
the following: we introduce a conditional alignDRAW model, a generative model of images from
captions using a soft attention mechanism. The images generated by our alignDRAW model are
reﬁned in a post-processing step by a deterministic Laplacian pyramid adversarial network (Denton
et al., 2015). We further illustrate how our method, learned on Microsoft COCO (Lin et al., 2014),
generalizes to captions describing novel scenes that are not seen in the dataset, such as “A stop sign
is ﬂying in blue skies” (see Fig. 1).

1

Published as a conference paper at ICLR 2016

A stop sign is ﬂying in
blue skies.

A herd of elephants ﬂy-
ing in the blue skies.

A toilet seat sits open in
the grass ﬁeld.

A person skiing on sand
clad vast desert.

Figure 1: Examples of generated images based on captions that describe novel scene compositions that are
highly unlikely to occur in real life. The captions describe a common object doing unusual things or set in a
strange location.

2 RELATED WORK

Deep Neural Networks have achieved signiﬁcant success in various tasks such as image recognition
(Krizhevsky et al., 2012), speech transcription (Graves et al., 2013), and machine translation (Bah-
danau et al., 2015). While most of the recent success has been achieved by discriminative models,
generative models have not yet enjoyed the same level of success. Most of the previous work in
generative models has focused on variants of Boltzmann Machines (Smolensky, 1986; Salakhutdi-
nov & Hinton, 2009) and Deep Belief Networks (Hinton et al., 2006). While these models are very
powerful, each iteration of training requires a computationally costly step of MCMC to approximate
derivatives of an intractable partition function (normalization constant), making it difﬁcult to scale
them to large datasets.

Kingma & Welling (2014), Rezende et al. (2014) have introduced the Variational Auto-Encoder
(VAE) which can be seen as a neural network with continuous latent variables. The encoder is used
to approximate a posterior distribution and the decoder is used to stochastically reconstruct the data
from latent variables. Gregor et al. (2015) further introduced the Deep Recurrent Attention Writer
(DRAW), extending the VAE approach by incorporating a novel differentiable attention mechanism.

Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are another type of generative
models that use noise-contrastive estimation (Gutmann & Hyv¨arinen, 2010) to avoid calculating
an intractable partition function. The model consists of a generator that generates samples using
a uniform distribution and a discriminator that discriminates between real and generated images.
Recently, Denton et al. (2015) have scaled those models by training conditional GANs at each level
of a Laplacian pyramid of images.

While many of the previous approaches have focused on unconditional models or models condi-
tioned on labels, in this paper we develop a generative model of images conditioned on captions.

3 MODEL

Our proposed model deﬁnes a generative process of images conditioned on captions. In particular,
captions are represented as a sequence of consecutive words and images are represented as a se-
quence of patches drawn on a canvas ct over time t = 1, ..., T . The model can be viewed as a part
of the sequence-to-sequence framework (Sutskever et al., 2014; Cho et al., 2014; Srivastava et al.,
2015).

3.1 LANGUAGE MODEL: THE BIDIRECTIONAL ATTENTION RNN

Let y be the input caption, represented as a sequence of 1-of-K encoded words y = (y1, y2, ..., yN ),
where K is the size of the vocabulary and N is the length of the sequence. We obtain the caption sen-
tence representation by ﬁrst transforming each word yi to an m-dimensional vector representation
hlang
, i = 1, .., N using the Bidirectional RNN. In a Bidirectional RNN, the two LSTMs (Hochre-
i
iter & Schmidhuber, 1997) with forget gates (Gers et al., 2000) process the input sequence from both
forward and backward directions. The Forward LSTM computes the sequence of forward hidden
−→
−→
−→
h lang
h lang
h lang
N ] , whereas the Backward LSTM computes the sequence of back-
, ...,
states [
2
1
←−
←−
h lang
h lang
N ]. These hidden states are then concatenated together
ward hidden states [
,
2
1
−→
h lang
into the sequence hlang = [hlang
i

←−
h lang
, ..., hlang

N ], with hlang

, ...,
, hlang
2

], 1 ≤ i ≤ N .

←−
h lang
i

= [

1

,

,

i

2

Published as a conference paper at ICLR 2016

Figure 2: AlignDRAW model for generating images by learning an alignment between the input captions and
generating canvas. The caption is encoded using the Bidirectional RNN (left). The generative RNN takes a
latent sequence z1:T sampled from the prior along with the dynamic caption representation s1:T to generate
the canvas matrix cT , which is then used to generate the ﬁnal image x (right). The inference RNN is used to
compute approximate posterior Q over the latent sequence.

3.2

IMAGE MODEL: THE CONDITIONAL DRAW NETWORK

To generate an image x conditioned on the caption information y, we extended the DRAW net-
work (Gregor et al., 2015) to include caption representation hlang at each step, as shown in Fig. 2.
The conditional DRAW network is a stochastic recurrent neural network that consists of a sequence
of latent variables Zt ∈ RD, t = 1, .., T , where the output is accumulated over all T time-steps. For
simplicity in notation, the images x ∈ Rh×w are assumed to have size h-by-w and only one color
channel.

Unlike the original DRAW network where latent variables are independent spherical Gaussians
N (0, I), the latent variables in the proposed alignDRAW model have their mean and variance de-
pend on the previous hidden states of the generative LSTM hgen
t−1, except for P (Z1) = N (0, I).
Namely, the mean and variance of the prior distribution over Zt are parameterized by:

P (Zt|Z1:t−1) = N

µ(hgen

t−1), σ(hgen
t−1)

(cid:18)

(cid:19)
,

µ(hgen
σ(hgen

t−1) = tanh(Wµhgen
t−1) = exp (cid:0) tanh(Wσhgen

t−1),

t−1)(cid:1),

where Wµ ∈ RD×n, Wσ ∈ RD×n are the learned model parameters, and n is the dimensional-
ity of hgen
, the hidden state of the generative LSTM. Similar to (Bachman & Precup, 2015), we
have observed that the model performance is improved by including dependencies between latent
variables.

t

Formally, an image is generated by iteratively computing the following set of equations for t =
1, ..., T (see Fig. 2), with hgen
and c0 initialized to learned biases:
(cid:19)
,

zt ∼ P (Zt|Z1:t−1) = N

µ(hgen

t−1), σ(hgen
t−1)

(1)

(cid:18)

0

t−1, hlang),

st = align(hgen
t = LSTM gen(hgen
hgen
ct = ct−1 + write(hgen

t−1, [zt, st]),
),
(cid:89)

t

˜x ∼ P (x | y, Z1:T ) =

P (xi | y, Z1:T ) =

Bern(σ(cT,i)).

i

(cid:89)

i

The align function is used to compute the alignment between the input caption and intermediate
image generative steps (Bahdanau et al., 2015). Given the caption representation from the language
model, hlang = [hlang
N ], the align operator outputs a dynamic sentence representa-
tion st at each step through a weighted sum using alignment probabilities αt

, ..., hlang

, hlang
2

1

st = align(hgen

t−1, hlang) = αt

1hlang

1 + αt

2hlang

2 + ... + αt

1...N :
N hlang
N .

3

(2)

(3)

(4)

(5)

(6)

Published as a conference paper at ICLR 2016

The corresponding alignment probability αt
caption representation hlang and the current hidden state of the generative model hgen
t−1:
v(cid:62) tanh(U hlang

k for the kth word in the caption is obtained using the

exp

k + W hgen

t−1 + b)

(cid:16)

(cid:17)

αt

k =

(cid:80)N

i=1 exp

(cid:16)

v(cid:62) tanh(U hlang

i

+ W hgen

t−1 + b)

(cid:17) ,

(7)

where v ∈ Rl, U ∈ Rl×m, W ∈ Rl×n and b ∈ Rl are the learned model parameters of the alignment
model.
The LSTM gen function of Eq. 3 is deﬁned by the LSTM network with forget gates (Gers et al.,
2000) at a single time-step. To generate the next hidden state hgen
, the LSTM gen takes the previous
hidden state hgen
t−1 and combines it with the input from both the latent sample zt and the sentence
representation st.
The output of the LSTM gen function hgen
is then passed through the write operator which is added
to a cumulative canvas matrix ct ∈ Rh×w (Eq. 4). The write operator produces two arrays of 1D
Gaussian ﬁlter banks Fx(hgen
) ∈ Rw×p whose ﬁlter locations and scales are
t
t
computed from the generative LSTM hidden state hgen
(same as deﬁned in Gregor et al. (2015)).
t
The Gaussian ﬁlter banks are then applied to the generated p-by-p image patch K(hgen
) ∈ Rp×p,
placing it onto the canvas:

) ∈ Rh×p and Fy(hgen

t

t

t

∆ct = ct − ct−1 = write(hgen

) = Fx(hgen

)K(hgen

)Fy(hgen

t

)(cid:62).

t

t

t

(8)

Finally, each entry cT,i from the ﬁnal canvas matrix cT is transformed using a sigmoid function σ to
produce a conditional Bernoulli distribution with mean vector σ(cT ) over the h × w image pixels x
given the latent variables Z1:T and the input caption y1. In practice, when generating an image x,
instead of sampling from the conditional Bernoulli distribution, we simply use the conditional mean
x = σ(cT ).

3.3 LEARNING

(cid:88)

Z

The model is trained to maximize a variational lower bound L on the marginal likelihood of the
correct image x given the input caption y:

L =

Q(Z | x, y) log P (x | y, Z) − DKL (Q(Z | x, y) (cid:107) P (Z | y)) ≤ log P (x | y).

(9)

Similar to the DRAW model, the inference recurrent network produces an approximate posterior
Q(Z1:T | x, y) via a read operator, which reads a patch from an input image x using two arrays of
1D Gaussian ﬁlters (inverse of write from section 3.2) at each time-step t. Speciﬁcally,

ˆxt = x − σ(ct−1),
rt = read (xt, ˆxt, hgen

t−1),
t−1 , [rt, hgen
= LSTM inf er(hinf er
), σ(hinf er
,
t

µ(hinf er
t

t−1]),
(cid:17)
)

(cid:16)

hinf er
t

Q(Zt|x, y, Z1:t−1) = N

0

where ˆx is the error image and hinf er
is initialized to the learned bias b. Note that the inference
LSTM inf er takes as its input both the output of the read operator rt ∈ Rp×p, which depends on
the original input image x, and the previous state of the generative decoder hgen
t−1, which depends
on the latent sample history z1:t−1 and dynamic sentence representation st−1 (see Eq. 3). Hence,
the approximate posterior Q will depend on the input image x, the corresponding caption y, and the
latent history Z1:t−1, except for the ﬁrst step Q(Z1|x), which depends only on x.

The terms in the variational lower bound Eq. 9 can be rearranged using the law of total expectation.
Therefore, the variational bound L is calculated as follows:

log p(x | y, Z1:T ) −

DKL (Q(Zt | Z1:t−1, y, x) (cid:107) P (Zt | Z1:t−1, y))

(cid:34)

L =EQ(Z1:T | y,x)

− DKL (Q(Z1 | x) (cid:107) P (Z1)) .

1We also experimented with a conditional Gaussian observation model, but it worked worse compared to

the Bernoulli model.

T
(cid:88)

t=2

4

(10)

(11)

(12)

(13)

(cid:35)

(14)

Published as a conference paper at ICLR 2016

A yellow school bus
parked in a parking lot.

A red school bus parked
in a parking lot.

A green
parked in a parking lot.

school

bus

A blue school bus parked
in a parking lot.

The decadent chocolate
desert is on the table.

A bowl of bananas is on
the table.

A vintage photo of a cat. A vintage photo of a dog.

Figure 3: Top: Examples of changing the color while keeping the caption ﬁxed. Bottom: Examples of changing
the object while keeping the caption ﬁxed. The shown images are the probabilities σ(cT ). Best viewed in
colour.

The expectation can be approximated by L Monte Carlo samples ˜z1:T from Q(Z1:T | y, x):
(cid:35)

(cid:34)

log p(x | y, ˜zl

1:T ) −

DKL

(cid:0)Q(Zt | ˜zl

1:t−1, y, x) (cid:107) P (Zt | ˜zl

1:t−1, y)(cid:1)

L ≈

1
L

L
(cid:88)

l=1

T
(cid:88)

t=2

− DKL (Q(Z1 | x) (cid:107) P (Z1)) .

(15)

The model can be trained using stochastic gradient descent. In all of our experiments, we used
only a single sample from Q(Z1:T | y, x) for parameter learning. Training details, hyperparameter
settings, and the overall model architecture are speciﬁed in Appendix B. The code is available at
https://github.com/emansim/text2image.

3.4 GENERATING IMAGES FROM CAPTIONS

During the image generation step, we discard the inference network and instead sample from the
prior distribution. Due to the blurriness of samples generated by the DRAW model, we perform an
additional post processing step where we use an adversarial network trained on residuals of a Lapla-
cian pyramid conditioned on the skipthought representation (Kiros et al., 2015) of the captions to
sharpen the generated images, similar to (Denton et al., 2015). By ﬁxing the prior of the adversarial
generator to its mean, it gets treated as a deterministic neural network that allows us to deﬁne the
conditional data term in Eq. 14 on the sharpened images and estimate the variational lower bound
accordingly.

4 EXPERIMENTS

4.1 MICROSOFT COCO

Microsoft COCO (Lin et al., 2014) is a large dataset containing 82,783 images, each annotated with
at least 5 captions. The rich collection of images with a wide variety of styles, backgrounds and
objects makes the task of learning a good generative model very challenging. For consistency with
related work on caption generation, we used only the ﬁrst ﬁve captions when training and evaluating
our model. The images were resized to 32 × 32 pixels for consistency with other tiny image datasets
(Krizhevsky, 2009). In the following subsections, we analyzed both the qualitative and quantitative
aspects of our model as well as compared its performance with that of other, related generative
models.2 Appendix A further reports some additional experiments using the MNIST dataset.

4.1.1 ANALYSIS OF GENERATED IMAGES

The main goal of this work is to learn a model that can understand the semantic meaning expressed
in the textual descriptions of images, such as the properties of objects, the relationships between
them, and then use that knowledge to generate relevant images. To examine the understanding of

2To see more generated images, visit http://www.cs.toronto.edu/˜emansim/cap2im.html

5

Published as a conference paper at ICLR 2016

A very large commercial
plane ﬂying in blue skies.

A very large commer-
cial plane ﬂying in rainy
skies.

A herd of elephants walk-
ing across a dry grass
ﬁeld.

A herd of elephants walk-
ing across a green grass
ﬁeld.

Figure 4: Bottom: Examples of changing the background while keeping the caption ﬁxed. Top: The respective
nearest training images based on pixel-wise L2 distance. The nearest images from the training set also indicate
that the model was not simply copying the patterns it observed during the learning phase.

our model, we wrote a set of captions inspired by the COCO dataset and changed some words in the
captions to see whether the model made the relevant changes in the generated samples.

First, we explored whether the model understood one of the most basic properties of any object, the
color. In Fig. 3, we generated images of school buses with four different colors: yellow, red, green
and blue. Although, there are images of buses with different colors in the training set, all mentioned
school buses are speciﬁcally colored yellow. Despite that, the model managed to generate images of
an object that is visually reminiscent of a school bus that is painted with the speciﬁed color.

Apart from changing the colors of objects, we next examined whether changing the background of
the scene described in a caption would result in the appropriate changes in the generated samples.
The task of changing the background of an image is somewhat harder than just changing the color of
an object because the model will have to make alterations over a wider visual area. Nevertheless, as
shown in Fig. 4 changing the skies from blue to rainy in a caption as well as changing the grass type
from dry to green in another caption resulted in the appropriate changes in the generated image.

Despite a large number of ways of changing colors and backgrounds in descriptions, in general
we found that the model made appropriate changes as long as some similar pattern was present in
the training set. However, the model struggled when the visual difference between objects was very
small, such as when the objects have the same general shape and color. In Fig. 3, we demonstrate that
when we swap two objects that are both visually similar, for example cats and dogs, it is difﬁcult
to discriminate solely from the generated samples whether it is an image of a cat or dog, even
though we might notice an animal-like shape. This highlights a limitation of the model in that it has
difﬁculty modelling the ﬁne-grained details of objects.

As a test of model generalization, we tried generating images corresponding to captions that describe
scenarios that are highly unlikely to occur in real life. These captions describe a common object
doing unusual things or set in a strange location, for example “A toilet seat sits open in the grass
ﬁeld”. Even though some of these scenarios may never occur in real life, it is very easy for humans
to imagine the corresponding scene. Nevertheless, as you can see in Fig. 1, the model managed to
generate reasonable images.

4.1.2 ANALYSIS OF ATTENTION

After ﬂipping sets of words in the captions, we further explored which words the model attended
to when generating images. It turned out that during the generation step, the model mostly focused
on the speciﬁc words (or nearby words) that carried the main semantic meaning expressed in the
sentences. The attention values of words in sentences helped us interpret the reasons why the model
made the changes it did when we ﬂipped certain words. For example, in Fig. 5, top row, we can see
that when we ﬂipped the word “desert” to “forest”, the attention over words in the sentence did not
change drastically. This suggests that, in their respective sentences, the model looked at “desert”
and “forest” with relatively equal probability, and thus made the correct changes. In contrast, when
we swap words “beach” and “sun”, we can see a drastic change between sentences in the probability
distribution over words. By noting that the model completely ignores the word “sun” in the second

6

Published as a conference paper at ICLR 2016

A rider on a blue motor-
cycle in the desert.

A rider on a blue motor-
cycle in the forest.

A surfer, a woman, and a
child walk on the beach.

A surfer, a woman, and a
child walk on the sun.

alignDRAW

LAPGAN

Conv-Deconv VAE

Fully-Conn VAE

Figure 5: Top: Examples of most attended words while changing the background in the caption. Bottom: Four
different models displaying results from sampling caption A group of people walk on a beach with surf boards.

sentence, we can therefore gain a more thorough understanding of why we see no visual differences
between the images generated by each caption.

We also tried to analyze the way the model generated images. Unfortunately, we found that there
was no signiﬁcant connection between the patches drawn on canvas and the most attended words at
particular time-steps.

4.1.3 COMPARISON WITH OTHER MODELS

Quantitatively evaluating generative models remains a challenging task in itself as each method of
evaluation suffers from its own speciﬁc drawbacks. Compared to reporting classiﬁcation accuracies
in discriminative models, the measures deﬁning generative models are intractable most of the times
and might not correctly deﬁne the real quality of the model. To get a better comparison between
performances of different generative models, we report results on two different metrics as well as a
qualitative comparison of different generative models.

We compared the performance of the proposed model to the DRAW model conditioned on cap-
tions without the align function (noalignDRAW) as well as the DRAW model conditioned on
the skipthought vectors of (Kiros et al., 2015) (skipthoughtDRAW). All of the conditional DRAW
models were trained with a binary cross-entropy cost function, i.e. they had Bernoulli conditional
likelihoods. We also compared our model with Fully-Connected (Fully-Conn) and Convolutional-
Deconvolutional (Conv-Deconv) Variational Autoencoders which were trained with the least squares
cost function. The LAPGAN model of (Denton et al., 2015) was trained on a two level Lapla-
cian Pyramid with a GAN as a top layer generator and all stages were conditioned on the same
skipthought vector.

In Fig. 5, bottom row, we generated several samples from the prior of each of the current state-of-
the-art generative models, conditioned on the caption “A group of people walk on a beach with surf
boards”. While all of the samples look sharp, the images generated by LAPGAN look more noisy
and it is harder to make out deﬁnite objects, whereas the images generated by variational models
trained with least squares cost function have a watercolor effect on the images. We found that the
quality of generated samples was similar among different variants of conditional DRAW models.

As for the quantitative comparison of different models, we ﬁrst compare the performances of the
model trained with variational methods. We rank the images in the test set conditioned on the
captions based on the variational lower bound of the log-probabilities and then report the Precision-
Recall metric as an evaluation of the quality of the generative model (see Table 1.). Perhaps un-
surprisingly, generative models did not perform well on the image retrieval task. To deal with the
large computational complexity involved in looping through each test image, we create a shortlist
of one hundred images including the correct one, based on the images having the closest Euclidean
distance in the last fully-connected feature space of a VGG-like model (Simonyan & Zisserman,
2015) trained on the CIFAR dataset3 (Krizhevsky, 2009). Since there are “easy” images for which

3The architecture of the model is described here http://torch.ch/blog/2015/07/30/cifar.
html. The shortlist of test images used for evaluation can be downloaded from http://www.cs.
toronto.edu/˜emansim/cap2im/test-nns.pkl.

7

Published as a conference paper at ICLR 2016

Model
LAPGAN
Fully-Conn VAE
Conv-Deconv VAE
skipthoughtDRAW
noalignDRAW
alignDRAW

Microsoft COCO (prior to sharpening)
Image Retrieval
R@1 R@5 R@10 R@50 Med r
-
12.0
12.0
18.9
23.1
22.9

-
53.4
52.9
63.3
68.0
68.5

-
6.6
6.5
11.2
14.1
14.0

-
1.0
1.0
2.0
2.8
3.0

-
47
48
36
31
31

Image Similarity
SSI
0.08 ± 0.07
0.156 ± 0.10
0.164 ± 0.10
0.157 ± 0.11
0.155 ± 0.11
0.156 ± 0.11

Table 1: Retrieval results of different models. R@K is Recall@K (higher is better). Med r is the median rank
(lower is better). SSI is Structural Similarity Index, which is between −1 and 1 (higher is better).

the model assigns high log-probabilities independent of the query caption, we instead look at the
ratio of the likelihood of the image conditioned on the sentence to the likelihood of the image con-
ditioned on the mean sentence representation in the training set, following the retrieval protocol
of (Kiros et al., 2014b). We also found that the lower bound on the test log-probabilities decreased
for sharpened images, and that sharpening considerably hurt the retrieval results. Since sharpening
changes the statistics of images, the estimated log-probabilities of image pixels is not necessarily
a good metric. Some examples of generated images before and after sharpening are shown in Ap-
pendix C.

Instead of calculating error per pixel, we turn to a smarter metric, the Structural Similarity Index
(SSI) (Wang et al., 2004), which incorporates luminance and contrast masking into the error cal-
culation. Strong inter-dependencies of closer pixels are also taken into account and the metric is
calculated on small windows of the images. Due to independence property of test captions, we sam-
pled ﬁfty images from the prior of each generative model for every caption in the test set in order
to calculate SSI. As you can see on Table 1, SSI scores achieved by variational models were higher
compared to SSI score achieved by LAPGAN.

5 DISCUSSION

In this paper, we demonstrated that the alignDRAW model, a combination of a recurrent variational
autoencoder with an alignment model over words, succeeded in generating images that correspond
to a given input caption. By extensively using attentional mechanisms, our model gained several
advantages. Namely, the use of the visual attention mechanism allowed us to decompose the problem
of image generation into a series of steps instead of a single forward pass, while the attention over
words provided us an insight whenever our model failed to generate a relevant image. Additionally,
our model generated images corresponding to captions which generalized beyond the training set,
such as sentences describing novel scenarios which are highly unlikely to occur in real life.

Because the alignDRAW model tends to output slightly blurry samples, we augmented the model
with a sharpening post-processing step in which GAN generated edges which were added to the
alignDRAW samples. Unfortunately, this is not an ideal solution due to the fact that the whole
model was not trained in an end-to-end fashion. Therefore a direction of future work would be to
ﬁnd methods that can bypass the separate post-processing step and output sharp images directly in
an end-to-end manner.

Acknowledgments: This work was supported by Samsung and IARPA, Raytheon BBN Contract No.
D11PC20071. We would like to thank developers of Theano (Bastien et al., 2012), the authors of (Denton
et al., 2015) for open sourcing their code, and Ryan Kiros and Nitish Srivastava for helpful discussions.

REFERENCES

ICLR, 2015.

Bachman, Philip and Precup, Doina. Data generation as sequential decision making. In NIPS, 2015.

Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. In

Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian J., Bergeron, Arnaud,
Bouchard, Nicolas, Warde-Farley, David, and Bengio, Yoshua. Theano: new features and speed improve-
ments. CoRR, abs/1211.5590, 2012.

8

Published as a conference paper at ICLR 2016

Cho, K., van Merrienboer, B., G¨ulc¸ehre, C¸ ., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. Learning
phrase representations using RNN encoder-decoder for statistical machine translation. In EMNLP, 2014.

Denton, Emily L., Chintala, Soumith, Szlam, Arthur, and Fergus, Robert. Deep generative image models using

a laplacian pyramid of adversarial networks. In NIPS, 2015.

Gers, Felix, Schmidhuber, J¨urgen, and Cummins, Fred. Learning to forget: Continual prediction with lstm.

Neural Computation, 2000.

Goodfellow, Ian J., Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil,

Courville, Aaron C., and Bengio, Yoshua. Generative adversarial nets. In NIPS, 2014.

Graves, A., Jaitly, N., and Mohamed, A.-r. Hybrid speech recognition with deep bidirectional LSTM. In IEEE

Workshop on Automatic Speech Recognition and Understanding, 2013.

Gregor, Karol, Danihelka, Ivo, Graves, Alex, and Wierstra, Daan. DRAW: A recurrent neural network for

image generation. In ICML, 2015.

Gutmann, Michael and Hyv¨arinen, Aapo. Noise-contrastive estimation: A new estimation principle for unnor-

malized statistical models. In AISTATS, 2010.

Hinton, Geoffrey E., Osindero, Simon, and Teh, Yee Whye. A fast learning algorithm for deep belief nets.

Neural Computation, 2006.

Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural Computation, 1997.

Karpathy, Andrej and Li, Fei-Fei. Deep visual-semantic alignments for generating image descriptions.

In

CVPR, 2015.

Kingma, Diederik P. and Welling, Max. Auto-encoding variational bayes. In ICLR, 2014.

Kiros, R., Salakhutdinov, R., and Zemel, R. Multimodal neural language models. In ICML, 2014a.

Kiros, Ryan, Salakhutdinov, Ruslan, and Zemel, Richard S. Unifying visual-semantic embeddings with multi-

modal neural language models. CoRR, abs/1411.2539, 2014b.

Kiros, Ryan, Zhu, Yukun, Salakhutdinov, Ruslan, Zemel, Richard S., Torralba, Antonio, Urtasun, Raquel, and

Fidler, Sanja. Skip-thought vectors. In NIPS, 2015.

Krizhevsky, Alex. Learning multiple layers of features from tiny images. Master’s Thesis, Department of

Computer Science, University of Toronto, 2009.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convolutional

neural networks. In NIPS, 2012.

Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P., and Zitnick, C. L. Microsoft

COCO: Common objects in context. In ECCV, 2014.

Rezende, Danilo J., Mohamed, Shakir, and Wierstra, Daan. Stochastic backpropagation and variational infer-

ence in deep latent gaussian models. In ICML, 2014.

Salakhutdinov, Ruslan and Hinton, Geoffrey E. Deep boltzmann machines. In AISTATS, 2009.

Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image recognition.

In ICLR, 2015.

Smolensky, Paul.

Information processing in dynamical systems: foundations of harmony theory. Parallel

Distributed Processing: Explorations in the Microstructure of Cognition, 1986.

Srivastava, Nitish, Mansimov, Elman, and Salakhutdinov, Ruslan. Unsupervised learning of video representa-

tions using LSTMs. In ICML, 2015.

Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence to sequence learning with neural networks. In NIPS,

Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural image caption

2014.

generator. In CVPR, 2015.

Wang, Zhou, Bovik, Alan C., Sheikh, Hamid R., and Simoncelli, Eero P. Image quality assessment: from error

visibility to structural similarity. IEEE Transactions on Image Processing, 2004.

Xu, Kelvin, Ba, Jimmy, Kiros, Ryan, Cho, Kyunghyun, Courville, Aaron C., Salakhutdinov, Ruslan, Zemel,
Richard S., and Bengio, Yoshua. Show, attend and tell: Neural image caption generation with visual atten-
tion. In ICML, 2015.

9

Published as a conference paper at ICLR 2016

Figure 6: Examples of generating 60 × 60 MNIST images corresponding to respective captions. The captions
on the left column were part of the training set. The digits described in the captions on the right column were
hidden during training for the respective conﬁgurations.

APPENDIX A: MNIST WITH CAPTIONS

As an additional experiment, we trained our model on the MNIST dataset with artiﬁcial captions.
Either one or two digits from the MNIST training dataset were placed on a 60 × 60 blank image.
One digit was placed in one of the four (top-left, top-right, bottom-left or bottom-right) corners
of the image. Two digits were either placed horizontally or vertically in non-overlapping fashion.
The corresponding artiﬁcial captions speciﬁed the identity of each digit along with their relative
positions, e.g. “The digit three is at the top of the digit one”, or “The digit seven is at the bottom left
of the image”.

The generated images together with the attention alignments are displayed in Figure 6. The model
correctly displayed the speciﬁed digits at the described positions and even managed to generalize
reasonably well to the conﬁgurations that were never present during training. In the case of gener-
ating two digits, the model would dynamically attend to the digit in the caption it was drawing at
that particular time-step. Similarly, in the setting where the caption speciﬁed only a single digit, the
model would correctly attend to the digit in the caption during the whole generation process. In both
cases, the model placed small attention values on the words describing the position of digits in the
images.

APPENDIX B: TRAINING DETAILS

HYPERPARAMETERS

Each parameter in alignDRAW was initialized by sampling from a Gaussian distribution with mean
0 and standard deviation 0.01. The model was trained using RMSprop with an initial learning rate
of 0.001. For the Microsoft COCO task, we trained our model for 18 epochs. The learning rate
was reduced to 0.0001 after 11 epochs. For the MNIST with Captions task, the model was trained
for 150 epochs and the learning rate was reduced to 0.0001 after 110 epochs. During each epoch,
randomly created 10, 000 training samples were used for learning. The norm of the gradients was
clipped at 10 during training to avoid the exploding gradients problem.

We used a vocabulary size of K = 25323 and K = 22 for the Microsoft COCO and MNIST with
Captions datasets respectively. All capital letters in the words were converted to small letters as a
in the language model had
preprocessing step. For all tasks, the hidden states
128 units. Hence the dimensionality of the concatenated state of the Bidirectional LSTM hlang
=
−→
h lang
] was 256. The parameters in the align operator (Eq. 7) had a dimensionality of
[
i
l = 512, so that v ∈ R512, U ∈ R512×256, W ∈ R512×ngen
and b ∈ R512. The architectural
conﬁgurations of the alignDRAW models are shown in Table 2.

−→
h lang
i

←−
h lang
i

←−
h lang
i

and

,

i

10

Published as a conference paper at ICLR 2016

alignDRAW Model

Task

MS COCO
MNIST

#glimpses
T
32
32

Inference

Generative

Latent

Read Size Write Size

Dim of hinf er Dim of hgen Dim of Z

550
300

550
300

275
150

p
9
8

p
9
8

Table 2: The architectural conﬁgurations of alignDRAW models.

The GAN model used for sharpening had the same conﬁguration as the 28 × 28 model trained
by Denton et al. (2015) on the edge residuals of the CIFAR dataset. The conﬁguration can be
found at https://gist.github.com/soumith/e3f722173ea16c1ea0d9. The model
was trained for 6 epochs.

EVALUATION

Table 3 shows the estimated variational lower bounds on the average train/validation/test log-
probabilities. Note that the alignDRAW model does not suffer much from overﬁtting. The results
substantially worsen after sharpening test images.

Model

Train

skipthoughtDRAW -1794.29
-1792.14
-1792.15

noalignDRAW
alignDRAW

Validation
-1797.41
-1796.94
-1797.24

Test
-1791.37
-1791.15
-1791.53

Test (after sharpening)
-2045.84
-2051.07
-2042.31

Table 3: The lower bound on the average test log-probabilities of conditional DRAW models, trained on the
Microsoft COCO dataset.

11

Published as a conference paper at ICLR 2016

APPENDIX C: EFFECT OF SHARPENING IMAGES.

Some examples of generated images before (top row) and after (bottom row) sharpening im-
ages using an adversarial network trained on residuals of a Laplacian pyramid conditioned on the
skipthought vectors of the captions.

Figure 7: Effect of sharpening images.

12

