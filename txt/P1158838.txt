Prepared for submission to JHEP

QCD-Aware Recursive Neural Networks for Jet Physics

Gilles Louppea,b,1 Kyunghyun Chob Cyril Becota,2 Kyle Cranmera,b

aNew York University, Center for Cosmology & Particle Physics, 726 Broadway, New York, NY
bNew York University, Center for Data Science, 60 5th Ave., New York, NY

E-mail: g.louppe@uliege.be, kyunghyun.cho@nyu.edu,
cyril.becot@cern.ch, kyle.cranmer@nyu.edu

Abstract: Recent progress in applying machine learning for jet physics has been built
upon an analogy between calorimeters and images. In this work, we present a novel class
of recursive neural networks built instead upon an analogy between QCD and natural
languages.
In the analogy, four-momenta are like words and the clustering history of
sequential recombination jet algorithms is like the parsing of a sentence. Our approach
works directly with the four-momenta of a variable-length set of particles, and the jet-based
tree structure varies on an event-by-event basis. Our experiments highlight the ﬂexibility of
our method for building task-speciﬁc jet embeddings and show that recursive architectures
are signiﬁcantly more accurate and data eﬃcient than previous image-based networks. We
extend the analogy from individual jets (sentences) to full events (paragraphs), and show
for the ﬁrst time an event-level classiﬁer operating on all the stable particles produced in
an LHC event.

ArXiv ePrint: 1702.00748

8
1
0
2
 
l
u
J
 
3
1
 
 
]
h
p
-
p
e
h
[
 
 
2
v
8
4
7
0
0
.
2
0
7
1
:
v
i
X
r
a

1Currently at University of Li`ege
2Currently at DESY

Contents

1 Introduction

2 Problem statement

3 Recursive embedding

Individual jets

3.1
3.2 Full events

4 Data, Preprocessing and Experimental Setup

5 Experiments with Jet-Level Classiﬁcation

5.1 Performance studies
5.2

Infrared and Collinear Safety Studies

6 Experiments with event-level classiﬁcation

7 Related work

8 Conclusions

A Gated recursive jet embedding

B Gated recurrent event embedding

C Implementation details

1

3

3
3
5

6

7
7
11

12

14

16

20

21

21

1 Introduction

By far the most common structures seen in collisions at the Large Hadron Collider (LHC)
are collimated sprays of energetic hadrons referred to as ‘jets’. These jets are produced
from the fragmentation and hadronization of quarks and gluons as described by quantum
chromodynamics (QCD). Several goals for the LHC are centered around the treatment of
jets, and there has been an enormous amount of eﬀort from both the theoretical and ex-
perimental communities to develop techniques that are able to cope with the experimental
realities while maintaining precise theoretical properties. In particular, the communities
have converged on sequential recombination jet algorithms, methods to study jet substruc-
ture, and grooming techniques to provide robustness to pileup.

One compelling physics challenge is to search for highly boosted standard model par-
ticles decaying hadronically. For instance, if a hadronically decaying W boson is highly

– 1 –

boosted, then its decay products will merge into a single fat jet with a characteristic sub-
structure. Unfortunately, there is a large background from jets produced by more mundane
QCD processes. For this reason, several jet ‘taggers’ and variables sensitive to jet substruc-
ture have been proposed. Initially, this work was dominated by techniques inspired by our
intuition and knowledge of QCD; however, more recently there has been a wave of ap-
proaches that eschew this expert knowledge in favor of machine learning techniques. In
this paper, we present a hybrid approach that leverages the structure of sequential recom-
bination jet algorithms and deep neural networks.

Recent progress in applying machine learning techniques for jet physics has been built
upon an analogy between calorimeters and images [1–8]. These methods take a variable-
length set of 4-momenta and project them into a ﬁxed grid of η − φ towers or ‘pixels’ to
produce a ‘jet image’. The original jet classiﬁcation problem, hence, reduces to an image
classiﬁcation problem, lending itself to deep convolutional networks and other machine
learning algorithms. Despite their promising results, these models suﬀer from the fact that
they have many free parameters and that they require large amounts of data for training.
More importantly, the projection of jets into images also loses information, which impacts
classiﬁcation performance. The most obvious way to address this issue is to use a recurrent
neural network to process a sequence of 4-momenta as they are. However, it is not clear
how to order this sequence. While pT ordering is common in many contexts [5], it does
not capture important angular information critical for understanding the subtle structure
of jets.

In this work, we propose instead a solution for jet classiﬁcation based on an analogy
between QCD and natural languages, as inspired by several works from natural language
processing [9–14]. Much like a sentence is composed of words following a syntactic struc-
ture organized as a parse tree, a jet is also composed of 4-momenta following a structure
dictated by QCD and organized via the clustering history of a sequential recombination jet
algorithm. More speciﬁcally, our approach uses ‘recursive’ networks where the topology of
the network is given by the clustering history of a sequential recombination jet algorithm,
which varies on an event-by-event basis. This event-by-event adaptive structure can be
contrasted with the ‘recurrent’ networks that operate purely on sequences (see e.g., [15]).
The network is therefore given the 4-momenta without any loss of information, in a way
that also captures substructures, as motivated by physical theory.

It is convenient to think of the recursive neural network as providing a ‘jet embedding’,
which maps a set of 4-momenta into Rq. This embedding has ﬁxed length and can be fed
into a subsequent network used for classiﬁcation or regression. Thus the procedure can be
used for jet tagging or estimating parameters that characterize the jet, such as the masses of
resonances buried inside the jet. Importantly, the embedding and the subsequent network
can be trained jointly so that the embedding is optimized for the task at hand.

Extending the natural language analogy paragraphs of text are sequence of sentences,
In particular, we propose to embed the full particle
just as event are sequence of jets.
content of an event by feeding a sequence of jet-embeddings into a recurrent network. As
before, this event-level embedding can be fed into a subsequent network used for classiﬁ-
cation or regression. To our knowledge, this represents the ﬁrst machine learning model

– 2 –

operating on all the detectable particles in an event.

The remainder of the paper is structured as follows.

In Sec. 2, we formalize the
classiﬁcation tasks at the jet-level and event-level. We describe the proposed recursive
network architectures in Sec. 3 and detail the data samples and preprocessing used in
our experiments in Sec. 4. Our results are summarized and discussed ﬁrst in Sec. 5 for
experiments on a jet-level classiﬁcation problem, and then in Sec. 6 for experiments on
an event-level classiﬁcation problem. In Sec. 7, we relate our work to close contributions
from deep learning, natural language processing, and jet physics. Finally, we gather our
conclusions and directions for further works in Sec. 8.

2 Problem statement

We describe a collision event e ∈ E as being composed of a varying number of particles,
indexed by i, and where each particle is represented by its 4-momentum vector vi ∈ R4,
such that e = {vi|i = 1, . . . , N }.

The 4-momenta in each event can be clustered into jets with a sequential recombination
jet algorithm that recursively combines (by simply adding their 4-momenta) the pair i, i(cid:48)
that minimize

ii(cid:48) = min(p2α
dα

ti , p2α
ti(cid:48) )

∆R2
ii(cid:48)
R2

(2.1)

ti , p2α

ii(cid:48) is less than min(p2α

while dα
ti(cid:48) ) [16, 17]. These sequential recombination algorithms
have three hyper-parameters: R, pt,min, α, and jets with pt < pt,min are discarded. At that
point, the jet algorithm has clustered e into M jets, each of which can be represented by
a binary tree tj ∈ T indexed by j = 1, . . . , M with Nj leaves (corresponding to a subset
of the vi). In the following, we will consider the speciﬁc cases where α = 1, 0, −1, which
respectively correspond to the kt, Cambridge-Aachen and anti-kt algorithms.

In addition to jet algorithms, we consider a ‘random’ baseline that corresponds to
recombining particles at random to form random binary trees tj, along with ‘asc-pT ’ and
‘desc-pT ’ baselines, which correspond to degenerate binary trees formed from the sequences
of particles sorted respectively in ascending and descending order of pT .

For jet-level classiﬁcation or regression, each jet tj ∈ T in the training data comes
with labels or regression values yj ∈ Y jet. In this framework, our goal is to build a pre-
dictive model f jet : T (cid:55)→ Y jet minimizing some loss function Ljet. Similarly, for event-level
classiﬁcation or regression, we assume that each collision event el ∈ E in the training data
comes with labels or regression values yl ∈ Y event, and our goal is to build a predictive
model f event : E (cid:55)→ Y event minimizing some loss function Levent.

3 Recursive embedding

3.1

Individual jets

Let us ﬁrst consider the case of an individual jet whose particles are topologically structured
as a binary tree tj, e.g., based on a sequential recombination jet clustering algorithm or a
simple sequential sorting in pT . Let k = 1, . . . , 2Nj − 1 indexes the node of the binary tree

– 3 –

tj, and let the left and right children of node k be denoted by kL and kR respectively. Let
also kL always be the hardest child of k. By construction, we suppose that leaves k map
to particles i(k) while internal nodes correspond to recombinations. Using these notations,
we recursively deﬁne the embedding hjet

k ∈ Rq of node k in tj as

f jet(tj)

...

r
e
ﬁ
i
s
s
a
C

l

g
n
i
d
d
e
b
m
e

t
e
J

hjet

1 (tj)

hjet
k

hjet
kL

hjet
kR

v1 v2 ...

vNj

Figure 1. QCD-motivated recursive jet
embedding for classiﬁcation. For each
individual jet, the embedding hjet
1 (tj) is
computed recursively from the root node
down to the outer nodes of the binary tree
tj. The resulting embedding is chained
to a subsequent classiﬁer, as illustrated in
the top part of the ﬁgure. The topology of
the network in the bottom part is distinct
for each jet and is determined by a sequen-
tial recombination jet algorithm (e.g., kt
clustering).






(cid:40)

if k is a leaf

hjet

k =

uk







σ

Wh













hjet
kL
hjet
kR
uk







+ bh

otherwise

uk = σ (Wug(ok) + bu)

ok =

vi(k)
okL + okR

if k is a leaf

otherwise

(3.1)

(3.2)

(3.3)

where Wh ∈ Rq×3q, bh ∈ Rq, Wu ∈ Rq×4 and
bu ∈ Rq form together the shared parameters to
be learned, q is the size of the embedding, σ is the
ReLU activation function [18], and g is a function
extracting the kinematic features p, η, θ, φ, E, and
pT from the 4-momentum ok.

When applying Eqn. 3.1 recursively from the
root node k = 1 down to the outer nodes of
the binary tree tj, the resulting embedding, de-
noted hjet
1 (tj), eﬀectively summarizes the informa-
tion contained in the particles forming the jet into
a single vector. In particular, this recursive neural
network (RNN) embeds a binary tree of varying
shape and size into a vector of ﬁxed size. As a
result, the embedding hjet
1 (tj) can now be chained
to a subsequent classiﬁer or regressor to solve our
target supervised learning problem, as illustrated
in Figure 1. All parameters (i.e., of the recursive
jet embedding and of the classiﬁer) are learned
jointly using backpropagation through structure
[9] to minimize the loss Ljet, hence tailoring the
embedding to the speciﬁc requirements of the task.
Further implementation details, including an ef-
ﬁcient batched computation over distinct binary
trees, can be found in Appendix C.

– 4 –

Event embedding

Classiﬁer

v(t1)

v(t2)

v(tM )

hevent

M (e)

...

f event(e)

hjet

1 (t1)

hjet

1 (t2)

hjet

1 (tM )

...

...

Figure 2. QCD-motivated event embedding for classiﬁcation. The embedding of an event is
computed by feeding the sequence of pairs (v(tj), hjet
1 (tj)) over the jets it is made of, where v(tj) is
the unprocessed 4-momentum of the jet tj and hjet
1 (tj) is its embedding. The resulting event-level
embedding hevent
M (e) is chained to a subsequent classiﬁer, as illustrated in the right part of the
ﬁgure.

In addition to the recursive activation of Eqn. 3.1, we also consider and study its
extended version equipped with reset and update gates (see details in Appendix A). This
gated architecture allows the network to preferentially pass information along the left-child,
right-child, or their combination.

While we have not performed experiments, we point out that there is an analogous

style of architectures based on jet algorithms with 2 → 3 recombinations [17, 19, 20].

3.2 Full events

We now embed entire events e of variable size by feeding the embeddings of their individual
jets to an event-level sequence-based recurrent neural network.

As an illustrative example, we consider here a gated recurrent unit [21] (GRU) oper-
ating on the pT ordered sequence of pairs (v(tj), hjet
1 (tj)), for j = 1, . . . , M , where v(tj) is
the unprocessed 4-momentum of the jet tj and hjet
1 (tj) is its embedding. The ﬁnal output
hevent
M (e) (see Appendix B for details) of the GRU is chained to a subsequent classiﬁer to
solve an event-level classiﬁcation task. Again, all parameters (i.e., of the inner jet embed-
ding function, of the GRU, and of the classiﬁer) are learned jointly using backpropagation
through structure [9] to minimize the loss Levent. Figure 2 provides a schematic of the
full classiﬁcation model. In summary, combining two levels of recurrence provides a QCD-
motivated event-level embedding that eﬀectively operates at the hadron-level for all the
particles in the event.

– 5 –

In addition and for the purpose of comparison, we also consider the simpler baselines
where i) only the 4-momenta v(tj) of the jets are given as input to the GRU, without
augmentation with their embeddings, and ii) the 4-momenta vi of the constituents of
the event are all directly given as input to the GRU, without grouping them into jets or
providing the jet embeddings.

4 Data, Preprocessing and Experimental Setup

In order to focus attention on the impact of the network architectures and the projection
of input 4-momenta into images, we consider the same boosted W tagging example as used
in Refs. [1, 2, 4, 6]. The signal (y = 1) corresponds to a hadronically decaying W boson
with 200 < pT < 500 GeV, while the background (y = 0) corresponds to a QCD jet with
the same range of pT .

We are grateful to the authors of Ref. [6] for sharing the data used in their studies.
We obtained both the full-event records from their PYTHIA benchmark samples, including
both the particle-level data and the towers from the DELPHES detector simulation.
In
addition, we obtained the fully processed jet images of 25×25 pixels, which include the
initial R = 1 anti-kt jet clustering and subsequent trimming, translation, pixelisation,
rotation, reﬂection, cropping, and normalization preprocessing stages detailed in Ref. [2, 6].
Our training data was collected by sampling from the original data a total of 100,000
signal and background jets with equal prior. The testing data was assembled similarly by
sampling 100,000 signal and background jets, without overlap with the training data. For
direct comparison with Ref. [6], performance is evaluated at test time within the restricted
window of 250 < pT < 300 and 50 ≤ m ≤ 110, where the signal and background jets
are re-weighted to produce ﬂat pT distributions. Results are reported in terms of the
area under the ROC curve (ROC AUC) and of background rejection (i.e., 1/FPR) at
50% signal eﬃciency (R(cid:15)=50%). Average scores reported include uncertainty estimates that
come from training 30 models with distinct initial random seeds. About 2% of the models
had technical problems during training (e.g., due to numerical errors), so we applied a
simple algorithm to ensure robustness: we discarded models whose R(cid:15)=50% was outside of
3 standard deviations of the mean, where the mean and standard deviation were estimated
excluding the ﬁve best and worst performing models.

For our jet-level experiments we consider as input to the classiﬁers the 4-momenta vi
from both the particle-level data and the DELPHES towers. We also compare the performance
with and without the projection of those 4-momenta into images. While the image data
already included the full pre-processing steps, when considering particle-level and tower
inputs we performed the initial R = 1 anti-kt jet clustering to identify the constituents
of the highest pT jet t1 of each event, and then performed the subsequent translation,
rotation, and reﬂection pre-processing steps (omitting cropping and normalization). When
processing the image data, we inverted the normalization that enforced the sum of the
squares of the pixel intensities be equal to one.1

1In Ref. [2], the jet images did not include the DELPHES detector simulation, they were comparable to

our particle scenario with the additional discretization into pixels.

– 6 –

For our event-level experiments we were not able to use the data from Ref. [6] because
the signal sample corresponded to pp → W (→ J)Z(→ ν ¯ν) and the background to pp →
jj. Thus the signal was characterized by one high-pT jet and large missing energy from
Z(→ ν ¯ν) which is trivially separated from the dijet background. For this reason, we
generated our own PYTHIA and DELPHES samples of pp → W (cid:48) → W (→ J)Z(→ J) and
QCD background such that both the signal and background have two high-pT jets. We
use mW (cid:48) = 700 GeV and restrict ˆpt of the 2 → 2 scattering process to 300 < ˆpt < 350
GeV. Our focus is to demonstrate the scalability of our method to all the particles or
towers in an event, and not to provide a precise statement about physics reach for this
signal process. In this case each event e was clustered by the same anti-kt algorithm with
R = 1, and then the constituents of each jet were treated as in Sec. 3.1 (i.e., reclustered
using kt or a sequential ordering in pT to provide the network topology for a non-gated
embedding). Additionally, the constituents of each jet were pre-processed with translation,
rotation, and reﬂection as in the individual jet case. Training was carried out on a dataset
of 100,000 signal and background events with equal prior. Performance was evaluated on
an independent test set of 100,000 other events, as measured by the ROC AUC and R(cid:15)=80%
of the model predictions. Again, average scores are given with uncertainty estimates that
come from training 30 models with distinct initial random seeds.

In both jet-level and event-level experiments, the dimension of the embeddings q was
set to 40. Training was conducted using Adam [22] as an optimizer for 25 epochs, with
a batch size of 64 and a learning rate of 0.0005 decayed by a factor of 0.9 after every
epoch. These parameters were found to perform best on average, as determined through
an optimization of the hyper-parameters. Performance was monitored during training on
a validation set of 5000 samples to allow for early stopping and prevent from overﬁtting.

5 Experiments with Jet-Level Classiﬁcation

5.1 Performance studies

We carried out performance studies where we varied the following factors: the projection
of the 4-momenta into an image, the source of those 4-momenta, the topology of the RNN,
and the presence or absence of gating.

Impact of image projection The ﬁrst factor we studied was whether or not to project
the 4-momenta into an image as in Refs. [2, 6]. The architectures used in previous stud-
ies required a ﬁxed input (image) representation, and cannot be applied to the variable
length set of input 4-momenta. Conversely, we can apply the RNN architecture to the
discretized image 4-momenta. Table 1 shows that the RNN architecture based on a kt
topology performs almost as well as the MaxOut architecture in Ref. [6] when applied to
the image pre-processed 4-momenta coming from DELPHES towers. Importantly the RNN
architecture is much more data eﬃcient. While the MaxOut architecture in Ref. [6] has
975,693 parameters and was trained with 6M examples, the non-gated RNN architecture
has 8,481 parameters and was trained with 100,000 examples only.

– 7 –

Table 1. Summary of jet classiﬁcation performance for several approaches applied either to particle-
level inputs or towers from a DELPHES simulation.

Input

Architecture

ROC AUC

R(cid:15)=50%

towers
towers
towers

towers
towers
towers
towers
towers
towers
towers
towers
particles
particles
particles
particles
particles
particles

towers
towers
towers
towers
towers
towers
particles
particles
particles
particles
particles
particles

Projected into images

MaxOut
kt
kt (gated)

0.8418
0.8321 ± 0.0025
0.8277 ± 0.0028
Without image preprocessing
τ21
mass + τ21
kt
C/A
anti-kt
asc-pT
desc-pT
random
kt
C/A
anti-kt
asc-pT
desc-pT
random
With gating (see Appendix A)

0.7644
0.8212
0.8807 ± 0.0010
0.8831 ± 0.0010
0.8737 ± 0.0017
0.8835 ± 0.0009
0.8838 ± 0.0010
0.8704 ± 0.0011
0.9185 ± 0.0006
0.9192 ± 0.0008
0.9096 ± 0.0013
0.9130 ± 0.0031
0.9189 ± 0.0009
0.9121 ± 0.0008

–
12.7 ± 0.4
12.4 ± 0.3

6.79
11.31
24.1 ± 0.6
24.2 ± 0.7
22.3 ± 0.8
26.2 ± 0.7
25.1 ± 0.6
20.4 ± 0.3
68.3 ± 1.8
68.3 ± 3.6
51.7 ± 3.5
52.5 ± 7.3
70.4 ± 3.6
51.1 ± 2.0

kt
C/A
anti-kt
asc-pT
desc-pT
random
kt
C/A
anti-kt
asc-pT
desc-pT
random

25.4 ± 0.4
0.8822 ± 0.0006
26.2 ± 0.8
0.8861 ± 0.0014
24.4 ± 0.4
0.8804 ± 0.0010
0.8849 ± 0.0012
27.2 ± 0.8
0.8864 ± 0.0007 27.5 ± 0.6
22.8 ± 1.2
0.8751 ± 0.0029
74.3 ± 2.4
0.9195 ± 0.0009
81.8 ± 3.1
0.9222 ± 0.0007
68.3 ± 3.2
0.9156 ± 0.0012
54.8 ± 11.7
0.9137 ± 0.0046
83.3 ± 3.1
0.9212 ± 0.0005
50.7 ± 6.7
0.9106 ± 0.0035

Next, we compare the RNN classiﬁer based on a kt topology on tower 4-momenta with
and without image preprocessing. Table 1 and Fig. 3 show signiﬁcant gains in not using
jet images, improving ROC AUC from 0.8321 to 0.8807 (resp., R(cid:15)=50% from 12.7 to 24.1)
in the case of kt topologies. In addition, this result outperforms the MaxOut architecture

– 8 –

operating on images by a signiﬁcant margin. This suggests that the projection into an
image loses information and impacts classiﬁcation performance. We suspect the loss of
information to be due to some of the construction steps of jet images (i.e., pixelisation,
rotation, zooming, cropping and normalization). In particular, all are applied at the image-
level instead of being performed directly on the 4-momenta, which might induce artefacts
due to the lower resolution, particle superposition and aliasing. By contrast, the RNN is
able to work directly with the 4-momenta of a variable-length set of particles, without any
loss of information. For completeness, we also compare to the performance of a classiﬁer
based purely on the single n-subjettiness feature τ21 := τ2/τ1 and a classiﬁer based on two
features (the trimmed mass and τ21) [23]. In agreement with previous results based on
deep learning [2, 6], we see that our RNN classiﬁer clearly outperforms this variable.

Measurements of the 4-momenta The second factor we varied was the source of the
4-momenta. The towers scenario, corresponds to the case where the 4-momenta come
from the calorimeter simulation in DELPHES. While the calorimeter simulation is simplistic,
the granularity of the towers is quite large (10◦ in φ) and it does not take into account
that tracking detectors can provide very accurate momenta measurements for charged
particles that can be combined with calorimetry as in the particle ﬂow approach. Thus,
we also consider the particles scenario, which corresponds to an idealized case where the
4-momenta come from perfectly measured stable hadrons from PYTHIA. Table 1 and Fig. 3
show that further gains could be made with more accurate measurements of the 4-momenta,
improving e.g. ROC AUC from 0.8807 to 0.9185 (resp., R(cid:15)=50% from 24.1 to 68.3) in the case
of kt topologies. We also considered a case where the 4-momentum came from the DELPHES
particle ﬂow simulation and the data associated with each particle was augmented with
a particle-ﬂow identiﬁer distinguishing ± charged hadrons, photons, and neutral hadrons.
This is similar in motivation to Ref. [7], but we did not observe any signiﬁcant gains in
classiﬁcation performance with respect to the towers scenario.

Topology of the binary trees The third factor we studied was the topology of the
binary tree tj described in Sections 2 and 3.1 that dictates the recursive structure of the
RNN. We considered binary trees based on the anti-kt, Cambridge-Aachen (C/A), and
kt sequential recombination jet algorithms, along with random, asc-pT and desc-pT binary
trees. Table 1 and Fig. 4 show the performance of the RNN classiﬁer based on these various
topologies. Interestingly, the topology is signiﬁcant.

For instance, kt and C/A signiﬁcantly outperform the anti-kt topology on both tower
and particle inputs. This is consistent with intuition from previous jet substructure studies
where jets are typically reclustered with the kt algorithm. The fact that the topology is
important is further supported by the poor performance of the random binary tree topol-
ogy. We expected however that a simple sequence (represented as a degenerate binary tree)
based on ascending and descending pT ordering would not perform particularly well, par-
ticularly since the topology does not use any angular information. Surprisingly, the simple
descending pT ordering slightly outperforms the RNNs based on kt and C/A topologies.
The descending pT network has the highest pT 4-momenta near the root of the tree, which
we expect to be the most important. We suspect this is the reason that the descending

– 9 –

Figure 3. Jet classiﬁcation performance for various input representations of the RNN classiﬁer,
using kt topologies for the embedding. The plot shows that there is signiﬁcant improvement from
removing the image processing step and that signiﬁcant gains can be made with more accurate
measurements of the 4-momenta.

pT outperforms the ascending pT ordering on particles, but this is not supported by the
performance on towers. A similar observation was already made in the context of natural
languages [24–26], where tree-based models have at best only slightly outperformed sim-
pler sequence-based networks. While recursive networks appear as a principled choice, it
is conjectured that recurrent networks may in fact be able to discover and implicitly use
recursive compositional structure by themselves, without supervision.

Gating The last factor that we varied was whether or not to incorporate gating in the
RNN. Adding gating increases the number of parameters to 48,761, but this is still about
20 times smaller than the number of parameters in the MaxOut architectures used in
previous jet image studies. Table 1 shows the performance of the various RNN topologies
with gating. While results improve signiﬁcantly with gating, most notably in terms of
R(cid:15)=50%, the trends in terms of topologies remain unchanged.

Other variants Finally, we also considered a number of other variants. For example,
we jointly trained a classiﬁer with the concatenated embeddings obtained over kt and anti-
kt topologies, but saw no signiﬁcant performance gain. We also tested the performance
of recursive activations transferred across topologies. For instance, we used the recursive
activation learned with a kt topology when applied to an anti-kt topology and observed a
signiﬁcant loss in performance. We also considered particle and tower level inputs with an
additional trimming preprocessing step, which was used for the jet image studies, but we

– 10 –

Figure 4. Jet classiﬁcation performance of the RNN classiﬁer based on various network topologies
for the embedding (particles scenario). This plot shows that topology is signiﬁcant, as supported
by the fact that results for kt, C/A and desc-pT topologies improve over results for anti-kt, asc-pT
and random binary trees. Best results are achieved for C/A and desc-pT topologies, depending on
the metric considered.

saw a signiﬁcant loss in performance. While the trimming degraded classiﬁcation perfor-
mance, we did not evaluate the robustness to pileup that motivates trimming and other
jet grooming procedures.

5.2

Infrared and Collinear Safety Studies

In proposing variables to characterize substructure, physicists have been equally concerned
with classiﬁcation performance and the ability to ensure various theoretical properties
of those variables. In particular, initial work on jet algorithms focused on the Infrared-
Collinear (IRC) safe conditions:

• Infrared safety. The model is robust to augmenting e with additional particles

{vN +1, . . . , vN +K} with small transverse momentum.

• Collinear safety. The model is robust to a collinear splitting of a particle, which is
represented by replacing a particle vj ∈ e with two particles vj1 and vj2, such that
vj = vj1 + vj2 and vj1 · vj2 = ||vj1|| ||vj2|| − (cid:15).

The sequential recombination algorithms lead to an IRC-safe deﬁnition of jets, in the
sense that given the event e, the number of jets M and their 4-momenta v(tj) are IRC-safe.
An early motivation of this work is that basing the RNN topology on the sequential
recombination algorithms would provide an avenue to machine learning classiﬁers with some

– 11 –

theoretical guarantee of IRC safety. If one only wants to ensure robustness to only one soft
particle or one collinear split, this could be satisﬁed by simply running a single iteration
of the jet algorithm as a pre-processing step. However, it is diﬃcult to ensure a more
general notion of IRC safety on the embedding due to the non-linearities in the network.
Nevertheless, we can explicitly test the robustness of the embedding or the subsequent
classiﬁer to the addition of soft particles or collinear splits to the input 4-momenta.

Table 2 shows the results of a non-gated RNN trained on the nominal particle-level
input when applied to testing data with additional soft particles or collinear splits. The
collinear splits were uniform in the momentum fraction and maintained the small invariant
mass of the hadrons. We considered one or ten collinear splits on both random particles
and the highest pT particles. We see that while the 30 models trained with a descending pT
topology very slightly outperform the kt topology for almost scenarios, their performance
in terms of R(cid:15)=50% decreases relatively more rapidly when collinear splits are applied (see
e.g., the collinear10-max scenarios where the performance of kt decreases by 4%, while the
performance of pT decreases by 10%). This suggests a higher robustness towards collinear
splits for recursive networks based on kt topologies.

We also point out that the training of these networks is based solely on the classiﬁcation
loss for the nominal sample. If we are truly concerned with the IRC-safety considerations,
then it is natural to augment the training of the classiﬁers to be robust to these variations.
A number of modiﬁed training procedures exist, including e.g., the adversarial training
procedure described in Ref. [27].

6 Experiments with event-level classiﬁcation

As in the previous section, we carried out a number of performance studies. However, our
goal is mainly to demonstrate the relevance and scalability of the QCD-motivated approach
we propose, rather than making a statement about the physics reach of the signal process.
Results are discussed considering the idealized particles scenario, where the 4-momenta
come from perfectly measured stable hadrons from PYTHIA. Experiments for the towers
scenario (omitted here) reveal similar qualitative conclusions, though performance was
slightly worse for all models, as expected.

Number of jets The ﬁrst factor we varied was the maximum number of jets in the
sequence of embeddings given as input to the GRU. While the event-level embedding can
be computed over all the jets it is constituted by, QCD suggests that the 2 highest pT jets
hold most of the information to separate signal from background events, with only marginal
discriminating information left in the subsequent jets. As Table 3 and Fig. 5 show, there
is indeed signiﬁcant improvement in going from the hardest jet to the 2 hardest jets, while
there is no to little gain in considering more jets. Let us also emphasize that the event-level
models have only 18,681 parameters, and were trained on 100,000 training examples.

Topology of the binary trees The second factor we studied was the architecture of the
networks used for the inner embedding of the jets, for which we compare kt against descend-

– 12 –

Table 2. Performance of pre-trained RNN classiﬁers (without gating) applied to nominal and
modiﬁed particle inputs. The collinear1 (collinear10) scenarios correspond to applying collinear
splits to one (ten) random particles within the jet. The collinear1-max (collinear10-max) scenarios
correspond to applying collinear splits to the highest pT (ten highest pT ) particles in the jet. The
soft scenario corresponds to adding 200 particles with pT = 10−5 GeV uniformly in 0 < φ < 2π and
−5 < η < 5.

Scenario

Architecture

ROC AUC

nominal
nominal
collinear1
collinear1
collinear10
collinear10
collinear1-max
collinear1-max
collinear10-max
collinear10-max
soft
soft

kt
desc-pT
kt
desc-pT
kt
desc-pT
kt
desc-pT
kt
desc-pT
kt
desc-pT

0.9185 ± 0.0006
0.9189 ± 0.0009
0.9183 ± 0.0006
0.9188 ± 0.0010
0.9174 ± 0.0006
0.9178 ± 0.0011
0.9184 ± 0.0006
0.9191 ± 0.0010
0.9159 ± 0.0009
0.9140 ± 0.0016
0.9179 ± 0.0006
0.9188 ± 0.0009

R(cid:15)=50%
68.3 ± 1.8
70.4 ± 3.6
68.7 ± 2.0
70.7 ± 4.0
67.5 ± 2.6
67.9 ± 4.3
68.5 ± 2.8
72.4 ± 4.3
65.7 ± 2.7
63.5 ± 5.2
68.2 ± 2.3
70.2 ± 3.7

ing pT topologies. As in the previous section, best results are achieved with descending pT
topologies, though the diﬀerence is only marginal.

Other variants Finally, we also compare with baselines. With respect to an event-level
embedding computed only from the 4-momenta v(tj) (for j = 1, . . . , M ) of the jets, we
ﬁnd that augmenting the input to the GRU with jet-level embeddings yields signiﬁcant
improvement, e.g. improving ROC AUC from 0.9606 to 0.9875 (resp. R(cid:15)=80% from 21.1 to
174.5) when considering the 2 hardest jets case. This suggests that jet substructures are
important to separate signal from background events, and correctly learned when nesting
embeddings. Similarly, we observe that directly feeding the GRU with the 4-momenta
vi, for i = 1, . . . , N , of the constituents of the event performs signiﬁcantly worse. While
performance remains decent (e.g., with a ROC AUC of 0.8925 when feeding the 50 4-
momenta with largest pT ), this suggests that the recurrent network fails to leverage some
of the relevant information, which is otherwise easier to identify and learn when inputs
to the GRU come directly grouped as jets, themselves structured as trees. In contrast to
our previous results for jet-level experiments, this last comparison underlines the fact that
integrating domain knowledge by structuring the network topology is in some cases crucial
for performance.

Overall, this study shows that event embeddings inspired from QCD and produced
by nested recurrence, over the jets and over their constituents, is a promising avenue for
building eﬀective machine learning models. To our knowledge, this is the ﬁrst classiﬁer

– 13 –

Table 3. Summary of event classiﬁcation performance. Best results are achieved through nested
recurrence over the jets and over their constituents, as motivated by QCD.

Input

ROC AUC

R(cid:15)=80%

v(tj)
v(tj), hjet(kt)
v(tj), hjet(desc−pT )
j

j

v(tj)
v(tj), hjet(kt)
v(tj), hjet(desc−pT )
j

j

v(tj)
v(tj), hjet(kt)
v(tj), hjet(desc−pT )
j

j

Hardest jet
0.8909 ± 0.0007
0.9602 ± 0.0004

0.9594 ± 0.0010
2 hardest jets
0.9606 ± 0.0011
0.9866 ± 0.0007

5.6 ± 0.0
26.7 ± 0.7

25.6 ± 1.4

21.1 ± 1.1
156.9 ± 14.8

0.9875 ± 0.0006 174.5 ± 14.0
5 hardest jets
0.9576 ± 0.0019
0.9867 ± 0.0004

20.3 ± 0.9
152.8 ± 10.4

0.9872 ± 0.0003

167.8 ± 9.5

No jet clustering, desc-pT on vi

i = 1
i = 1, . . . , 50
i = 1, . . . , 100
i = 1, . . . , 200
i = 1, . . . , 400

0.6501 ± 0.0023
0.8925 ± 0.0079
0.8781 ± 0.0180
0.8846 ± 0.0091
0.8780 ± 0.0132

1.7 ± 0.0
5.6 ± 0.5
4.9 ± 0.6
5.2 ± 0.5
4.9 ± 0.5

operating at the hadron-level for all the particles in an event, in a way motivated in its
structure by QCD.

7 Related work

Neural networks in particle physics have a long history. They have been used in the
past for many tasks, including early work on quark-gluon discrimination [28, 29], particle
identiﬁcation [30], Higgs tagging [31] or track identiﬁcation [32]. In most of these, neural
networks appear as shallow multi-layer perceptrons where input features were designed by
experts to incorporate domain knowledge. More recently, the success of deep convolutional
networks has triggered a new body of work in jet physics, shifting the paradigm from
engineering input features to learning them automatically from raw data, e.g., as in these
works treating jets as images [1–8]. Our work builds instead upon an analogy between
QCD and natural languages, hence complementing the set of algorithms for jet physics
with techniques initially developed for natural language processing [9–14]. In addition, our
approach does not delegate the full modeling task to the machine. It allows to incorporate
domain knowledge in terms of the network architecture, speciﬁcally by structuring the
recursion stack for the embedding directly from QCD-inspired jet algorithms (see Sec. 3)

– 14 –

Figure 5.
Event classiﬁcation performance of the RNN classiﬁer when varying the maximum
number of jets given as input to the GRU. This plots shows there is signiﬁcant improvement from
going to the hardest to the 2 hardest jets, while there is no to little gain in considering more jets.

Between the time that this work appeared on the arXiv preprint server and submitted
for publication there has been a ﬂurry of activity connecting deep learning techniques and
jet physics (for reviews see Refs.[33–35]).
In particular the method described here was
also used for quark/gluon tagging in Ref. [36] and a variant of this method was used to
reconstruct a jet’s charge [37]. The authors of Ref. [38] used the tree structure deﬁned by
the jet clustering history to deﬁne a substructure ordering scheme for use with a sequential
recurrent neural network. Going the opposite direction, graph neural networks and mes-
sage passing neural networks have also been applied to the the same jet-level classiﬁcation
problem and data described in this work [39]. There has also been a spate of recent work on
using QCD-inspired variables, enforcing physical constraints into neural networks, and en-
suring infrared safety of neural network based approaches to jet physics [40–44]. Exploring
a complementary direction, several authors have developed ways to train machine learning
techniques using real data to avoid sensitivity to systematic eﬀects in the simulation [45–
48]. These recent training techniques are agnostic to the network architecture and can be
paired with the RNN architectures described here. Finally, deep learning techniques are
now being studied as generative models for jets, where the tree-based model mimics the
parton shower and can be trained on real data [49]. Learning generative modes for both
signal and background classes of jets can be used to deﬁne a classiﬁer in which each branch
of the tree can be interpreted as a contribution to a likelihood ratio discriminant [49].

– 15 –

8 Conclusions

Building upon an analogy between QCD and natural languages, we have presented in
this work a novel class of recursive neural networks for jet physics that are derived from
sequential recombination jet algorithms. Our experiments have revealed that preprocessing
steps applied to jet images during their construction (speciﬁcally the pixelisation) loses
information, which impacts classiﬁcation performance. By contrast, our recursive network
is able to work directly with the four-momenta of a variable-length set of particles, without
the loss of information due to discretization into pixels. Our experiments indicate that this
results in signiﬁcant gains in terms of accuracy and data eﬃciency with respect to previous
image-based networks. Finally, we also showed for the ﬁrst time a hierarchical, event-level
classiﬁcation model operating on all the hadrons of an event. Notably, our results showed
that incorporating domain knowledge derived from jet algorithms and encapsulated in
terms of the network architecture led to improved classiﬁcation performance.

While we initially expected recursive networks operating on jet recombination trees
to outperform simpler pT -ordered architectures, our results still clearly indicate that the
topology has an eﬀect on the ﬁnal performance of the classiﬁer. However, our initial
studies indicate that architectures based on jet trees are more robust to infrared radiation
and collinear splittings than the simpler pT -ordered architectures, which may outweigh
what at face value appears to be a small loss in performance. Accordingly, it would be
natural to include robustness to pileup, infrared radiation, and collinear splittings directly
in the training procedure [27]. Moreover, it is compelling to think of generalizations in
which the optimization would include the topology used for the embedding as learnable
component instead of considering it ﬁxed a priori. An immediate challenge of this approach
is that a discontinuous change in the topology (e.g., from varying α or R) makes the
loss non-diﬀerentiable and rules out standard back propagation optimization algorithms.
Nevertheless, solutions for learning composition orders have recently been proposed in
NLP, using either explicit supervision [50] or reinforcement learning [51]; both of which
could certainly be adapted to jet embeddings. Another promising generalization is to use
a graph-convolutional network that operates on a graph where the vertices correspond to
particle 4-momenta vi and the edge weights are given by dα
ii(cid:48) or a similar QCD-motivated
quantity [39, 52–58]. In conclusion, we feel conﬁdent that there is great potential in hybrid
techniques like this that incorporate physics knowledge and leverage the power of machine
learning.

Acknowledgments

We would like to thank the authors of Ref.[6] for sharing the data used in their studies and
Noel Dawe in particular for his responsiveness in clarifying details about their work. We
would also like to thank Joan Bruna for enlightening discussions about graph-convolutional
networks. Cranmer and Louppe are both supported through NSF ACI-1450310, addition-
ally Cranmer and Becot are supported through PHY-1505463 and PHY-1205376.

– 16 –

References

[1] J. Cogan, M. Kagan, E. Strauss and A. Schwarztman, Jet-Images: Computer Vision Inspired

Techniques for Jet Tagging, JHEP 02 (2015) 118 [1407.5675].

[2] L. de Oliveira, M. Kagan, L. Mackey, B. Nachman and A. Schwartzman, Jet-Images – Deep

Learning Edition, 1511.05190.

[3] L. G. Almeida, M. Backovi´c, M. Cliche, S. J. Lee and M. Perelstein, Playing Tag with ANN:
Boosted Top Identiﬁcation with Pattern Recognition, JHEP 07 (2015) 086 [1501.05968].

[4] P. Baldi, K. Bauer, C. Eng, P. Sadowski and D. Whiteson, Jet Substructure Classiﬁcation in

High-Energy Physics with Deep Neural Networks, 1603.09349.

[5] D. Guest, J. Collado, P. Baldi, S.-C. Hsu, G. Urban and D. Whiteson, Jet Flavor

Classiﬁcation in High-Energy Physics with Deep Neural Networks, Phys. Rev. D94 (2016)
112002 [1607.08633].

[6] J. Barnard, E. N. Dawe, M. J. Dolan and N. Rajcic, Parton Shower Uncertainties in Jet

Substructure Analyses with Deep Neural Networks, 1609.00607.

[7] P. T. Komiske, E. M. Metodiev and M. D. Schwartz, Deep learning in color: towards

automated quark/gluon jet discrimination, JHEP 01 (2017) 110 [1612.01551].

[8] G. Kasieczka, T. Plehn, M. Russell and T. Schell, Deep-learning Top Taggers or The End of

QCD?, 1701.08784.

[9] C. Goller and A. Kuchler, Learning task-dependent distributed representations by

backpropagation through structure, in Neural Networks, 1996., IEEE International
Conference on, vol. 1, pp. 347–352, IEEE, 1996.

[10] R. Socher, C. C. Lin, C. Manning and A. Y. Ng, Parsing natural scenes and natural language

with recursive neural networks, in Proceedings of the 28th international conference on
machine learning (ICML-11), pp. 129–136, 2011.

[11] R. Socher, J. Pennington, E. H. Huang, A. Y. Ng and C. D. Manning, Semi-supervised
recursive autoencoders for predicting sentiment distributions, in Proceedings of the
Conference on Empirical Methods in Natural Language Processing, pp. 151–161, Association
for Computational Linguistics, 2011.

[12] K. Cho, B. van Merri¨enboer, D. Bahdanau and Y. Bengio, On the properties of neural

machine translation: Encoder-decoder approaches, arXiv preprint arXiv:1409.1259 (2014) .

[13] K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk et al.,

Learning phrase representations using rnn encoder-decoder for statistical machine
translation, arXiv preprint arXiv:1406.1078 (2014) .

[14] X. Chen, X. Qiu, C. Zhu, S. Wu and X. Huang, Sentence modeling with gated recursive
neural network, in Proceedings of the 2015 Conference on Empirical Methods in Natural
Language Processing, pp. 793–798, 2015.

[15] I. Goodfellow, Y. Bengio and A. Courville, Deep Learning, ch. 10. MIT Press, 2016.

[16] M. Cacciari, G. P. Salam and G. Soyez, The Anti-k(t) jet clustering algorithm, JHEP 04

(2008) 063 [0802.1189].

[17] G. P. Salam, Towards Jetography, Eur. Phys. J. C67 (2010) 637 [0906.1833].

– 17 –

[18] V. Nair and G. E. Hinton, Rectiﬁed linear units improve restricted boltzmann machines, in

Proceedings of the 27th international conference on machine learning (ICML-10),
pp. 807–814, 2010.

[19] N. Fischer, S. Prestel, M. Ritzmann and P. Skands, Vincia for Hadron Colliders,

1605.06142.

[20] M. Ritzmann, D. A. Kosower and P. Skands, Antenna Showers with Hadronic Initial States,

Phys. Lett. B718 (2013) 1345 [1210.6345].

[21] J. Chung, C. Gulcehre, K. Cho and Y. Bengio, Empirical evaluation of gated recurrent neural

networks on sequence modeling, arXiv preprint arXiv:1412.3555 (2014) .

[22] D. Kingma and J. Ba, Adam: A method for stochastic optimization, arXiv preprint

[23] J. Thaler and K. Van Tilburg, Identifying Boosted Objects with N-subjettiness, JHEP 03

arXiv:1412.6980 (2014) .

(2011) 015 [1011.2268].

[24] S. R. Bowman, C. D. Manning and C. Potts, Tree-structured composition in neural networks

without tree-structured architectures, arXiv preprint arXiv:1506.04834 (2015) .

[25] S. R. Bowman, Modeling natural language semantics in learned representations, Ph.D. thesis,

[26] X. Shi, I. Padhi and K. Knight, Does string-based neural mt learn source syntax?, in Proc. of

STANFORD UNIVERSITY, 2016.

EMNLP, 2016.

preprint arXiv:1611.01046 (2016) .

Phys. Rev. Lett. 65 (1990) 1321.

Phys. B349 (1991) 675.

[27] G. Louppe, M. Kagan and K. Cranmer, Learning to Pivot with Adversarial Networks, arXiv

[28] L. Lonnblad, C. Peterson and T. Rognvaldsson, Finding Gluon Jets With a Neural Trigger,

[29] L. Lonnblad, C. Peterson and T. Rognvaldsson, Using neural networks to identify jets, Nucl.

[30] R. Sinkus and T. Voss, Particle identiﬁcation with neural networks using a rotational

invariant moment representation, Nucl. Instrum. Meth. A391 (1997) 360.

[31] P. Chiappetta, P. Colangelo, P. De Felice, G. Nardulli and G. Pasquariello, Higgs search by

neural networks at LHC, Phys. Lett. B322 (1994) 219 [hep-ph/9401343].

[32] B. H. Denby, Neural Networks and Cellular Automata in Experimental High-energy Physics,

Comput. Phys. Commun. 49 (1988) 429.

[33] A. J. Larkoski, I. Moult and B. Nachman, Jet Substructure at the Large Hadron Collider: A

Review of Recent Advances in Theory and Machine Learning, 1709.04464.

[34] D. Guest, K. Cranmer and D. Whiteson, Deep Learning and its Application to LHC Physics,

[35] M. Russell, Top quark physics in the Large Hadron Collider era, Ph.D. thesis, Glasgow U.,

[36] T. Cheng, Recursive Neural Networks in Quark/Gluon Tagging, Comput. Softw. Big Sci. 2

1806.11484.

2017. 1709.10508.

(2018) 3 [1711.02633].

[37] K. Fraser and M. D. Schwartz, Jet Charge and Machine Learning, 1803.08066.

– 18 –

[38] S. Egan, W. Fedorko, A. Lister, J. Pearkes and C. Gay, Long Short-Term Memory (LSTM)

networks with jet constituents for boosted top tagging at the LHC, 1711.09059.

[39] I. Henrion, K. Cranmer, J. Bruna, K. Cho, J. Brehmer, G. Louppe et al., Neural Message

Passing for Jet Physics, in Proceedings of the Deep Learning for Physical Sciences Workshop
at NIPS (2017), 2017, https://dl4physicalsciences.github.io/ﬁles/nips dlps 2017 29.pdf.

[40] A. Butter, G. Kasieczka, T. Plehn and M. Russell, Deep-learned Top Tagging with a Lorentz

[41] K. Datta and A. J. Larkoski, Novel Jet Observables from Machine Learning, JHEP 03

Layer, 1707.08966.

(2018) 086 [1710.01305].

[42] P. T. Komiske, E. M. Metodiev and J. Thaler, Energy ﬂow polynomials: A complete linear

basis for jet substructure, JHEP 04 (2018) 013 [1712.07124].

[43] S. H. Lim and M. M. Nojiri, Spectral Analysis of Jet Substructure with Neural Network:

[44] S. Choi, S. J. Lee and M. Perelstein, Infrared Safety of a Neural-Net Top Tagging Algorithm,

Boosted Higgs Case, 1807.03312.

1806.01263.

[45] E. M. Metodiev, B. Nachman and J. Thaler, Classiﬁcation without labels: Learning from

mixed samples in high energy physics, JHEP 10 (2017) 174 [1708.02949].

[46] P. T. Komiske, E. M. Metodiev, B. Nachman and M. D. Schwartz, Learning to Classify from

[47] J. H. Collins, K. Howe and B. Nachman, CWoLa Hunting: Extending the Bump Hunt with

Impure Samples, 1801.10158.

Machine Learning, 1805.02664.

[48] R. T. D’Agnolo and A. Wulzer, Learning New Physics from a Machine, 1806.02350.

[49] A. Andreassen, I. Feige, C. Frye and M. D. Schwartz, JUNIPR: a Framework for

Unsupervised Machine Learning in Particle Physics, 1804.09720.

[50] S. R. Bowman, J. Gauthier, A. Rastogi, R. Gupta, C. D. Manning and C. Potts, A fast
uniﬁed model for parsing and sentence understanding, arXiv preprint arXiv:1603.06021
(2016) .

[51] D. Yogatama, P. Blunsom, C. Dyer, E. Grefenstette and W. Ling, Learning to compose words

into sentences with reinforcement learning, arXiv preprint arXiv:1611.09100 (2016) .

[52] J. Bruna, W. Zaremba, A. Szlam and Y. LeCun, Spectral networks and locally connected

networks on graphs, CoRR abs/1312.6203 (2013) .

[53] M. Henaﬀ, J. Bruna and Y. LeCun, Deep convolutional networks on graph-structured data,

[54] Y. Li, D. Tarlow, M. Brockschmidt and R. S. Zemel, Gated graph sequence neural networks,

[55] M. Niepert, M. Ahmed and K. Kutzkov, Learning convolutional neural networks for graphs,

CoRR abs/1506.05163 (2015) .

CoRR abs/1511.05493 (2015) .

CoRR abs/1605.05273 (2016) .

[56] M. Deﬀerrard, X. Bresson and P. Vandergheynst, Convolutional neural networks on graphs

with fast localized spectral ﬁltering, CoRR abs/1606.09375 (2016) .

[57] T. N. Kipf and M. Welling, Semi-supervised classiﬁcation with graph convolutional networks,

arXiv preprint arXiv:1609.02907 (2016) .

– 19 –

[58] T. N. Kipf and M. Welling, Semi-supervised classiﬁcation with graph convolutional networks,

CoRR abs/1609.02907 (2016) .

[59] D. Maclaurin, D. Duvenaud, M. Johnson and R. P. Adams, “Autograd: Reverse-mode

diﬀerentiation of native Python.” http://github.com/HIPS/autograd, 2015.

A Gated recursive jet embedding

The recursive activation proposed in Sec. 3.1 suﬀers from two critical issues. First, it
assumes that left-child, right-child and local node information hjet
, uk are all equally
kL
relevant for computing the new activation, while only some of this information may be
needed and selected. Second, it forces information to pass through several levels of non-
linearities and does not allow to propagate unchanged from leaves to root. Addressing these
issues and generalizing from [12–14], we recursively deﬁne a recursive activation equipped
with reset and update gates as follows:

, hjet
kR

hjet

k =

k + zL (cid:12) hjet
kL

+ otherwise

uk
zH (cid:12) ˜hjet
(cid:44)→ zR (cid:12) hjet
kR

+ zN (cid:12) uk

if k is a leaf





(cid:40)

uk = σ (Wug(ok) + bu)

ok =

vi(k)
okL + okR


if k is a leaf

otherwise




˜hjet

k = σ


W˜h


 + b˜h





rL (cid:12) hjet
kL
rR (cid:12) hjet


kR
rN (cid:12) uk



= softmax

Wz

+ bz

































˜hjet
k
hjet
kL
hjet
kR
uk
hjet
kL
hjet
kR
uk


 = sigmoid


Wr





 + br

























zH
zL
zR
zN

rL
rR
rN

(A.1)

(A.2)

(A.3)

(A.4)

(A.5)

(A.6)

where W˜h ∈ Rq×3q, b˜h ∈ Rq, Wz ∈ Rq×4q, bz ∈ Rq, Wr ∈ Rq×3q, br ∈ Rq, Wu ∈ Rq×4
and bu ∈ Rq form together the shared parameters to be learned, σ is the ReLU activation
function and (cid:12) denotes the element-wise multiplication.

Intuitively, the reset gates rL, rR and rN control how to actively select and then
and the local node

merge the left-child embedding hjet
kL
information uk to form a new candidate activation ˜hjet
can
then be regarded as a choice among the candidate activation, the left-child embedding, the
right-child embedding and the local node information, as controlled by the update gates

, the right-child embedding hjet
kR

k . The ﬁnal embedding hjet

k

– 20 –

zH , zL, zR and zN . Finally, let us note that the proposed gated recursive embedding is
a generalization of Section 3.1, in the sense that the later corresponds to the case where
update gates are set to zH = 1, zL = 0, zR = 0 and zN = 0 and reset gates to rL = 1,
rR = 1 and rN = 1 for all nodes k.

B Gated recurrent event embedding

In this section, we formally deﬁne the gated recurrent event embedding introduced in
Sec. 3.2. Our event embedding function is a GRU [21] operating on the pT ordered sequence
of pairs (v(tj), hjet
1 (tj)), for j = 1, . . . , M , where v(tj) is the unprocessed 4-momentum
(φ, η, pT , m) of the jet tj and hjet
1 (tj) is its embedding. Its ﬁnal output hevent
j=M is recursively
deﬁned as follows:

hevent
j
˜hevent
j

j−1 + (1 − zj) (cid:12) ˜hevent

= zj (cid:12) hevent
= σ (cid:0)Whxxj + Whh(rj (cid:12) hevent

j

j−1 ) + bh

(cid:1)

xj =

(cid:34)

(cid:35)

v(tj)
hjet
1 (tj)

(B.1)

(B.2)

(B.3)

(cid:1)

zj = sigmoid (cid:0)Wzxxj + Wzhhevent
rj = sigmoid (cid:0)Wrxxj + Wrhhevent
where Whx ∈ Rr×4+q, Whh ∈ Rr×r, bh ∈ Rr, Wrx ∈ Rr×4+q, Wrh ∈ Rr×r, br ∈ Rr,
Wzx ∈ Rr×4+q, WzhRr×r and bz ∈ Rr are the parameters of the embedding function,
r is the size of the embedding, σ is the ReLU activation function, and hevent
In
the experiments of Sec. 6, only the 1, 2 or 5 hardest jets are considered in the sequence
j = 1, . . . , M , as ordered by ascending values of pT .

j−1 + bz
j−1 + br

(B.4)

(B.5)

= 0.

(cid:1)

0

C Implementation details

While tree-structured networks appear to be a principled choice in natural language pro-
cessing, they often have been overlooked in favor of sequence-based networks on the account
of their technical incompatibility with batch computation [50]. Because tree-structured
networks use a diﬀerent topology for each example, batching is indeed often impossible
in standard implementations, which prevents them from being trained eﬃciently on large
datasets. For this reason, the recursive jet embedding we introduced in Sec. 3.1 would
undergo the same technical issues if not implemented with caution.

In our implementation, we achieve batch computation by noticing that activations
from a same level in a recursive binary tree can be performed all at once, provided all nec-
essary computations from the deeper levels have already been performed. This principle
extends to the synchronized computation across multiple trees, which enables the batch
computation of our jet embeddings across many events. More speciﬁcally, the computation
of jet embeddings is preceded by a traversal of the recursion trees for all jets in the batch,
and whose purpose is to unroll and regroup computations by their level of recursion. Em-
beddings are then reconstructed level-wise in batch, in a bottom-up fashion, starting from
the deepest level of recursion across all trees.

– 21 –

Finally, learning is carried out through gradients obtained by the automatic diﬀer-
entiation of the full model chain on a batch of events (i.e., the recursive computation of
jet embeddings, the sequence-based recurrence to form the event embeddings, and the for-
ward pass through the classiﬁer). The implementation is written in native Python code and
makes use of Autograd [59] for the easy derivation of the gradients over dynamic structures.
Code is available at 2 under BSD license for further technical details.

2https://github.com/glouppe/recnn

– 22 –

Prepared for submission to JHEP

QCD-Aware Recursive Neural Networks for Jet Physics

Gilles Louppea,b,1 Kyunghyun Chob Cyril Becota,2 Kyle Cranmera,b

aNew York University, Center for Cosmology & Particle Physics, 726 Broadway, New York, NY
bNew York University, Center for Data Science, 60 5th Ave., New York, NY

E-mail: g.louppe@uliege.be, kyunghyun.cho@nyu.edu,
cyril.becot@cern.ch, kyle.cranmer@nyu.edu

Abstract: Recent progress in applying machine learning for jet physics has been built
upon an analogy between calorimeters and images. In this work, we present a novel class
of recursive neural networks built instead upon an analogy between QCD and natural
languages.
In the analogy, four-momenta are like words and the clustering history of
sequential recombination jet algorithms is like the parsing of a sentence. Our approach
works directly with the four-momenta of a variable-length set of particles, and the jet-based
tree structure varies on an event-by-event basis. Our experiments highlight the ﬂexibility of
our method for building task-speciﬁc jet embeddings and show that recursive architectures
are signiﬁcantly more accurate and data eﬃcient than previous image-based networks. We
extend the analogy from individual jets (sentences) to full events (paragraphs), and show
for the ﬁrst time an event-level classiﬁer operating on all the stable particles produced in
an LHC event.

ArXiv ePrint: 1702.00748

8
1
0
2
 
l
u
J
 
3
1
 
 
]
h
p
-
p
e
h
[
 
 
2
v
8
4
7
0
0
.
2
0
7
1
:
v
i
X
r
a

1Currently at University of Li`ege
2Currently at DESY

Contents

1 Introduction

2 Problem statement

3 Recursive embedding

Individual jets

3.1
3.2 Full events

4 Data, Preprocessing and Experimental Setup

5 Experiments with Jet-Level Classiﬁcation

5.1 Performance studies
5.2

Infrared and Collinear Safety Studies

6 Experiments with event-level classiﬁcation

7 Related work

8 Conclusions

A Gated recursive jet embedding

B Gated recurrent event embedding

C Implementation details

1

3

3
3
5

6

7
7
11

12

14

16

20

21

21

1 Introduction

By far the most common structures seen in collisions at the Large Hadron Collider (LHC)
are collimated sprays of energetic hadrons referred to as ‘jets’. These jets are produced
from the fragmentation and hadronization of quarks and gluons as described by quantum
chromodynamics (QCD). Several goals for the LHC are centered around the treatment of
jets, and there has been an enormous amount of eﬀort from both the theoretical and ex-
perimental communities to develop techniques that are able to cope with the experimental
realities while maintaining precise theoretical properties. In particular, the communities
have converged on sequential recombination jet algorithms, methods to study jet substruc-
ture, and grooming techniques to provide robustness to pileup.

One compelling physics challenge is to search for highly boosted standard model par-
ticles decaying hadronically. For instance, if a hadronically decaying W boson is highly

– 1 –

boosted, then its decay products will merge into a single fat jet with a characteristic sub-
structure. Unfortunately, there is a large background from jets produced by more mundane
QCD processes. For this reason, several jet ‘taggers’ and variables sensitive to jet substruc-
ture have been proposed. Initially, this work was dominated by techniques inspired by our
intuition and knowledge of QCD; however, more recently there has been a wave of ap-
proaches that eschew this expert knowledge in favor of machine learning techniques. In
this paper, we present a hybrid approach that leverages the structure of sequential recom-
bination jet algorithms and deep neural networks.

Recent progress in applying machine learning techniques for jet physics has been built
upon an analogy between calorimeters and images [1–8]. These methods take a variable-
length set of 4-momenta and project them into a ﬁxed grid of η − φ towers or ‘pixels’ to
produce a ‘jet image’. The original jet classiﬁcation problem, hence, reduces to an image
classiﬁcation problem, lending itself to deep convolutional networks and other machine
learning algorithms. Despite their promising results, these models suﬀer from the fact that
they have many free parameters and that they require large amounts of data for training.
More importantly, the projection of jets into images also loses information, which impacts
classiﬁcation performance. The most obvious way to address this issue is to use a recurrent
neural network to process a sequence of 4-momenta as they are. However, it is not clear
how to order this sequence. While pT ordering is common in many contexts [5], it does
not capture important angular information critical for understanding the subtle structure
of jets.

In this work, we propose instead a solution for jet classiﬁcation based on an analogy
between QCD and natural languages, as inspired by several works from natural language
processing [9–14]. Much like a sentence is composed of words following a syntactic struc-
ture organized as a parse tree, a jet is also composed of 4-momenta following a structure
dictated by QCD and organized via the clustering history of a sequential recombination jet
algorithm. More speciﬁcally, our approach uses ‘recursive’ networks where the topology of
the network is given by the clustering history of a sequential recombination jet algorithm,
which varies on an event-by-event basis. This event-by-event adaptive structure can be
contrasted with the ‘recurrent’ networks that operate purely on sequences (see e.g., [15]).
The network is therefore given the 4-momenta without any loss of information, in a way
that also captures substructures, as motivated by physical theory.

It is convenient to think of the recursive neural network as providing a ‘jet embedding’,
which maps a set of 4-momenta into Rq. This embedding has ﬁxed length and can be fed
into a subsequent network used for classiﬁcation or regression. Thus the procedure can be
used for jet tagging or estimating parameters that characterize the jet, such as the masses of
resonances buried inside the jet. Importantly, the embedding and the subsequent network
can be trained jointly so that the embedding is optimized for the task at hand.

Extending the natural language analogy paragraphs of text are sequence of sentences,
In particular, we propose to embed the full particle
just as event are sequence of jets.
content of an event by feeding a sequence of jet-embeddings into a recurrent network. As
before, this event-level embedding can be fed into a subsequent network used for classiﬁ-
cation or regression. To our knowledge, this represents the ﬁrst machine learning model

– 2 –

operating on all the detectable particles in an event.

The remainder of the paper is structured as follows.

In Sec. 2, we formalize the
classiﬁcation tasks at the jet-level and event-level. We describe the proposed recursive
network architectures in Sec. 3 and detail the data samples and preprocessing used in
our experiments in Sec. 4. Our results are summarized and discussed ﬁrst in Sec. 5 for
experiments on a jet-level classiﬁcation problem, and then in Sec. 6 for experiments on
an event-level classiﬁcation problem. In Sec. 7, we relate our work to close contributions
from deep learning, natural language processing, and jet physics. Finally, we gather our
conclusions and directions for further works in Sec. 8.

2 Problem statement

We describe a collision event e ∈ E as being composed of a varying number of particles,
indexed by i, and where each particle is represented by its 4-momentum vector vi ∈ R4,
such that e = {vi|i = 1, . . . , N }.

The 4-momenta in each event can be clustered into jets with a sequential recombination
jet algorithm that recursively combines (by simply adding their 4-momenta) the pair i, i(cid:48)
that minimize

ii(cid:48) = min(p2α
dα

ti , p2α
ti(cid:48) )

∆R2
ii(cid:48)
R2

(2.1)

ti , p2α

ii(cid:48) is less than min(p2α

while dα
ti(cid:48) ) [16, 17]. These sequential recombination algorithms
have three hyper-parameters: R, pt,min, α, and jets with pt < pt,min are discarded. At that
point, the jet algorithm has clustered e into M jets, each of which can be represented by
a binary tree tj ∈ T indexed by j = 1, . . . , M with Nj leaves (corresponding to a subset
of the vi). In the following, we will consider the speciﬁc cases where α = 1, 0, −1, which
respectively correspond to the kt, Cambridge-Aachen and anti-kt algorithms.

In addition to jet algorithms, we consider a ‘random’ baseline that corresponds to
recombining particles at random to form random binary trees tj, along with ‘asc-pT ’ and
‘desc-pT ’ baselines, which correspond to degenerate binary trees formed from the sequences
of particles sorted respectively in ascending and descending order of pT .

For jet-level classiﬁcation or regression, each jet tj ∈ T in the training data comes
with labels or regression values yj ∈ Y jet. In this framework, our goal is to build a pre-
dictive model f jet : T (cid:55)→ Y jet minimizing some loss function Ljet. Similarly, for event-level
classiﬁcation or regression, we assume that each collision event el ∈ E in the training data
comes with labels or regression values yl ∈ Y event, and our goal is to build a predictive
model f event : E (cid:55)→ Y event minimizing some loss function Levent.

3 Recursive embedding

3.1

Individual jets

Let us ﬁrst consider the case of an individual jet whose particles are topologically structured
as a binary tree tj, e.g., based on a sequential recombination jet clustering algorithm or a
simple sequential sorting in pT . Let k = 1, . . . , 2Nj − 1 indexes the node of the binary tree

– 3 –

tj, and let the left and right children of node k be denoted by kL and kR respectively. Let
also kL always be the hardest child of k. By construction, we suppose that leaves k map
to particles i(k) while internal nodes correspond to recombinations. Using these notations,
we recursively deﬁne the embedding hjet

k ∈ Rq of node k in tj as

f jet(tj)

...

r
e
ﬁ
i
s
s
a
C

l

g
n
i
d
d
e
b
m
e

t
e
J

hjet

1 (tj)

hjet
k

hjet
kL

hjet
kR

v1 v2 ...

vNj

Figure 1. QCD-motivated recursive jet
embedding for classiﬁcation. For each
individual jet, the embedding hjet
1 (tj) is
computed recursively from the root node
down to the outer nodes of the binary tree
tj. The resulting embedding is chained
to a subsequent classiﬁer, as illustrated in
the top part of the ﬁgure. The topology of
the network in the bottom part is distinct
for each jet and is determined by a sequen-
tial recombination jet algorithm (e.g., kt
clustering).






(cid:40)

if k is a leaf

hjet

k =

uk







σ

Wh













hjet
kL
hjet
kR
uk







+ bh

otherwise

uk = σ (Wug(ok) + bu)

ok =

vi(k)
okL + okR

if k is a leaf

otherwise

(3.1)

(3.2)

(3.3)

where Wh ∈ Rq×3q, bh ∈ Rq, Wu ∈ Rq×4 and
bu ∈ Rq form together the shared parameters to
be learned, q is the size of the embedding, σ is the
ReLU activation function [18], and g is a function
extracting the kinematic features p, η, θ, φ, E, and
pT from the 4-momentum ok.

When applying Eqn. 3.1 recursively from the
root node k = 1 down to the outer nodes of
the binary tree tj, the resulting embedding, de-
noted hjet
1 (tj), eﬀectively summarizes the informa-
tion contained in the particles forming the jet into
a single vector. In particular, this recursive neural
network (RNN) embeds a binary tree of varying
shape and size into a vector of ﬁxed size. As a
result, the embedding hjet
1 (tj) can now be chained
to a subsequent classiﬁer or regressor to solve our
target supervised learning problem, as illustrated
in Figure 1. All parameters (i.e., of the recursive
jet embedding and of the classiﬁer) are learned
jointly using backpropagation through structure
[9] to minimize the loss Ljet, hence tailoring the
embedding to the speciﬁc requirements of the task.
Further implementation details, including an ef-
ﬁcient batched computation over distinct binary
trees, can be found in Appendix C.

– 4 –

Event embedding

Classiﬁer

v(t1)

v(t2)

v(tM )

hevent

M (e)

...

f event(e)

hjet

1 (t1)

hjet

1 (t2)

hjet

1 (tM )

...

...

Figure 2. QCD-motivated event embedding for classiﬁcation. The embedding of an event is
computed by feeding the sequence of pairs (v(tj), hjet
1 (tj)) over the jets it is made of, where v(tj) is
the unprocessed 4-momentum of the jet tj and hjet
1 (tj) is its embedding. The resulting event-level
embedding hevent
M (e) is chained to a subsequent classiﬁer, as illustrated in the right part of the
ﬁgure.

In addition to the recursive activation of Eqn. 3.1, we also consider and study its
extended version equipped with reset and update gates (see details in Appendix A). This
gated architecture allows the network to preferentially pass information along the left-child,
right-child, or their combination.

While we have not performed experiments, we point out that there is an analogous

style of architectures based on jet algorithms with 2 → 3 recombinations [17, 19, 20].

3.2 Full events

We now embed entire events e of variable size by feeding the embeddings of their individual
jets to an event-level sequence-based recurrent neural network.

As an illustrative example, we consider here a gated recurrent unit [21] (GRU) oper-
ating on the pT ordered sequence of pairs (v(tj), hjet
1 (tj)), for j = 1, . . . , M , where v(tj) is
the unprocessed 4-momentum of the jet tj and hjet
1 (tj) is its embedding. The ﬁnal output
hevent
M (e) (see Appendix B for details) of the GRU is chained to a subsequent classiﬁer to
solve an event-level classiﬁcation task. Again, all parameters (i.e., of the inner jet embed-
ding function, of the GRU, and of the classiﬁer) are learned jointly using backpropagation
through structure [9] to minimize the loss Levent. Figure 2 provides a schematic of the
full classiﬁcation model. In summary, combining two levels of recurrence provides a QCD-
motivated event-level embedding that eﬀectively operates at the hadron-level for all the
particles in the event.

– 5 –

In addition and for the purpose of comparison, we also consider the simpler baselines
where i) only the 4-momenta v(tj) of the jets are given as input to the GRU, without
augmentation with their embeddings, and ii) the 4-momenta vi of the constituents of
the event are all directly given as input to the GRU, without grouping them into jets or
providing the jet embeddings.

4 Data, Preprocessing and Experimental Setup

In order to focus attention on the impact of the network architectures and the projection
of input 4-momenta into images, we consider the same boosted W tagging example as used
in Refs. [1, 2, 4, 6]. The signal (y = 1) corresponds to a hadronically decaying W boson
with 200 < pT < 500 GeV, while the background (y = 0) corresponds to a QCD jet with
the same range of pT .

We are grateful to the authors of Ref. [6] for sharing the data used in their studies.
We obtained both the full-event records from their PYTHIA benchmark samples, including
both the particle-level data and the towers from the DELPHES detector simulation.
In
addition, we obtained the fully processed jet images of 25×25 pixels, which include the
initial R = 1 anti-kt jet clustering and subsequent trimming, translation, pixelisation,
rotation, reﬂection, cropping, and normalization preprocessing stages detailed in Ref. [2, 6].
Our training data was collected by sampling from the original data a total of 100,000
signal and background jets with equal prior. The testing data was assembled similarly by
sampling 100,000 signal and background jets, without overlap with the training data. For
direct comparison with Ref. [6], performance is evaluated at test time within the restricted
window of 250 < pT < 300 and 50 ≤ m ≤ 110, where the signal and background jets
are re-weighted to produce ﬂat pT distributions. Results are reported in terms of the
area under the ROC curve (ROC AUC) and of background rejection (i.e., 1/FPR) at
50% signal eﬃciency (R(cid:15)=50%). Average scores reported include uncertainty estimates that
come from training 30 models with distinct initial random seeds. About 2% of the models
had technical problems during training (e.g., due to numerical errors), so we applied a
simple algorithm to ensure robustness: we discarded models whose R(cid:15)=50% was outside of
3 standard deviations of the mean, where the mean and standard deviation were estimated
excluding the ﬁve best and worst performing models.

For our jet-level experiments we consider as input to the classiﬁers the 4-momenta vi
from both the particle-level data and the DELPHES towers. We also compare the performance
with and without the projection of those 4-momenta into images. While the image data
already included the full pre-processing steps, when considering particle-level and tower
inputs we performed the initial R = 1 anti-kt jet clustering to identify the constituents
of the highest pT jet t1 of each event, and then performed the subsequent translation,
rotation, and reﬂection pre-processing steps (omitting cropping and normalization). When
processing the image data, we inverted the normalization that enforced the sum of the
squares of the pixel intensities be equal to one.1

1In Ref. [2], the jet images did not include the DELPHES detector simulation, they were comparable to

our particle scenario with the additional discretization into pixels.

– 6 –

For our event-level experiments we were not able to use the data from Ref. [6] because
the signal sample corresponded to pp → W (→ J)Z(→ ν ¯ν) and the background to pp →
jj. Thus the signal was characterized by one high-pT jet and large missing energy from
Z(→ ν ¯ν) which is trivially separated from the dijet background. For this reason, we
generated our own PYTHIA and DELPHES samples of pp → W (cid:48) → W (→ J)Z(→ J) and
QCD background such that both the signal and background have two high-pT jets. We
use mW (cid:48) = 700 GeV and restrict ˆpt of the 2 → 2 scattering process to 300 < ˆpt < 350
GeV. Our focus is to demonstrate the scalability of our method to all the particles or
towers in an event, and not to provide a precise statement about physics reach for this
signal process. In this case each event e was clustered by the same anti-kt algorithm with
R = 1, and then the constituents of each jet were treated as in Sec. 3.1 (i.e., reclustered
using kt or a sequential ordering in pT to provide the network topology for a non-gated
embedding). Additionally, the constituents of each jet were pre-processed with translation,
rotation, and reﬂection as in the individual jet case. Training was carried out on a dataset
of 100,000 signal and background events with equal prior. Performance was evaluated on
an independent test set of 100,000 other events, as measured by the ROC AUC and R(cid:15)=80%
of the model predictions. Again, average scores are given with uncertainty estimates that
come from training 30 models with distinct initial random seeds.

In both jet-level and event-level experiments, the dimension of the embeddings q was
set to 40. Training was conducted using Adam [22] as an optimizer for 25 epochs, with
a batch size of 64 and a learning rate of 0.0005 decayed by a factor of 0.9 after every
epoch. These parameters were found to perform best on average, as determined through
an optimization of the hyper-parameters. Performance was monitored during training on
a validation set of 5000 samples to allow for early stopping and prevent from overﬁtting.

5 Experiments with Jet-Level Classiﬁcation

5.1 Performance studies

We carried out performance studies where we varied the following factors: the projection
of the 4-momenta into an image, the source of those 4-momenta, the topology of the RNN,
and the presence or absence of gating.

Impact of image projection The ﬁrst factor we studied was whether or not to project
the 4-momenta into an image as in Refs. [2, 6]. The architectures used in previous stud-
ies required a ﬁxed input (image) representation, and cannot be applied to the variable
length set of input 4-momenta. Conversely, we can apply the RNN architecture to the
discretized image 4-momenta. Table 1 shows that the RNN architecture based on a kt
topology performs almost as well as the MaxOut architecture in Ref. [6] when applied to
the image pre-processed 4-momenta coming from DELPHES towers. Importantly the RNN
architecture is much more data eﬃcient. While the MaxOut architecture in Ref. [6] has
975,693 parameters and was trained with 6M examples, the non-gated RNN architecture
has 8,481 parameters and was trained with 100,000 examples only.

– 7 –

Table 1. Summary of jet classiﬁcation performance for several approaches applied either to particle-
level inputs or towers from a DELPHES simulation.

Input

Architecture

ROC AUC

R(cid:15)=50%

towers
towers
towers

towers
towers
towers
towers
towers
towers
towers
towers
particles
particles
particles
particles
particles
particles

towers
towers
towers
towers
towers
towers
particles
particles
particles
particles
particles
particles

Projected into images

MaxOut
kt
kt (gated)

0.8418
0.8321 ± 0.0025
0.8277 ± 0.0028
Without image preprocessing
τ21
mass + τ21
kt
C/A
anti-kt
asc-pT
desc-pT
random
kt
C/A
anti-kt
asc-pT
desc-pT
random
With gating (see Appendix A)

0.7644
0.8212
0.8807 ± 0.0010
0.8831 ± 0.0010
0.8737 ± 0.0017
0.8835 ± 0.0009
0.8838 ± 0.0010
0.8704 ± 0.0011
0.9185 ± 0.0006
0.9192 ± 0.0008
0.9096 ± 0.0013
0.9130 ± 0.0031
0.9189 ± 0.0009
0.9121 ± 0.0008

–
12.7 ± 0.4
12.4 ± 0.3

6.79
11.31
24.1 ± 0.6
24.2 ± 0.7
22.3 ± 0.8
26.2 ± 0.7
25.1 ± 0.6
20.4 ± 0.3
68.3 ± 1.8
68.3 ± 3.6
51.7 ± 3.5
52.5 ± 7.3
70.4 ± 3.6
51.1 ± 2.0

kt
C/A
anti-kt
asc-pT
desc-pT
random
kt
C/A
anti-kt
asc-pT
desc-pT
random

25.4 ± 0.4
0.8822 ± 0.0006
26.2 ± 0.8
0.8861 ± 0.0014
24.4 ± 0.4
0.8804 ± 0.0010
0.8849 ± 0.0012
27.2 ± 0.8
0.8864 ± 0.0007 27.5 ± 0.6
22.8 ± 1.2
0.8751 ± 0.0029
74.3 ± 2.4
0.9195 ± 0.0009
81.8 ± 3.1
0.9222 ± 0.0007
68.3 ± 3.2
0.9156 ± 0.0012
54.8 ± 11.7
0.9137 ± 0.0046
83.3 ± 3.1
0.9212 ± 0.0005
50.7 ± 6.7
0.9106 ± 0.0035

Next, we compare the RNN classiﬁer based on a kt topology on tower 4-momenta with
and without image preprocessing. Table 1 and Fig. 3 show signiﬁcant gains in not using
jet images, improving ROC AUC from 0.8321 to 0.8807 (resp., R(cid:15)=50% from 12.7 to 24.1)
in the case of kt topologies. In addition, this result outperforms the MaxOut architecture

– 8 –

operating on images by a signiﬁcant margin. This suggests that the projection into an
image loses information and impacts classiﬁcation performance. We suspect the loss of
information to be due to some of the construction steps of jet images (i.e., pixelisation,
rotation, zooming, cropping and normalization). In particular, all are applied at the image-
level instead of being performed directly on the 4-momenta, which might induce artefacts
due to the lower resolution, particle superposition and aliasing. By contrast, the RNN is
able to work directly with the 4-momenta of a variable-length set of particles, without any
loss of information. For completeness, we also compare to the performance of a classiﬁer
based purely on the single n-subjettiness feature τ21 := τ2/τ1 and a classiﬁer based on two
features (the trimmed mass and τ21) [23]. In agreement with previous results based on
deep learning [2, 6], we see that our RNN classiﬁer clearly outperforms this variable.

Measurements of the 4-momenta The second factor we varied was the source of the
4-momenta. The towers scenario, corresponds to the case where the 4-momenta come
from the calorimeter simulation in DELPHES. While the calorimeter simulation is simplistic,
the granularity of the towers is quite large (10◦ in φ) and it does not take into account
that tracking detectors can provide very accurate momenta measurements for charged
particles that can be combined with calorimetry as in the particle ﬂow approach. Thus,
we also consider the particles scenario, which corresponds to an idealized case where the
4-momenta come from perfectly measured stable hadrons from PYTHIA. Table 1 and Fig. 3
show that further gains could be made with more accurate measurements of the 4-momenta,
improving e.g. ROC AUC from 0.8807 to 0.9185 (resp., R(cid:15)=50% from 24.1 to 68.3) in the case
of kt topologies. We also considered a case where the 4-momentum came from the DELPHES
particle ﬂow simulation and the data associated with each particle was augmented with
a particle-ﬂow identiﬁer distinguishing ± charged hadrons, photons, and neutral hadrons.
This is similar in motivation to Ref. [7], but we did not observe any signiﬁcant gains in
classiﬁcation performance with respect to the towers scenario.

Topology of the binary trees The third factor we studied was the topology of the
binary tree tj described in Sections 2 and 3.1 that dictates the recursive structure of the
RNN. We considered binary trees based on the anti-kt, Cambridge-Aachen (C/A), and
kt sequential recombination jet algorithms, along with random, asc-pT and desc-pT binary
trees. Table 1 and Fig. 4 show the performance of the RNN classiﬁer based on these various
topologies. Interestingly, the topology is signiﬁcant.

For instance, kt and C/A signiﬁcantly outperform the anti-kt topology on both tower
and particle inputs. This is consistent with intuition from previous jet substructure studies
where jets are typically reclustered with the kt algorithm. The fact that the topology is
important is further supported by the poor performance of the random binary tree topol-
ogy. We expected however that a simple sequence (represented as a degenerate binary tree)
based on ascending and descending pT ordering would not perform particularly well, par-
ticularly since the topology does not use any angular information. Surprisingly, the simple
descending pT ordering slightly outperforms the RNNs based on kt and C/A topologies.
The descending pT network has the highest pT 4-momenta near the root of the tree, which
we expect to be the most important. We suspect this is the reason that the descending

– 9 –

Figure 3. Jet classiﬁcation performance for various input representations of the RNN classiﬁer,
using kt topologies for the embedding. The plot shows that there is signiﬁcant improvement from
removing the image processing step and that signiﬁcant gains can be made with more accurate
measurements of the 4-momenta.

pT outperforms the ascending pT ordering on particles, but this is not supported by the
performance on towers. A similar observation was already made in the context of natural
languages [24–26], where tree-based models have at best only slightly outperformed sim-
pler sequence-based networks. While recursive networks appear as a principled choice, it
is conjectured that recurrent networks may in fact be able to discover and implicitly use
recursive compositional structure by themselves, without supervision.

Gating The last factor that we varied was whether or not to incorporate gating in the
RNN. Adding gating increases the number of parameters to 48,761, but this is still about
20 times smaller than the number of parameters in the MaxOut architectures used in
previous jet image studies. Table 1 shows the performance of the various RNN topologies
with gating. While results improve signiﬁcantly with gating, most notably in terms of
R(cid:15)=50%, the trends in terms of topologies remain unchanged.

Other variants Finally, we also considered a number of other variants. For example,
we jointly trained a classiﬁer with the concatenated embeddings obtained over kt and anti-
kt topologies, but saw no signiﬁcant performance gain. We also tested the performance
of recursive activations transferred across topologies. For instance, we used the recursive
activation learned with a kt topology when applied to an anti-kt topology and observed a
signiﬁcant loss in performance. We also considered particle and tower level inputs with an
additional trimming preprocessing step, which was used for the jet image studies, but we

– 10 –

Figure 4. Jet classiﬁcation performance of the RNN classiﬁer based on various network topologies
for the embedding (particles scenario). This plot shows that topology is signiﬁcant, as supported
by the fact that results for kt, C/A and desc-pT topologies improve over results for anti-kt, asc-pT
and random binary trees. Best results are achieved for C/A and desc-pT topologies, depending on
the metric considered.

saw a signiﬁcant loss in performance. While the trimming degraded classiﬁcation perfor-
mance, we did not evaluate the robustness to pileup that motivates trimming and other
jet grooming procedures.

5.2

Infrared and Collinear Safety Studies

In proposing variables to characterize substructure, physicists have been equally concerned
with classiﬁcation performance and the ability to ensure various theoretical properties
of those variables. In particular, initial work on jet algorithms focused on the Infrared-
Collinear (IRC) safe conditions:

• Infrared safety. The model is robust to augmenting e with additional particles

{vN +1, . . . , vN +K} with small transverse momentum.

• Collinear safety. The model is robust to a collinear splitting of a particle, which is
represented by replacing a particle vj ∈ e with two particles vj1 and vj2, such that
vj = vj1 + vj2 and vj1 · vj2 = ||vj1|| ||vj2|| − (cid:15).

The sequential recombination algorithms lead to an IRC-safe deﬁnition of jets, in the
sense that given the event e, the number of jets M and their 4-momenta v(tj) are IRC-safe.
An early motivation of this work is that basing the RNN topology on the sequential
recombination algorithms would provide an avenue to machine learning classiﬁers with some

– 11 –

theoretical guarantee of IRC safety. If one only wants to ensure robustness to only one soft
particle or one collinear split, this could be satisﬁed by simply running a single iteration
of the jet algorithm as a pre-processing step. However, it is diﬃcult to ensure a more
general notion of IRC safety on the embedding due to the non-linearities in the network.
Nevertheless, we can explicitly test the robustness of the embedding or the subsequent
classiﬁer to the addition of soft particles or collinear splits to the input 4-momenta.

Table 2 shows the results of a non-gated RNN trained on the nominal particle-level
input when applied to testing data with additional soft particles or collinear splits. The
collinear splits were uniform in the momentum fraction and maintained the small invariant
mass of the hadrons. We considered one or ten collinear splits on both random particles
and the highest pT particles. We see that while the 30 models trained with a descending pT
topology very slightly outperform the kt topology for almost scenarios, their performance
in terms of R(cid:15)=50% decreases relatively more rapidly when collinear splits are applied (see
e.g., the collinear10-max scenarios where the performance of kt decreases by 4%, while the
performance of pT decreases by 10%). This suggests a higher robustness towards collinear
splits for recursive networks based on kt topologies.

We also point out that the training of these networks is based solely on the classiﬁcation
loss for the nominal sample. If we are truly concerned with the IRC-safety considerations,
then it is natural to augment the training of the classiﬁers to be robust to these variations.
A number of modiﬁed training procedures exist, including e.g., the adversarial training
procedure described in Ref. [27].

6 Experiments with event-level classiﬁcation

As in the previous section, we carried out a number of performance studies. However, our
goal is mainly to demonstrate the relevance and scalability of the QCD-motivated approach
we propose, rather than making a statement about the physics reach of the signal process.
Results are discussed considering the idealized particles scenario, where the 4-momenta
come from perfectly measured stable hadrons from PYTHIA. Experiments for the towers
scenario (omitted here) reveal similar qualitative conclusions, though performance was
slightly worse for all models, as expected.

Number of jets The ﬁrst factor we varied was the maximum number of jets in the
sequence of embeddings given as input to the GRU. While the event-level embedding can
be computed over all the jets it is constituted by, QCD suggests that the 2 highest pT jets
hold most of the information to separate signal from background events, with only marginal
discriminating information left in the subsequent jets. As Table 3 and Fig. 5 show, there
is indeed signiﬁcant improvement in going from the hardest jet to the 2 hardest jets, while
there is no to little gain in considering more jets. Let us also emphasize that the event-level
models have only 18,681 parameters, and were trained on 100,000 training examples.

Topology of the binary trees The second factor we studied was the architecture of the
networks used for the inner embedding of the jets, for which we compare kt against descend-

– 12 –

Table 2. Performance of pre-trained RNN classiﬁers (without gating) applied to nominal and
modiﬁed particle inputs. The collinear1 (collinear10) scenarios correspond to applying collinear
splits to one (ten) random particles within the jet. The collinear1-max (collinear10-max) scenarios
correspond to applying collinear splits to the highest pT (ten highest pT ) particles in the jet. The
soft scenario corresponds to adding 200 particles with pT = 10−5 GeV uniformly in 0 < φ < 2π and
−5 < η < 5.

Scenario

Architecture

ROC AUC

nominal
nominal
collinear1
collinear1
collinear10
collinear10
collinear1-max
collinear1-max
collinear10-max
collinear10-max
soft
soft

kt
desc-pT
kt
desc-pT
kt
desc-pT
kt
desc-pT
kt
desc-pT
kt
desc-pT

0.9185 ± 0.0006
0.9189 ± 0.0009
0.9183 ± 0.0006
0.9188 ± 0.0010
0.9174 ± 0.0006
0.9178 ± 0.0011
0.9184 ± 0.0006
0.9191 ± 0.0010
0.9159 ± 0.0009
0.9140 ± 0.0016
0.9179 ± 0.0006
0.9188 ± 0.0009

R(cid:15)=50%
68.3 ± 1.8
70.4 ± 3.6
68.7 ± 2.0
70.7 ± 4.0
67.5 ± 2.6
67.9 ± 4.3
68.5 ± 2.8
72.4 ± 4.3
65.7 ± 2.7
63.5 ± 5.2
68.2 ± 2.3
70.2 ± 3.7

ing pT topologies. As in the previous section, best results are achieved with descending pT
topologies, though the diﬀerence is only marginal.

Other variants Finally, we also compare with baselines. With respect to an event-level
embedding computed only from the 4-momenta v(tj) (for j = 1, . . . , M ) of the jets, we
ﬁnd that augmenting the input to the GRU with jet-level embeddings yields signiﬁcant
improvement, e.g. improving ROC AUC from 0.9606 to 0.9875 (resp. R(cid:15)=80% from 21.1 to
174.5) when considering the 2 hardest jets case. This suggests that jet substructures are
important to separate signal from background events, and correctly learned when nesting
embeddings. Similarly, we observe that directly feeding the GRU with the 4-momenta
vi, for i = 1, . . . , N , of the constituents of the event performs signiﬁcantly worse. While
performance remains decent (e.g., with a ROC AUC of 0.8925 when feeding the 50 4-
momenta with largest pT ), this suggests that the recurrent network fails to leverage some
of the relevant information, which is otherwise easier to identify and learn when inputs
to the GRU come directly grouped as jets, themselves structured as trees. In contrast to
our previous results for jet-level experiments, this last comparison underlines the fact that
integrating domain knowledge by structuring the network topology is in some cases crucial
for performance.

Overall, this study shows that event embeddings inspired from QCD and produced
by nested recurrence, over the jets and over their constituents, is a promising avenue for
building eﬀective machine learning models. To our knowledge, this is the ﬁrst classiﬁer

– 13 –

Table 3. Summary of event classiﬁcation performance. Best results are achieved through nested
recurrence over the jets and over their constituents, as motivated by QCD.

Input

ROC AUC

R(cid:15)=80%

v(tj)
v(tj), hjet(kt)
v(tj), hjet(desc−pT )
j

j

v(tj)
v(tj), hjet(kt)
v(tj), hjet(desc−pT )
j

j

v(tj)
v(tj), hjet(kt)
v(tj), hjet(desc−pT )
j

j

Hardest jet
0.8909 ± 0.0007
0.9602 ± 0.0004

0.9594 ± 0.0010
2 hardest jets
0.9606 ± 0.0011
0.9866 ± 0.0007

5.6 ± 0.0
26.7 ± 0.7

25.6 ± 1.4

21.1 ± 1.1
156.9 ± 14.8

0.9875 ± 0.0006 174.5 ± 14.0
5 hardest jets
0.9576 ± 0.0019
0.9867 ± 0.0004

20.3 ± 0.9
152.8 ± 10.4

0.9872 ± 0.0003

167.8 ± 9.5

No jet clustering, desc-pT on vi

i = 1
i = 1, . . . , 50
i = 1, . . . , 100
i = 1, . . . , 200
i = 1, . . . , 400

0.6501 ± 0.0023
0.8925 ± 0.0079
0.8781 ± 0.0180
0.8846 ± 0.0091
0.8780 ± 0.0132

1.7 ± 0.0
5.6 ± 0.5
4.9 ± 0.6
5.2 ± 0.5
4.9 ± 0.5

operating at the hadron-level for all the particles in an event, in a way motivated in its
structure by QCD.

7 Related work

Neural networks in particle physics have a long history. They have been used in the
past for many tasks, including early work on quark-gluon discrimination [28, 29], particle
identiﬁcation [30], Higgs tagging [31] or track identiﬁcation [32]. In most of these, neural
networks appear as shallow multi-layer perceptrons where input features were designed by
experts to incorporate domain knowledge. More recently, the success of deep convolutional
networks has triggered a new body of work in jet physics, shifting the paradigm from
engineering input features to learning them automatically from raw data, e.g., as in these
works treating jets as images [1–8]. Our work builds instead upon an analogy between
QCD and natural languages, hence complementing the set of algorithms for jet physics
with techniques initially developed for natural language processing [9–14]. In addition, our
approach does not delegate the full modeling task to the machine. It allows to incorporate
domain knowledge in terms of the network architecture, speciﬁcally by structuring the
recursion stack for the embedding directly from QCD-inspired jet algorithms (see Sec. 3)

– 14 –

Figure 5.
Event classiﬁcation performance of the RNN classiﬁer when varying the maximum
number of jets given as input to the GRU. This plots shows there is signiﬁcant improvement from
going to the hardest to the 2 hardest jets, while there is no to little gain in considering more jets.

Between the time that this work appeared on the arXiv preprint server and submitted
for publication there has been a ﬂurry of activity connecting deep learning techniques and
jet physics (for reviews see Refs.[33–35]).
In particular the method described here was
also used for quark/gluon tagging in Ref. [36] and a variant of this method was used to
reconstruct a jet’s charge [37]. The authors of Ref. [38] used the tree structure deﬁned by
the jet clustering history to deﬁne a substructure ordering scheme for use with a sequential
recurrent neural network. Going the opposite direction, graph neural networks and mes-
sage passing neural networks have also been applied to the the same jet-level classiﬁcation
problem and data described in this work [39]. There has also been a spate of recent work on
using QCD-inspired variables, enforcing physical constraints into neural networks, and en-
suring infrared safety of neural network based approaches to jet physics [40–44]. Exploring
a complementary direction, several authors have developed ways to train machine learning
techniques using real data to avoid sensitivity to systematic eﬀects in the simulation [45–
48]. These recent training techniques are agnostic to the network architecture and can be
paired with the RNN architectures described here. Finally, deep learning techniques are
now being studied as generative models for jets, where the tree-based model mimics the
parton shower and can be trained on real data [49]. Learning generative modes for both
signal and background classes of jets can be used to deﬁne a classiﬁer in which each branch
of the tree can be interpreted as a contribution to a likelihood ratio discriminant [49].

– 15 –

8 Conclusions

Building upon an analogy between QCD and natural languages, we have presented in
this work a novel class of recursive neural networks for jet physics that are derived from
sequential recombination jet algorithms. Our experiments have revealed that preprocessing
steps applied to jet images during their construction (speciﬁcally the pixelisation) loses
information, which impacts classiﬁcation performance. By contrast, our recursive network
is able to work directly with the four-momenta of a variable-length set of particles, without
the loss of information due to discretization into pixels. Our experiments indicate that this
results in signiﬁcant gains in terms of accuracy and data eﬃciency with respect to previous
image-based networks. Finally, we also showed for the ﬁrst time a hierarchical, event-level
classiﬁcation model operating on all the hadrons of an event. Notably, our results showed
that incorporating domain knowledge derived from jet algorithms and encapsulated in
terms of the network architecture led to improved classiﬁcation performance.

While we initially expected recursive networks operating on jet recombination trees
to outperform simpler pT -ordered architectures, our results still clearly indicate that the
topology has an eﬀect on the ﬁnal performance of the classiﬁer. However, our initial
studies indicate that architectures based on jet trees are more robust to infrared radiation
and collinear splittings than the simpler pT -ordered architectures, which may outweigh
what at face value appears to be a small loss in performance. Accordingly, it would be
natural to include robustness to pileup, infrared radiation, and collinear splittings directly
in the training procedure [27]. Moreover, it is compelling to think of generalizations in
which the optimization would include the topology used for the embedding as learnable
component instead of considering it ﬁxed a priori. An immediate challenge of this approach
is that a discontinuous change in the topology (e.g., from varying α or R) makes the
loss non-diﬀerentiable and rules out standard back propagation optimization algorithms.
Nevertheless, solutions for learning composition orders have recently been proposed in
NLP, using either explicit supervision [50] or reinforcement learning [51]; both of which
could certainly be adapted to jet embeddings. Another promising generalization is to use
a graph-convolutional network that operates on a graph where the vertices correspond to
particle 4-momenta vi and the edge weights are given by dα
ii(cid:48) or a similar QCD-motivated
quantity [39, 52–58]. In conclusion, we feel conﬁdent that there is great potential in hybrid
techniques like this that incorporate physics knowledge and leverage the power of machine
learning.

Acknowledgments

We would like to thank the authors of Ref.[6] for sharing the data used in their studies and
Noel Dawe in particular for his responsiveness in clarifying details about their work. We
would also like to thank Joan Bruna for enlightening discussions about graph-convolutional
networks. Cranmer and Louppe are both supported through NSF ACI-1450310, addition-
ally Cranmer and Becot are supported through PHY-1505463 and PHY-1205376.

– 16 –

References

[1] J. Cogan, M. Kagan, E. Strauss and A. Schwarztman, Jet-Images: Computer Vision Inspired

Techniques for Jet Tagging, JHEP 02 (2015) 118 [1407.5675].

[2] L. de Oliveira, M. Kagan, L. Mackey, B. Nachman and A. Schwartzman, Jet-Images – Deep

Learning Edition, 1511.05190.

[3] L. G. Almeida, M. Backovi´c, M. Cliche, S. J. Lee and M. Perelstein, Playing Tag with ANN:
Boosted Top Identiﬁcation with Pattern Recognition, JHEP 07 (2015) 086 [1501.05968].

[4] P. Baldi, K. Bauer, C. Eng, P. Sadowski and D. Whiteson, Jet Substructure Classiﬁcation in

High-Energy Physics with Deep Neural Networks, 1603.09349.

[5] D. Guest, J. Collado, P. Baldi, S.-C. Hsu, G. Urban and D. Whiteson, Jet Flavor

Classiﬁcation in High-Energy Physics with Deep Neural Networks, Phys. Rev. D94 (2016)
112002 [1607.08633].

[6] J. Barnard, E. N. Dawe, M. J. Dolan and N. Rajcic, Parton Shower Uncertainties in Jet

Substructure Analyses with Deep Neural Networks, 1609.00607.

[7] P. T. Komiske, E. M. Metodiev and M. D. Schwartz, Deep learning in color: towards

automated quark/gluon jet discrimination, JHEP 01 (2017) 110 [1612.01551].

[8] G. Kasieczka, T. Plehn, M. Russell and T. Schell, Deep-learning Top Taggers or The End of

QCD?, 1701.08784.

[9] C. Goller and A. Kuchler, Learning task-dependent distributed representations by

backpropagation through structure, in Neural Networks, 1996., IEEE International
Conference on, vol. 1, pp. 347–352, IEEE, 1996.

[10] R. Socher, C. C. Lin, C. Manning and A. Y. Ng, Parsing natural scenes and natural language

with recursive neural networks, in Proceedings of the 28th international conference on
machine learning (ICML-11), pp. 129–136, 2011.

[11] R. Socher, J. Pennington, E. H. Huang, A. Y. Ng and C. D. Manning, Semi-supervised
recursive autoencoders for predicting sentiment distributions, in Proceedings of the
Conference on Empirical Methods in Natural Language Processing, pp. 151–161, Association
for Computational Linguistics, 2011.

[12] K. Cho, B. van Merri¨enboer, D. Bahdanau and Y. Bengio, On the properties of neural

machine translation: Encoder-decoder approaches, arXiv preprint arXiv:1409.1259 (2014) .

[13] K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk et al.,

Learning phrase representations using rnn encoder-decoder for statistical machine
translation, arXiv preprint arXiv:1406.1078 (2014) .

[14] X. Chen, X. Qiu, C. Zhu, S. Wu and X. Huang, Sentence modeling with gated recursive
neural network, in Proceedings of the 2015 Conference on Empirical Methods in Natural
Language Processing, pp. 793–798, 2015.

[15] I. Goodfellow, Y. Bengio and A. Courville, Deep Learning, ch. 10. MIT Press, 2016.

[16] M. Cacciari, G. P. Salam and G. Soyez, The Anti-k(t) jet clustering algorithm, JHEP 04

(2008) 063 [0802.1189].

[17] G. P. Salam, Towards Jetography, Eur. Phys. J. C67 (2010) 637 [0906.1833].

– 17 –

[18] V. Nair and G. E. Hinton, Rectiﬁed linear units improve restricted boltzmann machines, in

Proceedings of the 27th international conference on machine learning (ICML-10),
pp. 807–814, 2010.

[19] N. Fischer, S. Prestel, M. Ritzmann and P. Skands, Vincia for Hadron Colliders,

1605.06142.

[20] M. Ritzmann, D. A. Kosower and P. Skands, Antenna Showers with Hadronic Initial States,

Phys. Lett. B718 (2013) 1345 [1210.6345].

[21] J. Chung, C. Gulcehre, K. Cho and Y. Bengio, Empirical evaluation of gated recurrent neural

networks on sequence modeling, arXiv preprint arXiv:1412.3555 (2014) .

[22] D. Kingma and J. Ba, Adam: A method for stochastic optimization, arXiv preprint

[23] J. Thaler and K. Van Tilburg, Identifying Boosted Objects with N-subjettiness, JHEP 03

arXiv:1412.6980 (2014) .

(2011) 015 [1011.2268].

[24] S. R. Bowman, C. D. Manning and C. Potts, Tree-structured composition in neural networks

without tree-structured architectures, arXiv preprint arXiv:1506.04834 (2015) .

[25] S. R. Bowman, Modeling natural language semantics in learned representations, Ph.D. thesis,

[26] X. Shi, I. Padhi and K. Knight, Does string-based neural mt learn source syntax?, in Proc. of

STANFORD UNIVERSITY, 2016.

EMNLP, 2016.

preprint arXiv:1611.01046 (2016) .

Phys. Rev. Lett. 65 (1990) 1321.

Phys. B349 (1991) 675.

[27] G. Louppe, M. Kagan and K. Cranmer, Learning to Pivot with Adversarial Networks, arXiv

[28] L. Lonnblad, C. Peterson and T. Rognvaldsson, Finding Gluon Jets With a Neural Trigger,

[29] L. Lonnblad, C. Peterson and T. Rognvaldsson, Using neural networks to identify jets, Nucl.

[30] R. Sinkus and T. Voss, Particle identiﬁcation with neural networks using a rotational

invariant moment representation, Nucl. Instrum. Meth. A391 (1997) 360.

[31] P. Chiappetta, P. Colangelo, P. De Felice, G. Nardulli and G. Pasquariello, Higgs search by

neural networks at LHC, Phys. Lett. B322 (1994) 219 [hep-ph/9401343].

[32] B. H. Denby, Neural Networks and Cellular Automata in Experimental High-energy Physics,

Comput. Phys. Commun. 49 (1988) 429.

[33] A. J. Larkoski, I. Moult and B. Nachman, Jet Substructure at the Large Hadron Collider: A

Review of Recent Advances in Theory and Machine Learning, 1709.04464.

[34] D. Guest, K. Cranmer and D. Whiteson, Deep Learning and its Application to LHC Physics,

[35] M. Russell, Top quark physics in the Large Hadron Collider era, Ph.D. thesis, Glasgow U.,

[36] T. Cheng, Recursive Neural Networks in Quark/Gluon Tagging, Comput. Softw. Big Sci. 2

1806.11484.

2017. 1709.10508.

(2018) 3 [1711.02633].

[37] K. Fraser and M. D. Schwartz, Jet Charge and Machine Learning, 1803.08066.

– 18 –

[38] S. Egan, W. Fedorko, A. Lister, J. Pearkes and C. Gay, Long Short-Term Memory (LSTM)

networks with jet constituents for boosted top tagging at the LHC, 1711.09059.

[39] I. Henrion, K. Cranmer, J. Bruna, K. Cho, J. Brehmer, G. Louppe et al., Neural Message

Passing for Jet Physics, in Proceedings of the Deep Learning for Physical Sciences Workshop
at NIPS (2017), 2017, https://dl4physicalsciences.github.io/ﬁles/nips dlps 2017 29.pdf.

[40] A. Butter, G. Kasieczka, T. Plehn and M. Russell, Deep-learned Top Tagging with a Lorentz

[41] K. Datta and A. J. Larkoski, Novel Jet Observables from Machine Learning, JHEP 03

Layer, 1707.08966.

(2018) 086 [1710.01305].

[42] P. T. Komiske, E. M. Metodiev and J. Thaler, Energy ﬂow polynomials: A complete linear

basis for jet substructure, JHEP 04 (2018) 013 [1712.07124].

[43] S. H. Lim and M. M. Nojiri, Spectral Analysis of Jet Substructure with Neural Network:

[44] S. Choi, S. J. Lee and M. Perelstein, Infrared Safety of a Neural-Net Top Tagging Algorithm,

Boosted Higgs Case, 1807.03312.

1806.01263.

[45] E. M. Metodiev, B. Nachman and J. Thaler, Classiﬁcation without labels: Learning from

mixed samples in high energy physics, JHEP 10 (2017) 174 [1708.02949].

[46] P. T. Komiske, E. M. Metodiev, B. Nachman and M. D. Schwartz, Learning to Classify from

[47] J. H. Collins, K. Howe and B. Nachman, CWoLa Hunting: Extending the Bump Hunt with

Impure Samples, 1801.10158.

Machine Learning, 1805.02664.

[48] R. T. D’Agnolo and A. Wulzer, Learning New Physics from a Machine, 1806.02350.

[49] A. Andreassen, I. Feige, C. Frye and M. D. Schwartz, JUNIPR: a Framework for

Unsupervised Machine Learning in Particle Physics, 1804.09720.

[50] S. R. Bowman, J. Gauthier, A. Rastogi, R. Gupta, C. D. Manning and C. Potts, A fast
uniﬁed model for parsing and sentence understanding, arXiv preprint arXiv:1603.06021
(2016) .

[51] D. Yogatama, P. Blunsom, C. Dyer, E. Grefenstette and W. Ling, Learning to compose words

into sentences with reinforcement learning, arXiv preprint arXiv:1611.09100 (2016) .

[52] J. Bruna, W. Zaremba, A. Szlam and Y. LeCun, Spectral networks and locally connected

networks on graphs, CoRR abs/1312.6203 (2013) .

[53] M. Henaﬀ, J. Bruna and Y. LeCun, Deep convolutional networks on graph-structured data,

[54] Y. Li, D. Tarlow, M. Brockschmidt and R. S. Zemel, Gated graph sequence neural networks,

[55] M. Niepert, M. Ahmed and K. Kutzkov, Learning convolutional neural networks for graphs,

CoRR abs/1506.05163 (2015) .

CoRR abs/1511.05493 (2015) .

CoRR abs/1605.05273 (2016) .

[56] M. Deﬀerrard, X. Bresson and P. Vandergheynst, Convolutional neural networks on graphs

with fast localized spectral ﬁltering, CoRR abs/1606.09375 (2016) .

[57] T. N. Kipf and M. Welling, Semi-supervised classiﬁcation with graph convolutional networks,

arXiv preprint arXiv:1609.02907 (2016) .

– 19 –

[58] T. N. Kipf and M. Welling, Semi-supervised classiﬁcation with graph convolutional networks,

CoRR abs/1609.02907 (2016) .

[59] D. Maclaurin, D. Duvenaud, M. Johnson and R. P. Adams, “Autograd: Reverse-mode

diﬀerentiation of native Python.” http://github.com/HIPS/autograd, 2015.

A Gated recursive jet embedding

The recursive activation proposed in Sec. 3.1 suﬀers from two critical issues. First, it
assumes that left-child, right-child and local node information hjet
, uk are all equally
kL
relevant for computing the new activation, while only some of this information may be
needed and selected. Second, it forces information to pass through several levels of non-
linearities and does not allow to propagate unchanged from leaves to root. Addressing these
issues and generalizing from [12–14], we recursively deﬁne a recursive activation equipped
with reset and update gates as follows:

, hjet
kR

hjet

k =

k + zL (cid:12) hjet
kL

+ otherwise

uk
zH (cid:12) ˜hjet
(cid:44)→ zR (cid:12) hjet
kR

+ zN (cid:12) uk

if k is a leaf





(cid:40)

uk = σ (Wug(ok) + bu)

ok =

vi(k)
okL + okR


if k is a leaf

otherwise




˜hjet

k = σ


W˜h


 + b˜h





rL (cid:12) hjet
kL
rR (cid:12) hjet


kR
rN (cid:12) uk



= softmax

Wz

+ bz

































˜hjet
k
hjet
kL
hjet
kR
uk
hjet
kL
hjet
kR
uk


 = sigmoid


Wr





 + br

























zH
zL
zR
zN

rL
rR
rN

(A.1)

(A.2)

(A.3)

(A.4)

(A.5)

(A.6)

where W˜h ∈ Rq×3q, b˜h ∈ Rq, Wz ∈ Rq×4q, bz ∈ Rq, Wr ∈ Rq×3q, br ∈ Rq, Wu ∈ Rq×4
and bu ∈ Rq form together the shared parameters to be learned, σ is the ReLU activation
function and (cid:12) denotes the element-wise multiplication.

Intuitively, the reset gates rL, rR and rN control how to actively select and then
and the local node

merge the left-child embedding hjet
kL
information uk to form a new candidate activation ˜hjet
can
then be regarded as a choice among the candidate activation, the left-child embedding, the
right-child embedding and the local node information, as controlled by the update gates

, the right-child embedding hjet
kR

k . The ﬁnal embedding hjet

k

– 20 –

zH , zL, zR and zN . Finally, let us note that the proposed gated recursive embedding is
a generalization of Section 3.1, in the sense that the later corresponds to the case where
update gates are set to zH = 1, zL = 0, zR = 0 and zN = 0 and reset gates to rL = 1,
rR = 1 and rN = 1 for all nodes k.

B Gated recurrent event embedding

In this section, we formally deﬁne the gated recurrent event embedding introduced in
Sec. 3.2. Our event embedding function is a GRU [21] operating on the pT ordered sequence
of pairs (v(tj), hjet
1 (tj)), for j = 1, . . . , M , where v(tj) is the unprocessed 4-momentum
(φ, η, pT , m) of the jet tj and hjet
1 (tj) is its embedding. Its ﬁnal output hevent
j=M is recursively
deﬁned as follows:

hevent
j
˜hevent
j

j−1 + (1 − zj) (cid:12) ˜hevent

= zj (cid:12) hevent
= σ (cid:0)Whxxj + Whh(rj (cid:12) hevent

j

j−1 ) + bh

(cid:1)

xj =

(cid:34)

(cid:35)

v(tj)
hjet
1 (tj)

(B.1)

(B.2)

(B.3)

(cid:1)

zj = sigmoid (cid:0)Wzxxj + Wzhhevent
rj = sigmoid (cid:0)Wrxxj + Wrhhevent
where Whx ∈ Rr×4+q, Whh ∈ Rr×r, bh ∈ Rr, Wrx ∈ Rr×4+q, Wrh ∈ Rr×r, br ∈ Rr,
Wzx ∈ Rr×4+q, WzhRr×r and bz ∈ Rr are the parameters of the embedding function,
r is the size of the embedding, σ is the ReLU activation function, and hevent
In
the experiments of Sec. 6, only the 1, 2 or 5 hardest jets are considered in the sequence
j = 1, . . . , M , as ordered by ascending values of pT .

j−1 + bz
j−1 + br

(B.4)

(B.5)

= 0.

(cid:1)

0

C Implementation details

While tree-structured networks appear to be a principled choice in natural language pro-
cessing, they often have been overlooked in favor of sequence-based networks on the account
of their technical incompatibility with batch computation [50]. Because tree-structured
networks use a diﬀerent topology for each example, batching is indeed often impossible
in standard implementations, which prevents them from being trained eﬃciently on large
datasets. For this reason, the recursive jet embedding we introduced in Sec. 3.1 would
undergo the same technical issues if not implemented with caution.

In our implementation, we achieve batch computation by noticing that activations
from a same level in a recursive binary tree can be performed all at once, provided all nec-
essary computations from the deeper levels have already been performed. This principle
extends to the synchronized computation across multiple trees, which enables the batch
computation of our jet embeddings across many events. More speciﬁcally, the computation
of jet embeddings is preceded by a traversal of the recursion trees for all jets in the batch,
and whose purpose is to unroll and regroup computations by their level of recursion. Em-
beddings are then reconstructed level-wise in batch, in a bottom-up fashion, starting from
the deepest level of recursion across all trees.

– 21 –

Finally, learning is carried out through gradients obtained by the automatic diﬀer-
entiation of the full model chain on a batch of events (i.e., the recursive computation of
jet embeddings, the sequence-based recurrence to form the event embeddings, and the for-
ward pass through the classiﬁer). The implementation is written in native Python code and
makes use of Autograd [59] for the easy derivation of the gradients over dynamic structures.
Code is available at 2 under BSD license for further technical details.

2https://github.com/glouppe/recnn

– 22 –

