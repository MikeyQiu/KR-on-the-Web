Inference Suboptimality in Variational Autoencoders

Chris Cremer 1 Xuechen Li 1 David Duvenaud 1

8
1
0
2
 
y
a
M
 
7
2
 
 
]

G
L
.
s
c
[
 
 
3
v
8
5
5
3
0
.
1
0
8
1
:
v
i
X
r
a

Abstract
Amortized inference allows latent-variable mod-
els trained via variational learning to scale to large
datasets. The quality of approximate inference is
determined by two factors: a) the capacity of the
variational distribution to match the true poste-
rior and b) the ability of the recognition network
to produce good variational parameters for each
datapoint. We examine approximate inference in
variational autoencoders in terms of these factors.
We ﬁnd that divergence from the true posterior
is often due to imperfect recognition networks,
rather than the limited complexity of the approx-
imating distribution. We show that this is due
partly to the generator learning to accommodate
the choice of approximation. Furthermore, we
show that the parameters used to increase the ex-
pressiveness of the approximation play a role in
generalizing inference rather than simply improv-
ing the complexity of the approximation.

1. Introduction

In this paper, we analyze inference suboptimality: the mis-
match between the true and approximate posterior. More
speciﬁcally, we are interested in understanding what factors
cause the gap between the marginal log-likelihood and the
evidence lower bound (ELBO) in variational autoencoders
(VAEs, Kingma & Welling (2014); Rezende et al. (2014)).
We refer to this as the inference gap. Moreover, we break
down the inference gap into two components: the approxi-
mation gap and the amortization gap. The approximation
gap comes from the inability of the variational distribution
family to exactly match the true posterior. The amortiza-
tion gap refers to the difference caused by amortizing the
variational parameters over the entire training set, instead of
optimizing for each training example individually. We refer
the reader to Table 1 for the deﬁnitions of the gaps and to

1Department of Computer Science, University of Toronto,
Toronto, Canada. Correspondence to: Chris Cremer <ccre-
mer@cs.toronto.edu>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

Fig. 1 for a simple illustration of the gaps. In Fig. 1, L[q]
refers to the ELBO evaluated using an amortized distribu-
tion q, as is typical of VAE training. In contrast, L[q∗] is the
ELBO evaluated using the optimal approximation within its
variational family.

There has been signiﬁcant work on improving variational
inference in VAEs through the development of expressive ap-
proximate posteriors (Rezende & Mohamed, 2015; Kingma
et al., 2016; Ranganath et al., 2016; Tomczak & Welling,
2016; 2017). These works have shown that with more ex-
pressive approximate posteriors, the model learns a better
distribution over the data. Our study aims to gain a bet-
ter understanding of the relationship between expressive
approximations and improved generative models.

Our experiments investigate how the choice of encoder,
posterior approximation, decoder, and optimization affect
the approximation and amortization gaps. We train VAE
models in a number of settings on the MNIST (LeCun et al.,
1998), Fashion-MNIST (Xiao et al., 2017), and CIFAR-10
(Krizhevsky & Hinton, 2009) datasets.

Our contributions are: a) we investigate inference subopti-
mality in terms of the approximation and amortization gaps,
providing insight to guide future improvements in VAE in-
ference, b) we quantitatively demonstrate that the learned
generative model accommodates the choice of approxima-
tion, and c) we demonstrate that parameterized functions
that improve the expressiveness of the approximation play a
signiﬁcant role in reducing amortization error.

Figure 1. Gaps in Inference

Inference Suboptimality in Variational Autoencoders

Term
Inference
Approximation
Amortization

Deﬁnition
log p(x) − L[q]
log p(x) − L[q∗]
L[q∗] − L[q]

VAE Formulation
KL (q(z|x)||p(z|x))
KL (q∗(z|x)||p(z|x))
KL (q(z|x)||p(z|x)) − KL (q∗(z|x)||p(z|x))

Table 1. Summary of Gap Terms. The middle column refers to the general case where our variational objective is a lower bound on the
marginal log-likelihood. The right most column demonstrates the speciﬁc case in VAEs. q∗(z|x) refers to the optimal approximation
within a family Q, i.e. q∗(z|x) = arg minq∈Q KL (q(z|x)||p(z|x)).

2. Background

2.1. Inference in Variational Autoencoders

Let x be the observed variable, z the latent variable, and
p(x, z) be their joint distribution. Given a dataset X =
{x1, x2, ..., xN }, we would like to maximize the marginal
log-likelihood with respect to the model parameters θ:

log pθ(X) =

log pθ(xi) =

log

pθ(xi, zi)dzi.

N
(cid:88)

i=1

(cid:90)

N
(cid:88)

i=1

In practice, the marginal log-likelihood is computationally
intractable due to the integration over the latent variable
z. Instead, VAEs introduce an inference network qφ(z|x)
to approximate the true posterior p(z|x) and optimize the
ELBO with respect to model parameters θ and inference
network parameters φ (parameterization subscripts omitted
for brevity):

log p(x) = Eq(z|x)

log

+ KL (q(z|x)||p(z|x))

(cid:20)

(cid:20)

(cid:19)(cid:21)

(cid:18) p(x, z)
q(z|x)

(cid:19)(cid:21)

(cid:18) p(x, z)
q(z|x)

≥ Eq(z|x)

log

= LVAE[q].

From the above equation, we see that the ELBO is tight
when q(z|x) = p(z|x). The choice of q(z|x) is often a
factorized Gaussian distribution for its simplicity and efﬁ-
ciency. By utilizing the inference network (also referred to
as encoder or recognition network), VAEs amortize infer-
ence over the entire dataset. Furthermore, the overall model
is trained by stochastically optimizing the ELBO using the
reparametrization trick (Kingma & Welling, 2014).

2.2. Expressive Approximate Posteriors

There are a number of strategies for increasing the expres-
siveness of approximate posteriors, going beyond the origi-
nal factorized-Gaussian. We brieﬂy summarize normalizing
ﬂows and auxiliary variables.

2.2.1. NORMALIZING FLOWS

Normalizing ﬂow (Rezende & Mohamed, 2015) is a change
of variables procedure for constructing complex distribu-
tions by transforming probability densities through a se-
ries of invertible mappings. Speciﬁcally, if we transform

a random variable z0 with distribution q0(z), the resulting
random variable zT = T (z0) has a distribution:

qT (zT ) = q0(z0)

det

(3)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂zT
∂z0

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−1

.

By successively applying these transformations, we can
build arbitrarily complex distributions. Stacking these trans-
formations remains tractable due to the determinant being
decomposable: det(AB) = det(A)det(B). An important
property of these transformations is that we can take ex-
pectations with respect to the transformed density qT (zT )
without explicitly knowing its formula due to the law of the
unconscious statistician (LOTUS):

EqT [h(zT )] = Eq0[h(fT (fT −1(...f1(z0))))].

(4)

Using equations (3) and (4), the lower bound with the trans-
formed approximation can be written as:





Ez0∼q0(z|x)


log




q0(z0|x) (cid:81)T

p(x, zT )
(cid:12)
(cid:12)det ∂zt
(cid:12)

t=1

∂zt−1

(cid:12)
(cid:12)
(cid:12)

(1)

(2)









 .

−1

(5)

The main constraint on these transformations is that the
determinant of their Jacobian needs to be easily computable.

2.2.2. AUXILIARY VARIABLES

Deep generative models can be extended with auxiliary vari-
ables which leave the generative model unchanged but make
the variational distribution more expressive. Just as hierar-
chical Bayesian models induce dependencies between data,
hierarchical variational models can induce dependencies be-
tween latent variables. The addition of the auxiliary variable
changes the lower bound to:

(cid:20)

Ez,v∼q(z,v|x)

log

(cid:20)

= Eq(z|x)

log

(cid:18) p(x, z)
q(z|x)

(cid:19)(cid:21)

(cid:18) p(x, z)r(v|x, z)
q(z, v|x)
(cid:19)

(cid:16)

− KL

q(v|z, x)(cid:107)r(v|x, z)

(6)

(cid:17)(cid:21)

(7)

where r(v|x, z) is called the reverse model. From Eqn. 7,
we see that this bound is looser than the regular ELBO,

Inference Suboptimality in Variational Autoencoders

however the extra ﬂexibility provided by the auxiliary vari-
able can result in a higher lower bound. This idea has been
employed in works such as auxiliary deep generative mod-
els (ADGM, (Maaløe et al., 2016)), hierarchical variational
models (HVM, (Ranganath et al., 2016)) and Hamiltonian
variational inference (HVI, (Salimans et al., 2015)).

3. Methods

3.1. Approximation and Amortization Gaps

The inference gap G is the difference between the marginal
log-likelihood log p(x) and a lower bound L[q]. Given
the distribution in the family that maximizes the bound,
q∗(z|x) = arg maxq∈Q L[q], the inference gap decomposes
as the sum of approximation and amortization gaps:

G = log p(x) − L[q] = log p(x) − L[q∗]
(cid:125)

(cid:124)

(cid:123)(cid:122)
Approximation

+ L[q∗] − L[q]
(cid:125)
(cid:123)(cid:122)
Amortization

(cid:124)

.

For VAEs, we can translate the gaps to KL divergences by
rearranging Eqn. (1):

(cid:124)

GVAE = KL(cid:0)q∗(z|x)||p(z|x)(cid:1)
(cid:125)
+ KL(cid:0)q(z|x)||p(z|x)(cid:1) − KL(cid:0)q∗(z|x)||p(z|x)(cid:1)
(cid:123)(cid:122)
(cid:125)
Amortization

(cid:123)(cid:122)
Approximation

(cid:124)

.

replaced with z and z2 with v. We refer to this approximate
distribution as qAF , where AF stands for auxiliary ﬂow. We
train this model by optimizing the following bound:





Eq0(z,v|x)


log




= L[qAF ].

p(x, zT )r(vT |x, zT )
(cid:16) ∂ztvt

(cid:12)
(cid:12)
(cid:12)det

∂zt−1vt−1

−1

(cid:17)(cid:12)
(cid:12)
(cid:12)

qT (zT , vT |x)











(11)

Note that this lower bound is looser as explained in Section
2.2.2. We refer readers to Section 6.1.2 in the Supplemen-
tary material for speciﬁc details of the ﬂow conﬁguration
adopted in the experiments.

3.3. Marginal Log-Likelihood Estimation and Evidence

Lower Bounds

In this section, we describe the estimates we use to compute
the bounds of the inference gaps: log p(x), L[q∗], and L[q].
We use two bounds to estimate the marginal log-likelihood,
log p(x): IWAE (Burda et al., 2016) and AIS (Neal, 2001).

The IWAE bound takes multiple importance weighted sam-
ples from the variational q distribution resulting in a tighter
lower bound than the VAE bound. The IWAE bound is
computed as:

(8)

log p(x) ≥ Ez1...zk∼q(z|x)

log

(cid:34)

(cid:32)

1
k

k
(cid:88)

i=1

p(x, zi)
q(zi|x)

(cid:33)(cid:35)

(12)

3.2. Flexible Approximate Posteriors

= LIWAE[q].

Our experiments involve expressive approximations which
use ﬂow transformations and auxiliary variables. The ﬂow
transformation that we employ is of the same type as the
transformations of Real NVP (Dinh et al., 2017). We parti-
tion the latent variable z into two, z1 and z2, then perform
the following transformations:

z(cid:48)
1 = z1 ◦ σ1(z2) + µ1(z2)
1) + µ2(z(cid:48)
2 = z2 ◦ σ2(z(cid:48)
z(cid:48)
1)

(9)

(10)

: Rn → Rn are differentiable
where σ1, σ2, µ1, µ2
mappings parameterized by neural nets and ◦ takes the
Hadamard or element-wise product. We partition the la-
tent variable by simply indexing the elements of the ﬁrst
half and the second half. The determinant of the combined
(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)det
transformation’s Jacobian,
(cid:12), can be easily eval-
uated. See section 6.4 of the Supplementary material for a
derivation. The lower bound of this approximation is the
same as Eqn. (5). We refer to this approximation as qF low.

(cid:16) ∂z(cid:48)
∂z

As the number of importance samples approaches inﬁnity,
the bound approaches the marginal log-likelihood.
It is
often used as an evaluation metric for generative models
(Burda et al., 2016; Kingma et al., 2016). AIS is potentially
an even tighter lower bound. AIS weights samples from
distributions which are sequentially annealed from an initial
proposal distribution to the true posterior. See Section 6.5
in the Supplementary material for further details regarding
AIS. To compute the AIS bound, we use 100 chains, each
with 10000 intermediate distributions, where each transition
consists of one HMC trajectory with 10 leapfrog steps. The
initial distribution for AIS is the prior, so that it is encoder-
independent.

We estimate the marginal log-likelihood by independently
computing our tightest lower bounds then take the maximum
of the two:

log ˆp(x) = max(LAIS, LIWAE).

We also experiment with an approximation that combines
ﬂow transformations and auxiliary variables. Let z ∈ Rn
be the variable of interest and v ∈ Rn the auxiliary variable.
The ﬂow is the same as equations (9) and (10), where z1 is

The L[q∗] and L[q] bounds are the standard ELBOs, LVAE,
from Eqn. (2), computed with either the amortized q or the
optimal q∗ (see below). When computing LVAE and LIWAE,
we use 5000 samples.

Inference Suboptimality in Variational Autoencoders

A

B

C

D

Figure 2. True Posterior and Approximate Distributions of a VAE with 2D latent space. The columns represent four different datapoints.
The green distributions are the true posterior distributions, highlighting the mismatch with the blue approximations. Amortized: Variational
parameters learned over the entire dataset. Optimal: Variational parameters optimized for each individual datapoint. Flow: Using a
ﬂexible approximate distribution.

3.4. Local Optimization of the Approximate

Distribution

To compute LVAE[q∗], we optimize the parameters of the
variational distribution for every datapoint. For the local op-
timization of qF F G, we initialize the mean and variance as
the prior, i.e. N (0, I). We optimize the mean and variance
using the Adam optimizer with a learning rate of 10−3. To
determine convergence, after every 100 optimization steps,
we compute the average of the previous 100 ELBO values
and compare it to the best achieved average. If it does not
improve for 10 consecutive iterations then the optimization
is terminated. For qF low and qAF , the same process is used
to optimize all of its parameters. All neural nets for the ﬂow
were initialized with a variant of the Xavier initilization
(Glorot & Bengio, 2010). We use 100 Monte Carlo samples
to compute the ELBO to reduce variance.

3.5. Validation of Bounds

The soundness of our empirical analysis depends on the
reliability of the marginal log-likelihood estimator. For
general importance sampling based estimators, the sample
variance of the normalized importance weights can serve as
an indicator of accuracy (Geweke, 1989; Neal, 2001). This
quantitative measure, however, can also be unreliable, e.g.
when the proposal misses an important mode of the target
distribution (Neal, 2001).

In this work, we follow (Wu et al., 2017) to empirically
validate our AIS estimates with Bidirectional Monte Carlo
(BDMC, Grosse et al. (2015; 2016)). In addition to a lower
bound provided by AIS, BDMC runs AIS chains backward
from exact posterior samples to obtain an upper bound on
the marginal log-likelihood. It should be noted that BDMC
relies on the assumption that the distribution of the simulated
data from the model roughly matches that of the real data.
This is due to the backward chain initializes from exact
posterior samples (Grosse et al., 2015).

For the MNIST and Fashion datasets, BDMC gives a gap
within 0.1 nat for a linear schedule AIS with 104 inter-
mediate distributions and 100 importance samples on 103
simulated datapoints. For 3-BIT CIFAR, the same AIS
setting gives a gap within 1 nat with the sigmoidial anneal-
ing schedule (Grosse et al., 2015) on 100 simulated data-
points. Loosely speaking, this should give us conﬁdence
in how well our AIS lower bounds reﬂect the marginal log-
likelihood computed on the real data.

4. Related Work

Much of the earlier work on variational inference focused
on optimizing the variational parameters locally for each
datapoint, e.g. the original Stochastic Variational Inference
scheme (SVI, Hoffman et al. (2013)). To scale inference to
large datasets, most related works utilize inference networks
to amortize the cost of inference over the entire dataset.

Inference Suboptimality in Variational Autoencoders

MNIST

3-BIT CIFAR

log ˆp(x)
LVAE[q∗
AF ]
LVAE[q∗
F F G]

LVAE[q]
Approximation
Amortization
Inference

qF F G
-89.80
-90.80
-91.23
-92.57
1.43
1.34
2.77

qAF
-88.94
-90.38
-113.54
-91.79
1.44
1.41
2.85

Fashion-MNIST
qAF
-97.41
-99.10
-132.46
-103.76
1.69
4.66
6.35

qF F G
-97.47
-98.92
-100.53
-104.75
3.06
4.22
7.28

qF F G
-816.9
-820.19
-831.65
-869.12
14.75
37.47
52.22

qAF
-820.56
-822.16
-861.62
-864.28
1.60
42.12
43.72

Table 2. Inference Gaps. The columns qF F G and qAF refer to the variational distribution used for training the model. These lower bounds
are computed on the training set and are in units of nats.

Our work analyses the error that these inference networks
introduce.

Most relevant to our work is the recent work of Krishnan
et al. (2017), which explicitly remarks on two sources of
error in variational learning with inference networks, and
proposes to optimize approximate inference locally from
an initialization output by the inference network. They
show improved training on high-dimensional, sparse data
with the hybrid method, claiming that local optimization
reduces the negative effects of random initialization in the
inference network early on in training. Thus their work
focuses on reducing the amortization gap early on in training.
Similar to this idea, Hoffman (2017) proposes to perform
approximate inference during model training with MCMC
at an initialization given by a variational distribution. Our
work provides a means of explaining these improvements
in terms of the sources of inference suboptimality that they
reduce.

5. Experimental Results

5.1. Intuition through Visualization

To begin, we would like to gain an intuitive visualization of
the gaps presented in Section 3.1. To this end, we trained
a VAE with a two-dimensional latent space on MNIST and
in Fig. 2 we show contour plots of various distributions
in the latent space. The ﬁrst row contains contour plots of
the true posteriors p(z|x) for four different training data-
points (columns). We have selected these four examples
to highlight different inference phenomena. The amortized
fully-factorized Gaussian (FFG) row refers to the output
of the recognition net, in this case, a FFG approximation.
Optimal FFG is the FFG that best ﬁts the posterior of the
datapoint. Optimal Flow is the optimal ﬁt of a ﬂexible distri-
bution to the same posterior, where the ﬂexible distribution
we use is described in Section 3.2.

Posterior A is an example of a distribution where a FFG can
ﬁt relatively well. Posterior B is an example of a posterior
with dependence between dimensions, demonstrating the

limitation of having a factorized approximation. Posterior C
highlights a shortcoming of performing amortization with a
limited-capacity recognition network, where the amortized
FFG shares little support with the true posterior. Posterior
D is a bi-modal distribution which demonstrates the ability
of the ﬂexible approximation to ﬁt to complex distributions,
in contrast to the simple FFG approximation. These obser-
vations raise the following question: in more typical VAEs,
is the amortization of inference the leading cause of the
distribution mismatch, or is it the limited expressiveness of
the approximation?

5.2. Amortization vs Approximation Gap

In this section, we compare how much the approximation
and amortization gaps each contribute to the total inference
gap. Table 2 are results of inference on the training set of
MNIST, Fashion-MNIST and 3-BIT CIFAR (a binarized
version of CIFAR-10, see Section 6.1.3 for details). For
each dataset, we trained models with two different approx-
imate posterior distributions: a fully-factorized Gaussian,
qF F G, and the ﬂexible distribution, qAF . Due to the com-
putational cost of optimizing the local parameters for each
datapoint, our evaluation is performed on a subset of 1000
datapoints for MNIST and Fashion-MNIST and a subset of
100 datapoints for 3-BIT CIFAR.

For MNIST, we see that the amortization and approximation
gaps each account for nearly half of the inference gap. On
the more difﬁcult Fashion-MNIST dataset, the amortization
gap is larger than the approximation gap. For CIFAR, we
see that the amortization gap is much more signiﬁcant com-
pared to the approximation gap. Thus, for the three datasets
and model architectures that we consider, the amortization
gap is likely to be the more prominent cause of inference
suboptimality, especially when the dataset becomes more
challenging to model. This indicates that improvements in
inference will likely be a result of reducing amortization
error, rather than approximation errors.

With these results in mind, would simply increasing the
capacity of the encoder improve the amortization gap? We

Inference Suboptimality in Variational Autoencoders

MNIST

MNIST

log ˆp(x)
LVAE[q∗
AF ]
LVAE[q∗
F F G]

LVAE[q]
Approximation
Amortization
Inference

qF F G
-89.61
-90.65
-91.07
-92.18
1.46
1.11
2.56

qAF
-88.99
-90.44
-108.71
-91.19
1.45
0.75
2.20

Fashion-MNIST
qAF
-96.18
-97.91
-129.70
-101.67
1.73
3.76
5.49

qF F G
-95.99
-97.40
-99.64
-102.73
3.65
3.09
6.74

qF F G
-89.82
-90.96
-90.84
-92.33
1.02
1.49
2.51

qAF
-89.52
-90.45
-92.25
-91.75
0.93
1.30
2.23

Fashion-MNIST
qAF
-102.88
-104.02
-105.80
-107.01
1.14
2.29
4.13

qF F G
-102.56
-103.73
-103.85
-106.90
1.29
3.05
4.34

Table 3. Left: Larger Encoder. Right: Models trained without entropy annealing. The columns qF F G and qAF refer to the variational
distribution used for training the model. The lower bounds are computed on the training set and are in units of nats.

examined this by training the MNIST and Fashion-MNIST
models from above but with larger encoders. See Section
6.1.2 for implementation details. Table 3 (left) are the results
of this experiment. Comparing to Table 2, we see that, for
both datasets and both variational distributions, using a
larger encoder results in the inference gap decreasing and
the decrease is mainly due to a reduction in the amortization
gap.

5.3. Inﬂuence of Flows on the Amortization Gap

The common reasoning for increasing the expressiveness
of the approximate posterior is to minimize the difference
between the true and approximate distributions, i.e. reduce
the approximation gap. However, given that the expressive
approximation is often accompanied by many additional
parameters, we would like to know how much inﬂuence it
has on the amortization error.

To investigate this, we trained a VAE on MNIST, discarded
the encoder, then retrained encoders with different approx-
imate distributions on the ﬁxed decoder. We ﬁxed the de-
coder so that the true posterior is constant for all the re-
trained encoders. The initial encoder was a two-layer MLP
with a factorized Gaussian distribution.
In order to em-
phasize a large amortization gap, the retrained encoders
had no hidden layers (ie. just linear transformations). For
the retraiend encoders, we tested three approximate distri-
butions: fully factorized Gaussian (qF F G), auxiliary ﬂow
(qAV ), and Flow (qF low). See Section 3.2 for the details of
these distributions.

The inference gaps of the retrained encoders on the training
set are shown in Table 4. As expected, we observe that the
small encoder with qF F G has a very large amortization gap.
However, when we use qAF or qF low as the approximate
distribution, we see the approximation gap decrease, but
more importantly, there is a signiﬁcant decrease in the amor-
tization gap. This indicates that the parameters used for
increasing the complexity of the approximation also play a
large role in diminishing the amortization error.

Variational Family
log ˆp(x)
LVAE[q∗]
LVAE[q]
Approximation
Amortization
Inference

qF F G
-84.70
-86.61
-129.83
1.91
43.22
45.13

qAF
-84.70
-85.48
-98.58
0.78
13.10
13.88

qF low
-84.70
-85.13
-97.99
0.43
12.86
13.29

Table 4. Inﬂuence of expressive approximations on the amortiza-
tion gap. The parameters used to increase the ﬂexibility of the
approximate distribution also reduce the amortization gap.

These results are expected given that the parameterization of
the Flow distribution can be interpreted as an instance of the
RevNet (Gomez et al., 2017) which has demonstrated that
Real-NVP transformations (Dinh et al., 2017) can model
complex functions similar to typical MLPs. Thus the ﬂow
transformations we employ should also be expected to in-
crease the expressiveness while also increasing the capacity
of the encoder. The implication of this observation is that
models which improve the ﬂexibility of their variational
approximation, and attribute their improved results to the
increased expressiveness, may have actually been due to the
reduction in amortization error.

5.4. Inﬂuence of Approximate Posterior on True

Posterior

To what extent does the posterior approximation affect the
learned model? Turner & Sahani (2011) studied the biases in
parameter learning induced by the variational approximation
when learning via variational Expectation-Maximization.
Similarly, we ask whether a factorized Gaussian approxima-
tion causes the true posterior to be more like a factorized
Gaussian? Burda et al. (2016) visually demonstrate that
when trained with an importance-weighted approximate pos-
terior, the resulting true posterior is more complex than those
trained with factorized Gaussian approximations. Just as it
is hard to evaluate a generative model by visually inspecting
samples, it is hard to judge how “Gaussian” the true poste-
rior is by visual inspection. We can quantitatively determine

Inference Suboptimality in Variational Autoencoders

Figure 3. Inference gaps over epochs trained on binarized Fashion-MNIST. Blue is the approximation gap. Orange is the amortization gap.
Standard is a VAE with FFG approximation. Flow is a VAE with a Flow approximation.

how close the posterior is to a fully-factorized Gaussian
(FFG) by comparing the marginal log-likelihood estimate
log ˆp(x) and the Optimal FFG bound LVAE[q∗
F F G]. This is
equivalent to estimating the KL divergence between the opti-
mal Gaussian and the true posterior, KL (q∗(z|x)||p(z|x)).

AF .

In Table 2 on MNIST, for the FFG trained model,
KL (q∗(z|x)||p(z|x)) is nearly the same for both q∗
F F G
and q∗
In contrast, on the model trained with qAF ,
KL (q∗(z|x)||p(z|x)) is much larger for q∗
AF .
This suggests that the true posterior of a FFG-trained model
is closer to FFG than the true posterior of the Flow-trained
model. The same observation can be made on the Fashion-
MNIST dataset. This implies that the decoder can learn to
have a true posterior that ﬁts better to the approximation.

F F G than q∗

These observations justify our results of Section 5.2. which
showed that the amortization error is often the main cause
of inference suboptimality. One reason for this is that the
generator accommodates the choice of approximation, thus
reducing the approximation error.

Generator Hidden Layers
log ˆp(x)

LVAE[q∗

F F G]

Approximation Gap

0
-100.52
-104.42
3.90

2
-84.78
-86.61
1.83

4
-82.19
-83.82
1.63

Table 5. Increased decoder capacity reduces the approximation
gap.

Given that we have seen that the generator can accommodate
the choice of approximation, our next question is whether a
generator with more capacity increases its ability to ﬁt to the
approximation. To this end, we trained VAEs with decoders
of different sizes and measured the approximation gaps on

the training set. Speciﬁcally, we trained decoders with 0, 2,
and 4 hidden layers on MNIST. See Table 5 for the results.
We see that as the capacity of the decoder increases, the ap-
proximation gap decreases. This result implies that the more
ﬂexible the generator is, the less ﬂexible the approximate
distribution needs to be to ensure accurate inference.

5.5. Inference Generalization

How well does amortized inference generalize at test time?
We address this question by visualizing the gaps on training
and validation datapoints across the training epochs. In
Fig. 3, the models are trained on 50000 binarized Fashion-
MNIST datapoints and the gaps are computed on a subset
of a 100 training and validation datapoints. The top and
bottom boundaries of the blue region represent log ˆp(x) and
L[q∗]. The bottom boundary of the orange region represents
L[q]. In other words, the blue region is the approximation
gap and the orange is the amortization gap.

In Fig. 3, the Standard model (top left) refers to a VAE of
latent size 20 trained with a factorized Gaussian approxi-
mate posterior. In this case, the encoder and decoder both
have two hidden layers each consisting of 200 hidden units.
The Flow model (top right) augments the Standard model
with a qF low variational distribution. Larger Decoder and
Larger Encoder models have factorized Gaussian distribu-
tions and increase the number of hidden layers to three and
the number of units in each layer to 500.

Firstly, we observe that for all models, the approximation
gap on the training and validation sets are roughly equivalent.
This indicates that the true posteriors of the held-out data are
similar to that of the training data. Secondly, we note that
for all models, the encoder overﬁts more than the decoder.

Inference Suboptimality in Variational Autoencoders

These observations resonate with the encoder overﬁtting
ﬁndings by Wu et al. (2017).

approximate distribution:

How does increasing decoder capacity affect inference on
held-out data? We know from Section 5.4 that increas-
ing generator capacity results in a posterior that better ﬁts
the approximation making posterior inference easier. Fur-
thermore, the Larger Decoder plot of Fig. 3 shows that
increasing generator capacity causes the model to be more
prone to overﬁtting. Thus, there is a tradeoff between ease
of inference and decoder overﬁtting.

5.5.1. ENCODER CAPACITY AND APPROXIMATION

EXPRESSIVENESS

We have seen in Sections 5.2 and 5.3 that expressive approx-
imations as well as increasing encoder capacity can lead to
a reduction in the amortization gap. This leads us to the fol-
lowing question: when should we increase encoder capacity
versus increasing the expressiveness of the approximation?
We answer this question in terms of how well each model
can generalize its efﬁcient inference (recognition network
and variational distribution) to held-out data.

In Fig. 3, we see that the Flow model and the Larger Encoder
model achieve similar log ˆp(x) on the validation set at the
end of training. However, we see that the L[q] bound of the
Larger Encoder model is signiﬁcantly lower than the L[q]
bound of the Flow model due to the encoder overﬁtting to
the training data. Although they both model the data nearly
equally well, the recognition net of the Larger Encoder
model is no longer suitable to perform inference on the
held-out data due to overﬁtting. Thus a potential rational
for utilizing expressive approximations is that they improve
generalization to held-out data in comparison to increasing
the encoder capacity.

We highlight that, in many scenarios, efﬁcient test time infer-
ence is not required and consequently, encoder overﬁtting
is not an issue, since we can use non-efﬁcient encoder-
independent methods to estimate log p(x), such as AIS,
IWAE with local optimization, or potentially retraining the
encoder on the held-out data. In contrast, when efﬁcient test
time inference is required, encoder generalization is impor-
tant and expressive approximations are likely advantageous.

5.6. Annealing the Entropy

Typical warm-up (Bowman et al., 2015; Sønderby et al.,
2016) refers to annealing the KL (q(z|x)||p(z)) term during
training. This can also be interpreted as performing maxi-
mum likelihood estimation (MLE) early on during training.
This optimization technique is known to help prevent the
latent variable from degrading to the prior (Burda et al.,
2016; Sønderby et al., 2016). We employ a similar anneal-
ing scheme during training by annealing the entropy of the

Ez∼q(z|x) [log p(x, z) − λ log q(z|x)] ,

where λ is annealed from 0 to 1 over training. This can be
interpreted as maximum a posteriori (MAP) in the initial
phase of training.

We ﬁnd that warm-up techniques, such as annealing the
entropy, are important for allowing the true posterior to be
more complex. Table 3 (right) are results from a model
trained without the entropy annealing schedule. Comparing
these results to Table 2, we observe that the difference be-
tween LVAE[q∗
AF ] is signiﬁcantly smaller
without entropy annealing. This indicates that the true pos-
terior is more Gaussian when entropy annealing is not used.
This suggests that, in addition to preventing the latent vari-
able from degrading to the prior, entropy annealing allows
the true posterior to better utilize the ﬂexibility of the ex-
pressive approximation.

F F G] and LVAE[q∗

6. Conclusion

In this paper, we investigated how encoder capacity, approx-
imation choice, decoder capacity, and model optimization
inﬂuence inference suboptimality in terms of the approxima-
tion and amortization gaps. We discovered that the amortiza-
tion gap can be a leading source to inference suboptimality
and that the generator can reduce the approximation gap by
learning a true posterior that ﬁts to the choice of approxima-
tion. We showed that the parameters used to increase the
expressiveness of the approximation play a role in general-
izing inference rather than simply improving the complexity
of the approximation. We conﬁrmed that increasing the
capacity of the encoder reduces the amortization error. Ad-
ditionally, we demonstrated that optimization techniques,
such as entropy annealing, help the generative model to
better utilize the ﬂexibility of expressive variational distri-
butions. Analyzing these gaps can be useful for guiding
improvements in VAEs. Future work includes evaluating
other types of expressive approximations, more complex
likelihood functions, and datasets.

References

Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Joze-
fowicz, R., and Bengio, S. Generating Sentences from a
Continuous Space. ArXiv e-prints, November 2015.

Burda, Y., Grosse, R., and Salakhutdinov, R. Importance

weighted autoencoders. In ICLR, 2016.

Clevert, Djork-Arn´e, Unterthiner, Thomas, and Hochre-
iter, Sepp.
Fast and accurate deep network learn-
ing by exponential linear units (elus). arXiv preprint
arXiv:1511.07289, 2015.

Inference Suboptimality in Variational Autoencoders

Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density estima-

tion using Real NVP. ICLR, 2017.

Geweke, John. Bayesian inference in econometric models
using monte carlo integration. Econometrica: Journal of
the Econometric Society, pp. 1317–1339, 1989.

Glorot, Xavier and Bengio, Yoshua. Understanding the
difﬁculty of training deep feedforward neural networks.
In Proceedings of the Thirteenth International Confer-
ence on Artiﬁcial Intelligence and Statistics, pp. 249–256,
2010.

Gomez, Aidan N, Ren, Mengye, Urtasun, Raquel, and
Grosse, Roger B. The reversible residual network: Back-
propagation without storing activations. In Advances in
Neural Information Processing Systems, pp. 2211–2221,
2017.

Grosse, R., Ghahramani, Z., and Adams, R. P. Sandwiching
the marginal likelihood using bidirectional monte carlo.
arXiv preprint arXiv:1511.02543, 2015.

Grosse, Roger B, Ancha, Siddharth, and Roy, Daniel M.
Measuring the reliability of mcmc inference with bidirec-
tional monte carlo. In Advances in Neural Information
Processing Systems, pp. 2451–2459, 2016.

Hoffman, Matthew D. Learning deep latent gaussian models
with markov chain monte carlo. In International Confer-
ence on Machine Learning, pp. 1510–1519, 2017.

Hoffman, Matthew D, Blei, David M, Wang, Chong, and
Paisley, John. Stochastic variational inference. The Jour-
nal of Machine Learning Research, 14(1):1303–1347,
2013.

Jarzynski, C. Nonequilibrium equality for free energy dif-
ferences. Physical Review Letters, 78(14):2690, 1997.

Kingma, Diederik and Ba, Jimmy. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.

Kingma, D.P. and Welling, M. Auto-Encoding Variational

Bayes. In ICLR, 2014.

Kingma, D.P., Salimans, T., Jozefowicz, R., Chen, X.,
Sutskever, I., and Welling, M.
Improving Variational
Inference with Inverse Autoregressive Flow. NIPS, 2016.

Krishnan, R. G., Liang, D., and Hoffman, M. On the chal-
lenges of learning with inference networks on sparse,
high-dimensional data. ArXiv e-prints, October 2017.

Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple
layers of features from tiny images. University of Toronto,
2009.

Larochelle, Hugo and Bengio, Yoshua. Classiﬁcation using
discriminative restricted boltzmann machines. In Pro-
ceedings of the 25th international conference on Machine
learning, pp. 536–543. ACM, 2008.

LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 1998.

Maaløe, L., Sønderby, CK., Sønderby, SK., and Winther, O.

Auxiliary Deep Generative Models. ICML, 2016.

Neal, R.M. Annealed importance sampling. Statistics and

Computing, 2001.

Ranganath, R., Tran, D., and Blei, D. M. Hierarchical

Variational Models. ICML, 2016.

Rezende, D.J. and Mohamed, S. Variational Inference with

Normalizing Flows. In ICML, 2015.

Rezende, D.J., Mohamed, S., and Wierstra, D. Stochastic
Backpropagation and Approximate Inference in Deep
Generative Models. ICML, 2014.

Salakhutdinov, R. and Murray, I. On the quantitative analy-
sis of deep belief networks. In Proceedings of the 25th
international conference on Machine learning, pp. 872–
879. ACM, 2008.

Salimans, T., Kingma, D.P., and Welling, M. Markov chain
monte carlo and variational inference: Bridging the gap.
In ICML, 2015.

Sønderby, Casper Kaae, Raiko, Tapani, Maaløe, Lars,
Sønderby, Søren Kaae, and Winther, Ole. Ladder varia-
tional autoencoders. In Advances in Neural Information
Processing Systems, pp. 3738–3746, 2016.

Tomczak, J. M. and Welling, M.

Improving Variational
Auto-Encoders using Householder Flow. ArXiv e-prints,
November 2016.

Tomczak, J. M. and Welling, M.

Improving Variational
Auto-Encoders using convex combination linear Inverse
Autoregressive Flow. ArXiv e-prints, June 2017.

Turner, R and Sahani, M. Two problems with variational ex-
pectation maximisation for time-series models. inference
and learning in dynamic models. Cambridge University
Press, pp. 104–123, 2011.

Wu, Y., Burda, Y., Salakhutdinov, R., and Grosse, R. On
the Quantitative Analysis of Decoder-Based Generative
Models. ICLR, 2017.

Xiao, Han, Rasul, Kashif, and Vollgraf, Roland. Fashion-
mnist: a novel image dataset for benchmarking machine
learning algorithms. github.com/zalandoresearch/fashion-
mnist, 2017.

Supplementary

Code for the experiments in this paper can be found at:
https://github.com/chriscremer/Inference-Suboptimality and
https://github.com/lxuechen/inference-suboptimality.

6.1.3. 3-BIT CIFAR

6.1. Model Architectures and Training

Hyperparameters

6.1.1. 2D VISUALIZATION

The VAE model of Fig. 2 uses a decoder p(x|z) with ar-
chitecture: 2 − 100 − 784, and an encoder q(z|x) with
architecture: 784 − 100 − 4. We use tanh activations and a
batch size of 50. The model is trained for 3000 epochs with
a learning rate of 10−4 using the ADAM optimizer (Kingma
& Ba, 2014).

6.1.2. MNIST & FASHION-MNIST

Both MNIST and Fashion-MNIST consist of a training and
test set with 60000 and 10000 datapoints respectively, where
each datapoint is a 28x28 grey-scale image. We rescale the
original images so that pixel values are within the range
[0, 1]. For MNIST, We use the statically binarized version
described by (Larochelle & Bengio, 2008). We also binarize
Fashion-MINST statically. For both datasets, we adopt the
Bernoulli likelihood for the generator.

The VAE models for MNIST and Fashion-MNIST exper-
iments have the same architecture. The encoder has two
hidden layers with 200 units each. The activation function is
chosen to be the exponential linear unit (ELU, Clevert et al.
(2015)), as we observe improved performance compared to
tanh. The latent space has 50 dimensions. The generator
is the reverse of the encoder. We follow the same learning
rate schedule and train for the same amount of epochs as
described by (Burda et al., 2016). All models are trained
with the a batch-size of 100 with ADAM.

In the large encoder setting, we change the number of hidden
units for the inference network to be 500, instead of 200.
The warm-up models are trained with a linear schedule over
the ﬁrst 400 epochs according to Section 5.6.

The auxiliary variable of requires a couple distributions:
q(v0|z0) and r(vT |zT ). These distributions are both factor-
ized Gaussians which are parameterized by MLP’s with two
hidden layers, 100 units each, with ELU activations.

The ﬂow transformation q(zt+1, vt+1|zt, vt) involves func-
tions σ1, σ2, µ1, and µ2 from Eqn. 9 and 10. These also
have two hidden layers with 100 units each and ELU units.

CIFAR-10 consists of a training and test dataset with 50000
and 10000 datapoints respectively, where each datapoint is
a 32 × 32 RGB image. We rescale individual pixel values
to be in the range [0, 1]. We then statically binarize the
scaled pixel values by setting individual pixel values of
channels to 1 if the rescaled value is greater than 0.5 and
0 otherwise. In this manner, we can model the observation
with a factorized Bernoulli likelihood. We call this binarized
CIFAR-10 dataset as 3-BIT CIFAR, since 3 bits are required
to encode each pixel, where 1 bit is needed for each of the
channels. We acknowledge that such binarization scheme
may reduce the complexity of the original problem, since
originally 24 bits were required to encode a single pixel.
Nevertheless, the 3-bit CIFAR dataset is still much more
challenging compared MNIST and Fashion. This is because
784 bits are required to encode one MNIST/Fashion image,
whereas for one 3-bit CIFAR image, 3072 bits are required.
Most notably, we were able to validate our AIS estimates
using BDMC with the simpliﬁed dataset. This, however,
was not achievable in any reasonable amount of time with
the original CIFAR-10 dataset.

For the latent variable, we use a 50-dimensional factorized
Gaussian for q(z|x). For all neural networks, ELU is cho-
sen to be the activation function. The inference network
consists of three 4 by 4 convolution layers with stride 2,
batch-norm, and 64, 128, 256 channels respectively. Then a
fully-connected layer outputs the 50-dimensional mean and
log-variance of the latent variable. Similarly, the generator
consists of a fully-connected layer outputting 256 by 2 by 2
tensors. Then three deconvolutional layers each with 4 by
4 ﬁlters, stride 2, batch-norm, and 128, 64, and 3 channels
respectively. For the model with expressive inference, we
use three normalizing ﬂow steps, where the parametric func-
tions in the ﬂow and auxiliary variable distribution also take
in a hidden layer of the encoder.

We use a learning rate of 10−3. Warm-up is applied with
a linear schedule over the ﬁrst 50 epochs. All models are
trained with a batch-size of 100 with ADAM. Early-stopping
is applied based on the performance computed with the
IWAE bound (k=1000) on the held-out set of 5000 examples
from the original training set.

Inference Suboptimality in Variational Autoencoders

6.2. Inference Generalization

These models are trained with batch size 50 and latent di-
mension size of 20. The rest of the hyperparameters are
equivalent to Section 6.1.2.

Architecture of qF low: The ﬂow transformation involves
functions σ1, σ2, µ1, and µ2 from Eqn. 9 and 10. Each
function is an MLP with a 50 unit hidden layer and ELU
activations. We apply this ﬂow transformation twice.

Fig. 4 are the plots for the qAF model. The transformations
are the same as qF low, but rather than partitioning the latent
variable, we introduce an auxiliary variable. The auxiliary
variable also requires a reverse model r(v|z) which is a
factorized Gaussian parameterized by an MLP with a 50
unit hidden layer and ELU activations.

Comparing AF in Fig. 4 to Flow in Fig. 3, we see that the
AF has a larger approximation gap. This increase is likely
due to the KL (q(v|z, x)(cid:107)r(v|x, z)) term of the auxiliary
variable lower bound from 2.2.2. This motivates also using
expressive approximations for the reverse model r(v|z).

Figure 4. Gaps over epochs of the AF (auxiliary ﬂow) model.

6.3. Inﬂuence of Flows On Amortization Gap

Experiment

The aim of this experiment is to show that the parameters
used for increasing the expressiveness of the approxima-
tion also contribute to reducing the amortization error. To
show this, we train a VAE on MNIST, discard the encoder,
then retrain two encoders on the ﬁxed decoder: one with a
factorized Gaussian distribution and the other with a param-
eterized ’ﬂow’ distribution. We use ﬁxed decoder so that
the true posterior is constant for both encoders. See 5.3 for
the results and below for the architecture details.

The architecture of the decoder is: DZ − 200 − 200 − DX .
The architecture of the encoder used to train the decoder
is DX − 200 − 200 − 2DZ. The approximate distribution
q(z|x) is a factorized Gaussian.

Next, we describe the encoders which were trained on the
ﬁxed trained decoder. In order to highlight a large amorti-
zation gap, we employed a very small encoder architecture:
DX − 2DZ. This encoder has no hidden layers, which

greatly impoverishes its ability and results in a large amorti-
zation gap.

We compare two approximate distributions q(z|x). Firstly,
we experiment with the typical fully factorized Gaussian
(FFG). The second is what we call a ﬂow distribution.
Speciﬁcally, we use the transformations of (Dinh et al.,
2017). We also include an auxiliary variable so we don’t
need to select how to divide the latent space for the trans-
formations. The approximate distribution over the la-
tent z and auxiliary variable v factorizes as: q(z, v|x) =
q(z|x)q(v). The q(v) distribution is simply a N(0,1) dis-
tribution. Since we’re using a auxiliary variable, we also
require the r(v|z) distribution which we parameterize as
r(v|z): [DZ] − 50 − 50 − 2DZ. The ﬂow transformation
is the same as in Section 3.2, which we apply twice.

6.4. Computation of the Determinant for Flow

The overall mapping f that performs (z, v) (cid:55)→ (z(cid:48), v(cid:48)) is
the composition of two sheer mappings f1 and f2 that re-
spectively perform (z, v) (cid:55)→ (z, v(cid:48)) and (z, v(cid:48)) (cid:55)→ (z(cid:48), v(cid:48)).
Since the Jacobian of either one of the sheer mappings is
diagonal, the determinant of the composed transformation’s
Jacobian Df can be easily computed:

det(Df ) = det(Df1)det(Df2)

=

(cid:16) n
(cid:89)

i=1

(cid:17)(cid:16) n
(cid:89)

σ2(v(cid:48))j

(cid:17)

.

σ1(z)i

j=1

6.5. Annealed Importance Sampling

Annealed importance sampling (AIS, Neal (2001); Jarzyn-
ski (1997)) is a means of computing a lower bound to
the marginal log-likelihood. Similarly to the importance
weighted bound, AIS must sample a proposal distribution
f1(z) and compute the density of these samples, however,
AIS then transforms the samples through a sequence of
reversible transitions Tt(z(cid:48)|z). The transitions anneal the
proposal distribution to the desired distribution fT (z).

Speciﬁcally, AIS samples an initial state z1 ∼ f1(z) and
sets an initial weight w1 = 1. For the following annealing
steps, zt is sampled from Tt(z(cid:48)|z) and the weight is updated
according to:

wt = wt−1

ft(zt−1)
ft−1(zt−1)

.

This procedure produces weight wT such that E [wT ] =
ZT /Z1, where ZT and Z1 are the normalizing constants of
fT (z) and f1(z) respectively. This pertains to estimating the
marginal likelihood when the target distribution is p(x, z)
when we integrate with respect to z.

Typically, the intermediate distributions are simply deﬁned
to be geometric averages: ft(z) = f1(z)1−βtfT (z)βt,

Inference Suboptimality in Variational Autoencoders

where βt is monotonically increasing with β1 = 0 and βT = 1.
When f1(z) = p(z) and fT (z) = p(x, z), the intermediate
distributions are: fi(x) = p(z)p(x|z)βi .

Model evaluation with AIS appears early on in the setting
of deep belief networks (Salakhutdinov & Murray, 2008).
AIS for decoder-based models was also used by Wu et al.
(2017).

6.6. Extra MNIST Inference Gaps

To demonstrate that a very small inference gap can be
achieved, even with a limited approximation such as a fac-
torized Gaussian, we train the model on a small dataset. In
this experiment, our training set consists of 1000 datapoints
randomly chosen from the original MNIST training set. The
training curves on this small datatset are shown in Fig. 5.
Even with a factorized Gaussian distribution, the inference
gap is very small: the AIS and IWAE bounds are overlap-
ping and the VAE is just slightly below. Yet, the model is
overﬁtting as seen by the decreasing test set bounds.

Figure 5. Training curves for a FFG and a Flow inference model on
MNIST. AIS provides the tightest lower bound and is independent
of encoder overﬁtting. There is little difference between FFG and
Flow models trained on the 1000 datapoints since inference is
nearly equivalent.

Inference Suboptimality in Variational Autoencoders

Chris Cremer 1 Xuechen Li 1 David Duvenaud 1

8
1
0
2
 
y
a
M
 
7
2
 
 
]

G
L
.
s
c
[
 
 
3
v
8
5
5
3
0
.
1
0
8
1
:
v
i
X
r
a

Abstract
Amortized inference allows latent-variable mod-
els trained via variational learning to scale to large
datasets. The quality of approximate inference is
determined by two factors: a) the capacity of the
variational distribution to match the true poste-
rior and b) the ability of the recognition network
to produce good variational parameters for each
datapoint. We examine approximate inference in
variational autoencoders in terms of these factors.
We ﬁnd that divergence from the true posterior
is often due to imperfect recognition networks,
rather than the limited complexity of the approx-
imating distribution. We show that this is due
partly to the generator learning to accommodate
the choice of approximation. Furthermore, we
show that the parameters used to increase the ex-
pressiveness of the approximation play a role in
generalizing inference rather than simply improv-
ing the complexity of the approximation.

1. Introduction

In this paper, we analyze inference suboptimality: the mis-
match between the true and approximate posterior. More
speciﬁcally, we are interested in understanding what factors
cause the gap between the marginal log-likelihood and the
evidence lower bound (ELBO) in variational autoencoders
(VAEs, Kingma & Welling (2014); Rezende et al. (2014)).
We refer to this as the inference gap. Moreover, we break
down the inference gap into two components: the approxi-
mation gap and the amortization gap. The approximation
gap comes from the inability of the variational distribution
family to exactly match the true posterior. The amortiza-
tion gap refers to the difference caused by amortizing the
variational parameters over the entire training set, instead of
optimizing for each training example individually. We refer
the reader to Table 1 for the deﬁnitions of the gaps and to

1Department of Computer Science, University of Toronto,
Toronto, Canada. Correspondence to: Chris Cremer <ccre-
mer@cs.toronto.edu>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

Fig. 1 for a simple illustration of the gaps. In Fig. 1, L[q]
refers to the ELBO evaluated using an amortized distribu-
tion q, as is typical of VAE training. In contrast, L[q∗] is the
ELBO evaluated using the optimal approximation within its
variational family.

There has been signiﬁcant work on improving variational
inference in VAEs through the development of expressive ap-
proximate posteriors (Rezende & Mohamed, 2015; Kingma
et al., 2016; Ranganath et al., 2016; Tomczak & Welling,
2016; 2017). These works have shown that with more ex-
pressive approximate posteriors, the model learns a better
distribution over the data. Our study aims to gain a bet-
ter understanding of the relationship between expressive
approximations and improved generative models.

Our experiments investigate how the choice of encoder,
posterior approximation, decoder, and optimization affect
the approximation and amortization gaps. We train VAE
models in a number of settings on the MNIST (LeCun et al.,
1998), Fashion-MNIST (Xiao et al., 2017), and CIFAR-10
(Krizhevsky & Hinton, 2009) datasets.

Our contributions are: a) we investigate inference subopti-
mality in terms of the approximation and amortization gaps,
providing insight to guide future improvements in VAE in-
ference, b) we quantitatively demonstrate that the learned
generative model accommodates the choice of approxima-
tion, and c) we demonstrate that parameterized functions
that improve the expressiveness of the approximation play a
signiﬁcant role in reducing amortization error.

Figure 1. Gaps in Inference

Inference Suboptimality in Variational Autoencoders

Term
Inference
Approximation
Amortization

Deﬁnition
log p(x) − L[q]
log p(x) − L[q∗]
L[q∗] − L[q]

VAE Formulation
KL (q(z|x)||p(z|x))
KL (q∗(z|x)||p(z|x))
KL (q(z|x)||p(z|x)) − KL (q∗(z|x)||p(z|x))

Table 1. Summary of Gap Terms. The middle column refers to the general case where our variational objective is a lower bound on the
marginal log-likelihood. The right most column demonstrates the speciﬁc case in VAEs. q∗(z|x) refers to the optimal approximation
within a family Q, i.e. q∗(z|x) = arg minq∈Q KL (q(z|x)||p(z|x)).

2. Background

2.1. Inference in Variational Autoencoders

Let x be the observed variable, z the latent variable, and
p(x, z) be their joint distribution. Given a dataset X =
{x1, x2, ..., xN }, we would like to maximize the marginal
log-likelihood with respect to the model parameters θ:

log pθ(X) =

log pθ(xi) =

log

pθ(xi, zi)dzi.

N
(cid:88)

i=1

(cid:90)

N
(cid:88)

i=1

In practice, the marginal log-likelihood is computationally
intractable due to the integration over the latent variable
z. Instead, VAEs introduce an inference network qφ(z|x)
to approximate the true posterior p(z|x) and optimize the
ELBO with respect to model parameters θ and inference
network parameters φ (parameterization subscripts omitted
for brevity):

log p(x) = Eq(z|x)

log

+ KL (q(z|x)||p(z|x))

(cid:20)

(cid:20)

(cid:19)(cid:21)

(cid:18) p(x, z)
q(z|x)

(cid:19)(cid:21)

(cid:18) p(x, z)
q(z|x)

≥ Eq(z|x)

log

= LVAE[q].

From the above equation, we see that the ELBO is tight
when q(z|x) = p(z|x). The choice of q(z|x) is often a
factorized Gaussian distribution for its simplicity and efﬁ-
ciency. By utilizing the inference network (also referred to
as encoder or recognition network), VAEs amortize infer-
ence over the entire dataset. Furthermore, the overall model
is trained by stochastically optimizing the ELBO using the
reparametrization trick (Kingma & Welling, 2014).

2.2. Expressive Approximate Posteriors

There are a number of strategies for increasing the expres-
siveness of approximate posteriors, going beyond the origi-
nal factorized-Gaussian. We brieﬂy summarize normalizing
ﬂows and auxiliary variables.

2.2.1. NORMALIZING FLOWS

Normalizing ﬂow (Rezende & Mohamed, 2015) is a change
of variables procedure for constructing complex distribu-
tions by transforming probability densities through a se-
ries of invertible mappings. Speciﬁcally, if we transform

a random variable z0 with distribution q0(z), the resulting
random variable zT = T (z0) has a distribution:

qT (zT ) = q0(z0)

det

(3)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂zT
∂z0

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−1

.

By successively applying these transformations, we can
build arbitrarily complex distributions. Stacking these trans-
formations remains tractable due to the determinant being
decomposable: det(AB) = det(A)det(B). An important
property of these transformations is that we can take ex-
pectations with respect to the transformed density qT (zT )
without explicitly knowing its formula due to the law of the
unconscious statistician (LOTUS):

EqT [h(zT )] = Eq0[h(fT (fT −1(...f1(z0))))].

(4)

Using equations (3) and (4), the lower bound with the trans-
formed approximation can be written as:





Ez0∼q0(z|x)


log




q0(z0|x) (cid:81)T

p(x, zT )
(cid:12)
(cid:12)det ∂zt
(cid:12)

t=1

∂zt−1

(cid:12)
(cid:12)
(cid:12)

(1)

(2)









 .

−1

(5)

The main constraint on these transformations is that the
determinant of their Jacobian needs to be easily computable.

2.2.2. AUXILIARY VARIABLES

Deep generative models can be extended with auxiliary vari-
ables which leave the generative model unchanged but make
the variational distribution more expressive. Just as hierar-
chical Bayesian models induce dependencies between data,
hierarchical variational models can induce dependencies be-
tween latent variables. The addition of the auxiliary variable
changes the lower bound to:

(cid:20)

Ez,v∼q(z,v|x)

log

(cid:20)

= Eq(z|x)

log

(cid:18) p(x, z)
q(z|x)

(cid:19)(cid:21)

(cid:18) p(x, z)r(v|x, z)
q(z, v|x)
(cid:19)

(cid:16)

− KL

q(v|z, x)(cid:107)r(v|x, z)

(6)

(cid:17)(cid:21)

(7)

where r(v|x, z) is called the reverse model. From Eqn. 7,
we see that this bound is looser than the regular ELBO,

Inference Suboptimality in Variational Autoencoders

however the extra ﬂexibility provided by the auxiliary vari-
able can result in a higher lower bound. This idea has been
employed in works such as auxiliary deep generative mod-
els (ADGM, (Maaløe et al., 2016)), hierarchical variational
models (HVM, (Ranganath et al., 2016)) and Hamiltonian
variational inference (HVI, (Salimans et al., 2015)).

3. Methods

3.1. Approximation and Amortization Gaps

The inference gap G is the difference between the marginal
log-likelihood log p(x) and a lower bound L[q]. Given
the distribution in the family that maximizes the bound,
q∗(z|x) = arg maxq∈Q L[q], the inference gap decomposes
as the sum of approximation and amortization gaps:

G = log p(x) − L[q] = log p(x) − L[q∗]
(cid:125)

(cid:124)

(cid:123)(cid:122)
Approximation

+ L[q∗] − L[q]
(cid:125)
(cid:123)(cid:122)
Amortization

(cid:124)

.

For VAEs, we can translate the gaps to KL divergences by
rearranging Eqn. (1):

(cid:124)

GVAE = KL(cid:0)q∗(z|x)||p(z|x)(cid:1)
(cid:125)
+ KL(cid:0)q(z|x)||p(z|x)(cid:1) − KL(cid:0)q∗(z|x)||p(z|x)(cid:1)
(cid:123)(cid:122)
(cid:125)
Amortization

(cid:123)(cid:122)
Approximation

(cid:124)

.

replaced with z and z2 with v. We refer to this approximate
distribution as qAF , where AF stands for auxiliary ﬂow. We
train this model by optimizing the following bound:





Eq0(z,v|x)


log




= L[qAF ].

p(x, zT )r(vT |x, zT )
(cid:16) ∂ztvt

(cid:12)
(cid:12)
(cid:12)det

∂zt−1vt−1

−1

(cid:17)(cid:12)
(cid:12)
(cid:12)

qT (zT , vT |x)











(11)

Note that this lower bound is looser as explained in Section
2.2.2. We refer readers to Section 6.1.2 in the Supplemen-
tary material for speciﬁc details of the ﬂow conﬁguration
adopted in the experiments.

3.3. Marginal Log-Likelihood Estimation and Evidence

Lower Bounds

In this section, we describe the estimates we use to compute
the bounds of the inference gaps: log p(x), L[q∗], and L[q].
We use two bounds to estimate the marginal log-likelihood,
log p(x): IWAE (Burda et al., 2016) and AIS (Neal, 2001).

The IWAE bound takes multiple importance weighted sam-
ples from the variational q distribution resulting in a tighter
lower bound than the VAE bound. The IWAE bound is
computed as:

(8)

log p(x) ≥ Ez1...zk∼q(z|x)

log

(cid:34)

(cid:32)

1
k

k
(cid:88)

i=1

p(x, zi)
q(zi|x)

(cid:33)(cid:35)

(12)

3.2. Flexible Approximate Posteriors

= LIWAE[q].

Our experiments involve expressive approximations which
use ﬂow transformations and auxiliary variables. The ﬂow
transformation that we employ is of the same type as the
transformations of Real NVP (Dinh et al., 2017). We parti-
tion the latent variable z into two, z1 and z2, then perform
the following transformations:

z(cid:48)
1 = z1 ◦ σ1(z2) + µ1(z2)
1) + µ2(z(cid:48)
2 = z2 ◦ σ2(z(cid:48)
z(cid:48)
1)

(9)

(10)

: Rn → Rn are differentiable
where σ1, σ2, µ1, µ2
mappings parameterized by neural nets and ◦ takes the
Hadamard or element-wise product. We partition the la-
tent variable by simply indexing the elements of the ﬁrst
half and the second half. The determinant of the combined
(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)det
transformation’s Jacobian,
(cid:12), can be easily eval-
uated. See section 6.4 of the Supplementary material for a
derivation. The lower bound of this approximation is the
same as Eqn. (5). We refer to this approximation as qF low.

(cid:16) ∂z(cid:48)
∂z

As the number of importance samples approaches inﬁnity,
the bound approaches the marginal log-likelihood.
It is
often used as an evaluation metric for generative models
(Burda et al., 2016; Kingma et al., 2016). AIS is potentially
an even tighter lower bound. AIS weights samples from
distributions which are sequentially annealed from an initial
proposal distribution to the true posterior. See Section 6.5
in the Supplementary material for further details regarding
AIS. To compute the AIS bound, we use 100 chains, each
with 10000 intermediate distributions, where each transition
consists of one HMC trajectory with 10 leapfrog steps. The
initial distribution for AIS is the prior, so that it is encoder-
independent.

We estimate the marginal log-likelihood by independently
computing our tightest lower bounds then take the maximum
of the two:

log ˆp(x) = max(LAIS, LIWAE).

We also experiment with an approximation that combines
ﬂow transformations and auxiliary variables. Let z ∈ Rn
be the variable of interest and v ∈ Rn the auxiliary variable.
The ﬂow is the same as equations (9) and (10), where z1 is

The L[q∗] and L[q] bounds are the standard ELBOs, LVAE,
from Eqn. (2), computed with either the amortized q or the
optimal q∗ (see below). When computing LVAE and LIWAE,
we use 5000 samples.

Inference Suboptimality in Variational Autoencoders

A

B

C

D

Figure 2. True Posterior and Approximate Distributions of a VAE with 2D latent space. The columns represent four different datapoints.
The green distributions are the true posterior distributions, highlighting the mismatch with the blue approximations. Amortized: Variational
parameters learned over the entire dataset. Optimal: Variational parameters optimized for each individual datapoint. Flow: Using a
ﬂexible approximate distribution.

3.4. Local Optimization of the Approximate

Distribution

To compute LVAE[q∗], we optimize the parameters of the
variational distribution for every datapoint. For the local op-
timization of qF F G, we initialize the mean and variance as
the prior, i.e. N (0, I). We optimize the mean and variance
using the Adam optimizer with a learning rate of 10−3. To
determine convergence, after every 100 optimization steps,
we compute the average of the previous 100 ELBO values
and compare it to the best achieved average. If it does not
improve for 10 consecutive iterations then the optimization
is terminated. For qF low and qAF , the same process is used
to optimize all of its parameters. All neural nets for the ﬂow
were initialized with a variant of the Xavier initilization
(Glorot & Bengio, 2010). We use 100 Monte Carlo samples
to compute the ELBO to reduce variance.

3.5. Validation of Bounds

The soundness of our empirical analysis depends on the
reliability of the marginal log-likelihood estimator. For
general importance sampling based estimators, the sample
variance of the normalized importance weights can serve as
an indicator of accuracy (Geweke, 1989; Neal, 2001). This
quantitative measure, however, can also be unreliable, e.g.
when the proposal misses an important mode of the target
distribution (Neal, 2001).

In this work, we follow (Wu et al., 2017) to empirically
validate our AIS estimates with Bidirectional Monte Carlo
(BDMC, Grosse et al. (2015; 2016)). In addition to a lower
bound provided by AIS, BDMC runs AIS chains backward
from exact posterior samples to obtain an upper bound on
the marginal log-likelihood. It should be noted that BDMC
relies on the assumption that the distribution of the simulated
data from the model roughly matches that of the real data.
This is due to the backward chain initializes from exact
posterior samples (Grosse et al., 2015).

For the MNIST and Fashion datasets, BDMC gives a gap
within 0.1 nat for a linear schedule AIS with 104 inter-
mediate distributions and 100 importance samples on 103
simulated datapoints. For 3-BIT CIFAR, the same AIS
setting gives a gap within 1 nat with the sigmoidial anneal-
ing schedule (Grosse et al., 2015) on 100 simulated data-
points. Loosely speaking, this should give us conﬁdence
in how well our AIS lower bounds reﬂect the marginal log-
likelihood computed on the real data.

4. Related Work

Much of the earlier work on variational inference focused
on optimizing the variational parameters locally for each
datapoint, e.g. the original Stochastic Variational Inference
scheme (SVI, Hoffman et al. (2013)). To scale inference to
large datasets, most related works utilize inference networks
to amortize the cost of inference over the entire dataset.

Inference Suboptimality in Variational Autoencoders

MNIST

3-BIT CIFAR

log ˆp(x)
LVAE[q∗
AF ]
LVAE[q∗
F F G]

LVAE[q]
Approximation
Amortization
Inference

qF F G
-89.80
-90.80
-91.23
-92.57
1.43
1.34
2.77

qAF
-88.94
-90.38
-113.54
-91.79
1.44
1.41
2.85

Fashion-MNIST
qAF
-97.41
-99.10
-132.46
-103.76
1.69
4.66
6.35

qF F G
-97.47
-98.92
-100.53
-104.75
3.06
4.22
7.28

qF F G
-816.9
-820.19
-831.65
-869.12
14.75
37.47
52.22

qAF
-820.56
-822.16
-861.62
-864.28
1.60
42.12
43.72

Table 2. Inference Gaps. The columns qF F G and qAF refer to the variational distribution used for training the model. These lower bounds
are computed on the training set and are in units of nats.

Our work analyses the error that these inference networks
introduce.

Most relevant to our work is the recent work of Krishnan
et al. (2017), which explicitly remarks on two sources of
error in variational learning with inference networks, and
proposes to optimize approximate inference locally from
an initialization output by the inference network. They
show improved training on high-dimensional, sparse data
with the hybrid method, claiming that local optimization
reduces the negative effects of random initialization in the
inference network early on in training. Thus their work
focuses on reducing the amortization gap early on in training.
Similar to this idea, Hoffman (2017) proposes to perform
approximate inference during model training with MCMC
at an initialization given by a variational distribution. Our
work provides a means of explaining these improvements
in terms of the sources of inference suboptimality that they
reduce.

5. Experimental Results

5.1. Intuition through Visualization

To begin, we would like to gain an intuitive visualization of
the gaps presented in Section 3.1. To this end, we trained
a VAE with a two-dimensional latent space on MNIST and
in Fig. 2 we show contour plots of various distributions
in the latent space. The ﬁrst row contains contour plots of
the true posteriors p(z|x) for four different training data-
points (columns). We have selected these four examples
to highlight different inference phenomena. The amortized
fully-factorized Gaussian (FFG) row refers to the output
of the recognition net, in this case, a FFG approximation.
Optimal FFG is the FFG that best ﬁts the posterior of the
datapoint. Optimal Flow is the optimal ﬁt of a ﬂexible distri-
bution to the same posterior, where the ﬂexible distribution
we use is described in Section 3.2.

Posterior A is an example of a distribution where a FFG can
ﬁt relatively well. Posterior B is an example of a posterior
with dependence between dimensions, demonstrating the

limitation of having a factorized approximation. Posterior C
highlights a shortcoming of performing amortization with a
limited-capacity recognition network, where the amortized
FFG shares little support with the true posterior. Posterior
D is a bi-modal distribution which demonstrates the ability
of the ﬂexible approximation to ﬁt to complex distributions,
in contrast to the simple FFG approximation. These obser-
vations raise the following question: in more typical VAEs,
is the amortization of inference the leading cause of the
distribution mismatch, or is it the limited expressiveness of
the approximation?

5.2. Amortization vs Approximation Gap

In this section, we compare how much the approximation
and amortization gaps each contribute to the total inference
gap. Table 2 are results of inference on the training set of
MNIST, Fashion-MNIST and 3-BIT CIFAR (a binarized
version of CIFAR-10, see Section 6.1.3 for details). For
each dataset, we trained models with two different approx-
imate posterior distributions: a fully-factorized Gaussian,
qF F G, and the ﬂexible distribution, qAF . Due to the com-
putational cost of optimizing the local parameters for each
datapoint, our evaluation is performed on a subset of 1000
datapoints for MNIST and Fashion-MNIST and a subset of
100 datapoints for 3-BIT CIFAR.

For MNIST, we see that the amortization and approximation
gaps each account for nearly half of the inference gap. On
the more difﬁcult Fashion-MNIST dataset, the amortization
gap is larger than the approximation gap. For CIFAR, we
see that the amortization gap is much more signiﬁcant com-
pared to the approximation gap. Thus, for the three datasets
and model architectures that we consider, the amortization
gap is likely to be the more prominent cause of inference
suboptimality, especially when the dataset becomes more
challenging to model. This indicates that improvements in
inference will likely be a result of reducing amortization
error, rather than approximation errors.

With these results in mind, would simply increasing the
capacity of the encoder improve the amortization gap? We

Inference Suboptimality in Variational Autoencoders

MNIST

MNIST

log ˆp(x)
LVAE[q∗
AF ]
LVAE[q∗
F F G]

LVAE[q]
Approximation
Amortization
Inference

qF F G
-89.61
-90.65
-91.07
-92.18
1.46
1.11
2.56

qAF
-88.99
-90.44
-108.71
-91.19
1.45
0.75
2.20

Fashion-MNIST
qAF
-96.18
-97.91
-129.70
-101.67
1.73
3.76
5.49

qF F G
-95.99
-97.40
-99.64
-102.73
3.65
3.09
6.74

qF F G
-89.82
-90.96
-90.84
-92.33
1.02
1.49
2.51

qAF
-89.52
-90.45
-92.25
-91.75
0.93
1.30
2.23

Fashion-MNIST
qAF
-102.88
-104.02
-105.80
-107.01
1.14
2.29
4.13

qF F G
-102.56
-103.73
-103.85
-106.90
1.29
3.05
4.34

Table 3. Left: Larger Encoder. Right: Models trained without entropy annealing. The columns qF F G and qAF refer to the variational
distribution used for training the model. The lower bounds are computed on the training set and are in units of nats.

examined this by training the MNIST and Fashion-MNIST
models from above but with larger encoders. See Section
6.1.2 for implementation details. Table 3 (left) are the results
of this experiment. Comparing to Table 2, we see that, for
both datasets and both variational distributions, using a
larger encoder results in the inference gap decreasing and
the decrease is mainly due to a reduction in the amortization
gap.

5.3. Inﬂuence of Flows on the Amortization Gap

The common reasoning for increasing the expressiveness
of the approximate posterior is to minimize the difference
between the true and approximate distributions, i.e. reduce
the approximation gap. However, given that the expressive
approximation is often accompanied by many additional
parameters, we would like to know how much inﬂuence it
has on the amortization error.

To investigate this, we trained a VAE on MNIST, discarded
the encoder, then retrained encoders with different approx-
imate distributions on the ﬁxed decoder. We ﬁxed the de-
coder so that the true posterior is constant for all the re-
trained encoders. The initial encoder was a two-layer MLP
with a factorized Gaussian distribution.
In order to em-
phasize a large amortization gap, the retrained encoders
had no hidden layers (ie. just linear transformations). For
the retraiend encoders, we tested three approximate distri-
butions: fully factorized Gaussian (qF F G), auxiliary ﬂow
(qAV ), and Flow (qF low). See Section 3.2 for the details of
these distributions.

The inference gaps of the retrained encoders on the training
set are shown in Table 4. As expected, we observe that the
small encoder with qF F G has a very large amortization gap.
However, when we use qAF or qF low as the approximate
distribution, we see the approximation gap decrease, but
more importantly, there is a signiﬁcant decrease in the amor-
tization gap. This indicates that the parameters used for
increasing the complexity of the approximation also play a
large role in diminishing the amortization error.

Variational Family
log ˆp(x)
LVAE[q∗]
LVAE[q]
Approximation
Amortization
Inference

qF F G
-84.70
-86.61
-129.83
1.91
43.22
45.13

qAF
-84.70
-85.48
-98.58
0.78
13.10
13.88

qF low
-84.70
-85.13
-97.99
0.43
12.86
13.29

Table 4. Inﬂuence of expressive approximations on the amortiza-
tion gap. The parameters used to increase the ﬂexibility of the
approximate distribution also reduce the amortization gap.

These results are expected given that the parameterization of
the Flow distribution can be interpreted as an instance of the
RevNet (Gomez et al., 2017) which has demonstrated that
Real-NVP transformations (Dinh et al., 2017) can model
complex functions similar to typical MLPs. Thus the ﬂow
transformations we employ should also be expected to in-
crease the expressiveness while also increasing the capacity
of the encoder. The implication of this observation is that
models which improve the ﬂexibility of their variational
approximation, and attribute their improved results to the
increased expressiveness, may have actually been due to the
reduction in amortization error.

5.4. Inﬂuence of Approximate Posterior on True

Posterior

To what extent does the posterior approximation affect the
learned model? Turner & Sahani (2011) studied the biases in
parameter learning induced by the variational approximation
when learning via variational Expectation-Maximization.
Similarly, we ask whether a factorized Gaussian approxima-
tion causes the true posterior to be more like a factorized
Gaussian? Burda et al. (2016) visually demonstrate that
when trained with an importance-weighted approximate pos-
terior, the resulting true posterior is more complex than those
trained with factorized Gaussian approximations. Just as it
is hard to evaluate a generative model by visually inspecting
samples, it is hard to judge how “Gaussian” the true poste-
rior is by visual inspection. We can quantitatively determine

Inference Suboptimality in Variational Autoencoders

Figure 3. Inference gaps over epochs trained on binarized Fashion-MNIST. Blue is the approximation gap. Orange is the amortization gap.
Standard is a VAE with FFG approximation. Flow is a VAE with a Flow approximation.

how close the posterior is to a fully-factorized Gaussian
(FFG) by comparing the marginal log-likelihood estimate
log ˆp(x) and the Optimal FFG bound LVAE[q∗
F F G]. This is
equivalent to estimating the KL divergence between the opti-
mal Gaussian and the true posterior, KL (q∗(z|x)||p(z|x)).

AF .

In Table 2 on MNIST, for the FFG trained model,
KL (q∗(z|x)||p(z|x)) is nearly the same for both q∗
F F G
and q∗
In contrast, on the model trained with qAF ,
KL (q∗(z|x)||p(z|x)) is much larger for q∗
AF .
This suggests that the true posterior of a FFG-trained model
is closer to FFG than the true posterior of the Flow-trained
model. The same observation can be made on the Fashion-
MNIST dataset. This implies that the decoder can learn to
have a true posterior that ﬁts better to the approximation.

F F G than q∗

These observations justify our results of Section 5.2. which
showed that the amortization error is often the main cause
of inference suboptimality. One reason for this is that the
generator accommodates the choice of approximation, thus
reducing the approximation error.

Generator Hidden Layers
log ˆp(x)

LVAE[q∗

F F G]

Approximation Gap

0
-100.52
-104.42
3.90

2
-84.78
-86.61
1.83

4
-82.19
-83.82
1.63

Table 5. Increased decoder capacity reduces the approximation
gap.

Given that we have seen that the generator can accommodate
the choice of approximation, our next question is whether a
generator with more capacity increases its ability to ﬁt to the
approximation. To this end, we trained VAEs with decoders
of different sizes and measured the approximation gaps on

the training set. Speciﬁcally, we trained decoders with 0, 2,
and 4 hidden layers on MNIST. See Table 5 for the results.
We see that as the capacity of the decoder increases, the ap-
proximation gap decreases. This result implies that the more
ﬂexible the generator is, the less ﬂexible the approximate
distribution needs to be to ensure accurate inference.

5.5. Inference Generalization

How well does amortized inference generalize at test time?
We address this question by visualizing the gaps on training
and validation datapoints across the training epochs. In
Fig. 3, the models are trained on 50000 binarized Fashion-
MNIST datapoints and the gaps are computed on a subset
of a 100 training and validation datapoints. The top and
bottom boundaries of the blue region represent log ˆp(x) and
L[q∗]. The bottom boundary of the orange region represents
L[q]. In other words, the blue region is the approximation
gap and the orange is the amortization gap.

In Fig. 3, the Standard model (top left) refers to a VAE of
latent size 20 trained with a factorized Gaussian approxi-
mate posterior. In this case, the encoder and decoder both
have two hidden layers each consisting of 200 hidden units.
The Flow model (top right) augments the Standard model
with a qF low variational distribution. Larger Decoder and
Larger Encoder models have factorized Gaussian distribu-
tions and increase the number of hidden layers to three and
the number of units in each layer to 500.

Firstly, we observe that for all models, the approximation
gap on the training and validation sets are roughly equivalent.
This indicates that the true posteriors of the held-out data are
similar to that of the training data. Secondly, we note that
for all models, the encoder overﬁts more than the decoder.

Inference Suboptimality in Variational Autoencoders

These observations resonate with the encoder overﬁtting
ﬁndings by Wu et al. (2017).

approximate distribution:

How does increasing decoder capacity affect inference on
held-out data? We know from Section 5.4 that increas-
ing generator capacity results in a posterior that better ﬁts
the approximation making posterior inference easier. Fur-
thermore, the Larger Decoder plot of Fig. 3 shows that
increasing generator capacity causes the model to be more
prone to overﬁtting. Thus, there is a tradeoff between ease
of inference and decoder overﬁtting.

5.5.1. ENCODER CAPACITY AND APPROXIMATION

EXPRESSIVENESS

We have seen in Sections 5.2 and 5.3 that expressive approx-
imations as well as increasing encoder capacity can lead to
a reduction in the amortization gap. This leads us to the fol-
lowing question: when should we increase encoder capacity
versus increasing the expressiveness of the approximation?
We answer this question in terms of how well each model
can generalize its efﬁcient inference (recognition network
and variational distribution) to held-out data.

In Fig. 3, we see that the Flow model and the Larger Encoder
model achieve similar log ˆp(x) on the validation set at the
end of training. However, we see that the L[q] bound of the
Larger Encoder model is signiﬁcantly lower than the L[q]
bound of the Flow model due to the encoder overﬁtting to
the training data. Although they both model the data nearly
equally well, the recognition net of the Larger Encoder
model is no longer suitable to perform inference on the
held-out data due to overﬁtting. Thus a potential rational
for utilizing expressive approximations is that they improve
generalization to held-out data in comparison to increasing
the encoder capacity.

We highlight that, in many scenarios, efﬁcient test time infer-
ence is not required and consequently, encoder overﬁtting
is not an issue, since we can use non-efﬁcient encoder-
independent methods to estimate log p(x), such as AIS,
IWAE with local optimization, or potentially retraining the
encoder on the held-out data. In contrast, when efﬁcient test
time inference is required, encoder generalization is impor-
tant and expressive approximations are likely advantageous.

5.6. Annealing the Entropy

Typical warm-up (Bowman et al., 2015; Sønderby et al.,
2016) refers to annealing the KL (q(z|x)||p(z)) term during
training. This can also be interpreted as performing maxi-
mum likelihood estimation (MLE) early on during training.
This optimization technique is known to help prevent the
latent variable from degrading to the prior (Burda et al.,
2016; Sønderby et al., 2016). We employ a similar anneal-
ing scheme during training by annealing the entropy of the

Ez∼q(z|x) [log p(x, z) − λ log q(z|x)] ,

where λ is annealed from 0 to 1 over training. This can be
interpreted as maximum a posteriori (MAP) in the initial
phase of training.

We ﬁnd that warm-up techniques, such as annealing the
entropy, are important for allowing the true posterior to be
more complex. Table 3 (right) are results from a model
trained without the entropy annealing schedule. Comparing
these results to Table 2, we observe that the difference be-
tween LVAE[q∗
AF ] is signiﬁcantly smaller
without entropy annealing. This indicates that the true pos-
terior is more Gaussian when entropy annealing is not used.
This suggests that, in addition to preventing the latent vari-
able from degrading to the prior, entropy annealing allows
the true posterior to better utilize the ﬂexibility of the ex-
pressive approximation.

F F G] and LVAE[q∗

6. Conclusion

In this paper, we investigated how encoder capacity, approx-
imation choice, decoder capacity, and model optimization
inﬂuence inference suboptimality in terms of the approxima-
tion and amortization gaps. We discovered that the amortiza-
tion gap can be a leading source to inference suboptimality
and that the generator can reduce the approximation gap by
learning a true posterior that ﬁts to the choice of approxima-
tion. We showed that the parameters used to increase the
expressiveness of the approximation play a role in general-
izing inference rather than simply improving the complexity
of the approximation. We conﬁrmed that increasing the
capacity of the encoder reduces the amortization error. Ad-
ditionally, we demonstrated that optimization techniques,
such as entropy annealing, help the generative model to
better utilize the ﬂexibility of expressive variational distri-
butions. Analyzing these gaps can be useful for guiding
improvements in VAEs. Future work includes evaluating
other types of expressive approximations, more complex
likelihood functions, and datasets.

References

Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Joze-
fowicz, R., and Bengio, S. Generating Sentences from a
Continuous Space. ArXiv e-prints, November 2015.

Burda, Y., Grosse, R., and Salakhutdinov, R. Importance

weighted autoencoders. In ICLR, 2016.

Clevert, Djork-Arn´e, Unterthiner, Thomas, and Hochre-
iter, Sepp.
Fast and accurate deep network learn-
ing by exponential linear units (elus). arXiv preprint
arXiv:1511.07289, 2015.

Inference Suboptimality in Variational Autoencoders

Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density estima-

tion using Real NVP. ICLR, 2017.

Geweke, John. Bayesian inference in econometric models
using monte carlo integration. Econometrica: Journal of
the Econometric Society, pp. 1317–1339, 1989.

Glorot, Xavier and Bengio, Yoshua. Understanding the
difﬁculty of training deep feedforward neural networks.
In Proceedings of the Thirteenth International Confer-
ence on Artiﬁcial Intelligence and Statistics, pp. 249–256,
2010.

Gomez, Aidan N, Ren, Mengye, Urtasun, Raquel, and
Grosse, Roger B. The reversible residual network: Back-
propagation without storing activations. In Advances in
Neural Information Processing Systems, pp. 2211–2221,
2017.

Grosse, R., Ghahramani, Z., and Adams, R. P. Sandwiching
the marginal likelihood using bidirectional monte carlo.
arXiv preprint arXiv:1511.02543, 2015.

Grosse, Roger B, Ancha, Siddharth, and Roy, Daniel M.
Measuring the reliability of mcmc inference with bidirec-
tional monte carlo. In Advances in Neural Information
Processing Systems, pp. 2451–2459, 2016.

Hoffman, Matthew D. Learning deep latent gaussian models
with markov chain monte carlo. In International Confer-
ence on Machine Learning, pp. 1510–1519, 2017.

Hoffman, Matthew D, Blei, David M, Wang, Chong, and
Paisley, John. Stochastic variational inference. The Jour-
nal of Machine Learning Research, 14(1):1303–1347,
2013.

Jarzynski, C. Nonequilibrium equality for free energy dif-
ferences. Physical Review Letters, 78(14):2690, 1997.

Kingma, Diederik and Ba, Jimmy. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.

Kingma, D.P. and Welling, M. Auto-Encoding Variational

Bayes. In ICLR, 2014.

Kingma, D.P., Salimans, T., Jozefowicz, R., Chen, X.,
Sutskever, I., and Welling, M.
Improving Variational
Inference with Inverse Autoregressive Flow. NIPS, 2016.

Krishnan, R. G., Liang, D., and Hoffman, M. On the chal-
lenges of learning with inference networks on sparse,
high-dimensional data. ArXiv e-prints, October 2017.

Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple
layers of features from tiny images. University of Toronto,
2009.

Larochelle, Hugo and Bengio, Yoshua. Classiﬁcation using
discriminative restricted boltzmann machines. In Pro-
ceedings of the 25th international conference on Machine
learning, pp. 536–543. ACM, 2008.

LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 1998.

Maaløe, L., Sønderby, CK., Sønderby, SK., and Winther, O.

Auxiliary Deep Generative Models. ICML, 2016.

Neal, R.M. Annealed importance sampling. Statistics and

Computing, 2001.

Ranganath, R., Tran, D., and Blei, D. M. Hierarchical

Variational Models. ICML, 2016.

Rezende, D.J. and Mohamed, S. Variational Inference with

Normalizing Flows. In ICML, 2015.

Rezende, D.J., Mohamed, S., and Wierstra, D. Stochastic
Backpropagation and Approximate Inference in Deep
Generative Models. ICML, 2014.

Salakhutdinov, R. and Murray, I. On the quantitative analy-
sis of deep belief networks. In Proceedings of the 25th
international conference on Machine learning, pp. 872–
879. ACM, 2008.

Salimans, T., Kingma, D.P., and Welling, M. Markov chain
monte carlo and variational inference: Bridging the gap.
In ICML, 2015.

Sønderby, Casper Kaae, Raiko, Tapani, Maaløe, Lars,
Sønderby, Søren Kaae, and Winther, Ole. Ladder varia-
tional autoencoders. In Advances in Neural Information
Processing Systems, pp. 3738–3746, 2016.

Tomczak, J. M. and Welling, M.

Improving Variational
Auto-Encoders using Householder Flow. ArXiv e-prints,
November 2016.

Tomczak, J. M. and Welling, M.

Improving Variational
Auto-Encoders using convex combination linear Inverse
Autoregressive Flow. ArXiv e-prints, June 2017.

Turner, R and Sahani, M. Two problems with variational ex-
pectation maximisation for time-series models. inference
and learning in dynamic models. Cambridge University
Press, pp. 104–123, 2011.

Wu, Y., Burda, Y., Salakhutdinov, R., and Grosse, R. On
the Quantitative Analysis of Decoder-Based Generative
Models. ICLR, 2017.

Xiao, Han, Rasul, Kashif, and Vollgraf, Roland. Fashion-
mnist: a novel image dataset for benchmarking machine
learning algorithms. github.com/zalandoresearch/fashion-
mnist, 2017.

Supplementary

Code for the experiments in this paper can be found at:
https://github.com/chriscremer/Inference-Suboptimality and
https://github.com/lxuechen/inference-suboptimality.

6.1.3. 3-BIT CIFAR

6.1. Model Architectures and Training

Hyperparameters

6.1.1. 2D VISUALIZATION

The VAE model of Fig. 2 uses a decoder p(x|z) with ar-
chitecture: 2 − 100 − 784, and an encoder q(z|x) with
architecture: 784 − 100 − 4. We use tanh activations and a
batch size of 50. The model is trained for 3000 epochs with
a learning rate of 10−4 using the ADAM optimizer (Kingma
& Ba, 2014).

6.1.2. MNIST & FASHION-MNIST

Both MNIST and Fashion-MNIST consist of a training and
test set with 60000 and 10000 datapoints respectively, where
each datapoint is a 28x28 grey-scale image. We rescale the
original images so that pixel values are within the range
[0, 1]. For MNIST, We use the statically binarized version
described by (Larochelle & Bengio, 2008). We also binarize
Fashion-MINST statically. For both datasets, we adopt the
Bernoulli likelihood for the generator.

The VAE models for MNIST and Fashion-MNIST exper-
iments have the same architecture. The encoder has two
hidden layers with 200 units each. The activation function is
chosen to be the exponential linear unit (ELU, Clevert et al.
(2015)), as we observe improved performance compared to
tanh. The latent space has 50 dimensions. The generator
is the reverse of the encoder. We follow the same learning
rate schedule and train for the same amount of epochs as
described by (Burda et al., 2016). All models are trained
with the a batch-size of 100 with ADAM.

In the large encoder setting, we change the number of hidden
units for the inference network to be 500, instead of 200.
The warm-up models are trained with a linear schedule over
the ﬁrst 400 epochs according to Section 5.6.

The auxiliary variable of requires a couple distributions:
q(v0|z0) and r(vT |zT ). These distributions are both factor-
ized Gaussians which are parameterized by MLP’s with two
hidden layers, 100 units each, with ELU activations.

The ﬂow transformation q(zt+1, vt+1|zt, vt) involves func-
tions σ1, σ2, µ1, and µ2 from Eqn. 9 and 10. These also
have two hidden layers with 100 units each and ELU units.

CIFAR-10 consists of a training and test dataset with 50000
and 10000 datapoints respectively, where each datapoint is
a 32 × 32 RGB image. We rescale individual pixel values
to be in the range [0, 1]. We then statically binarize the
scaled pixel values by setting individual pixel values of
channels to 1 if the rescaled value is greater than 0.5 and
0 otherwise. In this manner, we can model the observation
with a factorized Bernoulli likelihood. We call this binarized
CIFAR-10 dataset as 3-BIT CIFAR, since 3 bits are required
to encode each pixel, where 1 bit is needed for each of the
channels. We acknowledge that such binarization scheme
may reduce the complexity of the original problem, since
originally 24 bits were required to encode a single pixel.
Nevertheless, the 3-bit CIFAR dataset is still much more
challenging compared MNIST and Fashion. This is because
784 bits are required to encode one MNIST/Fashion image,
whereas for one 3-bit CIFAR image, 3072 bits are required.
Most notably, we were able to validate our AIS estimates
using BDMC with the simpliﬁed dataset. This, however,
was not achievable in any reasonable amount of time with
the original CIFAR-10 dataset.

For the latent variable, we use a 50-dimensional factorized
Gaussian for q(z|x). For all neural networks, ELU is cho-
sen to be the activation function. The inference network
consists of three 4 by 4 convolution layers with stride 2,
batch-norm, and 64, 128, 256 channels respectively. Then a
fully-connected layer outputs the 50-dimensional mean and
log-variance of the latent variable. Similarly, the generator
consists of a fully-connected layer outputting 256 by 2 by 2
tensors. Then three deconvolutional layers each with 4 by
4 ﬁlters, stride 2, batch-norm, and 128, 64, and 3 channels
respectively. For the model with expressive inference, we
use three normalizing ﬂow steps, where the parametric func-
tions in the ﬂow and auxiliary variable distribution also take
in a hidden layer of the encoder.

We use a learning rate of 10−3. Warm-up is applied with
a linear schedule over the ﬁrst 50 epochs. All models are
trained with a batch-size of 100 with ADAM. Early-stopping
is applied based on the performance computed with the
IWAE bound (k=1000) on the held-out set of 5000 examples
from the original training set.

Inference Suboptimality in Variational Autoencoders

6.2. Inference Generalization

These models are trained with batch size 50 and latent di-
mension size of 20. The rest of the hyperparameters are
equivalent to Section 6.1.2.

Architecture of qF low: The ﬂow transformation involves
functions σ1, σ2, µ1, and µ2 from Eqn. 9 and 10. Each
function is an MLP with a 50 unit hidden layer and ELU
activations. We apply this ﬂow transformation twice.

Fig. 4 are the plots for the qAF model. The transformations
are the same as qF low, but rather than partitioning the latent
variable, we introduce an auxiliary variable. The auxiliary
variable also requires a reverse model r(v|z) which is a
factorized Gaussian parameterized by an MLP with a 50
unit hidden layer and ELU activations.

Comparing AF in Fig. 4 to Flow in Fig. 3, we see that the
AF has a larger approximation gap. This increase is likely
due to the KL (q(v|z, x)(cid:107)r(v|x, z)) term of the auxiliary
variable lower bound from 2.2.2. This motivates also using
expressive approximations for the reverse model r(v|z).

Figure 4. Gaps over epochs of the AF (auxiliary ﬂow) model.

6.3. Inﬂuence of Flows On Amortization Gap

Experiment

The aim of this experiment is to show that the parameters
used for increasing the expressiveness of the approxima-
tion also contribute to reducing the amortization error. To
show this, we train a VAE on MNIST, discard the encoder,
then retrain two encoders on the ﬁxed decoder: one with a
factorized Gaussian distribution and the other with a param-
eterized ’ﬂow’ distribution. We use ﬁxed decoder so that
the true posterior is constant for both encoders. See 5.3 for
the results and below for the architecture details.

The architecture of the decoder is: DZ − 200 − 200 − DX .
The architecture of the encoder used to train the decoder
is DX − 200 − 200 − 2DZ. The approximate distribution
q(z|x) is a factorized Gaussian.

Next, we describe the encoders which were trained on the
ﬁxed trained decoder. In order to highlight a large amorti-
zation gap, we employed a very small encoder architecture:
DX − 2DZ. This encoder has no hidden layers, which

greatly impoverishes its ability and results in a large amorti-
zation gap.

We compare two approximate distributions q(z|x). Firstly,
we experiment with the typical fully factorized Gaussian
(FFG). The second is what we call a ﬂow distribution.
Speciﬁcally, we use the transformations of (Dinh et al.,
2017). We also include an auxiliary variable so we don’t
need to select how to divide the latent space for the trans-
formations. The approximate distribution over the la-
tent z and auxiliary variable v factorizes as: q(z, v|x) =
q(z|x)q(v). The q(v) distribution is simply a N(0,1) dis-
tribution. Since we’re using a auxiliary variable, we also
require the r(v|z) distribution which we parameterize as
r(v|z): [DZ] − 50 − 50 − 2DZ. The ﬂow transformation
is the same as in Section 3.2, which we apply twice.

6.4. Computation of the Determinant for Flow

The overall mapping f that performs (z, v) (cid:55)→ (z(cid:48), v(cid:48)) is
the composition of two sheer mappings f1 and f2 that re-
spectively perform (z, v) (cid:55)→ (z, v(cid:48)) and (z, v(cid:48)) (cid:55)→ (z(cid:48), v(cid:48)).
Since the Jacobian of either one of the sheer mappings is
diagonal, the determinant of the composed transformation’s
Jacobian Df can be easily computed:

det(Df ) = det(Df1)det(Df2)

=

(cid:16) n
(cid:89)

i=1

(cid:17)(cid:16) n
(cid:89)

σ2(v(cid:48))j

(cid:17)

.

σ1(z)i

j=1

6.5. Annealed Importance Sampling

Annealed importance sampling (AIS, Neal (2001); Jarzyn-
ski (1997)) is a means of computing a lower bound to
the marginal log-likelihood. Similarly to the importance
weighted bound, AIS must sample a proposal distribution
f1(z) and compute the density of these samples, however,
AIS then transforms the samples through a sequence of
reversible transitions Tt(z(cid:48)|z). The transitions anneal the
proposal distribution to the desired distribution fT (z).

Speciﬁcally, AIS samples an initial state z1 ∼ f1(z) and
sets an initial weight w1 = 1. For the following annealing
steps, zt is sampled from Tt(z(cid:48)|z) and the weight is updated
according to:

wt = wt−1

ft(zt−1)
ft−1(zt−1)

.

This procedure produces weight wT such that E [wT ] =
ZT /Z1, where ZT and Z1 are the normalizing constants of
fT (z) and f1(z) respectively. This pertains to estimating the
marginal likelihood when the target distribution is p(x, z)
when we integrate with respect to z.

Typically, the intermediate distributions are simply deﬁned
to be geometric averages: ft(z) = f1(z)1−βtfT (z)βt,

Inference Suboptimality in Variational Autoencoders

where βt is monotonically increasing with β1 = 0 and βT = 1.
When f1(z) = p(z) and fT (z) = p(x, z), the intermediate
distributions are: fi(x) = p(z)p(x|z)βi .

Model evaluation with AIS appears early on in the setting
of deep belief networks (Salakhutdinov & Murray, 2008).
AIS for decoder-based models was also used by Wu et al.
(2017).

6.6. Extra MNIST Inference Gaps

To demonstrate that a very small inference gap can be
achieved, even with a limited approximation such as a fac-
torized Gaussian, we train the model on a small dataset. In
this experiment, our training set consists of 1000 datapoints
randomly chosen from the original MNIST training set. The
training curves on this small datatset are shown in Fig. 5.
Even with a factorized Gaussian distribution, the inference
gap is very small: the AIS and IWAE bounds are overlap-
ping and the VAE is just slightly below. Yet, the model is
overﬁtting as seen by the decreasing test set bounds.

Figure 5. Training curves for a FFG and a Flow inference model on
MNIST. AIS provides the tightest lower bound and is independent
of encoder overﬁtting. There is little difference between FFG and
Flow models trained on the 1000 datapoints since inference is
nearly equivalent.

Inference Suboptimality in Variational Autoencoders

Chris Cremer 1 Xuechen Li 1 David Duvenaud 1

8
1
0
2
 
y
a
M
 
7
2
 
 
]

G
L
.
s
c
[
 
 
3
v
8
5
5
3
0
.
1
0
8
1
:
v
i
X
r
a

Abstract
Amortized inference allows latent-variable mod-
els trained via variational learning to scale to large
datasets. The quality of approximate inference is
determined by two factors: a) the capacity of the
variational distribution to match the true poste-
rior and b) the ability of the recognition network
to produce good variational parameters for each
datapoint. We examine approximate inference in
variational autoencoders in terms of these factors.
We ﬁnd that divergence from the true posterior
is often due to imperfect recognition networks,
rather than the limited complexity of the approx-
imating distribution. We show that this is due
partly to the generator learning to accommodate
the choice of approximation. Furthermore, we
show that the parameters used to increase the ex-
pressiveness of the approximation play a role in
generalizing inference rather than simply improv-
ing the complexity of the approximation.

1. Introduction

In this paper, we analyze inference suboptimality: the mis-
match between the true and approximate posterior. More
speciﬁcally, we are interested in understanding what factors
cause the gap between the marginal log-likelihood and the
evidence lower bound (ELBO) in variational autoencoders
(VAEs, Kingma & Welling (2014); Rezende et al. (2014)).
We refer to this as the inference gap. Moreover, we break
down the inference gap into two components: the approxi-
mation gap and the amortization gap. The approximation
gap comes from the inability of the variational distribution
family to exactly match the true posterior. The amortiza-
tion gap refers to the difference caused by amortizing the
variational parameters over the entire training set, instead of
optimizing for each training example individually. We refer
the reader to Table 1 for the deﬁnitions of the gaps and to

1Department of Computer Science, University of Toronto,
Toronto, Canada. Correspondence to: Chris Cremer <ccre-
mer@cs.toronto.edu>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

Fig. 1 for a simple illustration of the gaps. In Fig. 1, L[q]
refers to the ELBO evaluated using an amortized distribu-
tion q, as is typical of VAE training. In contrast, L[q∗] is the
ELBO evaluated using the optimal approximation within its
variational family.

There has been signiﬁcant work on improving variational
inference in VAEs through the development of expressive ap-
proximate posteriors (Rezende & Mohamed, 2015; Kingma
et al., 2016; Ranganath et al., 2016; Tomczak & Welling,
2016; 2017). These works have shown that with more ex-
pressive approximate posteriors, the model learns a better
distribution over the data. Our study aims to gain a bet-
ter understanding of the relationship between expressive
approximations and improved generative models.

Our experiments investigate how the choice of encoder,
posterior approximation, decoder, and optimization affect
the approximation and amortization gaps. We train VAE
models in a number of settings on the MNIST (LeCun et al.,
1998), Fashion-MNIST (Xiao et al., 2017), and CIFAR-10
(Krizhevsky & Hinton, 2009) datasets.

Our contributions are: a) we investigate inference subopti-
mality in terms of the approximation and amortization gaps,
providing insight to guide future improvements in VAE in-
ference, b) we quantitatively demonstrate that the learned
generative model accommodates the choice of approxima-
tion, and c) we demonstrate that parameterized functions
that improve the expressiveness of the approximation play a
signiﬁcant role in reducing amortization error.

Figure 1. Gaps in Inference

Inference Suboptimality in Variational Autoencoders

Term
Inference
Approximation
Amortization

Deﬁnition
log p(x) − L[q]
log p(x) − L[q∗]
L[q∗] − L[q]

VAE Formulation
KL (q(z|x)||p(z|x))
KL (q∗(z|x)||p(z|x))
KL (q(z|x)||p(z|x)) − KL (q∗(z|x)||p(z|x))

Table 1. Summary of Gap Terms. The middle column refers to the general case where our variational objective is a lower bound on the
marginal log-likelihood. The right most column demonstrates the speciﬁc case in VAEs. q∗(z|x) refers to the optimal approximation
within a family Q, i.e. q∗(z|x) = arg minq∈Q KL (q(z|x)||p(z|x)).

2. Background

2.1. Inference in Variational Autoencoders

Let x be the observed variable, z the latent variable, and
p(x, z) be their joint distribution. Given a dataset X =
{x1, x2, ..., xN }, we would like to maximize the marginal
log-likelihood with respect to the model parameters θ:

log pθ(X) =

log pθ(xi) =

log

pθ(xi, zi)dzi.

N
(cid:88)

i=1

(cid:90)

N
(cid:88)

i=1

In practice, the marginal log-likelihood is computationally
intractable due to the integration over the latent variable
z. Instead, VAEs introduce an inference network qφ(z|x)
to approximate the true posterior p(z|x) and optimize the
ELBO with respect to model parameters θ and inference
network parameters φ (parameterization subscripts omitted
for brevity):

log p(x) = Eq(z|x)

log

+ KL (q(z|x)||p(z|x))

(cid:20)

(cid:20)

(cid:19)(cid:21)

(cid:18) p(x, z)
q(z|x)

(cid:19)(cid:21)

(cid:18) p(x, z)
q(z|x)

≥ Eq(z|x)

log

= LVAE[q].

From the above equation, we see that the ELBO is tight
when q(z|x) = p(z|x). The choice of q(z|x) is often a
factorized Gaussian distribution for its simplicity and efﬁ-
ciency. By utilizing the inference network (also referred to
as encoder or recognition network), VAEs amortize infer-
ence over the entire dataset. Furthermore, the overall model
is trained by stochastically optimizing the ELBO using the
reparametrization trick (Kingma & Welling, 2014).

2.2. Expressive Approximate Posteriors

There are a number of strategies for increasing the expres-
siveness of approximate posteriors, going beyond the origi-
nal factorized-Gaussian. We brieﬂy summarize normalizing
ﬂows and auxiliary variables.

2.2.1. NORMALIZING FLOWS

Normalizing ﬂow (Rezende & Mohamed, 2015) is a change
of variables procedure for constructing complex distribu-
tions by transforming probability densities through a se-
ries of invertible mappings. Speciﬁcally, if we transform

a random variable z0 with distribution q0(z), the resulting
random variable zT = T (z0) has a distribution:

qT (zT ) = q0(z0)

det

(3)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂zT
∂z0

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−1

.

By successively applying these transformations, we can
build arbitrarily complex distributions. Stacking these trans-
formations remains tractable due to the determinant being
decomposable: det(AB) = det(A)det(B). An important
property of these transformations is that we can take ex-
pectations with respect to the transformed density qT (zT )
without explicitly knowing its formula due to the law of the
unconscious statistician (LOTUS):

EqT [h(zT )] = Eq0[h(fT (fT −1(...f1(z0))))].

(4)

Using equations (3) and (4), the lower bound with the trans-
formed approximation can be written as:





Ez0∼q0(z|x)


log




q0(z0|x) (cid:81)T

p(x, zT )
(cid:12)
(cid:12)det ∂zt
(cid:12)

t=1

∂zt−1

(cid:12)
(cid:12)
(cid:12)

(1)

(2)









 .

−1

(5)

The main constraint on these transformations is that the
determinant of their Jacobian needs to be easily computable.

2.2.2. AUXILIARY VARIABLES

Deep generative models can be extended with auxiliary vari-
ables which leave the generative model unchanged but make
the variational distribution more expressive. Just as hierar-
chical Bayesian models induce dependencies between data,
hierarchical variational models can induce dependencies be-
tween latent variables. The addition of the auxiliary variable
changes the lower bound to:

(cid:20)

Ez,v∼q(z,v|x)

log

(cid:20)

= Eq(z|x)

log

(cid:18) p(x, z)
q(z|x)

(cid:19)(cid:21)

(cid:18) p(x, z)r(v|x, z)
q(z, v|x)
(cid:19)

(cid:16)

− KL

q(v|z, x)(cid:107)r(v|x, z)

(6)

(cid:17)(cid:21)

(7)

where r(v|x, z) is called the reverse model. From Eqn. 7,
we see that this bound is looser than the regular ELBO,

Inference Suboptimality in Variational Autoencoders

however the extra ﬂexibility provided by the auxiliary vari-
able can result in a higher lower bound. This idea has been
employed in works such as auxiliary deep generative mod-
els (ADGM, (Maaløe et al., 2016)), hierarchical variational
models (HVM, (Ranganath et al., 2016)) and Hamiltonian
variational inference (HVI, (Salimans et al., 2015)).

3. Methods

3.1. Approximation and Amortization Gaps

The inference gap G is the difference between the marginal
log-likelihood log p(x) and a lower bound L[q]. Given
the distribution in the family that maximizes the bound,
q∗(z|x) = arg maxq∈Q L[q], the inference gap decomposes
as the sum of approximation and amortization gaps:

G = log p(x) − L[q] = log p(x) − L[q∗]
(cid:125)

(cid:124)

(cid:123)(cid:122)
Approximation

+ L[q∗] − L[q]
(cid:125)
(cid:123)(cid:122)
Amortization

(cid:124)

.

For VAEs, we can translate the gaps to KL divergences by
rearranging Eqn. (1):

(cid:124)

GVAE = KL(cid:0)q∗(z|x)||p(z|x)(cid:1)
(cid:125)
+ KL(cid:0)q(z|x)||p(z|x)(cid:1) − KL(cid:0)q∗(z|x)||p(z|x)(cid:1)
(cid:123)(cid:122)
(cid:125)
Amortization

(cid:123)(cid:122)
Approximation

(cid:124)

.

replaced with z and z2 with v. We refer to this approximate
distribution as qAF , where AF stands for auxiliary ﬂow. We
train this model by optimizing the following bound:





Eq0(z,v|x)


log




= L[qAF ].

p(x, zT )r(vT |x, zT )
(cid:16) ∂ztvt

(cid:12)
(cid:12)
(cid:12)det

∂zt−1vt−1

−1

(cid:17)(cid:12)
(cid:12)
(cid:12)

qT (zT , vT |x)











(11)

Note that this lower bound is looser as explained in Section
2.2.2. We refer readers to Section 6.1.2 in the Supplemen-
tary material for speciﬁc details of the ﬂow conﬁguration
adopted in the experiments.

3.3. Marginal Log-Likelihood Estimation and Evidence

Lower Bounds

In this section, we describe the estimates we use to compute
the bounds of the inference gaps: log p(x), L[q∗], and L[q].
We use two bounds to estimate the marginal log-likelihood,
log p(x): IWAE (Burda et al., 2016) and AIS (Neal, 2001).

The IWAE bound takes multiple importance weighted sam-
ples from the variational q distribution resulting in a tighter
lower bound than the VAE bound. The IWAE bound is
computed as:

(8)

log p(x) ≥ Ez1...zk∼q(z|x)

log

(cid:34)

(cid:32)

1
k

k
(cid:88)

i=1

p(x, zi)
q(zi|x)

(cid:33)(cid:35)

(12)

3.2. Flexible Approximate Posteriors

= LIWAE[q].

Our experiments involve expressive approximations which
use ﬂow transformations and auxiliary variables. The ﬂow
transformation that we employ is of the same type as the
transformations of Real NVP (Dinh et al., 2017). We parti-
tion the latent variable z into two, z1 and z2, then perform
the following transformations:

z(cid:48)
1 = z1 ◦ σ1(z2) + µ1(z2)
1) + µ2(z(cid:48)
2 = z2 ◦ σ2(z(cid:48)
z(cid:48)
1)

(9)

(10)

: Rn → Rn are differentiable
where σ1, σ2, µ1, µ2
mappings parameterized by neural nets and ◦ takes the
Hadamard or element-wise product. We partition the la-
tent variable by simply indexing the elements of the ﬁrst
half and the second half. The determinant of the combined
(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)det
transformation’s Jacobian,
(cid:12), can be easily eval-
uated. See section 6.4 of the Supplementary material for a
derivation. The lower bound of this approximation is the
same as Eqn. (5). We refer to this approximation as qF low.

(cid:16) ∂z(cid:48)
∂z

As the number of importance samples approaches inﬁnity,
the bound approaches the marginal log-likelihood.
It is
often used as an evaluation metric for generative models
(Burda et al., 2016; Kingma et al., 2016). AIS is potentially
an even tighter lower bound. AIS weights samples from
distributions which are sequentially annealed from an initial
proposal distribution to the true posterior. See Section 6.5
in the Supplementary material for further details regarding
AIS. To compute the AIS bound, we use 100 chains, each
with 10000 intermediate distributions, where each transition
consists of one HMC trajectory with 10 leapfrog steps. The
initial distribution for AIS is the prior, so that it is encoder-
independent.

We estimate the marginal log-likelihood by independently
computing our tightest lower bounds then take the maximum
of the two:

log ˆp(x) = max(LAIS, LIWAE).

We also experiment with an approximation that combines
ﬂow transformations and auxiliary variables. Let z ∈ Rn
be the variable of interest and v ∈ Rn the auxiliary variable.
The ﬂow is the same as equations (9) and (10), where z1 is

The L[q∗] and L[q] bounds are the standard ELBOs, LVAE,
from Eqn. (2), computed with either the amortized q or the
optimal q∗ (see below). When computing LVAE and LIWAE,
we use 5000 samples.

Inference Suboptimality in Variational Autoencoders

A

B

C

D

Figure 2. True Posterior and Approximate Distributions of a VAE with 2D latent space. The columns represent four different datapoints.
The green distributions are the true posterior distributions, highlighting the mismatch with the blue approximations. Amortized: Variational
parameters learned over the entire dataset. Optimal: Variational parameters optimized for each individual datapoint. Flow: Using a
ﬂexible approximate distribution.

3.4. Local Optimization of the Approximate

Distribution

To compute LVAE[q∗], we optimize the parameters of the
variational distribution for every datapoint. For the local op-
timization of qF F G, we initialize the mean and variance as
the prior, i.e. N (0, I). We optimize the mean and variance
using the Adam optimizer with a learning rate of 10−3. To
determine convergence, after every 100 optimization steps,
we compute the average of the previous 100 ELBO values
and compare it to the best achieved average. If it does not
improve for 10 consecutive iterations then the optimization
is terminated. For qF low and qAF , the same process is used
to optimize all of its parameters. All neural nets for the ﬂow
were initialized with a variant of the Xavier initilization
(Glorot & Bengio, 2010). We use 100 Monte Carlo samples
to compute the ELBO to reduce variance.

3.5. Validation of Bounds

The soundness of our empirical analysis depends on the
reliability of the marginal log-likelihood estimator. For
general importance sampling based estimators, the sample
variance of the normalized importance weights can serve as
an indicator of accuracy (Geweke, 1989; Neal, 2001). This
quantitative measure, however, can also be unreliable, e.g.
when the proposal misses an important mode of the target
distribution (Neal, 2001).

In this work, we follow (Wu et al., 2017) to empirically
validate our AIS estimates with Bidirectional Monte Carlo
(BDMC, Grosse et al. (2015; 2016)). In addition to a lower
bound provided by AIS, BDMC runs AIS chains backward
from exact posterior samples to obtain an upper bound on
the marginal log-likelihood. It should be noted that BDMC
relies on the assumption that the distribution of the simulated
data from the model roughly matches that of the real data.
This is due to the backward chain initializes from exact
posterior samples (Grosse et al., 2015).

For the MNIST and Fashion datasets, BDMC gives a gap
within 0.1 nat for a linear schedule AIS with 104 inter-
mediate distributions and 100 importance samples on 103
simulated datapoints. For 3-BIT CIFAR, the same AIS
setting gives a gap within 1 nat with the sigmoidial anneal-
ing schedule (Grosse et al., 2015) on 100 simulated data-
points. Loosely speaking, this should give us conﬁdence
in how well our AIS lower bounds reﬂect the marginal log-
likelihood computed on the real data.

4. Related Work

Much of the earlier work on variational inference focused
on optimizing the variational parameters locally for each
datapoint, e.g. the original Stochastic Variational Inference
scheme (SVI, Hoffman et al. (2013)). To scale inference to
large datasets, most related works utilize inference networks
to amortize the cost of inference over the entire dataset.

Inference Suboptimality in Variational Autoencoders

MNIST

3-BIT CIFAR

log ˆp(x)
LVAE[q∗
AF ]
LVAE[q∗
F F G]

LVAE[q]
Approximation
Amortization
Inference

qF F G
-89.80
-90.80
-91.23
-92.57
1.43
1.34
2.77

qAF
-88.94
-90.38
-113.54
-91.79
1.44
1.41
2.85

Fashion-MNIST
qAF
-97.41
-99.10
-132.46
-103.76
1.69
4.66
6.35

qF F G
-97.47
-98.92
-100.53
-104.75
3.06
4.22
7.28

qF F G
-816.9
-820.19
-831.65
-869.12
14.75
37.47
52.22

qAF
-820.56
-822.16
-861.62
-864.28
1.60
42.12
43.72

Table 2. Inference Gaps. The columns qF F G and qAF refer to the variational distribution used for training the model. These lower bounds
are computed on the training set and are in units of nats.

Our work analyses the error that these inference networks
introduce.

Most relevant to our work is the recent work of Krishnan
et al. (2017), which explicitly remarks on two sources of
error in variational learning with inference networks, and
proposes to optimize approximate inference locally from
an initialization output by the inference network. They
show improved training on high-dimensional, sparse data
with the hybrid method, claiming that local optimization
reduces the negative effects of random initialization in the
inference network early on in training. Thus their work
focuses on reducing the amortization gap early on in training.
Similar to this idea, Hoffman (2017) proposes to perform
approximate inference during model training with MCMC
at an initialization given by a variational distribution. Our
work provides a means of explaining these improvements
in terms of the sources of inference suboptimality that they
reduce.

5. Experimental Results

5.1. Intuition through Visualization

To begin, we would like to gain an intuitive visualization of
the gaps presented in Section 3.1. To this end, we trained
a VAE with a two-dimensional latent space on MNIST and
in Fig. 2 we show contour plots of various distributions
in the latent space. The ﬁrst row contains contour plots of
the true posteriors p(z|x) for four different training data-
points (columns). We have selected these four examples
to highlight different inference phenomena. The amortized
fully-factorized Gaussian (FFG) row refers to the output
of the recognition net, in this case, a FFG approximation.
Optimal FFG is the FFG that best ﬁts the posterior of the
datapoint. Optimal Flow is the optimal ﬁt of a ﬂexible distri-
bution to the same posterior, where the ﬂexible distribution
we use is described in Section 3.2.

Posterior A is an example of a distribution where a FFG can
ﬁt relatively well. Posterior B is an example of a posterior
with dependence between dimensions, demonstrating the

limitation of having a factorized approximation. Posterior C
highlights a shortcoming of performing amortization with a
limited-capacity recognition network, where the amortized
FFG shares little support with the true posterior. Posterior
D is a bi-modal distribution which demonstrates the ability
of the ﬂexible approximation to ﬁt to complex distributions,
in contrast to the simple FFG approximation. These obser-
vations raise the following question: in more typical VAEs,
is the amortization of inference the leading cause of the
distribution mismatch, or is it the limited expressiveness of
the approximation?

5.2. Amortization vs Approximation Gap

In this section, we compare how much the approximation
and amortization gaps each contribute to the total inference
gap. Table 2 are results of inference on the training set of
MNIST, Fashion-MNIST and 3-BIT CIFAR (a binarized
version of CIFAR-10, see Section 6.1.3 for details). For
each dataset, we trained models with two different approx-
imate posterior distributions: a fully-factorized Gaussian,
qF F G, and the ﬂexible distribution, qAF . Due to the com-
putational cost of optimizing the local parameters for each
datapoint, our evaluation is performed on a subset of 1000
datapoints for MNIST and Fashion-MNIST and a subset of
100 datapoints for 3-BIT CIFAR.

For MNIST, we see that the amortization and approximation
gaps each account for nearly half of the inference gap. On
the more difﬁcult Fashion-MNIST dataset, the amortization
gap is larger than the approximation gap. For CIFAR, we
see that the amortization gap is much more signiﬁcant com-
pared to the approximation gap. Thus, for the three datasets
and model architectures that we consider, the amortization
gap is likely to be the more prominent cause of inference
suboptimality, especially when the dataset becomes more
challenging to model. This indicates that improvements in
inference will likely be a result of reducing amortization
error, rather than approximation errors.

With these results in mind, would simply increasing the
capacity of the encoder improve the amortization gap? We

Inference Suboptimality in Variational Autoencoders

MNIST

MNIST

log ˆp(x)
LVAE[q∗
AF ]
LVAE[q∗
F F G]

LVAE[q]
Approximation
Amortization
Inference

qF F G
-89.61
-90.65
-91.07
-92.18
1.46
1.11
2.56

qAF
-88.99
-90.44
-108.71
-91.19
1.45
0.75
2.20

Fashion-MNIST
qAF
-96.18
-97.91
-129.70
-101.67
1.73
3.76
5.49

qF F G
-95.99
-97.40
-99.64
-102.73
3.65
3.09
6.74

qF F G
-89.82
-90.96
-90.84
-92.33
1.02
1.49
2.51

qAF
-89.52
-90.45
-92.25
-91.75
0.93
1.30
2.23

Fashion-MNIST
qAF
-102.88
-104.02
-105.80
-107.01
1.14
2.29
4.13

qF F G
-102.56
-103.73
-103.85
-106.90
1.29
3.05
4.34

Table 3. Left: Larger Encoder. Right: Models trained without entropy annealing. The columns qF F G and qAF refer to the variational
distribution used for training the model. The lower bounds are computed on the training set and are in units of nats.

examined this by training the MNIST and Fashion-MNIST
models from above but with larger encoders. See Section
6.1.2 for implementation details. Table 3 (left) are the results
of this experiment. Comparing to Table 2, we see that, for
both datasets and both variational distributions, using a
larger encoder results in the inference gap decreasing and
the decrease is mainly due to a reduction in the amortization
gap.

5.3. Inﬂuence of Flows on the Amortization Gap

The common reasoning for increasing the expressiveness
of the approximate posterior is to minimize the difference
between the true and approximate distributions, i.e. reduce
the approximation gap. However, given that the expressive
approximation is often accompanied by many additional
parameters, we would like to know how much inﬂuence it
has on the amortization error.

To investigate this, we trained a VAE on MNIST, discarded
the encoder, then retrained encoders with different approx-
imate distributions on the ﬁxed decoder. We ﬁxed the de-
coder so that the true posterior is constant for all the re-
trained encoders. The initial encoder was a two-layer MLP
with a factorized Gaussian distribution.
In order to em-
phasize a large amortization gap, the retrained encoders
had no hidden layers (ie. just linear transformations). For
the retraiend encoders, we tested three approximate distri-
butions: fully factorized Gaussian (qF F G), auxiliary ﬂow
(qAV ), and Flow (qF low). See Section 3.2 for the details of
these distributions.

The inference gaps of the retrained encoders on the training
set are shown in Table 4. As expected, we observe that the
small encoder with qF F G has a very large amortization gap.
However, when we use qAF or qF low as the approximate
distribution, we see the approximation gap decrease, but
more importantly, there is a signiﬁcant decrease in the amor-
tization gap. This indicates that the parameters used for
increasing the complexity of the approximation also play a
large role in diminishing the amortization error.

Variational Family
log ˆp(x)
LVAE[q∗]
LVAE[q]
Approximation
Amortization
Inference

qF F G
-84.70
-86.61
-129.83
1.91
43.22
45.13

qAF
-84.70
-85.48
-98.58
0.78
13.10
13.88

qF low
-84.70
-85.13
-97.99
0.43
12.86
13.29

Table 4. Inﬂuence of expressive approximations on the amortiza-
tion gap. The parameters used to increase the ﬂexibility of the
approximate distribution also reduce the amortization gap.

These results are expected given that the parameterization of
the Flow distribution can be interpreted as an instance of the
RevNet (Gomez et al., 2017) which has demonstrated that
Real-NVP transformations (Dinh et al., 2017) can model
complex functions similar to typical MLPs. Thus the ﬂow
transformations we employ should also be expected to in-
crease the expressiveness while also increasing the capacity
of the encoder. The implication of this observation is that
models which improve the ﬂexibility of their variational
approximation, and attribute their improved results to the
increased expressiveness, may have actually been due to the
reduction in amortization error.

5.4. Inﬂuence of Approximate Posterior on True

Posterior

To what extent does the posterior approximation affect the
learned model? Turner & Sahani (2011) studied the biases in
parameter learning induced by the variational approximation
when learning via variational Expectation-Maximization.
Similarly, we ask whether a factorized Gaussian approxima-
tion causes the true posterior to be more like a factorized
Gaussian? Burda et al. (2016) visually demonstrate that
when trained with an importance-weighted approximate pos-
terior, the resulting true posterior is more complex than those
trained with factorized Gaussian approximations. Just as it
is hard to evaluate a generative model by visually inspecting
samples, it is hard to judge how “Gaussian” the true poste-
rior is by visual inspection. We can quantitatively determine

Inference Suboptimality in Variational Autoencoders

Figure 3. Inference gaps over epochs trained on binarized Fashion-MNIST. Blue is the approximation gap. Orange is the amortization gap.
Standard is a VAE with FFG approximation. Flow is a VAE with a Flow approximation.

how close the posterior is to a fully-factorized Gaussian
(FFG) by comparing the marginal log-likelihood estimate
log ˆp(x) and the Optimal FFG bound LVAE[q∗
F F G]. This is
equivalent to estimating the KL divergence between the opti-
mal Gaussian and the true posterior, KL (q∗(z|x)||p(z|x)).

AF .

In Table 2 on MNIST, for the FFG trained model,
KL (q∗(z|x)||p(z|x)) is nearly the same for both q∗
F F G
and q∗
In contrast, on the model trained with qAF ,
KL (q∗(z|x)||p(z|x)) is much larger for q∗
AF .
This suggests that the true posterior of a FFG-trained model
is closer to FFG than the true posterior of the Flow-trained
model. The same observation can be made on the Fashion-
MNIST dataset. This implies that the decoder can learn to
have a true posterior that ﬁts better to the approximation.

F F G than q∗

These observations justify our results of Section 5.2. which
showed that the amortization error is often the main cause
of inference suboptimality. One reason for this is that the
generator accommodates the choice of approximation, thus
reducing the approximation error.

Generator Hidden Layers
log ˆp(x)

LVAE[q∗

F F G]

Approximation Gap

0
-100.52
-104.42
3.90

2
-84.78
-86.61
1.83

4
-82.19
-83.82
1.63

Table 5. Increased decoder capacity reduces the approximation
gap.

Given that we have seen that the generator can accommodate
the choice of approximation, our next question is whether a
generator with more capacity increases its ability to ﬁt to the
approximation. To this end, we trained VAEs with decoders
of different sizes and measured the approximation gaps on

the training set. Speciﬁcally, we trained decoders with 0, 2,
and 4 hidden layers on MNIST. See Table 5 for the results.
We see that as the capacity of the decoder increases, the ap-
proximation gap decreases. This result implies that the more
ﬂexible the generator is, the less ﬂexible the approximate
distribution needs to be to ensure accurate inference.

5.5. Inference Generalization

How well does amortized inference generalize at test time?
We address this question by visualizing the gaps on training
and validation datapoints across the training epochs. In
Fig. 3, the models are trained on 50000 binarized Fashion-
MNIST datapoints and the gaps are computed on a subset
of a 100 training and validation datapoints. The top and
bottom boundaries of the blue region represent log ˆp(x) and
L[q∗]. The bottom boundary of the orange region represents
L[q]. In other words, the blue region is the approximation
gap and the orange is the amortization gap.

In Fig. 3, the Standard model (top left) refers to a VAE of
latent size 20 trained with a factorized Gaussian approxi-
mate posterior. In this case, the encoder and decoder both
have two hidden layers each consisting of 200 hidden units.
The Flow model (top right) augments the Standard model
with a qF low variational distribution. Larger Decoder and
Larger Encoder models have factorized Gaussian distribu-
tions and increase the number of hidden layers to three and
the number of units in each layer to 500.

Firstly, we observe that for all models, the approximation
gap on the training and validation sets are roughly equivalent.
This indicates that the true posteriors of the held-out data are
similar to that of the training data. Secondly, we note that
for all models, the encoder overﬁts more than the decoder.

Inference Suboptimality in Variational Autoencoders

These observations resonate with the encoder overﬁtting
ﬁndings by Wu et al. (2017).

approximate distribution:

How does increasing decoder capacity affect inference on
held-out data? We know from Section 5.4 that increas-
ing generator capacity results in a posterior that better ﬁts
the approximation making posterior inference easier. Fur-
thermore, the Larger Decoder plot of Fig. 3 shows that
increasing generator capacity causes the model to be more
prone to overﬁtting. Thus, there is a tradeoff between ease
of inference and decoder overﬁtting.

5.5.1. ENCODER CAPACITY AND APPROXIMATION

EXPRESSIVENESS

We have seen in Sections 5.2 and 5.3 that expressive approx-
imations as well as increasing encoder capacity can lead to
a reduction in the amortization gap. This leads us to the fol-
lowing question: when should we increase encoder capacity
versus increasing the expressiveness of the approximation?
We answer this question in terms of how well each model
can generalize its efﬁcient inference (recognition network
and variational distribution) to held-out data.

In Fig. 3, we see that the Flow model and the Larger Encoder
model achieve similar log ˆp(x) on the validation set at the
end of training. However, we see that the L[q] bound of the
Larger Encoder model is signiﬁcantly lower than the L[q]
bound of the Flow model due to the encoder overﬁtting to
the training data. Although they both model the data nearly
equally well, the recognition net of the Larger Encoder
model is no longer suitable to perform inference on the
held-out data due to overﬁtting. Thus a potential rational
for utilizing expressive approximations is that they improve
generalization to held-out data in comparison to increasing
the encoder capacity.

We highlight that, in many scenarios, efﬁcient test time infer-
ence is not required and consequently, encoder overﬁtting
is not an issue, since we can use non-efﬁcient encoder-
independent methods to estimate log p(x), such as AIS,
IWAE with local optimization, or potentially retraining the
encoder on the held-out data. In contrast, when efﬁcient test
time inference is required, encoder generalization is impor-
tant and expressive approximations are likely advantageous.

5.6. Annealing the Entropy

Typical warm-up (Bowman et al., 2015; Sønderby et al.,
2016) refers to annealing the KL (q(z|x)||p(z)) term during
training. This can also be interpreted as performing maxi-
mum likelihood estimation (MLE) early on during training.
This optimization technique is known to help prevent the
latent variable from degrading to the prior (Burda et al.,
2016; Sønderby et al., 2016). We employ a similar anneal-
ing scheme during training by annealing the entropy of the

Ez∼q(z|x) [log p(x, z) − λ log q(z|x)] ,

where λ is annealed from 0 to 1 over training. This can be
interpreted as maximum a posteriori (MAP) in the initial
phase of training.

We ﬁnd that warm-up techniques, such as annealing the
entropy, are important for allowing the true posterior to be
more complex. Table 3 (right) are results from a model
trained without the entropy annealing schedule. Comparing
these results to Table 2, we observe that the difference be-
tween LVAE[q∗
AF ] is signiﬁcantly smaller
without entropy annealing. This indicates that the true pos-
terior is more Gaussian when entropy annealing is not used.
This suggests that, in addition to preventing the latent vari-
able from degrading to the prior, entropy annealing allows
the true posterior to better utilize the ﬂexibility of the ex-
pressive approximation.

F F G] and LVAE[q∗

6. Conclusion

In this paper, we investigated how encoder capacity, approx-
imation choice, decoder capacity, and model optimization
inﬂuence inference suboptimality in terms of the approxima-
tion and amortization gaps. We discovered that the amortiza-
tion gap can be a leading source to inference suboptimality
and that the generator can reduce the approximation gap by
learning a true posterior that ﬁts to the choice of approxima-
tion. We showed that the parameters used to increase the
expressiveness of the approximation play a role in general-
izing inference rather than simply improving the complexity
of the approximation. We conﬁrmed that increasing the
capacity of the encoder reduces the amortization error. Ad-
ditionally, we demonstrated that optimization techniques,
such as entropy annealing, help the generative model to
better utilize the ﬂexibility of expressive variational distri-
butions. Analyzing these gaps can be useful for guiding
improvements in VAEs. Future work includes evaluating
other types of expressive approximations, more complex
likelihood functions, and datasets.

References

Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Joze-
fowicz, R., and Bengio, S. Generating Sentences from a
Continuous Space. ArXiv e-prints, November 2015.

Burda, Y., Grosse, R., and Salakhutdinov, R. Importance

weighted autoencoders. In ICLR, 2016.

Clevert, Djork-Arn´e, Unterthiner, Thomas, and Hochre-
iter, Sepp.
Fast and accurate deep network learn-
ing by exponential linear units (elus). arXiv preprint
arXiv:1511.07289, 2015.

Inference Suboptimality in Variational Autoencoders

Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density estima-

tion using Real NVP. ICLR, 2017.

Geweke, John. Bayesian inference in econometric models
using monte carlo integration. Econometrica: Journal of
the Econometric Society, pp. 1317–1339, 1989.

Glorot, Xavier and Bengio, Yoshua. Understanding the
difﬁculty of training deep feedforward neural networks.
In Proceedings of the Thirteenth International Confer-
ence on Artiﬁcial Intelligence and Statistics, pp. 249–256,
2010.

Gomez, Aidan N, Ren, Mengye, Urtasun, Raquel, and
Grosse, Roger B. The reversible residual network: Back-
propagation without storing activations. In Advances in
Neural Information Processing Systems, pp. 2211–2221,
2017.

Grosse, R., Ghahramani, Z., and Adams, R. P. Sandwiching
the marginal likelihood using bidirectional monte carlo.
arXiv preprint arXiv:1511.02543, 2015.

Grosse, Roger B, Ancha, Siddharth, and Roy, Daniel M.
Measuring the reliability of mcmc inference with bidirec-
tional monte carlo. In Advances in Neural Information
Processing Systems, pp. 2451–2459, 2016.

Hoffman, Matthew D. Learning deep latent gaussian models
with markov chain monte carlo. In International Confer-
ence on Machine Learning, pp. 1510–1519, 2017.

Hoffman, Matthew D, Blei, David M, Wang, Chong, and
Paisley, John. Stochastic variational inference. The Jour-
nal of Machine Learning Research, 14(1):1303–1347,
2013.

Jarzynski, C. Nonequilibrium equality for free energy dif-
ferences. Physical Review Letters, 78(14):2690, 1997.

Kingma, Diederik and Ba, Jimmy. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.

Kingma, D.P. and Welling, M. Auto-Encoding Variational

Bayes. In ICLR, 2014.

Kingma, D.P., Salimans, T., Jozefowicz, R., Chen, X.,
Sutskever, I., and Welling, M.
Improving Variational
Inference with Inverse Autoregressive Flow. NIPS, 2016.

Krishnan, R. G., Liang, D., and Hoffman, M. On the chal-
lenges of learning with inference networks on sparse,
high-dimensional data. ArXiv e-prints, October 2017.

Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple
layers of features from tiny images. University of Toronto,
2009.

Larochelle, Hugo and Bengio, Yoshua. Classiﬁcation using
discriminative restricted boltzmann machines. In Pro-
ceedings of the 25th international conference on Machine
learning, pp. 536–543. ACM, 2008.

LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 1998.

Maaløe, L., Sønderby, CK., Sønderby, SK., and Winther, O.

Auxiliary Deep Generative Models. ICML, 2016.

Neal, R.M. Annealed importance sampling. Statistics and

Computing, 2001.

Ranganath, R., Tran, D., and Blei, D. M. Hierarchical

Variational Models. ICML, 2016.

Rezende, D.J. and Mohamed, S. Variational Inference with

Normalizing Flows. In ICML, 2015.

Rezende, D.J., Mohamed, S., and Wierstra, D. Stochastic
Backpropagation and Approximate Inference in Deep
Generative Models. ICML, 2014.

Salakhutdinov, R. and Murray, I. On the quantitative analy-
sis of deep belief networks. In Proceedings of the 25th
international conference on Machine learning, pp. 872–
879. ACM, 2008.

Salimans, T., Kingma, D.P., and Welling, M. Markov chain
monte carlo and variational inference: Bridging the gap.
In ICML, 2015.

Sønderby, Casper Kaae, Raiko, Tapani, Maaløe, Lars,
Sønderby, Søren Kaae, and Winther, Ole. Ladder varia-
tional autoencoders. In Advances in Neural Information
Processing Systems, pp. 3738–3746, 2016.

Tomczak, J. M. and Welling, M.

Improving Variational
Auto-Encoders using Householder Flow. ArXiv e-prints,
November 2016.

Tomczak, J. M. and Welling, M.

Improving Variational
Auto-Encoders using convex combination linear Inverse
Autoregressive Flow. ArXiv e-prints, June 2017.

Turner, R and Sahani, M. Two problems with variational ex-
pectation maximisation for time-series models. inference
and learning in dynamic models. Cambridge University
Press, pp. 104–123, 2011.

Wu, Y., Burda, Y., Salakhutdinov, R., and Grosse, R. On
the Quantitative Analysis of Decoder-Based Generative
Models. ICLR, 2017.

Xiao, Han, Rasul, Kashif, and Vollgraf, Roland. Fashion-
mnist: a novel image dataset for benchmarking machine
learning algorithms. github.com/zalandoresearch/fashion-
mnist, 2017.

Supplementary

Code for the experiments in this paper can be found at:
https://github.com/chriscremer/Inference-Suboptimality and
https://github.com/lxuechen/inference-suboptimality.

6.1.3. 3-BIT CIFAR

6.1. Model Architectures and Training

Hyperparameters

6.1.1. 2D VISUALIZATION

The VAE model of Fig. 2 uses a decoder p(x|z) with ar-
chitecture: 2 − 100 − 784, and an encoder q(z|x) with
architecture: 784 − 100 − 4. We use tanh activations and a
batch size of 50. The model is trained for 3000 epochs with
a learning rate of 10−4 using the ADAM optimizer (Kingma
& Ba, 2014).

6.1.2. MNIST & FASHION-MNIST

Both MNIST and Fashion-MNIST consist of a training and
test set with 60000 and 10000 datapoints respectively, where
each datapoint is a 28x28 grey-scale image. We rescale the
original images so that pixel values are within the range
[0, 1]. For MNIST, We use the statically binarized version
described by (Larochelle & Bengio, 2008). We also binarize
Fashion-MINST statically. For both datasets, we adopt the
Bernoulli likelihood for the generator.

The VAE models for MNIST and Fashion-MNIST exper-
iments have the same architecture. The encoder has two
hidden layers with 200 units each. The activation function is
chosen to be the exponential linear unit (ELU, Clevert et al.
(2015)), as we observe improved performance compared to
tanh. The latent space has 50 dimensions. The generator
is the reverse of the encoder. We follow the same learning
rate schedule and train for the same amount of epochs as
described by (Burda et al., 2016). All models are trained
with the a batch-size of 100 with ADAM.

In the large encoder setting, we change the number of hidden
units for the inference network to be 500, instead of 200.
The warm-up models are trained with a linear schedule over
the ﬁrst 400 epochs according to Section 5.6.

The auxiliary variable of requires a couple distributions:
q(v0|z0) and r(vT |zT ). These distributions are both factor-
ized Gaussians which are parameterized by MLP’s with two
hidden layers, 100 units each, with ELU activations.

The ﬂow transformation q(zt+1, vt+1|zt, vt) involves func-
tions σ1, σ2, µ1, and µ2 from Eqn. 9 and 10. These also
have two hidden layers with 100 units each and ELU units.

CIFAR-10 consists of a training and test dataset with 50000
and 10000 datapoints respectively, where each datapoint is
a 32 × 32 RGB image. We rescale individual pixel values
to be in the range [0, 1]. We then statically binarize the
scaled pixel values by setting individual pixel values of
channels to 1 if the rescaled value is greater than 0.5 and
0 otherwise. In this manner, we can model the observation
with a factorized Bernoulli likelihood. We call this binarized
CIFAR-10 dataset as 3-BIT CIFAR, since 3 bits are required
to encode each pixel, where 1 bit is needed for each of the
channels. We acknowledge that such binarization scheme
may reduce the complexity of the original problem, since
originally 24 bits were required to encode a single pixel.
Nevertheless, the 3-bit CIFAR dataset is still much more
challenging compared MNIST and Fashion. This is because
784 bits are required to encode one MNIST/Fashion image,
whereas for one 3-bit CIFAR image, 3072 bits are required.
Most notably, we were able to validate our AIS estimates
using BDMC with the simpliﬁed dataset. This, however,
was not achievable in any reasonable amount of time with
the original CIFAR-10 dataset.

For the latent variable, we use a 50-dimensional factorized
Gaussian for q(z|x). For all neural networks, ELU is cho-
sen to be the activation function. The inference network
consists of three 4 by 4 convolution layers with stride 2,
batch-norm, and 64, 128, 256 channels respectively. Then a
fully-connected layer outputs the 50-dimensional mean and
log-variance of the latent variable. Similarly, the generator
consists of a fully-connected layer outputting 256 by 2 by 2
tensors. Then three deconvolutional layers each with 4 by
4 ﬁlters, stride 2, batch-norm, and 128, 64, and 3 channels
respectively. For the model with expressive inference, we
use three normalizing ﬂow steps, where the parametric func-
tions in the ﬂow and auxiliary variable distribution also take
in a hidden layer of the encoder.

We use a learning rate of 10−3. Warm-up is applied with
a linear schedule over the ﬁrst 50 epochs. All models are
trained with a batch-size of 100 with ADAM. Early-stopping
is applied based on the performance computed with the
IWAE bound (k=1000) on the held-out set of 5000 examples
from the original training set.

Inference Suboptimality in Variational Autoencoders

6.2. Inference Generalization

These models are trained with batch size 50 and latent di-
mension size of 20. The rest of the hyperparameters are
equivalent to Section 6.1.2.

Architecture of qF low: The ﬂow transformation involves
functions σ1, σ2, µ1, and µ2 from Eqn. 9 and 10. Each
function is an MLP with a 50 unit hidden layer and ELU
activations. We apply this ﬂow transformation twice.

Fig. 4 are the plots for the qAF model. The transformations
are the same as qF low, but rather than partitioning the latent
variable, we introduce an auxiliary variable. The auxiliary
variable also requires a reverse model r(v|z) which is a
factorized Gaussian parameterized by an MLP with a 50
unit hidden layer and ELU activations.

Comparing AF in Fig. 4 to Flow in Fig. 3, we see that the
AF has a larger approximation gap. This increase is likely
due to the KL (q(v|z, x)(cid:107)r(v|x, z)) term of the auxiliary
variable lower bound from 2.2.2. This motivates also using
expressive approximations for the reverse model r(v|z).

Figure 4. Gaps over epochs of the AF (auxiliary ﬂow) model.

6.3. Inﬂuence of Flows On Amortization Gap

Experiment

The aim of this experiment is to show that the parameters
used for increasing the expressiveness of the approxima-
tion also contribute to reducing the amortization error. To
show this, we train a VAE on MNIST, discard the encoder,
then retrain two encoders on the ﬁxed decoder: one with a
factorized Gaussian distribution and the other with a param-
eterized ’ﬂow’ distribution. We use ﬁxed decoder so that
the true posterior is constant for both encoders. See 5.3 for
the results and below for the architecture details.

The architecture of the decoder is: DZ − 200 − 200 − DX .
The architecture of the encoder used to train the decoder
is DX − 200 − 200 − 2DZ. The approximate distribution
q(z|x) is a factorized Gaussian.

Next, we describe the encoders which were trained on the
ﬁxed trained decoder. In order to highlight a large amorti-
zation gap, we employed a very small encoder architecture:
DX − 2DZ. This encoder has no hidden layers, which

greatly impoverishes its ability and results in a large amorti-
zation gap.

We compare two approximate distributions q(z|x). Firstly,
we experiment with the typical fully factorized Gaussian
(FFG). The second is what we call a ﬂow distribution.
Speciﬁcally, we use the transformations of (Dinh et al.,
2017). We also include an auxiliary variable so we don’t
need to select how to divide the latent space for the trans-
formations. The approximate distribution over the la-
tent z and auxiliary variable v factorizes as: q(z, v|x) =
q(z|x)q(v). The q(v) distribution is simply a N(0,1) dis-
tribution. Since we’re using a auxiliary variable, we also
require the r(v|z) distribution which we parameterize as
r(v|z): [DZ] − 50 − 50 − 2DZ. The ﬂow transformation
is the same as in Section 3.2, which we apply twice.

6.4. Computation of the Determinant for Flow

The overall mapping f that performs (z, v) (cid:55)→ (z(cid:48), v(cid:48)) is
the composition of two sheer mappings f1 and f2 that re-
spectively perform (z, v) (cid:55)→ (z, v(cid:48)) and (z, v(cid:48)) (cid:55)→ (z(cid:48), v(cid:48)).
Since the Jacobian of either one of the sheer mappings is
diagonal, the determinant of the composed transformation’s
Jacobian Df can be easily computed:

det(Df ) = det(Df1)det(Df2)

=

(cid:16) n
(cid:89)

i=1

(cid:17)(cid:16) n
(cid:89)

σ2(v(cid:48))j

(cid:17)

.

σ1(z)i

j=1

6.5. Annealed Importance Sampling

Annealed importance sampling (AIS, Neal (2001); Jarzyn-
ski (1997)) is a means of computing a lower bound to
the marginal log-likelihood. Similarly to the importance
weighted bound, AIS must sample a proposal distribution
f1(z) and compute the density of these samples, however,
AIS then transforms the samples through a sequence of
reversible transitions Tt(z(cid:48)|z). The transitions anneal the
proposal distribution to the desired distribution fT (z).

Speciﬁcally, AIS samples an initial state z1 ∼ f1(z) and
sets an initial weight w1 = 1. For the following annealing
steps, zt is sampled from Tt(z(cid:48)|z) and the weight is updated
according to:

wt = wt−1

ft(zt−1)
ft−1(zt−1)

.

This procedure produces weight wT such that E [wT ] =
ZT /Z1, where ZT and Z1 are the normalizing constants of
fT (z) and f1(z) respectively. This pertains to estimating the
marginal likelihood when the target distribution is p(x, z)
when we integrate with respect to z.

Typically, the intermediate distributions are simply deﬁned
to be geometric averages: ft(z) = f1(z)1−βtfT (z)βt,

Inference Suboptimality in Variational Autoencoders

where βt is monotonically increasing with β1 = 0 and βT = 1.
When f1(z) = p(z) and fT (z) = p(x, z), the intermediate
distributions are: fi(x) = p(z)p(x|z)βi .

Model evaluation with AIS appears early on in the setting
of deep belief networks (Salakhutdinov & Murray, 2008).
AIS for decoder-based models was also used by Wu et al.
(2017).

6.6. Extra MNIST Inference Gaps

To demonstrate that a very small inference gap can be
achieved, even with a limited approximation such as a fac-
torized Gaussian, we train the model on a small dataset. In
this experiment, our training set consists of 1000 datapoints
randomly chosen from the original MNIST training set. The
training curves on this small datatset are shown in Fig. 5.
Even with a factorized Gaussian distribution, the inference
gap is very small: the AIS and IWAE bounds are overlap-
ping and the VAE is just slightly below. Yet, the model is
overﬁtting as seen by the decreasing test set bounds.

Figure 5. Training curves for a FFG and a Flow inference model on
MNIST. AIS provides the tightest lower bound and is independent
of encoder overﬁtting. There is little difference between FFG and
Flow models trained on the 1000 datapoints since inference is
nearly equivalent.

Inference Suboptimality in Variational Autoencoders

Chris Cremer 1 Xuechen Li 1 David Duvenaud 1

8
1
0
2
 
y
a
M
 
7
2
 
 
]

G
L
.
s
c
[
 
 
3
v
8
5
5
3
0
.
1
0
8
1
:
v
i
X
r
a

Abstract
Amortized inference allows latent-variable mod-
els trained via variational learning to scale to large
datasets. The quality of approximate inference is
determined by two factors: a) the capacity of the
variational distribution to match the true poste-
rior and b) the ability of the recognition network
to produce good variational parameters for each
datapoint. We examine approximate inference in
variational autoencoders in terms of these factors.
We ﬁnd that divergence from the true posterior
is often due to imperfect recognition networks,
rather than the limited complexity of the approx-
imating distribution. We show that this is due
partly to the generator learning to accommodate
the choice of approximation. Furthermore, we
show that the parameters used to increase the ex-
pressiveness of the approximation play a role in
generalizing inference rather than simply improv-
ing the complexity of the approximation.

1. Introduction

In this paper, we analyze inference suboptimality: the mis-
match between the true and approximate posterior. More
speciﬁcally, we are interested in understanding what factors
cause the gap between the marginal log-likelihood and the
evidence lower bound (ELBO) in variational autoencoders
(VAEs, Kingma & Welling (2014); Rezende et al. (2014)).
We refer to this as the inference gap. Moreover, we break
down the inference gap into two components: the approxi-
mation gap and the amortization gap. The approximation
gap comes from the inability of the variational distribution
family to exactly match the true posterior. The amortiza-
tion gap refers to the difference caused by amortizing the
variational parameters over the entire training set, instead of
optimizing for each training example individually. We refer
the reader to Table 1 for the deﬁnitions of the gaps and to

1Department of Computer Science, University of Toronto,
Toronto, Canada. Correspondence to: Chris Cremer <ccre-
mer@cs.toronto.edu>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

Fig. 1 for a simple illustration of the gaps. In Fig. 1, L[q]
refers to the ELBO evaluated using an amortized distribu-
tion q, as is typical of VAE training. In contrast, L[q∗] is the
ELBO evaluated using the optimal approximation within its
variational family.

There has been signiﬁcant work on improving variational
inference in VAEs through the development of expressive ap-
proximate posteriors (Rezende & Mohamed, 2015; Kingma
et al., 2016; Ranganath et al., 2016; Tomczak & Welling,
2016; 2017). These works have shown that with more ex-
pressive approximate posteriors, the model learns a better
distribution over the data. Our study aims to gain a bet-
ter understanding of the relationship between expressive
approximations and improved generative models.

Our experiments investigate how the choice of encoder,
posterior approximation, decoder, and optimization affect
the approximation and amortization gaps. We train VAE
models in a number of settings on the MNIST (LeCun et al.,
1998), Fashion-MNIST (Xiao et al., 2017), and CIFAR-10
(Krizhevsky & Hinton, 2009) datasets.

Our contributions are: a) we investigate inference subopti-
mality in terms of the approximation and amortization gaps,
providing insight to guide future improvements in VAE in-
ference, b) we quantitatively demonstrate that the learned
generative model accommodates the choice of approxima-
tion, and c) we demonstrate that parameterized functions
that improve the expressiveness of the approximation play a
signiﬁcant role in reducing amortization error.

Figure 1. Gaps in Inference

Inference Suboptimality in Variational Autoencoders

Term
Inference
Approximation
Amortization

Deﬁnition
log p(x) − L[q]
log p(x) − L[q∗]
L[q∗] − L[q]

VAE Formulation
KL (q(z|x)||p(z|x))
KL (q∗(z|x)||p(z|x))
KL (q(z|x)||p(z|x)) − KL (q∗(z|x)||p(z|x))

Table 1. Summary of Gap Terms. The middle column refers to the general case where our variational objective is a lower bound on the
marginal log-likelihood. The right most column demonstrates the speciﬁc case in VAEs. q∗(z|x) refers to the optimal approximation
within a family Q, i.e. q∗(z|x) = arg minq∈Q KL (q(z|x)||p(z|x)).

2. Background

2.1. Inference in Variational Autoencoders

Let x be the observed variable, z the latent variable, and
p(x, z) be their joint distribution. Given a dataset X =
{x1, x2, ..., xN }, we would like to maximize the marginal
log-likelihood with respect to the model parameters θ:

log pθ(X) =

log pθ(xi) =

log

pθ(xi, zi)dzi.

N
(cid:88)

i=1

(cid:90)

N
(cid:88)

i=1

In practice, the marginal log-likelihood is computationally
intractable due to the integration over the latent variable
z. Instead, VAEs introduce an inference network qφ(z|x)
to approximate the true posterior p(z|x) and optimize the
ELBO with respect to model parameters θ and inference
network parameters φ (parameterization subscripts omitted
for brevity):

log p(x) = Eq(z|x)

log

+ KL (q(z|x)||p(z|x))

(cid:20)

(cid:20)

(cid:19)(cid:21)

(cid:18) p(x, z)
q(z|x)

(cid:19)(cid:21)

(cid:18) p(x, z)
q(z|x)

≥ Eq(z|x)

log

= LVAE[q].

From the above equation, we see that the ELBO is tight
when q(z|x) = p(z|x). The choice of q(z|x) is often a
factorized Gaussian distribution for its simplicity and efﬁ-
ciency. By utilizing the inference network (also referred to
as encoder or recognition network), VAEs amortize infer-
ence over the entire dataset. Furthermore, the overall model
is trained by stochastically optimizing the ELBO using the
reparametrization trick (Kingma & Welling, 2014).

2.2. Expressive Approximate Posteriors

There are a number of strategies for increasing the expres-
siveness of approximate posteriors, going beyond the origi-
nal factorized-Gaussian. We brieﬂy summarize normalizing
ﬂows and auxiliary variables.

2.2.1. NORMALIZING FLOWS

Normalizing ﬂow (Rezende & Mohamed, 2015) is a change
of variables procedure for constructing complex distribu-
tions by transforming probability densities through a se-
ries of invertible mappings. Speciﬁcally, if we transform

a random variable z0 with distribution q0(z), the resulting
random variable zT = T (z0) has a distribution:

qT (zT ) = q0(z0)

det

(3)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂zT
∂z0

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−1

.

By successively applying these transformations, we can
build arbitrarily complex distributions. Stacking these trans-
formations remains tractable due to the determinant being
decomposable: det(AB) = det(A)det(B). An important
property of these transformations is that we can take ex-
pectations with respect to the transformed density qT (zT )
without explicitly knowing its formula due to the law of the
unconscious statistician (LOTUS):

EqT [h(zT )] = Eq0[h(fT (fT −1(...f1(z0))))].

(4)

Using equations (3) and (4), the lower bound with the trans-
formed approximation can be written as:





Ez0∼q0(z|x)


log




q0(z0|x) (cid:81)T

p(x, zT )
(cid:12)
(cid:12)det ∂zt
(cid:12)

t=1

∂zt−1

(cid:12)
(cid:12)
(cid:12)

(1)

(2)









 .

−1

(5)

The main constraint on these transformations is that the
determinant of their Jacobian needs to be easily computable.

2.2.2. AUXILIARY VARIABLES

Deep generative models can be extended with auxiliary vari-
ables which leave the generative model unchanged but make
the variational distribution more expressive. Just as hierar-
chical Bayesian models induce dependencies between data,
hierarchical variational models can induce dependencies be-
tween latent variables. The addition of the auxiliary variable
changes the lower bound to:

(cid:20)

Ez,v∼q(z,v|x)

log

(cid:20)

= Eq(z|x)

log

(cid:18) p(x, z)
q(z|x)

(cid:19)(cid:21)

(cid:18) p(x, z)r(v|x, z)
q(z, v|x)
(cid:19)

(cid:16)

− KL

q(v|z, x)(cid:107)r(v|x, z)

(6)

(cid:17)(cid:21)

(7)

where r(v|x, z) is called the reverse model. From Eqn. 7,
we see that this bound is looser than the regular ELBO,

Inference Suboptimality in Variational Autoencoders

however the extra ﬂexibility provided by the auxiliary vari-
able can result in a higher lower bound. This idea has been
employed in works such as auxiliary deep generative mod-
els (ADGM, (Maaløe et al., 2016)), hierarchical variational
models (HVM, (Ranganath et al., 2016)) and Hamiltonian
variational inference (HVI, (Salimans et al., 2015)).

3. Methods

3.1. Approximation and Amortization Gaps

The inference gap G is the difference between the marginal
log-likelihood log p(x) and a lower bound L[q]. Given
the distribution in the family that maximizes the bound,
q∗(z|x) = arg maxq∈Q L[q], the inference gap decomposes
as the sum of approximation and amortization gaps:

G = log p(x) − L[q] = log p(x) − L[q∗]
(cid:125)

(cid:124)

(cid:123)(cid:122)
Approximation

+ L[q∗] − L[q]
(cid:125)
(cid:123)(cid:122)
Amortization

(cid:124)

.

For VAEs, we can translate the gaps to KL divergences by
rearranging Eqn. (1):

(cid:124)

GVAE = KL(cid:0)q∗(z|x)||p(z|x)(cid:1)
(cid:125)
+ KL(cid:0)q(z|x)||p(z|x)(cid:1) − KL(cid:0)q∗(z|x)||p(z|x)(cid:1)
(cid:123)(cid:122)
(cid:125)
Amortization

(cid:123)(cid:122)
Approximation

(cid:124)

.

replaced with z and z2 with v. We refer to this approximate
distribution as qAF , where AF stands for auxiliary ﬂow. We
train this model by optimizing the following bound:





Eq0(z,v|x)


log




= L[qAF ].

p(x, zT )r(vT |x, zT )
(cid:16) ∂ztvt

(cid:12)
(cid:12)
(cid:12)det

∂zt−1vt−1

−1

(cid:17)(cid:12)
(cid:12)
(cid:12)

qT (zT , vT |x)











(11)

Note that this lower bound is looser as explained in Section
2.2.2. We refer readers to Section 6.1.2 in the Supplemen-
tary material for speciﬁc details of the ﬂow conﬁguration
adopted in the experiments.

3.3. Marginal Log-Likelihood Estimation and Evidence

Lower Bounds

In this section, we describe the estimates we use to compute
the bounds of the inference gaps: log p(x), L[q∗], and L[q].
We use two bounds to estimate the marginal log-likelihood,
log p(x): IWAE (Burda et al., 2016) and AIS (Neal, 2001).

The IWAE bound takes multiple importance weighted sam-
ples from the variational q distribution resulting in a tighter
lower bound than the VAE bound. The IWAE bound is
computed as:

(8)

log p(x) ≥ Ez1...zk∼q(z|x)

log

(cid:34)

(cid:32)

1
k

k
(cid:88)

i=1

p(x, zi)
q(zi|x)

(cid:33)(cid:35)

(12)

3.2. Flexible Approximate Posteriors

= LIWAE[q].

Our experiments involve expressive approximations which
use ﬂow transformations and auxiliary variables. The ﬂow
transformation that we employ is of the same type as the
transformations of Real NVP (Dinh et al., 2017). We parti-
tion the latent variable z into two, z1 and z2, then perform
the following transformations:

z(cid:48)
1 = z1 ◦ σ1(z2) + µ1(z2)
1) + µ2(z(cid:48)
2 = z2 ◦ σ2(z(cid:48)
z(cid:48)
1)

(9)

(10)

: Rn → Rn are differentiable
where σ1, σ2, µ1, µ2
mappings parameterized by neural nets and ◦ takes the
Hadamard or element-wise product. We partition the la-
tent variable by simply indexing the elements of the ﬁrst
half and the second half. The determinant of the combined
(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)det
transformation’s Jacobian,
(cid:12), can be easily eval-
uated. See section 6.4 of the Supplementary material for a
derivation. The lower bound of this approximation is the
same as Eqn. (5). We refer to this approximation as qF low.

(cid:16) ∂z(cid:48)
∂z

As the number of importance samples approaches inﬁnity,
the bound approaches the marginal log-likelihood.
It is
often used as an evaluation metric for generative models
(Burda et al., 2016; Kingma et al., 2016). AIS is potentially
an even tighter lower bound. AIS weights samples from
distributions which are sequentially annealed from an initial
proposal distribution to the true posterior. See Section 6.5
in the Supplementary material for further details regarding
AIS. To compute the AIS bound, we use 100 chains, each
with 10000 intermediate distributions, where each transition
consists of one HMC trajectory with 10 leapfrog steps. The
initial distribution for AIS is the prior, so that it is encoder-
independent.

We estimate the marginal log-likelihood by independently
computing our tightest lower bounds then take the maximum
of the two:

log ˆp(x) = max(LAIS, LIWAE).

We also experiment with an approximation that combines
ﬂow transformations and auxiliary variables. Let z ∈ Rn
be the variable of interest and v ∈ Rn the auxiliary variable.
The ﬂow is the same as equations (9) and (10), where z1 is

The L[q∗] and L[q] bounds are the standard ELBOs, LVAE,
from Eqn. (2), computed with either the amortized q or the
optimal q∗ (see below). When computing LVAE and LIWAE,
we use 5000 samples.

Inference Suboptimality in Variational Autoencoders

A

B

C

D

Figure 2. True Posterior and Approximate Distributions of a VAE with 2D latent space. The columns represent four different datapoints.
The green distributions are the true posterior distributions, highlighting the mismatch with the blue approximations. Amortized: Variational
parameters learned over the entire dataset. Optimal: Variational parameters optimized for each individual datapoint. Flow: Using a
ﬂexible approximate distribution.

3.4. Local Optimization of the Approximate

Distribution

To compute LVAE[q∗], we optimize the parameters of the
variational distribution for every datapoint. For the local op-
timization of qF F G, we initialize the mean and variance as
the prior, i.e. N (0, I). We optimize the mean and variance
using the Adam optimizer with a learning rate of 10−3. To
determine convergence, after every 100 optimization steps,
we compute the average of the previous 100 ELBO values
and compare it to the best achieved average. If it does not
improve for 10 consecutive iterations then the optimization
is terminated. For qF low and qAF , the same process is used
to optimize all of its parameters. All neural nets for the ﬂow
were initialized with a variant of the Xavier initilization
(Glorot & Bengio, 2010). We use 100 Monte Carlo samples
to compute the ELBO to reduce variance.

3.5. Validation of Bounds

The soundness of our empirical analysis depends on the
reliability of the marginal log-likelihood estimator. For
general importance sampling based estimators, the sample
variance of the normalized importance weights can serve as
an indicator of accuracy (Geweke, 1989; Neal, 2001). This
quantitative measure, however, can also be unreliable, e.g.
when the proposal misses an important mode of the target
distribution (Neal, 2001).

In this work, we follow (Wu et al., 2017) to empirically
validate our AIS estimates with Bidirectional Monte Carlo
(BDMC, Grosse et al. (2015; 2016)). In addition to a lower
bound provided by AIS, BDMC runs AIS chains backward
from exact posterior samples to obtain an upper bound on
the marginal log-likelihood. It should be noted that BDMC
relies on the assumption that the distribution of the simulated
data from the model roughly matches that of the real data.
This is due to the backward chain initializes from exact
posterior samples (Grosse et al., 2015).

For the MNIST and Fashion datasets, BDMC gives a gap
within 0.1 nat for a linear schedule AIS with 104 inter-
mediate distributions and 100 importance samples on 103
simulated datapoints. For 3-BIT CIFAR, the same AIS
setting gives a gap within 1 nat with the sigmoidial anneal-
ing schedule (Grosse et al., 2015) on 100 simulated data-
points. Loosely speaking, this should give us conﬁdence
in how well our AIS lower bounds reﬂect the marginal log-
likelihood computed on the real data.

4. Related Work

Much of the earlier work on variational inference focused
on optimizing the variational parameters locally for each
datapoint, e.g. the original Stochastic Variational Inference
scheme (SVI, Hoffman et al. (2013)). To scale inference to
large datasets, most related works utilize inference networks
to amortize the cost of inference over the entire dataset.

Inference Suboptimality in Variational Autoencoders

MNIST

3-BIT CIFAR

log ˆp(x)
LVAE[q∗
AF ]
LVAE[q∗
F F G]

LVAE[q]
Approximation
Amortization
Inference

qF F G
-89.80
-90.80
-91.23
-92.57
1.43
1.34
2.77

qAF
-88.94
-90.38
-113.54
-91.79
1.44
1.41
2.85

Fashion-MNIST
qAF
-97.41
-99.10
-132.46
-103.76
1.69
4.66
6.35

qF F G
-97.47
-98.92
-100.53
-104.75
3.06
4.22
7.28

qF F G
-816.9
-820.19
-831.65
-869.12
14.75
37.47
52.22

qAF
-820.56
-822.16
-861.62
-864.28
1.60
42.12
43.72

Table 2. Inference Gaps. The columns qF F G and qAF refer to the variational distribution used for training the model. These lower bounds
are computed on the training set and are in units of nats.

Our work analyses the error that these inference networks
introduce.

Most relevant to our work is the recent work of Krishnan
et al. (2017), which explicitly remarks on two sources of
error in variational learning with inference networks, and
proposes to optimize approximate inference locally from
an initialization output by the inference network. They
show improved training on high-dimensional, sparse data
with the hybrid method, claiming that local optimization
reduces the negative effects of random initialization in the
inference network early on in training. Thus their work
focuses on reducing the amortization gap early on in training.
Similar to this idea, Hoffman (2017) proposes to perform
approximate inference during model training with MCMC
at an initialization given by a variational distribution. Our
work provides a means of explaining these improvements
in terms of the sources of inference suboptimality that they
reduce.

5. Experimental Results

5.1. Intuition through Visualization

To begin, we would like to gain an intuitive visualization of
the gaps presented in Section 3.1. To this end, we trained
a VAE with a two-dimensional latent space on MNIST and
in Fig. 2 we show contour plots of various distributions
in the latent space. The ﬁrst row contains contour plots of
the true posteriors p(z|x) for four different training data-
points (columns). We have selected these four examples
to highlight different inference phenomena. The amortized
fully-factorized Gaussian (FFG) row refers to the output
of the recognition net, in this case, a FFG approximation.
Optimal FFG is the FFG that best ﬁts the posterior of the
datapoint. Optimal Flow is the optimal ﬁt of a ﬂexible distri-
bution to the same posterior, where the ﬂexible distribution
we use is described in Section 3.2.

Posterior A is an example of a distribution where a FFG can
ﬁt relatively well. Posterior B is an example of a posterior
with dependence between dimensions, demonstrating the

limitation of having a factorized approximation. Posterior C
highlights a shortcoming of performing amortization with a
limited-capacity recognition network, where the amortized
FFG shares little support with the true posterior. Posterior
D is a bi-modal distribution which demonstrates the ability
of the ﬂexible approximation to ﬁt to complex distributions,
in contrast to the simple FFG approximation. These obser-
vations raise the following question: in more typical VAEs,
is the amortization of inference the leading cause of the
distribution mismatch, or is it the limited expressiveness of
the approximation?

5.2. Amortization vs Approximation Gap

In this section, we compare how much the approximation
and amortization gaps each contribute to the total inference
gap. Table 2 are results of inference on the training set of
MNIST, Fashion-MNIST and 3-BIT CIFAR (a binarized
version of CIFAR-10, see Section 6.1.3 for details). For
each dataset, we trained models with two different approx-
imate posterior distributions: a fully-factorized Gaussian,
qF F G, and the ﬂexible distribution, qAF . Due to the com-
putational cost of optimizing the local parameters for each
datapoint, our evaluation is performed on a subset of 1000
datapoints for MNIST and Fashion-MNIST and a subset of
100 datapoints for 3-BIT CIFAR.

For MNIST, we see that the amortization and approximation
gaps each account for nearly half of the inference gap. On
the more difﬁcult Fashion-MNIST dataset, the amortization
gap is larger than the approximation gap. For CIFAR, we
see that the amortization gap is much more signiﬁcant com-
pared to the approximation gap. Thus, for the three datasets
and model architectures that we consider, the amortization
gap is likely to be the more prominent cause of inference
suboptimality, especially when the dataset becomes more
challenging to model. This indicates that improvements in
inference will likely be a result of reducing amortization
error, rather than approximation errors.

With these results in mind, would simply increasing the
capacity of the encoder improve the amortization gap? We

Inference Suboptimality in Variational Autoencoders

MNIST

MNIST

log ˆp(x)
LVAE[q∗
AF ]
LVAE[q∗
F F G]

LVAE[q]
Approximation
Amortization
Inference

qF F G
-89.61
-90.65
-91.07
-92.18
1.46
1.11
2.56

qAF
-88.99
-90.44
-108.71
-91.19
1.45
0.75
2.20

Fashion-MNIST
qAF
-96.18
-97.91
-129.70
-101.67
1.73
3.76
5.49

qF F G
-95.99
-97.40
-99.64
-102.73
3.65
3.09
6.74

qF F G
-89.82
-90.96
-90.84
-92.33
1.02
1.49
2.51

qAF
-89.52
-90.45
-92.25
-91.75
0.93
1.30
2.23

Fashion-MNIST
qAF
-102.88
-104.02
-105.80
-107.01
1.14
2.29
4.13

qF F G
-102.56
-103.73
-103.85
-106.90
1.29
3.05
4.34

Table 3. Left: Larger Encoder. Right: Models trained without entropy annealing. The columns qF F G and qAF refer to the variational
distribution used for training the model. The lower bounds are computed on the training set and are in units of nats.

examined this by training the MNIST and Fashion-MNIST
models from above but with larger encoders. See Section
6.1.2 for implementation details. Table 3 (left) are the results
of this experiment. Comparing to Table 2, we see that, for
both datasets and both variational distributions, using a
larger encoder results in the inference gap decreasing and
the decrease is mainly due to a reduction in the amortization
gap.

5.3. Inﬂuence of Flows on the Amortization Gap

The common reasoning for increasing the expressiveness
of the approximate posterior is to minimize the difference
between the true and approximate distributions, i.e. reduce
the approximation gap. However, given that the expressive
approximation is often accompanied by many additional
parameters, we would like to know how much inﬂuence it
has on the amortization error.

To investigate this, we trained a VAE on MNIST, discarded
the encoder, then retrained encoders with different approx-
imate distributions on the ﬁxed decoder. We ﬁxed the de-
coder so that the true posterior is constant for all the re-
trained encoders. The initial encoder was a two-layer MLP
with a factorized Gaussian distribution.
In order to em-
phasize a large amortization gap, the retrained encoders
had no hidden layers (ie. just linear transformations). For
the retraiend encoders, we tested three approximate distri-
butions: fully factorized Gaussian (qF F G), auxiliary ﬂow
(qAV ), and Flow (qF low). See Section 3.2 for the details of
these distributions.

The inference gaps of the retrained encoders on the training
set are shown in Table 4. As expected, we observe that the
small encoder with qF F G has a very large amortization gap.
However, when we use qAF or qF low as the approximate
distribution, we see the approximation gap decrease, but
more importantly, there is a signiﬁcant decrease in the amor-
tization gap. This indicates that the parameters used for
increasing the complexity of the approximation also play a
large role in diminishing the amortization error.

Variational Family
log ˆp(x)
LVAE[q∗]
LVAE[q]
Approximation
Amortization
Inference

qF F G
-84.70
-86.61
-129.83
1.91
43.22
45.13

qAF
-84.70
-85.48
-98.58
0.78
13.10
13.88

qF low
-84.70
-85.13
-97.99
0.43
12.86
13.29

Table 4. Inﬂuence of expressive approximations on the amortiza-
tion gap. The parameters used to increase the ﬂexibility of the
approximate distribution also reduce the amortization gap.

These results are expected given that the parameterization of
the Flow distribution can be interpreted as an instance of the
RevNet (Gomez et al., 2017) which has demonstrated that
Real-NVP transformations (Dinh et al., 2017) can model
complex functions similar to typical MLPs. Thus the ﬂow
transformations we employ should also be expected to in-
crease the expressiveness while also increasing the capacity
of the encoder. The implication of this observation is that
models which improve the ﬂexibility of their variational
approximation, and attribute their improved results to the
increased expressiveness, may have actually been due to the
reduction in amortization error.

5.4. Inﬂuence of Approximate Posterior on True

Posterior

To what extent does the posterior approximation affect the
learned model? Turner & Sahani (2011) studied the biases in
parameter learning induced by the variational approximation
when learning via variational Expectation-Maximization.
Similarly, we ask whether a factorized Gaussian approxima-
tion causes the true posterior to be more like a factorized
Gaussian? Burda et al. (2016) visually demonstrate that
when trained with an importance-weighted approximate pos-
terior, the resulting true posterior is more complex than those
trained with factorized Gaussian approximations. Just as it
is hard to evaluate a generative model by visually inspecting
samples, it is hard to judge how “Gaussian” the true poste-
rior is by visual inspection. We can quantitatively determine

Inference Suboptimality in Variational Autoencoders

Figure 3. Inference gaps over epochs trained on binarized Fashion-MNIST. Blue is the approximation gap. Orange is the amortization gap.
Standard is a VAE with FFG approximation. Flow is a VAE with a Flow approximation.

how close the posterior is to a fully-factorized Gaussian
(FFG) by comparing the marginal log-likelihood estimate
log ˆp(x) and the Optimal FFG bound LVAE[q∗
F F G]. This is
equivalent to estimating the KL divergence between the opti-
mal Gaussian and the true posterior, KL (q∗(z|x)||p(z|x)).

AF .

In Table 2 on MNIST, for the FFG trained model,
KL (q∗(z|x)||p(z|x)) is nearly the same for both q∗
F F G
and q∗
In contrast, on the model trained with qAF ,
KL (q∗(z|x)||p(z|x)) is much larger for q∗
AF .
This suggests that the true posterior of a FFG-trained model
is closer to FFG than the true posterior of the Flow-trained
model. The same observation can be made on the Fashion-
MNIST dataset. This implies that the decoder can learn to
have a true posterior that ﬁts better to the approximation.

F F G than q∗

These observations justify our results of Section 5.2. which
showed that the amortization error is often the main cause
of inference suboptimality. One reason for this is that the
generator accommodates the choice of approximation, thus
reducing the approximation error.

Generator Hidden Layers
log ˆp(x)

LVAE[q∗

F F G]

Approximation Gap

0
-100.52
-104.42
3.90

2
-84.78
-86.61
1.83

4
-82.19
-83.82
1.63

Table 5. Increased decoder capacity reduces the approximation
gap.

Given that we have seen that the generator can accommodate
the choice of approximation, our next question is whether a
generator with more capacity increases its ability to ﬁt to the
approximation. To this end, we trained VAEs with decoders
of different sizes and measured the approximation gaps on

the training set. Speciﬁcally, we trained decoders with 0, 2,
and 4 hidden layers on MNIST. See Table 5 for the results.
We see that as the capacity of the decoder increases, the ap-
proximation gap decreases. This result implies that the more
ﬂexible the generator is, the less ﬂexible the approximate
distribution needs to be to ensure accurate inference.

5.5. Inference Generalization

How well does amortized inference generalize at test time?
We address this question by visualizing the gaps on training
and validation datapoints across the training epochs. In
Fig. 3, the models are trained on 50000 binarized Fashion-
MNIST datapoints and the gaps are computed on a subset
of a 100 training and validation datapoints. The top and
bottom boundaries of the blue region represent log ˆp(x) and
L[q∗]. The bottom boundary of the orange region represents
L[q]. In other words, the blue region is the approximation
gap and the orange is the amortization gap.

In Fig. 3, the Standard model (top left) refers to a VAE of
latent size 20 trained with a factorized Gaussian approxi-
mate posterior. In this case, the encoder and decoder both
have two hidden layers each consisting of 200 hidden units.
The Flow model (top right) augments the Standard model
with a qF low variational distribution. Larger Decoder and
Larger Encoder models have factorized Gaussian distribu-
tions and increase the number of hidden layers to three and
the number of units in each layer to 500.

Firstly, we observe that for all models, the approximation
gap on the training and validation sets are roughly equivalent.
This indicates that the true posteriors of the held-out data are
similar to that of the training data. Secondly, we note that
for all models, the encoder overﬁts more than the decoder.

Inference Suboptimality in Variational Autoencoders

These observations resonate with the encoder overﬁtting
ﬁndings by Wu et al. (2017).

approximate distribution:

How does increasing decoder capacity affect inference on
held-out data? We know from Section 5.4 that increas-
ing generator capacity results in a posterior that better ﬁts
the approximation making posterior inference easier. Fur-
thermore, the Larger Decoder plot of Fig. 3 shows that
increasing generator capacity causes the model to be more
prone to overﬁtting. Thus, there is a tradeoff between ease
of inference and decoder overﬁtting.

5.5.1. ENCODER CAPACITY AND APPROXIMATION

EXPRESSIVENESS

We have seen in Sections 5.2 and 5.3 that expressive approx-
imations as well as increasing encoder capacity can lead to
a reduction in the amortization gap. This leads us to the fol-
lowing question: when should we increase encoder capacity
versus increasing the expressiveness of the approximation?
We answer this question in terms of how well each model
can generalize its efﬁcient inference (recognition network
and variational distribution) to held-out data.

In Fig. 3, we see that the Flow model and the Larger Encoder
model achieve similar log ˆp(x) on the validation set at the
end of training. However, we see that the L[q] bound of the
Larger Encoder model is signiﬁcantly lower than the L[q]
bound of the Flow model due to the encoder overﬁtting to
the training data. Although they both model the data nearly
equally well, the recognition net of the Larger Encoder
model is no longer suitable to perform inference on the
held-out data due to overﬁtting. Thus a potential rational
for utilizing expressive approximations is that they improve
generalization to held-out data in comparison to increasing
the encoder capacity.

We highlight that, in many scenarios, efﬁcient test time infer-
ence is not required and consequently, encoder overﬁtting
is not an issue, since we can use non-efﬁcient encoder-
independent methods to estimate log p(x), such as AIS,
IWAE with local optimization, or potentially retraining the
encoder on the held-out data. In contrast, when efﬁcient test
time inference is required, encoder generalization is impor-
tant and expressive approximations are likely advantageous.

5.6. Annealing the Entropy

Typical warm-up (Bowman et al., 2015; Sønderby et al.,
2016) refers to annealing the KL (q(z|x)||p(z)) term during
training. This can also be interpreted as performing maxi-
mum likelihood estimation (MLE) early on during training.
This optimization technique is known to help prevent the
latent variable from degrading to the prior (Burda et al.,
2016; Sønderby et al., 2016). We employ a similar anneal-
ing scheme during training by annealing the entropy of the

Ez∼q(z|x) [log p(x, z) − λ log q(z|x)] ,

where λ is annealed from 0 to 1 over training. This can be
interpreted as maximum a posteriori (MAP) in the initial
phase of training.

We ﬁnd that warm-up techniques, such as annealing the
entropy, are important for allowing the true posterior to be
more complex. Table 3 (right) are results from a model
trained without the entropy annealing schedule. Comparing
these results to Table 2, we observe that the difference be-
tween LVAE[q∗
AF ] is signiﬁcantly smaller
without entropy annealing. This indicates that the true pos-
terior is more Gaussian when entropy annealing is not used.
This suggests that, in addition to preventing the latent vari-
able from degrading to the prior, entropy annealing allows
the true posterior to better utilize the ﬂexibility of the ex-
pressive approximation.

F F G] and LVAE[q∗

6. Conclusion

In this paper, we investigated how encoder capacity, approx-
imation choice, decoder capacity, and model optimization
inﬂuence inference suboptimality in terms of the approxima-
tion and amortization gaps. We discovered that the amortiza-
tion gap can be a leading source to inference suboptimality
and that the generator can reduce the approximation gap by
learning a true posterior that ﬁts to the choice of approxima-
tion. We showed that the parameters used to increase the
expressiveness of the approximation play a role in general-
izing inference rather than simply improving the complexity
of the approximation. We conﬁrmed that increasing the
capacity of the encoder reduces the amortization error. Ad-
ditionally, we demonstrated that optimization techniques,
such as entropy annealing, help the generative model to
better utilize the ﬂexibility of expressive variational distri-
butions. Analyzing these gaps can be useful for guiding
improvements in VAEs. Future work includes evaluating
other types of expressive approximations, more complex
likelihood functions, and datasets.

References

Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Joze-
fowicz, R., and Bengio, S. Generating Sentences from a
Continuous Space. ArXiv e-prints, November 2015.

Burda, Y., Grosse, R., and Salakhutdinov, R. Importance

weighted autoencoders. In ICLR, 2016.

Clevert, Djork-Arn´e, Unterthiner, Thomas, and Hochre-
iter, Sepp.
Fast and accurate deep network learn-
ing by exponential linear units (elus). arXiv preprint
arXiv:1511.07289, 2015.

Inference Suboptimality in Variational Autoencoders

Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density estima-

tion using Real NVP. ICLR, 2017.

Geweke, John. Bayesian inference in econometric models
using monte carlo integration. Econometrica: Journal of
the Econometric Society, pp. 1317–1339, 1989.

Glorot, Xavier and Bengio, Yoshua. Understanding the
difﬁculty of training deep feedforward neural networks.
In Proceedings of the Thirteenth International Confer-
ence on Artiﬁcial Intelligence and Statistics, pp. 249–256,
2010.

Gomez, Aidan N, Ren, Mengye, Urtasun, Raquel, and
Grosse, Roger B. The reversible residual network: Back-
propagation without storing activations. In Advances in
Neural Information Processing Systems, pp. 2211–2221,
2017.

Grosse, R., Ghahramani, Z., and Adams, R. P. Sandwiching
the marginal likelihood using bidirectional monte carlo.
arXiv preprint arXiv:1511.02543, 2015.

Grosse, Roger B, Ancha, Siddharth, and Roy, Daniel M.
Measuring the reliability of mcmc inference with bidirec-
tional monte carlo. In Advances in Neural Information
Processing Systems, pp. 2451–2459, 2016.

Hoffman, Matthew D. Learning deep latent gaussian models
with markov chain monte carlo. In International Confer-
ence on Machine Learning, pp. 1510–1519, 2017.

Hoffman, Matthew D, Blei, David M, Wang, Chong, and
Paisley, John. Stochastic variational inference. The Jour-
nal of Machine Learning Research, 14(1):1303–1347,
2013.

Jarzynski, C. Nonequilibrium equality for free energy dif-
ferences. Physical Review Letters, 78(14):2690, 1997.

Kingma, Diederik and Ba, Jimmy. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.

Kingma, D.P. and Welling, M. Auto-Encoding Variational

Bayes. In ICLR, 2014.

Kingma, D.P., Salimans, T., Jozefowicz, R., Chen, X.,
Sutskever, I., and Welling, M.
Improving Variational
Inference with Inverse Autoregressive Flow. NIPS, 2016.

Krishnan, R. G., Liang, D., and Hoffman, M. On the chal-
lenges of learning with inference networks on sparse,
high-dimensional data. ArXiv e-prints, October 2017.

Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple
layers of features from tiny images. University of Toronto,
2009.

Larochelle, Hugo and Bengio, Yoshua. Classiﬁcation using
discriminative restricted boltzmann machines. In Pro-
ceedings of the 25th international conference on Machine
learning, pp. 536–543. ACM, 2008.

LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 1998.

Maaløe, L., Sønderby, CK., Sønderby, SK., and Winther, O.

Auxiliary Deep Generative Models. ICML, 2016.

Neal, R.M. Annealed importance sampling. Statistics and

Computing, 2001.

Ranganath, R., Tran, D., and Blei, D. M. Hierarchical

Variational Models. ICML, 2016.

Rezende, D.J. and Mohamed, S. Variational Inference with

Normalizing Flows. In ICML, 2015.

Rezende, D.J., Mohamed, S., and Wierstra, D. Stochastic
Backpropagation and Approximate Inference in Deep
Generative Models. ICML, 2014.

Salakhutdinov, R. and Murray, I. On the quantitative analy-
sis of deep belief networks. In Proceedings of the 25th
international conference on Machine learning, pp. 872–
879. ACM, 2008.

Salimans, T., Kingma, D.P., and Welling, M. Markov chain
monte carlo and variational inference: Bridging the gap.
In ICML, 2015.

Sønderby, Casper Kaae, Raiko, Tapani, Maaløe, Lars,
Sønderby, Søren Kaae, and Winther, Ole. Ladder varia-
tional autoencoders. In Advances in Neural Information
Processing Systems, pp. 3738–3746, 2016.

Tomczak, J. M. and Welling, M.

Improving Variational
Auto-Encoders using Householder Flow. ArXiv e-prints,
November 2016.

Tomczak, J. M. and Welling, M.

Improving Variational
Auto-Encoders using convex combination linear Inverse
Autoregressive Flow. ArXiv e-prints, June 2017.

Turner, R and Sahani, M. Two problems with variational ex-
pectation maximisation for time-series models. inference
and learning in dynamic models. Cambridge University
Press, pp. 104–123, 2011.

Wu, Y., Burda, Y., Salakhutdinov, R., and Grosse, R. On
the Quantitative Analysis of Decoder-Based Generative
Models. ICLR, 2017.

Xiao, Han, Rasul, Kashif, and Vollgraf, Roland. Fashion-
mnist: a novel image dataset for benchmarking machine
learning algorithms. github.com/zalandoresearch/fashion-
mnist, 2017.

Supplementary

Code for the experiments in this paper can be found at:
https://github.com/chriscremer/Inference-Suboptimality and
https://github.com/lxuechen/inference-suboptimality.

6.1.3. 3-BIT CIFAR

6.1. Model Architectures and Training

Hyperparameters

6.1.1. 2D VISUALIZATION

The VAE model of Fig. 2 uses a decoder p(x|z) with ar-
chitecture: 2 − 100 − 784, and an encoder q(z|x) with
architecture: 784 − 100 − 4. We use tanh activations and a
batch size of 50. The model is trained for 3000 epochs with
a learning rate of 10−4 using the ADAM optimizer (Kingma
& Ba, 2014).

6.1.2. MNIST & FASHION-MNIST

Both MNIST and Fashion-MNIST consist of a training and
test set with 60000 and 10000 datapoints respectively, where
each datapoint is a 28x28 grey-scale image. We rescale the
original images so that pixel values are within the range
[0, 1]. For MNIST, We use the statically binarized version
described by (Larochelle & Bengio, 2008). We also binarize
Fashion-MINST statically. For both datasets, we adopt the
Bernoulli likelihood for the generator.

The VAE models for MNIST and Fashion-MNIST exper-
iments have the same architecture. The encoder has two
hidden layers with 200 units each. The activation function is
chosen to be the exponential linear unit (ELU, Clevert et al.
(2015)), as we observe improved performance compared to
tanh. The latent space has 50 dimensions. The generator
is the reverse of the encoder. We follow the same learning
rate schedule and train for the same amount of epochs as
described by (Burda et al., 2016). All models are trained
with the a batch-size of 100 with ADAM.

In the large encoder setting, we change the number of hidden
units for the inference network to be 500, instead of 200.
The warm-up models are trained with a linear schedule over
the ﬁrst 400 epochs according to Section 5.6.

The auxiliary variable of requires a couple distributions:
q(v0|z0) and r(vT |zT ). These distributions are both factor-
ized Gaussians which are parameterized by MLP’s with two
hidden layers, 100 units each, with ELU activations.

The ﬂow transformation q(zt+1, vt+1|zt, vt) involves func-
tions σ1, σ2, µ1, and µ2 from Eqn. 9 and 10. These also
have two hidden layers with 100 units each and ELU units.

CIFAR-10 consists of a training and test dataset with 50000
and 10000 datapoints respectively, where each datapoint is
a 32 × 32 RGB image. We rescale individual pixel values
to be in the range [0, 1]. We then statically binarize the
scaled pixel values by setting individual pixel values of
channels to 1 if the rescaled value is greater than 0.5 and
0 otherwise. In this manner, we can model the observation
with a factorized Bernoulli likelihood. We call this binarized
CIFAR-10 dataset as 3-BIT CIFAR, since 3 bits are required
to encode each pixel, where 1 bit is needed for each of the
channels. We acknowledge that such binarization scheme
may reduce the complexity of the original problem, since
originally 24 bits were required to encode a single pixel.
Nevertheless, the 3-bit CIFAR dataset is still much more
challenging compared MNIST and Fashion. This is because
784 bits are required to encode one MNIST/Fashion image,
whereas for one 3-bit CIFAR image, 3072 bits are required.
Most notably, we were able to validate our AIS estimates
using BDMC with the simpliﬁed dataset. This, however,
was not achievable in any reasonable amount of time with
the original CIFAR-10 dataset.

For the latent variable, we use a 50-dimensional factorized
Gaussian for q(z|x). For all neural networks, ELU is cho-
sen to be the activation function. The inference network
consists of three 4 by 4 convolution layers with stride 2,
batch-norm, and 64, 128, 256 channels respectively. Then a
fully-connected layer outputs the 50-dimensional mean and
log-variance of the latent variable. Similarly, the generator
consists of a fully-connected layer outputting 256 by 2 by 2
tensors. Then three deconvolutional layers each with 4 by
4 ﬁlters, stride 2, batch-norm, and 128, 64, and 3 channels
respectively. For the model with expressive inference, we
use three normalizing ﬂow steps, where the parametric func-
tions in the ﬂow and auxiliary variable distribution also take
in a hidden layer of the encoder.

We use a learning rate of 10−3. Warm-up is applied with
a linear schedule over the ﬁrst 50 epochs. All models are
trained with a batch-size of 100 with ADAM. Early-stopping
is applied based on the performance computed with the
IWAE bound (k=1000) on the held-out set of 5000 examples
from the original training set.

Inference Suboptimality in Variational Autoencoders

6.2. Inference Generalization

These models are trained with batch size 50 and latent di-
mension size of 20. The rest of the hyperparameters are
equivalent to Section 6.1.2.

Architecture of qF low: The ﬂow transformation involves
functions σ1, σ2, µ1, and µ2 from Eqn. 9 and 10. Each
function is an MLP with a 50 unit hidden layer and ELU
activations. We apply this ﬂow transformation twice.

Fig. 4 are the plots for the qAF model. The transformations
are the same as qF low, but rather than partitioning the latent
variable, we introduce an auxiliary variable. The auxiliary
variable also requires a reverse model r(v|z) which is a
factorized Gaussian parameterized by an MLP with a 50
unit hidden layer and ELU activations.

Comparing AF in Fig. 4 to Flow in Fig. 3, we see that the
AF has a larger approximation gap. This increase is likely
due to the KL (q(v|z, x)(cid:107)r(v|x, z)) term of the auxiliary
variable lower bound from 2.2.2. This motivates also using
expressive approximations for the reverse model r(v|z).

Figure 4. Gaps over epochs of the AF (auxiliary ﬂow) model.

6.3. Inﬂuence of Flows On Amortization Gap

Experiment

The aim of this experiment is to show that the parameters
used for increasing the expressiveness of the approxima-
tion also contribute to reducing the amortization error. To
show this, we train a VAE on MNIST, discard the encoder,
then retrain two encoders on the ﬁxed decoder: one with a
factorized Gaussian distribution and the other with a param-
eterized ’ﬂow’ distribution. We use ﬁxed decoder so that
the true posterior is constant for both encoders. See 5.3 for
the results and below for the architecture details.

The architecture of the decoder is: DZ − 200 − 200 − DX .
The architecture of the encoder used to train the decoder
is DX − 200 − 200 − 2DZ. The approximate distribution
q(z|x) is a factorized Gaussian.

Next, we describe the encoders which were trained on the
ﬁxed trained decoder. In order to highlight a large amorti-
zation gap, we employed a very small encoder architecture:
DX − 2DZ. This encoder has no hidden layers, which

greatly impoverishes its ability and results in a large amorti-
zation gap.

We compare two approximate distributions q(z|x). Firstly,
we experiment with the typical fully factorized Gaussian
(FFG). The second is what we call a ﬂow distribution.
Speciﬁcally, we use the transformations of (Dinh et al.,
2017). We also include an auxiliary variable so we don’t
need to select how to divide the latent space for the trans-
formations. The approximate distribution over the la-
tent z and auxiliary variable v factorizes as: q(z, v|x) =
q(z|x)q(v). The q(v) distribution is simply a N(0,1) dis-
tribution. Since we’re using a auxiliary variable, we also
require the r(v|z) distribution which we parameterize as
r(v|z): [DZ] − 50 − 50 − 2DZ. The ﬂow transformation
is the same as in Section 3.2, which we apply twice.

6.4. Computation of the Determinant for Flow

The overall mapping f that performs (z, v) (cid:55)→ (z(cid:48), v(cid:48)) is
the composition of two sheer mappings f1 and f2 that re-
spectively perform (z, v) (cid:55)→ (z, v(cid:48)) and (z, v(cid:48)) (cid:55)→ (z(cid:48), v(cid:48)).
Since the Jacobian of either one of the sheer mappings is
diagonal, the determinant of the composed transformation’s
Jacobian Df can be easily computed:

det(Df ) = det(Df1)det(Df2)

=

(cid:16) n
(cid:89)

i=1

(cid:17)(cid:16) n
(cid:89)

σ2(v(cid:48))j

(cid:17)

.

σ1(z)i

j=1

6.5. Annealed Importance Sampling

Annealed importance sampling (AIS, Neal (2001); Jarzyn-
ski (1997)) is a means of computing a lower bound to
the marginal log-likelihood. Similarly to the importance
weighted bound, AIS must sample a proposal distribution
f1(z) and compute the density of these samples, however,
AIS then transforms the samples through a sequence of
reversible transitions Tt(z(cid:48)|z). The transitions anneal the
proposal distribution to the desired distribution fT (z).

Speciﬁcally, AIS samples an initial state z1 ∼ f1(z) and
sets an initial weight w1 = 1. For the following annealing
steps, zt is sampled from Tt(z(cid:48)|z) and the weight is updated
according to:

wt = wt−1

ft(zt−1)
ft−1(zt−1)

.

This procedure produces weight wT such that E [wT ] =
ZT /Z1, where ZT and Z1 are the normalizing constants of
fT (z) and f1(z) respectively. This pertains to estimating the
marginal likelihood when the target distribution is p(x, z)
when we integrate with respect to z.

Typically, the intermediate distributions are simply deﬁned
to be geometric averages: ft(z) = f1(z)1−βtfT (z)βt,

Inference Suboptimality in Variational Autoencoders

where βt is monotonically increasing with β1 = 0 and βT = 1.
When f1(z) = p(z) and fT (z) = p(x, z), the intermediate
distributions are: fi(x) = p(z)p(x|z)βi .

Model evaluation with AIS appears early on in the setting
of deep belief networks (Salakhutdinov & Murray, 2008).
AIS for decoder-based models was also used by Wu et al.
(2017).

6.6. Extra MNIST Inference Gaps

To demonstrate that a very small inference gap can be
achieved, even with a limited approximation such as a fac-
torized Gaussian, we train the model on a small dataset. In
this experiment, our training set consists of 1000 datapoints
randomly chosen from the original MNIST training set. The
training curves on this small datatset are shown in Fig. 5.
Even with a factorized Gaussian distribution, the inference
gap is very small: the AIS and IWAE bounds are overlap-
ping and the VAE is just slightly below. Yet, the model is
overﬁtting as seen by the decreasing test set bounds.

Figure 5. Training curves for a FFG and a Flow inference model on
MNIST. AIS provides the tightest lower bound and is independent
of encoder overﬁtting. There is little difference between FFG and
Flow models trained on the 1000 datapoints since inference is
nearly equivalent.

