Efﬁcient Architecture Search by Network Transformation

Han Cai1, Tianyao Chen1, Weinan Zhang1∗, Yong Yu1, Jun Wang2
1Shanghai Jiao Tong University, 2University College London
{hcai,tychen,wnzhang,yyu}@apex.sjtu.edu.cn, j.wang@cs.ucl.ac.uk

7
1
0
2
 
v
o
N
 
1
2
 
 
]

G
L
.
s
c
[
 
 
2
v
3
7
8
4
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

Techniques for automatically designing deep neural net-
work architectures such as reinforcement learning based ap-
proaches have recently shown promising results. However,
their success is based on vast computational resources (e.g.
hundreds of GPUs), making them difﬁcult to be widely used.
A noticeable limitation is that they still design and train each
network from scratch during the exploration of the architec-
ture space, which is highly inefﬁcient. In this paper, we pro-
pose a new framework toward efﬁcient architecture search by
exploring the architecture space based on the current network
and reusing its weights. We employ a reinforcement learn-
ing agent as the meta-controller, whose action is to grow the
network depth or layer width with function-preserving trans-
formations. As such, the previously validated networks can
be reused for further exploration, thus saves a large amount
of computational cost. We apply our method to explore the
architecture space of the plain convolutional neural networks
(no skip-connections, branching etc.) on image benchmark
datasets (CIFAR-10, SVHN) with restricted computational
resources (5 GPUs). Our method can design highly com-
petitive networks that outperform existing networks using
the same design scheme. On CIFAR-10, our model with-
out skip-connections achieves 4.23% test error rate, exceed-
ing a vast majority of modern architectures and approaching
DenseNet. Furthermore, by applying our method to explore
the DenseNet architecture space, we are able to achieve more
accurate networks with fewer parameters.

Introduction
The great success of deep neural networks in various chal-
lenging applications (Krizhevsky, Sutskever, and Hinton
2012; Bahdanau, Cho, and Bengio 2014; Silver et al. 2016)
has led to a paradigm shift from feature designing to archi-
tecture designing, which still remains a laborious task and
requires human expertise. In recent years, many techniques
for automating the architecture design process have been
proposed (Snoek, Larochelle, and Adams 2012; Bergstra
and Bengio 2012; Baker et al. 2017; Zoph and Le 2017;
Real et al. 2017; Negrinho and Gordon 2017), and promis-
ing results of designing competitive models against human-
designed models are reported on some benchmark datasets

∗Correspondence to Weinan Zhang.

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

(Zoph and Le 2017; Real et al. 2017). Despite the promising
results as reported, their success is based on vast computa-
tional resources (e.g. hundreds of GPUs), making them dif-
ﬁcult to be used in practice for individual researchers, small
sized companies, or university research teams. Another key
drawback is that they still design and train each network
from scratch during exploring the architecture space without
any leverage of previously explored networks, which results
in high computational resources waste.

In fact, during the architecture design process, many
slightly different networks are trained for the same task.
Apart from their ﬁnal validation performances that are used
to guide exploration, we should also have access to their
architectures, weights, training curves etc., which contain
abundant knowledge and can be leveraged to accelerate the
architecture design process just like human experts (Chen,
Goodfellow, and Shlens 2015; Klein et al. 2017). Further-
more, there are typically many well-designed architectures,
by human or automatic architecture designing methods, that
have achieved good performances at the target task. Under
restricted computational resources limits, instead of totally
neglecting these existing networks and exploring the archi-
tecture space from scratch (which does not guarantee to re-
sult in better performance architectures), a more economical
and efﬁcient alternative could be exploring the architecture
space based on these successful networks and reusing their
weights.

In this paper, we propose a new framework, called EAS,
Efﬁcient Architecture Search, where the meta-controller ex-
plores the architecture space by network transformation op-
erations such as widening a certain layer (more units or ﬁl-
ters), inserting a layer, adding skip-connections etc., given
an existing network trained on the same task. To reuse
weights, we consider the class of function-preserving trans-
formations (Chen, Goodfellow, and Shlens 2015) that allow
to initialize the new network to represent the same function
as the given network but use different parameterization to
be further trained to improve the performance, which can
signiﬁcantly accelerate the training of the new network es-
pecially for large networks. Furthermore, we combine our
framework with recent advances of reinforcement learn-
ing (RL) based automatic architecture designing methods
(Baker et al. 2017; Zoph and Le 2017), and employ a RL
based agent as the meta-controller.

Our experiments of exploring the architecture space of
the plain convolutional neural networks (CNNs), which
purely consists of convolutional, fully-connected and pool-
ing layers without skip-connections, branching etc., on im-
age benchmark datasets (CIFAR-10, SVHN), show that EAS
with limited computational resources (5 GPUs) can design
competitive architectures. The best plain model designed
by EAS on CIFAR-10 with standard data augmentation
achieves 4.23% test error rate, even better than many modern
architectures that use skip-connections. We further apply our
method to explore the DenseNet (Huang et al. 2017) archi-
tecture space, and achieve 4.66% test error rate on CIFAR-
10 without data augmentation and 3.44% on CIFAR-10 with
standard data augmentation, surpassing the best results given
by the original DenseNet while still maintaining fewer pa-
rameters.

Related Work and Background
Automatic Architecture Designing There is a long stand-
ing study on automatic architecture designing. Neuro-
evolution algorithms which mimic the evolution processes
in the nature, are one of the earliest automatic architec-
ture designing methods (Miller, Todd, and Hegde 1989;
Stanley and Miikkulainen 2002). Authors in (Real et al.
2017) used neuro-evolution algorithms to explore a large
CNN architecture space and achieved networks which can
match performances of human-designed models. In paral-
lel, automatic architecture designing has also been stud-
ied in the context of Bayesian optimization (Bergstra and
Bengio 2012; Domhan, Springenberg, and Hutter 2015;
Mendoza et al. 2016). Recently, reinforcement learning is in-
troduced in automatic architecture designing and has shown
strong empirical results. Authors in (Baker et al. 2017) pre-
sented a Q-learning agent to sequentially pick CNN layers;
authors in (Zoph and Le 2017) used an auto-regressive recur-
rent network to generate a variable-length string that speci-
ﬁes the architecture of a neural network and trained the re-
current network with policy gradient.

As the above solutions rely on designing or training
networks from scratch, signiﬁcant computational resources
have been wasted during the construction. In this paper,
we aim to address the efﬁciency problem. Technically, we
allow to reuse the existing networks trained on the same
task and take network transformation actions. Both function-
preserving transformations and an alternative RL based
meta-controller are used to explore the architecture space.
Moreover, we notice that there are some complementary
techniques, such as learning curve prediction (Klein et al.
2017), for improving the efﬁciency, which can be combined
with our method.

Network Transformation and Knowledge Transfer
Generally, any modiﬁcation to a given network can be
viewed as a network transformation operation. In this pa-
per, since our aim is to utilize knowledge stored in previ-
ously trained networks, we focus on identifying the kind
of network transformation operations that would be able to
reuse pre-existing models. The idea of reusing pre-existing
models or knowledge transfer between neural networks

has been studied before. Net2Net technique introduced in
(Chen, Goodfellow, and Shlens 2015) describes two speciﬁc
function-preserving transformations, namely Net2WiderNet
and Net2DeeperNet, which respectively initialize a wider or
deeper student network to represent the same functionality
of the given teacher network and have proved to signiﬁcantly
accelerate the training of the student network especially for
large networks. Similar function-preserving schemes have
also been proposed in ResNet particularly for training very
deep architectures (He et al. 2016a). Additionally, the net-
work compression technique presented in (Han et al. 2015)
prunes less important connections (low-weight connections)
in order to shrink the size of neural networks without reduc-
ing their accuracy.

In this paper, instead, we focus on utilizing such network
transformations to reuse pre-existing models to efﬁciently
and economically explore the architecture space for auto-
matic architecture designing.

Reinforcement Learning Background Our meta-
in this work is based on RL (Sutton and
controller
Barto 1998), techniques for training the agent to max-
imize the cumulative reward when interacting with
an environment (Cai et al. 2017). We use the REIN-
FORCE algorithm (Williams 1992) similar
to (Zoph
and Le 2017) for updating the meta-controller, while
other advanced policy gradient methods (Kakade 2002;
Schulman et al. 2015) can be applied analogously. Our
action space is, however, different with that of (Zoph and Le
2017) or any other RL based approach (Baker et al. 2017),
as our actions are the network transformation operations
like adding, deleting, widening, etc., while others are
speciﬁc conﬁgurations of a newly created network layer
on the top of preceding layers. Speciﬁcally, we model the
automatic architecture design procedure as a sequential
decision making process, where the state is the current
network architecture and the action is the corresponding
network transformation operation. After T steps of network
transformations, the ﬁnal network architecture, along with
its weights transferred from the initial input network, is then
trained in the real data to get the validation performance
to calculate the reward signal, which is further used to
update the meta-controller via policy gradient algorithms
to maximize the expected validation performances of the
designed networks by the meta-controller.

Architecture Search by Net Transformation
In this section, we ﬁrst introduce the overall framework
of our meta-controller, and then show how each speciﬁc
network transformation decision is made under it. We
later extend the function-preserving transformations to the
DenseNet (Huang et al. 2017) architecture space where di-
rectly applying the original Net2Net operations can be prob-
lematic since the output of a layer will be fed to all subse-
quent layers.

We consider learning a meta-controller to generate net-
work transformation actions given the current network ar-
chitecture, which is speciﬁed with a variable-length string
(Zoph and Le 2017). To be able to generate various types

Figure 1: Overview of the RL based meta-controller in EAS,
which consists of an encoder network for encoding the ar-
chitecture and multiple separate actor networks for taking
network transformation actions.

Figure 2: Net2Wider actor, which uses a shared sigmoid
classiﬁer to simultaneously determine whether to widen
each layer based on its hidden state given by the encoder
network.

of network transformation actions while keeping the meta-
controller simple, we use an encoder network to learn a low-
dimensional representation of the given architecture, which
is then fed into each separate actor network to generate
a certain type of network transformation actions. Further-
more, to handle variable-length network architectures as in-
put and take the whole input architecture into considera-
tion when making decisions, the encoder network is imple-
mented with a bidirectional recurrent network (Schuster and
Paliwal 1997) with an input embedding layer. The overall
framework is illustrated in Figure 1, which is an analogue
of end-to-end sequence to sequence learning (Sutskever,
Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2014).

Actor Networks
Given the low dimensional representation of the input archi-
tecture, each actor network makes necessary decisions for
taking a certain type of network transformation actions. In
this work, we introduce two speciﬁc actor networks, namely
Net2Wider actor and Net2Deeper actor which correspond to
Net2WiderNet and Net2DeeperNet respectively.

Net2Wider Actor Net2WiderNet operation allows to re-
place a layer with a wider layer, meaning more units for
fully-connected layers, or more ﬁlters for convolutional lay-
ers, while preserving the functionality. For example, con-
sider a convolutional layer with kernel Kl whose shape is
(kl
i , f l
h, f l
h denote the ﬁlter width and
height, while f l
o denote the number of input and out-
put channels. To replace this layer with a wider layer that
has ˆf l
o) output channels, we should ﬁrst introduce a
random remapping function Gl, which is deﬁned as

o) where kl
i and f l

w and kl

o (> f l

w, kl

(cid:40)

Gl(j) =

j
random sample from {1, · · · , f l

1 ≤ j ≤ f l
o
o < j ≤ ˆf l

o

o} f l

.

(1)

With the remapping function Gl, we have the new kernel ˆKl
for the wider layer with shape (kl

i , ˆf l
o)
ˆK l[x, y, i, j] = K l[x, y, i, Gl(j)].

w, kl

h, f l

(2)

As such, the ﬁrst f l
of ˆKl are directly copied from Kl while the remaining ˆf l

o entries in the output channel dimension
o −

f l
o entries are created by choosing randomly as deﬁned in
Gl. Accordingly, the new output of the wider layer is ˆOl
with ˆOl(j) = Ol(Gl(j)), where Ol is the output of the
original layer and we only show the channel dimension to
make the notation simpler.

To preserve the functionality, the kernel Kl+1 of the next
layer should also be modiﬁed due to the replication in its
input. The new kernel ˆKl+1 with shape (kl+1
i =
ˆf l
o, f l+1
o

h , ˆf l+1

w , kl+1

) is given as

ˆK l+1[x, y, j, k] =

K l+1[x, y, Gl(j), k]
(cid:12){z|Gl(z) = Gl(j)}(cid:12)
(cid:12)
(cid:12)

.

(3)

For further details, we refer to the original Net2Net work
(Chen, Goodfellow, and Shlens 2015).

In our work, to be ﬂexible and efﬁcient, the Net2Wider
actor simultaneously determines whether each layer should
be extended. Speciﬁcally, for each layer, this decision is
carried out by a shared sigmoid classiﬁer given the hid-
den state of the layer learned by the bidirectional encoder
network. Moreover, we follow previous work and search
the number of ﬁlters for convolutional layers and units for
fully-connected layers in a discrete space. Therefore, if the
Net2Wider actor decides to widen a layer, the number of
ﬁlters or units of the layer increases to the next discrete
level, e.g. from 32 to 64. The structure of Net2Wider actor
is shown in Figure 2.

Net2Deeper Actor Net2DeeperNet operation allows to
insert a new layer that is initialized as adding an identity
mapping between two layers so as to preserve the functional-
ity. For a new convolutional layer, the kernel is set to be iden-
tity ﬁlters while for a new fully-connected layer, the weight
matrix is set to be identity matrix. Thus the new layer is set
with the same number of ﬁlters or units as the layer below
at ﬁrst, and could further get wider when Net2WiderNet op-
eration is performed on it. To fully preserve the functional-
ity, Net2DeeperNet operation has a constraint on the activa-
tion function φ, i.e. φ must satisfy φ(Iφ(v)) = φ(v) for all
vectors v. This property holds for rectiﬁed linear activation
(ReLU) but fails for sigmoid and tanh activation. However,
we can still reuse weights of existing networks with sigmoid

et al. 2017), directly applying the original Net2Net opera-
tions can be problematic. In this section, we introduce sev-
eral extensions to the original Net2Net operations to enable
function-preserving transformations for DenseNet.

Different from the plain CNN, in DenseNet, the lth layer
would receive the outputs of all preceding layers as input,
which are concatenated on the channel dimension, denoted
as [O0, O1, · · · , Ol−1], while its output Ol would be fed to
all subsequent layers.

Denote the kernel of the lth layer as Kl with shape
o). To replace the lth layer with a wider layer
i , f l
(kl
h, f l
w, kl
that has ˆf l
o output channels while preserving the function-
ality, the creation of the new kernel ˆKl in the lth layer
is the same as the original Net2WiderNet operation (see
Eq. (1) and Eq. (2)). As such, the new output of the wider
layer is ˆOl with ˆOl(j) = Ol(Gl(j)), where Gl is the
random remapping function as deﬁned in Eq. (1). Since
the output of the lth layer will be fed to all subsequent
layers in DenseNet, the replication in ˆOl will result in
replication in the inputs of all layers after the lth layer.
As such, instead of only modifying the kernel of the next
layer as done in the original Net2WiderNet operation, we
need to modify the kernels of all subsequent layers in
DenseNet. For the mth layer where m > l, its input be-
comes [O0, · · · , Ol−1, ˆOl, Ol+1, · · · , Om−1] after widen-
ing the lth layer, thus from the perspective of mth layer, the
equivalent random remapping function ˆGm can be written
as

ˆGm(j) =






j
f 0:l
o +Gl(j)
j − ˆf l
o +f l
o

o

1 ≤ j ≤ f 0:l
f 0:l
o < j ≤ f 0:l
o + ˆf l
f 0:l

o + ˆf l
o < j ≤ f 0:m

o

o + ˆf l

o −f l
o

,

(4)

o = (cid:80)l−1

v=0 f v

where f 0:l
o is the number of input channels for
the lth layer, the ﬁrst part corresponds to [O0, · · · , Ol−1],
the second part corresponds to [ ˆOl], and the last part corre-
sponds to [Ol+1, · · · , Om−1]. A simple example of ˆGm is
given as

ˆGm : {1, · · · , 5,

ˆOl
(cid:122) (cid:125)(cid:124) (cid:123)
6, 7, 8, 9, 10, 11} → {1, · · · , 5,

ˆOl
(cid:122) (cid:125)(cid:124) (cid:123)
6, 7, 6, 6, 8, 9}

where Gl : {1, 2, 3, 4} → {1, 2, 1, 1}.

Accordingly the new kernel of mth layer can be given by
Eq. (3) with Gl replaced with ˆGm.

To insert a new layer in DenseNet, suppose the new
layer is inserted after the lth layer. Denote the output of
the new layer as Onew, and its input is [O0, O1, · · · , Ol].
Therefore, for the mth (m > l) layer, its new input after
the insertion is [O0, O1, · · · , Ol, Onew, Ol+1, · · · , Om−1].
To preserve the functionality, similar to the Net2WiderNet
case, Onew should be the replication of some entries in
[O0, O1, · · · , Ol]. It is possible, since the input of the new
layer is [O0, O1, · · · , Ol]. Each ﬁlter in the new layer can
be represented with a tensor, denoted as ˆF with shape
i = f 0:l+1
(knew
), where knew
denote the
width and height of the ﬁlter, and f new
is the number of in-
put channels. To make the output of ˆF to be a replication

w and knew
i

h , f new

w , knew

h

o

Figure 3: Net2Deeper actor, which uses a recurrent network
to sequentially determine where to insert the new layer and
corresponding parameters for the new layer based on the ﬁ-
nal hidden state of the encoder network given the input ar-
chitecture.

or tanh activation, which could be useful compared to ran-
dom initialization. Additionally, when using batch normal-
ization (Ioffe and Szegedy 2015), we need to set output scale
and output bias of the batch normalization layer to undo the
normalization, rather than initialize them as ones and zeros.
Further details about the Net2DeeperNet operation is pro-
vided in the original paper (Chen, Goodfellow, and Shlens
2015).

The structure of the Net2Deeper actor is shown in Fig-
ure 3, which is a recurrent network whose hidden state is
initialized with the ﬁnal hidden state of the encoder net-
work. Similar to previous work (Baker et al. 2017), we al-
low the Net2Deeper actor to insert one new layer at each
step. Speciﬁcally, we divide a CNN architecture into sev-
eral blocks according to the pooling layers and Net2Deeper
actor sequentially determines which block to insert the new
layer, a speciﬁc index within the block and parameters of the
new layer. For a new convolutional layer, the agent needs to
determine the ﬁlter size and the stride while for a new fully-
connected layer, no parameter prediction is needed. In CNN
architectures, any fully-connected layer should be on the top
of all convolutional and pooling layers. To avoid resulting in
unreasonable architectures, if the Net2Deeper actor decides
to insert a new layer after a fully-connected layer or the ﬁnal
global average pooling layer, the new layer is restricted to be
a fully-connected layer, otherwise it must be a convolutional
layer.

Function-preserving Transformation for DenseNet
The original Net2Net operations proposed in (Chen, Good-
fellow, and Shlens 2015) are discussed under the scenarios
where the network is arranged layer-by-layer, i.e. the output
of a layer is only fed to its next layer. As such, in some mod-
ern CNN architectures where the output of a layer would be
fed to multiple subsequent layers, such as DenseNet (Huang

of the nth entry in [O0, O1, · · · , Ol], we can set ˆF (using
the special case that knew
h = 3 for illustration) as the
following

w = knew

ˆF [x, y, n] =





0
0
0

0
1
0



 ,

0
0
0

(5)

while all other values in ˆF are set to be 0. Note that n can be
chosen randomly from {1, · · · , f 0:l+1
} for each ﬁlter. After
all ﬁlters in the new layer are set, we can form an equivalent
random remapping function for all subsequent layers as is
done in Eq. (4) and modify their kernels accordingly.

o

Experiments and Results

In line with the previous work (Baker et al. 2017; Zoph and
Le 2017; Real et al. 2017), we apply the proposed EAS on
image benchmark datasets (CIFAR-10 and SVHN) to ex-
plore high performance CNN architectures for the image
classiﬁcation task1. Notice that the performances of the ﬁnal
designed models largely depend on the architecture space
and the computational resources. In our experiments, we
evaluate EAS in two different settings. In all cases, we use
restricted computational resources (5 GPUs) compared to
the previous work such as (Zoph and Le 2017) that used
800 GPUs. In the ﬁrst setting, we apply EAS to explore the
plain CNN architecture space, which purely consists of con-
volutional, pooling and fully-connected layers. While in the
second setting, we apply EAS to explore the DenseNet ar-
chitecture space.

Image Datasets

CIFAR-10 The CIFAR-10 dataset (Krizhevsky and Hin-
ton 2009) consists of 50,000 training images and 10,000
test images. We use a standard data augmentation scheme
that is widely used for CIFAR-10 (Huang et al. 2017), and
denote the augmented dataset as C10+ while the original
dataset is denoted as C10. For preprocessing, we normal-
ized the images using the channel means and standard de-
viations. Following the previous work (Baker et al. 2017;
Zoph and Le 2017), we randomly sample 5,000 images from
the training set to form a validation set while using the re-
maining 45,000 images for training during exploring the ar-
chitecture space.

SVHN The Street View House Numbers (SVHN) dataset
(Netzer et al. 2011) contains 73,257 images in the original
training set, 26,032 images in the test set, and 531,131 addi-
tional images in the extra training set. For preprocessing, we
divide the pixel values by 255 and do not perform any data
augmentation, as is done in (Huang et al. 2017). We follow
(Baker et al. 2017) and use the original training set during
the architecture search phase with 5,000 randomly sampled
images as the validation set, while training the ﬁnal discov-
ered architectures using all the training data, including the
original training set and extra training set.

1Experiment code and discovered top architectures along with

weights: https://github.com/han-cai/EAS

Figure 4: Progress of two stages architecture search on C10+
in the plain CNN architecture space.

Training Details
For the meta-controller, we use a one-layer bidirectional
LSTM with 50 hidden units as the encoder network (Fig-
ure 1) with an embedding size of 16, and train it with the
ADAM optimizer (Kingma and Ba 2015).

At each step, the meta-controller samples 10 networks by
taking network transformation actions. Since the sampled
networks are not trained from scratch but we reuse weights
of the given network in our scenario, they are then trained for
20 epochs, a relative small number compared to 50 epochs in
(Zoph and Le 2017). Besides, we use a smaller initial learn-
ing rate for this reason. Other settings for training networks
on CIFAR-10 and SVHN, are similar to (Huang et al. 2017;
Zoph and Le 2017). Speciﬁcally, we use the SGD with a
Nesterov momentum (Sutskever et al. 2013) of 0.9, a weight
decay of 0.0001, a batch size of 64. The initial learning rate
is 0.02 and is further annealed with a cosine learning rate
decay (Gastaldi 2017). The accuracy in the held-out valida-
tion set is used to compute the reward signal for each sam-
pled network. Since the gain of improving the accuracy from
90% to 91% should be much larger than from 60% to 61%,
instead of directly using the validation accuracy accv as the
reward, as done in (Zoph and Le 2017), we perform a non-
linear transformation on accv, i.e. tan(accv × π/2), and use
the transformed value as the reward. Additionally, we use
an exponential moving average of previous rewards, with a
decay of 0.95 as the baseline function to reduce the variance.

Explore Plain CNN Architecture Space
We start applying EAS to explore the plain CNN archi-
tecture space. Following the previous automatic architec-
ture designing methods (Baker et al. 2017; Zoph and Le
2017), EAS searches layer parameters in a discrete and lim-
ited space. For every convolutional layer, the ﬁlter size is
chosen from {1, 3, 5} and the number of ﬁlters is cho-
sen from {16, 32, 64, 96, 128, 192, 256, 320, 384, 448, 512},
while the stride is ﬁxed to be 1 (Baker et al. 2017). For every
fully-connected layer, the number of units is chosen from
{64, 128, 256, 384, 512, 640, 768, 896, 1024}. Additionally,

Table 1: Simple start point network. C(n, f, l) denotes a convolutional layer with n ﬁlters, ﬁlter size f and stride l; P(f, l, MAX)
and P(f, l, AVG) denote a max and an average pooling layer with ﬁlter size f and stride l respectively; FC(n) denotes a fully-
connected layer with n units; SM(n) denotes a softmax layer with n output units.

Model Architecture
C(16, 3, 1), P(2, 2, MAX), C(32, 3, 1), P(2, 2, MAX), C(64, 3, 1),
P(2, 2, MAX), C(128, 3, 1), P(4, 4, AVG), FC(256), SM(10)

Validation Accuracy (%)

87.07

we use ReLU and batch normalization for each convolu-
tional or fully-connected layer. For SVHN, we add a dropout
layer after each convolutional layer (except the ﬁrst layer)
and use a dropout rate of 0.2 (Huang et al. 2017).

Start with Small Network We begin the exploration on
C10+, using a small network (see Table 1), which achieves
87.07% accuracy in the held-out validation set, as the start
point. Different from (Zoph and Le 2017; Baker et al. 2017),
EAS is not restricted to start from empty and can ﬂexibly
use any discovered architecture as the new start point. As
such, to take the advantage of such ﬂexibility and also re-
duce the search space for saving the computational resources
and time, we divide the whole architecture search process
into two stages where we allow the meta-controller to take 5
steps of Net2Deeper action and 4 steps of Net2Wider action
in the ﬁrst stage. After 300 networks are sampled, we take
the network which performs best currently and train it with
a longer period of time (100 epochs) to be used as the start
point for the second stage. Similarly, in the second stage, we
also allow the meta-controller to take 5 steps of Net2Deeper
action and 4 steps of Net2Wider action and stop exploration
after 150 networks are sampled.

The progress of the two stages architecture search is
shown in Figure 4, where we can ﬁnd that EAS gradu-
ally learns to pick high performance architectures at each
stage. As EAS takes function-preserving transformations to
explore the architecture space, we can also ﬁnd that the
sampled architectures consistently perform better than the
start point network at each stage. Thus it is usually “safe”
to explore the architecture space with EAS. We take the
top networks discovered during the second stage and fur-
ther train the networks with 300 epochs using the full train-
ing set. Finally, the best model achieves 95.11% test ac-
curacy (i.e. 4.89% test error rate). Furthermore, to justify
the transferability of the discovered networks, we train the
top architecture (95.11% test accuracy) on SVHN from ran-
dom initialization with 40 epochs using the full training
set and achieves 98.17% test accuracy (i.e. 1.83% test er-
ror rate), better than both human-designed and automatically
designed architectures that are in the plain CNN architecture
space (see Table 2).

We would like to emphasize that the required computa-
tional resources to achieve this result is much smaller than
those required in (Zoph and Le 2017; Real et al. 2017).
Speciﬁcally, it takes less than 2 days on 5 GeForce GTX
1080 GPUs with totally 450 networks trained to achieve
4.89% test error rate on C10+ starting from a small network.

Further Explore Larger Architecture Space To further
search better architectures in the plain CNN architecture

Table 2: Test error rate (%) comparison with CNNs that use
convolutional, fully-connected and pooling layers alone.

human
designed

auto
designed

Model
Maxout (Goodfellow et al. 2013)
NIN (Lin, Chen, and Yan 2013)
All-CNN (Springenberg et al. 2014)
VGGnet (Simonyan and Zisserman 2015)
MetaQNN (Baker et al. 2017) (depth=7)
MetaQNN (Baker et al. 2017) (ensemble)
EAS (plain CNN, depth=16)
EAS (plain CNN, depth=20)

C10+
9.38
8.81
7.25
7.25
6.92
-
4.89
4.23

SVHN
2.47
2.35
-
-
-
2.06
1.83
1.73

space, in the second experiment, we use the top architec-
tures discovered in the ﬁrst experiment, as the start points
to explore a larger architecture space on C10+ and SVHN.
This experiment on each dataset takes around 2 days on 5
GPUs.

The summarized results of comparing with human-
designed and automatically designed architectures that use
a similar design scheme (plain CNN), are reported in Table
2, where we can ﬁnd that the top model designed by EAS
on the plain CNN architecture space outperforms all similar
models by a large margin. Speciﬁcally, comparing to human-
designed models, the test error rate drops from 7.25% to
4.23% on C10+ and from 2.35% to 1.73% on SVHN. While
comparing to MetaQNN, the Q-learning based automatic ar-
chitecture designing method, EAS achieves a relative test er-
ror rate reduction of 38.9% on C10+ and 16.0% on SVHN.
We also notice that the best model designed by MetaQNN
on C10+ only has a depth of 7, though the maximum is set
to be 18 in the original paper (Baker et al. 2017). We sup-
pose maybe they trained each designed network from scratch
and used an aggressive training strategy to accelerate train-
ing, which resulted in many networks under performed, es-
pecially for deep networks. Since we reuse the weights of
pre-existing networks, the deep networks are validated more
accurately in EAS, and we can thus design deeper and more
accurate networks than MetaQNN.

We also report the comparison with state-of-the-art ar-
chitectures that use advanced techniques such as skip-
connections, branching etc., on C10+ in Table 3. Though it
is not a fair comparison since we do not incorporate such
advanced techniques into the search space in this experi-
ment, we still ﬁnd that the top model designed by EAS is
highly competitive even comparing to these state-of-the-art
modern architectures. Speciﬁcally, the 20-layers plain CNN
with 23.4M parameters outperforms ResNet, its stochas-
tic depth variant and its pre-activation variant. It also ap-
proaches the best result given by DenseNet. When com-
paring to automatic architecture designing methods that in-

Table 3: Test error rate (%) comparison with state-of-the-art architectures.

Model
ResNet (He et al. 2016a)
ResNet (stochastic depth) (Huang et al. 2017)
Wide ResNet (Zagoruyko and Komodakis 2016)
Wide ResNet (Zagoruyko and Komodakis 2016)
ResNet (pre-activation) (He et al. 2016b)
DenseNet (L = 40, k = 12) (Huang et al. 2017)
DenseNet-BC (L = 100, k = 12) (Huang et al. 2017)
DenseNet-BC (L = 190, k = 40) (Huang et al. 2017)
Large-Scale Evolution (250 GPUs)(Real et al. 2017)
NAS (predicting strides, 800 GPUs) (Zoph and Le 2017)
NAS (max pooling, 800 GPUs) (Zoph and Le 2017)
NAS (post-processing, 800 GPUs) (Zoph and Le 2017)
EAS (plain CNN, 5 GPUs)

Depth
110
1202
16
28
1001
40
100
190
-
20
39
39
20

Params C10+
6.61
1.7M
4.91
10.2M
4.81
11.0M
4.17
36.5M
4.62
10.2M
5.24
1.0M
4.51
0.8M
3.46
25.6M
5.40
5.4M
6.01
2.5M
4.47
7.1M
3.65
37.4M
4.23
23.4M

human
designed

auto
designed

action. The result is reported in Figure 5, which shows that
the RL based meta-controller can effectively focus on the
right search direction, while the random search cannot (left
plot), and thus ﬁnd high performance architectures more ef-
ﬁciently than random search.

Explore DenseNet Architecture Space
We also apply EAS to explore the DenseNet architecture
space. We use the DenseNet-BC (L = 40, k = 40) as
the start point. The growth rate, i.e. the width of the non-
bottleneck layer is chosen from {40, 44, 48, 52, 56, 60, 64},
and the result is reported in Table 4. We ﬁnd that by ap-
plying EAS to explore the DenseNet architecture space, we
achieve a test error rate of 4.66% on C10, better than the
best result, i.e. 5.19% given by the original DenseNet while
having 43.79% less parameters. On C10+, we achieve a test
error rate of 3.44%, also outperforming the best result, i.e.
3.46% given by the original DenseNet while having 58.20%
less parameters.

Conclusion
In this paper, we presented EAS, a new framework to-
ward economical and efﬁcient architecture search, where
the meta-controller is implemented as a RL agent. It learns
to take actions for network transformation to explore the
architecture space. By starting from an existing network
and reusing its weights via the class of function-preserving
transformation operations, EAS is able to utilize knowledge
stored in previously trained networks and take advantage
of the existing successful architectures in the target task to
explore the architecture space efﬁciently. Our experiments
have demonstrated EAS’s outstanding performance and ef-
ﬁciency compared with several strong baselines. For future
work, we would like to explore more network transforma-
tion operations and apply EAS for different purposes such
as searching networks that not only have high accuracy but
also keep a balance between the size and the performance.

Acknowledgments
This research was sponsored by Huawei Innovation Re-
search Program, NSFC (61702327) and Shanghai Sailing
Program (17YF1428200).

Figure 5: Comparison between RL based meta-controller
and random search on C10+.

Table 4: Test error rate (%) results of exploring DenseNet
architecture space with EAS.

Model
DenseNet (L = 100, k = 24)
DenseNet-BC (L = 250, k = 24)
DenseNet-BC (L = 190, k = 40)
NAS (post-processing)
EAS (DenseNet on C10)
EAS (DenseNet on C10+)

Depth
100
250
190
39
70
76

Params C10 C10+
3.74
27.2M 5.83
3.62
15.3M 5.19
3.46
25.6M
3.65
37.4M
8.6M 4.66
-
3.44
10.7M

-
-

-

corporate skip-connections into their search space, our 20-
layers plain model beats most of them except NAS with
post-processing, that is much deeper and has more param-
eters than our model. Moreover, we only use 5 GPUs and
train hundreds of networks while they use 800 GPUs and
train tens of thousands of networks.

Comparison Between RL and Random Search Our
framework is not restricted to use the RL based meta-
controller. Beside RL, one can also take network transfor-
mation actions to explore the architecture space by random
search, which can be effective in some cases (Bergstra and
Bengio 2012). In this experiment, we compare the perfor-
mances of the RL based meta-controller and the random
search meta-controller in the architecture space that is used
in the above experiments. Speciﬁcally, we use the network
in Table 1 as the start point and let the meta-controller to
take 5 steps of Net2Deeper action and 4 steps of Net2Wider

[Netzer et al. 2011] Netzer, Y.; Wang, T.; Coates, A.; Bissacco, A.;
Wu, B.; and Ng, A. Y. 2011. Reading digits in natural images with
unsupervised feature learning. In NIPS workshop on deep learning
and unsupervised feature learning.

[Real et al. 2017] Real, E.; Moore, S.; Selle, A.; Saxena, S.; Sue-
matsu, Y. L.; Le, Q.; and Kurakin, A. 2017. Large-scale evolution
of image classiﬁers. ICML.

[Schulman et al. 2015] Schulman, J.; Levine, S.; Abbeel, P.; Jordan,
M. I.; and Moritz, P. 2015. Trust region policy optimization. In
ICML.

[Schuster and Paliwal 1997] Schuster, M., and Paliwal, K. K. 1997.
Bidirectional recurrent neural networks. IEEE Transactions on Sig-
nal Processing.

[Silver et al. 2016] Silver, D.; Huang, A.; Maddison, C. J.; Guez,
A.; Sifre, L.; Van Den Driessche, G.; Schrittwieser, J.; Antonoglou,
I.; Panneershelvam, V.; Lanctot, M.; et al. 2016. Mastering the
game of go with deep neural networks and tree search. Nature.
[Simonyan and Zisserman 2015] Simonyan, K., and Zisserman, A.
2015. Very deep convolutional networks for large-scale image
recognition. ICLR.

[Snoek, Larochelle, and Adams 2012] Snoek, J.; Larochelle, H.;
and Adams, R. P. 2012. Practical bayesian optimization of ma-
chine learning algorithms. In NIPS.

[Springenberg et al. 2014] Springenberg, J. T.; Dosovitskiy, A.;
Brox, T.; and Riedmiller, M. 2014. Striving for simplicity: The
all convolutional net. arXiv preprint arXiv:1412.6806.

[Stanley and Miikkulainen 2002] Stanley, K. O., and Miikkulainen,
R. 2002. Evolving neural networks through augmenting topolo-
gies. Evolutionary computation.

[Sutskever et al. 2013] Sutskever, I.; Martens, J.; Dahl, G.; and Hin-
ton, G. 2013. On the importance of initialization and momentum
in deep learning. In ICML.

[Sutskever, Vinyals, and Le 2014] Sutskever, I.; Vinyals, O.; and
Le, Q. V. 2014. Sequence to sequence learning with neural net-
works. In NIPS.

[Sutton and Barto 1998] Sutton, R. S., and Barto, A. G. 1998. Re-

inforcement learning: An introduction. MIT press Cambridge.

[Williams 1992] Williams, R. J. 1992. Simple statistical gradient-
learning.

following algorithms for connectionist reinforcement
Machine learning.

[Zagoruyko and Komodakis 2016] Zagoruyko, S., and Komodakis,
arXiv preprint
Wide residual networks.

N.
2016.
arXiv:1605.07146.

[Zoph and Le 2017] Zoph, B., and Le, Q. V. 2017. Neural architec-

ture search with reinforcement learning. ICLR.

References
[Bahdanau, Cho, and Bengio 2014] Bahdanau, D.; Cho, K.; and
Bengio, Y. 2014. Neural machine translation by jointly learning to
align and translate. ICLR.

[Baker et al. 2017] Baker, B.; Gupta, O.; Naik, N.; and Raskar, R.
2017. Designing neural network architectures using reinforcement
learning. ICLR.

[Bergstra and Bengio 2012] Bergstra, J., and Bengio, Y. 2012. Ran-

dom search for hyper-parameter optimization. JMLR.

[Cai et al. 2017] Cai, H.; Ren, K.; Zhang, W.; Malialis, K.; Wang,
J.; Yu, Y.; and Guo, D. 2017. Real-time bidding by reinforcement
learning in display advertising. In WSDM.

[Chen, Goodfellow, and Shlens 2015] Chen, T.; Goodfellow, I.; and
Shlens, J. 2015. Net2net: Accelerating learning via knowledge
transfer. ICLR.

[Domhan, Springenberg, and Hutter 2015] Domhan, T.; Springen-
berg, J. T.; and Hutter, F. 2015. Speeding up automatic hyper-
parameter optimization of deep neural networks by extrapolation
of learning curves. In IJCAI.

[Gastaldi 2017] Gastaldi, X. 2017. Shake-shake regularization.

arXiv preprint arXiv:1705.07485.
[Goodfellow et al. 2013] Goodfellow,

J.; Warde-Farley, D.;
Mirza, M.; Courville, A.; and Bengio, Y. 2013. Maxout networks.
ICML.

I.

[Han et al. 2015] Han, S.; Pool, J.; Tran, J.; and Dally, W. 2015.
Learning both weights and connections for efﬁcient neural net-
work. In NIPS.

[He et al. 2016a] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016a.

Deep residual learning for image recognition. In CVPR.

[He et al. 2016b] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016b.

Identity mappings in deep residual networks. In ECCV.

[Huang et al. 2017] Huang, G.; Liu, Z.; Weinberger, K. Q.; and
van der Maaten, L. 2017. Densely connected convolutional net-
works. CVPR.

[Ioffe and Szegedy 2015] Ioffe, S., and Szegedy, C. 2015. Batch
normalization: Accelerating deep network training by reducing in-
ternal covariate shift. ICML.

[Kakade 2002] Kakade, S. 2002. A natural policy gradient. NIPS.
[Kingma and Ba 2015] Kingma, D., and Ba, J. 2015. Adam: A

method for stochastic optimization. ICLR.

[Klein et al. 2017] Klein, A.; Falkner, S.; Springenberg, J. T.; and
Hutter, F. 2017. Learning curve prediction with bayesian neural
networks. ICLR.

[Krizhevsky and Hinton 2009] Krizhevsky, A., and Hinton, G.

2009. Learning multiple layers of features from tiny images.

[Krizhevsky, Sutskever, and Hinton 2012] Krizhevsky,

Sutskever, I.; and Hinton, G. E.
tion with deep convolutional neural networks. In NIPS.

2012.

A.;
Imagenet classiﬁca-

[Lin, Chen, and Yan 2013] Lin, M.; Chen, Q.; and Yan, S. 2013.

Network in network. arXiv preprint arXiv:1312.4400.

[Mendoza et al. 2016] Mendoza, H.; Klein, A.; Feurer, M.; Sprin-
genberg, J. T.; and Hutter, F. 2016. Towards automatically-tuned
neural networks. In Workshop on Automatic Machine Learning.
[Miller, Todd, and Hegde 1989] Miller, G. F.; Todd, P. M.; and
Hegde, S. U. 1989. Designing neural networks using genetic algo-
rithms. In ICGA. Morgan Kaufmann Publishers Inc.

[Negrinho and Gordon 2017] Negrinho, R., and Gordon, G. 2017.
Deeparchitect: Automatically designing and training deep architec-
tures. arXiv preprint arXiv:1704.08792.

Efﬁcient Architecture Search by Network Transformation

Han Cai1, Tianyao Chen1, Weinan Zhang1∗, Yong Yu1, Jun Wang2
1Shanghai Jiao Tong University, 2University College London
{hcai,tychen,wnzhang,yyu}@apex.sjtu.edu.cn, j.wang@cs.ucl.ac.uk

7
1
0
2
 
v
o
N
 
1
2
 
 
]

G
L
.
s
c
[
 
 
2
v
3
7
8
4
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

Techniques for automatically designing deep neural net-
work architectures such as reinforcement learning based ap-
proaches have recently shown promising results. However,
their success is based on vast computational resources (e.g.
hundreds of GPUs), making them difﬁcult to be widely used.
A noticeable limitation is that they still design and train each
network from scratch during the exploration of the architec-
ture space, which is highly inefﬁcient. In this paper, we pro-
pose a new framework toward efﬁcient architecture search by
exploring the architecture space based on the current network
and reusing its weights. We employ a reinforcement learn-
ing agent as the meta-controller, whose action is to grow the
network depth or layer width with function-preserving trans-
formations. As such, the previously validated networks can
be reused for further exploration, thus saves a large amount
of computational cost. We apply our method to explore the
architecture space of the plain convolutional neural networks
(no skip-connections, branching etc.) on image benchmark
datasets (CIFAR-10, SVHN) with restricted computational
resources (5 GPUs). Our method can design highly com-
petitive networks that outperform existing networks using
the same design scheme. On CIFAR-10, our model with-
out skip-connections achieves 4.23% test error rate, exceed-
ing a vast majority of modern architectures and approaching
DenseNet. Furthermore, by applying our method to explore
the DenseNet architecture space, we are able to achieve more
accurate networks with fewer parameters.

Introduction
The great success of deep neural networks in various chal-
lenging applications (Krizhevsky, Sutskever, and Hinton
2012; Bahdanau, Cho, and Bengio 2014; Silver et al. 2016)
has led to a paradigm shift from feature designing to archi-
tecture designing, which still remains a laborious task and
requires human expertise. In recent years, many techniques
for automating the architecture design process have been
proposed (Snoek, Larochelle, and Adams 2012; Bergstra
and Bengio 2012; Baker et al. 2017; Zoph and Le 2017;
Real et al. 2017; Negrinho and Gordon 2017), and promis-
ing results of designing competitive models against human-
designed models are reported on some benchmark datasets

∗Correspondence to Weinan Zhang.

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

(Zoph and Le 2017; Real et al. 2017). Despite the promising
results as reported, their success is based on vast computa-
tional resources (e.g. hundreds of GPUs), making them dif-
ﬁcult to be used in practice for individual researchers, small
sized companies, or university research teams. Another key
drawback is that they still design and train each network
from scratch during exploring the architecture space without
any leverage of previously explored networks, which results
in high computational resources waste.

In fact, during the architecture design process, many
slightly different networks are trained for the same task.
Apart from their ﬁnal validation performances that are used
to guide exploration, we should also have access to their
architectures, weights, training curves etc., which contain
abundant knowledge and can be leveraged to accelerate the
architecture design process just like human experts (Chen,
Goodfellow, and Shlens 2015; Klein et al. 2017). Further-
more, there are typically many well-designed architectures,
by human or automatic architecture designing methods, that
have achieved good performances at the target task. Under
restricted computational resources limits, instead of totally
neglecting these existing networks and exploring the archi-
tecture space from scratch (which does not guarantee to re-
sult in better performance architectures), a more economical
and efﬁcient alternative could be exploring the architecture
space based on these successful networks and reusing their
weights.

In this paper, we propose a new framework, called EAS,
Efﬁcient Architecture Search, where the meta-controller ex-
plores the architecture space by network transformation op-
erations such as widening a certain layer (more units or ﬁl-
ters), inserting a layer, adding skip-connections etc., given
an existing network trained on the same task. To reuse
weights, we consider the class of function-preserving trans-
formations (Chen, Goodfellow, and Shlens 2015) that allow
to initialize the new network to represent the same function
as the given network but use different parameterization to
be further trained to improve the performance, which can
signiﬁcantly accelerate the training of the new network es-
pecially for large networks. Furthermore, we combine our
framework with recent advances of reinforcement learn-
ing (RL) based automatic architecture designing methods
(Baker et al. 2017; Zoph and Le 2017), and employ a RL
based agent as the meta-controller.

Our experiments of exploring the architecture space of
the plain convolutional neural networks (CNNs), which
purely consists of convolutional, fully-connected and pool-
ing layers without skip-connections, branching etc., on im-
age benchmark datasets (CIFAR-10, SVHN), show that EAS
with limited computational resources (5 GPUs) can design
competitive architectures. The best plain model designed
by EAS on CIFAR-10 with standard data augmentation
achieves 4.23% test error rate, even better than many modern
architectures that use skip-connections. We further apply our
method to explore the DenseNet (Huang et al. 2017) archi-
tecture space, and achieve 4.66% test error rate on CIFAR-
10 without data augmentation and 3.44% on CIFAR-10 with
standard data augmentation, surpassing the best results given
by the original DenseNet while still maintaining fewer pa-
rameters.

Related Work and Background
Automatic Architecture Designing There is a long stand-
ing study on automatic architecture designing. Neuro-
evolution algorithms which mimic the evolution processes
in the nature, are one of the earliest automatic architec-
ture designing methods (Miller, Todd, and Hegde 1989;
Stanley and Miikkulainen 2002). Authors in (Real et al.
2017) used neuro-evolution algorithms to explore a large
CNN architecture space and achieved networks which can
match performances of human-designed models. In paral-
lel, automatic architecture designing has also been stud-
ied in the context of Bayesian optimization (Bergstra and
Bengio 2012; Domhan, Springenberg, and Hutter 2015;
Mendoza et al. 2016). Recently, reinforcement learning is in-
troduced in automatic architecture designing and has shown
strong empirical results. Authors in (Baker et al. 2017) pre-
sented a Q-learning agent to sequentially pick CNN layers;
authors in (Zoph and Le 2017) used an auto-regressive recur-
rent network to generate a variable-length string that speci-
ﬁes the architecture of a neural network and trained the re-
current network with policy gradient.

As the above solutions rely on designing or training
networks from scratch, signiﬁcant computational resources
have been wasted during the construction. In this paper,
we aim to address the efﬁciency problem. Technically, we
allow to reuse the existing networks trained on the same
task and take network transformation actions. Both function-
preserving transformations and an alternative RL based
meta-controller are used to explore the architecture space.
Moreover, we notice that there are some complementary
techniques, such as learning curve prediction (Klein et al.
2017), for improving the efﬁciency, which can be combined
with our method.

Network Transformation and Knowledge Transfer
Generally, any modiﬁcation to a given network can be
viewed as a network transformation operation. In this pa-
per, since our aim is to utilize knowledge stored in previ-
ously trained networks, we focus on identifying the kind
of network transformation operations that would be able to
reuse pre-existing models. The idea of reusing pre-existing
models or knowledge transfer between neural networks

has been studied before. Net2Net technique introduced in
(Chen, Goodfellow, and Shlens 2015) describes two speciﬁc
function-preserving transformations, namely Net2WiderNet
and Net2DeeperNet, which respectively initialize a wider or
deeper student network to represent the same functionality
of the given teacher network and have proved to signiﬁcantly
accelerate the training of the student network especially for
large networks. Similar function-preserving schemes have
also been proposed in ResNet particularly for training very
deep architectures (He et al. 2016a). Additionally, the net-
work compression technique presented in (Han et al. 2015)
prunes less important connections (low-weight connections)
in order to shrink the size of neural networks without reduc-
ing their accuracy.

In this paper, instead, we focus on utilizing such network
transformations to reuse pre-existing models to efﬁciently
and economically explore the architecture space for auto-
matic architecture designing.

Reinforcement Learning Background Our meta-
in this work is based on RL (Sutton and
controller
Barto 1998), techniques for training the agent to max-
imize the cumulative reward when interacting with
an environment (Cai et al. 2017). We use the REIN-
FORCE algorithm (Williams 1992) similar
to (Zoph
and Le 2017) for updating the meta-controller, while
other advanced policy gradient methods (Kakade 2002;
Schulman et al. 2015) can be applied analogously. Our
action space is, however, different with that of (Zoph and Le
2017) or any other RL based approach (Baker et al. 2017),
as our actions are the network transformation operations
like adding, deleting, widening, etc., while others are
speciﬁc conﬁgurations of a newly created network layer
on the top of preceding layers. Speciﬁcally, we model the
automatic architecture design procedure as a sequential
decision making process, where the state is the current
network architecture and the action is the corresponding
network transformation operation. After T steps of network
transformations, the ﬁnal network architecture, along with
its weights transferred from the initial input network, is then
trained in the real data to get the validation performance
to calculate the reward signal, which is further used to
update the meta-controller via policy gradient algorithms
to maximize the expected validation performances of the
designed networks by the meta-controller.

Architecture Search by Net Transformation
In this section, we ﬁrst introduce the overall framework
of our meta-controller, and then show how each speciﬁc
network transformation decision is made under it. We
later extend the function-preserving transformations to the
DenseNet (Huang et al. 2017) architecture space where di-
rectly applying the original Net2Net operations can be prob-
lematic since the output of a layer will be fed to all subse-
quent layers.

We consider learning a meta-controller to generate net-
work transformation actions given the current network ar-
chitecture, which is speciﬁed with a variable-length string
(Zoph and Le 2017). To be able to generate various types

Figure 1: Overview of the RL based meta-controller in EAS,
which consists of an encoder network for encoding the ar-
chitecture and multiple separate actor networks for taking
network transformation actions.

Figure 2: Net2Wider actor, which uses a shared sigmoid
classiﬁer to simultaneously determine whether to widen
each layer based on its hidden state given by the encoder
network.

of network transformation actions while keeping the meta-
controller simple, we use an encoder network to learn a low-
dimensional representation of the given architecture, which
is then fed into each separate actor network to generate
a certain type of network transformation actions. Further-
more, to handle variable-length network architectures as in-
put and take the whole input architecture into considera-
tion when making decisions, the encoder network is imple-
mented with a bidirectional recurrent network (Schuster and
Paliwal 1997) with an input embedding layer. The overall
framework is illustrated in Figure 1, which is an analogue
of end-to-end sequence to sequence learning (Sutskever,
Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2014).

Actor Networks
Given the low dimensional representation of the input archi-
tecture, each actor network makes necessary decisions for
taking a certain type of network transformation actions. In
this work, we introduce two speciﬁc actor networks, namely
Net2Wider actor and Net2Deeper actor which correspond to
Net2WiderNet and Net2DeeperNet respectively.

Net2Wider Actor Net2WiderNet operation allows to re-
place a layer with a wider layer, meaning more units for
fully-connected layers, or more ﬁlters for convolutional lay-
ers, while preserving the functionality. For example, con-
sider a convolutional layer with kernel Kl whose shape is
(kl
i , f l
h, f l
h denote the ﬁlter width and
height, while f l
o denote the number of input and out-
put channels. To replace this layer with a wider layer that
has ˆf l
o) output channels, we should ﬁrst introduce a
random remapping function Gl, which is deﬁned as

o) where kl
i and f l

w and kl

o (> f l

w, kl

(cid:40)

Gl(j) =

j
random sample from {1, · · · , f l

1 ≤ j ≤ f l
o
o < j ≤ ˆf l

o

o} f l

.

(1)

With the remapping function Gl, we have the new kernel ˆKl
for the wider layer with shape (kl

i , ˆf l
o)
ˆK l[x, y, i, j] = K l[x, y, i, Gl(j)].

w, kl

h, f l

(2)

As such, the ﬁrst f l
of ˆKl are directly copied from Kl while the remaining ˆf l

o entries in the output channel dimension
o −

f l
o entries are created by choosing randomly as deﬁned in
Gl. Accordingly, the new output of the wider layer is ˆOl
with ˆOl(j) = Ol(Gl(j)), where Ol is the output of the
original layer and we only show the channel dimension to
make the notation simpler.

To preserve the functionality, the kernel Kl+1 of the next
layer should also be modiﬁed due to the replication in its
input. The new kernel ˆKl+1 with shape (kl+1
i =
ˆf l
o, f l+1
o

h , ˆf l+1

w , kl+1

) is given as

ˆK l+1[x, y, j, k] =

K l+1[x, y, Gl(j), k]
(cid:12){z|Gl(z) = Gl(j)}(cid:12)
(cid:12)
(cid:12)

.

(3)

For further details, we refer to the original Net2Net work
(Chen, Goodfellow, and Shlens 2015).

In our work, to be ﬂexible and efﬁcient, the Net2Wider
actor simultaneously determines whether each layer should
be extended. Speciﬁcally, for each layer, this decision is
carried out by a shared sigmoid classiﬁer given the hid-
den state of the layer learned by the bidirectional encoder
network. Moreover, we follow previous work and search
the number of ﬁlters for convolutional layers and units for
fully-connected layers in a discrete space. Therefore, if the
Net2Wider actor decides to widen a layer, the number of
ﬁlters or units of the layer increases to the next discrete
level, e.g. from 32 to 64. The structure of Net2Wider actor
is shown in Figure 2.

Net2Deeper Actor Net2DeeperNet operation allows to
insert a new layer that is initialized as adding an identity
mapping between two layers so as to preserve the functional-
ity. For a new convolutional layer, the kernel is set to be iden-
tity ﬁlters while for a new fully-connected layer, the weight
matrix is set to be identity matrix. Thus the new layer is set
with the same number of ﬁlters or units as the layer below
at ﬁrst, and could further get wider when Net2WiderNet op-
eration is performed on it. To fully preserve the functional-
ity, Net2DeeperNet operation has a constraint on the activa-
tion function φ, i.e. φ must satisfy φ(Iφ(v)) = φ(v) for all
vectors v. This property holds for rectiﬁed linear activation
(ReLU) but fails for sigmoid and tanh activation. However,
we can still reuse weights of existing networks with sigmoid

et al. 2017), directly applying the original Net2Net opera-
tions can be problematic. In this section, we introduce sev-
eral extensions to the original Net2Net operations to enable
function-preserving transformations for DenseNet.

Different from the plain CNN, in DenseNet, the lth layer
would receive the outputs of all preceding layers as input,
which are concatenated on the channel dimension, denoted
as [O0, O1, · · · , Ol−1], while its output Ol would be fed to
all subsequent layers.

Denote the kernel of the lth layer as Kl with shape
o). To replace the lth layer with a wider layer
i , f l
(kl
h, f l
w, kl
that has ˆf l
o output channels while preserving the function-
ality, the creation of the new kernel ˆKl in the lth layer
is the same as the original Net2WiderNet operation (see
Eq. (1) and Eq. (2)). As such, the new output of the wider
layer is ˆOl with ˆOl(j) = Ol(Gl(j)), where Gl is the
random remapping function as deﬁned in Eq. (1). Since
the output of the lth layer will be fed to all subsequent
layers in DenseNet, the replication in ˆOl will result in
replication in the inputs of all layers after the lth layer.
As such, instead of only modifying the kernel of the next
layer as done in the original Net2WiderNet operation, we
need to modify the kernels of all subsequent layers in
DenseNet. For the mth layer where m > l, its input be-
comes [O0, · · · , Ol−1, ˆOl, Ol+1, · · · , Om−1] after widen-
ing the lth layer, thus from the perspective of mth layer, the
equivalent random remapping function ˆGm can be written
as

ˆGm(j) =






j
f 0:l
o +Gl(j)
j − ˆf l
o +f l
o

o

1 ≤ j ≤ f 0:l
f 0:l
o < j ≤ f 0:l
o + ˆf l
f 0:l

o + ˆf l
o < j ≤ f 0:m

o

o + ˆf l

o −f l
o

,

(4)

o = (cid:80)l−1

v=0 f v

where f 0:l
o is the number of input channels for
the lth layer, the ﬁrst part corresponds to [O0, · · · , Ol−1],
the second part corresponds to [ ˆOl], and the last part corre-
sponds to [Ol+1, · · · , Om−1]. A simple example of ˆGm is
given as

ˆGm : {1, · · · , 5,

ˆOl
(cid:122) (cid:125)(cid:124) (cid:123)
6, 7, 8, 9, 10, 11} → {1, · · · , 5,

ˆOl
(cid:122) (cid:125)(cid:124) (cid:123)
6, 7, 6, 6, 8, 9}

where Gl : {1, 2, 3, 4} → {1, 2, 1, 1}.

Accordingly the new kernel of mth layer can be given by
Eq. (3) with Gl replaced with ˆGm.

To insert a new layer in DenseNet, suppose the new
layer is inserted after the lth layer. Denote the output of
the new layer as Onew, and its input is [O0, O1, · · · , Ol].
Therefore, for the mth (m > l) layer, its new input after
the insertion is [O0, O1, · · · , Ol, Onew, Ol+1, · · · , Om−1].
To preserve the functionality, similar to the Net2WiderNet
case, Onew should be the replication of some entries in
[O0, O1, · · · , Ol]. It is possible, since the input of the new
layer is [O0, O1, · · · , Ol]. Each ﬁlter in the new layer can
be represented with a tensor, denoted as ˆF with shape
i = f 0:l+1
(knew
), where knew
denote the
width and height of the ﬁlter, and f new
is the number of in-
put channels. To make the output of ˆF to be a replication

w and knew
i

h , f new

w , knew

h

o

Figure 3: Net2Deeper actor, which uses a recurrent network
to sequentially determine where to insert the new layer and
corresponding parameters for the new layer based on the ﬁ-
nal hidden state of the encoder network given the input ar-
chitecture.

or tanh activation, which could be useful compared to ran-
dom initialization. Additionally, when using batch normal-
ization (Ioffe and Szegedy 2015), we need to set output scale
and output bias of the batch normalization layer to undo the
normalization, rather than initialize them as ones and zeros.
Further details about the Net2DeeperNet operation is pro-
vided in the original paper (Chen, Goodfellow, and Shlens
2015).

The structure of the Net2Deeper actor is shown in Fig-
ure 3, which is a recurrent network whose hidden state is
initialized with the ﬁnal hidden state of the encoder net-
work. Similar to previous work (Baker et al. 2017), we al-
low the Net2Deeper actor to insert one new layer at each
step. Speciﬁcally, we divide a CNN architecture into sev-
eral blocks according to the pooling layers and Net2Deeper
actor sequentially determines which block to insert the new
layer, a speciﬁc index within the block and parameters of the
new layer. For a new convolutional layer, the agent needs to
determine the ﬁlter size and the stride while for a new fully-
connected layer, no parameter prediction is needed. In CNN
architectures, any fully-connected layer should be on the top
of all convolutional and pooling layers. To avoid resulting in
unreasonable architectures, if the Net2Deeper actor decides
to insert a new layer after a fully-connected layer or the ﬁnal
global average pooling layer, the new layer is restricted to be
a fully-connected layer, otherwise it must be a convolutional
layer.

Function-preserving Transformation for DenseNet
The original Net2Net operations proposed in (Chen, Good-
fellow, and Shlens 2015) are discussed under the scenarios
where the network is arranged layer-by-layer, i.e. the output
of a layer is only fed to its next layer. As such, in some mod-
ern CNN architectures where the output of a layer would be
fed to multiple subsequent layers, such as DenseNet (Huang

of the nth entry in [O0, O1, · · · , Ol], we can set ˆF (using
the special case that knew
h = 3 for illustration) as the
following

w = knew

ˆF [x, y, n] =





0
0
0

0
1
0



 ,

0
0
0

(5)

while all other values in ˆF are set to be 0. Note that n can be
chosen randomly from {1, · · · , f 0:l+1
} for each ﬁlter. After
all ﬁlters in the new layer are set, we can form an equivalent
random remapping function for all subsequent layers as is
done in Eq. (4) and modify their kernels accordingly.

o

Experiments and Results

In line with the previous work (Baker et al. 2017; Zoph and
Le 2017; Real et al. 2017), we apply the proposed EAS on
image benchmark datasets (CIFAR-10 and SVHN) to ex-
plore high performance CNN architectures for the image
classiﬁcation task1. Notice that the performances of the ﬁnal
designed models largely depend on the architecture space
and the computational resources. In our experiments, we
evaluate EAS in two different settings. In all cases, we use
restricted computational resources (5 GPUs) compared to
the previous work such as (Zoph and Le 2017) that used
800 GPUs. In the ﬁrst setting, we apply EAS to explore the
plain CNN architecture space, which purely consists of con-
volutional, pooling and fully-connected layers. While in the
second setting, we apply EAS to explore the DenseNet ar-
chitecture space.

Image Datasets

CIFAR-10 The CIFAR-10 dataset (Krizhevsky and Hin-
ton 2009) consists of 50,000 training images and 10,000
test images. We use a standard data augmentation scheme
that is widely used for CIFAR-10 (Huang et al. 2017), and
denote the augmented dataset as C10+ while the original
dataset is denoted as C10. For preprocessing, we normal-
ized the images using the channel means and standard de-
viations. Following the previous work (Baker et al. 2017;
Zoph and Le 2017), we randomly sample 5,000 images from
the training set to form a validation set while using the re-
maining 45,000 images for training during exploring the ar-
chitecture space.

SVHN The Street View House Numbers (SVHN) dataset
(Netzer et al. 2011) contains 73,257 images in the original
training set, 26,032 images in the test set, and 531,131 addi-
tional images in the extra training set. For preprocessing, we
divide the pixel values by 255 and do not perform any data
augmentation, as is done in (Huang et al. 2017). We follow
(Baker et al. 2017) and use the original training set during
the architecture search phase with 5,000 randomly sampled
images as the validation set, while training the ﬁnal discov-
ered architectures using all the training data, including the
original training set and extra training set.

1Experiment code and discovered top architectures along with

weights: https://github.com/han-cai/EAS

Figure 4: Progress of two stages architecture search on C10+
in the plain CNN architecture space.

Training Details
For the meta-controller, we use a one-layer bidirectional
LSTM with 50 hidden units as the encoder network (Fig-
ure 1) with an embedding size of 16, and train it with the
ADAM optimizer (Kingma and Ba 2015).

At each step, the meta-controller samples 10 networks by
taking network transformation actions. Since the sampled
networks are not trained from scratch but we reuse weights
of the given network in our scenario, they are then trained for
20 epochs, a relative small number compared to 50 epochs in
(Zoph and Le 2017). Besides, we use a smaller initial learn-
ing rate for this reason. Other settings for training networks
on CIFAR-10 and SVHN, are similar to (Huang et al. 2017;
Zoph and Le 2017). Speciﬁcally, we use the SGD with a
Nesterov momentum (Sutskever et al. 2013) of 0.9, a weight
decay of 0.0001, a batch size of 64. The initial learning rate
is 0.02 and is further annealed with a cosine learning rate
decay (Gastaldi 2017). The accuracy in the held-out valida-
tion set is used to compute the reward signal for each sam-
pled network. Since the gain of improving the accuracy from
90% to 91% should be much larger than from 60% to 61%,
instead of directly using the validation accuracy accv as the
reward, as done in (Zoph and Le 2017), we perform a non-
linear transformation on accv, i.e. tan(accv × π/2), and use
the transformed value as the reward. Additionally, we use
an exponential moving average of previous rewards, with a
decay of 0.95 as the baseline function to reduce the variance.

Explore Plain CNN Architecture Space
We start applying EAS to explore the plain CNN archi-
tecture space. Following the previous automatic architec-
ture designing methods (Baker et al. 2017; Zoph and Le
2017), EAS searches layer parameters in a discrete and lim-
ited space. For every convolutional layer, the ﬁlter size is
chosen from {1, 3, 5} and the number of ﬁlters is cho-
sen from {16, 32, 64, 96, 128, 192, 256, 320, 384, 448, 512},
while the stride is ﬁxed to be 1 (Baker et al. 2017). For every
fully-connected layer, the number of units is chosen from
{64, 128, 256, 384, 512, 640, 768, 896, 1024}. Additionally,

Table 1: Simple start point network. C(n, f, l) denotes a convolutional layer with n ﬁlters, ﬁlter size f and stride l; P(f, l, MAX)
and P(f, l, AVG) denote a max and an average pooling layer with ﬁlter size f and stride l respectively; FC(n) denotes a fully-
connected layer with n units; SM(n) denotes a softmax layer with n output units.

Model Architecture
C(16, 3, 1), P(2, 2, MAX), C(32, 3, 1), P(2, 2, MAX), C(64, 3, 1),
P(2, 2, MAX), C(128, 3, 1), P(4, 4, AVG), FC(256), SM(10)

Validation Accuracy (%)

87.07

we use ReLU and batch normalization for each convolu-
tional or fully-connected layer. For SVHN, we add a dropout
layer after each convolutional layer (except the ﬁrst layer)
and use a dropout rate of 0.2 (Huang et al. 2017).

Start with Small Network We begin the exploration on
C10+, using a small network (see Table 1), which achieves
87.07% accuracy in the held-out validation set, as the start
point. Different from (Zoph and Le 2017; Baker et al. 2017),
EAS is not restricted to start from empty and can ﬂexibly
use any discovered architecture as the new start point. As
such, to take the advantage of such ﬂexibility and also re-
duce the search space for saving the computational resources
and time, we divide the whole architecture search process
into two stages where we allow the meta-controller to take 5
steps of Net2Deeper action and 4 steps of Net2Wider action
in the ﬁrst stage. After 300 networks are sampled, we take
the network which performs best currently and train it with
a longer period of time (100 epochs) to be used as the start
point for the second stage. Similarly, in the second stage, we
also allow the meta-controller to take 5 steps of Net2Deeper
action and 4 steps of Net2Wider action and stop exploration
after 150 networks are sampled.

The progress of the two stages architecture search is
shown in Figure 4, where we can ﬁnd that EAS gradu-
ally learns to pick high performance architectures at each
stage. As EAS takes function-preserving transformations to
explore the architecture space, we can also ﬁnd that the
sampled architectures consistently perform better than the
start point network at each stage. Thus it is usually “safe”
to explore the architecture space with EAS. We take the
top networks discovered during the second stage and fur-
ther train the networks with 300 epochs using the full train-
ing set. Finally, the best model achieves 95.11% test ac-
curacy (i.e. 4.89% test error rate). Furthermore, to justify
the transferability of the discovered networks, we train the
top architecture (95.11% test accuracy) on SVHN from ran-
dom initialization with 40 epochs using the full training
set and achieves 98.17% test accuracy (i.e. 1.83% test er-
ror rate), better than both human-designed and automatically
designed architectures that are in the plain CNN architecture
space (see Table 2).

We would like to emphasize that the required computa-
tional resources to achieve this result is much smaller than
those required in (Zoph and Le 2017; Real et al. 2017).
Speciﬁcally, it takes less than 2 days on 5 GeForce GTX
1080 GPUs with totally 450 networks trained to achieve
4.89% test error rate on C10+ starting from a small network.

Further Explore Larger Architecture Space To further
search better architectures in the plain CNN architecture

Table 2: Test error rate (%) comparison with CNNs that use
convolutional, fully-connected and pooling layers alone.

human
designed

auto
designed

Model
Maxout (Goodfellow et al. 2013)
NIN (Lin, Chen, and Yan 2013)
All-CNN (Springenberg et al. 2014)
VGGnet (Simonyan and Zisserman 2015)
MetaQNN (Baker et al. 2017) (depth=7)
MetaQNN (Baker et al. 2017) (ensemble)
EAS (plain CNN, depth=16)
EAS (plain CNN, depth=20)

C10+
9.38
8.81
7.25
7.25
6.92
-
4.89
4.23

SVHN
2.47
2.35
-
-
-
2.06
1.83
1.73

space, in the second experiment, we use the top architec-
tures discovered in the ﬁrst experiment, as the start points
to explore a larger architecture space on C10+ and SVHN.
This experiment on each dataset takes around 2 days on 5
GPUs.

The summarized results of comparing with human-
designed and automatically designed architectures that use
a similar design scheme (plain CNN), are reported in Table
2, where we can ﬁnd that the top model designed by EAS
on the plain CNN architecture space outperforms all similar
models by a large margin. Speciﬁcally, comparing to human-
designed models, the test error rate drops from 7.25% to
4.23% on C10+ and from 2.35% to 1.73% on SVHN. While
comparing to MetaQNN, the Q-learning based automatic ar-
chitecture designing method, EAS achieves a relative test er-
ror rate reduction of 38.9% on C10+ and 16.0% on SVHN.
We also notice that the best model designed by MetaQNN
on C10+ only has a depth of 7, though the maximum is set
to be 18 in the original paper (Baker et al. 2017). We sup-
pose maybe they trained each designed network from scratch
and used an aggressive training strategy to accelerate train-
ing, which resulted in many networks under performed, es-
pecially for deep networks. Since we reuse the weights of
pre-existing networks, the deep networks are validated more
accurately in EAS, and we can thus design deeper and more
accurate networks than MetaQNN.

We also report the comparison with state-of-the-art ar-
chitectures that use advanced techniques such as skip-
connections, branching etc., on C10+ in Table 3. Though it
is not a fair comparison since we do not incorporate such
advanced techniques into the search space in this experi-
ment, we still ﬁnd that the top model designed by EAS is
highly competitive even comparing to these state-of-the-art
modern architectures. Speciﬁcally, the 20-layers plain CNN
with 23.4M parameters outperforms ResNet, its stochas-
tic depth variant and its pre-activation variant. It also ap-
proaches the best result given by DenseNet. When com-
paring to automatic architecture designing methods that in-

Table 3: Test error rate (%) comparison with state-of-the-art architectures.

Model
ResNet (He et al. 2016a)
ResNet (stochastic depth) (Huang et al. 2017)
Wide ResNet (Zagoruyko and Komodakis 2016)
Wide ResNet (Zagoruyko and Komodakis 2016)
ResNet (pre-activation) (He et al. 2016b)
DenseNet (L = 40, k = 12) (Huang et al. 2017)
DenseNet-BC (L = 100, k = 12) (Huang et al. 2017)
DenseNet-BC (L = 190, k = 40) (Huang et al. 2017)
Large-Scale Evolution (250 GPUs)(Real et al. 2017)
NAS (predicting strides, 800 GPUs) (Zoph and Le 2017)
NAS (max pooling, 800 GPUs) (Zoph and Le 2017)
NAS (post-processing, 800 GPUs) (Zoph and Le 2017)
EAS (plain CNN, 5 GPUs)

Depth
110
1202
16
28
1001
40
100
190
-
20
39
39
20

Params C10+
6.61
1.7M
4.91
10.2M
4.81
11.0M
4.17
36.5M
4.62
10.2M
5.24
1.0M
4.51
0.8M
3.46
25.6M
5.40
5.4M
6.01
2.5M
4.47
7.1M
3.65
37.4M
4.23
23.4M

human
designed

auto
designed

action. The result is reported in Figure 5, which shows that
the RL based meta-controller can effectively focus on the
right search direction, while the random search cannot (left
plot), and thus ﬁnd high performance architectures more ef-
ﬁciently than random search.

Explore DenseNet Architecture Space
We also apply EAS to explore the DenseNet architecture
space. We use the DenseNet-BC (L = 40, k = 40) as
the start point. The growth rate, i.e. the width of the non-
bottleneck layer is chosen from {40, 44, 48, 52, 56, 60, 64},
and the result is reported in Table 4. We ﬁnd that by ap-
plying EAS to explore the DenseNet architecture space, we
achieve a test error rate of 4.66% on C10, better than the
best result, i.e. 5.19% given by the original DenseNet while
having 43.79% less parameters. On C10+, we achieve a test
error rate of 3.44%, also outperforming the best result, i.e.
3.46% given by the original DenseNet while having 58.20%
less parameters.

Conclusion
In this paper, we presented EAS, a new framework to-
ward economical and efﬁcient architecture search, where
the meta-controller is implemented as a RL agent. It learns
to take actions for network transformation to explore the
architecture space. By starting from an existing network
and reusing its weights via the class of function-preserving
transformation operations, EAS is able to utilize knowledge
stored in previously trained networks and take advantage
of the existing successful architectures in the target task to
explore the architecture space efﬁciently. Our experiments
have demonstrated EAS’s outstanding performance and ef-
ﬁciency compared with several strong baselines. For future
work, we would like to explore more network transforma-
tion operations and apply EAS for different purposes such
as searching networks that not only have high accuracy but
also keep a balance between the size and the performance.

Acknowledgments
This research was sponsored by Huawei Innovation Re-
search Program, NSFC (61702327) and Shanghai Sailing
Program (17YF1428200).

Figure 5: Comparison between RL based meta-controller
and random search on C10+.

Table 4: Test error rate (%) results of exploring DenseNet
architecture space with EAS.

Model
DenseNet (L = 100, k = 24)
DenseNet-BC (L = 250, k = 24)
DenseNet-BC (L = 190, k = 40)
NAS (post-processing)
EAS (DenseNet on C10)
EAS (DenseNet on C10+)

Depth
100
250
190
39
70
76

Params C10 C10+
3.74
27.2M 5.83
3.62
15.3M 5.19
3.46
25.6M
3.65
37.4M
8.6M 4.66
-
3.44
10.7M

-
-

-

corporate skip-connections into their search space, our 20-
layers plain model beats most of them except NAS with
post-processing, that is much deeper and has more param-
eters than our model. Moreover, we only use 5 GPUs and
train hundreds of networks while they use 800 GPUs and
train tens of thousands of networks.

Comparison Between RL and Random Search Our
framework is not restricted to use the RL based meta-
controller. Beside RL, one can also take network transfor-
mation actions to explore the architecture space by random
search, which can be effective in some cases (Bergstra and
Bengio 2012). In this experiment, we compare the perfor-
mances of the RL based meta-controller and the random
search meta-controller in the architecture space that is used
in the above experiments. Speciﬁcally, we use the network
in Table 1 as the start point and let the meta-controller to
take 5 steps of Net2Deeper action and 4 steps of Net2Wider

[Netzer et al. 2011] Netzer, Y.; Wang, T.; Coates, A.; Bissacco, A.;
Wu, B.; and Ng, A. Y. 2011. Reading digits in natural images with
unsupervised feature learning. In NIPS workshop on deep learning
and unsupervised feature learning.

[Real et al. 2017] Real, E.; Moore, S.; Selle, A.; Saxena, S.; Sue-
matsu, Y. L.; Le, Q.; and Kurakin, A. 2017. Large-scale evolution
of image classiﬁers. ICML.

[Schulman et al. 2015] Schulman, J.; Levine, S.; Abbeel, P.; Jordan,
M. I.; and Moritz, P. 2015. Trust region policy optimization. In
ICML.

[Schuster and Paliwal 1997] Schuster, M., and Paliwal, K. K. 1997.
Bidirectional recurrent neural networks. IEEE Transactions on Sig-
nal Processing.

[Silver et al. 2016] Silver, D.; Huang, A.; Maddison, C. J.; Guez,
A.; Sifre, L.; Van Den Driessche, G.; Schrittwieser, J.; Antonoglou,
I.; Panneershelvam, V.; Lanctot, M.; et al. 2016. Mastering the
game of go with deep neural networks and tree search. Nature.
[Simonyan and Zisserman 2015] Simonyan, K., and Zisserman, A.
2015. Very deep convolutional networks for large-scale image
recognition. ICLR.

[Snoek, Larochelle, and Adams 2012] Snoek, J.; Larochelle, H.;
and Adams, R. P. 2012. Practical bayesian optimization of ma-
chine learning algorithms. In NIPS.

[Springenberg et al. 2014] Springenberg, J. T.; Dosovitskiy, A.;
Brox, T.; and Riedmiller, M. 2014. Striving for simplicity: The
all convolutional net. arXiv preprint arXiv:1412.6806.

[Stanley and Miikkulainen 2002] Stanley, K. O., and Miikkulainen,
R. 2002. Evolving neural networks through augmenting topolo-
gies. Evolutionary computation.

[Sutskever et al. 2013] Sutskever, I.; Martens, J.; Dahl, G.; and Hin-
ton, G. 2013. On the importance of initialization and momentum
in deep learning. In ICML.

[Sutskever, Vinyals, and Le 2014] Sutskever, I.; Vinyals, O.; and
Le, Q. V. 2014. Sequence to sequence learning with neural net-
works. In NIPS.

[Sutton and Barto 1998] Sutton, R. S., and Barto, A. G. 1998. Re-

inforcement learning: An introduction. MIT press Cambridge.

[Williams 1992] Williams, R. J. 1992. Simple statistical gradient-
learning.

following algorithms for connectionist reinforcement
Machine learning.

[Zagoruyko and Komodakis 2016] Zagoruyko, S., and Komodakis,
arXiv preprint
Wide residual networks.

N.
2016.
arXiv:1605.07146.

[Zoph and Le 2017] Zoph, B., and Le, Q. V. 2017. Neural architec-

ture search with reinforcement learning. ICLR.

References
[Bahdanau, Cho, and Bengio 2014] Bahdanau, D.; Cho, K.; and
Bengio, Y. 2014. Neural machine translation by jointly learning to
align and translate. ICLR.

[Baker et al. 2017] Baker, B.; Gupta, O.; Naik, N.; and Raskar, R.
2017. Designing neural network architectures using reinforcement
learning. ICLR.

[Bergstra and Bengio 2012] Bergstra, J., and Bengio, Y. 2012. Ran-

dom search for hyper-parameter optimization. JMLR.

[Cai et al. 2017] Cai, H.; Ren, K.; Zhang, W.; Malialis, K.; Wang,
J.; Yu, Y.; and Guo, D. 2017. Real-time bidding by reinforcement
learning in display advertising. In WSDM.

[Chen, Goodfellow, and Shlens 2015] Chen, T.; Goodfellow, I.; and
Shlens, J. 2015. Net2net: Accelerating learning via knowledge
transfer. ICLR.

[Domhan, Springenberg, and Hutter 2015] Domhan, T.; Springen-
berg, J. T.; and Hutter, F. 2015. Speeding up automatic hyper-
parameter optimization of deep neural networks by extrapolation
of learning curves. In IJCAI.

[Gastaldi 2017] Gastaldi, X. 2017. Shake-shake regularization.

arXiv preprint arXiv:1705.07485.
[Goodfellow et al. 2013] Goodfellow,

J.; Warde-Farley, D.;
Mirza, M.; Courville, A.; and Bengio, Y. 2013. Maxout networks.
ICML.

I.

[Han et al. 2015] Han, S.; Pool, J.; Tran, J.; and Dally, W. 2015.
Learning both weights and connections for efﬁcient neural net-
work. In NIPS.

[He et al. 2016a] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016a.

Deep residual learning for image recognition. In CVPR.

[He et al. 2016b] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016b.

Identity mappings in deep residual networks. In ECCV.

[Huang et al. 2017] Huang, G.; Liu, Z.; Weinberger, K. Q.; and
van der Maaten, L. 2017. Densely connected convolutional net-
works. CVPR.

[Ioffe and Szegedy 2015] Ioffe, S., and Szegedy, C. 2015. Batch
normalization: Accelerating deep network training by reducing in-
ternal covariate shift. ICML.

[Kakade 2002] Kakade, S. 2002. A natural policy gradient. NIPS.
[Kingma and Ba 2015] Kingma, D., and Ba, J. 2015. Adam: A

method for stochastic optimization. ICLR.

[Klein et al. 2017] Klein, A.; Falkner, S.; Springenberg, J. T.; and
Hutter, F. 2017. Learning curve prediction with bayesian neural
networks. ICLR.

[Krizhevsky and Hinton 2009] Krizhevsky, A., and Hinton, G.

2009. Learning multiple layers of features from tiny images.

[Krizhevsky, Sutskever, and Hinton 2012] Krizhevsky,

Sutskever, I.; and Hinton, G. E.
tion with deep convolutional neural networks. In NIPS.

2012.

A.;
Imagenet classiﬁca-

[Lin, Chen, and Yan 2013] Lin, M.; Chen, Q.; and Yan, S. 2013.

Network in network. arXiv preprint arXiv:1312.4400.

[Mendoza et al. 2016] Mendoza, H.; Klein, A.; Feurer, M.; Sprin-
genberg, J. T.; and Hutter, F. 2016. Towards automatically-tuned
neural networks. In Workshop on Automatic Machine Learning.
[Miller, Todd, and Hegde 1989] Miller, G. F.; Todd, P. M.; and
Hegde, S. U. 1989. Designing neural networks using genetic algo-
rithms. In ICGA. Morgan Kaufmann Publishers Inc.

[Negrinho and Gordon 2017] Negrinho, R., and Gordon, G. 2017.
Deeparchitect: Automatically designing and training deep architec-
tures. arXiv preprint arXiv:1704.08792.

Efﬁcient Architecture Search by Network Transformation

Han Cai1, Tianyao Chen1, Weinan Zhang1∗, Yong Yu1, Jun Wang2
1Shanghai Jiao Tong University, 2University College London
{hcai,tychen,wnzhang,yyu}@apex.sjtu.edu.cn, j.wang@cs.ucl.ac.uk

7
1
0
2
 
v
o
N
 
1
2
 
 
]

G
L
.
s
c
[
 
 
2
v
3
7
8
4
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

Techniques for automatically designing deep neural net-
work architectures such as reinforcement learning based ap-
proaches have recently shown promising results. However,
their success is based on vast computational resources (e.g.
hundreds of GPUs), making them difﬁcult to be widely used.
A noticeable limitation is that they still design and train each
network from scratch during the exploration of the architec-
ture space, which is highly inefﬁcient. In this paper, we pro-
pose a new framework toward efﬁcient architecture search by
exploring the architecture space based on the current network
and reusing its weights. We employ a reinforcement learn-
ing agent as the meta-controller, whose action is to grow the
network depth or layer width with function-preserving trans-
formations. As such, the previously validated networks can
be reused for further exploration, thus saves a large amount
of computational cost. We apply our method to explore the
architecture space of the plain convolutional neural networks
(no skip-connections, branching etc.) on image benchmark
datasets (CIFAR-10, SVHN) with restricted computational
resources (5 GPUs). Our method can design highly com-
petitive networks that outperform existing networks using
the same design scheme. On CIFAR-10, our model with-
out skip-connections achieves 4.23% test error rate, exceed-
ing a vast majority of modern architectures and approaching
DenseNet. Furthermore, by applying our method to explore
the DenseNet architecture space, we are able to achieve more
accurate networks with fewer parameters.

Introduction
The great success of deep neural networks in various chal-
lenging applications (Krizhevsky, Sutskever, and Hinton
2012; Bahdanau, Cho, and Bengio 2014; Silver et al. 2016)
has led to a paradigm shift from feature designing to archi-
tecture designing, which still remains a laborious task and
requires human expertise. In recent years, many techniques
for automating the architecture design process have been
proposed (Snoek, Larochelle, and Adams 2012; Bergstra
and Bengio 2012; Baker et al. 2017; Zoph and Le 2017;
Real et al. 2017; Negrinho and Gordon 2017), and promis-
ing results of designing competitive models against human-
designed models are reported on some benchmark datasets

∗Correspondence to Weinan Zhang.

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

(Zoph and Le 2017; Real et al. 2017). Despite the promising
results as reported, their success is based on vast computa-
tional resources (e.g. hundreds of GPUs), making them dif-
ﬁcult to be used in practice for individual researchers, small
sized companies, or university research teams. Another key
drawback is that they still design and train each network
from scratch during exploring the architecture space without
any leverage of previously explored networks, which results
in high computational resources waste.

In fact, during the architecture design process, many
slightly different networks are trained for the same task.
Apart from their ﬁnal validation performances that are used
to guide exploration, we should also have access to their
architectures, weights, training curves etc., which contain
abundant knowledge and can be leveraged to accelerate the
architecture design process just like human experts (Chen,
Goodfellow, and Shlens 2015; Klein et al. 2017). Further-
more, there are typically many well-designed architectures,
by human or automatic architecture designing methods, that
have achieved good performances at the target task. Under
restricted computational resources limits, instead of totally
neglecting these existing networks and exploring the archi-
tecture space from scratch (which does not guarantee to re-
sult in better performance architectures), a more economical
and efﬁcient alternative could be exploring the architecture
space based on these successful networks and reusing their
weights.

In this paper, we propose a new framework, called EAS,
Efﬁcient Architecture Search, where the meta-controller ex-
plores the architecture space by network transformation op-
erations such as widening a certain layer (more units or ﬁl-
ters), inserting a layer, adding skip-connections etc., given
an existing network trained on the same task. To reuse
weights, we consider the class of function-preserving trans-
formations (Chen, Goodfellow, and Shlens 2015) that allow
to initialize the new network to represent the same function
as the given network but use different parameterization to
be further trained to improve the performance, which can
signiﬁcantly accelerate the training of the new network es-
pecially for large networks. Furthermore, we combine our
framework with recent advances of reinforcement learn-
ing (RL) based automatic architecture designing methods
(Baker et al. 2017; Zoph and Le 2017), and employ a RL
based agent as the meta-controller.

Our experiments of exploring the architecture space of
the plain convolutional neural networks (CNNs), which
purely consists of convolutional, fully-connected and pool-
ing layers without skip-connections, branching etc., on im-
age benchmark datasets (CIFAR-10, SVHN), show that EAS
with limited computational resources (5 GPUs) can design
competitive architectures. The best plain model designed
by EAS on CIFAR-10 with standard data augmentation
achieves 4.23% test error rate, even better than many modern
architectures that use skip-connections. We further apply our
method to explore the DenseNet (Huang et al. 2017) archi-
tecture space, and achieve 4.66% test error rate on CIFAR-
10 without data augmentation and 3.44% on CIFAR-10 with
standard data augmentation, surpassing the best results given
by the original DenseNet while still maintaining fewer pa-
rameters.

Related Work and Background
Automatic Architecture Designing There is a long stand-
ing study on automatic architecture designing. Neuro-
evolution algorithms which mimic the evolution processes
in the nature, are one of the earliest automatic architec-
ture designing methods (Miller, Todd, and Hegde 1989;
Stanley and Miikkulainen 2002). Authors in (Real et al.
2017) used neuro-evolution algorithms to explore a large
CNN architecture space and achieved networks which can
match performances of human-designed models. In paral-
lel, automatic architecture designing has also been stud-
ied in the context of Bayesian optimization (Bergstra and
Bengio 2012; Domhan, Springenberg, and Hutter 2015;
Mendoza et al. 2016). Recently, reinforcement learning is in-
troduced in automatic architecture designing and has shown
strong empirical results. Authors in (Baker et al. 2017) pre-
sented a Q-learning agent to sequentially pick CNN layers;
authors in (Zoph and Le 2017) used an auto-regressive recur-
rent network to generate a variable-length string that speci-
ﬁes the architecture of a neural network and trained the re-
current network with policy gradient.

As the above solutions rely on designing or training
networks from scratch, signiﬁcant computational resources
have been wasted during the construction. In this paper,
we aim to address the efﬁciency problem. Technically, we
allow to reuse the existing networks trained on the same
task and take network transformation actions. Both function-
preserving transformations and an alternative RL based
meta-controller are used to explore the architecture space.
Moreover, we notice that there are some complementary
techniques, such as learning curve prediction (Klein et al.
2017), for improving the efﬁciency, which can be combined
with our method.

Network Transformation and Knowledge Transfer
Generally, any modiﬁcation to a given network can be
viewed as a network transformation operation. In this pa-
per, since our aim is to utilize knowledge stored in previ-
ously trained networks, we focus on identifying the kind
of network transformation operations that would be able to
reuse pre-existing models. The idea of reusing pre-existing
models or knowledge transfer between neural networks

has been studied before. Net2Net technique introduced in
(Chen, Goodfellow, and Shlens 2015) describes two speciﬁc
function-preserving transformations, namely Net2WiderNet
and Net2DeeperNet, which respectively initialize a wider or
deeper student network to represent the same functionality
of the given teacher network and have proved to signiﬁcantly
accelerate the training of the student network especially for
large networks. Similar function-preserving schemes have
also been proposed in ResNet particularly for training very
deep architectures (He et al. 2016a). Additionally, the net-
work compression technique presented in (Han et al. 2015)
prunes less important connections (low-weight connections)
in order to shrink the size of neural networks without reduc-
ing their accuracy.

In this paper, instead, we focus on utilizing such network
transformations to reuse pre-existing models to efﬁciently
and economically explore the architecture space for auto-
matic architecture designing.

Reinforcement Learning Background Our meta-
in this work is based on RL (Sutton and
controller
Barto 1998), techniques for training the agent to max-
imize the cumulative reward when interacting with
an environment (Cai et al. 2017). We use the REIN-
FORCE algorithm (Williams 1992) similar
to (Zoph
and Le 2017) for updating the meta-controller, while
other advanced policy gradient methods (Kakade 2002;
Schulman et al. 2015) can be applied analogously. Our
action space is, however, different with that of (Zoph and Le
2017) or any other RL based approach (Baker et al. 2017),
as our actions are the network transformation operations
like adding, deleting, widening, etc., while others are
speciﬁc conﬁgurations of a newly created network layer
on the top of preceding layers. Speciﬁcally, we model the
automatic architecture design procedure as a sequential
decision making process, where the state is the current
network architecture and the action is the corresponding
network transformation operation. After T steps of network
transformations, the ﬁnal network architecture, along with
its weights transferred from the initial input network, is then
trained in the real data to get the validation performance
to calculate the reward signal, which is further used to
update the meta-controller via policy gradient algorithms
to maximize the expected validation performances of the
designed networks by the meta-controller.

Architecture Search by Net Transformation
In this section, we ﬁrst introduce the overall framework
of our meta-controller, and then show how each speciﬁc
network transformation decision is made under it. We
later extend the function-preserving transformations to the
DenseNet (Huang et al. 2017) architecture space where di-
rectly applying the original Net2Net operations can be prob-
lematic since the output of a layer will be fed to all subse-
quent layers.

We consider learning a meta-controller to generate net-
work transformation actions given the current network ar-
chitecture, which is speciﬁed with a variable-length string
(Zoph and Le 2017). To be able to generate various types

Figure 1: Overview of the RL based meta-controller in EAS,
which consists of an encoder network for encoding the ar-
chitecture and multiple separate actor networks for taking
network transformation actions.

Figure 2: Net2Wider actor, which uses a shared sigmoid
classiﬁer to simultaneously determine whether to widen
each layer based on its hidden state given by the encoder
network.

of network transformation actions while keeping the meta-
controller simple, we use an encoder network to learn a low-
dimensional representation of the given architecture, which
is then fed into each separate actor network to generate
a certain type of network transformation actions. Further-
more, to handle variable-length network architectures as in-
put and take the whole input architecture into considera-
tion when making decisions, the encoder network is imple-
mented with a bidirectional recurrent network (Schuster and
Paliwal 1997) with an input embedding layer. The overall
framework is illustrated in Figure 1, which is an analogue
of end-to-end sequence to sequence learning (Sutskever,
Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2014).

Actor Networks
Given the low dimensional representation of the input archi-
tecture, each actor network makes necessary decisions for
taking a certain type of network transformation actions. In
this work, we introduce two speciﬁc actor networks, namely
Net2Wider actor and Net2Deeper actor which correspond to
Net2WiderNet and Net2DeeperNet respectively.

Net2Wider Actor Net2WiderNet operation allows to re-
place a layer with a wider layer, meaning more units for
fully-connected layers, or more ﬁlters for convolutional lay-
ers, while preserving the functionality. For example, con-
sider a convolutional layer with kernel Kl whose shape is
(kl
i , f l
h, f l
h denote the ﬁlter width and
height, while f l
o denote the number of input and out-
put channels. To replace this layer with a wider layer that
has ˆf l
o) output channels, we should ﬁrst introduce a
random remapping function Gl, which is deﬁned as

o) where kl
i and f l

w and kl

o (> f l

w, kl

(cid:40)

Gl(j) =

j
random sample from {1, · · · , f l

1 ≤ j ≤ f l
o
o < j ≤ ˆf l

o

o} f l

.

(1)

With the remapping function Gl, we have the new kernel ˆKl
for the wider layer with shape (kl

i , ˆf l
o)
ˆK l[x, y, i, j] = K l[x, y, i, Gl(j)].

w, kl

h, f l

(2)

As such, the ﬁrst f l
of ˆKl are directly copied from Kl while the remaining ˆf l

o entries in the output channel dimension
o −

f l
o entries are created by choosing randomly as deﬁned in
Gl. Accordingly, the new output of the wider layer is ˆOl
with ˆOl(j) = Ol(Gl(j)), where Ol is the output of the
original layer and we only show the channel dimension to
make the notation simpler.

To preserve the functionality, the kernel Kl+1 of the next
layer should also be modiﬁed due to the replication in its
input. The new kernel ˆKl+1 with shape (kl+1
i =
ˆf l
o, f l+1
o

h , ˆf l+1

w , kl+1

) is given as

ˆK l+1[x, y, j, k] =

K l+1[x, y, Gl(j), k]
(cid:12){z|Gl(z) = Gl(j)}(cid:12)
(cid:12)
(cid:12)

.

(3)

For further details, we refer to the original Net2Net work
(Chen, Goodfellow, and Shlens 2015).

In our work, to be ﬂexible and efﬁcient, the Net2Wider
actor simultaneously determines whether each layer should
be extended. Speciﬁcally, for each layer, this decision is
carried out by a shared sigmoid classiﬁer given the hid-
den state of the layer learned by the bidirectional encoder
network. Moreover, we follow previous work and search
the number of ﬁlters for convolutional layers and units for
fully-connected layers in a discrete space. Therefore, if the
Net2Wider actor decides to widen a layer, the number of
ﬁlters or units of the layer increases to the next discrete
level, e.g. from 32 to 64. The structure of Net2Wider actor
is shown in Figure 2.

Net2Deeper Actor Net2DeeperNet operation allows to
insert a new layer that is initialized as adding an identity
mapping between two layers so as to preserve the functional-
ity. For a new convolutional layer, the kernel is set to be iden-
tity ﬁlters while for a new fully-connected layer, the weight
matrix is set to be identity matrix. Thus the new layer is set
with the same number of ﬁlters or units as the layer below
at ﬁrst, and could further get wider when Net2WiderNet op-
eration is performed on it. To fully preserve the functional-
ity, Net2DeeperNet operation has a constraint on the activa-
tion function φ, i.e. φ must satisfy φ(Iφ(v)) = φ(v) for all
vectors v. This property holds for rectiﬁed linear activation
(ReLU) but fails for sigmoid and tanh activation. However,
we can still reuse weights of existing networks with sigmoid

et al. 2017), directly applying the original Net2Net opera-
tions can be problematic. In this section, we introduce sev-
eral extensions to the original Net2Net operations to enable
function-preserving transformations for DenseNet.

Different from the plain CNN, in DenseNet, the lth layer
would receive the outputs of all preceding layers as input,
which are concatenated on the channel dimension, denoted
as [O0, O1, · · · , Ol−1], while its output Ol would be fed to
all subsequent layers.

Denote the kernel of the lth layer as Kl with shape
o). To replace the lth layer with a wider layer
i , f l
(kl
h, f l
w, kl
that has ˆf l
o output channels while preserving the function-
ality, the creation of the new kernel ˆKl in the lth layer
is the same as the original Net2WiderNet operation (see
Eq. (1) and Eq. (2)). As such, the new output of the wider
layer is ˆOl with ˆOl(j) = Ol(Gl(j)), where Gl is the
random remapping function as deﬁned in Eq. (1). Since
the output of the lth layer will be fed to all subsequent
layers in DenseNet, the replication in ˆOl will result in
replication in the inputs of all layers after the lth layer.
As such, instead of only modifying the kernel of the next
layer as done in the original Net2WiderNet operation, we
need to modify the kernels of all subsequent layers in
DenseNet. For the mth layer where m > l, its input be-
comes [O0, · · · , Ol−1, ˆOl, Ol+1, · · · , Om−1] after widen-
ing the lth layer, thus from the perspective of mth layer, the
equivalent random remapping function ˆGm can be written
as

ˆGm(j) =






j
f 0:l
o +Gl(j)
j − ˆf l
o +f l
o

o

1 ≤ j ≤ f 0:l
f 0:l
o < j ≤ f 0:l
o + ˆf l
f 0:l

o + ˆf l
o < j ≤ f 0:m

o

o + ˆf l

o −f l
o

,

(4)

o = (cid:80)l−1

v=0 f v

where f 0:l
o is the number of input channels for
the lth layer, the ﬁrst part corresponds to [O0, · · · , Ol−1],
the second part corresponds to [ ˆOl], and the last part corre-
sponds to [Ol+1, · · · , Om−1]. A simple example of ˆGm is
given as

ˆGm : {1, · · · , 5,

ˆOl
(cid:122) (cid:125)(cid:124) (cid:123)
6, 7, 8, 9, 10, 11} → {1, · · · , 5,

ˆOl
(cid:122) (cid:125)(cid:124) (cid:123)
6, 7, 6, 6, 8, 9}

where Gl : {1, 2, 3, 4} → {1, 2, 1, 1}.

Accordingly the new kernel of mth layer can be given by
Eq. (3) with Gl replaced with ˆGm.

To insert a new layer in DenseNet, suppose the new
layer is inserted after the lth layer. Denote the output of
the new layer as Onew, and its input is [O0, O1, · · · , Ol].
Therefore, for the mth (m > l) layer, its new input after
the insertion is [O0, O1, · · · , Ol, Onew, Ol+1, · · · , Om−1].
To preserve the functionality, similar to the Net2WiderNet
case, Onew should be the replication of some entries in
[O0, O1, · · · , Ol]. It is possible, since the input of the new
layer is [O0, O1, · · · , Ol]. Each ﬁlter in the new layer can
be represented with a tensor, denoted as ˆF with shape
i = f 0:l+1
(knew
), where knew
denote the
width and height of the ﬁlter, and f new
is the number of in-
put channels. To make the output of ˆF to be a replication

w and knew
i

h , f new

w , knew

h

o

Figure 3: Net2Deeper actor, which uses a recurrent network
to sequentially determine where to insert the new layer and
corresponding parameters for the new layer based on the ﬁ-
nal hidden state of the encoder network given the input ar-
chitecture.

or tanh activation, which could be useful compared to ran-
dom initialization. Additionally, when using batch normal-
ization (Ioffe and Szegedy 2015), we need to set output scale
and output bias of the batch normalization layer to undo the
normalization, rather than initialize them as ones and zeros.
Further details about the Net2DeeperNet operation is pro-
vided in the original paper (Chen, Goodfellow, and Shlens
2015).

The structure of the Net2Deeper actor is shown in Fig-
ure 3, which is a recurrent network whose hidden state is
initialized with the ﬁnal hidden state of the encoder net-
work. Similar to previous work (Baker et al. 2017), we al-
low the Net2Deeper actor to insert one new layer at each
step. Speciﬁcally, we divide a CNN architecture into sev-
eral blocks according to the pooling layers and Net2Deeper
actor sequentially determines which block to insert the new
layer, a speciﬁc index within the block and parameters of the
new layer. For a new convolutional layer, the agent needs to
determine the ﬁlter size and the stride while for a new fully-
connected layer, no parameter prediction is needed. In CNN
architectures, any fully-connected layer should be on the top
of all convolutional and pooling layers. To avoid resulting in
unreasonable architectures, if the Net2Deeper actor decides
to insert a new layer after a fully-connected layer or the ﬁnal
global average pooling layer, the new layer is restricted to be
a fully-connected layer, otherwise it must be a convolutional
layer.

Function-preserving Transformation for DenseNet
The original Net2Net operations proposed in (Chen, Good-
fellow, and Shlens 2015) are discussed under the scenarios
where the network is arranged layer-by-layer, i.e. the output
of a layer is only fed to its next layer. As such, in some mod-
ern CNN architectures where the output of a layer would be
fed to multiple subsequent layers, such as DenseNet (Huang

of the nth entry in [O0, O1, · · · , Ol], we can set ˆF (using
the special case that knew
h = 3 for illustration) as the
following

w = knew

ˆF [x, y, n] =





0
0
0

0
1
0



 ,

0
0
0

(5)

while all other values in ˆF are set to be 0. Note that n can be
chosen randomly from {1, · · · , f 0:l+1
} for each ﬁlter. After
all ﬁlters in the new layer are set, we can form an equivalent
random remapping function for all subsequent layers as is
done in Eq. (4) and modify their kernels accordingly.

o

Experiments and Results

In line with the previous work (Baker et al. 2017; Zoph and
Le 2017; Real et al. 2017), we apply the proposed EAS on
image benchmark datasets (CIFAR-10 and SVHN) to ex-
plore high performance CNN architectures for the image
classiﬁcation task1. Notice that the performances of the ﬁnal
designed models largely depend on the architecture space
and the computational resources. In our experiments, we
evaluate EAS in two different settings. In all cases, we use
restricted computational resources (5 GPUs) compared to
the previous work such as (Zoph and Le 2017) that used
800 GPUs. In the ﬁrst setting, we apply EAS to explore the
plain CNN architecture space, which purely consists of con-
volutional, pooling and fully-connected layers. While in the
second setting, we apply EAS to explore the DenseNet ar-
chitecture space.

Image Datasets

CIFAR-10 The CIFAR-10 dataset (Krizhevsky and Hin-
ton 2009) consists of 50,000 training images and 10,000
test images. We use a standard data augmentation scheme
that is widely used for CIFAR-10 (Huang et al. 2017), and
denote the augmented dataset as C10+ while the original
dataset is denoted as C10. For preprocessing, we normal-
ized the images using the channel means and standard de-
viations. Following the previous work (Baker et al. 2017;
Zoph and Le 2017), we randomly sample 5,000 images from
the training set to form a validation set while using the re-
maining 45,000 images for training during exploring the ar-
chitecture space.

SVHN The Street View House Numbers (SVHN) dataset
(Netzer et al. 2011) contains 73,257 images in the original
training set, 26,032 images in the test set, and 531,131 addi-
tional images in the extra training set. For preprocessing, we
divide the pixel values by 255 and do not perform any data
augmentation, as is done in (Huang et al. 2017). We follow
(Baker et al. 2017) and use the original training set during
the architecture search phase with 5,000 randomly sampled
images as the validation set, while training the ﬁnal discov-
ered architectures using all the training data, including the
original training set and extra training set.

1Experiment code and discovered top architectures along with

weights: https://github.com/han-cai/EAS

Figure 4: Progress of two stages architecture search on C10+
in the plain CNN architecture space.

Training Details
For the meta-controller, we use a one-layer bidirectional
LSTM with 50 hidden units as the encoder network (Fig-
ure 1) with an embedding size of 16, and train it with the
ADAM optimizer (Kingma and Ba 2015).

At each step, the meta-controller samples 10 networks by
taking network transformation actions. Since the sampled
networks are not trained from scratch but we reuse weights
of the given network in our scenario, they are then trained for
20 epochs, a relative small number compared to 50 epochs in
(Zoph and Le 2017). Besides, we use a smaller initial learn-
ing rate for this reason. Other settings for training networks
on CIFAR-10 and SVHN, are similar to (Huang et al. 2017;
Zoph and Le 2017). Speciﬁcally, we use the SGD with a
Nesterov momentum (Sutskever et al. 2013) of 0.9, a weight
decay of 0.0001, a batch size of 64. The initial learning rate
is 0.02 and is further annealed with a cosine learning rate
decay (Gastaldi 2017). The accuracy in the held-out valida-
tion set is used to compute the reward signal for each sam-
pled network. Since the gain of improving the accuracy from
90% to 91% should be much larger than from 60% to 61%,
instead of directly using the validation accuracy accv as the
reward, as done in (Zoph and Le 2017), we perform a non-
linear transformation on accv, i.e. tan(accv × π/2), and use
the transformed value as the reward. Additionally, we use
an exponential moving average of previous rewards, with a
decay of 0.95 as the baseline function to reduce the variance.

Explore Plain CNN Architecture Space
We start applying EAS to explore the plain CNN archi-
tecture space. Following the previous automatic architec-
ture designing methods (Baker et al. 2017; Zoph and Le
2017), EAS searches layer parameters in a discrete and lim-
ited space. For every convolutional layer, the ﬁlter size is
chosen from {1, 3, 5} and the number of ﬁlters is cho-
sen from {16, 32, 64, 96, 128, 192, 256, 320, 384, 448, 512},
while the stride is ﬁxed to be 1 (Baker et al. 2017). For every
fully-connected layer, the number of units is chosen from
{64, 128, 256, 384, 512, 640, 768, 896, 1024}. Additionally,

Table 1: Simple start point network. C(n, f, l) denotes a convolutional layer with n ﬁlters, ﬁlter size f and stride l; P(f, l, MAX)
and P(f, l, AVG) denote a max and an average pooling layer with ﬁlter size f and stride l respectively; FC(n) denotes a fully-
connected layer with n units; SM(n) denotes a softmax layer with n output units.

Model Architecture
C(16, 3, 1), P(2, 2, MAX), C(32, 3, 1), P(2, 2, MAX), C(64, 3, 1),
P(2, 2, MAX), C(128, 3, 1), P(4, 4, AVG), FC(256), SM(10)

Validation Accuracy (%)

87.07

we use ReLU and batch normalization for each convolu-
tional or fully-connected layer. For SVHN, we add a dropout
layer after each convolutional layer (except the ﬁrst layer)
and use a dropout rate of 0.2 (Huang et al. 2017).

Start with Small Network We begin the exploration on
C10+, using a small network (see Table 1), which achieves
87.07% accuracy in the held-out validation set, as the start
point. Different from (Zoph and Le 2017; Baker et al. 2017),
EAS is not restricted to start from empty and can ﬂexibly
use any discovered architecture as the new start point. As
such, to take the advantage of such ﬂexibility and also re-
duce the search space for saving the computational resources
and time, we divide the whole architecture search process
into two stages where we allow the meta-controller to take 5
steps of Net2Deeper action and 4 steps of Net2Wider action
in the ﬁrst stage. After 300 networks are sampled, we take
the network which performs best currently and train it with
a longer period of time (100 epochs) to be used as the start
point for the second stage. Similarly, in the second stage, we
also allow the meta-controller to take 5 steps of Net2Deeper
action and 4 steps of Net2Wider action and stop exploration
after 150 networks are sampled.

The progress of the two stages architecture search is
shown in Figure 4, where we can ﬁnd that EAS gradu-
ally learns to pick high performance architectures at each
stage. As EAS takes function-preserving transformations to
explore the architecture space, we can also ﬁnd that the
sampled architectures consistently perform better than the
start point network at each stage. Thus it is usually “safe”
to explore the architecture space with EAS. We take the
top networks discovered during the second stage and fur-
ther train the networks with 300 epochs using the full train-
ing set. Finally, the best model achieves 95.11% test ac-
curacy (i.e. 4.89% test error rate). Furthermore, to justify
the transferability of the discovered networks, we train the
top architecture (95.11% test accuracy) on SVHN from ran-
dom initialization with 40 epochs using the full training
set and achieves 98.17% test accuracy (i.e. 1.83% test er-
ror rate), better than both human-designed and automatically
designed architectures that are in the plain CNN architecture
space (see Table 2).

We would like to emphasize that the required computa-
tional resources to achieve this result is much smaller than
those required in (Zoph and Le 2017; Real et al. 2017).
Speciﬁcally, it takes less than 2 days on 5 GeForce GTX
1080 GPUs with totally 450 networks trained to achieve
4.89% test error rate on C10+ starting from a small network.

Further Explore Larger Architecture Space To further
search better architectures in the plain CNN architecture

Table 2: Test error rate (%) comparison with CNNs that use
convolutional, fully-connected and pooling layers alone.

human
designed

auto
designed

Model
Maxout (Goodfellow et al. 2013)
NIN (Lin, Chen, and Yan 2013)
All-CNN (Springenberg et al. 2014)
VGGnet (Simonyan and Zisserman 2015)
MetaQNN (Baker et al. 2017) (depth=7)
MetaQNN (Baker et al. 2017) (ensemble)
EAS (plain CNN, depth=16)
EAS (plain CNN, depth=20)

C10+
9.38
8.81
7.25
7.25
6.92
-
4.89
4.23

SVHN
2.47
2.35
-
-
-
2.06
1.83
1.73

space, in the second experiment, we use the top architec-
tures discovered in the ﬁrst experiment, as the start points
to explore a larger architecture space on C10+ and SVHN.
This experiment on each dataset takes around 2 days on 5
GPUs.

The summarized results of comparing with human-
designed and automatically designed architectures that use
a similar design scheme (plain CNN), are reported in Table
2, where we can ﬁnd that the top model designed by EAS
on the plain CNN architecture space outperforms all similar
models by a large margin. Speciﬁcally, comparing to human-
designed models, the test error rate drops from 7.25% to
4.23% on C10+ and from 2.35% to 1.73% on SVHN. While
comparing to MetaQNN, the Q-learning based automatic ar-
chitecture designing method, EAS achieves a relative test er-
ror rate reduction of 38.9% on C10+ and 16.0% on SVHN.
We also notice that the best model designed by MetaQNN
on C10+ only has a depth of 7, though the maximum is set
to be 18 in the original paper (Baker et al. 2017). We sup-
pose maybe they trained each designed network from scratch
and used an aggressive training strategy to accelerate train-
ing, which resulted in many networks under performed, es-
pecially for deep networks. Since we reuse the weights of
pre-existing networks, the deep networks are validated more
accurately in EAS, and we can thus design deeper and more
accurate networks than MetaQNN.

We also report the comparison with state-of-the-art ar-
chitectures that use advanced techniques such as skip-
connections, branching etc., on C10+ in Table 3. Though it
is not a fair comparison since we do not incorporate such
advanced techniques into the search space in this experi-
ment, we still ﬁnd that the top model designed by EAS is
highly competitive even comparing to these state-of-the-art
modern architectures. Speciﬁcally, the 20-layers plain CNN
with 23.4M parameters outperforms ResNet, its stochas-
tic depth variant and its pre-activation variant. It also ap-
proaches the best result given by DenseNet. When com-
paring to automatic architecture designing methods that in-

Table 3: Test error rate (%) comparison with state-of-the-art architectures.

Model
ResNet (He et al. 2016a)
ResNet (stochastic depth) (Huang et al. 2017)
Wide ResNet (Zagoruyko and Komodakis 2016)
Wide ResNet (Zagoruyko and Komodakis 2016)
ResNet (pre-activation) (He et al. 2016b)
DenseNet (L = 40, k = 12) (Huang et al. 2017)
DenseNet-BC (L = 100, k = 12) (Huang et al. 2017)
DenseNet-BC (L = 190, k = 40) (Huang et al. 2017)
Large-Scale Evolution (250 GPUs)(Real et al. 2017)
NAS (predicting strides, 800 GPUs) (Zoph and Le 2017)
NAS (max pooling, 800 GPUs) (Zoph and Le 2017)
NAS (post-processing, 800 GPUs) (Zoph and Le 2017)
EAS (plain CNN, 5 GPUs)

Depth
110
1202
16
28
1001
40
100
190
-
20
39
39
20

Params C10+
6.61
1.7M
4.91
10.2M
4.81
11.0M
4.17
36.5M
4.62
10.2M
5.24
1.0M
4.51
0.8M
3.46
25.6M
5.40
5.4M
6.01
2.5M
4.47
7.1M
3.65
37.4M
4.23
23.4M

human
designed

auto
designed

action. The result is reported in Figure 5, which shows that
the RL based meta-controller can effectively focus on the
right search direction, while the random search cannot (left
plot), and thus ﬁnd high performance architectures more ef-
ﬁciently than random search.

Explore DenseNet Architecture Space
We also apply EAS to explore the DenseNet architecture
space. We use the DenseNet-BC (L = 40, k = 40) as
the start point. The growth rate, i.e. the width of the non-
bottleneck layer is chosen from {40, 44, 48, 52, 56, 60, 64},
and the result is reported in Table 4. We ﬁnd that by ap-
plying EAS to explore the DenseNet architecture space, we
achieve a test error rate of 4.66% on C10, better than the
best result, i.e. 5.19% given by the original DenseNet while
having 43.79% less parameters. On C10+, we achieve a test
error rate of 3.44%, also outperforming the best result, i.e.
3.46% given by the original DenseNet while having 58.20%
less parameters.

Conclusion
In this paper, we presented EAS, a new framework to-
ward economical and efﬁcient architecture search, where
the meta-controller is implemented as a RL agent. It learns
to take actions for network transformation to explore the
architecture space. By starting from an existing network
and reusing its weights via the class of function-preserving
transformation operations, EAS is able to utilize knowledge
stored in previously trained networks and take advantage
of the existing successful architectures in the target task to
explore the architecture space efﬁciently. Our experiments
have demonstrated EAS’s outstanding performance and ef-
ﬁciency compared with several strong baselines. For future
work, we would like to explore more network transforma-
tion operations and apply EAS for different purposes such
as searching networks that not only have high accuracy but
also keep a balance between the size and the performance.

Acknowledgments
This research was sponsored by Huawei Innovation Re-
search Program, NSFC (61702327) and Shanghai Sailing
Program (17YF1428200).

Figure 5: Comparison between RL based meta-controller
and random search on C10+.

Table 4: Test error rate (%) results of exploring DenseNet
architecture space with EAS.

Model
DenseNet (L = 100, k = 24)
DenseNet-BC (L = 250, k = 24)
DenseNet-BC (L = 190, k = 40)
NAS (post-processing)
EAS (DenseNet on C10)
EAS (DenseNet on C10+)

Depth
100
250
190
39
70
76

Params C10 C10+
3.74
27.2M 5.83
3.62
15.3M 5.19
3.46
25.6M
3.65
37.4M
8.6M 4.66
-
3.44
10.7M

-
-

-

corporate skip-connections into their search space, our 20-
layers plain model beats most of them except NAS with
post-processing, that is much deeper and has more param-
eters than our model. Moreover, we only use 5 GPUs and
train hundreds of networks while they use 800 GPUs and
train tens of thousands of networks.

Comparison Between RL and Random Search Our
framework is not restricted to use the RL based meta-
controller. Beside RL, one can also take network transfor-
mation actions to explore the architecture space by random
search, which can be effective in some cases (Bergstra and
Bengio 2012). In this experiment, we compare the perfor-
mances of the RL based meta-controller and the random
search meta-controller in the architecture space that is used
in the above experiments. Speciﬁcally, we use the network
in Table 1 as the start point and let the meta-controller to
take 5 steps of Net2Deeper action and 4 steps of Net2Wider

[Netzer et al. 2011] Netzer, Y.; Wang, T.; Coates, A.; Bissacco, A.;
Wu, B.; and Ng, A. Y. 2011. Reading digits in natural images with
unsupervised feature learning. In NIPS workshop on deep learning
and unsupervised feature learning.

[Real et al. 2017] Real, E.; Moore, S.; Selle, A.; Saxena, S.; Sue-
matsu, Y. L.; Le, Q.; and Kurakin, A. 2017. Large-scale evolution
of image classiﬁers. ICML.

[Schulman et al. 2015] Schulman, J.; Levine, S.; Abbeel, P.; Jordan,
M. I.; and Moritz, P. 2015. Trust region policy optimization. In
ICML.

[Schuster and Paliwal 1997] Schuster, M., and Paliwal, K. K. 1997.
Bidirectional recurrent neural networks. IEEE Transactions on Sig-
nal Processing.

[Silver et al. 2016] Silver, D.; Huang, A.; Maddison, C. J.; Guez,
A.; Sifre, L.; Van Den Driessche, G.; Schrittwieser, J.; Antonoglou,
I.; Panneershelvam, V.; Lanctot, M.; et al. 2016. Mastering the
game of go with deep neural networks and tree search. Nature.
[Simonyan and Zisserman 2015] Simonyan, K., and Zisserman, A.
2015. Very deep convolutional networks for large-scale image
recognition. ICLR.

[Snoek, Larochelle, and Adams 2012] Snoek, J.; Larochelle, H.;
and Adams, R. P. 2012. Practical bayesian optimization of ma-
chine learning algorithms. In NIPS.

[Springenberg et al. 2014] Springenberg, J. T.; Dosovitskiy, A.;
Brox, T.; and Riedmiller, M. 2014. Striving for simplicity: The
all convolutional net. arXiv preprint arXiv:1412.6806.

[Stanley and Miikkulainen 2002] Stanley, K. O., and Miikkulainen,
R. 2002. Evolving neural networks through augmenting topolo-
gies. Evolutionary computation.

[Sutskever et al. 2013] Sutskever, I.; Martens, J.; Dahl, G.; and Hin-
ton, G. 2013. On the importance of initialization and momentum
in deep learning. In ICML.

[Sutskever, Vinyals, and Le 2014] Sutskever, I.; Vinyals, O.; and
Le, Q. V. 2014. Sequence to sequence learning with neural net-
works. In NIPS.

[Sutton and Barto 1998] Sutton, R. S., and Barto, A. G. 1998. Re-

inforcement learning: An introduction. MIT press Cambridge.

[Williams 1992] Williams, R. J. 1992. Simple statistical gradient-
learning.

following algorithms for connectionist reinforcement
Machine learning.

[Zagoruyko and Komodakis 2016] Zagoruyko, S., and Komodakis,
arXiv preprint
Wide residual networks.

N.
2016.
arXiv:1605.07146.

[Zoph and Le 2017] Zoph, B., and Le, Q. V. 2017. Neural architec-

ture search with reinforcement learning. ICLR.

References
[Bahdanau, Cho, and Bengio 2014] Bahdanau, D.; Cho, K.; and
Bengio, Y. 2014. Neural machine translation by jointly learning to
align and translate. ICLR.

[Baker et al. 2017] Baker, B.; Gupta, O.; Naik, N.; and Raskar, R.
2017. Designing neural network architectures using reinforcement
learning. ICLR.

[Bergstra and Bengio 2012] Bergstra, J., and Bengio, Y. 2012. Ran-

dom search for hyper-parameter optimization. JMLR.

[Cai et al. 2017] Cai, H.; Ren, K.; Zhang, W.; Malialis, K.; Wang,
J.; Yu, Y.; and Guo, D. 2017. Real-time bidding by reinforcement
learning in display advertising. In WSDM.

[Chen, Goodfellow, and Shlens 2015] Chen, T.; Goodfellow, I.; and
Shlens, J. 2015. Net2net: Accelerating learning via knowledge
transfer. ICLR.

[Domhan, Springenberg, and Hutter 2015] Domhan, T.; Springen-
berg, J. T.; and Hutter, F. 2015. Speeding up automatic hyper-
parameter optimization of deep neural networks by extrapolation
of learning curves. In IJCAI.

[Gastaldi 2017] Gastaldi, X. 2017. Shake-shake regularization.

arXiv preprint arXiv:1705.07485.
[Goodfellow et al. 2013] Goodfellow,

J.; Warde-Farley, D.;
Mirza, M.; Courville, A.; and Bengio, Y. 2013. Maxout networks.
ICML.

I.

[Han et al. 2015] Han, S.; Pool, J.; Tran, J.; and Dally, W. 2015.
Learning both weights and connections for efﬁcient neural net-
work. In NIPS.

[He et al. 2016a] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016a.

Deep residual learning for image recognition. In CVPR.

[He et al. 2016b] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016b.

Identity mappings in deep residual networks. In ECCV.

[Huang et al. 2017] Huang, G.; Liu, Z.; Weinberger, K. Q.; and
van der Maaten, L. 2017. Densely connected convolutional net-
works. CVPR.

[Ioffe and Szegedy 2015] Ioffe, S., and Szegedy, C. 2015. Batch
normalization: Accelerating deep network training by reducing in-
ternal covariate shift. ICML.

[Kakade 2002] Kakade, S. 2002. A natural policy gradient. NIPS.
[Kingma and Ba 2015] Kingma, D., and Ba, J. 2015. Adam: A

method for stochastic optimization. ICLR.

[Klein et al. 2017] Klein, A.; Falkner, S.; Springenberg, J. T.; and
Hutter, F. 2017. Learning curve prediction with bayesian neural
networks. ICLR.

[Krizhevsky and Hinton 2009] Krizhevsky, A., and Hinton, G.

2009. Learning multiple layers of features from tiny images.

[Krizhevsky, Sutskever, and Hinton 2012] Krizhevsky,

Sutskever, I.; and Hinton, G. E.
tion with deep convolutional neural networks. In NIPS.

2012.

A.;
Imagenet classiﬁca-

[Lin, Chen, and Yan 2013] Lin, M.; Chen, Q.; and Yan, S. 2013.

Network in network. arXiv preprint arXiv:1312.4400.

[Mendoza et al. 2016] Mendoza, H.; Klein, A.; Feurer, M.; Sprin-
genberg, J. T.; and Hutter, F. 2016. Towards automatically-tuned
neural networks. In Workshop on Automatic Machine Learning.
[Miller, Todd, and Hegde 1989] Miller, G. F.; Todd, P. M.; and
Hegde, S. U. 1989. Designing neural networks using genetic algo-
rithms. In ICGA. Morgan Kaufmann Publishers Inc.

[Negrinho and Gordon 2017] Negrinho, R., and Gordon, G. 2017.
Deeparchitect: Automatically designing and training deep architec-
tures. arXiv preprint arXiv:1704.08792.

Efﬁcient Architecture Search by Network Transformation

Han Cai1, Tianyao Chen1, Weinan Zhang1∗, Yong Yu1, Jun Wang2
1Shanghai Jiao Tong University, 2University College London
{hcai,tychen,wnzhang,yyu}@apex.sjtu.edu.cn, j.wang@cs.ucl.ac.uk

7
1
0
2
 
v
o
N
 
1
2
 
 
]

G
L
.
s
c
[
 
 
2
v
3
7
8
4
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

Techniques for automatically designing deep neural net-
work architectures such as reinforcement learning based ap-
proaches have recently shown promising results. However,
their success is based on vast computational resources (e.g.
hundreds of GPUs), making them difﬁcult to be widely used.
A noticeable limitation is that they still design and train each
network from scratch during the exploration of the architec-
ture space, which is highly inefﬁcient. In this paper, we pro-
pose a new framework toward efﬁcient architecture search by
exploring the architecture space based on the current network
and reusing its weights. We employ a reinforcement learn-
ing agent as the meta-controller, whose action is to grow the
network depth or layer width with function-preserving trans-
formations. As such, the previously validated networks can
be reused for further exploration, thus saves a large amount
of computational cost. We apply our method to explore the
architecture space of the plain convolutional neural networks
(no skip-connections, branching etc.) on image benchmark
datasets (CIFAR-10, SVHN) with restricted computational
resources (5 GPUs). Our method can design highly com-
petitive networks that outperform existing networks using
the same design scheme. On CIFAR-10, our model with-
out skip-connections achieves 4.23% test error rate, exceed-
ing a vast majority of modern architectures and approaching
DenseNet. Furthermore, by applying our method to explore
the DenseNet architecture space, we are able to achieve more
accurate networks with fewer parameters.

Introduction
The great success of deep neural networks in various chal-
lenging applications (Krizhevsky, Sutskever, and Hinton
2012; Bahdanau, Cho, and Bengio 2014; Silver et al. 2016)
has led to a paradigm shift from feature designing to archi-
tecture designing, which still remains a laborious task and
requires human expertise. In recent years, many techniques
for automating the architecture design process have been
proposed (Snoek, Larochelle, and Adams 2012; Bergstra
and Bengio 2012; Baker et al. 2017; Zoph and Le 2017;
Real et al. 2017; Negrinho and Gordon 2017), and promis-
ing results of designing competitive models against human-
designed models are reported on some benchmark datasets

∗Correspondence to Weinan Zhang.

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

(Zoph and Le 2017; Real et al. 2017). Despite the promising
results as reported, their success is based on vast computa-
tional resources (e.g. hundreds of GPUs), making them dif-
ﬁcult to be used in practice for individual researchers, small
sized companies, or university research teams. Another key
drawback is that they still design and train each network
from scratch during exploring the architecture space without
any leverage of previously explored networks, which results
in high computational resources waste.

In fact, during the architecture design process, many
slightly different networks are trained for the same task.
Apart from their ﬁnal validation performances that are used
to guide exploration, we should also have access to their
architectures, weights, training curves etc., which contain
abundant knowledge and can be leveraged to accelerate the
architecture design process just like human experts (Chen,
Goodfellow, and Shlens 2015; Klein et al. 2017). Further-
more, there are typically many well-designed architectures,
by human or automatic architecture designing methods, that
have achieved good performances at the target task. Under
restricted computational resources limits, instead of totally
neglecting these existing networks and exploring the archi-
tecture space from scratch (which does not guarantee to re-
sult in better performance architectures), a more economical
and efﬁcient alternative could be exploring the architecture
space based on these successful networks and reusing their
weights.

In this paper, we propose a new framework, called EAS,
Efﬁcient Architecture Search, where the meta-controller ex-
plores the architecture space by network transformation op-
erations such as widening a certain layer (more units or ﬁl-
ters), inserting a layer, adding skip-connections etc., given
an existing network trained on the same task. To reuse
weights, we consider the class of function-preserving trans-
formations (Chen, Goodfellow, and Shlens 2015) that allow
to initialize the new network to represent the same function
as the given network but use different parameterization to
be further trained to improve the performance, which can
signiﬁcantly accelerate the training of the new network es-
pecially for large networks. Furthermore, we combine our
framework with recent advances of reinforcement learn-
ing (RL) based automatic architecture designing methods
(Baker et al. 2017; Zoph and Le 2017), and employ a RL
based agent as the meta-controller.

Our experiments of exploring the architecture space of
the plain convolutional neural networks (CNNs), which
purely consists of convolutional, fully-connected and pool-
ing layers without skip-connections, branching etc., on im-
age benchmark datasets (CIFAR-10, SVHN), show that EAS
with limited computational resources (5 GPUs) can design
competitive architectures. The best plain model designed
by EAS on CIFAR-10 with standard data augmentation
achieves 4.23% test error rate, even better than many modern
architectures that use skip-connections. We further apply our
method to explore the DenseNet (Huang et al. 2017) archi-
tecture space, and achieve 4.66% test error rate on CIFAR-
10 without data augmentation and 3.44% on CIFAR-10 with
standard data augmentation, surpassing the best results given
by the original DenseNet while still maintaining fewer pa-
rameters.

Related Work and Background
Automatic Architecture Designing There is a long stand-
ing study on automatic architecture designing. Neuro-
evolution algorithms which mimic the evolution processes
in the nature, are one of the earliest automatic architec-
ture designing methods (Miller, Todd, and Hegde 1989;
Stanley and Miikkulainen 2002). Authors in (Real et al.
2017) used neuro-evolution algorithms to explore a large
CNN architecture space and achieved networks which can
match performances of human-designed models. In paral-
lel, automatic architecture designing has also been stud-
ied in the context of Bayesian optimization (Bergstra and
Bengio 2012; Domhan, Springenberg, and Hutter 2015;
Mendoza et al. 2016). Recently, reinforcement learning is in-
troduced in automatic architecture designing and has shown
strong empirical results. Authors in (Baker et al. 2017) pre-
sented a Q-learning agent to sequentially pick CNN layers;
authors in (Zoph and Le 2017) used an auto-regressive recur-
rent network to generate a variable-length string that speci-
ﬁes the architecture of a neural network and trained the re-
current network with policy gradient.

As the above solutions rely on designing or training
networks from scratch, signiﬁcant computational resources
have been wasted during the construction. In this paper,
we aim to address the efﬁciency problem. Technically, we
allow to reuse the existing networks trained on the same
task and take network transformation actions. Both function-
preserving transformations and an alternative RL based
meta-controller are used to explore the architecture space.
Moreover, we notice that there are some complementary
techniques, such as learning curve prediction (Klein et al.
2017), for improving the efﬁciency, which can be combined
with our method.

Network Transformation and Knowledge Transfer
Generally, any modiﬁcation to a given network can be
viewed as a network transformation operation. In this pa-
per, since our aim is to utilize knowledge stored in previ-
ously trained networks, we focus on identifying the kind
of network transformation operations that would be able to
reuse pre-existing models. The idea of reusing pre-existing
models or knowledge transfer between neural networks

has been studied before. Net2Net technique introduced in
(Chen, Goodfellow, and Shlens 2015) describes two speciﬁc
function-preserving transformations, namely Net2WiderNet
and Net2DeeperNet, which respectively initialize a wider or
deeper student network to represent the same functionality
of the given teacher network and have proved to signiﬁcantly
accelerate the training of the student network especially for
large networks. Similar function-preserving schemes have
also been proposed in ResNet particularly for training very
deep architectures (He et al. 2016a). Additionally, the net-
work compression technique presented in (Han et al. 2015)
prunes less important connections (low-weight connections)
in order to shrink the size of neural networks without reduc-
ing their accuracy.

In this paper, instead, we focus on utilizing such network
transformations to reuse pre-existing models to efﬁciently
and economically explore the architecture space for auto-
matic architecture designing.

Reinforcement Learning Background Our meta-
in this work is based on RL (Sutton and
controller
Barto 1998), techniques for training the agent to max-
imize the cumulative reward when interacting with
an environment (Cai et al. 2017). We use the REIN-
FORCE algorithm (Williams 1992) similar
to (Zoph
and Le 2017) for updating the meta-controller, while
other advanced policy gradient methods (Kakade 2002;
Schulman et al. 2015) can be applied analogously. Our
action space is, however, different with that of (Zoph and Le
2017) or any other RL based approach (Baker et al. 2017),
as our actions are the network transformation operations
like adding, deleting, widening, etc., while others are
speciﬁc conﬁgurations of a newly created network layer
on the top of preceding layers. Speciﬁcally, we model the
automatic architecture design procedure as a sequential
decision making process, where the state is the current
network architecture and the action is the corresponding
network transformation operation. After T steps of network
transformations, the ﬁnal network architecture, along with
its weights transferred from the initial input network, is then
trained in the real data to get the validation performance
to calculate the reward signal, which is further used to
update the meta-controller via policy gradient algorithms
to maximize the expected validation performances of the
designed networks by the meta-controller.

Architecture Search by Net Transformation
In this section, we ﬁrst introduce the overall framework
of our meta-controller, and then show how each speciﬁc
network transformation decision is made under it. We
later extend the function-preserving transformations to the
DenseNet (Huang et al. 2017) architecture space where di-
rectly applying the original Net2Net operations can be prob-
lematic since the output of a layer will be fed to all subse-
quent layers.

We consider learning a meta-controller to generate net-
work transformation actions given the current network ar-
chitecture, which is speciﬁed with a variable-length string
(Zoph and Le 2017). To be able to generate various types

Figure 1: Overview of the RL based meta-controller in EAS,
which consists of an encoder network for encoding the ar-
chitecture and multiple separate actor networks for taking
network transformation actions.

Figure 2: Net2Wider actor, which uses a shared sigmoid
classiﬁer to simultaneously determine whether to widen
each layer based on its hidden state given by the encoder
network.

of network transformation actions while keeping the meta-
controller simple, we use an encoder network to learn a low-
dimensional representation of the given architecture, which
is then fed into each separate actor network to generate
a certain type of network transformation actions. Further-
more, to handle variable-length network architectures as in-
put and take the whole input architecture into considera-
tion when making decisions, the encoder network is imple-
mented with a bidirectional recurrent network (Schuster and
Paliwal 1997) with an input embedding layer. The overall
framework is illustrated in Figure 1, which is an analogue
of end-to-end sequence to sequence learning (Sutskever,
Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2014).

Actor Networks
Given the low dimensional representation of the input archi-
tecture, each actor network makes necessary decisions for
taking a certain type of network transformation actions. In
this work, we introduce two speciﬁc actor networks, namely
Net2Wider actor and Net2Deeper actor which correspond to
Net2WiderNet and Net2DeeperNet respectively.

Net2Wider Actor Net2WiderNet operation allows to re-
place a layer with a wider layer, meaning more units for
fully-connected layers, or more ﬁlters for convolutional lay-
ers, while preserving the functionality. For example, con-
sider a convolutional layer with kernel Kl whose shape is
(kl
i , f l
h, f l
h denote the ﬁlter width and
height, while f l
o denote the number of input and out-
put channels. To replace this layer with a wider layer that
has ˆf l
o) output channels, we should ﬁrst introduce a
random remapping function Gl, which is deﬁned as

o) where kl
i and f l

w and kl

o (> f l

w, kl

(cid:40)

Gl(j) =

j
random sample from {1, · · · , f l

1 ≤ j ≤ f l
o
o < j ≤ ˆf l

o

o} f l

.

(1)

With the remapping function Gl, we have the new kernel ˆKl
for the wider layer with shape (kl

i , ˆf l
o)
ˆK l[x, y, i, j] = K l[x, y, i, Gl(j)].

w, kl

h, f l

(2)

As such, the ﬁrst f l
of ˆKl are directly copied from Kl while the remaining ˆf l

o entries in the output channel dimension
o −

f l
o entries are created by choosing randomly as deﬁned in
Gl. Accordingly, the new output of the wider layer is ˆOl
with ˆOl(j) = Ol(Gl(j)), where Ol is the output of the
original layer and we only show the channel dimension to
make the notation simpler.

To preserve the functionality, the kernel Kl+1 of the next
layer should also be modiﬁed due to the replication in its
input. The new kernel ˆKl+1 with shape (kl+1
i =
ˆf l
o, f l+1
o

h , ˆf l+1

w , kl+1

) is given as

ˆK l+1[x, y, j, k] =

K l+1[x, y, Gl(j), k]
(cid:12){z|Gl(z) = Gl(j)}(cid:12)
(cid:12)
(cid:12)

.

(3)

For further details, we refer to the original Net2Net work
(Chen, Goodfellow, and Shlens 2015).

In our work, to be ﬂexible and efﬁcient, the Net2Wider
actor simultaneously determines whether each layer should
be extended. Speciﬁcally, for each layer, this decision is
carried out by a shared sigmoid classiﬁer given the hid-
den state of the layer learned by the bidirectional encoder
network. Moreover, we follow previous work and search
the number of ﬁlters for convolutional layers and units for
fully-connected layers in a discrete space. Therefore, if the
Net2Wider actor decides to widen a layer, the number of
ﬁlters or units of the layer increases to the next discrete
level, e.g. from 32 to 64. The structure of Net2Wider actor
is shown in Figure 2.

Net2Deeper Actor Net2DeeperNet operation allows to
insert a new layer that is initialized as adding an identity
mapping between two layers so as to preserve the functional-
ity. For a new convolutional layer, the kernel is set to be iden-
tity ﬁlters while for a new fully-connected layer, the weight
matrix is set to be identity matrix. Thus the new layer is set
with the same number of ﬁlters or units as the layer below
at ﬁrst, and could further get wider when Net2WiderNet op-
eration is performed on it. To fully preserve the functional-
ity, Net2DeeperNet operation has a constraint on the activa-
tion function φ, i.e. φ must satisfy φ(Iφ(v)) = φ(v) for all
vectors v. This property holds for rectiﬁed linear activation
(ReLU) but fails for sigmoid and tanh activation. However,
we can still reuse weights of existing networks with sigmoid

et al. 2017), directly applying the original Net2Net opera-
tions can be problematic. In this section, we introduce sev-
eral extensions to the original Net2Net operations to enable
function-preserving transformations for DenseNet.

Different from the plain CNN, in DenseNet, the lth layer
would receive the outputs of all preceding layers as input,
which are concatenated on the channel dimension, denoted
as [O0, O1, · · · , Ol−1], while its output Ol would be fed to
all subsequent layers.

Denote the kernel of the lth layer as Kl with shape
o). To replace the lth layer with a wider layer
i , f l
(kl
h, f l
w, kl
that has ˆf l
o output channels while preserving the function-
ality, the creation of the new kernel ˆKl in the lth layer
is the same as the original Net2WiderNet operation (see
Eq. (1) and Eq. (2)). As such, the new output of the wider
layer is ˆOl with ˆOl(j) = Ol(Gl(j)), where Gl is the
random remapping function as deﬁned in Eq. (1). Since
the output of the lth layer will be fed to all subsequent
layers in DenseNet, the replication in ˆOl will result in
replication in the inputs of all layers after the lth layer.
As such, instead of only modifying the kernel of the next
layer as done in the original Net2WiderNet operation, we
need to modify the kernels of all subsequent layers in
DenseNet. For the mth layer where m > l, its input be-
comes [O0, · · · , Ol−1, ˆOl, Ol+1, · · · , Om−1] after widen-
ing the lth layer, thus from the perspective of mth layer, the
equivalent random remapping function ˆGm can be written
as

ˆGm(j) =






j
f 0:l
o +Gl(j)
j − ˆf l
o +f l
o

o

1 ≤ j ≤ f 0:l
f 0:l
o < j ≤ f 0:l
o + ˆf l
f 0:l

o + ˆf l
o < j ≤ f 0:m

o

o + ˆf l

o −f l
o

,

(4)

o = (cid:80)l−1

v=0 f v

where f 0:l
o is the number of input channels for
the lth layer, the ﬁrst part corresponds to [O0, · · · , Ol−1],
the second part corresponds to [ ˆOl], and the last part corre-
sponds to [Ol+1, · · · , Om−1]. A simple example of ˆGm is
given as

ˆGm : {1, · · · , 5,

ˆOl
(cid:122) (cid:125)(cid:124) (cid:123)
6, 7, 8, 9, 10, 11} → {1, · · · , 5,

ˆOl
(cid:122) (cid:125)(cid:124) (cid:123)
6, 7, 6, 6, 8, 9}

where Gl : {1, 2, 3, 4} → {1, 2, 1, 1}.

Accordingly the new kernel of mth layer can be given by
Eq. (3) with Gl replaced with ˆGm.

To insert a new layer in DenseNet, suppose the new
layer is inserted after the lth layer. Denote the output of
the new layer as Onew, and its input is [O0, O1, · · · , Ol].
Therefore, for the mth (m > l) layer, its new input after
the insertion is [O0, O1, · · · , Ol, Onew, Ol+1, · · · , Om−1].
To preserve the functionality, similar to the Net2WiderNet
case, Onew should be the replication of some entries in
[O0, O1, · · · , Ol]. It is possible, since the input of the new
layer is [O0, O1, · · · , Ol]. Each ﬁlter in the new layer can
be represented with a tensor, denoted as ˆF with shape
i = f 0:l+1
(knew
), where knew
denote the
width and height of the ﬁlter, and f new
is the number of in-
put channels. To make the output of ˆF to be a replication

w and knew
i

h , f new

w , knew

h

o

Figure 3: Net2Deeper actor, which uses a recurrent network
to sequentially determine where to insert the new layer and
corresponding parameters for the new layer based on the ﬁ-
nal hidden state of the encoder network given the input ar-
chitecture.

or tanh activation, which could be useful compared to ran-
dom initialization. Additionally, when using batch normal-
ization (Ioffe and Szegedy 2015), we need to set output scale
and output bias of the batch normalization layer to undo the
normalization, rather than initialize them as ones and zeros.
Further details about the Net2DeeperNet operation is pro-
vided in the original paper (Chen, Goodfellow, and Shlens
2015).

The structure of the Net2Deeper actor is shown in Fig-
ure 3, which is a recurrent network whose hidden state is
initialized with the ﬁnal hidden state of the encoder net-
work. Similar to previous work (Baker et al. 2017), we al-
low the Net2Deeper actor to insert one new layer at each
step. Speciﬁcally, we divide a CNN architecture into sev-
eral blocks according to the pooling layers and Net2Deeper
actor sequentially determines which block to insert the new
layer, a speciﬁc index within the block and parameters of the
new layer. For a new convolutional layer, the agent needs to
determine the ﬁlter size and the stride while for a new fully-
connected layer, no parameter prediction is needed. In CNN
architectures, any fully-connected layer should be on the top
of all convolutional and pooling layers. To avoid resulting in
unreasonable architectures, if the Net2Deeper actor decides
to insert a new layer after a fully-connected layer or the ﬁnal
global average pooling layer, the new layer is restricted to be
a fully-connected layer, otherwise it must be a convolutional
layer.

Function-preserving Transformation for DenseNet
The original Net2Net operations proposed in (Chen, Good-
fellow, and Shlens 2015) are discussed under the scenarios
where the network is arranged layer-by-layer, i.e. the output
of a layer is only fed to its next layer. As such, in some mod-
ern CNN architectures where the output of a layer would be
fed to multiple subsequent layers, such as DenseNet (Huang

of the nth entry in [O0, O1, · · · , Ol], we can set ˆF (using
the special case that knew
h = 3 for illustration) as the
following

w = knew

ˆF [x, y, n] =





0
0
0

0
1
0



 ,

0
0
0

(5)

while all other values in ˆF are set to be 0. Note that n can be
chosen randomly from {1, · · · , f 0:l+1
} for each ﬁlter. After
all ﬁlters in the new layer are set, we can form an equivalent
random remapping function for all subsequent layers as is
done in Eq. (4) and modify their kernels accordingly.

o

Experiments and Results

In line with the previous work (Baker et al. 2017; Zoph and
Le 2017; Real et al. 2017), we apply the proposed EAS on
image benchmark datasets (CIFAR-10 and SVHN) to ex-
plore high performance CNN architectures for the image
classiﬁcation task1. Notice that the performances of the ﬁnal
designed models largely depend on the architecture space
and the computational resources. In our experiments, we
evaluate EAS in two different settings. In all cases, we use
restricted computational resources (5 GPUs) compared to
the previous work such as (Zoph and Le 2017) that used
800 GPUs. In the ﬁrst setting, we apply EAS to explore the
plain CNN architecture space, which purely consists of con-
volutional, pooling and fully-connected layers. While in the
second setting, we apply EAS to explore the DenseNet ar-
chitecture space.

Image Datasets

CIFAR-10 The CIFAR-10 dataset (Krizhevsky and Hin-
ton 2009) consists of 50,000 training images and 10,000
test images. We use a standard data augmentation scheme
that is widely used for CIFAR-10 (Huang et al. 2017), and
denote the augmented dataset as C10+ while the original
dataset is denoted as C10. For preprocessing, we normal-
ized the images using the channel means and standard de-
viations. Following the previous work (Baker et al. 2017;
Zoph and Le 2017), we randomly sample 5,000 images from
the training set to form a validation set while using the re-
maining 45,000 images for training during exploring the ar-
chitecture space.

SVHN The Street View House Numbers (SVHN) dataset
(Netzer et al. 2011) contains 73,257 images in the original
training set, 26,032 images in the test set, and 531,131 addi-
tional images in the extra training set. For preprocessing, we
divide the pixel values by 255 and do not perform any data
augmentation, as is done in (Huang et al. 2017). We follow
(Baker et al. 2017) and use the original training set during
the architecture search phase with 5,000 randomly sampled
images as the validation set, while training the ﬁnal discov-
ered architectures using all the training data, including the
original training set and extra training set.

1Experiment code and discovered top architectures along with

weights: https://github.com/han-cai/EAS

Figure 4: Progress of two stages architecture search on C10+
in the plain CNN architecture space.

Training Details
For the meta-controller, we use a one-layer bidirectional
LSTM with 50 hidden units as the encoder network (Fig-
ure 1) with an embedding size of 16, and train it with the
ADAM optimizer (Kingma and Ba 2015).

At each step, the meta-controller samples 10 networks by
taking network transformation actions. Since the sampled
networks are not trained from scratch but we reuse weights
of the given network in our scenario, they are then trained for
20 epochs, a relative small number compared to 50 epochs in
(Zoph and Le 2017). Besides, we use a smaller initial learn-
ing rate for this reason. Other settings for training networks
on CIFAR-10 and SVHN, are similar to (Huang et al. 2017;
Zoph and Le 2017). Speciﬁcally, we use the SGD with a
Nesterov momentum (Sutskever et al. 2013) of 0.9, a weight
decay of 0.0001, a batch size of 64. The initial learning rate
is 0.02 and is further annealed with a cosine learning rate
decay (Gastaldi 2017). The accuracy in the held-out valida-
tion set is used to compute the reward signal for each sam-
pled network. Since the gain of improving the accuracy from
90% to 91% should be much larger than from 60% to 61%,
instead of directly using the validation accuracy accv as the
reward, as done in (Zoph and Le 2017), we perform a non-
linear transformation on accv, i.e. tan(accv × π/2), and use
the transformed value as the reward. Additionally, we use
an exponential moving average of previous rewards, with a
decay of 0.95 as the baseline function to reduce the variance.

Explore Plain CNN Architecture Space
We start applying EAS to explore the plain CNN archi-
tecture space. Following the previous automatic architec-
ture designing methods (Baker et al. 2017; Zoph and Le
2017), EAS searches layer parameters in a discrete and lim-
ited space. For every convolutional layer, the ﬁlter size is
chosen from {1, 3, 5} and the number of ﬁlters is cho-
sen from {16, 32, 64, 96, 128, 192, 256, 320, 384, 448, 512},
while the stride is ﬁxed to be 1 (Baker et al. 2017). For every
fully-connected layer, the number of units is chosen from
{64, 128, 256, 384, 512, 640, 768, 896, 1024}. Additionally,

Table 1: Simple start point network. C(n, f, l) denotes a convolutional layer with n ﬁlters, ﬁlter size f and stride l; P(f, l, MAX)
and P(f, l, AVG) denote a max and an average pooling layer with ﬁlter size f and stride l respectively; FC(n) denotes a fully-
connected layer with n units; SM(n) denotes a softmax layer with n output units.

Model Architecture
C(16, 3, 1), P(2, 2, MAX), C(32, 3, 1), P(2, 2, MAX), C(64, 3, 1),
P(2, 2, MAX), C(128, 3, 1), P(4, 4, AVG), FC(256), SM(10)

Validation Accuracy (%)

87.07

we use ReLU and batch normalization for each convolu-
tional or fully-connected layer. For SVHN, we add a dropout
layer after each convolutional layer (except the ﬁrst layer)
and use a dropout rate of 0.2 (Huang et al. 2017).

Start with Small Network We begin the exploration on
C10+, using a small network (see Table 1), which achieves
87.07% accuracy in the held-out validation set, as the start
point. Different from (Zoph and Le 2017; Baker et al. 2017),
EAS is not restricted to start from empty and can ﬂexibly
use any discovered architecture as the new start point. As
such, to take the advantage of such ﬂexibility and also re-
duce the search space for saving the computational resources
and time, we divide the whole architecture search process
into two stages where we allow the meta-controller to take 5
steps of Net2Deeper action and 4 steps of Net2Wider action
in the ﬁrst stage. After 300 networks are sampled, we take
the network which performs best currently and train it with
a longer period of time (100 epochs) to be used as the start
point for the second stage. Similarly, in the second stage, we
also allow the meta-controller to take 5 steps of Net2Deeper
action and 4 steps of Net2Wider action and stop exploration
after 150 networks are sampled.

The progress of the two stages architecture search is
shown in Figure 4, where we can ﬁnd that EAS gradu-
ally learns to pick high performance architectures at each
stage. As EAS takes function-preserving transformations to
explore the architecture space, we can also ﬁnd that the
sampled architectures consistently perform better than the
start point network at each stage. Thus it is usually “safe”
to explore the architecture space with EAS. We take the
top networks discovered during the second stage and fur-
ther train the networks with 300 epochs using the full train-
ing set. Finally, the best model achieves 95.11% test ac-
curacy (i.e. 4.89% test error rate). Furthermore, to justify
the transferability of the discovered networks, we train the
top architecture (95.11% test accuracy) on SVHN from ran-
dom initialization with 40 epochs using the full training
set and achieves 98.17% test accuracy (i.e. 1.83% test er-
ror rate), better than both human-designed and automatically
designed architectures that are in the plain CNN architecture
space (see Table 2).

We would like to emphasize that the required computa-
tional resources to achieve this result is much smaller than
those required in (Zoph and Le 2017; Real et al. 2017).
Speciﬁcally, it takes less than 2 days on 5 GeForce GTX
1080 GPUs with totally 450 networks trained to achieve
4.89% test error rate on C10+ starting from a small network.

Further Explore Larger Architecture Space To further
search better architectures in the plain CNN architecture

Table 2: Test error rate (%) comparison with CNNs that use
convolutional, fully-connected and pooling layers alone.

human
designed

auto
designed

Model
Maxout (Goodfellow et al. 2013)
NIN (Lin, Chen, and Yan 2013)
All-CNN (Springenberg et al. 2014)
VGGnet (Simonyan and Zisserman 2015)
MetaQNN (Baker et al. 2017) (depth=7)
MetaQNN (Baker et al. 2017) (ensemble)
EAS (plain CNN, depth=16)
EAS (plain CNN, depth=20)

C10+
9.38
8.81
7.25
7.25
6.92
-
4.89
4.23

SVHN
2.47
2.35
-
-
-
2.06
1.83
1.73

space, in the second experiment, we use the top architec-
tures discovered in the ﬁrst experiment, as the start points
to explore a larger architecture space on C10+ and SVHN.
This experiment on each dataset takes around 2 days on 5
GPUs.

The summarized results of comparing with human-
designed and automatically designed architectures that use
a similar design scheme (plain CNN), are reported in Table
2, where we can ﬁnd that the top model designed by EAS
on the plain CNN architecture space outperforms all similar
models by a large margin. Speciﬁcally, comparing to human-
designed models, the test error rate drops from 7.25% to
4.23% on C10+ and from 2.35% to 1.73% on SVHN. While
comparing to MetaQNN, the Q-learning based automatic ar-
chitecture designing method, EAS achieves a relative test er-
ror rate reduction of 38.9% on C10+ and 16.0% on SVHN.
We also notice that the best model designed by MetaQNN
on C10+ only has a depth of 7, though the maximum is set
to be 18 in the original paper (Baker et al. 2017). We sup-
pose maybe they trained each designed network from scratch
and used an aggressive training strategy to accelerate train-
ing, which resulted in many networks under performed, es-
pecially for deep networks. Since we reuse the weights of
pre-existing networks, the deep networks are validated more
accurately in EAS, and we can thus design deeper and more
accurate networks than MetaQNN.

We also report the comparison with state-of-the-art ar-
chitectures that use advanced techniques such as skip-
connections, branching etc., on C10+ in Table 3. Though it
is not a fair comparison since we do not incorporate such
advanced techniques into the search space in this experi-
ment, we still ﬁnd that the top model designed by EAS is
highly competitive even comparing to these state-of-the-art
modern architectures. Speciﬁcally, the 20-layers plain CNN
with 23.4M parameters outperforms ResNet, its stochas-
tic depth variant and its pre-activation variant. It also ap-
proaches the best result given by DenseNet. When com-
paring to automatic architecture designing methods that in-

Table 3: Test error rate (%) comparison with state-of-the-art architectures.

Model
ResNet (He et al. 2016a)
ResNet (stochastic depth) (Huang et al. 2017)
Wide ResNet (Zagoruyko and Komodakis 2016)
Wide ResNet (Zagoruyko and Komodakis 2016)
ResNet (pre-activation) (He et al. 2016b)
DenseNet (L = 40, k = 12) (Huang et al. 2017)
DenseNet-BC (L = 100, k = 12) (Huang et al. 2017)
DenseNet-BC (L = 190, k = 40) (Huang et al. 2017)
Large-Scale Evolution (250 GPUs)(Real et al. 2017)
NAS (predicting strides, 800 GPUs) (Zoph and Le 2017)
NAS (max pooling, 800 GPUs) (Zoph and Le 2017)
NAS (post-processing, 800 GPUs) (Zoph and Le 2017)
EAS (plain CNN, 5 GPUs)

Depth
110
1202
16
28
1001
40
100
190
-
20
39
39
20

Params C10+
6.61
1.7M
4.91
10.2M
4.81
11.0M
4.17
36.5M
4.62
10.2M
5.24
1.0M
4.51
0.8M
3.46
25.6M
5.40
5.4M
6.01
2.5M
4.47
7.1M
3.65
37.4M
4.23
23.4M

human
designed

auto
designed

action. The result is reported in Figure 5, which shows that
the RL based meta-controller can effectively focus on the
right search direction, while the random search cannot (left
plot), and thus ﬁnd high performance architectures more ef-
ﬁciently than random search.

Explore DenseNet Architecture Space
We also apply EAS to explore the DenseNet architecture
space. We use the DenseNet-BC (L = 40, k = 40) as
the start point. The growth rate, i.e. the width of the non-
bottleneck layer is chosen from {40, 44, 48, 52, 56, 60, 64},
and the result is reported in Table 4. We ﬁnd that by ap-
plying EAS to explore the DenseNet architecture space, we
achieve a test error rate of 4.66% on C10, better than the
best result, i.e. 5.19% given by the original DenseNet while
having 43.79% less parameters. On C10+, we achieve a test
error rate of 3.44%, also outperforming the best result, i.e.
3.46% given by the original DenseNet while having 58.20%
less parameters.

Conclusion
In this paper, we presented EAS, a new framework to-
ward economical and efﬁcient architecture search, where
the meta-controller is implemented as a RL agent. It learns
to take actions for network transformation to explore the
architecture space. By starting from an existing network
and reusing its weights via the class of function-preserving
transformation operations, EAS is able to utilize knowledge
stored in previously trained networks and take advantage
of the existing successful architectures in the target task to
explore the architecture space efﬁciently. Our experiments
have demonstrated EAS’s outstanding performance and ef-
ﬁciency compared with several strong baselines. For future
work, we would like to explore more network transforma-
tion operations and apply EAS for different purposes such
as searching networks that not only have high accuracy but
also keep a balance between the size and the performance.

Acknowledgments
This research was sponsored by Huawei Innovation Re-
search Program, NSFC (61702327) and Shanghai Sailing
Program (17YF1428200).

Figure 5: Comparison between RL based meta-controller
and random search on C10+.

Table 4: Test error rate (%) results of exploring DenseNet
architecture space with EAS.

Model
DenseNet (L = 100, k = 24)
DenseNet-BC (L = 250, k = 24)
DenseNet-BC (L = 190, k = 40)
NAS (post-processing)
EAS (DenseNet on C10)
EAS (DenseNet on C10+)

Depth
100
250
190
39
70
76

Params C10 C10+
3.74
27.2M 5.83
3.62
15.3M 5.19
3.46
25.6M
3.65
37.4M
8.6M 4.66
-
3.44
10.7M

-
-

-

corporate skip-connections into their search space, our 20-
layers plain model beats most of them except NAS with
post-processing, that is much deeper and has more param-
eters than our model. Moreover, we only use 5 GPUs and
train hundreds of networks while they use 800 GPUs and
train tens of thousands of networks.

Comparison Between RL and Random Search Our
framework is not restricted to use the RL based meta-
controller. Beside RL, one can also take network transfor-
mation actions to explore the architecture space by random
search, which can be effective in some cases (Bergstra and
Bengio 2012). In this experiment, we compare the perfor-
mances of the RL based meta-controller and the random
search meta-controller in the architecture space that is used
in the above experiments. Speciﬁcally, we use the network
in Table 1 as the start point and let the meta-controller to
take 5 steps of Net2Deeper action and 4 steps of Net2Wider

[Netzer et al. 2011] Netzer, Y.; Wang, T.; Coates, A.; Bissacco, A.;
Wu, B.; and Ng, A. Y. 2011. Reading digits in natural images with
unsupervised feature learning. In NIPS workshop on deep learning
and unsupervised feature learning.

[Real et al. 2017] Real, E.; Moore, S.; Selle, A.; Saxena, S.; Sue-
matsu, Y. L.; Le, Q.; and Kurakin, A. 2017. Large-scale evolution
of image classiﬁers. ICML.

[Schulman et al. 2015] Schulman, J.; Levine, S.; Abbeel, P.; Jordan,
M. I.; and Moritz, P. 2015. Trust region policy optimization. In
ICML.

[Schuster and Paliwal 1997] Schuster, M., and Paliwal, K. K. 1997.
Bidirectional recurrent neural networks. IEEE Transactions on Sig-
nal Processing.

[Silver et al. 2016] Silver, D.; Huang, A.; Maddison, C. J.; Guez,
A.; Sifre, L.; Van Den Driessche, G.; Schrittwieser, J.; Antonoglou,
I.; Panneershelvam, V.; Lanctot, M.; et al. 2016. Mastering the
game of go with deep neural networks and tree search. Nature.
[Simonyan and Zisserman 2015] Simonyan, K., and Zisserman, A.
2015. Very deep convolutional networks for large-scale image
recognition. ICLR.

[Snoek, Larochelle, and Adams 2012] Snoek, J.; Larochelle, H.;
and Adams, R. P. 2012. Practical bayesian optimization of ma-
chine learning algorithms. In NIPS.

[Springenberg et al. 2014] Springenberg, J. T.; Dosovitskiy, A.;
Brox, T.; and Riedmiller, M. 2014. Striving for simplicity: The
all convolutional net. arXiv preprint arXiv:1412.6806.

[Stanley and Miikkulainen 2002] Stanley, K. O., and Miikkulainen,
R. 2002. Evolving neural networks through augmenting topolo-
gies. Evolutionary computation.

[Sutskever et al. 2013] Sutskever, I.; Martens, J.; Dahl, G.; and Hin-
ton, G. 2013. On the importance of initialization and momentum
in deep learning. In ICML.

[Sutskever, Vinyals, and Le 2014] Sutskever, I.; Vinyals, O.; and
Le, Q. V. 2014. Sequence to sequence learning with neural net-
works. In NIPS.

[Sutton and Barto 1998] Sutton, R. S., and Barto, A. G. 1998. Re-

inforcement learning: An introduction. MIT press Cambridge.

[Williams 1992] Williams, R. J. 1992. Simple statistical gradient-
learning.

following algorithms for connectionist reinforcement
Machine learning.

[Zagoruyko and Komodakis 2016] Zagoruyko, S., and Komodakis,
arXiv preprint
Wide residual networks.

N.
2016.
arXiv:1605.07146.

[Zoph and Le 2017] Zoph, B., and Le, Q. V. 2017. Neural architec-

ture search with reinforcement learning. ICLR.

References
[Bahdanau, Cho, and Bengio 2014] Bahdanau, D.; Cho, K.; and
Bengio, Y. 2014. Neural machine translation by jointly learning to
align and translate. ICLR.

[Baker et al. 2017] Baker, B.; Gupta, O.; Naik, N.; and Raskar, R.
2017. Designing neural network architectures using reinforcement
learning. ICLR.

[Bergstra and Bengio 2012] Bergstra, J., and Bengio, Y. 2012. Ran-

dom search for hyper-parameter optimization. JMLR.

[Cai et al. 2017] Cai, H.; Ren, K.; Zhang, W.; Malialis, K.; Wang,
J.; Yu, Y.; and Guo, D. 2017. Real-time bidding by reinforcement
learning in display advertising. In WSDM.

[Chen, Goodfellow, and Shlens 2015] Chen, T.; Goodfellow, I.; and
Shlens, J. 2015. Net2net: Accelerating learning via knowledge
transfer. ICLR.

[Domhan, Springenberg, and Hutter 2015] Domhan, T.; Springen-
berg, J. T.; and Hutter, F. 2015. Speeding up automatic hyper-
parameter optimization of deep neural networks by extrapolation
of learning curves. In IJCAI.

[Gastaldi 2017] Gastaldi, X. 2017. Shake-shake regularization.

arXiv preprint arXiv:1705.07485.
[Goodfellow et al. 2013] Goodfellow,

J.; Warde-Farley, D.;
Mirza, M.; Courville, A.; and Bengio, Y. 2013. Maxout networks.
ICML.

I.

[Han et al. 2015] Han, S.; Pool, J.; Tran, J.; and Dally, W. 2015.
Learning both weights and connections for efﬁcient neural net-
work. In NIPS.

[He et al. 2016a] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016a.

Deep residual learning for image recognition. In CVPR.

[He et al. 2016b] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016b.

Identity mappings in deep residual networks. In ECCV.

[Huang et al. 2017] Huang, G.; Liu, Z.; Weinberger, K. Q.; and
van der Maaten, L. 2017. Densely connected convolutional net-
works. CVPR.

[Ioffe and Szegedy 2015] Ioffe, S., and Szegedy, C. 2015. Batch
normalization: Accelerating deep network training by reducing in-
ternal covariate shift. ICML.

[Kakade 2002] Kakade, S. 2002. A natural policy gradient. NIPS.
[Kingma and Ba 2015] Kingma, D., and Ba, J. 2015. Adam: A

method for stochastic optimization. ICLR.

[Klein et al. 2017] Klein, A.; Falkner, S.; Springenberg, J. T.; and
Hutter, F. 2017. Learning curve prediction with bayesian neural
networks. ICLR.

[Krizhevsky and Hinton 2009] Krizhevsky, A., and Hinton, G.

2009. Learning multiple layers of features from tiny images.

[Krizhevsky, Sutskever, and Hinton 2012] Krizhevsky,

Sutskever, I.; and Hinton, G. E.
tion with deep convolutional neural networks. In NIPS.

2012.

A.;
Imagenet classiﬁca-

[Lin, Chen, and Yan 2013] Lin, M.; Chen, Q.; and Yan, S. 2013.

Network in network. arXiv preprint arXiv:1312.4400.

[Mendoza et al. 2016] Mendoza, H.; Klein, A.; Feurer, M.; Sprin-
genberg, J. T.; and Hutter, F. 2016. Towards automatically-tuned
neural networks. In Workshop on Automatic Machine Learning.
[Miller, Todd, and Hegde 1989] Miller, G. F.; Todd, P. M.; and
Hegde, S. U. 1989. Designing neural networks using genetic algo-
rithms. In ICGA. Morgan Kaufmann Publishers Inc.

[Negrinho and Gordon 2017] Negrinho, R., and Gordon, G. 2017.
Deeparchitect: Automatically designing and training deep architec-
tures. arXiv preprint arXiv:1704.08792.

