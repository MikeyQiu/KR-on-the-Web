Kernel Adaptive Metropolis-Hastings

4
1
0
2
 
n
u
J
 

2
1
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
2
0
3
5
.
7
0
3
1
:
v
i
X
r
a

Dino Sejdinovic⋆
Heiko Strathmann⋆
Maria Lomeli Garcia⋆
Christophe Andrieu‡
Arthur Gretton⋆
⋆Gatsby Unit, CSML, University College London, UK and ‡School of Mathematics, University of Bristol, UK

DINO@GATSBY.UCL.AC.UK
UCABHST@GATSBY.UCL.AC.UK
MLOMELI@GATSBY.UCL.AC.UK
C.ANDRIEU@BRISTOL.AC.UK
ARTHUR.GRETTON@GMAIL.COM

Abstract

A Kernel Adaptive Metropolis-Hastings algo-
rithm is introduced, for the purpose of sampling
from a target distribution with strongly nonlin-
ear support. The algorithm embeds the trajec-
tory of the Markov chain into a reproducing ker-
nel Hilbert space (RKHS), such that the fea-
ture space covariance of the samples informs
the choice of proposal. The procedure is com-
putationally efﬁcient and straightforward to im-
plement, since the RKHS moves can be inte-
grated out analytically: our proposal distribu-
tion in the original space is a normal distribution
whose mean and covariance depend on where
the current sample lies in the support of the tar-
get distribution, and adapts to its local covari-
ance structure. Furthermore, the procedure re-
quires neither gradients nor any other higher or-
der information about the target, making it par-
ticularly attractive for contexts such as Pseudo-
Marginal MCMC. Kernel Adaptive Metropolis-
Hastings outperforms competing ﬁxed and adap-
tive samplers on multivariate, highly nonlinear
target distributions, arising in both real-world
and synthetic examples.

1. Introduction

The choice of the proposal distribution is known to be
crucial for the design of Metropolis-Hastings algorithms,
and methods for adapting the proposal distribution to in-
crease the sampler’s efﬁciency based on the history of the
Markov chain have been widely studied. These methods
often aim to learn the covariance structure of the target
distribution, and adapt the proposal accordingly. Adaptive
MCMC samplers were ﬁrst studied by Haario et al. (1999;

Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copy-
right 2014 by the author(s).

2001), where the authors propose to update the proposal
distribution along the sampling process. Based on the chain
history, they estimate the covariance of the target distribu-
tion and construct a Gaussian proposal centered at the cur-
rent chain state, with a particular choice of the scaling fac-
tor from Gelman et al. (1996). More sophisticated schemes
are presented by Andrieu & Thoms (2008), e.g., adaptive
scaling, component-wise scaling, and principal component
updates.

While these strategies are beneﬁcial for distributions that
show high anisotropy (e.g., by ensuring the proposal uses
the right scaling in all principal directions), they may still
suffer from low acceptance probability and slow mixing
when the target distributions are strongly nonlinear, and
the directions of large variance depend on the current lo-
cation of the sampler in the support. In the present work,
we develop an adaptive Metropolis-Hastings algorithm in
which samples are mapped to a reproducing kernel Hilbert
space, and the proposal distribution is chosen according to
the covariance in this feature space (Sch¨olkopf et al., 1998;
Smola et al., 2001). Unlike earlier adaptive approaches,
the resulting proposal distributions are locally adaptive in
input space, and oriented towards nearby regions of high
density, rather than simply matching the global covari-
ance structure of the distribution. Our approach com-
bines a move in the feature space with a stochastic step
towards the nearest input space point, where the feature
space move can be analytically integrated out. Thus,
the implementation of the procedure is straightforward:
the proposal is simply a multivariate Gaussian in the in-
put space, with location-dependent covariance which is
informed by the feature space representation of the tar-
get. Furthermore, the resulting Metropolis-Hastings sam-
pler only requires the ability to evaluate the unnormal-
ized density of the target (or its unbiased estimate, as
in Pseudo-Marginal MCMC of Andrieu & Roberts, 2009),
and no gradient evaluation is needed, making it applicable
to situations where more sophisticated schemes based on
Hamiltonian Monte Carlo (HMC) or Metropolis Adjusted

Kernel Adaptive Metropolis Hastings

Langevin Algorithms (MALA) (Roberts & Stramer, 2003;
Girolami & Calderhead, 2011) cannot be applied.

We begin our presentation in Section 2, with a brief
overview of existing adaptive Metropolis approaches; we
also review covariance operators in the RKHS. Based on
these operators, we describe a sampling strategy for Gaus-
sian measures in the RKHS in Section 3, and introduce a
cost function for constructing proposal distributions.
In
Section 4, we outline our main algorithm, termed Kernel
Adaptive Metropolis-Hastings (MCMC Kameleon). We
provide experimental comparisons with other ﬁxed and
adaptive samplers in Section 5, where we show superior
performance in the context of Pseudo-Marginal MCMC
for Bayesian classiﬁcation, and on synthetic target distri-
butions with highly nonlinear shape.

2. Background

= Rd be
Adaptive Metropolis Algorithms. Let
the domain of interest, and denote the unnormalized
target density on
let Σt =
Σt(x0, x1, . . . , xt−1) denote an estimate of the covariance
matrix of the target density based on the chain history
t−1
i=0. The original adaptive Metropolis at the current
xi}
{
state of the chain state xt = y uses the proposal

by π. Additionally,

X

X

qt(

y) =

(y, ν2Σt),

(1)

·|

N
where ν = 2.38/√d is a ﬁxed scaling factor from
Gelman et al. (1996). This choice of scaling factor was
shown to be optimal (in terms of efﬁciency measures) for
the usual Metropolis algorithm. While this optimality re-
sult does not hold for Adaptive Metropolis, it can never-
theless be used as a heuristic. Alternatively, the scale ν
can also be adapted at each step as in Andrieu & Thoms
(2008, Algorithm 4) to obtain the acceptance rate from
Gelman et al. (1996), a∗ = 0.234.

X

X × X →

Embeddings

and Covariance Opera-
RKHS
tors. According to the Moore-Aronszajn theorem
(Berlinet & Thomas-Agnan, 2004, p. 19), for every sym-
R,
metric, positive deﬁnite function (kernel) k :
there is an associated reproducing kernel Hilbert space
with reproducing kernel
Hk of real-valued functions on
k. The map ϕ :
, x) is called
X → Hk, ϕ : x
the canonical feature map of k. This feature map or
embedding of a single point can be extended to that of
its kernel embedding is
a probability measure P on
an element µP ∈ Hk, given by µP =
, x) dP (x)
k(
·
(Berlinet & Thomas-Agnan, 2004; Fukumizu et al., 2004;
Smola et al., 2007). If a measurable kernel k is bounded,
it is straightforward to show using the Riesz represen-
tation theorem that the mean embedding µk(P ) exists
. For many interesting
for all probability measures on

k(
·

7→

X

´

:

X

7→

bounded kernels k, including the Gaussian, Laplacian and
inverse multi-quadratics, the kernel embedding P
µP
is injective. Such kernels are said to be characteristic
(Sriperumbudur et al., 2010; 2011), since each distribution
is uniquely characterized by its embedding (in the same
way that every probability distribution has a unique
characteristic function). The kernel embedding µP is the
representer of expectations of smooth functions w.r.t. P ,
i.e.,
f (x)dP (x). Given samples
∈ Hk,
f
f, µP iHk
h
∀
n
z =
P , the embedding of the empirical measure
zi}
i=1 ∼
{
n
is µz = 1
, zi).
i=1 k(
n
·
Hk → Hk for a
Next, the covariance operator CP :
probability measure P is given by CP =
k(
⊗
·
´
µP (Baker, 1973; Fukumizu et al.,
k(
µP ⊗
·
∈ Hk the tensor product is de-
2004), where for a, b, c
a. The covariance oper-
ﬁned as (a
b, c
b)c =
⊗
ator has the property that
=
EP f EP g.
EP (f g)

f, CP g
h

∈ Hk,

iHk
f, g
∀

, x) dP (x)

iHk

, x)

P

=

−

´

h

−

1
n

⊗

⊗

µz

P

, zi)

, zi)

k(
·

n
i=1 k(
·

Our approach is based on the idea that the nonlinear sup-
port of a target density may be learned using Kernel Princi-
pal Component Analysis (Kernel PCA) (Sch¨olkopf et al.,
1998; Smola et al., 2001),
this being linear PCA on
the empirical covariance operator in the RKHS, Cz =
1
µz, computed on the
n
−
sample z deﬁned above. The empirical covariance op-
erator behaves as expected: applying the tensor prod-
uct deﬁnition gives
n
1
i=1 f (zi)
n

= 1
n
−
. By analogy with algo-
rithms which use linear PCA directions to inform M-H pro-
(cid:0)
posals (Andrieu & Thoms, 2008, Algorithm 8), nonlinear
PCA directions can be encoded in the proposal construc-
tion, as described in Appendix C. Alternatively, one can
focus on a Gaussian measure on the RKHS determined by
the empirical covariance operator Cz rather than extract-
ing its eigendirections, which is the approach we pursue in
this contribution. This generalizes the proposal (1), which
considers the Gaussian measure induced by the empirical
covariance matrix on the original space.

f, Czg
iHk
h
n
i=1 g(zi)

n
i=1 f (zi)g(zi)

(cid:1) (cid:0)

P

P

P

(cid:1)

3. Sampling in RKHS

We next describe the proposal distribution at iteration t of
the MCMC chain. We will assume that a subset of the chain
n
history, denoted z =
1, is available. Our
i=1, n
proposal is constructed by ﬁrst considering the samples in
the RKHS associated to the empirical covariance operator,
and then performing a gradient descent step on a cost func-
tion associated with those samples.

zi}
{

≤

−

t

Gaussian Measure of the Covariance Operator. We
will work with the Gaussian measure on the RKHS
Hk
n
with mean k(
zi}
i=1
·
is the subset of the chain history. While there is no analogue

, y) and covariance ν2Cz, where z =

{

Kernel Adaptive Metropolis Hastings

Samples {zi}200
i=1

Current position y

βiβj (k(
·

, zi)

−

µz)

(k(
·

⊗

, zj)

−

µz)

Figure1. Heatmaps (white denotes large) and gradients of g(x)
for two samples of β and ﬁxed z.

f

(cid:16)

−

−

1
2ν2

k(
·

N
k(
·

, y), C−1

, y), ν2Cz)

(f ; k(
·
Hk

of a Lebesgue measure in an inﬁnite dimensional RKHS, it
is instructive (albeit with some abuse of notation) to denote
this measure in the “density form”

∝
. As Cz is
z (f
exp
a ﬁnite-rank operator, this measure is supported only on a
ﬁnite-dimensional afﬁne space k(
z =
, y) +
·
n
span
i=1 is the subspace spanned by the canonical
features of z. It can be shown that a sample from this mea-
sure has the form f = k(
µz] ,
, y) +
·
n I) is isotropic. Indeed, to see that f
where β
has the correct covariance structure, note that:

(cid:17)
(cid:11)
z, where
H

n
i=1 βi [k(
·

, zi)
}

(0, ν2

k(
·

∼ N

, zi)

, y))

P

H

−

−

{

(cid:10)

, y))

(f

k(
·

−

⊗

, y))]

−

k(
·

n

n

E [(f

= E




ν2
n

=

i=1
X
n

j=1
X

i=1
X

, zi)

(k(
·

−

µz)

(k(
·

⊗

, zi)

−

µz) = ν2Cz.





and a Gaussian Process

Due to the equivalence in the RKHS between a
Gaussian measure
(GP)
(Berlinet & Thomas-Agnan, 2004, Ch.
4), we can
think of the RKHS samples f as trajectories of the GP with
mean m(x) = k(x, y) and covariance function
κ(x, x′) = cov [f (x), f (x′)]
ν2
n

µz(x)) (k(x′, zi)

(k(x, zi)

=

µz(x′)) .

n

−

−

i=1
X

The covariance function κ of this GP is therefore the kernel
k convolved with itself with respect to the empirical mea-
sure associated to the samples z, and draws from this GP
therefore lie in a smaller RKHS; see Saitoh (1997, p. 21)
for details.

P

, zi)

, y) +

n
i=1 βi [k(
·

Obtaining Target Samples through Gradient Descent.
We have seen how to obtain the RKHS sample f =
µz] from the Gaussian mea-
k(
−
·
sure in the RKHS. This sample does not in general have a
= Rd;
corresponding pre-image in the original domain
X
i.e., there is no point x∗
, x∗). If
such that f = k(
·
∈ X
there were such a point, then we could use it as a proposal
in the original domain. Therefore, we are ideally looking
, x∗) is
for a point x∗
close to f in the RKHS norm. We consider the optimization
problem

whose canonical feature map k(
·

∈ X

arg min
x∈X

kk (·, x) − f k

2
Hk

=

In general, this is a non-convex minimization problem, and
may be difﬁcult to solve (Bakir et al., 2003). Rather than
solving it for every new vector of coefﬁcients β, which
would lead to an excessive computational burden for ev-
ery proposal made, we simply make a single descent step
along the gradient of the cost function,

g(x) = k(x, x)

2k(x, y)

2

βi [k(x, zi)

µz(x)] ,

−

−

−

n

i=1
X

(2)

i.e., the proposed new point is

x∗ = y

η

∇xg(x)

|x=y + ξ,

−

where η is a gradient step size parameter and ξ

∼
(0, γ2I) is an additional isotropic ’exploration’ term
It will be useful to split the
|x=y =
|x=y −

N
after the gradient step.
scaled gradient at y into two terms as η
Mz,yHβ), where ay =
η (ay −
∇xk(x, y)
2
Mz,y = 2 [

∇xg(x)
∇xk(x, x)

|x=y,

(3)

∇xk(x, z1)

n matrix, and H = I

|x=y, . . . ,
−

∇xk(x, zn)
×

1
n 1n×n is the n

|x=y]
n centering

is a d
×
matrix.

Figure 1 plots g(x) and its gradients for several samples of
β-coefﬁcients, in the case where the underlying z-samples
are from the two-dimensional nonlinear Banana target dis-
tribution of Haario et al. (1999). It can be seen that g may
have multiple local minima, and that it varies most along
the high-density regions of the Banana distribution.

4. MCMC Kameleon Algorithm

4.1. Proposal Distribution

arg min

k(x, x) − 2k(x, y) − 2

βi [k(x, zi) − µz(x)]

.

x∈X (

We now have a recipe to construct a proposal that is able to
adapt to the local covariance structure for the current chain

)

n

i=1
X

Kernel Adaptive Metropolis Hastings

MCMC Kameleon
Input: unnormalized target π, subsample size n, scaling
∞
t=0, kernel k,
parameters ν, γ, adaptation probabilities

pt}

{

At iteration t + 1,

•

1. With probability pt, update a random subsample

min(n,t)
i=1

z =

zi}

xi}
{
2. Sample proposed point x∗ from qz(
·|

of the chain history

{

(xt, γ2I + ν2Mz,xt HM ⊤
(3) and H = I

N
given in Eq.
centering matrix,

t−1
i=0,
xt) =
z,xt), where Mz,xtis
1
n 1n×n is the

−

3. Accept/Reject with the Metropolis-Hastings ac-

ceptance probability A(xt, x∗) in Eq. (4),

xt+1 =

x∗, w.p. A(xt, x∗),
xt, w.p. 1

A(xt, x∗).

(

−

state y. This proposal depends on a subset of the chain
history z, and is denoted by qz(
y). While we will later
simplify this proposal by integrating out the moves in the
RKHS, it is instructive to think of the proposal generating
process as:

·|

1. Sample β
ﬁcients).

∼ N

(0, ν2I) (n

1 normal of RKHS coef-

×

•

This represents an RKHS sample f = k(
·

, y) +
µz] which is the goal of the

n
i=1 βi [k(
·

−
cost function g(x).
P

, zi)

2. Move along the gradient of g:

x∗ = y

−

η

∇xg(x)

|x=y + ξ.

•

This gives a proposal x∗
ηMz,yHβ, γ2I) (d
space).

×

y, β

ηay +
1 normal in the original

∼ N

(y

−

|

2

|x=y −

∇xk(x, x)

Our ﬁrst step in the derivation of the explicit proposal den-
sity is to show that as long as k is a differentiable positive
deﬁnite kernel, the term ay vanishes.
Proposition 1. Let k be a differentiable positive deﬁnite
|x=y = 0.
kernel. Then ay =
Since ay = 0, the gradient step size η always appears to-
gether with β, so we merge η and the scale ν of the β-
coefﬁcients into a single scale parameter, and set η = 1
henceforth. Furthermore, since both p(β) and pz(x∗
y, β)
are multivariate Gaussian densities, the proposal density
qz(x∗
y, β)dβ can be computed analyt-
ically. We therefore get the following closed form expres-
sion for the proposal distribution.

∇xk(x, y)

p(β)pz(x∗

y) =

´

|

|

|

Proposition 2. qz(

y) =

(y, γ2I + ν2Mz,yHM ⊤

z,y).

·|

N

Figure2. 95% contours (red) of proposal distributions evaluated
at a number of points, for the ﬁrst two dimensions of the banana
target of Haario et al. (1999). Underneath is the density heatmap,
and the samples (blue) used to construct the proposals.

Proofs of the above Propositions are given in Appendix A.

With the derived proposal distribution, we proceed with the
standard Metropolis-Hastings accept/reject scheme, where
the proposed sample x∗ is accepted with probability

A(xt, x∗) = min

1,

π(x∗)qz(xt|
π(xt)qz(x∗
|

x∗)
xt)

,

(4)

(cid:27)

(cid:26)
giving rise to the MCMC Kameleon Algorithm. Note that
each π(x∗) and π(xt) could be replaced by their unbi-
ased estimates without impacting the invariant distribution
(Andrieu & Roberts, 2009).
The constructed family of proposals encodes local struc-
ture of the target distribution, which is learned based on the
subsample z. Figure 2 depicts the regions that contain 95%
y) at various
of the mass of the proposal distribution qz(
states y for a ﬁxed subsample z, where the Banana target
is used (details in Section 5). More examples of proposal
contours can be found in Appendix B.

·|

4.2. Properties of the Algorithm

·|

and convergence. MCMC
The update
schedule
n
Kameleon requires a subsample z =
i=1 at each
zi}
{
iteration of the algorithm, and the proposal distribution
y) is updated each time a new subsample z is obtained.
qz(
It is well known that a chain which keeps adapting the
proposal distribution need not converge to the correct
target (Andrieu & Thoms, 2008). To guarantee conver-
∞
gence, we introduce adaptation probabilities
t=0,
pt}
{
, and at iteration
such that pt →
t we update the subsample z with probability pt. As
adaptations occur with decreasing probability, Theorem 1
of Roberts & Rosenthal (2007) implies that the resulting
algorithm is ergodic and converges to the correct target.
Another straightforward way to guarantee convergence is
n
to ﬁx the set z =
i=1 after a “burn-in” phase; i.e., to
stop adapting Roberts & Rosenthal (2007, Proposition 2).
In this case, a “burn-in” phase is used to get a rough sketch
of the shape of the distribution: the initial samples need not

∞
t=1 pt =

zi}
{

0 and

P

∞

Kernel Adaptive Metropolis Hastings

come from a converged or even valid MCMC chain, and it
sufﬁces to have a scheme with good exploratory properties,
e.g., Welling & Teh (2011).
In MCMC Kameleon, the
term γ allows exploration in the initial iterations of the
chain (while the subsample z is still not informative about
the structure of the target) and provides regularization of
the proposal covariance in cases where it might become
ill-conditioned. Intuitively, a good approach to setting γ is
to slowly decrease it with each adaptation, such that the
learned covariance progressively dominates the proposal.

In Haario et al. (2001), the
Symmetry of the proposal.
proposal distribution is asymptotically symmetric due to
the vanishing adaptation property. Therefore, the authors
compute the standard Metropolis acceptance probability. In
our case, the proposal distribution is a Gaussian with mean
at the current state of the chain xt = y and covariance
γ2I + ν2Mz,yHM ⊤
z,y, where Mz,y depends both on the
n
current state y and a random subsample z =
i=1 of the
t−1
chain history
i=0. This proposal distribution is never
symmetric (as covariance of the proposal always depends
on the current state of the chain), and therefore we use the
Metropolis-Hastings acceptance probability to reﬂect this.

xi}
{

zi}
{

Relationship to MALA and Manifold MALA. The
Metropolis Adjusted Langevin Algorithm (MALA) algo-
rithm uses information about the gradient of the log-target
density at the current chain state to construct a proposed
point for the Metropolis step. Our approach does not re-
quire that the log-target density gradient be available or
computable. Kernel gradients in the matrix Mz,y are easily
obtained for commonly used kernels, including the Gaus-
sian kernel (see section 4.3), for which the computational
complexity is equal to evaluating the kernel itself. More-
over, while standard MALA simply shifts the mean of the
proposal distribution along the gradient and then adds an
isotropic exploration term, our proposal is centered at the
current state, and it is the covariance structure of the pro-
posal distribution that coerces the proposed points to be-
It would
long to the high-density regions of the target.
be straightforward to modify our approach to include a
drift term along the gradient of the log-density, should
such information be available, but it is unclear whether
this would provide additional performance gains. Fur-
ther work is required to elucidate possible connections be-
tween our approach and the use of a preconditioning ma-
trix (Roberts & Stramer, 2003) in the MALA proposal; i.e.,
where the exploration term is scaled with appropriate met-
ric tensor information, as in Riemannian manifold MALA
(Girolami & Calderhead, 2011).

4.3. Examples of Covariance Structure for Standard

Kernels

The proposal distributions in MCMC Kameleon are depen-
dant on the choice of the kernel k. To gain intuition re-

garding their covariance structure, we give two examples
below.

a

In

of
linear
obtain Mz,y
= 2Z⊤,

ker-
Linear kernel.
the
case
nel k(x, x′)
x⊤x′, we
=
=
∇xx⊤z1|x=y, . . . ,
∇xx⊤zn|x=y
so the
2
(y, γ2I + 4ν2Z⊤HZ);
proposal is given by qz(
y) =
(cid:3)
(cid:2)
N
thus, the proposal simply uses the scaled empirical co-
variance Z⊤HZ just like standard Adaptive Metropolis
(Haario et al., 1999), with an additional isotropic explo-
ration component, and depends on y only through the
mean.

·|

Gaussian kernel.
k(x, x′) = exp

′

In the case of a Gaussian kernel
∇xk(x, x′) =

x−x
k
2σ2

, since

−

k

2

2

(cid:18)

(cid:19)

1

x), we obtain

σ2 k(x, x′)(x′
2
σ2 [k(y, z1)(z1 −

Mz,y =

−

y), . . . , k(y, zn)(zn −

y)] .

Consider how this encodes the covariance structure of the
target distribution:

Rij = γ

δij

2

+

−

4ν 2(n − 1)
σ4n

n

a=1
X

4ν 2
σ4n

a6=b
X

[k(y, za)]

(za,i − yi)(za,j − yj)

2

k(y, za)k(y, zb)(za,i − yi)(zb,j − yj). (5)

As the ﬁrst two terms dominate, the previous points za
which are close to the current state y (for which k(y, za)
is large) have larger weights, and thus they have more in-
ﬂuence in determining the covariance of the proposal at y.

′

′

ϑ

k

k

(cid:18)

k2

Γ(ϑ)

Kϑ

x−x
ρ

x−x
ρ

In the Mat´ern family of kernels

Mat´ern kernel.
kϑ,ρ(x, x′) = 21−ϑ
Kϑ is the modiﬁed Bessel function of the second kind, we
obtain a form of the covariance structure very similar to
∇xkϑ,ρ(x, x′) =
that of the Gaussain kernel. In this case,
2ρ2(ϑ−1) kϑ−1,ρ(x, x′)(x′
x), so the only difference (apart
from the scalings) to (5) is that the weights are now deter-
mined by a “rougher” kernel kϑ−1,ρ of the same family.

, where

k2

−

(cid:19)

(cid:19)

(cid:18)

1

5. Experiments

·|

N

y) =

In the experiments, we compare the following samplers:
(SM) Standard Metropolis with the isotropic proposal
(y, ν2I) and scaling ν = 2.38/√d, (AM-
q(
FS) Adaptive Metropolis with a learned covariance ma-
trix and ﬁxed scaling ν = 2.38/√d, (AM-LS) Adaptive
Metropolis with a learned covariance matrix and scaling
learned to bring the acceptance rate close to α∗ = 0.234 as
described in Andrieu & Thoms (2008, Algorithm 4), and
(KAMH-LS) MCMC Kameleon with the scaling ν learned

Kernel Adaptive Metropolis Hastings

0

−1

−2

−3

−4

7
θ

−5

−6

−5

−4

−2

−1

0

−3

θ2

Figure3. Dimensions 2 and 7 of the marginal hyperparameter
posterior on the UCI Glass dataset

in the same fashion (γ was ﬁxed to 0.2), and which also
stops adapting the proposal after the burn-in of the chain
(in all experiments, we use a random subsample z of size
n = 1000, and a Gaussian kernel with bandwidth se-
lected according to the median heuristic). We consider the
following nonlinear targets: (1) the posterior distribution
of Gaussian Process (GP) classiﬁcation hyperparameters
(Filippone & Girolami, 2014) on the UCI glass dataset, and
(2) the synthetic banana-shaped distribution of Haario et al.
(1999) and a ﬂower-shaped disribution concentrated on a
circle with a periodic perturbation.

5.1. Pseudo-Marginal MCMC for GP Classiﬁcation

In the ﬁrst experiment, we illustrate usefulness of the
MCMC Kameleon sampler in the context of Bayesian clas-
siﬁcation with GPs (Williams & Barber, 1998). Consider
the joint distribution of latent variables f , labels y (with
covariate matrix X), and hyperparameters θ, given by

p(f , y, θ) = p(θ)p(f

θ)p(y

f ),

|

|

′

1

θ

}

(cid:16)

−

P

(0,

1, 1

∼ N

D
d=1

fi) =

θ) = exp

(xi,d−x
ℓ2
d

Kθ), with

i.e.,
1−exp(−yifi) where yi ∈ {−

where f
Kθ modeling the covariance
|
between latent variables evaluated at the input covariates:
j,d)2
1
Kθ)ij = κ(xi, x′
(
j|
2
and θd = log ℓ2
d. We restrict our attention to the bi-
(cid:17)
the likelihood is given by
nary logistic classiﬁer;
. We pursue
p(yi|
a fully Bayesian treatment, and estimate the posterior of
the hyperparameters θ. As observed by Murray & Adams
(2012), a Gibbs sampler on p(θ, f
y), which samples from
p(f
f , y)
is extremely sharp, drastically limiting the amount that
f , y. On the other hand,
any Markov chain can update θ
if we directly consider the marginal posterior p(θ
∝
p(y
θ)p(θ) of the hyperparameters, a much less peaked
distribution can be obtained. However, the marginal like-
lihood p(y
θ) is intractable for non-Gaussian likelihoods
p(y
f ), so it is not possible to analytically integrate out
the latent variables. Recently developed pseudo-marginal
MCMC methods (Andrieu & Roberts, 2009) enable exact

f , y) in turn, is problematic, as p(θ

θ, y) and p(θ

y)

|

|

|

|

|

|

|

|

|

inference on this problem (Filippone & Girolami, 2014),
by replacing p(y

θ) with an unbiased estimate

|

(6)

nimp

ˆp(y

θ) :=

|

1
nimp

p(y

f (i))

|

p(f (i)
q(f (i)

,

θ)
|
θ)
|

|

|

|

|

(cid:8)

∝

q(f

f (i)

y, θ)

f )p(f

nimp
i=1 ∼

i=1
X
where
θ) are nimp importance sam-
ples. In Filippone & Girolami (2014), the importance dis-
(cid:9)
tribution q(f
θ) is chosen as the Laplacian or as the Ex-
|
pectation Propagation (EP) approximation of p(f
p(y
θ), leading to state-of-the-art results.
We consider the UCI Glass dataset (Bache & Lichman,
2013), where classiﬁcation of window against non-window
glass is sought. Due to the heterogeneous structure of each
of the classes (i.e., non-window glass consists of contain-
ers, tableware and headlamps), there is no single consistent
set of lengthscales determining the decision boundary, so
one expects the posterior of the covariance bandwidths θd
to have a complicated (nonlinear) shape. This is illustrated
by the plot of the posterior projections to the dimensions
2 and 7 (out of 9) in Figure 3. Since the ground truth
for the hyperparameter posterior is not available, we ini-
tially ran 30 Standard Metropolis chains for 500,000 itera-
tions (with a 100,000 burn-in), kept every 1000-th sample
in each of the chains, and combined them. The resulting
samples were used as a benchmark, to evaluate the perfor-
mance of shorter single-chain runs of SM, AM-FS, AM-
LS and KAMH-LS. Each of these algorithms was run for
100,000 iterations (with a 20,000 burnin) and every 20-th
sample was kept. Two metrics were used in evaluating the
performance of the four samplers, relative to the large-scale
was computed
benchmark. First, the distance
ˆµθ −
between the mean ˆµθ estimated from each of the four sam-
(cid:13)
pler outputs, and the mean µb
θ on the benchmark sample
(cid:13)
(Fig. 4, left), as a function of sample size. Second, the
MMD (Borgwardt et al., 2006; Gretton et al., 2007) was
computed between each sampler output and the bench-
)3;
mark sample, using the polynomial kernel (1 +
i
i.e., the comparison was made in terms of all mixed mo-
ments of order up to 3 (Fig. 4, right). The ﬁgures indicate
that KAMH-LS approximates the benchmark sample bet-
ter than the competing approaches, where the effect is es-
pecially pronounced in the high order moments, indicating
that KAMH-LS thoroughly explores the distribution sup-
port in a relatively small number of samples.
We emphasise that, as for any pseudo-marginal MCMC
scheme, neither the likelihood itself, nor any higher-
the marginal posterior target
order information about
y), are available.
This makes HMC or MALA
p(θ
based approaches such as (Roberts & Stramer, 2003;
Girolami & Calderhead, 2011) unsuitable for this problem,
so it is very difﬁcult to deal with strongly nonlinear pos-
terior targets.
In contrast, as indicated in this example,
the MCMC Kameleon scheme is able to effectively sample

θ, θ′
h

µb
θ

(cid:13)
(cid:13)

2

|

Kernel Adaptive Metropolis Hastings

KAMH-LS
AM-LS
AM-FS
SM

2

bθ
µ
−

θ
ˆµ

e
c
n
a
t
s
i
d
n
a
e
m

0.50

0.45

(cid:13)(cid:13)
0.40

0.35
(cid:13)(cid:13)

0.30

0.25

0.20

0.15

0.10

0.5
0.4
0.3
0.2
0.1
0.0

1.0

0.8

0.6

0.4

0.2

0.0

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

KAMH-LS
AM-LS
AM-FS
SM

e
l
p
m
a
s
k
r
a
m
h
c
n
e
b

e
h
t

m
o
r
f

D
M
M

45

40

35

30

25

20

15

10

5

B

B

1000

2000

3000

4000

5000

1000

2000

3000

4000

5000

number of samples

number of samples

Figure4. The comparison of SM, AM-FS, AM-LS and KAMH-LS in terms of the distance between the estimated mean and the mean
on the benchmark sample (left) and in terms of the maximum mean discrepancy to the benchmark sample (right). The results are
averaged over 30 chains for each sampler. Error bars represent 80%-conﬁdence intervals.

Moderately twisted 8-dimensional

(0.03, 100) target; iterations: 40000, burn-in: 20000

SM

AM-FS

AM-LS

KAMH-LS

Accept ∈ [0, 1]

||ˆE[X]||/10

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Strongly twisted 8-dimensional

(0.1, 100) target; iterations: 80000, burn-in: 40000

SM

AM-FS

AM-LS

KAMH-LS

Accept ∈ [0, 1]

||ˆE[X]||/10

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

8-dimensional

(10, 6, 6, 1) target; iterations: 120000, burn-in: 60000

F

SM

AM-FS

AM-LS

KAMH-LS

Accept ∈ [0, 1]

||ˆE[X]||/10

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Figure5. Results for three nonlinear targets, averaged over 20 chains for each sampler. Accept is the acceptance rate scaled to the
interval [0, 1]. The norm of the mean ||ˆE[X]|| is scaled by 1/10 to ﬁt into the ﬁgure scalling, and the bars over the 0.1, . . . , 0.9-quantiles
represent the deviation from the exact quantiles, scaled by 10; i.e., 0.1 corresponds to 1% deviation. Error bars represent 80%-conﬁdence
intervals.

Kernel Adaptive Metropolis Hastings

from such nonlinear targets, and outperforms the vanilla
Metropolis methods, which are the only competing choices
in the pseudo-marginal context.

In addition, since the bulk of the cost for pseudo-marginal
MCMC is in importance sampling in order to obtain the
acceptance ratio, the additional cost imposed by KAMH-
LS is negligible. Indeed, we observed that there is an in-
crease of only 2-3% in terms of effective computation time
in comparison to all other samplers, for the chosen size of
the chain history subsample (n = 1000).

5.2. Synthetic examples

∼ N

In Haario et al. (1999),

(0, Σ) be a multivariate normal in d

the following
Banana target.
family of nonlinear target distributions is considered. Let
2 dimen-
X
sions, with Σ = diag(v, 1, . . . , 1), which undergoes the
transformation X
v),
1 −
(b, v). It is
and Yi = Xi for i
clear that EY = 0, and that

Y , where Y2 = X2 + b(X 2

→
= 2. We will write Y

≥

(y; b, v) =

(y1; 0, v)

B

N

(y2; b(y2

v), 1)

1−

N

(yj; 0, 1).

∼ B
d

N

j=3
Y

Flower target. The second target distribution we con-
sider is the d-dimensional ﬂower target
(r0, A, ω, σ),
with

F

F(x; r0, A, ω, σ) =

1 + x2
x2

2 − r0 − A cos (ωatan2 (x2, x1))

exp

−

 

2σ2

p
d

j=3
Y

×

N (xj; 0, 1).

This distribution concentrates around the r0-circle with a
periodic perturbation (with amplitude A and frequency ω)
in the ﬁrst two dimensions.

In these examples, exact quantile regions of the targets
can be computed analytically, so we can directly assess
performance without the need to estimate distribution dis-
tances on the basis of samples (i.e., by estimating MMD to
the benchmark sample). We compute the following mea-
sures of performance (similarly as in Haario et al. (1999);
Andrieu & Thoms (2008)) based on the chain after burn-in:
average acceptance rate, norm of the empirical mean (the
true mean is by construction zero for all targets), and the
deviation of the empirical quantiles from the true quantiles.
We consider 8-dimensional target distributions: the mod-
(0.03, 100) banana target (Figure 5, top)
erately twisted
and the strongly twisted
(0.1, 100) banana target (Figure
5, middle) and
(10, 6, 6, 1) ﬂower target (Figure 5, bot-
tom).

F

B

B

The results show that MCMC Kameleon is superior to the
competing samplers. Since the covariance of the proposal

adapts to the local structure of the target at the current chain
state, as illustrated in Figure 2, MCMC Kameleon does
not suffer from wrongly scaled proposal distributions. The
result is a signiﬁcantly improved quantile performance in
comparison to all competing samplers, as well as a com-
parable or superior norm of the empirical mean. SM has
a signiﬁcantly larger norm of the empirical mean, due to
its purely random walk behavior (e.g., the chain tends to
get stuck in one part of the space, and is not able to traverse
both tails of the banana target equally well). AM with ﬁxed
scale has a low acceptance rate (indicating that the scaling
of the proposal is too large), and even though the norm of
the empirical mean is much closer to the true value, quan-
tile performance of the chain is poor. Even if the estimated
covariance matrix closely resembles the true global covari-
ance matrix of the target, using it to construct proposal dis-
tributions at every state of the chain may not be the best
choice. For example, AM correctly captures scalings along
individual dimensions for the ﬂower target (the norm of its
empirical mean is close to its true value of zero) but fails to
capture local dependence structure. The ﬂower target, due
to its symmetry, has an isotropic covariance in the ﬁrst two
dimensions – even though they are highly dependent. This
leads to a mismatch in the scale of the covariance and the
scale of the target, which concentrates on a thin band in the
joint space. AM-LS has the “correct” acceptance rate, but
the quantile performance is even worse, as the scaling now
becomes too small to traverse high-density regions of the
target.

We have constructed a simple, versatile, adaptive, gradient-
free MCMC sampler that constructs a family of proposal
distributions based on the sample history of the chain.
These proposal distributions automatically conform to the
local covariance structure of the target distribution at the
current chain state.
In experiments, the sampler outper-
forms existing approaches on nonlinear target distributions,
both by exploring the entire support of these distributions,
and by returning accurate empirical quantiles, indicating
faster mixing. Possible extensions include incorporating
additional parametric information about the target densi-
ties, and exploring the tradeoff between the degree of sub-
sampling of the chain history and convergence of the sam-
pler.

Software. Python
MCMC
https://github.com/karlnapf/kameleon-mcmc.

implementation

Kameleon

available

of
at

is

Acknowledgments. D.S., H.S., M.L.G. and A.G. ac-
knowledge support of the Gatsby Charitable Foundation.
We thank Mark Girolami for insightful discussions and the
anonymous reviewers for useful comments.

!

6. Conclusions

Kernel Adaptive Metropolis Hastings

Saitoh, S. Integral transforms, reproducing kernels, and their ap-
plications. Pitman Research Notes in Mathematics 369, Long-
man Scientiﬁc & Techn., 1997.

Sch¨olkopf, B., Smola, A. J., and M¨uller, K.-R. Nonlinear compo-
nent analysis as a kernel eigenvalue problem. Neural Comput.,
10:1299–1319, 1998.

Smola, A., Gretton, A., Song, L., and Sch¨olkopf, B. A Hilbert
space embedding for distributions. In Proceedings of the Con-
ference on Algorithmic Learning Theory (ALT), pp. 13–31.
Springer, 2007.

Smola, A. J., Mika, S., Sch¨olkopf, B., and Williamson, R. C. Reg-
ularized principal manifolds. J. Mach. Learn. Res., 1:179–209,
2001.

Sriperumbudur, B., Gretton, A., Fukumizu, K., Lanckriet, G., and
Sch¨olkopf, B. Hilbert space embeddings and metrics on prob-
ability measures. J. Mach. Learn. Res., 11:1517–1561, 2010.

Sriperumbudur, B., Fukumizu, K., and Lanckriet, G. Universality,
characteristic kernels and RKHS embedding of measures. J.
Mach. Learn. Res., 12:2389–2410, 2011.

Steinwart, I. and Christmann, A.

Support Vector Machines.

Springer, 2008.

Welling, M. and Teh, Y.W. Bayesian learning via stochastic gra-
dient Langevin dynamics.
In Proc. of the 28th International
Conference on Machine Learning (ICML), pp. 681–688, 2011.

Williams, C.K.I. and Barber, D. Bayesian classiﬁcation with
IEEE Transactions on Pattern Analysis

Gaussian processes.
and Machine Intelligence, 20(12):1342–1351, 1998.

References

Andrieu, C. and Roberts, G.O. The pseudo-marginal approach
for efﬁcient Monte Carlo computations. Ann. Statist., 37(2):
697–725, 2009.

Andrieu, C. and Thoms, J. A tutorial on adaptive MCMC. Statis-

tics and Computing, 18(4):343–373, 2008.

Bache, K. and Lichman, M. UCI Machine Learning Repository,

2013. URL http://archive.ics.uci.edu/ml.

Baker, C. Joint measures and cross-covariance operators. Trans-
actions of the American Mathematical Society, 186:273–289,
1973.

Bakir, G., Weston, J., and Sch¨olkopf, B. Learning to ﬁnd pre-
In Advances in Neural Information Processing Sys-

images.
tems 16. MIT Press, 2003.

Berlinet, A. and Thomas-Agnan, C. Reproducing Kernel Hilbert

Spaces in Probability and Statistics. Kluwer, 2004.

Borgwardt, K. M., Gretton, A., Rasch, M. J., Kriegel, H.-P.,
Sch¨olkopf, B., and Smola, A. J. Integrating structured biologi-
cal data by kernel maximum mean discrepancy. Bioinformatics
(ISMB), 22(14):e49–e57, 2006.

Filippone, M. and Girolami, M. Pseudo-marginal Bayesian in-
ference for Gaussian Processes. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2014. doi: TPAMI.2014.
2316530.

Fukumizu, K., Bach, F. R., and Jordan, M. I. Dimensionality re-
duction for supervised learning with reproducing kernel Hilbert
spaces. J. Mach. Learn. Res., 5:73–99, 2004.

Gelman, A., Roberts, G. O., and Gilks, W. R. Efﬁcient Metropo-
In Bayesian statistics, 5 (Alicante, 1994),

lis jumping rules.
Oxford Sci. Publ., pp. 599–607. 1996.

Girolami, M. and Calderhead, B. Riemann manifold Langevin
and Hamiltonian Monte Carlo methods. Journal of the Royal
Statistical Society: Series B, 73(2):123–214, 2011.

Gretton, A., Borgwardt, K., Rasch, M., Sch¨olkopf, B., and Smola,
A. A kernel method for the two-sample problem. In Advances
in Neural Information Processing Systems 19, pp. 513–520,
2007.

Haario, H., Saksman, E., and Tamminen, J. Adaptive Proposal
Distribution for Random Walk Metropolis Algorithm. Comput.
Stat., 14(3):375–395, 1999.

Haario, H., Saksman, E., and Tamminen, J. An adaptive Metropo-

lis algorithm. Bernoulli, 7(2):223–242, 2001.

Murray, I. and Adams, R.P. Slice sampling covariance hyperpa-
In Advances in Neural

rameters of latent Gaussian models.
Information Processing Systems 23. 2012.

Roberts, G.O. and Rosenthal, J.S. Coupling and ergodicity of
J. Appl.

adaptive Markov chain Monte Carlo algorithms.
Probab., 44(2):458–475, 03 2007.

Roberts, G.O. and Stramer, O.

Langevin diffusions and
Metropolis-Hastings algorithms. Methodol. Comput. Appl.
Probab., 4:337–358, 2003.

A. Proofs

Kernel Adaptive Metropolis Hastings

Proposition 1. Let k be a differentiable positive deﬁnite kernel. Then

∇xk(x, x)

|x=y −

2

∇xk(x, y)

|x=y = 0.

Proof. Since k is a positive deﬁnite kernel there exists a Hilbert space
k(x, x′) =
ψ :
H →
to each y
the chain rule for

, such that
ϕ, where
h
(Rd, R) of τ at y, which
Rd associates a bounded linear operator from Rd to R (Steinwart & Christmann, 2008, Deﬁnition A.5.14). By

H
|x=y from the Fr´echet derivative Dτ (y)

iH. Consider ﬁrst the map τ : Rd
2
H . We can obtain
k

→ H
R, deﬁned by τ (x) = k(x, x). We write τ = ψ
◦

ϕ(x), ϕ(x′)
f
k

∇xk(x, x)

R, ψ(f ) =

and a feature map ϕ : Rd

∈ B

→

∈

Fr´echet derivatives (Steinwart & Christmann, 2008, Lemma A.5.15(b)), the value of Dτ (y) at some x′

Rd is

∈

[Dτ (y)] (x′) = [Dψ (ϕ(y))

Dϕ(y)] (x′),

◦

where Dϕ(y)
differentiable function (Steinwart & Christmann, 2008, Section 4.3). It is readily shown that Dψ [ϕ(y)] = 2
that

, R). The derivative Dϕ of the feature map exists whenever k is a
·iH, so

), and Dψ (ϕ(y))

ϕ(y),
h

(Rd,

(
H

∈ B

∈ B

H

Next, we consider the map κy(x) = k(x, y) =
a linear scalar function on

, Dψy (f ) =

H

[Dτ (y)] (x′) = 2

ϕ(y), [Dϕ(y)] (x′)
h
iH, i.e., κy = ψy ◦
ϕ(x), ϕ(y)
h
·iH. Again, by the chain rule:
ϕ(y),
h

iH .
ϕ where ψy(f ) =

[Dκy(y)] (x′) = [Dψy (ϕ(y))
=

◦

ϕ(y), [Dϕ(y)] (x′)
h

Dϕ(y)] (x′)
iH ,

f, ϕ(y)
h

iH. Since ψy is

2Dκy(y)) (x′) = 0, for all x′
and thus (Dτ (y)
derivatives can also be written as inner products with the gradients, (
(Dτ (y)

Rd, which proves the claim.

Rd, and we obtain equality of operators. Since Fr´echet
|x=y)⊤ x′ =

∇xk(x, x)

∇xk(x, y)

2Dκy(y)) (x′) = 0,

|x=y −

−

∈

2

x′
∀

∈

y) =

·|

N

(y, γ2I + ν2Mz,yHM ⊤

z,y).

−
Proposition 2. qz(

Proof. We start with

p(β)p(x∗

y, β) =

|

1
n+d

exp

−

(cid:18)

(2π)

exp

2 γdνn
1
2γ2 (x∗

y

−

1
2ν2 β⊤β

(cid:19)
Mz,yHβ)⊤ (x∗

−
(cid:18)
1
n+d

=

exp

(2π)

exp

2 γdνn
1
2

−

β⊤

(cid:18)

(cid:18)

(cid:18)

·

·

−
1
2γ2 (x∗
1
γ2 HM ⊤

−

(cid:18)
1
ν2 I +

−

y

−

−

Mz,yHβ)

(cid:19)

y)⊤ (x∗

y)

−

z,yMz,yH

β

(cid:19)

2
γ2 β⊤HM ⊤

z,y(x∗

y)

−

.

(cid:19)(cid:19)

(cid:19)

−

Now, we set

and application of the standard Gaussian integral

Σ−1 =

µ =

z,yMz,yH

1
ν2 I +
1
γ2 ΣHM ⊤

1
γ2 HM ⊤
z,y(x∗

y),

−

β⊤Σ−1β

2β⊤Σ−1µ

dβ =

exp

ˆ

1
2

−

−

(cid:18)
(2π)n/2√det Σ exp

(cid:0)

(cid:19)
(cid:1)
µ⊤Σ−1µ

1
2

(cid:18)

,

(cid:19)

This is just a d-dimensional Gaussian density where both the mean and covariance will, in general, depend on y. Let us
consider the exponent

leads to

Kernel Adaptive Metropolis Hastings

qz(x∗

y) =

|

√det Σ

exp

2 γdνn
1
2

(cid:18)
µ⊤Σ−1µ

d

(2π)

exp

·

(cid:18)

.

(cid:19)

1
2γ2 (x∗

−

−

y)⊤ (x∗

y)

−

(cid:19)

−

1
2γ2 (x∗
1
1
γ2 (x∗
2 (

−

−

y)⊤ (x∗

y) +

µ⊤Σ−1µ =

1
2

−

y)⊤ (x∗

y)

−

−

1
γ4 (x∗

−

−

y)⊤ Mz,yHΣHM ⊤

z,y (x∗

y)

=

−

)

1
2
n
γ2 Mz,yHΣHM ⊤

−

1

(x∗

y)⊤ R−1 (x∗

y)

,

−

−

o

R = γ2(I

= γ2

−

1
γ2 Mz,yHΣHM ⊤
γ2Σ−1

I + Mz,yH

z,y)−1

(cid:16)

(cid:0)

= γ2

I + Mz,yH

γ2
ν2 I
= γ2I + ν2Mz,yHM ⊤

 

(cid:18)

z,y.

(cid:19)

HM ⊤

z,yMz,yH

−1

HM ⊤
z,y

(cid:1)

(cid:17)

−
−1

HM ⊤
z,y

!

where R−1 = 1

γ2 (I

−

z,y). We can simplify the covariance R using the Woodbury identity to obtain:

Therefore, the proposal density is qz(

y) =

(y, γ2I + ν2Mz,yHM ⊤

z,y).

·|

N

B. Further details on synthetic experiments

Proposal contours for the Flower target. The d-dimensional ﬂower target

(r0, A, ω, σ) is given by

(x; r0, A, ω, σ) = exp

F

 − p

1 + x2
x2

2 −

r0 −

F
A cos (ωatan2 (x2, x1))
2σ2

(x3:d; 0, I).

! N

This distribution concentrates around the r0-circle with a periodic perturbation (with amplitude A and frequency ω) in the
ﬁrst two dimensions. For A = 0, we obtain a band around the r0-circle, which we term the ring target. Figure 6 gives the
contour plots of the MCMC Kameleon proposal distributions on two instances of the ﬂower target.

Convergence statistics for the Banana target. Figure 7 illustrates how the norm of the mean and quantile deviation
(shown for 0.5-quantile) for the strongly twisted Banana target decrease as a function of the number of iterations. This
shows that the trends observed in the main text persist along the evolution of the whole chain.

C. Principal Components Proposals

An alternative approach to the standard adaptive Metropolis, discussed in Andrieu & Thoms (2008, Algorithm 8), is to
m
extract m
j=1 from the estimated covariance matrix Σz and use the
(λj , vj)
}
{
proposal that takes form of a mixture of one-dimensional random walks along the principal eigendirections

d principal eigenvalue-eigenvector pairs

≤

qz (

y) =

·|

(y, ν2

j λjvjv⊤

j ).

ωjN

m

j=1
X

(7)

Kernel Adaptive Metropolis Hastings

Figure6. 95% contours (red) of proposal distributions evaluated at a number of points, for the ﬂower and the ring target. Underneath are
the density heatmaps, and the samples (blue) used to construct the proposals.

12

2

10
(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13)
8

X
ˆE

n
a
e
m
e
h
t

f
o
m
r
o
n

6

4

2

0

KAMH-LS
AM-LS
AM-FS
SM

e
c
n
e
r
e
f
f
i
d

e
l
i
t
n
a
u
q
-
5
.
0

0.12

0.10

0.08

0.06

0.04

0.02

0.00

0

KAMH-LS
AM-LS
AM-FS
SM

5000

10000

15000

20000

25000

30000

35000

40000

45000

5000

10000

15000

20000

25000

30000

35000

40000

45000

number of samples

number of samples

Figure7. Comparison of SM, AM-FS, AM-LS and KAMH-LS in terms of the norm of the estimated mean (left) and in terms of the
deviation from the 0.5-quantile (right) on the strongly twisted Banana distribution. The results are averaged over 20 chains for each
sampler. Error bars represent 80%-conﬁdence intervals.

Kernel Adaptive Metropolis Hastings

x∗ = y + ρνj

λjvj,

In other words, given the current chain state y, the j-th principal eigendirection is chosen with probability ωj (choice
ωj = λj /

m
l=1 λl is suggested), and the proposed point is

(8)

(9)

P

with ρ
∼ N
eigenvalue.

≤
i=1 ˜α(j)

i

(0, 1). Note that each eigendirection may have a different scaling factor νj in addition to the scaling with the

p

We can consider an analogous version of the update (8) performed in the RKHS

with m

n principal eigenvalue-eigenfunction pairs

, y) + ρνj

f = k(
·
(λj , vj)
}

{

λjvj ,

p
m
j=1. It is readily shown that the eigenfunctions vj =
⊤

n

−

, zi)

[k(
·

µz] lie in the subspace

˜α(j)
1
(cid:16)
are proportional to the eigenvector of the centered kernel matrix HKH, with normalization chosen so that
P
˜α(j)
has form
(cid:0)

(cid:17)
=
2
2 = 1 (so that the eigenfunctions have the unit RKHS norm). Therefore, the update (9)
(cid:13)
(cid:13)

z induced by z, and that the coefﬁcients vectors ˜α(j) =

HKH ˜α(j) = λj

˜α(j)
n
2
Hk

· · ·
vjk

˜α(j)

(cid:13)
(cid:13)

H

k

(cid:1)

⊤

n

f = k(
·

, y) +

β(j)
i

, zi)

[k(
·

−

µz] ,

i=1
X

˜α(j)

λj ˜α(j). But α(j) =

2
where β(j) = ρνj
2 =
2
2 = 1. Therefore, the appropriate scaling with eigenvalues is already included in the β-coefﬁcients, just like in
λj
the MCMC Kameleon, where the β-coefﬁcients are isotropic.
(cid:13)
(cid:13)

λj ˜α(j) are themselves the (unit norm) eigenvectors of HKH, as

Now, we can construct the MCMC PCA-Kameleon by simply substituting β-coefﬁcients with ρνjα(j), where j is the
selected eigendirection, and νj is the scaling factor associated to the j-th eigendirection. We have the following steps:

α(j)

p

p

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

1. Perform eigendecomposition of HKH to obtain the m

n eigenvectors

≤

m
j=1 .

αj}
{

2. Draw j

Discrete [ω1, . . . , ωm]

∼
(0, 1)

3. ρ

∼ N

4. x∗

y, ρ, j

|

∼ N

(y + ρνjMz,yHα(j), γ2I) (d

1 normal in the original space)

×

Similarly as before, we can simplify the proposal by integrating out the scale ρ of the moves in the RKHS.

Proposition 3. qz(

y) =

·|

Proof. We start with

m

j=1 ωjN

P

(y, γ2I + ν2

j Mz,yHα(j)

α(j)

⊤

HM ⊤

z,y).

(cid:0)

(cid:1)

p(ρ)p(x∗

y, ρ, j)

|

∝

exp

−

(cid:20)

y)⊤(x∗

1
2γ2 (x∗
1
2 ( 

−

1 +

ν2
j
γ2

y)

−

(cid:21)
⊤

(cid:16)

(cid:17)

exp

·

"−

By substituting

α(j)

HM ⊤

z,yMz,yHα(j)

ρ2

2ρ

−

νj
γ2

!

⊤

α(j)

HM ⊤

z,y (x∗

y)

.

)#

−

(cid:16)

(cid:17)

σ−2 = 1 +

α(j)

HM ⊤

z,yMz,yHα(j),

⊤

µ = σ2

(cid:16)

(cid:17)
α(j)

⊤

HM ⊤

z,y (x∗

(cid:18)

(cid:16)

(cid:17)

−

y)

,

(cid:19)

ν2
j
γ2
νj
γ2

p (x∗

y, j)

|

∝

exp

"−

1
γ2 (x∗

−

y)⊤(x∗

y)

−

−

ν2
j σ2
γ4

−

(x∗

y)⊤ Mz,yHα(j)

α(j)

⊤

HM ⊤

z,y (x∗

y)

−

)#

(cid:16)

(cid:17)

we integrate out ρ to obtain:

Kernel Adaptive Metropolis Hastings

1
2 (
1
2

(x∗

= exp

y)⊤ R−1 (x∗

−

−

(cid:20)
j σ2
ν2
γ2 Mz,yHα(j)

y)

−

(cid:21)

⊤

α(j)

HM ⊤
z,y

(cid:0)

(cid:1)

(cid:17)

where R−1 = 1
γ2
to obtain:

I

−

(cid:16)

R = γ2(I

−

= γ2

I +

j σ2
ν2
γ2 Mz,yHα(j)
(cid:16)
ν2
j σ2
γ2 Mz,yHα(j)

α(j)

HM ⊤

z,y)−1

⊤

(cid:17)

ν2
j σ2
γ2

1

−

 

(cid:16)

(cid:17)

= γ2

I +

ν2
j
γ2 Mz,yHα(j)

⊤

α(j)

HM ⊤
z,y

!





 

= γ2I + ν2

j Mz,yHα(j)

(cid:16)
α(j)

⊤

(cid:17)
HM ⊤

z,y.

(cid:16)
The claim follows after summing over the choice j of the eigendirection (w.p. ωj).

(cid:17)

. We can simplify the covariance R using the Woodbury identity

⊤

α(j)

HM ⊤

z,yMz,yHα(j)

−1

!

⊤

α(j)

(cid:16)

(cid:17)

HM ⊤

z,y



Kernel Adaptive Metropolis-Hastings

4
1
0
2
 
n
u
J
 

2
1
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
2
0
3
5
.
7
0
3
1
:
v
i
X
r
a

Dino Sejdinovic⋆
Heiko Strathmann⋆
Maria Lomeli Garcia⋆
Christophe Andrieu‡
Arthur Gretton⋆
⋆Gatsby Unit, CSML, University College London, UK and ‡School of Mathematics, University of Bristol, UK

DINO@GATSBY.UCL.AC.UK
UCABHST@GATSBY.UCL.AC.UK
MLOMELI@GATSBY.UCL.AC.UK
C.ANDRIEU@BRISTOL.AC.UK
ARTHUR.GRETTON@GMAIL.COM

Abstract

A Kernel Adaptive Metropolis-Hastings algo-
rithm is introduced, for the purpose of sampling
from a target distribution with strongly nonlin-
ear support. The algorithm embeds the trajec-
tory of the Markov chain into a reproducing ker-
nel Hilbert space (RKHS), such that the fea-
ture space covariance of the samples informs
the choice of proposal. The procedure is com-
putationally efﬁcient and straightforward to im-
plement, since the RKHS moves can be inte-
grated out analytically: our proposal distribu-
tion in the original space is a normal distribution
whose mean and covariance depend on where
the current sample lies in the support of the tar-
get distribution, and adapts to its local covari-
ance structure. Furthermore, the procedure re-
quires neither gradients nor any other higher or-
der information about the target, making it par-
ticularly attractive for contexts such as Pseudo-
Marginal MCMC. Kernel Adaptive Metropolis-
Hastings outperforms competing ﬁxed and adap-
tive samplers on multivariate, highly nonlinear
target distributions, arising in both real-world
and synthetic examples.

1. Introduction

The choice of the proposal distribution is known to be
crucial for the design of Metropolis-Hastings algorithms,
and methods for adapting the proposal distribution to in-
crease the sampler’s efﬁciency based on the history of the
Markov chain have been widely studied. These methods
often aim to learn the covariance structure of the target
distribution, and adapt the proposal accordingly. Adaptive
MCMC samplers were ﬁrst studied by Haario et al. (1999;

Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copy-
right 2014 by the author(s).

2001), where the authors propose to update the proposal
distribution along the sampling process. Based on the chain
history, they estimate the covariance of the target distribu-
tion and construct a Gaussian proposal centered at the cur-
rent chain state, with a particular choice of the scaling fac-
tor from Gelman et al. (1996). More sophisticated schemes
are presented by Andrieu & Thoms (2008), e.g., adaptive
scaling, component-wise scaling, and principal component
updates.

While these strategies are beneﬁcial for distributions that
show high anisotropy (e.g., by ensuring the proposal uses
the right scaling in all principal directions), they may still
suffer from low acceptance probability and slow mixing
when the target distributions are strongly nonlinear, and
the directions of large variance depend on the current lo-
cation of the sampler in the support. In the present work,
we develop an adaptive Metropolis-Hastings algorithm in
which samples are mapped to a reproducing kernel Hilbert
space, and the proposal distribution is chosen according to
the covariance in this feature space (Sch¨olkopf et al., 1998;
Smola et al., 2001). Unlike earlier adaptive approaches,
the resulting proposal distributions are locally adaptive in
input space, and oriented towards nearby regions of high
density, rather than simply matching the global covari-
ance structure of the distribution. Our approach com-
bines a move in the feature space with a stochastic step
towards the nearest input space point, where the feature
space move can be analytically integrated out. Thus,
the implementation of the procedure is straightforward:
the proposal is simply a multivariate Gaussian in the in-
put space, with location-dependent covariance which is
informed by the feature space representation of the tar-
get. Furthermore, the resulting Metropolis-Hastings sam-
pler only requires the ability to evaluate the unnormal-
ized density of the target (or its unbiased estimate, as
in Pseudo-Marginal MCMC of Andrieu & Roberts, 2009),
and no gradient evaluation is needed, making it applicable
to situations where more sophisticated schemes based on
Hamiltonian Monte Carlo (HMC) or Metropolis Adjusted

Kernel Adaptive Metropolis Hastings

Langevin Algorithms (MALA) (Roberts & Stramer, 2003;
Girolami & Calderhead, 2011) cannot be applied.

We begin our presentation in Section 2, with a brief
overview of existing adaptive Metropolis approaches; we
also review covariance operators in the RKHS. Based on
these operators, we describe a sampling strategy for Gaus-
sian measures in the RKHS in Section 3, and introduce a
cost function for constructing proposal distributions.
In
Section 4, we outline our main algorithm, termed Kernel
Adaptive Metropolis-Hastings (MCMC Kameleon). We
provide experimental comparisons with other ﬁxed and
adaptive samplers in Section 5, where we show superior
performance in the context of Pseudo-Marginal MCMC
for Bayesian classiﬁcation, and on synthetic target distri-
butions with highly nonlinear shape.

2. Background

= Rd be
Adaptive Metropolis Algorithms. Let
the domain of interest, and denote the unnormalized
target density on
let Σt =
Σt(x0, x1, . . . , xt−1) denote an estimate of the covariance
matrix of the target density based on the chain history
t−1
i=0. The original adaptive Metropolis at the current
xi}
{
state of the chain state xt = y uses the proposal

by π. Additionally,

X

X

qt(

y) =

(y, ν2Σt),

(1)

·|

N
where ν = 2.38/√d is a ﬁxed scaling factor from
Gelman et al. (1996). This choice of scaling factor was
shown to be optimal (in terms of efﬁciency measures) for
the usual Metropolis algorithm. While this optimality re-
sult does not hold for Adaptive Metropolis, it can never-
theless be used as a heuristic. Alternatively, the scale ν
can also be adapted at each step as in Andrieu & Thoms
(2008, Algorithm 4) to obtain the acceptance rate from
Gelman et al. (1996), a∗ = 0.234.

X

X × X →

Embeddings

and Covariance Opera-
RKHS
tors. According to the Moore-Aronszajn theorem
(Berlinet & Thomas-Agnan, 2004, p. 19), for every sym-
R,
metric, positive deﬁnite function (kernel) k :
there is an associated reproducing kernel Hilbert space
with reproducing kernel
Hk of real-valued functions on
k. The map ϕ :
, x) is called
X → Hk, ϕ : x
the canonical feature map of k. This feature map or
embedding of a single point can be extended to that of
its kernel embedding is
a probability measure P on
an element µP ∈ Hk, given by µP =
, x) dP (x)
k(
·
(Berlinet & Thomas-Agnan, 2004; Fukumizu et al., 2004;
Smola et al., 2007). If a measurable kernel k is bounded,
it is straightforward to show using the Riesz represen-
tation theorem that the mean embedding µk(P ) exists
. For many interesting
for all probability measures on

k(
·

7→

X

´

:

X

7→

bounded kernels k, including the Gaussian, Laplacian and
inverse multi-quadratics, the kernel embedding P
µP
is injective. Such kernels are said to be characteristic
(Sriperumbudur et al., 2010; 2011), since each distribution
is uniquely characterized by its embedding (in the same
way that every probability distribution has a unique
characteristic function). The kernel embedding µP is the
representer of expectations of smooth functions w.r.t. P ,
i.e.,
f (x)dP (x). Given samples
∈ Hk,
f
f, µP iHk
h
∀
n
z =
P , the embedding of the empirical measure
zi}
i=1 ∼
{
n
is µz = 1
, zi).
i=1 k(
n
·
Hk → Hk for a
Next, the covariance operator CP :
probability measure P is given by CP =
k(
⊗
·
´
µP (Baker, 1973; Fukumizu et al.,
k(
µP ⊗
·
∈ Hk the tensor product is de-
2004), where for a, b, c
a. The covariance oper-
ﬁned as (a
b, c
b)c =
⊗
ator has the property that
=
EP f EP g.
EP (f g)

f, CP g
h

∈ Hk,

iHk
f, g
∀

, x) dP (x)

iHk

, x)

P

=

−

´

h

−

1
n

⊗

⊗

µz

P

, zi)

, zi)

k(
·

n
i=1 k(
·

Our approach is based on the idea that the nonlinear sup-
port of a target density may be learned using Kernel Princi-
pal Component Analysis (Kernel PCA) (Sch¨olkopf et al.,
1998; Smola et al., 2001),
this being linear PCA on
the empirical covariance operator in the RKHS, Cz =
1
µz, computed on the
n
−
sample z deﬁned above. The empirical covariance op-
erator behaves as expected: applying the tensor prod-
uct deﬁnition gives
n
1
i=1 f (zi)
n

= 1
n
−
. By analogy with algo-
rithms which use linear PCA directions to inform M-H pro-
(cid:0)
posals (Andrieu & Thoms, 2008, Algorithm 8), nonlinear
PCA directions can be encoded in the proposal construc-
tion, as described in Appendix C. Alternatively, one can
focus on a Gaussian measure on the RKHS determined by
the empirical covariance operator Cz rather than extract-
ing its eigendirections, which is the approach we pursue in
this contribution. This generalizes the proposal (1), which
considers the Gaussian measure induced by the empirical
covariance matrix on the original space.

f, Czg
iHk
h
n
i=1 g(zi)

n
i=1 f (zi)g(zi)

(cid:1) (cid:0)

P

P

P

(cid:1)

3. Sampling in RKHS

We next describe the proposal distribution at iteration t of
the MCMC chain. We will assume that a subset of the chain
n
history, denoted z =
1, is available. Our
i=1, n
proposal is constructed by ﬁrst considering the samples in
the RKHS associated to the empirical covariance operator,
and then performing a gradient descent step on a cost func-
tion associated with those samples.

zi}
{

≤

−

t

Gaussian Measure of the Covariance Operator. We
will work with the Gaussian measure on the RKHS
Hk
n
with mean k(
zi}
i=1
·
is the subset of the chain history. While there is no analogue

, y) and covariance ν2Cz, where z =

{

Kernel Adaptive Metropolis Hastings

Samples {zi}200
i=1

Current position y

βiβj (k(
·

, zi)

−

µz)

(k(
·

⊗

, zj)

−

µz)

Figure1. Heatmaps (white denotes large) and gradients of g(x)
for two samples of β and ﬁxed z.

f

(cid:16)

−

−

1
2ν2

k(
·

N
k(
·

, y), C−1

, y), ν2Cz)

(f ; k(
·
Hk

of a Lebesgue measure in an inﬁnite dimensional RKHS, it
is instructive (albeit with some abuse of notation) to denote
this measure in the “density form”

∝
. As Cz is
z (f
exp
a ﬁnite-rank operator, this measure is supported only on a
ﬁnite-dimensional afﬁne space k(
z =
, y) +
·
n
span
i=1 is the subspace spanned by the canonical
features of z. It can be shown that a sample from this mea-
sure has the form f = k(
µz] ,
, y) +
·
n I) is isotropic. Indeed, to see that f
where β
has the correct covariance structure, note that:

(cid:17)
(cid:11)
z, where
H

n
i=1 βi [k(
·

, zi)
}

(0, ν2

k(
·

∼ N

, zi)

, y))

P

H

−

−

{

(cid:10)

, y))

(f

k(
·

−

⊗

, y))]

−

k(
·

n

n

E [(f

= E




ν2
n

=

i=1
X
n

j=1
X

i=1
X

, zi)

(k(
·

−

µz)

(k(
·

⊗

, zi)

−

µz) = ν2Cz.





and a Gaussian Process

Due to the equivalence in the RKHS between a
Gaussian measure
(GP)
(Berlinet & Thomas-Agnan, 2004, Ch.
4), we can
think of the RKHS samples f as trajectories of the GP with
mean m(x) = k(x, y) and covariance function
κ(x, x′) = cov [f (x), f (x′)]
ν2
n

µz(x)) (k(x′, zi)

(k(x, zi)

=

µz(x′)) .

n

−

−

i=1
X

The covariance function κ of this GP is therefore the kernel
k convolved with itself with respect to the empirical mea-
sure associated to the samples z, and draws from this GP
therefore lie in a smaller RKHS; see Saitoh (1997, p. 21)
for details.

P

, zi)

, y) +

n
i=1 βi [k(
·

Obtaining Target Samples through Gradient Descent.
We have seen how to obtain the RKHS sample f =
µz] from the Gaussian mea-
k(
−
·
sure in the RKHS. This sample does not in general have a
= Rd;
corresponding pre-image in the original domain
X
i.e., there is no point x∗
, x∗). If
such that f = k(
·
∈ X
there were such a point, then we could use it as a proposal
in the original domain. Therefore, we are ideally looking
, x∗) is
for a point x∗
close to f in the RKHS norm. We consider the optimization
problem

whose canonical feature map k(
·

∈ X

arg min
x∈X

kk (·, x) − f k

2
Hk

=

In general, this is a non-convex minimization problem, and
may be difﬁcult to solve (Bakir et al., 2003). Rather than
solving it for every new vector of coefﬁcients β, which
would lead to an excessive computational burden for ev-
ery proposal made, we simply make a single descent step
along the gradient of the cost function,

g(x) = k(x, x)

2k(x, y)

2

βi [k(x, zi)

µz(x)] ,

−

−

−

n

i=1
X

(2)

i.e., the proposed new point is

x∗ = y

η

∇xg(x)

|x=y + ξ,

−

where η is a gradient step size parameter and ξ

∼
(0, γ2I) is an additional isotropic ’exploration’ term
It will be useful to split the
|x=y =
|x=y −

N
after the gradient step.
scaled gradient at y into two terms as η
Mz,yHβ), where ay =
η (ay −
∇xk(x, y)
2
Mz,y = 2 [

∇xg(x)
∇xk(x, x)

|x=y,

(3)

∇xk(x, z1)

n matrix, and H = I

|x=y, . . . ,
−

∇xk(x, zn)
×

1
n 1n×n is the n

|x=y]
n centering

is a d
×
matrix.

Figure 1 plots g(x) and its gradients for several samples of
β-coefﬁcients, in the case where the underlying z-samples
are from the two-dimensional nonlinear Banana target dis-
tribution of Haario et al. (1999). It can be seen that g may
have multiple local minima, and that it varies most along
the high-density regions of the Banana distribution.

4. MCMC Kameleon Algorithm

4.1. Proposal Distribution

arg min

k(x, x) − 2k(x, y) − 2

βi [k(x, zi) − µz(x)]

.

x∈X (

We now have a recipe to construct a proposal that is able to
adapt to the local covariance structure for the current chain

)

n

i=1
X

Kernel Adaptive Metropolis Hastings

MCMC Kameleon
Input: unnormalized target π, subsample size n, scaling
∞
t=0, kernel k,
parameters ν, γ, adaptation probabilities

pt}

{

At iteration t + 1,

•

1. With probability pt, update a random subsample

min(n,t)
i=1

z =

zi}

xi}
{
2. Sample proposed point x∗ from qz(
·|

of the chain history

{

(xt, γ2I + ν2Mz,xt HM ⊤
(3) and H = I

N
given in Eq.
centering matrix,

t−1
i=0,
xt) =
z,xt), where Mz,xtis
1
n 1n×n is the

−

3. Accept/Reject with the Metropolis-Hastings ac-

ceptance probability A(xt, x∗) in Eq. (4),

xt+1 =

x∗, w.p. A(xt, x∗),
xt, w.p. 1

A(xt, x∗).

(

−

state y. This proposal depends on a subset of the chain
history z, and is denoted by qz(
y). While we will later
simplify this proposal by integrating out the moves in the
RKHS, it is instructive to think of the proposal generating
process as:

·|

1. Sample β
ﬁcients).

∼ N

(0, ν2I) (n

1 normal of RKHS coef-

×

•

This represents an RKHS sample f = k(
·

, y) +
µz] which is the goal of the

n
i=1 βi [k(
·

−
cost function g(x).
P

, zi)

2. Move along the gradient of g:

x∗ = y

−

η

∇xg(x)

|x=y + ξ.

•

This gives a proposal x∗
ηMz,yHβ, γ2I) (d
space).

×

y, β

ηay +
1 normal in the original

∼ N

(y

−

|

2

|x=y −

∇xk(x, x)

Our ﬁrst step in the derivation of the explicit proposal den-
sity is to show that as long as k is a differentiable positive
deﬁnite kernel, the term ay vanishes.
Proposition 1. Let k be a differentiable positive deﬁnite
|x=y = 0.
kernel. Then ay =
Since ay = 0, the gradient step size η always appears to-
gether with β, so we merge η and the scale ν of the β-
coefﬁcients into a single scale parameter, and set η = 1
henceforth. Furthermore, since both p(β) and pz(x∗
y, β)
are multivariate Gaussian densities, the proposal density
qz(x∗
y, β)dβ can be computed analyt-
ically. We therefore get the following closed form expres-
sion for the proposal distribution.

∇xk(x, y)

p(β)pz(x∗

y) =

´

|

|

|

Proposition 2. qz(

y) =

(y, γ2I + ν2Mz,yHM ⊤

z,y).

·|

N

Figure2. 95% contours (red) of proposal distributions evaluated
at a number of points, for the ﬁrst two dimensions of the banana
target of Haario et al. (1999). Underneath is the density heatmap,
and the samples (blue) used to construct the proposals.

Proofs of the above Propositions are given in Appendix A.

With the derived proposal distribution, we proceed with the
standard Metropolis-Hastings accept/reject scheme, where
the proposed sample x∗ is accepted with probability

A(xt, x∗) = min

1,

π(x∗)qz(xt|
π(xt)qz(x∗
|

x∗)
xt)

,

(4)

(cid:27)

(cid:26)
giving rise to the MCMC Kameleon Algorithm. Note that
each π(x∗) and π(xt) could be replaced by their unbi-
ased estimates without impacting the invariant distribution
(Andrieu & Roberts, 2009).
The constructed family of proposals encodes local struc-
ture of the target distribution, which is learned based on the
subsample z. Figure 2 depicts the regions that contain 95%
y) at various
of the mass of the proposal distribution qz(
states y for a ﬁxed subsample z, where the Banana target
is used (details in Section 5). More examples of proposal
contours can be found in Appendix B.

·|

4.2. Properties of the Algorithm

·|

and convergence. MCMC
The update
schedule
n
Kameleon requires a subsample z =
i=1 at each
zi}
{
iteration of the algorithm, and the proposal distribution
y) is updated each time a new subsample z is obtained.
qz(
It is well known that a chain which keeps adapting the
proposal distribution need not converge to the correct
target (Andrieu & Thoms, 2008). To guarantee conver-
∞
gence, we introduce adaptation probabilities
t=0,
pt}
{
, and at iteration
such that pt →
t we update the subsample z with probability pt. As
adaptations occur with decreasing probability, Theorem 1
of Roberts & Rosenthal (2007) implies that the resulting
algorithm is ergodic and converges to the correct target.
Another straightforward way to guarantee convergence is
n
to ﬁx the set z =
i=1 after a “burn-in” phase; i.e., to
stop adapting Roberts & Rosenthal (2007, Proposition 2).
In this case, a “burn-in” phase is used to get a rough sketch
of the shape of the distribution: the initial samples need not

∞
t=1 pt =

zi}
{

0 and

P

∞

Kernel Adaptive Metropolis Hastings

come from a converged or even valid MCMC chain, and it
sufﬁces to have a scheme with good exploratory properties,
e.g., Welling & Teh (2011).
In MCMC Kameleon, the
term γ allows exploration in the initial iterations of the
chain (while the subsample z is still not informative about
the structure of the target) and provides regularization of
the proposal covariance in cases where it might become
ill-conditioned. Intuitively, a good approach to setting γ is
to slowly decrease it with each adaptation, such that the
learned covariance progressively dominates the proposal.

In Haario et al. (2001), the
Symmetry of the proposal.
proposal distribution is asymptotically symmetric due to
the vanishing adaptation property. Therefore, the authors
compute the standard Metropolis acceptance probability. In
our case, the proposal distribution is a Gaussian with mean
at the current state of the chain xt = y and covariance
γ2I + ν2Mz,yHM ⊤
z,y, where Mz,y depends both on the
n
current state y and a random subsample z =
i=1 of the
t−1
chain history
i=0. This proposal distribution is never
symmetric (as covariance of the proposal always depends
on the current state of the chain), and therefore we use the
Metropolis-Hastings acceptance probability to reﬂect this.

xi}
{

zi}
{

Relationship to MALA and Manifold MALA. The
Metropolis Adjusted Langevin Algorithm (MALA) algo-
rithm uses information about the gradient of the log-target
density at the current chain state to construct a proposed
point for the Metropolis step. Our approach does not re-
quire that the log-target density gradient be available or
computable. Kernel gradients in the matrix Mz,y are easily
obtained for commonly used kernels, including the Gaus-
sian kernel (see section 4.3), for which the computational
complexity is equal to evaluating the kernel itself. More-
over, while standard MALA simply shifts the mean of the
proposal distribution along the gradient and then adds an
isotropic exploration term, our proposal is centered at the
current state, and it is the covariance structure of the pro-
posal distribution that coerces the proposed points to be-
It would
long to the high-density regions of the target.
be straightforward to modify our approach to include a
drift term along the gradient of the log-density, should
such information be available, but it is unclear whether
this would provide additional performance gains. Fur-
ther work is required to elucidate possible connections be-
tween our approach and the use of a preconditioning ma-
trix (Roberts & Stramer, 2003) in the MALA proposal; i.e.,
where the exploration term is scaled with appropriate met-
ric tensor information, as in Riemannian manifold MALA
(Girolami & Calderhead, 2011).

4.3. Examples of Covariance Structure for Standard

Kernels

The proposal distributions in MCMC Kameleon are depen-
dant on the choice of the kernel k. To gain intuition re-

garding their covariance structure, we give two examples
below.

a

In

of
linear
obtain Mz,y
= 2Z⊤,

ker-
Linear kernel.
the
case
nel k(x, x′)
x⊤x′, we
=
=
∇xx⊤z1|x=y, . . . ,
∇xx⊤zn|x=y
so the
2
(y, γ2I + 4ν2Z⊤HZ);
proposal is given by qz(
y) =
(cid:3)
(cid:2)
N
thus, the proposal simply uses the scaled empirical co-
variance Z⊤HZ just like standard Adaptive Metropolis
(Haario et al., 1999), with an additional isotropic explo-
ration component, and depends on y only through the
mean.

·|

Gaussian kernel.
k(x, x′) = exp

′

In the case of a Gaussian kernel
∇xk(x, x′) =

x−x
k
2σ2

, since

−

k

2

2

(cid:18)

(cid:19)

1

x), we obtain

σ2 k(x, x′)(x′
2
σ2 [k(y, z1)(z1 −

Mz,y =

−

y), . . . , k(y, zn)(zn −

y)] .

Consider how this encodes the covariance structure of the
target distribution:

Rij = γ

δij

2

+

−

4ν 2(n − 1)
σ4n

n

a=1
X

4ν 2
σ4n

a6=b
X

[k(y, za)]

(za,i − yi)(za,j − yj)

2

k(y, za)k(y, zb)(za,i − yi)(zb,j − yj). (5)

As the ﬁrst two terms dominate, the previous points za
which are close to the current state y (for which k(y, za)
is large) have larger weights, and thus they have more in-
ﬂuence in determining the covariance of the proposal at y.

′

′

ϑ

k

k

(cid:18)

k2

Γ(ϑ)

Kϑ

x−x
ρ

x−x
ρ

In the Mat´ern family of kernels

Mat´ern kernel.
kϑ,ρ(x, x′) = 21−ϑ
Kϑ is the modiﬁed Bessel function of the second kind, we
obtain a form of the covariance structure very similar to
∇xkϑ,ρ(x, x′) =
that of the Gaussain kernel. In this case,
2ρ2(ϑ−1) kϑ−1,ρ(x, x′)(x′
x), so the only difference (apart
from the scalings) to (5) is that the weights are now deter-
mined by a “rougher” kernel kϑ−1,ρ of the same family.

, where

k2

−

(cid:19)

(cid:19)

(cid:18)

1

5. Experiments

·|

N

y) =

In the experiments, we compare the following samplers:
(SM) Standard Metropolis with the isotropic proposal
(y, ν2I) and scaling ν = 2.38/√d, (AM-
q(
FS) Adaptive Metropolis with a learned covariance ma-
trix and ﬁxed scaling ν = 2.38/√d, (AM-LS) Adaptive
Metropolis with a learned covariance matrix and scaling
learned to bring the acceptance rate close to α∗ = 0.234 as
described in Andrieu & Thoms (2008, Algorithm 4), and
(KAMH-LS) MCMC Kameleon with the scaling ν learned

Kernel Adaptive Metropolis Hastings

0

−1

−2

−3

−4

7
θ

−5

−6

−5

−4

−2

−1

0

−3

θ2

Figure3. Dimensions 2 and 7 of the marginal hyperparameter
posterior on the UCI Glass dataset

in the same fashion (γ was ﬁxed to 0.2), and which also
stops adapting the proposal after the burn-in of the chain
(in all experiments, we use a random subsample z of size
n = 1000, and a Gaussian kernel with bandwidth se-
lected according to the median heuristic). We consider the
following nonlinear targets: (1) the posterior distribution
of Gaussian Process (GP) classiﬁcation hyperparameters
(Filippone & Girolami, 2014) on the UCI glass dataset, and
(2) the synthetic banana-shaped distribution of Haario et al.
(1999) and a ﬂower-shaped disribution concentrated on a
circle with a periodic perturbation.

5.1. Pseudo-Marginal MCMC for GP Classiﬁcation

In the ﬁrst experiment, we illustrate usefulness of the
MCMC Kameleon sampler in the context of Bayesian clas-
siﬁcation with GPs (Williams & Barber, 1998). Consider
the joint distribution of latent variables f , labels y (with
covariate matrix X), and hyperparameters θ, given by

p(f , y, θ) = p(θ)p(f

θ)p(y

f ),

|

|

′

1

θ

}

(cid:16)

−

P

(0,

1, 1

∼ N

D
d=1

fi) =

θ) = exp

(xi,d−x
ℓ2
d

Kθ), with

i.e.,
1−exp(−yifi) where yi ∈ {−

where f
Kθ modeling the covariance
|
between latent variables evaluated at the input covariates:
j,d)2
1
Kθ)ij = κ(xi, x′
(
j|
2
and θd = log ℓ2
d. We restrict our attention to the bi-
(cid:17)
the likelihood is given by
nary logistic classiﬁer;
. We pursue
p(yi|
a fully Bayesian treatment, and estimate the posterior of
the hyperparameters θ. As observed by Murray & Adams
(2012), a Gibbs sampler on p(θ, f
y), which samples from
p(f
f , y)
is extremely sharp, drastically limiting the amount that
f , y. On the other hand,
any Markov chain can update θ
if we directly consider the marginal posterior p(θ
∝
p(y
θ)p(θ) of the hyperparameters, a much less peaked
distribution can be obtained. However, the marginal like-
lihood p(y
θ) is intractable for non-Gaussian likelihoods
p(y
f ), so it is not possible to analytically integrate out
the latent variables. Recently developed pseudo-marginal
MCMC methods (Andrieu & Roberts, 2009) enable exact

f , y) in turn, is problematic, as p(θ

θ, y) and p(θ

y)

|

|

|

|

|

|

|

|

|

inference on this problem (Filippone & Girolami, 2014),
by replacing p(y

θ) with an unbiased estimate

|

(6)

nimp

ˆp(y

θ) :=

|

1
nimp

p(y

f (i))

|

p(f (i)
q(f (i)

,

θ)
|
θ)
|

|

|

|

|

(cid:8)

∝

q(f

f (i)

y, θ)

f )p(f

nimp
i=1 ∼

i=1
X
where
θ) are nimp importance sam-
ples. In Filippone & Girolami (2014), the importance dis-
(cid:9)
tribution q(f
θ) is chosen as the Laplacian or as the Ex-
|
pectation Propagation (EP) approximation of p(f
p(y
θ), leading to state-of-the-art results.
We consider the UCI Glass dataset (Bache & Lichman,
2013), where classiﬁcation of window against non-window
glass is sought. Due to the heterogeneous structure of each
of the classes (i.e., non-window glass consists of contain-
ers, tableware and headlamps), there is no single consistent
set of lengthscales determining the decision boundary, so
one expects the posterior of the covariance bandwidths θd
to have a complicated (nonlinear) shape. This is illustrated
by the plot of the posterior projections to the dimensions
2 and 7 (out of 9) in Figure 3. Since the ground truth
for the hyperparameter posterior is not available, we ini-
tially ran 30 Standard Metropolis chains for 500,000 itera-
tions (with a 100,000 burn-in), kept every 1000-th sample
in each of the chains, and combined them. The resulting
samples were used as a benchmark, to evaluate the perfor-
mance of shorter single-chain runs of SM, AM-FS, AM-
LS and KAMH-LS. Each of these algorithms was run for
100,000 iterations (with a 20,000 burnin) and every 20-th
sample was kept. Two metrics were used in evaluating the
performance of the four samplers, relative to the large-scale
was computed
benchmark. First, the distance
ˆµθ −
between the mean ˆµθ estimated from each of the four sam-
(cid:13)
pler outputs, and the mean µb
θ on the benchmark sample
(cid:13)
(Fig. 4, left), as a function of sample size. Second, the
MMD (Borgwardt et al., 2006; Gretton et al., 2007) was
computed between each sampler output and the bench-
)3;
mark sample, using the polynomial kernel (1 +
i
i.e., the comparison was made in terms of all mixed mo-
ments of order up to 3 (Fig. 4, right). The ﬁgures indicate
that KAMH-LS approximates the benchmark sample bet-
ter than the competing approaches, where the effect is es-
pecially pronounced in the high order moments, indicating
that KAMH-LS thoroughly explores the distribution sup-
port in a relatively small number of samples.
We emphasise that, as for any pseudo-marginal MCMC
scheme, neither the likelihood itself, nor any higher-
the marginal posterior target
order information about
y), are available.
This makes HMC or MALA
p(θ
based approaches such as (Roberts & Stramer, 2003;
Girolami & Calderhead, 2011) unsuitable for this problem,
so it is very difﬁcult to deal with strongly nonlinear pos-
terior targets.
In contrast, as indicated in this example,
the MCMC Kameleon scheme is able to effectively sample

θ, θ′
h

µb
θ

(cid:13)
(cid:13)

2

|

Kernel Adaptive Metropolis Hastings

KAMH-LS
AM-LS
AM-FS
SM

2

bθ
µ
−

θ
ˆµ

e
c
n
a
t
s
i
d
n
a
e
m

0.50

0.45

(cid:13)(cid:13)
0.40

0.35
(cid:13)(cid:13)

0.30

0.25

0.20

0.15

0.10

0.5
0.4
0.3
0.2
0.1
0.0

1.0

0.8

0.6

0.4

0.2

0.0

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

KAMH-LS
AM-LS
AM-FS
SM

e
l
p
m
a
s
k
r
a
m
h
c
n
e
b

e
h
t

m
o
r
f

D
M
M

45

40

35

30

25

20

15

10

5

B

B

1000

2000

3000

4000

5000

1000

2000

3000

4000

5000

number of samples

number of samples

Figure4. The comparison of SM, AM-FS, AM-LS and KAMH-LS in terms of the distance between the estimated mean and the mean
on the benchmark sample (left) and in terms of the maximum mean discrepancy to the benchmark sample (right). The results are
averaged over 30 chains for each sampler. Error bars represent 80%-conﬁdence intervals.

Moderately twisted 8-dimensional

(0.03, 100) target; iterations: 40000, burn-in: 20000

SM

AM-FS

AM-LS

KAMH-LS

Accept ∈ [0, 1]

||ˆE[X]||/10

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Strongly twisted 8-dimensional

(0.1, 100) target; iterations: 80000, burn-in: 40000

SM

AM-FS

AM-LS

KAMH-LS

Accept ∈ [0, 1]

||ˆE[X]||/10

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

8-dimensional

(10, 6, 6, 1) target; iterations: 120000, burn-in: 60000

F

SM

AM-FS

AM-LS

KAMH-LS

Accept ∈ [0, 1]

||ˆE[X]||/10

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Figure5. Results for three nonlinear targets, averaged over 20 chains for each sampler. Accept is the acceptance rate scaled to the
interval [0, 1]. The norm of the mean ||ˆE[X]|| is scaled by 1/10 to ﬁt into the ﬁgure scalling, and the bars over the 0.1, . . . , 0.9-quantiles
represent the deviation from the exact quantiles, scaled by 10; i.e., 0.1 corresponds to 1% deviation. Error bars represent 80%-conﬁdence
intervals.

Kernel Adaptive Metropolis Hastings

from such nonlinear targets, and outperforms the vanilla
Metropolis methods, which are the only competing choices
in the pseudo-marginal context.

In addition, since the bulk of the cost for pseudo-marginal
MCMC is in importance sampling in order to obtain the
acceptance ratio, the additional cost imposed by KAMH-
LS is negligible. Indeed, we observed that there is an in-
crease of only 2-3% in terms of effective computation time
in comparison to all other samplers, for the chosen size of
the chain history subsample (n = 1000).

5.2. Synthetic examples

∼ N

In Haario et al. (1999),

(0, Σ) be a multivariate normal in d

the following
Banana target.
family of nonlinear target distributions is considered. Let
2 dimen-
X
sions, with Σ = diag(v, 1, . . . , 1), which undergoes the
transformation X
v),
1 −
(b, v). It is
and Yi = Xi for i
clear that EY = 0, and that

Y , where Y2 = X2 + b(X 2

→
= 2. We will write Y

≥

(y; b, v) =

(y1; 0, v)

B

N

(y2; b(y2

v), 1)

1−

N

(yj; 0, 1).

∼ B
d

N

j=3
Y

Flower target. The second target distribution we con-
sider is the d-dimensional ﬂower target
(r0, A, ω, σ),
with

F

F(x; r0, A, ω, σ) =

1 + x2
x2

2 − r0 − A cos (ωatan2 (x2, x1))

exp

−

 

2σ2

p
d

j=3
Y

×

N (xj; 0, 1).

This distribution concentrates around the r0-circle with a
periodic perturbation (with amplitude A and frequency ω)
in the ﬁrst two dimensions.

In these examples, exact quantile regions of the targets
can be computed analytically, so we can directly assess
performance without the need to estimate distribution dis-
tances on the basis of samples (i.e., by estimating MMD to
the benchmark sample). We compute the following mea-
sures of performance (similarly as in Haario et al. (1999);
Andrieu & Thoms (2008)) based on the chain after burn-in:
average acceptance rate, norm of the empirical mean (the
true mean is by construction zero for all targets), and the
deviation of the empirical quantiles from the true quantiles.
We consider 8-dimensional target distributions: the mod-
(0.03, 100) banana target (Figure 5, top)
erately twisted
and the strongly twisted
(0.1, 100) banana target (Figure
5, middle) and
(10, 6, 6, 1) ﬂower target (Figure 5, bot-
tom).

F

B

B

The results show that MCMC Kameleon is superior to the
competing samplers. Since the covariance of the proposal

adapts to the local structure of the target at the current chain
state, as illustrated in Figure 2, MCMC Kameleon does
not suffer from wrongly scaled proposal distributions. The
result is a signiﬁcantly improved quantile performance in
comparison to all competing samplers, as well as a com-
parable or superior norm of the empirical mean. SM has
a signiﬁcantly larger norm of the empirical mean, due to
its purely random walk behavior (e.g., the chain tends to
get stuck in one part of the space, and is not able to traverse
both tails of the banana target equally well). AM with ﬁxed
scale has a low acceptance rate (indicating that the scaling
of the proposal is too large), and even though the norm of
the empirical mean is much closer to the true value, quan-
tile performance of the chain is poor. Even if the estimated
covariance matrix closely resembles the true global covari-
ance matrix of the target, using it to construct proposal dis-
tributions at every state of the chain may not be the best
choice. For example, AM correctly captures scalings along
individual dimensions for the ﬂower target (the norm of its
empirical mean is close to its true value of zero) but fails to
capture local dependence structure. The ﬂower target, due
to its symmetry, has an isotropic covariance in the ﬁrst two
dimensions – even though they are highly dependent. This
leads to a mismatch in the scale of the covariance and the
scale of the target, which concentrates on a thin band in the
joint space. AM-LS has the “correct” acceptance rate, but
the quantile performance is even worse, as the scaling now
becomes too small to traverse high-density regions of the
target.

We have constructed a simple, versatile, adaptive, gradient-
free MCMC sampler that constructs a family of proposal
distributions based on the sample history of the chain.
These proposal distributions automatically conform to the
local covariance structure of the target distribution at the
current chain state.
In experiments, the sampler outper-
forms existing approaches on nonlinear target distributions,
both by exploring the entire support of these distributions,
and by returning accurate empirical quantiles, indicating
faster mixing. Possible extensions include incorporating
additional parametric information about the target densi-
ties, and exploring the tradeoff between the degree of sub-
sampling of the chain history and convergence of the sam-
pler.

Software. Python
MCMC
https://github.com/karlnapf/kameleon-mcmc.

implementation

Kameleon

available

of
at

is

Acknowledgments. D.S., H.S., M.L.G. and A.G. ac-
knowledge support of the Gatsby Charitable Foundation.
We thank Mark Girolami for insightful discussions and the
anonymous reviewers for useful comments.

!

6. Conclusions

Kernel Adaptive Metropolis Hastings

Saitoh, S. Integral transforms, reproducing kernels, and their ap-
plications. Pitman Research Notes in Mathematics 369, Long-
man Scientiﬁc & Techn., 1997.

Sch¨olkopf, B., Smola, A. J., and M¨uller, K.-R. Nonlinear compo-
nent analysis as a kernel eigenvalue problem. Neural Comput.,
10:1299–1319, 1998.

Smola, A., Gretton, A., Song, L., and Sch¨olkopf, B. A Hilbert
space embedding for distributions. In Proceedings of the Con-
ference on Algorithmic Learning Theory (ALT), pp. 13–31.
Springer, 2007.

Smola, A. J., Mika, S., Sch¨olkopf, B., and Williamson, R. C. Reg-
ularized principal manifolds. J. Mach. Learn. Res., 1:179–209,
2001.

Sriperumbudur, B., Gretton, A., Fukumizu, K., Lanckriet, G., and
Sch¨olkopf, B. Hilbert space embeddings and metrics on prob-
ability measures. J. Mach. Learn. Res., 11:1517–1561, 2010.

Sriperumbudur, B., Fukumizu, K., and Lanckriet, G. Universality,
characteristic kernels and RKHS embedding of measures. J.
Mach. Learn. Res., 12:2389–2410, 2011.

Steinwart, I. and Christmann, A.

Support Vector Machines.

Springer, 2008.

Welling, M. and Teh, Y.W. Bayesian learning via stochastic gra-
dient Langevin dynamics.
In Proc. of the 28th International
Conference on Machine Learning (ICML), pp. 681–688, 2011.

Williams, C.K.I. and Barber, D. Bayesian classiﬁcation with
IEEE Transactions on Pattern Analysis

Gaussian processes.
and Machine Intelligence, 20(12):1342–1351, 1998.

References

Andrieu, C. and Roberts, G.O. The pseudo-marginal approach
for efﬁcient Monte Carlo computations. Ann. Statist., 37(2):
697–725, 2009.

Andrieu, C. and Thoms, J. A tutorial on adaptive MCMC. Statis-

tics and Computing, 18(4):343–373, 2008.

Bache, K. and Lichman, M. UCI Machine Learning Repository,

2013. URL http://archive.ics.uci.edu/ml.

Baker, C. Joint measures and cross-covariance operators. Trans-
actions of the American Mathematical Society, 186:273–289,
1973.

Bakir, G., Weston, J., and Sch¨olkopf, B. Learning to ﬁnd pre-
In Advances in Neural Information Processing Sys-

images.
tems 16. MIT Press, 2003.

Berlinet, A. and Thomas-Agnan, C. Reproducing Kernel Hilbert

Spaces in Probability and Statistics. Kluwer, 2004.

Borgwardt, K. M., Gretton, A., Rasch, M. J., Kriegel, H.-P.,
Sch¨olkopf, B., and Smola, A. J. Integrating structured biologi-
cal data by kernel maximum mean discrepancy. Bioinformatics
(ISMB), 22(14):e49–e57, 2006.

Filippone, M. and Girolami, M. Pseudo-marginal Bayesian in-
ference for Gaussian Processes. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2014. doi: TPAMI.2014.
2316530.

Fukumizu, K., Bach, F. R., and Jordan, M. I. Dimensionality re-
duction for supervised learning with reproducing kernel Hilbert
spaces. J. Mach. Learn. Res., 5:73–99, 2004.

Gelman, A., Roberts, G. O., and Gilks, W. R. Efﬁcient Metropo-
In Bayesian statistics, 5 (Alicante, 1994),

lis jumping rules.
Oxford Sci. Publ., pp. 599–607. 1996.

Girolami, M. and Calderhead, B. Riemann manifold Langevin
and Hamiltonian Monte Carlo methods. Journal of the Royal
Statistical Society: Series B, 73(2):123–214, 2011.

Gretton, A., Borgwardt, K., Rasch, M., Sch¨olkopf, B., and Smola,
A. A kernel method for the two-sample problem. In Advances
in Neural Information Processing Systems 19, pp. 513–520,
2007.

Haario, H., Saksman, E., and Tamminen, J. Adaptive Proposal
Distribution for Random Walk Metropolis Algorithm. Comput.
Stat., 14(3):375–395, 1999.

Haario, H., Saksman, E., and Tamminen, J. An adaptive Metropo-

lis algorithm. Bernoulli, 7(2):223–242, 2001.

Murray, I. and Adams, R.P. Slice sampling covariance hyperpa-
In Advances in Neural

rameters of latent Gaussian models.
Information Processing Systems 23. 2012.

Roberts, G.O. and Rosenthal, J.S. Coupling and ergodicity of
J. Appl.

adaptive Markov chain Monte Carlo algorithms.
Probab., 44(2):458–475, 03 2007.

Roberts, G.O. and Stramer, O.

Langevin diffusions and
Metropolis-Hastings algorithms. Methodol. Comput. Appl.
Probab., 4:337–358, 2003.

A. Proofs

Kernel Adaptive Metropolis Hastings

Proposition 1. Let k be a differentiable positive deﬁnite kernel. Then

∇xk(x, x)

|x=y −

2

∇xk(x, y)

|x=y = 0.

Proof. Since k is a positive deﬁnite kernel there exists a Hilbert space
k(x, x′) =
ψ :
H →
to each y
the chain rule for

, such that
ϕ, where
h
(Rd, R) of τ at y, which
Rd associates a bounded linear operator from Rd to R (Steinwart & Christmann, 2008, Deﬁnition A.5.14). By

H
|x=y from the Fr´echet derivative Dτ (y)

iH. Consider ﬁrst the map τ : Rd
2
H . We can obtain
k

→ H
R, deﬁned by τ (x) = k(x, x). We write τ = ψ
◦

ϕ(x), ϕ(x′)
f
k

∇xk(x, x)

R, ψ(f ) =

and a feature map ϕ : Rd

∈ B

→

∈

Fr´echet derivatives (Steinwart & Christmann, 2008, Lemma A.5.15(b)), the value of Dτ (y) at some x′

Rd is

∈

[Dτ (y)] (x′) = [Dψ (ϕ(y))

Dϕ(y)] (x′),

◦

where Dϕ(y)
differentiable function (Steinwart & Christmann, 2008, Section 4.3). It is readily shown that Dψ [ϕ(y)] = 2
that

, R). The derivative Dϕ of the feature map exists whenever k is a
·iH, so

), and Dψ (ϕ(y))

ϕ(y),
h

(Rd,

(
H

∈ B

∈ B

H

Next, we consider the map κy(x) = k(x, y) =
a linear scalar function on

, Dψy (f ) =

H

[Dτ (y)] (x′) = 2

ϕ(y), [Dϕ(y)] (x′)
h
iH, i.e., κy = ψy ◦
ϕ(x), ϕ(y)
h
·iH. Again, by the chain rule:
ϕ(y),
h

iH .
ϕ where ψy(f ) =

[Dκy(y)] (x′) = [Dψy (ϕ(y))
=

◦

ϕ(y), [Dϕ(y)] (x′)
h

Dϕ(y)] (x′)
iH ,

f, ϕ(y)
h

iH. Since ψy is

2Dκy(y)) (x′) = 0, for all x′
and thus (Dτ (y)
derivatives can also be written as inner products with the gradients, (
(Dτ (y)

Rd, which proves the claim.

Rd, and we obtain equality of operators. Since Fr´echet
|x=y)⊤ x′ =

∇xk(x, x)

∇xk(x, y)

2Dκy(y)) (x′) = 0,

|x=y −

−

∈

2

x′
∀

∈

y) =

·|

N

(y, γ2I + ν2Mz,yHM ⊤

z,y).

−
Proposition 2. qz(

Proof. We start with

p(β)p(x∗

y, β) =

|

1
n+d

exp

−

(cid:18)

(2π)

exp

2 γdνn
1
2γ2 (x∗

y

−

1
2ν2 β⊤β

(cid:19)
Mz,yHβ)⊤ (x∗

−
(cid:18)
1
n+d

=

exp

(2π)

exp

2 γdνn
1
2

−

β⊤

(cid:18)

(cid:18)

(cid:18)

·

·

−
1
2γ2 (x∗
1
γ2 HM ⊤

−

(cid:18)
1
ν2 I +

−

y

−

−

Mz,yHβ)

(cid:19)

y)⊤ (x∗

y)

−

z,yMz,yH

β

(cid:19)

2
γ2 β⊤HM ⊤

z,y(x∗

y)

−

.

(cid:19)(cid:19)

(cid:19)

−

Now, we set

and application of the standard Gaussian integral

Σ−1 =

µ =

z,yMz,yH

1
ν2 I +
1
γ2 ΣHM ⊤

1
γ2 HM ⊤
z,y(x∗

y),

−

β⊤Σ−1β

2β⊤Σ−1µ

dβ =

exp

ˆ

1
2

−

−

(cid:18)
(2π)n/2√det Σ exp

(cid:0)

(cid:19)
(cid:1)
µ⊤Σ−1µ

1
2

(cid:18)

,

(cid:19)

This is just a d-dimensional Gaussian density where both the mean and covariance will, in general, depend on y. Let us
consider the exponent

leads to

Kernel Adaptive Metropolis Hastings

qz(x∗

y) =

|

√det Σ

exp

2 γdνn
1
2

(cid:18)
µ⊤Σ−1µ

d

(2π)

exp

·

(cid:18)

.

(cid:19)

1
2γ2 (x∗

−

−

y)⊤ (x∗

y)

−

(cid:19)

−

1
2γ2 (x∗
1
1
γ2 (x∗
2 (

−

−

y)⊤ (x∗

y) +

µ⊤Σ−1µ =

1
2

−

y)⊤ (x∗

y)

−

−

1
γ4 (x∗

−

−

y)⊤ Mz,yHΣHM ⊤

z,y (x∗

y)

=

−

)

1
2
n
γ2 Mz,yHΣHM ⊤

−

1

(x∗

y)⊤ R−1 (x∗

y)

,

−

−

o

R = γ2(I

= γ2

−

1
γ2 Mz,yHΣHM ⊤
γ2Σ−1

I + Mz,yH

z,y)−1

(cid:16)

(cid:0)

= γ2

I + Mz,yH

γ2
ν2 I
= γ2I + ν2Mz,yHM ⊤

 

(cid:18)

z,y.

(cid:19)

HM ⊤

z,yMz,yH

−1

HM ⊤
z,y

(cid:1)

(cid:17)

−
−1

HM ⊤
z,y

!

where R−1 = 1

γ2 (I

−

z,y). We can simplify the covariance R using the Woodbury identity to obtain:

Therefore, the proposal density is qz(

y) =

(y, γ2I + ν2Mz,yHM ⊤

z,y).

·|

N

B. Further details on synthetic experiments

Proposal contours for the Flower target. The d-dimensional ﬂower target

(r0, A, ω, σ) is given by

(x; r0, A, ω, σ) = exp

F

 − p

1 + x2
x2

2 −

r0 −

F
A cos (ωatan2 (x2, x1))
2σ2

(x3:d; 0, I).

! N

This distribution concentrates around the r0-circle with a periodic perturbation (with amplitude A and frequency ω) in the
ﬁrst two dimensions. For A = 0, we obtain a band around the r0-circle, which we term the ring target. Figure 6 gives the
contour plots of the MCMC Kameleon proposal distributions on two instances of the ﬂower target.

Convergence statistics for the Banana target. Figure 7 illustrates how the norm of the mean and quantile deviation
(shown for 0.5-quantile) for the strongly twisted Banana target decrease as a function of the number of iterations. This
shows that the trends observed in the main text persist along the evolution of the whole chain.

C. Principal Components Proposals

An alternative approach to the standard adaptive Metropolis, discussed in Andrieu & Thoms (2008, Algorithm 8), is to
m
extract m
j=1 from the estimated covariance matrix Σz and use the
(λj , vj)
}
{
proposal that takes form of a mixture of one-dimensional random walks along the principal eigendirections

d principal eigenvalue-eigenvector pairs

≤

qz (

y) =

·|

(y, ν2

j λjvjv⊤

j ).

ωjN

m

j=1
X

(7)

Kernel Adaptive Metropolis Hastings

Figure6. 95% contours (red) of proposal distributions evaluated at a number of points, for the ﬂower and the ring target. Underneath are
the density heatmaps, and the samples (blue) used to construct the proposals.

12

2

10
(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13)
8

X
ˆE

n
a
e
m
e
h
t

f
o
m
r
o
n

6

4

2

0

KAMH-LS
AM-LS
AM-FS
SM

e
c
n
e
r
e
f
f
i
d

e
l
i
t
n
a
u
q
-
5
.
0

0.12

0.10

0.08

0.06

0.04

0.02

0.00

0

KAMH-LS
AM-LS
AM-FS
SM

5000

10000

15000

20000

25000

30000

35000

40000

45000

5000

10000

15000

20000

25000

30000

35000

40000

45000

number of samples

number of samples

Figure7. Comparison of SM, AM-FS, AM-LS and KAMH-LS in terms of the norm of the estimated mean (left) and in terms of the
deviation from the 0.5-quantile (right) on the strongly twisted Banana distribution. The results are averaged over 20 chains for each
sampler. Error bars represent 80%-conﬁdence intervals.

Kernel Adaptive Metropolis Hastings

x∗ = y + ρνj

λjvj,

In other words, given the current chain state y, the j-th principal eigendirection is chosen with probability ωj (choice
ωj = λj /

m
l=1 λl is suggested), and the proposed point is

(8)

(9)

P

with ρ
∼ N
eigenvalue.

≤
i=1 ˜α(j)

n

i

(0, 1). Note that each eigendirection may have a different scaling factor νj in addition to the scaling with the

p

We can consider an analogous version of the update (8) performed in the RKHS

with m

n principal eigenvalue-eigenfunction pairs

, y) + ρνj

f = k(
·
(λj , vj)
}

{

λjvj ,

p
m
j=1. It is readily shown that the eigenfunctions vj =
⊤

−

, zi)

[k(
·

µz] lie in the subspace

˜α(j)
1
(cid:16)
are proportional to the eigenvector of the centered kernel matrix HKH, with normalization chosen so that
P
˜α(j)
has form
(cid:0)

(cid:17)
=
2
2 = 1 (so that the eigenfunctions have the unit RKHS norm). Therefore, the update (9)
(cid:13)
(cid:13)

z induced by z, and that the coefﬁcients vectors ˜α(j) =

HKH ˜α(j) = λj

˜α(j)
n
2
Hk

· · ·
vjk

˜α(j)

(cid:13)
(cid:13)

H

k

(cid:1)

⊤

n

f = k(
·

, y) +

β(j)
i

, zi)

[k(
·

−

µz] ,

i=1
X

˜α(j)

λj ˜α(j). But α(j) =

2
where β(j) = ρνj
2 =
2
2 = 1. Therefore, the appropriate scaling with eigenvalues is already included in the β-coefﬁcients, just like in
λj
the MCMC Kameleon, where the β-coefﬁcients are isotropic.
(cid:13)
(cid:13)

λj ˜α(j) are themselves the (unit norm) eigenvectors of HKH, as

Now, we can construct the MCMC PCA-Kameleon by simply substituting β-coefﬁcients with ρνjα(j), where j is the
selected eigendirection, and νj is the scaling factor associated to the j-th eigendirection. We have the following steps:

α(j)

p

p

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

1. Perform eigendecomposition of HKH to obtain the m

n eigenvectors

≤

m
j=1 .

αj}
{

2. Draw j

Discrete [ω1, . . . , ωm]

∼
(0, 1)

3. ρ

∼ N

4. x∗

y, ρ, j

|

∼ N

(y + ρνjMz,yHα(j), γ2I) (d

1 normal in the original space)

×

Similarly as before, we can simplify the proposal by integrating out the scale ρ of the moves in the RKHS.

Proposition 3. qz(

y) =

·|

Proof. We start with

m

j=1 ωjN

P

(y, γ2I + ν2

j Mz,yHα(j)

α(j)

⊤

HM ⊤

z,y).

(cid:0)

(cid:1)

p(ρ)p(x∗

y, ρ, j)

|

∝

exp

−

(cid:20)

y)⊤(x∗

1
2γ2 (x∗
1
2 ( 

−

1 +

ν2
j
γ2

y)

−

(cid:21)
⊤

(cid:16)

(cid:17)

exp

·

"−

By substituting

α(j)

HM ⊤

z,yMz,yHα(j)

ρ2

2ρ

−

νj
γ2

!

⊤

α(j)

HM ⊤

z,y (x∗

y)

.

)#

−

(cid:16)

(cid:17)

σ−2 = 1 +

α(j)

HM ⊤

z,yMz,yHα(j),

⊤

µ = σ2

(cid:16)

(cid:17)
α(j)

⊤

HM ⊤

z,y (x∗

(cid:18)

(cid:16)

(cid:17)

−

y)

,

(cid:19)

ν2
j
γ2
νj
γ2

p (x∗

y, j)

|

∝

exp

"−

1
γ2 (x∗

−

y)⊤(x∗

y)

−

−

ν2
j σ2
γ4

−

(x∗

y)⊤ Mz,yHα(j)

α(j)

⊤

HM ⊤

z,y (x∗

y)

−

)#

(cid:16)

(cid:17)

we integrate out ρ to obtain:

Kernel Adaptive Metropolis Hastings

1
2 (
1
2

(x∗

= exp

y)⊤ R−1 (x∗

−

−

(cid:20)
j σ2
ν2
γ2 Mz,yHα(j)

y)

−

(cid:21)

⊤

α(j)

HM ⊤
z,y

(cid:0)

(cid:1)

(cid:17)

where R−1 = 1
γ2
to obtain:

I

−

(cid:16)

R = γ2(I

−

= γ2

I +

j σ2
ν2
γ2 Mz,yHα(j)
(cid:16)
ν2
j σ2
γ2 Mz,yHα(j)

α(j)

HM ⊤

z,y)−1

⊤

(cid:17)

ν2
j σ2
γ2

1

−

 

(cid:16)

(cid:17)

= γ2

I +

ν2
j
γ2 Mz,yHα(j)

⊤

α(j)

HM ⊤
z,y

!





 

= γ2I + ν2

j Mz,yHα(j)

(cid:16)
α(j)

⊤

(cid:17)
HM ⊤

z,y.

(cid:16)
The claim follows after summing over the choice j of the eigendirection (w.p. ωj).

(cid:17)

. We can simplify the covariance R using the Woodbury identity

⊤

α(j)

HM ⊤

z,yMz,yHα(j)

−1

!

⊤

α(j)

(cid:16)

(cid:17)

HM ⊤

z,y



Kernel Adaptive Metropolis-Hastings

4
1
0
2
 
n
u
J
 

2
1
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
2
0
3
5
.
7
0
3
1
:
v
i
X
r
a

Dino Sejdinovic⋆
Heiko Strathmann⋆
Maria Lomeli Garcia⋆
Christophe Andrieu‡
Arthur Gretton⋆
⋆Gatsby Unit, CSML, University College London, UK and ‡School of Mathematics, University of Bristol, UK

DINO@GATSBY.UCL.AC.UK
UCABHST@GATSBY.UCL.AC.UK
MLOMELI@GATSBY.UCL.AC.UK
C.ANDRIEU@BRISTOL.AC.UK
ARTHUR.GRETTON@GMAIL.COM

Abstract

A Kernel Adaptive Metropolis-Hastings algo-
rithm is introduced, for the purpose of sampling
from a target distribution with strongly nonlin-
ear support. The algorithm embeds the trajec-
tory of the Markov chain into a reproducing ker-
nel Hilbert space (RKHS), such that the fea-
ture space covariance of the samples informs
the choice of proposal. The procedure is com-
putationally efﬁcient and straightforward to im-
plement, since the RKHS moves can be inte-
grated out analytically: our proposal distribu-
tion in the original space is a normal distribution
whose mean and covariance depend on where
the current sample lies in the support of the tar-
get distribution, and adapts to its local covari-
ance structure. Furthermore, the procedure re-
quires neither gradients nor any other higher or-
der information about the target, making it par-
ticularly attractive for contexts such as Pseudo-
Marginal MCMC. Kernel Adaptive Metropolis-
Hastings outperforms competing ﬁxed and adap-
tive samplers on multivariate, highly nonlinear
target distributions, arising in both real-world
and synthetic examples.

1. Introduction

The choice of the proposal distribution is known to be
crucial for the design of Metropolis-Hastings algorithms,
and methods for adapting the proposal distribution to in-
crease the sampler’s efﬁciency based on the history of the
Markov chain have been widely studied. These methods
often aim to learn the covariance structure of the target
distribution, and adapt the proposal accordingly. Adaptive
MCMC samplers were ﬁrst studied by Haario et al. (1999;

Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copy-
right 2014 by the author(s).

2001), where the authors propose to update the proposal
distribution along the sampling process. Based on the chain
history, they estimate the covariance of the target distribu-
tion and construct a Gaussian proposal centered at the cur-
rent chain state, with a particular choice of the scaling fac-
tor from Gelman et al. (1996). More sophisticated schemes
are presented by Andrieu & Thoms (2008), e.g., adaptive
scaling, component-wise scaling, and principal component
updates.

While these strategies are beneﬁcial for distributions that
show high anisotropy (e.g., by ensuring the proposal uses
the right scaling in all principal directions), they may still
suffer from low acceptance probability and slow mixing
when the target distributions are strongly nonlinear, and
the directions of large variance depend on the current lo-
cation of the sampler in the support. In the present work,
we develop an adaptive Metropolis-Hastings algorithm in
which samples are mapped to a reproducing kernel Hilbert
space, and the proposal distribution is chosen according to
the covariance in this feature space (Sch¨olkopf et al., 1998;
Smola et al., 2001). Unlike earlier adaptive approaches,
the resulting proposal distributions are locally adaptive in
input space, and oriented towards nearby regions of high
density, rather than simply matching the global covari-
ance structure of the distribution. Our approach com-
bines a move in the feature space with a stochastic step
towards the nearest input space point, where the feature
space move can be analytically integrated out. Thus,
the implementation of the procedure is straightforward:
the proposal is simply a multivariate Gaussian in the in-
put space, with location-dependent covariance which is
informed by the feature space representation of the tar-
get. Furthermore, the resulting Metropolis-Hastings sam-
pler only requires the ability to evaluate the unnormal-
ized density of the target (or its unbiased estimate, as
in Pseudo-Marginal MCMC of Andrieu & Roberts, 2009),
and no gradient evaluation is needed, making it applicable
to situations where more sophisticated schemes based on
Hamiltonian Monte Carlo (HMC) or Metropolis Adjusted

Kernel Adaptive Metropolis Hastings

Langevin Algorithms (MALA) (Roberts & Stramer, 2003;
Girolami & Calderhead, 2011) cannot be applied.

We begin our presentation in Section 2, with a brief
overview of existing adaptive Metropolis approaches; we
also review covariance operators in the RKHS. Based on
these operators, we describe a sampling strategy for Gaus-
sian measures in the RKHS in Section 3, and introduce a
cost function for constructing proposal distributions.
In
Section 4, we outline our main algorithm, termed Kernel
Adaptive Metropolis-Hastings (MCMC Kameleon). We
provide experimental comparisons with other ﬁxed and
adaptive samplers in Section 5, where we show superior
performance in the context of Pseudo-Marginal MCMC
for Bayesian classiﬁcation, and on synthetic target distri-
butions with highly nonlinear shape.

2. Background

= Rd be
Adaptive Metropolis Algorithms. Let
the domain of interest, and denote the unnormalized
target density on
let Σt =
Σt(x0, x1, . . . , xt−1) denote an estimate of the covariance
matrix of the target density based on the chain history
t−1
i=0. The original adaptive Metropolis at the current
xi}
{
state of the chain state xt = y uses the proposal

by π. Additionally,

X

X

qt(

y) =

(y, ν2Σt),

(1)

·|

N
where ν = 2.38/√d is a ﬁxed scaling factor from
Gelman et al. (1996). This choice of scaling factor was
shown to be optimal (in terms of efﬁciency measures) for
the usual Metropolis algorithm. While this optimality re-
sult does not hold for Adaptive Metropolis, it can never-
theless be used as a heuristic. Alternatively, the scale ν
can also be adapted at each step as in Andrieu & Thoms
(2008, Algorithm 4) to obtain the acceptance rate from
Gelman et al. (1996), a∗ = 0.234.

X

X × X →

Embeddings

and Covariance Opera-
RKHS
tors. According to the Moore-Aronszajn theorem
(Berlinet & Thomas-Agnan, 2004, p. 19), for every sym-
R,
metric, positive deﬁnite function (kernel) k :
there is an associated reproducing kernel Hilbert space
with reproducing kernel
Hk of real-valued functions on
k. The map ϕ :
, x) is called
X → Hk, ϕ : x
the canonical feature map of k. This feature map or
embedding of a single point can be extended to that of
its kernel embedding is
a probability measure P on
an element µP ∈ Hk, given by µP =
, x) dP (x)
k(
·
(Berlinet & Thomas-Agnan, 2004; Fukumizu et al., 2004;
Smola et al., 2007). If a measurable kernel k is bounded,
it is straightforward to show using the Riesz represen-
tation theorem that the mean embedding µk(P ) exists
. For many interesting
for all probability measures on

k(
·

7→

X

´

:

X

7→

bounded kernels k, including the Gaussian, Laplacian and
inverse multi-quadratics, the kernel embedding P
µP
is injective. Such kernels are said to be characteristic
(Sriperumbudur et al., 2010; 2011), since each distribution
is uniquely characterized by its embedding (in the same
way that every probability distribution has a unique
characteristic function). The kernel embedding µP is the
representer of expectations of smooth functions w.r.t. P ,
i.e.,
f (x)dP (x). Given samples
∈ Hk,
f
f, µP iHk
h
∀
n
z =
P , the embedding of the empirical measure
zi}
i=1 ∼
{
n
is µz = 1
, zi).
i=1 k(
n
·
Hk → Hk for a
Next, the covariance operator CP :
probability measure P is given by CP =
k(
⊗
·
´
µP (Baker, 1973; Fukumizu et al.,
k(
µP ⊗
·
∈ Hk the tensor product is de-
2004), where for a, b, c
a. The covariance oper-
ﬁned as (a
b, c
b)c =
⊗
ator has the property that
=
EP f EP g.
EP (f g)

f, CP g
h

∈ Hk,

iHk
f, g
∀

, x) dP (x)

iHk

, x)

P

=

−

´

h

−

1
n

⊗

⊗

µz

P

, zi)

, zi)

k(
·

n
i=1 k(
·

Our approach is based on the idea that the nonlinear sup-
port of a target density may be learned using Kernel Princi-
pal Component Analysis (Kernel PCA) (Sch¨olkopf et al.,
1998; Smola et al., 2001),
this being linear PCA on
the empirical covariance operator in the RKHS, Cz =
1
µz, computed on the
n
−
sample z deﬁned above. The empirical covariance op-
erator behaves as expected: applying the tensor prod-
uct deﬁnition gives
n
1
i=1 f (zi)
n

= 1
n
−
. By analogy with algo-
rithms which use linear PCA directions to inform M-H pro-
(cid:0)
posals (Andrieu & Thoms, 2008, Algorithm 8), nonlinear
PCA directions can be encoded in the proposal construc-
tion, as described in Appendix C. Alternatively, one can
focus on a Gaussian measure on the RKHS determined by
the empirical covariance operator Cz rather than extract-
ing its eigendirections, which is the approach we pursue in
this contribution. This generalizes the proposal (1), which
considers the Gaussian measure induced by the empirical
covariance matrix on the original space.

f, Czg
iHk
h
n
i=1 g(zi)

n
i=1 f (zi)g(zi)

(cid:1) (cid:0)

P

P

P

(cid:1)

3. Sampling in RKHS

We next describe the proposal distribution at iteration t of
the MCMC chain. We will assume that a subset of the chain
n
history, denoted z =
1, is available. Our
i=1, n
proposal is constructed by ﬁrst considering the samples in
the RKHS associated to the empirical covariance operator,
and then performing a gradient descent step on a cost func-
tion associated with those samples.

zi}
{

≤

−

t

Gaussian Measure of the Covariance Operator. We
will work with the Gaussian measure on the RKHS
Hk
n
with mean k(
zi}
i=1
·
is the subset of the chain history. While there is no analogue

, y) and covariance ν2Cz, where z =

{

Kernel Adaptive Metropolis Hastings

Samples {zi}200
i=1

Current position y

βiβj (k(
·

, zi)

−

µz)

(k(
·

⊗

, zj)

−

µz)

Figure1. Heatmaps (white denotes large) and gradients of g(x)
for two samples of β and ﬁxed z.

f

(cid:16)

−

−

1
2ν2

k(
·

N
k(
·

, y), C−1

, y), ν2Cz)

(f ; k(
·
Hk

of a Lebesgue measure in an inﬁnite dimensional RKHS, it
is instructive (albeit with some abuse of notation) to denote
this measure in the “density form”

∝
. As Cz is
z (f
exp
a ﬁnite-rank operator, this measure is supported only on a
ﬁnite-dimensional afﬁne space k(
z =
, y) +
·
n
span
i=1 is the subspace spanned by the canonical
features of z. It can be shown that a sample from this mea-
sure has the form f = k(
µz] ,
, y) +
·
n I) is isotropic. Indeed, to see that f
where β
has the correct covariance structure, note that:

(cid:17)
(cid:11)
z, where
H

n
i=1 βi [k(
·

, zi)
}

(0, ν2

k(
·

∼ N

, zi)

, y))

P

H

−

−

{

(cid:10)

, y))

(f

k(
·

−

⊗

, y))]

−

k(
·

n

n

E [(f

= E




ν2
n

=

i=1
X
n

j=1
X

i=1
X

, zi)

(k(
·

−

µz)

(k(
·

⊗

, zi)

−

µz) = ν2Cz.





and a Gaussian Process

Due to the equivalence in the RKHS between a
Gaussian measure
(GP)
(Berlinet & Thomas-Agnan, 2004, Ch.
4), we can
think of the RKHS samples f as trajectories of the GP with
mean m(x) = k(x, y) and covariance function
κ(x, x′) = cov [f (x), f (x′)]
ν2
n

µz(x)) (k(x′, zi)

(k(x, zi)

=

µz(x′)) .

n

−

−

i=1
X

The covariance function κ of this GP is therefore the kernel
k convolved with itself with respect to the empirical mea-
sure associated to the samples z, and draws from this GP
therefore lie in a smaller RKHS; see Saitoh (1997, p. 21)
for details.

P

, zi)

, y) +

n
i=1 βi [k(
·

Obtaining Target Samples through Gradient Descent.
We have seen how to obtain the RKHS sample f =
µz] from the Gaussian mea-
k(
−
·
sure in the RKHS. This sample does not in general have a
= Rd;
corresponding pre-image in the original domain
X
i.e., there is no point x∗
, x∗). If
such that f = k(
·
∈ X
there were such a point, then we could use it as a proposal
in the original domain. Therefore, we are ideally looking
, x∗) is
for a point x∗
close to f in the RKHS norm. We consider the optimization
problem

whose canonical feature map k(
·

∈ X

arg min
x∈X

kk (·, x) − f k

2
Hk

=

In general, this is a non-convex minimization problem, and
may be difﬁcult to solve (Bakir et al., 2003). Rather than
solving it for every new vector of coefﬁcients β, which
would lead to an excessive computational burden for ev-
ery proposal made, we simply make a single descent step
along the gradient of the cost function,

g(x) = k(x, x)

2k(x, y)

2

βi [k(x, zi)

µz(x)] ,

−

−

−

n

i=1
X

(2)

i.e., the proposed new point is

x∗ = y

η

∇xg(x)

|x=y + ξ,

−

where η is a gradient step size parameter and ξ

∼
(0, γ2I) is an additional isotropic ’exploration’ term
It will be useful to split the
|x=y =
|x=y −

N
after the gradient step.
scaled gradient at y into two terms as η
Mz,yHβ), where ay =
η (ay −
∇xk(x, y)
2
Mz,y = 2 [

∇xg(x)
∇xk(x, x)

|x=y,

(3)

∇xk(x, z1)

n matrix, and H = I

|x=y, . . . ,
−

∇xk(x, zn)
×

1
n 1n×n is the n

|x=y]
n centering

is a d
×
matrix.

Figure 1 plots g(x) and its gradients for several samples of
β-coefﬁcients, in the case where the underlying z-samples
are from the two-dimensional nonlinear Banana target dis-
tribution of Haario et al. (1999). It can be seen that g may
have multiple local minima, and that it varies most along
the high-density regions of the Banana distribution.

4. MCMC Kameleon Algorithm

4.1. Proposal Distribution

arg min

k(x, x) − 2k(x, y) − 2

βi [k(x, zi) − µz(x)]

.

x∈X (

We now have a recipe to construct a proposal that is able to
adapt to the local covariance structure for the current chain

)

n

i=1
X

Kernel Adaptive Metropolis Hastings

MCMC Kameleon
Input: unnormalized target π, subsample size n, scaling
∞
t=0, kernel k,
parameters ν, γ, adaptation probabilities

pt}

{

At iteration t + 1,

•

1. With probability pt, update a random subsample

min(n,t)
i=1

z =

zi}

xi}
{
2. Sample proposed point x∗ from qz(
·|

of the chain history

{

(xt, γ2I + ν2Mz,xt HM ⊤
(3) and H = I

N
given in Eq.
centering matrix,

t−1
i=0,
xt) =
z,xt), where Mz,xtis
1
n 1n×n is the

−

3. Accept/Reject with the Metropolis-Hastings ac-

ceptance probability A(xt, x∗) in Eq. (4),

xt+1 =

x∗, w.p. A(xt, x∗),
xt, w.p. 1

A(xt, x∗).

(

−

state y. This proposal depends on a subset of the chain
history z, and is denoted by qz(
y). While we will later
simplify this proposal by integrating out the moves in the
RKHS, it is instructive to think of the proposal generating
process as:

·|

1. Sample β
ﬁcients).

∼ N

(0, ν2I) (n

1 normal of RKHS coef-

×

•

This represents an RKHS sample f = k(
·

, y) +
µz] which is the goal of the

n
i=1 βi [k(
·

−
cost function g(x).
P

, zi)

2. Move along the gradient of g:

x∗ = y

−

η

∇xg(x)

|x=y + ξ.

•

This gives a proposal x∗
ηMz,yHβ, γ2I) (d
space).

×

y, β

ηay +
1 normal in the original

∼ N

(y

−

|

2

|x=y −

∇xk(x, x)

Our ﬁrst step in the derivation of the explicit proposal den-
sity is to show that as long as k is a differentiable positive
deﬁnite kernel, the term ay vanishes.
Proposition 1. Let k be a differentiable positive deﬁnite
|x=y = 0.
kernel. Then ay =
Since ay = 0, the gradient step size η always appears to-
gether with β, so we merge η and the scale ν of the β-
coefﬁcients into a single scale parameter, and set η = 1
henceforth. Furthermore, since both p(β) and pz(x∗
y, β)
are multivariate Gaussian densities, the proposal density
qz(x∗
y, β)dβ can be computed analyt-
ically. We therefore get the following closed form expres-
sion for the proposal distribution.

∇xk(x, y)

p(β)pz(x∗

y) =

´

|

|

|

Proposition 2. qz(

y) =

(y, γ2I + ν2Mz,yHM ⊤

z,y).

·|

N

Figure2. 95% contours (red) of proposal distributions evaluated
at a number of points, for the ﬁrst two dimensions of the banana
target of Haario et al. (1999). Underneath is the density heatmap,
and the samples (blue) used to construct the proposals.

Proofs of the above Propositions are given in Appendix A.

With the derived proposal distribution, we proceed with the
standard Metropolis-Hastings accept/reject scheme, where
the proposed sample x∗ is accepted with probability

A(xt, x∗) = min

1,

π(x∗)qz(xt|
π(xt)qz(x∗
|

x∗)
xt)

,

(4)

(cid:27)

(cid:26)
giving rise to the MCMC Kameleon Algorithm. Note that
each π(x∗) and π(xt) could be replaced by their unbi-
ased estimates without impacting the invariant distribution
(Andrieu & Roberts, 2009).
The constructed family of proposals encodes local struc-
ture of the target distribution, which is learned based on the
subsample z. Figure 2 depicts the regions that contain 95%
y) at various
of the mass of the proposal distribution qz(
states y for a ﬁxed subsample z, where the Banana target
is used (details in Section 5). More examples of proposal
contours can be found in Appendix B.

·|

4.2. Properties of the Algorithm

·|

and convergence. MCMC
The update
schedule
n
Kameleon requires a subsample z =
i=1 at each
zi}
{
iteration of the algorithm, and the proposal distribution
y) is updated each time a new subsample z is obtained.
qz(
It is well known that a chain which keeps adapting the
proposal distribution need not converge to the correct
target (Andrieu & Thoms, 2008). To guarantee conver-
∞
gence, we introduce adaptation probabilities
t=0,
pt}
{
, and at iteration
such that pt →
t we update the subsample z with probability pt. As
adaptations occur with decreasing probability, Theorem 1
of Roberts & Rosenthal (2007) implies that the resulting
algorithm is ergodic and converges to the correct target.
Another straightforward way to guarantee convergence is
n
to ﬁx the set z =
i=1 after a “burn-in” phase; i.e., to
stop adapting Roberts & Rosenthal (2007, Proposition 2).
In this case, a “burn-in” phase is used to get a rough sketch
of the shape of the distribution: the initial samples need not

∞
t=1 pt =

zi}
{

0 and

P

∞

Kernel Adaptive Metropolis Hastings

come from a converged or even valid MCMC chain, and it
sufﬁces to have a scheme with good exploratory properties,
e.g., Welling & Teh (2011).
In MCMC Kameleon, the
term γ allows exploration in the initial iterations of the
chain (while the subsample z is still not informative about
the structure of the target) and provides regularization of
the proposal covariance in cases where it might become
ill-conditioned. Intuitively, a good approach to setting γ is
to slowly decrease it with each adaptation, such that the
learned covariance progressively dominates the proposal.

In Haario et al. (2001), the
Symmetry of the proposal.
proposal distribution is asymptotically symmetric due to
the vanishing adaptation property. Therefore, the authors
compute the standard Metropolis acceptance probability. In
our case, the proposal distribution is a Gaussian with mean
at the current state of the chain xt = y and covariance
γ2I + ν2Mz,yHM ⊤
z,y, where Mz,y depends both on the
n
current state y and a random subsample z =
i=1 of the
t−1
chain history
i=0. This proposal distribution is never
symmetric (as covariance of the proposal always depends
on the current state of the chain), and therefore we use the
Metropolis-Hastings acceptance probability to reﬂect this.

xi}
{

zi}
{

Relationship to MALA and Manifold MALA. The
Metropolis Adjusted Langevin Algorithm (MALA) algo-
rithm uses information about the gradient of the log-target
density at the current chain state to construct a proposed
point for the Metropolis step. Our approach does not re-
quire that the log-target density gradient be available or
computable. Kernel gradients in the matrix Mz,y are easily
obtained for commonly used kernels, including the Gaus-
sian kernel (see section 4.3), for which the computational
complexity is equal to evaluating the kernel itself. More-
over, while standard MALA simply shifts the mean of the
proposal distribution along the gradient and then adds an
isotropic exploration term, our proposal is centered at the
current state, and it is the covariance structure of the pro-
posal distribution that coerces the proposed points to be-
It would
long to the high-density regions of the target.
be straightforward to modify our approach to include a
drift term along the gradient of the log-density, should
such information be available, but it is unclear whether
this would provide additional performance gains. Fur-
ther work is required to elucidate possible connections be-
tween our approach and the use of a preconditioning ma-
trix (Roberts & Stramer, 2003) in the MALA proposal; i.e.,
where the exploration term is scaled with appropriate met-
ric tensor information, as in Riemannian manifold MALA
(Girolami & Calderhead, 2011).

4.3. Examples of Covariance Structure for Standard

Kernels

The proposal distributions in MCMC Kameleon are depen-
dant on the choice of the kernel k. To gain intuition re-

garding their covariance structure, we give two examples
below.

a

In

of
linear
obtain Mz,y
= 2Z⊤,

ker-
Linear kernel.
the
case
nel k(x, x′)
x⊤x′, we
=
=
∇xx⊤z1|x=y, . . . ,
∇xx⊤zn|x=y
so the
2
(y, γ2I + 4ν2Z⊤HZ);
proposal is given by qz(
y) =
(cid:3)
(cid:2)
N
thus, the proposal simply uses the scaled empirical co-
variance Z⊤HZ just like standard Adaptive Metropolis
(Haario et al., 1999), with an additional isotropic explo-
ration component, and depends on y only through the
mean.

·|

Gaussian kernel.
k(x, x′) = exp

′

In the case of a Gaussian kernel
∇xk(x, x′) =

x−x
k
2σ2

, since

−

k

2

2

(cid:18)

(cid:19)

1

x), we obtain

σ2 k(x, x′)(x′
2
σ2 [k(y, z1)(z1 −

Mz,y =

−

y), . . . , k(y, zn)(zn −

y)] .

Consider how this encodes the covariance structure of the
target distribution:

Rij = γ

δij

2

+

−

4ν 2(n − 1)
σ4n

n

a=1
X

4ν 2
σ4n

a6=b
X

[k(y, za)]

(za,i − yi)(za,j − yj)

2

k(y, za)k(y, zb)(za,i − yi)(zb,j − yj). (5)

As the ﬁrst two terms dominate, the previous points za
which are close to the current state y (for which k(y, za)
is large) have larger weights, and thus they have more in-
ﬂuence in determining the covariance of the proposal at y.

′

′

ϑ

k

k

(cid:18)

k2

Γ(ϑ)

Kϑ

x−x
ρ

x−x
ρ

In the Mat´ern family of kernels

Mat´ern kernel.
kϑ,ρ(x, x′) = 21−ϑ
Kϑ is the modiﬁed Bessel function of the second kind, we
obtain a form of the covariance structure very similar to
∇xkϑ,ρ(x, x′) =
that of the Gaussain kernel. In this case,
2ρ2(ϑ−1) kϑ−1,ρ(x, x′)(x′
x), so the only difference (apart
from the scalings) to (5) is that the weights are now deter-
mined by a “rougher” kernel kϑ−1,ρ of the same family.

, where

k2

−

(cid:19)

(cid:19)

(cid:18)

1

5. Experiments

·|

N

y) =

In the experiments, we compare the following samplers:
(SM) Standard Metropolis with the isotropic proposal
(y, ν2I) and scaling ν = 2.38/√d, (AM-
q(
FS) Adaptive Metropolis with a learned covariance ma-
trix and ﬁxed scaling ν = 2.38/√d, (AM-LS) Adaptive
Metropolis with a learned covariance matrix and scaling
learned to bring the acceptance rate close to α∗ = 0.234 as
described in Andrieu & Thoms (2008, Algorithm 4), and
(KAMH-LS) MCMC Kameleon with the scaling ν learned

Kernel Adaptive Metropolis Hastings

0

−1

−2

−3

−4

7
θ

−5

−6

−5

−4

−2

−1

0

−3

θ2

Figure3. Dimensions 2 and 7 of the marginal hyperparameter
posterior on the UCI Glass dataset

in the same fashion (γ was ﬁxed to 0.2), and which also
stops adapting the proposal after the burn-in of the chain
(in all experiments, we use a random subsample z of size
n = 1000, and a Gaussian kernel with bandwidth se-
lected according to the median heuristic). We consider the
following nonlinear targets: (1) the posterior distribution
of Gaussian Process (GP) classiﬁcation hyperparameters
(Filippone & Girolami, 2014) on the UCI glass dataset, and
(2) the synthetic banana-shaped distribution of Haario et al.
(1999) and a ﬂower-shaped disribution concentrated on a
circle with a periodic perturbation.

5.1. Pseudo-Marginal MCMC for GP Classiﬁcation

In the ﬁrst experiment, we illustrate usefulness of the
MCMC Kameleon sampler in the context of Bayesian clas-
siﬁcation with GPs (Williams & Barber, 1998). Consider
the joint distribution of latent variables f , labels y (with
covariate matrix X), and hyperparameters θ, given by

p(f , y, θ) = p(θ)p(f

θ)p(y

f ),

|

|

′

1

θ

}

(cid:16)

−

P

(0,

1, 1

∼ N

D
d=1

fi) =

θ) = exp

(xi,d−x
ℓ2
d

Kθ), with

i.e.,
1−exp(−yifi) where yi ∈ {−

where f
Kθ modeling the covariance
|
between latent variables evaluated at the input covariates:
j,d)2
1
Kθ)ij = κ(xi, x′
(
j|
2
and θd = log ℓ2
d. We restrict our attention to the bi-
(cid:17)
the likelihood is given by
nary logistic classiﬁer;
. We pursue
p(yi|
a fully Bayesian treatment, and estimate the posterior of
the hyperparameters θ. As observed by Murray & Adams
(2012), a Gibbs sampler on p(θ, f
y), which samples from
p(f
f , y)
is extremely sharp, drastically limiting the amount that
f , y. On the other hand,
any Markov chain can update θ
if we directly consider the marginal posterior p(θ
∝
p(y
θ)p(θ) of the hyperparameters, a much less peaked
distribution can be obtained. However, the marginal like-
lihood p(y
θ) is intractable for non-Gaussian likelihoods
p(y
f ), so it is not possible to analytically integrate out
the latent variables. Recently developed pseudo-marginal
MCMC methods (Andrieu & Roberts, 2009) enable exact

f , y) in turn, is problematic, as p(θ

θ, y) and p(θ

y)

|

|

|

|

|

|

|

|

|

inference on this problem (Filippone & Girolami, 2014),
by replacing p(y

θ) with an unbiased estimate

|

(6)

nimp

ˆp(y

θ) :=

|

1
nimp

p(y

f (i))

|

p(f (i)
q(f (i)

,

θ)
|
θ)
|

|

|

|

|

(cid:8)

∝

q(f

f (i)

y, θ)

f )p(f

nimp
i=1 ∼

i=1
X
where
θ) are nimp importance sam-
ples. In Filippone & Girolami (2014), the importance dis-
(cid:9)
tribution q(f
θ) is chosen as the Laplacian or as the Ex-
|
pectation Propagation (EP) approximation of p(f
p(y
θ), leading to state-of-the-art results.
We consider the UCI Glass dataset (Bache & Lichman,
2013), where classiﬁcation of window against non-window
glass is sought. Due to the heterogeneous structure of each
of the classes (i.e., non-window glass consists of contain-
ers, tableware and headlamps), there is no single consistent
set of lengthscales determining the decision boundary, so
one expects the posterior of the covariance bandwidths θd
to have a complicated (nonlinear) shape. This is illustrated
by the plot of the posterior projections to the dimensions
2 and 7 (out of 9) in Figure 3. Since the ground truth
for the hyperparameter posterior is not available, we ini-
tially ran 30 Standard Metropolis chains for 500,000 itera-
tions (with a 100,000 burn-in), kept every 1000-th sample
in each of the chains, and combined them. The resulting
samples were used as a benchmark, to evaluate the perfor-
mance of shorter single-chain runs of SM, AM-FS, AM-
LS and KAMH-LS. Each of these algorithms was run for
100,000 iterations (with a 20,000 burnin) and every 20-th
sample was kept. Two metrics were used in evaluating the
performance of the four samplers, relative to the large-scale
was computed
benchmark. First, the distance
ˆµθ −
between the mean ˆµθ estimated from each of the four sam-
(cid:13)
pler outputs, and the mean µb
θ on the benchmark sample
(cid:13)
(Fig. 4, left), as a function of sample size. Second, the
MMD (Borgwardt et al., 2006; Gretton et al., 2007) was
computed between each sampler output and the bench-
)3;
mark sample, using the polynomial kernel (1 +
i
i.e., the comparison was made in terms of all mixed mo-
ments of order up to 3 (Fig. 4, right). The ﬁgures indicate
that KAMH-LS approximates the benchmark sample bet-
ter than the competing approaches, where the effect is es-
pecially pronounced in the high order moments, indicating
that KAMH-LS thoroughly explores the distribution sup-
port in a relatively small number of samples.
We emphasise that, as for any pseudo-marginal MCMC
scheme, neither the likelihood itself, nor any higher-
the marginal posterior target
order information about
y), are available.
This makes HMC or MALA
p(θ
based approaches such as (Roberts & Stramer, 2003;
Girolami & Calderhead, 2011) unsuitable for this problem,
so it is very difﬁcult to deal with strongly nonlinear pos-
terior targets.
In contrast, as indicated in this example,
the MCMC Kameleon scheme is able to effectively sample

θ, θ′
h

µb
θ

(cid:13)
(cid:13)

2

|

Kernel Adaptive Metropolis Hastings

KAMH-LS
AM-LS
AM-FS
SM

2

bθ
µ
−

θ
ˆµ

e
c
n
a
t
s
i
d
n
a
e
m

0.50

0.45

(cid:13)(cid:13)
0.40

0.35
(cid:13)(cid:13)

0.30

0.25

0.20

0.15

0.10

0.5
0.4
0.3
0.2
0.1
0.0

1.0

0.8

0.6

0.4

0.2

0.0

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

KAMH-LS
AM-LS
AM-FS
SM

e
l
p
m
a
s
k
r
a
m
h
c
n
e
b

e
h
t

m
o
r
f

D
M
M

45

40

35

30

25

20

15

10

5

B

B

1000

2000

3000

4000

5000

1000

2000

3000

4000

5000

number of samples

number of samples

Figure4. The comparison of SM, AM-FS, AM-LS and KAMH-LS in terms of the distance between the estimated mean and the mean
on the benchmark sample (left) and in terms of the maximum mean discrepancy to the benchmark sample (right). The results are
averaged over 30 chains for each sampler. Error bars represent 80%-conﬁdence intervals.

Moderately twisted 8-dimensional

(0.03, 100) target; iterations: 40000, burn-in: 20000

SM

AM-FS

AM-LS

KAMH-LS

Accept ∈ [0, 1]

||ˆE[X]||/10

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Strongly twisted 8-dimensional

(0.1, 100) target; iterations: 80000, burn-in: 40000

SM

AM-FS

AM-LS

KAMH-LS

Accept ∈ [0, 1]

||ˆE[X]||/10

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

8-dimensional

(10, 6, 6, 1) target; iterations: 120000, burn-in: 60000

F

SM

AM-FS

AM-LS

KAMH-LS

Accept ∈ [0, 1]

||ˆE[X]||/10

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Figure5. Results for three nonlinear targets, averaged over 20 chains for each sampler. Accept is the acceptance rate scaled to the
interval [0, 1]. The norm of the mean ||ˆE[X]|| is scaled by 1/10 to ﬁt into the ﬁgure scalling, and the bars over the 0.1, . . . , 0.9-quantiles
represent the deviation from the exact quantiles, scaled by 10; i.e., 0.1 corresponds to 1% deviation. Error bars represent 80%-conﬁdence
intervals.

Kernel Adaptive Metropolis Hastings

from such nonlinear targets, and outperforms the vanilla
Metropolis methods, which are the only competing choices
in the pseudo-marginal context.

In addition, since the bulk of the cost for pseudo-marginal
MCMC is in importance sampling in order to obtain the
acceptance ratio, the additional cost imposed by KAMH-
LS is negligible. Indeed, we observed that there is an in-
crease of only 2-3% in terms of effective computation time
in comparison to all other samplers, for the chosen size of
the chain history subsample (n = 1000).

5.2. Synthetic examples

∼ N

In Haario et al. (1999),

(0, Σ) be a multivariate normal in d

the following
Banana target.
family of nonlinear target distributions is considered. Let
2 dimen-
X
sions, with Σ = diag(v, 1, . . . , 1), which undergoes the
transformation X
v),
1 −
(b, v). It is
and Yi = Xi for i
clear that EY = 0, and that

Y , where Y2 = X2 + b(X 2

→
= 2. We will write Y

≥

(y; b, v) =

(y1; 0, v)

B

N

(y2; b(y2

v), 1)

1−

N

(yj; 0, 1).

∼ B
d

N

j=3
Y

Flower target. The second target distribution we con-
sider is the d-dimensional ﬂower target
(r0, A, ω, σ),
with

F

F(x; r0, A, ω, σ) =

1 + x2
x2

2 − r0 − A cos (ωatan2 (x2, x1))

exp

−

 

2σ2

p
d

j=3
Y

×

N (xj; 0, 1).

This distribution concentrates around the r0-circle with a
periodic perturbation (with amplitude A and frequency ω)
in the ﬁrst two dimensions.

In these examples, exact quantile regions of the targets
can be computed analytically, so we can directly assess
performance without the need to estimate distribution dis-
tances on the basis of samples (i.e., by estimating MMD to
the benchmark sample). We compute the following mea-
sures of performance (similarly as in Haario et al. (1999);
Andrieu & Thoms (2008)) based on the chain after burn-in:
average acceptance rate, norm of the empirical mean (the
true mean is by construction zero for all targets), and the
deviation of the empirical quantiles from the true quantiles.
We consider 8-dimensional target distributions: the mod-
(0.03, 100) banana target (Figure 5, top)
erately twisted
and the strongly twisted
(0.1, 100) banana target (Figure
5, middle) and
(10, 6, 6, 1) ﬂower target (Figure 5, bot-
tom).

F

B

B

The results show that MCMC Kameleon is superior to the
competing samplers. Since the covariance of the proposal

adapts to the local structure of the target at the current chain
state, as illustrated in Figure 2, MCMC Kameleon does
not suffer from wrongly scaled proposal distributions. The
result is a signiﬁcantly improved quantile performance in
comparison to all competing samplers, as well as a com-
parable or superior norm of the empirical mean. SM has
a signiﬁcantly larger norm of the empirical mean, due to
its purely random walk behavior (e.g., the chain tends to
get stuck in one part of the space, and is not able to traverse
both tails of the banana target equally well). AM with ﬁxed
scale has a low acceptance rate (indicating that the scaling
of the proposal is too large), and even though the norm of
the empirical mean is much closer to the true value, quan-
tile performance of the chain is poor. Even if the estimated
covariance matrix closely resembles the true global covari-
ance matrix of the target, using it to construct proposal dis-
tributions at every state of the chain may not be the best
choice. For example, AM correctly captures scalings along
individual dimensions for the ﬂower target (the norm of its
empirical mean is close to its true value of zero) but fails to
capture local dependence structure. The ﬂower target, due
to its symmetry, has an isotropic covariance in the ﬁrst two
dimensions – even though they are highly dependent. This
leads to a mismatch in the scale of the covariance and the
scale of the target, which concentrates on a thin band in the
joint space. AM-LS has the “correct” acceptance rate, but
the quantile performance is even worse, as the scaling now
becomes too small to traverse high-density regions of the
target.

We have constructed a simple, versatile, adaptive, gradient-
free MCMC sampler that constructs a family of proposal
distributions based on the sample history of the chain.
These proposal distributions automatically conform to the
local covariance structure of the target distribution at the
current chain state.
In experiments, the sampler outper-
forms existing approaches on nonlinear target distributions,
both by exploring the entire support of these distributions,
and by returning accurate empirical quantiles, indicating
faster mixing. Possible extensions include incorporating
additional parametric information about the target densi-
ties, and exploring the tradeoff between the degree of sub-
sampling of the chain history and convergence of the sam-
pler.

Software. Python
MCMC
https://github.com/karlnapf/kameleon-mcmc.

implementation

Kameleon

available

of
at

is

Acknowledgments. D.S., H.S., M.L.G. and A.G. ac-
knowledge support of the Gatsby Charitable Foundation.
We thank Mark Girolami for insightful discussions and the
anonymous reviewers for useful comments.

!

6. Conclusions

Kernel Adaptive Metropolis Hastings

Saitoh, S. Integral transforms, reproducing kernels, and their ap-
plications. Pitman Research Notes in Mathematics 369, Long-
man Scientiﬁc & Techn., 1997.

Sch¨olkopf, B., Smola, A. J., and M¨uller, K.-R. Nonlinear compo-
nent analysis as a kernel eigenvalue problem. Neural Comput.,
10:1299–1319, 1998.

Smola, A., Gretton, A., Song, L., and Sch¨olkopf, B. A Hilbert
space embedding for distributions. In Proceedings of the Con-
ference on Algorithmic Learning Theory (ALT), pp. 13–31.
Springer, 2007.

Smola, A. J., Mika, S., Sch¨olkopf, B., and Williamson, R. C. Reg-
ularized principal manifolds. J. Mach. Learn. Res., 1:179–209,
2001.

Sriperumbudur, B., Gretton, A., Fukumizu, K., Lanckriet, G., and
Sch¨olkopf, B. Hilbert space embeddings and metrics on prob-
ability measures. J. Mach. Learn. Res., 11:1517–1561, 2010.

Sriperumbudur, B., Fukumizu, K., and Lanckriet, G. Universality,
characteristic kernels and RKHS embedding of measures. J.
Mach. Learn. Res., 12:2389–2410, 2011.

Steinwart, I. and Christmann, A.

Support Vector Machines.

Springer, 2008.

Welling, M. and Teh, Y.W. Bayesian learning via stochastic gra-
dient Langevin dynamics.
In Proc. of the 28th International
Conference on Machine Learning (ICML), pp. 681–688, 2011.

Williams, C.K.I. and Barber, D. Bayesian classiﬁcation with
IEEE Transactions on Pattern Analysis

Gaussian processes.
and Machine Intelligence, 20(12):1342–1351, 1998.

References

Andrieu, C. and Roberts, G.O. The pseudo-marginal approach
for efﬁcient Monte Carlo computations. Ann. Statist., 37(2):
697–725, 2009.

Andrieu, C. and Thoms, J. A tutorial on adaptive MCMC. Statis-

tics and Computing, 18(4):343–373, 2008.

Bache, K. and Lichman, M. UCI Machine Learning Repository,

2013. URL http://archive.ics.uci.edu/ml.

Baker, C. Joint measures and cross-covariance operators. Trans-
actions of the American Mathematical Society, 186:273–289,
1973.

Bakir, G., Weston, J., and Sch¨olkopf, B. Learning to ﬁnd pre-
In Advances in Neural Information Processing Sys-

images.
tems 16. MIT Press, 2003.

Berlinet, A. and Thomas-Agnan, C. Reproducing Kernel Hilbert

Spaces in Probability and Statistics. Kluwer, 2004.

Borgwardt, K. M., Gretton, A., Rasch, M. J., Kriegel, H.-P.,
Sch¨olkopf, B., and Smola, A. J. Integrating structured biologi-
cal data by kernel maximum mean discrepancy. Bioinformatics
(ISMB), 22(14):e49–e57, 2006.

Filippone, M. and Girolami, M. Pseudo-marginal Bayesian in-
ference for Gaussian Processes. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2014. doi: TPAMI.2014.
2316530.

Fukumizu, K., Bach, F. R., and Jordan, M. I. Dimensionality re-
duction for supervised learning with reproducing kernel Hilbert
spaces. J. Mach. Learn. Res., 5:73–99, 2004.

Gelman, A., Roberts, G. O., and Gilks, W. R. Efﬁcient Metropo-
In Bayesian statistics, 5 (Alicante, 1994),

lis jumping rules.
Oxford Sci. Publ., pp. 599–607. 1996.

Girolami, M. and Calderhead, B. Riemann manifold Langevin
and Hamiltonian Monte Carlo methods. Journal of the Royal
Statistical Society: Series B, 73(2):123–214, 2011.

Gretton, A., Borgwardt, K., Rasch, M., Sch¨olkopf, B., and Smola,
A. A kernel method for the two-sample problem. In Advances
in Neural Information Processing Systems 19, pp. 513–520,
2007.

Haario, H., Saksman, E., and Tamminen, J. Adaptive Proposal
Distribution for Random Walk Metropolis Algorithm. Comput.
Stat., 14(3):375–395, 1999.

Haario, H., Saksman, E., and Tamminen, J. An adaptive Metropo-

lis algorithm. Bernoulli, 7(2):223–242, 2001.

Murray, I. and Adams, R.P. Slice sampling covariance hyperpa-
In Advances in Neural

rameters of latent Gaussian models.
Information Processing Systems 23. 2012.

Roberts, G.O. and Rosenthal, J.S. Coupling and ergodicity of
J. Appl.

adaptive Markov chain Monte Carlo algorithms.
Probab., 44(2):458–475, 03 2007.

Roberts, G.O. and Stramer, O.

Langevin diffusions and
Metropolis-Hastings algorithms. Methodol. Comput. Appl.
Probab., 4:337–358, 2003.

A. Proofs

Kernel Adaptive Metropolis Hastings

Proposition 1. Let k be a differentiable positive deﬁnite kernel. Then

∇xk(x, x)

|x=y −

2

∇xk(x, y)

|x=y = 0.

Proof. Since k is a positive deﬁnite kernel there exists a Hilbert space
k(x, x′) =
ψ :
H →
to each y
the chain rule for

, such that
ϕ, where
h
(Rd, R) of τ at y, which
Rd associates a bounded linear operator from Rd to R (Steinwart & Christmann, 2008, Deﬁnition A.5.14). By

H
|x=y from the Fr´echet derivative Dτ (y)

iH. Consider ﬁrst the map τ : Rd
2
H . We can obtain
k

→ H
R, deﬁned by τ (x) = k(x, x). We write τ = ψ
◦

ϕ(x), ϕ(x′)
f
k

∇xk(x, x)

R, ψ(f ) =

and a feature map ϕ : Rd

∈ B

→

∈

Fr´echet derivatives (Steinwart & Christmann, 2008, Lemma A.5.15(b)), the value of Dτ (y) at some x′

Rd is

∈

[Dτ (y)] (x′) = [Dψ (ϕ(y))

Dϕ(y)] (x′),

◦

where Dϕ(y)
differentiable function (Steinwart & Christmann, 2008, Section 4.3). It is readily shown that Dψ [ϕ(y)] = 2
that

, R). The derivative Dϕ of the feature map exists whenever k is a
·iH, so

), and Dψ (ϕ(y))

ϕ(y),
h

(Rd,

(
H

∈ B

∈ B

H

Next, we consider the map κy(x) = k(x, y) =
a linear scalar function on

, Dψy (f ) =

H

[Dτ (y)] (x′) = 2

ϕ(y), [Dϕ(y)] (x′)
h
iH, i.e., κy = ψy ◦
ϕ(x), ϕ(y)
h
·iH. Again, by the chain rule:
ϕ(y),
h

iH .
ϕ where ψy(f ) =

[Dκy(y)] (x′) = [Dψy (ϕ(y))
=

◦

ϕ(y), [Dϕ(y)] (x′)
h

Dϕ(y)] (x′)
iH ,

f, ϕ(y)
h

iH. Since ψy is

2Dκy(y)) (x′) = 0, for all x′
and thus (Dτ (y)
derivatives can also be written as inner products with the gradients, (
(Dτ (y)

Rd, which proves the claim.

Rd, and we obtain equality of operators. Since Fr´echet
|x=y)⊤ x′ =

∇xk(x, x)

∇xk(x, y)

2Dκy(y)) (x′) = 0,

|x=y −

−

∈

2

x′
∀

∈

y) =

·|

N

(y, γ2I + ν2Mz,yHM ⊤

z,y).

−
Proposition 2. qz(

Proof. We start with

p(β)p(x∗

y, β) =

|

1
n+d

exp

−

(cid:18)

(2π)

exp

2 γdνn
1
2γ2 (x∗

y

−

1
2ν2 β⊤β

(cid:19)
Mz,yHβ)⊤ (x∗

−
(cid:18)
1
n+d

=

exp

(2π)

exp

2 γdνn
1
2

−

β⊤

(cid:18)

(cid:18)

(cid:18)

·

·

−
1
2γ2 (x∗
1
γ2 HM ⊤

−

(cid:18)
1
ν2 I +

−

y

−

−

Mz,yHβ)

(cid:19)

y)⊤ (x∗

y)

−

z,yMz,yH

β

(cid:19)

2
γ2 β⊤HM ⊤

z,y(x∗

y)

−

.

(cid:19)(cid:19)

(cid:19)

−

Now, we set

and application of the standard Gaussian integral

Σ−1 =

µ =

z,yMz,yH

1
ν2 I +
1
γ2 ΣHM ⊤

1
γ2 HM ⊤
z,y(x∗

y),

−

β⊤Σ−1β

2β⊤Σ−1µ

dβ =

exp

ˆ

1
2

−

−

(cid:18)
(2π)n/2√det Σ exp

(cid:0)

(cid:19)
(cid:1)
µ⊤Σ−1µ

1
2

(cid:18)

,

(cid:19)

This is just a d-dimensional Gaussian density where both the mean and covariance will, in general, depend on y. Let us
consider the exponent

leads to

Kernel Adaptive Metropolis Hastings

qz(x∗

y) =

|

√det Σ

exp

2 γdνn
1
2

(cid:18)
µ⊤Σ−1µ

d

(2π)

exp

·

(cid:18)

.

(cid:19)

1
2γ2 (x∗

−

−

y)⊤ (x∗

y)

−

(cid:19)

−

1
2γ2 (x∗
1
1
γ2 (x∗
2 (

−

−

y)⊤ (x∗

y) +

µ⊤Σ−1µ =

1
2

−

y)⊤ (x∗

y)

−

−

1
γ4 (x∗

−

−

y)⊤ Mz,yHΣHM ⊤

z,y (x∗

y)

=

−

)

1
2
n
γ2 Mz,yHΣHM ⊤

−

1

(x∗

y)⊤ R−1 (x∗

y)

,

−

−

o

R = γ2(I

= γ2

−

1
γ2 Mz,yHΣHM ⊤
γ2Σ−1

I + Mz,yH

z,y)−1

(cid:16)

(cid:0)

= γ2

I + Mz,yH

γ2
ν2 I
= γ2I + ν2Mz,yHM ⊤

 

(cid:18)

z,y.

(cid:19)

HM ⊤

z,yMz,yH

−1

HM ⊤
z,y

(cid:1)

(cid:17)

−
−1

HM ⊤
z,y

!

where R−1 = 1

γ2 (I

−

z,y). We can simplify the covariance R using the Woodbury identity to obtain:

Therefore, the proposal density is qz(

y) =

(y, γ2I + ν2Mz,yHM ⊤

z,y).

·|

N

B. Further details on synthetic experiments

Proposal contours for the Flower target. The d-dimensional ﬂower target

(r0, A, ω, σ) is given by

(x; r0, A, ω, σ) = exp

F

 − p

1 + x2
x2

2 −

r0 −

F
A cos (ωatan2 (x2, x1))
2σ2

(x3:d; 0, I).

! N

This distribution concentrates around the r0-circle with a periodic perturbation (with amplitude A and frequency ω) in the
ﬁrst two dimensions. For A = 0, we obtain a band around the r0-circle, which we term the ring target. Figure 6 gives the
contour plots of the MCMC Kameleon proposal distributions on two instances of the ﬂower target.

Convergence statistics for the Banana target. Figure 7 illustrates how the norm of the mean and quantile deviation
(shown for 0.5-quantile) for the strongly twisted Banana target decrease as a function of the number of iterations. This
shows that the trends observed in the main text persist along the evolution of the whole chain.

C. Principal Components Proposals

An alternative approach to the standard adaptive Metropolis, discussed in Andrieu & Thoms (2008, Algorithm 8), is to
m
extract m
j=1 from the estimated covariance matrix Σz and use the
(λj , vj)
}
{
proposal that takes form of a mixture of one-dimensional random walks along the principal eigendirections

d principal eigenvalue-eigenvector pairs

≤

qz (

y) =

·|

(y, ν2

j λjvjv⊤

j ).

ωjN

m

j=1
X

(7)

Kernel Adaptive Metropolis Hastings

Figure6. 95% contours (red) of proposal distributions evaluated at a number of points, for the ﬂower and the ring target. Underneath are
the density heatmaps, and the samples (blue) used to construct the proposals.

12

2

10
(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13)
8

X
ˆE

n
a
e
m
e
h
t

f
o
m
r
o
n

6

4

2

0

KAMH-LS
AM-LS
AM-FS
SM

e
c
n
e
r
e
f
f
i
d

e
l
i
t
n
a
u
q
-
5
.
0

0.12

0.10

0.08

0.06

0.04

0.02

0.00

0

KAMH-LS
AM-LS
AM-FS
SM

5000

10000

15000

20000

25000

30000

35000

40000

45000

5000

10000

15000

20000

25000

30000

35000

40000

45000

number of samples

number of samples

Figure7. Comparison of SM, AM-FS, AM-LS and KAMH-LS in terms of the norm of the estimated mean (left) and in terms of the
deviation from the 0.5-quantile (right) on the strongly twisted Banana distribution. The results are averaged over 20 chains for each
sampler. Error bars represent 80%-conﬁdence intervals.

Kernel Adaptive Metropolis Hastings

x∗ = y + ρνj

λjvj,

In other words, given the current chain state y, the j-th principal eigendirection is chosen with probability ωj (choice
ωj = λj /

m
l=1 λl is suggested), and the proposed point is

(8)

(9)

P

with ρ
∼ N
eigenvalue.

≤
i=1 ˜α(j)

n

(0, 1). Note that each eigendirection may have a different scaling factor νj in addition to the scaling with the

p

We can consider an analogous version of the update (8) performed in the RKHS

with m

n principal eigenvalue-eigenfunction pairs

, y) + ρνj

f = k(
·
(λj , vj)
}

{

λjvj ,

p
m
j=1. It is readily shown that the eigenfunctions vj =
⊤

i

−

, zi)

[k(
·

µz] lie in the subspace

˜α(j)
1
(cid:16)
are proportional to the eigenvector of the centered kernel matrix HKH, with normalization chosen so that
P
˜α(j)
has form
(cid:0)

(cid:17)
=
2
2 = 1 (so that the eigenfunctions have the unit RKHS norm). Therefore, the update (9)
(cid:13)
(cid:13)

z induced by z, and that the coefﬁcients vectors ˜α(j) =

HKH ˜α(j) = λj

˜α(j)
n
2
Hk

· · ·
vjk

˜α(j)

(cid:13)
(cid:13)

H

k

(cid:1)

⊤

n

f = k(
·

, y) +

β(j)
i

, zi)

[k(
·

−

µz] ,

i=1
X

˜α(j)

λj ˜α(j). But α(j) =

2
where β(j) = ρνj
2 =
2
2 = 1. Therefore, the appropriate scaling with eigenvalues is already included in the β-coefﬁcients, just like in
λj
the MCMC Kameleon, where the β-coefﬁcients are isotropic.
(cid:13)
(cid:13)

λj ˜α(j) are themselves the (unit norm) eigenvectors of HKH, as

Now, we can construct the MCMC PCA-Kameleon by simply substituting β-coefﬁcients with ρνjα(j), where j is the
selected eigendirection, and νj is the scaling factor associated to the j-th eigendirection. We have the following steps:

α(j)

p

p

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

1. Perform eigendecomposition of HKH to obtain the m

n eigenvectors

≤

m
j=1 .

αj}
{

2. Draw j

Discrete [ω1, . . . , ωm]

∼
(0, 1)

3. ρ

∼ N

4. x∗

y, ρ, j

|

∼ N

(y + ρνjMz,yHα(j), γ2I) (d

1 normal in the original space)

×

Similarly as before, we can simplify the proposal by integrating out the scale ρ of the moves in the RKHS.

Proposition 3. qz(

y) =

·|

Proof. We start with

m

j=1 ωjN

P

(y, γ2I + ν2

j Mz,yHα(j)

α(j)

⊤

HM ⊤

z,y).

(cid:0)

(cid:1)

p(ρ)p(x∗

y, ρ, j)

|

∝

exp

−

(cid:20)

y)⊤(x∗

1
2γ2 (x∗
1
2 ( 

−

1 +

ν2
j
γ2

y)

−

(cid:21)
⊤

(cid:16)

(cid:17)

exp

·

"−

By substituting

α(j)

HM ⊤

z,yMz,yHα(j)

ρ2

2ρ

−

νj
γ2

!

⊤

α(j)

HM ⊤

z,y (x∗

y)

.

)#

−

(cid:16)

(cid:17)

σ−2 = 1 +

α(j)

HM ⊤

z,yMz,yHα(j),

⊤

µ = σ2

(cid:16)

(cid:17)
α(j)

⊤

HM ⊤

z,y (x∗

(cid:18)

(cid:16)

(cid:17)

−

y)

,

(cid:19)

ν2
j
γ2
νj
γ2

p (x∗

y, j)

|

∝

exp

"−

1
γ2 (x∗

−

y)⊤(x∗

y)

−

−

ν2
j σ2
γ4

−

(x∗

y)⊤ Mz,yHα(j)

α(j)

⊤

HM ⊤

z,y (x∗

y)

−

)#

(cid:16)

(cid:17)

we integrate out ρ to obtain:

Kernel Adaptive Metropolis Hastings

1
2 (
1
2

(x∗

= exp

y)⊤ R−1 (x∗

−

−

(cid:20)
j σ2
ν2
γ2 Mz,yHα(j)

y)

−

(cid:21)

⊤

α(j)

HM ⊤
z,y

(cid:0)

(cid:1)

(cid:17)

where R−1 = 1
γ2
to obtain:

I

−

(cid:16)

R = γ2(I

−

= γ2

I +

j σ2
ν2
γ2 Mz,yHα(j)
(cid:16)
ν2
j σ2
γ2 Mz,yHα(j)

α(j)

HM ⊤

z,y)−1

⊤

(cid:17)

ν2
j σ2
γ2

1

−

 

(cid:16)

(cid:17)

= γ2

I +

ν2
j
γ2 Mz,yHα(j)

⊤

α(j)

HM ⊤
z,y

!





 

= γ2I + ν2

j Mz,yHα(j)

(cid:16)
α(j)

⊤

(cid:17)
HM ⊤

z,y.

(cid:16)
The claim follows after summing over the choice j of the eigendirection (w.p. ωj).

(cid:17)

. We can simplify the covariance R using the Woodbury identity

⊤

α(j)

HM ⊤

z,yMz,yHα(j)

−1

!

⊤

α(j)

(cid:16)

(cid:17)

HM ⊤

z,y



Kernel Adaptive Metropolis-Hastings

4
1
0
2
 
n
u
J
 

2
1
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
2
0
3
5
.
7
0
3
1
:
v
i
X
r
a

Dino Sejdinovic⋆
Heiko Strathmann⋆
Maria Lomeli Garcia⋆
Christophe Andrieu‡
Arthur Gretton⋆
⋆Gatsby Unit, CSML, University College London, UK and ‡School of Mathematics, University of Bristol, UK

DINO@GATSBY.UCL.AC.UK
UCABHST@GATSBY.UCL.AC.UK
MLOMELI@GATSBY.UCL.AC.UK
C.ANDRIEU@BRISTOL.AC.UK
ARTHUR.GRETTON@GMAIL.COM

Abstract

A Kernel Adaptive Metropolis-Hastings algo-
rithm is introduced, for the purpose of sampling
from a target distribution with strongly nonlin-
ear support. The algorithm embeds the trajec-
tory of the Markov chain into a reproducing ker-
nel Hilbert space (RKHS), such that the fea-
ture space covariance of the samples informs
the choice of proposal. The procedure is com-
putationally efﬁcient and straightforward to im-
plement, since the RKHS moves can be inte-
grated out analytically: our proposal distribu-
tion in the original space is a normal distribution
whose mean and covariance depend on where
the current sample lies in the support of the tar-
get distribution, and adapts to its local covari-
ance structure. Furthermore, the procedure re-
quires neither gradients nor any other higher or-
der information about the target, making it par-
ticularly attractive for contexts such as Pseudo-
Marginal MCMC. Kernel Adaptive Metropolis-
Hastings outperforms competing ﬁxed and adap-
tive samplers on multivariate, highly nonlinear
target distributions, arising in both real-world
and synthetic examples.

1. Introduction

The choice of the proposal distribution is known to be
crucial for the design of Metropolis-Hastings algorithms,
and methods for adapting the proposal distribution to in-
crease the sampler’s efﬁciency based on the history of the
Markov chain have been widely studied. These methods
often aim to learn the covariance structure of the target
distribution, and adapt the proposal accordingly. Adaptive
MCMC samplers were ﬁrst studied by Haario et al. (1999;

Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copy-
right 2014 by the author(s).

2001), where the authors propose to update the proposal
distribution along the sampling process. Based on the chain
history, they estimate the covariance of the target distribu-
tion and construct a Gaussian proposal centered at the cur-
rent chain state, with a particular choice of the scaling fac-
tor from Gelman et al. (1996). More sophisticated schemes
are presented by Andrieu & Thoms (2008), e.g., adaptive
scaling, component-wise scaling, and principal component
updates.

While these strategies are beneﬁcial for distributions that
show high anisotropy (e.g., by ensuring the proposal uses
the right scaling in all principal directions), they may still
suffer from low acceptance probability and slow mixing
when the target distributions are strongly nonlinear, and
the directions of large variance depend on the current lo-
cation of the sampler in the support. In the present work,
we develop an adaptive Metropolis-Hastings algorithm in
which samples are mapped to a reproducing kernel Hilbert
space, and the proposal distribution is chosen according to
the covariance in this feature space (Sch¨olkopf et al., 1998;
Smola et al., 2001). Unlike earlier adaptive approaches,
the resulting proposal distributions are locally adaptive in
input space, and oriented towards nearby regions of high
density, rather than simply matching the global covari-
ance structure of the distribution. Our approach com-
bines a move in the feature space with a stochastic step
towards the nearest input space point, where the feature
space move can be analytically integrated out. Thus,
the implementation of the procedure is straightforward:
the proposal is simply a multivariate Gaussian in the in-
put space, with location-dependent covariance which is
informed by the feature space representation of the tar-
get. Furthermore, the resulting Metropolis-Hastings sam-
pler only requires the ability to evaluate the unnormal-
ized density of the target (or its unbiased estimate, as
in Pseudo-Marginal MCMC of Andrieu & Roberts, 2009),
and no gradient evaluation is needed, making it applicable
to situations where more sophisticated schemes based on
Hamiltonian Monte Carlo (HMC) or Metropolis Adjusted

Kernel Adaptive Metropolis Hastings

Langevin Algorithms (MALA) (Roberts & Stramer, 2003;
Girolami & Calderhead, 2011) cannot be applied.

We begin our presentation in Section 2, with a brief
overview of existing adaptive Metropolis approaches; we
also review covariance operators in the RKHS. Based on
these operators, we describe a sampling strategy for Gaus-
sian measures in the RKHS in Section 3, and introduce a
cost function for constructing proposal distributions.
In
Section 4, we outline our main algorithm, termed Kernel
Adaptive Metropolis-Hastings (MCMC Kameleon). We
provide experimental comparisons with other ﬁxed and
adaptive samplers in Section 5, where we show superior
performance in the context of Pseudo-Marginal MCMC
for Bayesian classiﬁcation, and on synthetic target distri-
butions with highly nonlinear shape.

2. Background

= Rd be
Adaptive Metropolis Algorithms. Let
the domain of interest, and denote the unnormalized
target density on
let Σt =
Σt(x0, x1, . . . , xt−1) denote an estimate of the covariance
matrix of the target density based on the chain history
t−1
i=0. The original adaptive Metropolis at the current
xi}
{
state of the chain state xt = y uses the proposal

by π. Additionally,

X

X

qt(

y) =

(y, ν2Σt),

(1)

·|

N
where ν = 2.38/√d is a ﬁxed scaling factor from
Gelman et al. (1996). This choice of scaling factor was
shown to be optimal (in terms of efﬁciency measures) for
the usual Metropolis algorithm. While this optimality re-
sult does not hold for Adaptive Metropolis, it can never-
theless be used as a heuristic. Alternatively, the scale ν
can also be adapted at each step as in Andrieu & Thoms
(2008, Algorithm 4) to obtain the acceptance rate from
Gelman et al. (1996), a∗ = 0.234.

X

X × X →

Embeddings

and Covariance Opera-
RKHS
tors. According to the Moore-Aronszajn theorem
(Berlinet & Thomas-Agnan, 2004, p. 19), for every sym-
R,
metric, positive deﬁnite function (kernel) k :
there is an associated reproducing kernel Hilbert space
with reproducing kernel
Hk of real-valued functions on
k. The map ϕ :
, x) is called
X → Hk, ϕ : x
the canonical feature map of k. This feature map or
embedding of a single point can be extended to that of
its kernel embedding is
a probability measure P on
an element µP ∈ Hk, given by µP =
, x) dP (x)
k(
·
(Berlinet & Thomas-Agnan, 2004; Fukumizu et al., 2004;
Smola et al., 2007). If a measurable kernel k is bounded,
it is straightforward to show using the Riesz represen-
tation theorem that the mean embedding µk(P ) exists
. For many interesting
for all probability measures on

k(
·

7→

X

´

:

X

7→

bounded kernels k, including the Gaussian, Laplacian and
inverse multi-quadratics, the kernel embedding P
µP
is injective. Such kernels are said to be characteristic
(Sriperumbudur et al., 2010; 2011), since each distribution
is uniquely characterized by its embedding (in the same
way that every probability distribution has a unique
characteristic function). The kernel embedding µP is the
representer of expectations of smooth functions w.r.t. P ,
i.e.,
f (x)dP (x). Given samples
∈ Hk,
f
f, µP iHk
h
∀
n
z =
P , the embedding of the empirical measure
zi}
i=1 ∼
{
n
is µz = 1
, zi).
i=1 k(
n
·
Hk → Hk for a
Next, the covariance operator CP :
probability measure P is given by CP =
k(
⊗
·
´
µP (Baker, 1973; Fukumizu et al.,
k(
µP ⊗
·
∈ Hk the tensor product is de-
2004), where for a, b, c
a. The covariance oper-
ﬁned as (a
b, c
b)c =
⊗
ator has the property that
=
EP f EP g.
EP (f g)

f, CP g
h

∈ Hk,

iHk
f, g
∀

, x) dP (x)

iHk

, x)

P

=

−

´

h

−

1
n

⊗

⊗

µz

P

, zi)

, zi)

k(
·

n
i=1 k(
·

Our approach is based on the idea that the nonlinear sup-
port of a target density may be learned using Kernel Princi-
pal Component Analysis (Kernel PCA) (Sch¨olkopf et al.,
1998; Smola et al., 2001),
this being linear PCA on
the empirical covariance operator in the RKHS, Cz =
1
µz, computed on the
n
−
sample z deﬁned above. The empirical covariance op-
erator behaves as expected: applying the tensor prod-
uct deﬁnition gives
n
1
i=1 f (zi)
n

= 1
n
−
. By analogy with algo-
rithms which use linear PCA directions to inform M-H pro-
(cid:0)
posals (Andrieu & Thoms, 2008, Algorithm 8), nonlinear
PCA directions can be encoded in the proposal construc-
tion, as described in Appendix C. Alternatively, one can
focus on a Gaussian measure on the RKHS determined by
the empirical covariance operator Cz rather than extract-
ing its eigendirections, which is the approach we pursue in
this contribution. This generalizes the proposal (1), which
considers the Gaussian measure induced by the empirical
covariance matrix on the original space.

f, Czg
iHk
h
n
i=1 g(zi)

n
i=1 f (zi)g(zi)

(cid:1) (cid:0)

P

P

P

(cid:1)

3. Sampling in RKHS

We next describe the proposal distribution at iteration t of
the MCMC chain. We will assume that a subset of the chain
n
history, denoted z =
1, is available. Our
i=1, n
proposal is constructed by ﬁrst considering the samples in
the RKHS associated to the empirical covariance operator,
and then performing a gradient descent step on a cost func-
tion associated with those samples.

zi}
{

≤

−

t

Gaussian Measure of the Covariance Operator. We
will work with the Gaussian measure on the RKHS
Hk
n
with mean k(
zi}
i=1
·
is the subset of the chain history. While there is no analogue

, y) and covariance ν2Cz, where z =

{

Kernel Adaptive Metropolis Hastings

Samples {zi}200
i=1

Current position y

βiβj (k(
·

, zi)

−

µz)

(k(
·

⊗

, zj)

−

µz)

Figure1. Heatmaps (white denotes large) and gradients of g(x)
for two samples of β and ﬁxed z.

f

(cid:16)

−

−

1
2ν2

k(
·

N
k(
·

, y), C−1

, y), ν2Cz)

(f ; k(
·
Hk

of a Lebesgue measure in an inﬁnite dimensional RKHS, it
is instructive (albeit with some abuse of notation) to denote
this measure in the “density form”

∝
. As Cz is
z (f
exp
a ﬁnite-rank operator, this measure is supported only on a
ﬁnite-dimensional afﬁne space k(
z =
, y) +
·
n
span
i=1 is the subspace spanned by the canonical
features of z. It can be shown that a sample from this mea-
sure has the form f = k(
µz] ,
, y) +
·
n I) is isotropic. Indeed, to see that f
where β
has the correct covariance structure, note that:

(cid:17)
(cid:11)
z, where
H

n
i=1 βi [k(
·

, zi)
}

(0, ν2

k(
·

∼ N

, zi)

, y))

P

H

−

−

{

(cid:10)

, y))

(f

k(
·

−

⊗

, y))]

−

k(
·

n

n

E [(f

= E




ν2
n

=

i=1
X
n

j=1
X

i=1
X

, zi)

(k(
·

−

µz)

(k(
·

⊗

, zi)

−

µz) = ν2Cz.





and a Gaussian Process

Due to the equivalence in the RKHS between a
Gaussian measure
(GP)
(Berlinet & Thomas-Agnan, 2004, Ch.
4), we can
think of the RKHS samples f as trajectories of the GP with
mean m(x) = k(x, y) and covariance function
κ(x, x′) = cov [f (x), f (x′)]
ν2
n

µz(x)) (k(x′, zi)

(k(x, zi)

=

µz(x′)) .

n

−

−

i=1
X

The covariance function κ of this GP is therefore the kernel
k convolved with itself with respect to the empirical mea-
sure associated to the samples z, and draws from this GP
therefore lie in a smaller RKHS; see Saitoh (1997, p. 21)
for details.

P

, zi)

, y) +

n
i=1 βi [k(
·

Obtaining Target Samples through Gradient Descent.
We have seen how to obtain the RKHS sample f =
µz] from the Gaussian mea-
k(
−
·
sure in the RKHS. This sample does not in general have a
= Rd;
corresponding pre-image in the original domain
X
i.e., there is no point x∗
, x∗). If
such that f = k(
·
∈ X
there were such a point, then we could use it as a proposal
in the original domain. Therefore, we are ideally looking
, x∗) is
for a point x∗
close to f in the RKHS norm. We consider the optimization
problem

whose canonical feature map k(
·

∈ X

arg min
x∈X

kk (·, x) − f k

2
Hk

=

In general, this is a non-convex minimization problem, and
may be difﬁcult to solve (Bakir et al., 2003). Rather than
solving it for every new vector of coefﬁcients β, which
would lead to an excessive computational burden for ev-
ery proposal made, we simply make a single descent step
along the gradient of the cost function,

g(x) = k(x, x)

2k(x, y)

2

βi [k(x, zi)

µz(x)] ,

−

−

−

n

i=1
X

(2)

i.e., the proposed new point is

x∗ = y

η

∇xg(x)

|x=y + ξ,

−

where η is a gradient step size parameter and ξ

∼
(0, γ2I) is an additional isotropic ’exploration’ term
It will be useful to split the
|x=y =
|x=y −

N
after the gradient step.
scaled gradient at y into two terms as η
Mz,yHβ), where ay =
η (ay −
∇xk(x, y)
2
Mz,y = 2 [

∇xg(x)
∇xk(x, x)

|x=y,

(3)

∇xk(x, z1)

n matrix, and H = I

|x=y, . . . ,
−

∇xk(x, zn)
×

1
n 1n×n is the n

|x=y]
n centering

is a d
×
matrix.

Figure 1 plots g(x) and its gradients for several samples of
β-coefﬁcients, in the case where the underlying z-samples
are from the two-dimensional nonlinear Banana target dis-
tribution of Haario et al. (1999). It can be seen that g may
have multiple local minima, and that it varies most along
the high-density regions of the Banana distribution.

4. MCMC Kameleon Algorithm

4.1. Proposal Distribution

arg min

k(x, x) − 2k(x, y) − 2

βi [k(x, zi) − µz(x)]

.

x∈X (

We now have a recipe to construct a proposal that is able to
adapt to the local covariance structure for the current chain

)

n

i=1
X

Kernel Adaptive Metropolis Hastings

MCMC Kameleon
Input: unnormalized target π, subsample size n, scaling
∞
t=0, kernel k,
parameters ν, γ, adaptation probabilities

pt}

{

At iteration t + 1,

•

1. With probability pt, update a random subsample

min(n,t)
i=1

z =

zi}

xi}
{
2. Sample proposed point x∗ from qz(
·|

of the chain history

{

(xt, γ2I + ν2Mz,xt HM ⊤
(3) and H = I

N
given in Eq.
centering matrix,

t−1
i=0,
xt) =
z,xt), where Mz,xtis
1
n 1n×n is the

−

3. Accept/Reject with the Metropolis-Hastings ac-

ceptance probability A(xt, x∗) in Eq. (4),

xt+1 =

x∗, w.p. A(xt, x∗),
xt, w.p. 1

A(xt, x∗).

(

−

state y. This proposal depends on a subset of the chain
history z, and is denoted by qz(
y). While we will later
simplify this proposal by integrating out the moves in the
RKHS, it is instructive to think of the proposal generating
process as:

·|

1. Sample β
ﬁcients).

∼ N

(0, ν2I) (n

1 normal of RKHS coef-

×

•

This represents an RKHS sample f = k(
·

, y) +
µz] which is the goal of the

n
i=1 βi [k(
·

−
cost function g(x).
P

, zi)

2. Move along the gradient of g:

x∗ = y

−

η

∇xg(x)

|x=y + ξ.

•

This gives a proposal x∗
ηMz,yHβ, γ2I) (d
space).

×

y, β

ηay +
1 normal in the original

∼ N

(y

−

|

2

|x=y −

∇xk(x, x)

Our ﬁrst step in the derivation of the explicit proposal den-
sity is to show that as long as k is a differentiable positive
deﬁnite kernel, the term ay vanishes.
Proposition 1. Let k be a differentiable positive deﬁnite
|x=y = 0.
kernel. Then ay =
Since ay = 0, the gradient step size η always appears to-
gether with β, so we merge η and the scale ν of the β-
coefﬁcients into a single scale parameter, and set η = 1
henceforth. Furthermore, since both p(β) and pz(x∗
y, β)
are multivariate Gaussian densities, the proposal density
qz(x∗
y, β)dβ can be computed analyt-
ically. We therefore get the following closed form expres-
sion for the proposal distribution.

∇xk(x, y)

p(β)pz(x∗

y) =

´

|

|

|

Proposition 2. qz(

y) =

(y, γ2I + ν2Mz,yHM ⊤

z,y).

·|

N

Figure2. 95% contours (red) of proposal distributions evaluated
at a number of points, for the ﬁrst two dimensions of the banana
target of Haario et al. (1999). Underneath is the density heatmap,
and the samples (blue) used to construct the proposals.

Proofs of the above Propositions are given in Appendix A.

With the derived proposal distribution, we proceed with the
standard Metropolis-Hastings accept/reject scheme, where
the proposed sample x∗ is accepted with probability

A(xt, x∗) = min

1,

π(x∗)qz(xt|
π(xt)qz(x∗
|

x∗)
xt)

,

(4)

(cid:27)

(cid:26)
giving rise to the MCMC Kameleon Algorithm. Note that
each π(x∗) and π(xt) could be replaced by their unbi-
ased estimates without impacting the invariant distribution
(Andrieu & Roberts, 2009).
The constructed family of proposals encodes local struc-
ture of the target distribution, which is learned based on the
subsample z. Figure 2 depicts the regions that contain 95%
y) at various
of the mass of the proposal distribution qz(
states y for a ﬁxed subsample z, where the Banana target
is used (details in Section 5). More examples of proposal
contours can be found in Appendix B.

·|

4.2. Properties of the Algorithm

·|

and convergence. MCMC
The update
schedule
n
Kameleon requires a subsample z =
i=1 at each
zi}
{
iteration of the algorithm, and the proposal distribution
y) is updated each time a new subsample z is obtained.
qz(
It is well known that a chain which keeps adapting the
proposal distribution need not converge to the correct
target (Andrieu & Thoms, 2008). To guarantee conver-
∞
gence, we introduce adaptation probabilities
t=0,
pt}
{
, and at iteration
such that pt →
t we update the subsample z with probability pt. As
adaptations occur with decreasing probability, Theorem 1
of Roberts & Rosenthal (2007) implies that the resulting
algorithm is ergodic and converges to the correct target.
Another straightforward way to guarantee convergence is
n
to ﬁx the set z =
i=1 after a “burn-in” phase; i.e., to
stop adapting Roberts & Rosenthal (2007, Proposition 2).
In this case, a “burn-in” phase is used to get a rough sketch
of the shape of the distribution: the initial samples need not

∞
t=1 pt =

zi}
{

0 and

P

∞

Kernel Adaptive Metropolis Hastings

come from a converged or even valid MCMC chain, and it
sufﬁces to have a scheme with good exploratory properties,
e.g., Welling & Teh (2011).
In MCMC Kameleon, the
term γ allows exploration in the initial iterations of the
chain (while the subsample z is still not informative about
the structure of the target) and provides regularization of
the proposal covariance in cases where it might become
ill-conditioned. Intuitively, a good approach to setting γ is
to slowly decrease it with each adaptation, such that the
learned covariance progressively dominates the proposal.

In Haario et al. (2001), the
Symmetry of the proposal.
proposal distribution is asymptotically symmetric due to
the vanishing adaptation property. Therefore, the authors
compute the standard Metropolis acceptance probability. In
our case, the proposal distribution is a Gaussian with mean
at the current state of the chain xt = y and covariance
γ2I + ν2Mz,yHM ⊤
z,y, where Mz,y depends both on the
n
current state y and a random subsample z =
i=1 of the
t−1
chain history
i=0. This proposal distribution is never
symmetric (as covariance of the proposal always depends
on the current state of the chain), and therefore we use the
Metropolis-Hastings acceptance probability to reﬂect this.

xi}
{

zi}
{

Relationship to MALA and Manifold MALA. The
Metropolis Adjusted Langevin Algorithm (MALA) algo-
rithm uses information about the gradient of the log-target
density at the current chain state to construct a proposed
point for the Metropolis step. Our approach does not re-
quire that the log-target density gradient be available or
computable. Kernel gradients in the matrix Mz,y are easily
obtained for commonly used kernels, including the Gaus-
sian kernel (see section 4.3), for which the computational
complexity is equal to evaluating the kernel itself. More-
over, while standard MALA simply shifts the mean of the
proposal distribution along the gradient and then adds an
isotropic exploration term, our proposal is centered at the
current state, and it is the covariance structure of the pro-
posal distribution that coerces the proposed points to be-
It would
long to the high-density regions of the target.
be straightforward to modify our approach to include a
drift term along the gradient of the log-density, should
such information be available, but it is unclear whether
this would provide additional performance gains. Fur-
ther work is required to elucidate possible connections be-
tween our approach and the use of a preconditioning ma-
trix (Roberts & Stramer, 2003) in the MALA proposal; i.e.,
where the exploration term is scaled with appropriate met-
ric tensor information, as in Riemannian manifold MALA
(Girolami & Calderhead, 2011).

4.3. Examples of Covariance Structure for Standard

Kernels

The proposal distributions in MCMC Kameleon are depen-
dant on the choice of the kernel k. To gain intuition re-

garding their covariance structure, we give two examples
below.

a

In

of
linear
obtain Mz,y
= 2Z⊤,

ker-
Linear kernel.
the
case
nel k(x, x′)
x⊤x′, we
=
=
∇xx⊤z1|x=y, . . . ,
∇xx⊤zn|x=y
so the
2
(y, γ2I + 4ν2Z⊤HZ);
proposal is given by qz(
y) =
(cid:3)
(cid:2)
N
thus, the proposal simply uses the scaled empirical co-
variance Z⊤HZ just like standard Adaptive Metropolis
(Haario et al., 1999), with an additional isotropic explo-
ration component, and depends on y only through the
mean.

·|

Gaussian kernel.
k(x, x′) = exp

′

In the case of a Gaussian kernel
∇xk(x, x′) =

x−x
k
2σ2

, since

−

k

2

2

(cid:18)

(cid:19)

1

x), we obtain

σ2 k(x, x′)(x′
2
σ2 [k(y, z1)(z1 −

Mz,y =

−

y), . . . , k(y, zn)(zn −

y)] .

Consider how this encodes the covariance structure of the
target distribution:

Rij = γ

δij

2

+

−

4ν 2(n − 1)
σ4n

n

a=1
X

4ν 2
σ4n

a6=b
X

[k(y, za)]

(za,i − yi)(za,j − yj)

2

k(y, za)k(y, zb)(za,i − yi)(zb,j − yj). (5)

As the ﬁrst two terms dominate, the previous points za
which are close to the current state y (for which k(y, za)
is large) have larger weights, and thus they have more in-
ﬂuence in determining the covariance of the proposal at y.

′

′

ϑ

k

k

(cid:18)

k2

Γ(ϑ)

Kϑ

x−x
ρ

x−x
ρ

In the Mat´ern family of kernels

Mat´ern kernel.
kϑ,ρ(x, x′) = 21−ϑ
Kϑ is the modiﬁed Bessel function of the second kind, we
obtain a form of the covariance structure very similar to
∇xkϑ,ρ(x, x′) =
that of the Gaussain kernel. In this case,
2ρ2(ϑ−1) kϑ−1,ρ(x, x′)(x′
x), so the only difference (apart
from the scalings) to (5) is that the weights are now deter-
mined by a “rougher” kernel kϑ−1,ρ of the same family.

, where

k2

−

(cid:19)

(cid:19)

(cid:18)

1

5. Experiments

·|

N

y) =

In the experiments, we compare the following samplers:
(SM) Standard Metropolis with the isotropic proposal
(y, ν2I) and scaling ν = 2.38/√d, (AM-
q(
FS) Adaptive Metropolis with a learned covariance ma-
trix and ﬁxed scaling ν = 2.38/√d, (AM-LS) Adaptive
Metropolis with a learned covariance matrix and scaling
learned to bring the acceptance rate close to α∗ = 0.234 as
described in Andrieu & Thoms (2008, Algorithm 4), and
(KAMH-LS) MCMC Kameleon with the scaling ν learned

Kernel Adaptive Metropolis Hastings

0

−1

−2

−3

−4

7
θ

−5

−6

−5

−4

−2

−1

0

−3

θ2

Figure3. Dimensions 2 and 7 of the marginal hyperparameter
posterior on the UCI Glass dataset

in the same fashion (γ was ﬁxed to 0.2), and which also
stops adapting the proposal after the burn-in of the chain
(in all experiments, we use a random subsample z of size
n = 1000, and a Gaussian kernel with bandwidth se-
lected according to the median heuristic). We consider the
following nonlinear targets: (1) the posterior distribution
of Gaussian Process (GP) classiﬁcation hyperparameters
(Filippone & Girolami, 2014) on the UCI glass dataset, and
(2) the synthetic banana-shaped distribution of Haario et al.
(1999) and a ﬂower-shaped disribution concentrated on a
circle with a periodic perturbation.

5.1. Pseudo-Marginal MCMC for GP Classiﬁcation

In the ﬁrst experiment, we illustrate usefulness of the
MCMC Kameleon sampler in the context of Bayesian clas-
siﬁcation with GPs (Williams & Barber, 1998). Consider
the joint distribution of latent variables f , labels y (with
covariate matrix X), and hyperparameters θ, given by

p(f , y, θ) = p(θ)p(f

θ)p(y

f ),

|

|

′

1

θ

}

(cid:16)

−

P

(0,

1, 1

∼ N

D
d=1

fi) =

θ) = exp

(xi,d−x
ℓ2
d

Kθ), with

i.e.,
1−exp(−yifi) where yi ∈ {−

where f
Kθ modeling the covariance
|
between latent variables evaluated at the input covariates:
j,d)2
1
Kθ)ij = κ(xi, x′
(
j|
2
and θd = log ℓ2
d. We restrict our attention to the bi-
(cid:17)
the likelihood is given by
nary logistic classiﬁer;
. We pursue
p(yi|
a fully Bayesian treatment, and estimate the posterior of
the hyperparameters θ. As observed by Murray & Adams
(2012), a Gibbs sampler on p(θ, f
y), which samples from
p(f
f , y)
is extremely sharp, drastically limiting the amount that
f , y. On the other hand,
any Markov chain can update θ
if we directly consider the marginal posterior p(θ
∝
p(y
θ)p(θ) of the hyperparameters, a much less peaked
distribution can be obtained. However, the marginal like-
lihood p(y
θ) is intractable for non-Gaussian likelihoods
p(y
f ), so it is not possible to analytically integrate out
the latent variables. Recently developed pseudo-marginal
MCMC methods (Andrieu & Roberts, 2009) enable exact

f , y) in turn, is problematic, as p(θ

θ, y) and p(θ

y)

|

|

|

|

|

|

|

|

|

inference on this problem (Filippone & Girolami, 2014),
by replacing p(y

θ) with an unbiased estimate

|

(6)

nimp

ˆp(y

θ) :=

|

1
nimp

p(y

f (i))

|

p(f (i)
q(f (i)

,

θ)
|
θ)
|

|

|

|

|

(cid:8)

∝

q(f

f (i)

y, θ)

f )p(f

nimp
i=1 ∼

i=1
X
where
θ) are nimp importance sam-
ples. In Filippone & Girolami (2014), the importance dis-
(cid:9)
tribution q(f
θ) is chosen as the Laplacian or as the Ex-
|
pectation Propagation (EP) approximation of p(f
p(y
θ), leading to state-of-the-art results.
We consider the UCI Glass dataset (Bache & Lichman,
2013), where classiﬁcation of window against non-window
glass is sought. Due to the heterogeneous structure of each
of the classes (i.e., non-window glass consists of contain-
ers, tableware and headlamps), there is no single consistent
set of lengthscales determining the decision boundary, so
one expects the posterior of the covariance bandwidths θd
to have a complicated (nonlinear) shape. This is illustrated
by the plot of the posterior projections to the dimensions
2 and 7 (out of 9) in Figure 3. Since the ground truth
for the hyperparameter posterior is not available, we ini-
tially ran 30 Standard Metropolis chains for 500,000 itera-
tions (with a 100,000 burn-in), kept every 1000-th sample
in each of the chains, and combined them. The resulting
samples were used as a benchmark, to evaluate the perfor-
mance of shorter single-chain runs of SM, AM-FS, AM-
LS and KAMH-LS. Each of these algorithms was run for
100,000 iterations (with a 20,000 burnin) and every 20-th
sample was kept. Two metrics were used in evaluating the
performance of the four samplers, relative to the large-scale
was computed
benchmark. First, the distance
ˆµθ −
between the mean ˆµθ estimated from each of the four sam-
(cid:13)
pler outputs, and the mean µb
θ on the benchmark sample
(cid:13)
(Fig. 4, left), as a function of sample size. Second, the
MMD (Borgwardt et al., 2006; Gretton et al., 2007) was
computed between each sampler output and the bench-
)3;
mark sample, using the polynomial kernel (1 +
i
i.e., the comparison was made in terms of all mixed mo-
ments of order up to 3 (Fig. 4, right). The ﬁgures indicate
that KAMH-LS approximates the benchmark sample bet-
ter than the competing approaches, where the effect is es-
pecially pronounced in the high order moments, indicating
that KAMH-LS thoroughly explores the distribution sup-
port in a relatively small number of samples.
We emphasise that, as for any pseudo-marginal MCMC
scheme, neither the likelihood itself, nor any higher-
the marginal posterior target
order information about
y), are available.
This makes HMC or MALA
p(θ
based approaches such as (Roberts & Stramer, 2003;
Girolami & Calderhead, 2011) unsuitable for this problem,
so it is very difﬁcult to deal with strongly nonlinear pos-
terior targets.
In contrast, as indicated in this example,
the MCMC Kameleon scheme is able to effectively sample

θ, θ′
h

µb
θ

(cid:13)
(cid:13)

2

|

Kernel Adaptive Metropolis Hastings

KAMH-LS
AM-LS
AM-FS
SM

2

bθ
µ
−

θ
ˆµ

e
c
n
a
t
s
i
d
n
a
e
m

0.50

0.45

(cid:13)(cid:13)
0.40

0.35
(cid:13)(cid:13)

0.30

0.25

0.20

0.15

0.10

0.5
0.4
0.3
0.2
0.1
0.0

1.0

0.8

0.6

0.4

0.2

0.0

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

KAMH-LS
AM-LS
AM-FS
SM

e
l
p
m
a
s
k
r
a
m
h
c
n
e
b

e
h
t

m
o
r
f

D
M
M

45

40

35

30

25

20

15

10

5

B

B

1000

2000

3000

4000

5000

1000

2000

3000

4000

5000

number of samples

number of samples

Figure4. The comparison of SM, AM-FS, AM-LS and KAMH-LS in terms of the distance between the estimated mean and the mean
on the benchmark sample (left) and in terms of the maximum mean discrepancy to the benchmark sample (right). The results are
averaged over 30 chains for each sampler. Error bars represent 80%-conﬁdence intervals.

Moderately twisted 8-dimensional

(0.03, 100) target; iterations: 40000, burn-in: 20000

SM

AM-FS

AM-LS

KAMH-LS

Accept ∈ [0, 1]

||ˆE[X]||/10

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Strongly twisted 8-dimensional

(0.1, 100) target; iterations: 80000, burn-in: 40000

SM

AM-FS

AM-LS

KAMH-LS

Accept ∈ [0, 1]

||ˆE[X]||/10

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

8-dimensional

(10, 6, 6, 1) target; iterations: 120000, burn-in: 60000

F

SM

AM-FS

AM-LS

KAMH-LS

Accept ∈ [0, 1]

||ˆE[X]||/10

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Figure5. Results for three nonlinear targets, averaged over 20 chains for each sampler. Accept is the acceptance rate scaled to the
interval [0, 1]. The norm of the mean ||ˆE[X]|| is scaled by 1/10 to ﬁt into the ﬁgure scalling, and the bars over the 0.1, . . . , 0.9-quantiles
represent the deviation from the exact quantiles, scaled by 10; i.e., 0.1 corresponds to 1% deviation. Error bars represent 80%-conﬁdence
intervals.

Kernel Adaptive Metropolis Hastings

from such nonlinear targets, and outperforms the vanilla
Metropolis methods, which are the only competing choices
in the pseudo-marginal context.

In addition, since the bulk of the cost for pseudo-marginal
MCMC is in importance sampling in order to obtain the
acceptance ratio, the additional cost imposed by KAMH-
LS is negligible. Indeed, we observed that there is an in-
crease of only 2-3% in terms of effective computation time
in comparison to all other samplers, for the chosen size of
the chain history subsample (n = 1000).

5.2. Synthetic examples

∼ N

In Haario et al. (1999),

(0, Σ) be a multivariate normal in d

the following
Banana target.
family of nonlinear target distributions is considered. Let
2 dimen-
X
sions, with Σ = diag(v, 1, . . . , 1), which undergoes the
transformation X
v),
1 −
(b, v). It is
and Yi = Xi for i
clear that EY = 0, and that

Y , where Y2 = X2 + b(X 2

→
= 2. We will write Y

≥

(y; b, v) =

(y1; 0, v)

B

N

(y2; b(y2

v), 1)

1−

N

(yj; 0, 1).

∼ B
d

N

j=3
Y

Flower target. The second target distribution we con-
sider is the d-dimensional ﬂower target
(r0, A, ω, σ),
with

F

F(x; r0, A, ω, σ) =

1 + x2
x2

2 − r0 − A cos (ωatan2 (x2, x1))

exp

−

 

2σ2

p
d

j=3
Y

×

N (xj; 0, 1).

This distribution concentrates around the r0-circle with a
periodic perturbation (with amplitude A and frequency ω)
in the ﬁrst two dimensions.

In these examples, exact quantile regions of the targets
can be computed analytically, so we can directly assess
performance without the need to estimate distribution dis-
tances on the basis of samples (i.e., by estimating MMD to
the benchmark sample). We compute the following mea-
sures of performance (similarly as in Haario et al. (1999);
Andrieu & Thoms (2008)) based on the chain after burn-in:
average acceptance rate, norm of the empirical mean (the
true mean is by construction zero for all targets), and the
deviation of the empirical quantiles from the true quantiles.
We consider 8-dimensional target distributions: the mod-
(0.03, 100) banana target (Figure 5, top)
erately twisted
and the strongly twisted
(0.1, 100) banana target (Figure
5, middle) and
(10, 6, 6, 1) ﬂower target (Figure 5, bot-
tom).

F

B

B

The results show that MCMC Kameleon is superior to the
competing samplers. Since the covariance of the proposal

adapts to the local structure of the target at the current chain
state, as illustrated in Figure 2, MCMC Kameleon does
not suffer from wrongly scaled proposal distributions. The
result is a signiﬁcantly improved quantile performance in
comparison to all competing samplers, as well as a com-
parable or superior norm of the empirical mean. SM has
a signiﬁcantly larger norm of the empirical mean, due to
its purely random walk behavior (e.g., the chain tends to
get stuck in one part of the space, and is not able to traverse
both tails of the banana target equally well). AM with ﬁxed
scale has a low acceptance rate (indicating that the scaling
of the proposal is too large), and even though the norm of
the empirical mean is much closer to the true value, quan-
tile performance of the chain is poor. Even if the estimated
covariance matrix closely resembles the true global covari-
ance matrix of the target, using it to construct proposal dis-
tributions at every state of the chain may not be the best
choice. For example, AM correctly captures scalings along
individual dimensions for the ﬂower target (the norm of its
empirical mean is close to its true value of zero) but fails to
capture local dependence structure. The ﬂower target, due
to its symmetry, has an isotropic covariance in the ﬁrst two
dimensions – even though they are highly dependent. This
leads to a mismatch in the scale of the covariance and the
scale of the target, which concentrates on a thin band in the
joint space. AM-LS has the “correct” acceptance rate, but
the quantile performance is even worse, as the scaling now
becomes too small to traverse high-density regions of the
target.

We have constructed a simple, versatile, adaptive, gradient-
free MCMC sampler that constructs a family of proposal
distributions based on the sample history of the chain.
These proposal distributions automatically conform to the
local covariance structure of the target distribution at the
current chain state.
In experiments, the sampler outper-
forms existing approaches on nonlinear target distributions,
both by exploring the entire support of these distributions,
and by returning accurate empirical quantiles, indicating
faster mixing. Possible extensions include incorporating
additional parametric information about the target densi-
ties, and exploring the tradeoff between the degree of sub-
sampling of the chain history and convergence of the sam-
pler.

Software. Python
MCMC
https://github.com/karlnapf/kameleon-mcmc.

implementation

Kameleon

available

of
at

is

Acknowledgments. D.S., H.S., M.L.G. and A.G. ac-
knowledge support of the Gatsby Charitable Foundation.
We thank Mark Girolami for insightful discussions and the
anonymous reviewers for useful comments.

!

6. Conclusions

Kernel Adaptive Metropolis Hastings

Saitoh, S. Integral transforms, reproducing kernels, and their ap-
plications. Pitman Research Notes in Mathematics 369, Long-
man Scientiﬁc & Techn., 1997.

Sch¨olkopf, B., Smola, A. J., and M¨uller, K.-R. Nonlinear compo-
nent analysis as a kernel eigenvalue problem. Neural Comput.,
10:1299–1319, 1998.

Smola, A., Gretton, A., Song, L., and Sch¨olkopf, B. A Hilbert
space embedding for distributions. In Proceedings of the Con-
ference on Algorithmic Learning Theory (ALT), pp. 13–31.
Springer, 2007.

Smola, A. J., Mika, S., Sch¨olkopf, B., and Williamson, R. C. Reg-
ularized principal manifolds. J. Mach. Learn. Res., 1:179–209,
2001.

Sriperumbudur, B., Gretton, A., Fukumizu, K., Lanckriet, G., and
Sch¨olkopf, B. Hilbert space embeddings and metrics on prob-
ability measures. J. Mach. Learn. Res., 11:1517–1561, 2010.

Sriperumbudur, B., Fukumizu, K., and Lanckriet, G. Universality,
characteristic kernels and RKHS embedding of measures. J.
Mach. Learn. Res., 12:2389–2410, 2011.

Steinwart, I. and Christmann, A.

Support Vector Machines.

Springer, 2008.

Welling, M. and Teh, Y.W. Bayesian learning via stochastic gra-
dient Langevin dynamics.
In Proc. of the 28th International
Conference on Machine Learning (ICML), pp. 681–688, 2011.

Williams, C.K.I. and Barber, D. Bayesian classiﬁcation with
IEEE Transactions on Pattern Analysis

Gaussian processes.
and Machine Intelligence, 20(12):1342–1351, 1998.

References

Andrieu, C. and Roberts, G.O. The pseudo-marginal approach
for efﬁcient Monte Carlo computations. Ann. Statist., 37(2):
697–725, 2009.

Andrieu, C. and Thoms, J. A tutorial on adaptive MCMC. Statis-

tics and Computing, 18(4):343–373, 2008.

Bache, K. and Lichman, M. UCI Machine Learning Repository,

2013. URL http://archive.ics.uci.edu/ml.

Baker, C. Joint measures and cross-covariance operators. Trans-
actions of the American Mathematical Society, 186:273–289,
1973.

Bakir, G., Weston, J., and Sch¨olkopf, B. Learning to ﬁnd pre-
In Advances in Neural Information Processing Sys-

images.
tems 16. MIT Press, 2003.

Berlinet, A. and Thomas-Agnan, C. Reproducing Kernel Hilbert

Spaces in Probability and Statistics. Kluwer, 2004.

Borgwardt, K. M., Gretton, A., Rasch, M. J., Kriegel, H.-P.,
Sch¨olkopf, B., and Smola, A. J. Integrating structured biologi-
cal data by kernel maximum mean discrepancy. Bioinformatics
(ISMB), 22(14):e49–e57, 2006.

Filippone, M. and Girolami, M. Pseudo-marginal Bayesian in-
ference for Gaussian Processes. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2014. doi: TPAMI.2014.
2316530.

Fukumizu, K., Bach, F. R., and Jordan, M. I. Dimensionality re-
duction for supervised learning with reproducing kernel Hilbert
spaces. J. Mach. Learn. Res., 5:73–99, 2004.

Gelman, A., Roberts, G. O., and Gilks, W. R. Efﬁcient Metropo-
In Bayesian statistics, 5 (Alicante, 1994),

lis jumping rules.
Oxford Sci. Publ., pp. 599–607. 1996.

Girolami, M. and Calderhead, B. Riemann manifold Langevin
and Hamiltonian Monte Carlo methods. Journal of the Royal
Statistical Society: Series B, 73(2):123–214, 2011.

Gretton, A., Borgwardt, K., Rasch, M., Sch¨olkopf, B., and Smola,
A. A kernel method for the two-sample problem. In Advances
in Neural Information Processing Systems 19, pp. 513–520,
2007.

Haario, H., Saksman, E., and Tamminen, J. Adaptive Proposal
Distribution for Random Walk Metropolis Algorithm. Comput.
Stat., 14(3):375–395, 1999.

Haario, H., Saksman, E., and Tamminen, J. An adaptive Metropo-

lis algorithm. Bernoulli, 7(2):223–242, 2001.

Murray, I. and Adams, R.P. Slice sampling covariance hyperpa-
In Advances in Neural

rameters of latent Gaussian models.
Information Processing Systems 23. 2012.

Roberts, G.O. and Rosenthal, J.S. Coupling and ergodicity of
J. Appl.

adaptive Markov chain Monte Carlo algorithms.
Probab., 44(2):458–475, 03 2007.

Roberts, G.O. and Stramer, O.

Langevin diffusions and
Metropolis-Hastings algorithms. Methodol. Comput. Appl.
Probab., 4:337–358, 2003.

A. Proofs

Kernel Adaptive Metropolis Hastings

Proposition 1. Let k be a differentiable positive deﬁnite kernel. Then

∇xk(x, x)

|x=y −

2

∇xk(x, y)

|x=y = 0.

Proof. Since k is a positive deﬁnite kernel there exists a Hilbert space
k(x, x′) =
ψ :
H →
to each y
the chain rule for

, such that
ϕ, where
h
(Rd, R) of τ at y, which
Rd associates a bounded linear operator from Rd to R (Steinwart & Christmann, 2008, Deﬁnition A.5.14). By

H
|x=y from the Fr´echet derivative Dτ (y)

iH. Consider ﬁrst the map τ : Rd
2
H . We can obtain
k

→ H
R, deﬁned by τ (x) = k(x, x). We write τ = ψ
◦

ϕ(x), ϕ(x′)
f
k

∇xk(x, x)

R, ψ(f ) =

and a feature map ϕ : Rd

∈ B

→

∈

Fr´echet derivatives (Steinwart & Christmann, 2008, Lemma A.5.15(b)), the value of Dτ (y) at some x′

Rd is

∈

[Dτ (y)] (x′) = [Dψ (ϕ(y))

Dϕ(y)] (x′),

◦

where Dϕ(y)
differentiable function (Steinwart & Christmann, 2008, Section 4.3). It is readily shown that Dψ [ϕ(y)] = 2
that

, R). The derivative Dϕ of the feature map exists whenever k is a
·iH, so

), and Dψ (ϕ(y))

ϕ(y),
h

(Rd,

(
H

∈ B

∈ B

H

Next, we consider the map κy(x) = k(x, y) =
a linear scalar function on

, Dψy (f ) =

H

[Dτ (y)] (x′) = 2

ϕ(y), [Dϕ(y)] (x′)
h
iH, i.e., κy = ψy ◦
ϕ(x), ϕ(y)
h
·iH. Again, by the chain rule:
ϕ(y),
h

iH .
ϕ where ψy(f ) =

[Dκy(y)] (x′) = [Dψy (ϕ(y))
=

◦

ϕ(y), [Dϕ(y)] (x′)
h

Dϕ(y)] (x′)
iH ,

f, ϕ(y)
h

iH. Since ψy is

2Dκy(y)) (x′) = 0, for all x′
and thus (Dτ (y)
derivatives can also be written as inner products with the gradients, (
(Dτ (y)

Rd, which proves the claim.

Rd, and we obtain equality of operators. Since Fr´echet
|x=y)⊤ x′ =

∇xk(x, x)

∇xk(x, y)

2Dκy(y)) (x′) = 0,

|x=y −

−

∈

2

x′
∀

∈

y) =

·|

N

(y, γ2I + ν2Mz,yHM ⊤

z,y).

−
Proposition 2. qz(

Proof. We start with

p(β)p(x∗

y, β) =

|

1
n+d

exp

−

(cid:18)

(2π)

exp

2 γdνn
1
2γ2 (x∗

y

−

1
2ν2 β⊤β

(cid:19)
Mz,yHβ)⊤ (x∗

−
(cid:18)
1
n+d

=

exp

(2π)

exp

2 γdνn
1
2

−

β⊤

(cid:18)

(cid:18)

(cid:18)

·

·

−
1
2γ2 (x∗
1
γ2 HM ⊤

−

(cid:18)
1
ν2 I +

−

y

−

−

Mz,yHβ)

(cid:19)

y)⊤ (x∗

y)

−

z,yMz,yH

β

(cid:19)

2
γ2 β⊤HM ⊤

z,y(x∗

y)

−

.

(cid:19)(cid:19)

(cid:19)

−

Now, we set

and application of the standard Gaussian integral

Σ−1 =

µ =

z,yMz,yH

1
ν2 I +
1
γ2 ΣHM ⊤

1
γ2 HM ⊤
z,y(x∗

y),

−

β⊤Σ−1β

2β⊤Σ−1µ

dβ =

exp

ˆ

1
2

−

−

(cid:18)
(2π)n/2√det Σ exp

(cid:0)

(cid:19)
(cid:1)
µ⊤Σ−1µ

1
2

(cid:18)

,

(cid:19)

This is just a d-dimensional Gaussian density where both the mean and covariance will, in general, depend on y. Let us
consider the exponent

leads to

Kernel Adaptive Metropolis Hastings

qz(x∗

y) =

|

√det Σ

exp

2 γdνn
1
2

(cid:18)
µ⊤Σ−1µ

d

(2π)

exp

·

(cid:18)

.

(cid:19)

1
2γ2 (x∗

−

−

y)⊤ (x∗

y)

−

(cid:19)

−

1
2γ2 (x∗
1
1
γ2 (x∗
2 (

−

−

y)⊤ (x∗

y) +

µ⊤Σ−1µ =

1
2

−

y)⊤ (x∗

y)

−

−

1
γ4 (x∗

−

−

y)⊤ Mz,yHΣHM ⊤

z,y (x∗

y)

=

−

)

1
2
n
γ2 Mz,yHΣHM ⊤

−

1

(x∗

y)⊤ R−1 (x∗

y)

,

−

−

o

R = γ2(I

= γ2

−

1
γ2 Mz,yHΣHM ⊤
γ2Σ−1

I + Mz,yH

z,y)−1

(cid:16)

(cid:0)

= γ2

I + Mz,yH

γ2
ν2 I
= γ2I + ν2Mz,yHM ⊤

 

(cid:18)

z,y.

(cid:19)

HM ⊤

z,yMz,yH

−1

HM ⊤
z,y

(cid:1)

(cid:17)

−
−1

HM ⊤
z,y

!

where R−1 = 1

γ2 (I

−

z,y). We can simplify the covariance R using the Woodbury identity to obtain:

Therefore, the proposal density is qz(

y) =

(y, γ2I + ν2Mz,yHM ⊤

z,y).

·|

N

B. Further details on synthetic experiments

Proposal contours for the Flower target. The d-dimensional ﬂower target

(r0, A, ω, σ) is given by

(x; r0, A, ω, σ) = exp

F

 − p

1 + x2
x2

2 −

r0 −

F
A cos (ωatan2 (x2, x1))
2σ2

(x3:d; 0, I).

! N

This distribution concentrates around the r0-circle with a periodic perturbation (with amplitude A and frequency ω) in the
ﬁrst two dimensions. For A = 0, we obtain a band around the r0-circle, which we term the ring target. Figure 6 gives the
contour plots of the MCMC Kameleon proposal distributions on two instances of the ﬂower target.

Convergence statistics for the Banana target. Figure 7 illustrates how the norm of the mean and quantile deviation
(shown for 0.5-quantile) for the strongly twisted Banana target decrease as a function of the number of iterations. This
shows that the trends observed in the main text persist along the evolution of the whole chain.

C. Principal Components Proposals

An alternative approach to the standard adaptive Metropolis, discussed in Andrieu & Thoms (2008, Algorithm 8), is to
m
extract m
j=1 from the estimated covariance matrix Σz and use the
(λj , vj)
}
{
proposal that takes form of a mixture of one-dimensional random walks along the principal eigendirections

d principal eigenvalue-eigenvector pairs

≤

qz (

y) =

·|

(y, ν2

j λjvjv⊤

j ).

ωjN

m

j=1
X

(7)

Kernel Adaptive Metropolis Hastings

Figure6. 95% contours (red) of proposal distributions evaluated at a number of points, for the ﬂower and the ring target. Underneath are
the density heatmaps, and the samples (blue) used to construct the proposals.

12

2

10
(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13)
8

X
ˆE

n
a
e
m
e
h
t

f
o
m
r
o
n

6

4

2

0

KAMH-LS
AM-LS
AM-FS
SM

e
c
n
e
r
e
f
f
i
d

e
l
i
t
n
a
u
q
-
5
.
0

0.12

0.10

0.08

0.06

0.04

0.02

0.00

0

KAMH-LS
AM-LS
AM-FS
SM

5000

10000

15000

20000

25000

30000

35000

40000

45000

5000

10000

15000

20000

25000

30000

35000

40000

45000

number of samples

number of samples

Figure7. Comparison of SM, AM-FS, AM-LS and KAMH-LS in terms of the norm of the estimated mean (left) and in terms of the
deviation from the 0.5-quantile (right) on the strongly twisted Banana distribution. The results are averaged over 20 chains for each
sampler. Error bars represent 80%-conﬁdence intervals.

Kernel Adaptive Metropolis Hastings

x∗ = y + ρνj

λjvj,

In other words, given the current chain state y, the j-th principal eigendirection is chosen with probability ωj (choice
ωj = λj /

m
l=1 λl is suggested), and the proposed point is

(8)

(9)

P

with ρ
∼ N
eigenvalue.

≤
i=1 ˜α(j)

(0, 1). Note that each eigendirection may have a different scaling factor νj in addition to the scaling with the

p

We can consider an analogous version of the update (8) performed in the RKHS

with m

n principal eigenvalue-eigenfunction pairs

, y) + ρνj

f = k(
·
(λj , vj)
}

{

λjvj ,

p
m
j=1. It is readily shown that the eigenfunctions vj =
⊤

i

n

−

, zi)

[k(
·

µz] lie in the subspace

˜α(j)
1
(cid:16)
are proportional to the eigenvector of the centered kernel matrix HKH, with normalization chosen so that
P
˜α(j)
has form
(cid:0)

(cid:17)
=
2
2 = 1 (so that the eigenfunctions have the unit RKHS norm). Therefore, the update (9)
(cid:13)
(cid:13)

z induced by z, and that the coefﬁcients vectors ˜α(j) =

HKH ˜α(j) = λj

˜α(j)
n
2
Hk

· · ·
vjk

˜α(j)

(cid:13)
(cid:13)

H

k

(cid:1)

⊤

n

f = k(
·

, y) +

β(j)
i

, zi)

[k(
·

−

µz] ,

i=1
X

˜α(j)

λj ˜α(j). But α(j) =

2
where β(j) = ρνj
2 =
2
2 = 1. Therefore, the appropriate scaling with eigenvalues is already included in the β-coefﬁcients, just like in
λj
the MCMC Kameleon, where the β-coefﬁcients are isotropic.
(cid:13)
(cid:13)

λj ˜α(j) are themselves the (unit norm) eigenvectors of HKH, as

Now, we can construct the MCMC PCA-Kameleon by simply substituting β-coefﬁcients with ρνjα(j), where j is the
selected eigendirection, and νj is the scaling factor associated to the j-th eigendirection. We have the following steps:

α(j)

p

p

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

1. Perform eigendecomposition of HKH to obtain the m

n eigenvectors

≤

m
j=1 .

αj}
{

2. Draw j

Discrete [ω1, . . . , ωm]

∼
(0, 1)

3. ρ

∼ N

4. x∗

y, ρ, j

|

∼ N

(y + ρνjMz,yHα(j), γ2I) (d

1 normal in the original space)

×

Similarly as before, we can simplify the proposal by integrating out the scale ρ of the moves in the RKHS.

Proposition 3. qz(

y) =

·|

Proof. We start with

m

j=1 ωjN

P

(y, γ2I + ν2

j Mz,yHα(j)

α(j)

⊤

HM ⊤

z,y).

(cid:0)

(cid:1)

p(ρ)p(x∗

y, ρ, j)

|

∝

exp

−

(cid:20)

y)⊤(x∗

1
2γ2 (x∗
1
2 ( 

−

1 +

ν2
j
γ2

y)

−

(cid:21)
⊤

(cid:16)

(cid:17)

exp

·

"−

By substituting

α(j)

HM ⊤

z,yMz,yHα(j)

ρ2

2ρ

−

νj
γ2

!

⊤

α(j)

HM ⊤

z,y (x∗

y)

.

)#

−

(cid:16)

(cid:17)

σ−2 = 1 +

α(j)

HM ⊤

z,yMz,yHα(j),

⊤

µ = σ2

(cid:16)

(cid:17)
α(j)

⊤

HM ⊤

z,y (x∗

(cid:18)

(cid:16)

(cid:17)

−

y)

,

(cid:19)

ν2
j
γ2
νj
γ2

p (x∗

y, j)

|

∝

exp

"−

1
γ2 (x∗

−

y)⊤(x∗

y)

−

−

ν2
j σ2
γ4

−

(x∗

y)⊤ Mz,yHα(j)

α(j)

⊤

HM ⊤

z,y (x∗

y)

−

)#

(cid:16)

(cid:17)

we integrate out ρ to obtain:

Kernel Adaptive Metropolis Hastings

1
2 (
1
2

(x∗

= exp

y)⊤ R−1 (x∗

−

−

(cid:20)
j σ2
ν2
γ2 Mz,yHα(j)

y)

−

(cid:21)

⊤

α(j)

HM ⊤
z,y

(cid:0)

(cid:1)

(cid:17)

where R−1 = 1
γ2
to obtain:

I

−

(cid:16)

R = γ2(I

−

= γ2

I +

j σ2
ν2
γ2 Mz,yHα(j)
(cid:16)
ν2
j σ2
γ2 Mz,yHα(j)

α(j)

HM ⊤

z,y)−1

⊤

(cid:17)

ν2
j σ2
γ2

1

−

 

(cid:16)

(cid:17)

= γ2

I +

ν2
j
γ2 Mz,yHα(j)

⊤

α(j)

HM ⊤
z,y

!





 

= γ2I + ν2

j Mz,yHα(j)

(cid:16)
α(j)

⊤

(cid:17)
HM ⊤

z,y.

(cid:16)
The claim follows after summing over the choice j of the eigendirection (w.p. ωj).

(cid:17)

. We can simplify the covariance R using the Woodbury identity

⊤

α(j)

HM ⊤

z,yMz,yHα(j)

−1

!

⊤

α(j)

(cid:16)

(cid:17)

HM ⊤

z,y



