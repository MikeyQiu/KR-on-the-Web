6
1
0
2
 
g
u
A
 
9
2
 
 
]

V
C
.
s
c
[
 
 
3
v
0
5
8
0
0
.
6
0
6
1
:
v
i
X
r
a

Face Detection with End-to-End Integration of a
ConvNet and a 3D Model

Yunzhu Li1,∗, Benyuan Sun1,(cid:63), Tianfu Wu2 and Yizhou Wang1

1Nat’l Engineering Laboratory for Video Technology,
Key Laboratory of Machine Perception (MoE),
Cooperative Medianet Innovation Center, Shanghai
Sch’l of EECS, Peking University, Beijing, 100871, China
2Department of ECE and the Visual Narrative Cluster,
North Carolina State University, Raleigh, USA
{leo.liyunzhu, sunbenyuan, Yizhou.Wang}@pku.edu.cn, tianfu wu@ncsu.edu

Abstract. This paper presents a method for face detection in the wild,
which integrates a ConvNet and a 3D mean face model in an end-to-end
multi-task discriminative learning framework. The 3D mean face model
is predeﬁned and ﬁxed (e.g., we used the one provided in the AFLW
dataset [20]). The ConvNet consists of two components: (i) The face pro-
posal component computes face bounding box proposals via estimating
facial key-points and the 3D transformation (rotation and translation)
parameters for each predicted key-point w.r.t. the 3D mean face model.
(ii) The face veriﬁcation component computes detection results by prun-
ing and reﬁning proposals based on facial key-points based conﬁguration
pooling. The proposed method addresses two issues in adapting state-
of-the-art generic object detection ConvNets (e.g., faster R-CNN [32])
for face detection: (i) One is to eliminate the heuristic design of prede-
ﬁned anchor boxes in the region proposals network (RPN) by exploit-
ing a 3D mean face model. (ii) The other is to replace the generic RoI
(Region-of-Interest) pooling layer with a conﬁguration pooling layer to
respect underlying object structures. The multi-task loss consists of three
terms: the classiﬁcation Softmax loss and the location smooth l1-losses
[14] of both the facial key-points and the face bounding boxes. In ex-
periments, our ConvNet is trained on the AFLW dataset [20] only and
tested on the FDDB benchmark [19] with ﬁne-tuning and on the AFW
benchmark [41] without ﬁne-tuning. The proposed method obtains very
competitive state-of-the-art performance in the two benchmarks.

Keywords: Face Detection, Face 3D Model, ConvNet, Deep Learning,
Multi-task Learning

1 Introduction

1.1 Motivation and Objective

Face detection has been used as a core module in a wide spectrum of applications
such as surveillance, mobile communication and human-computer interaction. It

(cid:63) Y. Li and B. Sun contributed equally to this work and are joint ﬁrst authors.

2

Y. Li, B. Sun, T. Wu and Y. Wang

is arguably one of the most successful applications of computer vision. Face
detection in the wild continues to play an important role in the era of visual
big data (e.g., images and videos on the web and in social media). However, it
remains a challenging problem in computer vision due to the large appearance
variations caused by nuisance variabilities including viewpoints, occlusion, facial
expression, resolution, illumination and cosmetics, etc.

Fig. 1. Some example results in the FDDB face benchmark [19] computed by the
proposed method. For each testing image, we show the detection results (left) and the
corresponding heat map of facial key-points with the legend shown in the right most
column. (Best viewed in color)

It has been a long history that computer vision researchers study how to learn
a better representation for unconstrained faces [40,12,34]. Recently, together with
large-scale annotated image datasets such as the ImageNet [8], deep ConvNets
[22,21] have made signiﬁcant progress in generic object detection [14,32,16], as
well as in face detection [23,30]. The success is generally considered to be due
to the region proposal methods and region-based ConvNets (R-CNN) [15]. The
two factors used to be addressed separately (e.g., the popular combination of
the Selective Search [37] and R-CNNs pretrained on the ImageNet), and now
they are integrated through introducing the region proposal networks (RPNs)
as done in the faster-RCNN [32] or are merged into a single pipeline for speeding
up the detection as done in [31,26]. In R-CNNs, one key layer is the so-called
RoI (Region-of-Interest) pooling layer [14], which divides a valid RoI (e.g., an
object bounding box proposal) evenly into a grid with a ﬁxed spatial extent
(e.g., 7 × 7) and then uses max-pooling to convert the features inside the RoI
into a small feature map. In this paper, we are interested in adapting state-of-
the-art ConvNets of generic object detection (e.g., the faster R-CNN [32]) for
face detection by overcoming the following two limitations:

i) RPNs need to predeﬁne a number of anchor boxes (with diﬀerent aspect
ratios and sizes), which requires potentially tedious parameter tuning in
training and is sensitive to the (unknown) distribution of the aspect ratios
and sizes of the object instances in a random testing image.

Face Detection with a ConvNet and a 3D Model

3

Fig. 2. Illustration of the proposed method of an end-to-end integration of a ConvNet
and a 3D model for face detection (Top), and some intermediate and the ﬁnal detection
results for an input testing image (Bottom). See the legend for the classiﬁcation score
heat map in Figure 1. The 3D mean face model is predeﬁned and ﬁxed in both training
and testing. The key idea of the proposed method is to learn a ConvNet to estimate
the 3D transformation parameters (rotation and translation) w.r.t. the 3D mean face
model to generate accurate face proposals and predict the face key points. The proposed
ConvNet is trained in a multi-task discriminative training framework consisting of the
classiﬁcation Softmax loss and the location smooth l1-losses [14] of both the facial
key-points and the face bounding boxes. It is surprisingly simple w.r.t. its competitive
state-of-the-art performance compared to the other methods in the popular FDDB
benchmark [19] and the AFW benchmark [41]. See text for details.

ii) The RoI pooling layer in R-CNNs is predeﬁned and generic to all object cat-
egories without exploiting the underlying object structural conﬁgurations,
which either are available from the annotations in the training dataset (e.g.,
the facial landmark annotations in the AFLW dataset [20]) as done in [41]
or can be pursued during learning (such as the deformable part-based mod-
els [10,30]).

To address the two above issues in learning ConvNets for face detection, we
propose to integrate a ConvNet and a 3D mean face model in an end-to-end
multi-task discriminative learning framework. Figure 1 shows some results of
the proposed method.

4

Y. Li, B. Sun, T. Wu and Y. Wang

1.2 Method Overview

Figure 2 illustrates the proposed method. We use 10 facial key-points in this pa-
per, including “LeftEyeLeftCorner”, “RightEyeRightCorner”, “LeftEar”, “Nose-
Left”, “NoseRight”, “RightEar”, “MouthLeftCorner”, “MouthRightCorner”, “Chin-
Center”, “CenterBetweenEyes” (see an example image in the left-top of Fig-
ure 2). The 3D mean face model is then represented by the corresponding ten
3D facial key-points. The architecture of our ConvNet is straight-forward when
taking into account a 3D model (see Section 3.2 for details).

The key idea is to learn a ConvNet to (i) estimate the 3D trans-
formation parameters (rotation and translation) w.r.t. the 3D mean
face model for each detected facial key-point so that we can generate
face bounding box proposals and (ii) predict facial key-points for each
face instance more accurately. Leveraging the 3D mean face model is able
to “kill two birds with one stone”: Firstly, we can eliminate the manually heuris-
tic design of anchor boxes in RPNs. Secondly, instead of using the generic RoI
pooling, we devise a “conﬁguration pooling” layer so as to respect the object
structural conﬁgurations in a meaningful and principled way. In other words,
we propose to learn to compute the proposals in a straight-forward top-down
manner, instead of to design the bottom-up heuristic and then learn related re-
gression parameters. To do so, we assume a 3D mean face model is available and
facial key-points are annotated in the training dataset. Thanks to many excellent
existing work in collecting and annotating face datasets, we can easily obtain
both for faces nowadays. In learning, we have multiple types of losses involved
in the objective loss function, including classiﬁcation Softmax loss and location
smooth l1-loss [14] of facial key-points, and location smooth l1-loss of face bound-
ing boxes respectively, so we formulate the learning of the proposed ConvNet
under the multi-task discriminative deep learning framework (see Section 3.3).
In summary, we provide a clean and straight-forward solution for end-to-end
integration of a ConvNet and a 3D model for face detection 1. In addition to the
competitive performance w.r.t the state-of-the-art face detection methods on the
FDDB and AFW benchmarks, the proposed method is surprisingly simple and
it is able to detect challenging faces (e.g., small, blurry, heavily occluded and
extreme poses).

Potentially, the proposed method can be utilized to learn to detect other
rigid or semi-rigid object categories (such as cars) if the required information
(such as the 3D model and key-point/part annotation) are provided in training.

2 Related Work

There are a tremendous amount of existing works on face detection or generic
object detection. We refer to [40] for a more thorough survey on face detection.
We discuss some of the most relevant ones in this section.
1 We use the open source deep learning package, MXNet [5], in our implementa-
tion. The full source code is released at https://github.com/tfwu/FaceDetection-
ConvNet-3D

Face Detection with a ConvNet and a 3D Model

5

In human/animal vision, how the brain distills a representation of objects
from retinal input is one of the central challenges for systems neuroscience, and
many works have been focused on the ecologically important class of objects–
faces. Studies using fMRI experiments in the macaque reveal that faces are
represented by a system of six discrete, strongly interconnected regions which
illustrates hierarchical information processing in the brain [12], as well as some
other results [34]. These ﬁndings provide some biologically-plausible evidences
for supporting the usage of deep learning based approaches in face detection and
analysis.

The seminal work of Viola and Jones [38] made face detection by a computer
vision system feasible in real world applications, which trained a cascade of Ad-
aBoost classiﬁers using Haar wavelet features. Many works followed this direction
with diﬀerent extensions proposed in four aspects: appearance features (beside
Haar) including Histogram of Oriented Gradients (HOG) [7], Aggregate Channel
Features (ACF) [9], Local Binary Pattern (LBP) features [1] and SURF [3], etc.;
detector structures (beside cascade) including the the scalar tree [11] and the
width-ﬁrst-search tree [18], etc.; strong classiﬁer learning (beside AdaBoost) in-
cluding RealBoost [33] and GentleBoost [13], ect; weak classiﬁer learning (beside
stump function) including the histogram method [25] and the joint binarizations
of Haar-like feature [28], etc..

Most of the recent face detectors are based on the deformable part-based
model (DPM) [10,41,27] with HOG features used, where a face is represented by
a collection of parts deﬁned based on either facial landmarks or heuristic pursuit
as done in the original DPM. [27] showed that a properly trained vanilla DPM
can yield signiﬁcant improvement for face detection.

More recent advances in deep learning [22,21] further boosted the face detec-
tion performance by learning more discriminative features from large-scale raw
data, going beyond those handcrafted ones. In the FDDB benchmark, most of the
face detectors with top performance are based on ConvNets [30,23], combining
with cascade [23] and more explicit structure [39].

3D information has been exploited in learning object models in diﬀerent ways.
Some works [29,35] used a mixture of 3D view based templates by dividing the
view sphere into a number of sectors. [24,17] utilized 3D models in extracting
features and inferring the object pose hypothesis based on EM or DP. [36] used
a 3D face model for aligning faces in learning ConvNets for face recognition. Our
work resembles [2] in exploiting 3D model in face detection, which obtained very
good performance in the FDDB benchmark. [2] computes meaningful 3D pose
candidates by image-based regression from detected face key-points with tradi-
tional handcrafted features, and veriﬁes the 3D pose candidates by a parameter
sensitive classiﬁer based on diﬀerence features relative to the 3D pose. Our work
integrates a ConvNet and a 3D model in an end-to-end multi-task discriminative
learning fashion, which is more straightforward and simpler compared to [2].

Our Contributions. The proposed method contributes to face detection in

three aspects.

6

Y. Li, B. Sun, T. Wu and Y. Wang

i) It presents a simple yet eﬀective method to integrate a ConvNet and a 3D
model in an end-to-end learning with multi-task loss used for face detection
in the wild.

ii) It addresses two limitations in adapting the state-of-the-art faster RCNN [32]
for face detection: eliminating the heuristic design of anchor boxes by lever-
aging a 3D model, and replacing the generic and predeﬁned RoI pooling
with a conﬁguration pooling which exploits the underlying object structural
conﬁgurations.

iii) It obtains very competitive state-of-the-art performance in the FDDB [19]

and AFW [41] benchmarks.

Paper Organization. The remainder of this paper is organized as follows. Sec-
tion 3 presents the method of face detection using a 3D model and details of our
ConvNet including its architecture and training procedure. Section 4 presents
details of experimental settings and shows the experimental results in the FDDB
and AFW benchmarks. Section 5 ﬁrst concludes this paper and then discuss some
on-going and future work to extend the proposed work.

3 The Proposed Method

In this section, we introduce the notations and present details of the proposed
method.

3.1

3D Mean Face Model and Face Representation

In this paper, a 3D mean face model is represented by a collection of n 3D
key-points in the form of (x, y, z) and then is denoted by a n × 3 matrix, F (3).
Usually, each key-point has its own semantic name. We use the 3D mean face
model in the AFLW dataset [20] which consists of 21 key-points. We select 10
key-points as stated above.

A face, denoted by f , is presented by its 3D transformation parameters, Θ,
for rotation and translation, and a collection of 2D key-points, F (2), in the form
of (x, y) (with the number being less than or equal to n). Hence, f = (Θ, F (2)).
The 3D transformation parameters Θ are deﬁned by,

Θ = (µ, s, A(3)),

where µ represents a 2D translation (dx, dy), s a scaling factor, and A(3) a 3 × 3
rotation matrix. We can compute the predicted 2D key-points by,

ˆF (2) = µ + s · π(A(3) · F (3)),

where π() projects a 3D key-point to a 2D one, that is, π : R3 → R2 and
π(x, y, z) = (x, y). Due to the projection π(), we only need 8 parameters out of
the original 12 parameters. Let A(2) denote a 2 × 3 matrix, which is composed
by the top two rows of A(3). We can re-produce the predicted 2D key-points by,

ˆF (2) = µ + A(2) · F (3)

(1)

(2)

(3)

Face Detection with a ConvNet and a 3D Model

7

which makes it easy to implement the computation of back-propagation in train-
ing our ConvNet.

Note that we use the ﬁrst sector in a 4-sector X-Y coordinate system to
deﬁne all the positions, that is, the origin point (0, 0) is deﬁned by the left-
bottom corner in an image lattice.

In face datasets, faces are usually annotated with bounding boxes. In the
FDDB benchmark [19], however, faces are annotated with ellipses and detection
performance are evaluated based on ellipses. Given a set of predicted 2D key-
points ˆF (2), we can compute proposals in both ellipse form and bounding box
form.

Computing a Face Ellipse and a Face Bounding Box based on a set of Pre-
dicted 2D Key-Points. For a given ˆF (2), we ﬁrst predict the position of the top
of head by,

(cid:0) x
y

(cid:1)

TopOfHead = 2 × (cid:0) x

y

(cid:1)

CenterBetweenEyes − (cid:0) x

y

(cid:1)

ChinCenter.

Based on the keypoints of a face proposal, we can compute its ellipse and bound-
ing box.

Face Ellipse. We ﬁrst compute the outer rectangle. We use as one axis the
line segment between the top-of-the-head key-point and the chin key-point, and
then compute the minimum rectangle, usually a rotated rectangle, which covers
all the key-points. Then, we can compute the ellipse using the two edges of the
(rotated) rectangle as the major and minor axes respectively.

Face Bounding Box. We compute a face bounding box by the minimum up-
right rectangle which covers all the key-points, which is also adopted in the
FDDB benchmark [19].

3.2 The Architecture of Our ConvNet

As illustrated in Figure 2, the architecture of our ConvNet consists of:

i) Convolution, ReLu and MaxPooling Layers. We adopt the VGG [4] design
in our experiments which has shown superior performance in a series of tasks.
There are 5 groups and each group has 3 convolution and ReLu consecutive
layers followed by a MaxPooling layer except for the 5th group. The spatial
extent of the ﬁnal feature map is of 16 times smaller than that of an input image
due to the sub-sampling.

ii) An Upsampling Layer. Since we will measure the location diﬀerence be-
tween the input facial key-points and the predicted ones, we add an upsampling
layer to compensate the sub-sampling eﬀects in previous layers. It is implemented
by deconvolution. We upsample the feature maps to 8 times bigger in size (i.e.,
the upsampled feature maps are still quarter size of an input image) consider-
ing the trade-oﬀ between key-point location accuracy, memory consumption and
computation eﬃciency.

iii) A Facial Key-point Label Prediction Layer. There are 11 labels (10 facial
key-points and 1 background class). It is used to compute the classiﬁcation
Softmax loss based on the input in training.

8

Y. Li, B. Sun, T. Wu and Y. Wang

iv) A 3D Transformation Parameter Estimation Layer. This is the key ob-
servation in this paper. Originally, there are 12 parameters in total consisting of
2D translation, scaling and 3 × 3 rotation matrix. Since we focus on the 2D pro-
jected key-points, we only need to account for 8 parameters (see the derivation
above).

v) A Face Proposal Layer. At each position, based on the 3D mean face
model and the estimated 3D transformation parameters, we can compute a face
proposal consisting of 10 predicted facial key-points and the corresponding face
bounding box. The score of a face proposal is the sum of log probabilities of
the 10 predicted facial key-points. The predicated key-points will be used to
compute the smooth l1 loss [14] w.r.t. the ground-truth key-points. We apply
the non-maximum suppression (NMS) to the face proposals in which the overlap
between two bounding boxes a and b is computed by |a|∩|b|
(where | · | represents
the area of a bounding box), instead of the traditional intersection-over-union,
accounting for the fact that it is rarely observed that one face is inside another
one.

|b|

vi) A Conﬁguration Pooling Layer. After NMS, for each face proposal, we
pool the features based on the predicted 10 facial key-points. Here, for simplicity,
we use all the 10 key-points without considering the invisibilities of certain key-
points in diﬀerent face examples.

vii) A Face Bounding Box Regression Layer. It is used to further reﬁne face
bounding boxes in the spirit similar to the method [14]. Based on the conﬁgura-
tion pooling, we add two fully-connected layers to implement the regression. It
is used to compute the smooth l1 loss of face bounding boxes.

Denote by ω all the parameters in our ConvNet, which will be estimated

through multi-task discriminative end-to-end learning.

3.3 The End-to-End Training

Input Data. Denote by C = {0, 1, · · · , 10} as the key-point labels where (cid:96) = 0
represents the background class. We use the image-centric sampling trick as done
in [14,32]. Without loss of generality, considering a training image with only one
face appeared, we have its bounding box, B = (x, y, w, h) and m 2D key-points
(m ≤ 10), {(xi, (cid:96)i)m
i=1} where xi = (xi, yi) is the 2D position of the ith key-point
and (cid:96)i ≥ 1 ∈ C. We randomly sample m locations outside the face bounding box
B as the background class, {(xi, (cid:96)i)2m
i=m+1} (where (cid:96)i = 0, ∀i > m). Note that
in our ConvNet, we use the coordinate of the upsampled feature map which is
half size along both axes of the original input. All the key-points and bounding
boxes are deﬁned accordingly based on ground-truth annotation.

The Classiﬁcation Softmax Loss of Key-point Labels. At each position

xi, our ConvNet outputs a discrete probability distribution, pxi = (pxi
over the 11 classes, which is computed by the Softmax over the 11 scores as
usual [21]. Then, we have the loss,

0 , pxi

1 , · · · , pxi

10),

Lcls(ω) = −

1
2m

2m
(cid:88)

i=1

log(pxi
(cid:96)i

)

(4)

Face Detection with a ConvNet and a 3D Model

9

The Smooth l1 Loss of Key-point Locations. At each key-point location
xi ((cid:96)i ≥ 1), we compute a face proposal based on the estimated 3D parameters
and the 3D mean face, denoted by {(ˆx(i)
j=1} the predicted 10 keypoints.
So, for each key-point location xi, we will have m predicted locations, denoted
by ˆxi,j (j = 1, · · · , m). We follow the deﬁnition in [14] to compute the smooth
l1 loss for each axis individually.
m
(cid:88)

j , ˆ(cid:96)(i)

j )10

m
(cid:88)

(cid:88)

Smoothl1(ti − ˆti,j)

Lpt

loc(ω) =

1
m2

i=1

j=1

t∈{x,y}

where the smooth term is deﬁned by,
(cid:40)

Smoothl1 (a) =

0.5a2
|a| − 0.5

if |a| < 1
otherwise.

Faceness Score. The faceness score of a face proposal in our ConvNet is

computed by the sum of log probabilities of the predicted key-points,

Score(ˆxi, ˆ(cid:96)i) =

10
(cid:88)

i=1

log(pˆxi
ˆ(cid:96)i

)

where for simplicity we do not account for the invisibilities of certain key-points.
So the current faceness score has the issue of potential double-counting, espe-
cially for low-resolution faces. We observed that it hurts the quantitative perfor-
mance in our experiments. We will address this issue in future work. See some
heat maps of key-points in Figure 1.

The Smooth l1 Loss of Bounding Boxes. For each face bounding box
proposal ˆB (after NMS), our ConvNet computes its bounding box regression
oﬀsets, t = (tx, ty, tw, th), where t speciﬁes a scale-invariant translation and
log-space height/width shift relative to a proposal, as done in [14,32]. For the
ground-truth bounding box B, we do the same parameterization and have v =
(vx, vy, vw, vh). Assuming that there are K bounding box proposals, we have,

Lbox

loc (ω) =

1
K

K
(cid:88)

(cid:88)

k=1

i∈{x,y,w,h}

Smoothl1(ti − vi)

So, the overall loss function is deﬁned by,

L(ω) = Lcls(ω) + Lpt

loc(ω) + Lbox

loc (ω),

where the third term depends on the output of the ﬁrst two terms, which makes
the loss minimization more challenging. We adopt a method to implement the
diﬀerentiable bounding box warping layer, similar to [6].

4 Experiments

In this section, we present the training procedure and implementation details
and then show evaluation results on the FDDB [19] and AFW [41] benchmarks.

(5)

(6)

(7)

(8)

(9)

10

Y. Li, B. Sun, T. Wu and Y. Wang

Category

Category
Background

Accuracy
97.94% LeftEyeLeftCorner

Accuracy
99.12%
95.50%
LeftEar
RightEyeRightCorner 94.57%
97.78%
NoseRight
98.48%
97.97%
MouthLeftCorner
91.44%
98.64%
98.65%
ChinCenter
96.04% AverageDetectionRate 97.50%

NoseLeft
RightEar
MouthRightCorner
CenterBetweenEyes

Table 1. Classiﬁcation accuracy of the key-points in the AFLW validation set at the
end training.

4.1 Experimental Settings

The Training Dataset. The only dataset we used for training our model is the
AFLW dataset [20], which contains 25, 993 annotated faces in real-world images.
The facial key-points are annotated upon visibility w.r.t. a 3D mean face model
with 21 landmarks. Of the images 70% are used for training while the remaining
is reserved as a validation set.

Training process. For convenience, the short edge of every image is resized
to 600 pixels while preserving the aspect ratio (as done in the faster RCNN [32]),
thus our model learns how to handle faces under various scale. To handle faces
of diﬀerent resolution, we randomly blur images using Gaussian ﬁlters in pre-
processing. Apart from the rescaling and blurring, no other preprocessing mech-
anisms (e.g., random crop or left-right ﬂipping) are used.

We adopt the method of image-centric sampling [14,32] which uses one im-
age at a time in training. Under the consideration that grids around the labeled
position share almost the same context information, thus the 3 × 3 grids around
every labeled key-point’s position are also regarded as the same positive exam-
ples, and we randomly choose the same amount of background examples outside
the bounding boxes. The convolution ﬁlters are initialized by the VGG-16 [4]
pretrained on the ImageNet [8]. We train the network for 13 epoch, and during
the process, the learning rate is modiﬁed from 0.01 to 0.0001.

4.2 Evaluation of the intermediate results

Key-points classiﬁcation in the validation dataset. As are shown by the
heat maps in Figure 1, our model is capable of detecting facial key-points with
rough face conﬁgurations preserved, which shows the eﬀectiveness of exploiting
the 3D mean face model. Table 1 shows the key-point classiﬁcation accuracy on
the validation set in the last epoch in training.

Face proposals. To evaluate the quality of our face proposals, we ﬁrst show
some qualitative results on the FDDB dataset in Fig. 3. These ellipses are directly
calculated from the predicted 3D transformation parameters, forming several
clusters around face instances. We also evaluate the quantitative results of face
proposals. After a non-maximum suppression of IoU 0.7, the recall rate of 93.67%
is obtained with average 34.4 proposals per image.

Face Detection with a ConvNet and a 3D Model

11

Fig. 3. Examples of face proposals computed using predicted 3D transformation pa-
rameters without non-maximum suppression. For clarity, we randomly sample 1/30 of
the original number of proposals.

Fig. 4. FDDB results based on discrete scores using face bounding boxes in evaluation.
The recall rates are computed against 2000 false positives.

4.3 Face Detection Results

To show the eﬀectiveness of our method, we test our model on two popular face
detection benchmarks: FDDB [19] and AFW [41].

Results on FDDB. FDDB is a challenge benchmark for face detection in
unconstrained environment, which contains the annotations for 5171 faces in a
set of 2845 images. We evaluate our results by using the evaluation code provided
by the FDDB authors. The results on the FDDB dataset are shown in Figure 4.
Our result is represented by ”Ours-Conv3D”, which surpasses the recall rate of
90% when encountering 2000 false positives and is competitive to the state-of-
the-art methods. We compare with published methods only. Only DP2MFD [30]
is slightly better than our model on discrete scores. It’s worth noting that we beat
all other methods on continuous scores. This is partly caused by the predeﬁned

12

Y. Li, B. Sun, T. Wu and Y. Wang

Fig. 5. FDDB results based on continuous scores using face ellipses in evaluation. The
recall rates are computed against 2000 false positives.

3D face model helps us better describe the pose and part locations of faces.
We refer to the FDDB result webpage2 for details of the published methods
evaluated on it (Fig. 4 and Fig. 5).

When comparing with recent work Faceness [39], we both recognize that one
of the central issues to alleviate the problems of the occlusion and pose variation
is to introduce facial part detector. However, our mechanism of computing face
bounding box candidates is more straight forward since we explicitly integrate
the structural information of a 3D mean face model instead of using a heuristic
way of assuming the facial part distribution over a bounding box.

Results on AFW. AFW dataset contains 205 images with faces in various
poses and view points. We use the evaluation toolbox provided by [27], which
contains updated annotations for the AFW dataset where the original annota-
tions are not comprehensive enough. Since the method of labeling face bounding
boxes in AFW is diﬀerent from that of in FDDB, we only use face proposals with-
out conﬁguration pooling and bounding box regression. The results on AFW are
shown in Figure 6.

In our current implementation, there is one major limitation that prevents us
from achieving better results. We do not explicitly handle invisible facial parts,
which would be harmful when calculating the faceness score according to Eqn. 7,
we will reﬁne the method and introduce mechanisms of handling the invisible
problem in future work. More detection results on both datasets are shown in
Figure 7 and Figure 8.

2 http://vis-www.cs.umass.edu/fddb/results.html

Face Detection with a ConvNet and a 3D Model

13

Fig. 6. Precision-recall curves on the AFW dataset (AP = average precision) without
conﬁguration pool and face bounding box regression used.

5 Conclusion and Discussion

We have presented a method of end-to-end integration of a ConvNet and a 3D
model for face detection in the wild. Our method is a clean and straightfor-
ward solution when taking into account a 3D model in face detection. It also
addresses two issues in state-of-the-art generic object detection ConvNets: elim-
inating heuristic design of anchor boxes by leveraging a 3D model, and overcom-
ing generic and predeﬁned RoI pooling by conﬁguration pooling which exploits
underlying object conﬁgurations. In experiments, we tested our method on two
benchmarks, the FDDB dataset and the AFW dataset, with very compatible
state-of-the-art performance obtained. We analyzed the experimental results and
pointed out some current limitations.

In our on-going work, we are working on addressing the doubling-counting
issue of the faceness score in the current implementation. We are also working on
extending the proposed method for other types of rigid/semi-rigid object classes
(e.g., cars). We expect that we will have a uniﬁed model for cars and faces which
can achieve state-of-the-art performance, which will be very useful in a lot of
practical applications such as surveillance and driveless cars.

Acknowledgement. Y. Li, B. Sun and Y. Wang were supported in part
by China 973 Program under Grant no. 2015CB351800, and NSFC-61231010,
61527804, 61421062, 61210005. T. Wu was supported by the ECE startup fund
201473-02119 at NCSU. T. Wu also gratefully acknowledge the support of NVIDIA
Corporation with the donation of one GPU.

14

Y. Li, B. Sun, T. Wu and Y. Wang

Fig. 7. Some qualitative results on the FDDB dataset

Fig. 8. Some qualitative results on the AFW dataset

Face Detection with a ConvNet and a 3D Model

15

References

1. Ahonen, T., Hadid, A., Pietik¨ainen, M.: Face description with local binary patterns:
Application to face recognition. IEEE Trans. Pattern Anal. Mach. Intell. 28(12),
2037–2041 (2006)

2. Barbu, A., Gramajo, G.: Face detection using a 3d model on face keypoints. CoRR

abs/1404.3596 (2014)

3. Bay, H., Ess, A., Tuytelaars, T., Gool, L.J.V.: Speeded-up robust features (SURF).

Computer Vision and Image Understanding 110(3), 346–359 (2008)

4. Chatﬁeld, K., Simonyan, K., Vedaldi, A., Zisserman, A.: Return of the devil in the
details: Delving deep into convolutional nets. In: British Machine Vision Conference
(2014)

5. Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao, T., Xu, B., Zhang,
C., Zhang, Z.: Mxnet: A ﬂexible and eﬃcient machine learning library for hetero-
geneous distributed systems. CoRR abs/1512.01274 (2015)

6. Dai, J., He, K., Sun, J.: Instance-aware semantic segmentation via multi-task net-

7. Dalal, N., Triggs, B.: Histograms of oriented gradients for human detection. In:

work cascades. In: CVPR (2016)

CVPR (2005)

8. Deng, J., Dong, W., Socher, R., Li, L., Li, K., Li, F.: Imagenet: A large-scale

hierarchical image database. In: CVPR. pp. 248–255 (2009)

9. Doll´ar, P., Appel, R., Belongie, S.J., Perona, P.: Fast feature pyramids for object

detection. IEEE Trans. Pattern Anal. Mach. Intell. 36(8), 1532–1545 (2014)
10. Felzenszwalb, P.F., Girshick, R.B., McAllester, D.A., Ramanan, D.: Object detec-
tion with discriminatively trained part-based models. IEEE Trans. Pattern Anal.
Mach. Intell. 32(9), 1627–1645 (2010)

11. Fleuret, F., Geman, D.: Coarse-to-ﬁne face detection. International Journal of

Computer Vision 41(1/2), 85–107 (2001)

12. Freiwald, W.A., Tsao, D.Y.: Functional compartmentalization and viewpoint gen-
eralization within the macaque face-processing system. Science 330(6005), 845–851
(2010)

13. Friedman, J., Hastie, T., Tibshirani, R.: Additive logistic regression: a statistical

view of boosting. Annals of Statistics 28, 2000 (1998)

14. Girshick, R.: Fast R-CNN. In: ICCV (2015)
15. Girshick, R.B., Donahue, J., Darrell, T., Malik, J.: Region-based convolutional
networks for accurate object detection and segmentation. IEEE Trans. Pattern
Anal. Mach. Intell. 38(1), 142–158 (2016)

16. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

In: CVPR (2016)

17. Hu, W., Zhu, S.: Learning 3d object templates by quantizing geometry and ap-
pearance spaces. IEEE Trans. Pattern Anal. Mach. Intell. 37(6), 1190–1205 (2015)
18. Huang, C., Ai, H., Li, Y., Lao, S.: High-performance rotation invariant multiview
face detection. IEEE Trans. Pattern Anal. Mach. Intell. 29(4), 671–686 (2007)
19. Jain, V., Learned-Miller, E.: Fddb: A benchmark for face detection in un-
constrained settings. Tech. Rep. UM-CS-2010-009, University of Massachusetts,
Amherst (2010)

20. Koestinger, M., Wohlhart, P., Roth, P.M., Bischof, H.: Annotated facial landmarks
in the wild: A large-scale, real-world database for facial landmark localization.
In: First IEEE International Workshop on Benchmarking Facial Image Analysis
Technologies (2011)

16

Y. Li, B. Sun, T. Wu and Y. Wang

21. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-

volutional neural networks. In: NIPS (2012)

22. LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based learning applied to

document recognition. Proceedings of the IEEE 86(11), 2278–2324 (1998)

23. Li, H., Lin, Z., Shen, X., Brandt, J., Hua, G.: A convolutional neural network

cascade for face detection. In: CVPR (2015)

24. Liebelt, J., Schmid, C.: Multi-view object class detection with a 3d geometric

model. In: CVPR (2010)

25. Liu, C., Shum, H.: Kullback-leibler boosting. In: CVPR (2003)
26. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.:
SSD: Single shot multibox detector. arXiv preprint arXiv:1512.02325 (2015)
27. Mathias, M., Benenson, R., Pedersoli, M., Gool, L.V.: Face detection without bells

and whistles. In: ECCV (2014)

28. Mita, T., Kaneko, T., Hori, O.: Joint haar-like features for face detection. In: ICCV

29. Payet, N., Todorovic, S.: From contours to 3d object detection and pose estimation.

(2005)

In: ICCV (2011)

30. Ranjan, R., Patel, V.M., Chellappa, R.: A deep pyramid deformable part model
for face detection. In: IEEE 7th International Conference on Biometrics Theory,
Applications and Systems (2015)

31. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Uniﬁed,

real-time object detection. In: CVPR (2016)

32. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object

detection with region proposal networks. In: NIPS (2015)

33. Schapire, R.E., Singer, Y.: Improved boosting algorithms using conﬁdence-rated

predictions. Machine Learning 37(3), 297–336 (1999)

34. Sinha, P., Balas, B., Ostrovsky, Y., Russell, R.: Face recognition by humans: 19
results all computer vision researchers should know about. Proceedings of the IEEE
94(11), 1948–1962 (2006)

35. Su, H., Sun, M., Li, F., Savarese, S.: Learning a dense multi-view representation
for detection, viewpoint classiﬁcation and synthesis of object categories. In: ICCV
(2009)

36. Taigman, Y., Yang, M., Ranzato, M., Wolf, L.: Deepface: Closing the gap to human-

level performance in face veriﬁcation. In: CVPR (2014)

37. Uijlings, J.R.R., van de Sande, K.E.A., Gevers, T., Smeulders, A.W.M.: Selective
search for object recognition. International Journal of Computer Vision 104(2),
154–171 (2013)

38. Viola, P.A., Jones, M.J.: Robust real-time face detection. International Journal of

Computer Vision 57(2), 137–154 (2004)

39. Yang, S., Luo, P., Loy, C.C., Tang, X.: From facial parts responses to face detection:

A deep learning approach. In: ICCV (2015)

40. Zafeiriou, S., Zhang, C., Zhang, Z.: A survey on face detection in the wild. Comput.

Vis. Image Underst. 138(C), 1–24 (Sep 2015)

41. Zhu, X., Ramanan, D.: Face detection, pose estimation, and landmark localization

in the wild. In: CVPR (2012)

6
1
0
2
 
g
u
A
 
9
2
 
 
]

V
C
.
s
c
[
 
 
3
v
0
5
8
0
0
.
6
0
6
1
:
v
i
X
r
a

Face Detection with End-to-End Integration of a
ConvNet and a 3D Model

Yunzhu Li1,∗, Benyuan Sun1,(cid:63), Tianfu Wu2 and Yizhou Wang1

1Nat’l Engineering Laboratory for Video Technology,
Key Laboratory of Machine Perception (MoE),
Cooperative Medianet Innovation Center, Shanghai
Sch’l of EECS, Peking University, Beijing, 100871, China
2Department of ECE and the Visual Narrative Cluster,
North Carolina State University, Raleigh, USA
{leo.liyunzhu, sunbenyuan, Yizhou.Wang}@pku.edu.cn, tianfu wu@ncsu.edu

Abstract. This paper presents a method for face detection in the wild,
which integrates a ConvNet and a 3D mean face model in an end-to-end
multi-task discriminative learning framework. The 3D mean face model
is predeﬁned and ﬁxed (e.g., we used the one provided in the AFLW
dataset [20]). The ConvNet consists of two components: (i) The face pro-
posal component computes face bounding box proposals via estimating
facial key-points and the 3D transformation (rotation and translation)
parameters for each predicted key-point w.r.t. the 3D mean face model.
(ii) The face veriﬁcation component computes detection results by prun-
ing and reﬁning proposals based on facial key-points based conﬁguration
pooling. The proposed method addresses two issues in adapting state-
of-the-art generic object detection ConvNets (e.g., faster R-CNN [32])
for face detection: (i) One is to eliminate the heuristic design of prede-
ﬁned anchor boxes in the region proposals network (RPN) by exploit-
ing a 3D mean face model. (ii) The other is to replace the generic RoI
(Region-of-Interest) pooling layer with a conﬁguration pooling layer to
respect underlying object structures. The multi-task loss consists of three
terms: the classiﬁcation Softmax loss and the location smooth l1-losses
[14] of both the facial key-points and the face bounding boxes. In ex-
periments, our ConvNet is trained on the AFLW dataset [20] only and
tested on the FDDB benchmark [19] with ﬁne-tuning and on the AFW
benchmark [41] without ﬁne-tuning. The proposed method obtains very
competitive state-of-the-art performance in the two benchmarks.

Keywords: Face Detection, Face 3D Model, ConvNet, Deep Learning,
Multi-task Learning

1 Introduction

1.1 Motivation and Objective

Face detection has been used as a core module in a wide spectrum of applications
such as surveillance, mobile communication and human-computer interaction. It

(cid:63) Y. Li and B. Sun contributed equally to this work and are joint ﬁrst authors.

2

Y. Li, B. Sun, T. Wu and Y. Wang

is arguably one of the most successful applications of computer vision. Face
detection in the wild continues to play an important role in the era of visual
big data (e.g., images and videos on the web and in social media). However, it
remains a challenging problem in computer vision due to the large appearance
variations caused by nuisance variabilities including viewpoints, occlusion, facial
expression, resolution, illumination and cosmetics, etc.

Fig. 1. Some example results in the FDDB face benchmark [19] computed by the
proposed method. For each testing image, we show the detection results (left) and the
corresponding heat map of facial key-points with the legend shown in the right most
column. (Best viewed in color)

It has been a long history that computer vision researchers study how to learn
a better representation for unconstrained faces [40,12,34]. Recently, together with
large-scale annotated image datasets such as the ImageNet [8], deep ConvNets
[22,21] have made signiﬁcant progress in generic object detection [14,32,16], as
well as in face detection [23,30]. The success is generally considered to be due
to the region proposal methods and region-based ConvNets (R-CNN) [15]. The
two factors used to be addressed separately (e.g., the popular combination of
the Selective Search [37] and R-CNNs pretrained on the ImageNet), and now
they are integrated through introducing the region proposal networks (RPNs)
as done in the faster-RCNN [32] or are merged into a single pipeline for speeding
up the detection as done in [31,26]. In R-CNNs, one key layer is the so-called
RoI (Region-of-Interest) pooling layer [14], which divides a valid RoI (e.g., an
object bounding box proposal) evenly into a grid with a ﬁxed spatial extent
(e.g., 7 × 7) and then uses max-pooling to convert the features inside the RoI
into a small feature map. In this paper, we are interested in adapting state-of-
the-art ConvNets of generic object detection (e.g., the faster R-CNN [32]) for
face detection by overcoming the following two limitations:

i) RPNs need to predeﬁne a number of anchor boxes (with diﬀerent aspect
ratios and sizes), which requires potentially tedious parameter tuning in
training and is sensitive to the (unknown) distribution of the aspect ratios
and sizes of the object instances in a random testing image.

Face Detection with a ConvNet and a 3D Model

3

Fig. 2. Illustration of the proposed method of an end-to-end integration of a ConvNet
and a 3D model for face detection (Top), and some intermediate and the ﬁnal detection
results for an input testing image (Bottom). See the legend for the classiﬁcation score
heat map in Figure 1. The 3D mean face model is predeﬁned and ﬁxed in both training
and testing. The key idea of the proposed method is to learn a ConvNet to estimate
the 3D transformation parameters (rotation and translation) w.r.t. the 3D mean face
model to generate accurate face proposals and predict the face key points. The proposed
ConvNet is trained in a multi-task discriminative training framework consisting of the
classiﬁcation Softmax loss and the location smooth l1-losses [14] of both the facial
key-points and the face bounding boxes. It is surprisingly simple w.r.t. its competitive
state-of-the-art performance compared to the other methods in the popular FDDB
benchmark [19] and the AFW benchmark [41]. See text for details.

ii) The RoI pooling layer in R-CNNs is predeﬁned and generic to all object cat-
egories without exploiting the underlying object structural conﬁgurations,
which either are available from the annotations in the training dataset (e.g.,
the facial landmark annotations in the AFLW dataset [20]) as done in [41]
or can be pursued during learning (such as the deformable part-based mod-
els [10,30]).

To address the two above issues in learning ConvNets for face detection, we
propose to integrate a ConvNet and a 3D mean face model in an end-to-end
multi-task discriminative learning framework. Figure 1 shows some results of
the proposed method.

4

Y. Li, B. Sun, T. Wu and Y. Wang

1.2 Method Overview

Figure 2 illustrates the proposed method. We use 10 facial key-points in this pa-
per, including “LeftEyeLeftCorner”, “RightEyeRightCorner”, “LeftEar”, “Nose-
Left”, “NoseRight”, “RightEar”, “MouthLeftCorner”, “MouthRightCorner”, “Chin-
Center”, “CenterBetweenEyes” (see an example image in the left-top of Fig-
ure 2). The 3D mean face model is then represented by the corresponding ten
3D facial key-points. The architecture of our ConvNet is straight-forward when
taking into account a 3D model (see Section 3.2 for details).

The key idea is to learn a ConvNet to (i) estimate the 3D trans-
formation parameters (rotation and translation) w.r.t. the 3D mean
face model for each detected facial key-point so that we can generate
face bounding box proposals and (ii) predict facial key-points for each
face instance more accurately. Leveraging the 3D mean face model is able
to “kill two birds with one stone”: Firstly, we can eliminate the manually heuris-
tic design of anchor boxes in RPNs. Secondly, instead of using the generic RoI
pooling, we devise a “conﬁguration pooling” layer so as to respect the object
structural conﬁgurations in a meaningful and principled way. In other words,
we propose to learn to compute the proposals in a straight-forward top-down
manner, instead of to design the bottom-up heuristic and then learn related re-
gression parameters. To do so, we assume a 3D mean face model is available and
facial key-points are annotated in the training dataset. Thanks to many excellent
existing work in collecting and annotating face datasets, we can easily obtain
both for faces nowadays. In learning, we have multiple types of losses involved
in the objective loss function, including classiﬁcation Softmax loss and location
smooth l1-loss [14] of facial key-points, and location smooth l1-loss of face bound-
ing boxes respectively, so we formulate the learning of the proposed ConvNet
under the multi-task discriminative deep learning framework (see Section 3.3).
In summary, we provide a clean and straight-forward solution for end-to-end
integration of a ConvNet and a 3D model for face detection 1. In addition to the
competitive performance w.r.t the state-of-the-art face detection methods on the
FDDB and AFW benchmarks, the proposed method is surprisingly simple and
it is able to detect challenging faces (e.g., small, blurry, heavily occluded and
extreme poses).

Potentially, the proposed method can be utilized to learn to detect other
rigid or semi-rigid object categories (such as cars) if the required information
(such as the 3D model and key-point/part annotation) are provided in training.

2 Related Work

There are a tremendous amount of existing works on face detection or generic
object detection. We refer to [40] for a more thorough survey on face detection.
We discuss some of the most relevant ones in this section.
1 We use the open source deep learning package, MXNet [5], in our implementa-
tion. The full source code is released at https://github.com/tfwu/FaceDetection-
ConvNet-3D

Face Detection with a ConvNet and a 3D Model

5

In human/animal vision, how the brain distills a representation of objects
from retinal input is one of the central challenges for systems neuroscience, and
many works have been focused on the ecologically important class of objects–
faces. Studies using fMRI experiments in the macaque reveal that faces are
represented by a system of six discrete, strongly interconnected regions which
illustrates hierarchical information processing in the brain [12], as well as some
other results [34]. These ﬁndings provide some biologically-plausible evidences
for supporting the usage of deep learning based approaches in face detection and
analysis.

The seminal work of Viola and Jones [38] made face detection by a computer
vision system feasible in real world applications, which trained a cascade of Ad-
aBoost classiﬁers using Haar wavelet features. Many works followed this direction
with diﬀerent extensions proposed in four aspects: appearance features (beside
Haar) including Histogram of Oriented Gradients (HOG) [7], Aggregate Channel
Features (ACF) [9], Local Binary Pattern (LBP) features [1] and SURF [3], etc.;
detector structures (beside cascade) including the the scalar tree [11] and the
width-ﬁrst-search tree [18], etc.; strong classiﬁer learning (beside AdaBoost) in-
cluding RealBoost [33] and GentleBoost [13], ect; weak classiﬁer learning (beside
stump function) including the histogram method [25] and the joint binarizations
of Haar-like feature [28], etc..

Most of the recent face detectors are based on the deformable part-based
model (DPM) [10,41,27] with HOG features used, where a face is represented by
a collection of parts deﬁned based on either facial landmarks or heuristic pursuit
as done in the original DPM. [27] showed that a properly trained vanilla DPM
can yield signiﬁcant improvement for face detection.

More recent advances in deep learning [22,21] further boosted the face detec-
tion performance by learning more discriminative features from large-scale raw
data, going beyond those handcrafted ones. In the FDDB benchmark, most of the
face detectors with top performance are based on ConvNets [30,23], combining
with cascade [23] and more explicit structure [39].

3D information has been exploited in learning object models in diﬀerent ways.
Some works [29,35] used a mixture of 3D view based templates by dividing the
view sphere into a number of sectors. [24,17] utilized 3D models in extracting
features and inferring the object pose hypothesis based on EM or DP. [36] used
a 3D face model for aligning faces in learning ConvNets for face recognition. Our
work resembles [2] in exploiting 3D model in face detection, which obtained very
good performance in the FDDB benchmark. [2] computes meaningful 3D pose
candidates by image-based regression from detected face key-points with tradi-
tional handcrafted features, and veriﬁes the 3D pose candidates by a parameter
sensitive classiﬁer based on diﬀerence features relative to the 3D pose. Our work
integrates a ConvNet and a 3D model in an end-to-end multi-task discriminative
learning fashion, which is more straightforward and simpler compared to [2].

Our Contributions. The proposed method contributes to face detection in

three aspects.

6

Y. Li, B. Sun, T. Wu and Y. Wang

i) It presents a simple yet eﬀective method to integrate a ConvNet and a 3D
model in an end-to-end learning with multi-task loss used for face detection
in the wild.

ii) It addresses two limitations in adapting the state-of-the-art faster RCNN [32]
for face detection: eliminating the heuristic design of anchor boxes by lever-
aging a 3D model, and replacing the generic and predeﬁned RoI pooling
with a conﬁguration pooling which exploits the underlying object structural
conﬁgurations.

iii) It obtains very competitive state-of-the-art performance in the FDDB [19]

and AFW [41] benchmarks.

Paper Organization. The remainder of this paper is organized as follows. Sec-
tion 3 presents the method of face detection using a 3D model and details of our
ConvNet including its architecture and training procedure. Section 4 presents
details of experimental settings and shows the experimental results in the FDDB
and AFW benchmarks. Section 5 ﬁrst concludes this paper and then discuss some
on-going and future work to extend the proposed work.

3 The Proposed Method

In this section, we introduce the notations and present details of the proposed
method.

3.1

3D Mean Face Model and Face Representation

In this paper, a 3D mean face model is represented by a collection of n 3D
key-points in the form of (x, y, z) and then is denoted by a n × 3 matrix, F (3).
Usually, each key-point has its own semantic name. We use the 3D mean face
model in the AFLW dataset [20] which consists of 21 key-points. We select 10
key-points as stated above.

A face, denoted by f , is presented by its 3D transformation parameters, Θ,
for rotation and translation, and a collection of 2D key-points, F (2), in the form
of (x, y) (with the number being less than or equal to n). Hence, f = (Θ, F (2)).
The 3D transformation parameters Θ are deﬁned by,

Θ = (µ, s, A(3)),

where µ represents a 2D translation (dx, dy), s a scaling factor, and A(3) a 3 × 3
rotation matrix. We can compute the predicted 2D key-points by,

ˆF (2) = µ + s · π(A(3) · F (3)),

where π() projects a 3D key-point to a 2D one, that is, π : R3 → R2 and
π(x, y, z) = (x, y). Due to the projection π(), we only need 8 parameters out of
the original 12 parameters. Let A(2) denote a 2 × 3 matrix, which is composed
by the top two rows of A(3). We can re-produce the predicted 2D key-points by,

ˆF (2) = µ + A(2) · F (3)

(1)

(2)

(3)

Face Detection with a ConvNet and a 3D Model

7

which makes it easy to implement the computation of back-propagation in train-
ing our ConvNet.

Note that we use the ﬁrst sector in a 4-sector X-Y coordinate system to
deﬁne all the positions, that is, the origin point (0, 0) is deﬁned by the left-
bottom corner in an image lattice.

In face datasets, faces are usually annotated with bounding boxes. In the
FDDB benchmark [19], however, faces are annotated with ellipses and detection
performance are evaluated based on ellipses. Given a set of predicted 2D key-
points ˆF (2), we can compute proposals in both ellipse form and bounding box
form.

Computing a Face Ellipse and a Face Bounding Box based on a set of Pre-
dicted 2D Key-Points. For a given ˆF (2), we ﬁrst predict the position of the top
of head by,

(cid:0) x
y

(cid:1)

TopOfHead = 2 × (cid:0) x

y

(cid:1)

CenterBetweenEyes − (cid:0) x

y

(cid:1)

ChinCenter.

Based on the keypoints of a face proposal, we can compute its ellipse and bound-
ing box.

Face Ellipse. We ﬁrst compute the outer rectangle. We use as one axis the
line segment between the top-of-the-head key-point and the chin key-point, and
then compute the minimum rectangle, usually a rotated rectangle, which covers
all the key-points. Then, we can compute the ellipse using the two edges of the
(rotated) rectangle as the major and minor axes respectively.

Face Bounding Box. We compute a face bounding box by the minimum up-
right rectangle which covers all the key-points, which is also adopted in the
FDDB benchmark [19].

3.2 The Architecture of Our ConvNet

As illustrated in Figure 2, the architecture of our ConvNet consists of:

i) Convolution, ReLu and MaxPooling Layers. We adopt the VGG [4] design
in our experiments which has shown superior performance in a series of tasks.
There are 5 groups and each group has 3 convolution and ReLu consecutive
layers followed by a MaxPooling layer except for the 5th group. The spatial
extent of the ﬁnal feature map is of 16 times smaller than that of an input image
due to the sub-sampling.

ii) An Upsampling Layer. Since we will measure the location diﬀerence be-
tween the input facial key-points and the predicted ones, we add an upsampling
layer to compensate the sub-sampling eﬀects in previous layers. It is implemented
by deconvolution. We upsample the feature maps to 8 times bigger in size (i.e.,
the upsampled feature maps are still quarter size of an input image) consider-
ing the trade-oﬀ between key-point location accuracy, memory consumption and
computation eﬃciency.

iii) A Facial Key-point Label Prediction Layer. There are 11 labels (10 facial
key-points and 1 background class). It is used to compute the classiﬁcation
Softmax loss based on the input in training.

8

Y. Li, B. Sun, T. Wu and Y. Wang

iv) A 3D Transformation Parameter Estimation Layer. This is the key ob-
servation in this paper. Originally, there are 12 parameters in total consisting of
2D translation, scaling and 3 × 3 rotation matrix. Since we focus on the 2D pro-
jected key-points, we only need to account for 8 parameters (see the derivation
above).

v) A Face Proposal Layer. At each position, based on the 3D mean face
model and the estimated 3D transformation parameters, we can compute a face
proposal consisting of 10 predicted facial key-points and the corresponding face
bounding box. The score of a face proposal is the sum of log probabilities of
the 10 predicted facial key-points. The predicated key-points will be used to
compute the smooth l1 loss [14] w.r.t. the ground-truth key-points. We apply
the non-maximum suppression (NMS) to the face proposals in which the overlap
between two bounding boxes a and b is computed by |a|∩|b|
(where | · | represents
the area of a bounding box), instead of the traditional intersection-over-union,
accounting for the fact that it is rarely observed that one face is inside another
one.

|b|

vi) A Conﬁguration Pooling Layer. After NMS, for each face proposal, we
pool the features based on the predicted 10 facial key-points. Here, for simplicity,
we use all the 10 key-points without considering the invisibilities of certain key-
points in diﬀerent face examples.

vii) A Face Bounding Box Regression Layer. It is used to further reﬁne face
bounding boxes in the spirit similar to the method [14]. Based on the conﬁgura-
tion pooling, we add two fully-connected layers to implement the regression. It
is used to compute the smooth l1 loss of face bounding boxes.

Denote by ω all the parameters in our ConvNet, which will be estimated

through multi-task discriminative end-to-end learning.

3.3 The End-to-End Training

Input Data. Denote by C = {0, 1, · · · , 10} as the key-point labels where (cid:96) = 0
represents the background class. We use the image-centric sampling trick as done
in [14,32]. Without loss of generality, considering a training image with only one
face appeared, we have its bounding box, B = (x, y, w, h) and m 2D key-points
(m ≤ 10), {(xi, (cid:96)i)m
i=1} where xi = (xi, yi) is the 2D position of the ith key-point
and (cid:96)i ≥ 1 ∈ C. We randomly sample m locations outside the face bounding box
B as the background class, {(xi, (cid:96)i)2m
i=m+1} (where (cid:96)i = 0, ∀i > m). Note that
in our ConvNet, we use the coordinate of the upsampled feature map which is
half size along both axes of the original input. All the key-points and bounding
boxes are deﬁned accordingly based on ground-truth annotation.

The Classiﬁcation Softmax Loss of Key-point Labels. At each position

xi, our ConvNet outputs a discrete probability distribution, pxi = (pxi
over the 11 classes, which is computed by the Softmax over the 11 scores as
usual [21]. Then, we have the loss,

0 , pxi

1 , · · · , pxi

10),

Lcls(ω) = −

1
2m

2m
(cid:88)

i=1

log(pxi
(cid:96)i

)

(4)

Face Detection with a ConvNet and a 3D Model

9

The Smooth l1 Loss of Key-point Locations. At each key-point location
xi ((cid:96)i ≥ 1), we compute a face proposal based on the estimated 3D parameters
and the 3D mean face, denoted by {(ˆx(i)
j=1} the predicted 10 keypoints.
So, for each key-point location xi, we will have m predicted locations, denoted
by ˆxi,j (j = 1, · · · , m). We follow the deﬁnition in [14] to compute the smooth
l1 loss for each axis individually.
m
(cid:88)

j , ˆ(cid:96)(i)

j )10

m
(cid:88)

(cid:88)

Smoothl1(ti − ˆti,j)

Lpt

loc(ω) =

1
m2

i=1

j=1

t∈{x,y}

where the smooth term is deﬁned by,
(cid:40)

Smoothl1 (a) =

0.5a2
|a| − 0.5

if |a| < 1
otherwise.

Faceness Score. The faceness score of a face proposal in our ConvNet is

computed by the sum of log probabilities of the predicted key-points,

Score(ˆxi, ˆ(cid:96)i) =

10
(cid:88)

i=1

log(pˆxi
ˆ(cid:96)i

)

where for simplicity we do not account for the invisibilities of certain key-points.
So the current faceness score has the issue of potential double-counting, espe-
cially for low-resolution faces. We observed that it hurts the quantitative perfor-
mance in our experiments. We will address this issue in future work. See some
heat maps of key-points in Figure 1.

The Smooth l1 Loss of Bounding Boxes. For each face bounding box
proposal ˆB (after NMS), our ConvNet computes its bounding box regression
oﬀsets, t = (tx, ty, tw, th), where t speciﬁes a scale-invariant translation and
log-space height/width shift relative to a proposal, as done in [14,32]. For the
ground-truth bounding box B, we do the same parameterization and have v =
(vx, vy, vw, vh). Assuming that there are K bounding box proposals, we have,

Lbox

loc (ω) =

1
K

K
(cid:88)

(cid:88)

k=1

i∈{x,y,w,h}

Smoothl1(ti − vi)

So, the overall loss function is deﬁned by,

L(ω) = Lcls(ω) + Lpt

loc(ω) + Lbox

loc (ω),

where the third term depends on the output of the ﬁrst two terms, which makes
the loss minimization more challenging. We adopt a method to implement the
diﬀerentiable bounding box warping layer, similar to [6].

4 Experiments

In this section, we present the training procedure and implementation details
and then show evaluation results on the FDDB [19] and AFW [41] benchmarks.

(5)

(6)

(7)

(8)

(9)

10

Y. Li, B. Sun, T. Wu and Y. Wang

Category

Category
Background

Accuracy
97.94% LeftEyeLeftCorner

Accuracy
99.12%
95.50%
LeftEar
RightEyeRightCorner 94.57%
97.78%
NoseRight
98.48%
97.97%
MouthLeftCorner
91.44%
98.64%
98.65%
ChinCenter
96.04% AverageDetectionRate 97.50%

NoseLeft
RightEar
MouthRightCorner
CenterBetweenEyes

Table 1. Classiﬁcation accuracy of the key-points in the AFLW validation set at the
end training.

4.1 Experimental Settings

The Training Dataset. The only dataset we used for training our model is the
AFLW dataset [20], which contains 25, 993 annotated faces in real-world images.
The facial key-points are annotated upon visibility w.r.t. a 3D mean face model
with 21 landmarks. Of the images 70% are used for training while the remaining
is reserved as a validation set.

Training process. For convenience, the short edge of every image is resized
to 600 pixels while preserving the aspect ratio (as done in the faster RCNN [32]),
thus our model learns how to handle faces under various scale. To handle faces
of diﬀerent resolution, we randomly blur images using Gaussian ﬁlters in pre-
processing. Apart from the rescaling and blurring, no other preprocessing mech-
anisms (e.g., random crop or left-right ﬂipping) are used.

We adopt the method of image-centric sampling [14,32] which uses one im-
age at a time in training. Under the consideration that grids around the labeled
position share almost the same context information, thus the 3 × 3 grids around
every labeled key-point’s position are also regarded as the same positive exam-
ples, and we randomly choose the same amount of background examples outside
the bounding boxes. The convolution ﬁlters are initialized by the VGG-16 [4]
pretrained on the ImageNet [8]. We train the network for 13 epoch, and during
the process, the learning rate is modiﬁed from 0.01 to 0.0001.

4.2 Evaluation of the intermediate results

Key-points classiﬁcation in the validation dataset. As are shown by the
heat maps in Figure 1, our model is capable of detecting facial key-points with
rough face conﬁgurations preserved, which shows the eﬀectiveness of exploiting
the 3D mean face model. Table 1 shows the key-point classiﬁcation accuracy on
the validation set in the last epoch in training.

Face proposals. To evaluate the quality of our face proposals, we ﬁrst show
some qualitative results on the FDDB dataset in Fig. 3. These ellipses are directly
calculated from the predicted 3D transformation parameters, forming several
clusters around face instances. We also evaluate the quantitative results of face
proposals. After a non-maximum suppression of IoU 0.7, the recall rate of 93.67%
is obtained with average 34.4 proposals per image.

Face Detection with a ConvNet and a 3D Model

11

Fig. 3. Examples of face proposals computed using predicted 3D transformation pa-
rameters without non-maximum suppression. For clarity, we randomly sample 1/30 of
the original number of proposals.

Fig. 4. FDDB results based on discrete scores using face bounding boxes in evaluation.
The recall rates are computed against 2000 false positives.

4.3 Face Detection Results

To show the eﬀectiveness of our method, we test our model on two popular face
detection benchmarks: FDDB [19] and AFW [41].

Results on FDDB. FDDB is a challenge benchmark for face detection in
unconstrained environment, which contains the annotations for 5171 faces in a
set of 2845 images. We evaluate our results by using the evaluation code provided
by the FDDB authors. The results on the FDDB dataset are shown in Figure 4.
Our result is represented by ”Ours-Conv3D”, which surpasses the recall rate of
90% when encountering 2000 false positives and is competitive to the state-of-
the-art methods. We compare with published methods only. Only DP2MFD [30]
is slightly better than our model on discrete scores. It’s worth noting that we beat
all other methods on continuous scores. This is partly caused by the predeﬁned

12

Y. Li, B. Sun, T. Wu and Y. Wang

Fig. 5. FDDB results based on continuous scores using face ellipses in evaluation. The
recall rates are computed against 2000 false positives.

3D face model helps us better describe the pose and part locations of faces.
We refer to the FDDB result webpage2 for details of the published methods
evaluated on it (Fig. 4 and Fig. 5).

When comparing with recent work Faceness [39], we both recognize that one
of the central issues to alleviate the problems of the occlusion and pose variation
is to introduce facial part detector. However, our mechanism of computing face
bounding box candidates is more straight forward since we explicitly integrate
the structural information of a 3D mean face model instead of using a heuristic
way of assuming the facial part distribution over a bounding box.

Results on AFW. AFW dataset contains 205 images with faces in various
poses and view points. We use the evaluation toolbox provided by [27], which
contains updated annotations for the AFW dataset where the original annota-
tions are not comprehensive enough. Since the method of labeling face bounding
boxes in AFW is diﬀerent from that of in FDDB, we only use face proposals with-
out conﬁguration pooling and bounding box regression. The results on AFW are
shown in Figure 6.

In our current implementation, there is one major limitation that prevents us
from achieving better results. We do not explicitly handle invisible facial parts,
which would be harmful when calculating the faceness score according to Eqn. 7,
we will reﬁne the method and introduce mechanisms of handling the invisible
problem in future work. More detection results on both datasets are shown in
Figure 7 and Figure 8.

2 http://vis-www.cs.umass.edu/fddb/results.html

Face Detection with a ConvNet and a 3D Model

13

Fig. 6. Precision-recall curves on the AFW dataset (AP = average precision) without
conﬁguration pool and face bounding box regression used.

5 Conclusion and Discussion

We have presented a method of end-to-end integration of a ConvNet and a 3D
model for face detection in the wild. Our method is a clean and straightfor-
ward solution when taking into account a 3D model in face detection. It also
addresses two issues in state-of-the-art generic object detection ConvNets: elim-
inating heuristic design of anchor boxes by leveraging a 3D model, and overcom-
ing generic and predeﬁned RoI pooling by conﬁguration pooling which exploits
underlying object conﬁgurations. In experiments, we tested our method on two
benchmarks, the FDDB dataset and the AFW dataset, with very compatible
state-of-the-art performance obtained. We analyzed the experimental results and
pointed out some current limitations.

In our on-going work, we are working on addressing the doubling-counting
issue of the faceness score in the current implementation. We are also working on
extending the proposed method for other types of rigid/semi-rigid object classes
(e.g., cars). We expect that we will have a uniﬁed model for cars and faces which
can achieve state-of-the-art performance, which will be very useful in a lot of
practical applications such as surveillance and driveless cars.

Acknowledgement. Y. Li, B. Sun and Y. Wang were supported in part
by China 973 Program under Grant no. 2015CB351800, and NSFC-61231010,
61527804, 61421062, 61210005. T. Wu was supported by the ECE startup fund
201473-02119 at NCSU. T. Wu also gratefully acknowledge the support of NVIDIA
Corporation with the donation of one GPU.

14

Y. Li, B. Sun, T. Wu and Y. Wang

Fig. 7. Some qualitative results on the FDDB dataset

Fig. 8. Some qualitative results on the AFW dataset

Face Detection with a ConvNet and a 3D Model

15

References

1. Ahonen, T., Hadid, A., Pietik¨ainen, M.: Face description with local binary patterns:
Application to face recognition. IEEE Trans. Pattern Anal. Mach. Intell. 28(12),
2037–2041 (2006)

2. Barbu, A., Gramajo, G.: Face detection using a 3d model on face keypoints. CoRR

abs/1404.3596 (2014)

3. Bay, H., Ess, A., Tuytelaars, T., Gool, L.J.V.: Speeded-up robust features (SURF).

Computer Vision and Image Understanding 110(3), 346–359 (2008)

4. Chatﬁeld, K., Simonyan, K., Vedaldi, A., Zisserman, A.: Return of the devil in the
details: Delving deep into convolutional nets. In: British Machine Vision Conference
(2014)

5. Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao, T., Xu, B., Zhang,
C., Zhang, Z.: Mxnet: A ﬂexible and eﬃcient machine learning library for hetero-
geneous distributed systems. CoRR abs/1512.01274 (2015)

6. Dai, J., He, K., Sun, J.: Instance-aware semantic segmentation via multi-task net-

7. Dalal, N., Triggs, B.: Histograms of oriented gradients for human detection. In:

work cascades. In: CVPR (2016)

CVPR (2005)

8. Deng, J., Dong, W., Socher, R., Li, L., Li, K., Li, F.: Imagenet: A large-scale

hierarchical image database. In: CVPR. pp. 248–255 (2009)

9. Doll´ar, P., Appel, R., Belongie, S.J., Perona, P.: Fast feature pyramids for object

detection. IEEE Trans. Pattern Anal. Mach. Intell. 36(8), 1532–1545 (2014)
10. Felzenszwalb, P.F., Girshick, R.B., McAllester, D.A., Ramanan, D.: Object detec-
tion with discriminatively trained part-based models. IEEE Trans. Pattern Anal.
Mach. Intell. 32(9), 1627–1645 (2010)

11. Fleuret, F., Geman, D.: Coarse-to-ﬁne face detection. International Journal of

Computer Vision 41(1/2), 85–107 (2001)

12. Freiwald, W.A., Tsao, D.Y.: Functional compartmentalization and viewpoint gen-
eralization within the macaque face-processing system. Science 330(6005), 845–851
(2010)

13. Friedman, J., Hastie, T., Tibshirani, R.: Additive logistic regression: a statistical

view of boosting. Annals of Statistics 28, 2000 (1998)

14. Girshick, R.: Fast R-CNN. In: ICCV (2015)
15. Girshick, R.B., Donahue, J., Darrell, T., Malik, J.: Region-based convolutional
networks for accurate object detection and segmentation. IEEE Trans. Pattern
Anal. Mach. Intell. 38(1), 142–158 (2016)

16. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

In: CVPR (2016)

17. Hu, W., Zhu, S.: Learning 3d object templates by quantizing geometry and ap-
pearance spaces. IEEE Trans. Pattern Anal. Mach. Intell. 37(6), 1190–1205 (2015)
18. Huang, C., Ai, H., Li, Y., Lao, S.: High-performance rotation invariant multiview
face detection. IEEE Trans. Pattern Anal. Mach. Intell. 29(4), 671–686 (2007)
19. Jain, V., Learned-Miller, E.: Fddb: A benchmark for face detection in un-
constrained settings. Tech. Rep. UM-CS-2010-009, University of Massachusetts,
Amherst (2010)

20. Koestinger, M., Wohlhart, P., Roth, P.M., Bischof, H.: Annotated facial landmarks
in the wild: A large-scale, real-world database for facial landmark localization.
In: First IEEE International Workshop on Benchmarking Facial Image Analysis
Technologies (2011)

16

Y. Li, B. Sun, T. Wu and Y. Wang

21. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-

volutional neural networks. In: NIPS (2012)

22. LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based learning applied to

document recognition. Proceedings of the IEEE 86(11), 2278–2324 (1998)

23. Li, H., Lin, Z., Shen, X., Brandt, J., Hua, G.: A convolutional neural network

cascade for face detection. In: CVPR (2015)

24. Liebelt, J., Schmid, C.: Multi-view object class detection with a 3d geometric

model. In: CVPR (2010)

25. Liu, C., Shum, H.: Kullback-leibler boosting. In: CVPR (2003)
26. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.:
SSD: Single shot multibox detector. arXiv preprint arXiv:1512.02325 (2015)
27. Mathias, M., Benenson, R., Pedersoli, M., Gool, L.V.: Face detection without bells

and whistles. In: ECCV (2014)

28. Mita, T., Kaneko, T., Hori, O.: Joint haar-like features for face detection. In: ICCV

29. Payet, N., Todorovic, S.: From contours to 3d object detection and pose estimation.

(2005)

In: ICCV (2011)

30. Ranjan, R., Patel, V.M., Chellappa, R.: A deep pyramid deformable part model
for face detection. In: IEEE 7th International Conference on Biometrics Theory,
Applications and Systems (2015)

31. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Uniﬁed,

real-time object detection. In: CVPR (2016)

32. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object

detection with region proposal networks. In: NIPS (2015)

33. Schapire, R.E., Singer, Y.: Improved boosting algorithms using conﬁdence-rated

predictions. Machine Learning 37(3), 297–336 (1999)

34. Sinha, P., Balas, B., Ostrovsky, Y., Russell, R.: Face recognition by humans: 19
results all computer vision researchers should know about. Proceedings of the IEEE
94(11), 1948–1962 (2006)

35. Su, H., Sun, M., Li, F., Savarese, S.: Learning a dense multi-view representation
for detection, viewpoint classiﬁcation and synthesis of object categories. In: ICCV
(2009)

36. Taigman, Y., Yang, M., Ranzato, M., Wolf, L.: Deepface: Closing the gap to human-

level performance in face veriﬁcation. In: CVPR (2014)

37. Uijlings, J.R.R., van de Sande, K.E.A., Gevers, T., Smeulders, A.W.M.: Selective
search for object recognition. International Journal of Computer Vision 104(2),
154–171 (2013)

38. Viola, P.A., Jones, M.J.: Robust real-time face detection. International Journal of

Computer Vision 57(2), 137–154 (2004)

39. Yang, S., Luo, P., Loy, C.C., Tang, X.: From facial parts responses to face detection:

A deep learning approach. In: ICCV (2015)

40. Zafeiriou, S., Zhang, C., Zhang, Z.: A survey on face detection in the wild. Comput.

Vis. Image Underst. 138(C), 1–24 (Sep 2015)

41. Zhu, X., Ramanan, D.: Face detection, pose estimation, and landmark localization

in the wild. In: CVPR (2012)

6
1
0
2
 
g
u
A
 
9
2
 
 
]

V
C
.
s
c
[
 
 
3
v
0
5
8
0
0
.
6
0
6
1
:
v
i
X
r
a

Face Detection with End-to-End Integration of a
ConvNet and a 3D Model

Yunzhu Li1,∗, Benyuan Sun1,(cid:63), Tianfu Wu2 and Yizhou Wang1

1Nat’l Engineering Laboratory for Video Technology,
Key Laboratory of Machine Perception (MoE),
Cooperative Medianet Innovation Center, Shanghai
Sch’l of EECS, Peking University, Beijing, 100871, China
2Department of ECE and the Visual Narrative Cluster,
North Carolina State University, Raleigh, USA
{leo.liyunzhu, sunbenyuan, Yizhou.Wang}@pku.edu.cn, tianfu wu@ncsu.edu

Abstract. This paper presents a method for face detection in the wild,
which integrates a ConvNet and a 3D mean face model in an end-to-end
multi-task discriminative learning framework. The 3D mean face model
is predeﬁned and ﬁxed (e.g., we used the one provided in the AFLW
dataset [20]). The ConvNet consists of two components: (i) The face pro-
posal component computes face bounding box proposals via estimating
facial key-points and the 3D transformation (rotation and translation)
parameters for each predicted key-point w.r.t. the 3D mean face model.
(ii) The face veriﬁcation component computes detection results by prun-
ing and reﬁning proposals based on facial key-points based conﬁguration
pooling. The proposed method addresses two issues in adapting state-
of-the-art generic object detection ConvNets (e.g., faster R-CNN [32])
for face detection: (i) One is to eliminate the heuristic design of prede-
ﬁned anchor boxes in the region proposals network (RPN) by exploit-
ing a 3D mean face model. (ii) The other is to replace the generic RoI
(Region-of-Interest) pooling layer with a conﬁguration pooling layer to
respect underlying object structures. The multi-task loss consists of three
terms: the classiﬁcation Softmax loss and the location smooth l1-losses
[14] of both the facial key-points and the face bounding boxes. In ex-
periments, our ConvNet is trained on the AFLW dataset [20] only and
tested on the FDDB benchmark [19] with ﬁne-tuning and on the AFW
benchmark [41] without ﬁne-tuning. The proposed method obtains very
competitive state-of-the-art performance in the two benchmarks.

Keywords: Face Detection, Face 3D Model, ConvNet, Deep Learning,
Multi-task Learning

1 Introduction

1.1 Motivation and Objective

Face detection has been used as a core module in a wide spectrum of applications
such as surveillance, mobile communication and human-computer interaction. It

(cid:63) Y. Li and B. Sun contributed equally to this work and are joint ﬁrst authors.

2

Y. Li, B. Sun, T. Wu and Y. Wang

is arguably one of the most successful applications of computer vision. Face
detection in the wild continues to play an important role in the era of visual
big data (e.g., images and videos on the web and in social media). However, it
remains a challenging problem in computer vision due to the large appearance
variations caused by nuisance variabilities including viewpoints, occlusion, facial
expression, resolution, illumination and cosmetics, etc.

Fig. 1. Some example results in the FDDB face benchmark [19] computed by the
proposed method. For each testing image, we show the detection results (left) and the
corresponding heat map of facial key-points with the legend shown in the right most
column. (Best viewed in color)

It has been a long history that computer vision researchers study how to learn
a better representation for unconstrained faces [40,12,34]. Recently, together with
large-scale annotated image datasets such as the ImageNet [8], deep ConvNets
[22,21] have made signiﬁcant progress in generic object detection [14,32,16], as
well as in face detection [23,30]. The success is generally considered to be due
to the region proposal methods and region-based ConvNets (R-CNN) [15]. The
two factors used to be addressed separately (e.g., the popular combination of
the Selective Search [37] and R-CNNs pretrained on the ImageNet), and now
they are integrated through introducing the region proposal networks (RPNs)
as done in the faster-RCNN [32] or are merged into a single pipeline for speeding
up the detection as done in [31,26]. In R-CNNs, one key layer is the so-called
RoI (Region-of-Interest) pooling layer [14], which divides a valid RoI (e.g., an
object bounding box proposal) evenly into a grid with a ﬁxed spatial extent
(e.g., 7 × 7) and then uses max-pooling to convert the features inside the RoI
into a small feature map. In this paper, we are interested in adapting state-of-
the-art ConvNets of generic object detection (e.g., the faster R-CNN [32]) for
face detection by overcoming the following two limitations:

i) RPNs need to predeﬁne a number of anchor boxes (with diﬀerent aspect
ratios and sizes), which requires potentially tedious parameter tuning in
training and is sensitive to the (unknown) distribution of the aspect ratios
and sizes of the object instances in a random testing image.

Face Detection with a ConvNet and a 3D Model

3

Fig. 2. Illustration of the proposed method of an end-to-end integration of a ConvNet
and a 3D model for face detection (Top), and some intermediate and the ﬁnal detection
results for an input testing image (Bottom). See the legend for the classiﬁcation score
heat map in Figure 1. The 3D mean face model is predeﬁned and ﬁxed in both training
and testing. The key idea of the proposed method is to learn a ConvNet to estimate
the 3D transformation parameters (rotation and translation) w.r.t. the 3D mean face
model to generate accurate face proposals and predict the face key points. The proposed
ConvNet is trained in a multi-task discriminative training framework consisting of the
classiﬁcation Softmax loss and the location smooth l1-losses [14] of both the facial
key-points and the face bounding boxes. It is surprisingly simple w.r.t. its competitive
state-of-the-art performance compared to the other methods in the popular FDDB
benchmark [19] and the AFW benchmark [41]. See text for details.

ii) The RoI pooling layer in R-CNNs is predeﬁned and generic to all object cat-
egories without exploiting the underlying object structural conﬁgurations,
which either are available from the annotations in the training dataset (e.g.,
the facial landmark annotations in the AFLW dataset [20]) as done in [41]
or can be pursued during learning (such as the deformable part-based mod-
els [10,30]).

To address the two above issues in learning ConvNets for face detection, we
propose to integrate a ConvNet and a 3D mean face model in an end-to-end
multi-task discriminative learning framework. Figure 1 shows some results of
the proposed method.

4

Y. Li, B. Sun, T. Wu and Y. Wang

1.2 Method Overview

Figure 2 illustrates the proposed method. We use 10 facial key-points in this pa-
per, including “LeftEyeLeftCorner”, “RightEyeRightCorner”, “LeftEar”, “Nose-
Left”, “NoseRight”, “RightEar”, “MouthLeftCorner”, “MouthRightCorner”, “Chin-
Center”, “CenterBetweenEyes” (see an example image in the left-top of Fig-
ure 2). The 3D mean face model is then represented by the corresponding ten
3D facial key-points. The architecture of our ConvNet is straight-forward when
taking into account a 3D model (see Section 3.2 for details).

The key idea is to learn a ConvNet to (i) estimate the 3D trans-
formation parameters (rotation and translation) w.r.t. the 3D mean
face model for each detected facial key-point so that we can generate
face bounding box proposals and (ii) predict facial key-points for each
face instance more accurately. Leveraging the 3D mean face model is able
to “kill two birds with one stone”: Firstly, we can eliminate the manually heuris-
tic design of anchor boxes in RPNs. Secondly, instead of using the generic RoI
pooling, we devise a “conﬁguration pooling” layer so as to respect the object
structural conﬁgurations in a meaningful and principled way. In other words,
we propose to learn to compute the proposals in a straight-forward top-down
manner, instead of to design the bottom-up heuristic and then learn related re-
gression parameters. To do so, we assume a 3D mean face model is available and
facial key-points are annotated in the training dataset. Thanks to many excellent
existing work in collecting and annotating face datasets, we can easily obtain
both for faces nowadays. In learning, we have multiple types of losses involved
in the objective loss function, including classiﬁcation Softmax loss and location
smooth l1-loss [14] of facial key-points, and location smooth l1-loss of face bound-
ing boxes respectively, so we formulate the learning of the proposed ConvNet
under the multi-task discriminative deep learning framework (see Section 3.3).
In summary, we provide a clean and straight-forward solution for end-to-end
integration of a ConvNet and a 3D model for face detection 1. In addition to the
competitive performance w.r.t the state-of-the-art face detection methods on the
FDDB and AFW benchmarks, the proposed method is surprisingly simple and
it is able to detect challenging faces (e.g., small, blurry, heavily occluded and
extreme poses).

Potentially, the proposed method can be utilized to learn to detect other
rigid or semi-rigid object categories (such as cars) if the required information
(such as the 3D model and key-point/part annotation) are provided in training.

2 Related Work

There are a tremendous amount of existing works on face detection or generic
object detection. We refer to [40] for a more thorough survey on face detection.
We discuss some of the most relevant ones in this section.
1 We use the open source deep learning package, MXNet [5], in our implementa-
tion. The full source code is released at https://github.com/tfwu/FaceDetection-
ConvNet-3D

Face Detection with a ConvNet and a 3D Model

5

In human/animal vision, how the brain distills a representation of objects
from retinal input is one of the central challenges for systems neuroscience, and
many works have been focused on the ecologically important class of objects–
faces. Studies using fMRI experiments in the macaque reveal that faces are
represented by a system of six discrete, strongly interconnected regions which
illustrates hierarchical information processing in the brain [12], as well as some
other results [34]. These ﬁndings provide some biologically-plausible evidences
for supporting the usage of deep learning based approaches in face detection and
analysis.

The seminal work of Viola and Jones [38] made face detection by a computer
vision system feasible in real world applications, which trained a cascade of Ad-
aBoost classiﬁers using Haar wavelet features. Many works followed this direction
with diﬀerent extensions proposed in four aspects: appearance features (beside
Haar) including Histogram of Oriented Gradients (HOG) [7], Aggregate Channel
Features (ACF) [9], Local Binary Pattern (LBP) features [1] and SURF [3], etc.;
detector structures (beside cascade) including the the scalar tree [11] and the
width-ﬁrst-search tree [18], etc.; strong classiﬁer learning (beside AdaBoost) in-
cluding RealBoost [33] and GentleBoost [13], ect; weak classiﬁer learning (beside
stump function) including the histogram method [25] and the joint binarizations
of Haar-like feature [28], etc..

Most of the recent face detectors are based on the deformable part-based
model (DPM) [10,41,27] with HOG features used, where a face is represented by
a collection of parts deﬁned based on either facial landmarks or heuristic pursuit
as done in the original DPM. [27] showed that a properly trained vanilla DPM
can yield signiﬁcant improvement for face detection.

More recent advances in deep learning [22,21] further boosted the face detec-
tion performance by learning more discriminative features from large-scale raw
data, going beyond those handcrafted ones. In the FDDB benchmark, most of the
face detectors with top performance are based on ConvNets [30,23], combining
with cascade [23] and more explicit structure [39].

3D information has been exploited in learning object models in diﬀerent ways.
Some works [29,35] used a mixture of 3D view based templates by dividing the
view sphere into a number of sectors. [24,17] utilized 3D models in extracting
features and inferring the object pose hypothesis based on EM or DP. [36] used
a 3D face model for aligning faces in learning ConvNets for face recognition. Our
work resembles [2] in exploiting 3D model in face detection, which obtained very
good performance in the FDDB benchmark. [2] computes meaningful 3D pose
candidates by image-based regression from detected face key-points with tradi-
tional handcrafted features, and veriﬁes the 3D pose candidates by a parameter
sensitive classiﬁer based on diﬀerence features relative to the 3D pose. Our work
integrates a ConvNet and a 3D model in an end-to-end multi-task discriminative
learning fashion, which is more straightforward and simpler compared to [2].

Our Contributions. The proposed method contributes to face detection in

three aspects.

6

Y. Li, B. Sun, T. Wu and Y. Wang

i) It presents a simple yet eﬀective method to integrate a ConvNet and a 3D
model in an end-to-end learning with multi-task loss used for face detection
in the wild.

ii) It addresses two limitations in adapting the state-of-the-art faster RCNN [32]
for face detection: eliminating the heuristic design of anchor boxes by lever-
aging a 3D model, and replacing the generic and predeﬁned RoI pooling
with a conﬁguration pooling which exploits the underlying object structural
conﬁgurations.

iii) It obtains very competitive state-of-the-art performance in the FDDB [19]

and AFW [41] benchmarks.

Paper Organization. The remainder of this paper is organized as follows. Sec-
tion 3 presents the method of face detection using a 3D model and details of our
ConvNet including its architecture and training procedure. Section 4 presents
details of experimental settings and shows the experimental results in the FDDB
and AFW benchmarks. Section 5 ﬁrst concludes this paper and then discuss some
on-going and future work to extend the proposed work.

3 The Proposed Method

In this section, we introduce the notations and present details of the proposed
method.

3.1

3D Mean Face Model and Face Representation

In this paper, a 3D mean face model is represented by a collection of n 3D
key-points in the form of (x, y, z) and then is denoted by a n × 3 matrix, F (3).
Usually, each key-point has its own semantic name. We use the 3D mean face
model in the AFLW dataset [20] which consists of 21 key-points. We select 10
key-points as stated above.

A face, denoted by f , is presented by its 3D transformation parameters, Θ,
for rotation and translation, and a collection of 2D key-points, F (2), in the form
of (x, y) (with the number being less than or equal to n). Hence, f = (Θ, F (2)).
The 3D transformation parameters Θ are deﬁned by,

Θ = (µ, s, A(3)),

where µ represents a 2D translation (dx, dy), s a scaling factor, and A(3) a 3 × 3
rotation matrix. We can compute the predicted 2D key-points by,

ˆF (2) = µ + s · π(A(3) · F (3)),

where π() projects a 3D key-point to a 2D one, that is, π : R3 → R2 and
π(x, y, z) = (x, y). Due to the projection π(), we only need 8 parameters out of
the original 12 parameters. Let A(2) denote a 2 × 3 matrix, which is composed
by the top two rows of A(3). We can re-produce the predicted 2D key-points by,

ˆF (2) = µ + A(2) · F (3)

(1)

(2)

(3)

Face Detection with a ConvNet and a 3D Model

7

which makes it easy to implement the computation of back-propagation in train-
ing our ConvNet.

Note that we use the ﬁrst sector in a 4-sector X-Y coordinate system to
deﬁne all the positions, that is, the origin point (0, 0) is deﬁned by the left-
bottom corner in an image lattice.

In face datasets, faces are usually annotated with bounding boxes. In the
FDDB benchmark [19], however, faces are annotated with ellipses and detection
performance are evaluated based on ellipses. Given a set of predicted 2D key-
points ˆF (2), we can compute proposals in both ellipse form and bounding box
form.

Computing a Face Ellipse and a Face Bounding Box based on a set of Pre-
dicted 2D Key-Points. For a given ˆF (2), we ﬁrst predict the position of the top
of head by,

(cid:0) x
y

(cid:1)

TopOfHead = 2 × (cid:0) x

y

(cid:1)

CenterBetweenEyes − (cid:0) x

y

(cid:1)

ChinCenter.

Based on the keypoints of a face proposal, we can compute its ellipse and bound-
ing box.

Face Ellipse. We ﬁrst compute the outer rectangle. We use as one axis the
line segment between the top-of-the-head key-point and the chin key-point, and
then compute the minimum rectangle, usually a rotated rectangle, which covers
all the key-points. Then, we can compute the ellipse using the two edges of the
(rotated) rectangle as the major and minor axes respectively.

Face Bounding Box. We compute a face bounding box by the minimum up-
right rectangle which covers all the key-points, which is also adopted in the
FDDB benchmark [19].

3.2 The Architecture of Our ConvNet

As illustrated in Figure 2, the architecture of our ConvNet consists of:

i) Convolution, ReLu and MaxPooling Layers. We adopt the VGG [4] design
in our experiments which has shown superior performance in a series of tasks.
There are 5 groups and each group has 3 convolution and ReLu consecutive
layers followed by a MaxPooling layer except for the 5th group. The spatial
extent of the ﬁnal feature map is of 16 times smaller than that of an input image
due to the sub-sampling.

ii) An Upsampling Layer. Since we will measure the location diﬀerence be-
tween the input facial key-points and the predicted ones, we add an upsampling
layer to compensate the sub-sampling eﬀects in previous layers. It is implemented
by deconvolution. We upsample the feature maps to 8 times bigger in size (i.e.,
the upsampled feature maps are still quarter size of an input image) consider-
ing the trade-oﬀ between key-point location accuracy, memory consumption and
computation eﬃciency.

iii) A Facial Key-point Label Prediction Layer. There are 11 labels (10 facial
key-points and 1 background class). It is used to compute the classiﬁcation
Softmax loss based on the input in training.

8

Y. Li, B. Sun, T. Wu and Y. Wang

iv) A 3D Transformation Parameter Estimation Layer. This is the key ob-
servation in this paper. Originally, there are 12 parameters in total consisting of
2D translation, scaling and 3 × 3 rotation matrix. Since we focus on the 2D pro-
jected key-points, we only need to account for 8 parameters (see the derivation
above).

v) A Face Proposal Layer. At each position, based on the 3D mean face
model and the estimated 3D transformation parameters, we can compute a face
proposal consisting of 10 predicted facial key-points and the corresponding face
bounding box. The score of a face proposal is the sum of log probabilities of
the 10 predicted facial key-points. The predicated key-points will be used to
compute the smooth l1 loss [14] w.r.t. the ground-truth key-points. We apply
the non-maximum suppression (NMS) to the face proposals in which the overlap
between two bounding boxes a and b is computed by |a|∩|b|
(where | · | represents
the area of a bounding box), instead of the traditional intersection-over-union,
accounting for the fact that it is rarely observed that one face is inside another
one.

|b|

vi) A Conﬁguration Pooling Layer. After NMS, for each face proposal, we
pool the features based on the predicted 10 facial key-points. Here, for simplicity,
we use all the 10 key-points without considering the invisibilities of certain key-
points in diﬀerent face examples.

vii) A Face Bounding Box Regression Layer. It is used to further reﬁne face
bounding boxes in the spirit similar to the method [14]. Based on the conﬁgura-
tion pooling, we add two fully-connected layers to implement the regression. It
is used to compute the smooth l1 loss of face bounding boxes.

Denote by ω all the parameters in our ConvNet, which will be estimated

through multi-task discriminative end-to-end learning.

3.3 The End-to-End Training

Input Data. Denote by C = {0, 1, · · · , 10} as the key-point labels where (cid:96) = 0
represents the background class. We use the image-centric sampling trick as done
in [14,32]. Without loss of generality, considering a training image with only one
face appeared, we have its bounding box, B = (x, y, w, h) and m 2D key-points
(m ≤ 10), {(xi, (cid:96)i)m
i=1} where xi = (xi, yi) is the 2D position of the ith key-point
and (cid:96)i ≥ 1 ∈ C. We randomly sample m locations outside the face bounding box
B as the background class, {(xi, (cid:96)i)2m
i=m+1} (where (cid:96)i = 0, ∀i > m). Note that
in our ConvNet, we use the coordinate of the upsampled feature map which is
half size along both axes of the original input. All the key-points and bounding
boxes are deﬁned accordingly based on ground-truth annotation.

The Classiﬁcation Softmax Loss of Key-point Labels. At each position

xi, our ConvNet outputs a discrete probability distribution, pxi = (pxi
over the 11 classes, which is computed by the Softmax over the 11 scores as
usual [21]. Then, we have the loss,

0 , pxi

1 , · · · , pxi

10),

Lcls(ω) = −

1
2m

2m
(cid:88)

i=1

log(pxi
(cid:96)i

)

(4)

Face Detection with a ConvNet and a 3D Model

9

The Smooth l1 Loss of Key-point Locations. At each key-point location
xi ((cid:96)i ≥ 1), we compute a face proposal based on the estimated 3D parameters
and the 3D mean face, denoted by {(ˆx(i)
j=1} the predicted 10 keypoints.
So, for each key-point location xi, we will have m predicted locations, denoted
by ˆxi,j (j = 1, · · · , m). We follow the deﬁnition in [14] to compute the smooth
l1 loss for each axis individually.
m
(cid:88)

j , ˆ(cid:96)(i)

j )10

m
(cid:88)

(cid:88)

Smoothl1(ti − ˆti,j)

Lpt

loc(ω) =

1
m2

i=1

j=1

t∈{x,y}

where the smooth term is deﬁned by,
(cid:40)

Smoothl1 (a) =

0.5a2
|a| − 0.5

if |a| < 1
otherwise.

Faceness Score. The faceness score of a face proposal in our ConvNet is

computed by the sum of log probabilities of the predicted key-points,

Score(ˆxi, ˆ(cid:96)i) =

10
(cid:88)

i=1

log(pˆxi
ˆ(cid:96)i

)

where for simplicity we do not account for the invisibilities of certain key-points.
So the current faceness score has the issue of potential double-counting, espe-
cially for low-resolution faces. We observed that it hurts the quantitative perfor-
mance in our experiments. We will address this issue in future work. See some
heat maps of key-points in Figure 1.

The Smooth l1 Loss of Bounding Boxes. For each face bounding box
proposal ˆB (after NMS), our ConvNet computes its bounding box regression
oﬀsets, t = (tx, ty, tw, th), where t speciﬁes a scale-invariant translation and
log-space height/width shift relative to a proposal, as done in [14,32]. For the
ground-truth bounding box B, we do the same parameterization and have v =
(vx, vy, vw, vh). Assuming that there are K bounding box proposals, we have,

Lbox

loc (ω) =

1
K

K
(cid:88)

(cid:88)

k=1

i∈{x,y,w,h}

Smoothl1(ti − vi)

So, the overall loss function is deﬁned by,

L(ω) = Lcls(ω) + Lpt

loc(ω) + Lbox

loc (ω),

where the third term depends on the output of the ﬁrst two terms, which makes
the loss minimization more challenging. We adopt a method to implement the
diﬀerentiable bounding box warping layer, similar to [6].

4 Experiments

In this section, we present the training procedure and implementation details
and then show evaluation results on the FDDB [19] and AFW [41] benchmarks.

(5)

(6)

(7)

(8)

(9)

10

Y. Li, B. Sun, T. Wu and Y. Wang

Category

Category
Background

Accuracy
97.94% LeftEyeLeftCorner

Accuracy
99.12%
95.50%
LeftEar
RightEyeRightCorner 94.57%
97.78%
NoseRight
98.48%
97.97%
MouthLeftCorner
91.44%
98.64%
98.65%
ChinCenter
96.04% AverageDetectionRate 97.50%

NoseLeft
RightEar
MouthRightCorner
CenterBetweenEyes

Table 1. Classiﬁcation accuracy of the key-points in the AFLW validation set at the
end training.

4.1 Experimental Settings

The Training Dataset. The only dataset we used for training our model is the
AFLW dataset [20], which contains 25, 993 annotated faces in real-world images.
The facial key-points are annotated upon visibility w.r.t. a 3D mean face model
with 21 landmarks. Of the images 70% are used for training while the remaining
is reserved as a validation set.

Training process. For convenience, the short edge of every image is resized
to 600 pixels while preserving the aspect ratio (as done in the faster RCNN [32]),
thus our model learns how to handle faces under various scale. To handle faces
of diﬀerent resolution, we randomly blur images using Gaussian ﬁlters in pre-
processing. Apart from the rescaling and blurring, no other preprocessing mech-
anisms (e.g., random crop or left-right ﬂipping) are used.

We adopt the method of image-centric sampling [14,32] which uses one im-
age at a time in training. Under the consideration that grids around the labeled
position share almost the same context information, thus the 3 × 3 grids around
every labeled key-point’s position are also regarded as the same positive exam-
ples, and we randomly choose the same amount of background examples outside
the bounding boxes. The convolution ﬁlters are initialized by the VGG-16 [4]
pretrained on the ImageNet [8]. We train the network for 13 epoch, and during
the process, the learning rate is modiﬁed from 0.01 to 0.0001.

4.2 Evaluation of the intermediate results

Key-points classiﬁcation in the validation dataset. As are shown by the
heat maps in Figure 1, our model is capable of detecting facial key-points with
rough face conﬁgurations preserved, which shows the eﬀectiveness of exploiting
the 3D mean face model. Table 1 shows the key-point classiﬁcation accuracy on
the validation set in the last epoch in training.

Face proposals. To evaluate the quality of our face proposals, we ﬁrst show
some qualitative results on the FDDB dataset in Fig. 3. These ellipses are directly
calculated from the predicted 3D transformation parameters, forming several
clusters around face instances. We also evaluate the quantitative results of face
proposals. After a non-maximum suppression of IoU 0.7, the recall rate of 93.67%
is obtained with average 34.4 proposals per image.

Face Detection with a ConvNet and a 3D Model

11

Fig. 3. Examples of face proposals computed using predicted 3D transformation pa-
rameters without non-maximum suppression. For clarity, we randomly sample 1/30 of
the original number of proposals.

Fig. 4. FDDB results based on discrete scores using face bounding boxes in evaluation.
The recall rates are computed against 2000 false positives.

4.3 Face Detection Results

To show the eﬀectiveness of our method, we test our model on two popular face
detection benchmarks: FDDB [19] and AFW [41].

Results on FDDB. FDDB is a challenge benchmark for face detection in
unconstrained environment, which contains the annotations for 5171 faces in a
set of 2845 images. We evaluate our results by using the evaluation code provided
by the FDDB authors. The results on the FDDB dataset are shown in Figure 4.
Our result is represented by ”Ours-Conv3D”, which surpasses the recall rate of
90% when encountering 2000 false positives and is competitive to the state-of-
the-art methods. We compare with published methods only. Only DP2MFD [30]
is slightly better than our model on discrete scores. It’s worth noting that we beat
all other methods on continuous scores. This is partly caused by the predeﬁned

12

Y. Li, B. Sun, T. Wu and Y. Wang

Fig. 5. FDDB results based on continuous scores using face ellipses in evaluation. The
recall rates are computed against 2000 false positives.

3D face model helps us better describe the pose and part locations of faces.
We refer to the FDDB result webpage2 for details of the published methods
evaluated on it (Fig. 4 and Fig. 5).

When comparing with recent work Faceness [39], we both recognize that one
of the central issues to alleviate the problems of the occlusion and pose variation
is to introduce facial part detector. However, our mechanism of computing face
bounding box candidates is more straight forward since we explicitly integrate
the structural information of a 3D mean face model instead of using a heuristic
way of assuming the facial part distribution over a bounding box.

Results on AFW. AFW dataset contains 205 images with faces in various
poses and view points. We use the evaluation toolbox provided by [27], which
contains updated annotations for the AFW dataset where the original annota-
tions are not comprehensive enough. Since the method of labeling face bounding
boxes in AFW is diﬀerent from that of in FDDB, we only use face proposals with-
out conﬁguration pooling and bounding box regression. The results on AFW are
shown in Figure 6.

In our current implementation, there is one major limitation that prevents us
from achieving better results. We do not explicitly handle invisible facial parts,
which would be harmful when calculating the faceness score according to Eqn. 7,
we will reﬁne the method and introduce mechanisms of handling the invisible
problem in future work. More detection results on both datasets are shown in
Figure 7 and Figure 8.

2 http://vis-www.cs.umass.edu/fddb/results.html

Face Detection with a ConvNet and a 3D Model

13

Fig. 6. Precision-recall curves on the AFW dataset (AP = average precision) without
conﬁguration pool and face bounding box regression used.

5 Conclusion and Discussion

We have presented a method of end-to-end integration of a ConvNet and a 3D
model for face detection in the wild. Our method is a clean and straightfor-
ward solution when taking into account a 3D model in face detection. It also
addresses two issues in state-of-the-art generic object detection ConvNets: elim-
inating heuristic design of anchor boxes by leveraging a 3D model, and overcom-
ing generic and predeﬁned RoI pooling by conﬁguration pooling which exploits
underlying object conﬁgurations. In experiments, we tested our method on two
benchmarks, the FDDB dataset and the AFW dataset, with very compatible
state-of-the-art performance obtained. We analyzed the experimental results and
pointed out some current limitations.

In our on-going work, we are working on addressing the doubling-counting
issue of the faceness score in the current implementation. We are also working on
extending the proposed method for other types of rigid/semi-rigid object classes
(e.g., cars). We expect that we will have a uniﬁed model for cars and faces which
can achieve state-of-the-art performance, which will be very useful in a lot of
practical applications such as surveillance and driveless cars.

Acknowledgement. Y. Li, B. Sun and Y. Wang were supported in part
by China 973 Program under Grant no. 2015CB351800, and NSFC-61231010,
61527804, 61421062, 61210005. T. Wu was supported by the ECE startup fund
201473-02119 at NCSU. T. Wu also gratefully acknowledge the support of NVIDIA
Corporation with the donation of one GPU.

14

Y. Li, B. Sun, T. Wu and Y. Wang

Fig. 7. Some qualitative results on the FDDB dataset

Fig. 8. Some qualitative results on the AFW dataset

Face Detection with a ConvNet and a 3D Model

15

References

1. Ahonen, T., Hadid, A., Pietik¨ainen, M.: Face description with local binary patterns:
Application to face recognition. IEEE Trans. Pattern Anal. Mach. Intell. 28(12),
2037–2041 (2006)

2. Barbu, A., Gramajo, G.: Face detection using a 3d model on face keypoints. CoRR

abs/1404.3596 (2014)

3. Bay, H., Ess, A., Tuytelaars, T., Gool, L.J.V.: Speeded-up robust features (SURF).

Computer Vision and Image Understanding 110(3), 346–359 (2008)

4. Chatﬁeld, K., Simonyan, K., Vedaldi, A., Zisserman, A.: Return of the devil in the
details: Delving deep into convolutional nets. In: British Machine Vision Conference
(2014)

5. Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao, T., Xu, B., Zhang,
C., Zhang, Z.: Mxnet: A ﬂexible and eﬃcient machine learning library for hetero-
geneous distributed systems. CoRR abs/1512.01274 (2015)

6. Dai, J., He, K., Sun, J.: Instance-aware semantic segmentation via multi-task net-

7. Dalal, N., Triggs, B.: Histograms of oriented gradients for human detection. In:

work cascades. In: CVPR (2016)

CVPR (2005)

8. Deng, J., Dong, W., Socher, R., Li, L., Li, K., Li, F.: Imagenet: A large-scale

hierarchical image database. In: CVPR. pp. 248–255 (2009)

9. Doll´ar, P., Appel, R., Belongie, S.J., Perona, P.: Fast feature pyramids for object

detection. IEEE Trans. Pattern Anal. Mach. Intell. 36(8), 1532–1545 (2014)
10. Felzenszwalb, P.F., Girshick, R.B., McAllester, D.A., Ramanan, D.: Object detec-
tion with discriminatively trained part-based models. IEEE Trans. Pattern Anal.
Mach. Intell. 32(9), 1627–1645 (2010)

11. Fleuret, F., Geman, D.: Coarse-to-ﬁne face detection. International Journal of

Computer Vision 41(1/2), 85–107 (2001)

12. Freiwald, W.A., Tsao, D.Y.: Functional compartmentalization and viewpoint gen-
eralization within the macaque face-processing system. Science 330(6005), 845–851
(2010)

13. Friedman, J., Hastie, T., Tibshirani, R.: Additive logistic regression: a statistical

view of boosting. Annals of Statistics 28, 2000 (1998)

14. Girshick, R.: Fast R-CNN. In: ICCV (2015)
15. Girshick, R.B., Donahue, J., Darrell, T., Malik, J.: Region-based convolutional
networks for accurate object detection and segmentation. IEEE Trans. Pattern
Anal. Mach. Intell. 38(1), 142–158 (2016)

16. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

In: CVPR (2016)

17. Hu, W., Zhu, S.: Learning 3d object templates by quantizing geometry and ap-
pearance spaces. IEEE Trans. Pattern Anal. Mach. Intell. 37(6), 1190–1205 (2015)
18. Huang, C., Ai, H., Li, Y., Lao, S.: High-performance rotation invariant multiview
face detection. IEEE Trans. Pattern Anal. Mach. Intell. 29(4), 671–686 (2007)
19. Jain, V., Learned-Miller, E.: Fddb: A benchmark for face detection in un-
constrained settings. Tech. Rep. UM-CS-2010-009, University of Massachusetts,
Amherst (2010)

20. Koestinger, M., Wohlhart, P., Roth, P.M., Bischof, H.: Annotated facial landmarks
in the wild: A large-scale, real-world database for facial landmark localization.
In: First IEEE International Workshop on Benchmarking Facial Image Analysis
Technologies (2011)

16

Y. Li, B. Sun, T. Wu and Y. Wang

21. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-

volutional neural networks. In: NIPS (2012)

22. LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based learning applied to

document recognition. Proceedings of the IEEE 86(11), 2278–2324 (1998)

23. Li, H., Lin, Z., Shen, X., Brandt, J., Hua, G.: A convolutional neural network

cascade for face detection. In: CVPR (2015)

24. Liebelt, J., Schmid, C.: Multi-view object class detection with a 3d geometric

model. In: CVPR (2010)

25. Liu, C., Shum, H.: Kullback-leibler boosting. In: CVPR (2003)
26. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.:
SSD: Single shot multibox detector. arXiv preprint arXiv:1512.02325 (2015)
27. Mathias, M., Benenson, R., Pedersoli, M., Gool, L.V.: Face detection without bells

and whistles. In: ECCV (2014)

28. Mita, T., Kaneko, T., Hori, O.: Joint haar-like features for face detection. In: ICCV

29. Payet, N., Todorovic, S.: From contours to 3d object detection and pose estimation.

(2005)

In: ICCV (2011)

30. Ranjan, R., Patel, V.M., Chellappa, R.: A deep pyramid deformable part model
for face detection. In: IEEE 7th International Conference on Biometrics Theory,
Applications and Systems (2015)

31. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Uniﬁed,

real-time object detection. In: CVPR (2016)

32. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object

detection with region proposal networks. In: NIPS (2015)

33. Schapire, R.E., Singer, Y.: Improved boosting algorithms using conﬁdence-rated

predictions. Machine Learning 37(3), 297–336 (1999)

34. Sinha, P., Balas, B., Ostrovsky, Y., Russell, R.: Face recognition by humans: 19
results all computer vision researchers should know about. Proceedings of the IEEE
94(11), 1948–1962 (2006)

35. Su, H., Sun, M., Li, F., Savarese, S.: Learning a dense multi-view representation
for detection, viewpoint classiﬁcation and synthesis of object categories. In: ICCV
(2009)

36. Taigman, Y., Yang, M., Ranzato, M., Wolf, L.: Deepface: Closing the gap to human-

level performance in face veriﬁcation. In: CVPR (2014)

37. Uijlings, J.R.R., van de Sande, K.E.A., Gevers, T., Smeulders, A.W.M.: Selective
search for object recognition. International Journal of Computer Vision 104(2),
154–171 (2013)

38. Viola, P.A., Jones, M.J.: Robust real-time face detection. International Journal of

Computer Vision 57(2), 137–154 (2004)

39. Yang, S., Luo, P., Loy, C.C., Tang, X.: From facial parts responses to face detection:

A deep learning approach. In: ICCV (2015)

40. Zafeiriou, S., Zhang, C., Zhang, Z.: A survey on face detection in the wild. Comput.

Vis. Image Underst. 138(C), 1–24 (Sep 2015)

41. Zhu, X., Ramanan, D.: Face detection, pose estimation, and landmark localization

in the wild. In: CVPR (2012)

6
1
0
2
 
g
u
A
 
9
2
 
 
]

V
C
.
s
c
[
 
 
3
v
0
5
8
0
0
.
6
0
6
1
:
v
i
X
r
a

Face Detection with End-to-End Integration of a
ConvNet and a 3D Model

Yunzhu Li1,∗, Benyuan Sun1,(cid:63), Tianfu Wu2 and Yizhou Wang1

1Nat’l Engineering Laboratory for Video Technology,
Key Laboratory of Machine Perception (MoE),
Cooperative Medianet Innovation Center, Shanghai
Sch’l of EECS, Peking University, Beijing, 100871, China
2Department of ECE and the Visual Narrative Cluster,
North Carolina State University, Raleigh, USA
{leo.liyunzhu, sunbenyuan, Yizhou.Wang}@pku.edu.cn, tianfu wu@ncsu.edu

Abstract. This paper presents a method for face detection in the wild,
which integrates a ConvNet and a 3D mean face model in an end-to-end
multi-task discriminative learning framework. The 3D mean face model
is predeﬁned and ﬁxed (e.g., we used the one provided in the AFLW
dataset [20]). The ConvNet consists of two components: (i) The face pro-
posal component computes face bounding box proposals via estimating
facial key-points and the 3D transformation (rotation and translation)
parameters for each predicted key-point w.r.t. the 3D mean face model.
(ii) The face veriﬁcation component computes detection results by prun-
ing and reﬁning proposals based on facial key-points based conﬁguration
pooling. The proposed method addresses two issues in adapting state-
of-the-art generic object detection ConvNets (e.g., faster R-CNN [32])
for face detection: (i) One is to eliminate the heuristic design of prede-
ﬁned anchor boxes in the region proposals network (RPN) by exploit-
ing a 3D mean face model. (ii) The other is to replace the generic RoI
(Region-of-Interest) pooling layer with a conﬁguration pooling layer to
respect underlying object structures. The multi-task loss consists of three
terms: the classiﬁcation Softmax loss and the location smooth l1-losses
[14] of both the facial key-points and the face bounding boxes. In ex-
periments, our ConvNet is trained on the AFLW dataset [20] only and
tested on the FDDB benchmark [19] with ﬁne-tuning and on the AFW
benchmark [41] without ﬁne-tuning. The proposed method obtains very
competitive state-of-the-art performance in the two benchmarks.

Keywords: Face Detection, Face 3D Model, ConvNet, Deep Learning,
Multi-task Learning

1 Introduction

1.1 Motivation and Objective

Face detection has been used as a core module in a wide spectrum of applications
such as surveillance, mobile communication and human-computer interaction. It

(cid:63) Y. Li and B. Sun contributed equally to this work and are joint ﬁrst authors.

2

Y. Li, B. Sun, T. Wu and Y. Wang

is arguably one of the most successful applications of computer vision. Face
detection in the wild continues to play an important role in the era of visual
big data (e.g., images and videos on the web and in social media). However, it
remains a challenging problem in computer vision due to the large appearance
variations caused by nuisance variabilities including viewpoints, occlusion, facial
expression, resolution, illumination and cosmetics, etc.

Fig. 1. Some example results in the FDDB face benchmark [19] computed by the
proposed method. For each testing image, we show the detection results (left) and the
corresponding heat map of facial key-points with the legend shown in the right most
column. (Best viewed in color)

It has been a long history that computer vision researchers study how to learn
a better representation for unconstrained faces [40,12,34]. Recently, together with
large-scale annotated image datasets such as the ImageNet [8], deep ConvNets
[22,21] have made signiﬁcant progress in generic object detection [14,32,16], as
well as in face detection [23,30]. The success is generally considered to be due
to the region proposal methods and region-based ConvNets (R-CNN) [15]. The
two factors used to be addressed separately (e.g., the popular combination of
the Selective Search [37] and R-CNNs pretrained on the ImageNet), and now
they are integrated through introducing the region proposal networks (RPNs)
as done in the faster-RCNN [32] or are merged into a single pipeline for speeding
up the detection as done in [31,26]. In R-CNNs, one key layer is the so-called
RoI (Region-of-Interest) pooling layer [14], which divides a valid RoI (e.g., an
object bounding box proposal) evenly into a grid with a ﬁxed spatial extent
(e.g., 7 × 7) and then uses max-pooling to convert the features inside the RoI
into a small feature map. In this paper, we are interested in adapting state-of-
the-art ConvNets of generic object detection (e.g., the faster R-CNN [32]) for
face detection by overcoming the following two limitations:

i) RPNs need to predeﬁne a number of anchor boxes (with diﬀerent aspect
ratios and sizes), which requires potentially tedious parameter tuning in
training and is sensitive to the (unknown) distribution of the aspect ratios
and sizes of the object instances in a random testing image.

Face Detection with a ConvNet and a 3D Model

3

Fig. 2. Illustration of the proposed method of an end-to-end integration of a ConvNet
and a 3D model for face detection (Top), and some intermediate and the ﬁnal detection
results for an input testing image (Bottom). See the legend for the classiﬁcation score
heat map in Figure 1. The 3D mean face model is predeﬁned and ﬁxed in both training
and testing. The key idea of the proposed method is to learn a ConvNet to estimate
the 3D transformation parameters (rotation and translation) w.r.t. the 3D mean face
model to generate accurate face proposals and predict the face key points. The proposed
ConvNet is trained in a multi-task discriminative training framework consisting of the
classiﬁcation Softmax loss and the location smooth l1-losses [14] of both the facial
key-points and the face bounding boxes. It is surprisingly simple w.r.t. its competitive
state-of-the-art performance compared to the other methods in the popular FDDB
benchmark [19] and the AFW benchmark [41]. See text for details.

ii) The RoI pooling layer in R-CNNs is predeﬁned and generic to all object cat-
egories without exploiting the underlying object structural conﬁgurations,
which either are available from the annotations in the training dataset (e.g.,
the facial landmark annotations in the AFLW dataset [20]) as done in [41]
or can be pursued during learning (such as the deformable part-based mod-
els [10,30]).

To address the two above issues in learning ConvNets for face detection, we
propose to integrate a ConvNet and a 3D mean face model in an end-to-end
multi-task discriminative learning framework. Figure 1 shows some results of
the proposed method.

4

Y. Li, B. Sun, T. Wu and Y. Wang

1.2 Method Overview

Figure 2 illustrates the proposed method. We use 10 facial key-points in this pa-
per, including “LeftEyeLeftCorner”, “RightEyeRightCorner”, “LeftEar”, “Nose-
Left”, “NoseRight”, “RightEar”, “MouthLeftCorner”, “MouthRightCorner”, “Chin-
Center”, “CenterBetweenEyes” (see an example image in the left-top of Fig-
ure 2). The 3D mean face model is then represented by the corresponding ten
3D facial key-points. The architecture of our ConvNet is straight-forward when
taking into account a 3D model (see Section 3.2 for details).

The key idea is to learn a ConvNet to (i) estimate the 3D trans-
formation parameters (rotation and translation) w.r.t. the 3D mean
face model for each detected facial key-point so that we can generate
face bounding box proposals and (ii) predict facial key-points for each
face instance more accurately. Leveraging the 3D mean face model is able
to “kill two birds with one stone”: Firstly, we can eliminate the manually heuris-
tic design of anchor boxes in RPNs. Secondly, instead of using the generic RoI
pooling, we devise a “conﬁguration pooling” layer so as to respect the object
structural conﬁgurations in a meaningful and principled way. In other words,
we propose to learn to compute the proposals in a straight-forward top-down
manner, instead of to design the bottom-up heuristic and then learn related re-
gression parameters. To do so, we assume a 3D mean face model is available and
facial key-points are annotated in the training dataset. Thanks to many excellent
existing work in collecting and annotating face datasets, we can easily obtain
both for faces nowadays. In learning, we have multiple types of losses involved
in the objective loss function, including classiﬁcation Softmax loss and location
smooth l1-loss [14] of facial key-points, and location smooth l1-loss of face bound-
ing boxes respectively, so we formulate the learning of the proposed ConvNet
under the multi-task discriminative deep learning framework (see Section 3.3).
In summary, we provide a clean and straight-forward solution for end-to-end
integration of a ConvNet and a 3D model for face detection 1. In addition to the
competitive performance w.r.t the state-of-the-art face detection methods on the
FDDB and AFW benchmarks, the proposed method is surprisingly simple and
it is able to detect challenging faces (e.g., small, blurry, heavily occluded and
extreme poses).

Potentially, the proposed method can be utilized to learn to detect other
rigid or semi-rigid object categories (such as cars) if the required information
(such as the 3D model and key-point/part annotation) are provided in training.

2 Related Work

There are a tremendous amount of existing works on face detection or generic
object detection. We refer to [40] for a more thorough survey on face detection.
We discuss some of the most relevant ones in this section.
1 We use the open source deep learning package, MXNet [5], in our implementa-
tion. The full source code is released at https://github.com/tfwu/FaceDetection-
ConvNet-3D

Face Detection with a ConvNet and a 3D Model

5

In human/animal vision, how the brain distills a representation of objects
from retinal input is one of the central challenges for systems neuroscience, and
many works have been focused on the ecologically important class of objects–
faces. Studies using fMRI experiments in the macaque reveal that faces are
represented by a system of six discrete, strongly interconnected regions which
illustrates hierarchical information processing in the brain [12], as well as some
other results [34]. These ﬁndings provide some biologically-plausible evidences
for supporting the usage of deep learning based approaches in face detection and
analysis.

The seminal work of Viola and Jones [38] made face detection by a computer
vision system feasible in real world applications, which trained a cascade of Ad-
aBoost classiﬁers using Haar wavelet features. Many works followed this direction
with diﬀerent extensions proposed in four aspects: appearance features (beside
Haar) including Histogram of Oriented Gradients (HOG) [7], Aggregate Channel
Features (ACF) [9], Local Binary Pattern (LBP) features [1] and SURF [3], etc.;
detector structures (beside cascade) including the the scalar tree [11] and the
width-ﬁrst-search tree [18], etc.; strong classiﬁer learning (beside AdaBoost) in-
cluding RealBoost [33] and GentleBoost [13], ect; weak classiﬁer learning (beside
stump function) including the histogram method [25] and the joint binarizations
of Haar-like feature [28], etc..

Most of the recent face detectors are based on the deformable part-based
model (DPM) [10,41,27] with HOG features used, where a face is represented by
a collection of parts deﬁned based on either facial landmarks or heuristic pursuit
as done in the original DPM. [27] showed that a properly trained vanilla DPM
can yield signiﬁcant improvement for face detection.

More recent advances in deep learning [22,21] further boosted the face detec-
tion performance by learning more discriminative features from large-scale raw
data, going beyond those handcrafted ones. In the FDDB benchmark, most of the
face detectors with top performance are based on ConvNets [30,23], combining
with cascade [23] and more explicit structure [39].

3D information has been exploited in learning object models in diﬀerent ways.
Some works [29,35] used a mixture of 3D view based templates by dividing the
view sphere into a number of sectors. [24,17] utilized 3D models in extracting
features and inferring the object pose hypothesis based on EM or DP. [36] used
a 3D face model for aligning faces in learning ConvNets for face recognition. Our
work resembles [2] in exploiting 3D model in face detection, which obtained very
good performance in the FDDB benchmark. [2] computes meaningful 3D pose
candidates by image-based regression from detected face key-points with tradi-
tional handcrafted features, and veriﬁes the 3D pose candidates by a parameter
sensitive classiﬁer based on diﬀerence features relative to the 3D pose. Our work
integrates a ConvNet and a 3D model in an end-to-end multi-task discriminative
learning fashion, which is more straightforward and simpler compared to [2].

Our Contributions. The proposed method contributes to face detection in

three aspects.

6

Y. Li, B. Sun, T. Wu and Y. Wang

i) It presents a simple yet eﬀective method to integrate a ConvNet and a 3D
model in an end-to-end learning with multi-task loss used for face detection
in the wild.

ii) It addresses two limitations in adapting the state-of-the-art faster RCNN [32]
for face detection: eliminating the heuristic design of anchor boxes by lever-
aging a 3D model, and replacing the generic and predeﬁned RoI pooling
with a conﬁguration pooling which exploits the underlying object structural
conﬁgurations.

iii) It obtains very competitive state-of-the-art performance in the FDDB [19]

and AFW [41] benchmarks.

Paper Organization. The remainder of this paper is organized as follows. Sec-
tion 3 presents the method of face detection using a 3D model and details of our
ConvNet including its architecture and training procedure. Section 4 presents
details of experimental settings and shows the experimental results in the FDDB
and AFW benchmarks. Section 5 ﬁrst concludes this paper and then discuss some
on-going and future work to extend the proposed work.

3 The Proposed Method

In this section, we introduce the notations and present details of the proposed
method.

3.1

3D Mean Face Model and Face Representation

In this paper, a 3D mean face model is represented by a collection of n 3D
key-points in the form of (x, y, z) and then is denoted by a n × 3 matrix, F (3).
Usually, each key-point has its own semantic name. We use the 3D mean face
model in the AFLW dataset [20] which consists of 21 key-points. We select 10
key-points as stated above.

A face, denoted by f , is presented by its 3D transformation parameters, Θ,
for rotation and translation, and a collection of 2D key-points, F (2), in the form
of (x, y) (with the number being less than or equal to n). Hence, f = (Θ, F (2)).
The 3D transformation parameters Θ are deﬁned by,

Θ = (µ, s, A(3)),

where µ represents a 2D translation (dx, dy), s a scaling factor, and A(3) a 3 × 3
rotation matrix. We can compute the predicted 2D key-points by,

ˆF (2) = µ + s · π(A(3) · F (3)),

where π() projects a 3D key-point to a 2D one, that is, π : R3 → R2 and
π(x, y, z) = (x, y). Due to the projection π(), we only need 8 parameters out of
the original 12 parameters. Let A(2) denote a 2 × 3 matrix, which is composed
by the top two rows of A(3). We can re-produce the predicted 2D key-points by,

ˆF (2) = µ + A(2) · F (3)

(1)

(2)

(3)

Face Detection with a ConvNet and a 3D Model

7

which makes it easy to implement the computation of back-propagation in train-
ing our ConvNet.

Note that we use the ﬁrst sector in a 4-sector X-Y coordinate system to
deﬁne all the positions, that is, the origin point (0, 0) is deﬁned by the left-
bottom corner in an image lattice.

In face datasets, faces are usually annotated with bounding boxes. In the
FDDB benchmark [19], however, faces are annotated with ellipses and detection
performance are evaluated based on ellipses. Given a set of predicted 2D key-
points ˆF (2), we can compute proposals in both ellipse form and bounding box
form.

Computing a Face Ellipse and a Face Bounding Box based on a set of Pre-
dicted 2D Key-Points. For a given ˆF (2), we ﬁrst predict the position of the top
of head by,

(cid:0) x
y

(cid:1)

TopOfHead = 2 × (cid:0) x

y

(cid:1)

CenterBetweenEyes − (cid:0) x

y

(cid:1)

ChinCenter.

Based on the keypoints of a face proposal, we can compute its ellipse and bound-
ing box.

Face Ellipse. We ﬁrst compute the outer rectangle. We use as one axis the
line segment between the top-of-the-head key-point and the chin key-point, and
then compute the minimum rectangle, usually a rotated rectangle, which covers
all the key-points. Then, we can compute the ellipse using the two edges of the
(rotated) rectangle as the major and minor axes respectively.

Face Bounding Box. We compute a face bounding box by the minimum up-
right rectangle which covers all the key-points, which is also adopted in the
FDDB benchmark [19].

3.2 The Architecture of Our ConvNet

As illustrated in Figure 2, the architecture of our ConvNet consists of:

i) Convolution, ReLu and MaxPooling Layers. We adopt the VGG [4] design
in our experiments which has shown superior performance in a series of tasks.
There are 5 groups and each group has 3 convolution and ReLu consecutive
layers followed by a MaxPooling layer except for the 5th group. The spatial
extent of the ﬁnal feature map is of 16 times smaller than that of an input image
due to the sub-sampling.

ii) An Upsampling Layer. Since we will measure the location diﬀerence be-
tween the input facial key-points and the predicted ones, we add an upsampling
layer to compensate the sub-sampling eﬀects in previous layers. It is implemented
by deconvolution. We upsample the feature maps to 8 times bigger in size (i.e.,
the upsampled feature maps are still quarter size of an input image) consider-
ing the trade-oﬀ between key-point location accuracy, memory consumption and
computation eﬃciency.

iii) A Facial Key-point Label Prediction Layer. There are 11 labels (10 facial
key-points and 1 background class). It is used to compute the classiﬁcation
Softmax loss based on the input in training.

8

Y. Li, B. Sun, T. Wu and Y. Wang

iv) A 3D Transformation Parameter Estimation Layer. This is the key ob-
servation in this paper. Originally, there are 12 parameters in total consisting of
2D translation, scaling and 3 × 3 rotation matrix. Since we focus on the 2D pro-
jected key-points, we only need to account for 8 parameters (see the derivation
above).

v) A Face Proposal Layer. At each position, based on the 3D mean face
model and the estimated 3D transformation parameters, we can compute a face
proposal consisting of 10 predicted facial key-points and the corresponding face
bounding box. The score of a face proposal is the sum of log probabilities of
the 10 predicted facial key-points. The predicated key-points will be used to
compute the smooth l1 loss [14] w.r.t. the ground-truth key-points. We apply
the non-maximum suppression (NMS) to the face proposals in which the overlap
between two bounding boxes a and b is computed by |a|∩|b|
(where | · | represents
the area of a bounding box), instead of the traditional intersection-over-union,
accounting for the fact that it is rarely observed that one face is inside another
one.

|b|

vi) A Conﬁguration Pooling Layer. After NMS, for each face proposal, we
pool the features based on the predicted 10 facial key-points. Here, for simplicity,
we use all the 10 key-points without considering the invisibilities of certain key-
points in diﬀerent face examples.

vii) A Face Bounding Box Regression Layer. It is used to further reﬁne face
bounding boxes in the spirit similar to the method [14]. Based on the conﬁgura-
tion pooling, we add two fully-connected layers to implement the regression. It
is used to compute the smooth l1 loss of face bounding boxes.

Denote by ω all the parameters in our ConvNet, which will be estimated

through multi-task discriminative end-to-end learning.

3.3 The End-to-End Training

Input Data. Denote by C = {0, 1, · · · , 10} as the key-point labels where (cid:96) = 0
represents the background class. We use the image-centric sampling trick as done
in [14,32]. Without loss of generality, considering a training image with only one
face appeared, we have its bounding box, B = (x, y, w, h) and m 2D key-points
(m ≤ 10), {(xi, (cid:96)i)m
i=1} where xi = (xi, yi) is the 2D position of the ith key-point
and (cid:96)i ≥ 1 ∈ C. We randomly sample m locations outside the face bounding box
B as the background class, {(xi, (cid:96)i)2m
i=m+1} (where (cid:96)i = 0, ∀i > m). Note that
in our ConvNet, we use the coordinate of the upsampled feature map which is
half size along both axes of the original input. All the key-points and bounding
boxes are deﬁned accordingly based on ground-truth annotation.

The Classiﬁcation Softmax Loss of Key-point Labels. At each position

xi, our ConvNet outputs a discrete probability distribution, pxi = (pxi
over the 11 classes, which is computed by the Softmax over the 11 scores as
usual [21]. Then, we have the loss,

0 , pxi

1 , · · · , pxi

10),

Lcls(ω) = −

1
2m

2m
(cid:88)

i=1

log(pxi
(cid:96)i

)

(4)

Face Detection with a ConvNet and a 3D Model

9

The Smooth l1 Loss of Key-point Locations. At each key-point location
xi ((cid:96)i ≥ 1), we compute a face proposal based on the estimated 3D parameters
and the 3D mean face, denoted by {(ˆx(i)
j=1} the predicted 10 keypoints.
So, for each key-point location xi, we will have m predicted locations, denoted
by ˆxi,j (j = 1, · · · , m). We follow the deﬁnition in [14] to compute the smooth
l1 loss for each axis individually.
m
(cid:88)

j , ˆ(cid:96)(i)

j )10

m
(cid:88)

(cid:88)

Smoothl1(ti − ˆti,j)

Lpt

loc(ω) =

1
m2

i=1

j=1

t∈{x,y}

where the smooth term is deﬁned by,
(cid:40)

Smoothl1 (a) =

0.5a2
|a| − 0.5

if |a| < 1
otherwise.

Faceness Score. The faceness score of a face proposal in our ConvNet is

computed by the sum of log probabilities of the predicted key-points,

Score(ˆxi, ˆ(cid:96)i) =

10
(cid:88)

i=1

log(pˆxi
ˆ(cid:96)i

)

where for simplicity we do not account for the invisibilities of certain key-points.
So the current faceness score has the issue of potential double-counting, espe-
cially for low-resolution faces. We observed that it hurts the quantitative perfor-
mance in our experiments. We will address this issue in future work. See some
heat maps of key-points in Figure 1.

The Smooth l1 Loss of Bounding Boxes. For each face bounding box
proposal ˆB (after NMS), our ConvNet computes its bounding box regression
oﬀsets, t = (tx, ty, tw, th), where t speciﬁes a scale-invariant translation and
log-space height/width shift relative to a proposal, as done in [14,32]. For the
ground-truth bounding box B, we do the same parameterization and have v =
(vx, vy, vw, vh). Assuming that there are K bounding box proposals, we have,

Lbox

loc (ω) =

1
K

K
(cid:88)

(cid:88)

k=1

i∈{x,y,w,h}

Smoothl1(ti − vi)

So, the overall loss function is deﬁned by,

L(ω) = Lcls(ω) + Lpt

loc(ω) + Lbox

loc (ω),

where the third term depends on the output of the ﬁrst two terms, which makes
the loss minimization more challenging. We adopt a method to implement the
diﬀerentiable bounding box warping layer, similar to [6].

4 Experiments

In this section, we present the training procedure and implementation details
and then show evaluation results on the FDDB [19] and AFW [41] benchmarks.

(5)

(6)

(7)

(8)

(9)

10

Y. Li, B. Sun, T. Wu and Y. Wang

Category

Category
Background

Accuracy
97.94% LeftEyeLeftCorner

Accuracy
99.12%
95.50%
LeftEar
RightEyeRightCorner 94.57%
97.78%
NoseRight
98.48%
97.97%
MouthLeftCorner
91.44%
98.64%
98.65%
ChinCenter
96.04% AverageDetectionRate 97.50%

NoseLeft
RightEar
MouthRightCorner
CenterBetweenEyes

Table 1. Classiﬁcation accuracy of the key-points in the AFLW validation set at the
end training.

4.1 Experimental Settings

The Training Dataset. The only dataset we used for training our model is the
AFLW dataset [20], which contains 25, 993 annotated faces in real-world images.
The facial key-points are annotated upon visibility w.r.t. a 3D mean face model
with 21 landmarks. Of the images 70% are used for training while the remaining
is reserved as a validation set.

Training process. For convenience, the short edge of every image is resized
to 600 pixels while preserving the aspect ratio (as done in the faster RCNN [32]),
thus our model learns how to handle faces under various scale. To handle faces
of diﬀerent resolution, we randomly blur images using Gaussian ﬁlters in pre-
processing. Apart from the rescaling and blurring, no other preprocessing mech-
anisms (e.g., random crop or left-right ﬂipping) are used.

We adopt the method of image-centric sampling [14,32] which uses one im-
age at a time in training. Under the consideration that grids around the labeled
position share almost the same context information, thus the 3 × 3 grids around
every labeled key-point’s position are also regarded as the same positive exam-
ples, and we randomly choose the same amount of background examples outside
the bounding boxes. The convolution ﬁlters are initialized by the VGG-16 [4]
pretrained on the ImageNet [8]. We train the network for 13 epoch, and during
the process, the learning rate is modiﬁed from 0.01 to 0.0001.

4.2 Evaluation of the intermediate results

Key-points classiﬁcation in the validation dataset. As are shown by the
heat maps in Figure 1, our model is capable of detecting facial key-points with
rough face conﬁgurations preserved, which shows the eﬀectiveness of exploiting
the 3D mean face model. Table 1 shows the key-point classiﬁcation accuracy on
the validation set in the last epoch in training.

Face proposals. To evaluate the quality of our face proposals, we ﬁrst show
some qualitative results on the FDDB dataset in Fig. 3. These ellipses are directly
calculated from the predicted 3D transformation parameters, forming several
clusters around face instances. We also evaluate the quantitative results of face
proposals. After a non-maximum suppression of IoU 0.7, the recall rate of 93.67%
is obtained with average 34.4 proposals per image.

Face Detection with a ConvNet and a 3D Model

11

Fig. 3. Examples of face proposals computed using predicted 3D transformation pa-
rameters without non-maximum suppression. For clarity, we randomly sample 1/30 of
the original number of proposals.

Fig. 4. FDDB results based on discrete scores using face bounding boxes in evaluation.
The recall rates are computed against 2000 false positives.

4.3 Face Detection Results

To show the eﬀectiveness of our method, we test our model on two popular face
detection benchmarks: FDDB [19] and AFW [41].

Results on FDDB. FDDB is a challenge benchmark for face detection in
unconstrained environment, which contains the annotations for 5171 faces in a
set of 2845 images. We evaluate our results by using the evaluation code provided
by the FDDB authors. The results on the FDDB dataset are shown in Figure 4.
Our result is represented by ”Ours-Conv3D”, which surpasses the recall rate of
90% when encountering 2000 false positives and is competitive to the state-of-
the-art methods. We compare with published methods only. Only DP2MFD [30]
is slightly better than our model on discrete scores. It’s worth noting that we beat
all other methods on continuous scores. This is partly caused by the predeﬁned

12

Y. Li, B. Sun, T. Wu and Y. Wang

Fig. 5. FDDB results based on continuous scores using face ellipses in evaluation. The
recall rates are computed against 2000 false positives.

3D face model helps us better describe the pose and part locations of faces.
We refer to the FDDB result webpage2 for details of the published methods
evaluated on it (Fig. 4 and Fig. 5).

When comparing with recent work Faceness [39], we both recognize that one
of the central issues to alleviate the problems of the occlusion and pose variation
is to introduce facial part detector. However, our mechanism of computing face
bounding box candidates is more straight forward since we explicitly integrate
the structural information of a 3D mean face model instead of using a heuristic
way of assuming the facial part distribution over a bounding box.

Results on AFW. AFW dataset contains 205 images with faces in various
poses and view points. We use the evaluation toolbox provided by [27], which
contains updated annotations for the AFW dataset where the original annota-
tions are not comprehensive enough. Since the method of labeling face bounding
boxes in AFW is diﬀerent from that of in FDDB, we only use face proposals with-
out conﬁguration pooling and bounding box regression. The results on AFW are
shown in Figure 6.

In our current implementation, there is one major limitation that prevents us
from achieving better results. We do not explicitly handle invisible facial parts,
which would be harmful when calculating the faceness score according to Eqn. 7,
we will reﬁne the method and introduce mechanisms of handling the invisible
problem in future work. More detection results on both datasets are shown in
Figure 7 and Figure 8.

2 http://vis-www.cs.umass.edu/fddb/results.html

Face Detection with a ConvNet and a 3D Model

13

Fig. 6. Precision-recall curves on the AFW dataset (AP = average precision) without
conﬁguration pool and face bounding box regression used.

5 Conclusion and Discussion

We have presented a method of end-to-end integration of a ConvNet and a 3D
model for face detection in the wild. Our method is a clean and straightfor-
ward solution when taking into account a 3D model in face detection. It also
addresses two issues in state-of-the-art generic object detection ConvNets: elim-
inating heuristic design of anchor boxes by leveraging a 3D model, and overcom-
ing generic and predeﬁned RoI pooling by conﬁguration pooling which exploits
underlying object conﬁgurations. In experiments, we tested our method on two
benchmarks, the FDDB dataset and the AFW dataset, with very compatible
state-of-the-art performance obtained. We analyzed the experimental results and
pointed out some current limitations.

In our on-going work, we are working on addressing the doubling-counting
issue of the faceness score in the current implementation. We are also working on
extending the proposed method for other types of rigid/semi-rigid object classes
(e.g., cars). We expect that we will have a uniﬁed model for cars and faces which
can achieve state-of-the-art performance, which will be very useful in a lot of
practical applications such as surveillance and driveless cars.

Acknowledgement. Y. Li, B. Sun and Y. Wang were supported in part
by China 973 Program under Grant no. 2015CB351800, and NSFC-61231010,
61527804, 61421062, 61210005. T. Wu was supported by the ECE startup fund
201473-02119 at NCSU. T. Wu also gratefully acknowledge the support of NVIDIA
Corporation with the donation of one GPU.

14

Y. Li, B. Sun, T. Wu and Y. Wang

Fig. 7. Some qualitative results on the FDDB dataset

Fig. 8. Some qualitative results on the AFW dataset

Face Detection with a ConvNet and a 3D Model

15

References

1. Ahonen, T., Hadid, A., Pietik¨ainen, M.: Face description with local binary patterns:
Application to face recognition. IEEE Trans. Pattern Anal. Mach. Intell. 28(12),
2037–2041 (2006)

2. Barbu, A., Gramajo, G.: Face detection using a 3d model on face keypoints. CoRR

abs/1404.3596 (2014)

3. Bay, H., Ess, A., Tuytelaars, T., Gool, L.J.V.: Speeded-up robust features (SURF).

Computer Vision and Image Understanding 110(3), 346–359 (2008)

4. Chatﬁeld, K., Simonyan, K., Vedaldi, A., Zisserman, A.: Return of the devil in the
details: Delving deep into convolutional nets. In: British Machine Vision Conference
(2014)

5. Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao, T., Xu, B., Zhang,
C., Zhang, Z.: Mxnet: A ﬂexible and eﬃcient machine learning library for hetero-
geneous distributed systems. CoRR abs/1512.01274 (2015)

6. Dai, J., He, K., Sun, J.: Instance-aware semantic segmentation via multi-task net-

7. Dalal, N., Triggs, B.: Histograms of oriented gradients for human detection. In:

work cascades. In: CVPR (2016)

CVPR (2005)

8. Deng, J., Dong, W., Socher, R., Li, L., Li, K., Li, F.: Imagenet: A large-scale

hierarchical image database. In: CVPR. pp. 248–255 (2009)

9. Doll´ar, P., Appel, R., Belongie, S.J., Perona, P.: Fast feature pyramids for object

detection. IEEE Trans. Pattern Anal. Mach. Intell. 36(8), 1532–1545 (2014)
10. Felzenszwalb, P.F., Girshick, R.B., McAllester, D.A., Ramanan, D.: Object detec-
tion with discriminatively trained part-based models. IEEE Trans. Pattern Anal.
Mach. Intell. 32(9), 1627–1645 (2010)

11. Fleuret, F., Geman, D.: Coarse-to-ﬁne face detection. International Journal of

Computer Vision 41(1/2), 85–107 (2001)

12. Freiwald, W.A., Tsao, D.Y.: Functional compartmentalization and viewpoint gen-
eralization within the macaque face-processing system. Science 330(6005), 845–851
(2010)

13. Friedman, J., Hastie, T., Tibshirani, R.: Additive logistic regression: a statistical

view of boosting. Annals of Statistics 28, 2000 (1998)

14. Girshick, R.: Fast R-CNN. In: ICCV (2015)
15. Girshick, R.B., Donahue, J., Darrell, T., Malik, J.: Region-based convolutional
networks for accurate object detection and segmentation. IEEE Trans. Pattern
Anal. Mach. Intell. 38(1), 142–158 (2016)

16. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

In: CVPR (2016)

17. Hu, W., Zhu, S.: Learning 3d object templates by quantizing geometry and ap-
pearance spaces. IEEE Trans. Pattern Anal. Mach. Intell. 37(6), 1190–1205 (2015)
18. Huang, C., Ai, H., Li, Y., Lao, S.: High-performance rotation invariant multiview
face detection. IEEE Trans. Pattern Anal. Mach. Intell. 29(4), 671–686 (2007)
19. Jain, V., Learned-Miller, E.: Fddb: A benchmark for face detection in un-
constrained settings. Tech. Rep. UM-CS-2010-009, University of Massachusetts,
Amherst (2010)

20. Koestinger, M., Wohlhart, P., Roth, P.M., Bischof, H.: Annotated facial landmarks
in the wild: A large-scale, real-world database for facial landmark localization.
In: First IEEE International Workshop on Benchmarking Facial Image Analysis
Technologies (2011)

16

Y. Li, B. Sun, T. Wu and Y. Wang

21. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-

volutional neural networks. In: NIPS (2012)

22. LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based learning applied to

document recognition. Proceedings of the IEEE 86(11), 2278–2324 (1998)

23. Li, H., Lin, Z., Shen, X., Brandt, J., Hua, G.: A convolutional neural network

cascade for face detection. In: CVPR (2015)

24. Liebelt, J., Schmid, C.: Multi-view object class detection with a 3d geometric

model. In: CVPR (2010)

25. Liu, C., Shum, H.: Kullback-leibler boosting. In: CVPR (2003)
26. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.:
SSD: Single shot multibox detector. arXiv preprint arXiv:1512.02325 (2015)
27. Mathias, M., Benenson, R., Pedersoli, M., Gool, L.V.: Face detection without bells

and whistles. In: ECCV (2014)

28. Mita, T., Kaneko, T., Hori, O.: Joint haar-like features for face detection. In: ICCV

29. Payet, N., Todorovic, S.: From contours to 3d object detection and pose estimation.

(2005)

In: ICCV (2011)

30. Ranjan, R., Patel, V.M., Chellappa, R.: A deep pyramid deformable part model
for face detection. In: IEEE 7th International Conference on Biometrics Theory,
Applications and Systems (2015)

31. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Uniﬁed,

real-time object detection. In: CVPR (2016)

32. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object

detection with region proposal networks. In: NIPS (2015)

33. Schapire, R.E., Singer, Y.: Improved boosting algorithms using conﬁdence-rated

predictions. Machine Learning 37(3), 297–336 (1999)

34. Sinha, P., Balas, B., Ostrovsky, Y., Russell, R.: Face recognition by humans: 19
results all computer vision researchers should know about. Proceedings of the IEEE
94(11), 1948–1962 (2006)

35. Su, H., Sun, M., Li, F., Savarese, S.: Learning a dense multi-view representation
for detection, viewpoint classiﬁcation and synthesis of object categories. In: ICCV
(2009)

36. Taigman, Y., Yang, M., Ranzato, M., Wolf, L.: Deepface: Closing the gap to human-

level performance in face veriﬁcation. In: CVPR (2014)

37. Uijlings, J.R.R., van de Sande, K.E.A., Gevers, T., Smeulders, A.W.M.: Selective
search for object recognition. International Journal of Computer Vision 104(2),
154–171 (2013)

38. Viola, P.A., Jones, M.J.: Robust real-time face detection. International Journal of

Computer Vision 57(2), 137–154 (2004)

39. Yang, S., Luo, P., Loy, C.C., Tang, X.: From facial parts responses to face detection:

A deep learning approach. In: ICCV (2015)

40. Zafeiriou, S., Zhang, C., Zhang, Z.: A survey on face detection in the wild. Comput.

Vis. Image Underst. 138(C), 1–24 (Sep 2015)

41. Zhu, X., Ramanan, D.: Face detection, pose estimation, and landmark localization

in the wild. In: CVPR (2012)

