8
1
0
2
 
v
o
N
 
9
2
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
5
6
5
1
1
.
5
0
8
1
:
v
i
X
r
a

On gradient regularizers for MMD GANs

Michael Arbel∗
Gatsby Computational Neuroscience Unit
University College London
michael.n.arbel@gmail.com

Dougal J. Sutherland∗
Gatsby Computational Neuroscience Unit
University College London
dougal@gmail.com

Mikołaj Bi ´nkowski
Department of Mathematics
Imperial College London
mikbinkowski@gmail.com

Arthur Gretton
Gatsby Computational Neuroscience Unit
University College London
arthur.gretton@gmail.com

Abstract

We propose a principled method for gradient-based regularization of the critic of
GAN-like models trained by adversarially optimizing the kernel of a Maximum
Mean Discrepancy (MMD). We show that controlling the gradient of the critic
is vital to having a sensible loss function, and devise a method to enforce exact,
analytical gradient constraints at no additional cost compared to existing approxi-
mate techniques based on additive regularizers. The new loss function is provably
continuous, and experiments show that it stabilizes and accelerates training, giving
image generation models that outperform state-of-the art methods on 160 × 160
CelebA and 64 × 64 unconditional ImageNet.

1

Introduction

There has been an explosion of interest in implicit generative models (IGMs) over the last few years,
especially after the introduction of generative adversarial networks (GANs) [16]. These models
allow approximate samples from a complex high-dimensional target distribution P, using a model
distribution Qθ, where estimation of likelihoods, exact inference, and so on are not tractable. GAN-
type IGMs have yielded very impressive empirical results, particularly for image generation, far
beyond the quality of samples seen from most earlier generative models [e.g. 18, 22, 23, 24, 38].

These excellent results, however, have depended on adding a variety of methods of regularization and
other tricks to stabilize the notoriously difﬁcult optimization problem of GANs [38, 42]. Some of
this difﬁculty is perhaps because when a GAN is viewed as minimizing a discrepancy DGAN(P, Qθ),
its gradient ∇θ DGAN(P, Qθ) does not provide useful signal to the generator if the target and model
distributions are not absolutely continuous, as is nearly always the case [2].

An alternative set of losses are the integral probability metrics (IPMs) [36], which can give credit to
models Qθ “near” to the target distribution P [3, 8, Section 4 of 15]. IPMs are deﬁned in terms of a
critic function: a “well behaved” function with large amplitude where P and Qθ differ most. The IPM
is the difference in the expected critic under P and Qθ, and is zero when the distributions agree. The
Wasserstein IPMs, whose critics are made smooth via a Lipschitz constraint, have been particularly
successful in IGMs [3, 14, 18]. But the Lipschitz constraint must hold uniformly, which can be hard
to enforce. A popular approximation has been to apply a gradient constraint only in expectation [18]:
the critic’s gradient norm is constrained to be small on points chosen uniformly between P and Q.

Another class of IPMs used as IGM losses are the Maximum Mean Discrepancies (MMDs) [17],
as in [13, 28]. Here the critic function is a member of a reproducing kernel Hilbert space (except
in [50], who learn a deep approximation to an RKHS critic). Better performance can be obtained,

∗These authors contributed equally.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.

however, when the MMD kernel is not based directly on image pixels, but on learned features of
images. Wasserstein-inspired gradient regularization approaches can be used on the MMD critic
when learning these features: [27] uses weight clipping [3], and [5, 7] use a gradient penalty [18].

The recent Sobolev GAN [33] uses a similar constraint on the expected gradient norm, but phrases it
as estimating a Sobolev IPM rather than loosely approximating Wasserstein. This expectation can be
taken over the same distribution as [18], but other measures are also proposed, such as (P + Qθ) /2.
A second recent approach, the spectrally normalized GAN [32], controls the Lipschitz constant of
the critic by enforcing the spectral norms of the weight matrices to be 1. Gradient penalties also
beneﬁt GANs based on f -divergences [37]: for instance, the spectral normalization technique of [32]
can be applied to the critic network of an f -GAN. Alternatively, a gradient penalty can be deﬁned
to approximate the effect of blurring P and Qθ with noise [40], which addresses the problem of
non-overlapping support [2]. This approach has recently been shown to yield locally convergent
optimization in some cases with non-continuous distributions, where the original GAN does not [30].

In this paper, we introduce a novel regularization for the MMD GAN critic of [5, 7, 27], which
directly targets generator performance, rather than adopting regularization methods intended to
approximate Wasserstein distances [3, 18]. The new MMD regularizer derives from an approach
widely used in semi-supervised learning [10, Section 2], where the aim is to deﬁne a classiﬁcation
function f which is positive on P (the positive class) and negative on Qθ (negative class), in the
absence of labels on many of the samples. The decision boundary between the classes is assumed
to be in a region of low density for both P and Qθ: f should therefore be ﬂat where P and Qθ have
support (areas with constant label), and have a larger slope in regions of low density. Bousquet et al.
[10] propose as their regularizer on f a sum of the variance and a density-weighted gradient norm.
We adopt a related penalty on the MMD critic, with the difference that we only apply the penalty on P:
thus, the critic is ﬂatter where P has high mass, but does not vanish on the generator samples from Qθ
(which we optimize). In excluding Qθ from the critic function constraint, we also avoid the concern
raised by [32] that a critic depending on Qθ will change with the current minibatch – potentially
leading to less stable learning. The resulting discrepancy is no longer an integral probability metric:
it is asymmetric, and the critic function class depends on the target P being approximated.

We ﬁrst discuss in Section 2 how MMD-based losses can be used to learn implicit generative models,
and how a naive approach could fail. This motivates our new discrepancies, introduced in Section 3.
Section 4 demonstrates that these losses outperform state-of-the-art models for image generation.

2 Learning implicit generative models with MMD-based losses

An IGM is a model Qθ which aims to approximate a target distribution P over a space X ⊆ Rd.
We will deﬁne Qθ by a generator function Gθ : Z → X , implemented as a deep network with
parameters θ, where Z is a space of latent codes, say R128. We assume a ﬁxed distribution on Z,
say Z ∼ Uniform (cid:0)[−1, 1]128(cid:1), and call Qθ the distribution of Gθ(Z). We will consider learning by
minimizing a discrepancy D between distributions, with D(P, Qθ) ≥ 0 and D(P, P) = 0, which we
call our loss. We aim to minimize D(P, Qθ) with stochastic gradient descent on an estimator of D.
In the present work, we will build losses D based on the Maximum Mean Discrepancy,

MMDk(P, Q) =

EX∼P[f (X)] − EY ∼Q[f (Y )],

(1)

sup
f : (cid:107)f (cid:107)Hk ≤1

an integral probability metric where the critic class is the unit ball within Hk, the reproducing
kernel Hilbert space with a kernel k. The optimization in (1) admits a simple closed-form optimal
critic, f ∗(t) ∝ EX∼P[k(X, t)] − EY ∼Q[k(Y, t)]. There is also an unbiased, closed-form estimator of
MMD2
k with appealing statistical properties [17] – in particular, its sample complexity is independent
of the dimension of X , compared to the exponential dependence [52] of the Wasserstein distance

W(P, Q) =

sup
f : (cid:107)f (cid:107)Lip≤1

EX∼P[f (X)] − EY ∼Q[f (Y )].

(2)

The MMD is continuous in the weak topology for any bounded kernel with Lipschitz embeddings [46,
D−→ P, then MMD(Pn, P) → 0.
Theorem 3.2(b)], meaning that if Pn converges in distribution to P, Pn
W−→ P implies
(W is continuous in the slightly stronger Wasserstein topology [51, Deﬁnition 6.9]; Pn

2

D−→ P, and the two notions coincide if X is bounded.) Continuity means the loss can provide
Pn
better signal to the generator as Qθ approaches P, as opposed to e.g. Jensen-Shannon where the loss
could be constant until suddenly jumping to 0 [e.g. 3, Example 1]. The MMD is also strict, meaning
it is zero iff P = Qθ, for characteristic kernels [45]. The Gaussian kernel yields an MMD both
continuous in the weak topology and strict. Thus in principle, one need not conduct any alternating
optimization in an IGM at all, but merely choose generator parameters θ to minimize MMDk.

Despite these appealing properties, using simple pixel-level kernels leads to poor generator samples
[8, 13, 28, 48]. More recent MMD GANs [5, 7, 27] achieve better results by using a parameterized
family of kernels, {kψ}ψ∈Ψ, in the Optimized MMD loss previously studied by [44, 46]:

DΨ

MMD(P, Q) := sup
ψ∈Ψ

MMDkψ (P, Q).

(3)

We primarily consider kernels deﬁned by some ﬁxed kernel K on top of a learned low-dimensional
representation φψ : X → Rs, i.e. kψ(x, y) = K(φψ(x), φψ(y)), denoted kψ = K ◦ φψ. In practice,
K is a simple characteristic kernel, e.g. Gaussian, and φψ is usually a deep network with output
dimension say s = 16 [7] or even s = 1 (in our experiments). If φψ is powerful enough, this choice
is sufﬁcient; we need not try to ensure each kψ is characteristic, as did [27].
Proposition 1. Suppose k = K ◦ φψ, with K characteristic and {φψ} rich enough that for any
P (cid:54)= Q, there is a ψ ∈ Ψ for which φψ#P (cid:54)= φψ#Q.2 Then if P (cid:54)= Q, DΨ
Proof. Let ˆψ ∈ Ψ be such that φ ˆψ(P) (cid:54)= φ ˆψ(Q). Then, since K is characteristic,

MMD(P, Q) > 0.

DΨ

MMD(P, Q) = sup
ψ∈Ψ

MMDK(φψ#P, φψ#Q) ≥ MMDK(φ ˆψ#P, φ ˆψ#Q) > 0.

MMD, one can conduct alternating optimization to estimate a ˆψ and then update the
To estimate DΨ
generator according to MMDk ˆψ
, similar to the scheme used in GANs and WGANs. (This form of
estimator is justiﬁed by an envelope theorem [31], although it is invariably biased [7].) Unlike DGAN
or W, ﬁxing a ˆψ and optimizing the generator still yields a sensible distance MMDk ˆψ
Early attempts at minimizing DΨ
could be because for some kernel classes, DΨ
Example 1 (DiracGAN [30]). We wish to model a point mass at the origin of R, P = δ0, with any
possible point mass, Qθ = δθ for θ ∈ R. We use a Gaussian kernel of any bandwidth, which can be
2 (a − b)2(cid:1). Then
written as kψ = K ◦ φψ with φψ(x) = ψx for ψ ∈ Ψ = R and K(a, b) = exp (cid:0)− 1
θ (cid:54)= 0
2
θ = 0

MMD in an IGM, though, were unsuccessful [48, footnote 7]. This
MMD is stronger than Wasserstein or MMD.

(δ0, δθ) = 2 (cid:0)1 − exp (cid:0)− 1

MMD(δ0, δθ) =

2 ψ2θ2(cid:1)(cid:1) ,

MMD2
kψ

(cid:26)√
0

DΨ

.

.

Considering DΨ
distance is not continuous in the weak or Wasserstein topologies.

2 (cid:54)→ 0, even though δ1/n

MMD(δ0, δ1/n) =

W−→ δ0, shows that the Optimized MMD

√

This also causes optimization issues. Figure 1 (a) shows gradient vector ﬁelds in parameter space,
(δ0, δθ)(cid:1). Some sequences following v (e.g. A)
v(θ, ψ) ∝ (cid:0) − ∇θ MMD2
(δ0, δθ), ∇ψ MMD2
kψ
kψ
converge to an optimal solution (0, ψ), but some (B) move in the wrong direction, and others (C) are
stuck because there is essentially no gradient. Figure 1 (c, red) shows that the optimal DΨ
MMD critic
is very sharp near P and Q; this is less true for cases where the algorithm converged.
We can avoid these issues if we ensure a bounded Lipschitz critic:3
Proposition 2. Assume the critics fψ(x) = (EX∼P kψ(X, x) − EY ∼Q kψ(Y, x))/ MMDkψ (P, Q)
are uniformly bounded and have a common Lipschitz constant: supx∈X ,ψ∈Ψ|fψ(x)| < ∞ and
supψ∈Ψ(cid:107)fψ(cid:107)Lip < ∞. In particular, this holds when kψ = K ◦ φψ and

sup
a∈Rs

K(a, a) < ∞,

(cid:107)K(a, ·) − K(b, ·)(cid:107)HK ≤ LK(cid:107)a − b(cid:107)Rs,

(cid:107)φψ(cid:107)Lip ≤ Lφ < ∞.

sup
ψ∈Ψ

Then DΨ

MMD is continuous in the weak topology: if Pn

D−→ P, then DΨ

MMD(Pn, P) → 0.

2 f #P denotes the pushforward of a distribution: if X ∼ P, then f (X) ∼ f #P.
3[27, Theorem 4] makes a similar claim to Proposition 2, but its proof was incorrect: it tries to uniformly

bound MMDkψ ≤ W 2, but the bound used is for a Wasserstein in terms of (cid:107)kψ(x, ·) − kψ(y, ·)(cid:107)Hkψ

.

3

Figure 1: The setting of Example 1. (a, b): parameter-space gradient ﬁelds for the MMD and the
SMMD (Section 3.3); the horizontal axis is θ, and the vertical 1/ψ. (c): optimal MMD critics for
θ = 20 with different kernels. (d): the MMD and the distances of Section 3 optimized over ψ.

Proof. The main result is [12, Corollary 11.3.4]. To show the claim for kψ = K ◦ φψ, note that
|fψ(x) − fψ(y)| ≤ (cid:107)fψ(cid:107)Hkψ

(cid:107)kψ(x, ·) − kψ(y, ·)(cid:107)Hkψ

, which since (cid:107)fψ(cid:107)Hkψ

= 1 is

(cid:107)K(φψ(x), ·) − K(φψ(y), ·)(cid:107)HK ≤ LK(cid:107)φψ(x) − φψ(y)(cid:107)Rs ≤ LKLφ(cid:107)x − y(cid:107)Rd .

Indeed, if we put a box constraint on ψ [27] or regularize the gradient of the critic function [7],
the resulting MMD GAN generally matches or outperforms WGAN-based models. Unfortunately,
though, an additive gradient penalty doesn’t substantially change the vector ﬁeld of Figure 1 (a), as
shown in Figure 5 (Appendix B). We will propose distances with much better convergence behavior.

3 New discrepancies for learning implicit generative models

Our aim here is to introduce a discrepancy that can provide useful gradient information when used as
an IGM loss. Proofs of results in this section are deferred to Appendix A.

3.1 Lipschitz Maximum Mean Discrepancy

Proposition 2 shows that an MMD-like discrepancy can be continuous under the weak topology even
when optimizing over kernels, if we directly restrict the critic functions to be Lipschitz. We can easily
deﬁne such a distance, which we call the Lipschitz MMD: for some λ > 0,

≤1

f ∈Hk : (cid:107)f (cid:107)2

LipMMDk,λ(P, Q) :=

EX∼P [f (X)] − EY ∼Q [f (Y )] .

sup
Lip+λ(cid:107)f (cid:107)2
For a universal kernel k, we conjecture that limλ→0 LipMMDk,λ(P, Q) → W(P, Q). But for any k
and λ, LipMMD is upper-bounded by W, as (4) optimizes over a smaller set of functions than (2).
Thus DΨ,λ
LipMMD(P, Q) := supψ∈Ψ LipMMDkψ,λ(P, Q) is also upper-bounded by W, and hence is
continuous in the Wasserstein topology. It also shows excellent empirical behavior on Example 1
(Figure 1 (d), and Figure 5 in Appendix B). But estimating LipMMDk,λ, let alone DΨ,λ
LipMMD, is in
general extremely difﬁcult (Appendix D), as ﬁnding (cid:107)f (cid:107)Lip requires optimization in the input space.
Constraining the mean gradient rather than the maximum, as we will do next, is far more tractable.

(4)

Hk

4

3.2 Gradient-Constrained Maximum Mean Discrepancy

sup
f ∈Hk : (cid:107)f (cid:107)S(µ),k,λ≤1
L2(µ) + (cid:107)∇f (cid:107)2

We deﬁne the Gradient-Constrained MMD for λ > 0 and using some measure µ as

GCMMDµ,k,λ(P, Q) :=

EX∼P [f (X)] − EY ∼Q [f (Y )] ,

(5)

where (cid:107)f (cid:107)2

S(µ),k,λ := (cid:107)f (cid:107)2

L2(µ) + λ(cid:107)f (cid:107)2
Hk

(6)
L2(µ) = (cid:82) (cid:107)·(cid:107)2 µ(dx) denotes the squared L2 norm. Rather than directly constraining the Lipschitz
(cid:107)·(cid:107)2
constant, the second term (cid:107)∇f (cid:107)2
L2(µ) encourages the function f to be ﬂat where µ has mass. In
experiments we use µ = P, ﬂattening the critic near the target sample. We add the ﬁrst term following
[10]: in one dimension and with µ uniform, (cid:107)·(cid:107)S(µ),·,0 is then an RKHS norm with the kernel
κ(x, y) = exp(−(cid:107)x − y(cid:107)), which is also a Sobolev space. The correspondence to a Sobolev norm is
lost in higher dimensions [53, Ch. 10], but we also found the ﬁrst term to be beneﬁcial in practice.

.

We can exploit some properties of Hk to compute (5) analytically. Call the difference in kernel mean
embeddings η := EX∼P[k(X, ·)] − EY ∼Q[k(Y, ·)] ∈ Hk; recall MMD(P, Q) = (cid:107)η(cid:107)Hk .
Proposition 3. Let ˆµ = (cid:80)M
RM d with (m, i)th entry4 ∂iη(Xm). Then under Assumptions (A) to (D) in Appendix A.1,

m=1 δXm . Deﬁne η(X) ∈ RM with mth entry η(Xm), and ∇η(X) ∈

GCMMD2

ˆµ,k,λ(P, Q) =

(cid:0)MMD2(P, Q) − ¯P (η)(cid:1)

1
λ
(cid:20) η(X)
∇η(X)

¯P (η) =

(cid:21)T (cid:18)(cid:20)K GT
G H

(cid:21)

+ M λIM +M d

(cid:19)−1 (cid:20) η(X)
∇η(X)

(cid:21)

,

where K is the kernel matrix Km,m(cid:48) = k(Xm, Xm(cid:48)), G is the matrix of left derivatives 5 G(m,i),m(cid:48) =
∂ik(Xm, Xm(cid:48)), and H that of derivatives of both arguments H(m,i),(m(cid:48),j) = ∂i∂j+dk(Xm, Xm(cid:48)).
As long as P and Q have integrable ﬁrst moments, and µ has second moments, Assumptions (A)
to (D) are satisﬁed e.g. by a Gaussian or linear kernel on top of a differentiable φψ. We can thus
estimate the GCMMD based on samples from P, Q, and µ by using the empirical mean ˆη for η.

This discrepancy indeed works well in practice: Appendix F.2 shows that optimizing our estimate
of Dµ,Ψ,λ
GCMMD = supψ∈Ψ GCMMDµ,kψ,λ yields a good generative model on MNIST. But the linear
system of size M + M d is impractical: even on 28 × 28 images and using a low-rank approximation,
the model took days to converge. We therefore design a less expensive discrepancy in the next section.

The GCMMD is related to some discrepancies previously used in IGM training. The Fisher GAN [34]
uses only the variance constraint (cid:107)f (cid:107)2
L2(µ) ≤ 1,
along with a vanishing boundary condition on f to ensure a well-deﬁned solution (although this was
not used in the implementation, and can cause very unintuitive critic behavior; see Appendix C).
The authors considered several choices of µ, including the WGAN-GP measure [18] and mixtures
(P + Qθ) /2. Rather than enforcing the constraints in closed form as we do, though, these models
used additive regularization. We will compare to the Sobolev GAN in experiments.

L2(µ) ≤ 1. The Sobolev GAN [33] constrains (cid:107)∇f (cid:107)2

3.3 Scaled Maximum Mean Discrepancy

We will now derive a lower bound on the Gradient-Constrained MMD which retains many of its
attractive qualities but can be estimated in time linear in the dimension d.
Proposition 4. Make Assumptions (A) to (D). For any f ∈ Hk, (cid:107)f (cid:107)S(µ),k,λ ≤ σ−1

µ,k,λ(cid:107)f (cid:107)Hk , where

(cid:118)
(cid:117)
(cid:117)
(cid:116)λ +

(cid:46)

(cid:90)

σµ,k,λ := 1

k(x, x)µ(dx) +

d
(cid:88)

i=1

(cid:90) ∂2k(y, z)
∂yi∂zi

(cid:12)
(cid:12)
(cid:12)(y,z)=(x,x)

µ(dx).

We then deﬁne the Scaled Maximum Mean Discrepancy based on this bound of Proposition 4:

SMMDµ,k,λ(P, Q) :=

sup

EX∼P [f (X)]−EY ∼Q [f (Y )] = σµ,k,λ MMDk(P, Q). (7)

f : σ−1

µ,k,λ(cid:107)f (cid:107)H≤1

4We use (m, i) to denote (m − 1)d + i; thus ∇η(X) stacks ∇η(X1), . . . , ∇η(XM ) into one vector.
5We use ∂ik(x, y) to denote the partial derivative with respect to xi, and ∂i+dk(x, y) that for yi.

5

Because the constraint in the optimization of (7) is more restrictive than in that of (5), we have
that SMMDµ,k,λ(P, Q) ≤ GCMMDµ,k,λ(P, Q). The Sobolev norm (cid:107)f (cid:107)S(µ),λ, and a fortiori the
gradient norm under µ, is thus also controlled for the SMMD critic. We also show in Appendix F.1
that SMMDµ,k,λ behaves similarly to GCMMDµ,k,λ on Gaussians.
(cid:3).
If kψ = K ◦ φψ and K(a, b) = g(−(cid:107)a − b(cid:107)2), then σ−2
(cid:3). Estimating
k,µ,λ = λ + Eµ
Or if K is linear, K(a, b) = aTb, then σ−2
these terms based on samples from µ is straightforward, giving a natural estimator for the SMMD.

k,µ,λ = λ + g(0) + 2|g(cid:48)(0)| Eµ

(cid:2)(cid:107)φψ(X)(cid:107)2 + (cid:107)∇φψ(X)(cid:107)2

(cid:2)(cid:107)∇φψ(X)(cid:107)2

F

F

Of course, if µ and k are ﬁxed, the SMMD is simply a constant times the MMD, and so behaves
in essentially the same way as the MMD. But optimizing the SMMD over a kernel family Ψ,
Dµ,Ψ,λ

SMMD(P, Q) := supψ∈Ψ SMMDµ,kψ,λ(P, Q), gives a distance very different from DΨ

MMD (3).

Figure 1 (b) shows the vector ﬁeld for the Optimized SMMD loss in Example 1, using the WGAN-
GP measure µ = Uniform(0, θ). The optimization surface is far more amenable: in particular
the location C, which formerly had an extremely small gradient that made learning effectively
impossible, now converges very quickly by ﬁrst reducing the critic gradient until some signal is
available. Figure 1 (d) demonstrates that Dµ,Ψ,λ
LipMMD but in sharp contrast
to DΨ

SMMD, like Dµ,Ψ,λ
MMD, is continuous with respect to the location θ and provides a strong gradient towards 0.

GCMMD and DΨ,λ

SMMD is continuous in the Wasserstein topology under some conditions:

We can establish that Dµ,Ψ,λ
Theorem 1. Let kψ = K ◦ φψ, with φψ : X → Rs a fully-connected L-layer network with
Leaky-ReLUα activations whose layers do not increase in width, and K satisfying mild smoothness
conditions QK < ∞ (Assumptions (II) to (V) in Appendix A.2). Let Ψκ be the set of parameters where
each layer’s weight matrices have condition number cond(W l) = (cid:107)W l(cid:107)/ σmin(W l) ≤ κ < ∞. If µ
has a density (Assumption (I)), then

Thus if Pn

W−→ P, then Dµ,Ψκ,λ

Dµ,Ψκ,λ

SMMD (P, Q) ≤

QKκL/2
√
dLαL/2
SMMD (Pn, P) → 0, even if µ is chosen to depend on P and Q.

W(P, Q).

L2(µ) = Eµ(cid:107)∇fψ(X)(cid:107)2 does
Uniform bounds vs bounds in expectation Controlling (cid:107)∇fψ(cid:107)2
not necessarily imply a bound on (cid:107)f (cid:107)Lip ≥ supx∈X (cid:107)∇fψ(X)(cid:107), and so does not in general give
continuity via Proposition 2. Theorem 1 implies that when the network’s weights are well-conditioned,
it is sufﬁcient to only control (cid:107)∇fψ(cid:107)2

L2(µ), which is far easier in practice than controlling (cid:107)f (cid:107)Lip.

If we instead tried to directly controlled (cid:107)f (cid:107)Lip with e.g. spectral normalization (SN) [32], we
could signiﬁcantly reduce the expressiveness of the parametric family. In Example 1, constraining
(cid:107)φψ(cid:107)Lip = 1 limits us to only Ψ = {1}. Thus D{1}
MMD is simply the MMD with an RBF kernel
of bandwidth 1, which has poor gradients when θ is far from 0 (Figure 1 (c), blue). The Cauchy-
Schwartz bound of Proposition 4 allows jointly adjusting the smoothness of kψ and the critic f , while
SN must control the two independently. Relatedly, limiting (cid:107)φ(cid:107)Lip by limiting the Lipschitz norm of
each layer could substantially reduce capacity, while (cid:107)∇fψ(cid:107)L2(µ) need not be decomposed by layer.
Another advantage is that µ provides a data-dependent measure of complexity as in [10]: we do not
needlessly prevent ourselves from using critics that behave poorly only far from the data.

Spectral parametrization When the generator is near a local optimum, the critic might identify
only one direction on which Qθ and P differ. If the generator parameterization is such that there
is no local way for the generator to correct it, the critic may begin to single-mindedly focus on
this difference, choosing redundant convolutional ﬁlters and causing the condition number of the
weights to diverge. If this occurs, the generator will be motivated to ﬁx this single direction while
ignoring all other aspects of the distributions, after which it may become stuck. We can help avoid
this collapse by using a critic parameterization that encourages diverse ﬁlters with higher-rank weight
matrices. Miyato et al. [32] propose to parameterize the weight matrices as W = γ ¯W /(cid:107) ¯W (cid:107)op,
where (cid:107) ¯W (cid:107)op is the spectral norm of ¯W . This parametrization works particularly well with Dµ,Ψ,λ
SMMD;
Figure 2 (b) shows the singular values of the second layer of a critic’s network (and Figure 9, in
Appendix F.3, shows more layers), while Figure 2 (d) shows the evolution of the condition number
during training. The conditioning of the weight matrix remains stable throughout training with
spectral parametrization, while it worsens through training in the default case.

6

4 Experiments

We evaluated unsupervised image generation on three datasets: CIFAR-10 [26] (60 000 images,
32 × 32), CelebA [29] (202 599 face images, resized and cropped to 160 × 160 as in [7]), and the
more challenging ILSVRC2012 (ImageNet) dataset [41] (1 281 167 images, resized to 64 × 64).
Code for all of these experiments is available at github.com/MichaelArbel/Scaled-MMD-GAN.
Losses All models are based on a scalar-output critic network φψ : X → R, except MMDGAN-GP
where φψ : X → R16 as in [7]. The WGAN and Sobolev GAN use a critic f = φψ, while the
GAN uses a discriminator Dψ(x) = 1/(1 + exp(−φψ(x))). The MMD-based methods use a kernel
kψ(x, y) = exp(−(φψ(x) − φψ(y))2/2), except for MMDGAN-GP which uses a mixture of RQ
kernels as in [7]. Increasing the output dimension of the critic or using a different kernel didn’t
substantially change the performance of our proposed method. We also consider SMMD with a linear
top-level kernel, k(x, y) = φψ(x)φψ(y); because this becomes essentially identical to a WGAN
(Appendix E), we refer to this method as SWGAN. SMMD and SWGAN use µ = P; Sobolev GAN
uses µ = (P + Q)/2 as in [33]. We choose λ and an overall scaling to obtain the losses:

SMMD:

(cid:92)MMD

2
kψ
1 + 10 EˆP [(cid:107)∇φψ(X)(cid:107)2
F ]

(P, Qθ)

, SWGAN:

(cid:113)

EˆP [φψ(X)] − E ˆQθ

[φψ(X)]

1 + 10 (cid:0)EˆP [|φψ(X)|2] + EˆP [(cid:107)∇φψ(X)(cid:107)2

F ](cid:1)

.

Architecture For CIFAR-10, we used the CNN architecture proposed by [32] with a 7-layer critic
and a 4-layer generator. For CelebA, we used a 5-layer DCGAN discriminator and a 10-layer ResNet
generator as in [7]. For ImageNet, we used a 10-layer ResNet for both the generator and discriminator.
In all experiments we used 64 ﬁlters for the smallest convolutional layer, and double it at each layer
(CelebA/ImageNet) or every other layer (CIFAR-10). The input codes for the generator are drawn
from Uniform (cid:0)[−1, 1]128(cid:1). We consider two parameterizations for each critic: a standard one where
the parameters can take any real value, and a spectral parametrization (denoted SN-) as above [32].
Models without explicit gradient control (SN-GAN, SN-MMDGAN, SN-MMGAN-L2, SN-WGAN)
ﬁx γ = 1, for spectral normalization; others learn γ, using a spectral parameterization.

Training All models were trained for 150 000 generator updates on a single GPU, except for ImageNet
where the model was trained on 3 GPUs simultaneously. To limit communication overhead we
averaged the MMD estimate on each GPU, giving the block MMD estimator [54]. We always used
64 samples per GPU from each of P and Q, and 5 critic updates per generator step. We used initial
learning rates of 0.0001 for CIFAR-10 and CelebA, 0.0002 for ImageNet, and decayed these rates
using the KID adaptive scheme of [7]: every 2 000 steps, generator samples are compared to those
from 20 000 steps ago, and if the relative KID test [9] fails to show an improvement three consecutive
times, the learning rate is decayed by 0.8. We used the Adam optimizer [25] with β1 = 0.5, β2 = 0.9.

Evaluation To compare the sample quality of different models, we considered three different scores
based on the Inception network [49] trained for ImageNet classiﬁcation, all using default parameters
in the implementation of [7]. The Inception Score (IS) [42] is based on the entropy of predicted
labels; higher values are better. Though standard, this metric has many issues, particularly on datasets
other than ImageNet [4, 7, 20]. The FID [20] instead measures the similarity of samples from the
generator and the target as the Wasserstein-2 distance between Gaussians ﬁt to their intermediate
representations. It is more sensible than the IS and becoming standard, but its estimator is strongly
biased [7]. The KID [7] is similar to FID, but by using a polynomial-kernel MMD its estimates enjoy
better statistical properties and are easier to compare. (A similar score was recommended by [21].)

Results Table 1a presents the scores for models trained on both CIFAR-10 and CelebA datasets. On
CIFAR-10, SN-SWGAN and SN-SMMDGAN performed comparably to SN-GAN. But on CelebA,
SN-SWGAN and SN-SMMDGAN dramatically outperformed the other methods with the same
architecture in all three metrics. It also trained faster, and consistently outperformed other methods
over multiple initializations (Figure 2 (a)). It is worth noting that SN-SWGAN far outperformed
WGAN-GP on both datasets. Table 1b presents the scores for SMMDGAN and SN-SMMDGAN
trained on ImageNet, and the scores of pre-trained models using BGAN [6] and SN-GAN [32].6 The

6These models are courtesy of the respective authors and also trained at 64 × 64 resolution. SN-GAN used
the same architecture as our model, but trained for 250 000 generator iterations; BS-GAN used a similar 5-layer
ResNet architecture and trained for 74 epochs, comparable to SN-GAN.

7

Figure 2: The training process on CelebA. (a) KID scores. We report a ﬁnal score for SN-GAN
slightly before its sudden failure mode; MMDGAN and SN-MMDGAN were unstable and had scores
around 100. (b) Singular values of the second layer, both early (dashed) and late (solid) in training.
(c) σ−2
µ,k,λ for several MMD-based methods. (d) The condition number in the ﬁrst layer through
training. SN alone does not control σµ,k,λ, and SMMD alone does not control the condition number.

(a) Scaled MMD GAN with SN

(b) SN-GAN

(c) Boundary Seeking GAN

(d) Scaled MMD GAN with SN

(e) Scaled WGAN with SN

(f) MMD GAN with GP+L2

Figure 3: Samples from various models. Top: 64 × 64 ImageNet; bottom: 160 × 160 CelebA.

8

Table 1: Mean (standard deviation) of score estimates, based on 50 000 samples from each model.

(a) CIFAR-10 and CelebA.

Method

WGAN-GP
MMDGAN-GP-L2
Sobolev-GAN
SMMDGAN
SN-GAN
SN-SWGAN
SN-SMMDGAN

CIFAR-10
IS

FID

6.9±0.2
6.9±0.1
7.0±0.1
7.0±0.1
7.2±0.1
7.2±0.1
7.3±0.1

31.1±0.2
31.4±0.3
30.3±0.3
31.5±0.4
26.7±0.2
28.5±0.2
25.0±0.3

KID×103

22.2±1.1
23.3±1.1
22.3±1.2
22.2±1.1
16.1±0.9
17.6±1.1
16.6±2.0

CelebA
IS

2.7±0.0
2.6±0.0
2.9±0.0
2.7±0.0
2.7±0.0
2.8±0.0
2.8±0.0

FID

KID×103

29.2±0.2
20.5±0.2
16.4±0.1
18.4±0.2
22.6±0.1
14.1±0.2
12.4±0.2

22.0±1.0
13.0±1.0
10.6±0.5
11.5±0.8
14.6±1.1
7.7±0.5
6.1±0.4

(b) ImageNet.

Method

IS

FID

10.7±0.4
BGAN
11.2±0.1
SN-GAN
SMMDGAN
10.7±0.2
SN-SMMDGAN 10.9±0.1

43.9±0.3
47.5±0.1
38.4±0.3
36.6±0.2

KID×103

47.0±1.1
44.4±2.2
39.3±2.5
34.6±1.6

proposed methods substantially outperformed both methods in FID and KID scores. Figure 3 shows
samples on ImageNet and CelebA; Appendix F.4 has more.

Spectrally normalized WGANs / MMDGANs To control for the contribution of the spectral
parametrization to the performance, we evaluated variants of MMDGANs, WGANs and Sobolev-
GAN using spectral normalization (in Table 2, Appendix F.3). WGAN and Sobolev-GAN led to
unstable training and didn’t converge at all (Figure 11) despite many attempts. MMDGAN converged
on CIFAR-10 (Figure 11) but was unstable on CelebA (Figure 10). The gradient control due to SN
is thus probably too loose for these methods. This is reinforced by Figure 2 (c), which shows that
the expected gradient of the critic network is much better-controlled by SMMD, even when SN is
used. We also considered variants of these models with a learned γ while also adding a gradient
penalty and an L2 penalty on critic activations [7, footnote 19]. These generally behaved similarly to
MMDGAN, and didn’t lead to substantial improvements. We ran the same experiments on CelebA,
but aborted the runs early when it became clear that training was not successful.

Rank collapse We occasionally observed the failure mode for SMMD where the critic becomes
low-rank, discussed in Section 3.3, especially on CelebA; this failure was obvious even in the training
objective. Figure 2 (b) is one of these examples. Spectral parametrization seemed to prevent this
behavior. We also found one could avoid collapse by reverting to an earlier checkpoint and increasing
the RKHS regularization parameter λ, but did not do this for any of the experiments here.

5 Conclusion

We studied gradient regularization for MMD-based critics in implicit generative models, clarifying
how previous techniques relate to the DΨ
MMD loss. Based on these insights, we proposed the Gradient-
Constrained MMD and its approximation the Scaled MMD, a new loss function for IGMs that
controls gradient behavior in a principled way and obtains excellent performance in practice.

One interesting area of future study for these distances is their behavior when used to diffuse particles
distributed as Q towards particles distributed as P. Mroueh et al. [33, Appendix A.1] began such a
study for the Sobolev GAN loss; [35] proved convergence and studied discrete-time approximations.

Another area to explore is the geometry of these losses, as studied by Bottou et al. [8], who showed
potential advantages of the Wasserstein geometry over the MMD. Their results, though, do not
address any distances based on optimized kernels; the new distances introduced here might have
interesting geometry of their own.

9

References

[1] B. Amos and J. Z. Kolter. “OptNet: Differentiable Optimization as a Layer in Neural Net-

works.” ICML. 2017. arXiv: 1703.00443.

[2] M. Arjovsky and L. Bottou. “Towards Principled Methods for Training Generative Adversarial

Networks.” ICLR. 2017. arXiv: 1701.04862.

[3] M. Arjovsky, S. Chintala, and L. Bottou. “Wasserstein Generative Adversarial Networks.”

ICML. 2017. arXiv: 1701.07875.

[4] S. Barratt and R. Sharma. A Note on the Inception Score. 2018. arXiv: 1801.01973.
[5] M. G. Bellemare, I. Danihelka, W. Dabney, S. Mohamed, B. Lakshminarayanan, S. Hoyer,
and R. Munos. The Cramer Distance as a Solution to Biased Wasserstein Gradients. 2017.
arXiv: 1705.10743.

[6] D. Berthelot, T. Schumm, and L. Metz. BEGAN: Boundary Equilibrium Generative Adversarial

[7] M. Bi´nkowski, D. J. Sutherland, M. Arbel, and A. Gretton. “Demystifying MMD GANs.”

Networks. 2017. arXiv: 1703.10717.

ICLR. 2018. arXiv: 1801.01401.

[8] L. Bottou, M. Arjovsky, D. Lopez-Paz, and M. Oquab. “Geometrical Insights for Implicit
Generative Modeling.” Braverman Readings in Machine Learning: Key Iedas from Inception
to Current State. Ed. by L. Rozonoer, B. Mirkin, and I. Muchnik. LNAI Vol. 11100. Springer,
2018, pp. 229–268. arXiv: 1712.07822.

[9] W. Bounliphone, E. Belilovsky, M. B. Blaschko, I. Antonoglou, and A. Gretton. “A Test of
Relative Similarity For Model Selection in Generative Models.” ICLR. 2016. arXiv: 1511.
04581.

[10] O. Bousquet, O. Chapelle, and M. Hein. “Measure Based Regularization.” NIPS. 2004.
[11] A. Brock, T. Lim, J. M. Ritchie, and N. Weston. “Neural Photo Editing with Introspective

Adversarial Networks.” ICLR. 2017. arXiv: 1609.07093.

[12] R. M. Dudley. Real Analysis and Probability. 2nd ed. Cambridge University Press, 2002.
[13] G. K. Dziugaite, D. M. Roy, and Z. Ghahramani. “Training generative neural networks via

Maximum Mean Discrepancy optimization.” UAI. 2015. arXiv: 1505.03906.

[14] A. Genevay, G. Peyré, and M. Cuturi. “Learning Generative Models with Sinkhorn Diver-

gences.” AISTATS. 2018. arXiv: 1706.00292.

[15] T. Gneiting and A. E. Raftery. “Strictly proper scoring rules, prediction, and estimation.” JASA

102.477 (2007), pp. 359–378.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio. “Generative Adversarial Nets.” NIPS. 2014. arXiv: 1406.2661.

[17] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Schölkopf, and A. J. Smola. “A Kernel Two-

Sample Test.” JMLR 13 (2012).
I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. “Improved Training of
Wasserstein GANs.” NIPS. 2017. arXiv: 1704.00028.

[19] A. Güngör. “Some bounds for the product of singular values.” International Journal of

[16]

[18]

Contemporary Mathematical Sciences (2007).

[20] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, G. Klambauer, and S. Hochreiter. “GANs
Trained by a Two Time-Scale Update Rule Converge to a Nash Equilibrium.” NIPS. 2017.
arXiv: 1706.08500.

[21] G. Huang, Y. Yuan, Q. Xu, C. Guo, Y. Sun, F. Wu, and K. Weinberger. An empirical study on

evaluation metrics of generative adversarial networks. 2018. arXiv: 1806.07755.

[22] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. “Multimodal Unsupervised Image-to-Image

Translation.” ECCV. 2018. arXiv: 1804.04732.

[23] Y. Jin, K. Zhang, M. Li, Y. Tian, H. Zhu, and Z. Fang. Towards the Automatic Anime Characters

Creation with Generative Adversarial Networks. 2017. arXiv: 1708.05509.

[24] T. Karras, T. Aila, S. Laine, and J. Lehtinen. “Progressive Growing of GANs for Improved

Quality, Stability, and Variation.” ICLR. 2018. arXiv: 1710.10196.

[25] D. Kingma and J. Ba. “Adam: A Method for Stochastic Optimization.” ICLR. 2015. arXiv:

1412.6980.

[26] A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images. 2009.

10

arXiv: 1502.02761.

arXiv: 1411.7766.

(2002), pp. 583–601.

1711.04894.

[27] C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. Póczos. “MMD GAN: Towards Deeper

Understanding of Moment Matching Network.” NIPS. 2017. arXiv: 1705.08584.

[28] Y. Li, K. Swersky, and R. Zemel. “Generative Moment Matching Networks.” ICML. 2015.

[29] Z. Liu, P. Luo, X. Wang, and X. Tang. “Deep learning face attributes in the wild.” ICCV. 2015.

[30] L. Mescheder, A. Geiger, and S. Nowozin. “Which Training Methods for GANs do actually

Converge?” ICML. 2018. arXiv: 1801.04406.

[31] P. Milgrom and I. Segal. “Envelope theorems for arbitrary choice sets.” Econometrica 70.2

[32] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. “Spectral Normalization for Generative

Adversarial Networks.” ICLR. 2018. arXiv: 1802.05927.

[33] Y. Mroueh, C.-L. Li, T. Sercu, A. Raj, and Y. Cheng. “Sobolev GAN.” ICLR. 2018. arXiv:

[34] Y. Mroueh and T. Sercu. “Fisher GAN.” NIPS. 2017. arXiv: 1705.09675.
[35] Y. Mroueh, T. Sercu, and A. Raj. Regularized Kernel and Neural Sobolev Descent: Dynamic

MMD Transport. 2018. arXiv: 1805.12062.

[36] A. Müller. “Integral Probability Metrics and their Generating Classes of Functions.” Advances

in Applied Probability 29.2 (1997), pp. 429–443.

[37] S. Nowozin, B. Cseke, and R. Tomioka. “f-GAN: Training Generative Neural Samplers using

Variational Divergence Minimization.” NIPS. 2016. arXiv: 1606.00709.

[38] A. Radford, L. Metz, and S. Chintala. “Unsupervised Representation Learning with Deep
Convolutional Generative Adversarial Networks.” ICLR. 2016. arXiv: 1511.06434.
J. R. Retherford. “Review: J. Diestel and J. J. Uhl, Jr., Vector measures.” Bull. Amer. Math.
Soc. 84.4 (July 1978), pp. 681–685.

[39]

[40] K. Roth, A. Lucchi, S. Nowozin, and T. Hofmann. “Stabilizing Training of Generative Adver-

sarial Networks through Regularization.” NIPS. 2017. arXiv: 1705.09367.

[41] O. Russakovsky et al. ImageNet Large Scale Visual Recognition Challenge. 2014. arXiv:

1409.0575.

[42] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. “Improved

[43]

Techniques for Training GANs.” NIPS. 2016. arXiv: 1606.03498.
J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University
Press, 2004.

[44] B. K. Sriperumbudur, K. Fukumizu, A. Gretton, G. R. G. Lanckriet, and B. Schölkopf. “Kernel

choice and classiﬁability for RKHS embeddings of probability distributions.” NIPS. 2009.

[45] B. K. Sriperumbudur, K. Fukumizu, and G. R. G. Lanckriet. “Universality, Characteristic
Kernels and RKHS Embedding of Measures.” JMLR 12 (2011), pp. 2389–2410. arXiv: 1003.
0887.

[46] B. Sriperumbudur. “On the optimal estimation of probability mesaures in weak and strong

topologies.” Bernoulli 22.3 (2016), pp. 1839–1893. arXiv: 1310.8240.
I. Steinwart and A. Christmann. Support Vector Machines. Springer, 2008.

[47]
[48] D. J. Sutherland, H.-Y. Tung, H. Strathmann, S. De, A. Ramdas, A. Smola, and A. Gretton.
“Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy.” ICLR.
2017. arXiv: 1611.04488.

[49] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. “Rethinking the Inception

Architecture for Computer Vision.” CVPR. 2016. arXiv: 1512.00567.

[50] T. Unterthiner, B. Nessler, C. Seward, G. Klambauer, M. Heusel, H. Ramsauer, and S. Hochre-
iter. “Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields.” ICLR. 2018.
arXiv: 1708.08819.

[51] C. Villani. Optimal Transport: Old and New. Springer, 2009.
[52]

J. Weed and F. Bach. “Sharp asymptotic and ﬁnite-sample rates of convergence of empirical
measures in Wasserstein distance.” Bernoulli (forthcoming). arXiv: 1707.00087.
[53] H. Wendland. Scattered Data Approximation. Cambridge University Press, 2005.
[54] W. Zaremba, A. Gretton, and M. B. Blaschko. “B-tests: Low Variance Kernel Two-Sample

Tests.” NIPS. 2013. arXiv: 1307.1954.

11

A Proofs

We ﬁrst review some basic properties of Reproducing Kernel Hilbert Spaces. We consider here a
separable RKHS H with basis (ei)i∈I , where I is either ﬁnite if H is ﬁnite-dimensional, or I = N
otherwise. We also assume that the reproducing kernel k is continuously twice differentiable.

We use a slightly nonstandard notation for derivatives: ∂if (x) denotes the ith partial derivative of f
evaluated at x, and ∂i∂j+dk(x, y) denotes ∂2k(a,b)
∂ai∂bj

|(a,b)=(x,y).

Then the following reproducing properties hold for any given function f in H [47, Lemma 4.34]:
f (x) =(cid:104)f, k(x, .)(cid:105)H
∂if (x) =(cid:104)f, ∂ik(x, .)(cid:105)H.

(8)
(9)

We say that an operator A : H (cid:55)→ H is Hilbert-Schmidt if (cid:107)A(cid:107)2
H is ﬁnite. (cid:107)A(cid:107)HS
is called the Hilbert-Schmidt norm of A. The space of Hilbert-Schmidt operators itself a Hilbert
space with the inner product (cid:104)A, B(cid:105)HS = (cid:80)
i∈I (cid:104)Aei, Bei(cid:105)H. Moreover, we say that an operator A
is trace-class if its trace norm is ﬁnite, i.e. (cid:107)A(cid:107)1 = (cid:80)
2 ei(cid:105)H < ∞. The outer product
f ⊗ g for f, g ∈ H gives an H → H operator such that (f ⊗ g)v = (cid:104)g, v(cid:105)Hf for all v in H.

i∈I (cid:104)ei, (A∗A) 1

i∈I (cid:107)Aei(cid:107)2

HS = (cid:80)

Given two vectors f and g in H and a Hilbert-Schmidt operator A we have the following properties:

(i) The outer product f ⊗ g is a Hilbert-Schmidt operator with Hilbert-Schmidt norm given by:

(ii) The inner product between two rank-one operators f ⊗ g and u ⊗ v is (cid:104)f ⊗ g, u ⊗ v(cid:105)HS =

(cid:107)f ⊗ g(cid:107)HS = (cid:107)f (cid:107)H(cid:107)g(cid:107)H.

(cid:104)f, u(cid:105)H(cid:104)g, v(cid:105)H.

(iii) The following identity holds: (cid:104)f, Ag(cid:105)H = (cid:104)f ⊗ g, A(cid:105)HS.

Deﬁne the following covariance-type operators:

d
(cid:88)

i=1

Dx = k(x, ·) ⊗ k(x, ·) +

∂ik(x, ·) ⊗ ∂ik(x, ·) Dµ = EX∼µ DX Dµ,λ = Dµ + λI;

(10)

these are useful in that, using (8) and (9), (cid:104)f, Dxg(cid:105) = f (x)g(x) + (cid:80)d

i=1 ∂if (x) ∂ig(x).

A.1 Deﬁnitions and estimators of the new distances

We will need the following assumptions about the distributions P and Q, the measure µ, and the
kernel k:

(A) P and Q have integrable ﬁrst moments.
(B) (cid:112)k(x, x) grows at most linearly in x: for all x in X , (cid:112)k(x, x) ≤ C((cid:107)x(cid:107) + 1) for some

constant C.

(C) The kernel k is twice continuously differentiable.
(D) The functions x (cid:55)→ k(x, x) and x (cid:55)→ ∂i∂i+dk(x, x) for 1 ≤ i ≤ d are µ-integrable.

When k = K ◦ φψ, Assumption (B) is automatically satisﬁed by a K such as the Gaussian; when K
is linear, it is true for a quite general class of networks φψ [7, Lemma 1].

We will ﬁrst give a form for the Gradient-Constrained MMD (5) in terms of the operator (10):
Proposition 5. Under Assumptions (A) to (D), the Gradient-Constrained MMD is given by

GCMMDµ,k,λ(P, Q) =

(cid:104)η, D−1

µ,λη(cid:105)H.

(cid:113)

(11)

Proof of Proposition 5. Let f be a function in H. We will ﬁrst express the squared λ-regularized
Sobolev norm of f (6) as a quadratic form in H. Recalling the reproducing properties of (8) and (9),
we have:

(cid:107)f (cid:107)2

S(µ),k,λ =

(cid:104)f, k(x, ·)(cid:105)2

H µ(dx) +

(cid:104)f, ∂ik(x, ·)(cid:105)2

H µ(dx) + λ(cid:107)f (cid:107)2
H.

(cid:90)

d
(cid:88)

(cid:90)

i=1

12

Using Property (ii) and the operator (10), one further gets

(cid:107)f (cid:107)2

S(µ),k,λ =

(cid:104)f ⊗ f, Dx(cid:105)HS µ(dx) + λ(cid:107)f (cid:107)2
H.

(cid:90)

Under Assumption (D), and using Lemma 6, one can take the integral inside the inner product, which
leads to (cid:107)f (cid:107)2

H. Finally, using Property (iii) it follows that

S(µ),k,λ = (cid:104)f ⊗ f, Dµ(cid:105)HS + λ(cid:107)f (cid:107)2

(cid:107)f (cid:107)2

S(µ),k,λ = (cid:104)f, Dµ,λf (cid:105)H.

Under Assumptions (A) and (B), Lemma 6 applies, and it follows that k(x, ·) is also Bochner
integrable under P and Q. Thus

EP [(cid:104)f, k(x, ·)(cid:105)H] − EQ [(cid:104)f, k(x, ·)(cid:105)H] = (cid:104)f, EP [k(x, ·)] − EP [k(x, ·)](cid:105)H = (cid:104)f, η(cid:105)H,

where η is deﬁned as this difference in mean embeddings.

Since Dµ,λ is symmetric positive deﬁnite, its square-root D

1
2

For any f ∈ H, let g = D
corresponding f = D− 1

1
2

µ,λ is well-deﬁned and is also invertible.
H. Note that for any g ∈ H, there is a
µ,λg. Thus we can re-express the maximization problem in (5) in terms of g:

µ,λf , so that (cid:104)f, Dµ,λf (cid:105)H = (cid:107)g(cid:107)2

2

GCMMDµ,k,λ(P, Q) :=

sup
f ∈H
(cid:104)f,Dµ,λf (cid:105)H≤1
(cid:104)g, D− 1

2

= sup
g∈H
(cid:107)g(cid:107)H≤1

(cid:104)f, η(cid:105)H = sup
g∈H
(cid:107)g(cid:107)H≤1

(cid:104)D− 1

2

µ,λg, η(cid:105)H

µ,λη(cid:105)H = (cid:107)D− 1

2

µ,λη(cid:107)H =

(cid:104)η, D−1

µ,λη(cid:105)H.

(cid:113)

Proposition 5, though, involves inverting the inﬁnite-dimensional operator Dµ,λ and thus doesn’t
directly give us a computable estimator. Proposition 3 solves this problem in the case where µ is a
discrete measure:
Proposition 3. Let ˆµ = (cid:80)M
m=1 δXm be an empirical measure of M points. Let η(X) ∈ RM have
mth entry η(Xm), and ∇η(X) ∈ RM d have (m, i)th entry7 ∂iη(Xm). Then under Assumptions (A)
to (D), the Gradient-Constrained MMD is

GCMMD2

ˆµ,k,λ(P, Q) =

(cid:0)MMD2(P, Q) − ¯P (η)(cid:1)

1
λ
(cid:20) η(X)
∇η(X)

¯P (η) =

(cid:21)T (cid:18)(cid:20)K GT
G H

(cid:21)

+ M λIM +M d

(cid:19)−1 (cid:20) η(X)
∇η(X)

(cid:21)

,

where K is the kernel matrix Km,m(cid:48) = k(Xm, Xm(cid:48)), G is the matrix of left derivatives G(m,i),m(cid:48) =
∂ik(Xm, Xm(cid:48)), and H that of derivatives of both arguments H(m,i),(m(cid:48),j) = ∂i∂j+dk(Xm, Xm(cid:48)).

Before proving Proposition 3, we note the following interesting alternate form. Let ¯ei be the ith
standard basis vector for RM +M d, and deﬁne T : H → RM +M d as the linear operator

T =

¯em ⊗ k(Xm, ·) +

¯em+(m,i) ⊗ ∂ik(Xm, ·).

M
(cid:88)

m=1

M
(cid:88)

d
(cid:88)

m=1

i=1

Then

(cid:21)

(cid:20) η(X)
∇η(X)

(cid:21)

(cid:20)K GT
G H

= T η, and

= T T ∗. Thus we can write

GCMMD2

ˆµ,k,λ =

(cid:10)η, (cid:0)I − T ∗(T T ∗ + M λI)−1T (cid:1) η(cid:11)

H .

1
λ

7We use (m, i) to denote (m − 1)d + i; thus ∇η(X) stacks ∇η(X1), . . . , ∇η(XM ) into one vector.

13

Proof of Proposition 3. Let g ∈ H be the solution to the regression problem Dµ,λg = η:

g(Xm)k(Xm, ·) +

∂ig(Xm)∂ik(Xm, ·)

+ λg = η

(cid:35)

d
(cid:88)

i=1

1
M

(cid:34)

M
(cid:88)

m=1

g =

η −

1
λ

1
λM

(cid:34)

M
(cid:88)

m=1

g(Xm)k(Xm, ·) +

∂ig(Xm)∂ik(Xm, ·)

.

(12)

Taking the inner product of both sides of (12) with k(Xm(cid:48), ·) for each 1 ≤ m(cid:48) ≤ M yields the
following M equations:

g(Xm(cid:48)) =

η(Xm(cid:48)) −

g(Xm)Km,m(cid:48) +

∂ig(Xm) G(m,i),m(cid:48)

.

(13)

Doing the same with ∂jk(Xm(cid:48), ·) gives M d equations:

∂jg(Xm(cid:48)) =

∂jη(Xm(cid:48)) −

g(Xm)G(m(cid:48),j),m +

∂ig(Xm)H(m,i),(m(cid:48),j)

.

(14)

(cid:35)

(cid:35)

(cid:35)

(cid:35)

From (12), it is clear that g is a linear combination of the form:

g(x) =

η(x) −

αmk(Xm, x) +

βm,i∂ik(Xm, x)

,

1
λ

1
λM

(cid:34)

M
(cid:88)

m=1

1
λ

1
λM

(cid:34)

M
(cid:88)

m=1

1
λ

1
λM

(cid:34)

M
(cid:88)

m=1

d
(cid:88)

i=1

d
(cid:88)

i=1

d
(cid:88)

i=1

d
(cid:88)

i=1

where the coefﬁcients α := (αm = g(Xm))1≤m≤M and β := (βm,i = ∂ig(Xm))1≤m≤M
1≤i≤d
the system of equations (13) and (14). We can rewrite this system as

satisfy

(cid:20)K + M λIM
G

GT
H + M λIM d

(cid:21)

(cid:21) (cid:20)α
β

= M

(cid:21)

(cid:20) η(X)
∇η(X)

,

where IM , IM d are the identity matrices of dimension M , M d. Since K and H must be positive
semideﬁnite, an inverse exists. We conclude by noticing that
(cid:34)

(cid:35)

GCMMDˆµ,k,λ(P, Q)2 = (cid:104)η, g(cid:105)H =

(cid:107)η(cid:107)2

H −

αmη(Xm) +

βm,i∂iη(Xm)

.

1
λ

1
λM

M
(cid:88)

m=1

d
(cid:88)

i=1

The following result was key to our deﬁnition of the SMMD in Section 3.3.
Proposition 4. Under Assumptions (A) to (D), we have for all f ∈ H that

where σk,µ,λ := 1/

λ + (cid:82) k(x, x)µ(dx) + (cid:80)d

i=1

(cid:113)

(cid:107)f (cid:107)S(µ),k,λ ≤ σ−1

µ,k,λ(cid:107)f (cid:107)Hk ,
(cid:82) ∂i∂i+dk(x, x)µ(dx).

Proof of Proposition 4. The key idea here is to use the Cauchy-Schwarz inequality for the Hilbert-
Schmidt inner product. Letting f ∈ H, (cid:107)f (cid:107)2
S(µ),k,λ is
(cid:90)

(cid:90)

f (x)2 µ(dx) +

(cid:107)∇f (x)(cid:107)2 µ(dx) + λ(cid:107)f (cid:107)2
H

(cid:104)f, k(x, ·) ⊗ k(x, ·)f (cid:105)H µ(dx) +

(cid:104)f, ∂ik(x, ·) ⊗ ∂ik(x, ·)f (cid:105)H µ(dx) + λ(cid:107)f (cid:107)2
H

(cid:104)f ⊗ f, k(x, ·) ⊗ k(x, ·)(cid:105)HS µ(dx) +

(cid:104)f ⊗ f, ∂ik(x, ·) ⊗ ∂ik(x, ·)(cid:105)HS µ(dx) + λ(cid:107)f (cid:107)2
H

k(x, x) µ(dx) +

∂i∂i+dk(x, x) µ(dx) + λ

.

(cid:35)

d
(cid:88)

(cid:90)

i=1

(a) follows from the reproducing properties (8) and (9) and Property (ii). (b) is obtained using
Property (iii), while (c) follows from the Cauchy-Schwarz inequality and Property (i).

(cid:90)

(a)
=

(cid:90)

(b)
=

(cid:34)(cid:90)

(c)
≤ (cid:107)f (cid:107)2
H

d
(cid:88)

(cid:90)

i=1

d
(cid:88)

(cid:90)

i=1

14

Lemma 6. Under Assumption (D), Dx is Bochner integrable and its integral Dµ is a trace-class
symmetric positive semi-deﬁnite operator with Dµ,λ = D+λI invertible for any positive λ. Moreover,
for any Hilbert-Schmidt operator A we have: (cid:104)A, Dµ(cid:105)HS = (cid:82) (cid:104)A, Dx(cid:105)HS µ(dx).
Under Assumptions (A) and (B), k(x, ·) is Bochner integrable with respect to any probability
distribution P with ﬁnite ﬁrst moment and the following relation holds: (cid:104)f, EP [k(x, ·)](cid:105)H =
EP [(cid:104)f, k(x, ·)(cid:105)H] for all f in H.

Proof. The operator Dx is positive self-adjoint. It is also trace-class, as by the triangle inequality

(cid:107)Dx(cid:107)1 ≤ (cid:107)k(x, ·) ⊗ k(x, ·)(cid:107)1 +

(cid:107)∂ik(x, ·) ⊗ ∂ik(x, ·)(cid:107)1

d
(cid:88)

i=1

= (cid:107)k(x, ·)(cid:107)2

H +

(cid:107)∂ik(x, ·)(cid:107)2

H < ∞.

d
(cid:88)

i=1

By Assumption (D), we have that (cid:82) (cid:107)Dx(cid:107)1 µ(dx) < ∞ which implies that Dx is µ-integrable in
the Bochner sense [39, Deﬁnition 1 and Theorem 2]. Its integral Dµ is trace-class and satisﬁes
(cid:107)Dµ(cid:107)1 ≤ (cid:82) (cid:107)Dx(cid:107)1 µ(dx). This allows to have (cid:104)A, Dµ(cid:105)HS = (cid:82) (cid:104)A, Dx(cid:105)HS µ(dx) for all Hilbert-
Schmidt operators A. Moreover, the integral preserves the symmetry and positivity. It follows that
Dµ,λ is invertible.
The Bochner integrability of k(x, ·) under a distribution P with ﬁnite moment follows directly from
Assumptions (A) and (B), since (cid:82) (cid:107)k(x, ·)(cid:107) P(dx) ≤ C (cid:82) ((cid:107)x(cid:107) + 1) P(dx) < ∞. This allows us to
write (cid:104)f, EP[k(x, ·)](cid:105)H = EP[(cid:104)f, k(x, ·)(cid:105)H].

A.2 Continuity of the Optimized Scaled MMD in the Wasserstein topology

To prove Theorem 1, we we will ﬁrst need some new notation.

We assume the kernel is k = K ◦ φψ, i.e. kψ(x, y) = K(φψ(x), φψ(y)), where the representation
function φψ is a network φψ(X) : Rd → RdL consisting of L fully-connected layers:

h0
ψ(X) = X
ψ(X) = W lσl−1(hl−1
hl
φψ(X) = hL

ψ(X).

ψ (X)) + bl

for 1 ≤ l ≤ L

(15)

The intermediate representations hl
ψ(X) are of dimension dl, the weights W l are matrices in
Rdl×dl−1 , and biases bl are vectors in Rdl . The elementwise activation function σ is given by
σ0(x) = x, and for l > 0 the activation σl is a leaky ReLU with leak coefﬁcient 0 < α < 1:

σl(x) = σ(x) =

for l > 0.

(16)

(cid:26)x

x > 0
αx x ≤ 0

The parameter ψ is the concatenation of all the layer parameters:

ψ = (cid:0)(W L, bL), (W L−1, bL−1), . . . , (W 1, b1)(cid:1) .

We denote by Ψ the set of all such possible parameters, i.e. Ψ = RdL×dL−1 ×RdL ×· · ·×Rd1×d×Rd1 .
Deﬁne the following restrictions of Ψ:

Ψκ := (cid:8)ψ ∈ Ψ | ∀1 ≤ l ≤ L, cond(W l) ≤ κ(cid:9)
1 := (cid:8)ψ ∈ Ψκ | ∀1 ≤ l ≤ L, (cid:107)W l(cid:107) = 1(cid:9) .
Ψκ

(17)

(18)

Ψκ is the set of those parameters such that W l have a small condition number, cond(W ) =
σmax(W )/σmin(W ). Ψκ
1 is the set of per-layer normalized parameters with a condition number
bounded by κ.

15

Recall the deﬁnition of Scaled MMD, (7), where λ > 0 and µ is a probability measure:

SMMDµ,k,λ(P, Q) := σµ,k,λ MMDk(P, Q)

σk,µ,λ := 1/

k(x, x) µ(dx) +

∂i∂i+dk(x, x) µ(dx).

(cid:118)
(cid:117)
(cid:117)
(cid:116)λ +

(cid:90)

d
(cid:88)

(cid:90)

i=1

The Optimized SMMD over the restricted set Ψκ is given by:

Dµ,Ψκ,λ

SMMD (P, Q) := sup
ψ∈Ψκ

SMMDµ,kψ,λ .

The constraint to ψ ∈ Ψκ is critical to the proof. In practice, using a spectral parametrization helps
enforce this assumption, as shown in Figures 2 and 9. Other regularization methods, like orthogonal
normalization [11], are also possible.

We will use the following assumptions:

(I) µ is a probability distribution absolutely continuous with respect to the Lebesgue measure.
(II) The dimensions of the weights are decreasing per layer: dl+1 ≤ dl for all 0 ≤ l ≤ L − 1.
(III) The non-linearity used is Leaky-ReLU, (16), with leak coefﬁcient α ∈ (0, 1).
(IV) The top-level kernel K is globally Lipschitz in the RKHS norm: there exists a positive
constant LK > 0 such that (cid:107)K(a, .) − K(b, .)(cid:107) ≤ LK(cid:107)a − b(cid:107) for all a and b in RdL.

(V) There is some γK > 0 for which K satisﬁes

∇b∇cK(b, c)(cid:12)

(cid:12)(b,c)=(a,a) (cid:23) γ2I

for all a ∈ RdL.

(19)

Assumption (I) ensures that the points where φψ(X) is not differentiable are reached with probability
0 under µ. This assumption can be easily satisﬁed e.g. if we deﬁne µ by adding Gaussian noise to P.
Assumption (II) helps ensure that the span of W l is never contained in the null space of W l+1. Using
Leaky-ReLU as a non-linearity, Assumption (III), further ensures that the network φψ is locally
full-rank almost everywhere; this might not be true with ReLU activations, where it could be always
0. Assumptions (II) and (III) can be easily satisﬁed by design of the network.

Assumptions (IV) and (V) only depend on the top-level kernel K and are easy to satisfy in practice.
In particular, they always hold for a smooth translation-invariant kernel, such as the Gaussian, as well
as the linear kernel.

We are now ready to prove Theorem 1.
Theorem 1. Under Assumptions (I) to (V),

Dµ,Ψκ,λ

SMMD (P, Q) ≤

LK κL/2
√

dL αL/2

γ

W(P, Q),

which implies that if Pn

W−→ P, then Dµ,Ψκ,λ

SMMD (Pn, P) → 0.

Proof. Deﬁne the pseudo-distance corresponding to the kernel kψ

dψ(x, y) = (cid:107)kψ(x, ·) − kψ(y, ·)(cid:107)Hψ =

kψ(x, x) + kψ(y, y) − 2kψ(x, y).

(cid:113)

Denote by W dψ (P, Q) the optimal transport metric between P and Q using the cost dψ, given by

W dψ (P, Q) = inf

E(X,Y )∼π [dψ(X, Y )] .

π∈Π(P,Q)

where Π is the set of couplings with marginals P and Q. By Lemma 7,

MMDψ(P, Q) ≤ W dψ (P, Q).

Recall that φψ is Lipschitz, (cid:107)φψ(cid:107)Lip < ∞, so along with Assumption (IV) we have that

dψ(x, y) ≤ LK(cid:107)φψ(x) − φψ(y)(cid:107) ≤ LK(cid:107)φψ(cid:107)Lip(cid:107)x − y(cid:107).

16

Thus

so that

W dψ (P, Q) ≤ inf

E(X,Y )∼π [LK(cid:107)φψ(cid:107)Lip(cid:107)X − Y (cid:107)] = LK(cid:107)φψ(cid:107)Lip W(P, Q),

π∈Π(P,Q)

where W is the standard Wasserstein distance (2), and so

MMDψ(P, Q) ≤ Lk(cid:107)φψ(cid:107)Lip W(P, Q).

We have that ∂i∂i+dk(x, y) = [∂iφψ(x)]T (cid:104)
∇a∇bK(a, b)(cid:12)
[∂iφψ(y)] , where the
middle term is a dL × dL matrix and the outer terms are vectors of length dL. Thus Assumption (V)
implies that ∂i∂i+dk(x, x) ≥ γ2

(cid:12)(a,b)=(φψ(x),φψ(y))

K(cid:107)∂iφψ(x)(cid:107)2, and hence

(cid:105)

σ−2
µ,k,λ ≥ γ2

K

E[(cid:107)∇φψ(X)(cid:107)2
F ]

SMMD2

ψ(P, Q) = σ2

µ,k,λ MMD2

ψ(P, Q) ≤

K(cid:107)φψ(cid:107)2
L2
E [(cid:107)∇φψ(X)(cid:107)2
F ]

Lip

γ2
K

W 2(P, Q).

Using Lemma 8, we can write φψ(X) = α(ψ)φ ¯ψ(X) with ¯ψ ∈ Ψκ

1 . Then we have

(cid:107)φψ(cid:107)2
Lip
(cid:107)∇φψ(X)(cid:107)2
F

(cid:104)

Eµ

(cid:105) =

α(ψ)2(cid:107)φ ¯ψ(cid:107)2
(cid:104)(cid:13)
(cid:13)∇φ ¯ψ(X)(cid:13)
2
(cid:13)
F

Lip

α(ψ)2 Eµ

(cid:105) ≤

1
(cid:104)(cid:13)
(cid:13)∇φ ¯ψ(X)(cid:13)
2
(cid:13)
F

(cid:105) ,

Eµ

where we used (cid:107)φ ¯ψ(cid:107)Lip ≤ (cid:81)L
(cid:107)∇φ ¯ψ(X)(cid:107)2

F ≥ dL(α/κ)L. Using Assumption (I), this implies that

l=1(cid:107) ¯W l(cid:107) = 1. But by Lemma 9, for Lebesgue-almost all X,

(cid:107)φψ(cid:107)2
Lip
(cid:107)∇φψ(X)(cid:107)2
F

(cid:104)

Eµ

(cid:105) ≤

1
(cid:2)(cid:107)∇φ ¯ψ(X)(cid:107)2

F ](cid:3) ≤

Eµ

κL
dLαL .

Thus for any ψ ∈ Ψκ,

SMMDψ(P, Q) ≤

LK κL/2
√

γK

dL αL/2

W(P, Q).

The desired bound on Dµ,Ψκ,λ

SMMD follows immediately.

Lemma 7. Let (x, y) (cid:55)→ k(x, y) be the continuous kernel of an RKHS H deﬁned on a Polish space X ,
and deﬁne the corresponding pseudo-distance dk(x, y) := (cid:107)k(x, ·) − k(y, ·)(cid:107)H. Then the following
inequality holds for any distributions P and Q on X , including when the quantities are inﬁnite:

MMDk(P, Q) ≤ W dk (P, Q).

Proof. Let P and Q be two probability distributions, and let Π(P, Q) be the set of couplings between
them. Let π∗ ∈ argmin(X,Y )∼π[ck(X, Y )] be an optimal coupling, which is guaranteed to exist
[51, Theorem 4.1]; by deﬁnition W dk (P, Q) = E(X,Y )∼π∗ [dk(X, Y )]. When W dk (P, Q) = ∞ the
inequality trivially holds, so assume that W dk (P, Q) < ∞.
Take a sample (X, Y ) ∼ π(cid:63) and a function f ∈ H with (cid:107)f (cid:107)H ≤ 1. By the Cauchy-Schwarz
inequality,

(cid:107)f (X) − f (Y )(cid:107) ≤ (cid:107)f (cid:107)H(cid:107)k(X, ·) − k(Y, ·)(cid:107)H ≤ (cid:107)k(X, ·) − k(Y, ·)(cid:107)H.

Taking the expectation with respect to π(cid:63), we obtain

Eπ(cid:63) [|f (X) − f (Y )|] ≤ Eπ(cid:63) [(cid:107)k(X, ·) − k(Y, ·)(cid:107)H].
The right-hand side is just the deﬁnition of W dk (P, Q). By Jensen’s inequality, the left-hand side is
lower-bounded by

|Eπ∗ [f (X) − f (Y )]| = |EX∼P[f (X)] − EY ∼Q[f (Y )]|

since π(cid:63) has marginals P and Q. We have shown so far that for any f ∈ H with (cid:107)f (cid:107)H ≤ 1,

the result follows by taking the supremum over f .

|EP[f (X)] − EQ[f (Y )]| ≤ W ck (P, Q);

17

Lemma 8. Let ψ = ((W L, bL), (W L−1, bL−1), . . . , (W 1, b1)) ∈ Ψκ. There exists a corresponding
scalar α(ψ) and ¯ψ = (( ¯W L, ¯bL), ( ¯W L−1, ¯bL−1), . . . , ( ¯W 1, ¯b1)) ∈ Ψκ
1 , deﬁned by (18), such that
for all X,

φψ(X) = α(ψ) φ ¯ψ(X).

Proof. Set ¯W l = 1
1
m=1(cid:107)W m(cid:107)
number is unchanged, cond( ¯W l) = cond(W l) ≤ κ, and (cid:107) ¯W l(cid:107) = 1, so ¯ψ ∈ Φκ
see from (16) that

l=1(cid:107)W l(cid:107). Note that the condition
1 . It is also easy to

(cid:107)W l(cid:107) W l, ¯bl =

bl, and α(ψ) = (cid:81)L

(cid:81)l

so that

hl
¯ψ(X) =

1
m=1(cid:107)W m(cid:107)

(cid:81)l

hl
ψ(X)

α(ψ)hL

¯ψ(X) =

hL
ψ(X) = φψ(X).

(cid:81)L

(cid:81)L

l=1(cid:107)W l(cid:107)
l=1(cid:107)W l(cid:107)

Lemma 9. Make Assumptions (II) and (III), and let ψ ∈ Ψκ
intermediate activation is exactly zero,

1 . Then the set of inputs for which any

has zero Lebesgue measure. Moreover, for any X /∈ N ψ, ∇X φψ(X) exists and

Nψ =

L
(cid:91)

dl(cid:91)

l=1

k=1

(cid:110)
X ∈ Rd | (cid:0)hl

ψ(X)(cid:1)

k

(cid:111)

= 0

,

(cid:107)∇X φψ(X)(cid:107)2

F ≥

dLαL
κL .

(M l

X )k = σ(cid:48)

l(hl

k(X)) =

(cid:26)1 hl
α hl

k(X) > 0
k(X) < 0

;

Proof. First, note that the network representation at layer l is piecewise afﬁne. Speciﬁcally, deﬁne
M l

X ∈ Rdl by, using Assumption (III),

it is undeﬁned when any hl

k(X) = 0, i.e. when X ∈ N ψ. Let V l

ψ(X) = W lσl−1(hl−1
hl

ψ (X)) + bl = V l

X := W l diag (cid:0)M l−1
X X + bl,

X

(cid:1). Then

ψ(X) = Wl
hl
X = V l
X = V l
1 , we have (cid:107)W l(cid:107) = 1 and σmin(W l) ≥ 1/κ; also, (cid:107)M l
X (cid:107) ≤ 1, and using Assumption (II) with Lemma 10 gives σmin(Wl

X X + bl
X V l−1

X bl−1 + bl, and Wl

X · · · V 1

X ,

X , so long as X /∈ N ψ.

X (cid:107) ≤ 1, σmin(M l

X ) ≥ α.
X ) ≥ (α/κ)l. In

(20)

and thus

where b0

X = 0, bl
Because ψ ∈ Ψκ
Thus (cid:107)Wl
particular, each Wl

Next, note that bl
H l

X = (M l

X , M l−1

X is full-rank.
X and Wl
X , . . . , M 1

X each only depend on X through the activation patterns M l
X ) denote the full activation patterns up to level l, we can thus write

X . Letting

ψ(X) = WH l
hl

X X + bH l

X .

There are only ﬁnitely many possible values for H l
we have that

X ; we denote the set of such values as Hl. Then

L
(cid:91)

dL(cid:91)

(cid:91)

Nψ ⊆

l=0

k=1

H l∈Hl

(cid:110)
X ∈ Rd | WH l

k X + bH l

k = 0

(cid:111)

.

Because each WH l
is of rank dl, each set in the union is either empty or an afﬁne subspace of
k
dimension d − dl. As each dl > 0, each set in the ﬁnite union has zero Lebesgue measure, and Nψ
also has zero Lebesgue measure.

18

We will now show that the activation patterns are piecewise constant, so that ∇X hl
1 , we have (cid:107)hl
all X /∈ N ψ. Because ψ ∈ Ψκ
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12) ≤ (cid:107)X − X (cid:48)(cid:107).
(cid:12)

ψ(cid:107)Lip ≤ 1, and in particular

ψ(X (cid:48))(cid:1)

ψ(X)(cid:1)

− (cid:0)hl

(cid:0)hl

k

k

ψ(X) = WH l

X for

Thus,
minl=1,...,L mink=1,...,dl
for all l and k,

take some X /∈ N ψ, and ﬁnd the smallest absolute value of its activations, (cid:15) =
(cid:12)
(cid:12)
(cid:12); clearly (cid:15) > 0. For any X (cid:48) with (cid:107)X − X (cid:48)(cid:107) < (cid:15), we know that

(cid:17)
hl
ψ(X)

(cid:12)
(cid:12)
(cid:12)

(cid:16)

k

sign

(cid:16)(cid:0)hl

ψ(X)(cid:1)

(cid:17)

k

= sign

(cid:16)(cid:0)hl

ψ(X (cid:48))(cid:1)

(cid:17)

,

k

implying that H l
Finally, we obtain

X = H l

X (cid:48) as well as X (cid:48) /∈ N ψ. Thus for any point X /∈ N ψ, ∇φψ(X) = WH L
X .

(cid:107)∇φψ(X)(cid:107)2

F = (cid:107)WH L

X (cid:107)2

F ≥ dL σmin

(cid:16)

WH L

X

(cid:17)2

≥

dLαL
κL .

Lemma 10. Let A ∈ Rm×n, B ∈ Rn×p, with m ≥ n ≥ p. Then σmin(AB) ≥ σmin(A) σmin(B).

Proof. A more general version of this result can be found in [19, Theorem 2]; we provide a proof
here for completeness.
If B has a nontrivial null space, σmin(B) = 0 and the inequality holds. Otherwise, let Rn
Rn \ {0}. Recall that for C ∈ Rm×n with m ≥ n,
(cid:115)

∗ denote

(cid:113)

σmin(C) =

λmin(C TC) =

xTC TCx
xTx

inf
x∈Rn
∗

= inf
x∈Rn
∗

(cid:107)Cx(cid:107)
(cid:107)x(cid:107)

.

Thus, as Bx (cid:54)= 0 for x (cid:54)= 0,

σmin(AB) = inf
x∈Rp
∗
(cid:18)

(cid:107)ABx(cid:107)
(cid:107)x(cid:107)
(cid:107)ABx(cid:107)
(cid:107)Bx(cid:107)
(cid:107)Ay(cid:107)
(cid:107)y(cid:107)

= inf
x∈Rp
∗

(cid:19) (cid:18)

(cid:107)ABx(cid:107)(cid:107)Bx(cid:107)
(cid:107)Bx(cid:107)(cid:107)x(cid:107)
(cid:19)
(cid:107)Bx(cid:107)
inf
(cid:107)x(cid:107)
x∈Rp
∗
(cid:19)
(cid:107)Bx(cid:107)
(cid:107)x(cid:107)

inf
x∈Rp
∗

(cid:19) (cid:18)

≥

≥

(cid:18)

inf
x∈Rp
∗

inf
y∈Rn
∗

= σmin(A) σmin(B).

A.2.1 When some of the assumptions don’t hold

Here we analyze through simple examples what happens when the condition number can be un-
bounded, and when Assumption (II), about decreasing widths of the network, is violated.

Condition Number: We start by a ﬁrst example where the condition number can be arbitrarily
high. We consider a two-layer network on R2, deﬁned by

φα(X) = [1 −1] σ(WαX)

Wα =

(21)

(cid:20)1
1

(cid:21)

1
1 + α

where α > 0. As α approaches 0 the matrix Wα becomes singular which means that its condition
number blows up. We are interested in analyzing the behavior of the Lipschitz constant of φ and the
expected squared norm of its gradient under µ as α approaches 0.

One can easily compute the squared norm of the gradient of φ which is given by

(cid:107)∇φα(X)(cid:107)2 =






α2
X ∈ A1
γ2α2
X ∈ A2
(1 − γ)2 + (1 + α − γ)2 X ∈ A3
(1 − γ)2 + (γα + γ − 1)2 X ∈ A4

(22)

19

Here A1, A2, A3 and A4 are deﬁned by (23) and are represented in Figure 4:
A1 := {X ∈ R2|X1 + X2 ≥ 0 X1 + (1 + α)X2 ≥ 0}
A2 := {X ∈ R2|X1 + X2 < 0 X1 + (1 + α)X2 < 0}
A3 := {X ∈ R2|X1 + X2 < 0 X1 + (1 + α)X2 ≥ 0}
A4 := {X ∈ R2|X1 + X2 ≥ 0 X1 + (1 + α)X2 < 0}

(23)

(24)

(25)

(26)

Figure 4: Decomposition of R2 into 4 regions A1, A2, A3 and A4 as deﬁned in (23). As α approaches
0, the area of sets A3 and A4 becomes negligible.

It is easy to see that whenever µ has a density, the probability of the sets A3 and A4 goes to 0 are
α → 0. Hence one can deduce that Eµ[(cid:107)∇φα(X)(cid:107)2] → 0 when α → 0. On the other hand, the
squared Lipschitz constant of φ is given by (1 − γ)2 + (1 + α − γ)2 which converges to 2(1 − γ)2.
This shows that controlling the expectation of the gradient doesn’t allow to effectively control the
Lipschitz constant of φ.

Monotonicity of the dimensions: We would like to consider a second example where Assump-
tion (II) doesn’t hold. Consider the following two layer network deﬁned by:
(cid:35)

φ(X) = [−1

0

1] σ(WβX)

Wβ :=

(cid:34)1
0
1
0
1 β

for β > 0. Note that Wβ is a full rank matrix, but Assumption (II) doesn’t hold. Depending on the
sign of the components of WβX one has the following expression for (cid:107)∇φα(X)(cid:107)2:

where (Bi)1≤i≤6 are deﬁned by (26)

(cid:107)∇φα(X)(cid:107)2 =






β2
X ∈ B1
γ2β2
X ∈ B2
β2
X ∈ B3
(1 − γ)2 + γ2β2 X ∈ B4
(1 − γ)2 + β2
X ∈ B5
γ2β2
X ∈ B6

B1 := {X ∈ R2|X1 ≥ 0 X2 ≥ 0}
B2 := {X ∈ R2|X1 < 0 X2 < 0}
B3 := {X ∈ R2|X1 ≥ X2 < 0 X1 + βX2 ≥ 0}
B4 := {X ∈ R2|X1 ≥ X2 < 0 X1 + βX2 < 0}
B5 := {X ∈ R2|X1 > 0 X2 ≥ 0 X1 + βX2 ≥ 0}
B6 := {X ∈ R2|X1 > 0 X2 ≥ 0 X1 + βX2 < 0}

20

The squared Lipschitz constant is given by (cid:107)φ(cid:107)2
the gradient of φ is given by:

L(1 − γ)2 + β2 while the expected squared norm of

Eµ[(cid:107)φ(X)(cid:107)2] = 3β2(p(B1 ∪ B3 ∪ B5) + γ2p(B2 ∪ B4 ∪ B6)) + (1 − γ)2p(B4 ∪ B5).

(27)
Again the set B4 ∪ B5 becomes negligible as β approaches 0 which implies that Eµ[(cid:107)φ(X)(cid:107)2] → 0.
L converges to (1 − γ)2. Note that unlike in the ﬁrst example in (21), the
On the other hand (cid:107)φ(cid:107)2
matrix Wβ has a bounded condition number. In this example, the columns of W0 are all in the null
1], which implies ∇φ0(X) = 0 for all X ∈ R2, even though all matrices have full
space of [−1 0
rank.

B DiracGAN vector ﬁelds for more losses

Figure 5: Vector ﬁelds for different losses with respect to the generator parameter θ and the feature
representation parameter ψ; the losses use a Gaussian kernel, and are shown in (28). Following [30],
P = δ0, Q = δθ and φψ(x) = ψx. The curves show the result of taking simultaneous gradient steps
in (θ, ψ) beginning from three initial parameter values.

Figure 5 shows parameter vector ﬁelds, like those in Figure 6, for Example 1 for a variety of different
losses:

MMD: − MMD2
ψ
MMD-GP: − MMD2
ψ +λ EP[((cid:107)∇f (X)(cid:107) − 1)2]
MMD-GP-Unif: − MMD2
ψ +λ E

(cid:101)X(cid:39)µ∗ [((cid:107)∇f ( (cid:101)X)(cid:107) − 1)2]

SN-MMD: − 2 MMD1(P, Q)2

Sobolev-MMD: − MMD2
CenteredSobolev-MMD: − MMD2

ψ +λ(E(P+Q)/2[(cid:107)∇f (X)(cid:107)2] − 1)2
ψ +λ(E(P+Q)/2[(cid:107)∇f (X)(cid:107)2])2

(28)

LipMMD: − LipMMD2
GC-MMD: − GCMMD2
SMMD: − SMMD2

kψ,λ

kψ,P,λ

N (0,102),kψ,λ

21

2

The squared MMD between δ0 and δθ under a Gaussian kernel of bandwidth 1/ψ and is given by
2(1 − e− ψ2θ2
). MMD-GP-unif uses a gradient penalty as in [7] where each samples from µ∗ is
obtained by ﬁrst sampling X and Y from P and Q and then sampling uniformly between X and
Y . MMD-GP uses the same gradient penalty, but the expectation is taken under P rather than µ∗.
SN-MMD refers to MMD with spectral normalization; here this means that ψ = 1. Sobolev-MMD
refers to the loss used in [33] with the quadratic penalty only. GCMMDµ,k,λ is deﬁned by (5), with
µ = N (0, 102).

C Vector ﬁelds of Gradient-Constrained MMD and Sobolev GAN critics

Mroueh et al. [33] argue that the gradient of the critic (...) deﬁnes a transportation plan for moving
the distribution mass (from generated to reference distribution) and present the solution of Sobolev
PDE for 2-dimensional Gaussians. We observed that in this simple example the gradient of the
Sobolev critic can be very high outside of the areas of high density, which is not the case with the
Gradient-Constrained MMD. Figure 6 presents critic gradients in both cases, using µ = (P + Q)/2
for both.

(a) Gradient-Constrained MMD critic gradient.

(b) Sobolev IPM critic gradient.

Figure 6: Vector ﬁelds of critic gradients between two Gaussians. The grey arrows show normalized
gradients, i.e. gradient directions, while the black ones are the actual gradients. Note that for the
Sobolev critic, gradients norms are orders of magnitudes higher on the right hand side of the plot
than in the areas of high density of the given distributions.

This unintuitive behavior is most likely related to the vanishing boundary condition, assummed by
Sobolev GAN. Solving the actual Sobolev PDE, we found that the Sobolev critic has very high
gradients close to the boundary in order to match the condition; moreover, these gradients point in
opposite directions to the target distribution.

D An estimator for Lipschitz MMD

We now describe brieﬂy how to estimate the Lipschitz MMD in low dimensions. Recall that

LipMMDk,λ(P, Q) =

f ∈Hk : (cid:107)f (cid:107)2

sup
Lip+λ(cid:107)f (cid:107)2

≤1

Hk

EX∼P[f (X)] − EX∼Q[f (Y )].

For f ∈ Hk, it is the case that

(cid:107)f (cid:107)2

Lip = sup
x∈Rd

(cid:107)∇f (x)(cid:107)2 = sup
x∈Rd

(cid:104)∂ik(x, ·), f (cid:105)2

Hk

= sup
x∈Rd

f,

[∂ik(x, ·) ⊗ ∂ik(x, ·)] f

.

d
(cid:88)

i=1

(cid:42)

d
(cid:88)

i=1

(cid:43)

Hk

Thus we can approximate the constraint (cid:107)f (cid:107)2
Lip + λ(cid:107)f (cid:107)2
≤ 1 by enforcing the constraint on a set
Hk
of m points {Zi} reasonably densely covering the region around the supports of P and Q, rather

22

than enforcing it at every point in X . An estimator of the Lipschitz MMD based on X ∼ PnX and
Y ∼ QnY is

(cid:92)LipMMDk,λ (X, Y, Z) ≈ sup

nX(cid:88)

1
nX

f (Xj) −

nY(cid:88)

f (Yj)

1
nY

f ∈Hk

j=1

j=1
s.t. ∀j, (cid:107)∇f (Zj)(cid:107)2 + λ(cid:107)f (cid:107)2

≤ 1.

Hk

(29)

By the generalized representer theorem, the optimal f for (29) will be of the form

f (·) =

αjk(Xj, ·) +

βjk(Yj, ·) +

γ(i,j)∂ik(Zj, ·).

nX(cid:88)

j=1

nY(cid:88)

j=1

d
(cid:88)

m
(cid:88)

i=1

j=1

Writing δ = (α, β, γ), the objective function is linear in δ,
· · · − 1
nY

− 1
nY

(cid:2) 1
nX

1
nX

· · ·

0

· · ·

0(cid:3) δ.

The constraints are quadratic, built from the following matrices, where the X and Y samples are
concatenated together, as are the derivatives with each dimension of the Z samples:
















K :=

B :=

H :=

k(X1, X1)
...
k(YnX , X1)
∂1k(Z1, X1)
...
∂dk(Zm, X1)

∂1∂1+dk(Z1, Z1)
...
∂d∂1+dk(Zm, Z1)

· · ·
. . .
· · ·

k(X1, YnY )
...
k(YnY , YnY )






· · ·
. . .
· · ·






∂1k(Z1, YnY )
...
∂dk(Zm, YnY )
· · ·
. . .
· · ·

∂1∂d+dk(Z1, Zm)
...
∂d∂d+dk(Zm, Zm)




 .

Given these matrices, and letting Oj = (cid:80)d
vector in Rmd, we have that
(cid:21)
(cid:20)K BT
B H

= δT

(cid:107)f (cid:107)2

Hk

δ

(cid:107)∇f (Zj)(cid:107)2 =

i=1 e(i,j)eT

(i,j) where e(i,j) is the (i, j)th standard basis

d
(cid:88)

i=1

(∂if (Zj))2 = δT

(cid:20)BTOjB BTOjH
HOjB HOjH

(cid:21)

δ.

Thus the optimization problem (29) is a linear problem with convex quadratic constraints, which can
be solved by standard convex optimization software. The approximation is reasonable only if we can
effectively cover the region of interest with densely spaced {Zi}; it requires a nontrivial amount of
computation even for the very simple 1-dimensional toy problem of Example 1.

One advantage of this estimator, though, is that ﬁnding its derivative with respect to the input points
or the kernel parameterization is almost free once we have computed the estimate, as long as our
solver has computed the dual variables µ corresponding to the constraints in (29). We just need to
exploit the envelope theorem and then differentiate the KKT conditions, as done for instance in [1].
The differential of (29) ends up being, assuming the optimum of (29) is at ˆδ ∈ RnX +nY +md and
ˆµ ∈ Rm,

d (cid:92)LipMMDk,λ(X, Y, Z) = ˆδT

· · ·

1
nX

− 1
nY

· · · − 1
nY

(cid:3)T

−

ˆµj

ˆδT(dPj)ˆδ

(cid:21)

(cid:20)dK
dB

(cid:2) 1
nX

m
(cid:88)

j=1

dPj :=

(cid:20)(dB)TOjB + BTOj(dH)
(dH)OjB + HOj(dB)

(dB)TOjH + BTOj(dH)
(dH)OjH + HOj(dH)

(cid:21)

+ λ

(cid:20)dK dBT
dB dH

(cid:21)

.

E Near-equivalence of WGAN and linear-kernel MMD GANs

For an MMD GAN-GP with kernel k(x, y) = φ(x)φ(y), we have that
MMDk(P, Q) = |EP φ(x) − EQ φ(Y )|

23

and the corresponding critic function is

η(t)
(cid:107)η(cid:107)H

=

EX∼P φ(X)φ(t) − EY ∼Q φ(Y )φ(t)
|EP φ(X) − EQ φ(Y )|

= sign (EX∼P φ(X) − EY ∼Q φ(Y )) φ(t).

Thus if we assume EX∼P φ(X) > EY ∼Q φ(Y ), as that is the goal of our critic training, we see that
the MMD becomes identical to the WGAN loss, and the gradient penalty is applied to the same
function.
(MMD GANs, however, would typically train on the unbiased estimator of MMD2, giving a very
slightly different loss function. [7] also applied the gradient penalty to η rather than the true critic
η/(cid:107)η(cid:107).)

The SMMD with a linear kernel is thus analogous to applying the scaling operator to a WGAN; hence
the name SWGAN.

F Additional experiments

F.1 Comparison of Gradient-Constrained MMD to Scaled MMD

Figure 7 shows the behavior of the MMD, the Gradient-Constrained SMMD, and the Scaled MMD
when comparing Gaussian distributions. We can see that MMD ∝ SMMD and the Gradient-
Constrained MMD behave similarly in this case, and that optimizing the SMMD and the Gradient-
Constrained MMD is also similar. Optimizing the MMD would yield an essentially constant distance.

F.2 IGMs with Optimized Gradient-Constrained MMD loss

We implemented the estimator of Proposition 3 using the empirical mean estimator of η, and sharing
samples for µ = P. To handle the large but approximately low-rank matrix system, we used an
incomplete Cholesky decomposition [43, Algorithm 5.12] to obtain R ∈ R(cid:96)×M (1+d) such that
(cid:20)K GT
G H

≈ RTR. Then the Woodbury matrix identity allows an efﬁcient evaluation:

(cid:21)

(cid:0)RTR + M λI(cid:1)−1

=

(cid:0)I − R(RRT + M λI)−1R(cid:1) .

1
M λ

Even though only a small (cid:96) is required for a good approximation, and the full matrices K, G, and
H need never be constructed, backpropagation through this procedure is slow and not especially
GPU-friendly; training on CPU was faster. Thus we were only able to run the estimator on MNIST,
and even that took days to conduct the optimization on powerful workstations.

The learned models, however, were reasonable. Using a DCGAN architecture, batches of size 64,
and a procedure that otherwise agreed with the setup of Section 4, samples with and without spectral
normalization are shown in Figures 8a and 8b. After the points in training shown, however, the same
rank collapse as discussed in Section 4 occurred. Here it seems that spectral normalization may have
delayed the collapse, but not prevented it. Figure 8c shows generator loss estimates through training,
including the obvious peak at collapse; Figure 8d shows KID scores based on the MNIST-trained
convnet representation [7], including comparable SMMD models for context. The fact that SMMD
models converged somewhat faster than Gradient-Constrained MMD models here may be more
related to properties of the estimator of Proposition 3 rather than the distances; more work would be
needed to fully compare the behavior of the two distances.

F.3 Spectral normalization and Scaled MMD

Figure 9 shows the distribution of critic weight singular values, like Figure 2, at more layers. Figure 11
and Table 2 show results for the spectral normalization variants considered in the experiments.
MMDGAN, with neither spectral normalization nor a gradient penalty, did surprisingly well in this
case, though it fails badly in other situations.

Figure 9 compares the decay of singular values for layer of the critic’s network at both early and
later stages of training in two cases: with or without the spectral parametrization. The model was
trained on CelebA using SMMD. Figure 11 shows the evolution per iteration of Inception score,

24

Figure 7: Plots of various distances between one dimensional Gaussians, where P = N (0, 0.12), and
the colors show log D(P, N (µ, σ2)). All distances use λ = 1. Top left: MMD with a Gaussian kernel
of bandwidth ψ = 0.1. Top right: MMD with bandwidth ψ = 10. Middle left: Gradient-Constrained
MMD with bandwidth ψ = 0.1. Middle right: Gradient-Constrained MMD with bandwidth ψ = 10.
Bottom left: Optimized SSMD, allowing any ψ ∈ R. Bottom right: Optimized Gradient-Constrained
MMD.

25

(a) Without spectral
normalization; 32 000
generator iterations.

(b) With
spectral
normalization; 41 000
generator iterations.

Figure 8: The MNIST models with Optimized Gradient-Constrained MMD loss.

(c) Generator losses.

(d) KID scores.

FID and KID for Sobolev-GAN, MMDGAN and variants of MMDGAN and WGAN using spectral
normalization. It is often the case that this parametrization alone is not enough to achieve good
results.

Figure 9: Singular values at different layers, for the same setup as Figure 2.

F.4 Additional samples

Figures 12 and 13 give extra samples from the models.

26

µ,k,λ MMD2

k for SMMDGAN and SN-SMMDGAN, and MMD2

Figure 10: Evolution of various quantities per generator iteration on CelebA during training. 4
models are considered: (SMMDGAN, SN-SMMDGAN, MMDGAN, SN-MMDGAN). (a) Loss:
SMMD2 = σ2
k for MMDGAN and
SN-MMDGAN. The loss saturates for MMDGAN (green); spectral normalization allows some
improvement in loss, but training is still unstable (orange). SMMDGAN and SN-SMMDGAN both
lead to stable, fast training (blue and red). (b) SMMD controls the critic complexity well, as expected
(blue and red); SN has little effect on the complexity (orange). (c) Ratio of the highest singular value
to the smallest for the ﬁrst layer of the critic network: σmax/σmin. SMMD tends to increase the
condition number of the weights during training (blue), while SN helps controlling it (red). (d) KID
score during training: Only variants using SMMD lead to stable training in this case.

Figure 11: Evolution per iteration of different scores for variants of methods, mostly using spectral
normalization, on CIFAR-10.

27

Table 2: Mean (standard deviation) of score evaluations on CIFAR-10 for different methods using
Spectral Normalization.

Method

MMDGAN
SN-WGAN
SN-WGAN-GP
SN-Sobolev-GAN
SN-MMDGAN-GP
SN-MMDGAN-L2
SN-MMDGAN
SN-MMDGAN-GP-L2
SN-SMMDGAN

IS

FID

KID×103

5.5±0.0
2.2±0.0
2.5±0.0
2.9±0.0
4.6±0.1
7.1±0.1
6.9±0.1
6.9±0.2
7.3±0.1

73.9±0.1
208.5±0.2
154.3±0.2
140.2±0.2
96.8±0.4
31.9±0.2
31.5±0.2
32.3±0.3
25.0±0.3

39.4±1.5
178.9±1.5
125.3±0.9
130.0±1.9
59.5±1.4
21.7±0.9
21.7±1.0
20.9±1.1
16.6±2.0

Figure 12: Samples from a generator trained on ImageNet dataset using Scaled MMD with Spectral
Normalization: SN-SMMDGAN.

28

(a) SNGAN

(b) SobolevGAN

(c) MMDGAN-GP-L2

(d) SN-SMMD GAN

(e) SN SWGAN

(f) SMMD GAN

Figure 13: Comparison of samples from different models trained on CelebA with 160×160 resolution.

29

8
1
0
2
 
v
o
N
 
9
2
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
5
6
5
1
1
.
5
0
8
1
:
v
i
X
r
a

On gradient regularizers for MMD GANs

Michael Arbel∗
Gatsby Computational Neuroscience Unit
University College London
michael.n.arbel@gmail.com

Dougal J. Sutherland∗
Gatsby Computational Neuroscience Unit
University College London
dougal@gmail.com

Mikołaj Bi ´nkowski
Department of Mathematics
Imperial College London
mikbinkowski@gmail.com

Arthur Gretton
Gatsby Computational Neuroscience Unit
University College London
arthur.gretton@gmail.com

Abstract

We propose a principled method for gradient-based regularization of the critic of
GAN-like models trained by adversarially optimizing the kernel of a Maximum
Mean Discrepancy (MMD). We show that controlling the gradient of the critic
is vital to having a sensible loss function, and devise a method to enforce exact,
analytical gradient constraints at no additional cost compared to existing approxi-
mate techniques based on additive regularizers. The new loss function is provably
continuous, and experiments show that it stabilizes and accelerates training, giving
image generation models that outperform state-of-the art methods on 160 × 160
CelebA and 64 × 64 unconditional ImageNet.

1

Introduction

There has been an explosion of interest in implicit generative models (IGMs) over the last few years,
especially after the introduction of generative adversarial networks (GANs) [16]. These models
allow approximate samples from a complex high-dimensional target distribution P, using a model
distribution Qθ, where estimation of likelihoods, exact inference, and so on are not tractable. GAN-
type IGMs have yielded very impressive empirical results, particularly for image generation, far
beyond the quality of samples seen from most earlier generative models [e.g. 18, 22, 23, 24, 38].

These excellent results, however, have depended on adding a variety of methods of regularization and
other tricks to stabilize the notoriously difﬁcult optimization problem of GANs [38, 42]. Some of
this difﬁculty is perhaps because when a GAN is viewed as minimizing a discrepancy DGAN(P, Qθ),
its gradient ∇θ DGAN(P, Qθ) does not provide useful signal to the generator if the target and model
distributions are not absolutely continuous, as is nearly always the case [2].

An alternative set of losses are the integral probability metrics (IPMs) [36], which can give credit to
models Qθ “near” to the target distribution P [3, 8, Section 4 of 15]. IPMs are deﬁned in terms of a
critic function: a “well behaved” function with large amplitude where P and Qθ differ most. The IPM
is the difference in the expected critic under P and Qθ, and is zero when the distributions agree. The
Wasserstein IPMs, whose critics are made smooth via a Lipschitz constraint, have been particularly
successful in IGMs [3, 14, 18]. But the Lipschitz constraint must hold uniformly, which can be hard
to enforce. A popular approximation has been to apply a gradient constraint only in expectation [18]:
the critic’s gradient norm is constrained to be small on points chosen uniformly between P and Q.

Another class of IPMs used as IGM losses are the Maximum Mean Discrepancies (MMDs) [17],
as in [13, 28]. Here the critic function is a member of a reproducing kernel Hilbert space (except
in [50], who learn a deep approximation to an RKHS critic). Better performance can be obtained,

∗These authors contributed equally.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.

however, when the MMD kernel is not based directly on image pixels, but on learned features of
images. Wasserstein-inspired gradient regularization approaches can be used on the MMD critic
when learning these features: [27] uses weight clipping [3], and [5, 7] use a gradient penalty [18].

The recent Sobolev GAN [33] uses a similar constraint on the expected gradient norm, but phrases it
as estimating a Sobolev IPM rather than loosely approximating Wasserstein. This expectation can be
taken over the same distribution as [18], but other measures are also proposed, such as (P + Qθ) /2.
A second recent approach, the spectrally normalized GAN [32], controls the Lipschitz constant of
the critic by enforcing the spectral norms of the weight matrices to be 1. Gradient penalties also
beneﬁt GANs based on f -divergences [37]: for instance, the spectral normalization technique of [32]
can be applied to the critic network of an f -GAN. Alternatively, a gradient penalty can be deﬁned
to approximate the effect of blurring P and Qθ with noise [40], which addresses the problem of
non-overlapping support [2]. This approach has recently been shown to yield locally convergent
optimization in some cases with non-continuous distributions, where the original GAN does not [30].

In this paper, we introduce a novel regularization for the MMD GAN critic of [5, 7, 27], which
directly targets generator performance, rather than adopting regularization methods intended to
approximate Wasserstein distances [3, 18]. The new MMD regularizer derives from an approach
widely used in semi-supervised learning [10, Section 2], where the aim is to deﬁne a classiﬁcation
function f which is positive on P (the positive class) and negative on Qθ (negative class), in the
absence of labels on many of the samples. The decision boundary between the classes is assumed
to be in a region of low density for both P and Qθ: f should therefore be ﬂat where P and Qθ have
support (areas with constant label), and have a larger slope in regions of low density. Bousquet et al.
[10] propose as their regularizer on f a sum of the variance and a density-weighted gradient norm.
We adopt a related penalty on the MMD critic, with the difference that we only apply the penalty on P:
thus, the critic is ﬂatter where P has high mass, but does not vanish on the generator samples from Qθ
(which we optimize). In excluding Qθ from the critic function constraint, we also avoid the concern
raised by [32] that a critic depending on Qθ will change with the current minibatch – potentially
leading to less stable learning. The resulting discrepancy is no longer an integral probability metric:
it is asymmetric, and the critic function class depends on the target P being approximated.

We ﬁrst discuss in Section 2 how MMD-based losses can be used to learn implicit generative models,
and how a naive approach could fail. This motivates our new discrepancies, introduced in Section 3.
Section 4 demonstrates that these losses outperform state-of-the-art models for image generation.

2 Learning implicit generative models with MMD-based losses

An IGM is a model Qθ which aims to approximate a target distribution P over a space X ⊆ Rd.
We will deﬁne Qθ by a generator function Gθ : Z → X , implemented as a deep network with
parameters θ, where Z is a space of latent codes, say R128. We assume a ﬁxed distribution on Z,
say Z ∼ Uniform (cid:0)[−1, 1]128(cid:1), and call Qθ the distribution of Gθ(Z). We will consider learning by
minimizing a discrepancy D between distributions, with D(P, Qθ) ≥ 0 and D(P, P) = 0, which we
call our loss. We aim to minimize D(P, Qθ) with stochastic gradient descent on an estimator of D.
In the present work, we will build losses D based on the Maximum Mean Discrepancy,

MMDk(P, Q) =

EX∼P[f (X)] − EY ∼Q[f (Y )],

(1)

sup
f : (cid:107)f (cid:107)Hk ≤1

an integral probability metric where the critic class is the unit ball within Hk, the reproducing
kernel Hilbert space with a kernel k. The optimization in (1) admits a simple closed-form optimal
critic, f ∗(t) ∝ EX∼P[k(X, t)] − EY ∼Q[k(Y, t)]. There is also an unbiased, closed-form estimator of
MMD2
k with appealing statistical properties [17] – in particular, its sample complexity is independent
of the dimension of X , compared to the exponential dependence [52] of the Wasserstein distance

W(P, Q) =

sup
f : (cid:107)f (cid:107)Lip≤1

EX∼P[f (X)] − EY ∼Q[f (Y )].

(2)

The MMD is continuous in the weak topology for any bounded kernel with Lipschitz embeddings [46,
D−→ P, then MMD(Pn, P) → 0.
Theorem 3.2(b)], meaning that if Pn converges in distribution to P, Pn
W−→ P implies
(W is continuous in the slightly stronger Wasserstein topology [51, Deﬁnition 6.9]; Pn

2

D−→ P, and the two notions coincide if X is bounded.) Continuity means the loss can provide
Pn
better signal to the generator as Qθ approaches P, as opposed to e.g. Jensen-Shannon where the loss
could be constant until suddenly jumping to 0 [e.g. 3, Example 1]. The MMD is also strict, meaning
it is zero iff P = Qθ, for characteristic kernels [45]. The Gaussian kernel yields an MMD both
continuous in the weak topology and strict. Thus in principle, one need not conduct any alternating
optimization in an IGM at all, but merely choose generator parameters θ to minimize MMDk.

Despite these appealing properties, using simple pixel-level kernels leads to poor generator samples
[8, 13, 28, 48]. More recent MMD GANs [5, 7, 27] achieve better results by using a parameterized
family of kernels, {kψ}ψ∈Ψ, in the Optimized MMD loss previously studied by [44, 46]:

DΨ

MMD(P, Q) := sup
ψ∈Ψ

MMDkψ (P, Q).

(3)

We primarily consider kernels deﬁned by some ﬁxed kernel K on top of a learned low-dimensional
representation φψ : X → Rs, i.e. kψ(x, y) = K(φψ(x), φψ(y)), denoted kψ = K ◦ φψ. In practice,
K is a simple characteristic kernel, e.g. Gaussian, and φψ is usually a deep network with output
dimension say s = 16 [7] or even s = 1 (in our experiments). If φψ is powerful enough, this choice
is sufﬁcient; we need not try to ensure each kψ is characteristic, as did [27].
Proposition 1. Suppose k = K ◦ φψ, with K characteristic and {φψ} rich enough that for any
P (cid:54)= Q, there is a ψ ∈ Ψ for which φψ#P (cid:54)= φψ#Q.2 Then if P (cid:54)= Q, DΨ
Proof. Let ˆψ ∈ Ψ be such that φ ˆψ(P) (cid:54)= φ ˆψ(Q). Then, since K is characteristic,

MMD(P, Q) > 0.

DΨ

MMD(P, Q) = sup
ψ∈Ψ

MMDK(φψ#P, φψ#Q) ≥ MMDK(φ ˆψ#P, φ ˆψ#Q) > 0.

MMD, one can conduct alternating optimization to estimate a ˆψ and then update the
To estimate DΨ
generator according to MMDk ˆψ
, similar to the scheme used in GANs and WGANs. (This form of
estimator is justiﬁed by an envelope theorem [31], although it is invariably biased [7].) Unlike DGAN
or W, ﬁxing a ˆψ and optimizing the generator still yields a sensible distance MMDk ˆψ
Early attempts at minimizing DΨ
could be because for some kernel classes, DΨ
Example 1 (DiracGAN [30]). We wish to model a point mass at the origin of R, P = δ0, with any
possible point mass, Qθ = δθ for θ ∈ R. We use a Gaussian kernel of any bandwidth, which can be
2 (a − b)2(cid:1). Then
written as kψ = K ◦ φψ with φψ(x) = ψx for ψ ∈ Ψ = R and K(a, b) = exp (cid:0)− 1
θ (cid:54)= 0
2
θ = 0

MMD in an IGM, though, were unsuccessful [48, footnote 7]. This
MMD is stronger than Wasserstein or MMD.

(δ0, δθ) = 2 (cid:0)1 − exp (cid:0)− 1

MMD(δ0, δθ) =

2 ψ2θ2(cid:1)(cid:1) ,

MMD2
kψ

(cid:26)√
0

DΨ

.

.

Considering DΨ
distance is not continuous in the weak or Wasserstein topologies.

2 (cid:54)→ 0, even though δ1/n

MMD(δ0, δ1/n) =

W−→ δ0, shows that the Optimized MMD

√

This also causes optimization issues. Figure 1 (a) shows gradient vector ﬁelds in parameter space,
(δ0, δθ)(cid:1). Some sequences following v (e.g. A)
v(θ, ψ) ∝ (cid:0) − ∇θ MMD2
(δ0, δθ), ∇ψ MMD2
kψ
kψ
converge to an optimal solution (0, ψ), but some (B) move in the wrong direction, and others (C) are
stuck because there is essentially no gradient. Figure 1 (c, red) shows that the optimal DΨ
MMD critic
is very sharp near P and Q; this is less true for cases where the algorithm converged.
We can avoid these issues if we ensure a bounded Lipschitz critic:3
Proposition 2. Assume the critics fψ(x) = (EX∼P kψ(X, x) − EY ∼Q kψ(Y, x))/ MMDkψ (P, Q)
are uniformly bounded and have a common Lipschitz constant: supx∈X ,ψ∈Ψ|fψ(x)| < ∞ and
supψ∈Ψ(cid:107)fψ(cid:107)Lip < ∞. In particular, this holds when kψ = K ◦ φψ and

sup
a∈Rs

K(a, a) < ∞,

(cid:107)K(a, ·) − K(b, ·)(cid:107)HK ≤ LK(cid:107)a − b(cid:107)Rs,

(cid:107)φψ(cid:107)Lip ≤ Lφ < ∞.

sup
ψ∈Ψ

Then DΨ

MMD is continuous in the weak topology: if Pn

D−→ P, then DΨ

MMD(Pn, P) → 0.

2 f #P denotes the pushforward of a distribution: if X ∼ P, then f (X) ∼ f #P.
3[27, Theorem 4] makes a similar claim to Proposition 2, but its proof was incorrect: it tries to uniformly

bound MMDkψ ≤ W 2, but the bound used is for a Wasserstein in terms of (cid:107)kψ(x, ·) − kψ(y, ·)(cid:107)Hkψ

.

3

Figure 1: The setting of Example 1. (a, b): parameter-space gradient ﬁelds for the MMD and the
SMMD (Section 3.3); the horizontal axis is θ, and the vertical 1/ψ. (c): optimal MMD critics for
θ = 20 with different kernels. (d): the MMD and the distances of Section 3 optimized over ψ.

Proof. The main result is [12, Corollary 11.3.4]. To show the claim for kψ = K ◦ φψ, note that
|fψ(x) − fψ(y)| ≤ (cid:107)fψ(cid:107)Hkψ

(cid:107)kψ(x, ·) − kψ(y, ·)(cid:107)Hkψ

, which since (cid:107)fψ(cid:107)Hkψ

= 1 is

(cid:107)K(φψ(x), ·) − K(φψ(y), ·)(cid:107)HK ≤ LK(cid:107)φψ(x) − φψ(y)(cid:107)Rs ≤ LKLφ(cid:107)x − y(cid:107)Rd .

Indeed, if we put a box constraint on ψ [27] or regularize the gradient of the critic function [7],
the resulting MMD GAN generally matches or outperforms WGAN-based models. Unfortunately,
though, an additive gradient penalty doesn’t substantially change the vector ﬁeld of Figure 1 (a), as
shown in Figure 5 (Appendix B). We will propose distances with much better convergence behavior.

3 New discrepancies for learning implicit generative models

Our aim here is to introduce a discrepancy that can provide useful gradient information when used as
an IGM loss. Proofs of results in this section are deferred to Appendix A.

3.1 Lipschitz Maximum Mean Discrepancy

Proposition 2 shows that an MMD-like discrepancy can be continuous under the weak topology even
when optimizing over kernels, if we directly restrict the critic functions to be Lipschitz. We can easily
deﬁne such a distance, which we call the Lipschitz MMD: for some λ > 0,

≤1

f ∈Hk : (cid:107)f (cid:107)2

LipMMDk,λ(P, Q) :=

EX∼P [f (X)] − EY ∼Q [f (Y )] .

sup
Lip+λ(cid:107)f (cid:107)2
For a universal kernel k, we conjecture that limλ→0 LipMMDk,λ(P, Q) → W(P, Q). But for any k
and λ, LipMMD is upper-bounded by W, as (4) optimizes over a smaller set of functions than (2).
Thus DΨ,λ
LipMMD(P, Q) := supψ∈Ψ LipMMDkψ,λ(P, Q) is also upper-bounded by W, and hence is
continuous in the Wasserstein topology. It also shows excellent empirical behavior on Example 1
(Figure 1 (d), and Figure 5 in Appendix B). But estimating LipMMDk,λ, let alone DΨ,λ
LipMMD, is in
general extremely difﬁcult (Appendix D), as ﬁnding (cid:107)f (cid:107)Lip requires optimization in the input space.
Constraining the mean gradient rather than the maximum, as we will do next, is far more tractable.

(4)

Hk

4

3.2 Gradient-Constrained Maximum Mean Discrepancy

sup
f ∈Hk : (cid:107)f (cid:107)S(µ),k,λ≤1
L2(µ) + (cid:107)∇f (cid:107)2

We deﬁne the Gradient-Constrained MMD for λ > 0 and using some measure µ as

GCMMDµ,k,λ(P, Q) :=

EX∼P [f (X)] − EY ∼Q [f (Y )] ,

(5)

where (cid:107)f (cid:107)2

S(µ),k,λ := (cid:107)f (cid:107)2

L2(µ) + λ(cid:107)f (cid:107)2
Hk

(6)
L2(µ) = (cid:82) (cid:107)·(cid:107)2 µ(dx) denotes the squared L2 norm. Rather than directly constraining the Lipschitz
(cid:107)·(cid:107)2
constant, the second term (cid:107)∇f (cid:107)2
L2(µ) encourages the function f to be ﬂat where µ has mass. In
experiments we use µ = P, ﬂattening the critic near the target sample. We add the ﬁrst term following
[10]: in one dimension and with µ uniform, (cid:107)·(cid:107)S(µ),·,0 is then an RKHS norm with the kernel
κ(x, y) = exp(−(cid:107)x − y(cid:107)), which is also a Sobolev space. The correspondence to a Sobolev norm is
lost in higher dimensions [53, Ch. 10], but we also found the ﬁrst term to be beneﬁcial in practice.

.

We can exploit some properties of Hk to compute (5) analytically. Call the difference in kernel mean
embeddings η := EX∼P[k(X, ·)] − EY ∼Q[k(Y, ·)] ∈ Hk; recall MMD(P, Q) = (cid:107)η(cid:107)Hk .
Proposition 3. Let ˆµ = (cid:80)M
RM d with (m, i)th entry4 ∂iη(Xm). Then under Assumptions (A) to (D) in Appendix A.1,

m=1 δXm . Deﬁne η(X) ∈ RM with mth entry η(Xm), and ∇η(X) ∈

GCMMD2

ˆµ,k,λ(P, Q) =

(cid:0)MMD2(P, Q) − ¯P (η)(cid:1)

1
λ
(cid:20) η(X)
∇η(X)

¯P (η) =

(cid:21)T (cid:18)(cid:20)K GT
G H

(cid:21)

+ M λIM +M d

(cid:19)−1 (cid:20) η(X)
∇η(X)

(cid:21)

,

where K is the kernel matrix Km,m(cid:48) = k(Xm, Xm(cid:48)), G is the matrix of left derivatives 5 G(m,i),m(cid:48) =
∂ik(Xm, Xm(cid:48)), and H that of derivatives of both arguments H(m,i),(m(cid:48),j) = ∂i∂j+dk(Xm, Xm(cid:48)).
As long as P and Q have integrable ﬁrst moments, and µ has second moments, Assumptions (A)
to (D) are satisﬁed e.g. by a Gaussian or linear kernel on top of a differentiable φψ. We can thus
estimate the GCMMD based on samples from P, Q, and µ by using the empirical mean ˆη for η.

This discrepancy indeed works well in practice: Appendix F.2 shows that optimizing our estimate
of Dµ,Ψ,λ
GCMMD = supψ∈Ψ GCMMDµ,kψ,λ yields a good generative model on MNIST. But the linear
system of size M + M d is impractical: even on 28 × 28 images and using a low-rank approximation,
the model took days to converge. We therefore design a less expensive discrepancy in the next section.

The GCMMD is related to some discrepancies previously used in IGM training. The Fisher GAN [34]
uses only the variance constraint (cid:107)f (cid:107)2
L2(µ) ≤ 1,
along with a vanishing boundary condition on f to ensure a well-deﬁned solution (although this was
not used in the implementation, and can cause very unintuitive critic behavior; see Appendix C).
The authors considered several choices of µ, including the WGAN-GP measure [18] and mixtures
(P + Qθ) /2. Rather than enforcing the constraints in closed form as we do, though, these models
used additive regularization. We will compare to the Sobolev GAN in experiments.

L2(µ) ≤ 1. The Sobolev GAN [33] constrains (cid:107)∇f (cid:107)2

3.3 Scaled Maximum Mean Discrepancy

We will now derive a lower bound on the Gradient-Constrained MMD which retains many of its
attractive qualities but can be estimated in time linear in the dimension d.
Proposition 4. Make Assumptions (A) to (D). For any f ∈ Hk, (cid:107)f (cid:107)S(µ),k,λ ≤ σ−1

µ,k,λ(cid:107)f (cid:107)Hk , where

(cid:118)
(cid:117)
(cid:117)
(cid:116)λ +

(cid:46)

(cid:90)

σµ,k,λ := 1

k(x, x)µ(dx) +

d
(cid:88)

i=1

(cid:90) ∂2k(y, z)
∂yi∂zi

(cid:12)
(cid:12)
(cid:12)(y,z)=(x,x)

µ(dx).

We then deﬁne the Scaled Maximum Mean Discrepancy based on this bound of Proposition 4:

SMMDµ,k,λ(P, Q) :=

sup

EX∼P [f (X)]−EY ∼Q [f (Y )] = σµ,k,λ MMDk(P, Q). (7)

f : σ−1

µ,k,λ(cid:107)f (cid:107)H≤1

4We use (m, i) to denote (m − 1)d + i; thus ∇η(X) stacks ∇η(X1), . . . , ∇η(XM ) into one vector.
5We use ∂ik(x, y) to denote the partial derivative with respect to xi, and ∂i+dk(x, y) that for yi.

5

Because the constraint in the optimization of (7) is more restrictive than in that of (5), we have
that SMMDµ,k,λ(P, Q) ≤ GCMMDµ,k,λ(P, Q). The Sobolev norm (cid:107)f (cid:107)S(µ),λ, and a fortiori the
gradient norm under µ, is thus also controlled for the SMMD critic. We also show in Appendix F.1
that SMMDµ,k,λ behaves similarly to GCMMDµ,k,λ on Gaussians.
(cid:3).
If kψ = K ◦ φψ and K(a, b) = g(−(cid:107)a − b(cid:107)2), then σ−2
(cid:3). Estimating
k,µ,λ = λ + Eµ
Or if K is linear, K(a, b) = aTb, then σ−2
these terms based on samples from µ is straightforward, giving a natural estimator for the SMMD.

k,µ,λ = λ + g(0) + 2|g(cid:48)(0)| Eµ

(cid:2)(cid:107)φψ(X)(cid:107)2 + (cid:107)∇φψ(X)(cid:107)2

(cid:2)(cid:107)∇φψ(X)(cid:107)2

F

F

Of course, if µ and k are ﬁxed, the SMMD is simply a constant times the MMD, and so behaves
in essentially the same way as the MMD. But optimizing the SMMD over a kernel family Ψ,
Dµ,Ψ,λ

SMMD(P, Q) := supψ∈Ψ SMMDµ,kψ,λ(P, Q), gives a distance very different from DΨ

MMD (3).

Figure 1 (b) shows the vector ﬁeld for the Optimized SMMD loss in Example 1, using the WGAN-
GP measure µ = Uniform(0, θ). The optimization surface is far more amenable: in particular
the location C, which formerly had an extremely small gradient that made learning effectively
impossible, now converges very quickly by ﬁrst reducing the critic gradient until some signal is
available. Figure 1 (d) demonstrates that Dµ,Ψ,λ
LipMMD but in sharp contrast
to DΨ

SMMD, like Dµ,Ψ,λ
MMD, is continuous with respect to the location θ and provides a strong gradient towards 0.

GCMMD and DΨ,λ

SMMD is continuous in the Wasserstein topology under some conditions:

We can establish that Dµ,Ψ,λ
Theorem 1. Let kψ = K ◦ φψ, with φψ : X → Rs a fully-connected L-layer network with
Leaky-ReLUα activations whose layers do not increase in width, and K satisfying mild smoothness
conditions QK < ∞ (Assumptions (II) to (V) in Appendix A.2). Let Ψκ be the set of parameters where
each layer’s weight matrices have condition number cond(W l) = (cid:107)W l(cid:107)/ σmin(W l) ≤ κ < ∞. If µ
has a density (Assumption (I)), then

Thus if Pn

W−→ P, then Dµ,Ψκ,λ

Dµ,Ψκ,λ

SMMD (P, Q) ≤

QKκL/2
√
dLαL/2
SMMD (Pn, P) → 0, even if µ is chosen to depend on P and Q.

W(P, Q).

L2(µ) = Eµ(cid:107)∇fψ(X)(cid:107)2 does
Uniform bounds vs bounds in expectation Controlling (cid:107)∇fψ(cid:107)2
not necessarily imply a bound on (cid:107)f (cid:107)Lip ≥ supx∈X (cid:107)∇fψ(X)(cid:107), and so does not in general give
continuity via Proposition 2. Theorem 1 implies that when the network’s weights are well-conditioned,
it is sufﬁcient to only control (cid:107)∇fψ(cid:107)2

L2(µ), which is far easier in practice than controlling (cid:107)f (cid:107)Lip.

If we instead tried to directly controlled (cid:107)f (cid:107)Lip with e.g. spectral normalization (SN) [32], we
could signiﬁcantly reduce the expressiveness of the parametric family. In Example 1, constraining
(cid:107)φψ(cid:107)Lip = 1 limits us to only Ψ = {1}. Thus D{1}
MMD is simply the MMD with an RBF kernel
of bandwidth 1, which has poor gradients when θ is far from 0 (Figure 1 (c), blue). The Cauchy-
Schwartz bound of Proposition 4 allows jointly adjusting the smoothness of kψ and the critic f , while
SN must control the two independently. Relatedly, limiting (cid:107)φ(cid:107)Lip by limiting the Lipschitz norm of
each layer could substantially reduce capacity, while (cid:107)∇fψ(cid:107)L2(µ) need not be decomposed by layer.
Another advantage is that µ provides a data-dependent measure of complexity as in [10]: we do not
needlessly prevent ourselves from using critics that behave poorly only far from the data.

Spectral parametrization When the generator is near a local optimum, the critic might identify
only one direction on which Qθ and P differ. If the generator parameterization is such that there
is no local way for the generator to correct it, the critic may begin to single-mindedly focus on
this difference, choosing redundant convolutional ﬁlters and causing the condition number of the
weights to diverge. If this occurs, the generator will be motivated to ﬁx this single direction while
ignoring all other aspects of the distributions, after which it may become stuck. We can help avoid
this collapse by using a critic parameterization that encourages diverse ﬁlters with higher-rank weight
matrices. Miyato et al. [32] propose to parameterize the weight matrices as W = γ ¯W /(cid:107) ¯W (cid:107)op,
where (cid:107) ¯W (cid:107)op is the spectral norm of ¯W . This parametrization works particularly well with Dµ,Ψ,λ
SMMD;
Figure 2 (b) shows the singular values of the second layer of a critic’s network (and Figure 9, in
Appendix F.3, shows more layers), while Figure 2 (d) shows the evolution of the condition number
during training. The conditioning of the weight matrix remains stable throughout training with
spectral parametrization, while it worsens through training in the default case.

6

4 Experiments

We evaluated unsupervised image generation on three datasets: CIFAR-10 [26] (60 000 images,
32 × 32), CelebA [29] (202 599 face images, resized and cropped to 160 × 160 as in [7]), and the
more challenging ILSVRC2012 (ImageNet) dataset [41] (1 281 167 images, resized to 64 × 64).
Code for all of these experiments is available at github.com/MichaelArbel/Scaled-MMD-GAN.
Losses All models are based on a scalar-output critic network φψ : X → R, except MMDGAN-GP
where φψ : X → R16 as in [7]. The WGAN and Sobolev GAN use a critic f = φψ, while the
GAN uses a discriminator Dψ(x) = 1/(1 + exp(−φψ(x))). The MMD-based methods use a kernel
kψ(x, y) = exp(−(φψ(x) − φψ(y))2/2), except for MMDGAN-GP which uses a mixture of RQ
kernels as in [7]. Increasing the output dimension of the critic or using a different kernel didn’t
substantially change the performance of our proposed method. We also consider SMMD with a linear
top-level kernel, k(x, y) = φψ(x)φψ(y); because this becomes essentially identical to a WGAN
(Appendix E), we refer to this method as SWGAN. SMMD and SWGAN use µ = P; Sobolev GAN
uses µ = (P + Q)/2 as in [33]. We choose λ and an overall scaling to obtain the losses:

SMMD:

(cid:92)MMD

2
kψ
1 + 10 EˆP [(cid:107)∇φψ(X)(cid:107)2
F ]

(P, Qθ)

, SWGAN:

(cid:113)

EˆP [φψ(X)] − E ˆQθ

[φψ(X)]

1 + 10 (cid:0)EˆP [|φψ(X)|2] + EˆP [(cid:107)∇φψ(X)(cid:107)2

F ](cid:1)

.

Architecture For CIFAR-10, we used the CNN architecture proposed by [32] with a 7-layer critic
and a 4-layer generator. For CelebA, we used a 5-layer DCGAN discriminator and a 10-layer ResNet
generator as in [7]. For ImageNet, we used a 10-layer ResNet for both the generator and discriminator.
In all experiments we used 64 ﬁlters for the smallest convolutional layer, and double it at each layer
(CelebA/ImageNet) or every other layer (CIFAR-10). The input codes for the generator are drawn
from Uniform (cid:0)[−1, 1]128(cid:1). We consider two parameterizations for each critic: a standard one where
the parameters can take any real value, and a spectral parametrization (denoted SN-) as above [32].
Models without explicit gradient control (SN-GAN, SN-MMDGAN, SN-MMGAN-L2, SN-WGAN)
ﬁx γ = 1, for spectral normalization; others learn γ, using a spectral parameterization.

Training All models were trained for 150 000 generator updates on a single GPU, except for ImageNet
where the model was trained on 3 GPUs simultaneously. To limit communication overhead we
averaged the MMD estimate on each GPU, giving the block MMD estimator [54]. We always used
64 samples per GPU from each of P and Q, and 5 critic updates per generator step. We used initial
learning rates of 0.0001 for CIFAR-10 and CelebA, 0.0002 for ImageNet, and decayed these rates
using the KID adaptive scheme of [7]: every 2 000 steps, generator samples are compared to those
from 20 000 steps ago, and if the relative KID test [9] fails to show an improvement three consecutive
times, the learning rate is decayed by 0.8. We used the Adam optimizer [25] with β1 = 0.5, β2 = 0.9.

Evaluation To compare the sample quality of different models, we considered three different scores
based on the Inception network [49] trained for ImageNet classiﬁcation, all using default parameters
in the implementation of [7]. The Inception Score (IS) [42] is based on the entropy of predicted
labels; higher values are better. Though standard, this metric has many issues, particularly on datasets
other than ImageNet [4, 7, 20]. The FID [20] instead measures the similarity of samples from the
generator and the target as the Wasserstein-2 distance between Gaussians ﬁt to their intermediate
representations. It is more sensible than the IS and becoming standard, but its estimator is strongly
biased [7]. The KID [7] is similar to FID, but by using a polynomial-kernel MMD its estimates enjoy
better statistical properties and are easier to compare. (A similar score was recommended by [21].)

Results Table 1a presents the scores for models trained on both CIFAR-10 and CelebA datasets. On
CIFAR-10, SN-SWGAN and SN-SMMDGAN performed comparably to SN-GAN. But on CelebA,
SN-SWGAN and SN-SMMDGAN dramatically outperformed the other methods with the same
architecture in all three metrics. It also trained faster, and consistently outperformed other methods
over multiple initializations (Figure 2 (a)). It is worth noting that SN-SWGAN far outperformed
WGAN-GP on both datasets. Table 1b presents the scores for SMMDGAN and SN-SMMDGAN
trained on ImageNet, and the scores of pre-trained models using BGAN [6] and SN-GAN [32].6 The

6These models are courtesy of the respective authors and also trained at 64 × 64 resolution. SN-GAN used
the same architecture as our model, but trained for 250 000 generator iterations; BS-GAN used a similar 5-layer
ResNet architecture and trained for 74 epochs, comparable to SN-GAN.

7

Figure 2: The training process on CelebA. (a) KID scores. We report a ﬁnal score for SN-GAN
slightly before its sudden failure mode; MMDGAN and SN-MMDGAN were unstable and had scores
around 100. (b) Singular values of the second layer, both early (dashed) and late (solid) in training.
(c) σ−2
µ,k,λ for several MMD-based methods. (d) The condition number in the ﬁrst layer through
training. SN alone does not control σµ,k,λ, and SMMD alone does not control the condition number.

(a) Scaled MMD GAN with SN

(b) SN-GAN

(c) Boundary Seeking GAN

(d) Scaled MMD GAN with SN

(e) Scaled WGAN with SN

(f) MMD GAN with GP+L2

Figure 3: Samples from various models. Top: 64 × 64 ImageNet; bottom: 160 × 160 CelebA.

8

Table 1: Mean (standard deviation) of score estimates, based on 50 000 samples from each model.

(a) CIFAR-10 and CelebA.

Method

WGAN-GP
MMDGAN-GP-L2
Sobolev-GAN
SMMDGAN
SN-GAN
SN-SWGAN
SN-SMMDGAN

CIFAR-10
IS

FID

6.9±0.2
6.9±0.1
7.0±0.1
7.0±0.1
7.2±0.1
7.2±0.1
7.3±0.1

31.1±0.2
31.4±0.3
30.3±0.3
31.5±0.4
26.7±0.2
28.5±0.2
25.0±0.3

KID×103

22.2±1.1
23.3±1.1
22.3±1.2
22.2±1.1
16.1±0.9
17.6±1.1
16.6±2.0

CelebA
IS

2.7±0.0
2.6±0.0
2.9±0.0
2.7±0.0
2.7±0.0
2.8±0.0
2.8±0.0

FID

KID×103

29.2±0.2
20.5±0.2
16.4±0.1
18.4±0.2
22.6±0.1
14.1±0.2
12.4±0.2

22.0±1.0
13.0±1.0
10.6±0.5
11.5±0.8
14.6±1.1
7.7±0.5
6.1±0.4

(b) ImageNet.

Method

IS

FID

10.7±0.4
BGAN
11.2±0.1
SN-GAN
SMMDGAN
10.7±0.2
SN-SMMDGAN 10.9±0.1

43.9±0.3
47.5±0.1
38.4±0.3
36.6±0.2

KID×103

47.0±1.1
44.4±2.2
39.3±2.5
34.6±1.6

proposed methods substantially outperformed both methods in FID and KID scores. Figure 3 shows
samples on ImageNet and CelebA; Appendix F.4 has more.

Spectrally normalized WGANs / MMDGANs To control for the contribution of the spectral
parametrization to the performance, we evaluated variants of MMDGANs, WGANs and Sobolev-
GAN using spectral normalization (in Table 2, Appendix F.3). WGAN and Sobolev-GAN led to
unstable training and didn’t converge at all (Figure 11) despite many attempts. MMDGAN converged
on CIFAR-10 (Figure 11) but was unstable on CelebA (Figure 10). The gradient control due to SN
is thus probably too loose for these methods. This is reinforced by Figure 2 (c), which shows that
the expected gradient of the critic network is much better-controlled by SMMD, even when SN is
used. We also considered variants of these models with a learned γ while also adding a gradient
penalty and an L2 penalty on critic activations [7, footnote 19]. These generally behaved similarly to
MMDGAN, and didn’t lead to substantial improvements. We ran the same experiments on CelebA,
but aborted the runs early when it became clear that training was not successful.

Rank collapse We occasionally observed the failure mode for SMMD where the critic becomes
low-rank, discussed in Section 3.3, especially on CelebA; this failure was obvious even in the training
objective. Figure 2 (b) is one of these examples. Spectral parametrization seemed to prevent this
behavior. We also found one could avoid collapse by reverting to an earlier checkpoint and increasing
the RKHS regularization parameter λ, but did not do this for any of the experiments here.

5 Conclusion

We studied gradient regularization for MMD-based critics in implicit generative models, clarifying
how previous techniques relate to the DΨ
MMD loss. Based on these insights, we proposed the Gradient-
Constrained MMD and its approximation the Scaled MMD, a new loss function for IGMs that
controls gradient behavior in a principled way and obtains excellent performance in practice.

One interesting area of future study for these distances is their behavior when used to diffuse particles
distributed as Q towards particles distributed as P. Mroueh et al. [33, Appendix A.1] began such a
study for the Sobolev GAN loss; [35] proved convergence and studied discrete-time approximations.

Another area to explore is the geometry of these losses, as studied by Bottou et al. [8], who showed
potential advantages of the Wasserstein geometry over the MMD. Their results, though, do not
address any distances based on optimized kernels; the new distances introduced here might have
interesting geometry of their own.

9

References

[1] B. Amos and J. Z. Kolter. “OptNet: Differentiable Optimization as a Layer in Neural Net-

works.” ICML. 2017. arXiv: 1703.00443.

[2] M. Arjovsky and L. Bottou. “Towards Principled Methods for Training Generative Adversarial

Networks.” ICLR. 2017. arXiv: 1701.04862.

[3] M. Arjovsky, S. Chintala, and L. Bottou. “Wasserstein Generative Adversarial Networks.”

ICML. 2017. arXiv: 1701.07875.

[4] S. Barratt and R. Sharma. A Note on the Inception Score. 2018. arXiv: 1801.01973.
[5] M. G. Bellemare, I. Danihelka, W. Dabney, S. Mohamed, B. Lakshminarayanan, S. Hoyer,
and R. Munos. The Cramer Distance as a Solution to Biased Wasserstein Gradients. 2017.
arXiv: 1705.10743.

[6] D. Berthelot, T. Schumm, and L. Metz. BEGAN: Boundary Equilibrium Generative Adversarial

[7] M. Bi´nkowski, D. J. Sutherland, M. Arbel, and A. Gretton. “Demystifying MMD GANs.”

Networks. 2017. arXiv: 1703.10717.

ICLR. 2018. arXiv: 1801.01401.

[8] L. Bottou, M. Arjovsky, D. Lopez-Paz, and M. Oquab. “Geometrical Insights for Implicit
Generative Modeling.” Braverman Readings in Machine Learning: Key Iedas from Inception
to Current State. Ed. by L. Rozonoer, B. Mirkin, and I. Muchnik. LNAI Vol. 11100. Springer,
2018, pp. 229–268. arXiv: 1712.07822.

[9] W. Bounliphone, E. Belilovsky, M. B. Blaschko, I. Antonoglou, and A. Gretton. “A Test of
Relative Similarity For Model Selection in Generative Models.” ICLR. 2016. arXiv: 1511.
04581.

[10] O. Bousquet, O. Chapelle, and M. Hein. “Measure Based Regularization.” NIPS. 2004.
[11] A. Brock, T. Lim, J. M. Ritchie, and N. Weston. “Neural Photo Editing with Introspective

Adversarial Networks.” ICLR. 2017. arXiv: 1609.07093.

[12] R. M. Dudley. Real Analysis and Probability. 2nd ed. Cambridge University Press, 2002.
[13] G. K. Dziugaite, D. M. Roy, and Z. Ghahramani. “Training generative neural networks via

Maximum Mean Discrepancy optimization.” UAI. 2015. arXiv: 1505.03906.

[14] A. Genevay, G. Peyré, and M. Cuturi. “Learning Generative Models with Sinkhorn Diver-

gences.” AISTATS. 2018. arXiv: 1706.00292.

[15] T. Gneiting and A. E. Raftery. “Strictly proper scoring rules, prediction, and estimation.” JASA

102.477 (2007), pp. 359–378.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio. “Generative Adversarial Nets.” NIPS. 2014. arXiv: 1406.2661.

[17] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Schölkopf, and A. J. Smola. “A Kernel Two-

Sample Test.” JMLR 13 (2012).
I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. “Improved Training of
Wasserstein GANs.” NIPS. 2017. arXiv: 1704.00028.

[19] A. Güngör. “Some bounds for the product of singular values.” International Journal of

[16]

[18]

Contemporary Mathematical Sciences (2007).

[20] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, G. Klambauer, and S. Hochreiter. “GANs
Trained by a Two Time-Scale Update Rule Converge to a Nash Equilibrium.” NIPS. 2017.
arXiv: 1706.08500.

[21] G. Huang, Y. Yuan, Q. Xu, C. Guo, Y. Sun, F. Wu, and K. Weinberger. An empirical study on

evaluation metrics of generative adversarial networks. 2018. arXiv: 1806.07755.

[22] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. “Multimodal Unsupervised Image-to-Image

Translation.” ECCV. 2018. arXiv: 1804.04732.

[23] Y. Jin, K. Zhang, M. Li, Y. Tian, H. Zhu, and Z. Fang. Towards the Automatic Anime Characters

Creation with Generative Adversarial Networks. 2017. arXiv: 1708.05509.

[24] T. Karras, T. Aila, S. Laine, and J. Lehtinen. “Progressive Growing of GANs for Improved

Quality, Stability, and Variation.” ICLR. 2018. arXiv: 1710.10196.

[25] D. Kingma and J. Ba. “Adam: A Method for Stochastic Optimization.” ICLR. 2015. arXiv:

1412.6980.

[26] A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images. 2009.

10

arXiv: 1502.02761.

arXiv: 1411.7766.

(2002), pp. 583–601.

1711.04894.

[27] C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. Póczos. “MMD GAN: Towards Deeper

Understanding of Moment Matching Network.” NIPS. 2017. arXiv: 1705.08584.

[28] Y. Li, K. Swersky, and R. Zemel. “Generative Moment Matching Networks.” ICML. 2015.

[29] Z. Liu, P. Luo, X. Wang, and X. Tang. “Deep learning face attributes in the wild.” ICCV. 2015.

[30] L. Mescheder, A. Geiger, and S. Nowozin. “Which Training Methods for GANs do actually

Converge?” ICML. 2018. arXiv: 1801.04406.

[31] P. Milgrom and I. Segal. “Envelope theorems for arbitrary choice sets.” Econometrica 70.2

[32] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. “Spectral Normalization for Generative

Adversarial Networks.” ICLR. 2018. arXiv: 1802.05927.

[33] Y. Mroueh, C.-L. Li, T. Sercu, A. Raj, and Y. Cheng. “Sobolev GAN.” ICLR. 2018. arXiv:

[34] Y. Mroueh and T. Sercu. “Fisher GAN.” NIPS. 2017. arXiv: 1705.09675.
[35] Y. Mroueh, T. Sercu, and A. Raj. Regularized Kernel and Neural Sobolev Descent: Dynamic

MMD Transport. 2018. arXiv: 1805.12062.

[36] A. Müller. “Integral Probability Metrics and their Generating Classes of Functions.” Advances

in Applied Probability 29.2 (1997), pp. 429–443.

[37] S. Nowozin, B. Cseke, and R. Tomioka. “f-GAN: Training Generative Neural Samplers using

Variational Divergence Minimization.” NIPS. 2016. arXiv: 1606.00709.

[38] A. Radford, L. Metz, and S. Chintala. “Unsupervised Representation Learning with Deep
Convolutional Generative Adversarial Networks.” ICLR. 2016. arXiv: 1511.06434.
J. R. Retherford. “Review: J. Diestel and J. J. Uhl, Jr., Vector measures.” Bull. Amer. Math.
Soc. 84.4 (July 1978), pp. 681–685.

[39]

[40] K. Roth, A. Lucchi, S. Nowozin, and T. Hofmann. “Stabilizing Training of Generative Adver-

sarial Networks through Regularization.” NIPS. 2017. arXiv: 1705.09367.

[41] O. Russakovsky et al. ImageNet Large Scale Visual Recognition Challenge. 2014. arXiv:

1409.0575.

[42] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. “Improved

[43]

Techniques for Training GANs.” NIPS. 2016. arXiv: 1606.03498.
J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University
Press, 2004.

[44] B. K. Sriperumbudur, K. Fukumizu, A. Gretton, G. R. G. Lanckriet, and B. Schölkopf. “Kernel

choice and classiﬁability for RKHS embeddings of probability distributions.” NIPS. 2009.

[45] B. K. Sriperumbudur, K. Fukumizu, and G. R. G. Lanckriet. “Universality, Characteristic
Kernels and RKHS Embedding of Measures.” JMLR 12 (2011), pp. 2389–2410. arXiv: 1003.
0887.

[46] B. Sriperumbudur. “On the optimal estimation of probability mesaures in weak and strong

topologies.” Bernoulli 22.3 (2016), pp. 1839–1893. arXiv: 1310.8240.
I. Steinwart and A. Christmann. Support Vector Machines. Springer, 2008.

[47]
[48] D. J. Sutherland, H.-Y. Tung, H. Strathmann, S. De, A. Ramdas, A. Smola, and A. Gretton.
“Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy.” ICLR.
2017. arXiv: 1611.04488.

[49] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. “Rethinking the Inception

Architecture for Computer Vision.” CVPR. 2016. arXiv: 1512.00567.

[50] T. Unterthiner, B. Nessler, C. Seward, G. Klambauer, M. Heusel, H. Ramsauer, and S. Hochre-
iter. “Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields.” ICLR. 2018.
arXiv: 1708.08819.

[51] C. Villani. Optimal Transport: Old and New. Springer, 2009.
[52]

J. Weed and F. Bach. “Sharp asymptotic and ﬁnite-sample rates of convergence of empirical
measures in Wasserstein distance.” Bernoulli (forthcoming). arXiv: 1707.00087.
[53] H. Wendland. Scattered Data Approximation. Cambridge University Press, 2005.
[54] W. Zaremba, A. Gretton, and M. B. Blaschko. “B-tests: Low Variance Kernel Two-Sample

Tests.” NIPS. 2013. arXiv: 1307.1954.

11

A Proofs

We ﬁrst review some basic properties of Reproducing Kernel Hilbert Spaces. We consider here a
separable RKHS H with basis (ei)i∈I , where I is either ﬁnite if H is ﬁnite-dimensional, or I = N
otherwise. We also assume that the reproducing kernel k is continuously twice differentiable.

We use a slightly nonstandard notation for derivatives: ∂if (x) denotes the ith partial derivative of f
evaluated at x, and ∂i∂j+dk(x, y) denotes ∂2k(a,b)
∂ai∂bj

|(a,b)=(x,y).

Then the following reproducing properties hold for any given function f in H [47, Lemma 4.34]:
f (x) =(cid:104)f, k(x, .)(cid:105)H
∂if (x) =(cid:104)f, ∂ik(x, .)(cid:105)H.

(8)
(9)

We say that an operator A : H (cid:55)→ H is Hilbert-Schmidt if (cid:107)A(cid:107)2
H is ﬁnite. (cid:107)A(cid:107)HS
is called the Hilbert-Schmidt norm of A. The space of Hilbert-Schmidt operators itself a Hilbert
space with the inner product (cid:104)A, B(cid:105)HS = (cid:80)
i∈I (cid:104)Aei, Bei(cid:105)H. Moreover, we say that an operator A
is trace-class if its trace norm is ﬁnite, i.e. (cid:107)A(cid:107)1 = (cid:80)
2 ei(cid:105)H < ∞. The outer product
f ⊗ g for f, g ∈ H gives an H → H operator such that (f ⊗ g)v = (cid:104)g, v(cid:105)Hf for all v in H.

i∈I (cid:104)ei, (A∗A) 1

i∈I (cid:107)Aei(cid:107)2

HS = (cid:80)

Given two vectors f and g in H and a Hilbert-Schmidt operator A we have the following properties:

(i) The outer product f ⊗ g is a Hilbert-Schmidt operator with Hilbert-Schmidt norm given by:

(ii) The inner product between two rank-one operators f ⊗ g and u ⊗ v is (cid:104)f ⊗ g, u ⊗ v(cid:105)HS =

(cid:107)f ⊗ g(cid:107)HS = (cid:107)f (cid:107)H(cid:107)g(cid:107)H.

(cid:104)f, u(cid:105)H(cid:104)g, v(cid:105)H.

(iii) The following identity holds: (cid:104)f, Ag(cid:105)H = (cid:104)f ⊗ g, A(cid:105)HS.

Deﬁne the following covariance-type operators:

d
(cid:88)

i=1

Dx = k(x, ·) ⊗ k(x, ·) +

∂ik(x, ·) ⊗ ∂ik(x, ·) Dµ = EX∼µ DX Dµ,λ = Dµ + λI;

(10)

these are useful in that, using (8) and (9), (cid:104)f, Dxg(cid:105) = f (x)g(x) + (cid:80)d

i=1 ∂if (x) ∂ig(x).

A.1 Deﬁnitions and estimators of the new distances

We will need the following assumptions about the distributions P and Q, the measure µ, and the
kernel k:

(A) P and Q have integrable ﬁrst moments.
(B) (cid:112)k(x, x) grows at most linearly in x: for all x in X , (cid:112)k(x, x) ≤ C((cid:107)x(cid:107) + 1) for some

constant C.

(C) The kernel k is twice continuously differentiable.
(D) The functions x (cid:55)→ k(x, x) and x (cid:55)→ ∂i∂i+dk(x, x) for 1 ≤ i ≤ d are µ-integrable.

When k = K ◦ φψ, Assumption (B) is automatically satisﬁed by a K such as the Gaussian; when K
is linear, it is true for a quite general class of networks φψ [7, Lemma 1].

We will ﬁrst give a form for the Gradient-Constrained MMD (5) in terms of the operator (10):
Proposition 5. Under Assumptions (A) to (D), the Gradient-Constrained MMD is given by

GCMMDµ,k,λ(P, Q) =

(cid:104)η, D−1

µ,λη(cid:105)H.

(cid:113)

(11)

Proof of Proposition 5. Let f be a function in H. We will ﬁrst express the squared λ-regularized
Sobolev norm of f (6) as a quadratic form in H. Recalling the reproducing properties of (8) and (9),
we have:

(cid:107)f (cid:107)2

S(µ),k,λ =

(cid:104)f, k(x, ·)(cid:105)2

H µ(dx) +

(cid:104)f, ∂ik(x, ·)(cid:105)2

H µ(dx) + λ(cid:107)f (cid:107)2
H.

(cid:90)

d
(cid:88)

(cid:90)

i=1

12

Using Property (ii) and the operator (10), one further gets

(cid:107)f (cid:107)2

S(µ),k,λ =

(cid:104)f ⊗ f, Dx(cid:105)HS µ(dx) + λ(cid:107)f (cid:107)2
H.

(cid:90)

Under Assumption (D), and using Lemma 6, one can take the integral inside the inner product, which
leads to (cid:107)f (cid:107)2

H. Finally, using Property (iii) it follows that

S(µ),k,λ = (cid:104)f ⊗ f, Dµ(cid:105)HS + λ(cid:107)f (cid:107)2

(cid:107)f (cid:107)2

S(µ),k,λ = (cid:104)f, Dµ,λf (cid:105)H.

Under Assumptions (A) and (B), Lemma 6 applies, and it follows that k(x, ·) is also Bochner
integrable under P and Q. Thus

EP [(cid:104)f, k(x, ·)(cid:105)H] − EQ [(cid:104)f, k(x, ·)(cid:105)H] = (cid:104)f, EP [k(x, ·)] − EP [k(x, ·)](cid:105)H = (cid:104)f, η(cid:105)H,

where η is deﬁned as this difference in mean embeddings.

Since Dµ,λ is symmetric positive deﬁnite, its square-root D

1
2

For any f ∈ H, let g = D
corresponding f = D− 1

1
2

µ,λ is well-deﬁned and is also invertible.
H. Note that for any g ∈ H, there is a
µ,λg. Thus we can re-express the maximization problem in (5) in terms of g:

µ,λf , so that (cid:104)f, Dµ,λf (cid:105)H = (cid:107)g(cid:107)2

2

GCMMDµ,k,λ(P, Q) :=

sup
f ∈H
(cid:104)f,Dµ,λf (cid:105)H≤1
(cid:104)g, D− 1

2

= sup
g∈H
(cid:107)g(cid:107)H≤1

(cid:104)f, η(cid:105)H = sup
g∈H
(cid:107)g(cid:107)H≤1

(cid:104)D− 1

2

µ,λg, η(cid:105)H

µ,λη(cid:105)H = (cid:107)D− 1

2

µ,λη(cid:107)H =

(cid:104)η, D−1

µ,λη(cid:105)H.

(cid:113)

Proposition 5, though, involves inverting the inﬁnite-dimensional operator Dµ,λ and thus doesn’t
directly give us a computable estimator. Proposition 3 solves this problem in the case where µ is a
discrete measure:
Proposition 3. Let ˆµ = (cid:80)M
m=1 δXm be an empirical measure of M points. Let η(X) ∈ RM have
mth entry η(Xm), and ∇η(X) ∈ RM d have (m, i)th entry7 ∂iη(Xm). Then under Assumptions (A)
to (D), the Gradient-Constrained MMD is

GCMMD2

ˆµ,k,λ(P, Q) =

(cid:0)MMD2(P, Q) − ¯P (η)(cid:1)

1
λ
(cid:20) η(X)
∇η(X)

¯P (η) =

(cid:21)T (cid:18)(cid:20)K GT
G H

(cid:21)

+ M λIM +M d

(cid:19)−1 (cid:20) η(X)
∇η(X)

(cid:21)

,

where K is the kernel matrix Km,m(cid:48) = k(Xm, Xm(cid:48)), G is the matrix of left derivatives G(m,i),m(cid:48) =
∂ik(Xm, Xm(cid:48)), and H that of derivatives of both arguments H(m,i),(m(cid:48),j) = ∂i∂j+dk(Xm, Xm(cid:48)).

Before proving Proposition 3, we note the following interesting alternate form. Let ¯ei be the ith
standard basis vector for RM +M d, and deﬁne T : H → RM +M d as the linear operator

T =

¯em ⊗ k(Xm, ·) +

¯em+(m,i) ⊗ ∂ik(Xm, ·).

M
(cid:88)

m=1

M
(cid:88)

d
(cid:88)

m=1

i=1

Then

(cid:21)

(cid:20) η(X)
∇η(X)

(cid:21)

(cid:20)K GT
G H

= T η, and

= T T ∗. Thus we can write

GCMMD2

ˆµ,k,λ =

(cid:10)η, (cid:0)I − T ∗(T T ∗ + M λI)−1T (cid:1) η(cid:11)

H .

1
λ

7We use (m, i) to denote (m − 1)d + i; thus ∇η(X) stacks ∇η(X1), . . . , ∇η(XM ) into one vector.

13

Proof of Proposition 3. Let g ∈ H be the solution to the regression problem Dµ,λg = η:

g(Xm)k(Xm, ·) +

∂ig(Xm)∂ik(Xm, ·)

+ λg = η

(cid:35)

d
(cid:88)

i=1

1
M

(cid:34)

M
(cid:88)

m=1

g =

η −

1
λ

1
λM

(cid:34)

M
(cid:88)

m=1

g(Xm)k(Xm, ·) +

∂ig(Xm)∂ik(Xm, ·)

.

(12)

Taking the inner product of both sides of (12) with k(Xm(cid:48), ·) for each 1 ≤ m(cid:48) ≤ M yields the
following M equations:

g(Xm(cid:48)) =

η(Xm(cid:48)) −

g(Xm)Km,m(cid:48) +

∂ig(Xm) G(m,i),m(cid:48)

.

(13)

Doing the same with ∂jk(Xm(cid:48), ·) gives M d equations:

∂jg(Xm(cid:48)) =

∂jη(Xm(cid:48)) −

g(Xm)G(m(cid:48),j),m +

∂ig(Xm)H(m,i),(m(cid:48),j)

.

(14)

(cid:35)

(cid:35)

(cid:35)

(cid:35)

From (12), it is clear that g is a linear combination of the form:

g(x) =

η(x) −

αmk(Xm, x) +

βm,i∂ik(Xm, x)

,

1
λ

1
λM

(cid:34)

M
(cid:88)

m=1

1
λ

1
λM

(cid:34)

M
(cid:88)

m=1

1
λ

1
λM

(cid:34)

M
(cid:88)

m=1

d
(cid:88)

i=1

d
(cid:88)

i=1

d
(cid:88)

i=1

d
(cid:88)

i=1

where the coefﬁcients α := (αm = g(Xm))1≤m≤M and β := (βm,i = ∂ig(Xm))1≤m≤M
1≤i≤d
the system of equations (13) and (14). We can rewrite this system as

satisfy

(cid:20)K + M λIM
G

GT
H + M λIM d

(cid:21)

(cid:21) (cid:20)α
β

= M

(cid:21)

(cid:20) η(X)
∇η(X)

,

where IM , IM d are the identity matrices of dimension M , M d. Since K and H must be positive
semideﬁnite, an inverse exists. We conclude by noticing that
(cid:34)

(cid:35)

GCMMDˆµ,k,λ(P, Q)2 = (cid:104)η, g(cid:105)H =

(cid:107)η(cid:107)2

H −

αmη(Xm) +

βm,i∂iη(Xm)

.

1
λ

1
λM

M
(cid:88)

m=1

d
(cid:88)

i=1

The following result was key to our deﬁnition of the SMMD in Section 3.3.
Proposition 4. Under Assumptions (A) to (D), we have for all f ∈ H that

where σk,µ,λ := 1/

λ + (cid:82) k(x, x)µ(dx) + (cid:80)d

i=1

(cid:113)

(cid:107)f (cid:107)S(µ),k,λ ≤ σ−1

µ,k,λ(cid:107)f (cid:107)Hk ,
(cid:82) ∂i∂i+dk(x, x)µ(dx).

Proof of Proposition 4. The key idea here is to use the Cauchy-Schwarz inequality for the Hilbert-
Schmidt inner product. Letting f ∈ H, (cid:107)f (cid:107)2
S(µ),k,λ is
(cid:90)

(cid:90)

f (x)2 µ(dx) +

(cid:107)∇f (x)(cid:107)2 µ(dx) + λ(cid:107)f (cid:107)2
H

(cid:104)f, k(x, ·) ⊗ k(x, ·)f (cid:105)H µ(dx) +

(cid:104)f, ∂ik(x, ·) ⊗ ∂ik(x, ·)f (cid:105)H µ(dx) + λ(cid:107)f (cid:107)2
H

(cid:104)f ⊗ f, k(x, ·) ⊗ k(x, ·)(cid:105)HS µ(dx) +

(cid:104)f ⊗ f, ∂ik(x, ·) ⊗ ∂ik(x, ·)(cid:105)HS µ(dx) + λ(cid:107)f (cid:107)2
H

k(x, x) µ(dx) +

∂i∂i+dk(x, x) µ(dx) + λ

.

(cid:35)

d
(cid:88)

(cid:90)

i=1

(a) follows from the reproducing properties (8) and (9) and Property (ii). (b) is obtained using
Property (iii), while (c) follows from the Cauchy-Schwarz inequality and Property (i).

(cid:90)

(a)
=

(cid:90)

(b)
=

(cid:34)(cid:90)

(c)
≤ (cid:107)f (cid:107)2
H

d
(cid:88)

(cid:90)

i=1

d
(cid:88)

(cid:90)

i=1

14

Lemma 6. Under Assumption (D), Dx is Bochner integrable and its integral Dµ is a trace-class
symmetric positive semi-deﬁnite operator with Dµ,λ = D+λI invertible for any positive λ. Moreover,
for any Hilbert-Schmidt operator A we have: (cid:104)A, Dµ(cid:105)HS = (cid:82) (cid:104)A, Dx(cid:105)HS µ(dx).
Under Assumptions (A) and (B), k(x, ·) is Bochner integrable with respect to any probability
distribution P with ﬁnite ﬁrst moment and the following relation holds: (cid:104)f, EP [k(x, ·)](cid:105)H =
EP [(cid:104)f, k(x, ·)(cid:105)H] for all f in H.

Proof. The operator Dx is positive self-adjoint. It is also trace-class, as by the triangle inequality

(cid:107)Dx(cid:107)1 ≤ (cid:107)k(x, ·) ⊗ k(x, ·)(cid:107)1 +

(cid:107)∂ik(x, ·) ⊗ ∂ik(x, ·)(cid:107)1

d
(cid:88)

i=1

= (cid:107)k(x, ·)(cid:107)2

H +

(cid:107)∂ik(x, ·)(cid:107)2

H < ∞.

d
(cid:88)

i=1

By Assumption (D), we have that (cid:82) (cid:107)Dx(cid:107)1 µ(dx) < ∞ which implies that Dx is µ-integrable in
the Bochner sense [39, Deﬁnition 1 and Theorem 2]. Its integral Dµ is trace-class and satisﬁes
(cid:107)Dµ(cid:107)1 ≤ (cid:82) (cid:107)Dx(cid:107)1 µ(dx). This allows to have (cid:104)A, Dµ(cid:105)HS = (cid:82) (cid:104)A, Dx(cid:105)HS µ(dx) for all Hilbert-
Schmidt operators A. Moreover, the integral preserves the symmetry and positivity. It follows that
Dµ,λ is invertible.
The Bochner integrability of k(x, ·) under a distribution P with ﬁnite moment follows directly from
Assumptions (A) and (B), since (cid:82) (cid:107)k(x, ·)(cid:107) P(dx) ≤ C (cid:82) ((cid:107)x(cid:107) + 1) P(dx) < ∞. This allows us to
write (cid:104)f, EP[k(x, ·)](cid:105)H = EP[(cid:104)f, k(x, ·)(cid:105)H].

A.2 Continuity of the Optimized Scaled MMD in the Wasserstein topology

To prove Theorem 1, we we will ﬁrst need some new notation.

We assume the kernel is k = K ◦ φψ, i.e. kψ(x, y) = K(φψ(x), φψ(y)), where the representation
function φψ is a network φψ(X) : Rd → RdL consisting of L fully-connected layers:

h0
ψ(X) = X
ψ(X) = W lσl−1(hl−1
hl
φψ(X) = hL

ψ(X).

ψ (X)) + bl

for 1 ≤ l ≤ L

(15)

The intermediate representations hl
ψ(X) are of dimension dl, the weights W l are matrices in
Rdl×dl−1 , and biases bl are vectors in Rdl . The elementwise activation function σ is given by
σ0(x) = x, and for l > 0 the activation σl is a leaky ReLU with leak coefﬁcient 0 < α < 1:

σl(x) = σ(x) =

for l > 0.

(16)

(cid:26)x

x > 0
αx x ≤ 0

The parameter ψ is the concatenation of all the layer parameters:

ψ = (cid:0)(W L, bL), (W L−1, bL−1), . . . , (W 1, b1)(cid:1) .

We denote by Ψ the set of all such possible parameters, i.e. Ψ = RdL×dL−1 ×RdL ×· · ·×Rd1×d×Rd1 .
Deﬁne the following restrictions of Ψ:

Ψκ := (cid:8)ψ ∈ Ψ | ∀1 ≤ l ≤ L, cond(W l) ≤ κ(cid:9)
1 := (cid:8)ψ ∈ Ψκ | ∀1 ≤ l ≤ L, (cid:107)W l(cid:107) = 1(cid:9) .
Ψκ

(17)

(18)

Ψκ is the set of those parameters such that W l have a small condition number, cond(W ) =
σmax(W )/σmin(W ). Ψκ
1 is the set of per-layer normalized parameters with a condition number
bounded by κ.

15

Recall the deﬁnition of Scaled MMD, (7), where λ > 0 and µ is a probability measure:

SMMDµ,k,λ(P, Q) := σµ,k,λ MMDk(P, Q)

σk,µ,λ := 1/

k(x, x) µ(dx) +

∂i∂i+dk(x, x) µ(dx).

(cid:118)
(cid:117)
(cid:117)
(cid:116)λ +

(cid:90)

d
(cid:88)

(cid:90)

i=1

The Optimized SMMD over the restricted set Ψκ is given by:

Dµ,Ψκ,λ

SMMD (P, Q) := sup
ψ∈Ψκ

SMMDµ,kψ,λ .

The constraint to ψ ∈ Ψκ is critical to the proof. In practice, using a spectral parametrization helps
enforce this assumption, as shown in Figures 2 and 9. Other regularization methods, like orthogonal
normalization [11], are also possible.

We will use the following assumptions:

(I) µ is a probability distribution absolutely continuous with respect to the Lebesgue measure.
(II) The dimensions of the weights are decreasing per layer: dl+1 ≤ dl for all 0 ≤ l ≤ L − 1.
(III) The non-linearity used is Leaky-ReLU, (16), with leak coefﬁcient α ∈ (0, 1).
(IV) The top-level kernel K is globally Lipschitz in the RKHS norm: there exists a positive
constant LK > 0 such that (cid:107)K(a, .) − K(b, .)(cid:107) ≤ LK(cid:107)a − b(cid:107) for all a and b in RdL.

(V) There is some γK > 0 for which K satisﬁes

∇b∇cK(b, c)(cid:12)

(cid:12)(b,c)=(a,a) (cid:23) γ2I

for all a ∈ RdL.

(19)

Assumption (I) ensures that the points where φψ(X) is not differentiable are reached with probability
0 under µ. This assumption can be easily satisﬁed e.g. if we deﬁne µ by adding Gaussian noise to P.
Assumption (II) helps ensure that the span of W l is never contained in the null space of W l+1. Using
Leaky-ReLU as a non-linearity, Assumption (III), further ensures that the network φψ is locally
full-rank almost everywhere; this might not be true with ReLU activations, where it could be always
0. Assumptions (II) and (III) can be easily satisﬁed by design of the network.

Assumptions (IV) and (V) only depend on the top-level kernel K and are easy to satisfy in practice.
In particular, they always hold for a smooth translation-invariant kernel, such as the Gaussian, as well
as the linear kernel.

We are now ready to prove Theorem 1.
Theorem 1. Under Assumptions (I) to (V),

Dµ,Ψκ,λ

SMMD (P, Q) ≤

LK κL/2
√

dL αL/2

γ

W(P, Q),

which implies that if Pn

W−→ P, then Dµ,Ψκ,λ

SMMD (Pn, P) → 0.

Proof. Deﬁne the pseudo-distance corresponding to the kernel kψ

dψ(x, y) = (cid:107)kψ(x, ·) − kψ(y, ·)(cid:107)Hψ =

kψ(x, x) + kψ(y, y) − 2kψ(x, y).

(cid:113)

Denote by W dψ (P, Q) the optimal transport metric between P and Q using the cost dψ, given by

W dψ (P, Q) = inf

E(X,Y )∼π [dψ(X, Y )] .

π∈Π(P,Q)

where Π is the set of couplings with marginals P and Q. By Lemma 7,

MMDψ(P, Q) ≤ W dψ (P, Q).

Recall that φψ is Lipschitz, (cid:107)φψ(cid:107)Lip < ∞, so along with Assumption (IV) we have that

dψ(x, y) ≤ LK(cid:107)φψ(x) − φψ(y)(cid:107) ≤ LK(cid:107)φψ(cid:107)Lip(cid:107)x − y(cid:107).

16

Thus

so that

W dψ (P, Q) ≤ inf

E(X,Y )∼π [LK(cid:107)φψ(cid:107)Lip(cid:107)X − Y (cid:107)] = LK(cid:107)φψ(cid:107)Lip W(P, Q),

π∈Π(P,Q)

where W is the standard Wasserstein distance (2), and so

MMDψ(P, Q) ≤ Lk(cid:107)φψ(cid:107)Lip W(P, Q).

We have that ∂i∂i+dk(x, y) = [∂iφψ(x)]T (cid:104)
∇a∇bK(a, b)(cid:12)
[∂iφψ(y)] , where the
middle term is a dL × dL matrix and the outer terms are vectors of length dL. Thus Assumption (V)
implies that ∂i∂i+dk(x, x) ≥ γ2

(cid:12)(a,b)=(φψ(x),φψ(y))

K(cid:107)∂iφψ(x)(cid:107)2, and hence

(cid:105)

σ−2
µ,k,λ ≥ γ2

K

E[(cid:107)∇φψ(X)(cid:107)2
F ]

SMMD2

ψ(P, Q) = σ2

µ,k,λ MMD2

ψ(P, Q) ≤

K(cid:107)φψ(cid:107)2
L2
E [(cid:107)∇φψ(X)(cid:107)2
F ]

Lip

γ2
K

W 2(P, Q).

Using Lemma 8, we can write φψ(X) = α(ψ)φ ¯ψ(X) with ¯ψ ∈ Ψκ

1 . Then we have

(cid:107)φψ(cid:107)2
Lip
(cid:107)∇φψ(X)(cid:107)2
F

(cid:104)

Eµ

(cid:105) =

α(ψ)2(cid:107)φ ¯ψ(cid:107)2
(cid:104)(cid:13)
(cid:13)∇φ ¯ψ(X)(cid:13)
2
(cid:13)
F

Lip

α(ψ)2 Eµ

(cid:105) ≤

1
(cid:104)(cid:13)
(cid:13)∇φ ¯ψ(X)(cid:13)
2
(cid:13)
F

(cid:105) ,

Eµ

where we used (cid:107)φ ¯ψ(cid:107)Lip ≤ (cid:81)L
(cid:107)∇φ ¯ψ(X)(cid:107)2

F ≥ dL(α/κ)L. Using Assumption (I), this implies that

l=1(cid:107) ¯W l(cid:107) = 1. But by Lemma 9, for Lebesgue-almost all X,

(cid:107)φψ(cid:107)2
Lip
(cid:107)∇φψ(X)(cid:107)2
F

(cid:104)

Eµ

(cid:105) ≤

1
(cid:2)(cid:107)∇φ ¯ψ(X)(cid:107)2

F ](cid:3) ≤

Eµ

κL
dLαL .

Thus for any ψ ∈ Ψκ,

SMMDψ(P, Q) ≤

LK κL/2
√

γK

dL αL/2

W(P, Q).

The desired bound on Dµ,Ψκ,λ

SMMD follows immediately.

Lemma 7. Let (x, y) (cid:55)→ k(x, y) be the continuous kernel of an RKHS H deﬁned on a Polish space X ,
and deﬁne the corresponding pseudo-distance dk(x, y) := (cid:107)k(x, ·) − k(y, ·)(cid:107)H. Then the following
inequality holds for any distributions P and Q on X , including when the quantities are inﬁnite:

MMDk(P, Q) ≤ W dk (P, Q).

Proof. Let P and Q be two probability distributions, and let Π(P, Q) be the set of couplings between
them. Let π∗ ∈ argmin(X,Y )∼π[ck(X, Y )] be an optimal coupling, which is guaranteed to exist
[51, Theorem 4.1]; by deﬁnition W dk (P, Q) = E(X,Y )∼π∗ [dk(X, Y )]. When W dk (P, Q) = ∞ the
inequality trivially holds, so assume that W dk (P, Q) < ∞.
Take a sample (X, Y ) ∼ π(cid:63) and a function f ∈ H with (cid:107)f (cid:107)H ≤ 1. By the Cauchy-Schwarz
inequality,

(cid:107)f (X) − f (Y )(cid:107) ≤ (cid:107)f (cid:107)H(cid:107)k(X, ·) − k(Y, ·)(cid:107)H ≤ (cid:107)k(X, ·) − k(Y, ·)(cid:107)H.

Taking the expectation with respect to π(cid:63), we obtain

Eπ(cid:63) [|f (X) − f (Y )|] ≤ Eπ(cid:63) [(cid:107)k(X, ·) − k(Y, ·)(cid:107)H].
The right-hand side is just the deﬁnition of W dk (P, Q). By Jensen’s inequality, the left-hand side is
lower-bounded by

|Eπ∗ [f (X) − f (Y )]| = |EX∼P[f (X)] − EY ∼Q[f (Y )]|

since π(cid:63) has marginals P and Q. We have shown so far that for any f ∈ H with (cid:107)f (cid:107)H ≤ 1,

the result follows by taking the supremum over f .

|EP[f (X)] − EQ[f (Y )]| ≤ W ck (P, Q);

17

Lemma 8. Let ψ = ((W L, bL), (W L−1, bL−1), . . . , (W 1, b1)) ∈ Ψκ. There exists a corresponding
scalar α(ψ) and ¯ψ = (( ¯W L, ¯bL), ( ¯W L−1, ¯bL−1), . . . , ( ¯W 1, ¯b1)) ∈ Ψκ
1 , deﬁned by (18), such that
for all X,

φψ(X) = α(ψ) φ ¯ψ(X).

Proof. Set ¯W l = 1
1
m=1(cid:107)W m(cid:107)
number is unchanged, cond( ¯W l) = cond(W l) ≤ κ, and (cid:107) ¯W l(cid:107) = 1, so ¯ψ ∈ Φκ
see from (16) that

l=1(cid:107)W l(cid:107). Note that the condition
1 . It is also easy to

(cid:107)W l(cid:107) W l, ¯bl =

bl, and α(ψ) = (cid:81)L

(cid:81)l

so that

hl
¯ψ(X) =

1
m=1(cid:107)W m(cid:107)

(cid:81)l

hl
ψ(X)

α(ψ)hL

¯ψ(X) =

hL
ψ(X) = φψ(X).

(cid:81)L

(cid:81)L

l=1(cid:107)W l(cid:107)
l=1(cid:107)W l(cid:107)

Lemma 9. Make Assumptions (II) and (III), and let ψ ∈ Ψκ
intermediate activation is exactly zero,

1 . Then the set of inputs for which any

has zero Lebesgue measure. Moreover, for any X /∈ N ψ, ∇X φψ(X) exists and

Nψ =

L
(cid:91)

dl(cid:91)

l=1

k=1

(cid:110)
X ∈ Rd | (cid:0)hl

ψ(X)(cid:1)

k

(cid:111)

= 0

,

(cid:107)∇X φψ(X)(cid:107)2

F ≥

dLαL
κL .

(M l

X )k = σ(cid:48)

l(hl

k(X)) =

(cid:26)1 hl
α hl

k(X) > 0
k(X) < 0

;

Proof. First, note that the network representation at layer l is piecewise afﬁne. Speciﬁcally, deﬁne
M l

X ∈ Rdl by, using Assumption (III),

it is undeﬁned when any hl

k(X) = 0, i.e. when X ∈ N ψ. Let V l

ψ(X) = W lσl−1(hl−1
hl

ψ (X)) + bl = V l

X := W l diag (cid:0)M l−1
X X + bl,

X

(cid:1). Then

ψ(X) = Wl
hl
X = V l
X = V l
1 , we have (cid:107)W l(cid:107) = 1 and σmin(W l) ≥ 1/κ; also, (cid:107)M l
X (cid:107) ≤ 1, and using Assumption (II) with Lemma 10 gives σmin(Wl

X X + bl
X V l−1

X bl−1 + bl, and Wl

X · · · V 1

X ,

X , so long as X /∈ N ψ.

X (cid:107) ≤ 1, σmin(M l

X ) ≥ α.
X ) ≥ (α/κ)l. In

(20)

and thus

where b0

X = 0, bl
Because ψ ∈ Ψκ
Thus (cid:107)Wl
particular, each Wl

Next, note that bl
H l

X = (M l

X , M l−1

X is full-rank.
X and Wl
X , . . . , M 1

X each only depend on X through the activation patterns M l
X ) denote the full activation patterns up to level l, we can thus write

X . Letting

ψ(X) = WH l
hl

X X + bH l

X .

There are only ﬁnitely many possible values for H l
we have that

X ; we denote the set of such values as Hl. Then

L
(cid:91)

dL(cid:91)

(cid:91)

Nψ ⊆

l=0

k=1

H l∈Hl

(cid:110)
X ∈ Rd | WH l

k X + bH l

k = 0

(cid:111)

.

Because each WH l
is of rank dl, each set in the union is either empty or an afﬁne subspace of
k
dimension d − dl. As each dl > 0, each set in the ﬁnite union has zero Lebesgue measure, and Nψ
also has zero Lebesgue measure.

18

We will now show that the activation patterns are piecewise constant, so that ∇X hl
1 , we have (cid:107)hl
all X /∈ N ψ. Because ψ ∈ Ψκ
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12) ≤ (cid:107)X − X (cid:48)(cid:107).
(cid:12)

ψ(cid:107)Lip ≤ 1, and in particular

ψ(X (cid:48))(cid:1)

ψ(X)(cid:1)

− (cid:0)hl

(cid:0)hl

k

k

ψ(X) = WH l

X for

Thus,
minl=1,...,L mink=1,...,dl
for all l and k,

take some X /∈ N ψ, and ﬁnd the smallest absolute value of its activations, (cid:15) =
(cid:12)
(cid:12)
(cid:12); clearly (cid:15) > 0. For any X (cid:48) with (cid:107)X − X (cid:48)(cid:107) < (cid:15), we know that

(cid:17)
hl
ψ(X)

(cid:12)
(cid:12)
(cid:12)

(cid:16)

k

sign

(cid:16)(cid:0)hl

ψ(X)(cid:1)

(cid:17)

k

= sign

(cid:16)(cid:0)hl

ψ(X (cid:48))(cid:1)

(cid:17)

,

k

implying that H l
Finally, we obtain

X = H l

X (cid:48) as well as X (cid:48) /∈ N ψ. Thus for any point X /∈ N ψ, ∇φψ(X) = WH L
X .

(cid:107)∇φψ(X)(cid:107)2

F = (cid:107)WH L

X (cid:107)2

F ≥ dL σmin

(cid:16)

WH L

X

(cid:17)2

≥

dLαL
κL .

Lemma 10. Let A ∈ Rm×n, B ∈ Rn×p, with m ≥ n ≥ p. Then σmin(AB) ≥ σmin(A) σmin(B).

Proof. A more general version of this result can be found in [19, Theorem 2]; we provide a proof
here for completeness.
If B has a nontrivial null space, σmin(B) = 0 and the inequality holds. Otherwise, let Rn
Rn \ {0}. Recall that for C ∈ Rm×n with m ≥ n,
(cid:115)

∗ denote

(cid:113)

σmin(C) =

λmin(C TC) =

xTC TCx
xTx

inf
x∈Rn
∗

= inf
x∈Rn
∗

(cid:107)Cx(cid:107)
(cid:107)x(cid:107)

.

Thus, as Bx (cid:54)= 0 for x (cid:54)= 0,

σmin(AB) = inf
x∈Rp
∗
(cid:18)

(cid:107)ABx(cid:107)
(cid:107)x(cid:107)
(cid:107)ABx(cid:107)
(cid:107)Bx(cid:107)
(cid:107)Ay(cid:107)
(cid:107)y(cid:107)

= inf
x∈Rp
∗

(cid:19) (cid:18)

(cid:107)ABx(cid:107)(cid:107)Bx(cid:107)
(cid:107)Bx(cid:107)(cid:107)x(cid:107)
(cid:19)
(cid:107)Bx(cid:107)
inf
(cid:107)x(cid:107)
x∈Rp
∗
(cid:19)
(cid:107)Bx(cid:107)
(cid:107)x(cid:107)

inf
x∈Rp
∗

(cid:19) (cid:18)

≥

≥

(cid:18)

inf
x∈Rp
∗

inf
y∈Rn
∗

= σmin(A) σmin(B).

A.2.1 When some of the assumptions don’t hold

Here we analyze through simple examples what happens when the condition number can be un-
bounded, and when Assumption (II), about decreasing widths of the network, is violated.

Condition Number: We start by a ﬁrst example where the condition number can be arbitrarily
high. We consider a two-layer network on R2, deﬁned by

φα(X) = [1 −1] σ(WαX)

Wα =

(21)

(cid:20)1
1

(cid:21)

1
1 + α

where α > 0. As α approaches 0 the matrix Wα becomes singular which means that its condition
number blows up. We are interested in analyzing the behavior of the Lipschitz constant of φ and the
expected squared norm of its gradient under µ as α approaches 0.

One can easily compute the squared norm of the gradient of φ which is given by

(cid:107)∇φα(X)(cid:107)2 =






α2
X ∈ A1
γ2α2
X ∈ A2
(1 − γ)2 + (1 + α − γ)2 X ∈ A3
(1 − γ)2 + (γα + γ − 1)2 X ∈ A4

(22)

19

Here A1, A2, A3 and A4 are deﬁned by (23) and are represented in Figure 4:
A1 := {X ∈ R2|X1 + X2 ≥ 0 X1 + (1 + α)X2 ≥ 0}
A2 := {X ∈ R2|X1 + X2 < 0 X1 + (1 + α)X2 < 0}
A3 := {X ∈ R2|X1 + X2 < 0 X1 + (1 + α)X2 ≥ 0}
A4 := {X ∈ R2|X1 + X2 ≥ 0 X1 + (1 + α)X2 < 0}

(23)

(24)

(25)

(26)

Figure 4: Decomposition of R2 into 4 regions A1, A2, A3 and A4 as deﬁned in (23). As α approaches
0, the area of sets A3 and A4 becomes negligible.

It is easy to see that whenever µ has a density, the probability of the sets A3 and A4 goes to 0 are
α → 0. Hence one can deduce that Eµ[(cid:107)∇φα(X)(cid:107)2] → 0 when α → 0. On the other hand, the
squared Lipschitz constant of φ is given by (1 − γ)2 + (1 + α − γ)2 which converges to 2(1 − γ)2.
This shows that controlling the expectation of the gradient doesn’t allow to effectively control the
Lipschitz constant of φ.

Monotonicity of the dimensions: We would like to consider a second example where Assump-
tion (II) doesn’t hold. Consider the following two layer network deﬁned by:
(cid:35)

φ(X) = [−1

0

1] σ(WβX)

Wβ :=

(cid:34)1
0
1
0
1 β

for β > 0. Note that Wβ is a full rank matrix, but Assumption (II) doesn’t hold. Depending on the
sign of the components of WβX one has the following expression for (cid:107)∇φα(X)(cid:107)2:

where (Bi)1≤i≤6 are deﬁned by (26)

(cid:107)∇φα(X)(cid:107)2 =






β2
X ∈ B1
γ2β2
X ∈ B2
β2
X ∈ B3
(1 − γ)2 + γ2β2 X ∈ B4
(1 − γ)2 + β2
X ∈ B5
γ2β2
X ∈ B6

B1 := {X ∈ R2|X1 ≥ 0 X2 ≥ 0}
B2 := {X ∈ R2|X1 < 0 X2 < 0}
B3 := {X ∈ R2|X1 ≥ X2 < 0 X1 + βX2 ≥ 0}
B4 := {X ∈ R2|X1 ≥ X2 < 0 X1 + βX2 < 0}
B5 := {X ∈ R2|X1 > 0 X2 ≥ 0 X1 + βX2 ≥ 0}
B6 := {X ∈ R2|X1 > 0 X2 ≥ 0 X1 + βX2 < 0}

20

The squared Lipschitz constant is given by (cid:107)φ(cid:107)2
the gradient of φ is given by:

L(1 − γ)2 + β2 while the expected squared norm of

Eµ[(cid:107)φ(X)(cid:107)2] = 3β2(p(B1 ∪ B3 ∪ B5) + γ2p(B2 ∪ B4 ∪ B6)) + (1 − γ)2p(B4 ∪ B5).

(27)
Again the set B4 ∪ B5 becomes negligible as β approaches 0 which implies that Eµ[(cid:107)φ(X)(cid:107)2] → 0.
L converges to (1 − γ)2. Note that unlike in the ﬁrst example in (21), the
On the other hand (cid:107)φ(cid:107)2
matrix Wβ has a bounded condition number. In this example, the columns of W0 are all in the null
1], which implies ∇φ0(X) = 0 for all X ∈ R2, even though all matrices have full
space of [−1 0
rank.

B DiracGAN vector ﬁelds for more losses

Figure 5: Vector ﬁelds for different losses with respect to the generator parameter θ and the feature
representation parameter ψ; the losses use a Gaussian kernel, and are shown in (28). Following [30],
P = δ0, Q = δθ and φψ(x) = ψx. The curves show the result of taking simultaneous gradient steps
in (θ, ψ) beginning from three initial parameter values.

Figure 5 shows parameter vector ﬁelds, like those in Figure 6, for Example 1 for a variety of different
losses:

MMD: − MMD2
ψ
MMD-GP: − MMD2
ψ +λ EP[((cid:107)∇f (X)(cid:107) − 1)2]
MMD-GP-Unif: − MMD2
ψ +λ E

(cid:101)X(cid:39)µ∗ [((cid:107)∇f ( (cid:101)X)(cid:107) − 1)2]

SN-MMD: − 2 MMD1(P, Q)2

Sobolev-MMD: − MMD2
CenteredSobolev-MMD: − MMD2

ψ +λ(E(P+Q)/2[(cid:107)∇f (X)(cid:107)2] − 1)2
ψ +λ(E(P+Q)/2[(cid:107)∇f (X)(cid:107)2])2

(28)

LipMMD: − LipMMD2
GC-MMD: − GCMMD2
SMMD: − SMMD2

kψ,λ

kψ,P,λ

N (0,102),kψ,λ

21

2

The squared MMD between δ0 and δθ under a Gaussian kernel of bandwidth 1/ψ and is given by
2(1 − e− ψ2θ2
). MMD-GP-unif uses a gradient penalty as in [7] where each samples from µ∗ is
obtained by ﬁrst sampling X and Y from P and Q and then sampling uniformly between X and
Y . MMD-GP uses the same gradient penalty, but the expectation is taken under P rather than µ∗.
SN-MMD refers to MMD with spectral normalization; here this means that ψ = 1. Sobolev-MMD
refers to the loss used in [33] with the quadratic penalty only. GCMMDµ,k,λ is deﬁned by (5), with
µ = N (0, 102).

C Vector ﬁelds of Gradient-Constrained MMD and Sobolev GAN critics

Mroueh et al. [33] argue that the gradient of the critic (...) deﬁnes a transportation plan for moving
the distribution mass (from generated to reference distribution) and present the solution of Sobolev
PDE for 2-dimensional Gaussians. We observed that in this simple example the gradient of the
Sobolev critic can be very high outside of the areas of high density, which is not the case with the
Gradient-Constrained MMD. Figure 6 presents critic gradients in both cases, using µ = (P + Q)/2
for both.

(a) Gradient-Constrained MMD critic gradient.

(b) Sobolev IPM critic gradient.

Figure 6: Vector ﬁelds of critic gradients between two Gaussians. The grey arrows show normalized
gradients, i.e. gradient directions, while the black ones are the actual gradients. Note that for the
Sobolev critic, gradients norms are orders of magnitudes higher on the right hand side of the plot
than in the areas of high density of the given distributions.

This unintuitive behavior is most likely related to the vanishing boundary condition, assummed by
Sobolev GAN. Solving the actual Sobolev PDE, we found that the Sobolev critic has very high
gradients close to the boundary in order to match the condition; moreover, these gradients point in
opposite directions to the target distribution.

D An estimator for Lipschitz MMD

We now describe brieﬂy how to estimate the Lipschitz MMD in low dimensions. Recall that

LipMMDk,λ(P, Q) =

f ∈Hk : (cid:107)f (cid:107)2

sup
Lip+λ(cid:107)f (cid:107)2

≤1

Hk

EX∼P[f (X)] − EX∼Q[f (Y )].

For f ∈ Hk, it is the case that

(cid:107)f (cid:107)2

Lip = sup
x∈Rd

(cid:107)∇f (x)(cid:107)2 = sup
x∈Rd

(cid:104)∂ik(x, ·), f (cid:105)2

Hk

= sup
x∈Rd

f,

[∂ik(x, ·) ⊗ ∂ik(x, ·)] f

.

d
(cid:88)

i=1

(cid:42)

d
(cid:88)

i=1

(cid:43)

Hk

Thus we can approximate the constraint (cid:107)f (cid:107)2
Lip + λ(cid:107)f (cid:107)2
≤ 1 by enforcing the constraint on a set
Hk
of m points {Zi} reasonably densely covering the region around the supports of P and Q, rather

22

than enforcing it at every point in X . An estimator of the Lipschitz MMD based on X ∼ PnX and
Y ∼ QnY is

(cid:92)LipMMDk,λ (X, Y, Z) ≈ sup

nX(cid:88)

1
nX

f (Xj) −

nY(cid:88)

f (Yj)

1
nY

f ∈Hk

j=1

j=1
s.t. ∀j, (cid:107)∇f (Zj)(cid:107)2 + λ(cid:107)f (cid:107)2

≤ 1.

Hk

(29)

By the generalized representer theorem, the optimal f for (29) will be of the form

f (·) =

αjk(Xj, ·) +

βjk(Yj, ·) +

γ(i,j)∂ik(Zj, ·).

nX(cid:88)

j=1

nY(cid:88)

j=1

d
(cid:88)

m
(cid:88)

i=1

j=1

Writing δ = (α, β, γ), the objective function is linear in δ,
· · · − 1
nY

− 1
nY

(cid:2) 1
nX

1
nX

· · ·

0

· · ·

0(cid:3) δ.

The constraints are quadratic, built from the following matrices, where the X and Y samples are
concatenated together, as are the derivatives with each dimension of the Z samples:
















K :=

B :=

H :=

k(X1, X1)
...
k(YnX , X1)
∂1k(Z1, X1)
...
∂dk(Zm, X1)

∂1∂1+dk(Z1, Z1)
...
∂d∂1+dk(Zm, Z1)

· · ·
. . .
· · ·

k(X1, YnY )
...
k(YnY , YnY )






· · ·
. . .
· · ·






∂1k(Z1, YnY )
...
∂dk(Zm, YnY )
· · ·
. . .
· · ·

∂1∂d+dk(Z1, Zm)
...
∂d∂d+dk(Zm, Zm)




 .

Given these matrices, and letting Oj = (cid:80)d
vector in Rmd, we have that
(cid:21)
(cid:20)K BT
B H

= δT

(cid:107)f (cid:107)2

Hk

δ

(cid:107)∇f (Zj)(cid:107)2 =

i=1 e(i,j)eT

(i,j) where e(i,j) is the (i, j)th standard basis

d
(cid:88)

i=1

(∂if (Zj))2 = δT

(cid:20)BTOjB BTOjH
HOjB HOjH

(cid:21)

δ.

Thus the optimization problem (29) is a linear problem with convex quadratic constraints, which can
be solved by standard convex optimization software. The approximation is reasonable only if we can
effectively cover the region of interest with densely spaced {Zi}; it requires a nontrivial amount of
computation even for the very simple 1-dimensional toy problem of Example 1.

One advantage of this estimator, though, is that ﬁnding its derivative with respect to the input points
or the kernel parameterization is almost free once we have computed the estimate, as long as our
solver has computed the dual variables µ corresponding to the constraints in (29). We just need to
exploit the envelope theorem and then differentiate the KKT conditions, as done for instance in [1].
The differential of (29) ends up being, assuming the optimum of (29) is at ˆδ ∈ RnX +nY +md and
ˆµ ∈ Rm,

d (cid:92)LipMMDk,λ(X, Y, Z) = ˆδT

· · ·

1
nX

− 1
nY

· · · − 1
nY

(cid:3)T

−

ˆµj

ˆδT(dPj)ˆδ

(cid:21)

(cid:20)dK
dB

(cid:2) 1
nX

m
(cid:88)

j=1

dPj :=

(cid:20)(dB)TOjB + BTOj(dH)
(dH)OjB + HOj(dB)

(dB)TOjH + BTOj(dH)
(dH)OjH + HOj(dH)

(cid:21)

+ λ

(cid:20)dK dBT
dB dH

(cid:21)

.

E Near-equivalence of WGAN and linear-kernel MMD GANs

For an MMD GAN-GP with kernel k(x, y) = φ(x)φ(y), we have that
MMDk(P, Q) = |EP φ(x) − EQ φ(Y )|

23

and the corresponding critic function is

η(t)
(cid:107)η(cid:107)H

=

EX∼P φ(X)φ(t) − EY ∼Q φ(Y )φ(t)
|EP φ(X) − EQ φ(Y )|

= sign (EX∼P φ(X) − EY ∼Q φ(Y )) φ(t).

Thus if we assume EX∼P φ(X) > EY ∼Q φ(Y ), as that is the goal of our critic training, we see that
the MMD becomes identical to the WGAN loss, and the gradient penalty is applied to the same
function.
(MMD GANs, however, would typically train on the unbiased estimator of MMD2, giving a very
slightly different loss function. [7] also applied the gradient penalty to η rather than the true critic
η/(cid:107)η(cid:107).)

The SMMD with a linear kernel is thus analogous to applying the scaling operator to a WGAN; hence
the name SWGAN.

F Additional experiments

F.1 Comparison of Gradient-Constrained MMD to Scaled MMD

Figure 7 shows the behavior of the MMD, the Gradient-Constrained SMMD, and the Scaled MMD
when comparing Gaussian distributions. We can see that MMD ∝ SMMD and the Gradient-
Constrained MMD behave similarly in this case, and that optimizing the SMMD and the Gradient-
Constrained MMD is also similar. Optimizing the MMD would yield an essentially constant distance.

F.2 IGMs with Optimized Gradient-Constrained MMD loss

We implemented the estimator of Proposition 3 using the empirical mean estimator of η, and sharing
samples for µ = P. To handle the large but approximately low-rank matrix system, we used an
incomplete Cholesky decomposition [43, Algorithm 5.12] to obtain R ∈ R(cid:96)×M (1+d) such that
(cid:20)K GT
G H

≈ RTR. Then the Woodbury matrix identity allows an efﬁcient evaluation:

(cid:21)

(cid:0)RTR + M λI(cid:1)−1

=

(cid:0)I − R(RRT + M λI)−1R(cid:1) .

1
M λ

Even though only a small (cid:96) is required for a good approximation, and the full matrices K, G, and
H need never be constructed, backpropagation through this procedure is slow and not especially
GPU-friendly; training on CPU was faster. Thus we were only able to run the estimator on MNIST,
and even that took days to conduct the optimization on powerful workstations.

The learned models, however, were reasonable. Using a DCGAN architecture, batches of size 64,
and a procedure that otherwise agreed with the setup of Section 4, samples with and without spectral
normalization are shown in Figures 8a and 8b. After the points in training shown, however, the same
rank collapse as discussed in Section 4 occurred. Here it seems that spectral normalization may have
delayed the collapse, but not prevented it. Figure 8c shows generator loss estimates through training,
including the obvious peak at collapse; Figure 8d shows KID scores based on the MNIST-trained
convnet representation [7], including comparable SMMD models for context. The fact that SMMD
models converged somewhat faster than Gradient-Constrained MMD models here may be more
related to properties of the estimator of Proposition 3 rather than the distances; more work would be
needed to fully compare the behavior of the two distances.

F.3 Spectral normalization and Scaled MMD

Figure 9 shows the distribution of critic weight singular values, like Figure 2, at more layers. Figure 11
and Table 2 show results for the spectral normalization variants considered in the experiments.
MMDGAN, with neither spectral normalization nor a gradient penalty, did surprisingly well in this
case, though it fails badly in other situations.

Figure 9 compares the decay of singular values for layer of the critic’s network at both early and
later stages of training in two cases: with or without the spectral parametrization. The model was
trained on CelebA using SMMD. Figure 11 shows the evolution per iteration of Inception score,

24

Figure 7: Plots of various distances between one dimensional Gaussians, where P = N (0, 0.12), and
the colors show log D(P, N (µ, σ2)). All distances use λ = 1. Top left: MMD with a Gaussian kernel
of bandwidth ψ = 0.1. Top right: MMD with bandwidth ψ = 10. Middle left: Gradient-Constrained
MMD with bandwidth ψ = 0.1. Middle right: Gradient-Constrained MMD with bandwidth ψ = 10.
Bottom left: Optimized SSMD, allowing any ψ ∈ R. Bottom right: Optimized Gradient-Constrained
MMD.

25

(a) Without spectral
normalization; 32 000
generator iterations.

(b) With
spectral
normalization; 41 000
generator iterations.

Figure 8: The MNIST models with Optimized Gradient-Constrained MMD loss.

(c) Generator losses.

(d) KID scores.

FID and KID for Sobolev-GAN, MMDGAN and variants of MMDGAN and WGAN using spectral
normalization. It is often the case that this parametrization alone is not enough to achieve good
results.

Figure 9: Singular values at different layers, for the same setup as Figure 2.

F.4 Additional samples

Figures 12 and 13 give extra samples from the models.

26

µ,k,λ MMD2

k for SMMDGAN and SN-SMMDGAN, and MMD2

Figure 10: Evolution of various quantities per generator iteration on CelebA during training. 4
models are considered: (SMMDGAN, SN-SMMDGAN, MMDGAN, SN-MMDGAN). (a) Loss:
SMMD2 = σ2
k for MMDGAN and
SN-MMDGAN. The loss saturates for MMDGAN (green); spectral normalization allows some
improvement in loss, but training is still unstable (orange). SMMDGAN and SN-SMMDGAN both
lead to stable, fast training (blue and red). (b) SMMD controls the critic complexity well, as expected
(blue and red); SN has little effect on the complexity (orange). (c) Ratio of the highest singular value
to the smallest for the ﬁrst layer of the critic network: σmax/σmin. SMMD tends to increase the
condition number of the weights during training (blue), while SN helps controlling it (red). (d) KID
score during training: Only variants using SMMD lead to stable training in this case.

Figure 11: Evolution per iteration of different scores for variants of methods, mostly using spectral
normalization, on CIFAR-10.

27

Table 2: Mean (standard deviation) of score evaluations on CIFAR-10 for different methods using
Spectral Normalization.

Method

MMDGAN
SN-WGAN
SN-WGAN-GP
SN-Sobolev-GAN
SN-MMDGAN-GP
SN-MMDGAN-L2
SN-MMDGAN
SN-MMDGAN-GP-L2
SN-SMMDGAN

IS

FID

KID×103

5.5±0.0
2.2±0.0
2.5±0.0
2.9±0.0
4.6±0.1
7.1±0.1
6.9±0.1
6.9±0.2
7.3±0.1

73.9±0.1
208.5±0.2
154.3±0.2
140.2±0.2
96.8±0.4
31.9±0.2
31.5±0.2
32.3±0.3
25.0±0.3

39.4±1.5
178.9±1.5
125.3±0.9
130.0±1.9
59.5±1.4
21.7±0.9
21.7±1.0
20.9±1.1
16.6±2.0

Figure 12: Samples from a generator trained on ImageNet dataset using Scaled MMD with Spectral
Normalization: SN-SMMDGAN.

28

(a) SNGAN

(b) SobolevGAN

(c) MMDGAN-GP-L2

(d) SN-SMMD GAN

(e) SN SWGAN

(f) SMMD GAN

Figure 13: Comparison of samples from different models trained on CelebA with 160×160 resolution.

29

8
1
0
2
 
v
o
N
 
9
2
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
5
6
5
1
1
.
5
0
8
1
:
v
i
X
r
a

On gradient regularizers for MMD GANs

Michael Arbel∗
Gatsby Computational Neuroscience Unit
University College London
michael.n.arbel@gmail.com

Dougal J. Sutherland∗
Gatsby Computational Neuroscience Unit
University College London
dougal@gmail.com

Mikołaj Bi ´nkowski
Department of Mathematics
Imperial College London
mikbinkowski@gmail.com

Arthur Gretton
Gatsby Computational Neuroscience Unit
University College London
arthur.gretton@gmail.com

Abstract

We propose a principled method for gradient-based regularization of the critic of
GAN-like models trained by adversarially optimizing the kernel of a Maximum
Mean Discrepancy (MMD). We show that controlling the gradient of the critic
is vital to having a sensible loss function, and devise a method to enforce exact,
analytical gradient constraints at no additional cost compared to existing approxi-
mate techniques based on additive regularizers. The new loss function is provably
continuous, and experiments show that it stabilizes and accelerates training, giving
image generation models that outperform state-of-the art methods on 160 × 160
CelebA and 64 × 64 unconditional ImageNet.

1

Introduction

There has been an explosion of interest in implicit generative models (IGMs) over the last few years,
especially after the introduction of generative adversarial networks (GANs) [16]. These models
allow approximate samples from a complex high-dimensional target distribution P, using a model
distribution Qθ, where estimation of likelihoods, exact inference, and so on are not tractable. GAN-
type IGMs have yielded very impressive empirical results, particularly for image generation, far
beyond the quality of samples seen from most earlier generative models [e.g. 18, 22, 23, 24, 38].

These excellent results, however, have depended on adding a variety of methods of regularization and
other tricks to stabilize the notoriously difﬁcult optimization problem of GANs [38, 42]. Some of
this difﬁculty is perhaps because when a GAN is viewed as minimizing a discrepancy DGAN(P, Qθ),
its gradient ∇θ DGAN(P, Qθ) does not provide useful signal to the generator if the target and model
distributions are not absolutely continuous, as is nearly always the case [2].

An alternative set of losses are the integral probability metrics (IPMs) [36], which can give credit to
models Qθ “near” to the target distribution P [3, 8, Section 4 of 15]. IPMs are deﬁned in terms of a
critic function: a “well behaved” function with large amplitude where P and Qθ differ most. The IPM
is the difference in the expected critic under P and Qθ, and is zero when the distributions agree. The
Wasserstein IPMs, whose critics are made smooth via a Lipschitz constraint, have been particularly
successful in IGMs [3, 14, 18]. But the Lipschitz constraint must hold uniformly, which can be hard
to enforce. A popular approximation has been to apply a gradient constraint only in expectation [18]:
the critic’s gradient norm is constrained to be small on points chosen uniformly between P and Q.

Another class of IPMs used as IGM losses are the Maximum Mean Discrepancies (MMDs) [17],
as in [13, 28]. Here the critic function is a member of a reproducing kernel Hilbert space (except
in [50], who learn a deep approximation to an RKHS critic). Better performance can be obtained,

∗These authors contributed equally.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.

however, when the MMD kernel is not based directly on image pixels, but on learned features of
images. Wasserstein-inspired gradient regularization approaches can be used on the MMD critic
when learning these features: [27] uses weight clipping [3], and [5, 7] use a gradient penalty [18].

The recent Sobolev GAN [33] uses a similar constraint on the expected gradient norm, but phrases it
as estimating a Sobolev IPM rather than loosely approximating Wasserstein. This expectation can be
taken over the same distribution as [18], but other measures are also proposed, such as (P + Qθ) /2.
A second recent approach, the spectrally normalized GAN [32], controls the Lipschitz constant of
the critic by enforcing the spectral norms of the weight matrices to be 1. Gradient penalties also
beneﬁt GANs based on f -divergences [37]: for instance, the spectral normalization technique of [32]
can be applied to the critic network of an f -GAN. Alternatively, a gradient penalty can be deﬁned
to approximate the effect of blurring P and Qθ with noise [40], which addresses the problem of
non-overlapping support [2]. This approach has recently been shown to yield locally convergent
optimization in some cases with non-continuous distributions, where the original GAN does not [30].

In this paper, we introduce a novel regularization for the MMD GAN critic of [5, 7, 27], which
directly targets generator performance, rather than adopting regularization methods intended to
approximate Wasserstein distances [3, 18]. The new MMD regularizer derives from an approach
widely used in semi-supervised learning [10, Section 2], where the aim is to deﬁne a classiﬁcation
function f which is positive on P (the positive class) and negative on Qθ (negative class), in the
absence of labels on many of the samples. The decision boundary between the classes is assumed
to be in a region of low density for both P and Qθ: f should therefore be ﬂat where P and Qθ have
support (areas with constant label), and have a larger slope in regions of low density. Bousquet et al.
[10] propose as their regularizer on f a sum of the variance and a density-weighted gradient norm.
We adopt a related penalty on the MMD critic, with the difference that we only apply the penalty on P:
thus, the critic is ﬂatter where P has high mass, but does not vanish on the generator samples from Qθ
(which we optimize). In excluding Qθ from the critic function constraint, we also avoid the concern
raised by [32] that a critic depending on Qθ will change with the current minibatch – potentially
leading to less stable learning. The resulting discrepancy is no longer an integral probability metric:
it is asymmetric, and the critic function class depends on the target P being approximated.

We ﬁrst discuss in Section 2 how MMD-based losses can be used to learn implicit generative models,
and how a naive approach could fail. This motivates our new discrepancies, introduced in Section 3.
Section 4 demonstrates that these losses outperform state-of-the-art models for image generation.

2 Learning implicit generative models with MMD-based losses

An IGM is a model Qθ which aims to approximate a target distribution P over a space X ⊆ Rd.
We will deﬁne Qθ by a generator function Gθ : Z → X , implemented as a deep network with
parameters θ, where Z is a space of latent codes, say R128. We assume a ﬁxed distribution on Z,
say Z ∼ Uniform (cid:0)[−1, 1]128(cid:1), and call Qθ the distribution of Gθ(Z). We will consider learning by
minimizing a discrepancy D between distributions, with D(P, Qθ) ≥ 0 and D(P, P) = 0, which we
call our loss. We aim to minimize D(P, Qθ) with stochastic gradient descent on an estimator of D.
In the present work, we will build losses D based on the Maximum Mean Discrepancy,

MMDk(P, Q) =

EX∼P[f (X)] − EY ∼Q[f (Y )],

(1)

sup
f : (cid:107)f (cid:107)Hk ≤1

an integral probability metric where the critic class is the unit ball within Hk, the reproducing
kernel Hilbert space with a kernel k. The optimization in (1) admits a simple closed-form optimal
critic, f ∗(t) ∝ EX∼P[k(X, t)] − EY ∼Q[k(Y, t)]. There is also an unbiased, closed-form estimator of
MMD2
k with appealing statistical properties [17] – in particular, its sample complexity is independent
of the dimension of X , compared to the exponential dependence [52] of the Wasserstein distance

W(P, Q) =

sup
f : (cid:107)f (cid:107)Lip≤1

EX∼P[f (X)] − EY ∼Q[f (Y )].

(2)

The MMD is continuous in the weak topology for any bounded kernel with Lipschitz embeddings [46,
D−→ P, then MMD(Pn, P) → 0.
Theorem 3.2(b)], meaning that if Pn converges in distribution to P, Pn
W−→ P implies
(W is continuous in the slightly stronger Wasserstein topology [51, Deﬁnition 6.9]; Pn

2

D−→ P, and the two notions coincide if X is bounded.) Continuity means the loss can provide
Pn
better signal to the generator as Qθ approaches P, as opposed to e.g. Jensen-Shannon where the loss
could be constant until suddenly jumping to 0 [e.g. 3, Example 1]. The MMD is also strict, meaning
it is zero iff P = Qθ, for characteristic kernels [45]. The Gaussian kernel yields an MMD both
continuous in the weak topology and strict. Thus in principle, one need not conduct any alternating
optimization in an IGM at all, but merely choose generator parameters θ to minimize MMDk.

Despite these appealing properties, using simple pixel-level kernels leads to poor generator samples
[8, 13, 28, 48]. More recent MMD GANs [5, 7, 27] achieve better results by using a parameterized
family of kernels, {kψ}ψ∈Ψ, in the Optimized MMD loss previously studied by [44, 46]:

DΨ

MMD(P, Q) := sup
ψ∈Ψ

MMDkψ (P, Q).

(3)

We primarily consider kernels deﬁned by some ﬁxed kernel K on top of a learned low-dimensional
representation φψ : X → Rs, i.e. kψ(x, y) = K(φψ(x), φψ(y)), denoted kψ = K ◦ φψ. In practice,
K is a simple characteristic kernel, e.g. Gaussian, and φψ is usually a deep network with output
dimension say s = 16 [7] or even s = 1 (in our experiments). If φψ is powerful enough, this choice
is sufﬁcient; we need not try to ensure each kψ is characteristic, as did [27].
Proposition 1. Suppose k = K ◦ φψ, with K characteristic and {φψ} rich enough that for any
P (cid:54)= Q, there is a ψ ∈ Ψ for which φψ#P (cid:54)= φψ#Q.2 Then if P (cid:54)= Q, DΨ
Proof. Let ˆψ ∈ Ψ be such that φ ˆψ(P) (cid:54)= φ ˆψ(Q). Then, since K is characteristic,

MMD(P, Q) > 0.

DΨ

MMD(P, Q) = sup
ψ∈Ψ

MMDK(φψ#P, φψ#Q) ≥ MMDK(φ ˆψ#P, φ ˆψ#Q) > 0.

MMD, one can conduct alternating optimization to estimate a ˆψ and then update the
To estimate DΨ
generator according to MMDk ˆψ
, similar to the scheme used in GANs and WGANs. (This form of
estimator is justiﬁed by an envelope theorem [31], although it is invariably biased [7].) Unlike DGAN
or W, ﬁxing a ˆψ and optimizing the generator still yields a sensible distance MMDk ˆψ
Early attempts at minimizing DΨ
could be because for some kernel classes, DΨ
Example 1 (DiracGAN [30]). We wish to model a point mass at the origin of R, P = δ0, with any
possible point mass, Qθ = δθ for θ ∈ R. We use a Gaussian kernel of any bandwidth, which can be
2 (a − b)2(cid:1). Then
written as kψ = K ◦ φψ with φψ(x) = ψx for ψ ∈ Ψ = R and K(a, b) = exp (cid:0)− 1
θ (cid:54)= 0
2
θ = 0

MMD in an IGM, though, were unsuccessful [48, footnote 7]. This
MMD is stronger than Wasserstein or MMD.

(δ0, δθ) = 2 (cid:0)1 − exp (cid:0)− 1

MMD(δ0, δθ) =

2 ψ2θ2(cid:1)(cid:1) ,

MMD2
kψ

(cid:26)√
0

DΨ

.

.

Considering DΨ
distance is not continuous in the weak or Wasserstein topologies.

2 (cid:54)→ 0, even though δ1/n

MMD(δ0, δ1/n) =

W−→ δ0, shows that the Optimized MMD

√

This also causes optimization issues. Figure 1 (a) shows gradient vector ﬁelds in parameter space,
(δ0, δθ)(cid:1). Some sequences following v (e.g. A)
v(θ, ψ) ∝ (cid:0) − ∇θ MMD2
(δ0, δθ), ∇ψ MMD2
kψ
kψ
converge to an optimal solution (0, ψ), but some (B) move in the wrong direction, and others (C) are
stuck because there is essentially no gradient. Figure 1 (c, red) shows that the optimal DΨ
MMD critic
is very sharp near P and Q; this is less true for cases where the algorithm converged.
We can avoid these issues if we ensure a bounded Lipschitz critic:3
Proposition 2. Assume the critics fψ(x) = (EX∼P kψ(X, x) − EY ∼Q kψ(Y, x))/ MMDkψ (P, Q)
are uniformly bounded and have a common Lipschitz constant: supx∈X ,ψ∈Ψ|fψ(x)| < ∞ and
supψ∈Ψ(cid:107)fψ(cid:107)Lip < ∞. In particular, this holds when kψ = K ◦ φψ and

sup
a∈Rs

K(a, a) < ∞,

(cid:107)K(a, ·) − K(b, ·)(cid:107)HK ≤ LK(cid:107)a − b(cid:107)Rs,

(cid:107)φψ(cid:107)Lip ≤ Lφ < ∞.

sup
ψ∈Ψ

Then DΨ

MMD is continuous in the weak topology: if Pn

D−→ P, then DΨ

MMD(Pn, P) → 0.

2 f #P denotes the pushforward of a distribution: if X ∼ P, then f (X) ∼ f #P.
3[27, Theorem 4] makes a similar claim to Proposition 2, but its proof was incorrect: it tries to uniformly

bound MMDkψ ≤ W 2, but the bound used is for a Wasserstein in terms of (cid:107)kψ(x, ·) − kψ(y, ·)(cid:107)Hkψ

.

3

Figure 1: The setting of Example 1. (a, b): parameter-space gradient ﬁelds for the MMD and the
SMMD (Section 3.3); the horizontal axis is θ, and the vertical 1/ψ. (c): optimal MMD critics for
θ = 20 with different kernels. (d): the MMD and the distances of Section 3 optimized over ψ.

Proof. The main result is [12, Corollary 11.3.4]. To show the claim for kψ = K ◦ φψ, note that
|fψ(x) − fψ(y)| ≤ (cid:107)fψ(cid:107)Hkψ

(cid:107)kψ(x, ·) − kψ(y, ·)(cid:107)Hkψ

, which since (cid:107)fψ(cid:107)Hkψ

= 1 is

(cid:107)K(φψ(x), ·) − K(φψ(y), ·)(cid:107)HK ≤ LK(cid:107)φψ(x) − φψ(y)(cid:107)Rs ≤ LKLφ(cid:107)x − y(cid:107)Rd .

Indeed, if we put a box constraint on ψ [27] or regularize the gradient of the critic function [7],
the resulting MMD GAN generally matches or outperforms WGAN-based models. Unfortunately,
though, an additive gradient penalty doesn’t substantially change the vector ﬁeld of Figure 1 (a), as
shown in Figure 5 (Appendix B). We will propose distances with much better convergence behavior.

3 New discrepancies for learning implicit generative models

Our aim here is to introduce a discrepancy that can provide useful gradient information when used as
an IGM loss. Proofs of results in this section are deferred to Appendix A.

3.1 Lipschitz Maximum Mean Discrepancy

Proposition 2 shows that an MMD-like discrepancy can be continuous under the weak topology even
when optimizing over kernels, if we directly restrict the critic functions to be Lipschitz. We can easily
deﬁne such a distance, which we call the Lipschitz MMD: for some λ > 0,

≤1

f ∈Hk : (cid:107)f (cid:107)2

LipMMDk,λ(P, Q) :=

EX∼P [f (X)] − EY ∼Q [f (Y )] .

sup
Lip+λ(cid:107)f (cid:107)2
For a universal kernel k, we conjecture that limλ→0 LipMMDk,λ(P, Q) → W(P, Q). But for any k
and λ, LipMMD is upper-bounded by W, as (4) optimizes over a smaller set of functions than (2).
Thus DΨ,λ
LipMMD(P, Q) := supψ∈Ψ LipMMDkψ,λ(P, Q) is also upper-bounded by W, and hence is
continuous in the Wasserstein topology. It also shows excellent empirical behavior on Example 1
(Figure 1 (d), and Figure 5 in Appendix B). But estimating LipMMDk,λ, let alone DΨ,λ
LipMMD, is in
general extremely difﬁcult (Appendix D), as ﬁnding (cid:107)f (cid:107)Lip requires optimization in the input space.
Constraining the mean gradient rather than the maximum, as we will do next, is far more tractable.

(4)

Hk

4

3.2 Gradient-Constrained Maximum Mean Discrepancy

sup
f ∈Hk : (cid:107)f (cid:107)S(µ),k,λ≤1
L2(µ) + (cid:107)∇f (cid:107)2

We deﬁne the Gradient-Constrained MMD for λ > 0 and using some measure µ as

GCMMDµ,k,λ(P, Q) :=

EX∼P [f (X)] − EY ∼Q [f (Y )] ,

(5)

where (cid:107)f (cid:107)2

S(µ),k,λ := (cid:107)f (cid:107)2

L2(µ) + λ(cid:107)f (cid:107)2
Hk

(6)
L2(µ) = (cid:82) (cid:107)·(cid:107)2 µ(dx) denotes the squared L2 norm. Rather than directly constraining the Lipschitz
(cid:107)·(cid:107)2
constant, the second term (cid:107)∇f (cid:107)2
L2(µ) encourages the function f to be ﬂat where µ has mass. In
experiments we use µ = P, ﬂattening the critic near the target sample. We add the ﬁrst term following
[10]: in one dimension and with µ uniform, (cid:107)·(cid:107)S(µ),·,0 is then an RKHS norm with the kernel
κ(x, y) = exp(−(cid:107)x − y(cid:107)), which is also a Sobolev space. The correspondence to a Sobolev norm is
lost in higher dimensions [53, Ch. 10], but we also found the ﬁrst term to be beneﬁcial in practice.

.

We can exploit some properties of Hk to compute (5) analytically. Call the difference in kernel mean
embeddings η := EX∼P[k(X, ·)] − EY ∼Q[k(Y, ·)] ∈ Hk; recall MMD(P, Q) = (cid:107)η(cid:107)Hk .
Proposition 3. Let ˆµ = (cid:80)M
RM d with (m, i)th entry4 ∂iη(Xm). Then under Assumptions (A) to (D) in Appendix A.1,

m=1 δXm . Deﬁne η(X) ∈ RM with mth entry η(Xm), and ∇η(X) ∈

GCMMD2

ˆµ,k,λ(P, Q) =

(cid:0)MMD2(P, Q) − ¯P (η)(cid:1)

1
λ
(cid:20) η(X)
∇η(X)

¯P (η) =

(cid:21)T (cid:18)(cid:20)K GT
G H

(cid:21)

+ M λIM +M d

(cid:19)−1 (cid:20) η(X)
∇η(X)

(cid:21)

,

where K is the kernel matrix Km,m(cid:48) = k(Xm, Xm(cid:48)), G is the matrix of left derivatives 5 G(m,i),m(cid:48) =
∂ik(Xm, Xm(cid:48)), and H that of derivatives of both arguments H(m,i),(m(cid:48),j) = ∂i∂j+dk(Xm, Xm(cid:48)).
As long as P and Q have integrable ﬁrst moments, and µ has second moments, Assumptions (A)
to (D) are satisﬁed e.g. by a Gaussian or linear kernel on top of a differentiable φψ. We can thus
estimate the GCMMD based on samples from P, Q, and µ by using the empirical mean ˆη for η.

This discrepancy indeed works well in practice: Appendix F.2 shows that optimizing our estimate
of Dµ,Ψ,λ
GCMMD = supψ∈Ψ GCMMDµ,kψ,λ yields a good generative model on MNIST. But the linear
system of size M + M d is impractical: even on 28 × 28 images and using a low-rank approximation,
the model took days to converge. We therefore design a less expensive discrepancy in the next section.

The GCMMD is related to some discrepancies previously used in IGM training. The Fisher GAN [34]
uses only the variance constraint (cid:107)f (cid:107)2
L2(µ) ≤ 1,
along with a vanishing boundary condition on f to ensure a well-deﬁned solution (although this was
not used in the implementation, and can cause very unintuitive critic behavior; see Appendix C).
The authors considered several choices of µ, including the WGAN-GP measure [18] and mixtures
(P + Qθ) /2. Rather than enforcing the constraints in closed form as we do, though, these models
used additive regularization. We will compare to the Sobolev GAN in experiments.

L2(µ) ≤ 1. The Sobolev GAN [33] constrains (cid:107)∇f (cid:107)2

3.3 Scaled Maximum Mean Discrepancy

We will now derive a lower bound on the Gradient-Constrained MMD which retains many of its
attractive qualities but can be estimated in time linear in the dimension d.
Proposition 4. Make Assumptions (A) to (D). For any f ∈ Hk, (cid:107)f (cid:107)S(µ),k,λ ≤ σ−1

µ,k,λ(cid:107)f (cid:107)Hk , where

(cid:118)
(cid:117)
(cid:117)
(cid:116)λ +

(cid:46)

(cid:90)

σµ,k,λ := 1

k(x, x)µ(dx) +

d
(cid:88)

i=1

(cid:90) ∂2k(y, z)
∂yi∂zi

(cid:12)
(cid:12)
(cid:12)(y,z)=(x,x)

µ(dx).

We then deﬁne the Scaled Maximum Mean Discrepancy based on this bound of Proposition 4:

SMMDµ,k,λ(P, Q) :=

sup

EX∼P [f (X)]−EY ∼Q [f (Y )] = σµ,k,λ MMDk(P, Q). (7)

f : σ−1

µ,k,λ(cid:107)f (cid:107)H≤1

4We use (m, i) to denote (m − 1)d + i; thus ∇η(X) stacks ∇η(X1), . . . , ∇η(XM ) into one vector.
5We use ∂ik(x, y) to denote the partial derivative with respect to xi, and ∂i+dk(x, y) that for yi.

5

Because the constraint in the optimization of (7) is more restrictive than in that of (5), we have
that SMMDµ,k,λ(P, Q) ≤ GCMMDµ,k,λ(P, Q). The Sobolev norm (cid:107)f (cid:107)S(µ),λ, and a fortiori the
gradient norm under µ, is thus also controlled for the SMMD critic. We also show in Appendix F.1
that SMMDµ,k,λ behaves similarly to GCMMDµ,k,λ on Gaussians.
(cid:3).
If kψ = K ◦ φψ and K(a, b) = g(−(cid:107)a − b(cid:107)2), then σ−2
(cid:3). Estimating
k,µ,λ = λ + Eµ
Or if K is linear, K(a, b) = aTb, then σ−2
these terms based on samples from µ is straightforward, giving a natural estimator for the SMMD.

k,µ,λ = λ + g(0) + 2|g(cid:48)(0)| Eµ

(cid:2)(cid:107)φψ(X)(cid:107)2 + (cid:107)∇φψ(X)(cid:107)2

(cid:2)(cid:107)∇φψ(X)(cid:107)2

F

F

Of course, if µ and k are ﬁxed, the SMMD is simply a constant times the MMD, and so behaves
in essentially the same way as the MMD. But optimizing the SMMD over a kernel family Ψ,
Dµ,Ψ,λ

SMMD(P, Q) := supψ∈Ψ SMMDµ,kψ,λ(P, Q), gives a distance very different from DΨ

MMD (3).

Figure 1 (b) shows the vector ﬁeld for the Optimized SMMD loss in Example 1, using the WGAN-
GP measure µ = Uniform(0, θ). The optimization surface is far more amenable: in particular
the location C, which formerly had an extremely small gradient that made learning effectively
impossible, now converges very quickly by ﬁrst reducing the critic gradient until some signal is
available. Figure 1 (d) demonstrates that Dµ,Ψ,λ
LipMMD but in sharp contrast
to DΨ

SMMD, like Dµ,Ψ,λ
MMD, is continuous with respect to the location θ and provides a strong gradient towards 0.

GCMMD and DΨ,λ

SMMD is continuous in the Wasserstein topology under some conditions:

We can establish that Dµ,Ψ,λ
Theorem 1. Let kψ = K ◦ φψ, with φψ : X → Rs a fully-connected L-layer network with
Leaky-ReLUα activations whose layers do not increase in width, and K satisfying mild smoothness
conditions QK < ∞ (Assumptions (II) to (V) in Appendix A.2). Let Ψκ be the set of parameters where
each layer’s weight matrices have condition number cond(W l) = (cid:107)W l(cid:107)/ σmin(W l) ≤ κ < ∞. If µ
has a density (Assumption (I)), then

Thus if Pn

W−→ P, then Dµ,Ψκ,λ

Dµ,Ψκ,λ

SMMD (P, Q) ≤

QKκL/2
√
dLαL/2
SMMD (Pn, P) → 0, even if µ is chosen to depend on P and Q.

W(P, Q).

L2(µ) = Eµ(cid:107)∇fψ(X)(cid:107)2 does
Uniform bounds vs bounds in expectation Controlling (cid:107)∇fψ(cid:107)2
not necessarily imply a bound on (cid:107)f (cid:107)Lip ≥ supx∈X (cid:107)∇fψ(X)(cid:107), and so does not in general give
continuity via Proposition 2. Theorem 1 implies that when the network’s weights are well-conditioned,
it is sufﬁcient to only control (cid:107)∇fψ(cid:107)2

L2(µ), which is far easier in practice than controlling (cid:107)f (cid:107)Lip.

If we instead tried to directly controlled (cid:107)f (cid:107)Lip with e.g. spectral normalization (SN) [32], we
could signiﬁcantly reduce the expressiveness of the parametric family. In Example 1, constraining
(cid:107)φψ(cid:107)Lip = 1 limits us to only Ψ = {1}. Thus D{1}
MMD is simply the MMD with an RBF kernel
of bandwidth 1, which has poor gradients when θ is far from 0 (Figure 1 (c), blue). The Cauchy-
Schwartz bound of Proposition 4 allows jointly adjusting the smoothness of kψ and the critic f , while
SN must control the two independently. Relatedly, limiting (cid:107)φ(cid:107)Lip by limiting the Lipschitz norm of
each layer could substantially reduce capacity, while (cid:107)∇fψ(cid:107)L2(µ) need not be decomposed by layer.
Another advantage is that µ provides a data-dependent measure of complexity as in [10]: we do not
needlessly prevent ourselves from using critics that behave poorly only far from the data.

Spectral parametrization When the generator is near a local optimum, the critic might identify
only one direction on which Qθ and P differ. If the generator parameterization is such that there
is no local way for the generator to correct it, the critic may begin to single-mindedly focus on
this difference, choosing redundant convolutional ﬁlters and causing the condition number of the
weights to diverge. If this occurs, the generator will be motivated to ﬁx this single direction while
ignoring all other aspects of the distributions, after which it may become stuck. We can help avoid
this collapse by using a critic parameterization that encourages diverse ﬁlters with higher-rank weight
matrices. Miyato et al. [32] propose to parameterize the weight matrices as W = γ ¯W /(cid:107) ¯W (cid:107)op,
where (cid:107) ¯W (cid:107)op is the spectral norm of ¯W . This parametrization works particularly well with Dµ,Ψ,λ
SMMD;
Figure 2 (b) shows the singular values of the second layer of a critic’s network (and Figure 9, in
Appendix F.3, shows more layers), while Figure 2 (d) shows the evolution of the condition number
during training. The conditioning of the weight matrix remains stable throughout training with
spectral parametrization, while it worsens through training in the default case.

6

4 Experiments

We evaluated unsupervised image generation on three datasets: CIFAR-10 [26] (60 000 images,
32 × 32), CelebA [29] (202 599 face images, resized and cropped to 160 × 160 as in [7]), and the
more challenging ILSVRC2012 (ImageNet) dataset [41] (1 281 167 images, resized to 64 × 64).
Code for all of these experiments is available at github.com/MichaelArbel/Scaled-MMD-GAN.
Losses All models are based on a scalar-output critic network φψ : X → R, except MMDGAN-GP
where φψ : X → R16 as in [7]. The WGAN and Sobolev GAN use a critic f = φψ, while the
GAN uses a discriminator Dψ(x) = 1/(1 + exp(−φψ(x))). The MMD-based methods use a kernel
kψ(x, y) = exp(−(φψ(x) − φψ(y))2/2), except for MMDGAN-GP which uses a mixture of RQ
kernels as in [7]. Increasing the output dimension of the critic or using a different kernel didn’t
substantially change the performance of our proposed method. We also consider SMMD with a linear
top-level kernel, k(x, y) = φψ(x)φψ(y); because this becomes essentially identical to a WGAN
(Appendix E), we refer to this method as SWGAN. SMMD and SWGAN use µ = P; Sobolev GAN
uses µ = (P + Q)/2 as in [33]. We choose λ and an overall scaling to obtain the losses:

SMMD:

(cid:92)MMD

2
kψ
1 + 10 EˆP [(cid:107)∇φψ(X)(cid:107)2
F ]

(P, Qθ)

, SWGAN:

(cid:113)

EˆP [φψ(X)] − E ˆQθ

[φψ(X)]

1 + 10 (cid:0)EˆP [|φψ(X)|2] + EˆP [(cid:107)∇φψ(X)(cid:107)2

F ](cid:1)

.

Architecture For CIFAR-10, we used the CNN architecture proposed by [32] with a 7-layer critic
and a 4-layer generator. For CelebA, we used a 5-layer DCGAN discriminator and a 10-layer ResNet
generator as in [7]. For ImageNet, we used a 10-layer ResNet for both the generator and discriminator.
In all experiments we used 64 ﬁlters for the smallest convolutional layer, and double it at each layer
(CelebA/ImageNet) or every other layer (CIFAR-10). The input codes for the generator are drawn
from Uniform (cid:0)[−1, 1]128(cid:1). We consider two parameterizations for each critic: a standard one where
the parameters can take any real value, and a spectral parametrization (denoted SN-) as above [32].
Models without explicit gradient control (SN-GAN, SN-MMDGAN, SN-MMGAN-L2, SN-WGAN)
ﬁx γ = 1, for spectral normalization; others learn γ, using a spectral parameterization.

Training All models were trained for 150 000 generator updates on a single GPU, except for ImageNet
where the model was trained on 3 GPUs simultaneously. To limit communication overhead we
averaged the MMD estimate on each GPU, giving the block MMD estimator [54]. We always used
64 samples per GPU from each of P and Q, and 5 critic updates per generator step. We used initial
learning rates of 0.0001 for CIFAR-10 and CelebA, 0.0002 for ImageNet, and decayed these rates
using the KID adaptive scheme of [7]: every 2 000 steps, generator samples are compared to those
from 20 000 steps ago, and if the relative KID test [9] fails to show an improvement three consecutive
times, the learning rate is decayed by 0.8. We used the Adam optimizer [25] with β1 = 0.5, β2 = 0.9.

Evaluation To compare the sample quality of different models, we considered three different scores
based on the Inception network [49] trained for ImageNet classiﬁcation, all using default parameters
in the implementation of [7]. The Inception Score (IS) [42] is based on the entropy of predicted
labels; higher values are better. Though standard, this metric has many issues, particularly on datasets
other than ImageNet [4, 7, 20]. The FID [20] instead measures the similarity of samples from the
generator and the target as the Wasserstein-2 distance between Gaussians ﬁt to their intermediate
representations. It is more sensible than the IS and becoming standard, but its estimator is strongly
biased [7]. The KID [7] is similar to FID, but by using a polynomial-kernel MMD its estimates enjoy
better statistical properties and are easier to compare. (A similar score was recommended by [21].)

Results Table 1a presents the scores for models trained on both CIFAR-10 and CelebA datasets. On
CIFAR-10, SN-SWGAN and SN-SMMDGAN performed comparably to SN-GAN. But on CelebA,
SN-SWGAN and SN-SMMDGAN dramatically outperformed the other methods with the same
architecture in all three metrics. It also trained faster, and consistently outperformed other methods
over multiple initializations (Figure 2 (a)). It is worth noting that SN-SWGAN far outperformed
WGAN-GP on both datasets. Table 1b presents the scores for SMMDGAN and SN-SMMDGAN
trained on ImageNet, and the scores of pre-trained models using BGAN [6] and SN-GAN [32].6 The

6These models are courtesy of the respective authors and also trained at 64 × 64 resolution. SN-GAN used
the same architecture as our model, but trained for 250 000 generator iterations; BS-GAN used a similar 5-layer
ResNet architecture and trained for 74 epochs, comparable to SN-GAN.

7

Figure 2: The training process on CelebA. (a) KID scores. We report a ﬁnal score for SN-GAN
slightly before its sudden failure mode; MMDGAN and SN-MMDGAN were unstable and had scores
around 100. (b) Singular values of the second layer, both early (dashed) and late (solid) in training.
(c) σ−2
µ,k,λ for several MMD-based methods. (d) The condition number in the ﬁrst layer through
training. SN alone does not control σµ,k,λ, and SMMD alone does not control the condition number.

(a) Scaled MMD GAN with SN

(b) SN-GAN

(c) Boundary Seeking GAN

(d) Scaled MMD GAN with SN

(e) Scaled WGAN with SN

(f) MMD GAN with GP+L2

Figure 3: Samples from various models. Top: 64 × 64 ImageNet; bottom: 160 × 160 CelebA.

8

Table 1: Mean (standard deviation) of score estimates, based on 50 000 samples from each model.

(a) CIFAR-10 and CelebA.

Method

WGAN-GP
MMDGAN-GP-L2
Sobolev-GAN
SMMDGAN
SN-GAN
SN-SWGAN
SN-SMMDGAN

CIFAR-10
IS

FID

6.9±0.2
6.9±0.1
7.0±0.1
7.0±0.1
7.2±0.1
7.2±0.1
7.3±0.1

31.1±0.2
31.4±0.3
30.3±0.3
31.5±0.4
26.7±0.2
28.5±0.2
25.0±0.3

KID×103

22.2±1.1
23.3±1.1
22.3±1.2
22.2±1.1
16.1±0.9
17.6±1.1
16.6±2.0

CelebA
IS

2.7±0.0
2.6±0.0
2.9±0.0
2.7±0.0
2.7±0.0
2.8±0.0
2.8±0.0

FID

KID×103

29.2±0.2
20.5±0.2
16.4±0.1
18.4±0.2
22.6±0.1
14.1±0.2
12.4±0.2

22.0±1.0
13.0±1.0
10.6±0.5
11.5±0.8
14.6±1.1
7.7±0.5
6.1±0.4

(b) ImageNet.

Method

IS

FID

10.7±0.4
BGAN
11.2±0.1
SN-GAN
SMMDGAN
10.7±0.2
SN-SMMDGAN 10.9±0.1

43.9±0.3
47.5±0.1
38.4±0.3
36.6±0.2

KID×103

47.0±1.1
44.4±2.2
39.3±2.5
34.6±1.6

proposed methods substantially outperformed both methods in FID and KID scores. Figure 3 shows
samples on ImageNet and CelebA; Appendix F.4 has more.

Spectrally normalized WGANs / MMDGANs To control for the contribution of the spectral
parametrization to the performance, we evaluated variants of MMDGANs, WGANs and Sobolev-
GAN using spectral normalization (in Table 2, Appendix F.3). WGAN and Sobolev-GAN led to
unstable training and didn’t converge at all (Figure 11) despite many attempts. MMDGAN converged
on CIFAR-10 (Figure 11) but was unstable on CelebA (Figure 10). The gradient control due to SN
is thus probably too loose for these methods. This is reinforced by Figure 2 (c), which shows that
the expected gradient of the critic network is much better-controlled by SMMD, even when SN is
used. We also considered variants of these models with a learned γ while also adding a gradient
penalty and an L2 penalty on critic activations [7, footnote 19]. These generally behaved similarly to
MMDGAN, and didn’t lead to substantial improvements. We ran the same experiments on CelebA,
but aborted the runs early when it became clear that training was not successful.

Rank collapse We occasionally observed the failure mode for SMMD where the critic becomes
low-rank, discussed in Section 3.3, especially on CelebA; this failure was obvious even in the training
objective. Figure 2 (b) is one of these examples. Spectral parametrization seemed to prevent this
behavior. We also found one could avoid collapse by reverting to an earlier checkpoint and increasing
the RKHS regularization parameter λ, but did not do this for any of the experiments here.

5 Conclusion

We studied gradient regularization for MMD-based critics in implicit generative models, clarifying
how previous techniques relate to the DΨ
MMD loss. Based on these insights, we proposed the Gradient-
Constrained MMD and its approximation the Scaled MMD, a new loss function for IGMs that
controls gradient behavior in a principled way and obtains excellent performance in practice.

One interesting area of future study for these distances is their behavior when used to diffuse particles
distributed as Q towards particles distributed as P. Mroueh et al. [33, Appendix A.1] began such a
study for the Sobolev GAN loss; [35] proved convergence and studied discrete-time approximations.

Another area to explore is the geometry of these losses, as studied by Bottou et al. [8], who showed
potential advantages of the Wasserstein geometry over the MMD. Their results, though, do not
address any distances based on optimized kernels; the new distances introduced here might have
interesting geometry of their own.

9

References

[1] B. Amos and J. Z. Kolter. “OptNet: Differentiable Optimization as a Layer in Neural Net-

works.” ICML. 2017. arXiv: 1703.00443.

[2] M. Arjovsky and L. Bottou. “Towards Principled Methods for Training Generative Adversarial

Networks.” ICLR. 2017. arXiv: 1701.04862.

[3] M. Arjovsky, S. Chintala, and L. Bottou. “Wasserstein Generative Adversarial Networks.”

ICML. 2017. arXiv: 1701.07875.

[4] S. Barratt and R. Sharma. A Note on the Inception Score. 2018. arXiv: 1801.01973.
[5] M. G. Bellemare, I. Danihelka, W. Dabney, S. Mohamed, B. Lakshminarayanan, S. Hoyer,
and R. Munos. The Cramer Distance as a Solution to Biased Wasserstein Gradients. 2017.
arXiv: 1705.10743.

[6] D. Berthelot, T. Schumm, and L. Metz. BEGAN: Boundary Equilibrium Generative Adversarial

[7] M. Bi´nkowski, D. J. Sutherland, M. Arbel, and A. Gretton. “Demystifying MMD GANs.”

Networks. 2017. arXiv: 1703.10717.

ICLR. 2018. arXiv: 1801.01401.

[8] L. Bottou, M. Arjovsky, D. Lopez-Paz, and M. Oquab. “Geometrical Insights for Implicit
Generative Modeling.” Braverman Readings in Machine Learning: Key Iedas from Inception
to Current State. Ed. by L. Rozonoer, B. Mirkin, and I. Muchnik. LNAI Vol. 11100. Springer,
2018, pp. 229–268. arXiv: 1712.07822.

[9] W. Bounliphone, E. Belilovsky, M. B. Blaschko, I. Antonoglou, and A. Gretton. “A Test of
Relative Similarity For Model Selection in Generative Models.” ICLR. 2016. arXiv: 1511.
04581.

[10] O. Bousquet, O. Chapelle, and M. Hein. “Measure Based Regularization.” NIPS. 2004.
[11] A. Brock, T. Lim, J. M. Ritchie, and N. Weston. “Neural Photo Editing with Introspective

Adversarial Networks.” ICLR. 2017. arXiv: 1609.07093.

[12] R. M. Dudley. Real Analysis and Probability. 2nd ed. Cambridge University Press, 2002.
[13] G. K. Dziugaite, D. M. Roy, and Z. Ghahramani. “Training generative neural networks via

Maximum Mean Discrepancy optimization.” UAI. 2015. arXiv: 1505.03906.

[14] A. Genevay, G. Peyré, and M. Cuturi. “Learning Generative Models with Sinkhorn Diver-

gences.” AISTATS. 2018. arXiv: 1706.00292.

[15] T. Gneiting and A. E. Raftery. “Strictly proper scoring rules, prediction, and estimation.” JASA

102.477 (2007), pp. 359–378.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio. “Generative Adversarial Nets.” NIPS. 2014. arXiv: 1406.2661.

[17] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Schölkopf, and A. J. Smola. “A Kernel Two-

Sample Test.” JMLR 13 (2012).
I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. “Improved Training of
Wasserstein GANs.” NIPS. 2017. arXiv: 1704.00028.

[19] A. Güngör. “Some bounds for the product of singular values.” International Journal of

[16]

[18]

Contemporary Mathematical Sciences (2007).

[20] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, G. Klambauer, and S. Hochreiter. “GANs
Trained by a Two Time-Scale Update Rule Converge to a Nash Equilibrium.” NIPS. 2017.
arXiv: 1706.08500.

[21] G. Huang, Y. Yuan, Q. Xu, C. Guo, Y. Sun, F. Wu, and K. Weinberger. An empirical study on

evaluation metrics of generative adversarial networks. 2018. arXiv: 1806.07755.

[22] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. “Multimodal Unsupervised Image-to-Image

Translation.” ECCV. 2018. arXiv: 1804.04732.

[23] Y. Jin, K. Zhang, M. Li, Y. Tian, H. Zhu, and Z. Fang. Towards the Automatic Anime Characters

Creation with Generative Adversarial Networks. 2017. arXiv: 1708.05509.

[24] T. Karras, T. Aila, S. Laine, and J. Lehtinen. “Progressive Growing of GANs for Improved

Quality, Stability, and Variation.” ICLR. 2018. arXiv: 1710.10196.

[25] D. Kingma and J. Ba. “Adam: A Method for Stochastic Optimization.” ICLR. 2015. arXiv:

1412.6980.

[26] A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images. 2009.

10

arXiv: 1502.02761.

arXiv: 1411.7766.

(2002), pp. 583–601.

1711.04894.

[27] C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. Póczos. “MMD GAN: Towards Deeper

Understanding of Moment Matching Network.” NIPS. 2017. arXiv: 1705.08584.

[28] Y. Li, K. Swersky, and R. Zemel. “Generative Moment Matching Networks.” ICML. 2015.

[29] Z. Liu, P. Luo, X. Wang, and X. Tang. “Deep learning face attributes in the wild.” ICCV. 2015.

[30] L. Mescheder, A. Geiger, and S. Nowozin. “Which Training Methods for GANs do actually

Converge?” ICML. 2018. arXiv: 1801.04406.

[31] P. Milgrom and I. Segal. “Envelope theorems for arbitrary choice sets.” Econometrica 70.2

[32] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. “Spectral Normalization for Generative

Adversarial Networks.” ICLR. 2018. arXiv: 1802.05927.

[33] Y. Mroueh, C.-L. Li, T. Sercu, A. Raj, and Y. Cheng. “Sobolev GAN.” ICLR. 2018. arXiv:

[34] Y. Mroueh and T. Sercu. “Fisher GAN.” NIPS. 2017. arXiv: 1705.09675.
[35] Y. Mroueh, T. Sercu, and A. Raj. Regularized Kernel and Neural Sobolev Descent: Dynamic

MMD Transport. 2018. arXiv: 1805.12062.

[36] A. Müller. “Integral Probability Metrics and their Generating Classes of Functions.” Advances

in Applied Probability 29.2 (1997), pp. 429–443.

[37] S. Nowozin, B. Cseke, and R. Tomioka. “f-GAN: Training Generative Neural Samplers using

Variational Divergence Minimization.” NIPS. 2016. arXiv: 1606.00709.

[38] A. Radford, L. Metz, and S. Chintala. “Unsupervised Representation Learning with Deep
Convolutional Generative Adversarial Networks.” ICLR. 2016. arXiv: 1511.06434.
J. R. Retherford. “Review: J. Diestel and J. J. Uhl, Jr., Vector measures.” Bull. Amer. Math.
Soc. 84.4 (July 1978), pp. 681–685.

[39]

[40] K. Roth, A. Lucchi, S. Nowozin, and T. Hofmann. “Stabilizing Training of Generative Adver-

sarial Networks through Regularization.” NIPS. 2017. arXiv: 1705.09367.

[41] O. Russakovsky et al. ImageNet Large Scale Visual Recognition Challenge. 2014. arXiv:

1409.0575.

[42] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. “Improved

[43]

Techniques for Training GANs.” NIPS. 2016. arXiv: 1606.03498.
J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University
Press, 2004.

[44] B. K. Sriperumbudur, K. Fukumizu, A. Gretton, G. R. G. Lanckriet, and B. Schölkopf. “Kernel

choice and classiﬁability for RKHS embeddings of probability distributions.” NIPS. 2009.

[45] B. K. Sriperumbudur, K. Fukumizu, and G. R. G. Lanckriet. “Universality, Characteristic
Kernels and RKHS Embedding of Measures.” JMLR 12 (2011), pp. 2389–2410. arXiv: 1003.
0887.

[46] B. Sriperumbudur. “On the optimal estimation of probability mesaures in weak and strong

topologies.” Bernoulli 22.3 (2016), pp. 1839–1893. arXiv: 1310.8240.
I. Steinwart and A. Christmann. Support Vector Machines. Springer, 2008.

[47]
[48] D. J. Sutherland, H.-Y. Tung, H. Strathmann, S. De, A. Ramdas, A. Smola, and A. Gretton.
“Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy.” ICLR.
2017. arXiv: 1611.04488.

[49] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. “Rethinking the Inception

Architecture for Computer Vision.” CVPR. 2016. arXiv: 1512.00567.

[50] T. Unterthiner, B. Nessler, C. Seward, G. Klambauer, M. Heusel, H. Ramsauer, and S. Hochre-
iter. “Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields.” ICLR. 2018.
arXiv: 1708.08819.

[51] C. Villani. Optimal Transport: Old and New. Springer, 2009.
[52]

J. Weed and F. Bach. “Sharp asymptotic and ﬁnite-sample rates of convergence of empirical
measures in Wasserstein distance.” Bernoulli (forthcoming). arXiv: 1707.00087.
[53] H. Wendland. Scattered Data Approximation. Cambridge University Press, 2005.
[54] W. Zaremba, A. Gretton, and M. B. Blaschko. “B-tests: Low Variance Kernel Two-Sample

Tests.” NIPS. 2013. arXiv: 1307.1954.

11

A Proofs

We ﬁrst review some basic properties of Reproducing Kernel Hilbert Spaces. We consider here a
separable RKHS H with basis (ei)i∈I , where I is either ﬁnite if H is ﬁnite-dimensional, or I = N
otherwise. We also assume that the reproducing kernel k is continuously twice differentiable.

We use a slightly nonstandard notation for derivatives: ∂if (x) denotes the ith partial derivative of f
evaluated at x, and ∂i∂j+dk(x, y) denotes ∂2k(a,b)
∂ai∂bj

|(a,b)=(x,y).

Then the following reproducing properties hold for any given function f in H [47, Lemma 4.34]:
f (x) =(cid:104)f, k(x, .)(cid:105)H
∂if (x) =(cid:104)f, ∂ik(x, .)(cid:105)H.

(8)
(9)

We say that an operator A : H (cid:55)→ H is Hilbert-Schmidt if (cid:107)A(cid:107)2
H is ﬁnite. (cid:107)A(cid:107)HS
is called the Hilbert-Schmidt norm of A. The space of Hilbert-Schmidt operators itself a Hilbert
space with the inner product (cid:104)A, B(cid:105)HS = (cid:80)
i∈I (cid:104)Aei, Bei(cid:105)H. Moreover, we say that an operator A
is trace-class if its trace norm is ﬁnite, i.e. (cid:107)A(cid:107)1 = (cid:80)
2 ei(cid:105)H < ∞. The outer product
f ⊗ g for f, g ∈ H gives an H → H operator such that (f ⊗ g)v = (cid:104)g, v(cid:105)Hf for all v in H.

i∈I (cid:104)ei, (A∗A) 1

i∈I (cid:107)Aei(cid:107)2

HS = (cid:80)

Given two vectors f and g in H and a Hilbert-Schmidt operator A we have the following properties:

(i) The outer product f ⊗ g is a Hilbert-Schmidt operator with Hilbert-Schmidt norm given by:

(ii) The inner product between two rank-one operators f ⊗ g and u ⊗ v is (cid:104)f ⊗ g, u ⊗ v(cid:105)HS =

(cid:107)f ⊗ g(cid:107)HS = (cid:107)f (cid:107)H(cid:107)g(cid:107)H.

(cid:104)f, u(cid:105)H(cid:104)g, v(cid:105)H.

(iii) The following identity holds: (cid:104)f, Ag(cid:105)H = (cid:104)f ⊗ g, A(cid:105)HS.

Deﬁne the following covariance-type operators:

d
(cid:88)

i=1

Dx = k(x, ·) ⊗ k(x, ·) +

∂ik(x, ·) ⊗ ∂ik(x, ·) Dµ = EX∼µ DX Dµ,λ = Dµ + λI;

(10)

these are useful in that, using (8) and (9), (cid:104)f, Dxg(cid:105) = f (x)g(x) + (cid:80)d

i=1 ∂if (x) ∂ig(x).

A.1 Deﬁnitions and estimators of the new distances

We will need the following assumptions about the distributions P and Q, the measure µ, and the
kernel k:

(A) P and Q have integrable ﬁrst moments.
(B) (cid:112)k(x, x) grows at most linearly in x: for all x in X , (cid:112)k(x, x) ≤ C((cid:107)x(cid:107) + 1) for some

constant C.

(C) The kernel k is twice continuously differentiable.
(D) The functions x (cid:55)→ k(x, x) and x (cid:55)→ ∂i∂i+dk(x, x) for 1 ≤ i ≤ d are µ-integrable.

When k = K ◦ φψ, Assumption (B) is automatically satisﬁed by a K such as the Gaussian; when K
is linear, it is true for a quite general class of networks φψ [7, Lemma 1].

We will ﬁrst give a form for the Gradient-Constrained MMD (5) in terms of the operator (10):
Proposition 5. Under Assumptions (A) to (D), the Gradient-Constrained MMD is given by

GCMMDµ,k,λ(P, Q) =

(cid:104)η, D−1

µ,λη(cid:105)H.

(cid:113)

(11)

Proof of Proposition 5. Let f be a function in H. We will ﬁrst express the squared λ-regularized
Sobolev norm of f (6) as a quadratic form in H. Recalling the reproducing properties of (8) and (9),
we have:

(cid:107)f (cid:107)2

S(µ),k,λ =

(cid:104)f, k(x, ·)(cid:105)2

H µ(dx) +

(cid:104)f, ∂ik(x, ·)(cid:105)2

H µ(dx) + λ(cid:107)f (cid:107)2
H.

(cid:90)

d
(cid:88)

(cid:90)

i=1

12

Using Property (ii) and the operator (10), one further gets

(cid:107)f (cid:107)2

S(µ),k,λ =

(cid:104)f ⊗ f, Dx(cid:105)HS µ(dx) + λ(cid:107)f (cid:107)2
H.

(cid:90)

Under Assumption (D), and using Lemma 6, one can take the integral inside the inner product, which
leads to (cid:107)f (cid:107)2

H. Finally, using Property (iii) it follows that

S(µ),k,λ = (cid:104)f ⊗ f, Dµ(cid:105)HS + λ(cid:107)f (cid:107)2

(cid:107)f (cid:107)2

S(µ),k,λ = (cid:104)f, Dµ,λf (cid:105)H.

Under Assumptions (A) and (B), Lemma 6 applies, and it follows that k(x, ·) is also Bochner
integrable under P and Q. Thus

EP [(cid:104)f, k(x, ·)(cid:105)H] − EQ [(cid:104)f, k(x, ·)(cid:105)H] = (cid:104)f, EP [k(x, ·)] − EP [k(x, ·)](cid:105)H = (cid:104)f, η(cid:105)H,

where η is deﬁned as this difference in mean embeddings.

Since Dµ,λ is symmetric positive deﬁnite, its square-root D

1
2

For any f ∈ H, let g = D
corresponding f = D− 1

1
2

µ,λ is well-deﬁned and is also invertible.
H. Note that for any g ∈ H, there is a
µ,λg. Thus we can re-express the maximization problem in (5) in terms of g:

µ,λf , so that (cid:104)f, Dµ,λf (cid:105)H = (cid:107)g(cid:107)2

2

GCMMDµ,k,λ(P, Q) :=

sup
f ∈H
(cid:104)f,Dµ,λf (cid:105)H≤1
(cid:104)g, D− 1

2

= sup
g∈H
(cid:107)g(cid:107)H≤1

(cid:104)f, η(cid:105)H = sup
g∈H
(cid:107)g(cid:107)H≤1

(cid:104)D− 1

2

µ,λg, η(cid:105)H

µ,λη(cid:105)H = (cid:107)D− 1

2

µ,λη(cid:107)H =

(cid:104)η, D−1

µ,λη(cid:105)H.

(cid:113)

Proposition 5, though, involves inverting the inﬁnite-dimensional operator Dµ,λ and thus doesn’t
directly give us a computable estimator. Proposition 3 solves this problem in the case where µ is a
discrete measure:
Proposition 3. Let ˆµ = (cid:80)M
m=1 δXm be an empirical measure of M points. Let η(X) ∈ RM have
mth entry η(Xm), and ∇η(X) ∈ RM d have (m, i)th entry7 ∂iη(Xm). Then under Assumptions (A)
to (D), the Gradient-Constrained MMD is

GCMMD2

ˆµ,k,λ(P, Q) =

(cid:0)MMD2(P, Q) − ¯P (η)(cid:1)

1
λ
(cid:20) η(X)
∇η(X)

¯P (η) =

(cid:21)T (cid:18)(cid:20)K GT
G H

(cid:21)

+ M λIM +M d

(cid:19)−1 (cid:20) η(X)
∇η(X)

(cid:21)

,

where K is the kernel matrix Km,m(cid:48) = k(Xm, Xm(cid:48)), G is the matrix of left derivatives G(m,i),m(cid:48) =
∂ik(Xm, Xm(cid:48)), and H that of derivatives of both arguments H(m,i),(m(cid:48),j) = ∂i∂j+dk(Xm, Xm(cid:48)).

Before proving Proposition 3, we note the following interesting alternate form. Let ¯ei be the ith
standard basis vector for RM +M d, and deﬁne T : H → RM +M d as the linear operator

T =

¯em ⊗ k(Xm, ·) +

¯em+(m,i) ⊗ ∂ik(Xm, ·).

M
(cid:88)

m=1

M
(cid:88)

d
(cid:88)

m=1

i=1

Then

(cid:21)

(cid:20) η(X)
∇η(X)

(cid:21)

(cid:20)K GT
G H

= T η, and

= T T ∗. Thus we can write

GCMMD2

ˆµ,k,λ =

(cid:10)η, (cid:0)I − T ∗(T T ∗ + M λI)−1T (cid:1) η(cid:11)

H .

1
λ

7We use (m, i) to denote (m − 1)d + i; thus ∇η(X) stacks ∇η(X1), . . . , ∇η(XM ) into one vector.

13

Proof of Proposition 3. Let g ∈ H be the solution to the regression problem Dµ,λg = η:

g(Xm)k(Xm, ·) +

∂ig(Xm)∂ik(Xm, ·)

+ λg = η

(cid:35)

d
(cid:88)

i=1

1
M

(cid:34)

M
(cid:88)

m=1

g =

η −

1
λ

1
λM

(cid:34)

M
(cid:88)

m=1

g(Xm)k(Xm, ·) +

∂ig(Xm)∂ik(Xm, ·)

.

(12)

Taking the inner product of both sides of (12) with k(Xm(cid:48), ·) for each 1 ≤ m(cid:48) ≤ M yields the
following M equations:

g(Xm(cid:48)) =

η(Xm(cid:48)) −

g(Xm)Km,m(cid:48) +

∂ig(Xm) G(m,i),m(cid:48)

.

(13)

Doing the same with ∂jk(Xm(cid:48), ·) gives M d equations:

∂jg(Xm(cid:48)) =

∂jη(Xm(cid:48)) −

g(Xm)G(m(cid:48),j),m +

∂ig(Xm)H(m,i),(m(cid:48),j)

.

(14)

(cid:35)

(cid:35)

(cid:35)

(cid:35)

From (12), it is clear that g is a linear combination of the form:

g(x) =

η(x) −

αmk(Xm, x) +

βm,i∂ik(Xm, x)

,

1
λ

1
λM

(cid:34)

M
(cid:88)

m=1

1
λ

1
λM

(cid:34)

M
(cid:88)

m=1

1
λ

1
λM

(cid:34)

M
(cid:88)

m=1

d
(cid:88)

i=1

d
(cid:88)

i=1

d
(cid:88)

i=1

d
(cid:88)

i=1

where the coefﬁcients α := (αm = g(Xm))1≤m≤M and β := (βm,i = ∂ig(Xm))1≤m≤M
1≤i≤d
the system of equations (13) and (14). We can rewrite this system as

satisfy

(cid:20)K + M λIM
G

GT
H + M λIM d

(cid:21)

(cid:21) (cid:20)α
β

= M

(cid:21)

(cid:20) η(X)
∇η(X)

,

where IM , IM d are the identity matrices of dimension M , M d. Since K and H must be positive
semideﬁnite, an inverse exists. We conclude by noticing that
(cid:34)

(cid:35)

GCMMDˆµ,k,λ(P, Q)2 = (cid:104)η, g(cid:105)H =

(cid:107)η(cid:107)2

H −

αmη(Xm) +

βm,i∂iη(Xm)

.

1
λ

1
λM

M
(cid:88)

m=1

d
(cid:88)

i=1

The following result was key to our deﬁnition of the SMMD in Section 3.3.
Proposition 4. Under Assumptions (A) to (D), we have for all f ∈ H that

where σk,µ,λ := 1/

λ + (cid:82) k(x, x)µ(dx) + (cid:80)d

i=1

(cid:113)

(cid:107)f (cid:107)S(µ),k,λ ≤ σ−1

µ,k,λ(cid:107)f (cid:107)Hk ,
(cid:82) ∂i∂i+dk(x, x)µ(dx).

Proof of Proposition 4. The key idea here is to use the Cauchy-Schwarz inequality for the Hilbert-
Schmidt inner product. Letting f ∈ H, (cid:107)f (cid:107)2
S(µ),k,λ is
(cid:90)

(cid:90)

f (x)2 µ(dx) +

(cid:107)∇f (x)(cid:107)2 µ(dx) + λ(cid:107)f (cid:107)2
H

(cid:104)f, k(x, ·) ⊗ k(x, ·)f (cid:105)H µ(dx) +

(cid:104)f, ∂ik(x, ·) ⊗ ∂ik(x, ·)f (cid:105)H µ(dx) + λ(cid:107)f (cid:107)2
H

(cid:104)f ⊗ f, k(x, ·) ⊗ k(x, ·)(cid:105)HS µ(dx) +

(cid:104)f ⊗ f, ∂ik(x, ·) ⊗ ∂ik(x, ·)(cid:105)HS µ(dx) + λ(cid:107)f (cid:107)2
H

k(x, x) µ(dx) +

∂i∂i+dk(x, x) µ(dx) + λ

.

(cid:35)

d
(cid:88)

(cid:90)

i=1

(a) follows from the reproducing properties (8) and (9) and Property (ii). (b) is obtained using
Property (iii), while (c) follows from the Cauchy-Schwarz inequality and Property (i).

(cid:90)

(a)
=

(cid:90)

(b)
=

(cid:34)(cid:90)

(c)
≤ (cid:107)f (cid:107)2
H

d
(cid:88)

(cid:90)

i=1

d
(cid:88)

(cid:90)

i=1

14

Lemma 6. Under Assumption (D), Dx is Bochner integrable and its integral Dµ is a trace-class
symmetric positive semi-deﬁnite operator with Dµ,λ = D+λI invertible for any positive λ. Moreover,
for any Hilbert-Schmidt operator A we have: (cid:104)A, Dµ(cid:105)HS = (cid:82) (cid:104)A, Dx(cid:105)HS µ(dx).
Under Assumptions (A) and (B), k(x, ·) is Bochner integrable with respect to any probability
distribution P with ﬁnite ﬁrst moment and the following relation holds: (cid:104)f, EP [k(x, ·)](cid:105)H =
EP [(cid:104)f, k(x, ·)(cid:105)H] for all f in H.

Proof. The operator Dx is positive self-adjoint. It is also trace-class, as by the triangle inequality

(cid:107)Dx(cid:107)1 ≤ (cid:107)k(x, ·) ⊗ k(x, ·)(cid:107)1 +

(cid:107)∂ik(x, ·) ⊗ ∂ik(x, ·)(cid:107)1

d
(cid:88)

i=1

= (cid:107)k(x, ·)(cid:107)2

H +

(cid:107)∂ik(x, ·)(cid:107)2

H < ∞.

d
(cid:88)

i=1

By Assumption (D), we have that (cid:82) (cid:107)Dx(cid:107)1 µ(dx) < ∞ which implies that Dx is µ-integrable in
the Bochner sense [39, Deﬁnition 1 and Theorem 2]. Its integral Dµ is trace-class and satisﬁes
(cid:107)Dµ(cid:107)1 ≤ (cid:82) (cid:107)Dx(cid:107)1 µ(dx). This allows to have (cid:104)A, Dµ(cid:105)HS = (cid:82) (cid:104)A, Dx(cid:105)HS µ(dx) for all Hilbert-
Schmidt operators A. Moreover, the integral preserves the symmetry and positivity. It follows that
Dµ,λ is invertible.
The Bochner integrability of k(x, ·) under a distribution P with ﬁnite moment follows directly from
Assumptions (A) and (B), since (cid:82) (cid:107)k(x, ·)(cid:107) P(dx) ≤ C (cid:82) ((cid:107)x(cid:107) + 1) P(dx) < ∞. This allows us to
write (cid:104)f, EP[k(x, ·)](cid:105)H = EP[(cid:104)f, k(x, ·)(cid:105)H].

A.2 Continuity of the Optimized Scaled MMD in the Wasserstein topology

To prove Theorem 1, we we will ﬁrst need some new notation.

We assume the kernel is k = K ◦ φψ, i.e. kψ(x, y) = K(φψ(x), φψ(y)), where the representation
function φψ is a network φψ(X) : Rd → RdL consisting of L fully-connected layers:

h0
ψ(X) = X
ψ(X) = W lσl−1(hl−1
hl
φψ(X) = hL

ψ(X).

ψ (X)) + bl

for 1 ≤ l ≤ L

(15)

The intermediate representations hl
ψ(X) are of dimension dl, the weights W l are matrices in
Rdl×dl−1 , and biases bl are vectors in Rdl . The elementwise activation function σ is given by
σ0(x) = x, and for l > 0 the activation σl is a leaky ReLU with leak coefﬁcient 0 < α < 1:

σl(x) = σ(x) =

for l > 0.

(16)

(cid:26)x

x > 0
αx x ≤ 0

The parameter ψ is the concatenation of all the layer parameters:

ψ = (cid:0)(W L, bL), (W L−1, bL−1), . . . , (W 1, b1)(cid:1) .

We denote by Ψ the set of all such possible parameters, i.e. Ψ = RdL×dL−1 ×RdL ×· · ·×Rd1×d×Rd1 .
Deﬁne the following restrictions of Ψ:

Ψκ := (cid:8)ψ ∈ Ψ | ∀1 ≤ l ≤ L, cond(W l) ≤ κ(cid:9)
1 := (cid:8)ψ ∈ Ψκ | ∀1 ≤ l ≤ L, (cid:107)W l(cid:107) = 1(cid:9) .
Ψκ

(17)

(18)

Ψκ is the set of those parameters such that W l have a small condition number, cond(W ) =
σmax(W )/σmin(W ). Ψκ
1 is the set of per-layer normalized parameters with a condition number
bounded by κ.

15

Recall the deﬁnition of Scaled MMD, (7), where λ > 0 and µ is a probability measure:

SMMDµ,k,λ(P, Q) := σµ,k,λ MMDk(P, Q)

σk,µ,λ := 1/

k(x, x) µ(dx) +

∂i∂i+dk(x, x) µ(dx).

(cid:118)
(cid:117)
(cid:117)
(cid:116)λ +

(cid:90)

d
(cid:88)

(cid:90)

i=1

The Optimized SMMD over the restricted set Ψκ is given by:

Dµ,Ψκ,λ

SMMD (P, Q) := sup
ψ∈Ψκ

SMMDµ,kψ,λ .

The constraint to ψ ∈ Ψκ is critical to the proof. In practice, using a spectral parametrization helps
enforce this assumption, as shown in Figures 2 and 9. Other regularization methods, like orthogonal
normalization [11], are also possible.

We will use the following assumptions:

(I) µ is a probability distribution absolutely continuous with respect to the Lebesgue measure.
(II) The dimensions of the weights are decreasing per layer: dl+1 ≤ dl for all 0 ≤ l ≤ L − 1.
(III) The non-linearity used is Leaky-ReLU, (16), with leak coefﬁcient α ∈ (0, 1).
(IV) The top-level kernel K is globally Lipschitz in the RKHS norm: there exists a positive
constant LK > 0 such that (cid:107)K(a, .) − K(b, .)(cid:107) ≤ LK(cid:107)a − b(cid:107) for all a and b in RdL.

(V) There is some γK > 0 for which K satisﬁes

∇b∇cK(b, c)(cid:12)

(cid:12)(b,c)=(a,a) (cid:23) γ2I

for all a ∈ RdL.

(19)

Assumption (I) ensures that the points where φψ(X) is not differentiable are reached with probability
0 under µ. This assumption can be easily satisﬁed e.g. if we deﬁne µ by adding Gaussian noise to P.
Assumption (II) helps ensure that the span of W l is never contained in the null space of W l+1. Using
Leaky-ReLU as a non-linearity, Assumption (III), further ensures that the network φψ is locally
full-rank almost everywhere; this might not be true with ReLU activations, where it could be always
0. Assumptions (II) and (III) can be easily satisﬁed by design of the network.

Assumptions (IV) and (V) only depend on the top-level kernel K and are easy to satisfy in practice.
In particular, they always hold for a smooth translation-invariant kernel, such as the Gaussian, as well
as the linear kernel.

We are now ready to prove Theorem 1.
Theorem 1. Under Assumptions (I) to (V),

Dµ,Ψκ,λ

SMMD (P, Q) ≤

LK κL/2
√

dL αL/2

γ

W(P, Q),

which implies that if Pn

W−→ P, then Dµ,Ψκ,λ

SMMD (Pn, P) → 0.

Proof. Deﬁne the pseudo-distance corresponding to the kernel kψ

dψ(x, y) = (cid:107)kψ(x, ·) − kψ(y, ·)(cid:107)Hψ =

kψ(x, x) + kψ(y, y) − 2kψ(x, y).

(cid:113)

Denote by W dψ (P, Q) the optimal transport metric between P and Q using the cost dψ, given by

W dψ (P, Q) = inf

E(X,Y )∼π [dψ(X, Y )] .

π∈Π(P,Q)

where Π is the set of couplings with marginals P and Q. By Lemma 7,

MMDψ(P, Q) ≤ W dψ (P, Q).

Recall that φψ is Lipschitz, (cid:107)φψ(cid:107)Lip < ∞, so along with Assumption (IV) we have that

dψ(x, y) ≤ LK(cid:107)φψ(x) − φψ(y)(cid:107) ≤ LK(cid:107)φψ(cid:107)Lip(cid:107)x − y(cid:107).

16

Thus

so that

W dψ (P, Q) ≤ inf

E(X,Y )∼π [LK(cid:107)φψ(cid:107)Lip(cid:107)X − Y (cid:107)] = LK(cid:107)φψ(cid:107)Lip W(P, Q),

π∈Π(P,Q)

where W is the standard Wasserstein distance (2), and so

MMDψ(P, Q) ≤ Lk(cid:107)φψ(cid:107)Lip W(P, Q).

We have that ∂i∂i+dk(x, y) = [∂iφψ(x)]T (cid:104)
∇a∇bK(a, b)(cid:12)
[∂iφψ(y)] , where the
middle term is a dL × dL matrix and the outer terms are vectors of length dL. Thus Assumption (V)
implies that ∂i∂i+dk(x, x) ≥ γ2

(cid:12)(a,b)=(φψ(x),φψ(y))

K(cid:107)∂iφψ(x)(cid:107)2, and hence

(cid:105)

σ−2
µ,k,λ ≥ γ2

K

E[(cid:107)∇φψ(X)(cid:107)2
F ]

SMMD2

ψ(P, Q) = σ2

µ,k,λ MMD2

ψ(P, Q) ≤

K(cid:107)φψ(cid:107)2
L2
E [(cid:107)∇φψ(X)(cid:107)2
F ]

Lip

γ2
K

W 2(P, Q).

Using Lemma 8, we can write φψ(X) = α(ψ)φ ¯ψ(X) with ¯ψ ∈ Ψκ

1 . Then we have

(cid:107)φψ(cid:107)2
Lip
(cid:107)∇φψ(X)(cid:107)2
F

(cid:104)

Eµ

(cid:105) =

α(ψ)2(cid:107)φ ¯ψ(cid:107)2
(cid:104)(cid:13)
(cid:13)∇φ ¯ψ(X)(cid:13)
2
(cid:13)
F

Lip

α(ψ)2 Eµ

(cid:105) ≤

1
(cid:104)(cid:13)
(cid:13)∇φ ¯ψ(X)(cid:13)
2
(cid:13)
F

(cid:105) ,

Eµ

where we used (cid:107)φ ¯ψ(cid:107)Lip ≤ (cid:81)L
(cid:107)∇φ ¯ψ(X)(cid:107)2

F ≥ dL(α/κ)L. Using Assumption (I), this implies that

l=1(cid:107) ¯W l(cid:107) = 1. But by Lemma 9, for Lebesgue-almost all X,

(cid:107)φψ(cid:107)2
Lip
(cid:107)∇φψ(X)(cid:107)2
F

(cid:104)

Eµ

(cid:105) ≤

1
(cid:2)(cid:107)∇φ ¯ψ(X)(cid:107)2

F ](cid:3) ≤

Eµ

κL
dLαL .

Thus for any ψ ∈ Ψκ,

SMMDψ(P, Q) ≤

LK κL/2
√

γK

dL αL/2

W(P, Q).

The desired bound on Dµ,Ψκ,λ

SMMD follows immediately.

Lemma 7. Let (x, y) (cid:55)→ k(x, y) be the continuous kernel of an RKHS H deﬁned on a Polish space X ,
and deﬁne the corresponding pseudo-distance dk(x, y) := (cid:107)k(x, ·) − k(y, ·)(cid:107)H. Then the following
inequality holds for any distributions P and Q on X , including when the quantities are inﬁnite:

MMDk(P, Q) ≤ W dk (P, Q).

Proof. Let P and Q be two probability distributions, and let Π(P, Q) be the set of couplings between
them. Let π∗ ∈ argmin(X,Y )∼π[ck(X, Y )] be an optimal coupling, which is guaranteed to exist
[51, Theorem 4.1]; by deﬁnition W dk (P, Q) = E(X,Y )∼π∗ [dk(X, Y )]. When W dk (P, Q) = ∞ the
inequality trivially holds, so assume that W dk (P, Q) < ∞.
Take a sample (X, Y ) ∼ π(cid:63) and a function f ∈ H with (cid:107)f (cid:107)H ≤ 1. By the Cauchy-Schwarz
inequality,

(cid:107)f (X) − f (Y )(cid:107) ≤ (cid:107)f (cid:107)H(cid:107)k(X, ·) − k(Y, ·)(cid:107)H ≤ (cid:107)k(X, ·) − k(Y, ·)(cid:107)H.

Taking the expectation with respect to π(cid:63), we obtain

Eπ(cid:63) [|f (X) − f (Y )|] ≤ Eπ(cid:63) [(cid:107)k(X, ·) − k(Y, ·)(cid:107)H].
The right-hand side is just the deﬁnition of W dk (P, Q). By Jensen’s inequality, the left-hand side is
lower-bounded by

|Eπ∗ [f (X) − f (Y )]| = |EX∼P[f (X)] − EY ∼Q[f (Y )]|

since π(cid:63) has marginals P and Q. We have shown so far that for any f ∈ H with (cid:107)f (cid:107)H ≤ 1,

the result follows by taking the supremum over f .

|EP[f (X)] − EQ[f (Y )]| ≤ W ck (P, Q);

17

Lemma 8. Let ψ = ((W L, bL), (W L−1, bL−1), . . . , (W 1, b1)) ∈ Ψκ. There exists a corresponding
scalar α(ψ) and ¯ψ = (( ¯W L, ¯bL), ( ¯W L−1, ¯bL−1), . . . , ( ¯W 1, ¯b1)) ∈ Ψκ
1 , deﬁned by (18), such that
for all X,

φψ(X) = α(ψ) φ ¯ψ(X).

Proof. Set ¯W l = 1
1
m=1(cid:107)W m(cid:107)
number is unchanged, cond( ¯W l) = cond(W l) ≤ κ, and (cid:107) ¯W l(cid:107) = 1, so ¯ψ ∈ Φκ
see from (16) that

l=1(cid:107)W l(cid:107). Note that the condition
1 . It is also easy to

(cid:107)W l(cid:107) W l, ¯bl =

bl, and α(ψ) = (cid:81)L

(cid:81)l

so that

hl
¯ψ(X) =

1
m=1(cid:107)W m(cid:107)

(cid:81)l

hl
ψ(X)

α(ψ)hL

¯ψ(X) =

hL
ψ(X) = φψ(X).

(cid:81)L

(cid:81)L

l=1(cid:107)W l(cid:107)
l=1(cid:107)W l(cid:107)

Lemma 9. Make Assumptions (II) and (III), and let ψ ∈ Ψκ
intermediate activation is exactly zero,

1 . Then the set of inputs for which any

has zero Lebesgue measure. Moreover, for any X /∈ N ψ, ∇X φψ(X) exists and

Nψ =

L
(cid:91)

dl(cid:91)

l=1

k=1

(cid:110)
X ∈ Rd | (cid:0)hl

ψ(X)(cid:1)

k

(cid:111)

= 0

,

(cid:107)∇X φψ(X)(cid:107)2

F ≥

dLαL
κL .

(M l

X )k = σ(cid:48)

l(hl

k(X)) =

(cid:26)1 hl
α hl

k(X) > 0
k(X) < 0

;

Proof. First, note that the network representation at layer l is piecewise afﬁne. Speciﬁcally, deﬁne
M l

X ∈ Rdl by, using Assumption (III),

it is undeﬁned when any hl

k(X) = 0, i.e. when X ∈ N ψ. Let V l

ψ(X) = W lσl−1(hl−1
hl

ψ (X)) + bl = V l

X := W l diag (cid:0)M l−1
X X + bl,

X

(cid:1). Then

ψ(X) = Wl
hl
X = V l
X = V l
1 , we have (cid:107)W l(cid:107) = 1 and σmin(W l) ≥ 1/κ; also, (cid:107)M l
X (cid:107) ≤ 1, and using Assumption (II) with Lemma 10 gives σmin(Wl

X X + bl
X V l−1

X bl−1 + bl, and Wl

X · · · V 1

X ,

X , so long as X /∈ N ψ.

X (cid:107) ≤ 1, σmin(M l

X ) ≥ α.
X ) ≥ (α/κ)l. In

(20)

and thus

where b0

X = 0, bl
Because ψ ∈ Ψκ
Thus (cid:107)Wl
particular, each Wl

Next, note that bl
H l

X = (M l

X , M l−1

X is full-rank.
X and Wl
X , . . . , M 1

X each only depend on X through the activation patterns M l
X ) denote the full activation patterns up to level l, we can thus write

X . Letting

ψ(X) = WH l
hl

X X + bH l

X .

There are only ﬁnitely many possible values for H l
we have that

X ; we denote the set of such values as Hl. Then

L
(cid:91)

dL(cid:91)

(cid:91)

Nψ ⊆

l=0

k=1

H l∈Hl

(cid:110)
X ∈ Rd | WH l

k X + bH l

k = 0

(cid:111)

.

Because each WH l
is of rank dl, each set in the union is either empty or an afﬁne subspace of
k
dimension d − dl. As each dl > 0, each set in the ﬁnite union has zero Lebesgue measure, and Nψ
also has zero Lebesgue measure.

18

We will now show that the activation patterns are piecewise constant, so that ∇X hl
1 , we have (cid:107)hl
all X /∈ N ψ. Because ψ ∈ Ψκ
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12) ≤ (cid:107)X − X (cid:48)(cid:107).
(cid:12)

ψ(cid:107)Lip ≤ 1, and in particular

ψ(X (cid:48))(cid:1)

ψ(X)(cid:1)

− (cid:0)hl

(cid:0)hl

k

k

ψ(X) = WH l

X for

Thus,
minl=1,...,L mink=1,...,dl
for all l and k,

take some X /∈ N ψ, and ﬁnd the smallest absolute value of its activations, (cid:15) =
(cid:12)
(cid:12)
(cid:12); clearly (cid:15) > 0. For any X (cid:48) with (cid:107)X − X (cid:48)(cid:107) < (cid:15), we know that

(cid:17)
hl
ψ(X)

(cid:12)
(cid:12)
(cid:12)

(cid:16)

k

sign

(cid:16)(cid:0)hl

ψ(X)(cid:1)

(cid:17)

k

= sign

(cid:16)(cid:0)hl

ψ(X (cid:48))(cid:1)

(cid:17)

,

k

implying that H l
Finally, we obtain

X = H l

X (cid:48) as well as X (cid:48) /∈ N ψ. Thus for any point X /∈ N ψ, ∇φψ(X) = WH L
X .

(cid:107)∇φψ(X)(cid:107)2

F = (cid:107)WH L

X (cid:107)2

F ≥ dL σmin

(cid:16)

WH L

X

(cid:17)2

≥

dLαL
κL .

Lemma 10. Let A ∈ Rm×n, B ∈ Rn×p, with m ≥ n ≥ p. Then σmin(AB) ≥ σmin(A) σmin(B).

Proof. A more general version of this result can be found in [19, Theorem 2]; we provide a proof
here for completeness.
If B has a nontrivial null space, σmin(B) = 0 and the inequality holds. Otherwise, let Rn
Rn \ {0}. Recall that for C ∈ Rm×n with m ≥ n,
(cid:115)

∗ denote

(cid:113)

σmin(C) =

λmin(C TC) =

xTC TCx
xTx

inf
x∈Rn
∗

= inf
x∈Rn
∗

(cid:107)Cx(cid:107)
(cid:107)x(cid:107)

.

Thus, as Bx (cid:54)= 0 for x (cid:54)= 0,

σmin(AB) = inf
x∈Rp
∗
(cid:18)

(cid:107)ABx(cid:107)
(cid:107)x(cid:107)
(cid:107)ABx(cid:107)
(cid:107)Bx(cid:107)
(cid:107)Ay(cid:107)
(cid:107)y(cid:107)

= inf
x∈Rp
∗

(cid:19) (cid:18)

(cid:107)ABx(cid:107)(cid:107)Bx(cid:107)
(cid:107)Bx(cid:107)(cid:107)x(cid:107)
(cid:19)
(cid:107)Bx(cid:107)
inf
(cid:107)x(cid:107)
x∈Rp
∗
(cid:19)
(cid:107)Bx(cid:107)
(cid:107)x(cid:107)

inf
x∈Rp
∗

(cid:19) (cid:18)

≥

≥

(cid:18)

inf
x∈Rp
∗

inf
y∈Rn
∗

= σmin(A) σmin(B).

A.2.1 When some of the assumptions don’t hold

Here we analyze through simple examples what happens when the condition number can be un-
bounded, and when Assumption (II), about decreasing widths of the network, is violated.

Condition Number: We start by a ﬁrst example where the condition number can be arbitrarily
high. We consider a two-layer network on R2, deﬁned by

φα(X) = [1 −1] σ(WαX)

Wα =

(21)

(cid:20)1
1

(cid:21)

1
1 + α

where α > 0. As α approaches 0 the matrix Wα becomes singular which means that its condition
number blows up. We are interested in analyzing the behavior of the Lipschitz constant of φ and the
expected squared norm of its gradient under µ as α approaches 0.

One can easily compute the squared norm of the gradient of φ which is given by

(cid:107)∇φα(X)(cid:107)2 =






α2
X ∈ A1
γ2α2
X ∈ A2
(1 − γ)2 + (1 + α − γ)2 X ∈ A3
(1 − γ)2 + (γα + γ − 1)2 X ∈ A4

(22)

19

Here A1, A2, A3 and A4 are deﬁned by (23) and are represented in Figure 4:
A1 := {X ∈ R2|X1 + X2 ≥ 0 X1 + (1 + α)X2 ≥ 0}
A2 := {X ∈ R2|X1 + X2 < 0 X1 + (1 + α)X2 < 0}
A3 := {X ∈ R2|X1 + X2 < 0 X1 + (1 + α)X2 ≥ 0}
A4 := {X ∈ R2|X1 + X2 ≥ 0 X1 + (1 + α)X2 < 0}

(23)

(24)

(25)

(26)

Figure 4: Decomposition of R2 into 4 regions A1, A2, A3 and A4 as deﬁned in (23). As α approaches
0, the area of sets A3 and A4 becomes negligible.

It is easy to see that whenever µ has a density, the probability of the sets A3 and A4 goes to 0 are
α → 0. Hence one can deduce that Eµ[(cid:107)∇φα(X)(cid:107)2] → 0 when α → 0. On the other hand, the
squared Lipschitz constant of φ is given by (1 − γ)2 + (1 + α − γ)2 which converges to 2(1 − γ)2.
This shows that controlling the expectation of the gradient doesn’t allow to effectively control the
Lipschitz constant of φ.

Monotonicity of the dimensions: We would like to consider a second example where Assump-
tion (II) doesn’t hold. Consider the following two layer network deﬁned by:
(cid:35)

φ(X) = [−1

0

1] σ(WβX)

Wβ :=

(cid:34)1
0
1
0
1 β

for β > 0. Note that Wβ is a full rank matrix, but Assumption (II) doesn’t hold. Depending on the
sign of the components of WβX one has the following expression for (cid:107)∇φα(X)(cid:107)2:

where (Bi)1≤i≤6 are deﬁned by (26)

(cid:107)∇φα(X)(cid:107)2 =






β2
X ∈ B1
γ2β2
X ∈ B2
β2
X ∈ B3
(1 − γ)2 + γ2β2 X ∈ B4
(1 − γ)2 + β2
X ∈ B5
γ2β2
X ∈ B6

B1 := {X ∈ R2|X1 ≥ 0 X2 ≥ 0}
B2 := {X ∈ R2|X1 < 0 X2 < 0}
B3 := {X ∈ R2|X1 ≥ X2 < 0 X1 + βX2 ≥ 0}
B4 := {X ∈ R2|X1 ≥ X2 < 0 X1 + βX2 < 0}
B5 := {X ∈ R2|X1 > 0 X2 ≥ 0 X1 + βX2 ≥ 0}
B6 := {X ∈ R2|X1 > 0 X2 ≥ 0 X1 + βX2 < 0}

20

The squared Lipschitz constant is given by (cid:107)φ(cid:107)2
the gradient of φ is given by:

L(1 − γ)2 + β2 while the expected squared norm of

Eµ[(cid:107)φ(X)(cid:107)2] = 3β2(p(B1 ∪ B3 ∪ B5) + γ2p(B2 ∪ B4 ∪ B6)) + (1 − γ)2p(B4 ∪ B5).

(27)
Again the set B4 ∪ B5 becomes negligible as β approaches 0 which implies that Eµ[(cid:107)φ(X)(cid:107)2] → 0.
L converges to (1 − γ)2. Note that unlike in the ﬁrst example in (21), the
On the other hand (cid:107)φ(cid:107)2
matrix Wβ has a bounded condition number. In this example, the columns of W0 are all in the null
1], which implies ∇φ0(X) = 0 for all X ∈ R2, even though all matrices have full
space of [−1 0
rank.

B DiracGAN vector ﬁelds for more losses

Figure 5: Vector ﬁelds for different losses with respect to the generator parameter θ and the feature
representation parameter ψ; the losses use a Gaussian kernel, and are shown in (28). Following [30],
P = δ0, Q = δθ and φψ(x) = ψx. The curves show the result of taking simultaneous gradient steps
in (θ, ψ) beginning from three initial parameter values.

Figure 5 shows parameter vector ﬁelds, like those in Figure 6, for Example 1 for a variety of different
losses:

MMD: − MMD2
ψ
MMD-GP: − MMD2
ψ +λ EP[((cid:107)∇f (X)(cid:107) − 1)2]
MMD-GP-Unif: − MMD2
ψ +λ E

(cid:101)X(cid:39)µ∗ [((cid:107)∇f ( (cid:101)X)(cid:107) − 1)2]

SN-MMD: − 2 MMD1(P, Q)2

Sobolev-MMD: − MMD2
CenteredSobolev-MMD: − MMD2

ψ +λ(E(P+Q)/2[(cid:107)∇f (X)(cid:107)2] − 1)2
ψ +λ(E(P+Q)/2[(cid:107)∇f (X)(cid:107)2])2

(28)

LipMMD: − LipMMD2
GC-MMD: − GCMMD2
SMMD: − SMMD2

kψ,λ

kψ,P,λ

N (0,102),kψ,λ

21

2

The squared MMD between δ0 and δθ under a Gaussian kernel of bandwidth 1/ψ and is given by
2(1 − e− ψ2θ2
). MMD-GP-unif uses a gradient penalty as in [7] where each samples from µ∗ is
obtained by ﬁrst sampling X and Y from P and Q and then sampling uniformly between X and
Y . MMD-GP uses the same gradient penalty, but the expectation is taken under P rather than µ∗.
SN-MMD refers to MMD with spectral normalization; here this means that ψ = 1. Sobolev-MMD
refers to the loss used in [33] with the quadratic penalty only. GCMMDµ,k,λ is deﬁned by (5), with
µ = N (0, 102).

C Vector ﬁelds of Gradient-Constrained MMD and Sobolev GAN critics

Mroueh et al. [33] argue that the gradient of the critic (...) deﬁnes a transportation plan for moving
the distribution mass (from generated to reference distribution) and present the solution of Sobolev
PDE for 2-dimensional Gaussians. We observed that in this simple example the gradient of the
Sobolev critic can be very high outside of the areas of high density, which is not the case with the
Gradient-Constrained MMD. Figure 6 presents critic gradients in both cases, using µ = (P + Q)/2
for both.

(a) Gradient-Constrained MMD critic gradient.

(b) Sobolev IPM critic gradient.

Figure 6: Vector ﬁelds of critic gradients between two Gaussians. The grey arrows show normalized
gradients, i.e. gradient directions, while the black ones are the actual gradients. Note that for the
Sobolev critic, gradients norms are orders of magnitudes higher on the right hand side of the plot
than in the areas of high density of the given distributions.

This unintuitive behavior is most likely related to the vanishing boundary condition, assummed by
Sobolev GAN. Solving the actual Sobolev PDE, we found that the Sobolev critic has very high
gradients close to the boundary in order to match the condition; moreover, these gradients point in
opposite directions to the target distribution.

D An estimator for Lipschitz MMD

We now describe brieﬂy how to estimate the Lipschitz MMD in low dimensions. Recall that

LipMMDk,λ(P, Q) =

f ∈Hk : (cid:107)f (cid:107)2

sup
Lip+λ(cid:107)f (cid:107)2

≤1

Hk

EX∼P[f (X)] − EX∼Q[f (Y )].

For f ∈ Hk, it is the case that

(cid:107)f (cid:107)2

Lip = sup
x∈Rd

(cid:107)∇f (x)(cid:107)2 = sup
x∈Rd

(cid:104)∂ik(x, ·), f (cid:105)2

Hk

= sup
x∈Rd

f,

[∂ik(x, ·) ⊗ ∂ik(x, ·)] f

.

d
(cid:88)

i=1

(cid:42)

d
(cid:88)

i=1

(cid:43)

Hk

Thus we can approximate the constraint (cid:107)f (cid:107)2
Lip + λ(cid:107)f (cid:107)2
≤ 1 by enforcing the constraint on a set
Hk
of m points {Zi} reasonably densely covering the region around the supports of P and Q, rather

22

than enforcing it at every point in X . An estimator of the Lipschitz MMD based on X ∼ PnX and
Y ∼ QnY is

(cid:92)LipMMDk,λ (X, Y, Z) ≈ sup

nX(cid:88)

1
nX

f (Xj) −

nY(cid:88)

f (Yj)

1
nY

f ∈Hk

j=1

j=1
s.t. ∀j, (cid:107)∇f (Zj)(cid:107)2 + λ(cid:107)f (cid:107)2

≤ 1.

Hk

(29)

By the generalized representer theorem, the optimal f for (29) will be of the form

f (·) =

αjk(Xj, ·) +

βjk(Yj, ·) +

γ(i,j)∂ik(Zj, ·).

nX(cid:88)

j=1

nY(cid:88)

j=1

d
(cid:88)

m
(cid:88)

i=1

j=1

Writing δ = (α, β, γ), the objective function is linear in δ,
· · · − 1
nY

− 1
nY

(cid:2) 1
nX

1
nX

· · ·

0

· · ·

0(cid:3) δ.

The constraints are quadratic, built from the following matrices, where the X and Y samples are
concatenated together, as are the derivatives with each dimension of the Z samples:
















K :=

B :=

H :=

k(X1, X1)
...
k(YnX , X1)
∂1k(Z1, X1)
...
∂dk(Zm, X1)

∂1∂1+dk(Z1, Z1)
...
∂d∂1+dk(Zm, Z1)

· · ·
. . .
· · ·

k(X1, YnY )
...
k(YnY , YnY )






· · ·
. . .
· · ·






∂1k(Z1, YnY )
...
∂dk(Zm, YnY )
· · ·
. . .
· · ·

∂1∂d+dk(Z1, Zm)
...
∂d∂d+dk(Zm, Zm)




 .

Given these matrices, and letting Oj = (cid:80)d
vector in Rmd, we have that
(cid:21)
(cid:20)K BT
B H

= δT

(cid:107)f (cid:107)2

Hk

δ

(cid:107)∇f (Zj)(cid:107)2 =

i=1 e(i,j)eT

(i,j) where e(i,j) is the (i, j)th standard basis

d
(cid:88)

i=1

(∂if (Zj))2 = δT

(cid:20)BTOjB BTOjH
HOjB HOjH

(cid:21)

δ.

Thus the optimization problem (29) is a linear problem with convex quadratic constraints, which can
be solved by standard convex optimization software. The approximation is reasonable only if we can
effectively cover the region of interest with densely spaced {Zi}; it requires a nontrivial amount of
computation even for the very simple 1-dimensional toy problem of Example 1.

One advantage of this estimator, though, is that ﬁnding its derivative with respect to the input points
or the kernel parameterization is almost free once we have computed the estimate, as long as our
solver has computed the dual variables µ corresponding to the constraints in (29). We just need to
exploit the envelope theorem and then differentiate the KKT conditions, as done for instance in [1].
The differential of (29) ends up being, assuming the optimum of (29) is at ˆδ ∈ RnX +nY +md and
ˆµ ∈ Rm,

d (cid:92)LipMMDk,λ(X, Y, Z) = ˆδT

· · ·

1
nX

− 1
nY

· · · − 1
nY

(cid:3)T

−

ˆµj

ˆδT(dPj)ˆδ

(cid:21)

(cid:20)dK
dB

(cid:2) 1
nX

m
(cid:88)

j=1

dPj :=

(cid:20)(dB)TOjB + BTOj(dH)
(dH)OjB + HOj(dB)

(dB)TOjH + BTOj(dH)
(dH)OjH + HOj(dH)

(cid:21)

+ λ

(cid:20)dK dBT
dB dH

(cid:21)

.

E Near-equivalence of WGAN and linear-kernel MMD GANs

For an MMD GAN-GP with kernel k(x, y) = φ(x)φ(y), we have that
MMDk(P, Q) = |EP φ(x) − EQ φ(Y )|

23

and the corresponding critic function is

η(t)
(cid:107)η(cid:107)H

=

EX∼P φ(X)φ(t) − EY ∼Q φ(Y )φ(t)
|EP φ(X) − EQ φ(Y )|

= sign (EX∼P φ(X) − EY ∼Q φ(Y )) φ(t).

Thus if we assume EX∼P φ(X) > EY ∼Q φ(Y ), as that is the goal of our critic training, we see that
the MMD becomes identical to the WGAN loss, and the gradient penalty is applied to the same
function.
(MMD GANs, however, would typically train on the unbiased estimator of MMD2, giving a very
slightly different loss function. [7] also applied the gradient penalty to η rather than the true critic
η/(cid:107)η(cid:107).)

The SMMD with a linear kernel is thus analogous to applying the scaling operator to a WGAN; hence
the name SWGAN.

F Additional experiments

F.1 Comparison of Gradient-Constrained MMD to Scaled MMD

Figure 7 shows the behavior of the MMD, the Gradient-Constrained SMMD, and the Scaled MMD
when comparing Gaussian distributions. We can see that MMD ∝ SMMD and the Gradient-
Constrained MMD behave similarly in this case, and that optimizing the SMMD and the Gradient-
Constrained MMD is also similar. Optimizing the MMD would yield an essentially constant distance.

F.2 IGMs with Optimized Gradient-Constrained MMD loss

We implemented the estimator of Proposition 3 using the empirical mean estimator of η, and sharing
samples for µ = P. To handle the large but approximately low-rank matrix system, we used an
incomplete Cholesky decomposition [43, Algorithm 5.12] to obtain R ∈ R(cid:96)×M (1+d) such that
(cid:20)K GT
G H

≈ RTR. Then the Woodbury matrix identity allows an efﬁcient evaluation:

(cid:21)

(cid:0)RTR + M λI(cid:1)−1

=

(cid:0)I − R(RRT + M λI)−1R(cid:1) .

1
M λ

Even though only a small (cid:96) is required for a good approximation, and the full matrices K, G, and
H need never be constructed, backpropagation through this procedure is slow and not especially
GPU-friendly; training on CPU was faster. Thus we were only able to run the estimator on MNIST,
and even that took days to conduct the optimization on powerful workstations.

The learned models, however, were reasonable. Using a DCGAN architecture, batches of size 64,
and a procedure that otherwise agreed with the setup of Section 4, samples with and without spectral
normalization are shown in Figures 8a and 8b. After the points in training shown, however, the same
rank collapse as discussed in Section 4 occurred. Here it seems that spectral normalization may have
delayed the collapse, but not prevented it. Figure 8c shows generator loss estimates through training,
including the obvious peak at collapse; Figure 8d shows KID scores based on the MNIST-trained
convnet representation [7], including comparable SMMD models for context. The fact that SMMD
models converged somewhat faster than Gradient-Constrained MMD models here may be more
related to properties of the estimator of Proposition 3 rather than the distances; more work would be
needed to fully compare the behavior of the two distances.

F.3 Spectral normalization and Scaled MMD

Figure 9 shows the distribution of critic weight singular values, like Figure 2, at more layers. Figure 11
and Table 2 show results for the spectral normalization variants considered in the experiments.
MMDGAN, with neither spectral normalization nor a gradient penalty, did surprisingly well in this
case, though it fails badly in other situations.

Figure 9 compares the decay of singular values for layer of the critic’s network at both early and
later stages of training in two cases: with or without the spectral parametrization. The model was
trained on CelebA using SMMD. Figure 11 shows the evolution per iteration of Inception score,

24

Figure 7: Plots of various distances between one dimensional Gaussians, where P = N (0, 0.12), and
the colors show log D(P, N (µ, σ2)). All distances use λ = 1. Top left: MMD with a Gaussian kernel
of bandwidth ψ = 0.1. Top right: MMD with bandwidth ψ = 10. Middle left: Gradient-Constrained
MMD with bandwidth ψ = 0.1. Middle right: Gradient-Constrained MMD with bandwidth ψ = 10.
Bottom left: Optimized SSMD, allowing any ψ ∈ R. Bottom right: Optimized Gradient-Constrained
MMD.

25

(a) Without spectral
normalization; 32 000
generator iterations.

(b) With
spectral
normalization; 41 000
generator iterations.

Figure 8: The MNIST models with Optimized Gradient-Constrained MMD loss.

(c) Generator losses.

(d) KID scores.

FID and KID for Sobolev-GAN, MMDGAN and variants of MMDGAN and WGAN using spectral
normalization. It is often the case that this parametrization alone is not enough to achieve good
results.

Figure 9: Singular values at different layers, for the same setup as Figure 2.

F.4 Additional samples

Figures 12 and 13 give extra samples from the models.

26

µ,k,λ MMD2

k for SMMDGAN and SN-SMMDGAN, and MMD2

Figure 10: Evolution of various quantities per generator iteration on CelebA during training. 4
models are considered: (SMMDGAN, SN-SMMDGAN, MMDGAN, SN-MMDGAN). (a) Loss:
SMMD2 = σ2
k for MMDGAN and
SN-MMDGAN. The loss saturates for MMDGAN (green); spectral normalization allows some
improvement in loss, but training is still unstable (orange). SMMDGAN and SN-SMMDGAN both
lead to stable, fast training (blue and red). (b) SMMD controls the critic complexity well, as expected
(blue and red); SN has little effect on the complexity (orange). (c) Ratio of the highest singular value
to the smallest for the ﬁrst layer of the critic network: σmax/σmin. SMMD tends to increase the
condition number of the weights during training (blue), while SN helps controlling it (red). (d) KID
score during training: Only variants using SMMD lead to stable training in this case.

Figure 11: Evolution per iteration of different scores for variants of methods, mostly using spectral
normalization, on CIFAR-10.

27

Table 2: Mean (standard deviation) of score evaluations on CIFAR-10 for different methods using
Spectral Normalization.

Method

MMDGAN
SN-WGAN
SN-WGAN-GP
SN-Sobolev-GAN
SN-MMDGAN-GP
SN-MMDGAN-L2
SN-MMDGAN
SN-MMDGAN-GP-L2
SN-SMMDGAN

IS

FID

KID×103

5.5±0.0
2.2±0.0
2.5±0.0
2.9±0.0
4.6±0.1
7.1±0.1
6.9±0.1
6.9±0.2
7.3±0.1

73.9±0.1
208.5±0.2
154.3±0.2
140.2±0.2
96.8±0.4
31.9±0.2
31.5±0.2
32.3±0.3
25.0±0.3

39.4±1.5
178.9±1.5
125.3±0.9
130.0±1.9
59.5±1.4
21.7±0.9
21.7±1.0
20.9±1.1
16.6±2.0

Figure 12: Samples from a generator trained on ImageNet dataset using Scaled MMD with Spectral
Normalization: SN-SMMDGAN.

28

(a) SNGAN

(b) SobolevGAN

(c) MMDGAN-GP-L2

(d) SN-SMMD GAN

(e) SN SWGAN

(f) SMMD GAN

Figure 13: Comparison of samples from different models trained on CelebA with 160×160 resolution.

29

8
1
0
2
 
v
o
N
 
9
2
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
5
6
5
1
1
.
5
0
8
1
:
v
i
X
r
a

On gradient regularizers for MMD GANs

Michael Arbel∗
Gatsby Computational Neuroscience Unit
University College London
michael.n.arbel@gmail.com

Dougal J. Sutherland∗
Gatsby Computational Neuroscience Unit
University College London
dougal@gmail.com

Mikołaj Bi ´nkowski
Department of Mathematics
Imperial College London
mikbinkowski@gmail.com

Arthur Gretton
Gatsby Computational Neuroscience Unit
University College London
arthur.gretton@gmail.com

Abstract

We propose a principled method for gradient-based regularization of the critic of
GAN-like models trained by adversarially optimizing the kernel of a Maximum
Mean Discrepancy (MMD). We show that controlling the gradient of the critic
is vital to having a sensible loss function, and devise a method to enforce exact,
analytical gradient constraints at no additional cost compared to existing approxi-
mate techniques based on additive regularizers. The new loss function is provably
continuous, and experiments show that it stabilizes and accelerates training, giving
image generation models that outperform state-of-the art methods on 160 × 160
CelebA and 64 × 64 unconditional ImageNet.

1

Introduction

There has been an explosion of interest in implicit generative models (IGMs) over the last few years,
especially after the introduction of generative adversarial networks (GANs) [16]. These models
allow approximate samples from a complex high-dimensional target distribution P, using a model
distribution Qθ, where estimation of likelihoods, exact inference, and so on are not tractable. GAN-
type IGMs have yielded very impressive empirical results, particularly for image generation, far
beyond the quality of samples seen from most earlier generative models [e.g. 18, 22, 23, 24, 38].

These excellent results, however, have depended on adding a variety of methods of regularization and
other tricks to stabilize the notoriously difﬁcult optimization problem of GANs [38, 42]. Some of
this difﬁculty is perhaps because when a GAN is viewed as minimizing a discrepancy DGAN(P, Qθ),
its gradient ∇θ DGAN(P, Qθ) does not provide useful signal to the generator if the target and model
distributions are not absolutely continuous, as is nearly always the case [2].

An alternative set of losses are the integral probability metrics (IPMs) [36], which can give credit to
models Qθ “near” to the target distribution P [3, 8, Section 4 of 15]. IPMs are deﬁned in terms of a
critic function: a “well behaved” function with large amplitude where P and Qθ differ most. The IPM
is the difference in the expected critic under P and Qθ, and is zero when the distributions agree. The
Wasserstein IPMs, whose critics are made smooth via a Lipschitz constraint, have been particularly
successful in IGMs [3, 14, 18]. But the Lipschitz constraint must hold uniformly, which can be hard
to enforce. A popular approximation has been to apply a gradient constraint only in expectation [18]:
the critic’s gradient norm is constrained to be small on points chosen uniformly between P and Q.

Another class of IPMs used as IGM losses are the Maximum Mean Discrepancies (MMDs) [17],
as in [13, 28]. Here the critic function is a member of a reproducing kernel Hilbert space (except
in [50], who learn a deep approximation to an RKHS critic). Better performance can be obtained,

∗These authors contributed equally.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.

however, when the MMD kernel is not based directly on image pixels, but on learned features of
images. Wasserstein-inspired gradient regularization approaches can be used on the MMD critic
when learning these features: [27] uses weight clipping [3], and [5, 7] use a gradient penalty [18].

The recent Sobolev GAN [33] uses a similar constraint on the expected gradient norm, but phrases it
as estimating a Sobolev IPM rather than loosely approximating Wasserstein. This expectation can be
taken over the same distribution as [18], but other measures are also proposed, such as (P + Qθ) /2.
A second recent approach, the spectrally normalized GAN [32], controls the Lipschitz constant of
the critic by enforcing the spectral norms of the weight matrices to be 1. Gradient penalties also
beneﬁt GANs based on f -divergences [37]: for instance, the spectral normalization technique of [32]
can be applied to the critic network of an f -GAN. Alternatively, a gradient penalty can be deﬁned
to approximate the effect of blurring P and Qθ with noise [40], which addresses the problem of
non-overlapping support [2]. This approach has recently been shown to yield locally convergent
optimization in some cases with non-continuous distributions, where the original GAN does not [30].

In this paper, we introduce a novel regularization for the MMD GAN critic of [5, 7, 27], which
directly targets generator performance, rather than adopting regularization methods intended to
approximate Wasserstein distances [3, 18]. The new MMD regularizer derives from an approach
widely used in semi-supervised learning [10, Section 2], where the aim is to deﬁne a classiﬁcation
function f which is positive on P (the positive class) and negative on Qθ (negative class), in the
absence of labels on many of the samples. The decision boundary between the classes is assumed
to be in a region of low density for both P and Qθ: f should therefore be ﬂat where P and Qθ have
support (areas with constant label), and have a larger slope in regions of low density. Bousquet et al.
[10] propose as their regularizer on f a sum of the variance and a density-weighted gradient norm.
We adopt a related penalty on the MMD critic, with the difference that we only apply the penalty on P:
thus, the critic is ﬂatter where P has high mass, but does not vanish on the generator samples from Qθ
(which we optimize). In excluding Qθ from the critic function constraint, we also avoid the concern
raised by [32] that a critic depending on Qθ will change with the current minibatch – potentially
leading to less stable learning. The resulting discrepancy is no longer an integral probability metric:
it is asymmetric, and the critic function class depends on the target P being approximated.

We ﬁrst discuss in Section 2 how MMD-based losses can be used to learn implicit generative models,
and how a naive approach could fail. This motivates our new discrepancies, introduced in Section 3.
Section 4 demonstrates that these losses outperform state-of-the-art models for image generation.

2 Learning implicit generative models with MMD-based losses

An IGM is a model Qθ which aims to approximate a target distribution P over a space X ⊆ Rd.
We will deﬁne Qθ by a generator function Gθ : Z → X , implemented as a deep network with
parameters θ, where Z is a space of latent codes, say R128. We assume a ﬁxed distribution on Z,
say Z ∼ Uniform (cid:0)[−1, 1]128(cid:1), and call Qθ the distribution of Gθ(Z). We will consider learning by
minimizing a discrepancy D between distributions, with D(P, Qθ) ≥ 0 and D(P, P) = 0, which we
call our loss. We aim to minimize D(P, Qθ) with stochastic gradient descent on an estimator of D.
In the present work, we will build losses D based on the Maximum Mean Discrepancy,

MMDk(P, Q) =

EX∼P[f (X)] − EY ∼Q[f (Y )],

(1)

sup
f : (cid:107)f (cid:107)Hk ≤1

an integral probability metric where the critic class is the unit ball within Hk, the reproducing
kernel Hilbert space with a kernel k. The optimization in (1) admits a simple closed-form optimal
critic, f ∗(t) ∝ EX∼P[k(X, t)] − EY ∼Q[k(Y, t)]. There is also an unbiased, closed-form estimator of
MMD2
k with appealing statistical properties [17] – in particular, its sample complexity is independent
of the dimension of X , compared to the exponential dependence [52] of the Wasserstein distance

W(P, Q) =

sup
f : (cid:107)f (cid:107)Lip≤1

EX∼P[f (X)] − EY ∼Q[f (Y )].

(2)

The MMD is continuous in the weak topology for any bounded kernel with Lipschitz embeddings [46,
D−→ P, then MMD(Pn, P) → 0.
Theorem 3.2(b)], meaning that if Pn converges in distribution to P, Pn
W−→ P implies
(W is continuous in the slightly stronger Wasserstein topology [51, Deﬁnition 6.9]; Pn

2

D−→ P, and the two notions coincide if X is bounded.) Continuity means the loss can provide
Pn
better signal to the generator as Qθ approaches P, as opposed to e.g. Jensen-Shannon where the loss
could be constant until suddenly jumping to 0 [e.g. 3, Example 1]. The MMD is also strict, meaning
it is zero iff P = Qθ, for characteristic kernels [45]. The Gaussian kernel yields an MMD both
continuous in the weak topology and strict. Thus in principle, one need not conduct any alternating
optimization in an IGM at all, but merely choose generator parameters θ to minimize MMDk.

Despite these appealing properties, using simple pixel-level kernels leads to poor generator samples
[8, 13, 28, 48]. More recent MMD GANs [5, 7, 27] achieve better results by using a parameterized
family of kernels, {kψ}ψ∈Ψ, in the Optimized MMD loss previously studied by [44, 46]:

DΨ

MMD(P, Q) := sup
ψ∈Ψ

MMDkψ (P, Q).

(3)

We primarily consider kernels deﬁned by some ﬁxed kernel K on top of a learned low-dimensional
representation φψ : X → Rs, i.e. kψ(x, y) = K(φψ(x), φψ(y)), denoted kψ = K ◦ φψ. In practice,
K is a simple characteristic kernel, e.g. Gaussian, and φψ is usually a deep network with output
dimension say s = 16 [7] or even s = 1 (in our experiments). If φψ is powerful enough, this choice
is sufﬁcient; we need not try to ensure each kψ is characteristic, as did [27].
Proposition 1. Suppose k = K ◦ φψ, with K characteristic and {φψ} rich enough that for any
P (cid:54)= Q, there is a ψ ∈ Ψ for which φψ#P (cid:54)= φψ#Q.2 Then if P (cid:54)= Q, DΨ
Proof. Let ˆψ ∈ Ψ be such that φ ˆψ(P) (cid:54)= φ ˆψ(Q). Then, since K is characteristic,

MMD(P, Q) > 0.

DΨ

MMD(P, Q) = sup
ψ∈Ψ

MMDK(φψ#P, φψ#Q) ≥ MMDK(φ ˆψ#P, φ ˆψ#Q) > 0.

MMD, one can conduct alternating optimization to estimate a ˆψ and then update the
To estimate DΨ
generator according to MMDk ˆψ
, similar to the scheme used in GANs and WGANs. (This form of
estimator is justiﬁed by an envelope theorem [31], although it is invariably biased [7].) Unlike DGAN
or W, ﬁxing a ˆψ and optimizing the generator still yields a sensible distance MMDk ˆψ
Early attempts at minimizing DΨ
could be because for some kernel classes, DΨ
Example 1 (DiracGAN [30]). We wish to model a point mass at the origin of R, P = δ0, with any
possible point mass, Qθ = δθ for θ ∈ R. We use a Gaussian kernel of any bandwidth, which can be
2 (a − b)2(cid:1). Then
written as kψ = K ◦ φψ with φψ(x) = ψx for ψ ∈ Ψ = R and K(a, b) = exp (cid:0)− 1
θ (cid:54)= 0
2
θ = 0

MMD in an IGM, though, were unsuccessful [48, footnote 7]. This
MMD is stronger than Wasserstein or MMD.

(δ0, δθ) = 2 (cid:0)1 − exp (cid:0)− 1

MMD(δ0, δθ) =

2 ψ2θ2(cid:1)(cid:1) ,

MMD2
kψ

(cid:26)√
0

DΨ

.

.

Considering DΨ
distance is not continuous in the weak or Wasserstein topologies.

2 (cid:54)→ 0, even though δ1/n

MMD(δ0, δ1/n) =

W−→ δ0, shows that the Optimized MMD

√

This also causes optimization issues. Figure 1 (a) shows gradient vector ﬁelds in parameter space,
(δ0, δθ)(cid:1). Some sequences following v (e.g. A)
v(θ, ψ) ∝ (cid:0) − ∇θ MMD2
(δ0, δθ), ∇ψ MMD2
kψ
kψ
converge to an optimal solution (0, ψ), but some (B) move in the wrong direction, and others (C) are
stuck because there is essentially no gradient. Figure 1 (c, red) shows that the optimal DΨ
MMD critic
is very sharp near P and Q; this is less true for cases where the algorithm converged.
We can avoid these issues if we ensure a bounded Lipschitz critic:3
Proposition 2. Assume the critics fψ(x) = (EX∼P kψ(X, x) − EY ∼Q kψ(Y, x))/ MMDkψ (P, Q)
are uniformly bounded and have a common Lipschitz constant: supx∈X ,ψ∈Ψ|fψ(x)| < ∞ and
supψ∈Ψ(cid:107)fψ(cid:107)Lip < ∞. In particular, this holds when kψ = K ◦ φψ and

sup
a∈Rs

K(a, a) < ∞,

(cid:107)K(a, ·) − K(b, ·)(cid:107)HK ≤ LK(cid:107)a − b(cid:107)Rs,

(cid:107)φψ(cid:107)Lip ≤ Lφ < ∞.

sup
ψ∈Ψ

Then DΨ

MMD is continuous in the weak topology: if Pn

D−→ P, then DΨ

MMD(Pn, P) → 0.

2 f #P denotes the pushforward of a distribution: if X ∼ P, then f (X) ∼ f #P.
3[27, Theorem 4] makes a similar claim to Proposition 2, but its proof was incorrect: it tries to uniformly

bound MMDkψ ≤ W 2, but the bound used is for a Wasserstein in terms of (cid:107)kψ(x, ·) − kψ(y, ·)(cid:107)Hkψ

.

3

Figure 1: The setting of Example 1. (a, b): parameter-space gradient ﬁelds for the MMD and the
SMMD (Section 3.3); the horizontal axis is θ, and the vertical 1/ψ. (c): optimal MMD critics for
θ = 20 with different kernels. (d): the MMD and the distances of Section 3 optimized over ψ.

Proof. The main result is [12, Corollary 11.3.4]. To show the claim for kψ = K ◦ φψ, note that
|fψ(x) − fψ(y)| ≤ (cid:107)fψ(cid:107)Hkψ

(cid:107)kψ(x, ·) − kψ(y, ·)(cid:107)Hkψ

, which since (cid:107)fψ(cid:107)Hkψ

= 1 is

(cid:107)K(φψ(x), ·) − K(φψ(y), ·)(cid:107)HK ≤ LK(cid:107)φψ(x) − φψ(y)(cid:107)Rs ≤ LKLφ(cid:107)x − y(cid:107)Rd .

Indeed, if we put a box constraint on ψ [27] or regularize the gradient of the critic function [7],
the resulting MMD GAN generally matches or outperforms WGAN-based models. Unfortunately,
though, an additive gradient penalty doesn’t substantially change the vector ﬁeld of Figure 1 (a), as
shown in Figure 5 (Appendix B). We will propose distances with much better convergence behavior.

3 New discrepancies for learning implicit generative models

Our aim here is to introduce a discrepancy that can provide useful gradient information when used as
an IGM loss. Proofs of results in this section are deferred to Appendix A.

3.1 Lipschitz Maximum Mean Discrepancy

Proposition 2 shows that an MMD-like discrepancy can be continuous under the weak topology even
when optimizing over kernels, if we directly restrict the critic functions to be Lipschitz. We can easily
deﬁne such a distance, which we call the Lipschitz MMD: for some λ > 0,

≤1

f ∈Hk : (cid:107)f (cid:107)2

LipMMDk,λ(P, Q) :=

EX∼P [f (X)] − EY ∼Q [f (Y )] .

sup
Lip+λ(cid:107)f (cid:107)2
For a universal kernel k, we conjecture that limλ→0 LipMMDk,λ(P, Q) → W(P, Q). But for any k
and λ, LipMMD is upper-bounded by W, as (4) optimizes over a smaller set of functions than (2).
Thus DΨ,λ
LipMMD(P, Q) := supψ∈Ψ LipMMDkψ,λ(P, Q) is also upper-bounded by W, and hence is
continuous in the Wasserstein topology. It also shows excellent empirical behavior on Example 1
(Figure 1 (d), and Figure 5 in Appendix B). But estimating LipMMDk,λ, let alone DΨ,λ
LipMMD, is in
general extremely difﬁcult (Appendix D), as ﬁnding (cid:107)f (cid:107)Lip requires optimization in the input space.
Constraining the mean gradient rather than the maximum, as we will do next, is far more tractable.

(4)

Hk

4

3.2 Gradient-Constrained Maximum Mean Discrepancy

sup
f ∈Hk : (cid:107)f (cid:107)S(µ),k,λ≤1
L2(µ) + (cid:107)∇f (cid:107)2

We deﬁne the Gradient-Constrained MMD for λ > 0 and using some measure µ as

GCMMDµ,k,λ(P, Q) :=

EX∼P [f (X)] − EY ∼Q [f (Y )] ,

(5)

where (cid:107)f (cid:107)2

S(µ),k,λ := (cid:107)f (cid:107)2

L2(µ) + λ(cid:107)f (cid:107)2
Hk

(6)
L2(µ) = (cid:82) (cid:107)·(cid:107)2 µ(dx) denotes the squared L2 norm. Rather than directly constraining the Lipschitz
(cid:107)·(cid:107)2
constant, the second term (cid:107)∇f (cid:107)2
L2(µ) encourages the function f to be ﬂat where µ has mass. In
experiments we use µ = P, ﬂattening the critic near the target sample. We add the ﬁrst term following
[10]: in one dimension and with µ uniform, (cid:107)·(cid:107)S(µ),·,0 is then an RKHS norm with the kernel
κ(x, y) = exp(−(cid:107)x − y(cid:107)), which is also a Sobolev space. The correspondence to a Sobolev norm is
lost in higher dimensions [53, Ch. 10], but we also found the ﬁrst term to be beneﬁcial in practice.

.

We can exploit some properties of Hk to compute (5) analytically. Call the difference in kernel mean
embeddings η := EX∼P[k(X, ·)] − EY ∼Q[k(Y, ·)] ∈ Hk; recall MMD(P, Q) = (cid:107)η(cid:107)Hk .
Proposition 3. Let ˆµ = (cid:80)M
RM d with (m, i)th entry4 ∂iη(Xm). Then under Assumptions (A) to (D) in Appendix A.1,

m=1 δXm . Deﬁne η(X) ∈ RM with mth entry η(Xm), and ∇η(X) ∈

GCMMD2

ˆµ,k,λ(P, Q) =

(cid:0)MMD2(P, Q) − ¯P (η)(cid:1)

1
λ
(cid:20) η(X)
∇η(X)

¯P (η) =

(cid:21)T (cid:18)(cid:20)K GT
G H

(cid:21)

+ M λIM +M d

(cid:19)−1 (cid:20) η(X)
∇η(X)

(cid:21)

,

where K is the kernel matrix Km,m(cid:48) = k(Xm, Xm(cid:48)), G is the matrix of left derivatives 5 G(m,i),m(cid:48) =
∂ik(Xm, Xm(cid:48)), and H that of derivatives of both arguments H(m,i),(m(cid:48),j) = ∂i∂j+dk(Xm, Xm(cid:48)).
As long as P and Q have integrable ﬁrst moments, and µ has second moments, Assumptions (A)
to (D) are satisﬁed e.g. by a Gaussian or linear kernel on top of a differentiable φψ. We can thus
estimate the GCMMD based on samples from P, Q, and µ by using the empirical mean ˆη for η.

This discrepancy indeed works well in practice: Appendix F.2 shows that optimizing our estimate
of Dµ,Ψ,λ
GCMMD = supψ∈Ψ GCMMDµ,kψ,λ yields a good generative model on MNIST. But the linear
system of size M + M d is impractical: even on 28 × 28 images and using a low-rank approximation,
the model took days to converge. We therefore design a less expensive discrepancy in the next section.

The GCMMD is related to some discrepancies previously used in IGM training. The Fisher GAN [34]
uses only the variance constraint (cid:107)f (cid:107)2
L2(µ) ≤ 1,
along with a vanishing boundary condition on f to ensure a well-deﬁned solution (although this was
not used in the implementation, and can cause very unintuitive critic behavior; see Appendix C).
The authors considered several choices of µ, including the WGAN-GP measure [18] and mixtures
(P + Qθ) /2. Rather than enforcing the constraints in closed form as we do, though, these models
used additive regularization. We will compare to the Sobolev GAN in experiments.

L2(µ) ≤ 1. The Sobolev GAN [33] constrains (cid:107)∇f (cid:107)2

3.3 Scaled Maximum Mean Discrepancy

We will now derive a lower bound on the Gradient-Constrained MMD which retains many of its
attractive qualities but can be estimated in time linear in the dimension d.
Proposition 4. Make Assumptions (A) to (D). For any f ∈ Hk, (cid:107)f (cid:107)S(µ),k,λ ≤ σ−1

µ,k,λ(cid:107)f (cid:107)Hk , where

(cid:118)
(cid:117)
(cid:117)
(cid:116)λ +

(cid:46)

(cid:90)

σµ,k,λ := 1

k(x, x)µ(dx) +

d
(cid:88)

i=1

(cid:90) ∂2k(y, z)
∂yi∂zi

(cid:12)
(cid:12)
(cid:12)(y,z)=(x,x)

µ(dx).

We then deﬁne the Scaled Maximum Mean Discrepancy based on this bound of Proposition 4:

SMMDµ,k,λ(P, Q) :=

sup

EX∼P [f (X)]−EY ∼Q [f (Y )] = σµ,k,λ MMDk(P, Q). (7)

f : σ−1

µ,k,λ(cid:107)f (cid:107)H≤1

4We use (m, i) to denote (m − 1)d + i; thus ∇η(X) stacks ∇η(X1), . . . , ∇η(XM ) into one vector.
5We use ∂ik(x, y) to denote the partial derivative with respect to xi, and ∂i+dk(x, y) that for yi.

5

Because the constraint in the optimization of (7) is more restrictive than in that of (5), we have
that SMMDµ,k,λ(P, Q) ≤ GCMMDµ,k,λ(P, Q). The Sobolev norm (cid:107)f (cid:107)S(µ),λ, and a fortiori the
gradient norm under µ, is thus also controlled for the SMMD critic. We also show in Appendix F.1
that SMMDµ,k,λ behaves similarly to GCMMDµ,k,λ on Gaussians.
(cid:3).
If kψ = K ◦ φψ and K(a, b) = g(−(cid:107)a − b(cid:107)2), then σ−2
(cid:3). Estimating
k,µ,λ = λ + Eµ
Or if K is linear, K(a, b) = aTb, then σ−2
these terms based on samples from µ is straightforward, giving a natural estimator for the SMMD.

k,µ,λ = λ + g(0) + 2|g(cid:48)(0)| Eµ

(cid:2)(cid:107)φψ(X)(cid:107)2 + (cid:107)∇φψ(X)(cid:107)2

(cid:2)(cid:107)∇φψ(X)(cid:107)2

F

F

Of course, if µ and k are ﬁxed, the SMMD is simply a constant times the MMD, and so behaves
in essentially the same way as the MMD. But optimizing the SMMD over a kernel family Ψ,
Dµ,Ψ,λ

SMMD(P, Q) := supψ∈Ψ SMMDµ,kψ,λ(P, Q), gives a distance very different from DΨ

MMD (3).

Figure 1 (b) shows the vector ﬁeld for the Optimized SMMD loss in Example 1, using the WGAN-
GP measure µ = Uniform(0, θ). The optimization surface is far more amenable: in particular
the location C, which formerly had an extremely small gradient that made learning effectively
impossible, now converges very quickly by ﬁrst reducing the critic gradient until some signal is
available. Figure 1 (d) demonstrates that Dµ,Ψ,λ
LipMMD but in sharp contrast
to DΨ

SMMD, like Dµ,Ψ,λ
MMD, is continuous with respect to the location θ and provides a strong gradient towards 0.

GCMMD and DΨ,λ

SMMD is continuous in the Wasserstein topology under some conditions:

We can establish that Dµ,Ψ,λ
Theorem 1. Let kψ = K ◦ φψ, with φψ : X → Rs a fully-connected L-layer network with
Leaky-ReLUα activations whose layers do not increase in width, and K satisfying mild smoothness
conditions QK < ∞ (Assumptions (II) to (V) in Appendix A.2). Let Ψκ be the set of parameters where
each layer’s weight matrices have condition number cond(W l) = (cid:107)W l(cid:107)/ σmin(W l) ≤ κ < ∞. If µ
has a density (Assumption (I)), then

Thus if Pn

W−→ P, then Dµ,Ψκ,λ

Dµ,Ψκ,λ

SMMD (P, Q) ≤

QKκL/2
√
dLαL/2
SMMD (Pn, P) → 0, even if µ is chosen to depend on P and Q.

W(P, Q).

L2(µ) = Eµ(cid:107)∇fψ(X)(cid:107)2 does
Uniform bounds vs bounds in expectation Controlling (cid:107)∇fψ(cid:107)2
not necessarily imply a bound on (cid:107)f (cid:107)Lip ≥ supx∈X (cid:107)∇fψ(X)(cid:107), and so does not in general give
continuity via Proposition 2. Theorem 1 implies that when the network’s weights are well-conditioned,
it is sufﬁcient to only control (cid:107)∇fψ(cid:107)2

L2(µ), which is far easier in practice than controlling (cid:107)f (cid:107)Lip.

If we instead tried to directly controlled (cid:107)f (cid:107)Lip with e.g. spectral normalization (SN) [32], we
could signiﬁcantly reduce the expressiveness of the parametric family. In Example 1, constraining
(cid:107)φψ(cid:107)Lip = 1 limits us to only Ψ = {1}. Thus D{1}
MMD is simply the MMD with an RBF kernel
of bandwidth 1, which has poor gradients when θ is far from 0 (Figure 1 (c), blue). The Cauchy-
Schwartz bound of Proposition 4 allows jointly adjusting the smoothness of kψ and the critic f , while
SN must control the two independently. Relatedly, limiting (cid:107)φ(cid:107)Lip by limiting the Lipschitz norm of
each layer could substantially reduce capacity, while (cid:107)∇fψ(cid:107)L2(µ) need not be decomposed by layer.
Another advantage is that µ provides a data-dependent measure of complexity as in [10]: we do not
needlessly prevent ourselves from using critics that behave poorly only far from the data.

Spectral parametrization When the generator is near a local optimum, the critic might identify
only one direction on which Qθ and P differ. If the generator parameterization is such that there
is no local way for the generator to correct it, the critic may begin to single-mindedly focus on
this difference, choosing redundant convolutional ﬁlters and causing the condition number of the
weights to diverge. If this occurs, the generator will be motivated to ﬁx this single direction while
ignoring all other aspects of the distributions, after which it may become stuck. We can help avoid
this collapse by using a critic parameterization that encourages diverse ﬁlters with higher-rank weight
matrices. Miyato et al. [32] propose to parameterize the weight matrices as W = γ ¯W /(cid:107) ¯W (cid:107)op,
where (cid:107) ¯W (cid:107)op is the spectral norm of ¯W . This parametrization works particularly well with Dµ,Ψ,λ
SMMD;
Figure 2 (b) shows the singular values of the second layer of a critic’s network (and Figure 9, in
Appendix F.3, shows more layers), while Figure 2 (d) shows the evolution of the condition number
during training. The conditioning of the weight matrix remains stable throughout training with
spectral parametrization, while it worsens through training in the default case.

6

4 Experiments

We evaluated unsupervised image generation on three datasets: CIFAR-10 [26] (60 000 images,
32 × 32), CelebA [29] (202 599 face images, resized and cropped to 160 × 160 as in [7]), and the
more challenging ILSVRC2012 (ImageNet) dataset [41] (1 281 167 images, resized to 64 × 64).
Code for all of these experiments is available at github.com/MichaelArbel/Scaled-MMD-GAN.
Losses All models are based on a scalar-output critic network φψ : X → R, except MMDGAN-GP
where φψ : X → R16 as in [7]. The WGAN and Sobolev GAN use a critic f = φψ, while the
GAN uses a discriminator Dψ(x) = 1/(1 + exp(−φψ(x))). The MMD-based methods use a kernel
kψ(x, y) = exp(−(φψ(x) − φψ(y))2/2), except for MMDGAN-GP which uses a mixture of RQ
kernels as in [7]. Increasing the output dimension of the critic or using a different kernel didn’t
substantially change the performance of our proposed method. We also consider SMMD with a linear
top-level kernel, k(x, y) = φψ(x)φψ(y); because this becomes essentially identical to a WGAN
(Appendix E), we refer to this method as SWGAN. SMMD and SWGAN use µ = P; Sobolev GAN
uses µ = (P + Q)/2 as in [33]. We choose λ and an overall scaling to obtain the losses:

SMMD:

(cid:92)MMD

2
kψ
1 + 10 EˆP [(cid:107)∇φψ(X)(cid:107)2
F ]

(P, Qθ)

, SWGAN:

(cid:113)

EˆP [φψ(X)] − E ˆQθ

[φψ(X)]

1 + 10 (cid:0)EˆP [|φψ(X)|2] + EˆP [(cid:107)∇φψ(X)(cid:107)2

F ](cid:1)

.

Architecture For CIFAR-10, we used the CNN architecture proposed by [32] with a 7-layer critic
and a 4-layer generator. For CelebA, we used a 5-layer DCGAN discriminator and a 10-layer ResNet
generator as in [7]. For ImageNet, we used a 10-layer ResNet for both the generator and discriminator.
In all experiments we used 64 ﬁlters for the smallest convolutional layer, and double it at each layer
(CelebA/ImageNet) or every other layer (CIFAR-10). The input codes for the generator are drawn
from Uniform (cid:0)[−1, 1]128(cid:1). We consider two parameterizations for each critic: a standard one where
the parameters can take any real value, and a spectral parametrization (denoted SN-) as above [32].
Models without explicit gradient control (SN-GAN, SN-MMDGAN, SN-MMGAN-L2, SN-WGAN)
ﬁx γ = 1, for spectral normalization; others learn γ, using a spectral parameterization.

Training All models were trained for 150 000 generator updates on a single GPU, except for ImageNet
where the model was trained on 3 GPUs simultaneously. To limit communication overhead we
averaged the MMD estimate on each GPU, giving the block MMD estimator [54]. We always used
64 samples per GPU from each of P and Q, and 5 critic updates per generator step. We used initial
learning rates of 0.0001 for CIFAR-10 and CelebA, 0.0002 for ImageNet, and decayed these rates
using the KID adaptive scheme of [7]: every 2 000 steps, generator samples are compared to those
from 20 000 steps ago, and if the relative KID test [9] fails to show an improvement three consecutive
times, the learning rate is decayed by 0.8. We used the Adam optimizer [25] with β1 = 0.5, β2 = 0.9.

Evaluation To compare the sample quality of different models, we considered three different scores
based on the Inception network [49] trained for ImageNet classiﬁcation, all using default parameters
in the implementation of [7]. The Inception Score (IS) [42] is based on the entropy of predicted
labels; higher values are better. Though standard, this metric has many issues, particularly on datasets
other than ImageNet [4, 7, 20]. The FID [20] instead measures the similarity of samples from the
generator and the target as the Wasserstein-2 distance between Gaussians ﬁt to their intermediate
representations. It is more sensible than the IS and becoming standard, but its estimator is strongly
biased [7]. The KID [7] is similar to FID, but by using a polynomial-kernel MMD its estimates enjoy
better statistical properties and are easier to compare. (A similar score was recommended by [21].)

Results Table 1a presents the scores for models trained on both CIFAR-10 and CelebA datasets. On
CIFAR-10, SN-SWGAN and SN-SMMDGAN performed comparably to SN-GAN. But on CelebA,
SN-SWGAN and SN-SMMDGAN dramatically outperformed the other methods with the same
architecture in all three metrics. It also trained faster, and consistently outperformed other methods
over multiple initializations (Figure 2 (a)). It is worth noting that SN-SWGAN far outperformed
WGAN-GP on both datasets. Table 1b presents the scores for SMMDGAN and SN-SMMDGAN
trained on ImageNet, and the scores of pre-trained models using BGAN [6] and SN-GAN [32].6 The

6These models are courtesy of the respective authors and also trained at 64 × 64 resolution. SN-GAN used
the same architecture as our model, but trained for 250 000 generator iterations; BS-GAN used a similar 5-layer
ResNet architecture and trained for 74 epochs, comparable to SN-GAN.

7

Figure 2: The training process on CelebA. (a) KID scores. We report a ﬁnal score for SN-GAN
slightly before its sudden failure mode; MMDGAN and SN-MMDGAN were unstable and had scores
around 100. (b) Singular values of the second layer, both early (dashed) and late (solid) in training.
(c) σ−2
µ,k,λ for several MMD-based methods. (d) The condition number in the ﬁrst layer through
training. SN alone does not control σµ,k,λ, and SMMD alone does not control the condition number.

(a) Scaled MMD GAN with SN

(b) SN-GAN

(c) Boundary Seeking GAN

(d) Scaled MMD GAN with SN

(e) Scaled WGAN with SN

(f) MMD GAN with GP+L2

Figure 3: Samples from various models. Top: 64 × 64 ImageNet; bottom: 160 × 160 CelebA.

8

Table 1: Mean (standard deviation) of score estimates, based on 50 000 samples from each model.

(a) CIFAR-10 and CelebA.

Method

WGAN-GP
MMDGAN-GP-L2
Sobolev-GAN
SMMDGAN
SN-GAN
SN-SWGAN
SN-SMMDGAN

CIFAR-10
IS

FID

6.9±0.2
6.9±0.1
7.0±0.1
7.0±0.1
7.2±0.1
7.2±0.1
7.3±0.1

31.1±0.2
31.4±0.3
30.3±0.3
31.5±0.4
26.7±0.2
28.5±0.2
25.0±0.3

KID×103

22.2±1.1
23.3±1.1
22.3±1.2
22.2±1.1
16.1±0.9
17.6±1.1
16.6±2.0

CelebA
IS

2.7±0.0
2.6±0.0
2.9±0.0
2.7±0.0
2.7±0.0
2.8±0.0
2.8±0.0

FID

KID×103

29.2±0.2
20.5±0.2
16.4±0.1
18.4±0.2
22.6±0.1
14.1±0.2
12.4±0.2

22.0±1.0
13.0±1.0
10.6±0.5
11.5±0.8
14.6±1.1
7.7±0.5
6.1±0.4

(b) ImageNet.

Method

IS

FID

10.7±0.4
BGAN
11.2±0.1
SN-GAN
SMMDGAN
10.7±0.2
SN-SMMDGAN 10.9±0.1

43.9±0.3
47.5±0.1
38.4±0.3
36.6±0.2

KID×103

47.0±1.1
44.4±2.2
39.3±2.5
34.6±1.6

proposed methods substantially outperformed both methods in FID and KID scores. Figure 3 shows
samples on ImageNet and CelebA; Appendix F.4 has more.

Spectrally normalized WGANs / MMDGANs To control for the contribution of the spectral
parametrization to the performance, we evaluated variants of MMDGANs, WGANs and Sobolev-
GAN using spectral normalization (in Table 2, Appendix F.3). WGAN and Sobolev-GAN led to
unstable training and didn’t converge at all (Figure 11) despite many attempts. MMDGAN converged
on CIFAR-10 (Figure 11) but was unstable on CelebA (Figure 10). The gradient control due to SN
is thus probably too loose for these methods. This is reinforced by Figure 2 (c), which shows that
the expected gradient of the critic network is much better-controlled by SMMD, even when SN is
used. We also considered variants of these models with a learned γ while also adding a gradient
penalty and an L2 penalty on critic activations [7, footnote 19]. These generally behaved similarly to
MMDGAN, and didn’t lead to substantial improvements. We ran the same experiments on CelebA,
but aborted the runs early when it became clear that training was not successful.

Rank collapse We occasionally observed the failure mode for SMMD where the critic becomes
low-rank, discussed in Section 3.3, especially on CelebA; this failure was obvious even in the training
objective. Figure 2 (b) is one of these examples. Spectral parametrization seemed to prevent this
behavior. We also found one could avoid collapse by reverting to an earlier checkpoint and increasing
the RKHS regularization parameter λ, but did not do this for any of the experiments here.

5 Conclusion

We studied gradient regularization for MMD-based critics in implicit generative models, clarifying
how previous techniques relate to the DΨ
MMD loss. Based on these insights, we proposed the Gradient-
Constrained MMD and its approximation the Scaled MMD, a new loss function for IGMs that
controls gradient behavior in a principled way and obtains excellent performance in practice.

One interesting area of future study for these distances is their behavior when used to diffuse particles
distributed as Q towards particles distributed as P. Mroueh et al. [33, Appendix A.1] began such a
study for the Sobolev GAN loss; [35] proved convergence and studied discrete-time approximations.

Another area to explore is the geometry of these losses, as studied by Bottou et al. [8], who showed
potential advantages of the Wasserstein geometry over the MMD. Their results, though, do not
address any distances based on optimized kernels; the new distances introduced here might have
interesting geometry of their own.

9

References

[1] B. Amos and J. Z. Kolter. “OptNet: Differentiable Optimization as a Layer in Neural Net-

works.” ICML. 2017. arXiv: 1703.00443.

[2] M. Arjovsky and L. Bottou. “Towards Principled Methods for Training Generative Adversarial

Networks.” ICLR. 2017. arXiv: 1701.04862.

[3] M. Arjovsky, S. Chintala, and L. Bottou. “Wasserstein Generative Adversarial Networks.”

ICML. 2017. arXiv: 1701.07875.

[4] S. Barratt and R. Sharma. A Note on the Inception Score. 2018. arXiv: 1801.01973.
[5] M. G. Bellemare, I. Danihelka, W. Dabney, S. Mohamed, B. Lakshminarayanan, S. Hoyer,
and R. Munos. The Cramer Distance as a Solution to Biased Wasserstein Gradients. 2017.
arXiv: 1705.10743.

[6] D. Berthelot, T. Schumm, and L. Metz. BEGAN: Boundary Equilibrium Generative Adversarial

[7] M. Bi´nkowski, D. J. Sutherland, M. Arbel, and A. Gretton. “Demystifying MMD GANs.”

Networks. 2017. arXiv: 1703.10717.

ICLR. 2018. arXiv: 1801.01401.

[8] L. Bottou, M. Arjovsky, D. Lopez-Paz, and M. Oquab. “Geometrical Insights for Implicit
Generative Modeling.” Braverman Readings in Machine Learning: Key Iedas from Inception
to Current State. Ed. by L. Rozonoer, B. Mirkin, and I. Muchnik. LNAI Vol. 11100. Springer,
2018, pp. 229–268. arXiv: 1712.07822.

[9] W. Bounliphone, E. Belilovsky, M. B. Blaschko, I. Antonoglou, and A. Gretton. “A Test of
Relative Similarity For Model Selection in Generative Models.” ICLR. 2016. arXiv: 1511.
04581.

[10] O. Bousquet, O. Chapelle, and M. Hein. “Measure Based Regularization.” NIPS. 2004.
[11] A. Brock, T. Lim, J. M. Ritchie, and N. Weston. “Neural Photo Editing with Introspective

Adversarial Networks.” ICLR. 2017. arXiv: 1609.07093.

[12] R. M. Dudley. Real Analysis and Probability. 2nd ed. Cambridge University Press, 2002.
[13] G. K. Dziugaite, D. M. Roy, and Z. Ghahramani. “Training generative neural networks via

Maximum Mean Discrepancy optimization.” UAI. 2015. arXiv: 1505.03906.

[14] A. Genevay, G. Peyré, and M. Cuturi. “Learning Generative Models with Sinkhorn Diver-

gences.” AISTATS. 2018. arXiv: 1706.00292.

[15] T. Gneiting and A. E. Raftery. “Strictly proper scoring rules, prediction, and estimation.” JASA

102.477 (2007), pp. 359–378.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio. “Generative Adversarial Nets.” NIPS. 2014. arXiv: 1406.2661.

[17] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Schölkopf, and A. J. Smola. “A Kernel Two-

Sample Test.” JMLR 13 (2012).
I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. “Improved Training of
Wasserstein GANs.” NIPS. 2017. arXiv: 1704.00028.

[19] A. Güngör. “Some bounds for the product of singular values.” International Journal of

[16]

[18]

Contemporary Mathematical Sciences (2007).

[20] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, G. Klambauer, and S. Hochreiter. “GANs
Trained by a Two Time-Scale Update Rule Converge to a Nash Equilibrium.” NIPS. 2017.
arXiv: 1706.08500.

[21] G. Huang, Y. Yuan, Q. Xu, C. Guo, Y. Sun, F. Wu, and K. Weinberger. An empirical study on

evaluation metrics of generative adversarial networks. 2018. arXiv: 1806.07755.

[22] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. “Multimodal Unsupervised Image-to-Image

Translation.” ECCV. 2018. arXiv: 1804.04732.

[23] Y. Jin, K. Zhang, M. Li, Y. Tian, H. Zhu, and Z. Fang. Towards the Automatic Anime Characters

Creation with Generative Adversarial Networks. 2017. arXiv: 1708.05509.

[24] T. Karras, T. Aila, S. Laine, and J. Lehtinen. “Progressive Growing of GANs for Improved

Quality, Stability, and Variation.” ICLR. 2018. arXiv: 1710.10196.

[25] D. Kingma and J. Ba. “Adam: A Method for Stochastic Optimization.” ICLR. 2015. arXiv:

1412.6980.

[26] A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images. 2009.

10

arXiv: 1502.02761.

arXiv: 1411.7766.

(2002), pp. 583–601.

1711.04894.

[27] C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. Póczos. “MMD GAN: Towards Deeper

Understanding of Moment Matching Network.” NIPS. 2017. arXiv: 1705.08584.

[28] Y. Li, K. Swersky, and R. Zemel. “Generative Moment Matching Networks.” ICML. 2015.

[29] Z. Liu, P. Luo, X. Wang, and X. Tang. “Deep learning face attributes in the wild.” ICCV. 2015.

[30] L. Mescheder, A. Geiger, and S. Nowozin. “Which Training Methods for GANs do actually

Converge?” ICML. 2018. arXiv: 1801.04406.

[31] P. Milgrom and I. Segal. “Envelope theorems for arbitrary choice sets.” Econometrica 70.2

[32] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. “Spectral Normalization for Generative

Adversarial Networks.” ICLR. 2018. arXiv: 1802.05927.

[33] Y. Mroueh, C.-L. Li, T. Sercu, A. Raj, and Y. Cheng. “Sobolev GAN.” ICLR. 2018. arXiv:

[34] Y. Mroueh and T. Sercu. “Fisher GAN.” NIPS. 2017. arXiv: 1705.09675.
[35] Y. Mroueh, T. Sercu, and A. Raj. Regularized Kernel and Neural Sobolev Descent: Dynamic

MMD Transport. 2018. arXiv: 1805.12062.

[36] A. Müller. “Integral Probability Metrics and their Generating Classes of Functions.” Advances

in Applied Probability 29.2 (1997), pp. 429–443.

[37] S. Nowozin, B. Cseke, and R. Tomioka. “f-GAN: Training Generative Neural Samplers using

Variational Divergence Minimization.” NIPS. 2016. arXiv: 1606.00709.

[38] A. Radford, L. Metz, and S. Chintala. “Unsupervised Representation Learning with Deep
Convolutional Generative Adversarial Networks.” ICLR. 2016. arXiv: 1511.06434.
J. R. Retherford. “Review: J. Diestel and J. J. Uhl, Jr., Vector measures.” Bull. Amer. Math.
Soc. 84.4 (July 1978), pp. 681–685.

[39]

[40] K. Roth, A. Lucchi, S. Nowozin, and T. Hofmann. “Stabilizing Training of Generative Adver-

sarial Networks through Regularization.” NIPS. 2017. arXiv: 1705.09367.

[41] O. Russakovsky et al. ImageNet Large Scale Visual Recognition Challenge. 2014. arXiv:

1409.0575.

[42] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. “Improved

[43]

Techniques for Training GANs.” NIPS. 2016. arXiv: 1606.03498.
J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University
Press, 2004.

[44] B. K. Sriperumbudur, K. Fukumizu, A. Gretton, G. R. G. Lanckriet, and B. Schölkopf. “Kernel

choice and classiﬁability for RKHS embeddings of probability distributions.” NIPS. 2009.

[45] B. K. Sriperumbudur, K. Fukumizu, and G. R. G. Lanckriet. “Universality, Characteristic
Kernels and RKHS Embedding of Measures.” JMLR 12 (2011), pp. 2389–2410. arXiv: 1003.
0887.

[46] B. Sriperumbudur. “On the optimal estimation of probability mesaures in weak and strong

topologies.” Bernoulli 22.3 (2016), pp. 1839–1893. arXiv: 1310.8240.
I. Steinwart and A. Christmann. Support Vector Machines. Springer, 2008.

[47]
[48] D. J. Sutherland, H.-Y. Tung, H. Strathmann, S. De, A. Ramdas, A. Smola, and A. Gretton.
“Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy.” ICLR.
2017. arXiv: 1611.04488.

[49] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. “Rethinking the Inception

Architecture for Computer Vision.” CVPR. 2016. arXiv: 1512.00567.

[50] T. Unterthiner, B. Nessler, C. Seward, G. Klambauer, M. Heusel, H. Ramsauer, and S. Hochre-
iter. “Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields.” ICLR. 2018.
arXiv: 1708.08819.

[51] C. Villani. Optimal Transport: Old and New. Springer, 2009.
[52]

J. Weed and F. Bach. “Sharp asymptotic and ﬁnite-sample rates of convergence of empirical
measures in Wasserstein distance.” Bernoulli (forthcoming). arXiv: 1707.00087.
[53] H. Wendland. Scattered Data Approximation. Cambridge University Press, 2005.
[54] W. Zaremba, A. Gretton, and M. B. Blaschko. “B-tests: Low Variance Kernel Two-Sample

Tests.” NIPS. 2013. arXiv: 1307.1954.

11

A Proofs

We ﬁrst review some basic properties of Reproducing Kernel Hilbert Spaces. We consider here a
separable RKHS H with basis (ei)i∈I , where I is either ﬁnite if H is ﬁnite-dimensional, or I = N
otherwise. We also assume that the reproducing kernel k is continuously twice differentiable.

We use a slightly nonstandard notation for derivatives: ∂if (x) denotes the ith partial derivative of f
evaluated at x, and ∂i∂j+dk(x, y) denotes ∂2k(a,b)
∂ai∂bj

|(a,b)=(x,y).

Then the following reproducing properties hold for any given function f in H [47, Lemma 4.34]:
f (x) =(cid:104)f, k(x, .)(cid:105)H
∂if (x) =(cid:104)f, ∂ik(x, .)(cid:105)H.

(8)
(9)

We say that an operator A : H (cid:55)→ H is Hilbert-Schmidt if (cid:107)A(cid:107)2
H is ﬁnite. (cid:107)A(cid:107)HS
is called the Hilbert-Schmidt norm of A. The space of Hilbert-Schmidt operators itself a Hilbert
space with the inner product (cid:104)A, B(cid:105)HS = (cid:80)
i∈I (cid:104)Aei, Bei(cid:105)H. Moreover, we say that an operator A
is trace-class if its trace norm is ﬁnite, i.e. (cid:107)A(cid:107)1 = (cid:80)
2 ei(cid:105)H < ∞. The outer product
f ⊗ g for f, g ∈ H gives an H → H operator such that (f ⊗ g)v = (cid:104)g, v(cid:105)Hf for all v in H.

i∈I (cid:104)ei, (A∗A) 1

i∈I (cid:107)Aei(cid:107)2

HS = (cid:80)

Given two vectors f and g in H and a Hilbert-Schmidt operator A we have the following properties:

(i) The outer product f ⊗ g is a Hilbert-Schmidt operator with Hilbert-Schmidt norm given by:

(ii) The inner product between two rank-one operators f ⊗ g and u ⊗ v is (cid:104)f ⊗ g, u ⊗ v(cid:105)HS =

(cid:107)f ⊗ g(cid:107)HS = (cid:107)f (cid:107)H(cid:107)g(cid:107)H.

(cid:104)f, u(cid:105)H(cid:104)g, v(cid:105)H.

(iii) The following identity holds: (cid:104)f, Ag(cid:105)H = (cid:104)f ⊗ g, A(cid:105)HS.

Deﬁne the following covariance-type operators:

d
(cid:88)

i=1

Dx = k(x, ·) ⊗ k(x, ·) +

∂ik(x, ·) ⊗ ∂ik(x, ·) Dµ = EX∼µ DX Dµ,λ = Dµ + λI;

(10)

these are useful in that, using (8) and (9), (cid:104)f, Dxg(cid:105) = f (x)g(x) + (cid:80)d

i=1 ∂if (x) ∂ig(x).

A.1 Deﬁnitions and estimators of the new distances

We will need the following assumptions about the distributions P and Q, the measure µ, and the
kernel k:

(A) P and Q have integrable ﬁrst moments.
(B) (cid:112)k(x, x) grows at most linearly in x: for all x in X , (cid:112)k(x, x) ≤ C((cid:107)x(cid:107) + 1) for some

constant C.

(C) The kernel k is twice continuously differentiable.
(D) The functions x (cid:55)→ k(x, x) and x (cid:55)→ ∂i∂i+dk(x, x) for 1 ≤ i ≤ d are µ-integrable.

When k = K ◦ φψ, Assumption (B) is automatically satisﬁed by a K such as the Gaussian; when K
is linear, it is true for a quite general class of networks φψ [7, Lemma 1].

We will ﬁrst give a form for the Gradient-Constrained MMD (5) in terms of the operator (10):
Proposition 5. Under Assumptions (A) to (D), the Gradient-Constrained MMD is given by

GCMMDµ,k,λ(P, Q) =

(cid:104)η, D−1

µ,λη(cid:105)H.

(cid:113)

(11)

Proof of Proposition 5. Let f be a function in H. We will ﬁrst express the squared λ-regularized
Sobolev norm of f (6) as a quadratic form in H. Recalling the reproducing properties of (8) and (9),
we have:

(cid:107)f (cid:107)2

S(µ),k,λ =

(cid:104)f, k(x, ·)(cid:105)2

H µ(dx) +

(cid:104)f, ∂ik(x, ·)(cid:105)2

H µ(dx) + λ(cid:107)f (cid:107)2
H.

(cid:90)

d
(cid:88)

(cid:90)

i=1

12

Using Property (ii) and the operator (10), one further gets

(cid:107)f (cid:107)2

S(µ),k,λ =

(cid:104)f ⊗ f, Dx(cid:105)HS µ(dx) + λ(cid:107)f (cid:107)2
H.

(cid:90)

Under Assumption (D), and using Lemma 6, one can take the integral inside the inner product, which
leads to (cid:107)f (cid:107)2

H. Finally, using Property (iii) it follows that

S(µ),k,λ = (cid:104)f ⊗ f, Dµ(cid:105)HS + λ(cid:107)f (cid:107)2

(cid:107)f (cid:107)2

S(µ),k,λ = (cid:104)f, Dµ,λf (cid:105)H.

Under Assumptions (A) and (B), Lemma 6 applies, and it follows that k(x, ·) is also Bochner
integrable under P and Q. Thus

EP [(cid:104)f, k(x, ·)(cid:105)H] − EQ [(cid:104)f, k(x, ·)(cid:105)H] = (cid:104)f, EP [k(x, ·)] − EP [k(x, ·)](cid:105)H = (cid:104)f, η(cid:105)H,

where η is deﬁned as this difference in mean embeddings.

Since Dµ,λ is symmetric positive deﬁnite, its square-root D

1
2

For any f ∈ H, let g = D
corresponding f = D− 1

1
2

µ,λ is well-deﬁned and is also invertible.
H. Note that for any g ∈ H, there is a
µ,λg. Thus we can re-express the maximization problem in (5) in terms of g:

µ,λf , so that (cid:104)f, Dµ,λf (cid:105)H = (cid:107)g(cid:107)2

2

GCMMDµ,k,λ(P, Q) :=

sup
f ∈H
(cid:104)f,Dµ,λf (cid:105)H≤1
(cid:104)g, D− 1

2

= sup
g∈H
(cid:107)g(cid:107)H≤1

(cid:104)f, η(cid:105)H = sup
g∈H
(cid:107)g(cid:107)H≤1

(cid:104)D− 1

2

µ,λg, η(cid:105)H

µ,λη(cid:105)H = (cid:107)D− 1

2

µ,λη(cid:107)H =

(cid:104)η, D−1

µ,λη(cid:105)H.

(cid:113)

Proposition 5, though, involves inverting the inﬁnite-dimensional operator Dµ,λ and thus doesn’t
directly give us a computable estimator. Proposition 3 solves this problem in the case where µ is a
discrete measure:
Proposition 3. Let ˆµ = (cid:80)M
m=1 δXm be an empirical measure of M points. Let η(X) ∈ RM have
mth entry η(Xm), and ∇η(X) ∈ RM d have (m, i)th entry7 ∂iη(Xm). Then under Assumptions (A)
to (D), the Gradient-Constrained MMD is

GCMMD2

ˆµ,k,λ(P, Q) =

(cid:0)MMD2(P, Q) − ¯P (η)(cid:1)

1
λ
(cid:20) η(X)
∇η(X)

¯P (η) =

(cid:21)T (cid:18)(cid:20)K GT
G H

(cid:21)

+ M λIM +M d

(cid:19)−1 (cid:20) η(X)
∇η(X)

(cid:21)

,

where K is the kernel matrix Km,m(cid:48) = k(Xm, Xm(cid:48)), G is the matrix of left derivatives G(m,i),m(cid:48) =
∂ik(Xm, Xm(cid:48)), and H that of derivatives of both arguments H(m,i),(m(cid:48),j) = ∂i∂j+dk(Xm, Xm(cid:48)).

Before proving Proposition 3, we note the following interesting alternate form. Let ¯ei be the ith
standard basis vector for RM +M d, and deﬁne T : H → RM +M d as the linear operator

T =

¯em ⊗ k(Xm, ·) +

¯em+(m,i) ⊗ ∂ik(Xm, ·).

M
(cid:88)

m=1

M
(cid:88)

d
(cid:88)

m=1

i=1

Then

(cid:21)

(cid:20) η(X)
∇η(X)

(cid:21)

(cid:20)K GT
G H

= T η, and

= T T ∗. Thus we can write

GCMMD2

ˆµ,k,λ =

(cid:10)η, (cid:0)I − T ∗(T T ∗ + M λI)−1T (cid:1) η(cid:11)

H .

1
λ

7We use (m, i) to denote (m − 1)d + i; thus ∇η(X) stacks ∇η(X1), . . . , ∇η(XM ) into one vector.

13

Proof of Proposition 3. Let g ∈ H be the solution to the regression problem Dµ,λg = η:

g(Xm)k(Xm, ·) +

∂ig(Xm)∂ik(Xm, ·)

+ λg = η

(cid:35)

d
(cid:88)

i=1

1
M

(cid:34)

M
(cid:88)

m=1

g =

η −

1
λ

1
λM

(cid:34)

M
(cid:88)

m=1

g(Xm)k(Xm, ·) +

∂ig(Xm)∂ik(Xm, ·)

.

(12)

Taking the inner product of both sides of (12) with k(Xm(cid:48), ·) for each 1 ≤ m(cid:48) ≤ M yields the
following M equations:

g(Xm(cid:48)) =

η(Xm(cid:48)) −

g(Xm)Km,m(cid:48) +

∂ig(Xm) G(m,i),m(cid:48)

.

(13)

Doing the same with ∂jk(Xm(cid:48), ·) gives M d equations:

∂jg(Xm(cid:48)) =

∂jη(Xm(cid:48)) −

g(Xm)G(m(cid:48),j),m +

∂ig(Xm)H(m,i),(m(cid:48),j)

.

(14)

(cid:35)

(cid:35)

(cid:35)

(cid:35)

From (12), it is clear that g is a linear combination of the form:

g(x) =

η(x) −

αmk(Xm, x) +

βm,i∂ik(Xm, x)

,

1
λ

1
λM

(cid:34)

M
(cid:88)

m=1

1
λ

1
λM

(cid:34)

M
(cid:88)

m=1

1
λ

1
λM

(cid:34)

M
(cid:88)

m=1

d
(cid:88)

i=1

d
(cid:88)

i=1

d
(cid:88)

i=1

d
(cid:88)

i=1

where the coefﬁcients α := (αm = g(Xm))1≤m≤M and β := (βm,i = ∂ig(Xm))1≤m≤M
1≤i≤d
the system of equations (13) and (14). We can rewrite this system as

satisfy

(cid:20)K + M λIM
G

GT
H + M λIM d

(cid:21)

(cid:21) (cid:20)α
β

= M

(cid:21)

(cid:20) η(X)
∇η(X)

,

where IM , IM d are the identity matrices of dimension M , M d. Since K and H must be positive
semideﬁnite, an inverse exists. We conclude by noticing that
(cid:34)

(cid:35)

GCMMDˆµ,k,λ(P, Q)2 = (cid:104)η, g(cid:105)H =

(cid:107)η(cid:107)2

H −

αmη(Xm) +

βm,i∂iη(Xm)

.

1
λ

1
λM

M
(cid:88)

m=1

d
(cid:88)

i=1

The following result was key to our deﬁnition of the SMMD in Section 3.3.
Proposition 4. Under Assumptions (A) to (D), we have for all f ∈ H that

where σk,µ,λ := 1/

λ + (cid:82) k(x, x)µ(dx) + (cid:80)d

i=1

(cid:113)

(cid:107)f (cid:107)S(µ),k,λ ≤ σ−1

µ,k,λ(cid:107)f (cid:107)Hk ,
(cid:82) ∂i∂i+dk(x, x)µ(dx).

Proof of Proposition 4. The key idea here is to use the Cauchy-Schwarz inequality for the Hilbert-
Schmidt inner product. Letting f ∈ H, (cid:107)f (cid:107)2
S(µ),k,λ is
(cid:90)

(cid:90)

f (x)2 µ(dx) +

(cid:107)∇f (x)(cid:107)2 µ(dx) + λ(cid:107)f (cid:107)2
H

(cid:104)f, k(x, ·) ⊗ k(x, ·)f (cid:105)H µ(dx) +

(cid:104)f, ∂ik(x, ·) ⊗ ∂ik(x, ·)f (cid:105)H µ(dx) + λ(cid:107)f (cid:107)2
H

(cid:104)f ⊗ f, k(x, ·) ⊗ k(x, ·)(cid:105)HS µ(dx) +

(cid:104)f ⊗ f, ∂ik(x, ·) ⊗ ∂ik(x, ·)(cid:105)HS µ(dx) + λ(cid:107)f (cid:107)2
H

k(x, x) µ(dx) +

∂i∂i+dk(x, x) µ(dx) + λ

.

(cid:35)

d
(cid:88)

(cid:90)

i=1

(a) follows from the reproducing properties (8) and (9) and Property (ii). (b) is obtained using
Property (iii), while (c) follows from the Cauchy-Schwarz inequality and Property (i).

(cid:90)

(a)
=

(cid:90)

(b)
=

(cid:34)(cid:90)

(c)
≤ (cid:107)f (cid:107)2
H

d
(cid:88)

(cid:90)

i=1

d
(cid:88)

(cid:90)

i=1

14

Lemma 6. Under Assumption (D), Dx is Bochner integrable and its integral Dµ is a trace-class
symmetric positive semi-deﬁnite operator with Dµ,λ = D+λI invertible for any positive λ. Moreover,
for any Hilbert-Schmidt operator A we have: (cid:104)A, Dµ(cid:105)HS = (cid:82) (cid:104)A, Dx(cid:105)HS µ(dx).
Under Assumptions (A) and (B), k(x, ·) is Bochner integrable with respect to any probability
distribution P with ﬁnite ﬁrst moment and the following relation holds: (cid:104)f, EP [k(x, ·)](cid:105)H =
EP [(cid:104)f, k(x, ·)(cid:105)H] for all f in H.

Proof. The operator Dx is positive self-adjoint. It is also trace-class, as by the triangle inequality

(cid:107)Dx(cid:107)1 ≤ (cid:107)k(x, ·) ⊗ k(x, ·)(cid:107)1 +

(cid:107)∂ik(x, ·) ⊗ ∂ik(x, ·)(cid:107)1

d
(cid:88)

i=1

= (cid:107)k(x, ·)(cid:107)2

H +

(cid:107)∂ik(x, ·)(cid:107)2

H < ∞.

d
(cid:88)

i=1

By Assumption (D), we have that (cid:82) (cid:107)Dx(cid:107)1 µ(dx) < ∞ which implies that Dx is µ-integrable in
the Bochner sense [39, Deﬁnition 1 and Theorem 2]. Its integral Dµ is trace-class and satisﬁes
(cid:107)Dµ(cid:107)1 ≤ (cid:82) (cid:107)Dx(cid:107)1 µ(dx). This allows to have (cid:104)A, Dµ(cid:105)HS = (cid:82) (cid:104)A, Dx(cid:105)HS µ(dx) for all Hilbert-
Schmidt operators A. Moreover, the integral preserves the symmetry and positivity. It follows that
Dµ,λ is invertible.
The Bochner integrability of k(x, ·) under a distribution P with ﬁnite moment follows directly from
Assumptions (A) and (B), since (cid:82) (cid:107)k(x, ·)(cid:107) P(dx) ≤ C (cid:82) ((cid:107)x(cid:107) + 1) P(dx) < ∞. This allows us to
write (cid:104)f, EP[k(x, ·)](cid:105)H = EP[(cid:104)f, k(x, ·)(cid:105)H].

A.2 Continuity of the Optimized Scaled MMD in the Wasserstein topology

To prove Theorem 1, we we will ﬁrst need some new notation.

We assume the kernel is k = K ◦ φψ, i.e. kψ(x, y) = K(φψ(x), φψ(y)), where the representation
function φψ is a network φψ(X) : Rd → RdL consisting of L fully-connected layers:

h0
ψ(X) = X
ψ(X) = W lσl−1(hl−1
hl
φψ(X) = hL

ψ(X).

ψ (X)) + bl

for 1 ≤ l ≤ L

(15)

The intermediate representations hl
ψ(X) are of dimension dl, the weights W l are matrices in
Rdl×dl−1 , and biases bl are vectors in Rdl . The elementwise activation function σ is given by
σ0(x) = x, and for l > 0 the activation σl is a leaky ReLU with leak coefﬁcient 0 < α < 1:

σl(x) = σ(x) =

for l > 0.

(16)

(cid:26)x

x > 0
αx x ≤ 0

The parameter ψ is the concatenation of all the layer parameters:

ψ = (cid:0)(W L, bL), (W L−1, bL−1), . . . , (W 1, b1)(cid:1) .

We denote by Ψ the set of all such possible parameters, i.e. Ψ = RdL×dL−1 ×RdL ×· · ·×Rd1×d×Rd1 .
Deﬁne the following restrictions of Ψ:

Ψκ := (cid:8)ψ ∈ Ψ | ∀1 ≤ l ≤ L, cond(W l) ≤ κ(cid:9)
1 := (cid:8)ψ ∈ Ψκ | ∀1 ≤ l ≤ L, (cid:107)W l(cid:107) = 1(cid:9) .
Ψκ

(17)

(18)

Ψκ is the set of those parameters such that W l have a small condition number, cond(W ) =
σmax(W )/σmin(W ). Ψκ
1 is the set of per-layer normalized parameters with a condition number
bounded by κ.

15

Recall the deﬁnition of Scaled MMD, (7), where λ > 0 and µ is a probability measure:

SMMDµ,k,λ(P, Q) := σµ,k,λ MMDk(P, Q)

σk,µ,λ := 1/

k(x, x) µ(dx) +

∂i∂i+dk(x, x) µ(dx).

(cid:118)
(cid:117)
(cid:117)
(cid:116)λ +

(cid:90)

d
(cid:88)

(cid:90)

i=1

The Optimized SMMD over the restricted set Ψκ is given by:

Dµ,Ψκ,λ

SMMD (P, Q) := sup
ψ∈Ψκ

SMMDµ,kψ,λ .

The constraint to ψ ∈ Ψκ is critical to the proof. In practice, using a spectral parametrization helps
enforce this assumption, as shown in Figures 2 and 9. Other regularization methods, like orthogonal
normalization [11], are also possible.

We will use the following assumptions:

(I) µ is a probability distribution absolutely continuous with respect to the Lebesgue measure.
(II) The dimensions of the weights are decreasing per layer: dl+1 ≤ dl for all 0 ≤ l ≤ L − 1.
(III) The non-linearity used is Leaky-ReLU, (16), with leak coefﬁcient α ∈ (0, 1).
(IV) The top-level kernel K is globally Lipschitz in the RKHS norm: there exists a positive
constant LK > 0 such that (cid:107)K(a, .) − K(b, .)(cid:107) ≤ LK(cid:107)a − b(cid:107) for all a and b in RdL.

(V) There is some γK > 0 for which K satisﬁes

∇b∇cK(b, c)(cid:12)

(cid:12)(b,c)=(a,a) (cid:23) γ2I

for all a ∈ RdL.

(19)

Assumption (I) ensures that the points where φψ(X) is not differentiable are reached with probability
0 under µ. This assumption can be easily satisﬁed e.g. if we deﬁne µ by adding Gaussian noise to P.
Assumption (II) helps ensure that the span of W l is never contained in the null space of W l+1. Using
Leaky-ReLU as a non-linearity, Assumption (III), further ensures that the network φψ is locally
full-rank almost everywhere; this might not be true with ReLU activations, where it could be always
0. Assumptions (II) and (III) can be easily satisﬁed by design of the network.

Assumptions (IV) and (V) only depend on the top-level kernel K and are easy to satisfy in practice.
In particular, they always hold for a smooth translation-invariant kernel, such as the Gaussian, as well
as the linear kernel.

We are now ready to prove Theorem 1.
Theorem 1. Under Assumptions (I) to (V),

Dµ,Ψκ,λ

SMMD (P, Q) ≤

LK κL/2
√

dL αL/2

γ

W(P, Q),

which implies that if Pn

W−→ P, then Dµ,Ψκ,λ

SMMD (Pn, P) → 0.

Proof. Deﬁne the pseudo-distance corresponding to the kernel kψ

dψ(x, y) = (cid:107)kψ(x, ·) − kψ(y, ·)(cid:107)Hψ =

kψ(x, x) + kψ(y, y) − 2kψ(x, y).

(cid:113)

Denote by W dψ (P, Q) the optimal transport metric between P and Q using the cost dψ, given by

W dψ (P, Q) = inf

E(X,Y )∼π [dψ(X, Y )] .

π∈Π(P,Q)

where Π is the set of couplings with marginals P and Q. By Lemma 7,

MMDψ(P, Q) ≤ W dψ (P, Q).

Recall that φψ is Lipschitz, (cid:107)φψ(cid:107)Lip < ∞, so along with Assumption (IV) we have that

dψ(x, y) ≤ LK(cid:107)φψ(x) − φψ(y)(cid:107) ≤ LK(cid:107)φψ(cid:107)Lip(cid:107)x − y(cid:107).

16

Thus

so that

W dψ (P, Q) ≤ inf

E(X,Y )∼π [LK(cid:107)φψ(cid:107)Lip(cid:107)X − Y (cid:107)] = LK(cid:107)φψ(cid:107)Lip W(P, Q),

π∈Π(P,Q)

where W is the standard Wasserstein distance (2), and so

MMDψ(P, Q) ≤ Lk(cid:107)φψ(cid:107)Lip W(P, Q).

We have that ∂i∂i+dk(x, y) = [∂iφψ(x)]T (cid:104)
∇a∇bK(a, b)(cid:12)
[∂iφψ(y)] , where the
middle term is a dL × dL matrix and the outer terms are vectors of length dL. Thus Assumption (V)
implies that ∂i∂i+dk(x, x) ≥ γ2

(cid:12)(a,b)=(φψ(x),φψ(y))

K(cid:107)∂iφψ(x)(cid:107)2, and hence

(cid:105)

σ−2
µ,k,λ ≥ γ2

K

E[(cid:107)∇φψ(X)(cid:107)2
F ]

SMMD2

ψ(P, Q) = σ2

µ,k,λ MMD2

ψ(P, Q) ≤

K(cid:107)φψ(cid:107)2
L2
E [(cid:107)∇φψ(X)(cid:107)2
F ]

Lip

γ2
K

W 2(P, Q).

Using Lemma 8, we can write φψ(X) = α(ψ)φ ¯ψ(X) with ¯ψ ∈ Ψκ

1 . Then we have

(cid:107)φψ(cid:107)2
Lip
(cid:107)∇φψ(X)(cid:107)2
F

(cid:104)

Eµ

(cid:105) =

α(ψ)2(cid:107)φ ¯ψ(cid:107)2
(cid:104)(cid:13)
(cid:13)∇φ ¯ψ(X)(cid:13)
2
(cid:13)
F

Lip

α(ψ)2 Eµ

(cid:105) ≤

1
(cid:104)(cid:13)
(cid:13)∇φ ¯ψ(X)(cid:13)
2
(cid:13)
F

(cid:105) ,

Eµ

where we used (cid:107)φ ¯ψ(cid:107)Lip ≤ (cid:81)L
(cid:107)∇φ ¯ψ(X)(cid:107)2

F ≥ dL(α/κ)L. Using Assumption (I), this implies that

l=1(cid:107) ¯W l(cid:107) = 1. But by Lemma 9, for Lebesgue-almost all X,

(cid:107)φψ(cid:107)2
Lip
(cid:107)∇φψ(X)(cid:107)2
F

(cid:104)

Eµ

(cid:105) ≤

1
(cid:2)(cid:107)∇φ ¯ψ(X)(cid:107)2

F ](cid:3) ≤

Eµ

κL
dLαL .

Thus for any ψ ∈ Ψκ,

SMMDψ(P, Q) ≤

LK κL/2
√

γK

dL αL/2

W(P, Q).

The desired bound on Dµ,Ψκ,λ

SMMD follows immediately.

Lemma 7. Let (x, y) (cid:55)→ k(x, y) be the continuous kernel of an RKHS H deﬁned on a Polish space X ,
and deﬁne the corresponding pseudo-distance dk(x, y) := (cid:107)k(x, ·) − k(y, ·)(cid:107)H. Then the following
inequality holds for any distributions P and Q on X , including when the quantities are inﬁnite:

MMDk(P, Q) ≤ W dk (P, Q).

Proof. Let P and Q be two probability distributions, and let Π(P, Q) be the set of couplings between
them. Let π∗ ∈ argmin(X,Y )∼π[ck(X, Y )] be an optimal coupling, which is guaranteed to exist
[51, Theorem 4.1]; by deﬁnition W dk (P, Q) = E(X,Y )∼π∗ [dk(X, Y )]. When W dk (P, Q) = ∞ the
inequality trivially holds, so assume that W dk (P, Q) < ∞.
Take a sample (X, Y ) ∼ π(cid:63) and a function f ∈ H with (cid:107)f (cid:107)H ≤ 1. By the Cauchy-Schwarz
inequality,

(cid:107)f (X) − f (Y )(cid:107) ≤ (cid:107)f (cid:107)H(cid:107)k(X, ·) − k(Y, ·)(cid:107)H ≤ (cid:107)k(X, ·) − k(Y, ·)(cid:107)H.

Taking the expectation with respect to π(cid:63), we obtain

Eπ(cid:63) [|f (X) − f (Y )|] ≤ Eπ(cid:63) [(cid:107)k(X, ·) − k(Y, ·)(cid:107)H].
The right-hand side is just the deﬁnition of W dk (P, Q). By Jensen’s inequality, the left-hand side is
lower-bounded by

|Eπ∗ [f (X) − f (Y )]| = |EX∼P[f (X)] − EY ∼Q[f (Y )]|

since π(cid:63) has marginals P and Q. We have shown so far that for any f ∈ H with (cid:107)f (cid:107)H ≤ 1,

the result follows by taking the supremum over f .

|EP[f (X)] − EQ[f (Y )]| ≤ W ck (P, Q);

17

Lemma 8. Let ψ = ((W L, bL), (W L−1, bL−1), . . . , (W 1, b1)) ∈ Ψκ. There exists a corresponding
scalar α(ψ) and ¯ψ = (( ¯W L, ¯bL), ( ¯W L−1, ¯bL−1), . . . , ( ¯W 1, ¯b1)) ∈ Ψκ
1 , deﬁned by (18), such that
for all X,

φψ(X) = α(ψ) φ ¯ψ(X).

Proof. Set ¯W l = 1
1
m=1(cid:107)W m(cid:107)
number is unchanged, cond( ¯W l) = cond(W l) ≤ κ, and (cid:107) ¯W l(cid:107) = 1, so ¯ψ ∈ Φκ
see from (16) that

l=1(cid:107)W l(cid:107). Note that the condition
1 . It is also easy to

(cid:107)W l(cid:107) W l, ¯bl =

bl, and α(ψ) = (cid:81)L

(cid:81)l

so that

hl
¯ψ(X) =

1
m=1(cid:107)W m(cid:107)

(cid:81)l

hl
ψ(X)

α(ψ)hL

¯ψ(X) =

hL
ψ(X) = φψ(X).

(cid:81)L

(cid:81)L

l=1(cid:107)W l(cid:107)
l=1(cid:107)W l(cid:107)

Lemma 9. Make Assumptions (II) and (III), and let ψ ∈ Ψκ
intermediate activation is exactly zero,

1 . Then the set of inputs for which any

has zero Lebesgue measure. Moreover, for any X /∈ N ψ, ∇X φψ(X) exists and

Nψ =

L
(cid:91)

dl(cid:91)

l=1

k=1

(cid:110)
X ∈ Rd | (cid:0)hl

ψ(X)(cid:1)

k

(cid:111)

= 0

,

(cid:107)∇X φψ(X)(cid:107)2

F ≥

dLαL
κL .

(M l

X )k = σ(cid:48)

l(hl

k(X)) =

(cid:26)1 hl
α hl

k(X) > 0
k(X) < 0

;

Proof. First, note that the network representation at layer l is piecewise afﬁne. Speciﬁcally, deﬁne
M l

X ∈ Rdl by, using Assumption (III),

it is undeﬁned when any hl

k(X) = 0, i.e. when X ∈ N ψ. Let V l

ψ(X) = W lσl−1(hl−1
hl

ψ (X)) + bl = V l

X := W l diag (cid:0)M l−1
X X + bl,

X

(cid:1). Then

ψ(X) = Wl
hl
X = V l
X = V l
1 , we have (cid:107)W l(cid:107) = 1 and σmin(W l) ≥ 1/κ; also, (cid:107)M l
X (cid:107) ≤ 1, and using Assumption (II) with Lemma 10 gives σmin(Wl

X X + bl
X V l−1

X bl−1 + bl, and Wl

X · · · V 1

X ,

X , so long as X /∈ N ψ.

X (cid:107) ≤ 1, σmin(M l

X ) ≥ α.
X ) ≥ (α/κ)l. In

(20)

and thus

where b0

X = 0, bl
Because ψ ∈ Ψκ
Thus (cid:107)Wl
particular, each Wl

Next, note that bl
H l

X = (M l

X , M l−1

X is full-rank.
X and Wl
X , . . . , M 1

X each only depend on X through the activation patterns M l
X ) denote the full activation patterns up to level l, we can thus write

X . Letting

ψ(X) = WH l
hl

X X + bH l

X .

There are only ﬁnitely many possible values for H l
we have that

X ; we denote the set of such values as Hl. Then

L
(cid:91)

dL(cid:91)

(cid:91)

Nψ ⊆

l=0

k=1

H l∈Hl

(cid:110)
X ∈ Rd | WH l

k X + bH l

k = 0

(cid:111)

.

Because each WH l
is of rank dl, each set in the union is either empty or an afﬁne subspace of
k
dimension d − dl. As each dl > 0, each set in the ﬁnite union has zero Lebesgue measure, and Nψ
also has zero Lebesgue measure.

18

We will now show that the activation patterns are piecewise constant, so that ∇X hl
1 , we have (cid:107)hl
all X /∈ N ψ. Because ψ ∈ Ψκ
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12) ≤ (cid:107)X − X (cid:48)(cid:107).
(cid:12)

ψ(cid:107)Lip ≤ 1, and in particular

ψ(X (cid:48))(cid:1)

ψ(X)(cid:1)

− (cid:0)hl

(cid:0)hl

k

k

ψ(X) = WH l

X for

Thus,
minl=1,...,L mink=1,...,dl
for all l and k,

take some X /∈ N ψ, and ﬁnd the smallest absolute value of its activations, (cid:15) =
(cid:12)
(cid:12)
(cid:12); clearly (cid:15) > 0. For any X (cid:48) with (cid:107)X − X (cid:48)(cid:107) < (cid:15), we know that

(cid:17)
hl
ψ(X)

(cid:12)
(cid:12)
(cid:12)

(cid:16)

k

sign

(cid:16)(cid:0)hl

ψ(X)(cid:1)

(cid:17)

k

= sign

(cid:16)(cid:0)hl

ψ(X (cid:48))(cid:1)

(cid:17)

,

k

implying that H l
Finally, we obtain

X = H l

X (cid:48) as well as X (cid:48) /∈ N ψ. Thus for any point X /∈ N ψ, ∇φψ(X) = WH L
X .

(cid:107)∇φψ(X)(cid:107)2

F = (cid:107)WH L

X (cid:107)2

F ≥ dL σmin

(cid:16)

WH L

X

(cid:17)2

≥

dLαL
κL .

Lemma 10. Let A ∈ Rm×n, B ∈ Rn×p, with m ≥ n ≥ p. Then σmin(AB) ≥ σmin(A) σmin(B).

Proof. A more general version of this result can be found in [19, Theorem 2]; we provide a proof
here for completeness.
If B has a nontrivial null space, σmin(B) = 0 and the inequality holds. Otherwise, let Rn
Rn \ {0}. Recall that for C ∈ Rm×n with m ≥ n,
(cid:115)

∗ denote

(cid:113)

σmin(C) =

λmin(C TC) =

xTC TCx
xTx

inf
x∈Rn
∗

= inf
x∈Rn
∗

(cid:107)Cx(cid:107)
(cid:107)x(cid:107)

.

Thus, as Bx (cid:54)= 0 for x (cid:54)= 0,

σmin(AB) = inf
x∈Rp
∗
(cid:18)

(cid:107)ABx(cid:107)
(cid:107)x(cid:107)
(cid:107)ABx(cid:107)
(cid:107)Bx(cid:107)
(cid:107)Ay(cid:107)
(cid:107)y(cid:107)

= inf
x∈Rp
∗

(cid:19) (cid:18)

(cid:107)ABx(cid:107)(cid:107)Bx(cid:107)
(cid:107)Bx(cid:107)(cid:107)x(cid:107)
(cid:19)
(cid:107)Bx(cid:107)
inf
(cid:107)x(cid:107)
x∈Rp
∗
(cid:19)
(cid:107)Bx(cid:107)
(cid:107)x(cid:107)

inf
x∈Rp
∗

(cid:19) (cid:18)

≥

≥

(cid:18)

inf
x∈Rp
∗

inf
y∈Rn
∗

= σmin(A) σmin(B).

A.2.1 When some of the assumptions don’t hold

Here we analyze through simple examples what happens when the condition number can be un-
bounded, and when Assumption (II), about decreasing widths of the network, is violated.

Condition Number: We start by a ﬁrst example where the condition number can be arbitrarily
high. We consider a two-layer network on R2, deﬁned by

φα(X) = [1 −1] σ(WαX)

Wα =

(21)

(cid:20)1
1

(cid:21)

1
1 + α

where α > 0. As α approaches 0 the matrix Wα becomes singular which means that its condition
number blows up. We are interested in analyzing the behavior of the Lipschitz constant of φ and the
expected squared norm of its gradient under µ as α approaches 0.

One can easily compute the squared norm of the gradient of φ which is given by

(cid:107)∇φα(X)(cid:107)2 =






α2
X ∈ A1
γ2α2
X ∈ A2
(1 − γ)2 + (1 + α − γ)2 X ∈ A3
(1 − γ)2 + (γα + γ − 1)2 X ∈ A4

(22)

19

Here A1, A2, A3 and A4 are deﬁned by (23) and are represented in Figure 4:
A1 := {X ∈ R2|X1 + X2 ≥ 0 X1 + (1 + α)X2 ≥ 0}
A2 := {X ∈ R2|X1 + X2 < 0 X1 + (1 + α)X2 < 0}
A3 := {X ∈ R2|X1 + X2 < 0 X1 + (1 + α)X2 ≥ 0}
A4 := {X ∈ R2|X1 + X2 ≥ 0 X1 + (1 + α)X2 < 0}

(23)

(24)

(25)

(26)

Figure 4: Decomposition of R2 into 4 regions A1, A2, A3 and A4 as deﬁned in (23). As α approaches
0, the area of sets A3 and A4 becomes negligible.

It is easy to see that whenever µ has a density, the probability of the sets A3 and A4 goes to 0 are
α → 0. Hence one can deduce that Eµ[(cid:107)∇φα(X)(cid:107)2] → 0 when α → 0. On the other hand, the
squared Lipschitz constant of φ is given by (1 − γ)2 + (1 + α − γ)2 which converges to 2(1 − γ)2.
This shows that controlling the expectation of the gradient doesn’t allow to effectively control the
Lipschitz constant of φ.

Monotonicity of the dimensions: We would like to consider a second example where Assump-
tion (II) doesn’t hold. Consider the following two layer network deﬁned by:
(cid:35)

φ(X) = [−1

0

1] σ(WβX)

Wβ :=

(cid:34)1
0
1
0
1 β

for β > 0. Note that Wβ is a full rank matrix, but Assumption (II) doesn’t hold. Depending on the
sign of the components of WβX one has the following expression for (cid:107)∇φα(X)(cid:107)2:

where (Bi)1≤i≤6 are deﬁned by (26)

(cid:107)∇φα(X)(cid:107)2 =






β2
X ∈ B1
γ2β2
X ∈ B2
β2
X ∈ B3
(1 − γ)2 + γ2β2 X ∈ B4
(1 − γ)2 + β2
X ∈ B5
γ2β2
X ∈ B6

B1 := {X ∈ R2|X1 ≥ 0 X2 ≥ 0}
B2 := {X ∈ R2|X1 < 0 X2 < 0}
B3 := {X ∈ R2|X1 ≥ X2 < 0 X1 + βX2 ≥ 0}
B4 := {X ∈ R2|X1 ≥ X2 < 0 X1 + βX2 < 0}
B5 := {X ∈ R2|X1 > 0 X2 ≥ 0 X1 + βX2 ≥ 0}
B6 := {X ∈ R2|X1 > 0 X2 ≥ 0 X1 + βX2 < 0}

20

The squared Lipschitz constant is given by (cid:107)φ(cid:107)2
the gradient of φ is given by:

L(1 − γ)2 + β2 while the expected squared norm of

Eµ[(cid:107)φ(X)(cid:107)2] = 3β2(p(B1 ∪ B3 ∪ B5) + γ2p(B2 ∪ B4 ∪ B6)) + (1 − γ)2p(B4 ∪ B5).

(27)
Again the set B4 ∪ B5 becomes negligible as β approaches 0 which implies that Eµ[(cid:107)φ(X)(cid:107)2] → 0.
L converges to (1 − γ)2. Note that unlike in the ﬁrst example in (21), the
On the other hand (cid:107)φ(cid:107)2
matrix Wβ has a bounded condition number. In this example, the columns of W0 are all in the null
1], which implies ∇φ0(X) = 0 for all X ∈ R2, even though all matrices have full
space of [−1 0
rank.

B DiracGAN vector ﬁelds for more losses

Figure 5: Vector ﬁelds for different losses with respect to the generator parameter θ and the feature
representation parameter ψ; the losses use a Gaussian kernel, and are shown in (28). Following [30],
P = δ0, Q = δθ and φψ(x) = ψx. The curves show the result of taking simultaneous gradient steps
in (θ, ψ) beginning from three initial parameter values.

Figure 5 shows parameter vector ﬁelds, like those in Figure 6, for Example 1 for a variety of different
losses:

MMD: − MMD2
ψ
MMD-GP: − MMD2
ψ +λ EP[((cid:107)∇f (X)(cid:107) − 1)2]
MMD-GP-Unif: − MMD2
ψ +λ E

(cid:101)X(cid:39)µ∗ [((cid:107)∇f ( (cid:101)X)(cid:107) − 1)2]

SN-MMD: − 2 MMD1(P, Q)2

Sobolev-MMD: − MMD2
CenteredSobolev-MMD: − MMD2

ψ +λ(E(P+Q)/2[(cid:107)∇f (X)(cid:107)2] − 1)2
ψ +λ(E(P+Q)/2[(cid:107)∇f (X)(cid:107)2])2

(28)

LipMMD: − LipMMD2
GC-MMD: − GCMMD2
SMMD: − SMMD2

kψ,λ

kψ,P,λ

N (0,102),kψ,λ

21

2

The squared MMD between δ0 and δθ under a Gaussian kernel of bandwidth 1/ψ and is given by
2(1 − e− ψ2θ2
). MMD-GP-unif uses a gradient penalty as in [7] where each samples from µ∗ is
obtained by ﬁrst sampling X and Y from P and Q and then sampling uniformly between X and
Y . MMD-GP uses the same gradient penalty, but the expectation is taken under P rather than µ∗.
SN-MMD refers to MMD with spectral normalization; here this means that ψ = 1. Sobolev-MMD
refers to the loss used in [33] with the quadratic penalty only. GCMMDµ,k,λ is deﬁned by (5), with
µ = N (0, 102).

C Vector ﬁelds of Gradient-Constrained MMD and Sobolev GAN critics

Mroueh et al. [33] argue that the gradient of the critic (...) deﬁnes a transportation plan for moving
the distribution mass (from generated to reference distribution) and present the solution of Sobolev
PDE for 2-dimensional Gaussians. We observed that in this simple example the gradient of the
Sobolev critic can be very high outside of the areas of high density, which is not the case with the
Gradient-Constrained MMD. Figure 6 presents critic gradients in both cases, using µ = (P + Q)/2
for both.

(a) Gradient-Constrained MMD critic gradient.

(b) Sobolev IPM critic gradient.

Figure 6: Vector ﬁelds of critic gradients between two Gaussians. The grey arrows show normalized
gradients, i.e. gradient directions, while the black ones are the actual gradients. Note that for the
Sobolev critic, gradients norms are orders of magnitudes higher on the right hand side of the plot
than in the areas of high density of the given distributions.

This unintuitive behavior is most likely related to the vanishing boundary condition, assummed by
Sobolev GAN. Solving the actual Sobolev PDE, we found that the Sobolev critic has very high
gradients close to the boundary in order to match the condition; moreover, these gradients point in
opposite directions to the target distribution.

D An estimator for Lipschitz MMD

We now describe brieﬂy how to estimate the Lipschitz MMD in low dimensions. Recall that

LipMMDk,λ(P, Q) =

f ∈Hk : (cid:107)f (cid:107)2

sup
Lip+λ(cid:107)f (cid:107)2

≤1

Hk

EX∼P[f (X)] − EX∼Q[f (Y )].

For f ∈ Hk, it is the case that

(cid:107)f (cid:107)2

Lip = sup
x∈Rd

(cid:107)∇f (x)(cid:107)2 = sup
x∈Rd

(cid:104)∂ik(x, ·), f (cid:105)2

Hk

= sup
x∈Rd

f,

[∂ik(x, ·) ⊗ ∂ik(x, ·)] f

.

d
(cid:88)

i=1

(cid:42)

d
(cid:88)

i=1

(cid:43)

Hk

Thus we can approximate the constraint (cid:107)f (cid:107)2
Lip + λ(cid:107)f (cid:107)2
≤ 1 by enforcing the constraint on a set
Hk
of m points {Zi} reasonably densely covering the region around the supports of P and Q, rather

22

than enforcing it at every point in X . An estimator of the Lipschitz MMD based on X ∼ PnX and
Y ∼ QnY is

(cid:92)LipMMDk,λ (X, Y, Z) ≈ sup

nX(cid:88)

1
nX

f (Xj) −

nY(cid:88)

f (Yj)

1
nY

f ∈Hk

j=1

j=1
s.t. ∀j, (cid:107)∇f (Zj)(cid:107)2 + λ(cid:107)f (cid:107)2

≤ 1.

Hk

(29)

By the generalized representer theorem, the optimal f for (29) will be of the form

f (·) =

αjk(Xj, ·) +

βjk(Yj, ·) +

γ(i,j)∂ik(Zj, ·).

nX(cid:88)

j=1

nY(cid:88)

j=1

d
(cid:88)

m
(cid:88)

i=1

j=1

Writing δ = (α, β, γ), the objective function is linear in δ,
· · · − 1
nY

− 1
nY

(cid:2) 1
nX

1
nX

· · ·

0

· · ·

0(cid:3) δ.

The constraints are quadratic, built from the following matrices, where the X and Y samples are
concatenated together, as are the derivatives with each dimension of the Z samples:
















K :=

B :=

H :=

k(X1, X1)
...
k(YnX , X1)
∂1k(Z1, X1)
...
∂dk(Zm, X1)

∂1∂1+dk(Z1, Z1)
...
∂d∂1+dk(Zm, Z1)

· · ·
. . .
· · ·

k(X1, YnY )
...
k(YnY , YnY )






· · ·
. . .
· · ·






∂1k(Z1, YnY )
...
∂dk(Zm, YnY )
· · ·
. . .
· · ·

∂1∂d+dk(Z1, Zm)
...
∂d∂d+dk(Zm, Zm)




 .

Given these matrices, and letting Oj = (cid:80)d
vector in Rmd, we have that
(cid:21)
(cid:20)K BT
B H

= δT

(cid:107)f (cid:107)2

Hk

δ

(cid:107)∇f (Zj)(cid:107)2 =

i=1 e(i,j)eT

(i,j) where e(i,j) is the (i, j)th standard basis

d
(cid:88)

i=1

(∂if (Zj))2 = δT

(cid:20)BTOjB BTOjH
HOjB HOjH

(cid:21)

δ.

Thus the optimization problem (29) is a linear problem with convex quadratic constraints, which can
be solved by standard convex optimization software. The approximation is reasonable only if we can
effectively cover the region of interest with densely spaced {Zi}; it requires a nontrivial amount of
computation even for the very simple 1-dimensional toy problem of Example 1.

One advantage of this estimator, though, is that ﬁnding its derivative with respect to the input points
or the kernel parameterization is almost free once we have computed the estimate, as long as our
solver has computed the dual variables µ corresponding to the constraints in (29). We just need to
exploit the envelope theorem and then differentiate the KKT conditions, as done for instance in [1].
The differential of (29) ends up being, assuming the optimum of (29) is at ˆδ ∈ RnX +nY +md and
ˆµ ∈ Rm,

d (cid:92)LipMMDk,λ(X, Y, Z) = ˆδT

· · ·

1
nX

− 1
nY

· · · − 1
nY

(cid:3)T

−

ˆµj

ˆδT(dPj)ˆδ

(cid:21)

(cid:20)dK
dB

(cid:2) 1
nX

m
(cid:88)

j=1

dPj :=

(cid:20)(dB)TOjB + BTOj(dH)
(dH)OjB + HOj(dB)

(dB)TOjH + BTOj(dH)
(dH)OjH + HOj(dH)

(cid:21)

+ λ

(cid:20)dK dBT
dB dH

(cid:21)

.

E Near-equivalence of WGAN and linear-kernel MMD GANs

For an MMD GAN-GP with kernel k(x, y) = φ(x)φ(y), we have that
MMDk(P, Q) = |EP φ(x) − EQ φ(Y )|

23

and the corresponding critic function is

η(t)
(cid:107)η(cid:107)H

=

EX∼P φ(X)φ(t) − EY ∼Q φ(Y )φ(t)
|EP φ(X) − EQ φ(Y )|

= sign (EX∼P φ(X) − EY ∼Q φ(Y )) φ(t).

Thus if we assume EX∼P φ(X) > EY ∼Q φ(Y ), as that is the goal of our critic training, we see that
the MMD becomes identical to the WGAN loss, and the gradient penalty is applied to the same
function.
(MMD GANs, however, would typically train on the unbiased estimator of MMD2, giving a very
slightly different loss function. [7] also applied the gradient penalty to η rather than the true critic
η/(cid:107)η(cid:107).)

The SMMD with a linear kernel is thus analogous to applying the scaling operator to a WGAN; hence
the name SWGAN.

F Additional experiments

F.1 Comparison of Gradient-Constrained MMD to Scaled MMD

Figure 7 shows the behavior of the MMD, the Gradient-Constrained SMMD, and the Scaled MMD
when comparing Gaussian distributions. We can see that MMD ∝ SMMD and the Gradient-
Constrained MMD behave similarly in this case, and that optimizing the SMMD and the Gradient-
Constrained MMD is also similar. Optimizing the MMD would yield an essentially constant distance.

F.2 IGMs with Optimized Gradient-Constrained MMD loss

We implemented the estimator of Proposition 3 using the empirical mean estimator of η, and sharing
samples for µ = P. To handle the large but approximately low-rank matrix system, we used an
incomplete Cholesky decomposition [43, Algorithm 5.12] to obtain R ∈ R(cid:96)×M (1+d) such that
(cid:20)K GT
G H

≈ RTR. Then the Woodbury matrix identity allows an efﬁcient evaluation:

(cid:21)

(cid:0)RTR + M λI(cid:1)−1

=

(cid:0)I − R(RRT + M λI)−1R(cid:1) .

1
M λ

Even though only a small (cid:96) is required for a good approximation, and the full matrices K, G, and
H need never be constructed, backpropagation through this procedure is slow and not especially
GPU-friendly; training on CPU was faster. Thus we were only able to run the estimator on MNIST,
and even that took days to conduct the optimization on powerful workstations.

The learned models, however, were reasonable. Using a DCGAN architecture, batches of size 64,
and a procedure that otherwise agreed with the setup of Section 4, samples with and without spectral
normalization are shown in Figures 8a and 8b. After the points in training shown, however, the same
rank collapse as discussed in Section 4 occurred. Here it seems that spectral normalization may have
delayed the collapse, but not prevented it. Figure 8c shows generator loss estimates through training,
including the obvious peak at collapse; Figure 8d shows KID scores based on the MNIST-trained
convnet representation [7], including comparable SMMD models for context. The fact that SMMD
models converged somewhat faster than Gradient-Constrained MMD models here may be more
related to properties of the estimator of Proposition 3 rather than the distances; more work would be
needed to fully compare the behavior of the two distances.

F.3 Spectral normalization and Scaled MMD

Figure 9 shows the distribution of critic weight singular values, like Figure 2, at more layers. Figure 11
and Table 2 show results for the spectral normalization variants considered in the experiments.
MMDGAN, with neither spectral normalization nor a gradient penalty, did surprisingly well in this
case, though it fails badly in other situations.

Figure 9 compares the decay of singular values for layer of the critic’s network at both early and
later stages of training in two cases: with or without the spectral parametrization. The model was
trained on CelebA using SMMD. Figure 11 shows the evolution per iteration of Inception score,

24

Figure 7: Plots of various distances between one dimensional Gaussians, where P = N (0, 0.12), and
the colors show log D(P, N (µ, σ2)). All distances use λ = 1. Top left: MMD with a Gaussian kernel
of bandwidth ψ = 0.1. Top right: MMD with bandwidth ψ = 10. Middle left: Gradient-Constrained
MMD with bandwidth ψ = 0.1. Middle right: Gradient-Constrained MMD with bandwidth ψ = 10.
Bottom left: Optimized SSMD, allowing any ψ ∈ R. Bottom right: Optimized Gradient-Constrained
MMD.

25

(a) Without spectral
normalization; 32 000
generator iterations.

(b) With
spectral
normalization; 41 000
generator iterations.

Figure 8: The MNIST models with Optimized Gradient-Constrained MMD loss.

(c) Generator losses.

(d) KID scores.

FID and KID for Sobolev-GAN, MMDGAN and variants of MMDGAN and WGAN using spectral
normalization. It is often the case that this parametrization alone is not enough to achieve good
results.

Figure 9: Singular values at different layers, for the same setup as Figure 2.

F.4 Additional samples

Figures 12 and 13 give extra samples from the models.

26

µ,k,λ MMD2

k for SMMDGAN and SN-SMMDGAN, and MMD2

Figure 10: Evolution of various quantities per generator iteration on CelebA during training. 4
models are considered: (SMMDGAN, SN-SMMDGAN, MMDGAN, SN-MMDGAN). (a) Loss:
SMMD2 = σ2
k for MMDGAN and
SN-MMDGAN. The loss saturates for MMDGAN (green); spectral normalization allows some
improvement in loss, but training is still unstable (orange). SMMDGAN and SN-SMMDGAN both
lead to stable, fast training (blue and red). (b) SMMD controls the critic complexity well, as expected
(blue and red); SN has little effect on the complexity (orange). (c) Ratio of the highest singular value
to the smallest for the ﬁrst layer of the critic network: σmax/σmin. SMMD tends to increase the
condition number of the weights during training (blue), while SN helps controlling it (red). (d) KID
score during training: Only variants using SMMD lead to stable training in this case.

Figure 11: Evolution per iteration of different scores for variants of methods, mostly using spectral
normalization, on CIFAR-10.

27

Table 2: Mean (standard deviation) of score evaluations on CIFAR-10 for different methods using
Spectral Normalization.

Method

MMDGAN
SN-WGAN
SN-WGAN-GP
SN-Sobolev-GAN
SN-MMDGAN-GP
SN-MMDGAN-L2
SN-MMDGAN
SN-MMDGAN-GP-L2
SN-SMMDGAN

IS

FID

KID×103

5.5±0.0
2.2±0.0
2.5±0.0
2.9±0.0
4.6±0.1
7.1±0.1
6.9±0.1
6.9±0.2
7.3±0.1

73.9±0.1
208.5±0.2
154.3±0.2
140.2±0.2
96.8±0.4
31.9±0.2
31.5±0.2
32.3±0.3
25.0±0.3

39.4±1.5
178.9±1.5
125.3±0.9
130.0±1.9
59.5±1.4
21.7±0.9
21.7±1.0
20.9±1.1
16.6±2.0

Figure 12: Samples from a generator trained on ImageNet dataset using Scaled MMD with Spectral
Normalization: SN-SMMDGAN.

28

(a) SNGAN

(b) SobolevGAN

(c) MMDGAN-GP-L2

(d) SN-SMMD GAN

(e) SN SWGAN

(f) SMMD GAN

Figure 13: Comparison of samples from different models trained on CelebA with 160×160 resolution.

29

