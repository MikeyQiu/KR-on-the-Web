0
2
0
2
 
b
e
F
 
3
 
 
]

V
C
.
s
c
[
 
 
1
v
8
4
0
1
0
.
2
0
0
2
:
v
i
X
r
a

PREPRINT - WORK IN PROGRESS

1

Multi-Channel Attention Selection GANs for
Guided Image-to-Image Translation

Hao Tang, Dan Xu, Yan Yan, Jason J. Corso, Philip H.S. Torr, Nicu Sebe

Abstract—We propose a novel model named Multi-Channel Attention Selection Generative Adversarial Network (SelectionGAN) for
guided image-to-image translation, where we translate an input image into another while respecting an external semantic guidance.
The proposed SelectionGAN explicitly utilizes the semantic guidance information and consists of two stages. In the ﬁrst stage, the input
image and the conditional semantic guidance are fed into a cycled semantic-guided generation network to produce initial coarse
results. In the second stage, we reﬁne the initial results by using the proposed multi-scale spatial pooling & channel selection module
and the multi-channel attention selection module. Moreover, uncertainty maps automatically learned from attention maps are used to
guide the pixel loss for better network optimization. Exhaustive experiments on four challenging guided image-to-image translation
tasks (face, hand, body and street view) demonstrate that our SelectionGAN is able to generate signiﬁcantly better results than the
state-of-the-art methods. Meanwhile, the proposed framework and modules are uniﬁed solutions and can be applied to solve other
generation tasks, such as semantic image synthesis. The code is available at https://github.com/Ha0Tang/SelectionGAN.

Index Terms—GANs, Deep Attention Selection, Cascade Generation, Guided Image-to-Image Translation.

(cid:70)

1 INTRODUCTION

G UIDED image-to-image translation is a task aiming at

synthesizing new images from an input image and
several external semantic guidance, as shown in Fig. 1. This
task has been gaining a lot interest especially from the com-
puter vision community, and has been widely investigated
in recent years. Due to different forms of semantic guidance,
e.g., segmentation maps, hand skeletons, facial landmarks
and pose skeleton, most of the existing methods for this class
of tasks are tailored toward speciﬁc applications, i.e., they
need to speciﬁcally design the network architectures and
training objectives according to different generation tasks.
For example, Ma et al. propose PG2 [1], which is a two-
stage framework and uses the pose mask loss for generating
person images based on an image of that person and human
pose keypoints. Tang et al. propose GestureGAN [2], which
is a forward-backward consistency architecture and adopt
the proposed color loss to generate novel hand gesture
images based on the input image and conditional hand
skeletons. Wang et al. propose the few-shot Vid2Vid frame-
work [3], which uses the carefully designed weight gener-
ation module to synthesize videos that realistically reﬂect
the style of the input image and the layout of conditional
segmentation maps.

Different

from previous works in guided image-to-
image translation, in this paper, we focus on developing a
framework that is application-independent. This makes our

• Hao Tang and Nicu Sebe are with the Department of Information Engi-
neering and Computer Science (DISI), University of Trento, Trento 38123,
Italy. E-mail: hao.tang@unitn.it

• Dan Xu and Philip H.S. Torr are with the Department of Engineering
Science, University of Oxford, Oxford OX1 2JD, United Kingdom.
• Yan Yan is with the Department of Computer Science, Texas State

•

University, San Marcos 78666, USA.
Jason J. Corso is with the Department of Electrical Engineering and
Computer Science, University of Michigan, Ann Arbor 48109, USA.

Manuscript revised on Feb 03, 2020.

framework and modules more widely applicable to many
generation tasks with different forms of semantic guidance.
To tackle this challenging problem, AlBahar and Huang [4]
recently proposed a bi-directional feature transformation to
better utilize the constraints of the semantic guidance. Al-
though this approach performed an interesting exploration,
we observe unsatisfactory aspects mainly in the generated
image layout and content details, which are due to three dif-
ferent reasons. First, since it is always costly to obtain man-
ually annotated semantic guidance, the semantic guidance
is usually produced from pre-trained models trained on
other large-scale datasets, e.g., pose skeletons are extracted
using OpenPose [5] and segmentation maps are extracted
using [6], [7], leading to insufﬁciently accurate predictions
for all the pixels, and thus misguiding the image generation
process. Second, we argue that the translation with a single
phase generation network is not able to capture the complex
image structural relationships between the source and target
domains, especially when source and target domains only
have little or even no overlap, e.g, person image generation
and cross-view image translation. Third, a three-channel
generation space may not be suitable enough for learning
a good mapping for this complex synthesis problem. Given
these problems, could we enlarge the generation space and
learn an automatic selection mechanism to synthesize more
ﬁne-grained generation results?

Based on these observations,

in this paper, we pro-
pose a novel Multi-Channel Attention Selection Generative
Adversarial Network (SelectionGAN), which contains two
generation stages. The overall framework of the proposed
SelectionGAN is shown in Fig. 2. In the ﬁrst stage, we learn
a cycled image-guidance generation sub-network, which
accepts a pair consisting of an image and the conditional
semantic guidance, and generates target images, which are
further fed into a semantic guidance generation network
to reconstruct the input semantic guidance. This cycled

PREPRINT - WORK IN PROGRESS

2

Fig. 1: SelectionGAN’s capabilities: (Top) Guided image-to-image translation (including cross-view image translation, hand
gesture translation, facial expression generation and person image generation): synthesizing images from a single input
image as well as semantic guidance (e.g., segmentation map, hand skeleton, facial landmark and human pose skeleton).
(Bottom) Semantic image synthesis: SelectionGAN simultaneously produces realistic images while respecting the spatial
semantic layout for both outdoor and indoor scenes.

guidance generation adds stronger supervision between the
image and guidance domains, facilitating the optimization
of the network.

The coarse outputs from the ﬁrst generation network,
including the input image, together with the deep feature
maps from the last layer, are input into the second stage
networks. We ﬁrst employ the proposed multi-scale spatial
pooling & channel selection module to enhance the multi-
scale features in both spatial and channel dimensions. Next,
several intermediate outputs are produced, and simulta-
neously we learn a set of multi-channel attention maps
with the same number as the intermediate generations.
These attention maps are used to spatially select from the
intermediate generations, and are combined to synthesize
a ﬁnal output. Finally, to overcome the inaccurate seman-
tic guidance issue, the multi-channel attention maps are
further used to generate uncertainty maps to guide the
reconstruction loss. Through extensive experimental eval-
uations, we demonstrate that SelectionGAN produces re-
markably better results than the existing baselines on four
different guided image-to-image translation tasks, i.e., seg-
mentation map guided cross-view image translation, hand
skeleton guided gesture-to-gesture translation, facial land-
mark guided expression-to-expression translation and pose
guided person image generation. Moreover, the proposed
framework and modules can be applied to other generation
tasks such as semantic image synthesis.

Overall, the contributions of this paper are as follows:
• A novel multi-channel attention selection GAN frame-
work (SelectionGAN) for guided image-to-image trans-
lation task is presented. It explores cascaded semantic
guidance with a coarse-to-ﬁne inference, and aims at
producing a more detailed synthesis from richer and more
diverse multiple intermediate generations.

• A novel multi-scale spatial pooling & channel selection
module is proposed, which is utilized to automatically
enhance the multi-scale feature representation in both

spatial and channel dimensions.

• A novel multi-channel attention selection module is pro-
posed, which is utilized to attentively select interested
intermediate generations and is able to signiﬁcantly boost
the quality of the ﬁnal output. The multi-channel attention
module also effectively learns uncertainty maps to guide
the pixel loss for more robust optimization.

• Extensive experiments clearly demonstrate the effective-
ness of the proposed SelectionGAN, and show state-of-
the-art results on four guided image-to-image translation
(including face, hand, body and street view) tasks. More-
over, we show the proposed SelectionGAN is effective on
other generation tasks such as semantic image synthesis.
Part of the material presented here appeared in [8]. The
current paper extends [8] in several ways. (1) We present
a more detailed analysis of related works by including
recently published works dealing with guided image-to-
image translation. (2) We propose a novel module, i.e.,
multi-scale spatial pooling & channel selection, to automat-
ically enhance the multi-scale feature representation in both
spatial and channel dimensions. Equipped with this new
module, our SelectionGAN proposed in [8] is upgraded to
SelectionGAN++. (3) We extent the proposed framework to
a more robust and general framework for handling different
guided image-to-image translation tasks. (4) We extend the
quantitative and qualitative experiments by comparing our
SelectionGAN and SelectionGAN++ with the very recent
works on four guided image-to-image translation tasks and
one semantic image synthesis task with 11 public datasets.

2 RELATED WORK

Generative Adversarial Networks (GANs) [9] have shown
the capability of generating high-quality images [10]. A
vanilla GAN model [9] has two important components: a
generator G and a discriminator D. The goal of G is to
generate photo-realistic images from a noise vector, while

PREPRINT - WORK IN PROGRESS

3

Fig. 2: Overview of the proposed SelectionGAN. Stage I presents a cycled semantic-guided generation sub-network which
accepts both the input image and the conditional semantic guidance, and simultaneously synthesizes the target images
and reconstructs the semantic guidance. Stage II takes the coarse predictions and the learned deep features from stage
I, and performs a ﬁne-grained generation using the proposed multi-scale spatial pooling & channel selection and the
multi-channel attention selection modules.

D is trying to distinguish between a real image and the
image generated by G. Although it is successfully used in
generating images of high visual ﬁdelity, there are still some
challenges, i.e., how to generate images in a conditional
setting. To generate domain-speciﬁc images, Conditional
GANs (CGANs) [11] have been proposed. One speciﬁc
application of CGANs is image-to-image translation [12].
Image-to-Image Translation frameworks learn a parametric
mapping between inputs and outputs. For example, Isola et
al. [12] propose Pix2pix, which is a supervised model and
uses a CGAN to learn a translation function from input to
output image domains. Based on Pix2pix, Wang et al. [13]
propose Pix2pixHD, which can turn semantic maps into
photo-realistic images.

Our work builds upon the recent advances in image-to-
image translation, i.e., Pix2pix, and aims to extend it to a
broader set of guided image-to-image translation problem,
which provides users with more input. Moreover, the pro-
posed multi-scale spatial pooling & channel selection and
the multi-channel attention selection modules are network-
agnostic and can be plugged into any existing CNN-based
generation architectures.
Guided Image-to-Image Translation is a variant of image-
to-image translation problem aimed at translating an in-
put image to a target image while respecting certain con-
strains speciﬁed by some external guidance, such as class
labels [14], [15], text descriptions [16], [17], human key-
point/skeleton [1], [2], [18], segmentation maps [3], [8],
[19], [20] and reference images [4], [21]. Given that differ-
ent generation tasks need different guidance information,
existing works are tailored to a speciﬁc application, i.e.,
with speciﬁcally designed network architectures and train-
ing objectives. For example, Ma et al. propose PG2 [1],
which is a two-stage framework and uses the pose mask
loss for generating person images based on an image of
that person and human pose keypoints. Tang et al. propose
GestureGAN [2], which is a forward-backward consistency
architecture and adopt the proposed color loss to gener-
ate novel hand gesture images based on the input image
and conditional hand skeletons. Wang et al. propose the
few-shot Vid2Vid framework [3], which uses a carefully
designed weight generation module to synthesize videos

that realistically reﬂect the style of the input image and the
layout of conditional segmentation maps.

Compared to existing works in guided image-to-image
translation, we develop a uniﬁed and robust framework that
is application-independent. In this way, the proposed frame-
work can be widely applied to many generation tasks with
different forms of guidance, such as scene segmentation
maps, hand skeletons, facial landmarks and human body
skeleton, as shown in Fig. 1.
Attention Learning in Image-to-Image Translation. Atten-
tion learning have been extensively exploited in computer
vision and natural language processing, e.g., [22], [23]. To
improve the image generation performance, the attention
mechanism has also been recently investigated in the image-
to-image translation tasks [24], [25], [26].

Unlike existing attention methods, we aim at a more
effective network design and propose a novel multi-channel
attention selection GAN, which allows to automatically
select from multiple diverse and rich intermediate gen-
erations, and thus signiﬁcantly improving the generation
quality. To the best of our knowledge, our model is the ﬁrst
attempt to incorporate a multi-channel attention selection
module within a GAN framework for image-to-image trans-
lation.

3 SELECTIONGAN

In this section we present the details of the proposed multi-
channel attention selection GAN. An illustration of the over-
all network structure is depicted in Fig. 2. In the ﬁrst stage,
we present a cascaded semantic-guided generation sub-
network, which utilizes the input image and the conditional
semantic guidance as inputs, and generate the target images
while respecting the semantic guidance.

These generated images are further input into a semantic
guidance generator to recover the input semantic guidance
forming a generation cycle. In the second stage, the coarse
synthesis and the deep features from the ﬁrst stage are
combined, and then are passed to the proposed multi-scale
spatial pooling & channel selection module to model the
long-range multi-scale dependencies between each channel
of feature representations. Thus the enhanced feature maps

PREPRINT - WORK IN PROGRESS

4

are fed to the proposed multi-channel attention selection
module, which aims at producing more ﬁne-grained syn-
thesis from a larger generation space and also at generating
uncertainty maps to jointly guide multiple optimization
losses.

selection modules to better utilize the coarse outputs from
the ﬁrst stage and to produce ﬁne-grained ﬁnal outputs. We
observed signiﬁcant improvement by using the proposed
cascade strategy, illustrated in the experimental part.

3.1 Cascade Semantic-Guided Generation

(cid:48)

(cid:48)

(cid:48)

(cid:48)

(cid:48)

(cid:48)

g as I

Semantic-Guided Generation. We target to translate an
input image to another while respecting the semantic guid-
ance. There are many strategies to incorporate the addi-
tional semantic guidance into the image-to-image transla-
tion model [4] and the most straight forward scheme is input
concatenation. Speciﬁcally, as shown in Fig. 2, we concate-
nate the input image Ia and the semantic guidance Sg, and
feed them into the image generator Gi and synthesize the
target image I
g=Gi(Ia, Sg). In this way, the semantic
guidance provides stronger supervision to guide the image-
to-image translation in the deep network.
Semantic-Guided Cycle. Existing guided image-to-image
translation methods [1], [4], [27] only use semantic guidance
as input to guide the image generation process, which
actually provide a weak guidance. Different from theirs,
we apply the semantic guidance not only as input but also
as part of the network’s output. Speciﬁcally, as shown in
Fig. 2, we propose a cycled semantic guidance generation
network to beneﬁt more the semantic guidance information
in learning jointly. The conditional semantic guidance Sg
together with the input image Ia are input into the image
generator Gi, and produce the synthesized image I
g. Then
g is further fed into the semantic guidance generator Gs
I
which reconstructs a new semantic guidance S
g. We can
g)=Gs(Gi(Ia, Sg)). Then
formalize the process as S
the optimization objective is to make S
g as close as possible
to Sg, which naturally forms a semantic guidance generation
cycle, i.e., [Ia, Sg] Gi→ I
g≈Sg. The two generators are
explicitly connected by the ground-truth semantic guidance,
which in this way provides extra constraints on the gen-
erators to better learn the semantic structure consistency.
We observe that the simultaneous generation of both the
images and the semantic guidance improves the generation
performance in our experiments section.
Cascade Generation. Due to the complexity of the tasks
such as in pose guided person image generation, input and
output domains usually have little overlap, which appar-
ently leads to ambiguity issues in the generation process.
Moreover, we observe that the image generator Gi outputs
a coarse synthesis after the ﬁrst stage, which yields blurred
image details and high pixel-level dissimilarity with the
target images. Both inspire us to explore a coarse-to-ﬁne
generation strategy in order to boost the synthesis perfor-
mance based on the coarse predictions. Cascade models
have been used in several other computer vision tasks such
as object detection [28] and semantic segmentation [29], and
have shown great effectiveness. In this paper, we introduce
the cascade strategy to deal with the guided image-to-
image translation problems. In both stages we have a basic
cycled semantic guidance generation sub-network, while in
the second stage, we propose two novel multi-scale spatial
pooling & channel selection and multi-channel attention

g=Gs(I

Gs→ S

(cid:48)
g

(cid:48)

(cid:48)

(cid:48)

3.2 Multi-Scale Spatial Pooling & Channel Selection

An overview of the proposed multi-scale spatial pooling &
channel selection module is shown in Fig. 3. The module
consists of a multi-scale spatial pooling and a multi-scale
channel selection components. In this way, the proposed
module can learn multi-scale deep feature interdependen-
cies in both spatial and channel dimensions.
Multi-Scale Spatial Pooling. Since there exists a large
object/scene deformation between the source domain and
the target domain, a single-scale feature may not be able
to capture all the necessary spatial information for a ﬁne-
grained generation. Thus, we propose a multi-scale spatial
pooling scheme, which uses a set of different kernel sizes
and strides to perform a global average pooling on the same
input features. By so doing, we obtain multi-scale features
with different receptive ﬁelds to perceive different spatial
contexts. More speciﬁcally, given the coarse inputs and the
deep features produced from the stage I, we ﬁrst concatenate
all of them as new features denoted as Fc∈RC×H×W for the
stage II as:

Fc = concat(Ia, I

g, Fi, Fs),

(cid:48)

(1)

where concat(·) is a function for channel-wise concatenation
operation; Fi and Fs are features from the last convolution
layers of the generators Gi and Gs, respectively. H and
W are width and height of the features, and C is the
number of channels. We apply a set of M spatial scales
{si}M
i=1 in pooling, resulting in pooled features with dif-
ferent spatial resolution. Different from the pooling scheme
used in [30] which directly combines all the features after
pooling, we ﬁrst select each pooled feature via an element-
wise multiplication with the input feature. Since in our
task the input features are from different sources, highly
correlated features would preserve more useful information
for the generation. Let us denote pl ups(·) as pooling at a
scale s followed by an up-sampling operation to rescale the
pooled feature at the same resolution, and ⊗ as element-
wise multiplication, we can formalize the whole process as
follows:
Fm ← concat(cid:0)Fc, Fc ⊗ pl up1(Fc), . . . , Fc ⊗ pl upM (Fc)),
(2)
which produces new multi-scale features Fm∈R4C×H×W
(in our experiments, we set M =3.) for the use in the
next multi-scale channel selection module. By doing so, the
“level” of features can be enriched by combining multiple
scale feature maps.
Multi-Scale Channel Selection. Each channel map of Fm
can be now regarded as a scale-speciﬁc response, and differ-
ent scale feature maps should be associated with each other.
To exploit the interdependencies between each scale features
of Fm, we propose a multi-scale channel selection module
to explicitly model interdependencies between channels of
multi-scale feature Fm. The structure of multi-scale channel
selection module is illustrated in Fig. 3.

PREPRINT - WORK IN PROGRESS

5

Fig. 3: Proposed multi-scale spatial pooling & channel selection module. The multi-scale spatial pooling pools features
from different receptive ﬁelds in order to have better generation of image details; the multi-scale channel selection
aims at automatically emphasizing interdependent channel maps by integrating associated features among all multi-scale
channel maps to improve deep feature representation. ⊕, ⊗, c(cid:13), s(cid:13) and ↑(cid:13) denote element-wise addition, element-wise
multiplication, channel-wise concatenation, softmax and up-sampling operation, respectively.

The channel attention map A can be obtained from the
multi-scale feature Fm. More speciﬁc, Fm is ﬁrst reshaped
to R4C×HW , and then a matrix multiplication is preformed
between Fm and the transpose of Fm. Next, we employ a
Softmax activation function to obtain the channel attention
map A∈R4C×4C . Each pixel Aji in A measures the ith
channel’s impact on the jth channel. In this way, the cor-
relation can be built between features from different scales.
Moreover, to reshape back to R4C×H×W , we perform a
matrix multiplication between A and the transpose of Fm.
Then, the result is multiplied by a parameter α and added to
the original feature Fm to obtain the channel-wise enhanced
feature F

m∈R4C×H×W ,

(cid:48)

(cid:48)

F

m = α

(AjiFmi) + Fmj.

(3)

4C
(cid:88)

i=1

(cid:48)

By doing so, each channel in the ﬁnal feature F
m is a
weighted sum of all channels and it models the long-range
dependencies between multi-scale feature maps. Finally, the
enhanced feature F
m is fed into a convolutional layer to ob-
c∈RC×H×W , which has the same size as the original
tain F
one Fc. This design ensures that the proposed multi-scale
spatial pooling & channel selection module can be plugged
into existing computer vision architectures.

(cid:48)

(cid:48)

3.3 Multi-Channel Attention Selection

In previous image-to-image translation works, the image
was generated only in a three-channel RGB space. We argue
that this is not enough for the complex translation problem
we are dealing with, and thus we explore using a larger
generation space to have a richer synthesis via constructing
multiple intermediate generations. Accordingly, we design
a multi-channel attention mechanism to automatically per-
form spatial and temporal selection from the generations to
synthesize a ﬁne-grained ﬁnal output.

(cid:48)

Given the

enhanced multi-scale

feature volume
c∈RC×H×W , where H and W are width and height of
F
the features, and C is the number of channels, we consider
two directions as shown in Fig. 4. One is for the generation
of multiple intermediate image synthesis and the other is for

G}N

G, bi

the generation of multi-channel attention maps. To produce
N different intermediate generations IG={I i
i=1, a convo-
lution operation is performed with N convolutional ﬁlters
{W i
i=1 followed by a tanh(·) non-linear activation
operation. For the generation of corresponding N attention
maps, the other group of ﬁlters {W i
i=1 is applied.
Then the intermediate generations and the attention maps
are calculated as follows:

A, bi

G}N

A}N

(cid:48)

I i
G = tanh(F
I i
A = Softmax(F

cW i
cW i

G + bi
A + bi

G),
A),

(cid:48)

for i = 1, . . . , N

for i = 1, . . . , N

(4)

where Softmax(·) is a channel-wise softmax function used
for the normalization. Finally, the learned attention maps
are utilized to perform channel-wise selection from each
intermediate generation as follows:

(cid:48)(cid:48)

g = (I 1
I

A ⊗ I 1

G) ⊕ · · · ⊕ (I N

A ⊗ I N
G )

(5)

(cid:48)(cid:48)

(cid:48)(cid:48)

(cid:48)(cid:48)

g =Gs(I

where I
g represents the ﬁnal synthesized generation se-
lected from the multiple diverse results, and ⊕ denotes the
element-wise addition. We also generate a ﬁnal semantic
i.e.,
guidance in the second stage as in the ﬁrst stage,
S
g ). Due to the same purpose of the two semantic
guidance generators, we use a single Gs twice by sharing the
parameters in both stages to reduce the network capacity.
Uncertainty-Guided Pixel Loss. As we discussed in the
introduction, the semantic guidance obtained from the pre-
trained model is not accurate for all the pixels, leading to
a wrong guidance during training. To tackle this issue, we
propose to learn uncertainty maps to control the optimiza-
tion loss as shown in Fig. 4. The uncertainty learning has
been investigated in [31] for multi-task learning, and here
we introduce it for solving the noisy semantic guidance
problem. Assume that we have K different loss maps which
need a guidance. The multiple generated attention maps
are ﬁrst concatenated and passed to a convolution layer
with K ﬁlters {W i
i=1 to produce a set of K uncertainty
maps. The reason for using the attention maps to generate
uncertainty maps is that the attention maps directly affect
the ﬁnal generation leading to a close connection with the

u}K

PREPRINT - WORK IN PROGRESS

6

Fig. 4: Proposed multi-channel attention selection module. The multi-channel attention selection aims at automatically
select from a set of intermediate diverse generations in a larger generation space to improve the generation quality; the
multi-channel attention module also effectively learns uncertainty maps to guide the pixel loss for robust joint images
and guidances optimization. ⊕, ⊗ and c(cid:13) denote element-wise addition, element-wise multiplication and channel-wise
concatenation, respectively.

loss. Let Li
i-th uncertainty map, we have:

p denote a pixel-level loss map and Ui denote the

Ui = σ(cid:0)W i

u(concat(I 1

A, . . . , I N

A ) + bi
u

(cid:1)

Li

p ←

Li
p
Ui

+ log Ui,

for i = 1, . . . , K

where σ(·) is a Sigmoid function for pixel-level normaliza-
tion. The uncertainty map is automatically learned and acts
as a weighting scheme to control the optimization loss.
Parameter-Sharing Discriminator. We extend the vanilla
discriminator in [12] to a parameter-sharing structure. In
the ﬁrst stage, this structure takes the real image Ia and
g or the ground-truth image Ig as
the generated image I
input. The discriminator D learns to tell whether a pair of
images from different domains is associated with each other
or not. In the second stage, it accepts the real image Ia and
g or the real image Ig as inputs. This
the generated image I
pairwise input encourages D to discriminate the diversity of
image structure and to capture the local-aware information.

(cid:48)(cid:48)

(cid:48)

3.4 Overall Optimization Objective
Adversarial Loss. In the ﬁrst stage, the adversarial loss of D
for distinguishing synthesized image pairs [Ia, I
g] from real
image pairs [Ia, Ig] is formulated as follows:

(cid:48)

LcGAN (Ia, I

(cid:48)

g) =EIa,Ig [log D(Ia, Ig)] +
log(1 − D(Ia, I

E

(cid:104)

Ia,I (cid:48)
g

(cid:105)

.

(cid:48)

g))

In the second stage, the adversarial loss of D for distin-
guishing synthesized image pairs [Ia, I
g ] from real image
pairs [Ia, Ig] is formulated as follows:

(cid:48)(cid:48)

LcGAN (Ia, I

(cid:48)(cid:48)

g )=EIa,Ig [log D(Ia, Ig)] +
log(1 − D(Ia, I

E

(cid:104)

Ia,I (cid:48)(cid:48)
g

(cid:105)

.

(cid:48)(cid:48)

g ))

Both losses aim to preserve the local structure information
and produce visually pleasing synthesized images. Thus,
the adversarial loss of the proposed SelectionGAN is the
sum of Eq. (7) and (8),

LcGAN = LcGAN (Ia, I

g) + λLcGAN (Ia, I

g ).

(9)

(cid:48)

(cid:48)(cid:48)

(7)

(8)

Overall Loss. The total optimization loss is a weighted sum
of the above losses. Generators Gi, Gs, multi-scale spatial
pooling & channel selection module Gm, multi-channel
attention selection network Ga and discriminator D are
trained in an end-to-end fashion optimizing the following
min-max function:

(6)

4
(cid:88)

i=1

min
{Gi,Gs,Gm,Ga}

max
{D}

L =

λiLi

p + LcGAN + λtvLtv.

(cid:48)(cid:48)

(cid:48)(cid:48)

(10)
where Li
p uses the L1 reconstruction to separately calculate
the pixel loss between the generated 4 images (i.e., I
g,
g ) and the corresponding real images. Ltv is the
g and S
I
total variation regularization [32] on the ﬁnal synthesized
g . λi and λtv are the trade-off parameters to control
image I
the relative importance of different objectives. The training
is performed by solving the min-max optimization problem.

g, S

(cid:48)(cid:48)

(cid:48)

(cid:48)

3.5 Implementation Details

Network Architecture. For a fair comparison, we employ
U-Net [12] as our generator architectures Gi and Gs. U-
Net is a network with skip connections between a down-
sampling encoder and an up-sampling decoder. Such ar-
chitecture comprehensively retains contextual and textural
information, which is crucial for removing artifacts and
padding textures. Since our focus is on the image gener-
ation task, Gi is more important than Gs. Thus we use
a deeper network for Gi and a shallow network for Gs,
such asymmetric architecture design can also be observed in
other generation papers [33]. Speciﬁcally, the ﬁlters in ﬁrst
convolutional layer of Gi and Gs are 64 and 4, respectively.
For the network Ga, the kernel size of convolutions for
generating the intermediate images and attention maps are
3×3 and 1×1, respectively. We adopt PatchGAN [12] for the
discriminator D.
Training Details. We mainly focus on four guided image-to-
image translation tasks in this paper. For cross-view image
translation, we follow [19] and use ReﬁneNet [6] and [7]
to generate segmentation maps on Dayton, SVA, Ego2Top

PREPRINT - WORK IN PROGRESS

7

TABLE 1: Ablations study of the proposed SelectionGAN.

Setup of SelectionGAN
Gi→ I
Gi→ I

Ia

Sg

(cid:48)
g
(cid:48)
g

Gi→ I

(cid:48)
g
(cid:48)
Gs→ S
g

[Ia, Sg]

[Ia, Sg]
Gi→ I
D + Uncertainty-Guided Pixel Loss
E + Multi-Channel Attention Selection
F + Total Variation Regularization
G + Multi-Scale Spatial Pooling

(cid:48)
g

SSIM PSNR

SD

0.4555

19.6574

18.8870

0.5223

22.4961

19.2648

0.5374

22.8345

19.2075

0.5438
0.5522
0.5989
0.6047
0.6167

22.9773
23.0317
23.7562
23.7956
23.9310

19.4568
19.5127
20.0000
20.0830
20.1214

A

B

C

D
E
F
G
H

TABLE 2: Quantitative results of coarse-to-ﬁne generation
on cross-view image translation task.

TABLE 3: Inﬂuence of the number of attention channels N .

Baseline
F
F
G
G
H
H

Stage I
√

Stage II
√

√

√

√

√

SSIM PSNR
23.1919
0.5551
23.7562
0.5989
23.2574
0.5680
23.7956
0.6047
23.1545
0.5567
23.9310
0.6167

SD
19.6311
20.0000
19.7371
20.0830
19.6034
20.1214

N
0
1
5
10
32

SSIM PSNR
22.9773
0.5438
23.0317
0.5522
23.8068
0.5901
0.5986
23.7336
23.8265
0.5950

SD
19.4568
19.5127
20.0033
19.9993
19.9086

35,532/8,884 image pairs in train/test split. Following [19],
[40], the aerial images are center-cropped to 224×224 and
resized to 256×256. For the ground level images and cor-
responding segmentation maps, we take the ﬁrst quarter
of both and resize them to 256×256. (iii) The Surround
Vehicle Awareness (SVA) dataset [41] is a synthetic dataset
collected from Grand Theft Auto V (GTAV) video game.
Following [37], we select every tenth frame to remove
redundancy in this dataset since the consecutive frames in
each set are very similar to each other. Thus, we collect
46,030/22,254 image pairs for training and testing, respec-
tively. (iv) The Ego2Top dataset [42] is more challenging and
contains different indoor and outdoor conditions. Each case
contains one top-view video and several egocentric videos
captured by the people visible in the top-view camera. This
dataset has more than 230,000 frames. For training data, we
follow [8] and randomly select 386,357 pairs and each pair
is composed of two images of the same scene but different
viewpoints. We randomly select 25,600 pairs for evaluation.
Parameter Settings. For a fair comparison, we adopt the
same training setup as in [12], [19]. All images are scaled to
256×256, and we enabled image ﬂipping and random crops
for data augmentation. Similar to [19], the experiments for
Dayton are trained for 35 epochs with batch size of 4. For
CVUSA, we follow the same setup as in [19], [40], and train
our network for 30 epochs with batch size of 4. For Ego2Top,
all models are trained with 10 epochs using batch size 8. For
SVA, all models are trained with 20 epoch using batch size 4.
Evaluation Metrics. Similar to [8], [19], we employ Incep-
tion Score [43], top-k prediction accuracy, KL score and
Fr´echet Inception Distance (FID) [44] for the quantitative
analysis. These metrics evaluate the generated images from
feature space. We also employ pixel-level
a high-level
similarity metrics to evaluate our method, i.e., Structural-
Similarity (SSIM) [45], Peak Signal-to-Noise Ratio (PSNR)
and Sharpness Difference (SD).
Baseline Models. We ﬁrst conduct an ablation study on
Dayton to evaluate the components of the proposed Se-

Fig. 5: Results of cross-view image translation generated
by the proposed SelectionGAN on different datasets. From
left to right: input image, segmentation map, ground truth,
uncertainty map, coarse result and reﬁned result.

datasets as training data, respectively. For facial expression
generation, we follow [34] and use OpenFace [5] to extract
facial landmarks on Radboud Faces dataset as training data.
For both hand gesture generation and human pose genera-
tion tasks, we follow [1], [2] and employ OpenPose [35] as
pose joints detector and ﬁlter out images where no human
hand and body are detected in the associated datasets.

We follow the optimization method in [9] to optimize
the proposed SelectionGAN, i.e., one gradient descent step
on discriminator and generators alternately. We ﬁrst train
Gi, Gs, Gm, Ga with D ﬁxed, and then train D with Gi,
Gs, Gm, Ga ﬁxed. The proposed SelectionGAN is trained
and optimized in an end-to-end fashion. We employ Adam
[36] with momentum terms β1=0.5 and β2=0.999 as our
solver. In our experiments, we set λtv=1e−6, λ1=100, λ2=1,
λ3=200 and λ4=2 in Eq. (10), and λ=4 in Eq. (9). The
number of attention channels N in Eq. (4) is set to 10. The
proposed SelectionGAN is implemented in PyTorch.

4 EXPERIMENTS

We conduct extensive experiments on a variety of guided
image-to-image translation tasks such as segmentation
map guided cross-view image translation, facial landmark
guided expression-to-expression translation, hand skeleton
guided gesture-to-gesture translation and pose skeleton
guided person image generation. Moreover, to explore the
generality of the proposed SelectionGAN on other gener-
ation tasks, we conduct experiments on the challenging
semantic image synthesis task.

4.1 Results on Cross-View Image Translation

Datasets. We follow [8], [19], [37] and perform experiments
on four public cross-view image translation datasets: (i)
The Dayton dataset [38], which contains 76,048 images
and the train/test split is 55,000/21,048. The images in
the original dataset have 354×354 resolution. We resize
them to 256×256. (ii) The CVUSA dataset [39] consists of

PREPRINT - WORK IN PROGRESS

8

Fig. 6: Results of cross-view image translation on SVA. From left to right: input image, ground truth, SelectionGAN++
(Ours), SelectionGAN (Our), X-Seq++, X-Fork++, Pix2pix++, H-Regions, H-Seq, H-Fork, H-SO, H-Pix2pix, X-Seq, X-Fork,
X-SO, X-Pix2pix and Zhai et al.
TABLE 4: Quantitative results of cross-view image translation on SVA. For all metrics except KL and FID, higher is better.
(∗) Inception Score for real (ground truth) data is 3.1282, 2.4932 and 3.4646 for all, top-1 and top-5 setups, respectively.

Method

Publish

X-Pix2pix [12]
X-SO [37]
X-Fork [19]
X-Seq [19]
H-Pix2pix [37]
H-SO [37]
H-Fork [37]
H-Seq [37]
H-Regions [37]
Pix2pix++ [12]
X-Fork++ [19]
X-Seq++ [19]
SelectionGAN
SelectionGAN++

CVPR 2017
CVIU 2019
CVPR 2018
CVPR 2018
CVIU 2019
CVIU 2019
CVIU 2019
CVIU 2019
CVIU 2019
CVPR 2017
CVPR 2018
CVPR 2018
Ours
Ours

Accuracy (%)

Inception Score∗

Top-1

Top-5

8.5961
7.5146
17.3794
19.5056
18.0706
5.2444
18.0182
20.7391
15.4803
8.8687
10.2658
11.2580
33.9055
35.9008

30.3288
30.9507
53.4725
57.1010
54.8068
26.4697
51.0756
57.5378
48.0767
34.5434
37.8405
36.8018
71.8779
73.3249

9.0260
10.3905
23.8315
25.8807
23.4400
5.2544
26.6747
28.5517
21.8225
9.2713
11.4138
11.9838
50.8878
52.5346

29.9102
38.9822
63.5045
65.3005
62.3072
31.9527
62.8166
67.4649
56.8994
35.7490
38.7976
36.9231
85.0019
86.9432

All
2.0131
2.4951
2.1888
2.2232
2.1906
2.3202
2.3202
2.2394
2.6328
2.5625
2.4280
2.6849
2.6576
2.7370

Top-1
1.7221
1.8940
1.9776
1.9842
1.9507
1.9410
1.9525
1.9892
2.0732
2.0879
2.0387
2.1325
2.1279
2.1914

Top-5
2.2370
2.6634
2.3664
2.4344
2.4069
2.7340
2.3918
2.4385
2.8347
2.7961
2.7630
2.9397
2.9267
3.0271

SSIM PSNR

SD

KL

FID

0.3206
0.4552
0.4235
0.4638
0.4327
0.4457
0.4240
0.4249
0.4044
0.3664
0.3406
0.3617
0.5752
0.5481

17.9944
21.5312
21.2400
22.3411
21.6860
21.7709
21.6327
21.4770
20.9848
17.6549
17.3937
17.4893
24.7136
24.2886

17.0254
17.5285
16.9371
17.4138
16.9468
17.3876
16.8653
17.5616
17.6858
18.4015
18.2153
18.4122
19.7302
19.2001

19.5533
12.0906
4.1925
3.7585
4.2894
12.8761
4.7246
4.4260
6.0638
13.1153
10.1403
11.8560
2.6183
2.5788

859.66
443.79
129.16
118.70
117.13
1452.88
109.43
95.12
88.78
220.23
166.33
154.80
26.09
37.17

TABLE 5: Quantitative results of cross-view image translation on CVUSA. For all metrics except KL, higher is better. (∗)
Inception Score for real (ground truth) data is 4.8741, 3.2959 and 4.9943 for all, top-1 and top-5 setups, respectively.

Method

Publish

SSIM

PSNR

SD

KL

Accuracy (%)

Inception Score∗

Top-5

Top-1

Zhai et al. [40]
Pix2pix [12]
X-SO [37]
X-Fork [19]
X-Seq [19]
Pix2pix++ [12]
X-Fork++ [19]
X-Seq++ [19]
SelectionGAN [8]

All
1.8434
3.2771
1.7575
3.4432
3.8151
3.2592
3.3758
3.3919
3.8074
TABLE 6: Quantitative evaluation of cross-view image translation on Dayton in a2g direction. For all metrics except KL,
higher is better. (∗) Inception Score for real (ground truth) data is 3.8319, 2.5753 and 3.9222 for all, top-1 and top-5 setups,
respectively.

27.43 ± 1.63
59.81 ± 2.12
414.25 ± 2.37
11.71 ± 1.55
15.52 ± 1.73
9.47 ± 1.69
7.18 ± 1.56
5.19 ± 1.31
2.96 ± 0.97

CVPR 2017
CVPR 2017
CVIU 2019
CVPR 2018
CVPR 2018
CVPR 2017
CVPR 2018
CVPR 2018
Ours

16.6184
18.5239
16.9919
18.6706
18.4378
18.9044
18.9856
18.9907
19.6100

17.4886
17.6578
17.6201
19.0509
18.8067
21.5739
21.6504
21.6733
23.1466

Top-5
1.8666
3.4312
1.7791
3.5567
4.0077
3.5078
3.5711
3.4858
3.9197

Top-1
1.5171
2.2219
1.4145
2.5447
2.6738
2.4175
2.5375
2.5474
2.7181

0.4147
0.3923
0.3451
0.4356
0.4231
0.4617
0.4769
0.4740
0.5323

14.03
9.25
0.21
31.24
24.14
41.87
49.65
54.61
65.51

42.09
25.81
6.14
50.51
42.91
57.26
64.47
67.12
74.32

13.97
7.33
0.29
20.58
15.98
26.45
31.03
34.69
41.52

52.29
32.67
9.08
63.66
54.41
72.87
81.16
83.46
89.66

Method

Publish

Pix2pix [12]
X-SO [37]
X-Fork [19]
X-Seq [19]
Pix2pix++ [12]
X-Fork++ [19]
X-Seq++ [19]
SelectionGAN [8]

CVPR 2017
CVIU 2019
CVPR 2018
CVPR 2018
CVPR 2017
CVPR 2018
CVPR 2018
Ours

Accuracy (%)

Inception Score∗

Top-1

Top-5

6.80
27.56
30.00
30.16
32.06
34.67
31.58
42.11

9.15
41.15
48.68
49.85
54.70
59.14
51.67
68.12

23.55
57.96
61.57
62.59
63.19
66.37
65.21
77.74

27.00
73.20
78.84
80.70
81.01
84.70
82.48
92.89

All
2.8515
2.9459
3.0720
2.7384
3.1709
3.0737
3.1703
3.0613

Top-1
1.9342
2.0963
2.2402
2.1304
2.1200
2.1508
2.2185
2.2707

Top-5
2.9083
2.9980
3.0932
2.7674
3.2001
3.0893
3.2444
3.1336

SSIM PSNR

SD

KL

0.4180
0.4772
0.4963
0.5031
0.4871
0.4982
0.4912
0.5938

17.6291
19.6203
19.8928
20.2803
21.6675
21.7260
21.7659
23.8874

19.2821
19.2939
19.4533
19.5258
18.8504
18.9402
18.9265
20.0174

38.26 ± 1.88
7.20 ± 1.37
6.00 ± 1.28
5.93 ± 1.32
5.49 ± 1.25
4.59 ± 1.16
4.94 ± 1.18
2.74 ± 0.86

lectionGAN. To reduce the training time, we randomly
select 1/3 samples from the whole 55,000/21,048 samples,
i.e., around 18,334 samples for training and 7,017 samples
for testing. The proposed SelectionGAN considers eight
baselines (A, B, C, D, E, F, G, H) as shown in Table 1.
(cid:48)
Baseline A uses a Pix2pix structure [12] and generates I
g
using a single image Ia. Baseline B uses the same Pix2pix
model and generates I
g using the corresponding semantic
guidance Sg. Baseline C also uses the Pix2pix structure, and
inputs the combination of a conditional image Ia and the
semantic guidance Sg to the generator Gi. Baseline D uses
the proposed cycled semantic guidance generation upon
Baseline C. Baseline E represents the pixel loss guided by

(cid:48)

the learned uncertainty maps. Baseline F employs the pro-
posed multi-channel attention selection module to generate
multiple intermediate generations, and to make the neural
network attentively select which part is more important
for generating the target image. Baseline G adds the total
variation regularization on the ﬁnal result I
g . Baseline H
employs the proposed multi-scale spatial pooling module
to reﬁne the features Fc from stage I. All the baseline
models are trained and tested on the same data using the
conﬁguration.
Ablation Analysis. The results of the ablation study are
shown in Table 1. We observe that Baseline B is better than
baseline A since Sg contains more structural information

(cid:48)(cid:48)

PREPRINT - WORK IN PROGRESS
9
TABLE 7: Quantitative results of cross-view image translation on Ego2Top. For all metrics except KL, higher is better. (∗)
Inception Score for real (ground truth) data is 6.4523, 2.8507 and 5.4662 for all, top-1 and top-5 setups, respectively.

Method

Publish

SSIM PSNR

SD

Inception Score∗

Accuracy (%)

All

Top-1

Top-5

Top-1

Top-5

CVPR 2017
Pix2pix [12]
CVPR 2018
X-Fork [19]
X-Seq [19]
CVPR 2018
Pix2pix++ [12] CVPR 2017
CVPR 2018
X-Fork++ [19]
CVPR 2018
X-Seq++ [19]
Ours
SelectionGAN

0.2213
0.2740
0.2738
0.3779
0.3560
0.3878
0.6024

15.7197
16.3709
16.3788
21.1346
20.5788
21.2327
26.6565

16.5949
17.3509
17.2624
17.8056
17.6183
17.9469
19.7755

2.5418
4.6447
4.5094
5.0833
5.2266
4.9890
5.6200

1.6797
2.1386
2.0276
2.4096
2.4100
2.3519
2.5328

2.4947
3.8417
3.6756
4.4595
4.5591
4.2881
4.7648

1.22
5.91
4.78
19.53
13.92
19.41
28.31

1.57
10.22
8.96
33.19
22.38
36.11
54.56

5.33
20.98
17.04
40.89
34.20
40.46
62.97

6.86
30.29
24.40
48.34
42.42
50.41
76.30

KL Score

120.46 ± 1.94
22.12 ± 1.65
25.19 ± 1.73
10.93 ± 1.87
17.34 ± 1.98
9.33 ± 1.64
3.05 ± 0.91

Fig. 7: Results of cross-view image translation on CVUSA.
From left to right: input image, ground truth, SelectionGAN
(Our), X-Seq++, X-Fork++, Pix2pix++, X-Seq, X-Fork, X-SO,
Pix2pix and Zhai et al.

Fig. 8: Results of cross-view image translation on Dayton.
From left to right: input image, ground truth, SelectionGAN
(Our), X-Seq++, X-Fork++, Pix2pix++, X-Seq, X-Fork, X-SO
and Pix2pix.

than Ia. By comparison Baseline A with Baseline C, the
semantic-guided generation improves SSIM, PSNR and SD
by 8.19, 3.1771 and 0.3205, respectively, which conﬁrms the
importance of the conditional semantic guidance informa-
tion. By using the proposed cycled semantic guidance gener-
ation, Baseline D further improves over C, meaning that the
proposed semantic guidance cycle structure indeed utilizes
the semantic guidance information in a more effective way,
conﬁrming our design motivation. Baseline E outperforms
D showing the importance of using the uncertainty maps
to guide the pixel loss map which contains an inaccurate
reconstruction loss due to the wrong semantic guidance
produced from the pre-trained models. Baseline F signiﬁ-
cantly outperforms E with around 4.67 points gain on the
SSIM metric, clearly demonstrating the effectiveness of the
proposed multi-channel attention selection scheme. We can
also observe from Table 1 that, by adding the proposed
multi-scale spatial pool scheme and the TV regulariza-
tion, the overall performance is further boosted. Finally,
we demonstrate the advantage of the proposed two-stage
strategy over the one-stage method. Several examples are
shown in Fig. 5, 13 and Table 2. It is obvious that the coarse-
to-ﬁne generation model is able to generate sharper results
and contains more details than the one-stage model, which
further conﬁrms our motivations.

Fig. 9: Results of cross-view image translation on Ego2Top.
From left to right: input image, segmentation map, ground
truth, Pix2pix++, X-Fork++, X-Seq++, SelectionGAN (Ours)
and uncertainty maps generated by SelectionGAN.
TABLE 8: Per-class accuracy and mean IOU for the gener-
ated segmentation maps on Dayton. For both metric, higher
is better.

Method

Publish

Per-Class Acc. mIOU

X-Fork [19]
X-Seq [19]
SelectionGAN

CVPR 2018
CVPR 2018
Ours

0.6262
0.4783
0.6415

0.4163
0.3187
0.5455

Inﬂuence of the Number of Attention Channels. We in-
vestigate the inﬂuence of the number of attention channel
N in Eq. (4). Results are shown in Table 3. We observe that
the performance tends to be stable after N =10. Thus, taking
both performance and training speed into consideration, we
have set N =10 in all our experiments.
SelectionGAN vs. SelectionGAN++. We also provide com-
parison results of SelectionGAN and SelectionGAN++ on
both SVA and Radboud Faces datasets. SelectionGAN is
proposed in our conference paper [8] and SelectionGAN++
is proposed in this paper. Results of cross-view image
translation are shown in Table 4 and Fig. 6. Results of facial
expression generation are shown in Table 9 and Fig. 11. We
can see that SelectionGAN++ achieves better results in both
ﬁgures and both tables (on most metrics), meaning that the
proposed multi-scale pooling & channel selection module
indeed enhances the feature representation, conﬁrming our
design motivation.
State-of-the-art Comparison. We compare our Selection-
GAN with several recently proposed state-of-the-art meth-
ods, which are Pix2pix [12], Zhai et al. [40], X-Fork [19], X-
Seq [19] and X-SO [37]. Moreover, to study the effectiveness
of SelectionGAN, we introduce three strong baselines which
use both segmentation maps and RGB images as inputs,
including Pix2pix++ [12], X-Fork++ [19], and X-Seq++ [19].
We implement Pix2pix++, X-Fork++ and X-Seq++ using
their public source code. The comparison results are shown
in Table 4, 5, 6 and 7. We can observe that SelectionGAN

PREPRINT - WORK IN PROGRESS

10

Fig. 10: Results of controllable cross-view image translation for both indoor and outdoor scenes. From left to right: input
image, segmentation map, ground truth, uncertainty maps generated by SelectionGAN and SelectionGAN (Ours).

TABLE 9: Quantitative results of facial expression genera-
tion on Radboud Faces. For all metrics except LPIPS, higher
is better.

Model

Publish

AMT ↑

SSIM ↑

PSNR ↑

LPIPS ↓

StarGAN [14]
Pix2pix [12]
GPGAN [46]
PG2 [1]
C2GAN [34]
SelectionGAN
SelectionGAN++

CVPR 2018
CVPR 2017
ICPR 2018
NeurIPS 2017
ACM MM 2019
Ours
Ours

24.7
13.4
0.3
28.4
34.2
37.5
39.1

0.8345
0.8217
0.8185
0.8462
0.8618
0.8760
0.8761

19.6451
19.9971
18.7211
20.1462
21.9192
27.5671
27.5158

N/A
0.1334
0.2531
0.1130
0.0934
0.0917
0.0905

consistently outperforms exiting methods on most metrics.
Qualitative Evaluation. Qualitative results are shown in
Fig. 6, 7, 8 and 9. It can be seen that our method generates
more clear details on objects/scenes such as road, tress,
clouds, car than the other comparison methods in the gen-
erated ground level images. For the generated aerial images
in Fig. 8, we can observe that grass, trees and house roofs
are well rendered compared to others. Moreover, the results
generated by our method are closer to the ground truth in
layout and structure.
Visualization of Learned Uncertainty Maps. In Fig. 5, 9
and 10, we show some samples of the generated uncertainty
maps. We can see that the generated uncertainty maps learn
the layout and structure of the target images. Note that most
textured regions are similar in our generation images, while
the junction/edge of different regions is uncertain, and thus
the model learns to highlight these parts.
Generated Semantic Guidances. Since the proposed Se-
lectionGAN can reconstruct the semantic guidance (here,
the segmentation maps), we also compare the generated
semantic guidance with X-Fork [19] and X-Seq [19] on
Dayton. Following [19], we compute the per-class accuracy
and mean IOU for the most common classes in this dataset
(see Table 8). We see that our SelectionGAN achieves better
results than X-Fork [19] and X-Seq [19] on both metrics.
Controllable Cross-View Image Translation. We further
adopt Ego2Top to conduct the controllable cross-view image
translation experiments. The quantitative and qualitative
results are shown in Table 7 and Fig. 10, respectively. As
shown in Fig. 10, given a single input image and some novel
segmentation maps, SelectionGAN is able to generate the
same scene but with different viewpoints in both indoor
and outdoor environments. Moreover, we observe that the
proposed SelectionGAN achieves signiﬁcantly better results

Fig. 11: Results of facial expression generation on Radboud
Faces. From left to right: input image, facial landmark,
ground truth, StarGAN, Pix2pix, GPGAN, PG2, C2GAN, Se-
lectionGAN (ours), SelectionGAN++ (ours) and uncertainty
map generated by SelectionGAN.

than existing methods in Table 7 and Fig. 9 on this challeng-
ing task.

4.2 Results on Facial Expression Generation

Datasets. We follow C2GAN [34] and conduct facial ex-
pression generation experiments on the Radboud Faces
dataset [49]. This dataset contains over 8,000 face im-
ages with eight different emotional expressions. We follow
C2GAN and all the images are resized to 256×256 without
any pre-processing. Then, we adopt OpenFace [5] to extract
facial landmarks as the ground truths. Consequently, we
collect 5,628 training image pairs and 1,407 testing pairs.
Parameter Settings. Following C2GAN [34], the experi-
ments on Radboud Faces are trained for 200 epochs with
batch size of 4.
Evaluation Metrics. Following C2GAN [34], we ﬁrst em-
ploy Structural Similarity (SSIM) [45] and Peak Signal-to-
Noise Ratio (PSNR) to evaluate the quantitative quality
of generated images by different methods. Moreover, we
adopt Amazon Mechanical Turk (AMT) perceptual studies
to evaluate the quality of the generated images. Speciﬁcally,
participants were shown a sequence of pairs of images, one
a real image and one fake image, and asked to click on the

PREPRINT - WORK IN PROGRESS

11

Fig. 12: Results of hand gesture-to-gesture translation on NTU Hand Digit (top) and the Senz3D (bottom) datasets. From
left to right: input image, hand skeleton, ground truth, PG2, SAMG, DPIG, PoseGAN, GestureGAN, SelectionGAN (ours)
and uncertainty map generated by SelectionGAN.
TABLE 10: Quantitative results of hand gesture-to-gesture translation on NTU Hand Digit and Senz3D datasets. For all
metrics except FID and FRD, higher is better.

Method

Publish

NTU Hand Digit

Senz3D

PSNR ↑

IS ↑

AMT ↑

FID ↓

FRD ↓

PSNR ↑

IS ↑

AMT ↑

FID ↓

FRD ↓

PG2 [1]
SAMG [47]
DPIG [48]
PoseGAN [27]
GestureGAN [2] ACM MM 2018
SelectionGAN

NeurIPS 2017
ACM MM 2017
CVPR 2018
CVPR 2018

Ours

28.2403
28.0185
30.6487
29.5471
32.6091
30.6465

2.4152
2.4919
2.4547
2.4017
2.5532
2.4472

3.5
2.6
7.1
9.3
26.1
15.8

24.2093
31.2841
6.7661
9.6725
7.5860
16.2159

2.6319
2.7453
2.6184
2.5846
2.5223
2.1560

26.5138
26.9545
26.9451
27.3014
27.9749
30.4036

3.3699
3.3285
3.3874
3.2147
3.4107
2.4595

2.8
2.3
6.9
8.6
22.6
14.1

31.7333
38.1758
26.2713
24.6712
18.4595
30.9775

3.0933
3.1006
3.0846
3.0467
2.9836
2.7014

the proposed SelectionGAN achieves the best results on all
metrics.
Qualitative Evaluation. Qualitative results are shown in
Fig. 11. Clearly, the image generated by our SelectionGAN
are more sharper and contains more image details compared
with other leading methods.
Visualization of Learned Uncertainty Maps. We also show
the learned uncertainty maps in Fig. 11. We observe that the
proposed SelectionGAN can generate different uncertainty
maps according to different facial expressions, which means
the proposed model can learn the difference between differ-
ent expression domains.

4.3 Results on Hand Gesture Translation

Datasets. We follow GestureGAN [2] and conduct exper-
iments on both NTU Hand Digit [54] and Senz3D [55]
datasets. NTU Hand Digit dataset contains 75,036 and 9,600
image pairs for training and testing sets, each of which is
comprised of two images of the same person but different
gestures. For Senz3D, which contains 135,504 pairs and
12,800 pairs for training and testing.
Parameter Settings. Images on both datasets are resized
to 256×256, and we enabled image ﬂipping and random
crops for data augmentation. Following GestureGAN [2],
the experiments on both datasets are trained for 20 epochs
with batch size of 4.
Evaluation Metrics. Following [2], we employ Peak Signal-
to-Noise Ratio (PSNR), Inception score (IS) [43], Fr´echet
Inception Distance (FID) [44] and Fr´echet ResNet Distance
(FRD) [2] to evaluate the generated images. Moreover, we
follow the same settings as in [2], [12] to conduct the
Amazon Mechanical Turk (AMT) perceptual studies.

Fig. 13: Results of controllable hand gesture-to-gesture
translation. From left to right: input image, hand skeleton,
ground truth, uncertainty map , coarse result and reﬁned
result.

image they thought was real. Finally, we also use a neural
network based metric LPIPS [50] to evaluate the proposed
method.

State-of-the-Art Comparison. We compare the proposed
SelectionGAN with several state-of-the-art methods, i.e.,
StarGAN [14], Pix2pix [12], GPGAN [51], PG2 [1] and
C2GAN [34]. Quantitative results of the SSIM, PSNR, LPIPS
and AMT metrics are show in Table 9. We can see that

PREPRINT - WORK IN PROGRESS

12

TABLE 11: Quantitative results of person image generation on Market-1501 and DeepFashion. For all metrics, higher is
better. (∗) denotes the results tested on our test set.

Method

Publish

PG2 [1]
DPIG [48]
PoseGAN [27]
C2GAN [34]
BTF [4]
PG2∗ [1]
PoseGAN∗ [27]
VUnet∗ [52]
Pose-Transfer∗ [53]
SelectionGAN
Real Data

NeurIPS 2017
CVPR 2018
CVPR 2018
ACM MM 2019
ICCV 2019
NeurIPS 2017
CVPR 2018
CVPR 2018
CVPR 2019
Ours
-

SSIM ↑
0.253
0.099
0.290
0.282
N/A
0.261
0.291
0.266
0.311
0.331
1.000

Market-1501

DeepFashion

IS ↑ Mask-SSIM ↑ Mask-IS ↑
3.460
3.483
3.185
3.349
N/A
3.495
3.230
2.965
3.323
3.449
3.890

3.435
3.491
3.502
3.510
N/A
3.367
3.502
3.549
3.773
3.376
3.706

0.792
0.614
0.805
0.811
N/A
0.782
0.807
0.793
0.811
0.816
1.000

SSIM ↑
0.762
0.614
0.756
N/A
0.767
0.773
0.760
0.763
0.773
0.776
1.000

IS ↑
3.090
3.228
3.439
N/A
3.220
3.163
3.362
3.440
3.209
3.341
4.053

TABLE 12: User study of person image generation (%). R2G
means the percentage of real images rated as generated w.r.t.
all real images. G2R means the percentage of generated
images rated as real w.r.t. all generated images. The results
of other methods are drawn from their papers.

Method

Publish

PG2 [1]
PoseGAN [27]
C2GAN [34]
Pose-Transfer [53]
SelectionGAN

NeurIPS 2017
CVPR 2018
ACM MM 2019
CVPR 2019
Ours

Market-1501 DeepFashion

R2G
11.2
22.67
23.20
32.23
34.64

G2R
R2G
G2R
14.9
9.2
5.5
50.24
24.61
12.42
46.70 N/A N/A
31.78
19.14
63.47
33.54
20.57
64.75

TABLE 13: Quantitative results of semantic image synthesis
on Cityscapes and ADE20K. For mIoU and Acc, higher is
better. For FID, lower is better.

Method

Publish

Cityscapes

ADE20K

mIoU ↑ Acc ↑

FID ↓ mIoU ↑ Acc ↑

FID ↓

ICCV 2017
CRN [56]
SIMS [57]
CVPR 2018
Pix2pixHD [13] CVPR 2018
CVPR 2019
GauGAN [58]
Ours
SelectionGAN

52.4
47.2
58.3
62.3
63.8

77.1
75.5
81.4
81.9
82.4

104.7
49.7
95.0
71.8
65.2

22.4
N/A
20.3
38.5
40.1

68.8
73.3
N/A N/A
81.8
69.2
33.9
79.9
33.1
81.2

TABLE 14: User preference study of semantic image synthe-
sis on Cityscapes and ADE20K. The numbers indicate the
percentage of users who favor the results of the proposed
method over the competing method. For this metric, higher
is better.

AMT ↑

Publish

Cityscapes ADE20K

Ours vs. CRN [56]
ICCV 2017
Ours vs. Pix2pixHD [13] CVPR 2018
CVPR 2018
Ours vs. SIMS [57]
CVPR 2019
Ours vs. GauGAN [58]

63.86
54.04
53.57
52.89

69.43
78.62
N/A
55.15

ours. We also provide results of user study in Table 10. Note
that the proposed SelectionGAN achieves the second best
results compared with other strong baselines.
Qualitative Evaluation. Qualitative results compared with
existing methods are shown in Fig. 12. We can see that
the proposed SelectionGAN achieves competitive results
compared with the leading approaches. Moreover, we show
the learned uncertainty maps in Fig. 12 and 13.
Controllable Hand Gesture Translation. In Fig. 13, we
provide results of controllable hand gesture translation. We
can see that the proposed SelectionGAN can translates a
single input image into several output images while each
one respecting the constraints speciﬁed in the provided
hand skeleton.

4.4 Results on Person Image Generation

Datasets. We follow Pose-Transfer [53] and conduct person
image generation experiments on both Market-1501 [59]

Fig. 14: Results of person image generation on Market-1501
(top) and DeepFashion (bottom). From left to right: input
image, pose skeleton, ground truth, PG2, VUNet, PoseGAN,
Pose-Transfer and SelectionGAN (Ours).

State-of-the-Art Comparison. We compare the proposed Se-
lectionGAN with the leading hand gesture translation meth-
ods, i.e., PG2 [1], SAMG [47], DPIG [48], PoseGAN [27] and
GestureGAN [2]. Comparison results are shown in Table 10.
We can see that our SelectionGAN achieves competitive
results on both datasets compared with existing methods
except GestureGAN. GestureGAN is a model carefully de-
signed for this task, thus it obtain slightly better results than

PREPRINT - WORK IN PROGRESS

13

Fig. 15: Results of semantic image synthesis on Cityscapes.
From left to right: input semantic label, ground truth, CRN,
SIMS, Pix2pixHD, GauGAN and SelectionGAN (ours).

Fig. 17: Generated segmentation maps on Cityscapes. From
left to right: ground truth, result generated by SelectionGAN
(ours), segmentation map generated on SelectionGAN’s re-
sult, result generated by GauGAN and segmentation map
generated on GauGAN’s result.

Fig. 16: Results of semantic image synthesis task on
ADE20K. From left to right: input semantic label, ground
truth, CRN, Pix2pixHD, GauGAN and SelectionGAN (ours).

and DeepFashion [60] datasets. Following [53], we collect
263,632 and 12,000 pairs for training and testing on Market-
1501. For DeepFashion, 101,966 and 8,570 pairs are ran-
domly selected for training and testing.
Parameter Settings. Following Pose-Transfer [53], images
are rescaled to 128×64 and 256×256 on Market-1501 and
DeepFashion datasets, respectively. Moreover, the experi-
ments on both datasets are trained for around 90k iteration
with batch size of 32 and 12 on Market-1501 and DeepFash-
ion, respectively.
Evaluation Metrics. Following previous works [1], [27], [27],
[34], we adopt Structure Similarity (SSIM) [45], Inception
score (IS) [43] and their corresponding masked versions, i.e.,
Mask-SSIM and Mask-IS, as our evaluation metrics. More-
over, we follow Pose-Transfer [53] and recruit 30 volunteers
to conduct a user study.
State-of-the-Art Comparison. We compare the proposed
SelectionGAN with several leading person image gener-
ation methods, i.e., PG2 [1], DPIG [48], PoseGAN [27],
VUnet [52], C2GAN [34], BTF [4] and Pose-Transfer [53].
Quantitative results of the SSIM, IS, Mask-SSIM and Mask-
IS metrics are show in Table 11. We can see that the proposed
SelectionGAN achieves competitive performance compared
with the carefully designed methods on this task such as
Pose-Transfer [53] and PoseGAN [27]. Moreover, we show
user study results in Table 12. We observe that our method
achieve better results over [1], [27], [34], [53], further vali-
dating that our generated images are more photo-realistic.
Qualitative Evaluation. Qualitative results are shown in
Fig. 14. The image generated by our SelectionGAN are more
realistic and sharp compared with other leading methods.
Moreover, the person layouts of generated images by our
method are closer to the target skeletons.

Fig. 18: Generated segmentation maps on ADE20K. From
left to right: ground truth, result generated by SelectionGAN
(ours), segmentation map generated on SelectionGAN’s re-
sult, result generated by GauGAN and segmentation map
generated on GauGAN’s result.

4.5 Results on Semantic Image Synthesis

To explore the generality of the proposed SelectionGAN on
other generation tasks, we also conduct experiments on the
challenging semantic image synthesis task.
Datasets. We follow GauGAN [58] and conduct semantic
image synthesis experiments on two challenging datasets,
i.e., Cityscapes [61] and ADE20K [7]. The training and
testing set sizes of Cityscapes are 2,975 and 500, respectively.
For ADE20K, which contains 150 semantic classes, and has
20,210 training and 2,000 validation images.
Parameter Settings. Images are rescaled to 512×256 and
256×256 on Cityscapes and ADE20K datasets, respectively.
Following GauGAN [58], the experiments on both datasets
are trained for 200 epochs with batch size of 32.
Evaluation Metrics. Following [58], we employ the mean
Intersection-over-Union (mIoU) and pixel accuracy (Acc) to
measure the segmentation accuracy. Speciﬁcally, we adopt
the state-of-the-art segmentation networks to evaluate the
generated images, i.e., DRN-D-105 [62] for Cityscapes and
UperNet101 [63] for ADE20K. We also employ the Fr´echet
Inception Distance (FID) [44] to measure the distance be-
tween the distribution of generated samples and the dis-
tribution of real samples. Finally, we follow GauGAN and
employ Amazon Mechanical Turk (AMT) to measure the
perceived visual ﬁdelity of the generated images.
State-of-the-Art Comparisons. We adopt several leading

PREPRINT - WORK IN PROGRESS

14

semantic image synthesis methods as our baselines, i.e,
Pix2pixHD [13], CRN [56], SIMS [57] and GauGAN [58].
Results of mIoU, Acc and FID are show in Table 13. We
note that the proposed SelectionGAN achieves better results
than the existing competing methods on both mIoU and
Acc metrics. For FID, the proposed SelectionGAN is only
worse than SIMS on Cityscapes. However, SIMS has poor
segmentation results. Moreover, we follow GauGAN and
provide AMT results in Table 14. We see that users favor our
translated images on both datasets compared with existing
leading methods.
Qualitative Evaluation. Qualitative results compared with
exiting methods are shown in Fig. 15 and 16. We observe
that the proposed SelectionGAN produces much better re-
sults with fewer visual artifacts than exiting methods.
Visualization of Generated Segmentation Maps. We follow
GauGAN and apply pre-trained segmentation networks
on the generated images to produce segmentation maps.
The intuition behind this is that if the generated images
are realistic, a well-trained semantic segmentation model
should be able to predict the ground truth label. Results
compared with the state-of-the-art methods, i.e., GauGAN,
are shown in Fig. 17 and 18. We observe that the proposed
SelectionGAN generates better semantic maps than Gau-
GAN on both datasets.

5 CONCLUSION
We propose the Multi-Channel Attention Selection GAN
(SelectionGAN) to address a novel image synthesizing task
by conditioning on a input image and several conditional
semantic guidances. In particular, we adopt a cascade strat-
egy to divide the generation procedure into two stages.
Stage I aims to capture the semantic structure of the tar-
get image and Stage II focus on more appearance details
via the proposed multi-scale spatial pooling & channel
selection and the multi-channel attention selection mod-
ules. We also propose an uncertainty map guided pixel
loss to solve the inaccurate semantic guidance issue for
better optimization. Extensive experimental results on four
guided image-to-image translation and one semantic image
synthesis tasks with 11 public datasets demonstrate that our
method obtains much better results than the state-of-the-art
approaches.

REFERENCES

[1] L. Ma, X. Jia, Q. Sun, B. Schiele, T. Tuytelaars, and L. Van Gool,
“Pose guided person image generation,” in NeurIPS, 2017. 1, 3, 4,
7, 10, 11, 12, 13

[2] H. Tang, W. Wang, D. Xu, Y. Yan, and N. Sebe, “Gesturegan for
hand gesture-to-gesture translation in the wild,” in ACM MM,
2018. 1, 3, 7, 11

[3] T.-C. Wang, M.-Y. Liu, A. Tao, G. Liu, J. Kautz, and B. Catanzaro,
“Few-shot video-to-video synthesis,” in NeurIPS, 2019. 1, 3
[4] B. AlBahar and J.-B. Huang, “Guided image-to-image translation
with bi-directional feature transformation,” in ICCV, 2019. 1, 3, 4,
12, 13

[5] B. Amos, B. Ludwiczuk, M. Satyanarayanan et al., “Openface:
A general-purpose face recognition library with mobile applica-
tions,” CMU School of Computer Science, vol. 6, 2016. 1, 7, 10
[6] G. Lin, A. Milan, C. Shen, and I. D. Reid, “Reﬁnenet: Multi-path
reﬁnement networks for high-resolution semantic segmentation.”
in CVPR, 2017. 1, 6

[7] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba,

“Scene parsing through ade20k dataset,” in CVPR, 2017. 1, 6, 13

[8] H. Tang, D. Xu, N. Sebe, Y. Wang, J. J. Corso, and Y. Yan, “Multi-
channel attention selection gan with cascaded semantic guidance
for cross-view image translation,” in CVPR, 2019. 2, 3, 7, 8, 9
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial
nets,” in NeurIPS, 2014. 2, 7

[9]

[10] T. Karras, S. Laine, and T. Aila, “A style-based generator architec-
ture for generative adversarial networks,” in CVPR, 2019. 2
[11] M. Mirza and S. Osindero, “Conditional generative adversarial

nets,” arXiv preprint:1411.1784, 2014. 3

[12] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image
translation with conditional adversarial networks,” in CVPR, 2017.
3, 6, 7, 8, 9, 10, 11

[13] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and B. Catan-
zaro, “High-resolution image synthesis and semantic manipula-
tion with conditional gans,” in CVPR, 2018. 3, 12, 13

[14] Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo, “Stargan:
Uniﬁed generative adversarial networks for multi-domain image-
to-image translation,” in CVPR, 2018. 3, 10, 11

[15] H. Tang, D. Xu, W. Wang, Y. Yan, and N. Sebe, “Dual generator
generative adversarial networks for multi-domain image-to-image
translation,” in ACCV, 2018. 3

[16] B. Li, X. Qi, T. Lukasiewicz, and P. Torr, “Controllable text-to-

image generation,” in NeurIPS, 2019. 3

[17] X. Yu, Y. Chen, S. Liu, T. Li, and G. Li, “Multi-mapping image-to-
image translation via learning disentanglement,” in NeurIPS, 2019.
3

[18] H. Dong, X. Liang, K. Gong, H. Lai, J. Zhu, and J. Yin, “Soft-
gated warping-gan for pose-guided person image synthesis,” in
NeurIPS, 2018. 3

[19] K. Regmi and A. Borji, “Cross-view image synthesis using condi-

tional gans,” in CVPR, 2018. 3, 6, 7, 8, 9, 10

[20] H. Tang, D. Xu, Y. Yan, P. H. Torr, and N. Sebe, “Local
class-speciﬁc and global image-level generative adversarial net-
works for semantic-guided scene generation,” arXiv preprint
arXiv:1912.12215, 2019. 3

[21] M. Wang, G.-Y. Yang, R. Li, R.-Z. Liang, S.-H. Zhang, P. Hall, S.-M.
Hu et al., “Example-guided style consistent image synthesis from
semantic labeling,” in CVPR, 2019. 3

[22] D. Xu, W. Wang, H. Tang, H. Liu, N. Sebe, and E. Ricci, “Structured
attention guided convolutional neural ﬁelds for monocular depth
estimation,” in CVPR, 2018. 3

[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
in NeurIPS, 2017. 3

[24] H. Tang, H. Liu, D. Xu, P. H. Torr, and N. Sebe, “Attention-
gan: Unpaired image-to-image translation using attention-guided
generative adversarial networks,” arXiv preprint arXiv:1911.11897,
2019. 3

[25] J. Kim, M. Kim, H. Kang, and K. Lee, “U-gat-it: unsupervised
generative attentional networks with adaptive layer-instance nor-
malization for image-to-image translation,” in ICLR, 2020. 3
[26] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, “Self-
attention generative adversarial networks,” in ICML, 2019. 3
[27] A. Siarohin, E. Sangineto, S. Lathuili`ere, and N. Sebe, “Deformable
gans for pose-based human image generation,” in CVPR, 2018. 4,
11, 12, 13

[28] D. Chen, S. Ren, Y. Wei, X. Cao, and J. Sun, “Joint cascade face

detection and alignment,” in ECCV, 2014. 4

[29] J. Dai, K. He, and J. Sun, “Instance-aware semantic segmentation

via multi-task network cascades,” in CVPR, 2016. 4

[30] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing

network,” in CVPR, 2017. 4

[31] A. Kendall, Y. Gal, and R. Cipolla, “Multi-task learning using
uncertainty to weigh losses for scene geometry and semantics,”
in CVPR, 2018. 5

[32] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real-time

style transfer and super-resolution,” in ECCV, 2016. 6

[33] H. Tang, D. Xu, H. Liu, and N. Sebe, “Asymmetric generative ad-
versarial networks for image-to-image translation,” arXiv preprint
arXiv:1912.06931, 2019. 6

[34] H. Tang, D. Xu, G. Liu, W. Wang, N. Sebe, and Y. Yan, “Cycle in
cycle generative adversarial networks for keypoint-guided image
generation,” in ACM MM, 2019. 7, 10, 11, 12, 13

[35] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, “Realtime multi-person
2d pose estimation using part afﬁnity ﬁelds,” in CVPR, 2017. 7

PREPRINT - WORK IN PROGRESS

15

[36] D. Kingma and J. Ba, “Adam: A method for stochastic optimiza-

tion,” in ICLR, 2015. 7

[37] K. Regmi and A. Borji, “Cross-view image synthesis using
geometry-guided conditional gans,” Elsevier CVIU, vol. 187, p.
102788, 2019. 7, 8, 9

[38] N. N. Vo and J. Hays, “Localizing and orienting street views using

overhead imagery,” in ECCV, 2016. 7

[39] S. Workman, R. Souvenir, and N. Jacobs, “Wide-area image geolo-

calization with aerial reference imagery,” in ICCV, 2015. 7

[40] M. Zhai, Z. Bessinger, S. Workman, and N. Jacobs, “Predicting
ground-level scene layout from aerial imagery,” in CVPR, 2017. 7,
8, 9

[41] A. Palazzi, G. Borghi, D. Abati, S. Calderara, and R. Cucchiara,
“Learning to map vehicles into birds eye view,” in ICIAP, 2017. 7
[42] S. Ardeshir and A. Borji, “Ego2top: Matching viewers in egocentric

and top-view videos,” in ECCV, 2016. 7

[43] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford,
and X. Chen, “Improved techniques for training gans,” in NeurIPS,
2016. 7, 11, 13

[44] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochre-
iter, “Gans trained by a two time-scale update rule converge to a
local nash equilibrium,” in NeurIPS, 2017. 7, 11, 13

[45] Z. Wang, A. C. Bovik, H. R. Sheikh, E. P. Simoncelli et al., “Image
quality assessment: from error visibility to structural similarity,”
IEEE TIP, vol. 13, no. 4, pp. 600–612, 2004. 7, 10, 13

[46] X. Di, V. A. Sindagi, and V. M. Patel, “Gp-gan: Gender preserving
gan for synthesizing faces from landmarks,” in ICPR, 2018. 10
[47] Y. Yan, J. Xu, B. Ni, W. Zhang, and X. Yang, “Skeleton-aided

articulated motion generation,” in ACM MM, 2017. 11

[48] L. Ma, Q. Sun, S. Georgoulis, L. Van Gool, B. Schiele, and M. Fritz,
“Disentangled person image generation,” in CVPR, 2018. 11, 12,
13

[49] O. Langner, R. Dotsch, G. Bijlstra, D. H. Wigboldus, S. T. Hawk,
and A. Van Knippenberg, “Presentation and validation of the
radboud faces database,” Taylor & Francis Cognition and emotion,
vol. 24, no. 8, pp. 1377–1388, 2010. 10

[50] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang,
“The unreasonable effectiveness of deep features as a perceptual
metric,” in CVPR, 2018. 11

[51] X. Di, V. A. Sindagi, and V. M. Patel, “Gp-gan: Gender preserving
gan for synthesizing faces from landmarks,” in ICPR, 2018. 11
[52] P. Esser, E. Sutter, and B. Ommer, “A variational u-net for con-
ditional appearance and shape generation,” in CVPR, 2018. 12,
13

[53] Z. Zhu, T. Huang, B. Shi, M. Yu, B. Wang, and X. Bai, “Progressive
pose attention transfer for person image generation,” in CVPR,
2019. 12, 13

[54] Z. Ren, J. Yuan, J. Meng, and Z. Zhang, “Robust part-based hand
gesture recognition using kinect sensor,” IEEE TMM, vol. 15, no. 5,
pp. 1110–1120, 2013. 11

[55] A. Memo and P. Zanuttigh, “Head-mounted gesture controlled
interface for human-computer interaction,” Springer MTA, vol. 77,
no. 1, pp. 27–53, 2018. 11

[56] Q. Chen and V. Koltun, “Photographic image synthesis with

cascaded reﬁnement networks,” in ICCV, 2017. 12, 13

[57] X. Qi, Q. Chen, J. Jia, and V. Koltun, “Semi-parametric image

synthesis,” in CVPR, 2018. 12, 13

[58] T. Park, M.-Y. Liu, T.-C. Wang, and J.-Y. Zhu, “Semantic image
synthesis with spatially-adaptive normalization,” in CVPR, 2019.
12, 13

[59] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, “Scalable

person re-identiﬁcation: A benchmark,” in CVPR, 2015. 12

[60] Z. Liu, P. Luo, S. Qiu, X. Wang, and X. Tang, “Deepfashion: Power-
ing robust clothes recognition and retrieval with rich annotations,”
in CVPR, 2016. 12

[61] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be-
nenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset
for semantic urban scene understanding,” in CVPR, 2016. 13
[62] F. Yu, V. Koltun, and T. Funkhouser, “Dilated residual networks,”

in CVPR, 2017. 13

[63] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun, “Uniﬁed perceptual

parsing for scene understanding,” in ECCV, 2018. 13

Hao Tang is a Ph.D. candidate in the Depart-
ment of Information Engineering and Computer
Science, and a member of Multimedia and Hu-
man Understanding Group (MHUG) led by Prof.
Nicu Sebe at the University of Trento. He re-
ceived the Master degree in computer applica-
tion technology in 2016 at the School of Elec-
tronics and Computer Engineering, Peking Uni-
versity, China. His research interests are ma-
chine learning, (deep) representation learning
and their applications to computer vision.

Dan Xu is a Postdoc researcher in Visual Ge-
ometric Group at the University of Oxford. He
received the Ph.D. Computer Science at the Uni-
versity of Trento. He was a research assistant in
the Multimedia Laboratory in the Department of
Electronic Engineering at the Chinese University
of Hong Kong. His research focuses on com-
puter vision, multimedia and machine learning.
He received the Intel best scientiﬁc paper award
at ICPR 2016.

Yan Yan is currently an assistant professor in
computer science at the Texas State University.
He received the Ph.D. degree from University of
Trento, Italy, in 2014. He was Research Fellow at
the University of Michigan (2016-2017) and Uni-
versity of Trento (2014-2016). He was a Visiting
Scholar with Carnegie Mellon University in 2013
and a Visiting Research Fellow with Advanced
Digital Sciences Center (ADSC), UIUC, Singa-
pore in 2015. Dr. Yan is the recipient of Best
Student Paper Award in ICPR 2014 and Best

Paper Award in ACM Multimedia 2015.

Jason J. Corso is currently a Professor of Elec-
trical Engineering and Computer Science at the
University of Michigan. He received his Ph.D. in
Computer Science at The Johns Hopkins Uni-
versity in 2005. He is a recipient of the NSF
CAREER award (2009), ARO Young Investigator
award (2010), Google Faculty Research Award
(2015) and on the DARPA CSSG. His main re-
search thrust is high-level computer vision and
its relationship to human language, robotics and
data science. He primarily focuses on problems
in video understanding such as video segmentation, activity recognition,
and video-to-text.

Philip H. S. Torr received the PhD degree from
Oxford University. After working for another three
years at Oxford, he worked for six years for
Microsoft Research, ﬁrst in Redmond, then in
Cambridge, founding the vision side of the Ma-
chine Learning and Perception Group. He is now
a professor at Oxford University. He has won
awards from top vision conferences, including
ICCV, CVPR, ECCV, NIPS and BMVC. He is a
senior member of the IEEE and a Royal Society
Wolfson Research Merit Award holder.

Nicu Sebe is Professor with the University of
Trento, Italy, leading the research in the areas
of multimedia information retrieval and human
behavior understanding. He was the General
CoChair of the IEEE FG Conference 2008 and
ACM Multimedia 2013, and the Program Chair
of the International Conference on Image and
Video Retrieval in 2007 and 2010, ACM Multi-
media 2007 and 2011. He is the Program Chair
of ICCV 2017 and ECCV 2016, and a General
Chair of ACM ICMR 2017 and ICPR 2020. He is

a fellow of the International Association for Pattern Recognition.

