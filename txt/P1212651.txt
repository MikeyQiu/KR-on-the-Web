Bi-Directional Differentiable Input Reconstruction
for Low-Resource Neural Machine Translation

Xing Niu
University of Maryland
xingniu@cs.umd.edu

Weijia Xu
University of Maryland
weijia@cs.umd.edu

Marine Carpuat
University of Maryland
marine@cs.umd.edu

9
1
0
2
 
r
p
A
 
3
 
 
]
L
C
.
s
c
[
 
 
2
v
6
1
1
1
0
.
1
1
8
1
:
v
i
X
r
a

Abstract

We aim to better exploit the limited amounts
of parallel text available in low-resource set-
tings by introducing a differentiable recon-
struction loss for neural machine translation
(NMT). This loss compares original inputs
to reconstructed inputs, obtained by back-
translating translation hypotheses into the in-
put language. We leverage differentiable sam-
pling and bi-directional NMT to train models
end-to-end, without introducing additional pa-
rameters. This approach achieves small but
consistent BLEU improvements on four lan-
guage pairs in both translation directions, and
outperforms an alternative differentiable re-
construction strategy based on hidden states.

1

Introduction

Neural Machine Translation (NMT) performance
degrades sharply when parallel training data is
limited (Koehn and Knowles, 2017). Past work
has addressed this problem by leveraging mono-
lingual data (Sennrich et al., 2016a; Ramachan-
dran et al., 2017) or multilingual parallel data
(Zoph et al., 2016; Johnson et al., 2017; Gu et al.,
2018a). We hypothesize that the traditional train-
ing can be complemented by better leveraging lim-
ited training data. To this end, we propose a new
training objective for this model by augmenting
the standard translation cross-entropy loss with a
differentiable input reconstruction loss to fur-
ther exploit the source side of parallel samples.

Input reconstruction is motivated by the idea
of round-trip translation. Suppose sentence f is
translated forward to e using model θf e and then
translated back to ˆf using model θef , then e is
more likely to be a good translation if the distance
between ˆf and f is small (Brislin, 1970). Prior
work applied round-trip translation to monolin-
gual examples and sampled the intermediate trans-
lation e from a K-best list generated by model

θf e using beam search (Cheng et al., 2016; He
et al., 2016). However, beam search is not differ-
entiable which prevents back-propagating recon-
struction errors to θf e. As a result, reinforcement
learning algorithms, or independent updates to θf e
and θef were required.

In this paper, we focus on the problem of mak-
ing input reconstruction differentiable to simplify
training. In past work, Tu et al. (2017) addressed
this issue by reconstructing source sentences from
the decoder’s hidden states. However, this re-
construction task can be artiﬁcially easy if hid-
den states over-memorize the input. This approach
also requires a separate auxiliary reconstructor,
which introduces additional parameters.

We propose instead to combine beneﬁts from
differentiable sampling and bi-directional NMT to
obtain a compact model that can be trained end-
to-end with back-propagation. Speciﬁcally,

• Translations are sampled using the Straight-
Through Gumbel Softmax (STGS) estima-
tor (Jang et al., 2017; Bengio et al., 2013),
which allows back-propagating reconstruc-
tion errors.

• Our approach builds on the bi-directional
NMT model (Niu et al., 2018; Johnson et al.,
2017), which improves low-resource transla-
tion by jointly modeling translation in both
directions (e.g., Swahili ↔ English). A sin-
gle bi-directional model is used as a translator
and a reconstructor (i.e. θef = θf e) without
introducing more parameters.

Experiments show that our approach outper-
forms reconstruction from hidden states.
It
achieves consistent improvements across various
low-resource language pairs and directions, show-
ing its effectiveness in making better use of limited
parallel data.

2 Background

3 Approach

Using round-trip translations (f → e → ˆf ) as a
training signal for NMT usually requires auxil-
iary models to perform back-translation and can-
not be trained end-to-end without reinforcement
learning. For instance, Cheng et al. (2016) added
a reconstruction loss for monolingual examples to
the training objective. He et al. (2016) evaluated
the quality of e by a language model and ˆf by
a reconstruction likelihood. Both approaches have
symmetric forward and backward translation mod-
els which are updated alternatively. This require
policy gradient algorithms for training, which are
not always stable.

Back-translation (Sennrich et al., 2016a) per-
forms half of the reconstruction process, by gener-
ating a synthetic source side for monolingual tar-
get language examples: e → ˆf . It uses an auxil-
iary backward model to generate the synthetic data
but only updates the parameters of the primary
forward model. Iteratively updating forward and
backward models (Zhang et al., 2018; Niu et al.,
2018) is an expensive solution as back-translations
are regenerated at each iteration.

Prior work has sought to simplify the opti-
mization of reconstruction losses by side-stepping
beam search. Tu et al. (2017) ﬁrst proposed to re-
construct NMT input from the decoder’s hidden
states while Wang et al. (2018a,b) suggested to use
both encoder and decoder hidden states to improve
translation of dropped pronouns. However, these
models might achieve low reconstruction errors by
learning to copy the input to hidden states. To
avoid copying the input, Artetxe et al. (2018) and
Lample et al. (2018) used denoising autoencoders
(Vincent et al., 2008) in unsupervised NMT.

Our approach is based instead on the Gum-
bel Softmax (Jang et al., 2017; Maddison et al.,
2017), which facilitates differentiable sampling of
sequences of discrete tokens. It has been success-
fully applied in many sequence generation tasks,
including artiﬁcial language emergence for multi-
agent communication (Havrylov and Titov, 2017),
composing tree structures from text (Choi et al.,
2018), and tasks under the umbrella of genera-
tive adversarial networks (Goodfellow et al., 2014)
such as generating the context-free grammar (Kus-
ner and Hern´andez-Lobato, 2016), machine com-
prehension (Wang et al., 2017) and machine trans-
lation (Gu et al., 2018b).

NMT is framed as a conditional language model,
where the probability of predicting target token et
at step t is conditioned on the previously generated
sequence of tokens e<t and the source sequence f
given the model parameter θ. Suppose each token
is indexed and represented as a one-hot vector, its
probability is realized as a softmax function over
a linear transformation a(ht) where ht is the de-
coder’s hidden state at step t:

P (et|e<t, f ; θ) = softmax(a(ht))(cid:62)et.

(1)

The hidden state is calculated by a neural network
g given the embeddings of the previous target to-
kens e<t in the embedding matrix E(e<t) and the
context ct coming from the source:

ht = g(E(e<t), ct).

(2)

In our bi-directional model, the source sentence
can be either f or e and is respectively translated
to e or f . The language is marked by a tag (e.g.,
<en>) at the beginning of each source sentence
(Johnson et al., 2017; Niu et al., 2018). To facil-
itate symmetric reconstruction, we also add lan-
guage tags to target sentences. The training data
corpus is then built by swapping the source and
target sentences of a parallel corpus and append-
ing the swapped version to the original.

3.1 Bi-Directional Reconstruction

Our bi-directional model performs both forward
translation and backward reconstruction. By con-
trast, uni-directional models require an auxiliary
reconstruction module, which introduces addi-
tional parameters. This module can be either
a decoder-based reconstructor (Tu et al., 2017;
Wang et al., 2018a,b) or a reversed dual NMT
model (Cheng et al., 2016; He et al., 2016; Wang
et al., 2018c; Zhang et al., 2018).

Here the reconstructor, which shares the same
parameter with the translator T (·), can also
be trained end-to-end by maximizing the log-
likelihood of reconstructing f :

LR =

log P (f | T (f ; θ); θ),

(3)

Combining with the forward translation likelihood

LT =

log P (e | f ; θ),

(4)

(cid:88)

f

(cid:88)

(f (cid:107)e)

we use L = LT +LR as the ﬁnal training objective
for f → e. The dual e → f model is trained si-
multaneously by swapping the language direction
in bi-directional NMT.

Reconstruction is reliable only with a model
that produces reasonable base translations. Fol-
lowing prior work (Tu et al., 2017; He et al., 2016;
Cheng et al., 2016), we pre-train a base model with
LT and ﬁne-tune it with LT + LR.

3.2 Differentiable Sampling

We use differentiable sampling to side-step beam
search and back-propagate error signals. We use
the Gumbel-Max reparameterization trick (Mad-
dison et al., 2014) to sample a translation token
at each time step from the softmax distribution in
Equation 1:

et = one-hot

(cid:16)

arg max
k

(cid:0)a(ht)k + Gk

(cid:1)(cid:17)

(5)

where Gk is i.i.d. and drawn from Gumbel(0, 1)1.
We use scaled Gumbel with parameter β,
i.e.
Gumbel(0, β), to control the randomness. The
sampling becomes deterministic (which is equiv-
alent to greedy search) as β approaches 0.

Since arg max is not a differentiable operation,
we approximate its gradient with the Straight-
Through Gumbel Softmax (STGS) (Jang et al.,
2017; Bengio et al., 2013): ∇θet ≈ ∇θ ˜et, where
˜et = softmax (cid:0)(a(ht) + G)/τ (cid:1)

(6)

As τ approaches 0, softmax is closer to arg max
but training might be more unstable. While the
STGS estimator is biased when τ is large, it per-
forms well in practice (Gu et al., 2018b; Choi
et al., 2018) and is sometimes faster and more ef-
fective than reinforcement learning (Havrylov and
Titov, 2017).

To generate coherent intermediate translations,
the decoder used for sampling only consumes its
previously predicted ˆe<t. This contrasts with
the usual teacher forcing strategy (Williams and
Zipser, 1989), which always feeds in the ground-
truth previous tokens e<t when predicting the
current token ˆet. With teacher forcing, the se-
quence concatenation [e<t; ˆet] is probably coher-
ent at each time step, but the actual predicted se-
quence [ˆe<t; ˆet] would break the continuity.2

1i.e. Gk = − log(− log(uk)) and uk ∼ Uniform(0, 1).
2Sampling with teacher
forcing yielded consistently

worse BLEU than baselines in preliminary experiments.

# sent.
SW↔EN
TL↔EN
SO↔EN
TR↔EN

Training
60,570
70,703
68,550
207,021

Dev.
500
704
844
1,001

Test
3,000
3,000
3,000
3,007

Table 1: Experiments are conducted on four low-
resource language pairs, in both translation directions.

4 Experiments

4.1 Tasks and Data

We evaluate our approach on four low-resource
language pairs. Parallel data for Swahili↔English
(SW↔EN), Tagalog↔English (TL↔EN)
and
Somali↔English (SO↔EN) contains a mix-
ture of domains such as news and weblogs
and is collected from the IARPA MATERIAL
program3,
the Global Voices parallel corpus4,
Common Crawl (Smith et al., 2013), and the
LORELEI Somali representative language pack
(LDC2018T11). The test samples are extracted
from the held-out ANALYSIS set of MATERIAL.
Parallel Turkish↔English (TR↔EN) data is pro-
vided by the WMT news translation task (Bojar
et al., 2018). We use pre-processed “corpus”,
training,
“newsdev2016”,
development and test sets.5

“newstest2017”

as

tokenization,

We apply normalization,

true-
casing, joint source-target BPE with 32,000 op-
erations (Sennrich et al., 2016b) and sentence-
ﬁltering (length 80 cutoff) to parallel data. Item-
ized data statistics after preprocessing can be
found in Table 1. We report case-insensitive
BLEU with the WMT standard ‘13a’ tokenization
using SacreBLEU (Post, 2018).

4.2 Model Conﬁguration and Baseline

We build NMT models upon the attentional RNN
encoder-decoder architecture (Bahdanau et al.,
2015) implemented in the Sockeye toolkit (Hieber
et al., 2017). Our translation model uses a bi-
directional encoder with a single LSTM layer of
size 512, multilayer perceptron attention with a
layer size of 512, and word representations of size
512. We apply layer normalization (Ba et al.,

3https://www.iarpa.gov/index.php/

research-programs/material

4http://casmacat.eu/corpus/

global-voices.html

5http://data.statmt.org/wmt18/

translation-task/preprocessed/

EN→SW
Model
33.60 ± 0.14
Baseline
HIDDEN 33.41 ± 0.15
-0.19 ± 0.24
33.92 ± 0.10
0.32 ± 0.12
33.97 ± 0.08
0.37 ± 0.09

∆
β = 0
∆
β = 0.5
∆

SW→EN
30.70 ± 0.19
30.91 ± 0.19
0.21 ± 0.14
31.37 ± 0.18
0.66 ± 0.11
31.39 ± 0.09
0.69 ± 0.11

EN→TL
27.23 ± 0.11
27.43 ± 0.14
0.19 ± 0.13
27.65 ± 0.09
0.42 ± 0.16
27.65 ± 0.10
0.42 ± 0.11

TL→EN
32.15 ± 0.21
32.20 ± 0.35
0.04 ± 0.17
32.75 ± 0.32
0.59 ± 0.13
32.65 ± 0.24
0.50 ± 0.08

EN→SO
12.25 ± 0.08
12.30 ± 0.11
0.05 ± 0.11
12.47 ± 0.08
0.22 ± 0.04
12.48 ± 0.09
0.23 ± 0.03

SO→EN
20.80 ± 0.12
20.72 ± 0.16
-0.08 ± 0.12
21.14 ± 0.19
0.35 ± 0.15
21.20 ± 0.14
0.41 ± 0.13

EN→TR
12.90 ± 0.04
12.77 ± 0.11
-0.13 ± 0.13
13.26 ± 0.07
0.36 ± 0.09
13.16 ± 0.08
0.25 ± 0.09

TR→EN
15.32 ± 0.11
15.34 ± 0.10
0.01 ± 0.07
15.60 ± 0.19
0.28 ± 0.11
15.52 ± 0.07
0.19 ± 0.05

Table 2: BLEU scores on eight translation directions. The numbers before and after ‘±’ are the mean and standard
deviation over ﬁve randomly seeded models. Our proposed methods (β = 0/0.5) achieve small but consistent
improvements. ∆BLEU scores are in bold if mean−std is above zero while in red if the mean is below zero.

2016) and add dropout to embeddings and RNNs
(Gal and Ghahramani, 2016) with probability 0.2.
We train using the Adam optimizer (Kingma and
Ba, 2015) with a batch size of 48 sentences and
we checkpoint the model every 1000 updates. The
learning rate for baseline models is initialized to
0.001 and reduced by 30% after 4 checkpoints
without improvement of perplexity on the devel-
opment set. Training stops after 10 checkpoints
without improvement.

The bi-directional NMT model ties source and
target embeddings to yield a bilingual vector
space. It also ties the output layer’s weights and
embeddings to achieve better performance in low-
resource scenarios (Press and Wolf, 2017; Nguyen
and Chiang, 2018).

We train ﬁve randomly seeded bi-directional
baseline models by optimizing the forward trans-
lation objective LT and report the mean and stan-
dard deviation of test BLEU. We ﬁne-tune base-
line models with objective LT + LR,
inherit-
ing all settings except the learning rate which is
re-initialized to 0.0001. Each randomly seeded
model is ﬁne-tuned independently, so we are able
to report the standard deviation of ∆BLEU.

4.3 Contrastive Reconstruction Model

We compare our approach with reconstruction
from hidden states (HIDDEN). Following the best
practice of Wang et al. (2018a), two reconstruc-
tors are used to take hidden states from both the
encoder and the decoder. The corresponding two
reconstruction losses and the canonical transla-
tion loss were originally uniformly weighted (i.e.
1, 1, 1), but we found that balancing the recon-
struction and translation losses yields better results
(i.e. 0.5, 0.5, 1) in preliminary experiments.6

We use the reconstructor exclusively to compute
the reconstruction training loss. It has also been

used to re-rank translation hypotheses in prior
work, but Tu et al. (2017) showed in ablation stud-
ies that the gains from re-ranking are small com-
pared to those from training.

4.4 Results

Table 2 shows that our reconstruction approach
achieves small but consistent BLEU improve-
ments over the baseline on all eight tasks.7

We evaluate the impact of the Gumbel Softmax
hyperparameters on the development set. We se-
lect τ = 2 and β = 0/0.5 based on training sta-
bility and BLEU. Greedy search (i.e. β = 0) per-
forms similarly as sampling with increased Gum-
bel noise (i.e. more random translation selection
when β = 0.5):
increased randomness in sam-
pling does not have a strong impact on BLEU,
even though random sampling may approximate
the data distribution better (Ott et al., 2018). We
hypothesize that more random translation selec-
tion introduces lower quality samples and there-
fore noisier training signals. This is consistent
with the observation that random sampling is less
effective for back-translation in low-resource set-
tings (Edunov et al., 2018).

Sampling-based reconstruction is effective even
if there is moderate domain mismatch between
the training and the test data, such as in the case
that the word type out-of-vocabulary (OOV) rate
of TR→EN is larger than 20%. Larger improve-
ments can be achieved when the test data is closer
to training examples. For example, the OOV rate
of SW→EN is much smaller than the OOV rate of
TR→EN and the former obtains higher ∆BLEU.

Our approach yields more consistent results
than reconstructing from hidden states. The lat-
ter fails to improve BLEU in more difﬁcult cases,
such as TR↔EN with high OOV rates. We ob-
serve extremely low training perplexity for HID-

6We observed around 0.2 BLEU gains for TR↔EN tasks.

7The improvements are signiﬁcant with p < 0.01.

(a) training set

(b) development set

Figure 1: Training curves of perplexity on the training and the development sets for TR↔EN. Reconstructing from
hidden states (HIDDEN) and reconstructing from sampled translations (β = 0) are compared. HIDDEN achieves
extremely low training perplexity and suffers from unstable training during the early stage.

DEN compared with our proposed approach (Fig-
ure 1a). This suggests that HIDDEN yields repre-
sentations that memorize the input rather than im-
prove output representations.

Another advantage of our approach is that all
parameters were jointly pre-trained, which results
in more stable training behavior. By contrast, re-
constructing from hidden states requires to initial-
ize the reconstructors independently and suffers
from unstable early training behavior (Figure 1).

5 Conclusion

We studied reconstructing the input of NMT from
its intermediate translations to better exploit train-
ing samples in low-resource settings. We used
a bi-directional NMT model and the Straight-
Through Gumbel Softmax to build a fully dif-
ferentiable reconstruction model that does not re-
quire any additional parameters. We empirically
demonstrated that our approach is effective in
low-resource scenarios.
In future work, we will
investigate the use of differentiable reconstruc-
tion from sampled sequences in unsupervised and
semi-supervised sequence generation tasks.
In
particular, we will exploit monolingual corpora in
addition to parallel corpora for NMT.

Acknowledgments

We thank the three anonymous reviewers for their
helpful comments and suggestions. We also thank
the members of the Computational Linguistics and
Information Processing (CLIP) lab at the Univer-
sity of Maryland for helpful discussions.

This research is based upon work supported
in part by an Amazon Web Services Machine
Learning Research Award, and by the Ofﬁce of

the Director of National Intelligence (ODNI), In-
telligence Advanced Research Projects Activity
(IARPA), via contract #FA8650-17-C-9117. The
views and conclusions contained herein are those
of the authors and should not be interpreted as
necessarily representing the ofﬁcial policies, ei-
ther expressed or implied, of ODNI, IARPA, or
the U.S. Government. The U.S. Government is
authorized to reproduce and distribute reprints for
governmental purposes notwithstanding any copy-
right annotation therein.

References

Mikel Artetxe, Gorka Labaka, Eneko Agirre, and
Kyunghyun Cho. 2018. Unsupervised neural ma-
chine translation. In Proceedings of the 6th Interna-
tional Conference on Learning Representations.

Lei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton.
2016. Layer normalization. CoRR, abs/1607.06450.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
In Proceedings of
learning to align and translate.
the 3th International Conference on Learning Rep-
resentations.

Yoshua Bengio, Nicholas L´eonard, and Aaron C.
Courville. 2013. Estimating or propagating gradi-
ents through stochastic neurons for conditional com-
putation. CoRR, abs/1308.3432.

Ondrej Bojar, Christian Federmann, Mark Fishel,
Yvette Graham, Barry Haddow, Matthias Huck,
Philipp Koehn, and Christof Monz. 2018. Find-
ings of the 2018 conference on machine translation
(WMT18). In Proceedings of the Third Conference
on Machine Translation, pages 272–303. Associa-
tion for Computational Linguistics.

Richard W. Brislin. 1970. Back-translation for cross-
cultural research. Journal of Cross-Cultural Psy-
chology, 1(3):185–216.

Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Semi-
supervised learning for neural machine translation.
In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics, pages
1965–1974. Association for Computational Linguis-
tics.

Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2018.
Learning to compose task-speciﬁc tree structures. In
Proceedings of the Thirty-Second AAAI Conference
on Artiﬁcial Intelligence, pages 5094–5101. AAAI
Press.

Sergey Edunov, Myle Ott, Michael Auli, and David
Grangier. 2018. Understanding back-translation at
In Proceedings of the 2018 Conference on
scale.
Empirical Methods in Natural Language Process-
ing, pages 489–500. Association for Computational
Linguistics.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
In Advances in Neural Informa-
neural networks.
tion Processing Systems 29, pages 1019–1027. Cur-
ran Associates, Inc.

Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio. 2014. Gen-
erative adversarial nets. In Advances in Neural In-
formation Processing Systems 27, pages 2672–2680.
Curran Associates, Inc.

Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor
O. K. Li. 2018a. Universal neural machine transla-
tion for extremely low resource languages. In Pro-
ceedings of the 2018 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
344–354. Association for Computational Linguis-
tics.

Jiatao Gu, Daniel Jiwoong Im, and Victor O. K. Li.
2018b. Neural machine translation with gumbel-
In Proceedings of the Thirty-
greedy decoding.
Second AAAI Conference on Artiﬁcial Intelligence,
pages 5125–5132. AAAI Press.

Serhii Havrylov and Ivan Titov. 2017. Emergence of
language with multi-agent games: Learning to com-
municate with sequences of symbols. In Advances
in Neural Information Processing Systems 30, pages
2146–2156. Curran Associates, Inc.

Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu,
Tie-Yan Liu, and Wei-Ying Ma. 2016. Dual learn-
ing for machine translation. In Advances in Neural
Information Processing Systems 29, pages 820–828.
Curran Associates, Inc.

Felix Hieber, Tobias Domhan, Michael Denkowski,
David Vilar, Artem Sokolov, Ann Clifton, and Matt
Post. 2017. Sockeye: A toolkit for neural machine
translation. CoRR, abs/1712.05690.

Eric Jang, Shixiang Gu, and Ben Poole. 2017. Cate-
gorical reparameterization with gumbel-softmax. In
Proceedings of the 5th International Conference on
Learning Representations.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Tho-
rat, Fernanda B. Vi´egas, Martin Wattenberg, Greg
Corrado, Macduff Hughes, and Jeffrey Dean. 2017.
Google’s multilingual neural machine translation
system: Enabling zero-shot translation. Transac-
tions of the Association for Computational Linguis-
tics, 5:339–351.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
In Proceed-
method for stochastic optimization.
ings of the 3th International Conference on Learn-
ing Representations.

Philipp Koehn and Rebecca Knowles. 2017. Six chal-
In Pro-
lenges for neural machine translation.
ceedings of the First Workshop on Neural Machine
Translation, pages 28–39. Association for Compu-
tational Linguistics.

Matt J. Kusner and Jos´e Miguel Hern´andez-Lobato.
2016. GANS for sequences of discrete elements
with the gumbel-softmax distribution. In Proceed-
ings of the NIPS 2016 Workshop on Adversarial
Training.

Guillaume

Ludovic Denoyer,

Lample,
and
Unsupervised
Marc’Aurelio Ranzato. 2018.
machine translation using monolingual corpora
the 6th International
only.
Conference on Learning Representations.

In Proceedings of

Chris J. Maddison, Andriy Mnih, and Yee Whye Teh.
2017. The concrete distribution: A continuous re-
laxation of discrete random variables. In Proceed-
ings of the 5th International Conference on Learn-
ing Representations.

Chris J. Maddison, Daniel Tarlow, and Tom Minka.
2014. A* sampling. In Advances in Neural Infor-
mation Processing Systems 27, pages 3086–3094.
Curran Associates, Inc.

Toan Q. Nguyen and David Chiang. 2018.

Improv-
ing lexical choice in neural machine translation. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 334–343. Association for Computational Lin-
guistics.

Xing Niu, Michael Denkowski, and Marine Carpuat.
2018. Bi-directional neural machine translation
with synthetic parallel data. In Proceedings of the
2nd Workshop on Neural Machine Translation and
Generation, pages 84–91. Association for Computa-
tional Linguistics.

Myle Ott, Michael Auli, David Grangier,

and
Marc’Aurelio Ranzato. 2018. Analyzing uncer-
In Proceed-
tainty in neural machine translation.
ings of the 35th International Conference on Ma-
chine Learning, volume 80 of Proceedings of Ma-
chine Learning Research, pages 3953–3962. PMLR.

Longyue Wang, Zhaopeng Tu, Shuming Shi, Tong
Zhang, Yvette Graham, and Qun Liu. 2018a. Trans-
lating pro-drop languages with reconstruction mod-
els. In Proceedings of the Thirty-Second AAAI Con-
ference on Artiﬁcial Intelligence, pages 4937–4945.
AAAI Press.

Longyue Wang, Zhaopeng Tu, Andy Way, and Qun
Liu. 2018b. Learning to jointly translate and pre-
dict dropped pronouns with a shared reconstruction
mechanism. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 2997–3002. Association for Compu-
tational Linguistics.

Yijun Wang, Yingce Xia, Li Zhao, Jiang Bian, Tao
Qin, Guiquan Liu, and Tie-Yan Liu. 2018c. Dual
transfer learning for neural machine translation with
marginal distribution regularization. In Proceedings
of the Thirty-Second AAAI Conference on Artiﬁcial
Intelligence, pages 5553–5560. AAAI Press.

Ronald J. Williams and David Zipser. 1989. A learn-
ing algorithm for continually running fully recurrent
neural networks. Neural Computation, 1(2):270–
280.

Zhirui Zhang, Shujie Liu, Mu Li, Ming Zhou, and En-
hong Chen. 2018. Joint training for neural machine
translation models with monolingual data. In Pro-
ceedings of the Thirty-Second AAAI Conference on
Artiﬁcial Intelligence, pages 555–562. AAAI Press.

Barret Zoph, Deniz Yuret, Jonathan May, and Kevin
Knight. 2016. Transfer learning for low-resource
In Proceedings of the
neural machine translation.
2016 Conference on Empirical Methods in Natural
Language Processing, pages 1568–1575. Associa-
tion for Computational Linguistics.

Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation, pages 186–191. Association
for Computational Linguistics.

Oﬁr Press and Lior Wolf. 2017. Using the output em-
bedding to improve language models. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Compu-
tational, pages 157–163. Association for Computa-
tional Linguistics.

Prajit Ramachandran, Peter J. Liu, and Quoc V. Le.
2017. Unsupervised pretraining for sequence to se-
quence learning. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, pages 383–391. Association for Com-
putational Linguistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving neural machine translation mod-
In Proceedings of the
els with monolingual data.
54th Annual Meeting of the Association for Com-
putational Linguistics, pages 86–96. Association for
Computational Linguistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics, pages 1715–1725. Association for Com-
putational Linguistics.

Jason R. Smith, Herve Saint-Amand, Magdalena Pla-
mada, Philipp Koehn, Chris Callison-Burch, and
Adam Lopez. 2013. Dirt cheap web-scale parallel
text from the common crawl. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 1374–1383. Association
for Computational Linguistics.

Zhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu,
and Hang Li. 2017. Neural machine translation with
In Proceedings of the Thirty-First
reconstruction.
AAAI Conference on Artiﬁcial Intelligence, pages
3097–3103. AAAI Press.

Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
In Proceedings of the 25th International
coders.
Conference on Machine Learning, pages 1096–
1103. ACM.

Bingning Wang, Kang Liu, and Jun Zhao. 2017. Con-
ditional generative adversarial networks for com-
monsense machine comprehension. In Proceedings
of the Twenty-Sixth International Joint Conference
on Artiﬁcial Intelligence, pages 4123–4129.

