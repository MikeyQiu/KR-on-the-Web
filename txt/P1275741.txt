7
1
0
2
 
y
a
M
 
4
2
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
1
3
9
8
0
.
5
0
7
1
:
v
i
X
r
a

Proximity Variational Inference

Jaan Altosaar
Department of Physics
Princeton University
altosaar@princeton.edu

Rajesh Ranganath
Department of Computer Science
Princeton University
rajeshr@cs.princeton.edu

David M. Blei
Data Science Institute
Departments of Computer Science and Statistics
Columbia University
david.blei@columbia.edu

Abstract

Variational inference is a powerful approach for approximate posterior inference.
However, it is sensitive to initialization and can be subject to poor local optima. In
this paper, we develop proximity variational inference (pvi). pvi is a new method
for optimizing the variational objective that constrains subsequent iterates of the
variational parameters to robustify the optimization path. Consequently, pvi is
less sensitive to initialization and optimization quirks and ﬁnds better local optima.
We demonstrate our method on three proximity statistics. We study pvi on a
Bernoulli factor model and sigmoid belief network with both real and synthetic
data and compare to deterministic annealing (Katahira et al., 2008). We highlight
the ﬂexibility of pvi by designing a proximity statistic for Bayesian deep learning
models such as the variational autoencoder (Kingma and Welling, 2014; Rezende
et al., 2014). Empirically, we show that pvi consistently ﬁnds better local optima
and gives better predictive performance.

1

Introduction

Variational inference (vi) is a powerful method for probabilistic modeling. vi uses optimization to
approximate diﬃcult-to-compute conditional distributions (Jordan et al., 1999). In its modern incarna-
tion, it has scaled Bayesian computation to large data sets (Hoﬀman et al., 2013), generalized to large
classes of models (Kingma and Welling, 2014; Ranganath et al., 2014; Rezende and Mohamed, 2015),
and has been deployed as a computational engine in probabilistic programming systems (Mansinghka
et al., 2014; Kucukelbir et al., 2015; Tran et al., 2016).
Despite these signiﬁcant advances, however, vi has drawbacks. For one, it tries to iteratively solve a
diﬃcult nonconvex optimization problem and its objective contains many local optima. Consequently,
vi is sensitive to initialization and easily gets stuck in a poor solution. We develop a new optimization
method for vi. Our method is less sensitive to initialization and ﬁnds better optima.

Consider a probability model p(z, x) and the goal of calculating the posterior p(z | x). The idea behind
vi is to posit a family of distributions over the hidden variables q(z; λ) and then ﬁt the variational
parameters λ to minimize the Kullback-Leibler (kl) divergence between the approximating family
and the exact posterior, kl(q(z; λ)||p(z | x)). The kl is not tractable so vi optimizes a proxy. That
proxy is the evidence lower bound (elbo),

L(λ) = E[log p(z, x)] − E[log q(z; λ)],

(1)

(a) Variational inference

(b) Proximity variational inference, Algorithm 2

Figure 1: Proximity variational inference (pvi) is robust to bad initialization. We study a
Bernoulli factor model. Model parameters are randomly initialized on a ring around the known true
parameters (in red) used to generate the data. The arrows start at these parameter initializations and
end at the ﬁnal parameter estimates (shown as green dots). (a) Variational inference with gradient
ascent suﬀers from multiple local optima and cannot reliably recover the truth. (b) pvi with an entropy
proximity statistic reliably infers the true parameters using Algorithm 2.

where expectations are taken with respect to q(z; λ). Maximizing the elbo with respect to λ is
equivalent to minimizing the kl divergence.

The issues around vi stem from the elbo and the iterative algorithms used to optimize it. When
the algorithm zeroes (or nearly zeroes) some of the support of q(z; λ), it becomes hard to later
“escape,” i.e., to add support for the conﬁgurations of the latent variables that have been assigned zero
probability (MacKay, 2003; Burda et al., 2015). This leads to poor local optima and to sensitivity to
the starting point, where a misguided initialization will lead to such optima. These problems happen
in both gradient-based and coordinate ascent methods. We address these issues with proximity
variational inference (pvi), a variational inference algorithm that is speciﬁcally designed to avoid
poor local optima and to be robust to diﬀerent initializations.

pvi builds on the proximity perspective of gradient ascent. The proximity perspective views each step
of gradient ascent as a constrained minimization of a Taylor expansion of the objective around the
previous step’s parameter (Spall, 2003; Boyd and Vandenberghe, 2004). The constraint, a proximity
constraint, enforces that the next point should be inside a Euclidean ball of the previous. The step
size relates to the size of that ball.

In vi, a constraint on the Euclidean distance means that all dimensions of the variational parameters are
equally constrained. We posit that this leads to problems; some dimensions need more regularization
than others. For example, consider a variational distribution that is Gaussian. A good optimization
will change the variance parameter more slowly than the mean parameter to prevent rapid changes
to the support. The Euclidean constraint cannot enforce this. Furthermore, the constraints enforced
by gradient descent are transient; the constraints are relative to the previous iterate—one poor move
during the optimization can lead to permanent optimization problems.

To this end, pvi uses proximity constraints that are more meaningful to variational inference and to
optimization of probability parameters. A constraint is deﬁned using a proximity statistic and distance
function. As one example we consider a constraint based on the entropy proximity statistic. This
limits the change in entropy of the variational approximation from one step to the next. Consider
again a Gaussian approximation. The entropy is a function of the variance alone and thus the entropy
constraint counters the pathologies induced by the Euclidean proximity constraint. We also study
constraints built from other proximity statistics, such as those that penalize the rapid changes in the
mean and variance of the approximate posterior.

Figure 1 provides an illustration of the advantages of pvi. Our goal is to estimate the parameters of a
factor analysis model with variational inference, i.e., using the posterior expectation under a ﬁtted
2

variational distribution. We run variational inference 100 times, each time initializing the estimates
(the model parameters) to a diﬀerent position on a ring around the truth.

In the ﬁgure, red points indicate the true value. The start locations of the green arrows indicate the
initialized estimates. Green points indicate the ﬁnal estimates, after optimizing from the initial points.
Panel (a) shows that optimizing the standard elbo with gradients leads to poor local optima and
misplaced estimates. Panel (b) illustrates that regardless of the initialization, pvi with an entropy
proximity statistic ﬁnd estimates that are close to the true value.

The rest of the paper is organized as follows. Section 2 reviews variational inference and the proximity
perspective of gradient optimization. Section 3 derives pvi; we develop four proximity constraints
and two algorithms for optimizing the elbo. We study three models in Section 4: a Bernoulli factor
model, a sigmoid belief network (Mnih and Rezende, 2016), and a variational autoencoder (Kingma
and Welling, 2014; Rezende et al., 2014). On the MNIST data set, pvi generally outperforms classical
methods for variational inference.

Related work. Recent work has proposed several related algorithms. Khan et al. (2015) and Theis
and Hoﬀman (2015) develop a method to optimize the elbo that imposes a soft limit on the change
in kl of consecutive variational approximations. This is equivalent to pvi with identity proximity
statistics and a kl distance function. Khan et al. (2016) extend both prior works to other divergence
functions. Their general approach is equivalent to pvi identity proximity statistics and distance
functions given by strongly-convex divergences. Compared to prior work, pvi generalizes to a broader
class of proximity statistics. We develop proximity statistics based on entropy, kl, orthogonal weight
matrices, and the mean and variance of the variational approximation.

The problem of model pruning in variational inference has also been studied and analytically solved
in a matrix factorization model in Nakajima et al. (2013)—this method is model-speciﬁc, whereas pvi
applies to a much broader class of latent variable models. Finally, deterministic annealing (Katahira
et al., 2008) consists of adding a temperature parameter to the entropy term in the elbo that is
annealed to one during inference. This is similar to pvi with the entropy proximity statistic which
keeps the entropy stable across iterations. Deterministic annealing enforces global penalization of
low-entropy conﬁgurations of latent variables rather than the smooth constraint used in pvi, and
cannot accommodate the range of proximity statistics we design in this work.

2 Variational inference

Consider a model p(x, z), where x is the observed data and z are the latent variables. As described
in Section 1, vi posits an approximating family q(z; λ) and maximizes the elbo in Equation (1).
Solving this optimization is equivalent to ﬁnding the variational approximation that minimizes kl
divergence to the exact posterior (Jordan et al., 1999; Wainwright and Jordan, 2008).

2.1 Gradient ascent has Euclidean proximity

Gradient ascent maximizes the elbo by repeatedly following its gradient. One view of this algorithm
is that it repeatedly maximizes the linearized elbo subject to a proximity constraint on the current
variational parameter (Spall, 2003). The name ‘proximity’ comes from constraining subsequent
parameters to remain close in the proximity statistic. In gradient ascent, the proximity statistic for
the variational parameters is the identity function f (λ) = λ, and the distance function is the square
diﬀerence.

Let λt be the variational parameters at iteration t and ρ be a constant. To obtain the next iterate λt+1,
gradient ascent maximizes the linearized elbo,

U (λt+1) =L(λt) + ∇L(λt)(cid:62)(λt+1 − λt) −

(λt+1 − λt)(cid:62)(λt+1 − λt).

(2)

1
2ρ

Speciﬁcally, this is the linearized elbo around λt subject to λt+1 being close in squared Euclidean
distance to λt.
Finding the λt+1 which maximizes Equation (2) yields

λt+1 = λt + ρ∇L(λt).

(3)

3

Algorithm 1: Proximity variational inference
Input: Initial parameters λ0, proximity statistic f (λ), distance function d
Output: Parameters λ of variational q(λ) that maximize the elbo objective
while L not converged do
λt+1 ← λt + Noise
while U not converged do

Update λt+1 ← λt+1 + ρ∇λU (λt+1)

end
λt ← λt+1

end
return λ

This is the familiar gradient ascent update with a step size of ρ. The step size ρ controls the radius
of the Euclidean ball which demarcates valid next steps for the parameters. Note that the Euclidean
constraint between subsequent iterates is implicit in all gradient ascent algorithms.

2.2 An example where variational inference fails

We study a setting where variational inference suﬀers from poor local optima. Consider a factor
model, with latent variables zik ∼ Bernoulli(π) and data xi ∼ Gaussian (cid:0)µ = (cid:80)
k zikµk, σ2 = 1(cid:1).
This is a “feature” model of real-valued data x; when one of the features is on (i.e., zik = 1), the ith
mean shifts according the that feature’s mean parameter (i.e., µk). Thus the binary latent variables
zik control which cluster means µk contribute to the distribution of xi.
The Bernoulli prior is parametrized by π; we choose a Bernoulli approximate posterior
q(zk; λk) = Bernoulli(λk). A common approach to vi is coordinate ascent (Bishop, 2006), where
we iteratively optimize each variational parameter. The optimal variational parameter for zik is




λik ∝ exp

E−zik

−






1
2σ2 (xi −

(cid:88)

j

zijµj)2








.

We can use this update in a variational EM setting. The corresponding gradient for µk is



∂L
∂µk

= −

1
σ2

(cid:88)

i

−xiλik + λikµk + λik

λijµj

 .



(cid:88)

j(cid:54)=k

(4)

(5)

Meditating on these two equations reveals a deﬁciency in mean-ﬁeld variational inference. First, if
the mean parameters µ are initialized far from the data then q∗(zik = 1) will be very small. The
reason is in Equation (4), where the squared diﬀerence between the data xi and the expected cluster
mean will be large and negative. Second, when the probability of cluster assignment is close to zero,
λik is small. This means that the norm of the gradient in Equation (5) will be small. Consequently,
learning will be slow. We see this phenomenon in Figure 1 (a). Variational inference arrives at poor
local optima and does not always recover the correct cluster means µk.

3 Proximity variational inference

We now develop proximity variational inference (pvi), a variational inference method that is robust
to initialization and can consistently reach good local optima (Section 3.1). pvi alters the notion of
proximity. We further restrict the iterates of the variational parameters by deforming the Euclidean
ball implicit in classical gradient ascent. This is done by choosing proximity statistics that are not
the identity function, and distance functions that are diﬀerent than the square diﬀerence. These
design choices help guide the variational parameters away from poor local optima (Section 3.2). One
drawback of the proximity perspective is that it requires an inner optimization at each step of the outer
optimization. We use a Taylor expansion to avoid this computational burden (Section 3.3).

4

Algorithm 2: Fast proximity variational inference
Input: Initial parameters λ0, adaptive learning rate optimizer, proximity statistic f (λ), distance d
Output: Parameters λ of the variational distribution q(λ) that maximize the elbo objective
while Lproximity not converged do

λt+1 = λt + ρ(∇L(λt) − k · (∇d(f (˜λ), f (λt))∇f (λt)).
˜λ = α˜λ + (1 − α)λt+1

end
return λ

3.1 Proximity constraints for variational inference

pvi enriches the proximity constraint in gradient ascent of the elbo. We want to develop constraints
on the iterates λt to counter the pathologies of standard variational inference.
Let f (·) be a proximity statistic, and let d be a diﬀerentiable distance function that measures distance
between proximity statistic iterates. A proximity constraint is the combination of a distance function
d applied to a proximity statistic f . (Recall that in classical gradient ascent, the Euclidean proximity
constraint uses the identity as the proximity statistic and the square diﬀerence as the distance.) Let k
be the scalar magnitude of the proximity constraint. We deﬁne the proximity update equation for the
variational parameters λt+1 to be

U (λt+1) =L(λt) + ∇L(λt)(cid:62)(λt+1 − λt)

−

1
2ρ

(λt+1 − λt)(cid:62)(λt+1 − λt) − k · d(f (˜λ), f (λt+1)),

(6)

where ˜λ is the variational parameter to which we are measuring closeness. In gradient ascent, this
is the previous parameter ˜λ = λt, but our construction can enforce proximity to more than just the
previous parameters. For example, we can set ˜λ to be an exponential moving average1—this adds
robustness to one-update optimization missteps.

The next parameters are found by maximizing Equation (6). This enforces that the variational
parameters between updates will remain close in the proximity statistic f (λ). For example, f (λ)
might be the entropy of the variational approximation; this can avoid zeroing out some of its support.
This procedure is detailed in Algorithm 1. The magnitude k of the constraint is a hyperparameter.
The inner optimization loop optimizes the update equation U at each step.

3.2 Proximity statistics for variational inference

We describe four proximity statistics f (λ) appropriate for variational inference. Together with a
distance function, these proximity statistics yield proximity constraints. (We study them in Sec-
tion 4.)

Entropy proximity statistic. Consider a constraint built from the entropy proximity statistic, f (λ) =
H(q(z; λ)). Informally, the entropy measures the amount of randomness present in a distribution.
High entropy distributions look more uniform across their support; low entropy distributions are
peaky.

Using the entropy in Equation (6) constrains all updates to have entropy close to their previous
update. When the variational distributions are initialized with large entropy, this statistic balances the
“zero-forcing” issue that is intrinsic to variational inference (MacKay, 2003). Figure 1 demonstrates
how pvi with an entropy constraint can correct this pathology.

kl proximity statistic. We can rewrite the elbo to include the kl between the approximate posterior
and the prior (Kingma and Welling, 2014),

L(λ) = E[log p(x | z)] − KL(q(z | x; λ)||p(z)).

1The exponential moving average of a variable λ is denoted ˜λ and is updated according to ˜λ ← α˜λ+(1−α)λ,

where α is a decay close to one.

5

Bad initialization

Marginal likelihood

Good initialization

Marginal likelihood

Inference method

Variational inference
Deterministic annealing
pvi, Entropy constraint
pvi, Mean/variance constraint

elbo

−663.7
−119.4
−118.0
−119.9

−636.7
−110.2
−110.0
−111.1

elbo

−122.1
−116.5
−114.1
−115.7

−114.1
−108.7
−107.5
−108.3

Table 1: Proximity variational inference improves on deterministic annealing (Katahira et al.,
2008) and vi in a one-layer sigmoid belief network. We report validation evidence lower bound
(elbo) and marginal likelihood on the binary MNIST dataset (Larochelle and Murray, 2011). The
model has one stochastic layer of 200 latent variables. pvi outperforms the classical variational
inference algorithm and deterministic annealing (Katahira et al., 2008).

Flexible models tend to minimize the kl divergence too quickly and get stuck in poor optima (Bowman
et al., 2016; Higgins et al., 2016). The choice of kl as a proximity statistic prevents the kl from
being optimized too quickly relative to the likelihood.

Mean/variance proximity statistic. A common theme in the problems with variational inference is
that the bulk of the probability mass can quickly move to a point where that dimension will no longer
be explored (Burda et al., 2015). One way to address this is to restrict the mean and variance of the
variational approximation to change slowly during optimization. This constraint only allows higher
order moments of the variational approximation to change rapidly. The mean µ = Eq(z;λ)[z] and
variance Var(z) = Eq(z;λ)[(z − µ)2] are the statistics f (λ) we constrain.
Orthogonal proximity statistic. In Bayesian deep learning models such as the variational autoen-
coder (Kingma and Welling, 2014; Rezende et al., 2014) it is common to parametrize the variational
distribution with a neural network. Orthogonal weight matrices make optimization easier in neural
networks by allowing gradients to propagate further (Saxe et al., 2013). We can exploit this fact to de-
sign an orthogonal proximity statistic for the weight matrices W of neural networks: f (W ) = W W (cid:62).
With an orthogonal initialization for the weights, this statistic enables eﬃcient optimization.

We gave four examples of proximity statistics that, together with a distance function, yield proximity
constraints. We emphasize that any function of the variational parameters f (λ) can be designed to
ameliorate issues with variational inference.

3.3 Linearizing the proximity constraint for fast proximity variational inference

pvi in Algorithm 1 requires optimizing the update equation, Equation (6), at each iteration. This rarely
has a closed-form solution and requires a separate optimization procedure that is computationally
expensive.

An alternative is to use a ﬁrst-order Taylor expansion of the proximity constraint. Let ∇d be the
gradient with respect to the second argument of the distance function, and f (˜λ) be the ﬁrst argument
to the distance. We compute the expansion around λt (the variational parameters at step t),

U (λt+1) =L(λt) + ∇L(λt)(cid:62)(λt+1 − λt) −

(λt+1 − λt)(cid:62)(λt+1 − λt)

1
2ρ

− k · (d(f (˜λ), f (λt)) + ∇d(f (˜λ), f (λt))∇f (λt)(cid:62)(λt+1 − λt)).

This linearization enjoys a closed-form solution for λt+1,

λt+1 = λt + ρ(∇L(λt) − k · (∇d(f (˜λ), f (λt))∇f (λt)).

(7)

Note that setting ˜λ to the current parameter λt removes the proximity constraint. Distance functions
are minimized at zero so their derivative is zero at that point.

Fast pvi is detailed in Algorithm 2. Unlike pvi in Algorithm 1, the update in Equation (7) does not
require an inner optimization. Fast pvi is tested in Section 4. The complexity of fast pvi is similar to
standard vi because fast pvi optimizes the elbo subject to the distance constraint in f . (The added
complexity comes from computing the derivative of f ; no inner optimization loop is required.)

6

Bad initialization

Marginal likelihood

Good initialization

Marginal likelihood

Inference method

Variational inference
Deterministic annealing
pvi, Entropy constraint
pvi, Mean/variance constraint

elbo

−504.5
−105.8
−106.0
−105.9

−464.3
−97.3
−98.3
−97.5

elbo

−117.4
−101.4
−99.9
−100.6

−105.1
−94.3
−93.7
−93.9

Table 2: Proximity variational inference improves over vi in a three-layer sigmoid belief network.
The model has three layers of 200 latent variables. We report the evidence lower bound (elbo)
and marginal likelihood on the MNIST (Larochelle and Murray, 2011) validation set. pvi and
deterministic annealing (Katahira et al., 2008) perform similarly for bad initialization; pvi improves
over deterministic annealing with good initialization.

Finally, note that fast pvi implies a global objective which varies over time. It is

Lproximity(λt+1) =Eq[log p(x, z)] − Eq[log q(λt+1)] − k · d(f (˜λ), f (λt+1)).
Because d is a distance, this remains a lower bound on the evidence, but where new variational
approximations remain close in f to previous iterations’ distributions.

4 Case studies

We developed proximity variational inference (pvi). We now empirically study pvi,2 variational
inference, and deterministic annealing (Katahira et al., 2008). To assess the robustness of inference
methods, we study both good and bad initializations. We evaluate whether the methods can recover
from poor initialization, and whether they improve on vi in well-initialized models.

We ﬁrst study sigmoid belief networks. We found vi fails to recover good solutions while pvi and
deterministic annealing recover from the initialization, and improve over vi. pvi yields further
improvements over deterministic annealing in terms of held-out values of the elbo and marginal
likelihood. We then studied a variational autoencoder model of images. Using an orthogonal proximity
statistic, pvi improves over classical vi.3

Hyperparameters. For pvi, we use the inverse Huber distance for d.4 The inverse Huber distance
penalizes smaller values than the square diﬀerence. For pvi Algorithm 2, we set the exponential moving
average decay constant for ˜λ to α = 0.9999. We set the constraint magnitude k (or temperature
parameter in deterministic annealing) to the initial absolute value of the elbo unless otherwise
speciﬁed. We explore two annealing schedules for pvi and deterministic annealing: a linear decay
and an exponential decay. For the exponential decay, the value of the magnitude at iteration t of T
total iterations is set to k · γ t
T where γ is the decay rate. We use the Adam optimizer (Kingma and
Ba, 2015).

4.1 Sigmoid belief network

The sigmoid belief network is a discrete latent variable model with layers of Bernoulli latent vari-
ables (Neal, 1992; Ranganath et al., 2015). It is commonly used to benchmark variational inference
algorithms (Mnih and Rezende, 2016). The approximate posterior is a collection of Bernoullis,
parameterized by an inference network with weights and biases. We ﬁt these variational parameters
with vi, deterministic annealing (Katahira et al., 2008), or pvi.

2Source code is available at https://github.com/altosaar/proximity_vi.
3We also compared pvi to Khan et al. (2015). Speciﬁcally, we tested pvi on the Bayesian logistic regression
model from that paper and with the same data. Because Bayesian logistic regression has a single mode, all
methods performed equally well. We note that we could not apply their algorithm to the sigmoid belief network
because it would require approximating diﬃcult iterated expectations.

4We deﬁne the inverse Huber distance d(x, y) to be |x − y| if |x − y| < 1 and 0.5(x − y)2 + 0.5 otherwise.

The constants ensure the function and its derivative are continuous at |x − y| = 1.

7

Inference method

Marginal likelihood

Variational inference
pvi, Orthogonal constraint

−94.2
−93.9

elbo

−101.0
−100.3

Table 3: Proximity variational inference with an orthogonal proximity statistic makes optimiza-
tion easier in a variational autoencoder model (Kingma and Welling, 2014; Rezende et al., 2014).
We report the held-out evidence lower bound (elbo) and estimates of the marginal likelihood on the
binarized MNIST (Larochelle and Murray, 2011) validation set.

A pervasive problem in applied vi is how to initialize parameters; thus to assess the robustness of
our method, we study good and bad initialization. For good initialization, we set the Bernoulli prior
to π = 0.5 and use the normalized initialization in Glorot and Bengio (2010) for the weights of the
generative neural network. For bad initialization, the Bernoulli prior is set to π = 0.001 and the
weights of the generative neural network are initialized to −100.
We learn the weights and biases of the model with gradient ascent. We use a step size of ρ = 10−3 and
train for 4 × 106 iterations with a batch size of 20. For pvi Algorithm 2 and deterministic annealing,
we grid search over exponential decays with rates γ ∈ {10−5, 10−6, ..., 10−10, 10−20, 10−30} and
report the best results for each algorithm. (We also explored linear decays but they did not perform as
well.) To reduce the variance of the gradients, we use the leave-one-out control variate of Mnih and
Rezende (2016) with 5 samples. (This is an extension to black-box variational inference (Ranganath
et al., 2014).)

Results on MNIST. We train a sigmoid belief network model on the binary MNIST dataset of
handwritten digits (Larochelle and Murray, 2011). For evaluation, we compute the elbo and held-out
marginal likelihood with importance sampling (5000 samples, as in Rezende et al. (2014)) on the
validation set of 104 digits. In Section 3.3 we show the results for a model with one layer of 200 latent
variables. pvi and deterministic annealing yield improvements in the held-out marginal likelihood in
comparison to vi. Table 2 displays similar results for a three-layer model with 200 latent variables
per layer. In both one and three-layer models the kl proximity statistic performs worse than the
mean/variance and entropy statistics; it requires diﬀerent decay schedules. Overall, pvi with the
entropy and mean/variance proximity statistics yields better performance than vi and deterministic
annealing.

4.2 Variational autoencoder

We study the variational autoencoder (Kingma and Welling, 2014; Rezende et al., 2014) on binary
MNIST data (Larochelle and Murray, 2011). The model has one layer of 100 Gaussian latent
variables. The inference network and generative network are chosen to have two hidden layers of size
200 with rectiﬁed linear unit activation functions. We use an orthogonal initialization for the inference
network weights. The learning rate is set to 10−3 and we run vi and pvi for 5 × 104 iterations. The
orthogonal proximity statistic changes rapidly during optimization, so we use constraint magnitudes
k ∈ {1, 10−1, 10−2, ..., 10−5}, with no decay, and report the best result. We compute the elbo and
importance-sampled marginal likelihood estimates on the validation set. In Table 3 we show that
pvi with the orthogonal proximity statistic on the weights of the inference network enables easier
optimization and improves over vi.

5 Discussion

We presented proximity variational inference, a ﬂexible method designed to avoid bad local optima
and to be robust to poor initializations. We showed that variational inference gets trapped in bad local
optima and cannot recover from poorly initialized parameters. The choice of proximity statistic f
and distance d enables the design of a variety of constraints. As examples of statistics, we gave the
entropy, kl divergence, orthogonal proximity statistic, and the mean and variance of the approximate
posterior. We evaluated our method in three models to demonstrate that it is easy to implement, readily
extendable, and leads to beneﬁcial statistical properties of variational inference algorithms.

8

Future work. Simplifying optimization is necessary for truly black-box variational inference. An
adaptive magnitude decay based on the value of the constraint should improve results (this could even
be done per-parameter). New proximity constraints are also easy to design and test. For example, the
variance of the gradients of the variational parameters is a valid proximity statistic—which can be
used to avoid variational approximations that have high variance gradients. Another set of interesting
proximity statistics are empirical statistics of the variational distribution, such as the mean, for when
analytic forms are unavailable. We also leave the design and study of constraints that admit coordinate
updates to future work.

Acknowledgments

The experiments presented in this article were performed on computational resources supported
by the Princeton Institute for Computational Science and Engineering (PICSciE), the Oﬃce of
Information Technology’s High Performance Computing Center and Visualization Laboratory at
Princeton University, Colin Raﬀel, and Dawen Liang. We are very grateful for their support. We
also thank Emtiyaz Khan and Ben Poole for helpful discussions, and Josko Plazonic for computation
support.

References

Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer New York.

Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Józefowicz, R., and Bengio, S. (2016). Generating
sentences from a continuous space. In Conference on Computational Natural Language Learning.

Boyd, S. and Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press.

Burda, Y., Grosse, R., and Salakhutdinov, R. (2015). Importance weighted autoencoders. International

Conference on Learning Representations.

Glorot, X. and Bengio, Y. (2010). Understanding the diﬃculty of training deep feedforward neural

networks. In Artiﬁcial Intelligence and Statistics, pages 249–256.

Higgins, I., Matthey, L., Glorot, X., Pal, A., Uria, B., Blundell, C., Mohamed, S., and Lerchner, A.
(2016). Early Visual Concept Learning with Unsupervised Deep Learning. ArXiv:1606.05579.

Hoﬀman, M. D., Blei, D. M., Wang, C., and Paisley, J. (2013). Stochastic variational inference.

Journal of Machine Learning Research, 14:1303–1347.

Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., and Saul, L. K. (1999). An introduction to variational

methods for graphical models. Machine Learning, 37(2):183–233.

Katahira, K., Watanabe, K., and Okada, M. (2008). Deterministic annealing variant of variational

bayes method. Journal of Physics: Conference Series, 95(1):012015.

Khan, M. E., Babanezhad, R., Lin, W., Schmidt, M., and Sugiyama, M. (2016). Faster stochastic
variational inference using proximal-gradient methods with general divergence functions.
In
Uncertainty in Artiﬁcial Intelligence, pages 319–328.

Khan, M. E., Baqué, P., Fleuret, F., and Fua, P. (2015). Kullback-leibler proximal variational inference.

In Advances in Neural Information Processing Systems, pages 3384–3392.

Kingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization.

International

Conference on Learning Representations.

Kingma, D. P. and Welling, M. (2014). Auto-encoding variational Bayes. In International Conference

on Learning Representations.

Kucukelbir, A., Ranganath, R., Gelman, A., and Blei, D. M. (2015). Automatic Variational Inference

in Stan. In Neural Information Processing Systems.

Larochelle, H. and Murray, I. (2011). The neural autoregressive distribution estimator. In Artiﬁcial

Intelligence and Statistics, volume 15, pages 29–37.

9

MacKay, D. (2003). Information Theory, Inference, and Learning Algorithms. Cambridge University

Press.

Mansinghka, V., Selsam, D., and Perov, Y. N. (2014). Venture: a higher-order probabilistic program-

ming platform with programmable inference. arXiv:1404.0099.

Mnih, A. and Rezende, D. J. (2016). Variational inference for monte carlo objectives. In International

Conference on Machine Learning, pages 2188–2196.

Nakajima, S., Sugiyama, M., Babacan, S. D., and Tomioka, R. (2013). Global analytic solution of
fully-observed variational bayesian matrix factorization. Journal of Machine Learning Research.

Neal, R. M. (1992). Connectionist learning of belief networks. Artiﬁcial intelligence, 56(1):71–113.

Ranganath, R., Gerrish, S., and Blei, D. M. (2014). Black box variational inference. In Artiﬁcial

Intelligence and Statistics.

Intelligence and Statistics.

Ranganath, R., Tang, L., Charlin, L., and Blei, D. M. (2015). Deep exponential families. In Artiﬁcial

Rezende, D. J. and Mohamed, S. (2015). Variational inference with normalizing ﬂows. In International

Conference on Machine Learning.

Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate

inference in deep generative models. In International Conference on Machine Learning.

Saxe, A. M., McClelland, J. L., and Ganguli, S. (2013). Exact solutions to the nonlinear dynamics of

learning in deep linear neural networks. arXiv preprint arXiv:1312.6120.

Spall, J. (2003). Introduction to Stochastic Search and Optimization: Estimation, Simulation, and

Control. Wiley.

Theis, L. and Hoﬀman, M. D. (2015). A trust-region method for stochastic variational inference with

applications to streaming data. Journal of Machine Learning Research.

Tran, D., Kucukelbir, A., Dieng, A. B., Rudolph, M., Liang, D., and Blei, D. M. (2016). Edward: A

library for probabilistic modeling, inference, and criticism. arXiv:1610.09787.

Wainwright, M. J. and Jordan, M. I. (2008). Graphical models, exponential families, and variational

inference. Foundations and Trends in Machine Learning, 1(1-2):1–305.

10

7
1
0
2
 
y
a
M
 
4
2
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
1
3
9
8
0
.
5
0
7
1
:
v
i
X
r
a

Proximity Variational Inference

Jaan Altosaar
Department of Physics
Princeton University
altosaar@princeton.edu

Rajesh Ranganath
Department of Computer Science
Princeton University
rajeshr@cs.princeton.edu

David M. Blei
Data Science Institute
Departments of Computer Science and Statistics
Columbia University
david.blei@columbia.edu

Abstract

Variational inference is a powerful approach for approximate posterior inference.
However, it is sensitive to initialization and can be subject to poor local optima. In
this paper, we develop proximity variational inference (pvi). pvi is a new method
for optimizing the variational objective that constrains subsequent iterates of the
variational parameters to robustify the optimization path. Consequently, pvi is
less sensitive to initialization and optimization quirks and ﬁnds better local optima.
We demonstrate our method on three proximity statistics. We study pvi on a
Bernoulli factor model and sigmoid belief network with both real and synthetic
data and compare to deterministic annealing (Katahira et al., 2008). We highlight
the ﬂexibility of pvi by designing a proximity statistic for Bayesian deep learning
models such as the variational autoencoder (Kingma and Welling, 2014; Rezende
et al., 2014). Empirically, we show that pvi consistently ﬁnds better local optima
and gives better predictive performance.

1

Introduction

Variational inference (vi) is a powerful method for probabilistic modeling. vi uses optimization to
approximate diﬃcult-to-compute conditional distributions (Jordan et al., 1999). In its modern incarna-
tion, it has scaled Bayesian computation to large data sets (Hoﬀman et al., 2013), generalized to large
classes of models (Kingma and Welling, 2014; Ranganath et al., 2014; Rezende and Mohamed, 2015),
and has been deployed as a computational engine in probabilistic programming systems (Mansinghka
et al., 2014; Kucukelbir et al., 2015; Tran et al., 2016).
Despite these signiﬁcant advances, however, vi has drawbacks. For one, it tries to iteratively solve a
diﬃcult nonconvex optimization problem and its objective contains many local optima. Consequently,
vi is sensitive to initialization and easily gets stuck in a poor solution. We develop a new optimization
method for vi. Our method is less sensitive to initialization and ﬁnds better optima.

Consider a probability model p(z, x) and the goal of calculating the posterior p(z | x). The idea behind
vi is to posit a family of distributions over the hidden variables q(z; λ) and then ﬁt the variational
parameters λ to minimize the Kullback-Leibler (kl) divergence between the approximating family
and the exact posterior, kl(q(z; λ)||p(z | x)). The kl is not tractable so vi optimizes a proxy. That
proxy is the evidence lower bound (elbo),

L(λ) = E[log p(z, x)] − E[log q(z; λ)],

(1)

(a) Variational inference

(b) Proximity variational inference, Algorithm 2

Figure 1: Proximity variational inference (pvi) is robust to bad initialization. We study a
Bernoulli factor model. Model parameters are randomly initialized on a ring around the known true
parameters (in red) used to generate the data. The arrows start at these parameter initializations and
end at the ﬁnal parameter estimates (shown as green dots). (a) Variational inference with gradient
ascent suﬀers from multiple local optima and cannot reliably recover the truth. (b) pvi with an entropy
proximity statistic reliably infers the true parameters using Algorithm 2.

where expectations are taken with respect to q(z; λ). Maximizing the elbo with respect to λ is
equivalent to minimizing the kl divergence.

The issues around vi stem from the elbo and the iterative algorithms used to optimize it. When
the algorithm zeroes (or nearly zeroes) some of the support of q(z; λ), it becomes hard to later
“escape,” i.e., to add support for the conﬁgurations of the latent variables that have been assigned zero
probability (MacKay, 2003; Burda et al., 2015). This leads to poor local optima and to sensitivity to
the starting point, where a misguided initialization will lead to such optima. These problems happen
in both gradient-based and coordinate ascent methods. We address these issues with proximity
variational inference (pvi), a variational inference algorithm that is speciﬁcally designed to avoid
poor local optima and to be robust to diﬀerent initializations.

pvi builds on the proximity perspective of gradient ascent. The proximity perspective views each step
of gradient ascent as a constrained minimization of a Taylor expansion of the objective around the
previous step’s parameter (Spall, 2003; Boyd and Vandenberghe, 2004). The constraint, a proximity
constraint, enforces that the next point should be inside a Euclidean ball of the previous. The step
size relates to the size of that ball.

In vi, a constraint on the Euclidean distance means that all dimensions of the variational parameters are
equally constrained. We posit that this leads to problems; some dimensions need more regularization
than others. For example, consider a variational distribution that is Gaussian. A good optimization
will change the variance parameter more slowly than the mean parameter to prevent rapid changes
to the support. The Euclidean constraint cannot enforce this. Furthermore, the constraints enforced
by gradient descent are transient; the constraints are relative to the previous iterate—one poor move
during the optimization can lead to permanent optimization problems.

To this end, pvi uses proximity constraints that are more meaningful to variational inference and to
optimization of probability parameters. A constraint is deﬁned using a proximity statistic and distance
function. As one example we consider a constraint based on the entropy proximity statistic. This
limits the change in entropy of the variational approximation from one step to the next. Consider
again a Gaussian approximation. The entropy is a function of the variance alone and thus the entropy
constraint counters the pathologies induced by the Euclidean proximity constraint. We also study
constraints built from other proximity statistics, such as those that penalize the rapid changes in the
mean and variance of the approximate posterior.

Figure 1 provides an illustration of the advantages of pvi. Our goal is to estimate the parameters of a
factor analysis model with variational inference, i.e., using the posterior expectation under a ﬁtted
2

variational distribution. We run variational inference 100 times, each time initializing the estimates
(the model parameters) to a diﬀerent position on a ring around the truth.

In the ﬁgure, red points indicate the true value. The start locations of the green arrows indicate the
initialized estimates. Green points indicate the ﬁnal estimates, after optimizing from the initial points.
Panel (a) shows that optimizing the standard elbo with gradients leads to poor local optima and
misplaced estimates. Panel (b) illustrates that regardless of the initialization, pvi with an entropy
proximity statistic ﬁnd estimates that are close to the true value.

The rest of the paper is organized as follows. Section 2 reviews variational inference and the proximity
perspective of gradient optimization. Section 3 derives pvi; we develop four proximity constraints
and two algorithms for optimizing the elbo. We study three models in Section 4: a Bernoulli factor
model, a sigmoid belief network (Mnih and Rezende, 2016), and a variational autoencoder (Kingma
and Welling, 2014; Rezende et al., 2014). On the MNIST data set, pvi generally outperforms classical
methods for variational inference.

Related work. Recent work has proposed several related algorithms. Khan et al. (2015) and Theis
and Hoﬀman (2015) develop a method to optimize the elbo that imposes a soft limit on the change
in kl of consecutive variational approximations. This is equivalent to pvi with identity proximity
statistics and a kl distance function. Khan et al. (2016) extend both prior works to other divergence
functions. Their general approach is equivalent to pvi identity proximity statistics and distance
functions given by strongly-convex divergences. Compared to prior work, pvi generalizes to a broader
class of proximity statistics. We develop proximity statistics based on entropy, kl, orthogonal weight
matrices, and the mean and variance of the variational approximation.

The problem of model pruning in variational inference has also been studied and analytically solved
in a matrix factorization model in Nakajima et al. (2013)—this method is model-speciﬁc, whereas pvi
applies to a much broader class of latent variable models. Finally, deterministic annealing (Katahira
et al., 2008) consists of adding a temperature parameter to the entropy term in the elbo that is
annealed to one during inference. This is similar to pvi with the entropy proximity statistic which
keeps the entropy stable across iterations. Deterministic annealing enforces global penalization of
low-entropy conﬁgurations of latent variables rather than the smooth constraint used in pvi, and
cannot accommodate the range of proximity statistics we design in this work.

2 Variational inference

Consider a model p(x, z), where x is the observed data and z are the latent variables. As described
in Section 1, vi posits an approximating family q(z; λ) and maximizes the elbo in Equation (1).
Solving this optimization is equivalent to ﬁnding the variational approximation that minimizes kl
divergence to the exact posterior (Jordan et al., 1999; Wainwright and Jordan, 2008).

2.1 Gradient ascent has Euclidean proximity

Gradient ascent maximizes the elbo by repeatedly following its gradient. One view of this algorithm
is that it repeatedly maximizes the linearized elbo subject to a proximity constraint on the current
variational parameter (Spall, 2003). The name ‘proximity’ comes from constraining subsequent
parameters to remain close in the proximity statistic. In gradient ascent, the proximity statistic for
the variational parameters is the identity function f (λ) = λ, and the distance function is the square
diﬀerence.

Let λt be the variational parameters at iteration t and ρ be a constant. To obtain the next iterate λt+1,
gradient ascent maximizes the linearized elbo,

U (λt+1) =L(λt) + ∇L(λt)(cid:62)(λt+1 − λt) −

(λt+1 − λt)(cid:62)(λt+1 − λt).

(2)

1
2ρ

Speciﬁcally, this is the linearized elbo around λt subject to λt+1 being close in squared Euclidean
distance to λt.
Finding the λt+1 which maximizes Equation (2) yields

λt+1 = λt + ρ∇L(λt).

(3)

3

Algorithm 1: Proximity variational inference
Input: Initial parameters λ0, proximity statistic f (λ), distance function d
Output: Parameters λ of variational q(λ) that maximize the elbo objective
while L not converged do
λt+1 ← λt + Noise
while U not converged do

Update λt+1 ← λt+1 + ρ∇λU (λt+1)

end
λt ← λt+1

end
return λ

This is the familiar gradient ascent update with a step size of ρ. The step size ρ controls the radius
of the Euclidean ball which demarcates valid next steps for the parameters. Note that the Euclidean
constraint between subsequent iterates is implicit in all gradient ascent algorithms.

2.2 An example where variational inference fails

We study a setting where variational inference suﬀers from poor local optima. Consider a factor
model, with latent variables zik ∼ Bernoulli(π) and data xi ∼ Gaussian (cid:0)µ = (cid:80)
k zikµk, σ2 = 1(cid:1).
This is a “feature” model of real-valued data x; when one of the features is on (i.e., zik = 1), the ith
mean shifts according the that feature’s mean parameter (i.e., µk). Thus the binary latent variables
zik control which cluster means µk contribute to the distribution of xi.
The Bernoulli prior is parametrized by π; we choose a Bernoulli approximate posterior
q(zk; λk) = Bernoulli(λk). A common approach to vi is coordinate ascent (Bishop, 2006), where
we iteratively optimize each variational parameter. The optimal variational parameter for zik is




λik ∝ exp

E−zik

−






1
2σ2 (xi −

(cid:88)

j

zijµj)2








.

We can use this update in a variational EM setting. The corresponding gradient for µk is



∂L
∂µk

= −

1
σ2

(cid:88)

i

−xiλik + λikµk + λik

λijµj

 .



(cid:88)

j(cid:54)=k

(4)

(5)

Meditating on these two equations reveals a deﬁciency in mean-ﬁeld variational inference. First, if
the mean parameters µ are initialized far from the data then q∗(zik = 1) will be very small. The
reason is in Equation (4), where the squared diﬀerence between the data xi and the expected cluster
mean will be large and negative. Second, when the probability of cluster assignment is close to zero,
λik is small. This means that the norm of the gradient in Equation (5) will be small. Consequently,
learning will be slow. We see this phenomenon in Figure 1 (a). Variational inference arrives at poor
local optima and does not always recover the correct cluster means µk.

3 Proximity variational inference

We now develop proximity variational inference (pvi), a variational inference method that is robust
to initialization and can consistently reach good local optima (Section 3.1). pvi alters the notion of
proximity. We further restrict the iterates of the variational parameters by deforming the Euclidean
ball implicit in classical gradient ascent. This is done by choosing proximity statistics that are not
the identity function, and distance functions that are diﬀerent than the square diﬀerence. These
design choices help guide the variational parameters away from poor local optima (Section 3.2). One
drawback of the proximity perspective is that it requires an inner optimization at each step of the outer
optimization. We use a Taylor expansion to avoid this computational burden (Section 3.3).

4

Algorithm 2: Fast proximity variational inference
Input: Initial parameters λ0, adaptive learning rate optimizer, proximity statistic f (λ), distance d
Output: Parameters λ of the variational distribution q(λ) that maximize the elbo objective
while Lproximity not converged do

λt+1 = λt + ρ(∇L(λt) − k · (∇d(f (˜λ), f (λt))∇f (λt)).
˜λ = α˜λ + (1 − α)λt+1

end
return λ

3.1 Proximity constraints for variational inference

pvi enriches the proximity constraint in gradient ascent of the elbo. We want to develop constraints
on the iterates λt to counter the pathologies of standard variational inference.
Let f (·) be a proximity statistic, and let d be a diﬀerentiable distance function that measures distance
between proximity statistic iterates. A proximity constraint is the combination of a distance function
d applied to a proximity statistic f . (Recall that in classical gradient ascent, the Euclidean proximity
constraint uses the identity as the proximity statistic and the square diﬀerence as the distance.) Let k
be the scalar magnitude of the proximity constraint. We deﬁne the proximity update equation for the
variational parameters λt+1 to be

U (λt+1) =L(λt) + ∇L(λt)(cid:62)(λt+1 − λt)

−

1
2ρ

(λt+1 − λt)(cid:62)(λt+1 − λt) − k · d(f (˜λ), f (λt+1)),

(6)

where ˜λ is the variational parameter to which we are measuring closeness. In gradient ascent, this
is the previous parameter ˜λ = λt, but our construction can enforce proximity to more than just the
previous parameters. For example, we can set ˜λ to be an exponential moving average1—this adds
robustness to one-update optimization missteps.

The next parameters are found by maximizing Equation (6). This enforces that the variational
parameters between updates will remain close in the proximity statistic f (λ). For example, f (λ)
might be the entropy of the variational approximation; this can avoid zeroing out some of its support.
This procedure is detailed in Algorithm 1. The magnitude k of the constraint is a hyperparameter.
The inner optimization loop optimizes the update equation U at each step.

3.2 Proximity statistics for variational inference

We describe four proximity statistics f (λ) appropriate for variational inference. Together with a
distance function, these proximity statistics yield proximity constraints. (We study them in Sec-
tion 4.)

Entropy proximity statistic. Consider a constraint built from the entropy proximity statistic, f (λ) =
H(q(z; λ)). Informally, the entropy measures the amount of randomness present in a distribution.
High entropy distributions look more uniform across their support; low entropy distributions are
peaky.

Using the entropy in Equation (6) constrains all updates to have entropy close to their previous
update. When the variational distributions are initialized with large entropy, this statistic balances the
“zero-forcing” issue that is intrinsic to variational inference (MacKay, 2003). Figure 1 demonstrates
how pvi with an entropy constraint can correct this pathology.

kl proximity statistic. We can rewrite the elbo to include the kl between the approximate posterior
and the prior (Kingma and Welling, 2014),

L(λ) = E[log p(x | z)] − KL(q(z | x; λ)||p(z)).

1The exponential moving average of a variable λ is denoted ˜λ and is updated according to ˜λ ← α˜λ+(1−α)λ,

where α is a decay close to one.

5

Bad initialization

Marginal likelihood

Good initialization

Marginal likelihood

Inference method

Variational inference
Deterministic annealing
pvi, Entropy constraint
pvi, Mean/variance constraint

elbo

−663.7
−119.4
−118.0
−119.9

−636.7
−110.2
−110.0
−111.1

elbo

−122.1
−116.5
−114.1
−115.7

−114.1
−108.7
−107.5
−108.3

Table 1: Proximity variational inference improves on deterministic annealing (Katahira et al.,
2008) and vi in a one-layer sigmoid belief network. We report validation evidence lower bound
(elbo) and marginal likelihood on the binary MNIST dataset (Larochelle and Murray, 2011). The
model has one stochastic layer of 200 latent variables. pvi outperforms the classical variational
inference algorithm and deterministic annealing (Katahira et al., 2008).

Flexible models tend to minimize the kl divergence too quickly and get stuck in poor optima (Bowman
et al., 2016; Higgins et al., 2016). The choice of kl as a proximity statistic prevents the kl from
being optimized too quickly relative to the likelihood.

Mean/variance proximity statistic. A common theme in the problems with variational inference is
that the bulk of the probability mass can quickly move to a point where that dimension will no longer
be explored (Burda et al., 2015). One way to address this is to restrict the mean and variance of the
variational approximation to change slowly during optimization. This constraint only allows higher
order moments of the variational approximation to change rapidly. The mean µ = Eq(z;λ)[z] and
variance Var(z) = Eq(z;λ)[(z − µ)2] are the statistics f (λ) we constrain.
Orthogonal proximity statistic. In Bayesian deep learning models such as the variational autoen-
coder (Kingma and Welling, 2014; Rezende et al., 2014) it is common to parametrize the variational
distribution with a neural network. Orthogonal weight matrices make optimization easier in neural
networks by allowing gradients to propagate further (Saxe et al., 2013). We can exploit this fact to de-
sign an orthogonal proximity statistic for the weight matrices W of neural networks: f (W ) = W W (cid:62).
With an orthogonal initialization for the weights, this statistic enables eﬃcient optimization.

We gave four examples of proximity statistics that, together with a distance function, yield proximity
constraints. We emphasize that any function of the variational parameters f (λ) can be designed to
ameliorate issues with variational inference.

3.3 Linearizing the proximity constraint for fast proximity variational inference

pvi in Algorithm 1 requires optimizing the update equation, Equation (6), at each iteration. This rarely
has a closed-form solution and requires a separate optimization procedure that is computationally
expensive.

An alternative is to use a ﬁrst-order Taylor expansion of the proximity constraint. Let ∇d be the
gradient with respect to the second argument of the distance function, and f (˜λ) be the ﬁrst argument
to the distance. We compute the expansion around λt (the variational parameters at step t),

U (λt+1) =L(λt) + ∇L(λt)(cid:62)(λt+1 − λt) −

(λt+1 − λt)(cid:62)(λt+1 − λt)

1
2ρ

− k · (d(f (˜λ), f (λt)) + ∇d(f (˜λ), f (λt))∇f (λt)(cid:62)(λt+1 − λt)).

This linearization enjoys a closed-form solution for λt+1,

λt+1 = λt + ρ(∇L(λt) − k · (∇d(f (˜λ), f (λt))∇f (λt)).

(7)

Note that setting ˜λ to the current parameter λt removes the proximity constraint. Distance functions
are minimized at zero so their derivative is zero at that point.

Fast pvi is detailed in Algorithm 2. Unlike pvi in Algorithm 1, the update in Equation (7) does not
require an inner optimization. Fast pvi is tested in Section 4. The complexity of fast pvi is similar to
standard vi because fast pvi optimizes the elbo subject to the distance constraint in f . (The added
complexity comes from computing the derivative of f ; no inner optimization loop is required.)

6

Bad initialization

Marginal likelihood

Good initialization

Marginal likelihood

Inference method

Variational inference
Deterministic annealing
pvi, Entropy constraint
pvi, Mean/variance constraint

elbo

−504.5
−105.8
−106.0
−105.9

−464.3
−97.3
−98.3
−97.5

elbo

−117.4
−101.4
−99.9
−100.6

−105.1
−94.3
−93.7
−93.9

Table 2: Proximity variational inference improves over vi in a three-layer sigmoid belief network.
The model has three layers of 200 latent variables. We report the evidence lower bound (elbo)
and marginal likelihood on the MNIST (Larochelle and Murray, 2011) validation set. pvi and
deterministic annealing (Katahira et al., 2008) perform similarly for bad initialization; pvi improves
over deterministic annealing with good initialization.

Finally, note that fast pvi implies a global objective which varies over time. It is

Lproximity(λt+1) =Eq[log p(x, z)] − Eq[log q(λt+1)] − k · d(f (˜λ), f (λt+1)).
Because d is a distance, this remains a lower bound on the evidence, but where new variational
approximations remain close in f to previous iterations’ distributions.

4 Case studies

We developed proximity variational inference (pvi). We now empirically study pvi,2 variational
inference, and deterministic annealing (Katahira et al., 2008). To assess the robustness of inference
methods, we study both good and bad initializations. We evaluate whether the methods can recover
from poor initialization, and whether they improve on vi in well-initialized models.

We ﬁrst study sigmoid belief networks. We found vi fails to recover good solutions while pvi and
deterministic annealing recover from the initialization, and improve over vi. pvi yields further
improvements over deterministic annealing in terms of held-out values of the elbo and marginal
likelihood. We then studied a variational autoencoder model of images. Using an orthogonal proximity
statistic, pvi improves over classical vi.3

Hyperparameters. For pvi, we use the inverse Huber distance for d.4 The inverse Huber distance
penalizes smaller values than the square diﬀerence. For pvi Algorithm 2, we set the exponential moving
average decay constant for ˜λ to α = 0.9999. We set the constraint magnitude k (or temperature
parameter in deterministic annealing) to the initial absolute value of the elbo unless otherwise
speciﬁed. We explore two annealing schedules for pvi and deterministic annealing: a linear decay
and an exponential decay. For the exponential decay, the value of the magnitude at iteration t of T
total iterations is set to k · γ t
T where γ is the decay rate. We use the Adam optimizer (Kingma and
Ba, 2015).

4.1 Sigmoid belief network

The sigmoid belief network is a discrete latent variable model with layers of Bernoulli latent vari-
ables (Neal, 1992; Ranganath et al., 2015). It is commonly used to benchmark variational inference
algorithms (Mnih and Rezende, 2016). The approximate posterior is a collection of Bernoullis,
parameterized by an inference network with weights and biases. We ﬁt these variational parameters
with vi, deterministic annealing (Katahira et al., 2008), or pvi.

2Source code is available at https://github.com/altosaar/proximity_vi.
3We also compared pvi to Khan et al. (2015). Speciﬁcally, we tested pvi on the Bayesian logistic regression
model from that paper and with the same data. Because Bayesian logistic regression has a single mode, all
methods performed equally well. We note that we could not apply their algorithm to the sigmoid belief network
because it would require approximating diﬃcult iterated expectations.

4We deﬁne the inverse Huber distance d(x, y) to be |x − y| if |x − y| < 1 and 0.5(x − y)2 + 0.5 otherwise.

The constants ensure the function and its derivative are continuous at |x − y| = 1.

7

Inference method

Marginal likelihood

Variational inference
pvi, Orthogonal constraint

−94.2
−93.9

elbo

−101.0
−100.3

Table 3: Proximity variational inference with an orthogonal proximity statistic makes optimiza-
tion easier in a variational autoencoder model (Kingma and Welling, 2014; Rezende et al., 2014).
We report the held-out evidence lower bound (elbo) and estimates of the marginal likelihood on the
binarized MNIST (Larochelle and Murray, 2011) validation set.

A pervasive problem in applied vi is how to initialize parameters; thus to assess the robustness of
our method, we study good and bad initialization. For good initialization, we set the Bernoulli prior
to π = 0.5 and use the normalized initialization in Glorot and Bengio (2010) for the weights of the
generative neural network. For bad initialization, the Bernoulli prior is set to π = 0.001 and the
weights of the generative neural network are initialized to −100.
We learn the weights and biases of the model with gradient ascent. We use a step size of ρ = 10−3 and
train for 4 × 106 iterations with a batch size of 20. For pvi Algorithm 2 and deterministic annealing,
we grid search over exponential decays with rates γ ∈ {10−5, 10−6, ..., 10−10, 10−20, 10−30} and
report the best results for each algorithm. (We also explored linear decays but they did not perform as
well.) To reduce the variance of the gradients, we use the leave-one-out control variate of Mnih and
Rezende (2016) with 5 samples. (This is an extension to black-box variational inference (Ranganath
et al., 2014).)

Results on MNIST. We train a sigmoid belief network model on the binary MNIST dataset of
handwritten digits (Larochelle and Murray, 2011). For evaluation, we compute the elbo and held-out
marginal likelihood with importance sampling (5000 samples, as in Rezende et al. (2014)) on the
validation set of 104 digits. In Section 3.3 we show the results for a model with one layer of 200 latent
variables. pvi and deterministic annealing yield improvements in the held-out marginal likelihood in
comparison to vi. Table 2 displays similar results for a three-layer model with 200 latent variables
per layer. In both one and three-layer models the kl proximity statistic performs worse than the
mean/variance and entropy statistics; it requires diﬀerent decay schedules. Overall, pvi with the
entropy and mean/variance proximity statistics yields better performance than vi and deterministic
annealing.

4.2 Variational autoencoder

We study the variational autoencoder (Kingma and Welling, 2014; Rezende et al., 2014) on binary
MNIST data (Larochelle and Murray, 2011). The model has one layer of 100 Gaussian latent
variables. The inference network and generative network are chosen to have two hidden layers of size
200 with rectiﬁed linear unit activation functions. We use an orthogonal initialization for the inference
network weights. The learning rate is set to 10−3 and we run vi and pvi for 5 × 104 iterations. The
orthogonal proximity statistic changes rapidly during optimization, so we use constraint magnitudes
k ∈ {1, 10−1, 10−2, ..., 10−5}, with no decay, and report the best result. We compute the elbo and
importance-sampled marginal likelihood estimates on the validation set. In Table 3 we show that
pvi with the orthogonal proximity statistic on the weights of the inference network enables easier
optimization and improves over vi.

5 Discussion

We presented proximity variational inference, a ﬂexible method designed to avoid bad local optima
and to be robust to poor initializations. We showed that variational inference gets trapped in bad local
optima and cannot recover from poorly initialized parameters. The choice of proximity statistic f
and distance d enables the design of a variety of constraints. As examples of statistics, we gave the
entropy, kl divergence, orthogonal proximity statistic, and the mean and variance of the approximate
posterior. We evaluated our method in three models to demonstrate that it is easy to implement, readily
extendable, and leads to beneﬁcial statistical properties of variational inference algorithms.

8

Future work. Simplifying optimization is necessary for truly black-box variational inference. An
adaptive magnitude decay based on the value of the constraint should improve results (this could even
be done per-parameter). New proximity constraints are also easy to design and test. For example, the
variance of the gradients of the variational parameters is a valid proximity statistic—which can be
used to avoid variational approximations that have high variance gradients. Another set of interesting
proximity statistics are empirical statistics of the variational distribution, such as the mean, for when
analytic forms are unavailable. We also leave the design and study of constraints that admit coordinate
updates to future work.

Acknowledgments

The experiments presented in this article were performed on computational resources supported
by the Princeton Institute for Computational Science and Engineering (PICSciE), the Oﬃce of
Information Technology’s High Performance Computing Center and Visualization Laboratory at
Princeton University, Colin Raﬀel, and Dawen Liang. We are very grateful for their support. We
also thank Emtiyaz Khan and Ben Poole for helpful discussions, and Josko Plazonic for computation
support.

References

Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer New York.

Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Józefowicz, R., and Bengio, S. (2016). Generating
sentences from a continuous space. In Conference on Computational Natural Language Learning.

Boyd, S. and Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press.

Burda, Y., Grosse, R., and Salakhutdinov, R. (2015). Importance weighted autoencoders. International

Conference on Learning Representations.

Glorot, X. and Bengio, Y. (2010). Understanding the diﬃculty of training deep feedforward neural

networks. In Artiﬁcial Intelligence and Statistics, pages 249–256.

Higgins, I., Matthey, L., Glorot, X., Pal, A., Uria, B., Blundell, C., Mohamed, S., and Lerchner, A.
(2016). Early Visual Concept Learning with Unsupervised Deep Learning. ArXiv:1606.05579.

Hoﬀman, M. D., Blei, D. M., Wang, C., and Paisley, J. (2013). Stochastic variational inference.

Journal of Machine Learning Research, 14:1303–1347.

Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., and Saul, L. K. (1999). An introduction to variational

methods for graphical models. Machine Learning, 37(2):183–233.

Katahira, K., Watanabe, K., and Okada, M. (2008). Deterministic annealing variant of variational

bayes method. Journal of Physics: Conference Series, 95(1):012015.

Khan, M. E., Babanezhad, R., Lin, W., Schmidt, M., and Sugiyama, M. (2016). Faster stochastic
variational inference using proximal-gradient methods with general divergence functions.
In
Uncertainty in Artiﬁcial Intelligence, pages 319–328.

Khan, M. E., Baqué, P., Fleuret, F., and Fua, P. (2015). Kullback-leibler proximal variational inference.

In Advances in Neural Information Processing Systems, pages 3384–3392.

Kingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization.

International

Conference on Learning Representations.

Kingma, D. P. and Welling, M. (2014). Auto-encoding variational Bayes. In International Conference

on Learning Representations.

Kucukelbir, A., Ranganath, R., Gelman, A., and Blei, D. M. (2015). Automatic Variational Inference

in Stan. In Neural Information Processing Systems.

Larochelle, H. and Murray, I. (2011). The neural autoregressive distribution estimator. In Artiﬁcial

Intelligence and Statistics, volume 15, pages 29–37.

9

MacKay, D. (2003). Information Theory, Inference, and Learning Algorithms. Cambridge University

Press.

Mansinghka, V., Selsam, D., and Perov, Y. N. (2014). Venture: a higher-order probabilistic program-

ming platform with programmable inference. arXiv:1404.0099.

Mnih, A. and Rezende, D. J. (2016). Variational inference for monte carlo objectives. In International

Conference on Machine Learning, pages 2188–2196.

Nakajima, S., Sugiyama, M., Babacan, S. D., and Tomioka, R. (2013). Global analytic solution of
fully-observed variational bayesian matrix factorization. Journal of Machine Learning Research.

Neal, R. M. (1992). Connectionist learning of belief networks. Artiﬁcial intelligence, 56(1):71–113.

Ranganath, R., Gerrish, S., and Blei, D. M. (2014). Black box variational inference. In Artiﬁcial

Intelligence and Statistics.

Intelligence and Statistics.

Ranganath, R., Tang, L., Charlin, L., and Blei, D. M. (2015). Deep exponential families. In Artiﬁcial

Rezende, D. J. and Mohamed, S. (2015). Variational inference with normalizing ﬂows. In International

Conference on Machine Learning.

Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate

inference in deep generative models. In International Conference on Machine Learning.

Saxe, A. M., McClelland, J. L., and Ganguli, S. (2013). Exact solutions to the nonlinear dynamics of

learning in deep linear neural networks. arXiv preprint arXiv:1312.6120.

Spall, J. (2003). Introduction to Stochastic Search and Optimization: Estimation, Simulation, and

Control. Wiley.

Theis, L. and Hoﬀman, M. D. (2015). A trust-region method for stochastic variational inference with

applications to streaming data. Journal of Machine Learning Research.

Tran, D., Kucukelbir, A., Dieng, A. B., Rudolph, M., Liang, D., and Blei, D. M. (2016). Edward: A

library for probabilistic modeling, inference, and criticism. arXiv:1610.09787.

Wainwright, M. J. and Jordan, M. I. (2008). Graphical models, exponential families, and variational

inference. Foundations and Trends in Machine Learning, 1(1-2):1–305.

10

7
1
0
2
 
y
a
M
 
4
2
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
1
3
9
8
0
.
5
0
7
1
:
v
i
X
r
a

Proximity Variational Inference

Jaan Altosaar
Department of Physics
Princeton University
altosaar@princeton.edu

Rajesh Ranganath
Department of Computer Science
Princeton University
rajeshr@cs.princeton.edu

David M. Blei
Data Science Institute
Departments of Computer Science and Statistics
Columbia University
david.blei@columbia.edu

Abstract

Variational inference is a powerful approach for approximate posterior inference.
However, it is sensitive to initialization and can be subject to poor local optima. In
this paper, we develop proximity variational inference (pvi). pvi is a new method
for optimizing the variational objective that constrains subsequent iterates of the
variational parameters to robustify the optimization path. Consequently, pvi is
less sensitive to initialization and optimization quirks and ﬁnds better local optima.
We demonstrate our method on three proximity statistics. We study pvi on a
Bernoulli factor model and sigmoid belief network with both real and synthetic
data and compare to deterministic annealing (Katahira et al., 2008). We highlight
the ﬂexibility of pvi by designing a proximity statistic for Bayesian deep learning
models such as the variational autoencoder (Kingma and Welling, 2014; Rezende
et al., 2014). Empirically, we show that pvi consistently ﬁnds better local optima
and gives better predictive performance.

1

Introduction

Variational inference (vi) is a powerful method for probabilistic modeling. vi uses optimization to
approximate diﬃcult-to-compute conditional distributions (Jordan et al., 1999). In its modern incarna-
tion, it has scaled Bayesian computation to large data sets (Hoﬀman et al., 2013), generalized to large
classes of models (Kingma and Welling, 2014; Ranganath et al., 2014; Rezende and Mohamed, 2015),
and has been deployed as a computational engine in probabilistic programming systems (Mansinghka
et al., 2014; Kucukelbir et al., 2015; Tran et al., 2016).
Despite these signiﬁcant advances, however, vi has drawbacks. For one, it tries to iteratively solve a
diﬃcult nonconvex optimization problem and its objective contains many local optima. Consequently,
vi is sensitive to initialization and easily gets stuck in a poor solution. We develop a new optimization
method for vi. Our method is less sensitive to initialization and ﬁnds better optima.

Consider a probability model p(z, x) and the goal of calculating the posterior p(z | x). The idea behind
vi is to posit a family of distributions over the hidden variables q(z; λ) and then ﬁt the variational
parameters λ to minimize the Kullback-Leibler (kl) divergence between the approximating family
and the exact posterior, kl(q(z; λ)||p(z | x)). The kl is not tractable so vi optimizes a proxy. That
proxy is the evidence lower bound (elbo),

L(λ) = E[log p(z, x)] − E[log q(z; λ)],

(1)

(a) Variational inference

(b) Proximity variational inference, Algorithm 2

Figure 1: Proximity variational inference (pvi) is robust to bad initialization. We study a
Bernoulli factor model. Model parameters are randomly initialized on a ring around the known true
parameters (in red) used to generate the data. The arrows start at these parameter initializations and
end at the ﬁnal parameter estimates (shown as green dots). (a) Variational inference with gradient
ascent suﬀers from multiple local optima and cannot reliably recover the truth. (b) pvi with an entropy
proximity statistic reliably infers the true parameters using Algorithm 2.

where expectations are taken with respect to q(z; λ). Maximizing the elbo with respect to λ is
equivalent to minimizing the kl divergence.

The issues around vi stem from the elbo and the iterative algorithms used to optimize it. When
the algorithm zeroes (or nearly zeroes) some of the support of q(z; λ), it becomes hard to later
“escape,” i.e., to add support for the conﬁgurations of the latent variables that have been assigned zero
probability (MacKay, 2003; Burda et al., 2015). This leads to poor local optima and to sensitivity to
the starting point, where a misguided initialization will lead to such optima. These problems happen
in both gradient-based and coordinate ascent methods. We address these issues with proximity
variational inference (pvi), a variational inference algorithm that is speciﬁcally designed to avoid
poor local optima and to be robust to diﬀerent initializations.

pvi builds on the proximity perspective of gradient ascent. The proximity perspective views each step
of gradient ascent as a constrained minimization of a Taylor expansion of the objective around the
previous step’s parameter (Spall, 2003; Boyd and Vandenberghe, 2004). The constraint, a proximity
constraint, enforces that the next point should be inside a Euclidean ball of the previous. The step
size relates to the size of that ball.

In vi, a constraint on the Euclidean distance means that all dimensions of the variational parameters are
equally constrained. We posit that this leads to problems; some dimensions need more regularization
than others. For example, consider a variational distribution that is Gaussian. A good optimization
will change the variance parameter more slowly than the mean parameter to prevent rapid changes
to the support. The Euclidean constraint cannot enforce this. Furthermore, the constraints enforced
by gradient descent are transient; the constraints are relative to the previous iterate—one poor move
during the optimization can lead to permanent optimization problems.

To this end, pvi uses proximity constraints that are more meaningful to variational inference and to
optimization of probability parameters. A constraint is deﬁned using a proximity statistic and distance
function. As one example we consider a constraint based on the entropy proximity statistic. This
limits the change in entropy of the variational approximation from one step to the next. Consider
again a Gaussian approximation. The entropy is a function of the variance alone and thus the entropy
constraint counters the pathologies induced by the Euclidean proximity constraint. We also study
constraints built from other proximity statistics, such as those that penalize the rapid changes in the
mean and variance of the approximate posterior.

Figure 1 provides an illustration of the advantages of pvi. Our goal is to estimate the parameters of a
factor analysis model with variational inference, i.e., using the posterior expectation under a ﬁtted
2

variational distribution. We run variational inference 100 times, each time initializing the estimates
(the model parameters) to a diﬀerent position on a ring around the truth.

In the ﬁgure, red points indicate the true value. The start locations of the green arrows indicate the
initialized estimates. Green points indicate the ﬁnal estimates, after optimizing from the initial points.
Panel (a) shows that optimizing the standard elbo with gradients leads to poor local optima and
misplaced estimates. Panel (b) illustrates that regardless of the initialization, pvi with an entropy
proximity statistic ﬁnd estimates that are close to the true value.

The rest of the paper is organized as follows. Section 2 reviews variational inference and the proximity
perspective of gradient optimization. Section 3 derives pvi; we develop four proximity constraints
and two algorithms for optimizing the elbo. We study three models in Section 4: a Bernoulli factor
model, a sigmoid belief network (Mnih and Rezende, 2016), and a variational autoencoder (Kingma
and Welling, 2014; Rezende et al., 2014). On the MNIST data set, pvi generally outperforms classical
methods for variational inference.

Related work. Recent work has proposed several related algorithms. Khan et al. (2015) and Theis
and Hoﬀman (2015) develop a method to optimize the elbo that imposes a soft limit on the change
in kl of consecutive variational approximations. This is equivalent to pvi with identity proximity
statistics and a kl distance function. Khan et al. (2016) extend both prior works to other divergence
functions. Their general approach is equivalent to pvi identity proximity statistics and distance
functions given by strongly-convex divergences. Compared to prior work, pvi generalizes to a broader
class of proximity statistics. We develop proximity statistics based on entropy, kl, orthogonal weight
matrices, and the mean and variance of the variational approximation.

The problem of model pruning in variational inference has also been studied and analytically solved
in a matrix factorization model in Nakajima et al. (2013)—this method is model-speciﬁc, whereas pvi
applies to a much broader class of latent variable models. Finally, deterministic annealing (Katahira
et al., 2008) consists of adding a temperature parameter to the entropy term in the elbo that is
annealed to one during inference. This is similar to pvi with the entropy proximity statistic which
keeps the entropy stable across iterations. Deterministic annealing enforces global penalization of
low-entropy conﬁgurations of latent variables rather than the smooth constraint used in pvi, and
cannot accommodate the range of proximity statistics we design in this work.

2 Variational inference

Consider a model p(x, z), where x is the observed data and z are the latent variables. As described
in Section 1, vi posits an approximating family q(z; λ) and maximizes the elbo in Equation (1).
Solving this optimization is equivalent to ﬁnding the variational approximation that minimizes kl
divergence to the exact posterior (Jordan et al., 1999; Wainwright and Jordan, 2008).

2.1 Gradient ascent has Euclidean proximity

Gradient ascent maximizes the elbo by repeatedly following its gradient. One view of this algorithm
is that it repeatedly maximizes the linearized elbo subject to a proximity constraint on the current
variational parameter (Spall, 2003). The name ‘proximity’ comes from constraining subsequent
parameters to remain close in the proximity statistic. In gradient ascent, the proximity statistic for
the variational parameters is the identity function f (λ) = λ, and the distance function is the square
diﬀerence.

Let λt be the variational parameters at iteration t and ρ be a constant. To obtain the next iterate λt+1,
gradient ascent maximizes the linearized elbo,

U (λt+1) =L(λt) + ∇L(λt)(cid:62)(λt+1 − λt) −

(λt+1 − λt)(cid:62)(λt+1 − λt).

(2)

1
2ρ

Speciﬁcally, this is the linearized elbo around λt subject to λt+1 being close in squared Euclidean
distance to λt.
Finding the λt+1 which maximizes Equation (2) yields

λt+1 = λt + ρ∇L(λt).

(3)

3

Algorithm 1: Proximity variational inference
Input: Initial parameters λ0, proximity statistic f (λ), distance function d
Output: Parameters λ of variational q(λ) that maximize the elbo objective
while L not converged do
λt+1 ← λt + Noise
while U not converged do

Update λt+1 ← λt+1 + ρ∇λU (λt+1)

end
λt ← λt+1

end
return λ

This is the familiar gradient ascent update with a step size of ρ. The step size ρ controls the radius
of the Euclidean ball which demarcates valid next steps for the parameters. Note that the Euclidean
constraint between subsequent iterates is implicit in all gradient ascent algorithms.

2.2 An example where variational inference fails

We study a setting where variational inference suﬀers from poor local optima. Consider a factor
model, with latent variables zik ∼ Bernoulli(π) and data xi ∼ Gaussian (cid:0)µ = (cid:80)
k zikµk, σ2 = 1(cid:1).
This is a “feature” model of real-valued data x; when one of the features is on (i.e., zik = 1), the ith
mean shifts according the that feature’s mean parameter (i.e., µk). Thus the binary latent variables
zik control which cluster means µk contribute to the distribution of xi.
The Bernoulli prior is parametrized by π; we choose a Bernoulli approximate posterior
q(zk; λk) = Bernoulli(λk). A common approach to vi is coordinate ascent (Bishop, 2006), where
we iteratively optimize each variational parameter. The optimal variational parameter for zik is




λik ∝ exp

E−zik

−






1
2σ2 (xi −

(cid:88)

j

zijµj)2








.

We can use this update in a variational EM setting. The corresponding gradient for µk is



∂L
∂µk

= −

1
σ2

(cid:88)

i

−xiλik + λikµk + λik

λijµj

 .



(cid:88)

j(cid:54)=k

(4)

(5)

Meditating on these two equations reveals a deﬁciency in mean-ﬁeld variational inference. First, if
the mean parameters µ are initialized far from the data then q∗(zik = 1) will be very small. The
reason is in Equation (4), where the squared diﬀerence between the data xi and the expected cluster
mean will be large and negative. Second, when the probability of cluster assignment is close to zero,
λik is small. This means that the norm of the gradient in Equation (5) will be small. Consequently,
learning will be slow. We see this phenomenon in Figure 1 (a). Variational inference arrives at poor
local optima and does not always recover the correct cluster means µk.

3 Proximity variational inference

We now develop proximity variational inference (pvi), a variational inference method that is robust
to initialization and can consistently reach good local optima (Section 3.1). pvi alters the notion of
proximity. We further restrict the iterates of the variational parameters by deforming the Euclidean
ball implicit in classical gradient ascent. This is done by choosing proximity statistics that are not
the identity function, and distance functions that are diﬀerent than the square diﬀerence. These
design choices help guide the variational parameters away from poor local optima (Section 3.2). One
drawback of the proximity perspective is that it requires an inner optimization at each step of the outer
optimization. We use a Taylor expansion to avoid this computational burden (Section 3.3).

4

Algorithm 2: Fast proximity variational inference
Input: Initial parameters λ0, adaptive learning rate optimizer, proximity statistic f (λ), distance d
Output: Parameters λ of the variational distribution q(λ) that maximize the elbo objective
while Lproximity not converged do

λt+1 = λt + ρ(∇L(λt) − k · (∇d(f (˜λ), f (λt))∇f (λt)).
˜λ = α˜λ + (1 − α)λt+1

end
return λ

3.1 Proximity constraints for variational inference

pvi enriches the proximity constraint in gradient ascent of the elbo. We want to develop constraints
on the iterates λt to counter the pathologies of standard variational inference.
Let f (·) be a proximity statistic, and let d be a diﬀerentiable distance function that measures distance
between proximity statistic iterates. A proximity constraint is the combination of a distance function
d applied to a proximity statistic f . (Recall that in classical gradient ascent, the Euclidean proximity
constraint uses the identity as the proximity statistic and the square diﬀerence as the distance.) Let k
be the scalar magnitude of the proximity constraint. We deﬁne the proximity update equation for the
variational parameters λt+1 to be

U (λt+1) =L(λt) + ∇L(λt)(cid:62)(λt+1 − λt)

−

1
2ρ

(λt+1 − λt)(cid:62)(λt+1 − λt) − k · d(f (˜λ), f (λt+1)),

(6)

where ˜λ is the variational parameter to which we are measuring closeness. In gradient ascent, this
is the previous parameter ˜λ = λt, but our construction can enforce proximity to more than just the
previous parameters. For example, we can set ˜λ to be an exponential moving average1—this adds
robustness to one-update optimization missteps.

The next parameters are found by maximizing Equation (6). This enforces that the variational
parameters between updates will remain close in the proximity statistic f (λ). For example, f (λ)
might be the entropy of the variational approximation; this can avoid zeroing out some of its support.
This procedure is detailed in Algorithm 1. The magnitude k of the constraint is a hyperparameter.
The inner optimization loop optimizes the update equation U at each step.

3.2 Proximity statistics for variational inference

We describe four proximity statistics f (λ) appropriate for variational inference. Together with a
distance function, these proximity statistics yield proximity constraints. (We study them in Sec-
tion 4.)

Entropy proximity statistic. Consider a constraint built from the entropy proximity statistic, f (λ) =
H(q(z; λ)). Informally, the entropy measures the amount of randomness present in a distribution.
High entropy distributions look more uniform across their support; low entropy distributions are
peaky.

Using the entropy in Equation (6) constrains all updates to have entropy close to their previous
update. When the variational distributions are initialized with large entropy, this statistic balances the
“zero-forcing” issue that is intrinsic to variational inference (MacKay, 2003). Figure 1 demonstrates
how pvi with an entropy constraint can correct this pathology.

kl proximity statistic. We can rewrite the elbo to include the kl between the approximate posterior
and the prior (Kingma and Welling, 2014),

L(λ) = E[log p(x | z)] − KL(q(z | x; λ)||p(z)).

1The exponential moving average of a variable λ is denoted ˜λ and is updated according to ˜λ ← α˜λ+(1−α)λ,

where α is a decay close to one.

5

Bad initialization

Marginal likelihood

Good initialization

Marginal likelihood

Inference method

Variational inference
Deterministic annealing
pvi, Entropy constraint
pvi, Mean/variance constraint

elbo

−663.7
−119.4
−118.0
−119.9

−636.7
−110.2
−110.0
−111.1

elbo

−122.1
−116.5
−114.1
−115.7

−114.1
−108.7
−107.5
−108.3

Table 1: Proximity variational inference improves on deterministic annealing (Katahira et al.,
2008) and vi in a one-layer sigmoid belief network. We report validation evidence lower bound
(elbo) and marginal likelihood on the binary MNIST dataset (Larochelle and Murray, 2011). The
model has one stochastic layer of 200 latent variables. pvi outperforms the classical variational
inference algorithm and deterministic annealing (Katahira et al., 2008).

Flexible models tend to minimize the kl divergence too quickly and get stuck in poor optima (Bowman
et al., 2016; Higgins et al., 2016). The choice of kl as a proximity statistic prevents the kl from
being optimized too quickly relative to the likelihood.

Mean/variance proximity statistic. A common theme in the problems with variational inference is
that the bulk of the probability mass can quickly move to a point where that dimension will no longer
be explored (Burda et al., 2015). One way to address this is to restrict the mean and variance of the
variational approximation to change slowly during optimization. This constraint only allows higher
order moments of the variational approximation to change rapidly. The mean µ = Eq(z;λ)[z] and
variance Var(z) = Eq(z;λ)[(z − µ)2] are the statistics f (λ) we constrain.
Orthogonal proximity statistic. In Bayesian deep learning models such as the variational autoen-
coder (Kingma and Welling, 2014; Rezende et al., 2014) it is common to parametrize the variational
distribution with a neural network. Orthogonal weight matrices make optimization easier in neural
networks by allowing gradients to propagate further (Saxe et al., 2013). We can exploit this fact to de-
sign an orthogonal proximity statistic for the weight matrices W of neural networks: f (W ) = W W (cid:62).
With an orthogonal initialization for the weights, this statistic enables eﬃcient optimization.

We gave four examples of proximity statistics that, together with a distance function, yield proximity
constraints. We emphasize that any function of the variational parameters f (λ) can be designed to
ameliorate issues with variational inference.

3.3 Linearizing the proximity constraint for fast proximity variational inference

pvi in Algorithm 1 requires optimizing the update equation, Equation (6), at each iteration. This rarely
has a closed-form solution and requires a separate optimization procedure that is computationally
expensive.

An alternative is to use a ﬁrst-order Taylor expansion of the proximity constraint. Let ∇d be the
gradient with respect to the second argument of the distance function, and f (˜λ) be the ﬁrst argument
to the distance. We compute the expansion around λt (the variational parameters at step t),

U (λt+1) =L(λt) + ∇L(λt)(cid:62)(λt+1 − λt) −

(λt+1 − λt)(cid:62)(λt+1 − λt)

1
2ρ

− k · (d(f (˜λ), f (λt)) + ∇d(f (˜λ), f (λt))∇f (λt)(cid:62)(λt+1 − λt)).

This linearization enjoys a closed-form solution for λt+1,

λt+1 = λt + ρ(∇L(λt) − k · (∇d(f (˜λ), f (λt))∇f (λt)).

(7)

Note that setting ˜λ to the current parameter λt removes the proximity constraint. Distance functions
are minimized at zero so their derivative is zero at that point.

Fast pvi is detailed in Algorithm 2. Unlike pvi in Algorithm 1, the update in Equation (7) does not
require an inner optimization. Fast pvi is tested in Section 4. The complexity of fast pvi is similar to
standard vi because fast pvi optimizes the elbo subject to the distance constraint in f . (The added
complexity comes from computing the derivative of f ; no inner optimization loop is required.)

6

Bad initialization

Marginal likelihood

Good initialization

Marginal likelihood

Inference method

Variational inference
Deterministic annealing
pvi, Entropy constraint
pvi, Mean/variance constraint

elbo

−504.5
−105.8
−106.0
−105.9

−464.3
−97.3
−98.3
−97.5

elbo

−117.4
−101.4
−99.9
−100.6

−105.1
−94.3
−93.7
−93.9

Table 2: Proximity variational inference improves over vi in a three-layer sigmoid belief network.
The model has three layers of 200 latent variables. We report the evidence lower bound (elbo)
and marginal likelihood on the MNIST (Larochelle and Murray, 2011) validation set. pvi and
deterministic annealing (Katahira et al., 2008) perform similarly for bad initialization; pvi improves
over deterministic annealing with good initialization.

Finally, note that fast pvi implies a global objective which varies over time. It is

Lproximity(λt+1) =Eq[log p(x, z)] − Eq[log q(λt+1)] − k · d(f (˜λ), f (λt+1)).
Because d is a distance, this remains a lower bound on the evidence, but where new variational
approximations remain close in f to previous iterations’ distributions.

4 Case studies

We developed proximity variational inference (pvi). We now empirically study pvi,2 variational
inference, and deterministic annealing (Katahira et al., 2008). To assess the robustness of inference
methods, we study both good and bad initializations. We evaluate whether the methods can recover
from poor initialization, and whether they improve on vi in well-initialized models.

We ﬁrst study sigmoid belief networks. We found vi fails to recover good solutions while pvi and
deterministic annealing recover from the initialization, and improve over vi. pvi yields further
improvements over deterministic annealing in terms of held-out values of the elbo and marginal
likelihood. We then studied a variational autoencoder model of images. Using an orthogonal proximity
statistic, pvi improves over classical vi.3

Hyperparameters. For pvi, we use the inverse Huber distance for d.4 The inverse Huber distance
penalizes smaller values than the square diﬀerence. For pvi Algorithm 2, we set the exponential moving
average decay constant for ˜λ to α = 0.9999. We set the constraint magnitude k (or temperature
parameter in deterministic annealing) to the initial absolute value of the elbo unless otherwise
speciﬁed. We explore two annealing schedules for pvi and deterministic annealing: a linear decay
and an exponential decay. For the exponential decay, the value of the magnitude at iteration t of T
total iterations is set to k · γ t
T where γ is the decay rate. We use the Adam optimizer (Kingma and
Ba, 2015).

4.1 Sigmoid belief network

The sigmoid belief network is a discrete latent variable model with layers of Bernoulli latent vari-
ables (Neal, 1992; Ranganath et al., 2015). It is commonly used to benchmark variational inference
algorithms (Mnih and Rezende, 2016). The approximate posterior is a collection of Bernoullis,
parameterized by an inference network with weights and biases. We ﬁt these variational parameters
with vi, deterministic annealing (Katahira et al., 2008), or pvi.

2Source code is available at https://github.com/altosaar/proximity_vi.
3We also compared pvi to Khan et al. (2015). Speciﬁcally, we tested pvi on the Bayesian logistic regression
model from that paper and with the same data. Because Bayesian logistic regression has a single mode, all
methods performed equally well. We note that we could not apply their algorithm to the sigmoid belief network
because it would require approximating diﬃcult iterated expectations.

4We deﬁne the inverse Huber distance d(x, y) to be |x − y| if |x − y| < 1 and 0.5(x − y)2 + 0.5 otherwise.

The constants ensure the function and its derivative are continuous at |x − y| = 1.

7

Inference method

Marginal likelihood

Variational inference
pvi, Orthogonal constraint

−94.2
−93.9

elbo

−101.0
−100.3

Table 3: Proximity variational inference with an orthogonal proximity statistic makes optimiza-
tion easier in a variational autoencoder model (Kingma and Welling, 2014; Rezende et al., 2014).
We report the held-out evidence lower bound (elbo) and estimates of the marginal likelihood on the
binarized MNIST (Larochelle and Murray, 2011) validation set.

A pervasive problem in applied vi is how to initialize parameters; thus to assess the robustness of
our method, we study good and bad initialization. For good initialization, we set the Bernoulli prior
to π = 0.5 and use the normalized initialization in Glorot and Bengio (2010) for the weights of the
generative neural network. For bad initialization, the Bernoulli prior is set to π = 0.001 and the
weights of the generative neural network are initialized to −100.
We learn the weights and biases of the model with gradient ascent. We use a step size of ρ = 10−3 and
train for 4 × 106 iterations with a batch size of 20. For pvi Algorithm 2 and deterministic annealing,
we grid search over exponential decays with rates γ ∈ {10−5, 10−6, ..., 10−10, 10−20, 10−30} and
report the best results for each algorithm. (We also explored linear decays but they did not perform as
well.) To reduce the variance of the gradients, we use the leave-one-out control variate of Mnih and
Rezende (2016) with 5 samples. (This is an extension to black-box variational inference (Ranganath
et al., 2014).)

Results on MNIST. We train a sigmoid belief network model on the binary MNIST dataset of
handwritten digits (Larochelle and Murray, 2011). For evaluation, we compute the elbo and held-out
marginal likelihood with importance sampling (5000 samples, as in Rezende et al. (2014)) on the
validation set of 104 digits. In Section 3.3 we show the results for a model with one layer of 200 latent
variables. pvi and deterministic annealing yield improvements in the held-out marginal likelihood in
comparison to vi. Table 2 displays similar results for a three-layer model with 200 latent variables
per layer. In both one and three-layer models the kl proximity statistic performs worse than the
mean/variance and entropy statistics; it requires diﬀerent decay schedules. Overall, pvi with the
entropy and mean/variance proximity statistics yields better performance than vi and deterministic
annealing.

4.2 Variational autoencoder

We study the variational autoencoder (Kingma and Welling, 2014; Rezende et al., 2014) on binary
MNIST data (Larochelle and Murray, 2011). The model has one layer of 100 Gaussian latent
variables. The inference network and generative network are chosen to have two hidden layers of size
200 with rectiﬁed linear unit activation functions. We use an orthogonal initialization for the inference
network weights. The learning rate is set to 10−3 and we run vi and pvi for 5 × 104 iterations. The
orthogonal proximity statistic changes rapidly during optimization, so we use constraint magnitudes
k ∈ {1, 10−1, 10−2, ..., 10−5}, with no decay, and report the best result. We compute the elbo and
importance-sampled marginal likelihood estimates on the validation set. In Table 3 we show that
pvi with the orthogonal proximity statistic on the weights of the inference network enables easier
optimization and improves over vi.

5 Discussion

We presented proximity variational inference, a ﬂexible method designed to avoid bad local optima
and to be robust to poor initializations. We showed that variational inference gets trapped in bad local
optima and cannot recover from poorly initialized parameters. The choice of proximity statistic f
and distance d enables the design of a variety of constraints. As examples of statistics, we gave the
entropy, kl divergence, orthogonal proximity statistic, and the mean and variance of the approximate
posterior. We evaluated our method in three models to demonstrate that it is easy to implement, readily
extendable, and leads to beneﬁcial statistical properties of variational inference algorithms.

8

Future work. Simplifying optimization is necessary for truly black-box variational inference. An
adaptive magnitude decay based on the value of the constraint should improve results (this could even
be done per-parameter). New proximity constraints are also easy to design and test. For example, the
variance of the gradients of the variational parameters is a valid proximity statistic—which can be
used to avoid variational approximations that have high variance gradients. Another set of interesting
proximity statistics are empirical statistics of the variational distribution, such as the mean, for when
analytic forms are unavailable. We also leave the design and study of constraints that admit coordinate
updates to future work.

Acknowledgments

The experiments presented in this article were performed on computational resources supported
by the Princeton Institute for Computational Science and Engineering (PICSciE), the Oﬃce of
Information Technology’s High Performance Computing Center and Visualization Laboratory at
Princeton University, Colin Raﬀel, and Dawen Liang. We are very grateful for their support. We
also thank Emtiyaz Khan and Ben Poole for helpful discussions, and Josko Plazonic for computation
support.

References

Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer New York.

Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Józefowicz, R., and Bengio, S. (2016). Generating
sentences from a continuous space. In Conference on Computational Natural Language Learning.

Boyd, S. and Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press.

Burda, Y., Grosse, R., and Salakhutdinov, R. (2015). Importance weighted autoencoders. International

Conference on Learning Representations.

Glorot, X. and Bengio, Y. (2010). Understanding the diﬃculty of training deep feedforward neural

networks. In Artiﬁcial Intelligence and Statistics, pages 249–256.

Higgins, I., Matthey, L., Glorot, X., Pal, A., Uria, B., Blundell, C., Mohamed, S., and Lerchner, A.
(2016). Early Visual Concept Learning with Unsupervised Deep Learning. ArXiv:1606.05579.

Hoﬀman, M. D., Blei, D. M., Wang, C., and Paisley, J. (2013). Stochastic variational inference.

Journal of Machine Learning Research, 14:1303–1347.

Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., and Saul, L. K. (1999). An introduction to variational

methods for graphical models. Machine Learning, 37(2):183–233.

Katahira, K., Watanabe, K., and Okada, M. (2008). Deterministic annealing variant of variational

bayes method. Journal of Physics: Conference Series, 95(1):012015.

Khan, M. E., Babanezhad, R., Lin, W., Schmidt, M., and Sugiyama, M. (2016). Faster stochastic
variational inference using proximal-gradient methods with general divergence functions.
In
Uncertainty in Artiﬁcial Intelligence, pages 319–328.

Khan, M. E., Baqué, P., Fleuret, F., and Fua, P. (2015). Kullback-leibler proximal variational inference.

In Advances in Neural Information Processing Systems, pages 3384–3392.

Kingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization.

International

Conference on Learning Representations.

Kingma, D. P. and Welling, M. (2014). Auto-encoding variational Bayes. In International Conference

on Learning Representations.

Kucukelbir, A., Ranganath, R., Gelman, A., and Blei, D. M. (2015). Automatic Variational Inference

in Stan. In Neural Information Processing Systems.

Larochelle, H. and Murray, I. (2011). The neural autoregressive distribution estimator. In Artiﬁcial

Intelligence and Statistics, volume 15, pages 29–37.

9

MacKay, D. (2003). Information Theory, Inference, and Learning Algorithms. Cambridge University

Press.

Mansinghka, V., Selsam, D., and Perov, Y. N. (2014). Venture: a higher-order probabilistic program-

ming platform with programmable inference. arXiv:1404.0099.

Mnih, A. and Rezende, D. J. (2016). Variational inference for monte carlo objectives. In International

Conference on Machine Learning, pages 2188–2196.

Nakajima, S., Sugiyama, M., Babacan, S. D., and Tomioka, R. (2013). Global analytic solution of
fully-observed variational bayesian matrix factorization. Journal of Machine Learning Research.

Neal, R. M. (1992). Connectionist learning of belief networks. Artiﬁcial intelligence, 56(1):71–113.

Ranganath, R., Gerrish, S., and Blei, D. M. (2014). Black box variational inference. In Artiﬁcial

Intelligence and Statistics.

Intelligence and Statistics.

Ranganath, R., Tang, L., Charlin, L., and Blei, D. M. (2015). Deep exponential families. In Artiﬁcial

Rezende, D. J. and Mohamed, S. (2015). Variational inference with normalizing ﬂows. In International

Conference on Machine Learning.

Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate

inference in deep generative models. In International Conference on Machine Learning.

Saxe, A. M., McClelland, J. L., and Ganguli, S. (2013). Exact solutions to the nonlinear dynamics of

learning in deep linear neural networks. arXiv preprint arXiv:1312.6120.

Spall, J. (2003). Introduction to Stochastic Search and Optimization: Estimation, Simulation, and

Control. Wiley.

Theis, L. and Hoﬀman, M. D. (2015). A trust-region method for stochastic variational inference with

applications to streaming data. Journal of Machine Learning Research.

Tran, D., Kucukelbir, A., Dieng, A. B., Rudolph, M., Liang, D., and Blei, D. M. (2016). Edward: A

library for probabilistic modeling, inference, and criticism. arXiv:1610.09787.

Wainwright, M. J. and Jordan, M. I. (2008). Graphical models, exponential families, and variational

inference. Foundations and Trends in Machine Learning, 1(1-2):1–305.

10

7
1
0
2
 
y
a
M
 
4
2
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
1
3
9
8
0
.
5
0
7
1
:
v
i
X
r
a

Proximity Variational Inference

Jaan Altosaar
Department of Physics
Princeton University
altosaar@princeton.edu

Rajesh Ranganath
Department of Computer Science
Princeton University
rajeshr@cs.princeton.edu

David M. Blei
Data Science Institute
Departments of Computer Science and Statistics
Columbia University
david.blei@columbia.edu

Abstract

Variational inference is a powerful approach for approximate posterior inference.
However, it is sensitive to initialization and can be subject to poor local optima. In
this paper, we develop proximity variational inference (pvi). pvi is a new method
for optimizing the variational objective that constrains subsequent iterates of the
variational parameters to robustify the optimization path. Consequently, pvi is
less sensitive to initialization and optimization quirks and ﬁnds better local optima.
We demonstrate our method on three proximity statistics. We study pvi on a
Bernoulli factor model and sigmoid belief network with both real and synthetic
data and compare to deterministic annealing (Katahira et al., 2008). We highlight
the ﬂexibility of pvi by designing a proximity statistic for Bayesian deep learning
models such as the variational autoencoder (Kingma and Welling, 2014; Rezende
et al., 2014). Empirically, we show that pvi consistently ﬁnds better local optima
and gives better predictive performance.

1

Introduction

Variational inference (vi) is a powerful method for probabilistic modeling. vi uses optimization to
approximate diﬃcult-to-compute conditional distributions (Jordan et al., 1999). In its modern incarna-
tion, it has scaled Bayesian computation to large data sets (Hoﬀman et al., 2013), generalized to large
classes of models (Kingma and Welling, 2014; Ranganath et al., 2014; Rezende and Mohamed, 2015),
and has been deployed as a computational engine in probabilistic programming systems (Mansinghka
et al., 2014; Kucukelbir et al., 2015; Tran et al., 2016).
Despite these signiﬁcant advances, however, vi has drawbacks. For one, it tries to iteratively solve a
diﬃcult nonconvex optimization problem and its objective contains many local optima. Consequently,
vi is sensitive to initialization and easily gets stuck in a poor solution. We develop a new optimization
method for vi. Our method is less sensitive to initialization and ﬁnds better optima.

Consider a probability model p(z, x) and the goal of calculating the posterior p(z | x). The idea behind
vi is to posit a family of distributions over the hidden variables q(z; λ) and then ﬁt the variational
parameters λ to minimize the Kullback-Leibler (kl) divergence between the approximating family
and the exact posterior, kl(q(z; λ)||p(z | x)). The kl is not tractable so vi optimizes a proxy. That
proxy is the evidence lower bound (elbo),

L(λ) = E[log p(z, x)] − E[log q(z; λ)],

(1)

(a) Variational inference

(b) Proximity variational inference, Algorithm 2

Figure 1: Proximity variational inference (pvi) is robust to bad initialization. We study a
Bernoulli factor model. Model parameters are randomly initialized on a ring around the known true
parameters (in red) used to generate the data. The arrows start at these parameter initializations and
end at the ﬁnal parameter estimates (shown as green dots). (a) Variational inference with gradient
ascent suﬀers from multiple local optima and cannot reliably recover the truth. (b) pvi with an entropy
proximity statistic reliably infers the true parameters using Algorithm 2.

where expectations are taken with respect to q(z; λ). Maximizing the elbo with respect to λ is
equivalent to minimizing the kl divergence.

The issues around vi stem from the elbo and the iterative algorithms used to optimize it. When
the algorithm zeroes (or nearly zeroes) some of the support of q(z; λ), it becomes hard to later
“escape,” i.e., to add support for the conﬁgurations of the latent variables that have been assigned zero
probability (MacKay, 2003; Burda et al., 2015). This leads to poor local optima and to sensitivity to
the starting point, where a misguided initialization will lead to such optima. These problems happen
in both gradient-based and coordinate ascent methods. We address these issues with proximity
variational inference (pvi), a variational inference algorithm that is speciﬁcally designed to avoid
poor local optima and to be robust to diﬀerent initializations.

pvi builds on the proximity perspective of gradient ascent. The proximity perspective views each step
of gradient ascent as a constrained minimization of a Taylor expansion of the objective around the
previous step’s parameter (Spall, 2003; Boyd and Vandenberghe, 2004). The constraint, a proximity
constraint, enforces that the next point should be inside a Euclidean ball of the previous. The step
size relates to the size of that ball.

In vi, a constraint on the Euclidean distance means that all dimensions of the variational parameters are
equally constrained. We posit that this leads to problems; some dimensions need more regularization
than others. For example, consider a variational distribution that is Gaussian. A good optimization
will change the variance parameter more slowly than the mean parameter to prevent rapid changes
to the support. The Euclidean constraint cannot enforce this. Furthermore, the constraints enforced
by gradient descent are transient; the constraints are relative to the previous iterate—one poor move
during the optimization can lead to permanent optimization problems.

To this end, pvi uses proximity constraints that are more meaningful to variational inference and to
optimization of probability parameters. A constraint is deﬁned using a proximity statistic and distance
function. As one example we consider a constraint based on the entropy proximity statistic. This
limits the change in entropy of the variational approximation from one step to the next. Consider
again a Gaussian approximation. The entropy is a function of the variance alone and thus the entropy
constraint counters the pathologies induced by the Euclidean proximity constraint. We also study
constraints built from other proximity statistics, such as those that penalize the rapid changes in the
mean and variance of the approximate posterior.

Figure 1 provides an illustration of the advantages of pvi. Our goal is to estimate the parameters of a
factor analysis model with variational inference, i.e., using the posterior expectation under a ﬁtted
2

variational distribution. We run variational inference 100 times, each time initializing the estimates
(the model parameters) to a diﬀerent position on a ring around the truth.

In the ﬁgure, red points indicate the true value. The start locations of the green arrows indicate the
initialized estimates. Green points indicate the ﬁnal estimates, after optimizing from the initial points.
Panel (a) shows that optimizing the standard elbo with gradients leads to poor local optima and
misplaced estimates. Panel (b) illustrates that regardless of the initialization, pvi with an entropy
proximity statistic ﬁnd estimates that are close to the true value.

The rest of the paper is organized as follows. Section 2 reviews variational inference and the proximity
perspective of gradient optimization. Section 3 derives pvi; we develop four proximity constraints
and two algorithms for optimizing the elbo. We study three models in Section 4: a Bernoulli factor
model, a sigmoid belief network (Mnih and Rezende, 2016), and a variational autoencoder (Kingma
and Welling, 2014; Rezende et al., 2014). On the MNIST data set, pvi generally outperforms classical
methods for variational inference.

Related work. Recent work has proposed several related algorithms. Khan et al. (2015) and Theis
and Hoﬀman (2015) develop a method to optimize the elbo that imposes a soft limit on the change
in kl of consecutive variational approximations. This is equivalent to pvi with identity proximity
statistics and a kl distance function. Khan et al. (2016) extend both prior works to other divergence
functions. Their general approach is equivalent to pvi identity proximity statistics and distance
functions given by strongly-convex divergences. Compared to prior work, pvi generalizes to a broader
class of proximity statistics. We develop proximity statistics based on entropy, kl, orthogonal weight
matrices, and the mean and variance of the variational approximation.

The problem of model pruning in variational inference has also been studied and analytically solved
in a matrix factorization model in Nakajima et al. (2013)—this method is model-speciﬁc, whereas pvi
applies to a much broader class of latent variable models. Finally, deterministic annealing (Katahira
et al., 2008) consists of adding a temperature parameter to the entropy term in the elbo that is
annealed to one during inference. This is similar to pvi with the entropy proximity statistic which
keeps the entropy stable across iterations. Deterministic annealing enforces global penalization of
low-entropy conﬁgurations of latent variables rather than the smooth constraint used in pvi, and
cannot accommodate the range of proximity statistics we design in this work.

2 Variational inference

Consider a model p(x, z), where x is the observed data and z are the latent variables. As described
in Section 1, vi posits an approximating family q(z; λ) and maximizes the elbo in Equation (1).
Solving this optimization is equivalent to ﬁnding the variational approximation that minimizes kl
divergence to the exact posterior (Jordan et al., 1999; Wainwright and Jordan, 2008).

2.1 Gradient ascent has Euclidean proximity

Gradient ascent maximizes the elbo by repeatedly following its gradient. One view of this algorithm
is that it repeatedly maximizes the linearized elbo subject to a proximity constraint on the current
variational parameter (Spall, 2003). The name ‘proximity’ comes from constraining subsequent
parameters to remain close in the proximity statistic. In gradient ascent, the proximity statistic for
the variational parameters is the identity function f (λ) = λ, and the distance function is the square
diﬀerence.

Let λt be the variational parameters at iteration t and ρ be a constant. To obtain the next iterate λt+1,
gradient ascent maximizes the linearized elbo,

U (λt+1) =L(λt) + ∇L(λt)(cid:62)(λt+1 − λt) −

(λt+1 − λt)(cid:62)(λt+1 − λt).

(2)

1
2ρ

Speciﬁcally, this is the linearized elbo around λt subject to λt+1 being close in squared Euclidean
distance to λt.
Finding the λt+1 which maximizes Equation (2) yields

λt+1 = λt + ρ∇L(λt).

(3)

3

Algorithm 1: Proximity variational inference
Input: Initial parameters λ0, proximity statistic f (λ), distance function d
Output: Parameters λ of variational q(λ) that maximize the elbo objective
while L not converged do
λt+1 ← λt + Noise
while U not converged do

Update λt+1 ← λt+1 + ρ∇λU (λt+1)

end
λt ← λt+1

end
return λ

This is the familiar gradient ascent update with a step size of ρ. The step size ρ controls the radius
of the Euclidean ball which demarcates valid next steps for the parameters. Note that the Euclidean
constraint between subsequent iterates is implicit in all gradient ascent algorithms.

2.2 An example where variational inference fails

We study a setting where variational inference suﬀers from poor local optima. Consider a factor
model, with latent variables zik ∼ Bernoulli(π) and data xi ∼ Gaussian (cid:0)µ = (cid:80)
k zikµk, σ2 = 1(cid:1).
This is a “feature” model of real-valued data x; when one of the features is on (i.e., zik = 1), the ith
mean shifts according the that feature’s mean parameter (i.e., µk). Thus the binary latent variables
zik control which cluster means µk contribute to the distribution of xi.
The Bernoulli prior is parametrized by π; we choose a Bernoulli approximate posterior
q(zk; λk) = Bernoulli(λk). A common approach to vi is coordinate ascent (Bishop, 2006), where
we iteratively optimize each variational parameter. The optimal variational parameter for zik is




λik ∝ exp

E−zik

−






1
2σ2 (xi −

(cid:88)

j

zijµj)2








.

We can use this update in a variational EM setting. The corresponding gradient for µk is



∂L
∂µk

= −

1
σ2

(cid:88)

i

−xiλik + λikµk + λik

λijµj

 .



(cid:88)

j(cid:54)=k

(4)

(5)

Meditating on these two equations reveals a deﬁciency in mean-ﬁeld variational inference. First, if
the mean parameters µ are initialized far from the data then q∗(zik = 1) will be very small. The
reason is in Equation (4), where the squared diﬀerence between the data xi and the expected cluster
mean will be large and negative. Second, when the probability of cluster assignment is close to zero,
λik is small. This means that the norm of the gradient in Equation (5) will be small. Consequently,
learning will be slow. We see this phenomenon in Figure 1 (a). Variational inference arrives at poor
local optima and does not always recover the correct cluster means µk.

3 Proximity variational inference

We now develop proximity variational inference (pvi), a variational inference method that is robust
to initialization and can consistently reach good local optima (Section 3.1). pvi alters the notion of
proximity. We further restrict the iterates of the variational parameters by deforming the Euclidean
ball implicit in classical gradient ascent. This is done by choosing proximity statistics that are not
the identity function, and distance functions that are diﬀerent than the square diﬀerence. These
design choices help guide the variational parameters away from poor local optima (Section 3.2). One
drawback of the proximity perspective is that it requires an inner optimization at each step of the outer
optimization. We use a Taylor expansion to avoid this computational burden (Section 3.3).

4

Algorithm 2: Fast proximity variational inference
Input: Initial parameters λ0, adaptive learning rate optimizer, proximity statistic f (λ), distance d
Output: Parameters λ of the variational distribution q(λ) that maximize the elbo objective
while Lproximity not converged do

λt+1 = λt + ρ(∇L(λt) − k · (∇d(f (˜λ), f (λt))∇f (λt)).
˜λ = α˜λ + (1 − α)λt+1

end
return λ

3.1 Proximity constraints for variational inference

pvi enriches the proximity constraint in gradient ascent of the elbo. We want to develop constraints
on the iterates λt to counter the pathologies of standard variational inference.
Let f (·) be a proximity statistic, and let d be a diﬀerentiable distance function that measures distance
between proximity statistic iterates. A proximity constraint is the combination of a distance function
d applied to a proximity statistic f . (Recall that in classical gradient ascent, the Euclidean proximity
constraint uses the identity as the proximity statistic and the square diﬀerence as the distance.) Let k
be the scalar magnitude of the proximity constraint. We deﬁne the proximity update equation for the
variational parameters λt+1 to be

U (λt+1) =L(λt) + ∇L(λt)(cid:62)(λt+1 − λt)

−

1
2ρ

(λt+1 − λt)(cid:62)(λt+1 − λt) − k · d(f (˜λ), f (λt+1)),

(6)

where ˜λ is the variational parameter to which we are measuring closeness. In gradient ascent, this
is the previous parameter ˜λ = λt, but our construction can enforce proximity to more than just the
previous parameters. For example, we can set ˜λ to be an exponential moving average1—this adds
robustness to one-update optimization missteps.

The next parameters are found by maximizing Equation (6). This enforces that the variational
parameters between updates will remain close in the proximity statistic f (λ). For example, f (λ)
might be the entropy of the variational approximation; this can avoid zeroing out some of its support.
This procedure is detailed in Algorithm 1. The magnitude k of the constraint is a hyperparameter.
The inner optimization loop optimizes the update equation U at each step.

3.2 Proximity statistics for variational inference

We describe four proximity statistics f (λ) appropriate for variational inference. Together with a
distance function, these proximity statistics yield proximity constraints. (We study them in Sec-
tion 4.)

Entropy proximity statistic. Consider a constraint built from the entropy proximity statistic, f (λ) =
H(q(z; λ)). Informally, the entropy measures the amount of randomness present in a distribution.
High entropy distributions look more uniform across their support; low entropy distributions are
peaky.

Using the entropy in Equation (6) constrains all updates to have entropy close to their previous
update. When the variational distributions are initialized with large entropy, this statistic balances the
“zero-forcing” issue that is intrinsic to variational inference (MacKay, 2003). Figure 1 demonstrates
how pvi with an entropy constraint can correct this pathology.

kl proximity statistic. We can rewrite the elbo to include the kl between the approximate posterior
and the prior (Kingma and Welling, 2014),

L(λ) = E[log p(x | z)] − KL(q(z | x; λ)||p(z)).

1The exponential moving average of a variable λ is denoted ˜λ and is updated according to ˜λ ← α˜λ+(1−α)λ,

where α is a decay close to one.

5

Bad initialization

Marginal likelihood

Good initialization

Marginal likelihood

Inference method

Variational inference
Deterministic annealing
pvi, Entropy constraint
pvi, Mean/variance constraint

elbo

−663.7
−119.4
−118.0
−119.9

−636.7
−110.2
−110.0
−111.1

elbo

−122.1
−116.5
−114.1
−115.7

−114.1
−108.7
−107.5
−108.3

Table 1: Proximity variational inference improves on deterministic annealing (Katahira et al.,
2008) and vi in a one-layer sigmoid belief network. We report validation evidence lower bound
(elbo) and marginal likelihood on the binary MNIST dataset (Larochelle and Murray, 2011). The
model has one stochastic layer of 200 latent variables. pvi outperforms the classical variational
inference algorithm and deterministic annealing (Katahira et al., 2008).

Flexible models tend to minimize the kl divergence too quickly and get stuck in poor optima (Bowman
et al., 2016; Higgins et al., 2016). The choice of kl as a proximity statistic prevents the kl from
being optimized too quickly relative to the likelihood.

Mean/variance proximity statistic. A common theme in the problems with variational inference is
that the bulk of the probability mass can quickly move to a point where that dimension will no longer
be explored (Burda et al., 2015). One way to address this is to restrict the mean and variance of the
variational approximation to change slowly during optimization. This constraint only allows higher
order moments of the variational approximation to change rapidly. The mean µ = Eq(z;λ)[z] and
variance Var(z) = Eq(z;λ)[(z − µ)2] are the statistics f (λ) we constrain.
Orthogonal proximity statistic. In Bayesian deep learning models such as the variational autoen-
coder (Kingma and Welling, 2014; Rezende et al., 2014) it is common to parametrize the variational
distribution with a neural network. Orthogonal weight matrices make optimization easier in neural
networks by allowing gradients to propagate further (Saxe et al., 2013). We can exploit this fact to de-
sign an orthogonal proximity statistic for the weight matrices W of neural networks: f (W ) = W W (cid:62).
With an orthogonal initialization for the weights, this statistic enables eﬃcient optimization.

We gave four examples of proximity statistics that, together with a distance function, yield proximity
constraints. We emphasize that any function of the variational parameters f (λ) can be designed to
ameliorate issues with variational inference.

3.3 Linearizing the proximity constraint for fast proximity variational inference

pvi in Algorithm 1 requires optimizing the update equation, Equation (6), at each iteration. This rarely
has a closed-form solution and requires a separate optimization procedure that is computationally
expensive.

An alternative is to use a ﬁrst-order Taylor expansion of the proximity constraint. Let ∇d be the
gradient with respect to the second argument of the distance function, and f (˜λ) be the ﬁrst argument
to the distance. We compute the expansion around λt (the variational parameters at step t),

U (λt+1) =L(λt) + ∇L(λt)(cid:62)(λt+1 − λt) −

(λt+1 − λt)(cid:62)(λt+1 − λt)

1
2ρ

− k · (d(f (˜λ), f (λt)) + ∇d(f (˜λ), f (λt))∇f (λt)(cid:62)(λt+1 − λt)).

This linearization enjoys a closed-form solution for λt+1,

λt+1 = λt + ρ(∇L(λt) − k · (∇d(f (˜λ), f (λt))∇f (λt)).

(7)

Note that setting ˜λ to the current parameter λt removes the proximity constraint. Distance functions
are minimized at zero so their derivative is zero at that point.

Fast pvi is detailed in Algorithm 2. Unlike pvi in Algorithm 1, the update in Equation (7) does not
require an inner optimization. Fast pvi is tested in Section 4. The complexity of fast pvi is similar to
standard vi because fast pvi optimizes the elbo subject to the distance constraint in f . (The added
complexity comes from computing the derivative of f ; no inner optimization loop is required.)

6

Bad initialization

Marginal likelihood

Good initialization

Marginal likelihood

Inference method

Variational inference
Deterministic annealing
pvi, Entropy constraint
pvi, Mean/variance constraint

elbo

−504.5
−105.8
−106.0
−105.9

−464.3
−97.3
−98.3
−97.5

elbo

−117.4
−101.4
−99.9
−100.6

−105.1
−94.3
−93.7
−93.9

Table 2: Proximity variational inference improves over vi in a three-layer sigmoid belief network.
The model has three layers of 200 latent variables. We report the evidence lower bound (elbo)
and marginal likelihood on the MNIST (Larochelle and Murray, 2011) validation set. pvi and
deterministic annealing (Katahira et al., 2008) perform similarly for bad initialization; pvi improves
over deterministic annealing with good initialization.

Finally, note that fast pvi implies a global objective which varies over time. It is

Lproximity(λt+1) =Eq[log p(x, z)] − Eq[log q(λt+1)] − k · d(f (˜λ), f (λt+1)).
Because d is a distance, this remains a lower bound on the evidence, but where new variational
approximations remain close in f to previous iterations’ distributions.

4 Case studies

We developed proximity variational inference (pvi). We now empirically study pvi,2 variational
inference, and deterministic annealing (Katahira et al., 2008). To assess the robustness of inference
methods, we study both good and bad initializations. We evaluate whether the methods can recover
from poor initialization, and whether they improve on vi in well-initialized models.

We ﬁrst study sigmoid belief networks. We found vi fails to recover good solutions while pvi and
deterministic annealing recover from the initialization, and improve over vi. pvi yields further
improvements over deterministic annealing in terms of held-out values of the elbo and marginal
likelihood. We then studied a variational autoencoder model of images. Using an orthogonal proximity
statistic, pvi improves over classical vi.3

Hyperparameters. For pvi, we use the inverse Huber distance for d.4 The inverse Huber distance
penalizes smaller values than the square diﬀerence. For pvi Algorithm 2, we set the exponential moving
average decay constant for ˜λ to α = 0.9999. We set the constraint magnitude k (or temperature
parameter in deterministic annealing) to the initial absolute value of the elbo unless otherwise
speciﬁed. We explore two annealing schedules for pvi and deterministic annealing: a linear decay
and an exponential decay. For the exponential decay, the value of the magnitude at iteration t of T
total iterations is set to k · γ t
T where γ is the decay rate. We use the Adam optimizer (Kingma and
Ba, 2015).

4.1 Sigmoid belief network

The sigmoid belief network is a discrete latent variable model with layers of Bernoulli latent vari-
ables (Neal, 1992; Ranganath et al., 2015). It is commonly used to benchmark variational inference
algorithms (Mnih and Rezende, 2016). The approximate posterior is a collection of Bernoullis,
parameterized by an inference network with weights and biases. We ﬁt these variational parameters
with vi, deterministic annealing (Katahira et al., 2008), or pvi.

2Source code is available at https://github.com/altosaar/proximity_vi.
3We also compared pvi to Khan et al. (2015). Speciﬁcally, we tested pvi on the Bayesian logistic regression
model from that paper and with the same data. Because Bayesian logistic regression has a single mode, all
methods performed equally well. We note that we could not apply their algorithm to the sigmoid belief network
because it would require approximating diﬃcult iterated expectations.

4We deﬁne the inverse Huber distance d(x, y) to be |x − y| if |x − y| < 1 and 0.5(x − y)2 + 0.5 otherwise.

The constants ensure the function and its derivative are continuous at |x − y| = 1.

7

Inference method

Marginal likelihood

Variational inference
pvi, Orthogonal constraint

−94.2
−93.9

elbo

−101.0
−100.3

Table 3: Proximity variational inference with an orthogonal proximity statistic makes optimiza-
tion easier in a variational autoencoder model (Kingma and Welling, 2014; Rezende et al., 2014).
We report the held-out evidence lower bound (elbo) and estimates of the marginal likelihood on the
binarized MNIST (Larochelle and Murray, 2011) validation set.

A pervasive problem in applied vi is how to initialize parameters; thus to assess the robustness of
our method, we study good and bad initialization. For good initialization, we set the Bernoulli prior
to π = 0.5 and use the normalized initialization in Glorot and Bengio (2010) for the weights of the
generative neural network. For bad initialization, the Bernoulli prior is set to π = 0.001 and the
weights of the generative neural network are initialized to −100.
We learn the weights and biases of the model with gradient ascent. We use a step size of ρ = 10−3 and
train for 4 × 106 iterations with a batch size of 20. For pvi Algorithm 2 and deterministic annealing,
we grid search over exponential decays with rates γ ∈ {10−5, 10−6, ..., 10−10, 10−20, 10−30} and
report the best results for each algorithm. (We also explored linear decays but they did not perform as
well.) To reduce the variance of the gradients, we use the leave-one-out control variate of Mnih and
Rezende (2016) with 5 samples. (This is an extension to black-box variational inference (Ranganath
et al., 2014).)

Results on MNIST. We train a sigmoid belief network model on the binary MNIST dataset of
handwritten digits (Larochelle and Murray, 2011). For evaluation, we compute the elbo and held-out
marginal likelihood with importance sampling (5000 samples, as in Rezende et al. (2014)) on the
validation set of 104 digits. In Section 3.3 we show the results for a model with one layer of 200 latent
variables. pvi and deterministic annealing yield improvements in the held-out marginal likelihood in
comparison to vi. Table 2 displays similar results for a three-layer model with 200 latent variables
per layer. In both one and three-layer models the kl proximity statistic performs worse than the
mean/variance and entropy statistics; it requires diﬀerent decay schedules. Overall, pvi with the
entropy and mean/variance proximity statistics yields better performance than vi and deterministic
annealing.

4.2 Variational autoencoder

We study the variational autoencoder (Kingma and Welling, 2014; Rezende et al., 2014) on binary
MNIST data (Larochelle and Murray, 2011). The model has one layer of 100 Gaussian latent
variables. The inference network and generative network are chosen to have two hidden layers of size
200 with rectiﬁed linear unit activation functions. We use an orthogonal initialization for the inference
network weights. The learning rate is set to 10−3 and we run vi and pvi for 5 × 104 iterations. The
orthogonal proximity statistic changes rapidly during optimization, so we use constraint magnitudes
k ∈ {1, 10−1, 10−2, ..., 10−5}, with no decay, and report the best result. We compute the elbo and
importance-sampled marginal likelihood estimates on the validation set. In Table 3 we show that
pvi with the orthogonal proximity statistic on the weights of the inference network enables easier
optimization and improves over vi.

5 Discussion

We presented proximity variational inference, a ﬂexible method designed to avoid bad local optima
and to be robust to poor initializations. We showed that variational inference gets trapped in bad local
optima and cannot recover from poorly initialized parameters. The choice of proximity statistic f
and distance d enables the design of a variety of constraints. As examples of statistics, we gave the
entropy, kl divergence, orthogonal proximity statistic, and the mean and variance of the approximate
posterior. We evaluated our method in three models to demonstrate that it is easy to implement, readily
extendable, and leads to beneﬁcial statistical properties of variational inference algorithms.

8

Future work. Simplifying optimization is necessary for truly black-box variational inference. An
adaptive magnitude decay based on the value of the constraint should improve results (this could even
be done per-parameter). New proximity constraints are also easy to design and test. For example, the
variance of the gradients of the variational parameters is a valid proximity statistic—which can be
used to avoid variational approximations that have high variance gradients. Another set of interesting
proximity statistics are empirical statistics of the variational distribution, such as the mean, for when
analytic forms are unavailable. We also leave the design and study of constraints that admit coordinate
updates to future work.

Acknowledgments

The experiments presented in this article were performed on computational resources supported
by the Princeton Institute for Computational Science and Engineering (PICSciE), the Oﬃce of
Information Technology’s High Performance Computing Center and Visualization Laboratory at
Princeton University, Colin Raﬀel, and Dawen Liang. We are very grateful for their support. We
also thank Emtiyaz Khan and Ben Poole for helpful discussions, and Josko Plazonic for computation
support.

References

Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer New York.

Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Józefowicz, R., and Bengio, S. (2016). Generating
sentences from a continuous space. In Conference on Computational Natural Language Learning.

Boyd, S. and Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press.

Burda, Y., Grosse, R., and Salakhutdinov, R. (2015). Importance weighted autoencoders. International

Conference on Learning Representations.

Glorot, X. and Bengio, Y. (2010). Understanding the diﬃculty of training deep feedforward neural

networks. In Artiﬁcial Intelligence and Statistics, pages 249–256.

Higgins, I., Matthey, L., Glorot, X., Pal, A., Uria, B., Blundell, C., Mohamed, S., and Lerchner, A.
(2016). Early Visual Concept Learning with Unsupervised Deep Learning. ArXiv:1606.05579.

Hoﬀman, M. D., Blei, D. M., Wang, C., and Paisley, J. (2013). Stochastic variational inference.

Journal of Machine Learning Research, 14:1303–1347.

Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., and Saul, L. K. (1999). An introduction to variational

methods for graphical models. Machine Learning, 37(2):183–233.

Katahira, K., Watanabe, K., and Okada, M. (2008). Deterministic annealing variant of variational

bayes method. Journal of Physics: Conference Series, 95(1):012015.

Khan, M. E., Babanezhad, R., Lin, W., Schmidt, M., and Sugiyama, M. (2016). Faster stochastic
variational inference using proximal-gradient methods with general divergence functions.
In
Uncertainty in Artiﬁcial Intelligence, pages 319–328.

Khan, M. E., Baqué, P., Fleuret, F., and Fua, P. (2015). Kullback-leibler proximal variational inference.

In Advances in Neural Information Processing Systems, pages 3384–3392.

Kingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization.

International

Conference on Learning Representations.

Kingma, D. P. and Welling, M. (2014). Auto-encoding variational Bayes. In International Conference

on Learning Representations.

Kucukelbir, A., Ranganath, R., Gelman, A., and Blei, D. M. (2015). Automatic Variational Inference

in Stan. In Neural Information Processing Systems.

Larochelle, H. and Murray, I. (2011). The neural autoregressive distribution estimator. In Artiﬁcial

Intelligence and Statistics, volume 15, pages 29–37.

9

MacKay, D. (2003). Information Theory, Inference, and Learning Algorithms. Cambridge University

Press.

Mansinghka, V., Selsam, D., and Perov, Y. N. (2014). Venture: a higher-order probabilistic program-

ming platform with programmable inference. arXiv:1404.0099.

Mnih, A. and Rezende, D. J. (2016). Variational inference for monte carlo objectives. In International

Conference on Machine Learning, pages 2188–2196.

Nakajima, S., Sugiyama, M., Babacan, S. D., and Tomioka, R. (2013). Global analytic solution of
fully-observed variational bayesian matrix factorization. Journal of Machine Learning Research.

Neal, R. M. (1992). Connectionist learning of belief networks. Artiﬁcial intelligence, 56(1):71–113.

Ranganath, R., Gerrish, S., and Blei, D. M. (2014). Black box variational inference. In Artiﬁcial

Intelligence and Statistics.

Intelligence and Statistics.

Ranganath, R., Tang, L., Charlin, L., and Blei, D. M. (2015). Deep exponential families. In Artiﬁcial

Rezende, D. J. and Mohamed, S. (2015). Variational inference with normalizing ﬂows. In International

Conference on Machine Learning.

Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate

inference in deep generative models. In International Conference on Machine Learning.

Saxe, A. M., McClelland, J. L., and Ganguli, S. (2013). Exact solutions to the nonlinear dynamics of

learning in deep linear neural networks. arXiv preprint arXiv:1312.6120.

Spall, J. (2003). Introduction to Stochastic Search and Optimization: Estimation, Simulation, and

Control. Wiley.

Theis, L. and Hoﬀman, M. D. (2015). A trust-region method for stochastic variational inference with

applications to streaming data. Journal of Machine Learning Research.

Tran, D., Kucukelbir, A., Dieng, A. B., Rudolph, M., Liang, D., and Blei, D. M. (2016). Edward: A

library for probabilistic modeling, inference, and criticism. arXiv:1610.09787.

Wainwright, M. J. and Jordan, M. I. (2008). Graphical models, exponential families, and variational

inference. Foundations and Trends in Machine Learning, 1(1-2):1–305.

10

