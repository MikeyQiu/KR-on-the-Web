Evaluating neural network explanation methods using
hybrid documents and morphosyntactic agreement

Nina Poerner, Benjamin Roth & Hinrich Sch ¨utze
Center for Information and Language Processing
LMU Munich, Germany
poerner@cis.lmu.de

Abstract

The behavior of deep neural networks
(DNNs) is hard to understand. This makes
it necessary to explore post hoc expla-
nation methods. We conduct
the ﬁrst
comprehensive evaluation of explanation
methods for NLP. To this end, we design
two novel evaluation paradigms that cover
two important classes of NLP problems:
small context and large context problems.
Both paradigms require no manual annota-
tion and are therefore broadly applicable.
We also introduce LIMSSE, an explana-
tion method inspired by LIME that is de-
signed for NLP. We show empirically that
LIMSSE, LRP and DeepLIFT are the most
effective explanation methods and recom-
mend them for explaining DNNs in NLP.

1

Introduction

DNNs are complex models that combine linear
transformations with different types of nonlinear-
ities. If the model is deep, i.e., has many layers,
then its behavior during training and inference is
notoriously hard to understand.

This is a problem for both scientiﬁc method-
ology and real-world deployment.
Scientiﬁc
methodology demands that we understand our
models. In the real world, a decision (e.g., “your
blog post is offensive and has been removed”) by
itself is often insufﬁcient; in addition, an expla-
nation of the decision may be required (e.g., “our
system ﬂagged the following words as offensive”).
The European Union plans to mandate that intelli-
gent systems used for sensitive applications pro-
vide such explanations (European General Data
Protection Regulation, expected 2018, cf. Good-
man and Flaxman (2016)).

A number of post hoc explanation methods for
DNNs have been proposed. Due to the complexity
of the DNNs they explain, these methods are nec-
essarily approximations and come with their own
sources of error. At this point, it is not clear which
of these methods to use when reliable explanations
for a speciﬁc DNN architecture are needed.

Deﬁnitions. (i) A task method solves an NLP

problem, e.g., a GRU that predicts sentiment.

(ii) An explanation method explains the behav-
ior of a task method on a speciﬁc input. For our
purpose, it is a function φ(t, k, X) that assigns
real-valued relevance scores for a target class k
(e.g., positive) to positions t in an input text X
(e.g., “great food”). For this example, an ex-
planation method might assign: φ(1, k, X) >
φ(2, k, X).

(iii) An (explanation) evaluation paradigm
quantitatively evaluates explanation methods for a
task method, e.g., by assigning them accuracies.

Contributions. (i) We present novel evaluation
paradigms for explanation methods for two classes
of common NLP tasks (see §2). Crucially, nei-
ther paradigm requires manual annotations and
our methodology is therefore broadly applicable.
(ii) Using these paradigms, we perform a com-
prehensive evaluation of explanation methods for
NLP (§3). We cover the most important classes
of task methods, RNNs and CNNs, as well as the
recently proposed Quasi-RNNs.

(iii) We introduce LIMSSE (§3.6), an expla-
nation method inspired by LIME (Ribeiro et al.,

tasks

task methods
explanation methods
evaluation paradigms

sentiment analysis,
morphological prediction, . . .
CNN, GRU, LSTM, . . .
LIMSSE, LRP, DeepLIFT, . . .
hybrid document,
morphosyntactic agreement

Table 1: Terminology with examples.

lrp

From : kolstad @ cae.wisc.edu ( Joel Kolstad ) Subject : Re : Can Radio Freq . Be Used To Measure Distance ? [...] What is the difference
between vertical and horizontal ? Gravity ? Does n’t gravity pull down the photons and cause a doppler shift or something ? ( Just kidding ! )

gradL2
1p

limssems

s

If you ﬁnd faith to be honest , show me how . David The whole denominational mindset only causes more problems , sadly . ( See section 7 for
details . ) Thank you . ’The Armenians just shot and shot . Maybe coz they ’re ’quality’ cars ; - ) 200 posts/day . [...]
If you ﬁnd faith to be honest , show me how . David The whole denominational mindset only causes more problems , sadly . ( See section 7 for
details . ) Thank you . ’The Armenians just shot and shot . Maybe coz they ’re ’quality’ cars ; - ) 200 posts/day . [...]

Figure 1: Top:
sci.electronics post (not hybrid). Underlined: Manual relevance ground truth.
Green: evidence for sci.electronics. Task method: CNN. Bottom: hybrid newsgroup post, classiﬁed
talk.politics.mideast. Green: evidence for talk.politics.mideast. Underlined: talk.politics.mideast frag-
ment. Task method: QGRU. Italics: OOV. Bold: rmax position. See supplementary for full texts.

2016) that is designed for word-order sensitive
task methods (e.g., RNNs, CNNs). We show em-
pirically that LIMSSE, LRP (Bach et al., 2015)
and DeepLIFT (Shrikumar et al., 2017) are the
most effective explanation methods (§4): LRP and
DeepLIFT are the most consistent methods, while
LIMSSE wins the hybrid document experiment.

2 Evaluation paradigms

In this section, we introduce two novel evalua-
tion paradigms for explanation methods on two
types of common NLP tasks, small context tasks
and large context tasks. Small context tasks are
deﬁned as those that can be solved by ﬁnding
short, self-contained indicators, such as words and
phrases, and weighing them up (i.e., tasks where
CNNs with pooling can be expected to perform
well). We design the hybrid document paradigm
for evaluating explanation methods on small con-
text tasks. Large context tasks require the cor-
rect handling of long-distance dependencies, such
as subject-verb agreement.1 We design the mor-
phosyntactic agreement paradigm for evaluating
explanation methods on large context tasks.

We could also use human judgments for
evaluation. While we use Mohseni and Ragan
(2018)’s manual relevance benchmark for com-
parison, there are two issues with it: (i) Due to
the cost of human labor, it is limited in size and
domain. (ii) More importantly, a good explana-
tion method should not reﬂect what humans at-
tend to, but what task methods attend to. For in-
stance, the family name “Kolstad” has 11 out of
its 13 appearances in the 20 newsgroups corpus in
sci.electronics posts. Thus, task methods probably
learn it as a sci.electronics indicator. Indeed, the

1Consider deciding the number of [verb] in “the children
in the green house said that the big telescope [verb]” vs.
“the children in the green house who broke the big telescope
[verb]”. The local contexts of “children” or “[verb]” do not
sufﬁce to solve this problem, instead, the large context of the
entire sentence has to be considered.

explanation method in Fig 1 (top) marks “Kolstad”
as relevant, but the human annotator does not.

2.1 Small context: Hybrid document

paradigm

Given a collection of documents, hybrid docu-
ments are created by randomly concatenating doc-
ument fragments. We assume that, on average, the
most relevant input for a class k in a hybrid doc-
ument is located in a fragment that stems from a
document with gold label k. Hence, an explana-
tion method succeeds if it places maximal rele-
vance for k inside the correct fragment.

Formally, let xt be a word inside hybrid docu-
ment X that originates from a document X(cid:48) with
gold label y(X(cid:48)). xt’s gold label y(X, t) is set
to y(X(cid:48)). Let f (X) be the class assigned to the
hybrid document by a task method, and let φ
be an explanation method as deﬁned above. Let
rmax(X, φ) denote the position of the maximally
relevant word in X for the predicted class f (X).
If this maximally relevant word comes from a doc-
ument with the correct gold label, the explanation
method is awarded a hit:
hit(φ, X) = I[y(cid:0)X, rmax(X, φ)(cid:1) = f (X)]
where I[P ] is 1 if P is true and 0 otherwise. In
Fig 1 (bottom), the explanation method gradL2
1p
places rmax outside the correct (underlined) frag-
ment. Therefore, it does not get a hit point, while
limssems

(1)

s does.

The pointing game accuracy of an explana-
tion method is calculated as its total number of
hit points divided by the number of possible hit
points. This is a form of the pointing game
paradigm from computer vision (Zhang et al.,
2016).

2.2 Large context: Morphosyntactic

agreement paradigm

Many natural languages display morphosyntactic
agreement between words v and w. A DNN that

graddot
(cid:82) s
lrp
limssebb
gradL2
(cid:82) s
occ1
limssems

s

the link provided by the editor above [encourages ...]
the link provided by the editor above [encourages ...]
the link provided by the editor above [encourages ...]

few if any events in history [are ...]
few if any events in history [are ...]
few if any events in history [are ...]

Figure 2: Top: verb context classiﬁed singular.
Green: evidence for singular. Task method: GRU.
Bottom: verb context classiﬁed plural. Green: ev-
idence for plural. Task method: LSTM. Under-
lined: subject. Bold: rmax position.

predicts the agreeing feature in w should pay at-
tention to v. For example, in the sentence “the
children with the telescope are home”, the num-
ber of the verb (plural for “are”) can be predicted
from the subject (“children”) without looking at
the verb. If the language allows for v and w to be
far apart (Fig 3, top), successful task methods have
to be able to handle large contexts.

Linzen et al. (2016) show that English verb
number can be predicted by a unidirectional
LSTM with accuracy > 99%, based on left context
alone. When a task method predicts the correct
number, we expect successful explanation meth-
ods to place maximal relevance on the subject:

hittarget(φ, X) = I[rmax(X, φ) = target(X)]

where target(X) is the location of the subject,
and rmax is calculated as above. Regardless of
whether the prediction is correct, we expect rmax
to fall onto a noun that has the predicted number:

hitfeat(φ, X) = I[feat(cid:0)X, rmax(X, φ)(cid:1) = f (X)]

where feat(X, t) is the morphological feature
(here: number) of xt. In Fig 2, rmax on “link”
gives a hittarget point (and a hitfeat point), rmax
on “editor” gives a hitfeat point. gradL2
(cid:82) s does not
get any points as “history” is not a plural noun.

Labels for this task can be automatically gen-
erated using part-of-speech taggers and parsers,
which are available for many languages.

3 Explanation methods

In this section, we deﬁne the explanation meth-
ods that will be evaluated. For our purpose, ex-
planation methods produce word relevance scores
φ(t, k, X), which are speciﬁc to a given class k
and a given input X. φ(t, k, X) > φ(t(cid:48), k, X)
means that xt contributed more than xt(cid:48) to the task
method’s (potential) decision to classify X as k.

3.1 Gradient-based explanation methods

Gradient-based explanation methods approximate
the contribution of some DNN input i to some out-
put o with o’s gradient with respect to i (Simonyan
et al., 2014). In the following, we consider two
output functions o(k, X), the unnormalized class
score s(k, X) and the class probability p(k|X):
s(k, X) = (cid:126)wk · (cid:126)h(X) + bk

(2)

(3)

(4)

p(k|X) =

exp(cid:0)s(k, X)(cid:1)
k(cid:48)=1 exp(cid:0)s(k(cid:48), X)(cid:1)

(cid:80)K

where k is the target class, (cid:126)h(X) the document
representation (e.g., an RNN’s ﬁnal hidden layer),
(cid:126)wk (resp. bk) k’s weight vector (resp. bias).
The simple gradient of o(k, X) w.r.t. i is:

grad1(i, k, X) =

∂o(k, X)
∂i

grad1 underestimates the importance of inputs
that saturate a nonlinearity (Shrikumar et al.,
2017). To address this, Sundararajan et al. (2017)
integrate over all gradients on a linear interpola-
tion α ∈ [0, 1] between a baseline input ¯X (here:
all-zero embeddings) and X:
grad(cid:82) (i, k, X) = (cid:82) 1

∂α

∂o(k, ¯X+α(X− ¯X))
∂i
α=0
∂o(k, ¯X+ m
M (X− ¯X))
∂i

(5)

≈ 1
M

(cid:80)M

m=1

where M is a big enough constant (here: 50).

In NLP, symbolic inputs (e.g., words) are often
represented as one-hot vectors (cid:126)xt ∈ {1, 0}|V | and
embedded via a real-valued matrix: (cid:126)et = M(cid:126)xt.
Gradients are computed with respect to individual
entries of E = [(cid:126)e1 . . . (cid:126)e|X|]. Bansal et al. (2016)
and Hechtlinger (2016) use the L2 norm to reduce
vectors of gradients to single values:

φgradL2(t, k, X) = ||grad((cid:126)et, k, E)||
where grad((cid:126)et, k, E) is a vector of elementwise
gradients w.r.t. (cid:126)et. Denil et al. (2015) use the dot
product of the gradient vector and the embedding2,
i.e., the gradient of the “hot” entry in (cid:126)xt:

(6)

(7)

φgraddot(t, k, X) = (cid:126)et · grad((cid:126)et, k, E)
We use “grad1” for Eq 4, “grad(cid:82) ” for Eq 5, “p”
for Eq 3, “s” for Eq 2, “L2” for Eq 6 and “dot”
for Eq 7. This gives us eight explanation meth-
1p , graddot
ods: gradL2
1p , gradL2
(cid:82) s,
(cid:82) s , graddot
gradL2
(cid:82) p .
, replace (cid:126)et with (cid:126)et − (cid:126)¯et. Since our baseline

(cid:82) p, graddot

1s , graddot

1s , gradL2

2For graddot

(cid:82)

embeddings are all-zeros, this is equivalent.

3.2 Layer-wise relevance propagation

Layer-wise relevance propagation (LRP)
is a
backpropagation-based explanation method devel-
oped for fully connected neural networks and
CNNs (Bach et al., 2015) and later extended to
LSTMs (Arras et al., 2017b).
In this paper, we
use Epsilon LRP (Eq 58, Bach et al. (2015)). Re-
member that the activation of neuron j, aj, is the
sum of weighted upstream activations, (cid:80)
i aiwi,j,
plus bias bj, squeezed through some nonlinearity.
We denote the pre-nonlinearity activation of j as
a(cid:48)
j. The relevance of j, R(j), is distributed to up-
stream neurons i proportionally to the contribution
that i makes to a(cid:48)

j in the forward pass:

R(i) =

R(j)

(cid:88)

j

aiwi,j
j + esign(a(cid:48)

a(cid:48)

j)

(8)

This ensures that relevance is conserved between
layers, with the exception of relevance attributed
to bj. To prevent numerical instabilities, esign(a(cid:48))
returns −(cid:15) if a(cid:48) < 0 and (cid:15) otherwise. We set (cid:15) =
.001. The full algorithm is:

R(Lk(cid:48)) = s(k, X)I[k(cid:48) = k]
... recursive application of Eq 8 ...

φlrp(t, k, X) =

R(et,j)

dim((cid:126)et)
(cid:88)

j=1

where L is the ﬁnal layer, k the target class and
R(et,j) the relevance of dimension j in the t’th
embedding vector. For (cid:15) → 0 and provided that all
nonlinearities up to the unnormalized class score
are relu, Epsilon LRP is equivalent to the prod-
uct of input and raw score gradient (here: graddot
1s )
(Kindermans et al., 2016). In our experiments, the
second requirement holds only for CNNs.

Experiments by Ancona et al. (2017) (see §6)
suggest that LRP does not work well for LSTMs
if all neurons – including gates – participate in
backpropagation. We therefore use Arras et al.
(2017b)’s modiﬁcation and treat sigmoid-activated
gates as time step-speciﬁc weights rather than neu-
rons. For instance, the relevance of LSTM candi-
date vector (cid:126)gt is calculated from memory vector (cid:126)ct
and input gate vector(cid:126)it as

in(cid:126)it do not receive any relevance themselves. See
supplementary material for formal deﬁnitions of
Epsilon LRP for different architectures.

3.3 DeepLIFT

DeepLIFT (Shrikumar et al., 2017) is another
backpropagation-based explanation method. Un-
it does not explain s(k, X), but
like LRP,
s(k, X)−s(k, ¯X), where ¯X is some baseline input
(here: all-zero embeddings). Following Ancona
et al. (2018) (Eq 4), we use this backpropagation
rule:

R(i) =

R(j)

(cid:88)

j

aiwi,j − ¯aiwi,j
j + esign(a(cid:48)

a(cid:48)
j − ¯a(cid:48)

j − ¯a(cid:48)
j)

where ¯a refers to the forward pass of the base-
line. Note that the original method has a dif-
ferent mechanism for avoiding small denomina-
tors; we use esign for compatibility with LRP.
The DeepLIFT algorithm is started with R(Lk(cid:48)) =
(cid:0)s(k, X)−s(k, ¯X)(cid:1)I[k(cid:48) = k]. On gated (Q)RNNs,
we proceed analogous to LRP and treat gates as
weights.

3.4 Cell decomposition for gated RNNs

The cell decomposition explanation method for
LSTMs (Murdoch and Szlam, 2017) decomposes
the unnormalized class score s(k, X) (Eq 2) into
additive contributions. For every time step t, we
compute how much of (cid:126)ct “survives” until the ﬁnal
step T and contributes to s(k, X). This is achieved
by applying all future forget gates (cid:126)f , the ﬁnal tanh
nonlinearity, the ﬁnal output gate (cid:126)oT , as well as the
class weights of k to (cid:126)ct. We call this quantity “net
load of t for class k”:

nl(t, k, X) = (cid:126)wk ·

(cid:16)
(cid:126)oT (cid:12) tanh(cid:0)(

(cid:126)fj) (cid:12) (cid:126)ct

(cid:1)(cid:17)

T
(cid:89)

j=t+1

where (cid:12) and (cid:81) are applied elementwise. The rel-
evance of t is its gain in net load relative to t − 1:
φdecomp(t, k, X) = nl(t, k, X) − nl(t − 1, k, X).
For GRU, we change the deﬁnition of net load:

nl(t, k, X) = (cid:126)wk · (cid:0)(

(cid:126)zj) (cid:12) (cid:126)ht

(cid:1)

T
(cid:89)

j=t+1

R(gt,d) = R(ct,d)

gt,d · it,d
ct,d + esign(ct,d)

where (cid:126)z are GRU update gates.

3.5

Input perturbation methods

This is equivalent to applying Eq 8 while treating
(cid:126)it as a diagonal weight matrix. The gate neurons

Input perturbation methods assume that the re-
moval or masking of relevant inputs changes the

output (Zeiler and Fergus, 2014). Omission-
based methods remove inputs completely (K´ad´ar
et al., 2017), while occlusion-based methods re-
place them with a baseline (Li et al., 2016b). In
computer vision, perturbations are usually applied
to patches, as neighboring pixels tend to correlate
(Zintgraf et al., 2017). To calculate the omitN
(resp. occN ) relevance of word xt, we delete (resp.
occlude), one at a time, all N -grams that contain
xt, and average the change in the unnormalized
class score from Eq 2:

(cid:2)s(k, [(cid:126)e1 . . . (cid:126)e|X|])
φ[omit|occ]N (t, k, X) = (cid:80)N
−s(k, [(cid:126)e1 . . . (cid:126)et−N −1+j](cid:107)¯E(cid:107)[(cid:126)et+j . . . (cid:126)e|X|])(cid:3) 1

j=1

N

where (cid:126)et are embedding vectors, (cid:107) denotes con-
catenation and ¯E is either a sequence of length
zero (φomit) or a sequence of N baseline (here:
all-zero) embedding vectors (φocc).

3.6 LIMSSE: LIME for NLP

Local Interpretable Model-agnostic Explanations
(LIME) (Ribeiro et al., 2016) is a framework
for explaining predictions of complex classiﬁers.
LIME approximates the behavior of classiﬁer f in
the neighborhood of input X with an interpretable
(here: linear) model. The interpretable model is
trained on samples Z1 . . . ZN (here: N = 3000),
which are randomly drawn from X, with “gold la-
bels” f (Z1) . . . f (ZN ).

Since RNNs and CNNs respect word or-
der, we cannot use the bag of words sam-
pling method from the original description
of LIME. Instead, we introduce Local Inter-
pretable Model-agnostic Substring-based Expla-
nations (LIMSSE). LIMSSE uniformly samples
a length ln (here: 1 ≤ ln ≤ 6) and a start-
ing point sn, which deﬁne the substring Zn =
[(cid:126)xsn . . . (cid:126)xsn+ln−1]. To the linear model, Zn is rep-
resented by a binary vector (cid:126)zn ∈ {0, 1}|X|, where
zn,t = I[sn ≤ t < sn + ln].

We learn a linear weight vector ˆ(cid:126)vk ∈ R|X|,
whose entries are word relevances for k,
i.e.,
φlimsse(t, k, X) = ˆvk,t. To optimize it, we experi-
ment with three loss functions. The ﬁrst, which we
will refer to as limssebb, assumes that our DNN is
a total black box that delivers only a classiﬁcation:

(cid:0)p(k(cid:48)|Zn)(cid:1). The black
where f (Zn) = argmaxk(cid:48)
box approach is maximally general, but insensitive
to the magnitude of evidence found in Zn. Hence,
we also test magnitude-sensitive loss functions:

ˆ(cid:126)vk = argmin

(cid:0)(cid:126)zn · (cid:126)vk − o(k, Zn)(cid:1)2

(cid:88)

n

(cid:126)vk

where o(k, Zn) is one of s(k, Zn) or p(k|Zn). We
refer to these as limssems
s

and limssems
p .

4 Experiments

4.1 Hybrid document experiment

For the hybrid document experiment, we use the
20 newsgroups corpus (topic classiﬁcation) (Lang,
1995) and reviews from the 10th yelp dataset
challenge (binary sentiment analysis)3. We train
ﬁve DNNs per corpus: a bidirectional GRU (Cho
et al., 2014), a bidirectional LSTM (Hochreiter
and Schmidhuber, 1997), a 1D CNN with global
max pooling (Collobert et al., 2011), a bidirec-
tional Quasi-GRU (QGRU), and a bidirectional
Quasi-LSTM (QLSTM). The Quasi-RNNs are 1D
CNNs with a feature-wise gated recursive pooling
layer (Bradbury et al., 2017). Word embeddings
are R300 and initialized with pre-trained GloVe
embeddings (Pennington et al., 2014)4. The main
layer has a hidden size of 150 (bidirectional ar-
chitectures: 75 dimensions per direction). For the
QRNNs and CNN, we use a kernel width of 5. In
all ﬁve architectures, the resulting document rep-
resentation is projected to 20 (resp.
two) dimen-
sions using a fully connected layer, followed by a
softmax. See supplementary material for details
on training and regularization.

After training, we sentence-tokenize the test
sets, shufﬂe the sentences, concatenate ten sen-
tences at a time and classify the resulting hybrid
documents. Documents that are assigned a class
that is not the gold label of at least one con-
stituent word are discarded (yelp: < 0.1%; 20
newsgroups: 14% - 20%). On the remaining docu-
ments, we use the explanation methods from §3 to
ﬁnd the maximally relevant word for each predic-
tion. The random baseline samples the maximally
relevant word from a uniform distribution.

For reference, we also evaluate on a hu-
man judgment benchmark (Mohseni and Ra-
It contains
gan (2018), Table 2, C11-C15).

3www.yelp.com/dataset_challenge
4http://nlp.stanford.edu/data/glove.

ˆ(cid:126)vk = argmin

−(cid:2)log(cid:0)σ((cid:126)zn · (cid:126)vk)(cid:1)I[f (Zn) = k]

(cid:88)

n

(cid:126)vk

+ log(cid:0)1 − σ((cid:126)zn · (cid:126)vk)(cid:1)I[f (Zn) (cid:54)= k](cid:3)

840B.300d.zip

column

C01 C02 C03 C04 C05 C06 C07 C08 C09 C10 C11 C12 C13 C14 C15 C16 C17 C18 C19 C20 C21 C22 C23 C24 C25 C26 C27

hybrid document experiment

man. groundtruth

morphosyntactic agreement experiment

yelp

20 newsgroups

20 newsgroups

f (X) = y(X)

hittarget

hitfeat

N
N
C

U
R
G

U
R
G
Q

M
T
S
L

M
T
S
L
Q

N
N
C

U
R
G

U
R
G
Q

M
T
S
L

M
T
S
L
Q

U
R
G

U
R
G
Q

M
T
S
L

M
T
S
L
Q

f (X) (cid:54)= y(X)
M
T
S
L
Q

U
R
G
Q

M
T
S
L

U
R
G

U
R
G

U
R
G
Q

M
T
S
L

M
T
S
L
Q

N
N
C

U
R
G

U
R
G
Q

M
T
S
L

M
T
S
L
Q

φ
gradL2
.61 .68 .67 .70 .68 .45 .47 .25 .33 .79
1s
gradL2
.57 .67 .67 .70 .74 .40 .43 .26 .34 .70
1p
gradL2
.71 .66 .69 .71 .70 .58 .32 .26 .21 .82
(cid:82) s
gradL2
.71 .70 .72 .71 .77 .56 .34 .30 .23 .81
(cid:82) p
graddot
.88 .85 .81 .77 .86 .79 .76 .59 .72 .89
1s
graddot
.92 .88 .84 .79 .95 .78 .72 .59 .72 .81
1p
graddot
.84 .90 .85 .87 .87 .81 .68 .60 .68 .89
(cid:82) s
graddot
.86 .89 .84 .89 .96 .80 .69 .62 .73 .89
(cid:82) p
omit1
.79 .82 .85 .87 .61 .78 .75 .54 .76 .82
omit3
.89 .80 .89 .88 .59 .79 .71 .72 .81 .76
omit7
.92 .88 .91 .91 .70 .79 .77 .77 .84 .84
occ1
.80 .71 .74 .84 .61 .78 .73 .60 .77 .82
occ3
.92 .61 .93 .85 .59 .78 .63 .74 .74 .76
occ7
.92 .77 .93 .90 .70 .78 .62 .74 .77 .84
decomp
.79 .88 .92 .88
-
lrp
.92 .87 .91 .84 .86 .82 .83 .79 .85 .89
deeplift
.91 .89 .94 .85 .87 .82 .83 .78 .84 .89
limssebb
.81 .82 .83 .84 .78 .78 .81 .78 .80 .84
limssems
.94 .94 .93 .93 .91 .85 .87 .83 .86 .89
limssems
.87 .88 .85 .86 .94 .85 .86 .83 .86 .90
random .69 .67 .70 .69 .66 .20 .19 .22 .22 .21
last
-
-
-
-
3022 ≤ N ≤ 3230
N

-
-
-
7551 ≤ N ≤ 7554

.75 .79 .77 .80

p

s

-

-

-

-

.26 .31 .07 .18 .74
.18 .35 .07 .13 .66
.23 .15 .11 .08 .76
.13 .08 .14 .01 .78
.80 .70 .14 .47 .79
.71 .59 .20 .44 .69
.82 .64 .21 .26 .80
.80 .53 .40 .54 .78
.80 .48 .33 .48 .65
.77 .37 .36 .49 .61
.77 .49 .44 .55 .65
.77 .49 .19 .10 .65
.74 .37 .32 .35 .61
.74 .35 .43 .39 .65
.54 .36 .72 .51
-
.85 .72 .74 .81 .79
.84 .72 .70 .81 .80
.52 .53 .53 .54 .57
.85 .84 .76 .84 .82
.81 .80 .74 .76 .76
.09 .09 .06 .06 .08
-
-
-
137 ≤ N ≤ 150

-

-

.48 .23 .63 .19 .52 .27 .73 .22 .09 .11 .19 .19
.48 .22 .63 .18 .53 .26 .73 .21 .09 .09 .18 .11
.69 .67 .68 .51 .73 .70 .75 .55 .19 .22 .20 .20
.68 .77 .50 .70 .74 .82 .54 .78 .19 .21 .19 .30
.81 .62 .73 .56 .85 .66 .81 .59 .42 .34 .46 .36
.79 .58 .74 .54 .83 .61 .81 .56 .41 .33 .46 .35
.90 .87 .78 .84 .94 .92 .83 .89 .54 .51 .46 .52
.87 .85 .68 .84 .93 .92 .74 .93 .53 .48 .42 .51
.81 .81 .79 .80 .86 .87 .86 .84 .43 .45 .44 .45
.74 .77 .73 .73 .82 .84 .82 .79 .41 .45 .42 .46
.76 .80 .66 .74 .85 .88 .78 .80 .40 .48 .43 .47
.91 .85 .86 .86 .94 .88 .89 .88 .50 .44 .46 .47
.74 .73 .71 .72 .78 .76 .76 .76 .43 .37 .41 .43
.64 .65 .63 .65 .73 .73 .72 .73 .36 .35 .39 .43
.84 .87 .86 .90 .90 .93 .92 .96 .52 .58 .57 .63
.90 .90 .86 .91 .95 .95 .91 .95 .58 .60 .52 .63
.91 .90 .85 .91 .95 .95 .90 .95 .59 .59 .52 .63
.43 .41 .44 .42 .54 .51 .56 .52 .39 .43 .42 .41
.62 .62 .67 .63 .75 .74 .82 .75 .52 .53 .55 .53
.62 .62 .67 .63 .75 .74 .82 .75 .51 .53 .55 .53
.27 .27 .27 .27 .33 .33 .33 .33 .12 .13 .12 .12
.66 .67 .66 .67 .76 .77 .76 .77 .21 .27 .25 .26

N ≈ 1400000

N ≈ 20000

Table 2: Pointing game accuracies in hybrid document experiment (left), on manually annotated bench-
mark (middle) and in morphosyntactic agreement experiment (right). hittarget (resp. hitfeat): maximal
relevance on subject (resp. on noun with the predicted number feature). Bold: top explanation method.
Underlined: within 5 points of top explanation method.

188 documents from the 20 newsgroups test set
(classes sci.med and sci.electronics), with one
manually created list of relevant words per doc-
ument. We discard documents that are incorrectly
classiﬁed (20% - 27%) and deﬁne: hit(φ, X) =
I[rmax(X, φ) ∈ gt(X)], where gt(X) is the man-
ual ground truth.

4.2 Morphosyntactic agreement experiment

For the morphosyntactic agreement experiment,
we use automatically annotated English Wikipedia
sentences by Linzen et al. (2016)5. For our pur-
pose, a sample consists of: all words preceding the
verb: X = [x1 · · · xT ]; part-of-speech (POS) tags:
pos(X, t) ∈ {VBZ, VBP, NN, NNS, . . .}; and the
position of the subject: target(X) ∈ [1, T ]. The
number feature is derived from the POS:

feat(X, t) =






Sg
Pl
n/a

if pos(X, t) ∈ {VBZ, NN}
if pos(X, t) ∈ {VBP, NNS}
otherwise

The gold label of a sentence is the number of its
verb, i.e., y(X) = feat(X, T + 1).

5www.tallinzen.net/media/rnn_

agreement/agr_50_mostcommon_10K.tsv.gz

As task methods, we replicate Linzen et al.
(2016)’s unidirectional LSTM (R50 randomly
initialized word embeddings, hidden size 50).
We also train unidirectional GRU, QGRU and
QLSTM architectures with the same dimension-
ality. We use the explanation methods from §3 to
ﬁnd the most relevant word for predictions on the
test set. As described in §2.2, explanation methods
are awarded a hittarget (resp. hitfeat) point if this
word is the subject (resp. a noun with the predicted
number feature). For reference, we use a random
baseline as well as a baseline that assumes that the
most relevant word directly precedes the verb.

5 Discussion

5.1 Explanation methods

Our experiments suggest that explanation methods
for neural NLP differ in quality.

As in previous work (see §6), gradient L2
norm (gradL2) performs poorly, especially on
RNNs. We assume that this is due to its inability
to distinguish relevances for and against k.

Gradient embedding dot product (graddot)
is competitive on CNN (Table 2, graddot
1p C05,
graddot
1s C10, C15), presumably because relu is
linear on positive inputs, so gradients are exact in-

decomp
deeplift
limssems

p

lrp

limssems

p

initially a pagan culture , detailed information about the return of the christian religion to the islands during the norse-era [is ...]
initially a pagan culture , detailed information about the return of the christian religion to the islands during the norse-era [is ...]
initially a pagan culture , detailed information about the return of the christian religion to the islands during the norse-era [is ...]

Your day is done . Deﬁnitely looking forward to going back . All three were outstanding ! I would highly recommend going here to anyone .
We will see if anyone returns the message my boyfriend left . The price is unbelievable ! And our guys are on lunch so we ca n’t ﬁt you in . ” It
’s good , standard froyo . The pork shoulder was THAT tender . Try it with the Tomato Basil cram sauce .
Your day is done . Deﬁnitely looking forward to going back . All three were outstanding ! I would highly recommend going here to anyone .
We will see if anyone returns the message my boyfriend left . The price is unbelievable ! And our guys are on lunch so we ca n’t ﬁt you in . ” It
’s good , standard froyo . The pork shoulder was THAT tender . Try it with the Tomato Basil cram sauce .

Figure 3: Top: verb context classiﬁed singular. Task method: LSTM. Bottom: hybrid yelp review,
classiﬁed positive. Task method: QLSTM.

stead of approximate. graddot also has decent per-
formance for GRU (graddot
(cid:82) s C{06,
11, 16, 20, 24}), perhaps because GRU hidden ac-
tivations are always in [-1,1], where tanh and σ
are approximately linear.

1p C01, graddot

Integrated gradient (grad(cid:82) ) mostly outper-
forms simple gradient (grad1), though not consis-
tently (C01, C07). Contrary to expectation, in-
tegration did not help much with the failure of
the gradient method on LSTM on 20 newsgroups
(graddot
in C08, C13), which we had
assumed to be due to saturation of tanh on large
absolute activations in (cid:126)c. Smaller intervals may be
needed to approximate the integration, however,
this means additional computational cost.

vs. graddot

1

(cid:82)

1s vs. graddot

The gradient of s(k, X) performs better or sim-
ilar to the gradient of p(k|X). The main exception
is yelp (graddot
1p , C01-C05). This is
probably due to conﬂation by p(k|X) of evidence
for k (numerator in Eq 3) and against competi-
tor classes (denominator). In a two-class scenario,
there is little incentive to keep classes separate,
leading to information ﬂow through the denomi-
In future work, we will replace the two-
nator.
way softmax with a one-way sigmoid such that
φ(t, 0, X) := −φ(t, 1, X).

LRP and DeepLIFT are the most consistent
explanation methods across evaluation paradigms
and task methods. (The comparatively low point-
ing game accuracies on the yelp QRNNs and CNN
(C02, C04, C05) are probably due to the fact
that they explain s(k, .) in a two-way softmax,
see above.) On CNN (C05, C10, C15), LRP
and graddot
1s perform almost identically, suggest-
ing that they are indeed quasi-equivalent on this ar-
chitecture (see §3.2). On (Q)RNNs, modiﬁed LRP
and DeepLIFT appear to be superior to the gradi-
1s , deeplift vs. graddot
ent method (lrp vs. graddot
(cid:82) s ,
C01-C04, C06-C09, C11-C14, C16-C27).

Decomposition performs well on LSTM, es-
pecially in the morphosyntactic agreement exper-

iment, but it is inconsistent on other architec-
tures. Gated RNNs have a long-term additive and
a multiplicative pathway, and the decomposition
method only detects information traveling via the
additive one. Miao et al. (2016) show qualita-
tively that GRUs often reorganize long-term mem-
ory abruptly, which might explain the difference
between LSTM and GRU. QRNNs only have ad-
ditive recurrent connections; however, given that
(cid:126)ct (resp. (cid:126)ht) is calculated by convolution over sev-
eral time steps, decomposition relevance can be in-
correctly attributed inside that window. This likely
is the reason for the stark difference between the
performance of decomposition on QRNNs in the
hybrid document experiment and on the manually
labeled data (C07, C09 vs. C12, C14). Overall,
we do not recommend the decomposition method,
because it fails to take into account all routes by
which information can be propagated.

Omission and occlusion produce inconsis-
tent results in the hybrid document experiment.
Shrikumar et al. (2017) show that perturbation
methods can lack sensitivity when there are more
relevant inputs than the “perturbation window”
covers. In the morphosyntactic agreement experi-
ment, omission is not competitive; we assume that
this is because it interferes too much with syntactic
structure. occ1 does better (esp. C16-C19), possi-
bly because an all-zero “placeholder” is less dis-
ruptive than word removal. But despite some high
scores, it is less consistent than other explanation
methods.

LIMSSE

Magnitude-sensitive

(limssems)
consistently outperforms black-box LIMSSE
(limssebb), which suggests that numerical out-
puts should be used for approximation where
possible.
In the hybrid document experiment,
magnitude-sensitive LIMSSE outperforms the
other explanation methods (exceptions: C03,
C05). However, it fails in the morphosyntactic
agreement experiment (C16-C27).
In fact, we
expect LIMSSE to be unsuited for large context

problems, as it cannot discover dependencies
whose range is bigger than a given text sample.
In Fig 3 (top), limssems
highlights any singular
p
noun without taking into account how that noun
ﬁts into the overall syntactic structure.

5.2 Evaluation paradigms

The assumptions made by our automatic evalua-
tion paradigms have exceptions: (i) the correlation
between fragment of origin and relevance does not
always hold (e.g., a positive review may contain
negative fragments, and will almost certainly con-
tain neutral fragments); (ii) in morphological pre-
diction, we cannot always expect the subject to be
the only predictor for number. In Fig 2 (bottom)
for example, “few” is a reasonable clue for plural
despite not being a noun. This imperfect ground
truth means that absolute pointing game accura-
cies should be taken with a grain of salt; but we
argue that this does not invalidate them for com-
parisons.

We also point out that there are characteristics
of explanations that may be desirable but are not
reﬂected by the pointing game. Consider Fig 3
(bottom). Both explanations get hit points, but the
lrp explanation appears “cleaner” than limssems
p ,
with relevance concentrated on fewer tokens.

6 Related work

6.1 Explanation methods

Explanation methods can be divided into local
and global methods (Doshi-Velez and Kim, 2017).
Global methods infer general statements about
what a DNN has learned, e.g., by clustering docu-
ments (Aubakirova and Bansal, 2016) or n-grams
(K´ad´ar et al., 2017) according to the neurons that
they activate. Li et al. (2016a) compare embed-
dings of speciﬁc words with reference points to
measure how drastically they were changed dur-
ing training. In computer vision, Simonyan et al.
(2014) optimize the input space to maximize the
activation of a speciﬁc neuron. Global explanation
methods are of limited value for explaining a spe-
ciﬁc prediction as they represent average behavior.
Therefore, we focus on local methods.

Local explanation methods explain a decision
taken for one speciﬁc input at a time. We have
attempted to include all important local methods
for NLP in our experiments (see §3). We do
not address self-explanatory models (e.g., atten-
tion (Bahdanau et al., 2015) or rationale models

(Lei et al., 2016)), as these are very speciﬁc archi-
tectures that may not be not applicable to all tasks.

6.2 Explanation evaluation

According to Doshi-Velez and Kim (2017)’s
taxonomy of explanation evaluation paradigms,
application-grounded paradigms test how well an
explanation method helps real users solve real
tasks (e.g., doctors judge automatic diagnoses);
human-grounded paradigms rely on proxy tasks
(e.g., humans rank task methods based on expla-
nations); functionally-grounded paradigms work
without human input, like our approach.

Arras et al. (2016) (cf. Samek et al. (2016))
propose a functionally-grounded explanation eval-
uation paradigm for NLP where words in a cor-
rectly (resp.
incorrectly) classiﬁed document are
deleted in descending (resp. ascending) order of
relevance. They assume that the fewer words must
be deleted to reduce (resp. increase) accuracy, the
better the explanations. According to this metric,
LRP (§3.2) outperforms gradL2 on CNNs (Arras
et al., 2016) and LSTMs (Arras et al., 2017b) on
20 newsgroups. Ancona et al. (2017) perform the
same experiment with a binary sentiment analy-
sis LSTM. Their graph shows occ1, graddot
and
graddot
tied in ﬁrst place, while LRP, DeepLIFT
and the gradient L1 norm lag behind. Note that
their treatment of LSTM gates in LRP / DeepLIFT
differs from our implementation.

1

(cid:82)

An issue with the word deletion paradigm is that
it uses syntactically broken inputs, which may in-
troduce artefacts (Sundararajan et al., 2017).
In
our hybrid document paradigm, inputs are syntac-
tically intact (though semantically incoherent at
the document level); the morphosyntactic agree-
ment paradigm uses unmodiﬁed inputs.

Another class of functionally-grounded evalu-
ation paradigms interprets the performance of a
secondary task method, on inputs that are derived
from (or altered by) an explanation method, as a
proxy for the quality of that explanation method.
Murdoch and Szlam (2017) build a rule-based
classiﬁer from the most relevant phrases in a cor-
pus (task method: LSTM). The classiﬁer based
on decomp (§3.4) outperforms the gradient-based
classiﬁer, which is in line with our results. Ar-
ras et al. (2017a) build document representations
by summing over word embeddings weighted by
relevance scores (task method: CNN). They show
that K-nearest neighbor performs better on doc-

ument representations derived with LRP than on
those derived with gradL2, which also matches our
results. Denil et al. (2015) condense documents
by extracting top-K relevant sentences, and let the
original task method (CNN) classify them. The
accuracy loss, relative to uncondensed documents,
is smaller for graddot than for heuristic baselines.
In the domain of human-based evaluation
paradigms, Ribeiro et al. (2016) compare differ-
ent variants of LIME (§3.6) by how well they help
non-experts clean a corpus from words that lead
to overﬁtting. Selvaraju et al. (2017) assess how
well explanation methods help non-experts iden-
tify the more accurate out of two object recogni-
tion CNNs. These experiments come closer to real
use cases than functionally-grounded paradigms;
however, they are less scalable.

7 Summary

We conducted the ﬁrst comprehensive evaluation
of explanation methods for NLP, an important un-
dertaking because there is a need for understand-
ing the behavior of DNNs.

To conduct this study, we introduced evalua-
tion paradigms for explanation methods for two
classes of NLP tasks, small context tasks (e.g.,
topic classiﬁcation) and large context tasks (e.g.,
morphological prediction). Neither paradigm re-
quires manual annotations. We also introduced
LIMSSE, a substring-based explanation method
inspired by LIME and designed for NLP.

Based on our experimental results, we recom-
mend LRP, DeepLIFT and LIMSSE for small con-
text tasks and LRP and DeepLIFT for large con-
text tasks, on all ﬁve DNN architectures that we
tested. On CNNs and possibly GRUs, the (inte-
grated) gradient embedding dot product is a good
alternative to DeepLIFT and LRP.

8 Code

package:

implementation of LIMSSE,
the gradi-
Our
perturbation and decomposition meth-
ent,
the
ods can be found in our branch of
www.github.com/
keras
NPoe/keras.
To re-run our experiments,
in www.github.com/NPoe/
see
neural-nlp-explanation-experiment.
Our LRP implementation (same repository) is
adapted from Arras et al. (2017b)6.

scripts

6https://github.com/ArrasL/LRP_for_

LSTM

References
Marco Ancona, Enea Ceolini, Cengiz ¨Oztireli, and
Markus Gross. 2017. A uniﬁed view of gradient-
based attribution methods for deep neural networks.
In Conference on Neural Information Processing
System, Long Beach, USA.

Marco Ancona, Enea Ceolini, Cengiz ¨Oztireli, and
Markus Gross. 2018. Towards better understanding
of gradient-based attribution methods for deep neu-
ral networks. In International Conference on Learn-
ing Representations, Vancouver, Canada.

Leila Arras, Franziska Horn, Gr´egoire Montavon,
Klaus-Robert M¨uller, and Wojciech Samek. 2016.
Explaining predictions of non-linear classiﬁers in
NLP. In First Workshop on Representation Learn-
ing for NLP, pages 1–7, Berlin, Germany.

Leila Arras, Franziska Horn, Gr´egoire Montavon,
Klaus-Robert M¨uller, and Wojciech Samek. 2017a.
What is relevant in a text document?: An inter-
PloS one,
pretable machine learning approach.
12(8):e0181142.

Leila Arras, Gr´egoire Montavon, Klaus-Robert M¨uller,
and Wojciech Samek. 2017b. Explaining recurrent
neural network predictions in sentiment analysis. In
Eighth Workshop on Computational Approaches to
Subjectivity, Sentiment and Social Media Analysis,
pages 159–168, Copenhagen, Denmark.

Malika Aubakirova and Mohit Bansal. 2016. Interpret-
ing neural networks to improve politeness compre-
hension. In Empirical Methods in Natural Language
Processing, page 2035–2041, Austin, USA.

Sebastian Bach, Alexander Binder, Gr´egoire Mon-
tavon, Frederick Klauschen, Klaus-Robert M¨uller,
and Wojciech Samek. 2015. On pixel-wise explana-
tions for non-linear classiﬁer decisions by layer-wise
relevance propagation. PloS one, 10(7):e0130140.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations, San Diego,
USA.

Trapit Bansal, David Belanger, and Andrew McCal-
lum. 2016. Ask the GRU: Multi-task learning for
In ACM Conference
deep text recommendations.
on Recommender Systems, pages 107–114, Boston,
USA.

James Bradbury, Stephen Merity, Caiming Xiong, and
Richard Socher. 2017. Quasi-recurrent neural net-
In International Conference on Learning
works.
Representations, Toulon, France.

Kyunghyun Cho, Bart Van Merri¨enboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. In Eighth Workshop on Syntax, Semantics
and Structure in Statistical Translation, pages 103–
111, Doha, Qatar.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
Journal of Machine Learning Research,
scratch.
12(Aug):2493–2537.

Misha Denil, Alban Demiraj, and Nando de Freitas.
2015. Extraction of salient sentences from labelled
documents. In International Conference on Learn-
ing Representations, San Diego, USA.

Finale Doshi-Velez and Been Kim. 2017. A roadmap
for a rigorous science of interpretability. CoRR,
abs/1702.08608.

Bryce Goodman and Seth Flaxman. 2016. European
union regulations on algorithmic decision-making
and a “right to explanation”. In ICML Workshop on
Human Interpretability in Machine Learning, pages
26–30, New York, USA.

Yotam Hechtlinger. 2016. Interpretation of prediction
models using the input gradient. In Conference on
Neural Information Processing Systems, Barcelona,
Spain.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Akos K´ad´ar, Grzegorz Chrupała, and Afra Alishahi.
2017. Representation of linguistic form and func-
tion in recurrent neural networks. Computational
Linguistics, 43(4):761–780.

Pieter-Jan Kindermans, Kristof Sch¨utt, Klaus-Robert
M¨uller, and Sven D¨ahne. 2016. Investigating the in-
ﬂuence of noise and distractors on the interpretation
of neural networks. In Conference on Neural Infor-
mation Processing Systems, Barcelona, Spain.

Ken Lang. 1995. Newsweeder: Learning to ﬁlter
In International Conference on Machine

netnews.
Learning, pages 331–339, Tahoe City, USA.

Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016.
In Empirical
Rationalizing neural predictions.
Methods in Natural Language Processing, pages
107–117, Austin, USA.

Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.
2016a. Visualizing and understanding neural mod-
In NAACL-HLT, pages 681–691, San
els in NLP.
Diego, USA.

Jiwei Li, Will Monroe, and Dan Jurafsky. 2016b. Un-
derstanding neural networks through representation
erasure. CoRR, abs/1612.08220.

Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.
2016. Assessing the ability of LSTMs to learn
syntax-sensitive dependencies. Transactions of the
Association for Computational Linguistics, 4:521–
535.

Yajie Miao, Jinyu Li, Yongqiang Wang, Shi-Xiong
Zhang, and Yifan Gong. 2016. Simplifying long
short-term memory acoustic models for fast train-
In International Conference
ing and decoding.
on Acoustics, Speech and Signal Processing, pages
2284–2288.

Sina Mohseni and Eric D Ragan. 2018. A human-
grounded evaluation benchmark for local explana-
tions of machine learning. CoRR, abs/1801.05075.

W James Murdoch and Arthur Szlam. 2017. Auto-
matic rule extraction from long short term memory
networks. In International Conference on Learning
Representations, Toulon, France.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543, Doha, Qatar.

Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2016. Why should I trust you?: Ex-
In ACM
plaining the predictions of any classiﬁer.
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 1135–1144, San
Francisco, California.

Wojciech Samek, Alexander Binder, Gr´egoire Mon-
tavon, Sebastian Lapuschkin, and Klaus-Robert
M¨uller. 2016. Evaluating the visualization of what
IEEE trans-
a deep neural network has learned.
actions on neural networks and learning systems,
28(11):2660–2673.

Ramprasaath R Selvaraju, Michael Cogswell, Ab-
hishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. 2017. Grad-cam: Visual expla-
nations from deep networks via gradient-based lo-
calization. In IEEE Conference on Computer Vision
and Pattern Recognition, pages 618–626, Honolulu,
Hawaii.

Avanti Shrikumar, Peyton Greenside, and Anshul Kun-
daje. 2017. Learning important features through
propagating activation differences. In International
Conference on Machine Learning, pages 3145–
3153, Sydney, Australia.

Karen Simonyan, Andrea Vedaldi, and Andrew Zisser-
man. 2014. Deep inside convolutional networks: Vi-
sualising image classiﬁcation models and saliency
In International Conference on Learning
maps.
Representations, Banff, Canada.

Mukund Sundararajan, Ankur Taly, and Qiqi Yan.
2017. Axiomatic attribution for deep networks.
In International Conference on Machine Learning,
Sydney, Australia.

Matthew D Zeiler and Rob Fergus. 2014. Visualizing
In Eu-
and understanding convolutional networks.
ropean Conference on Computer Vision, pages 818–
833, Z¨urich, Switzerland.

Jianming Zhang, Zhe Lin, Jonathan Brandt, Xiaohui
Shen, and Stan Sclaroff. 2016. Top-down neural at-
In European Con-
tention by excitation backprop.
ference on Computer Vision, pages 543–559, Ams-
terdam, Netherlands.

Luisa M Zintgraf, Taco S Cohen, Tameem Adel, and
Max Welling. 2017. Visualizing deep neural net-
work decisions: Prediction difference analysis.
In
International Conference on Learning Representa-
tions, Toulon, France.

Evaluating neural network explanation methods using
hybrid documents and morphosyntactic agreement

Nina Poerner, Benjamin Roth & Hinrich Sch ¨utze
Center for Information and Language Processing
LMU Munich, Germany
poerner@cis.lmu.de

Abstract

The behavior of deep neural networks
(DNNs) is hard to understand. This makes
it necessary to explore post hoc expla-
nation methods. We conduct
the ﬁrst
comprehensive evaluation of explanation
methods for NLP. To this end, we design
two novel evaluation paradigms that cover
two important classes of NLP problems:
small context and large context problems.
Both paradigms require no manual annota-
tion and are therefore broadly applicable.
We also introduce LIMSSE, an explana-
tion method inspired by LIME that is de-
signed for NLP. We show empirically that
LIMSSE, LRP and DeepLIFT are the most
effective explanation methods and recom-
mend them for explaining DNNs in NLP.

1

Introduction

DNNs are complex models that combine linear
transformations with different types of nonlinear-
ities. If the model is deep, i.e., has many layers,
then its behavior during training and inference is
notoriously hard to understand.

This is a problem for both scientiﬁc method-
ology and real-world deployment.
Scientiﬁc
methodology demands that we understand our
models. In the real world, a decision (e.g., “your
blog post is offensive and has been removed”) by
itself is often insufﬁcient; in addition, an expla-
nation of the decision may be required (e.g., “our
system ﬂagged the following words as offensive”).
The European Union plans to mandate that intelli-
gent systems used for sensitive applications pro-
vide such explanations (European General Data
Protection Regulation, expected 2018, cf. Good-
man and Flaxman (2016)).

A number of post hoc explanation methods for
DNNs have been proposed. Due to the complexity
of the DNNs they explain, these methods are nec-
essarily approximations and come with their own
sources of error. At this point, it is not clear which
of these methods to use when reliable explanations
for a speciﬁc DNN architecture are needed.

Deﬁnitions. (i) A task method solves an NLP

problem, e.g., a GRU that predicts sentiment.

(ii) An explanation method explains the behav-
ior of a task method on a speciﬁc input. For our
purpose, it is a function φ(t, k, X) that assigns
real-valued relevance scores for a target class k
(e.g., positive) to positions t in an input text X
(e.g., “great food”). For this example, an ex-
planation method might assign: φ(1, k, X) >
φ(2, k, X).

(iii) An (explanation) evaluation paradigm
quantitatively evaluates explanation methods for a
task method, e.g., by assigning them accuracies.

Contributions. (i) We present novel evaluation
paradigms for explanation methods for two classes
of common NLP tasks (see §2). Crucially, nei-
ther paradigm requires manual annotations and
our methodology is therefore broadly applicable.
(ii) Using these paradigms, we perform a com-
prehensive evaluation of explanation methods for
NLP (§3). We cover the most important classes
of task methods, RNNs and CNNs, as well as the
recently proposed Quasi-RNNs.

(iii) We introduce LIMSSE (§3.6), an expla-
nation method inspired by LIME (Ribeiro et al.,

tasks

task methods
explanation methods
evaluation paradigms

sentiment analysis,
morphological prediction, . . .
CNN, GRU, LSTM, . . .
LIMSSE, LRP, DeepLIFT, . . .
hybrid document,
morphosyntactic agreement

Table 1: Terminology with examples.

lrp

From : kolstad @ cae.wisc.edu ( Joel Kolstad ) Subject : Re : Can Radio Freq . Be Used To Measure Distance ? [...] What is the difference
between vertical and horizontal ? Gravity ? Does n’t gravity pull down the photons and cause a doppler shift or something ? ( Just kidding ! )

gradL2
1p

limssems

s

If you ﬁnd faith to be honest , show me how . David The whole denominational mindset only causes more problems , sadly . ( See section 7 for
details . ) Thank you . ’The Armenians just shot and shot . Maybe coz they ’re ’quality’ cars ; - ) 200 posts/day . [...]
If you ﬁnd faith to be honest , show me how . David The whole denominational mindset only causes more problems , sadly . ( See section 7 for
details . ) Thank you . ’The Armenians just shot and shot . Maybe coz they ’re ’quality’ cars ; - ) 200 posts/day . [...]

Figure 1: Top:
sci.electronics post (not hybrid). Underlined: Manual relevance ground truth.
Green: evidence for sci.electronics. Task method: CNN. Bottom: hybrid newsgroup post, classiﬁed
talk.politics.mideast. Green: evidence for talk.politics.mideast. Underlined: talk.politics.mideast frag-
ment. Task method: QGRU. Italics: OOV. Bold: rmax position. See supplementary for full texts.

2016) that is designed for word-order sensitive
task methods (e.g., RNNs, CNNs). We show em-
pirically that LIMSSE, LRP (Bach et al., 2015)
and DeepLIFT (Shrikumar et al., 2017) are the
most effective explanation methods (§4): LRP and
DeepLIFT are the most consistent methods, while
LIMSSE wins the hybrid document experiment.

2 Evaluation paradigms

In this section, we introduce two novel evalua-
tion paradigms for explanation methods on two
types of common NLP tasks, small context tasks
and large context tasks. Small context tasks are
deﬁned as those that can be solved by ﬁnding
short, self-contained indicators, such as words and
phrases, and weighing them up (i.e., tasks where
CNNs with pooling can be expected to perform
well). We design the hybrid document paradigm
for evaluating explanation methods on small con-
text tasks. Large context tasks require the cor-
rect handling of long-distance dependencies, such
as subject-verb agreement.1 We design the mor-
phosyntactic agreement paradigm for evaluating
explanation methods on large context tasks.

We could also use human judgments for
evaluation. While we use Mohseni and Ragan
(2018)’s manual relevance benchmark for com-
parison, there are two issues with it: (i) Due to
the cost of human labor, it is limited in size and
domain. (ii) More importantly, a good explana-
tion method should not reﬂect what humans at-
tend to, but what task methods attend to. For in-
stance, the family name “Kolstad” has 11 out of
its 13 appearances in the 20 newsgroups corpus in
sci.electronics posts. Thus, task methods probably
learn it as a sci.electronics indicator. Indeed, the

1Consider deciding the number of [verb] in “the children
in the green house said that the big telescope [verb]” vs.
“the children in the green house who broke the big telescope
[verb]”. The local contexts of “children” or “[verb]” do not
sufﬁce to solve this problem, instead, the large context of the
entire sentence has to be considered.

explanation method in Fig 1 (top) marks “Kolstad”
as relevant, but the human annotator does not.

2.1 Small context: Hybrid document

paradigm

Given a collection of documents, hybrid docu-
ments are created by randomly concatenating doc-
ument fragments. We assume that, on average, the
most relevant input for a class k in a hybrid doc-
ument is located in a fragment that stems from a
document with gold label k. Hence, an explana-
tion method succeeds if it places maximal rele-
vance for k inside the correct fragment.

Formally, let xt be a word inside hybrid docu-
ment X that originates from a document X(cid:48) with
gold label y(X(cid:48)). xt’s gold label y(X, t) is set
to y(X(cid:48)). Let f (X) be the class assigned to the
hybrid document by a task method, and let φ
be an explanation method as deﬁned above. Let
rmax(X, φ) denote the position of the maximally
relevant word in X for the predicted class f (X).
If this maximally relevant word comes from a doc-
ument with the correct gold label, the explanation
method is awarded a hit:
hit(φ, X) = I[y(cid:0)X, rmax(X, φ)(cid:1) = f (X)]
where I[P ] is 1 if P is true and 0 otherwise. In
Fig 1 (bottom), the explanation method gradL2
1p
places rmax outside the correct (underlined) frag-
ment. Therefore, it does not get a hit point, while
limssems

(1)

s does.

The pointing game accuracy of an explana-
tion method is calculated as its total number of
hit points divided by the number of possible hit
points. This is a form of the pointing game
paradigm from computer vision (Zhang et al.,
2016).

2.2 Large context: Morphosyntactic

agreement paradigm

Many natural languages display morphosyntactic
agreement between words v and w. A DNN that

graddot
(cid:82) s
lrp
limssebb
gradL2
(cid:82) s
occ1
limssems

s

the link provided by the editor above [encourages ...]
the link provided by the editor above [encourages ...]
the link provided by the editor above [encourages ...]

few if any events in history [are ...]
few if any events in history [are ...]
few if any events in history [are ...]

Figure 2: Top: verb context classiﬁed singular.
Green: evidence for singular. Task method: GRU.
Bottom: verb context classiﬁed plural. Green: ev-
idence for plural. Task method: LSTM. Under-
lined: subject. Bold: rmax position.

predicts the agreeing feature in w should pay at-
tention to v. For example, in the sentence “the
children with the telescope are home”, the num-
ber of the verb (plural for “are”) can be predicted
from the subject (“children”) without looking at
the verb. If the language allows for v and w to be
far apart (Fig 3, top), successful task methods have
to be able to handle large contexts.

Linzen et al. (2016) show that English verb
number can be predicted by a unidirectional
LSTM with accuracy > 99%, based on left context
alone. When a task method predicts the correct
number, we expect successful explanation meth-
ods to place maximal relevance on the subject:

hittarget(φ, X) = I[rmax(X, φ) = target(X)]

where target(X) is the location of the subject,
and rmax is calculated as above. Regardless of
whether the prediction is correct, we expect rmax
to fall onto a noun that has the predicted number:

hitfeat(φ, X) = I[feat(cid:0)X, rmax(X, φ)(cid:1) = f (X)]

where feat(X, t) is the morphological feature
(here: number) of xt. In Fig 2, rmax on “link”
gives a hittarget point (and a hitfeat point), rmax
on “editor” gives a hitfeat point. gradL2
(cid:82) s does not
get any points as “history” is not a plural noun.

Labels for this task can be automatically gen-
erated using part-of-speech taggers and parsers,
which are available for many languages.

3 Explanation methods

In this section, we deﬁne the explanation meth-
ods that will be evaluated. For our purpose, ex-
planation methods produce word relevance scores
φ(t, k, X), which are speciﬁc to a given class k
and a given input X. φ(t, k, X) > φ(t(cid:48), k, X)
means that xt contributed more than xt(cid:48) to the task
method’s (potential) decision to classify X as k.

3.1 Gradient-based explanation methods

Gradient-based explanation methods approximate
the contribution of some DNN input i to some out-
put o with o’s gradient with respect to i (Simonyan
et al., 2014). In the following, we consider two
output functions o(k, X), the unnormalized class
score s(k, X) and the class probability p(k|X):
s(k, X) = (cid:126)wk · (cid:126)h(X) + bk

(2)

(3)

(4)

p(k|X) =

exp(cid:0)s(k, X)(cid:1)
k(cid:48)=1 exp(cid:0)s(k(cid:48), X)(cid:1)

(cid:80)K

where k is the target class, (cid:126)h(X) the document
representation (e.g., an RNN’s ﬁnal hidden layer),
(cid:126)wk (resp. bk) k’s weight vector (resp. bias).
The simple gradient of o(k, X) w.r.t. i is:

grad1(i, k, X) =

∂o(k, X)
∂i

grad1 underestimates the importance of inputs
that saturate a nonlinearity (Shrikumar et al.,
2017). To address this, Sundararajan et al. (2017)
integrate over all gradients on a linear interpola-
tion α ∈ [0, 1] between a baseline input ¯X (here:
all-zero embeddings) and X:
grad(cid:82) (i, k, X) = (cid:82) 1

∂α

∂o(k, ¯X+α(X− ¯X))
∂i
α=0
∂o(k, ¯X+ m
M (X− ¯X))
∂i

(5)

≈ 1
M

(cid:80)M

m=1

where M is a big enough constant (here: 50).

In NLP, symbolic inputs (e.g., words) are often
represented as one-hot vectors (cid:126)xt ∈ {1, 0}|V | and
embedded via a real-valued matrix: (cid:126)et = M(cid:126)xt.
Gradients are computed with respect to individual
entries of E = [(cid:126)e1 . . . (cid:126)e|X|]. Bansal et al. (2016)
and Hechtlinger (2016) use the L2 norm to reduce
vectors of gradients to single values:

φgradL2(t, k, X) = ||grad((cid:126)et, k, E)||
where grad((cid:126)et, k, E) is a vector of elementwise
gradients w.r.t. (cid:126)et. Denil et al. (2015) use the dot
product of the gradient vector and the embedding2,
i.e., the gradient of the “hot” entry in (cid:126)xt:

(6)

(7)

φgraddot(t, k, X) = (cid:126)et · grad((cid:126)et, k, E)
We use “grad1” for Eq 4, “grad(cid:82) ” for Eq 5, “p”
for Eq 3, “s” for Eq 2, “L2” for Eq 6 and “dot”
for Eq 7. This gives us eight explanation meth-
1p , graddot
ods: gradL2
1p , gradL2
(cid:82) s,
(cid:82) s , graddot
gradL2
(cid:82) p .
, replace (cid:126)et with (cid:126)et − (cid:126)¯et. Since our baseline

(cid:82) p, graddot

1s , graddot

1s , gradL2

2For graddot

(cid:82)

embeddings are all-zeros, this is equivalent.

3.2 Layer-wise relevance propagation

Layer-wise relevance propagation (LRP)
is a
backpropagation-based explanation method devel-
oped for fully connected neural networks and
CNNs (Bach et al., 2015) and later extended to
LSTMs (Arras et al., 2017b).
In this paper, we
use Epsilon LRP (Eq 58, Bach et al. (2015)). Re-
member that the activation of neuron j, aj, is the
sum of weighted upstream activations, (cid:80)
i aiwi,j,
plus bias bj, squeezed through some nonlinearity.
We denote the pre-nonlinearity activation of j as
a(cid:48)
j. The relevance of j, R(j), is distributed to up-
stream neurons i proportionally to the contribution
that i makes to a(cid:48)

j in the forward pass:

R(i) =

R(j)

(cid:88)

j

aiwi,j
j + esign(a(cid:48)

a(cid:48)

j)

(8)

This ensures that relevance is conserved between
layers, with the exception of relevance attributed
to bj. To prevent numerical instabilities, esign(a(cid:48))
returns −(cid:15) if a(cid:48) < 0 and (cid:15) otherwise. We set (cid:15) =
.001. The full algorithm is:

R(Lk(cid:48)) = s(k, X)I[k(cid:48) = k]
... recursive application of Eq 8 ...

φlrp(t, k, X) =

R(et,j)

dim((cid:126)et)
(cid:88)

j=1

where L is the ﬁnal layer, k the target class and
R(et,j) the relevance of dimension j in the t’th
embedding vector. For (cid:15) → 0 and provided that all
nonlinearities up to the unnormalized class score
are relu, Epsilon LRP is equivalent to the prod-
uct of input and raw score gradient (here: graddot
1s )
(Kindermans et al., 2016). In our experiments, the
second requirement holds only for CNNs.

Experiments by Ancona et al. (2017) (see §6)
suggest that LRP does not work well for LSTMs
if all neurons – including gates – participate in
backpropagation. We therefore use Arras et al.
(2017b)’s modiﬁcation and treat sigmoid-activated
gates as time step-speciﬁc weights rather than neu-
rons. For instance, the relevance of LSTM candi-
date vector (cid:126)gt is calculated from memory vector (cid:126)ct
and input gate vector(cid:126)it as

in(cid:126)it do not receive any relevance themselves. See
supplementary material for formal deﬁnitions of
Epsilon LRP for different architectures.

3.3 DeepLIFT

DeepLIFT (Shrikumar et al., 2017) is another
backpropagation-based explanation method. Un-
it does not explain s(k, X), but
like LRP,
s(k, X)−s(k, ¯X), where ¯X is some baseline input
(here: all-zero embeddings). Following Ancona
et al. (2018) (Eq 4), we use this backpropagation
rule:

R(i) =

R(j)

(cid:88)

j

aiwi,j − ¯aiwi,j
j + esign(a(cid:48)

a(cid:48)
j − ¯a(cid:48)

j − ¯a(cid:48)
j)

where ¯a refers to the forward pass of the base-
line. Note that the original method has a dif-
ferent mechanism for avoiding small denomina-
tors; we use esign for compatibility with LRP.
The DeepLIFT algorithm is started with R(Lk(cid:48)) =
(cid:0)s(k, X)−s(k, ¯X)(cid:1)I[k(cid:48) = k]. On gated (Q)RNNs,
we proceed analogous to LRP and treat gates as
weights.

3.4 Cell decomposition for gated RNNs

The cell decomposition explanation method for
LSTMs (Murdoch and Szlam, 2017) decomposes
the unnormalized class score s(k, X) (Eq 2) into
additive contributions. For every time step t, we
compute how much of (cid:126)ct “survives” until the ﬁnal
step T and contributes to s(k, X). This is achieved
by applying all future forget gates (cid:126)f , the ﬁnal tanh
nonlinearity, the ﬁnal output gate (cid:126)oT , as well as the
class weights of k to (cid:126)ct. We call this quantity “net
load of t for class k”:

nl(t, k, X) = (cid:126)wk ·

(cid:16)
(cid:126)oT (cid:12) tanh(cid:0)(

(cid:126)fj) (cid:12) (cid:126)ct

(cid:1)(cid:17)

T
(cid:89)

j=t+1

where (cid:12) and (cid:81) are applied elementwise. The rel-
evance of t is its gain in net load relative to t − 1:
φdecomp(t, k, X) = nl(t, k, X) − nl(t − 1, k, X).
For GRU, we change the deﬁnition of net load:

nl(t, k, X) = (cid:126)wk · (cid:0)(

(cid:126)zj) (cid:12) (cid:126)ht

(cid:1)

T
(cid:89)

j=t+1

R(gt,d) = R(ct,d)

gt,d · it,d
ct,d + esign(ct,d)

where (cid:126)z are GRU update gates.

3.5

Input perturbation methods

This is equivalent to applying Eq 8 while treating
(cid:126)it as a diagonal weight matrix. The gate neurons

Input perturbation methods assume that the re-
moval or masking of relevant inputs changes the

output (Zeiler and Fergus, 2014). Omission-
based methods remove inputs completely (K´ad´ar
et al., 2017), while occlusion-based methods re-
place them with a baseline (Li et al., 2016b). In
computer vision, perturbations are usually applied
to patches, as neighboring pixels tend to correlate
(Zintgraf et al., 2017). To calculate the omitN
(resp. occN ) relevance of word xt, we delete (resp.
occlude), one at a time, all N -grams that contain
xt, and average the change in the unnormalized
class score from Eq 2:

(cid:2)s(k, [(cid:126)e1 . . . (cid:126)e|X|])
φ[omit|occ]N (t, k, X) = (cid:80)N
−s(k, [(cid:126)e1 . . . (cid:126)et−N −1+j](cid:107)¯E(cid:107)[(cid:126)et+j . . . (cid:126)e|X|])(cid:3) 1

j=1

N

where (cid:126)et are embedding vectors, (cid:107) denotes con-
catenation and ¯E is either a sequence of length
zero (φomit) or a sequence of N baseline (here:
all-zero) embedding vectors (φocc).

3.6 LIMSSE: LIME for NLP

Local Interpretable Model-agnostic Explanations
(LIME) (Ribeiro et al., 2016) is a framework
for explaining predictions of complex classiﬁers.
LIME approximates the behavior of classiﬁer f in
the neighborhood of input X with an interpretable
(here: linear) model. The interpretable model is
trained on samples Z1 . . . ZN (here: N = 3000),
which are randomly drawn from X, with “gold la-
bels” f (Z1) . . . f (ZN ).

Since RNNs and CNNs respect word or-
der, we cannot use the bag of words sam-
pling method from the original description
of LIME. Instead, we introduce Local Inter-
pretable Model-agnostic Substring-based Expla-
nations (LIMSSE). LIMSSE uniformly samples
a length ln (here: 1 ≤ ln ≤ 6) and a start-
ing point sn, which deﬁne the substring Zn =
[(cid:126)xsn . . . (cid:126)xsn+ln−1]. To the linear model, Zn is rep-
resented by a binary vector (cid:126)zn ∈ {0, 1}|X|, where
zn,t = I[sn ≤ t < sn + ln].

We learn a linear weight vector ˆ(cid:126)vk ∈ R|X|,
whose entries are word relevances for k,
i.e.,
φlimsse(t, k, X) = ˆvk,t. To optimize it, we experi-
ment with three loss functions. The ﬁrst, which we
will refer to as limssebb, assumes that our DNN is
a total black box that delivers only a classiﬁcation:

(cid:0)p(k(cid:48)|Zn)(cid:1). The black
where f (Zn) = argmaxk(cid:48)
box approach is maximally general, but insensitive
to the magnitude of evidence found in Zn. Hence,
we also test magnitude-sensitive loss functions:

ˆ(cid:126)vk = argmin

(cid:0)(cid:126)zn · (cid:126)vk − o(k, Zn)(cid:1)2

(cid:88)

n

(cid:126)vk

where o(k, Zn) is one of s(k, Zn) or p(k|Zn). We
refer to these as limssems
s

and limssems
p .

4 Experiments

4.1 Hybrid document experiment

For the hybrid document experiment, we use the
20 newsgroups corpus (topic classiﬁcation) (Lang,
1995) and reviews from the 10th yelp dataset
challenge (binary sentiment analysis)3. We train
ﬁve DNNs per corpus: a bidirectional GRU (Cho
et al., 2014), a bidirectional LSTM (Hochreiter
and Schmidhuber, 1997), a 1D CNN with global
max pooling (Collobert et al., 2011), a bidirec-
tional Quasi-GRU (QGRU), and a bidirectional
Quasi-LSTM (QLSTM). The Quasi-RNNs are 1D
CNNs with a feature-wise gated recursive pooling
layer (Bradbury et al., 2017). Word embeddings
are R300 and initialized with pre-trained GloVe
embeddings (Pennington et al., 2014)4. The main
layer has a hidden size of 150 (bidirectional ar-
chitectures: 75 dimensions per direction). For the
QRNNs and CNN, we use a kernel width of 5. In
all ﬁve architectures, the resulting document rep-
resentation is projected to 20 (resp.
two) dimen-
sions using a fully connected layer, followed by a
softmax. See supplementary material for details
on training and regularization.

After training, we sentence-tokenize the test
sets, shufﬂe the sentences, concatenate ten sen-
tences at a time and classify the resulting hybrid
documents. Documents that are assigned a class
that is not the gold label of at least one con-
stituent word are discarded (yelp: < 0.1%; 20
newsgroups: 14% - 20%). On the remaining docu-
ments, we use the explanation methods from §3 to
ﬁnd the maximally relevant word for each predic-
tion. The random baseline samples the maximally
relevant word from a uniform distribution.

For reference, we also evaluate on a hu-
man judgment benchmark (Mohseni and Ra-
It contains
gan (2018), Table 2, C11-C15).

3www.yelp.com/dataset_challenge
4http://nlp.stanford.edu/data/glove.

ˆ(cid:126)vk = argmin

−(cid:2)log(cid:0)σ((cid:126)zn · (cid:126)vk)(cid:1)I[f (Zn) = k]

(cid:88)

n

(cid:126)vk

+ log(cid:0)1 − σ((cid:126)zn · (cid:126)vk)(cid:1)I[f (Zn) (cid:54)= k](cid:3)

840B.300d.zip

column

C01 C02 C03 C04 C05 C06 C07 C08 C09 C10 C11 C12 C13 C14 C15 C16 C17 C18 C19 C20 C21 C22 C23 C24 C25 C26 C27

hybrid document experiment

man. groundtruth

morphosyntactic agreement experiment

yelp

20 newsgroups

20 newsgroups

f (X) = y(X)

hittarget

hitfeat

N
N
C

U
R
G

U
R
G
Q

M
T
S
L

M
T
S
L
Q

N
N
C

U
R
G

U
R
G
Q

M
T
S
L

M
T
S
L
Q

U
R
G

U
R
G
Q

M
T
S
L

M
T
S
L
Q

f (X) (cid:54)= y(X)
M
T
S
L
Q

U
R
G
Q

M
T
S
L

U
R
G

U
R
G

U
R
G
Q

M
T
S
L

M
T
S
L
Q

N
N
C

U
R
G

U
R
G
Q

M
T
S
L

M
T
S
L
Q

φ
gradL2
.61 .68 .67 .70 .68 .45 .47 .25 .33 .79
1s
gradL2
.57 .67 .67 .70 .74 .40 .43 .26 .34 .70
1p
gradL2
.71 .66 .69 .71 .70 .58 .32 .26 .21 .82
(cid:82) s
gradL2
.71 .70 .72 .71 .77 .56 .34 .30 .23 .81
(cid:82) p
graddot
.88 .85 .81 .77 .86 .79 .76 .59 .72 .89
1s
graddot
.92 .88 .84 .79 .95 .78 .72 .59 .72 .81
1p
graddot
.84 .90 .85 .87 .87 .81 .68 .60 .68 .89
(cid:82) s
graddot
.86 .89 .84 .89 .96 .80 .69 .62 .73 .89
(cid:82) p
omit1
.79 .82 .85 .87 .61 .78 .75 .54 .76 .82
omit3
.89 .80 .89 .88 .59 .79 .71 .72 .81 .76
omit7
.92 .88 .91 .91 .70 .79 .77 .77 .84 .84
occ1
.80 .71 .74 .84 .61 .78 .73 .60 .77 .82
occ3
.92 .61 .93 .85 .59 .78 .63 .74 .74 .76
occ7
.92 .77 .93 .90 .70 .78 .62 .74 .77 .84
decomp
.79 .88 .92 .88
-
lrp
.92 .87 .91 .84 .86 .82 .83 .79 .85 .89
deeplift
.91 .89 .94 .85 .87 .82 .83 .78 .84 .89
limssebb
.81 .82 .83 .84 .78 .78 .81 .78 .80 .84
limssems
.94 .94 .93 .93 .91 .85 .87 .83 .86 .89
limssems
.87 .88 .85 .86 .94 .85 .86 .83 .86 .90
random .69 .67 .70 .69 .66 .20 .19 .22 .22 .21
last
-
-
-
-
3022 ≤ N ≤ 3230
N

-
-
-
7551 ≤ N ≤ 7554

.75 .79 .77 .80

p

s

-

-

-

-

.26 .31 .07 .18 .74
.18 .35 .07 .13 .66
.23 .15 .11 .08 .76
.13 .08 .14 .01 .78
.80 .70 .14 .47 .79
.71 .59 .20 .44 .69
.82 .64 .21 .26 .80
.80 .53 .40 .54 .78
.80 .48 .33 .48 .65
.77 .37 .36 .49 .61
.77 .49 .44 .55 .65
.77 .49 .19 .10 .65
.74 .37 .32 .35 .61
.74 .35 .43 .39 .65
.54 .36 .72 .51
-
.85 .72 .74 .81 .79
.84 .72 .70 .81 .80
.52 .53 .53 .54 .57
.85 .84 .76 .84 .82
.81 .80 .74 .76 .76
.09 .09 .06 .06 .08
-
-
-
137 ≤ N ≤ 150

-

-

.48 .23 .63 .19 .52 .27 .73 .22 .09 .11 .19 .19
.48 .22 .63 .18 .53 .26 .73 .21 .09 .09 .18 .11
.69 .67 .68 .51 .73 .70 .75 .55 .19 .22 .20 .20
.68 .77 .50 .70 .74 .82 .54 .78 .19 .21 .19 .30
.81 .62 .73 .56 .85 .66 .81 .59 .42 .34 .46 .36
.79 .58 .74 .54 .83 .61 .81 .56 .41 .33 .46 .35
.90 .87 .78 .84 .94 .92 .83 .89 .54 .51 .46 .52
.87 .85 .68 .84 .93 .92 .74 .93 .53 .48 .42 .51
.81 .81 .79 .80 .86 .87 .86 .84 .43 .45 .44 .45
.74 .77 .73 .73 .82 .84 .82 .79 .41 .45 .42 .46
.76 .80 .66 .74 .85 .88 .78 .80 .40 .48 .43 .47
.91 .85 .86 .86 .94 .88 .89 .88 .50 .44 .46 .47
.74 .73 .71 .72 .78 .76 .76 .76 .43 .37 .41 .43
.64 .65 .63 .65 .73 .73 .72 .73 .36 .35 .39 .43
.84 .87 .86 .90 .90 .93 .92 .96 .52 .58 .57 .63
.90 .90 .86 .91 .95 .95 .91 .95 .58 .60 .52 .63
.91 .90 .85 .91 .95 .95 .90 .95 .59 .59 .52 .63
.43 .41 .44 .42 .54 .51 .56 .52 .39 .43 .42 .41
.62 .62 .67 .63 .75 .74 .82 .75 .52 .53 .55 .53
.62 .62 .67 .63 .75 .74 .82 .75 .51 .53 .55 .53
.27 .27 .27 .27 .33 .33 .33 .33 .12 .13 .12 .12
.66 .67 .66 .67 .76 .77 .76 .77 .21 .27 .25 .26

N ≈ 1400000

N ≈ 20000

Table 2: Pointing game accuracies in hybrid document experiment (left), on manually annotated bench-
mark (middle) and in morphosyntactic agreement experiment (right). hittarget (resp. hitfeat): maximal
relevance on subject (resp. on noun with the predicted number feature). Bold: top explanation method.
Underlined: within 5 points of top explanation method.

188 documents from the 20 newsgroups test set
(classes sci.med and sci.electronics), with one
manually created list of relevant words per doc-
ument. We discard documents that are incorrectly
classiﬁed (20% - 27%) and deﬁne: hit(φ, X) =
I[rmax(X, φ) ∈ gt(X)], where gt(X) is the man-
ual ground truth.

4.2 Morphosyntactic agreement experiment

For the morphosyntactic agreement experiment,
we use automatically annotated English Wikipedia
sentences by Linzen et al. (2016)5. For our pur-
pose, a sample consists of: all words preceding the
verb: X = [x1 · · · xT ]; part-of-speech (POS) tags:
pos(X, t) ∈ {VBZ, VBP, NN, NNS, . . .}; and the
position of the subject: target(X) ∈ [1, T ]. The
number feature is derived from the POS:

feat(X, t) =






Sg
Pl
n/a

if pos(X, t) ∈ {VBZ, NN}
if pos(X, t) ∈ {VBP, NNS}
otherwise

The gold label of a sentence is the number of its
verb, i.e., y(X) = feat(X, T + 1).

5www.tallinzen.net/media/rnn_

agreement/agr_50_mostcommon_10K.tsv.gz

As task methods, we replicate Linzen et al.
(2016)’s unidirectional LSTM (R50 randomly
initialized word embeddings, hidden size 50).
We also train unidirectional GRU, QGRU and
QLSTM architectures with the same dimension-
ality. We use the explanation methods from §3 to
ﬁnd the most relevant word for predictions on the
test set. As described in §2.2, explanation methods
are awarded a hittarget (resp. hitfeat) point if this
word is the subject (resp. a noun with the predicted
number feature). For reference, we use a random
baseline as well as a baseline that assumes that the
most relevant word directly precedes the verb.

5 Discussion

5.1 Explanation methods

Our experiments suggest that explanation methods
for neural NLP differ in quality.

As in previous work (see §6), gradient L2
norm (gradL2) performs poorly, especially on
RNNs. We assume that this is due to its inability
to distinguish relevances for and against k.

Gradient embedding dot product (graddot)
is competitive on CNN (Table 2, graddot
1p C05,
graddot
1s C10, C15), presumably because relu is
linear on positive inputs, so gradients are exact in-

decomp
deeplift
limssems

p

lrp

limssems

p

initially a pagan culture , detailed information about the return of the christian religion to the islands during the norse-era [is ...]
initially a pagan culture , detailed information about the return of the christian religion to the islands during the norse-era [is ...]
initially a pagan culture , detailed information about the return of the christian religion to the islands during the norse-era [is ...]

Your day is done . Deﬁnitely looking forward to going back . All three were outstanding ! I would highly recommend going here to anyone .
We will see if anyone returns the message my boyfriend left . The price is unbelievable ! And our guys are on lunch so we ca n’t ﬁt you in . ” It
’s good , standard froyo . The pork shoulder was THAT tender . Try it with the Tomato Basil cram sauce .
Your day is done . Deﬁnitely looking forward to going back . All three were outstanding ! I would highly recommend going here to anyone .
We will see if anyone returns the message my boyfriend left . The price is unbelievable ! And our guys are on lunch so we ca n’t ﬁt you in . ” It
’s good , standard froyo . The pork shoulder was THAT tender . Try it with the Tomato Basil cram sauce .

Figure 3: Top: verb context classiﬁed singular. Task method: LSTM. Bottom: hybrid yelp review,
classiﬁed positive. Task method: QLSTM.

stead of approximate. graddot also has decent per-
formance for GRU (graddot
(cid:82) s C{06,
11, 16, 20, 24}), perhaps because GRU hidden ac-
tivations are always in [-1,1], where tanh and σ
are approximately linear.

1p C01, graddot

Integrated gradient (grad(cid:82) ) mostly outper-
forms simple gradient (grad1), though not consis-
tently (C01, C07). Contrary to expectation, in-
tegration did not help much with the failure of
the gradient method on LSTM on 20 newsgroups
(graddot
in C08, C13), which we had
assumed to be due to saturation of tanh on large
absolute activations in (cid:126)c. Smaller intervals may be
needed to approximate the integration, however,
this means additional computational cost.

vs. graddot

1

(cid:82)

1s vs. graddot

The gradient of s(k, X) performs better or sim-
ilar to the gradient of p(k|X). The main exception
is yelp (graddot
1p , C01-C05). This is
probably due to conﬂation by p(k|X) of evidence
for k (numerator in Eq 3) and against competi-
tor classes (denominator). In a two-class scenario,
there is little incentive to keep classes separate,
leading to information ﬂow through the denomi-
In future work, we will replace the two-
nator.
way softmax with a one-way sigmoid such that
φ(t, 0, X) := −φ(t, 1, X).

LRP and DeepLIFT are the most consistent
explanation methods across evaluation paradigms
and task methods. (The comparatively low point-
ing game accuracies on the yelp QRNNs and CNN
(C02, C04, C05) are probably due to the fact
that they explain s(k, .) in a two-way softmax,
see above.) On CNN (C05, C10, C15), LRP
and graddot
1s perform almost identically, suggest-
ing that they are indeed quasi-equivalent on this ar-
chitecture (see §3.2). On (Q)RNNs, modiﬁed LRP
and DeepLIFT appear to be superior to the gradi-
1s , deeplift vs. graddot
ent method (lrp vs. graddot
(cid:82) s ,
C01-C04, C06-C09, C11-C14, C16-C27).

Decomposition performs well on LSTM, es-
pecially in the morphosyntactic agreement exper-

iment, but it is inconsistent on other architec-
tures. Gated RNNs have a long-term additive and
a multiplicative pathway, and the decomposition
method only detects information traveling via the
additive one. Miao et al. (2016) show qualita-
tively that GRUs often reorganize long-term mem-
ory abruptly, which might explain the difference
between LSTM and GRU. QRNNs only have ad-
ditive recurrent connections; however, given that
(cid:126)ct (resp. (cid:126)ht) is calculated by convolution over sev-
eral time steps, decomposition relevance can be in-
correctly attributed inside that window. This likely
is the reason for the stark difference between the
performance of decomposition on QRNNs in the
hybrid document experiment and on the manually
labeled data (C07, C09 vs. C12, C14). Overall,
we do not recommend the decomposition method,
because it fails to take into account all routes by
which information can be propagated.

Omission and occlusion produce inconsis-
tent results in the hybrid document experiment.
Shrikumar et al. (2017) show that perturbation
methods can lack sensitivity when there are more
relevant inputs than the “perturbation window”
covers. In the morphosyntactic agreement experi-
ment, omission is not competitive; we assume that
this is because it interferes too much with syntactic
structure. occ1 does better (esp. C16-C19), possi-
bly because an all-zero “placeholder” is less dis-
ruptive than word removal. But despite some high
scores, it is less consistent than other explanation
methods.

LIMSSE

Magnitude-sensitive

(limssems)
consistently outperforms black-box LIMSSE
(limssebb), which suggests that numerical out-
puts should be used for approximation where
possible.
In the hybrid document experiment,
magnitude-sensitive LIMSSE outperforms the
other explanation methods (exceptions: C03,
C05). However, it fails in the morphosyntactic
agreement experiment (C16-C27).
In fact, we
expect LIMSSE to be unsuited for large context

problems, as it cannot discover dependencies
whose range is bigger than a given text sample.
In Fig 3 (top), limssems
highlights any singular
p
noun without taking into account how that noun
ﬁts into the overall syntactic structure.

5.2 Evaluation paradigms

The assumptions made by our automatic evalua-
tion paradigms have exceptions: (i) the correlation
between fragment of origin and relevance does not
always hold (e.g., a positive review may contain
negative fragments, and will almost certainly con-
tain neutral fragments); (ii) in morphological pre-
diction, we cannot always expect the subject to be
the only predictor for number. In Fig 2 (bottom)
for example, “few” is a reasonable clue for plural
despite not being a noun. This imperfect ground
truth means that absolute pointing game accura-
cies should be taken with a grain of salt; but we
argue that this does not invalidate them for com-
parisons.

We also point out that there are characteristics
of explanations that may be desirable but are not
reﬂected by the pointing game. Consider Fig 3
(bottom). Both explanations get hit points, but the
lrp explanation appears “cleaner” than limssems
p ,
with relevance concentrated on fewer tokens.

6 Related work

6.1 Explanation methods

Explanation methods can be divided into local
and global methods (Doshi-Velez and Kim, 2017).
Global methods infer general statements about
what a DNN has learned, e.g., by clustering docu-
ments (Aubakirova and Bansal, 2016) or n-grams
(K´ad´ar et al., 2017) according to the neurons that
they activate. Li et al. (2016a) compare embed-
dings of speciﬁc words with reference points to
measure how drastically they were changed dur-
ing training. In computer vision, Simonyan et al.
(2014) optimize the input space to maximize the
activation of a speciﬁc neuron. Global explanation
methods are of limited value for explaining a spe-
ciﬁc prediction as they represent average behavior.
Therefore, we focus on local methods.

Local explanation methods explain a decision
taken for one speciﬁc input at a time. We have
attempted to include all important local methods
for NLP in our experiments (see §3). We do
not address self-explanatory models (e.g., atten-
tion (Bahdanau et al., 2015) or rationale models

(Lei et al., 2016)), as these are very speciﬁc archi-
tectures that may not be not applicable to all tasks.

6.2 Explanation evaluation

According to Doshi-Velez and Kim (2017)’s
taxonomy of explanation evaluation paradigms,
application-grounded paradigms test how well an
explanation method helps real users solve real
tasks (e.g., doctors judge automatic diagnoses);
human-grounded paradigms rely on proxy tasks
(e.g., humans rank task methods based on expla-
nations); functionally-grounded paradigms work
without human input, like our approach.

Arras et al. (2016) (cf. Samek et al. (2016))
propose a functionally-grounded explanation eval-
uation paradigm for NLP where words in a cor-
rectly (resp.
incorrectly) classiﬁed document are
deleted in descending (resp. ascending) order of
relevance. They assume that the fewer words must
be deleted to reduce (resp. increase) accuracy, the
better the explanations. According to this metric,
LRP (§3.2) outperforms gradL2 on CNNs (Arras
et al., 2016) and LSTMs (Arras et al., 2017b) on
20 newsgroups. Ancona et al. (2017) perform the
same experiment with a binary sentiment analy-
sis LSTM. Their graph shows occ1, graddot
and
graddot
tied in ﬁrst place, while LRP, DeepLIFT
and the gradient L1 norm lag behind. Note that
their treatment of LSTM gates in LRP / DeepLIFT
differs from our implementation.

1

(cid:82)

An issue with the word deletion paradigm is that
it uses syntactically broken inputs, which may in-
troduce artefacts (Sundararajan et al., 2017).
In
our hybrid document paradigm, inputs are syntac-
tically intact (though semantically incoherent at
the document level); the morphosyntactic agree-
ment paradigm uses unmodiﬁed inputs.

Another class of functionally-grounded evalu-
ation paradigms interprets the performance of a
secondary task method, on inputs that are derived
from (or altered by) an explanation method, as a
proxy for the quality of that explanation method.
Murdoch and Szlam (2017) build a rule-based
classiﬁer from the most relevant phrases in a cor-
pus (task method: LSTM). The classiﬁer based
on decomp (§3.4) outperforms the gradient-based
classiﬁer, which is in line with our results. Ar-
ras et al. (2017a) build document representations
by summing over word embeddings weighted by
relevance scores (task method: CNN). They show
that K-nearest neighbor performs better on doc-

ument representations derived with LRP than on
those derived with gradL2, which also matches our
results. Denil et al. (2015) condense documents
by extracting top-K relevant sentences, and let the
original task method (CNN) classify them. The
accuracy loss, relative to uncondensed documents,
is smaller for graddot than for heuristic baselines.
In the domain of human-based evaluation
paradigms, Ribeiro et al. (2016) compare differ-
ent variants of LIME (§3.6) by how well they help
non-experts clean a corpus from words that lead
to overﬁtting. Selvaraju et al. (2017) assess how
well explanation methods help non-experts iden-
tify the more accurate out of two object recogni-
tion CNNs. These experiments come closer to real
use cases than functionally-grounded paradigms;
however, they are less scalable.

7 Summary

We conducted the ﬁrst comprehensive evaluation
of explanation methods for NLP, an important un-
dertaking because there is a need for understand-
ing the behavior of DNNs.

To conduct this study, we introduced evalua-
tion paradigms for explanation methods for two
classes of NLP tasks, small context tasks (e.g.,
topic classiﬁcation) and large context tasks (e.g.,
morphological prediction). Neither paradigm re-
quires manual annotations. We also introduced
LIMSSE, a substring-based explanation method
inspired by LIME and designed for NLP.

Based on our experimental results, we recom-
mend LRP, DeepLIFT and LIMSSE for small con-
text tasks and LRP and DeepLIFT for large con-
text tasks, on all ﬁve DNN architectures that we
tested. On CNNs and possibly GRUs, the (inte-
grated) gradient embedding dot product is a good
alternative to DeepLIFT and LRP.

8 Code

package:

implementation of LIMSSE,
the gradi-
Our
perturbation and decomposition meth-
ent,
the
ods can be found in our branch of
www.github.com/
keras
NPoe/keras.
To re-run our experiments,
in www.github.com/NPoe/
see
neural-nlp-explanation-experiment.
Our LRP implementation (same repository) is
adapted from Arras et al. (2017b)6.

scripts

6https://github.com/ArrasL/LRP_for_

LSTM

References
Marco Ancona, Enea Ceolini, Cengiz ¨Oztireli, and
Markus Gross. 2017. A uniﬁed view of gradient-
based attribution methods for deep neural networks.
In Conference on Neural Information Processing
System, Long Beach, USA.

Marco Ancona, Enea Ceolini, Cengiz ¨Oztireli, and
Markus Gross. 2018. Towards better understanding
of gradient-based attribution methods for deep neu-
ral networks. In International Conference on Learn-
ing Representations, Vancouver, Canada.

Leila Arras, Franziska Horn, Gr´egoire Montavon,
Klaus-Robert M¨uller, and Wojciech Samek. 2016.
Explaining predictions of non-linear classiﬁers in
NLP. In First Workshop on Representation Learn-
ing for NLP, pages 1–7, Berlin, Germany.

Leila Arras, Franziska Horn, Gr´egoire Montavon,
Klaus-Robert M¨uller, and Wojciech Samek. 2017a.
What is relevant in a text document?: An inter-
PloS one,
pretable machine learning approach.
12(8):e0181142.

Leila Arras, Gr´egoire Montavon, Klaus-Robert M¨uller,
and Wojciech Samek. 2017b. Explaining recurrent
neural network predictions in sentiment analysis. In
Eighth Workshop on Computational Approaches to
Subjectivity, Sentiment and Social Media Analysis,
pages 159–168, Copenhagen, Denmark.

Malika Aubakirova and Mohit Bansal. 2016. Interpret-
ing neural networks to improve politeness compre-
hension. In Empirical Methods in Natural Language
Processing, page 2035–2041, Austin, USA.

Sebastian Bach, Alexander Binder, Gr´egoire Mon-
tavon, Frederick Klauschen, Klaus-Robert M¨uller,
and Wojciech Samek. 2015. On pixel-wise explana-
tions for non-linear classiﬁer decisions by layer-wise
relevance propagation. PloS one, 10(7):e0130140.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations, San Diego,
USA.

Trapit Bansal, David Belanger, and Andrew McCal-
lum. 2016. Ask the GRU: Multi-task learning for
In ACM Conference
deep text recommendations.
on Recommender Systems, pages 107–114, Boston,
USA.

James Bradbury, Stephen Merity, Caiming Xiong, and
Richard Socher. 2017. Quasi-recurrent neural net-
In International Conference on Learning
works.
Representations, Toulon, France.

Kyunghyun Cho, Bart Van Merri¨enboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. In Eighth Workshop on Syntax, Semantics
and Structure in Statistical Translation, pages 103–
111, Doha, Qatar.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
Journal of Machine Learning Research,
scratch.
12(Aug):2493–2537.

Misha Denil, Alban Demiraj, and Nando de Freitas.
2015. Extraction of salient sentences from labelled
documents. In International Conference on Learn-
ing Representations, San Diego, USA.

Finale Doshi-Velez and Been Kim. 2017. A roadmap
for a rigorous science of interpretability. CoRR,
abs/1702.08608.

Bryce Goodman and Seth Flaxman. 2016. European
union regulations on algorithmic decision-making
and a “right to explanation”. In ICML Workshop on
Human Interpretability in Machine Learning, pages
26–30, New York, USA.

Yotam Hechtlinger. 2016. Interpretation of prediction
models using the input gradient. In Conference on
Neural Information Processing Systems, Barcelona,
Spain.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Akos K´ad´ar, Grzegorz Chrupała, and Afra Alishahi.
2017. Representation of linguistic form and func-
tion in recurrent neural networks. Computational
Linguistics, 43(4):761–780.

Pieter-Jan Kindermans, Kristof Sch¨utt, Klaus-Robert
M¨uller, and Sven D¨ahne. 2016. Investigating the in-
ﬂuence of noise and distractors on the interpretation
of neural networks. In Conference on Neural Infor-
mation Processing Systems, Barcelona, Spain.

Ken Lang. 1995. Newsweeder: Learning to ﬁlter
In International Conference on Machine

netnews.
Learning, pages 331–339, Tahoe City, USA.

Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016.
In Empirical
Rationalizing neural predictions.
Methods in Natural Language Processing, pages
107–117, Austin, USA.

Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.
2016a. Visualizing and understanding neural mod-
In NAACL-HLT, pages 681–691, San
els in NLP.
Diego, USA.

Jiwei Li, Will Monroe, and Dan Jurafsky. 2016b. Un-
derstanding neural networks through representation
erasure. CoRR, abs/1612.08220.

Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.
2016. Assessing the ability of LSTMs to learn
syntax-sensitive dependencies. Transactions of the
Association for Computational Linguistics, 4:521–
535.

Yajie Miao, Jinyu Li, Yongqiang Wang, Shi-Xiong
Zhang, and Yifan Gong. 2016. Simplifying long
short-term memory acoustic models for fast train-
In International Conference
ing and decoding.
on Acoustics, Speech and Signal Processing, pages
2284–2288.

Sina Mohseni and Eric D Ragan. 2018. A human-
grounded evaluation benchmark for local explana-
tions of machine learning. CoRR, abs/1801.05075.

W James Murdoch and Arthur Szlam. 2017. Auto-
matic rule extraction from long short term memory
networks. In International Conference on Learning
Representations, Toulon, France.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543, Doha, Qatar.

Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2016. Why should I trust you?: Ex-
In ACM
plaining the predictions of any classiﬁer.
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 1135–1144, San
Francisco, California.

Wojciech Samek, Alexander Binder, Gr´egoire Mon-
tavon, Sebastian Lapuschkin, and Klaus-Robert
M¨uller. 2016. Evaluating the visualization of what
IEEE trans-
a deep neural network has learned.
actions on neural networks and learning systems,
28(11):2660–2673.

Ramprasaath R Selvaraju, Michael Cogswell, Ab-
hishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. 2017. Grad-cam: Visual expla-
nations from deep networks via gradient-based lo-
calization. In IEEE Conference on Computer Vision
and Pattern Recognition, pages 618–626, Honolulu,
Hawaii.

Avanti Shrikumar, Peyton Greenside, and Anshul Kun-
daje. 2017. Learning important features through
propagating activation differences. In International
Conference on Machine Learning, pages 3145–
3153, Sydney, Australia.

Karen Simonyan, Andrea Vedaldi, and Andrew Zisser-
man. 2014. Deep inside convolutional networks: Vi-
sualising image classiﬁcation models and saliency
In International Conference on Learning
maps.
Representations, Banff, Canada.

Mukund Sundararajan, Ankur Taly, and Qiqi Yan.
2017. Axiomatic attribution for deep networks.
In International Conference on Machine Learning,
Sydney, Australia.

Matthew D Zeiler and Rob Fergus. 2014. Visualizing
In Eu-
and understanding convolutional networks.
ropean Conference on Computer Vision, pages 818–
833, Z¨urich, Switzerland.

Jianming Zhang, Zhe Lin, Jonathan Brandt, Xiaohui
Shen, and Stan Sclaroff. 2016. Top-down neural at-
In European Con-
tention by excitation backprop.
ference on Computer Vision, pages 543–559, Ams-
terdam, Netherlands.

Luisa M Zintgraf, Taco S Cohen, Tameem Adel, and
Max Welling. 2017. Visualizing deep neural net-
work decisions: Prediction difference analysis.
In
International Conference on Learning Representa-
tions, Toulon, France.

Evaluating neural network explanation methods using
hybrid documents and morphosyntactic agreement

Nina Poerner, Benjamin Roth & Hinrich Sch ¨utze
Center for Information and Language Processing
LMU Munich, Germany
poerner@cis.lmu.de

Abstract

The behavior of deep neural networks
(DNNs) is hard to understand. This makes
it necessary to explore post hoc expla-
nation methods. We conduct
the ﬁrst
comprehensive evaluation of explanation
methods for NLP. To this end, we design
two novel evaluation paradigms that cover
two important classes of NLP problems:
small context and large context problems.
Both paradigms require no manual annota-
tion and are therefore broadly applicable.
We also introduce LIMSSE, an explana-
tion method inspired by LIME that is de-
signed for NLP. We show empirically that
LIMSSE, LRP and DeepLIFT are the most
effective explanation methods and recom-
mend them for explaining DNNs in NLP.

1

Introduction

DNNs are complex models that combine linear
transformations with different types of nonlinear-
ities. If the model is deep, i.e., has many layers,
then its behavior during training and inference is
notoriously hard to understand.

This is a problem for both scientiﬁc method-
ology and real-world deployment.
Scientiﬁc
methodology demands that we understand our
models. In the real world, a decision (e.g., “your
blog post is offensive and has been removed”) by
itself is often insufﬁcient; in addition, an expla-
nation of the decision may be required (e.g., “our
system ﬂagged the following words as offensive”).
The European Union plans to mandate that intelli-
gent systems used for sensitive applications pro-
vide such explanations (European General Data
Protection Regulation, expected 2018, cf. Good-
man and Flaxman (2016)).

A number of post hoc explanation methods for
DNNs have been proposed. Due to the complexity
of the DNNs they explain, these methods are nec-
essarily approximations and come with their own
sources of error. At this point, it is not clear which
of these methods to use when reliable explanations
for a speciﬁc DNN architecture are needed.

Deﬁnitions. (i) A task method solves an NLP

problem, e.g., a GRU that predicts sentiment.

(ii) An explanation method explains the behav-
ior of a task method on a speciﬁc input. For our
purpose, it is a function φ(t, k, X) that assigns
real-valued relevance scores for a target class k
(e.g., positive) to positions t in an input text X
(e.g., “great food”). For this example, an ex-
planation method might assign: φ(1, k, X) >
φ(2, k, X).

(iii) An (explanation) evaluation paradigm
quantitatively evaluates explanation methods for a
task method, e.g., by assigning them accuracies.

Contributions. (i) We present novel evaluation
paradigms for explanation methods for two classes
of common NLP tasks (see §2). Crucially, nei-
ther paradigm requires manual annotations and
our methodology is therefore broadly applicable.
(ii) Using these paradigms, we perform a com-
prehensive evaluation of explanation methods for
NLP (§3). We cover the most important classes
of task methods, RNNs and CNNs, as well as the
recently proposed Quasi-RNNs.

(iii) We introduce LIMSSE (§3.6), an expla-
nation method inspired by LIME (Ribeiro et al.,

tasks

task methods
explanation methods
evaluation paradigms

sentiment analysis,
morphological prediction, . . .
CNN, GRU, LSTM, . . .
LIMSSE, LRP, DeepLIFT, . . .
hybrid document,
morphosyntactic agreement

Table 1: Terminology with examples.

lrp

From : kolstad @ cae.wisc.edu ( Joel Kolstad ) Subject : Re : Can Radio Freq . Be Used To Measure Distance ? [...] What is the difference
between vertical and horizontal ? Gravity ? Does n’t gravity pull down the photons and cause a doppler shift or something ? ( Just kidding ! )

gradL2
1p

limssems

s

If you ﬁnd faith to be honest , show me how . David The whole denominational mindset only causes more problems , sadly . ( See section 7 for
details . ) Thank you . ’The Armenians just shot and shot . Maybe coz they ’re ’quality’ cars ; - ) 200 posts/day . [...]
If you ﬁnd faith to be honest , show me how . David The whole denominational mindset only causes more problems , sadly . ( See section 7 for
details . ) Thank you . ’The Armenians just shot and shot . Maybe coz they ’re ’quality’ cars ; - ) 200 posts/day . [...]

Figure 1: Top:
sci.electronics post (not hybrid). Underlined: Manual relevance ground truth.
Green: evidence for sci.electronics. Task method: CNN. Bottom: hybrid newsgroup post, classiﬁed
talk.politics.mideast. Green: evidence for talk.politics.mideast. Underlined: talk.politics.mideast frag-
ment. Task method: QGRU. Italics: OOV. Bold: rmax position. See supplementary for full texts.

2016) that is designed for word-order sensitive
task methods (e.g., RNNs, CNNs). We show em-
pirically that LIMSSE, LRP (Bach et al., 2015)
and DeepLIFT (Shrikumar et al., 2017) are the
most effective explanation methods (§4): LRP and
DeepLIFT are the most consistent methods, while
LIMSSE wins the hybrid document experiment.

2 Evaluation paradigms

In this section, we introduce two novel evalua-
tion paradigms for explanation methods on two
types of common NLP tasks, small context tasks
and large context tasks. Small context tasks are
deﬁned as those that can be solved by ﬁnding
short, self-contained indicators, such as words and
phrases, and weighing them up (i.e., tasks where
CNNs with pooling can be expected to perform
well). We design the hybrid document paradigm
for evaluating explanation methods on small con-
text tasks. Large context tasks require the cor-
rect handling of long-distance dependencies, such
as subject-verb agreement.1 We design the mor-
phosyntactic agreement paradigm for evaluating
explanation methods on large context tasks.

We could also use human judgments for
evaluation. While we use Mohseni and Ragan
(2018)’s manual relevance benchmark for com-
parison, there are two issues with it: (i) Due to
the cost of human labor, it is limited in size and
domain. (ii) More importantly, a good explana-
tion method should not reﬂect what humans at-
tend to, but what task methods attend to. For in-
stance, the family name “Kolstad” has 11 out of
its 13 appearances in the 20 newsgroups corpus in
sci.electronics posts. Thus, task methods probably
learn it as a sci.electronics indicator. Indeed, the

1Consider deciding the number of [verb] in “the children
in the green house said that the big telescope [verb]” vs.
“the children in the green house who broke the big telescope
[verb]”. The local contexts of “children” or “[verb]” do not
sufﬁce to solve this problem, instead, the large context of the
entire sentence has to be considered.

explanation method in Fig 1 (top) marks “Kolstad”
as relevant, but the human annotator does not.

2.1 Small context: Hybrid document

paradigm

Given a collection of documents, hybrid docu-
ments are created by randomly concatenating doc-
ument fragments. We assume that, on average, the
most relevant input for a class k in a hybrid doc-
ument is located in a fragment that stems from a
document with gold label k. Hence, an explana-
tion method succeeds if it places maximal rele-
vance for k inside the correct fragment.

Formally, let xt be a word inside hybrid docu-
ment X that originates from a document X(cid:48) with
gold label y(X(cid:48)). xt’s gold label y(X, t) is set
to y(X(cid:48)). Let f (X) be the class assigned to the
hybrid document by a task method, and let φ
be an explanation method as deﬁned above. Let
rmax(X, φ) denote the position of the maximally
relevant word in X for the predicted class f (X).
If this maximally relevant word comes from a doc-
ument with the correct gold label, the explanation
method is awarded a hit:
hit(φ, X) = I[y(cid:0)X, rmax(X, φ)(cid:1) = f (X)]
where I[P ] is 1 if P is true and 0 otherwise. In
Fig 1 (bottom), the explanation method gradL2
1p
places rmax outside the correct (underlined) frag-
ment. Therefore, it does not get a hit point, while
limssems

(1)

s does.

The pointing game accuracy of an explana-
tion method is calculated as its total number of
hit points divided by the number of possible hit
points. This is a form of the pointing game
paradigm from computer vision (Zhang et al.,
2016).

2.2 Large context: Morphosyntactic

agreement paradigm

Many natural languages display morphosyntactic
agreement between words v and w. A DNN that

graddot
(cid:82) s
lrp
limssebb
gradL2
(cid:82) s
occ1
limssems

s

the link provided by the editor above [encourages ...]
the link provided by the editor above [encourages ...]
the link provided by the editor above [encourages ...]

few if any events in history [are ...]
few if any events in history [are ...]
few if any events in history [are ...]

Figure 2: Top: verb context classiﬁed singular.
Green: evidence for singular. Task method: GRU.
Bottom: verb context classiﬁed plural. Green: ev-
idence for plural. Task method: LSTM. Under-
lined: subject. Bold: rmax position.

predicts the agreeing feature in w should pay at-
tention to v. For example, in the sentence “the
children with the telescope are home”, the num-
ber of the verb (plural for “are”) can be predicted
from the subject (“children”) without looking at
the verb. If the language allows for v and w to be
far apart (Fig 3, top), successful task methods have
to be able to handle large contexts.

Linzen et al. (2016) show that English verb
number can be predicted by a unidirectional
LSTM with accuracy > 99%, based on left context
alone. When a task method predicts the correct
number, we expect successful explanation meth-
ods to place maximal relevance on the subject:

hittarget(φ, X) = I[rmax(X, φ) = target(X)]

where target(X) is the location of the subject,
and rmax is calculated as above. Regardless of
whether the prediction is correct, we expect rmax
to fall onto a noun that has the predicted number:

hitfeat(φ, X) = I[feat(cid:0)X, rmax(X, φ)(cid:1) = f (X)]

where feat(X, t) is the morphological feature
(here: number) of xt. In Fig 2, rmax on “link”
gives a hittarget point (and a hitfeat point), rmax
on “editor” gives a hitfeat point. gradL2
(cid:82) s does not
get any points as “history” is not a plural noun.

Labels for this task can be automatically gen-
erated using part-of-speech taggers and parsers,
which are available for many languages.

3 Explanation methods

In this section, we deﬁne the explanation meth-
ods that will be evaluated. For our purpose, ex-
planation methods produce word relevance scores
φ(t, k, X), which are speciﬁc to a given class k
and a given input X. φ(t, k, X) > φ(t(cid:48), k, X)
means that xt contributed more than xt(cid:48) to the task
method’s (potential) decision to classify X as k.

3.1 Gradient-based explanation methods

Gradient-based explanation methods approximate
the contribution of some DNN input i to some out-
put o with o’s gradient with respect to i (Simonyan
et al., 2014). In the following, we consider two
output functions o(k, X), the unnormalized class
score s(k, X) and the class probability p(k|X):
s(k, X) = (cid:126)wk · (cid:126)h(X) + bk

(2)

(3)

(4)

p(k|X) =

exp(cid:0)s(k, X)(cid:1)
k(cid:48)=1 exp(cid:0)s(k(cid:48), X)(cid:1)

(cid:80)K

where k is the target class, (cid:126)h(X) the document
representation (e.g., an RNN’s ﬁnal hidden layer),
(cid:126)wk (resp. bk) k’s weight vector (resp. bias).
The simple gradient of o(k, X) w.r.t. i is:

grad1(i, k, X) =

∂o(k, X)
∂i

grad1 underestimates the importance of inputs
that saturate a nonlinearity (Shrikumar et al.,
2017). To address this, Sundararajan et al. (2017)
integrate over all gradients on a linear interpola-
tion α ∈ [0, 1] between a baseline input ¯X (here:
all-zero embeddings) and X:
grad(cid:82) (i, k, X) = (cid:82) 1

∂α

∂o(k, ¯X+α(X− ¯X))
∂i
α=0
∂o(k, ¯X+ m
M (X− ¯X))
∂i

(5)

≈ 1
M

(cid:80)M

m=1

where M is a big enough constant (here: 50).

In NLP, symbolic inputs (e.g., words) are often
represented as one-hot vectors (cid:126)xt ∈ {1, 0}|V | and
embedded via a real-valued matrix: (cid:126)et = M(cid:126)xt.
Gradients are computed with respect to individual
entries of E = [(cid:126)e1 . . . (cid:126)e|X|]. Bansal et al. (2016)
and Hechtlinger (2016) use the L2 norm to reduce
vectors of gradients to single values:

φgradL2(t, k, X) = ||grad((cid:126)et, k, E)||
where grad((cid:126)et, k, E) is a vector of elementwise
gradients w.r.t. (cid:126)et. Denil et al. (2015) use the dot
product of the gradient vector and the embedding2,
i.e., the gradient of the “hot” entry in (cid:126)xt:

(6)

(7)

φgraddot(t, k, X) = (cid:126)et · grad((cid:126)et, k, E)
We use “grad1” for Eq 4, “grad(cid:82) ” for Eq 5, “p”
for Eq 3, “s” for Eq 2, “L2” for Eq 6 and “dot”
for Eq 7. This gives us eight explanation meth-
1p , graddot
ods: gradL2
1p , gradL2
(cid:82) s,
(cid:82) s , graddot
gradL2
(cid:82) p .
, replace (cid:126)et with (cid:126)et − (cid:126)¯et. Since our baseline

(cid:82) p, graddot

1s , graddot

1s , gradL2

2For graddot

(cid:82)

embeddings are all-zeros, this is equivalent.

3.2 Layer-wise relevance propagation

Layer-wise relevance propagation (LRP)
is a
backpropagation-based explanation method devel-
oped for fully connected neural networks and
CNNs (Bach et al., 2015) and later extended to
LSTMs (Arras et al., 2017b).
In this paper, we
use Epsilon LRP (Eq 58, Bach et al. (2015)). Re-
member that the activation of neuron j, aj, is the
sum of weighted upstream activations, (cid:80)
i aiwi,j,
plus bias bj, squeezed through some nonlinearity.
We denote the pre-nonlinearity activation of j as
a(cid:48)
j. The relevance of j, R(j), is distributed to up-
stream neurons i proportionally to the contribution
that i makes to a(cid:48)

j in the forward pass:

R(i) =

R(j)

(cid:88)

j

aiwi,j
j + esign(a(cid:48)

a(cid:48)

j)

(8)

This ensures that relevance is conserved between
layers, with the exception of relevance attributed
to bj. To prevent numerical instabilities, esign(a(cid:48))
returns −(cid:15) if a(cid:48) < 0 and (cid:15) otherwise. We set (cid:15) =
.001. The full algorithm is:

R(Lk(cid:48)) = s(k, X)I[k(cid:48) = k]
... recursive application of Eq 8 ...

φlrp(t, k, X) =

R(et,j)

dim((cid:126)et)
(cid:88)

j=1

where L is the ﬁnal layer, k the target class and
R(et,j) the relevance of dimension j in the t’th
embedding vector. For (cid:15) → 0 and provided that all
nonlinearities up to the unnormalized class score
are relu, Epsilon LRP is equivalent to the prod-
uct of input and raw score gradient (here: graddot
1s )
(Kindermans et al., 2016). In our experiments, the
second requirement holds only for CNNs.

Experiments by Ancona et al. (2017) (see §6)
suggest that LRP does not work well for LSTMs
if all neurons – including gates – participate in
backpropagation. We therefore use Arras et al.
(2017b)’s modiﬁcation and treat sigmoid-activated
gates as time step-speciﬁc weights rather than neu-
rons. For instance, the relevance of LSTM candi-
date vector (cid:126)gt is calculated from memory vector (cid:126)ct
and input gate vector(cid:126)it as

in(cid:126)it do not receive any relevance themselves. See
supplementary material for formal deﬁnitions of
Epsilon LRP for different architectures.

3.3 DeepLIFT

DeepLIFT (Shrikumar et al., 2017) is another
backpropagation-based explanation method. Un-
it does not explain s(k, X), but
like LRP,
s(k, X)−s(k, ¯X), where ¯X is some baseline input
(here: all-zero embeddings). Following Ancona
et al. (2018) (Eq 4), we use this backpropagation
rule:

R(i) =

R(j)

(cid:88)

j

aiwi,j − ¯aiwi,j
j + esign(a(cid:48)

a(cid:48)
j − ¯a(cid:48)

j − ¯a(cid:48)
j)

where ¯a refers to the forward pass of the base-
line. Note that the original method has a dif-
ferent mechanism for avoiding small denomina-
tors; we use esign for compatibility with LRP.
The DeepLIFT algorithm is started with R(Lk(cid:48)) =
(cid:0)s(k, X)−s(k, ¯X)(cid:1)I[k(cid:48) = k]. On gated (Q)RNNs,
we proceed analogous to LRP and treat gates as
weights.

3.4 Cell decomposition for gated RNNs

The cell decomposition explanation method for
LSTMs (Murdoch and Szlam, 2017) decomposes
the unnormalized class score s(k, X) (Eq 2) into
additive contributions. For every time step t, we
compute how much of (cid:126)ct “survives” until the ﬁnal
step T and contributes to s(k, X). This is achieved
by applying all future forget gates (cid:126)f , the ﬁnal tanh
nonlinearity, the ﬁnal output gate (cid:126)oT , as well as the
class weights of k to (cid:126)ct. We call this quantity “net
load of t for class k”:

nl(t, k, X) = (cid:126)wk ·

(cid:16)
(cid:126)oT (cid:12) tanh(cid:0)(

(cid:126)fj) (cid:12) (cid:126)ct

(cid:1)(cid:17)

T
(cid:89)

j=t+1

where (cid:12) and (cid:81) are applied elementwise. The rel-
evance of t is its gain in net load relative to t − 1:
φdecomp(t, k, X) = nl(t, k, X) − nl(t − 1, k, X).
For GRU, we change the deﬁnition of net load:

nl(t, k, X) = (cid:126)wk · (cid:0)(

(cid:126)zj) (cid:12) (cid:126)ht

(cid:1)

T
(cid:89)

j=t+1

R(gt,d) = R(ct,d)

gt,d · it,d
ct,d + esign(ct,d)

where (cid:126)z are GRU update gates.

3.5

Input perturbation methods

This is equivalent to applying Eq 8 while treating
(cid:126)it as a diagonal weight matrix. The gate neurons

Input perturbation methods assume that the re-
moval or masking of relevant inputs changes the

output (Zeiler and Fergus, 2014). Omission-
based methods remove inputs completely (K´ad´ar
et al., 2017), while occlusion-based methods re-
place them with a baseline (Li et al., 2016b). In
computer vision, perturbations are usually applied
to patches, as neighboring pixels tend to correlate
(Zintgraf et al., 2017). To calculate the omitN
(resp. occN ) relevance of word xt, we delete (resp.
occlude), one at a time, all N -grams that contain
xt, and average the change in the unnormalized
class score from Eq 2:

(cid:2)s(k, [(cid:126)e1 . . . (cid:126)e|X|])
φ[omit|occ]N (t, k, X) = (cid:80)N
−s(k, [(cid:126)e1 . . . (cid:126)et−N −1+j](cid:107)¯E(cid:107)[(cid:126)et+j . . . (cid:126)e|X|])(cid:3) 1

j=1

N

where (cid:126)et are embedding vectors, (cid:107) denotes con-
catenation and ¯E is either a sequence of length
zero (φomit) or a sequence of N baseline (here:
all-zero) embedding vectors (φocc).

3.6 LIMSSE: LIME for NLP

Local Interpretable Model-agnostic Explanations
(LIME) (Ribeiro et al., 2016) is a framework
for explaining predictions of complex classiﬁers.
LIME approximates the behavior of classiﬁer f in
the neighborhood of input X with an interpretable
(here: linear) model. The interpretable model is
trained on samples Z1 . . . ZN (here: N = 3000),
which are randomly drawn from X, with “gold la-
bels” f (Z1) . . . f (ZN ).

Since RNNs and CNNs respect word or-
der, we cannot use the bag of words sam-
pling method from the original description
of LIME. Instead, we introduce Local Inter-
pretable Model-agnostic Substring-based Expla-
nations (LIMSSE). LIMSSE uniformly samples
a length ln (here: 1 ≤ ln ≤ 6) and a start-
ing point sn, which deﬁne the substring Zn =
[(cid:126)xsn . . . (cid:126)xsn+ln−1]. To the linear model, Zn is rep-
resented by a binary vector (cid:126)zn ∈ {0, 1}|X|, where
zn,t = I[sn ≤ t < sn + ln].

We learn a linear weight vector ˆ(cid:126)vk ∈ R|X|,
whose entries are word relevances for k,
i.e.,
φlimsse(t, k, X) = ˆvk,t. To optimize it, we experi-
ment with three loss functions. The ﬁrst, which we
will refer to as limssebb, assumes that our DNN is
a total black box that delivers only a classiﬁcation:

(cid:0)p(k(cid:48)|Zn)(cid:1). The black
where f (Zn) = argmaxk(cid:48)
box approach is maximally general, but insensitive
to the magnitude of evidence found in Zn. Hence,
we also test magnitude-sensitive loss functions:

ˆ(cid:126)vk = argmin

(cid:0)(cid:126)zn · (cid:126)vk − o(k, Zn)(cid:1)2

(cid:88)

n

(cid:126)vk

where o(k, Zn) is one of s(k, Zn) or p(k|Zn). We
refer to these as limssems
s

and limssems
p .

4 Experiments

4.1 Hybrid document experiment

For the hybrid document experiment, we use the
20 newsgroups corpus (topic classiﬁcation) (Lang,
1995) and reviews from the 10th yelp dataset
challenge (binary sentiment analysis)3. We train
ﬁve DNNs per corpus: a bidirectional GRU (Cho
et al., 2014), a bidirectional LSTM (Hochreiter
and Schmidhuber, 1997), a 1D CNN with global
max pooling (Collobert et al., 2011), a bidirec-
tional Quasi-GRU (QGRU), and a bidirectional
Quasi-LSTM (QLSTM). The Quasi-RNNs are 1D
CNNs with a feature-wise gated recursive pooling
layer (Bradbury et al., 2017). Word embeddings
are R300 and initialized with pre-trained GloVe
embeddings (Pennington et al., 2014)4. The main
layer has a hidden size of 150 (bidirectional ar-
chitectures: 75 dimensions per direction). For the
QRNNs and CNN, we use a kernel width of 5. In
all ﬁve architectures, the resulting document rep-
resentation is projected to 20 (resp.
two) dimen-
sions using a fully connected layer, followed by a
softmax. See supplementary material for details
on training and regularization.

After training, we sentence-tokenize the test
sets, shufﬂe the sentences, concatenate ten sen-
tences at a time and classify the resulting hybrid
documents. Documents that are assigned a class
that is not the gold label of at least one con-
stituent word are discarded (yelp: < 0.1%; 20
newsgroups: 14% - 20%). On the remaining docu-
ments, we use the explanation methods from §3 to
ﬁnd the maximally relevant word for each predic-
tion. The random baseline samples the maximally
relevant word from a uniform distribution.

For reference, we also evaluate on a hu-
man judgment benchmark (Mohseni and Ra-
It contains
gan (2018), Table 2, C11-C15).

3www.yelp.com/dataset_challenge
4http://nlp.stanford.edu/data/glove.

ˆ(cid:126)vk = argmin

−(cid:2)log(cid:0)σ((cid:126)zn · (cid:126)vk)(cid:1)I[f (Zn) = k]

(cid:88)

n

(cid:126)vk

+ log(cid:0)1 − σ((cid:126)zn · (cid:126)vk)(cid:1)I[f (Zn) (cid:54)= k](cid:3)

840B.300d.zip

column

C01 C02 C03 C04 C05 C06 C07 C08 C09 C10 C11 C12 C13 C14 C15 C16 C17 C18 C19 C20 C21 C22 C23 C24 C25 C26 C27

hybrid document experiment

man. groundtruth

morphosyntactic agreement experiment

yelp

20 newsgroups

20 newsgroups

f (X) = y(X)

hittarget

hitfeat

N
N
C

U
R
G

U
R
G
Q

M
T
S
L

M
T
S
L
Q

N
N
C

U
R
G

U
R
G
Q

M
T
S
L

M
T
S
L
Q

U
R
G

U
R
G
Q

M
T
S
L

M
T
S
L
Q

f (X) (cid:54)= y(X)
M
T
S
L
Q

U
R
G
Q

M
T
S
L

U
R
G

U
R
G

U
R
G
Q

M
T
S
L

M
T
S
L
Q

N
N
C

U
R
G

U
R
G
Q

M
T
S
L

M
T
S
L
Q

φ
gradL2
.61 .68 .67 .70 .68 .45 .47 .25 .33 .79
1s
gradL2
.57 .67 .67 .70 .74 .40 .43 .26 .34 .70
1p
gradL2
.71 .66 .69 .71 .70 .58 .32 .26 .21 .82
(cid:82) s
gradL2
.71 .70 .72 .71 .77 .56 .34 .30 .23 .81
(cid:82) p
graddot
.88 .85 .81 .77 .86 .79 .76 .59 .72 .89
1s
graddot
.92 .88 .84 .79 .95 .78 .72 .59 .72 .81
1p
graddot
.84 .90 .85 .87 .87 .81 .68 .60 .68 .89
(cid:82) s
graddot
.86 .89 .84 .89 .96 .80 .69 .62 .73 .89
(cid:82) p
omit1
.79 .82 .85 .87 .61 .78 .75 .54 .76 .82
omit3
.89 .80 .89 .88 .59 .79 .71 .72 .81 .76
omit7
.92 .88 .91 .91 .70 .79 .77 .77 .84 .84
occ1
.80 .71 .74 .84 .61 .78 .73 .60 .77 .82
occ3
.92 .61 .93 .85 .59 .78 .63 .74 .74 .76
occ7
.92 .77 .93 .90 .70 .78 .62 .74 .77 .84
decomp
.79 .88 .92 .88
-
lrp
.92 .87 .91 .84 .86 .82 .83 .79 .85 .89
deeplift
.91 .89 .94 .85 .87 .82 .83 .78 .84 .89
limssebb
.81 .82 .83 .84 .78 .78 .81 .78 .80 .84
limssems
.94 .94 .93 .93 .91 .85 .87 .83 .86 .89
limssems
.87 .88 .85 .86 .94 .85 .86 .83 .86 .90
random .69 .67 .70 .69 .66 .20 .19 .22 .22 .21
last
-
-
-
-
3022 ≤ N ≤ 3230
N

-
-
-
7551 ≤ N ≤ 7554

.75 .79 .77 .80

p

s

-

-

-

-

.26 .31 .07 .18 .74
.18 .35 .07 .13 .66
.23 .15 .11 .08 .76
.13 .08 .14 .01 .78
.80 .70 .14 .47 .79
.71 .59 .20 .44 .69
.82 .64 .21 .26 .80
.80 .53 .40 .54 .78
.80 .48 .33 .48 .65
.77 .37 .36 .49 .61
.77 .49 .44 .55 .65
.77 .49 .19 .10 .65
.74 .37 .32 .35 .61
.74 .35 .43 .39 .65
.54 .36 .72 .51
-
.85 .72 .74 .81 .79
.84 .72 .70 .81 .80
.52 .53 .53 .54 .57
.85 .84 .76 .84 .82
.81 .80 .74 .76 .76
.09 .09 .06 .06 .08
-
-
-
137 ≤ N ≤ 150

-

-

.48 .23 .63 .19 .52 .27 .73 .22 .09 .11 .19 .19
.48 .22 .63 .18 .53 .26 .73 .21 .09 .09 .18 .11
.69 .67 .68 .51 .73 .70 .75 .55 .19 .22 .20 .20
.68 .77 .50 .70 .74 .82 .54 .78 .19 .21 .19 .30
.81 .62 .73 .56 .85 .66 .81 .59 .42 .34 .46 .36
.79 .58 .74 .54 .83 .61 .81 .56 .41 .33 .46 .35
.90 .87 .78 .84 .94 .92 .83 .89 .54 .51 .46 .52
.87 .85 .68 .84 .93 .92 .74 .93 .53 .48 .42 .51
.81 .81 .79 .80 .86 .87 .86 .84 .43 .45 .44 .45
.74 .77 .73 .73 .82 .84 .82 .79 .41 .45 .42 .46
.76 .80 .66 .74 .85 .88 .78 .80 .40 .48 .43 .47
.91 .85 .86 .86 .94 .88 .89 .88 .50 .44 .46 .47
.74 .73 .71 .72 .78 .76 .76 .76 .43 .37 .41 .43
.64 .65 .63 .65 .73 .73 .72 .73 .36 .35 .39 .43
.84 .87 .86 .90 .90 .93 .92 .96 .52 .58 .57 .63
.90 .90 .86 .91 .95 .95 .91 .95 .58 .60 .52 .63
.91 .90 .85 .91 .95 .95 .90 .95 .59 .59 .52 .63
.43 .41 .44 .42 .54 .51 .56 .52 .39 .43 .42 .41
.62 .62 .67 .63 .75 .74 .82 .75 .52 .53 .55 .53
.62 .62 .67 .63 .75 .74 .82 .75 .51 .53 .55 .53
.27 .27 .27 .27 .33 .33 .33 .33 .12 .13 .12 .12
.66 .67 .66 .67 .76 .77 .76 .77 .21 .27 .25 .26

N ≈ 1400000

N ≈ 20000

Table 2: Pointing game accuracies in hybrid document experiment (left), on manually annotated bench-
mark (middle) and in morphosyntactic agreement experiment (right). hittarget (resp. hitfeat): maximal
relevance on subject (resp. on noun with the predicted number feature). Bold: top explanation method.
Underlined: within 5 points of top explanation method.

188 documents from the 20 newsgroups test set
(classes sci.med and sci.electronics), with one
manually created list of relevant words per doc-
ument. We discard documents that are incorrectly
classiﬁed (20% - 27%) and deﬁne: hit(φ, X) =
I[rmax(X, φ) ∈ gt(X)], where gt(X) is the man-
ual ground truth.

4.2 Morphosyntactic agreement experiment

For the morphosyntactic agreement experiment,
we use automatically annotated English Wikipedia
sentences by Linzen et al. (2016)5. For our pur-
pose, a sample consists of: all words preceding the
verb: X = [x1 · · · xT ]; part-of-speech (POS) tags:
pos(X, t) ∈ {VBZ, VBP, NN, NNS, . . .}; and the
position of the subject: target(X) ∈ [1, T ]. The
number feature is derived from the POS:

feat(X, t) =






Sg
Pl
n/a

if pos(X, t) ∈ {VBZ, NN}
if pos(X, t) ∈ {VBP, NNS}
otherwise

The gold label of a sentence is the number of its
verb, i.e., y(X) = feat(X, T + 1).

5www.tallinzen.net/media/rnn_

agreement/agr_50_mostcommon_10K.tsv.gz

As task methods, we replicate Linzen et al.
(2016)’s unidirectional LSTM (R50 randomly
initialized word embeddings, hidden size 50).
We also train unidirectional GRU, QGRU and
QLSTM architectures with the same dimension-
ality. We use the explanation methods from §3 to
ﬁnd the most relevant word for predictions on the
test set. As described in §2.2, explanation methods
are awarded a hittarget (resp. hitfeat) point if this
word is the subject (resp. a noun with the predicted
number feature). For reference, we use a random
baseline as well as a baseline that assumes that the
most relevant word directly precedes the verb.

5 Discussion

5.1 Explanation methods

Our experiments suggest that explanation methods
for neural NLP differ in quality.

As in previous work (see §6), gradient L2
norm (gradL2) performs poorly, especially on
RNNs. We assume that this is due to its inability
to distinguish relevances for and against k.

Gradient embedding dot product (graddot)
is competitive on CNN (Table 2, graddot
1p C05,
graddot
1s C10, C15), presumably because relu is
linear on positive inputs, so gradients are exact in-

decomp
deeplift
limssems

p

lrp

limssems

p

initially a pagan culture , detailed information about the return of the christian religion to the islands during the norse-era [is ...]
initially a pagan culture , detailed information about the return of the christian religion to the islands during the norse-era [is ...]
initially a pagan culture , detailed information about the return of the christian religion to the islands during the norse-era [is ...]

Your day is done . Deﬁnitely looking forward to going back . All three were outstanding ! I would highly recommend going here to anyone .
We will see if anyone returns the message my boyfriend left . The price is unbelievable ! And our guys are on lunch so we ca n’t ﬁt you in . ” It
’s good , standard froyo . The pork shoulder was THAT tender . Try it with the Tomato Basil cram sauce .
Your day is done . Deﬁnitely looking forward to going back . All three were outstanding ! I would highly recommend going here to anyone .
We will see if anyone returns the message my boyfriend left . The price is unbelievable ! And our guys are on lunch so we ca n’t ﬁt you in . ” It
’s good , standard froyo . The pork shoulder was THAT tender . Try it with the Tomato Basil cram sauce .

Figure 3: Top: verb context classiﬁed singular. Task method: LSTM. Bottom: hybrid yelp review,
classiﬁed positive. Task method: QLSTM.

stead of approximate. graddot also has decent per-
formance for GRU (graddot
(cid:82) s C{06,
11, 16, 20, 24}), perhaps because GRU hidden ac-
tivations are always in [-1,1], where tanh and σ
are approximately linear.

1p C01, graddot

Integrated gradient (grad(cid:82) ) mostly outper-
forms simple gradient (grad1), though not consis-
tently (C01, C07). Contrary to expectation, in-
tegration did not help much with the failure of
the gradient method on LSTM on 20 newsgroups
(graddot
in C08, C13), which we had
assumed to be due to saturation of tanh on large
absolute activations in (cid:126)c. Smaller intervals may be
needed to approximate the integration, however,
this means additional computational cost.

vs. graddot

1

(cid:82)

1s vs. graddot

The gradient of s(k, X) performs better or sim-
ilar to the gradient of p(k|X). The main exception
is yelp (graddot
1p , C01-C05). This is
probably due to conﬂation by p(k|X) of evidence
for k (numerator in Eq 3) and against competi-
tor classes (denominator). In a two-class scenario,
there is little incentive to keep classes separate,
leading to information ﬂow through the denomi-
In future work, we will replace the two-
nator.
way softmax with a one-way sigmoid such that
φ(t, 0, X) := −φ(t, 1, X).

LRP and DeepLIFT are the most consistent
explanation methods across evaluation paradigms
and task methods. (The comparatively low point-
ing game accuracies on the yelp QRNNs and CNN
(C02, C04, C05) are probably due to the fact
that they explain s(k, .) in a two-way softmax,
see above.) On CNN (C05, C10, C15), LRP
and graddot
1s perform almost identically, suggest-
ing that they are indeed quasi-equivalent on this ar-
chitecture (see §3.2). On (Q)RNNs, modiﬁed LRP
and DeepLIFT appear to be superior to the gradi-
1s , deeplift vs. graddot
ent method (lrp vs. graddot
(cid:82) s ,
C01-C04, C06-C09, C11-C14, C16-C27).

Decomposition performs well on LSTM, es-
pecially in the morphosyntactic agreement exper-

iment, but it is inconsistent on other architec-
tures. Gated RNNs have a long-term additive and
a multiplicative pathway, and the decomposition
method only detects information traveling via the
additive one. Miao et al. (2016) show qualita-
tively that GRUs often reorganize long-term mem-
ory abruptly, which might explain the difference
between LSTM and GRU. QRNNs only have ad-
ditive recurrent connections; however, given that
(cid:126)ct (resp. (cid:126)ht) is calculated by convolution over sev-
eral time steps, decomposition relevance can be in-
correctly attributed inside that window. This likely
is the reason for the stark difference between the
performance of decomposition on QRNNs in the
hybrid document experiment and on the manually
labeled data (C07, C09 vs. C12, C14). Overall,
we do not recommend the decomposition method,
because it fails to take into account all routes by
which information can be propagated.

Omission and occlusion produce inconsis-
tent results in the hybrid document experiment.
Shrikumar et al. (2017) show that perturbation
methods can lack sensitivity when there are more
relevant inputs than the “perturbation window”
covers. In the morphosyntactic agreement experi-
ment, omission is not competitive; we assume that
this is because it interferes too much with syntactic
structure. occ1 does better (esp. C16-C19), possi-
bly because an all-zero “placeholder” is less dis-
ruptive than word removal. But despite some high
scores, it is less consistent than other explanation
methods.

LIMSSE

Magnitude-sensitive

(limssems)
consistently outperforms black-box LIMSSE
(limssebb), which suggests that numerical out-
puts should be used for approximation where
possible.
In the hybrid document experiment,
magnitude-sensitive LIMSSE outperforms the
other explanation methods (exceptions: C03,
C05). However, it fails in the morphosyntactic
agreement experiment (C16-C27).
In fact, we
expect LIMSSE to be unsuited for large context

problems, as it cannot discover dependencies
whose range is bigger than a given text sample.
In Fig 3 (top), limssems
highlights any singular
p
noun without taking into account how that noun
ﬁts into the overall syntactic structure.

5.2 Evaluation paradigms

The assumptions made by our automatic evalua-
tion paradigms have exceptions: (i) the correlation
between fragment of origin and relevance does not
always hold (e.g., a positive review may contain
negative fragments, and will almost certainly con-
tain neutral fragments); (ii) in morphological pre-
diction, we cannot always expect the subject to be
the only predictor for number. In Fig 2 (bottom)
for example, “few” is a reasonable clue for plural
despite not being a noun. This imperfect ground
truth means that absolute pointing game accura-
cies should be taken with a grain of salt; but we
argue that this does not invalidate them for com-
parisons.

We also point out that there are characteristics
of explanations that may be desirable but are not
reﬂected by the pointing game. Consider Fig 3
(bottom). Both explanations get hit points, but the
lrp explanation appears “cleaner” than limssems
p ,
with relevance concentrated on fewer tokens.

6 Related work

6.1 Explanation methods

Explanation methods can be divided into local
and global methods (Doshi-Velez and Kim, 2017).
Global methods infer general statements about
what a DNN has learned, e.g., by clustering docu-
ments (Aubakirova and Bansal, 2016) or n-grams
(K´ad´ar et al., 2017) according to the neurons that
they activate. Li et al. (2016a) compare embed-
dings of speciﬁc words with reference points to
measure how drastically they were changed dur-
ing training. In computer vision, Simonyan et al.
(2014) optimize the input space to maximize the
activation of a speciﬁc neuron. Global explanation
methods are of limited value for explaining a spe-
ciﬁc prediction as they represent average behavior.
Therefore, we focus on local methods.

Local explanation methods explain a decision
taken for one speciﬁc input at a time. We have
attempted to include all important local methods
for NLP in our experiments (see §3). We do
not address self-explanatory models (e.g., atten-
tion (Bahdanau et al., 2015) or rationale models

(Lei et al., 2016)), as these are very speciﬁc archi-
tectures that may not be not applicable to all tasks.

6.2 Explanation evaluation

According to Doshi-Velez and Kim (2017)’s
taxonomy of explanation evaluation paradigms,
application-grounded paradigms test how well an
explanation method helps real users solve real
tasks (e.g., doctors judge automatic diagnoses);
human-grounded paradigms rely on proxy tasks
(e.g., humans rank task methods based on expla-
nations); functionally-grounded paradigms work
without human input, like our approach.

Arras et al. (2016) (cf. Samek et al. (2016))
propose a functionally-grounded explanation eval-
uation paradigm for NLP where words in a cor-
rectly (resp.
incorrectly) classiﬁed document are
deleted in descending (resp. ascending) order of
relevance. They assume that the fewer words must
be deleted to reduce (resp. increase) accuracy, the
better the explanations. According to this metric,
LRP (§3.2) outperforms gradL2 on CNNs (Arras
et al., 2016) and LSTMs (Arras et al., 2017b) on
20 newsgroups. Ancona et al. (2017) perform the
same experiment with a binary sentiment analy-
sis LSTM. Their graph shows occ1, graddot
and
graddot
tied in ﬁrst place, while LRP, DeepLIFT
and the gradient L1 norm lag behind. Note that
their treatment of LSTM gates in LRP / DeepLIFT
differs from our implementation.

1

(cid:82)

An issue with the word deletion paradigm is that
it uses syntactically broken inputs, which may in-
troduce artefacts (Sundararajan et al., 2017).
In
our hybrid document paradigm, inputs are syntac-
tically intact (though semantically incoherent at
the document level); the morphosyntactic agree-
ment paradigm uses unmodiﬁed inputs.

Another class of functionally-grounded evalu-
ation paradigms interprets the performance of a
secondary task method, on inputs that are derived
from (or altered by) an explanation method, as a
proxy for the quality of that explanation method.
Murdoch and Szlam (2017) build a rule-based
classiﬁer from the most relevant phrases in a cor-
pus (task method: LSTM). The classiﬁer based
on decomp (§3.4) outperforms the gradient-based
classiﬁer, which is in line with our results. Ar-
ras et al. (2017a) build document representations
by summing over word embeddings weighted by
relevance scores (task method: CNN). They show
that K-nearest neighbor performs better on doc-

ument representations derived with LRP than on
those derived with gradL2, which also matches our
results. Denil et al. (2015) condense documents
by extracting top-K relevant sentences, and let the
original task method (CNN) classify them. The
accuracy loss, relative to uncondensed documents,
is smaller for graddot than for heuristic baselines.
In the domain of human-based evaluation
paradigms, Ribeiro et al. (2016) compare differ-
ent variants of LIME (§3.6) by how well they help
non-experts clean a corpus from words that lead
to overﬁtting. Selvaraju et al. (2017) assess how
well explanation methods help non-experts iden-
tify the more accurate out of two object recogni-
tion CNNs. These experiments come closer to real
use cases than functionally-grounded paradigms;
however, they are less scalable.

7 Summary

We conducted the ﬁrst comprehensive evaluation
of explanation methods for NLP, an important un-
dertaking because there is a need for understand-
ing the behavior of DNNs.

To conduct this study, we introduced evalua-
tion paradigms for explanation methods for two
classes of NLP tasks, small context tasks (e.g.,
topic classiﬁcation) and large context tasks (e.g.,
morphological prediction). Neither paradigm re-
quires manual annotations. We also introduced
LIMSSE, a substring-based explanation method
inspired by LIME and designed for NLP.

Based on our experimental results, we recom-
mend LRP, DeepLIFT and LIMSSE for small con-
text tasks and LRP and DeepLIFT for large con-
text tasks, on all ﬁve DNN architectures that we
tested. On CNNs and possibly GRUs, the (inte-
grated) gradient embedding dot product is a good
alternative to DeepLIFT and LRP.

8 Code

package:

implementation of LIMSSE,
the gradi-
Our
perturbation and decomposition meth-
ent,
the
ods can be found in our branch of
www.github.com/
keras
NPoe/keras.
To re-run our experiments,
in www.github.com/NPoe/
see
neural-nlp-explanation-experiment.
Our LRP implementation (same repository) is
adapted from Arras et al. (2017b)6.

scripts

6https://github.com/ArrasL/LRP_for_

LSTM

References
Marco Ancona, Enea Ceolini, Cengiz ¨Oztireli, and
Markus Gross. 2017. A uniﬁed view of gradient-
based attribution methods for deep neural networks.
In Conference on Neural Information Processing
System, Long Beach, USA.

Marco Ancona, Enea Ceolini, Cengiz ¨Oztireli, and
Markus Gross. 2018. Towards better understanding
of gradient-based attribution methods for deep neu-
ral networks. In International Conference on Learn-
ing Representations, Vancouver, Canada.

Leila Arras, Franziska Horn, Gr´egoire Montavon,
Klaus-Robert M¨uller, and Wojciech Samek. 2016.
Explaining predictions of non-linear classiﬁers in
NLP. In First Workshop on Representation Learn-
ing for NLP, pages 1–7, Berlin, Germany.

Leila Arras, Franziska Horn, Gr´egoire Montavon,
Klaus-Robert M¨uller, and Wojciech Samek. 2017a.
What is relevant in a text document?: An inter-
PloS one,
pretable machine learning approach.
12(8):e0181142.

Leila Arras, Gr´egoire Montavon, Klaus-Robert M¨uller,
and Wojciech Samek. 2017b. Explaining recurrent
neural network predictions in sentiment analysis. In
Eighth Workshop on Computational Approaches to
Subjectivity, Sentiment and Social Media Analysis,
pages 159–168, Copenhagen, Denmark.

Malika Aubakirova and Mohit Bansal. 2016. Interpret-
ing neural networks to improve politeness compre-
hension. In Empirical Methods in Natural Language
Processing, page 2035–2041, Austin, USA.

Sebastian Bach, Alexander Binder, Gr´egoire Mon-
tavon, Frederick Klauschen, Klaus-Robert M¨uller,
and Wojciech Samek. 2015. On pixel-wise explana-
tions for non-linear classiﬁer decisions by layer-wise
relevance propagation. PloS one, 10(7):e0130140.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations, San Diego,
USA.

Trapit Bansal, David Belanger, and Andrew McCal-
lum. 2016. Ask the GRU: Multi-task learning for
In ACM Conference
deep text recommendations.
on Recommender Systems, pages 107–114, Boston,
USA.

James Bradbury, Stephen Merity, Caiming Xiong, and
Richard Socher. 2017. Quasi-recurrent neural net-
In International Conference on Learning
works.
Representations, Toulon, France.

Kyunghyun Cho, Bart Van Merri¨enboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. In Eighth Workshop on Syntax, Semantics
and Structure in Statistical Translation, pages 103–
111, Doha, Qatar.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
Journal of Machine Learning Research,
scratch.
12(Aug):2493–2537.

Misha Denil, Alban Demiraj, and Nando de Freitas.
2015. Extraction of salient sentences from labelled
documents. In International Conference on Learn-
ing Representations, San Diego, USA.

Finale Doshi-Velez and Been Kim. 2017. A roadmap
for a rigorous science of interpretability. CoRR,
abs/1702.08608.

Bryce Goodman and Seth Flaxman. 2016. European
union regulations on algorithmic decision-making
and a “right to explanation”. In ICML Workshop on
Human Interpretability in Machine Learning, pages
26–30, New York, USA.

Yotam Hechtlinger. 2016. Interpretation of prediction
models using the input gradient. In Conference on
Neural Information Processing Systems, Barcelona,
Spain.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Akos K´ad´ar, Grzegorz Chrupała, and Afra Alishahi.
2017. Representation of linguistic form and func-
tion in recurrent neural networks. Computational
Linguistics, 43(4):761–780.

Pieter-Jan Kindermans, Kristof Sch¨utt, Klaus-Robert
M¨uller, and Sven D¨ahne. 2016. Investigating the in-
ﬂuence of noise and distractors on the interpretation
of neural networks. In Conference on Neural Infor-
mation Processing Systems, Barcelona, Spain.

Ken Lang. 1995. Newsweeder: Learning to ﬁlter
In International Conference on Machine

netnews.
Learning, pages 331–339, Tahoe City, USA.

Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016.
In Empirical
Rationalizing neural predictions.
Methods in Natural Language Processing, pages
107–117, Austin, USA.

Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.
2016a. Visualizing and understanding neural mod-
In NAACL-HLT, pages 681–691, San
els in NLP.
Diego, USA.

Jiwei Li, Will Monroe, and Dan Jurafsky. 2016b. Un-
derstanding neural networks through representation
erasure. CoRR, abs/1612.08220.

Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.
2016. Assessing the ability of LSTMs to learn
syntax-sensitive dependencies. Transactions of the
Association for Computational Linguistics, 4:521–
535.

Yajie Miao, Jinyu Li, Yongqiang Wang, Shi-Xiong
Zhang, and Yifan Gong. 2016. Simplifying long
short-term memory acoustic models for fast train-
In International Conference
ing and decoding.
on Acoustics, Speech and Signal Processing, pages
2284–2288.

Sina Mohseni and Eric D Ragan. 2018. A human-
grounded evaluation benchmark for local explana-
tions of machine learning. CoRR, abs/1801.05075.

W James Murdoch and Arthur Szlam. 2017. Auto-
matic rule extraction from long short term memory
networks. In International Conference on Learning
Representations, Toulon, France.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543, Doha, Qatar.

Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2016. Why should I trust you?: Ex-
In ACM
plaining the predictions of any classiﬁer.
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 1135–1144, San
Francisco, California.

Wojciech Samek, Alexander Binder, Gr´egoire Mon-
tavon, Sebastian Lapuschkin, and Klaus-Robert
M¨uller. 2016. Evaluating the visualization of what
IEEE trans-
a deep neural network has learned.
actions on neural networks and learning systems,
28(11):2660–2673.

Ramprasaath R Selvaraju, Michael Cogswell, Ab-
hishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. 2017. Grad-cam: Visual expla-
nations from deep networks via gradient-based lo-
calization. In IEEE Conference on Computer Vision
and Pattern Recognition, pages 618–626, Honolulu,
Hawaii.

Avanti Shrikumar, Peyton Greenside, and Anshul Kun-
daje. 2017. Learning important features through
propagating activation differences. In International
Conference on Machine Learning, pages 3145–
3153, Sydney, Australia.

Karen Simonyan, Andrea Vedaldi, and Andrew Zisser-
man. 2014. Deep inside convolutional networks: Vi-
sualising image classiﬁcation models and saliency
In International Conference on Learning
maps.
Representations, Banff, Canada.

Mukund Sundararajan, Ankur Taly, and Qiqi Yan.
2017. Axiomatic attribution for deep networks.
In International Conference on Machine Learning,
Sydney, Australia.

Matthew D Zeiler and Rob Fergus. 2014. Visualizing
In Eu-
and understanding convolutional networks.
ropean Conference on Computer Vision, pages 818–
833, Z¨urich, Switzerland.

Jianming Zhang, Zhe Lin, Jonathan Brandt, Xiaohui
Shen, and Stan Sclaroff. 2016. Top-down neural at-
In European Con-
tention by excitation backprop.
ference on Computer Vision, pages 543–559, Ams-
terdam, Netherlands.

Luisa M Zintgraf, Taco S Cohen, Tameem Adel, and
Max Welling. 2017. Visualizing deep neural net-
work decisions: Prediction difference analysis.
In
International Conference on Learning Representa-
tions, Toulon, France.

Evaluating neural network explanation methods using
hybrid documents and morphosyntactic agreement

Nina Poerner, Benjamin Roth & Hinrich Sch ¨utze
Center for Information and Language Processing
LMU Munich, Germany
poerner@cis.lmu.de

Abstract

The behavior of deep neural networks
(DNNs) is hard to understand. This makes
it necessary to explore post hoc expla-
nation methods. We conduct
the ﬁrst
comprehensive evaluation of explanation
methods for NLP. To this end, we design
two novel evaluation paradigms that cover
two important classes of NLP problems:
small context and large context problems.
Both paradigms require no manual annota-
tion and are therefore broadly applicable.
We also introduce LIMSSE, an explana-
tion method inspired by LIME that is de-
signed for NLP. We show empirically that
LIMSSE, LRP and DeepLIFT are the most
effective explanation methods and recom-
mend them for explaining DNNs in NLP.

1

Introduction

DNNs are complex models that combine linear
transformations with different types of nonlinear-
ities. If the model is deep, i.e., has many layers,
then its behavior during training and inference is
notoriously hard to understand.

This is a problem for both scientiﬁc method-
ology and real-world deployment.
Scientiﬁc
methodology demands that we understand our
models. In the real world, a decision (e.g., “your
blog post is offensive and has been removed”) by
itself is often insufﬁcient; in addition, an expla-
nation of the decision may be required (e.g., “our
system ﬂagged the following words as offensive”).
The European Union plans to mandate that intelli-
gent systems used for sensitive applications pro-
vide such explanations (European General Data
Protection Regulation, expected 2018, cf. Good-
man and Flaxman (2016)).

A number of post hoc explanation methods for
DNNs have been proposed. Due to the complexity
of the DNNs they explain, these methods are nec-
essarily approximations and come with their own
sources of error. At this point, it is not clear which
of these methods to use when reliable explanations
for a speciﬁc DNN architecture are needed.

Deﬁnitions. (i) A task method solves an NLP

problem, e.g., a GRU that predicts sentiment.

(ii) An explanation method explains the behav-
ior of a task method on a speciﬁc input. For our
purpose, it is a function φ(t, k, X) that assigns
real-valued relevance scores for a target class k
(e.g., positive) to positions t in an input text X
(e.g., “great food”). For this example, an ex-
planation method might assign: φ(1, k, X) >
φ(2, k, X).

(iii) An (explanation) evaluation paradigm
quantitatively evaluates explanation methods for a
task method, e.g., by assigning them accuracies.

Contributions. (i) We present novel evaluation
paradigms for explanation methods for two classes
of common NLP tasks (see §2). Crucially, nei-
ther paradigm requires manual annotations and
our methodology is therefore broadly applicable.
(ii) Using these paradigms, we perform a com-
prehensive evaluation of explanation methods for
NLP (§3). We cover the most important classes
of task methods, RNNs and CNNs, as well as the
recently proposed Quasi-RNNs.

(iii) We introduce LIMSSE (§3.6), an expla-
nation method inspired by LIME (Ribeiro et al.,

tasks

task methods
explanation methods
evaluation paradigms

sentiment analysis,
morphological prediction, . . .
CNN, GRU, LSTM, . . .
LIMSSE, LRP, DeepLIFT, . . .
hybrid document,
morphosyntactic agreement

Table 1: Terminology with examples.

lrp

From : kolstad @ cae.wisc.edu ( Joel Kolstad ) Subject : Re : Can Radio Freq . Be Used To Measure Distance ? [...] What is the difference
between vertical and horizontal ? Gravity ? Does n’t gravity pull down the photons and cause a doppler shift or something ? ( Just kidding ! )

gradL2
1p

limssems

s

If you ﬁnd faith to be honest , show me how . David The whole denominational mindset only causes more problems , sadly . ( See section 7 for
details . ) Thank you . ’The Armenians just shot and shot . Maybe coz they ’re ’quality’ cars ; - ) 200 posts/day . [...]
If you ﬁnd faith to be honest , show me how . David The whole denominational mindset only causes more problems , sadly . ( See section 7 for
details . ) Thank you . ’The Armenians just shot and shot . Maybe coz they ’re ’quality’ cars ; - ) 200 posts/day . [...]

Figure 1: Top:
sci.electronics post (not hybrid). Underlined: Manual relevance ground truth.
Green: evidence for sci.electronics. Task method: CNN. Bottom: hybrid newsgroup post, classiﬁed
talk.politics.mideast. Green: evidence for talk.politics.mideast. Underlined: talk.politics.mideast frag-
ment. Task method: QGRU. Italics: OOV. Bold: rmax position. See supplementary for full texts.

2016) that is designed for word-order sensitive
task methods (e.g., RNNs, CNNs). We show em-
pirically that LIMSSE, LRP (Bach et al., 2015)
and DeepLIFT (Shrikumar et al., 2017) are the
most effective explanation methods (§4): LRP and
DeepLIFT are the most consistent methods, while
LIMSSE wins the hybrid document experiment.

2 Evaluation paradigms

In this section, we introduce two novel evalua-
tion paradigms for explanation methods on two
types of common NLP tasks, small context tasks
and large context tasks. Small context tasks are
deﬁned as those that can be solved by ﬁnding
short, self-contained indicators, such as words and
phrases, and weighing them up (i.e., tasks where
CNNs with pooling can be expected to perform
well). We design the hybrid document paradigm
for evaluating explanation methods on small con-
text tasks. Large context tasks require the cor-
rect handling of long-distance dependencies, such
as subject-verb agreement.1 We design the mor-
phosyntactic agreement paradigm for evaluating
explanation methods on large context tasks.

We could also use human judgments for
evaluation. While we use Mohseni and Ragan
(2018)’s manual relevance benchmark for com-
parison, there are two issues with it: (i) Due to
the cost of human labor, it is limited in size and
domain. (ii) More importantly, a good explana-
tion method should not reﬂect what humans at-
tend to, but what task methods attend to. For in-
stance, the family name “Kolstad” has 11 out of
its 13 appearances in the 20 newsgroups corpus in
sci.electronics posts. Thus, task methods probably
learn it as a sci.electronics indicator. Indeed, the

1Consider deciding the number of [verb] in “the children
in the green house said that the big telescope [verb]” vs.
“the children in the green house who broke the big telescope
[verb]”. The local contexts of “children” or “[verb]” do not
sufﬁce to solve this problem, instead, the large context of the
entire sentence has to be considered.

explanation method in Fig 1 (top) marks “Kolstad”
as relevant, but the human annotator does not.

2.1 Small context: Hybrid document

paradigm

Given a collection of documents, hybrid docu-
ments are created by randomly concatenating doc-
ument fragments. We assume that, on average, the
most relevant input for a class k in a hybrid doc-
ument is located in a fragment that stems from a
document with gold label k. Hence, an explana-
tion method succeeds if it places maximal rele-
vance for k inside the correct fragment.

Formally, let xt be a word inside hybrid docu-
ment X that originates from a document X(cid:48) with
gold label y(X(cid:48)). xt’s gold label y(X, t) is set
to y(X(cid:48)). Let f (X) be the class assigned to the
hybrid document by a task method, and let φ
be an explanation method as deﬁned above. Let
rmax(X, φ) denote the position of the maximally
relevant word in X for the predicted class f (X).
If this maximally relevant word comes from a doc-
ument with the correct gold label, the explanation
method is awarded a hit:
hit(φ, X) = I[y(cid:0)X, rmax(X, φ)(cid:1) = f (X)]
where I[P ] is 1 if P is true and 0 otherwise. In
Fig 1 (bottom), the explanation method gradL2
1p
places rmax outside the correct (underlined) frag-
ment. Therefore, it does not get a hit point, while
limssems

(1)

s does.

The pointing game accuracy of an explana-
tion method is calculated as its total number of
hit points divided by the number of possible hit
points. This is a form of the pointing game
paradigm from computer vision (Zhang et al.,
2016).

2.2 Large context: Morphosyntactic

agreement paradigm

Many natural languages display morphosyntactic
agreement between words v and w. A DNN that

graddot
(cid:82) s
lrp
limssebb
gradL2
(cid:82) s
occ1
limssems

s

the link provided by the editor above [encourages ...]
the link provided by the editor above [encourages ...]
the link provided by the editor above [encourages ...]

few if any events in history [are ...]
few if any events in history [are ...]
few if any events in history [are ...]

Figure 2: Top: verb context classiﬁed singular.
Green: evidence for singular. Task method: GRU.
Bottom: verb context classiﬁed plural. Green: ev-
idence for plural. Task method: LSTM. Under-
lined: subject. Bold: rmax position.

predicts the agreeing feature in w should pay at-
tention to v. For example, in the sentence “the
children with the telescope are home”, the num-
ber of the verb (plural for “are”) can be predicted
from the subject (“children”) without looking at
the verb. If the language allows for v and w to be
far apart (Fig 3, top), successful task methods have
to be able to handle large contexts.

Linzen et al. (2016) show that English verb
number can be predicted by a unidirectional
LSTM with accuracy > 99%, based on left context
alone. When a task method predicts the correct
number, we expect successful explanation meth-
ods to place maximal relevance on the subject:

hittarget(φ, X) = I[rmax(X, φ) = target(X)]

where target(X) is the location of the subject,
and rmax is calculated as above. Regardless of
whether the prediction is correct, we expect rmax
to fall onto a noun that has the predicted number:

hitfeat(φ, X) = I[feat(cid:0)X, rmax(X, φ)(cid:1) = f (X)]

where feat(X, t) is the morphological feature
(here: number) of xt. In Fig 2, rmax on “link”
gives a hittarget point (and a hitfeat point), rmax
on “editor” gives a hitfeat point. gradL2
(cid:82) s does not
get any points as “history” is not a plural noun.

Labels for this task can be automatically gen-
erated using part-of-speech taggers and parsers,
which are available for many languages.

3 Explanation methods

In this section, we deﬁne the explanation meth-
ods that will be evaluated. For our purpose, ex-
planation methods produce word relevance scores
φ(t, k, X), which are speciﬁc to a given class k
and a given input X. φ(t, k, X) > φ(t(cid:48), k, X)
means that xt contributed more than xt(cid:48) to the task
method’s (potential) decision to classify X as k.

3.1 Gradient-based explanation methods

Gradient-based explanation methods approximate
the contribution of some DNN input i to some out-
put o with o’s gradient with respect to i (Simonyan
et al., 2014). In the following, we consider two
output functions o(k, X), the unnormalized class
score s(k, X) and the class probability p(k|X):
s(k, X) = (cid:126)wk · (cid:126)h(X) + bk

(2)

(3)

(4)

p(k|X) =

exp(cid:0)s(k, X)(cid:1)
k(cid:48)=1 exp(cid:0)s(k(cid:48), X)(cid:1)

(cid:80)K

where k is the target class, (cid:126)h(X) the document
representation (e.g., an RNN’s ﬁnal hidden layer),
(cid:126)wk (resp. bk) k’s weight vector (resp. bias).
The simple gradient of o(k, X) w.r.t. i is:

grad1(i, k, X) =

∂o(k, X)
∂i

grad1 underestimates the importance of inputs
that saturate a nonlinearity (Shrikumar et al.,
2017). To address this, Sundararajan et al. (2017)
integrate over all gradients on a linear interpola-
tion α ∈ [0, 1] between a baseline input ¯X (here:
all-zero embeddings) and X:
grad(cid:82) (i, k, X) = (cid:82) 1

∂α

∂o(k, ¯X+α(X− ¯X))
∂i
α=0
∂o(k, ¯X+ m
M (X− ¯X))
∂i

(5)

≈ 1
M

(cid:80)M

m=1

where M is a big enough constant (here: 50).

In NLP, symbolic inputs (e.g., words) are often
represented as one-hot vectors (cid:126)xt ∈ {1, 0}|V | and
embedded via a real-valued matrix: (cid:126)et = M(cid:126)xt.
Gradients are computed with respect to individual
entries of E = [(cid:126)e1 . . . (cid:126)e|X|]. Bansal et al. (2016)
and Hechtlinger (2016) use the L2 norm to reduce
vectors of gradients to single values:

φgradL2(t, k, X) = ||grad((cid:126)et, k, E)||
where grad((cid:126)et, k, E) is a vector of elementwise
gradients w.r.t. (cid:126)et. Denil et al. (2015) use the dot
product of the gradient vector and the embedding2,
i.e., the gradient of the “hot” entry in (cid:126)xt:

(6)

(7)

φgraddot(t, k, X) = (cid:126)et · grad((cid:126)et, k, E)
We use “grad1” for Eq 4, “grad(cid:82) ” for Eq 5, “p”
for Eq 3, “s” for Eq 2, “L2” for Eq 6 and “dot”
for Eq 7. This gives us eight explanation meth-
1p , graddot
ods: gradL2
1p , gradL2
(cid:82) s,
(cid:82) s , graddot
gradL2
(cid:82) p .
, replace (cid:126)et with (cid:126)et − (cid:126)¯et. Since our baseline

(cid:82) p, graddot

1s , graddot

1s , gradL2

2For graddot

(cid:82)

embeddings are all-zeros, this is equivalent.

3.2 Layer-wise relevance propagation

Layer-wise relevance propagation (LRP)
is a
backpropagation-based explanation method devel-
oped for fully connected neural networks and
CNNs (Bach et al., 2015) and later extended to
LSTMs (Arras et al., 2017b).
In this paper, we
use Epsilon LRP (Eq 58, Bach et al. (2015)). Re-
member that the activation of neuron j, aj, is the
sum of weighted upstream activations, (cid:80)
i aiwi,j,
plus bias bj, squeezed through some nonlinearity.
We denote the pre-nonlinearity activation of j as
a(cid:48)
j. The relevance of j, R(j), is distributed to up-
stream neurons i proportionally to the contribution
that i makes to a(cid:48)

j in the forward pass:

R(i) =

R(j)

(cid:88)

j

aiwi,j
j + esign(a(cid:48)

a(cid:48)

j)

(8)

This ensures that relevance is conserved between
layers, with the exception of relevance attributed
to bj. To prevent numerical instabilities, esign(a(cid:48))
returns −(cid:15) if a(cid:48) < 0 and (cid:15) otherwise. We set (cid:15) =
.001. The full algorithm is:

R(Lk(cid:48)) = s(k, X)I[k(cid:48) = k]
... recursive application of Eq 8 ...

φlrp(t, k, X) =

R(et,j)

dim((cid:126)et)
(cid:88)

j=1

where L is the ﬁnal layer, k the target class and
R(et,j) the relevance of dimension j in the t’th
embedding vector. For (cid:15) → 0 and provided that all
nonlinearities up to the unnormalized class score
are relu, Epsilon LRP is equivalent to the prod-
uct of input and raw score gradient (here: graddot
1s )
(Kindermans et al., 2016). In our experiments, the
second requirement holds only for CNNs.

Experiments by Ancona et al. (2017) (see §6)
suggest that LRP does not work well for LSTMs
if all neurons – including gates – participate in
backpropagation. We therefore use Arras et al.
(2017b)’s modiﬁcation and treat sigmoid-activated
gates as time step-speciﬁc weights rather than neu-
rons. For instance, the relevance of LSTM candi-
date vector (cid:126)gt is calculated from memory vector (cid:126)ct
and input gate vector(cid:126)it as

in(cid:126)it do not receive any relevance themselves. See
supplementary material for formal deﬁnitions of
Epsilon LRP for different architectures.

3.3 DeepLIFT

DeepLIFT (Shrikumar et al., 2017) is another
backpropagation-based explanation method. Un-
it does not explain s(k, X), but
like LRP,
s(k, X)−s(k, ¯X), where ¯X is some baseline input
(here: all-zero embeddings). Following Ancona
et al. (2018) (Eq 4), we use this backpropagation
rule:

R(i) =

R(j)

(cid:88)

j

aiwi,j − ¯aiwi,j
j + esign(a(cid:48)

a(cid:48)
j − ¯a(cid:48)

j − ¯a(cid:48)
j)

where ¯a refers to the forward pass of the base-
line. Note that the original method has a dif-
ferent mechanism for avoiding small denomina-
tors; we use esign for compatibility with LRP.
The DeepLIFT algorithm is started with R(Lk(cid:48)) =
(cid:0)s(k, X)−s(k, ¯X)(cid:1)I[k(cid:48) = k]. On gated (Q)RNNs,
we proceed analogous to LRP and treat gates as
weights.

3.4 Cell decomposition for gated RNNs

The cell decomposition explanation method for
LSTMs (Murdoch and Szlam, 2017) decomposes
the unnormalized class score s(k, X) (Eq 2) into
additive contributions. For every time step t, we
compute how much of (cid:126)ct “survives” until the ﬁnal
step T and contributes to s(k, X). This is achieved
by applying all future forget gates (cid:126)f , the ﬁnal tanh
nonlinearity, the ﬁnal output gate (cid:126)oT , as well as the
class weights of k to (cid:126)ct. We call this quantity “net
load of t for class k”:

nl(t, k, X) = (cid:126)wk ·

(cid:16)
(cid:126)oT (cid:12) tanh(cid:0)(

(cid:126)fj) (cid:12) (cid:126)ct

(cid:1)(cid:17)

T
(cid:89)

j=t+1

where (cid:12) and (cid:81) are applied elementwise. The rel-
evance of t is its gain in net load relative to t − 1:
φdecomp(t, k, X) = nl(t, k, X) − nl(t − 1, k, X).
For GRU, we change the deﬁnition of net load:

nl(t, k, X) = (cid:126)wk · (cid:0)(

(cid:126)zj) (cid:12) (cid:126)ht

(cid:1)

T
(cid:89)

j=t+1

R(gt,d) = R(ct,d)

gt,d · it,d
ct,d + esign(ct,d)

where (cid:126)z are GRU update gates.

3.5

Input perturbation methods

This is equivalent to applying Eq 8 while treating
(cid:126)it as a diagonal weight matrix. The gate neurons

Input perturbation methods assume that the re-
moval or masking of relevant inputs changes the

output (Zeiler and Fergus, 2014). Omission-
based methods remove inputs completely (K´ad´ar
et al., 2017), while occlusion-based methods re-
place them with a baseline (Li et al., 2016b). In
computer vision, perturbations are usually applied
to patches, as neighboring pixels tend to correlate
(Zintgraf et al., 2017). To calculate the omitN
(resp. occN ) relevance of word xt, we delete (resp.
occlude), one at a time, all N -grams that contain
xt, and average the change in the unnormalized
class score from Eq 2:

(cid:2)s(k, [(cid:126)e1 . . . (cid:126)e|X|])
φ[omit|occ]N (t, k, X) = (cid:80)N
−s(k, [(cid:126)e1 . . . (cid:126)et−N −1+j](cid:107)¯E(cid:107)[(cid:126)et+j . . . (cid:126)e|X|])(cid:3) 1

j=1

N

where (cid:126)et are embedding vectors, (cid:107) denotes con-
catenation and ¯E is either a sequence of length
zero (φomit) or a sequence of N baseline (here:
all-zero) embedding vectors (φocc).

3.6 LIMSSE: LIME for NLP

Local Interpretable Model-agnostic Explanations
(LIME) (Ribeiro et al., 2016) is a framework
for explaining predictions of complex classiﬁers.
LIME approximates the behavior of classiﬁer f in
the neighborhood of input X with an interpretable
(here: linear) model. The interpretable model is
trained on samples Z1 . . . ZN (here: N = 3000),
which are randomly drawn from X, with “gold la-
bels” f (Z1) . . . f (ZN ).

Since RNNs and CNNs respect word or-
der, we cannot use the bag of words sam-
pling method from the original description
of LIME. Instead, we introduce Local Inter-
pretable Model-agnostic Substring-based Expla-
nations (LIMSSE). LIMSSE uniformly samples
a length ln (here: 1 ≤ ln ≤ 6) and a start-
ing point sn, which deﬁne the substring Zn =
[(cid:126)xsn . . . (cid:126)xsn+ln−1]. To the linear model, Zn is rep-
resented by a binary vector (cid:126)zn ∈ {0, 1}|X|, where
zn,t = I[sn ≤ t < sn + ln].

We learn a linear weight vector ˆ(cid:126)vk ∈ R|X|,
whose entries are word relevances for k,
i.e.,
φlimsse(t, k, X) = ˆvk,t. To optimize it, we experi-
ment with three loss functions. The ﬁrst, which we
will refer to as limssebb, assumes that our DNN is
a total black box that delivers only a classiﬁcation:

(cid:0)p(k(cid:48)|Zn)(cid:1). The black
where f (Zn) = argmaxk(cid:48)
box approach is maximally general, but insensitive
to the magnitude of evidence found in Zn. Hence,
we also test magnitude-sensitive loss functions:

ˆ(cid:126)vk = argmin

(cid:0)(cid:126)zn · (cid:126)vk − o(k, Zn)(cid:1)2

(cid:88)

n

(cid:126)vk

where o(k, Zn) is one of s(k, Zn) or p(k|Zn). We
refer to these as limssems
s

and limssems
p .

4 Experiments

4.1 Hybrid document experiment

For the hybrid document experiment, we use the
20 newsgroups corpus (topic classiﬁcation) (Lang,
1995) and reviews from the 10th yelp dataset
challenge (binary sentiment analysis)3. We train
ﬁve DNNs per corpus: a bidirectional GRU (Cho
et al., 2014), a bidirectional LSTM (Hochreiter
and Schmidhuber, 1997), a 1D CNN with global
max pooling (Collobert et al., 2011), a bidirec-
tional Quasi-GRU (QGRU), and a bidirectional
Quasi-LSTM (QLSTM). The Quasi-RNNs are 1D
CNNs with a feature-wise gated recursive pooling
layer (Bradbury et al., 2017). Word embeddings
are R300 and initialized with pre-trained GloVe
embeddings (Pennington et al., 2014)4. The main
layer has a hidden size of 150 (bidirectional ar-
chitectures: 75 dimensions per direction). For the
QRNNs and CNN, we use a kernel width of 5. In
all ﬁve architectures, the resulting document rep-
resentation is projected to 20 (resp.
two) dimen-
sions using a fully connected layer, followed by a
softmax. See supplementary material for details
on training and regularization.

After training, we sentence-tokenize the test
sets, shufﬂe the sentences, concatenate ten sen-
tences at a time and classify the resulting hybrid
documents. Documents that are assigned a class
that is not the gold label of at least one con-
stituent word are discarded (yelp: < 0.1%; 20
newsgroups: 14% - 20%). On the remaining docu-
ments, we use the explanation methods from §3 to
ﬁnd the maximally relevant word for each predic-
tion. The random baseline samples the maximally
relevant word from a uniform distribution.

For reference, we also evaluate on a hu-
man judgment benchmark (Mohseni and Ra-
It contains
gan (2018), Table 2, C11-C15).

3www.yelp.com/dataset_challenge
4http://nlp.stanford.edu/data/glove.

ˆ(cid:126)vk = argmin

−(cid:2)log(cid:0)σ((cid:126)zn · (cid:126)vk)(cid:1)I[f (Zn) = k]

(cid:88)

n

(cid:126)vk

+ log(cid:0)1 − σ((cid:126)zn · (cid:126)vk)(cid:1)I[f (Zn) (cid:54)= k](cid:3)

840B.300d.zip

column

C01 C02 C03 C04 C05 C06 C07 C08 C09 C10 C11 C12 C13 C14 C15 C16 C17 C18 C19 C20 C21 C22 C23 C24 C25 C26 C27

hybrid document experiment

man. groundtruth

morphosyntactic agreement experiment

yelp

20 newsgroups

20 newsgroups

f (X) = y(X)

hittarget

hitfeat

N
N
C

U
R
G

U
R
G
Q

M
T
S
L

M
T
S
L
Q

N
N
C

U
R
G

U
R
G
Q

M
T
S
L

M
T
S
L
Q

U
R
G

U
R
G
Q

M
T
S
L

M
T
S
L
Q

f (X) (cid:54)= y(X)
M
T
S
L
Q

U
R
G
Q

M
T
S
L

U
R
G

U
R
G

U
R
G
Q

M
T
S
L

M
T
S
L
Q

N
N
C

U
R
G

U
R
G
Q

M
T
S
L

M
T
S
L
Q

φ
gradL2
.61 .68 .67 .70 .68 .45 .47 .25 .33 .79
1s
gradL2
.57 .67 .67 .70 .74 .40 .43 .26 .34 .70
1p
gradL2
.71 .66 .69 .71 .70 .58 .32 .26 .21 .82
(cid:82) s
gradL2
.71 .70 .72 .71 .77 .56 .34 .30 .23 .81
(cid:82) p
graddot
.88 .85 .81 .77 .86 .79 .76 .59 .72 .89
1s
graddot
.92 .88 .84 .79 .95 .78 .72 .59 .72 .81
1p
graddot
.84 .90 .85 .87 .87 .81 .68 .60 .68 .89
(cid:82) s
graddot
.86 .89 .84 .89 .96 .80 .69 .62 .73 .89
(cid:82) p
omit1
.79 .82 .85 .87 .61 .78 .75 .54 .76 .82
omit3
.89 .80 .89 .88 .59 .79 .71 .72 .81 .76
omit7
.92 .88 .91 .91 .70 .79 .77 .77 .84 .84
occ1
.80 .71 .74 .84 .61 .78 .73 .60 .77 .82
occ3
.92 .61 .93 .85 .59 .78 .63 .74 .74 .76
occ7
.92 .77 .93 .90 .70 .78 .62 .74 .77 .84
decomp
.79 .88 .92 .88
-
lrp
.92 .87 .91 .84 .86 .82 .83 .79 .85 .89
deeplift
.91 .89 .94 .85 .87 .82 .83 .78 .84 .89
limssebb
.81 .82 .83 .84 .78 .78 .81 .78 .80 .84
limssems
.94 .94 .93 .93 .91 .85 .87 .83 .86 .89
limssems
.87 .88 .85 .86 .94 .85 .86 .83 .86 .90
random .69 .67 .70 .69 .66 .20 .19 .22 .22 .21
last
-
-
-
-
3022 ≤ N ≤ 3230
N

-
-
-
7551 ≤ N ≤ 7554

.75 .79 .77 .80

p

s

-

-

-

-

.26 .31 .07 .18 .74
.18 .35 .07 .13 .66
.23 .15 .11 .08 .76
.13 .08 .14 .01 .78
.80 .70 .14 .47 .79
.71 .59 .20 .44 .69
.82 .64 .21 .26 .80
.80 .53 .40 .54 .78
.80 .48 .33 .48 .65
.77 .37 .36 .49 .61
.77 .49 .44 .55 .65
.77 .49 .19 .10 .65
.74 .37 .32 .35 .61
.74 .35 .43 .39 .65
.54 .36 .72 .51
-
.85 .72 .74 .81 .79
.84 .72 .70 .81 .80
.52 .53 .53 .54 .57
.85 .84 .76 .84 .82
.81 .80 .74 .76 .76
.09 .09 .06 .06 .08
-
-
-
137 ≤ N ≤ 150

-

-

.48 .23 .63 .19 .52 .27 .73 .22 .09 .11 .19 .19
.48 .22 .63 .18 .53 .26 .73 .21 .09 .09 .18 .11
.69 .67 .68 .51 .73 .70 .75 .55 .19 .22 .20 .20
.68 .77 .50 .70 .74 .82 .54 .78 .19 .21 .19 .30
.81 .62 .73 .56 .85 .66 .81 .59 .42 .34 .46 .36
.79 .58 .74 .54 .83 .61 .81 .56 .41 .33 .46 .35
.90 .87 .78 .84 .94 .92 .83 .89 .54 .51 .46 .52
.87 .85 .68 .84 .93 .92 .74 .93 .53 .48 .42 .51
.81 .81 .79 .80 .86 .87 .86 .84 .43 .45 .44 .45
.74 .77 .73 .73 .82 .84 .82 .79 .41 .45 .42 .46
.76 .80 .66 .74 .85 .88 .78 .80 .40 .48 .43 .47
.91 .85 .86 .86 .94 .88 .89 .88 .50 .44 .46 .47
.74 .73 .71 .72 .78 .76 .76 .76 .43 .37 .41 .43
.64 .65 .63 .65 .73 .73 .72 .73 .36 .35 .39 .43
.84 .87 .86 .90 .90 .93 .92 .96 .52 .58 .57 .63
.90 .90 .86 .91 .95 .95 .91 .95 .58 .60 .52 .63
.91 .90 .85 .91 .95 .95 .90 .95 .59 .59 .52 .63
.43 .41 .44 .42 .54 .51 .56 .52 .39 .43 .42 .41
.62 .62 .67 .63 .75 .74 .82 .75 .52 .53 .55 .53
.62 .62 .67 .63 .75 .74 .82 .75 .51 .53 .55 .53
.27 .27 .27 .27 .33 .33 .33 .33 .12 .13 .12 .12
.66 .67 .66 .67 .76 .77 .76 .77 .21 .27 .25 .26

N ≈ 1400000

N ≈ 20000

Table 2: Pointing game accuracies in hybrid document experiment (left), on manually annotated bench-
mark (middle) and in morphosyntactic agreement experiment (right). hittarget (resp. hitfeat): maximal
relevance on subject (resp. on noun with the predicted number feature). Bold: top explanation method.
Underlined: within 5 points of top explanation method.

188 documents from the 20 newsgroups test set
(classes sci.med and sci.electronics), with one
manually created list of relevant words per doc-
ument. We discard documents that are incorrectly
classiﬁed (20% - 27%) and deﬁne: hit(φ, X) =
I[rmax(X, φ) ∈ gt(X)], where gt(X) is the man-
ual ground truth.

4.2 Morphosyntactic agreement experiment

For the morphosyntactic agreement experiment,
we use automatically annotated English Wikipedia
sentences by Linzen et al. (2016)5. For our pur-
pose, a sample consists of: all words preceding the
verb: X = [x1 · · · xT ]; part-of-speech (POS) tags:
pos(X, t) ∈ {VBZ, VBP, NN, NNS, . . .}; and the
position of the subject: target(X) ∈ [1, T ]. The
number feature is derived from the POS:

feat(X, t) =






Sg
Pl
n/a

if pos(X, t) ∈ {VBZ, NN}
if pos(X, t) ∈ {VBP, NNS}
otherwise

The gold label of a sentence is the number of its
verb, i.e., y(X) = feat(X, T + 1).

5www.tallinzen.net/media/rnn_

agreement/agr_50_mostcommon_10K.tsv.gz

As task methods, we replicate Linzen et al.
(2016)’s unidirectional LSTM (R50 randomly
initialized word embeddings, hidden size 50).
We also train unidirectional GRU, QGRU and
QLSTM architectures with the same dimension-
ality. We use the explanation methods from §3 to
ﬁnd the most relevant word for predictions on the
test set. As described in §2.2, explanation methods
are awarded a hittarget (resp. hitfeat) point if this
word is the subject (resp. a noun with the predicted
number feature). For reference, we use a random
baseline as well as a baseline that assumes that the
most relevant word directly precedes the verb.

5 Discussion

5.1 Explanation methods

Our experiments suggest that explanation methods
for neural NLP differ in quality.

As in previous work (see §6), gradient L2
norm (gradL2) performs poorly, especially on
RNNs. We assume that this is due to its inability
to distinguish relevances for and against k.

Gradient embedding dot product (graddot)
is competitive on CNN (Table 2, graddot
1p C05,
graddot
1s C10, C15), presumably because relu is
linear on positive inputs, so gradients are exact in-

decomp
deeplift
limssems

p

lrp

limssems

p

initially a pagan culture , detailed information about the return of the christian religion to the islands during the norse-era [is ...]
initially a pagan culture , detailed information about the return of the christian religion to the islands during the norse-era [is ...]
initially a pagan culture , detailed information about the return of the christian religion to the islands during the norse-era [is ...]

Your day is done . Deﬁnitely looking forward to going back . All three were outstanding ! I would highly recommend going here to anyone .
We will see if anyone returns the message my boyfriend left . The price is unbelievable ! And our guys are on lunch so we ca n’t ﬁt you in . ” It
’s good , standard froyo . The pork shoulder was THAT tender . Try it with the Tomato Basil cram sauce .
Your day is done . Deﬁnitely looking forward to going back . All three were outstanding ! I would highly recommend going here to anyone .
We will see if anyone returns the message my boyfriend left . The price is unbelievable ! And our guys are on lunch so we ca n’t ﬁt you in . ” It
’s good , standard froyo . The pork shoulder was THAT tender . Try it with the Tomato Basil cram sauce .

Figure 3: Top: verb context classiﬁed singular. Task method: LSTM. Bottom: hybrid yelp review,
classiﬁed positive. Task method: QLSTM.

stead of approximate. graddot also has decent per-
formance for GRU (graddot
(cid:82) s C{06,
11, 16, 20, 24}), perhaps because GRU hidden ac-
tivations are always in [-1,1], where tanh and σ
are approximately linear.

1p C01, graddot

Integrated gradient (grad(cid:82) ) mostly outper-
forms simple gradient (grad1), though not consis-
tently (C01, C07). Contrary to expectation, in-
tegration did not help much with the failure of
the gradient method on LSTM on 20 newsgroups
(graddot
in C08, C13), which we had
assumed to be due to saturation of tanh on large
absolute activations in (cid:126)c. Smaller intervals may be
needed to approximate the integration, however,
this means additional computational cost.

vs. graddot

1

(cid:82)

1s vs. graddot

The gradient of s(k, X) performs better or sim-
ilar to the gradient of p(k|X). The main exception
is yelp (graddot
1p , C01-C05). This is
probably due to conﬂation by p(k|X) of evidence
for k (numerator in Eq 3) and against competi-
tor classes (denominator). In a two-class scenario,
there is little incentive to keep classes separate,
leading to information ﬂow through the denomi-
In future work, we will replace the two-
nator.
way softmax with a one-way sigmoid such that
φ(t, 0, X) := −φ(t, 1, X).

LRP and DeepLIFT are the most consistent
explanation methods across evaluation paradigms
and task methods. (The comparatively low point-
ing game accuracies on the yelp QRNNs and CNN
(C02, C04, C05) are probably due to the fact
that they explain s(k, .) in a two-way softmax,
see above.) On CNN (C05, C10, C15), LRP
and graddot
1s perform almost identically, suggest-
ing that they are indeed quasi-equivalent on this ar-
chitecture (see §3.2). On (Q)RNNs, modiﬁed LRP
and DeepLIFT appear to be superior to the gradi-
1s , deeplift vs. graddot
ent method (lrp vs. graddot
(cid:82) s ,
C01-C04, C06-C09, C11-C14, C16-C27).

Decomposition performs well on LSTM, es-
pecially in the morphosyntactic agreement exper-

iment, but it is inconsistent on other architec-
tures. Gated RNNs have a long-term additive and
a multiplicative pathway, and the decomposition
method only detects information traveling via the
additive one. Miao et al. (2016) show qualita-
tively that GRUs often reorganize long-term mem-
ory abruptly, which might explain the difference
between LSTM and GRU. QRNNs only have ad-
ditive recurrent connections; however, given that
(cid:126)ct (resp. (cid:126)ht) is calculated by convolution over sev-
eral time steps, decomposition relevance can be in-
correctly attributed inside that window. This likely
is the reason for the stark difference between the
performance of decomposition on QRNNs in the
hybrid document experiment and on the manually
labeled data (C07, C09 vs. C12, C14). Overall,
we do not recommend the decomposition method,
because it fails to take into account all routes by
which information can be propagated.

Omission and occlusion produce inconsis-
tent results in the hybrid document experiment.
Shrikumar et al. (2017) show that perturbation
methods can lack sensitivity when there are more
relevant inputs than the “perturbation window”
covers. In the morphosyntactic agreement experi-
ment, omission is not competitive; we assume that
this is because it interferes too much with syntactic
structure. occ1 does better (esp. C16-C19), possi-
bly because an all-zero “placeholder” is less dis-
ruptive than word removal. But despite some high
scores, it is less consistent than other explanation
methods.

LIMSSE

Magnitude-sensitive

(limssems)
consistently outperforms black-box LIMSSE
(limssebb), which suggests that numerical out-
puts should be used for approximation where
possible.
In the hybrid document experiment,
magnitude-sensitive LIMSSE outperforms the
other explanation methods (exceptions: C03,
C05). However, it fails in the morphosyntactic
agreement experiment (C16-C27).
In fact, we
expect LIMSSE to be unsuited for large context

problems, as it cannot discover dependencies
whose range is bigger than a given text sample.
In Fig 3 (top), limssems
highlights any singular
p
noun without taking into account how that noun
ﬁts into the overall syntactic structure.

5.2 Evaluation paradigms

The assumptions made by our automatic evalua-
tion paradigms have exceptions: (i) the correlation
between fragment of origin and relevance does not
always hold (e.g., a positive review may contain
negative fragments, and will almost certainly con-
tain neutral fragments); (ii) in morphological pre-
diction, we cannot always expect the subject to be
the only predictor for number. In Fig 2 (bottom)
for example, “few” is a reasonable clue for plural
despite not being a noun. This imperfect ground
truth means that absolute pointing game accura-
cies should be taken with a grain of salt; but we
argue that this does not invalidate them for com-
parisons.

We also point out that there are characteristics
of explanations that may be desirable but are not
reﬂected by the pointing game. Consider Fig 3
(bottom). Both explanations get hit points, but the
lrp explanation appears “cleaner” than limssems
p ,
with relevance concentrated on fewer tokens.

6 Related work

6.1 Explanation methods

Explanation methods can be divided into local
and global methods (Doshi-Velez and Kim, 2017).
Global methods infer general statements about
what a DNN has learned, e.g., by clustering docu-
ments (Aubakirova and Bansal, 2016) or n-grams
(K´ad´ar et al., 2017) according to the neurons that
they activate. Li et al. (2016a) compare embed-
dings of speciﬁc words with reference points to
measure how drastically they were changed dur-
ing training. In computer vision, Simonyan et al.
(2014) optimize the input space to maximize the
activation of a speciﬁc neuron. Global explanation
methods are of limited value for explaining a spe-
ciﬁc prediction as they represent average behavior.
Therefore, we focus on local methods.

Local explanation methods explain a decision
taken for one speciﬁc input at a time. We have
attempted to include all important local methods
for NLP in our experiments (see §3). We do
not address self-explanatory models (e.g., atten-
tion (Bahdanau et al., 2015) or rationale models

(Lei et al., 2016)), as these are very speciﬁc archi-
tectures that may not be not applicable to all tasks.

6.2 Explanation evaluation

According to Doshi-Velez and Kim (2017)’s
taxonomy of explanation evaluation paradigms,
application-grounded paradigms test how well an
explanation method helps real users solve real
tasks (e.g., doctors judge automatic diagnoses);
human-grounded paradigms rely on proxy tasks
(e.g., humans rank task methods based on expla-
nations); functionally-grounded paradigms work
without human input, like our approach.

Arras et al. (2016) (cf. Samek et al. (2016))
propose a functionally-grounded explanation eval-
uation paradigm for NLP where words in a cor-
rectly (resp.
incorrectly) classiﬁed document are
deleted in descending (resp. ascending) order of
relevance. They assume that the fewer words must
be deleted to reduce (resp. increase) accuracy, the
better the explanations. According to this metric,
LRP (§3.2) outperforms gradL2 on CNNs (Arras
et al., 2016) and LSTMs (Arras et al., 2017b) on
20 newsgroups. Ancona et al. (2017) perform the
same experiment with a binary sentiment analy-
sis LSTM. Their graph shows occ1, graddot
and
graddot
tied in ﬁrst place, while LRP, DeepLIFT
and the gradient L1 norm lag behind. Note that
their treatment of LSTM gates in LRP / DeepLIFT
differs from our implementation.

1

(cid:82)

An issue with the word deletion paradigm is that
it uses syntactically broken inputs, which may in-
troduce artefacts (Sundararajan et al., 2017).
In
our hybrid document paradigm, inputs are syntac-
tically intact (though semantically incoherent at
the document level); the morphosyntactic agree-
ment paradigm uses unmodiﬁed inputs.

Another class of functionally-grounded evalu-
ation paradigms interprets the performance of a
secondary task method, on inputs that are derived
from (or altered by) an explanation method, as a
proxy for the quality of that explanation method.
Murdoch and Szlam (2017) build a rule-based
classiﬁer from the most relevant phrases in a cor-
pus (task method: LSTM). The classiﬁer based
on decomp (§3.4) outperforms the gradient-based
classiﬁer, which is in line with our results. Ar-
ras et al. (2017a) build document representations
by summing over word embeddings weighted by
relevance scores (task method: CNN). They show
that K-nearest neighbor performs better on doc-

ument representations derived with LRP than on
those derived with gradL2, which also matches our
results. Denil et al. (2015) condense documents
by extracting top-K relevant sentences, and let the
original task method (CNN) classify them. The
accuracy loss, relative to uncondensed documents,
is smaller for graddot than for heuristic baselines.
In the domain of human-based evaluation
paradigms, Ribeiro et al. (2016) compare differ-
ent variants of LIME (§3.6) by how well they help
non-experts clean a corpus from words that lead
to overﬁtting. Selvaraju et al. (2017) assess how
well explanation methods help non-experts iden-
tify the more accurate out of two object recogni-
tion CNNs. These experiments come closer to real
use cases than functionally-grounded paradigms;
however, they are less scalable.

7 Summary

We conducted the ﬁrst comprehensive evaluation
of explanation methods for NLP, an important un-
dertaking because there is a need for understand-
ing the behavior of DNNs.

To conduct this study, we introduced evalua-
tion paradigms for explanation methods for two
classes of NLP tasks, small context tasks (e.g.,
topic classiﬁcation) and large context tasks (e.g.,
morphological prediction). Neither paradigm re-
quires manual annotations. We also introduced
LIMSSE, a substring-based explanation method
inspired by LIME and designed for NLP.

Based on our experimental results, we recom-
mend LRP, DeepLIFT and LIMSSE for small con-
text tasks and LRP and DeepLIFT for large con-
text tasks, on all ﬁve DNN architectures that we
tested. On CNNs and possibly GRUs, the (inte-
grated) gradient embedding dot product is a good
alternative to DeepLIFT and LRP.

8 Code

package:

implementation of LIMSSE,
the gradi-
Our
perturbation and decomposition meth-
ent,
the
ods can be found in our branch of
www.github.com/
keras
NPoe/keras.
To re-run our experiments,
in www.github.com/NPoe/
see
neural-nlp-explanation-experiment.
Our LRP implementation (same repository) is
adapted from Arras et al. (2017b)6.

scripts

6https://github.com/ArrasL/LRP_for_

LSTM

References
Marco Ancona, Enea Ceolini, Cengiz ¨Oztireli, and
Markus Gross. 2017. A uniﬁed view of gradient-
based attribution methods for deep neural networks.
In Conference on Neural Information Processing
System, Long Beach, USA.

Marco Ancona, Enea Ceolini, Cengiz ¨Oztireli, and
Markus Gross. 2018. Towards better understanding
of gradient-based attribution methods for deep neu-
ral networks. In International Conference on Learn-
ing Representations, Vancouver, Canada.

Leila Arras, Franziska Horn, Gr´egoire Montavon,
Klaus-Robert M¨uller, and Wojciech Samek. 2016.
Explaining predictions of non-linear classiﬁers in
NLP. In First Workshop on Representation Learn-
ing for NLP, pages 1–7, Berlin, Germany.

Leila Arras, Franziska Horn, Gr´egoire Montavon,
Klaus-Robert M¨uller, and Wojciech Samek. 2017a.
What is relevant in a text document?: An inter-
PloS one,
pretable machine learning approach.
12(8):e0181142.

Leila Arras, Gr´egoire Montavon, Klaus-Robert M¨uller,
and Wojciech Samek. 2017b. Explaining recurrent
neural network predictions in sentiment analysis. In
Eighth Workshop on Computational Approaches to
Subjectivity, Sentiment and Social Media Analysis,
pages 159–168, Copenhagen, Denmark.

Malika Aubakirova and Mohit Bansal. 2016. Interpret-
ing neural networks to improve politeness compre-
hension. In Empirical Methods in Natural Language
Processing, page 2035–2041, Austin, USA.

Sebastian Bach, Alexander Binder, Gr´egoire Mon-
tavon, Frederick Klauschen, Klaus-Robert M¨uller,
and Wojciech Samek. 2015. On pixel-wise explana-
tions for non-linear classiﬁer decisions by layer-wise
relevance propagation. PloS one, 10(7):e0130140.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations, San Diego,
USA.

Trapit Bansal, David Belanger, and Andrew McCal-
lum. 2016. Ask the GRU: Multi-task learning for
In ACM Conference
deep text recommendations.
on Recommender Systems, pages 107–114, Boston,
USA.

James Bradbury, Stephen Merity, Caiming Xiong, and
Richard Socher. 2017. Quasi-recurrent neural net-
In International Conference on Learning
works.
Representations, Toulon, France.

Kyunghyun Cho, Bart Van Merri¨enboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. In Eighth Workshop on Syntax, Semantics
and Structure in Statistical Translation, pages 103–
111, Doha, Qatar.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
Journal of Machine Learning Research,
scratch.
12(Aug):2493–2537.

Misha Denil, Alban Demiraj, and Nando de Freitas.
2015. Extraction of salient sentences from labelled
documents. In International Conference on Learn-
ing Representations, San Diego, USA.

Finale Doshi-Velez and Been Kim. 2017. A roadmap
for a rigorous science of interpretability. CoRR,
abs/1702.08608.

Bryce Goodman and Seth Flaxman. 2016. European
union regulations on algorithmic decision-making
and a “right to explanation”. In ICML Workshop on
Human Interpretability in Machine Learning, pages
26–30, New York, USA.

Yotam Hechtlinger. 2016. Interpretation of prediction
models using the input gradient. In Conference on
Neural Information Processing Systems, Barcelona,
Spain.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Akos K´ad´ar, Grzegorz Chrupała, and Afra Alishahi.
2017. Representation of linguistic form and func-
tion in recurrent neural networks. Computational
Linguistics, 43(4):761–780.

Pieter-Jan Kindermans, Kristof Sch¨utt, Klaus-Robert
M¨uller, and Sven D¨ahne. 2016. Investigating the in-
ﬂuence of noise and distractors on the interpretation
of neural networks. In Conference on Neural Infor-
mation Processing Systems, Barcelona, Spain.

Ken Lang. 1995. Newsweeder: Learning to ﬁlter
In International Conference on Machine

netnews.
Learning, pages 331–339, Tahoe City, USA.

Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016.
In Empirical
Rationalizing neural predictions.
Methods in Natural Language Processing, pages
107–117, Austin, USA.

Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.
2016a. Visualizing and understanding neural mod-
In NAACL-HLT, pages 681–691, San
els in NLP.
Diego, USA.

Jiwei Li, Will Monroe, and Dan Jurafsky. 2016b. Un-
derstanding neural networks through representation
erasure. CoRR, abs/1612.08220.

Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.
2016. Assessing the ability of LSTMs to learn
syntax-sensitive dependencies. Transactions of the
Association for Computational Linguistics, 4:521–
535.

Yajie Miao, Jinyu Li, Yongqiang Wang, Shi-Xiong
Zhang, and Yifan Gong. 2016. Simplifying long
short-term memory acoustic models for fast train-
In International Conference
ing and decoding.
on Acoustics, Speech and Signal Processing, pages
2284–2288.

Sina Mohseni and Eric D Ragan. 2018. A human-
grounded evaluation benchmark for local explana-
tions of machine learning. CoRR, abs/1801.05075.

W James Murdoch and Arthur Szlam. 2017. Auto-
matic rule extraction from long short term memory
networks. In International Conference on Learning
Representations, Toulon, France.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543, Doha, Qatar.

Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2016. Why should I trust you?: Ex-
In ACM
plaining the predictions of any classiﬁer.
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 1135–1144, San
Francisco, California.

Wojciech Samek, Alexander Binder, Gr´egoire Mon-
tavon, Sebastian Lapuschkin, and Klaus-Robert
M¨uller. 2016. Evaluating the visualization of what
IEEE trans-
a deep neural network has learned.
actions on neural networks and learning systems,
28(11):2660–2673.

Ramprasaath R Selvaraju, Michael Cogswell, Ab-
hishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. 2017. Grad-cam: Visual expla-
nations from deep networks via gradient-based lo-
calization. In IEEE Conference on Computer Vision
and Pattern Recognition, pages 618–626, Honolulu,
Hawaii.

Avanti Shrikumar, Peyton Greenside, and Anshul Kun-
daje. 2017. Learning important features through
propagating activation differences. In International
Conference on Machine Learning, pages 3145–
3153, Sydney, Australia.

Karen Simonyan, Andrea Vedaldi, and Andrew Zisser-
man. 2014. Deep inside convolutional networks: Vi-
sualising image classiﬁcation models and saliency
In International Conference on Learning
maps.
Representations, Banff, Canada.

Mukund Sundararajan, Ankur Taly, and Qiqi Yan.
2017. Axiomatic attribution for deep networks.
In International Conference on Machine Learning,
Sydney, Australia.

Matthew D Zeiler and Rob Fergus. 2014. Visualizing
In Eu-
and understanding convolutional networks.
ropean Conference on Computer Vision, pages 818–
833, Z¨urich, Switzerland.

Jianming Zhang, Zhe Lin, Jonathan Brandt, Xiaohui
Shen, and Stan Sclaroff. 2016. Top-down neural at-
In European Con-
tention by excitation backprop.
ference on Computer Vision, pages 543–559, Ams-
terdam, Netherlands.

Luisa M Zintgraf, Taco S Cohen, Tameem Adel, and
Max Welling. 2017. Visualizing deep neural net-
work decisions: Prediction difference analysis.
In
International Conference on Learning Representa-
tions, Toulon, France.

