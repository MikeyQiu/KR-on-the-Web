8
1
0
2
 
g
u
A
 
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
7
8
9
3
0
.
2
0
8
1
:
v
i
X
r
a

Latent Variable Time-varying Network Inference

Federico Tomasi∗
Università degli Studi di Genova
16146 Genova, Italy
federico.tomasi@dibris.unige.it

Saverio Salzo
Istituto Italiano di Tecnologia
16163 Genova, Italy
saverio.salzo@iit.it

Veronica Tozzo∗
Università degli Studi di Genova
16146 Genova, Italy
veronica.tozzo@dibris.unige.it

Alessandro Verri
Università degli Studi di Genova
16146 Genova, Italy
alessandro.verri@unige.it

ABSTRACT
In many applications of finance, biology and sociology, complex
systems involve entities interacting with each other. These pro-
cesses have the peculiarity of evolving over time and of comprising
latent factors, which influence the system without being explicitly
measured. In this work we present latent variable time-varying
graphical lasso (LTGL), a method for multivariate time-series graph-
ical modelling that considers the influence of hidden or unmea-
surable factors. The estimation of the contribution of the latent
factors is embedded in the model which produces both sparse and
low-rank components for each time point. In particular, the first
component represents the connectivity structure of observable vari-
ables of the system, while the second represents the influence of
hidden factors, assumed to be few with respect to the observed
variables. Our model includes temporal consistency on both com-
ponents, providing an accurate evolutionary pattern of the system.
We derive a tractable optimisation algorithm based on alternating
direction method of multipliers, and develop a scalable and effi-
cient implementation which exploits proximity operators in closed
form. LTGL is extensively validated on synthetic data, achieving
optimal performance in terms of accuracy, structure learning and
scalability with respect to ground truth and state-of-the-art meth-
ods for graphical inference. We conclude with the application of
LTGL to real case studies, from biology and finance, to illustrate
how our method can be successfully employed to gain insights on
multivariate time-series data.

KEYWORDS
network inference; graphical models; latent variables; time-series;
convex optimization

ACM Reference Format:
Federico Tomasi, Veronica Tozzo, Saverio Salzo, and Alessandro Verri. 2018.
Latent Variable Time-varying Network Inference. In KDD ’18: The 24th ACM

∗These authors equally contributed to the paper.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
KDD ’18, August 19–23, 2018, London, United Kingdom
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5552-0/18/08. . . $15.00
https://doi.org/10.1145/3219819.3220121

SIGKDD International Conference on Knowledge Discovery & Data Mining,
August 19–23, 2018, London, United Kingdom. ACM, New York, NY, USA,
9 pages. https://doi.org/10.1145/3219819.3220121

1 INTRODUCTION
The problem of understanding complex systems arises in many
diverse contexts, such as financial markets [23, 29], social networks
[14] and biology [18, 19]. In such contexts, the goal is to analyse the
system in order to retrieve information on how the components be-
have. This requires accurate and interpretable mathematical models
whose parameters, in practice, need to be estimated from observa-
tions.
Mathematically, a system can be modelled as a network of interac-
tions (edges) between its entities (nodes). However, the underlying
structure of the variables within the system is usually not known a
priori. Nevertheless, observations of the system (i.e., data) incorpo-
rate information on the interactions between variables, since they
provide measurements of such variables acting in the system.
The problem of inferring a network of variable interactions from
data is known as network inference or graphical model selection
[15, 22]. During the last years, the graphical modelling problem
has received much attention, particularly for the availability of
an always increasing number of samples that are required for a
reliable network inference. Nonetheless, structure estimation of
complex systems remains challenging for many reasons. In this
work, we want to tackle two aspects: (i) the presence of global
hidden (or latent) factors, and (ii) the dynamic of systems that
evolve in time. We argue that the inference of a dynamical network
encoding a complex system requires a specific attention to both
aspects to result in a more realistic representation. In particular, a
system may be affected by (latent) factors not encoded in the model.
Such factors, acting in the system, influence how the observable
entities behave and, hence, how they are connected with each other
[10]. The consideration of hidden and unmeasured variables during
the inference process emerges as crucial to avoid misrepresenting
real-world data [26].
At the same time, a complex system depends on a temporal com-
ponent, which drives variable interactions to evolve consistently
during its extent. This means that the structure can either change
or remain stable according to the nature of the system itself. Hence,
the understanding of a complex system is bound to the observation
of its evolution. This is particularly evident in some applications,
such as biology, where the interest could be to understand the
response of the system to perturbation [28].

Related work. Latent variable models have been widely studied
in literature, and shown to outperform graphical models that only
consider observable variables [8, 11, 33]. At the same time, a set of
methods were designed to study the temporal component through
the inference of a dynamical network that incorporates prior knowl-
edge on the behaviour of the system [4, 17]. Time-series with latent
variables are considered to obtain a single graph which represents
the global system [2, 20]. However, to the best of our knowledge,
state-of-the-art methods for regularised network inference do not
consider simultaneously time and latent variables in the inference
of multiple connected networks.

Contribution. In this work we propose latent variable time-varying
graphical lasso (LTGL), a model for dynamical network inference
where the structure is influenced by latent factors. This can be
seen as an attempt to generalise both dynamical and latent variable
network inference under a single unified framework. In partic-
ular, starting from a set of observations of a system at different
time points, LTGL infers an interaction network of the observed
variables under the influence of latent factors, while taking in con-
sideration the temporal evolution of the system. The empirical
interaction network is decomposed into the true underlying struc-
ture of the network and the contribution of latent factors, under
the assumption that both observable variables and latent factors
interdependence follow a temporal non-random behaviour. For
this reason, the model allows to include prior knowledge on the
evolutionary pattern of the system. The imposition of such prior
knowledge benefits inference and subsequent analysis of the net-
work, accentuating precise dynamical patterns. This is particularly
important when the number of samples is low compared to the
number of observed and latent variables in the system. In fact, the
inference of the network at particular time points exploits the de-
pendence between consecutive temporal states. Such advantage
is achieved by a simultaneous inference of all the dynamical sys-
tem, that, mathematically, translates into imposing constraints on
the network behaviour. In this work we provide a set of possible
constraints that can be applied independently on both observed
and latent components, allowing for a wide range of evolutionary
patterns.
Figure 1 provides an example of the theoretical model assumed by
LTGL. Here, observed and latent variables (xi and zi ) are connected
in a slightly different way at each time. Note that the observations
of the system only involve variables xi , while the hidden factors zi
influence the system without being actually observed. Hence, when
analysing samples which are regulated from a dynamical network
with hidden factors, it is infeasible to precisely infer the identity of
latent variables, but only an estimation of their number and their
effect on the global system can be obtained.
Starting from the theoretical model we derived a minimisation
algorithm based on the alternating direction method of multipli-
ers (ADMM) [7]. The algorithm is divided into independent steps
using proximal operators, which can be solved by closed-form so-
lutions favouring a fast and scalable computation [12, 17, 24]. We
also provide the related implementation in a Python framework,
based on the use of highly optimised low-level libraries for nu-
merical computation. Experiments on synthetic data show LTGL
to achieve optimal performance in relation to ground truth and

Figure 1. A dynamical network with latent factors zi and observed
variables xi . At each time ti , all of the connections between la-
lines) and connections among ob-
tent and observed variables (
served variables (
lines) may change according to a specific tem-
poral behaviour.

to state-of-the-art methods for graphical modelling, in terms of
accuracy, structure learning and scalability. Moreover, we show the
computational efficiency of LTGL while increasing the number of
unknowns of the problem and the model complexity. We conclude
with the application of LTGL to real-world data sets to illustrate
how our method can be successfully employed to gain insights on
multivariate time-series data. In particular, we used biological and
financial data sets, to show the use of LTGL in different contexts.
In the first case, we analysed Escherichia coli response to pertur-
bation, correctly identified by our method. In the latter case, we
investigated on a financial data set, to show how the contribution
of latent factors is relevant for the understanding of the behaviour
of the system.

Outline. The paper is organised as follows. Section 2 includes a
background on the reference frameworks for static and dynamical
network inference. Section 3 contains the theoretical formulation
of the problem and the proposed method. Section 4 describes in
details the optimisation algorithm for the minimisation of the func-
tional. Section 5 and Section 6 illustrate the use of our method on
synthetic and real data, respectively. Finally, Section 7 concludes
with a discussion and future directions.

2 PRELIMINARIES
Given a graph G = (V, E), where V = {x1, . . . , xd } is a finite set
of vertices, and E ⊆ V × V is a set of edges, a graphical model
is a multivariate probability distribution on x1, . . . , xd variables
where the conditional independence between two variables xi and
xj given all the others is encoded in G [22]. The two variables xi and
xj are conditionally independent given the others if (xi , xj ) (cid:60) E
and (xj , xi ) (cid:60) E. In what follows, we consider only undirected
Gaussian graphical models (GGMs), where (i) there is no distinc-
tion between an edge (xi , xj ) ∈ E and (xj , xi ), and (ii) variables are
jointly distributed according to a multivariate Gaussian distribution
N (µ, Σ). Without loss of generality, we assume µ to be zero, thus
the distribution depends only on the covariance matrix Σ [11]. The
inverse covariance matrix Θ = Σ−1, called precision matrix, encodes
the conditional independence between pairs of variables. In partic-
ular, the precision matrix has a zero entry in the position i, j only
if (xi , xj ) (cid:60) E [22]. Hence, we can interpret the precision matrix
as the weighted adjacency matrix of G, encoding the dependence
between variables.

Network inference. Consider a series of samples drawn from a
multivariate Gaussian distribution X ∼ N (0, Σ), X ∈ Rn×d . Net-
work inference aims at recovering the graphical model of the d

variables, i.e., the interaction structure Θ, given n observed samples.
The graphical modelling problem has been extensively tackled in
literature by estimating the precision matrix Θ instead of the co-
variance matrix Σ (as, e.g., [5]) [15, 30, 34]. This has been shown
to improve the graphical model inference, particularly for high-
dimensional problems [25]. In such contexts, the assumption is that
a variable is conditionally dependent only on a subset of all the
others. Therefore, the estimation of the precision matrix may be
guided by a sparse prior, in such a way to restrict the number of
possible connections in the network to improve interpretability
and noise reduction. Also, the imposition of a sparse prior on the
problem helps with the identifiability of the graph, especially when
the available number of samples is low compared to the dimension
of the problem. A model for the inference of Θ including the sparse
prior is the graphical lasso [15]:

minimize
Θ

− ℓ(S, Θ) + α ∥Θ ∥od, 1,

(1)

where ℓ is the Gaussian log-likelihood (up to a constant and scaling
factor) defined as ℓ(S, Θ) = log det(Θ)−tr(SΘ) for Θ positive definite
n X ⊤X is the empirical covariance matrix. ∥ · ∥od,1 is the
and S = 1
off-diagonal ℓ1-norm, which promotes sparsity in the precision
matrix (excluding the diagonal).

Latent variable network inference. Often, real-world observa-
tions do not conform exactly to a sparse GGM. This is due to global
hidden factors that influence the system, which introduce spurious
dependencies between observed variables [10, 11]. For this reason,
GGMs can be extended by introducing latent variables able to rep-
resent factors which are not observed in the data. These latent
variables are not principal components, since they do not provide a
low-rank approximation of the graphical model. On the contrary,
such factors are added to the model in order to condition the statis-
tics of the observed variables. In particular, we consider both latent
and observed variables to have a common domain [11].
Let latent variables be indexed by a set H , and observed variables
by a set O. The precision matrix Θ of the joint distribution of both
latent and observed variables may be partitioned into four blocks:

Θ = Σ−1 =

ΘH

ΘH O

ΘO H

ΘO












.












Such blocks represent the conditional dependencies among latent
variables (ΘH ), observed variables (ΘO ), between latent and ob-
served (ΘH O ), and viceversa (ΘO H ). The marginal precision matrix
Σ−1
O of the observed variables is given by the Schur complement
w.r.t. the block ΘH [8]:

ˆΘO = Σ−1
O

= ΘO − ΘO H Θ−1

H ΘH O .

(2)
ΘO specifies the precision matrix of the conditional statistics of the
observed variables given the latent variables, while ΘO H Θ−1
H ΘH O
is a summary of the effect of marginalisation over the latent vari-
ables. Such matrix has a small rank if the number of latent variables
is small compared to the observed variables. Note that the rank is
an indicator of the number of latent variables H [9].
The effect of the marginalisation is scattered over many observed
variables, in such a way not to confound it with the true underlying
conditional sparse structure of ΘO [8]. Typically ˆΘO is not sparse
due to the low-rank term, while, with the addition of the latent
factors contribution, we can recover the true sparse GGM. For this

reason, the graphical lasso in Equation (1) has been extended with
the latent variable graphical lasso that includes the inference of a
low-rank term, using the form (2), as follows [8]:

˜Θ, ˜L = argmin
(Θ, L)
L≽0

− ℓ(S, Θ − L) + α ∥Θ ∥od, 1 + τ ∥L ∥∗ .

(3)

Here ˜Θ provides an estimate of ΘO (precision matrix of the ob-
served variables) while ˜L provides an estimate of ΘO H Θ−1
H ΘH O
(marginalisation over the latent variables). Note that S is the empir-
ical covariance matrix computed only on observed variables, since
no information on the latent ones is available.

Time-varying network inference. Problems (1) and (3) aim at
recovering the structure of the system at fixed time (static network
inference). However, complex systems have temporal dynamics that
regulate their overall functioning [1, 15]. Hence, the modelling
of such complex systems requires a dynamical network inference,
where the states of the network are co-dependent. This naturally
leads to the idea of temporal consistency, which assumes similarities
between consecutive states of the network. In fact, we can assume
that, for sufficiently close time points, a system shows negligible
differences. During the inference of a dynamical network, temporal
consistency may translate into the imposition of similarities among
temporally close networks [16]. In particular, graphical lasso with
temporal consistency results in time-varying graphical lasso [17]:

minimize
(Θ1, . . ., ΘT )

T
(cid:213)

(cid:20)

i =1

− ℓ(Si, Θi ) + α ∥Θi ∥od, 1

+ β

Ψ(Θi +1 − Θi )

(4)

(cid:21)

T −1
(cid:213)

i =1

i j | · |.

where the inference of a network at a single time point i is guided
by the states at adjacent time points. The network is encoded in
a sequence of precision matrices (Θ1, . . . , ΘT ) which model the
system at each time point i = 1, . . . ,T . The type of similarity
imposed to consecutive time points and its strength are specified by
the penalty function Ψ and the parameter β, respectively. Options
when choosing Ψ include the following [17]:
• Lasso penalty (ℓ1) - Ψ = (cid:205)
Encourages few edges to change between subsequent time points,
while the rest of the structure remains the same [12].
• Group lasso penalty - Ψ = (cid:205)
j ∥ ·j ∥2.
Encourages the graph to restructure at some time points and to
stay stable in others [16? ].
• Laplacian penalty - Ψ = (cid:205)
i j (·i j )2.
Encourages smooth transitions over time, for slow changes of the
global structure [? ].
• Max norm penalty (ℓ∞) - Ψ = (cid:205)
Encourages a block of nodes to change their structure with no
additional penalty with respect to the change of a single edge among
such nodes. In fact, ℓ∞ norm is influenced only from the most
changing element for each row.
• Row-column overlap penalty - Ψ = minV :A=V +V ⊤ (cid:205)
Encourages a major change of the network at a specific time, while
in the rest the network is enforced to remain constant. The choice
of p = 2 causes the penalty to be node-based, i.e., the penalty allows
for a perturbation of only some nodes [? ].

j (maxi | ·i j |).

j ∥Vj ∥p .

Dependently from prior assumptions on the problem, one may
choose the most appropriate penalty for the data at hand.

3 MODEL FORMULATION
In this work we propose a new statistical model for the inference
of networks that change consistently in time under the influence
of latent factors. We call such model latent variable time-varying
graphical lasso (LTGL). LTGL infers the dynamical network of com-
plex systems by decomposing the problem into two parts — simi-
larly to what has been done in [8] for static network inference. We
consider two components of the dynamical network: a true under-
lying structure on the observed variables and the contribution of
latent factors. This allows to factor out the contribution of hidden
variables, favouring a reliable modelling of the dynamical system.
The novelty of our method is the simultaneous inference of a dy-
namical network with latent factors that exploits the imposition of
behavioural consistency on both observed variables interactions
and latent influence through the use of penalisation terms. This
allows for an easier interpretation of the evolution of the dynamical
system while, at the same time, improving its graphical modelling.
The two separate (while closely related) components at each time
point are obtained by integrating the network inference with the
information coming from temporally different states of the network.
Formally, let Xi ∈ Rni ×d , for i = 1, . . . ,T , be a set of observations
measured at T different time points composed by ni samples of d
observed variables. (Note that, for each time point i, samples are
assumed to be drawn from the probability distribution on the ob-
served variables conditioned on the latent ones.) Let Si = 1
i Xi
ni
be the empirical covariance matrix at time i. The goal is to retrieve a
set of sparse matrices Θ = (Θ1, . . . , ΘT ) and a set of low-rank matri-
ces L = (L1, . . . , LT ) such that, at each time point i, Θi encodes the
conditional independences between the observed variables, while
Li provides the summary of marginalisation over latent variables
on the observed ones.
Consider Equation (3) at a specific time i. Here we want to im-
pose continuity between the structure and the hidden variables
contribution in time, therefore we enforce the difference between
consecutive graphs to abide certain constraints by adding two pe-
nalisation terms. Our LTGL model takes the following form:

X ⊤

minimize
(Θ, L)
Li ≽0

T
(cid:213)

(cid:20)

i =1

+ β

T −1
(cid:213)

i =1

− ℓ(Si, Θi − Li ) + α ∥Θi ∥od, 1 + τ ∥Li ∥∗

(cid:21)

Ψ(Θi +1 − Θi ) + η

Φ(Li +1 − Li ),

T −1
(cid:213)

i =1

where Ψ and Φ are both penalty functions that force the structure of
the network to change over time according to a certain behaviour,
by acting on Θ and L, respectively. Temporal consistency of both
the structure of the network and latent factors contribution is guar-
anteed by the use of such penalty functions, which benefits the
network inference in particular in presence of few available ob-
servations of the system. Possible choices for Ψ and Φ are listed
in Section 2. Their choice is arbitrary and it is based on the prior
knowledge on the respective components evolution in the system.
Also, note that Ψ and Φ are independent, which allows LTGL to
model a wide range of dynamical behaviours of complex systems.

4 MINIMISATION METHOD
Problem (5) is convex, provided that the penalty functions Ψ and
Φ are convex, and it is coercive because of the regularisers. Thus,

Problem (5) admits solutions. Nonetheless, its optimisation is chal-
lenging in practice due to the high number of unknown matrices
involved (2T , for a total of 2T d (d +1)
unknowns of the problem). A
2
suitable method for the minimisation is ADMM [7]. It allows to
decouple the variables obtaining a separable minimisation problem
which can be efficiently solved in parallel. The sub-problems ex-
ploit proximal operators which are (mostly) solvable in closed-form,
leading to a simple iterative algorithm.
In order to decouple the involved matrices, we define three dual
variables R, Z = (Z1, Z2) and W = (W1,W2) and two projections:

P1 : (Rd ×d )T → (Rd ×d )T −1

P2 : (Rd ×d )T → (Rd ×d )T −1

A (cid:55)→ (A1, . . . , AT −1)

A (cid:55)→ (A2, . . . , AT )

Problem (5) becomes:

minimize
(Θ, L, R, Z ,W )
Li ≽0

T
(cid:213)

(cid:20)

i =1

+ β

T −1
(cid:213)

i =1

− ℓ(Si, Ri ) + α ∥Θi ∥od, 1 + τ ∥Li ∥∗

(cid:21)

Ψ(Z2,i − Z1,i ) + η

Φ(W2,i − W1, i )

(6)

T −1
(cid:213)

i =1

s.t. R = Θ − L, Z1 = P1Θ, Z2 = P2Θ, W1 = P1L, W2 = P2L.

The corresponding augmented Lagrangian is as follows:
Lρ (Θ, L, R, Z, W, U)

− ℓ(Si, Ri ) + α ∥Θi ∥od, 1 + τ ∥Li ∥∗ + I(L ≽ 0)

(cid:21)






+β

Ψ(Z2,i − Z1,i ) + η

Φ(W2,i − W1,i )

T −1
(cid:213)

i =1

∥Ri −Θi +Li +U0,i ∥2 − ∥U0,i ∥2

(cid:21)

∥Θi −Z1,i +U1,i ∥2 − ∥U1,i ∥2 + ∥Θi +1 −Z2,i +U2,i ∥2 − ∥U2,i ∥2

∥Li −W1,i +U3,i ∥2 − ∥U3,i ∥2 + ∥Li +1 −W2,i +U4,i ∥2 − ∥U4,i ∥2

T
(cid:213)

(cid:20)

=

i =1
T −1
(cid:213)

i =1
T
(cid:213)

(cid:20)

i =1
T −1
(cid:213)

(cid:20)

i =1
T −1
(cid:213)

(cid:20)

i =1

+ ρ
2

+ ρ
2

+ ρ
2

(5)

where U = (U0, U1, U2, U3, U4) are the scaled dual variables.
The ADMM algorithm for Problem (6) writes down as follows:

for k = 1, . . .

Rk +1 = argmin

Lρ (Θk, Lk, R, Zk, Wk, Uk )

Θk +1 = argmin

Lρ (Θ, Lk, Rk +1, Zk, Wk, Uk )

R

Θ

Lk +1 = argmin

Lρ (Θk +1, L, Rk +1, Zk, Wk, Uk )

Zk +1 =

Wk +1 =

Uk +1 =

(cid:21)

(cid:21)

+

L
(cid:20) Z k +1
1
Z k +1
2
(cid:20) W k +1
1
W k +1
2
U k
0
U k
1
U k
2
U k
3
U k
4























= argmin
Z

= argmin
W

Lρ (Θk +1, Lk +1, Rk +1, Z, Wk, Uk )

Lρ (Θk +1, Lk +1, Rk +1, Zk +1, W, Uk )

Rk +1 − Θk +1 + Lk +1
P1Θk +1 − Z k +1
P2Θk +1 − Z k +1
P1Lk +1 − W k +1
P2Lk +1 − W k +1

2

1

1

2












.












(cid:21)

(cid:21)

(7)

(8)

4.1 R step
The minimisation problem involving the matrix R in (8) can be
split into parallel updates, since Lρ (Θ, L, R, Z, W, U) is separable
in the variables (R1, . . . , RT ). Therefore, each Ri at iteration k + 1
is given by:
Rk +1
i

∥R − Θk + Lk + U k

0, i ∥2

= argmin
R

= argmin
R

tr(Si R) − log det(R) + ρ
2
tr(Si R) − log det(R) + ρ
2

= argmin
R

tr(Si R) − log det(R) + ρ
2

(cid:13)
(cid:13)
(cid:13)

2

F

i

(cid:13)
(cid:13)R − Ak
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

R −

Ak
i

+ Ak ⊤
i
2

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F

− U k

= Θk
i

− Lk
i

with Ak
0,i . Note that the last equality in (9) follows
i
from the symmetry of R — which also guarantees the log det to be
well-defined. Equation (9) can be explicitly solved. Indeed, Fermat’s
rule yields:

Si − ρ

Ak
i

+ Ak ⊤
i
2

= R−1 − ρR .

Then the solution to Equation (10) is [12, 17, 32]:
(cid:19)

(cid:18)

(cid:113)

V k

−Ek +

(Ek )2 + 4ρI

V k ⊤

Rk +1
i

= 1
2ρ

where V k EkV k ⊤ is the eigenvalue decomposition of Si − ρ

Ak
i

+Ak ⊤
i
2

.

4.2 Θ step
Likewise the R step, the update of Θ in (8) can be done in a parallel
fashion, as follows:

Θk +1
i

= argmin
Θ

α ∥Θ ∥od, 1 + ρ
2

(cid:20) (cid:13)
(cid:13)Rk
(cid:13)

i − Θ + Lk
i

+ U k
0, i

(cid:13)
(cid:13)
(cid:13)

2

F

+ δ iT

(cid:13)
(cid:13)Θ − Z k
(cid:13)

1,i

+ U k
1, i

+ δ i 1

(cid:13)
(cid:13)
(cid:13)

2

F

2,i −1

+ U k

2,i −1

= argmin
Θ

α ∥Θ ∥od, 1 + (1 + δ iT + δ i 1)

(cid:13)
(cid:13)Θ − Z k
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)Θ − Bk
(cid:13)

i

2

F

ρ
2

where δ i j = 1 − δi j , with δi j Kronecker delta and
1,i ) + δ i 1(Z k

+ δ iT (Z k

+ U k
0,i

+ Rk
i

Lk
i

Bk
i

=

1,i − U k
1 + δ iT + δ i 1

2,i −1 − U k

2,i −1)

.

Problem (11) is solved as:

Θk +1
i

(Bk

= proxζ ∥·∥od, 1
, and Sζ (·) element-wise off-diagonal soft-

i ) = Sζ (Bk
i )

with ζ =

α
ρ(1+δ iT +δ i 1)
thresholding function.

(9)

(10)

(cid:21)

(cid:13)
(cid:13)
(cid:13)

2

F

(11)

4.3 L step
The parallel update of L in (8) can be written as:
τ tr(L) + I(L ≽ 0) + ρ
2

i − Θk +1

= argmin
L

Lk +1
i

i

(cid:20) (cid:13)
(cid:13)Rk +1
(cid:13)
(cid:13)
(cid:13)L − W k
(cid:13)

+ δ iT

(cid:13)
(cid:13)L − W k
(cid:13)

1,i

+ U k
3,i

+ δ i 1

(cid:13)
(cid:13)
(cid:13)

2

F

2,i −1

+ U k

4, i −1

τ tr(L) + I(L ≽ 0) + (1 + δ iT + δ i 1)

= argmin
L

= argmin
L

τ tr(L) + I(L ≽ 0) + (1 + δ iT + δ i 1)

+ L + U k
i, 0

2

F

(cid:13)
(cid:13)
(cid:13)
(cid:21)

2

(cid:13)
(cid:13)
(cid:13)

F
(cid:13)
(cid:13)
(cid:13)

2

F

ρ
2

ρ
2

i

(cid:13)
(cid:13)L − C k
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

L −

C k
i

+ C k ⊤
i
2

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(12)

where

C k
i

=

i − Rk +1
Θk +1

i − U k
0,i

+ δ iT (W1,i − U3,i ) + δ i 1(W2,i −1 − U4,i −1)

.

1 + δ iT + δ i 1

Note that the last equality in (12) follows from the symmetry of L.
Then, the solution to Problem (12) is [24]:
= V k ˜EV k ⊤

Lk +1
i
where V k EkV k⊤ is the eigenvalue decomposition of Ck

i , and

(cid:32)

˜Ej j = max

Ek
j j −

τ
ρ(1 + δ iT + δ i1)

(cid:33)

, 0

.

4.4 Z and W step
The dual variables Z and W enforce the network to behave in time
consistently with the choice of Ψ and Φ, respectively. Z is the dual
variable of Θ while W is the dual variable of L. For the sake of
brevity, we show only the steps regarding the update of Z — the
update of W is analogous. The dual variable Z is defined as (Z1, Z2).
Such matrices are not separable in Equation (6), thus they must be
jointly updated. The update of Z in (8) can be rewritten as follows:

(cid:35)

(cid:34)

Z k +1
1,i
Z k +1
2,i

= argmin
Z1, Z2

Let ˆΨ

(cid:21)

(cid:20) Z1
Z2

an unique update [17]:

β Ψ(Z2 − Z1) + ρ
2
+ ρ
2

∥Θk

i − Z1 + X k

1,i ∥2

∥Θk

i +1 − Z2 + X k

2,i ∥2 .

(13)

= Ψ(Z2 − Z1). Then, Problem (13) can be solved with

(cid:21)

(cid:20) Z k +1
1,i
Z k +1
2,i

= prox β
ρ

ˆΨ(·)

(cid:18) (cid:20) Θk
i
Θk
i +1

+ U k
1,i
+ U k
2,i

(cid:21) (cid:19)

.

The same holds for the W step. Hence, the proximal operator for
the update of W1,i and W2,i becomes:
(cid:20) W k +1
1,i
W k +1
2,i

(cid:18) (cid:20) Lk
i
Lk
i +1

+ U k
i, 3
+ U k
i, 4

= prox η
ρ

ˆΦ(·)

(cid:21) (cid:19)

(cid:21)

.

For the particular derivation of different proximal operators,
see [17].

4.5 Termination criterion
According to [7], the algorithm is said to converge if the primal
and dual residuals are sufficiently small, i.e. if ∥r k ∥2
2 ≤ ϵ pri and
∥sk ∥2
2 ≤ ϵ dual. At each iteration k these values are computed as
follows:

∥r k ∥2
2

∥s k ∥2
2

= ∥R k − Θk + Lk ∥2
1 ∥2
+ ∥P1Θk − Z k
F
F
1 ∥2
2 ∥2
+ ∥P2Lk − W k
+ ∥P1Lk − W k
F
F
= ρ (cid:0) ∥R k − R k −1 ∥2
1 − Z k −1
∥2
+ ∥Z k
+ ∥Z k
1
F
F
(cid:1)
2 − W k −1
1 − W k −1
∥2
∥2
2
1
F
F
ϵ pri = c + ϵ rel max(cid:0)D1, D2
ϵ dual = c + ϵ relρ (cid:0) ∥U k

+ ∥W k
(cid:1)

+ ∥W k

+ ∥P2Θk − Z k

1 ∥2
F

2 − Z k −1
2

∥2
F

0 ∥2
F

+ ∥U k

+ ∥U k

1 ∥2
F
where c = ϵ absd(5T − 4)1/2, ϵ abs and ϵ rel are arbitrary tolerance
+ ∥W k
+ ∥Z k
+ ∥W k
∥2
∥2
∥2
∥2
parameters, Dk
1
1
2
2
F
F
F
F
+ ∥P1Θk ∥2
= ∥Θk −Lk ∥2
+ ∥P1Lk ∥2
+ ∥P2Lk ∥2
and Dk
F .
2
F
F
F

+ ∥Z k
1
+ ∥P2Θk ∥2
F

= ∥Rk ∥2
F

+ ∥U k

+ ∥U k

4 ∥2
F

2 ∥2
F

3 ∥2
F

(cid:1)

4.6 Implementation
The minimisation algorithm is available as a Python framework1,
fully compatible with the scikit-learn library of machine learning
algorithms, providing a straightforward and intuitive interface. The
implementation relies on low-level high-performance libraries for
numerical computations and it exploits closed-form solutions for
proximal operators, leading to a fast and scalable minimisation
algorithm even with an increasing number of unknowns.

5 EXPERIMENTS
We performed experiments on synthetic data assessing the perfor-
mance of the method in terms of structure recovery and measure of
latent variables influence. The performance of LTGL was evaluated
with respect to the ground truth and to state-of-the-art methods
for graphical inference. In particular, we assessed two aspects of
such methods, that are modelling performance and scalability, in
separated experiments. Modelling performance was estimated by
comparing the inferred graphical model to the true network un-
derlying the data set. During the scalability experiment, instead,
we assessed the computational time for convergence needed for
increasing problem complexity.

5.1 Modelling performance
We evaluated LTGL modelling performance on two synthetic data
sets. The ground truth sets of matrices Θ = (Θ1, . . . , ΘT ) and
L = (L1, . . . , LT ) were obtained by perturbing initial matrices Θ1
and L1, according to a specific behaviour for T − 1 times, with the
guarantee that Θi − Li ≻ 0 and Li ≽ 0 for i = 1, . . . ,T . The initial
matrices were generated according to [33], following the form (2).
Θ1 and L1 correspond to ΘO and ΘO H Θ−1
H ΘH O , respectively, with
ΘH identity matrix and ΘH O = Θ⊤
O H . Note that, since ΘH has full
rank, the number of latent variables is H . In particular, for d ob-
served variables, n samples and T timestamps, we generated a data
set X ∈ (Rn×d )T sampled from T multivariate normal distributions
Xi ∼ Ni (0, Σi ), for i = 1, . . . ,T , and Σ−1
i
ℓ2
2 perturbation (p2). The first data set was generated by
perturbing the initial matrices with a random matrix of small ℓ2
2
norm. This perturbation assumes the differences between two
consecutive matrices to be small and bounded over time, i.e.,
∥Θi − Θi−1 ∥F ≤ ϵ for i = 2, . . . ,T . The bound ϵ on the norm
is chosen a priori.
The update of Li is done maintaining consistency with the theo-
retical model where Li = ΘO H,i Θ⊤
O H,i . Therefore, the update is
obtained by adding a random matrix with a small norm to ΘO H,i−1.
In this way, the rank of Li remains the same as the number of latent
variables and constant over time. Data were generated in R100 with
10 time stamps, conditioned on 20 latent variables. For each time
stamp, we drew 100 samples from the distribution. For this reason,
in this setting, the contribution of latent factors is predominant
with respect to the network evolution in time.

= Θi − Li .

5.1.1

5.1.2

ℓ1 perturbation (p1). A second data set was generated
according to a different perturbation model. Here, the precision
matrix was updated by randomly choosing an edge and swapping

1LTGL is open-source. The
https://github.com/fdtomasi/regain.

code

is

available under BSD-3-Clause

at

Table 1. Performance in terms of F1 score, accuracy (ACC), mean
rank error (MRE) and mean squared error (MSE) of LTGL with re-
spect to TVGL, LVGLASSO and GL. LTGL and TVGL are employed
with both ℓ2
2 and ℓ1 penalties, to show how the prior on the evolu-
tion of the network affects the outcome.

perturbation method

score

F1

ACC MRE MSE

ℓ2
2 (p2)

ℓ1 (p1)

LTGL (ℓ2
0.926
2)
LTGL (ℓ1)
0.898
TVGL (ℓ2
0.791
2)
TVGL (ℓ1)
0.791
LVGLASSO 0.815
GL
0.745
LTGL (ℓ2
2)
0.842
LTGL (ℓ1)
0.880
TVGL (ℓ2
2)
0.742
TVGL (ℓ1)
0.817
LVGLASSO 0.752
0.748
GL

0.994
0.993
0.980
0.980
0.988
0.974

0.974
0.981
0.950
0.968
0.964
0.951

0.70
0.70
-
-
2.80
-

0.29
0.28
-
-
0.74
-

0.007
0.007
0.003
0.003
0.007
0.004

0.013
0.013
0.009
0.009
0.013
0.007

Figure 2. Distribution of inferred ranks over time. For each method
that considers latent variables, we show the frequency of finding
a specific rank during the network inference. The vertical line in-
dicates the ground truth rank, around which all detected ranks lie.
Note that, in (p2), Li ∈ R100×100, therefore the range of possible ranks
is [0, 100]. For (p2), Li ∈ R50×50, hence the range is [0, 50].

its state, i.e., by removing or adding a connection between two
variables. This allows for a ℓ1-norm evolutionary pattern of the
network. Data were generated in R50 with 100 time stamps, con-
ditioned on 5 latent variables. For each time stamp, we drew 100
samples from the distribution. In this setting, the time consistency
affects the network more than the latent factor contribution.

5.1.3

Scores. We evaluated LTGL performance using different
scores measuring the divergence of the results from the ground
truth. In particular, the performance was evaluated in terms of F1
score, accuracy, mean rank error and mean squared error. We define
as true/false positive the number of correctly/incorrectly existing in-
ferred edges, true/false negative the number of correctly/incorrectly
missing inferred edges [18]. The scores are computed as follows.
• F1 score: indicates the quality of structure inference, as the har-
monic mean of precision and recall.
• Accuracy (ACC): evaluates the number of true existing and missing
connections in the network correctly inferred with respect to the
total number of connections.

Figure 3. Scalability comparison for LTGL in relation to other ADMM-based methods. The compared methods are initialised in the same
manner, i.e., with all variable interactions (not self-interacting) set to zero. For LVGLASSO and TVGL, we used their relative original imple-
mentations. Also, we ignore the computational time required for hyperparameters selection. LTGL outperforms the other methods for each
increasing time and dimensionality of the problem.

• Mean rank error (MRE): estimates the precision on the number of
inferred latent variables, based on the rank of the set of matrices ˜L
in relation to the ground truth. The MRE score is defined as:

MRE = 1
T

T
(cid:213)

i =1

(cid:12)rank(Li ) − rank( ˜Li )(cid:12)
(cid:12)
(cid:12).

A value close to 0 means that we are inferring the true number of
latent variables over time, while, viceversa, a high value indicates a
poor consideration of the contribution of the latent variables.
• Mean squared error (MSE): score how close is the inferred precision
matrix ˜Θ to the ground truth, in terms of the Frobenius norm:

MSE =

2
T d(d − 1)

T
(cid:213)

i =1

(cid:13)
(cid:13)
(cid:13)Θ

(u)
(u)
i − ˜Θ
i

(cid:13)
(cid:13)
(cid:13)F

,

where Θ(u) denotes the upper triangular part of Θ.

5.1.4 Discussion. Table 1 shows the performance of LTGL com-
pared to graphical lasso (GL) [15], latent variable graphical lasso
(LVGLASSO) [8, 24] and time-varying graphical lasso (TVGL) [17]
in terms of F1 score, accuracy, mean rank error (MRE) and mean
squared error (MSE), for both settings with ℓ2
2 (p2) and ℓ1 (p1) per-
turbation. Note that MRE is not available for all the methods since
neither GL or TVGL consider latent factors. LTGL and TVGL are
used with two temporal penalties according to the different per-
turbation models of data generation. In this way, we show how
the correct choice of the penalty for the problem at hand results in
a more accurate network estimation. In both (p2) and (p1), LTGL
outperforms the other methods for graphical modelling. In (p2), in
particular, LTGL correctly infers almost 99,5% of edges in all the dy-
namical network both with the ℓ2
2 and ℓ1 penalties. Nonetheless, the
use of ℓ2
2 penalty enhance the quality of the inference as expected
from the theoretical assumption made during data generation. The
choice of a proper penalty for the problem and the consideration
of time consistency is reflected also in a low MRE, which encom-
passes LVGLASSO ability in detecting latent factors (Figure 2). In
(p2), in fact, the number of latent variables with respect to both
observed variables and samples is high. Therefore, by exploiting
temporal consistency of the network, LTGL is able to improve the
latent factors estimation. Simultaneous consideration of time and
latent variable also positively influences the F1 score, i.e., structure
detection.
Above considerations also hold for the (p1) setting. Here, LTGL
achieves the best results in both F1 score and accuracy, while hav-
ing a low MRE. The adoption of ℓ1 penalty improves structure
estimation and latent factors detection, consistently with the data
generation model. Such settings were designed to show how the

prevalence of latent factors contribution or time consistency af-
fects the outcome of a network inference method. In (p2), where
the latent factors contribution is prevalent, network inference is
more precise when considering latent factors. In (p1), instead, the
number of time points is more relevant than the contribution of
latent factors, hence it is more effective to exploit time consistency
(both for latent and observed variables), evident from the results
of Table 1. LTGL benefits from both aspects, therefore leading to a
noticeable improvement of graphical modelling.

5.2 Scalability
Next, we performed a scalability analysis using LTGL with respect
to different ADMM-based solvers. We evaluated the performance
of our method in relation to LVGLASSO [24] and TVGL [17], both
implemented with closed-form solutions to ADMM subproblems.
In general, the complexity of the three compared solvers is the
same (up to a constant). The implementation of GL [15] was not
included in such experiment, since it is not based on ADMM but on
coordinate descent, and therefore it is not comparable to our method.
As in Section 5.1, we generated different data sets X ∈ (Rn×d )T
with different values of T and d. In particular, d ∈ [10, 400) and
T = {20, 50, 100}. We ran our experiments on a machine provided
with two CPUs (2.4 GHz, 8 cores each).
Figure 3 shows, for the three different time settings, the scalability
of the methods in terms of seconds per convergence considering
different number of unknowns of the problem (i.e., 2T d (d +1)
2 with d
observed variables and T times). In all settings, LTGL outperforms
LVGLASSO and TVGL in terms of seconds per convergence. In
particular, the computational time for convergence remains stable
disregarding the number of time points under consideration. We
emphasise that the most computationally expensive task performed
by our solver is represented by two eigenvalue decompositions,
with a complexity of O(d3), to solve both R and L steps (Section 4).

5.3 Model selection
The hyperparameters of the method have been selected by using a
cross-validation procedure. In particular, we used the Monte Carlo
Cross-Validation (MCCV) [27] that repeatedly splits the n samples
of the data set in two mutually exclusive sets. For each split, n · (1/ν )
samples are labelled as validation set and the remaining n · (1 − 1/ν )
as learning set. For each hyperparameter combination, the model
was trained on the learning set and the likelihood of the model
was estimated on the independent test set. Finally, we selected the
combination of hyperparameters based on the average maximum
likelihood of the model across multiple splits of the data set.

However, the number of possible combinations of LTGL hyperpa-
rameters can be arbitrarily large. In order to avoid the assessment
of a grid of models (which can be computationally expensive), we
used a Gaussian process-based Bayesian optimisation procedure to
choose the best combination of hyperparameters for each analysed
data set, based on the Expected Improvement (EI) strategy [31]. In
practice, assuming the dynamics of a real system to be unknown,
it is possible to select the most appropriate temporal penalty by
exploiting the same principles, i.e., via a model selection procedure
based on the likelihood of different temporal models.

6 APPLICATIONS TO REAL DATA
We applied LTGL to two real data sets, to show how the method
can be employed to infer useful insights on multivariate time-series
data. These data sets measure complex dynamical systems of dif-
ferent (biological and financial) nature, which are usually highly
dimensional and feature complicated interdependences between
variables. This fact makes them ideal candidates for an analysis
using graphical models.

6.1 Metabolomic Data
The physiology of Escherichia coli necessitates rapid changes of
its cellular and molecular network to adapt to environmental con-
ditions. E. coli is widely studied because of the efficiency in its
system response to perturbation. Following the analysis of [21], we
used LTGL on E. coli data to infer network modifications across
different time points evaluated before and after the application of
environmental condition perturbations. We analysed the behaviour
of metabolites, which have been shown to change consistently after
the perturbation. Samples underwent one of two types of stress,
namely cold and heat stress.
Perturbation response detection. We inferred the dynamical
network of E. coli metabolites using LTGL with a group lasso (ℓ2)
penalty on latent variable contribution and a Laplacian (ℓ2
2) penalty
on the observed network. In this way, we allow the latent variables
(which, in our model, could represent the stress or other factors)
to change their global influence at a specific time point, while re-
maining stable in all others. At the same time, by conditioning
the network on the latent variables, we allow the observed net-
work structure to change smoothly in time. Hence, we expect to
see a global shift of the network between the second and third
time points, that is when the perturbation has been introduced in
the system. Figure 4a shows the temporal deviation between time
points, both for Θ, L and the total observed system R = Θ − L.
Latent variables temporal deviation reaches a peak at time t2−3,
right after the application of the perturbation to the system. Instead,
the difference between consecutive Θs remains more stable. Con-
sistently, the difference between the observed networks Rs shows
a major change at the same time point. Hence we can distinguish
the underlying evolving structure of metabolites while detecting
the contribution of the latent variables which affect mostly the
total system. In accordance with [21], we observed a interaction
between isoleucine, threonine, phenylalanine and 2-aminobutyric
acid during the adaptation phase following the stress response (Fig-
ure 4b). Therefore, we can conclude that LTGL successfully inferred
a dynamical network which adjusts in response to perturbation, in
accordance with our prior knowledge about E. coli behaviour.

Figure 4. Structure change of E. coli metabolites subject to stress.
The perturbation happens between time t = 2 and t = 3 (ver-
tical dotted line). (a) Temporal deviation where each point repre-
sents the difference between the network at subsequent time points.
The highest deviation on the observed network R appears when the
stress was applied. This can be decomposed into two parts, the la-
tent factors L and the underlying structure of observed variables Θ.
(b) Structural changes of metabolites interactions before and after
the perturbation.

6.2 Stock market
Finance is another example of a complex dynamical system suitable
to be analysed through a graphical model. Stock prices, in particular,
are highly related to each other and subject to time and environ-
mental changes, i.e., events that modify the system behaviour but
are not directly related to companies share values [3]. Here, the
assumption is that each company, while being part of a global finan-
cial system, is directly dependent from only a subset of others. For
example, it is reasonable to expect that stock prices of a technology
company are not directly influenced by trend of companies on the
primary sector. The modelling power of LTGL allows to detect both
the evolution of relations between companies and environmental
changes happening at a particular point in time. In order to show
this, we analysed stock prices2 during the financial crisis of 2007-
2008. The experiment was designed to consider the latent influence
of the market drop on technology companies interactions.

Global market crisis detection. We used a group lasso (ℓ2)
penalty to detect global shifts of the network. Figure 5 shows two
major changes in both components of the network (latent and ob-
served), in correspondence of late 2007 and late 2008. In particular,
during October 2008 a global crisis of the market occurred, and this
effect is especially evident for the shift of latent variables. Also, the
observed network changes in correspondence of the latent variables
shift or immediately after, caused by the effect of the crisis on the
stock market. The latent factors influence explains how the change
of the network was due to external factors that globally affected the
market, and not to normal evolution of companies relationships.
We further investigated on the causes for the first shift. Indeed,
we found that in late 2007 it happened a drop of a big American
company that was later pointed out as the beginning of the global
crisis of the following year.

2Data are freely available on https://quantquote.com/historical-stock-data.

[7] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. 2010. Distributed Optimiza-
tion and Statistical Learning via the Alternating Direction Method of Multipliers.
Foundations and Trends® in Machine Learning 3, 1 (2010).

[8] V. Chandrasekaran, P. A. Parrilo, and A. S. Willsky. 2010. Latent variable graph-
ical model selection via convex optimization. In Communication, Control, and
Computing (Allerton), 2010 48th Annual Allerton Conference on. IEEE, 1610–1613.
[9] V. Chandrasekaran, S. Sanghavi, P. A. Parrilo, and A. S. Willsky. 2011. Rank-
sparsity incoherence for matrix decomposition. SIAM Journal on Optimization
21, 2 (2011), 572–596.

[10] M. J. Choi, V. Chandrasekaran, and A. S. Willsky. 2010. Gaussian multiresolution
models: Exploiting sparse Markov and covariance structure. IEEE Transactions
on Signal Processing 58, 3 (2010), 1012–1024.

[11] M. J. Choi, V. Y.F. Tan, A. Anandkumar, and A. S. Willsky. 2011. Learning latent

tree graphical models. JMLRh 12, May (2011), 1771–1812.

[12] P. Danaher, P. Wang, and D. M. Witten. 2014. The joint graphical lasso for inverse
covariance estimation across multiple classes. Journal of the Royal Statistical
Society. Series B: Statistical Methodology 76, 2 (2014), 373–397. arXiv:1111.0324
[13] C. Ding, X. He, and H. D. Simon. 2005. On the equivalence of nonnegative matrix
factorization and spectral clustering. In Proceedings of the 2005 SIAM International
Conference on Data Mining. SIAM, 606–610.

[14] A. Farasat, A. Nikolaev, S. N. Srihari, and R. H. Blair. 2015. Probabilistic graphical

models in modern social network analysis. SNAM 5, 1 (2015), 62.

[15] J. Friedman, T. Hastie, and R. Tibshirani. 2008. Sparse inverse covariance estima-

tion with the graphical lasso. Biostatistics 9, 3 (2008), 432–441.

[16] A. J. Gibberd and S. Roy. 2017. Multiple Changepoint Estimation in High-
Dimensional Gaussian Graphical Models. arXiv preprint arXiv:1712.05786 (2017).
[17] D. Hallac, Y. Park, S. Boyd, and J. Leskovec. 2017. Network Inference via the Time-
Varying Graphical Lasso. In Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD ’17). ACM, 205–213.
[18] M. Hecker, S. Lambeck, S. Toepfer, E. Van Someren, and R. Guthke. 2009. Gene
regulatory network inference: Data integration in dynamic models: A review.
Biosystems 96, 1 (2009), 86–103.

[19] L. Huang, L. Liao, and C. H. Wu. 2016. Inference of protein-protein interaction
networks from multiple heterogeneous data. EURASIP Journal on Bioinformatics
and Systems Biology 2016, 1 (2016), 1–9.

[20] A. Jalali and S. Sanghavi. 2011. Learning the dependence graph of time series

with latent factors. arXiv preprint arXiv:1106.1887 (2011).

[21] S. Jozefczuk, S. Klie, G. Catchpole, J. Szymanski, A. Cuadros-Inostroza, D. Stein-
hauser, J. Selbig, and L. Willmitzer. 2010. Metabolomic and transcriptomic stress
response of Escherichia coli. Molecular systems biology 6, 1 (2010), 364.

[22] S. L. Lauritzen. 1996. Graphical models. Vol. 17. Clarendon Press.
[23] H. Liu, F. Han, and C. Zhang. 2012. Transelliptical graphical models. In NIPS.

800–808.

[24] S. Ma, L. Xue, and H. Zou. 2013. Alternating Direction Methods for Latent
Variable Gaussian Graphical Model Selection. Neural Computation 25, 8 (aug
2013), 2172–2198.

[25] N. Meinshausen and P. Bühlmann. 2006. High-dimensional graphs and variable

selection with the lasso. The annals of statistics (2006), 1436–1462.

[26] Z. Meng, B. Eriksson, and A. Hero. 2014. Learning latent variable Gaussian

graphical models. In Proceedings of the 31st ICML. 1269–1277.

[27] A. M. Molinaro, R. Simon, and R. M. Pfeiffer. 2005. Prediction error estimation: a
comparison of resampling methods. Bioinformatics 21, 15 (2005), 3301–3307.
[28] E. J. Molinelli, A. Korkut, W. Wang, M. L. Miller, N. P. Gauthier, X. Jing, P. Kaushik,
Q. He, G. Mills, D. B. Solit, et al. 2013. Perturbation biology: inferring signaling
networks in cellular systems. PLoS computational biology 9, 12 (2013), e1003290.
[29] P. Orchard, F. Agakov, and A. Storkey. 2013. Bayesian inference in sparse Gaussian

graphical models. arXiv preprint arXiv:1309.7311 (2013).

[30] P. Ravikumar, M.J. Wainwright, G. Raskutti, B. Yu, et al. 2011. High-dimensional
covariance estimation by minimizing ℓ1s-penalized log-determinant divergence.
Electronic Journal of Statistics 5 (2011), 935–980.

[31] J. Snoek, H. Larochelle, and R. P. Adams. 2012. Practical Bayesian optimization

of machine learning algorithms. In NIPS. 2951–2959.

[32] D. M. Witten and R. Tibshirani. 2009. Covariance-regularized regression and
classification for high dimensional problems. Journal of the Royal Statistical
Society. Series B: Statistical Methodology 71, 3 (jun 2009), 615–636.

[33] M. Yuan. 2012. Discussion: Latent variable graphical model selection via convex

optimization. Ann. Statist. 40, 4 (08 2012), 1968–1972.

graphical model. Biometrika (2007), 19–35.

Figure 5. Temporal deviation for stock market data. We observed
two peaks, in correspondence of late 2007 and late 2008, when the
financial crisis happened.

7 CONCLUSIONS AND FUTURE WORK
In this work, we developed a novel method for graphical modelling
of multivariate time-series. The model considers simultaneously
the contribution of latent factors and time consistency in evolving
systems. Indeed, our work is an attempt to generalise both latent
variable and dynamical network inference. To this aim, we impose
prior knowledge on the problem through penalty terms that force
precision and latent matrices to be consistent in time. The choice
of proper penalty terms maintains the convexity of the minimised
functional and, along with the coercivity given by the regularisers,
it guarantees global convergence of the proposed minimisation
algorithm. Our experiments demonstrate the ability of LTGL in
the graphical modelling of synthetic and real-world data, where
the possibility to decompose the total network into two separated
components allows for a better understanding of the underlying
phenomenon.
We emphasise that our framework is modular in the choice of the
penalties, allowing for precise modelling of different and complex
behaviours of the system. This allows for a straightforward inclu-
sion of additional penalty terms, based on the prior knowledge on
the problem at hand. Possible extensions may involve alternative
evolutionary models for different complex systems, e.g. forcing sub-
groups of variables to behave consistently in time [6]. These could
lead to interesting results in time-series clustering and pattern dis-
covery. Further investigations may also head to the inference of the
exact contribution of latent factors starting from the L matrices we
are estimating, possibly using matrix factorisation methods [13].
Such developments may increase the expression power of the
method, leading to advances in data mining and to potential appli-
cations in diverse science fields.

Acknowledgments. The authors thank the anonymous reviewers
for their valuable comments.

REFERENCES
[1] R. Albert. 2007. Network inference, analysis, and modeling in systems biology.

The Plant cell 19, 11 (nov 2007), 3327–38.

bayesian networks with latent variables. In ICML. 249–257.

[3] J. Bai and S. Ng. 2006. Evaluating latent and observed factors in macroeconomics

and finance. Journal of Econometrics 131, 1-2 (2006), 507–537.

[4] E. Bianco-Martinez, N. Rubido, Ch. G. Antonopoulos, and M.S. Baptista. 2016.
Successful network inference from time-series data using mutual information
rate. Chaos: An Interdisciplinary Journal of Nonlinear Science 26, 4 (2016), 043102.
[5] J. Bien and R. J. Tibshirani. 2011. Sparse estimation of a covariance matrix.

Biometrika 98, 4 (dec 2011), 807–820.

[6] A. Bolstad, B. D. Van Veen, and R. Nowak. 2011. Causal network inference via
group sparse regularization. IEEE transactions on signal processing 59, 6 (2011).

[2] A. Anandkumar, D. Hsu, A. Javanmard, and S. Kakade. 2013. Learning linear

[34] M. Yuan and Y. Lin. 2007. Model selection and estimation in the Gaussian

8
1
0
2
 
g
u
A
 
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
7
8
9
3
0
.
2
0
8
1
:
v
i
X
r
a

Latent Variable Time-varying Network Inference

Federico Tomasi∗
Università degli Studi di Genova
16146 Genova, Italy
federico.tomasi@dibris.unige.it

Saverio Salzo
Istituto Italiano di Tecnologia
16163 Genova, Italy
saverio.salzo@iit.it

Veronica Tozzo∗
Università degli Studi di Genova
16146 Genova, Italy
veronica.tozzo@dibris.unige.it

Alessandro Verri
Università degli Studi di Genova
16146 Genova, Italy
alessandro.verri@unige.it

ABSTRACT
In many applications of finance, biology and sociology, complex
systems involve entities interacting with each other. These pro-
cesses have the peculiarity of evolving over time and of comprising
latent factors, which influence the system without being explicitly
measured. In this work we present latent variable time-varying
graphical lasso (LTGL), a method for multivariate time-series graph-
ical modelling that considers the influence of hidden or unmea-
surable factors. The estimation of the contribution of the latent
factors is embedded in the model which produces both sparse and
low-rank components for each time point. In particular, the first
component represents the connectivity structure of observable vari-
ables of the system, while the second represents the influence of
hidden factors, assumed to be few with respect to the observed
variables. Our model includes temporal consistency on both com-
ponents, providing an accurate evolutionary pattern of the system.
We derive a tractable optimisation algorithm based on alternating
direction method of multipliers, and develop a scalable and effi-
cient implementation which exploits proximity operators in closed
form. LTGL is extensively validated on synthetic data, achieving
optimal performance in terms of accuracy, structure learning and
scalability with respect to ground truth and state-of-the-art meth-
ods for graphical inference. We conclude with the application of
LTGL to real case studies, from biology and finance, to illustrate
how our method can be successfully employed to gain insights on
multivariate time-series data.

KEYWORDS
network inference; graphical models; latent variables; time-series;
convex optimization

ACM Reference Format:
Federico Tomasi, Veronica Tozzo, Saverio Salzo, and Alessandro Verri. 2018.
Latent Variable Time-varying Network Inference. In KDD ’18: The 24th ACM

∗These authors equally contributed to the paper.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
KDD ’18, August 19–23, 2018, London, United Kingdom
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5552-0/18/08. . . $15.00
https://doi.org/10.1145/3219819.3220121

SIGKDD International Conference on Knowledge Discovery & Data Mining,
August 19–23, 2018, London, United Kingdom. ACM, New York, NY, USA,
9 pages. https://doi.org/10.1145/3219819.3220121

1 INTRODUCTION
The problem of understanding complex systems arises in many
diverse contexts, such as financial markets [23, 29], social networks
[14] and biology [18, 19]. In such contexts, the goal is to analyse the
system in order to retrieve information on how the components be-
have. This requires accurate and interpretable mathematical models
whose parameters, in practice, need to be estimated from observa-
tions.
Mathematically, a system can be modelled as a network of interac-
tions (edges) between its entities (nodes). However, the underlying
structure of the variables within the system is usually not known a
priori. Nevertheless, observations of the system (i.e., data) incorpo-
rate information on the interactions between variables, since they
provide measurements of such variables acting in the system.
The problem of inferring a network of variable interactions from
data is known as network inference or graphical model selection
[15, 22]. During the last years, the graphical modelling problem
has received much attention, particularly for the availability of
an always increasing number of samples that are required for a
reliable network inference. Nonetheless, structure estimation of
complex systems remains challenging for many reasons. In this
work, we want to tackle two aspects: (i) the presence of global
hidden (or latent) factors, and (ii) the dynamic of systems that
evolve in time. We argue that the inference of a dynamical network
encoding a complex system requires a specific attention to both
aspects to result in a more realistic representation. In particular, a
system may be affected by (latent) factors not encoded in the model.
Such factors, acting in the system, influence how the observable
entities behave and, hence, how they are connected with each other
[10]. The consideration of hidden and unmeasured variables during
the inference process emerges as crucial to avoid misrepresenting
real-world data [26].
At the same time, a complex system depends on a temporal com-
ponent, which drives variable interactions to evolve consistently
during its extent. This means that the structure can either change
or remain stable according to the nature of the system itself. Hence,
the understanding of a complex system is bound to the observation
of its evolution. This is particularly evident in some applications,
such as biology, where the interest could be to understand the
response of the system to perturbation [28].

Related work. Latent variable models have been widely studied
in literature, and shown to outperform graphical models that only
consider observable variables [8, 11, 33]. At the same time, a set of
methods were designed to study the temporal component through
the inference of a dynamical network that incorporates prior knowl-
edge on the behaviour of the system [4, 17]. Time-series with latent
variables are considered to obtain a single graph which represents
the global system [2, 20]. However, to the best of our knowledge,
state-of-the-art methods for regularised network inference do not
consider simultaneously time and latent variables in the inference
of multiple connected networks.

Contribution. In this work we propose latent variable time-varying
graphical lasso (LTGL), a model for dynamical network inference
where the structure is influenced by latent factors. This can be
seen as an attempt to generalise both dynamical and latent variable
network inference under a single unified framework. In partic-
ular, starting from a set of observations of a system at different
time points, LTGL infers an interaction network of the observed
variables under the influence of latent factors, while taking in con-
sideration the temporal evolution of the system. The empirical
interaction network is decomposed into the true underlying struc-
ture of the network and the contribution of latent factors, under
the assumption that both observable variables and latent factors
interdependence follow a temporal non-random behaviour. For
this reason, the model allows to include prior knowledge on the
evolutionary pattern of the system. The imposition of such prior
knowledge benefits inference and subsequent analysis of the net-
work, accentuating precise dynamical patterns. This is particularly
important when the number of samples is low compared to the
number of observed and latent variables in the system. In fact, the
inference of the network at particular time points exploits the de-
pendence between consecutive temporal states. Such advantage
is achieved by a simultaneous inference of all the dynamical sys-
tem, that, mathematically, translates into imposing constraints on
the network behaviour. In this work we provide a set of possible
constraints that can be applied independently on both observed
and latent components, allowing for a wide range of evolutionary
patterns.
Figure 1 provides an example of the theoretical model assumed by
LTGL. Here, observed and latent variables (xi and zi ) are connected
in a slightly different way at each time. Note that the observations
of the system only involve variables xi , while the hidden factors zi
influence the system without being actually observed. Hence, when
analysing samples which are regulated from a dynamical network
with hidden factors, it is infeasible to precisely infer the identity of
latent variables, but only an estimation of their number and their
effect on the global system can be obtained.
Starting from the theoretical model we derived a minimisation
algorithm based on the alternating direction method of multipli-
ers (ADMM) [7]. The algorithm is divided into independent steps
using proximal operators, which can be solved by closed-form so-
lutions favouring a fast and scalable computation [12, 17, 24]. We
also provide the related implementation in a Python framework,
based on the use of highly optimised low-level libraries for nu-
merical computation. Experiments on synthetic data show LTGL
to achieve optimal performance in relation to ground truth and

Figure 1. A dynamical network with latent factors zi and observed
variables xi . At each time ti , all of the connections between la-
lines) and connections among ob-
tent and observed variables (
served variables (
lines) may change according to a specific tem-
poral behaviour.

to state-of-the-art methods for graphical modelling, in terms of
accuracy, structure learning and scalability. Moreover, we show the
computational efficiency of LTGL while increasing the number of
unknowns of the problem and the model complexity. We conclude
with the application of LTGL to real-world data sets to illustrate
how our method can be successfully employed to gain insights on
multivariate time-series data. In particular, we used biological and
financial data sets, to show the use of LTGL in different contexts.
In the first case, we analysed Escherichia coli response to pertur-
bation, correctly identified by our method. In the latter case, we
investigated on a financial data set, to show how the contribution
of latent factors is relevant for the understanding of the behaviour
of the system.

Outline. The paper is organised as follows. Section 2 includes a
background on the reference frameworks for static and dynamical
network inference. Section 3 contains the theoretical formulation
of the problem and the proposed method. Section 4 describes in
details the optimisation algorithm for the minimisation of the func-
tional. Section 5 and Section 6 illustrate the use of our method on
synthetic and real data, respectively. Finally, Section 7 concludes
with a discussion and future directions.

2 PRELIMINARIES
Given a graph G = (V, E), where V = {x1, . . . , xd } is a finite set
of vertices, and E ⊆ V × V is a set of edges, a graphical model
is a multivariate probability distribution on x1, . . . , xd variables
where the conditional independence between two variables xi and
xj given all the others is encoded in G [22]. The two variables xi and
xj are conditionally independent given the others if (xi , xj ) (cid:60) E
and (xj , xi ) (cid:60) E. In what follows, we consider only undirected
Gaussian graphical models (GGMs), where (i) there is no distinc-
tion between an edge (xi , xj ) ∈ E and (xj , xi ), and (ii) variables are
jointly distributed according to a multivariate Gaussian distribution
N (µ, Σ). Without loss of generality, we assume µ to be zero, thus
the distribution depends only on the covariance matrix Σ [11]. The
inverse covariance matrix Θ = Σ−1, called precision matrix, encodes
the conditional independence between pairs of variables. In partic-
ular, the precision matrix has a zero entry in the position i, j only
if (xi , xj ) (cid:60) E [22]. Hence, we can interpret the precision matrix
as the weighted adjacency matrix of G, encoding the dependence
between variables.

Network inference. Consider a series of samples drawn from a
multivariate Gaussian distribution X ∼ N (0, Σ), X ∈ Rn×d . Net-
work inference aims at recovering the graphical model of the d

variables, i.e., the interaction structure Θ, given n observed samples.
The graphical modelling problem has been extensively tackled in
literature by estimating the precision matrix Θ instead of the co-
variance matrix Σ (as, e.g., [5]) [15, 30, 34]. This has been shown
to improve the graphical model inference, particularly for high-
dimensional problems [25]. In such contexts, the assumption is that
a variable is conditionally dependent only on a subset of all the
others. Therefore, the estimation of the precision matrix may be
guided by a sparse prior, in such a way to restrict the number of
possible connections in the network to improve interpretability
and noise reduction. Also, the imposition of a sparse prior on the
problem helps with the identifiability of the graph, especially when
the available number of samples is low compared to the dimension
of the problem. A model for the inference of Θ including the sparse
prior is the graphical lasso [15]:

minimize
Θ

− ℓ(S, Θ) + α ∥Θ ∥od, 1,

(1)

where ℓ is the Gaussian log-likelihood (up to a constant and scaling
factor) defined as ℓ(S, Θ) = log det(Θ)−tr(SΘ) for Θ positive definite
n X ⊤X is the empirical covariance matrix. ∥ · ∥od,1 is the
and S = 1
off-diagonal ℓ1-norm, which promotes sparsity in the precision
matrix (excluding the diagonal).

Latent variable network inference. Often, real-world observa-
tions do not conform exactly to a sparse GGM. This is due to global
hidden factors that influence the system, which introduce spurious
dependencies between observed variables [10, 11]. For this reason,
GGMs can be extended by introducing latent variables able to rep-
resent factors which are not observed in the data. These latent
variables are not principal components, since they do not provide a
low-rank approximation of the graphical model. On the contrary,
such factors are added to the model in order to condition the statis-
tics of the observed variables. In particular, we consider both latent
and observed variables to have a common domain [11].
Let latent variables be indexed by a set H , and observed variables
by a set O. The precision matrix Θ of the joint distribution of both
latent and observed variables may be partitioned into four blocks:

Θ = Σ−1 =

ΘH

ΘH O

ΘO H

ΘO












.












Such blocks represent the conditional dependencies among latent
variables (ΘH ), observed variables (ΘO ), between latent and ob-
served (ΘH O ), and viceversa (ΘO H ). The marginal precision matrix
Σ−1
O of the observed variables is given by the Schur complement
w.r.t. the block ΘH [8]:

ˆΘO = Σ−1
O

= ΘO − ΘO H Θ−1

H ΘH O .

(2)
ΘO specifies the precision matrix of the conditional statistics of the
observed variables given the latent variables, while ΘO H Θ−1
H ΘH O
is a summary of the effect of marginalisation over the latent vari-
ables. Such matrix has a small rank if the number of latent variables
is small compared to the observed variables. Note that the rank is
an indicator of the number of latent variables H [9].
The effect of the marginalisation is scattered over many observed
variables, in such a way not to confound it with the true underlying
conditional sparse structure of ΘO [8]. Typically ˆΘO is not sparse
due to the low-rank term, while, with the addition of the latent
factors contribution, we can recover the true sparse GGM. For this

reason, the graphical lasso in Equation (1) has been extended with
the latent variable graphical lasso that includes the inference of a
low-rank term, using the form (2), as follows [8]:

˜Θ, ˜L = argmin
(Θ, L)
L≽0

− ℓ(S, Θ − L) + α ∥Θ ∥od, 1 + τ ∥L ∥∗ .

(3)

Here ˜Θ provides an estimate of ΘO (precision matrix of the ob-
served variables) while ˜L provides an estimate of ΘO H Θ−1
H ΘH O
(marginalisation over the latent variables). Note that S is the empir-
ical covariance matrix computed only on observed variables, since
no information on the latent ones is available.

Time-varying network inference. Problems (1) and (3) aim at
recovering the structure of the system at fixed time (static network
inference). However, complex systems have temporal dynamics that
regulate their overall functioning [1, 15]. Hence, the modelling
of such complex systems requires a dynamical network inference,
where the states of the network are co-dependent. This naturally
leads to the idea of temporal consistency, which assumes similarities
between consecutive states of the network. In fact, we can assume
that, for sufficiently close time points, a system shows negligible
differences. During the inference of a dynamical network, temporal
consistency may translate into the imposition of similarities among
temporally close networks [16]. In particular, graphical lasso with
temporal consistency results in time-varying graphical lasso [17]:

minimize
(Θ1, . . ., ΘT )

T
(cid:213)

(cid:20)

i =1

− ℓ(Si, Θi ) + α ∥Θi ∥od, 1

+ β

Ψ(Θi +1 − Θi )

(4)

(cid:21)

T −1
(cid:213)

i =1

i j | · |.

where the inference of a network at a single time point i is guided
by the states at adjacent time points. The network is encoded in
a sequence of precision matrices (Θ1, . . . , ΘT ) which model the
system at each time point i = 1, . . . ,T . The type of similarity
imposed to consecutive time points and its strength are specified by
the penalty function Ψ and the parameter β, respectively. Options
when choosing Ψ include the following [17]:
• Lasso penalty (ℓ1) - Ψ = (cid:205)
Encourages few edges to change between subsequent time points,
while the rest of the structure remains the same [12].
• Group lasso penalty - Ψ = (cid:205)
j ∥ ·j ∥2.
Encourages the graph to restructure at some time points and to
stay stable in others [16? ].
• Laplacian penalty - Ψ = (cid:205)
i j (·i j )2.
Encourages smooth transitions over time, for slow changes of the
global structure [? ].
• Max norm penalty (ℓ∞) - Ψ = (cid:205)
Encourages a block of nodes to change their structure with no
additional penalty with respect to the change of a single edge among
such nodes. In fact, ℓ∞ norm is influenced only from the most
changing element for each row.
• Row-column overlap penalty - Ψ = minV :A=V +V ⊤ (cid:205)
Encourages a major change of the network at a specific time, while
in the rest the network is enforced to remain constant. The choice
of p = 2 causes the penalty to be node-based, i.e., the penalty allows
for a perturbation of only some nodes [? ].

j (maxi | ·i j |).

j ∥Vj ∥p .

Dependently from prior assumptions on the problem, one may
choose the most appropriate penalty for the data at hand.

3 MODEL FORMULATION
In this work we propose a new statistical model for the inference
of networks that change consistently in time under the influence
of latent factors. We call such model latent variable time-varying
graphical lasso (LTGL). LTGL infers the dynamical network of com-
plex systems by decomposing the problem into two parts — simi-
larly to what has been done in [8] for static network inference. We
consider two components of the dynamical network: a true under-
lying structure on the observed variables and the contribution of
latent factors. This allows to factor out the contribution of hidden
variables, favouring a reliable modelling of the dynamical system.
The novelty of our method is the simultaneous inference of a dy-
namical network with latent factors that exploits the imposition of
behavioural consistency on both observed variables interactions
and latent influence through the use of penalisation terms. This
allows for an easier interpretation of the evolution of the dynamical
system while, at the same time, improving its graphical modelling.
The two separate (while closely related) components at each time
point are obtained by integrating the network inference with the
information coming from temporally different states of the network.
Formally, let Xi ∈ Rni ×d , for i = 1, . . . ,T , be a set of observations
measured at T different time points composed by ni samples of d
observed variables. (Note that, for each time point i, samples are
assumed to be drawn from the probability distribution on the ob-
served variables conditioned on the latent ones.) Let Si = 1
i Xi
ni
be the empirical covariance matrix at time i. The goal is to retrieve a
set of sparse matrices Θ = (Θ1, . . . , ΘT ) and a set of low-rank matri-
ces L = (L1, . . . , LT ) such that, at each time point i, Θi encodes the
conditional independences between the observed variables, while
Li provides the summary of marginalisation over latent variables
on the observed ones.
Consider Equation (3) at a specific time i. Here we want to im-
pose continuity between the structure and the hidden variables
contribution in time, therefore we enforce the difference between
consecutive graphs to abide certain constraints by adding two pe-
nalisation terms. Our LTGL model takes the following form:

X ⊤

minimize
(Θ, L)
Li ≽0

T
(cid:213)

(cid:20)

i =1

+ β

T −1
(cid:213)

i =1

− ℓ(Si, Θi − Li ) + α ∥Θi ∥od, 1 + τ ∥Li ∥∗

(cid:21)

Ψ(Θi +1 − Θi ) + η

Φ(Li +1 − Li ),

T −1
(cid:213)

i =1

where Ψ and Φ are both penalty functions that force the structure of
the network to change over time according to a certain behaviour,
by acting on Θ and L, respectively. Temporal consistency of both
the structure of the network and latent factors contribution is guar-
anteed by the use of such penalty functions, which benefits the
network inference in particular in presence of few available ob-
servations of the system. Possible choices for Ψ and Φ are listed
in Section 2. Their choice is arbitrary and it is based on the prior
knowledge on the respective components evolution in the system.
Also, note that Ψ and Φ are independent, which allows LTGL to
model a wide range of dynamical behaviours of complex systems.

4 MINIMISATION METHOD
Problem (5) is convex, provided that the penalty functions Ψ and
Φ are convex, and it is coercive because of the regularisers. Thus,

Problem (5) admits solutions. Nonetheless, its optimisation is chal-
lenging in practice due to the high number of unknown matrices
involved (2T , for a total of 2T d (d +1)
unknowns of the problem). A
2
suitable method for the minimisation is ADMM [7]. It allows to
decouple the variables obtaining a separable minimisation problem
which can be efficiently solved in parallel. The sub-problems ex-
ploit proximal operators which are (mostly) solvable in closed-form,
leading to a simple iterative algorithm.
In order to decouple the involved matrices, we define three dual
variables R, Z = (Z1, Z2) and W = (W1,W2) and two projections:

P1 : (Rd ×d )T → (Rd ×d )T −1

P2 : (Rd ×d )T → (Rd ×d )T −1

A (cid:55)→ (A1, . . . , AT −1)

A (cid:55)→ (A2, . . . , AT )

Problem (5) becomes:

minimize
(Θ, L, R, Z ,W )
Li ≽0

T
(cid:213)

(cid:20)

i =1

+ β

T −1
(cid:213)

i =1

− ℓ(Si, Ri ) + α ∥Θi ∥od, 1 + τ ∥Li ∥∗

(cid:21)

Ψ(Z2,i − Z1,i ) + η

Φ(W2,i − W1, i )

(6)

T −1
(cid:213)

i =1

s.t. R = Θ − L, Z1 = P1Θ, Z2 = P2Θ, W1 = P1L, W2 = P2L.

The corresponding augmented Lagrangian is as follows:
Lρ (Θ, L, R, Z, W, U)

− ℓ(Si, Ri ) + α ∥Θi ∥od, 1 + τ ∥Li ∥∗ + I(L ≽ 0)

(cid:21)






+β

Ψ(Z2,i − Z1,i ) + η

Φ(W2,i − W1,i )

T −1
(cid:213)

i =1

∥Ri −Θi +Li +U0,i ∥2 − ∥U0,i ∥2

(cid:21)

∥Θi −Z1,i +U1,i ∥2 − ∥U1,i ∥2 + ∥Θi +1 −Z2,i +U2,i ∥2 − ∥U2,i ∥2

∥Li −W1,i +U3,i ∥2 − ∥U3,i ∥2 + ∥Li +1 −W2,i +U4,i ∥2 − ∥U4,i ∥2

T
(cid:213)

(cid:20)

=

i =1
T −1
(cid:213)

i =1
T
(cid:213)

(cid:20)

i =1
T −1
(cid:213)

(cid:20)

i =1
T −1
(cid:213)

(cid:20)

i =1

+ ρ
2

+ ρ
2

+ ρ
2

(5)

where U = (U0, U1, U2, U3, U4) are the scaled dual variables.
The ADMM algorithm for Problem (6) writes down as follows:

for k = 1, . . .

Rk +1 = argmin

Lρ (Θk, Lk, R, Zk, Wk, Uk )

Θk +1 = argmin

Lρ (Θ, Lk, Rk +1, Zk, Wk, Uk )

R

Θ

Lk +1 = argmin

Lρ (Θk +1, L, Rk +1, Zk, Wk, Uk )

Zk +1 =

Wk +1 =

Uk +1 =

(cid:21)

(cid:21)

+

L
(cid:20) Z k +1
1
Z k +1
2
(cid:20) W k +1
1
W k +1
2
U k
0
U k
1
U k
2
U k
3
U k
4























= argmin
Z

= argmin
W

Lρ (Θk +1, Lk +1, Rk +1, Z, Wk, Uk )

Lρ (Θk +1, Lk +1, Rk +1, Zk +1, W, Uk )

Rk +1 − Θk +1 + Lk +1
P1Θk +1 − Z k +1
P2Θk +1 − Z k +1
P1Lk +1 − W k +1
P2Lk +1 − W k +1

2

1

1

2












.












(cid:21)

(cid:21)

(7)

(8)

4.1 R step
The minimisation problem involving the matrix R in (8) can be
split into parallel updates, since Lρ (Θ, L, R, Z, W, U) is separable
in the variables (R1, . . . , RT ). Therefore, each Ri at iteration k + 1
is given by:
Rk +1
i

∥R − Θk + Lk + U k

0, i ∥2

= argmin
R

= argmin
R

tr(Si R) − log det(R) + ρ
2
tr(Si R) − log det(R) + ρ
2

= argmin
R

tr(Si R) − log det(R) + ρ
2

(cid:13)
(cid:13)
(cid:13)

2

F

i

(cid:13)
(cid:13)R − Ak
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

R −

Ak
i

+ Ak ⊤
i
2

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F

− U k

= Θk
i

− Lk
i

with Ak
0,i . Note that the last equality in (9) follows
i
from the symmetry of R — which also guarantees the log det to be
well-defined. Equation (9) can be explicitly solved. Indeed, Fermat’s
rule yields:

Si − ρ

Ak
i

+ Ak ⊤
i
2

= R−1 − ρR .

Then the solution to Equation (10) is [12, 17, 32]:
(cid:19)

(cid:18)

(cid:113)

V k

−Ek +

(Ek )2 + 4ρI

V k ⊤

Rk +1
i

= 1
2ρ

where V k EkV k ⊤ is the eigenvalue decomposition of Si − ρ

Ak
i

+Ak ⊤
i
2

.

4.2 Θ step
Likewise the R step, the update of Θ in (8) can be done in a parallel
fashion, as follows:

Θk +1
i

= argmin
Θ

α ∥Θ ∥od, 1 + ρ
2

(cid:20) (cid:13)
(cid:13)Rk
(cid:13)

i − Θ + Lk
i

+ U k
0, i

(cid:13)
(cid:13)
(cid:13)

2

F

+ δ iT

(cid:13)
(cid:13)Θ − Z k
(cid:13)

1,i

+ U k
1, i

+ δ i 1

(cid:13)
(cid:13)
(cid:13)

2

F

2,i −1

+ U k

2,i −1

= argmin
Θ

α ∥Θ ∥od, 1 + (1 + δ iT + δ i 1)

(cid:13)
(cid:13)Θ − Z k
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)Θ − Bk
(cid:13)

i

2

F

ρ
2

where δ i j = 1 − δi j , with δi j Kronecker delta and
1,i ) + δ i 1(Z k

+ δ iT (Z k

+ U k
0,i

+ Rk
i

Lk
i

Bk
i

=

1,i − U k
1 + δ iT + δ i 1

2,i −1 − U k

2,i −1)

.

Problem (11) is solved as:

Θk +1
i

(Bk

= proxζ ∥·∥od, 1
, and Sζ (·) element-wise off-diagonal soft-

i ) = Sζ (Bk
i )

with ζ =

α
ρ(1+δ iT +δ i 1)
thresholding function.

(9)

(10)

(cid:21)

(cid:13)
(cid:13)
(cid:13)

2

F

(11)

4.3 L step
The parallel update of L in (8) can be written as:
τ tr(L) + I(L ≽ 0) + ρ
2

i − Θk +1

= argmin
L

Lk +1
i

i

(cid:20) (cid:13)
(cid:13)Rk +1
(cid:13)
(cid:13)
(cid:13)L − W k
(cid:13)

+ δ iT

(cid:13)
(cid:13)L − W k
(cid:13)

1,i

+ U k
3,i

+ δ i 1

(cid:13)
(cid:13)
(cid:13)

2

F

2,i −1

+ U k

4, i −1

τ tr(L) + I(L ≽ 0) + (1 + δ iT + δ i 1)

= argmin
L

= argmin
L

τ tr(L) + I(L ≽ 0) + (1 + δ iT + δ i 1)

+ L + U k
i, 0

2

F

(cid:13)
(cid:13)
(cid:13)
(cid:21)

2

(cid:13)
(cid:13)
(cid:13)

F
(cid:13)
(cid:13)
(cid:13)

2

F

ρ
2

ρ
2

i

(cid:13)
(cid:13)L − C k
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

L −

C k
i

+ C k ⊤
i
2

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(12)

where

C k
i

=

i − Rk +1
Θk +1

i − U k
0,i

+ δ iT (W1,i − U3,i ) + δ i 1(W2,i −1 − U4,i −1)

.

1 + δ iT + δ i 1

Note that the last equality in (12) follows from the symmetry of L.
Then, the solution to Problem (12) is [24]:
= V k ˜EV k ⊤

Lk +1
i
where V k EkV k⊤ is the eigenvalue decomposition of Ck

i , and

(cid:32)

˜Ej j = max

Ek
j j −

τ
ρ(1 + δ iT + δ i1)

(cid:33)

, 0

.

4.4 Z and W step
The dual variables Z and W enforce the network to behave in time
consistently with the choice of Ψ and Φ, respectively. Z is the dual
variable of Θ while W is the dual variable of L. For the sake of
brevity, we show only the steps regarding the update of Z — the
update of W is analogous. The dual variable Z is defined as (Z1, Z2).
Such matrices are not separable in Equation (6), thus they must be
jointly updated. The update of Z in (8) can be rewritten as follows:

(cid:35)

(cid:34)

Z k +1
1,i
Z k +1
2,i

= argmin
Z1, Z2

Let ˆΨ

(cid:21)

(cid:20) Z1
Z2

an unique update [17]:

β Ψ(Z2 − Z1) + ρ
2
+ ρ
2

∥Θk

i − Z1 + X k

1,i ∥2

∥Θk

i +1 − Z2 + X k

2,i ∥2 .

(13)

= Ψ(Z2 − Z1). Then, Problem (13) can be solved with

(cid:21)

(cid:20) Z k +1
1,i
Z k +1
2,i

= prox β
ρ

ˆΨ(·)

(cid:18) (cid:20) Θk
i
Θk
i +1

+ U k
1,i
+ U k
2,i

(cid:21) (cid:19)

.

The same holds for the W step. Hence, the proximal operator for
the update of W1,i and W2,i becomes:
(cid:20) W k +1
1,i
W k +1
2,i

(cid:18) (cid:20) Lk
i
Lk
i +1

+ U k
i, 3
+ U k
i, 4

= prox η
ρ

ˆΦ(·)

(cid:21) (cid:19)

(cid:21)

.

For the particular derivation of different proximal operators,
see [17].

4.5 Termination criterion
According to [7], the algorithm is said to converge if the primal
and dual residuals are sufficiently small, i.e. if ∥r k ∥2
2 ≤ ϵ pri and
∥sk ∥2
2 ≤ ϵ dual. At each iteration k these values are computed as
follows:

∥r k ∥2
2

∥s k ∥2
2

= ∥R k − Θk + Lk ∥2
1 ∥2
+ ∥P1Θk − Z k
F
F
1 ∥2
2 ∥2
+ ∥P2Lk − W k
+ ∥P1Lk − W k
F
F
= ρ (cid:0) ∥R k − R k −1 ∥2
1 − Z k −1
∥2
+ ∥Z k
+ ∥Z k
1
F
F
(cid:1)
2 − W k −1
1 − W k −1
∥2
∥2
2
1
F
F
ϵ pri = c + ϵ rel max(cid:0)D1, D2
ϵ dual = c + ϵ relρ (cid:0) ∥U k

+ ∥W k
(cid:1)

+ ∥W k

+ ∥P2Θk − Z k

1 ∥2
F

2 − Z k −1
2

∥2
F

0 ∥2
F

+ ∥U k

+ ∥U k

1 ∥2
F
where c = ϵ absd(5T − 4)1/2, ϵ abs and ϵ rel are arbitrary tolerance
+ ∥W k
+ ∥Z k
+ ∥W k
∥2
∥2
∥2
∥2
parameters, Dk
1
1
2
2
F
F
F
F
+ ∥P1Θk ∥2
= ∥Θk −Lk ∥2
+ ∥P1Lk ∥2
+ ∥P2Lk ∥2
and Dk
F .
2
F
F
F

+ ∥Z k
1
+ ∥P2Θk ∥2
F

= ∥Rk ∥2
F

+ ∥U k

+ ∥U k

4 ∥2
F

2 ∥2
F

3 ∥2
F

(cid:1)

4.6 Implementation
The minimisation algorithm is available as a Python framework1,
fully compatible with the scikit-learn library of machine learning
algorithms, providing a straightforward and intuitive interface. The
implementation relies on low-level high-performance libraries for
numerical computations and it exploits closed-form solutions for
proximal operators, leading to a fast and scalable minimisation
algorithm even with an increasing number of unknowns.

5 EXPERIMENTS
We performed experiments on synthetic data assessing the perfor-
mance of the method in terms of structure recovery and measure of
latent variables influence. The performance of LTGL was evaluated
with respect to the ground truth and to state-of-the-art methods
for graphical inference. In particular, we assessed two aspects of
such methods, that are modelling performance and scalability, in
separated experiments. Modelling performance was estimated by
comparing the inferred graphical model to the true network un-
derlying the data set. During the scalability experiment, instead,
we assessed the computational time for convergence needed for
increasing problem complexity.

5.1 Modelling performance
We evaluated LTGL modelling performance on two synthetic data
sets. The ground truth sets of matrices Θ = (Θ1, . . . , ΘT ) and
L = (L1, . . . , LT ) were obtained by perturbing initial matrices Θ1
and L1, according to a specific behaviour for T − 1 times, with the
guarantee that Θi − Li ≻ 0 and Li ≽ 0 for i = 1, . . . ,T . The initial
matrices were generated according to [33], following the form (2).
Θ1 and L1 correspond to ΘO and ΘO H Θ−1
H ΘH O , respectively, with
ΘH identity matrix and ΘH O = Θ⊤
O H . Note that, since ΘH has full
rank, the number of latent variables is H . In particular, for d ob-
served variables, n samples and T timestamps, we generated a data
set X ∈ (Rn×d )T sampled from T multivariate normal distributions
Xi ∼ Ni (0, Σi ), for i = 1, . . . ,T , and Σ−1
i
ℓ2
2 perturbation (p2). The first data set was generated by
perturbing the initial matrices with a random matrix of small ℓ2
2
norm. This perturbation assumes the differences between two
consecutive matrices to be small and bounded over time, i.e.,
∥Θi − Θi−1 ∥F ≤ ϵ for i = 2, . . . ,T . The bound ϵ on the norm
is chosen a priori.
The update of Li is done maintaining consistency with the theo-
retical model where Li = ΘO H,i Θ⊤
O H,i . Therefore, the update is
obtained by adding a random matrix with a small norm to ΘO H,i−1.
In this way, the rank of Li remains the same as the number of latent
variables and constant over time. Data were generated in R100 with
10 time stamps, conditioned on 20 latent variables. For each time
stamp, we drew 100 samples from the distribution. For this reason,
in this setting, the contribution of latent factors is predominant
with respect to the network evolution in time.

= Θi − Li .

5.1.1

5.1.2

ℓ1 perturbation (p1). A second data set was generated
according to a different perturbation model. Here, the precision
matrix was updated by randomly choosing an edge and swapping

1LTGL is open-source. The
https://github.com/fdtomasi/regain.

code

is

available under BSD-3-Clause

at

Table 1. Performance in terms of F1 score, accuracy (ACC), mean
rank error (MRE) and mean squared error (MSE) of LTGL with re-
spect to TVGL, LVGLASSO and GL. LTGL and TVGL are employed
with both ℓ2
2 and ℓ1 penalties, to show how the prior on the evolu-
tion of the network affects the outcome.

perturbation method

score

F1

ACC MRE MSE

ℓ2
2 (p2)

ℓ1 (p1)

LTGL (ℓ2
0.926
2)
LTGL (ℓ1)
0.898
TVGL (ℓ2
0.791
2)
TVGL (ℓ1)
0.791
LVGLASSO 0.815
GL
0.745
LTGL (ℓ2
2)
0.842
LTGL (ℓ1)
0.880
TVGL (ℓ2
2)
0.742
TVGL (ℓ1)
0.817
LVGLASSO 0.752
0.748
GL

0.994
0.993
0.980
0.980
0.988
0.974

0.974
0.981
0.950
0.968
0.964
0.951

0.70
0.70
-
-
2.80
-

0.29
0.28
-
-
0.74
-

0.007
0.007
0.003
0.003
0.007
0.004

0.013
0.013
0.009
0.009
0.013
0.007

Figure 2. Distribution of inferred ranks over time. For each method
that considers latent variables, we show the frequency of finding
a specific rank during the network inference. The vertical line in-
dicates the ground truth rank, around which all detected ranks lie.
Note that, in (p2), Li ∈ R100×100, therefore the range of possible ranks
is [0, 100]. For (p2), Li ∈ R50×50, hence the range is [0, 50].

its state, i.e., by removing or adding a connection between two
variables. This allows for a ℓ1-norm evolutionary pattern of the
network. Data were generated in R50 with 100 time stamps, con-
ditioned on 5 latent variables. For each time stamp, we drew 100
samples from the distribution. In this setting, the time consistency
affects the network more than the latent factor contribution.

5.1.3

Scores. We evaluated LTGL performance using different
scores measuring the divergence of the results from the ground
truth. In particular, the performance was evaluated in terms of F1
score, accuracy, mean rank error and mean squared error. We define
as true/false positive the number of correctly/incorrectly existing in-
ferred edges, true/false negative the number of correctly/incorrectly
missing inferred edges [18]. The scores are computed as follows.
• F1 score: indicates the quality of structure inference, as the har-
monic mean of precision and recall.
• Accuracy (ACC): evaluates the number of true existing and missing
connections in the network correctly inferred with respect to the
total number of connections.

Figure 3. Scalability comparison for LTGL in relation to other ADMM-based methods. The compared methods are initialised in the same
manner, i.e., with all variable interactions (not self-interacting) set to zero. For LVGLASSO and TVGL, we used their relative original imple-
mentations. Also, we ignore the computational time required for hyperparameters selection. LTGL outperforms the other methods for each
increasing time and dimensionality of the problem.

• Mean rank error (MRE): estimates the precision on the number of
inferred latent variables, based on the rank of the set of matrices ˜L
in relation to the ground truth. The MRE score is defined as:

MRE = 1
T

T
(cid:213)

i =1

(cid:12)rank(Li ) − rank( ˜Li )(cid:12)
(cid:12)
(cid:12).

A value close to 0 means that we are inferring the true number of
latent variables over time, while, viceversa, a high value indicates a
poor consideration of the contribution of the latent variables.
• Mean squared error (MSE): score how close is the inferred precision
matrix ˜Θ to the ground truth, in terms of the Frobenius norm:

MSE =

2
T d(d − 1)

T
(cid:213)

i =1

(cid:13)
(cid:13)
(cid:13)Θ

(u)
(u)
i − ˜Θ
i

(cid:13)
(cid:13)
(cid:13)F

,

where Θ(u) denotes the upper triangular part of Θ.

5.1.4 Discussion. Table 1 shows the performance of LTGL com-
pared to graphical lasso (GL) [15], latent variable graphical lasso
(LVGLASSO) [8, 24] and time-varying graphical lasso (TVGL) [17]
in terms of F1 score, accuracy, mean rank error (MRE) and mean
squared error (MSE), for both settings with ℓ2
2 (p2) and ℓ1 (p1) per-
turbation. Note that MRE is not available for all the methods since
neither GL or TVGL consider latent factors. LTGL and TVGL are
used with two temporal penalties according to the different per-
turbation models of data generation. In this way, we show how
the correct choice of the penalty for the problem at hand results in
a more accurate network estimation. In both (p2) and (p1), LTGL
outperforms the other methods for graphical modelling. In (p2), in
particular, LTGL correctly infers almost 99,5% of edges in all the dy-
namical network both with the ℓ2
2 and ℓ1 penalties. Nonetheless, the
use of ℓ2
2 penalty enhance the quality of the inference as expected
from the theoretical assumption made during data generation. The
choice of a proper penalty for the problem and the consideration
of time consistency is reflected also in a low MRE, which encom-
passes LVGLASSO ability in detecting latent factors (Figure 2). In
(p2), in fact, the number of latent variables with respect to both
observed variables and samples is high. Therefore, by exploiting
temporal consistency of the network, LTGL is able to improve the
latent factors estimation. Simultaneous consideration of time and
latent variable also positively influences the F1 score, i.e., structure
detection.
Above considerations also hold for the (p1) setting. Here, LTGL
achieves the best results in both F1 score and accuracy, while hav-
ing a low MRE. The adoption of ℓ1 penalty improves structure
estimation and latent factors detection, consistently with the data
generation model. Such settings were designed to show how the

prevalence of latent factors contribution or time consistency af-
fects the outcome of a network inference method. In (p2), where
the latent factors contribution is prevalent, network inference is
more precise when considering latent factors. In (p1), instead, the
number of time points is more relevant than the contribution of
latent factors, hence it is more effective to exploit time consistency
(both for latent and observed variables), evident from the results
of Table 1. LTGL benefits from both aspects, therefore leading to a
noticeable improvement of graphical modelling.

5.2 Scalability
Next, we performed a scalability analysis using LTGL with respect
to different ADMM-based solvers. We evaluated the performance
of our method in relation to LVGLASSO [24] and TVGL [17], both
implemented with closed-form solutions to ADMM subproblems.
In general, the complexity of the three compared solvers is the
same (up to a constant). The implementation of GL [15] was not
included in such experiment, since it is not based on ADMM but on
coordinate descent, and therefore it is not comparable to our method.
As in Section 5.1, we generated different data sets X ∈ (Rn×d )T
with different values of T and d. In particular, d ∈ [10, 400) and
T = {20, 50, 100}. We ran our experiments on a machine provided
with two CPUs (2.4 GHz, 8 cores each).
Figure 3 shows, for the three different time settings, the scalability
of the methods in terms of seconds per convergence considering
different number of unknowns of the problem (i.e., 2T d (d +1)
2 with d
observed variables and T times). In all settings, LTGL outperforms
LVGLASSO and TVGL in terms of seconds per convergence. In
particular, the computational time for convergence remains stable
disregarding the number of time points under consideration. We
emphasise that the most computationally expensive task performed
by our solver is represented by two eigenvalue decompositions,
with a complexity of O(d3), to solve both R and L steps (Section 4).

5.3 Model selection
The hyperparameters of the method have been selected by using a
cross-validation procedure. In particular, we used the Monte Carlo
Cross-Validation (MCCV) [27] that repeatedly splits the n samples
of the data set in two mutually exclusive sets. For each split, n · (1/ν )
samples are labelled as validation set and the remaining n · (1 − 1/ν )
as learning set. For each hyperparameter combination, the model
was trained on the learning set and the likelihood of the model
was estimated on the independent test set. Finally, we selected the
combination of hyperparameters based on the average maximum
likelihood of the model across multiple splits of the data set.

However, the number of possible combinations of LTGL hyperpa-
rameters can be arbitrarily large. In order to avoid the assessment
of a grid of models (which can be computationally expensive), we
used a Gaussian process-based Bayesian optimisation procedure to
choose the best combination of hyperparameters for each analysed
data set, based on the Expected Improvement (EI) strategy [31]. In
practice, assuming the dynamics of a real system to be unknown,
it is possible to select the most appropriate temporal penalty by
exploiting the same principles, i.e., via a model selection procedure
based on the likelihood of different temporal models.

6 APPLICATIONS TO REAL DATA
We applied LTGL to two real data sets, to show how the method
can be employed to infer useful insights on multivariate time-series
data. These data sets measure complex dynamical systems of dif-
ferent (biological and financial) nature, which are usually highly
dimensional and feature complicated interdependences between
variables. This fact makes them ideal candidates for an analysis
using graphical models.

6.1 Metabolomic Data
The physiology of Escherichia coli necessitates rapid changes of
its cellular and molecular network to adapt to environmental con-
ditions. E. coli is widely studied because of the efficiency in its
system response to perturbation. Following the analysis of [21], we
used LTGL on E. coli data to infer network modifications across
different time points evaluated before and after the application of
environmental condition perturbations. We analysed the behaviour
of metabolites, which have been shown to change consistently after
the perturbation. Samples underwent one of two types of stress,
namely cold and heat stress.
Perturbation response detection. We inferred the dynamical
network of E. coli metabolites using LTGL with a group lasso (ℓ2)
penalty on latent variable contribution and a Laplacian (ℓ2
2) penalty
on the observed network. In this way, we allow the latent variables
(which, in our model, could represent the stress or other factors)
to change their global influence at a specific time point, while re-
maining stable in all others. At the same time, by conditioning
the network on the latent variables, we allow the observed net-
work structure to change smoothly in time. Hence, we expect to
see a global shift of the network between the second and third
time points, that is when the perturbation has been introduced in
the system. Figure 4a shows the temporal deviation between time
points, both for Θ, L and the total observed system R = Θ − L.
Latent variables temporal deviation reaches a peak at time t2−3,
right after the application of the perturbation to the system. Instead,
the difference between consecutive Θs remains more stable. Con-
sistently, the difference between the observed networks Rs shows
a major change at the same time point. Hence we can distinguish
the underlying evolving structure of metabolites while detecting
the contribution of the latent variables which affect mostly the
total system. In accordance with [21], we observed a interaction
between isoleucine, threonine, phenylalanine and 2-aminobutyric
acid during the adaptation phase following the stress response (Fig-
ure 4b). Therefore, we can conclude that LTGL successfully inferred
a dynamical network which adjusts in response to perturbation, in
accordance with our prior knowledge about E. coli behaviour.

Figure 4. Structure change of E. coli metabolites subject to stress.
The perturbation happens between time t = 2 and t = 3 (ver-
tical dotted line). (a) Temporal deviation where each point repre-
sents the difference between the network at subsequent time points.
The highest deviation on the observed network R appears when the
stress was applied. This can be decomposed into two parts, the la-
tent factors L and the underlying structure of observed variables Θ.
(b) Structural changes of metabolites interactions before and after
the perturbation.

6.2 Stock market
Finance is another example of a complex dynamical system suitable
to be analysed through a graphical model. Stock prices, in particular,
are highly related to each other and subject to time and environ-
mental changes, i.e., events that modify the system behaviour but
are not directly related to companies share values [3]. Here, the
assumption is that each company, while being part of a global finan-
cial system, is directly dependent from only a subset of others. For
example, it is reasonable to expect that stock prices of a technology
company are not directly influenced by trend of companies on the
primary sector. The modelling power of LTGL allows to detect both
the evolution of relations between companies and environmental
changes happening at a particular point in time. In order to show
this, we analysed stock prices2 during the financial crisis of 2007-
2008. The experiment was designed to consider the latent influence
of the market drop on technology companies interactions.

Global market crisis detection. We used a group lasso (ℓ2)
penalty to detect global shifts of the network. Figure 5 shows two
major changes in both components of the network (latent and ob-
served), in correspondence of late 2007 and late 2008. In particular,
during October 2008 a global crisis of the market occurred, and this
effect is especially evident for the shift of latent variables. Also, the
observed network changes in correspondence of the latent variables
shift or immediately after, caused by the effect of the crisis on the
stock market. The latent factors influence explains how the change
of the network was due to external factors that globally affected the
market, and not to normal evolution of companies relationships.
We further investigated on the causes for the first shift. Indeed,
we found that in late 2007 it happened a drop of a big American
company that was later pointed out as the beginning of the global
crisis of the following year.

2Data are freely available on https://quantquote.com/historical-stock-data.

[7] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. 2010. Distributed Optimiza-
tion and Statistical Learning via the Alternating Direction Method of Multipliers.
Foundations and Trends® in Machine Learning 3, 1 (2010).

[8] V. Chandrasekaran, P. A. Parrilo, and A. S. Willsky. 2010. Latent variable graph-
ical model selection via convex optimization. In Communication, Control, and
Computing (Allerton), 2010 48th Annual Allerton Conference on. IEEE, 1610–1613.
[9] V. Chandrasekaran, S. Sanghavi, P. A. Parrilo, and A. S. Willsky. 2011. Rank-
sparsity incoherence for matrix decomposition. SIAM Journal on Optimization
21, 2 (2011), 572–596.

[10] M. J. Choi, V. Chandrasekaran, and A. S. Willsky. 2010. Gaussian multiresolution
models: Exploiting sparse Markov and covariance structure. IEEE Transactions
on Signal Processing 58, 3 (2010), 1012–1024.

[11] M. J. Choi, V. Y.F. Tan, A. Anandkumar, and A. S. Willsky. 2011. Learning latent

tree graphical models. JMLRh 12, May (2011), 1771–1812.

[12] P. Danaher, P. Wang, and D. M. Witten. 2014. The joint graphical lasso for inverse
covariance estimation across multiple classes. Journal of the Royal Statistical
Society. Series B: Statistical Methodology 76, 2 (2014), 373–397. arXiv:1111.0324
[13] C. Ding, X. He, and H. D. Simon. 2005. On the equivalence of nonnegative matrix
factorization and spectral clustering. In Proceedings of the 2005 SIAM International
Conference on Data Mining. SIAM, 606–610.

[14] A. Farasat, A. Nikolaev, S. N. Srihari, and R. H. Blair. 2015. Probabilistic graphical

models in modern social network analysis. SNAM 5, 1 (2015), 62.

[15] J. Friedman, T. Hastie, and R. Tibshirani. 2008. Sparse inverse covariance estima-

tion with the graphical lasso. Biostatistics 9, 3 (2008), 432–441.

[16] A. J. Gibberd and S. Roy. 2017. Multiple Changepoint Estimation in High-
Dimensional Gaussian Graphical Models. arXiv preprint arXiv:1712.05786 (2017).
[17] D. Hallac, Y. Park, S. Boyd, and J. Leskovec. 2017. Network Inference via the Time-
Varying Graphical Lasso. In Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD ’17). ACM, 205–213.
[18] M. Hecker, S. Lambeck, S. Toepfer, E. Van Someren, and R. Guthke. 2009. Gene
regulatory network inference: Data integration in dynamic models: A review.
Biosystems 96, 1 (2009), 86–103.

[19] L. Huang, L. Liao, and C. H. Wu. 2016. Inference of protein-protein interaction
networks from multiple heterogeneous data. EURASIP Journal on Bioinformatics
and Systems Biology 2016, 1 (2016), 1–9.

[20] A. Jalali and S. Sanghavi. 2011. Learning the dependence graph of time series

with latent factors. arXiv preprint arXiv:1106.1887 (2011).

[21] S. Jozefczuk, S. Klie, G. Catchpole, J. Szymanski, A. Cuadros-Inostroza, D. Stein-
hauser, J. Selbig, and L. Willmitzer. 2010. Metabolomic and transcriptomic stress
response of Escherichia coli. Molecular systems biology 6, 1 (2010), 364.

[22] S. L. Lauritzen. 1996. Graphical models. Vol. 17. Clarendon Press.
[23] H. Liu, F. Han, and C. Zhang. 2012. Transelliptical graphical models. In NIPS.

800–808.

[24] S. Ma, L. Xue, and H. Zou. 2013. Alternating Direction Methods for Latent
Variable Gaussian Graphical Model Selection. Neural Computation 25, 8 (aug
2013), 2172–2198.

[25] N. Meinshausen and P. Bühlmann. 2006. High-dimensional graphs and variable

selection with the lasso. The annals of statistics (2006), 1436–1462.

[26] Z. Meng, B. Eriksson, and A. Hero. 2014. Learning latent variable Gaussian

graphical models. In Proceedings of the 31st ICML. 1269–1277.

[27] A. M. Molinaro, R. Simon, and R. M. Pfeiffer. 2005. Prediction error estimation: a
comparison of resampling methods. Bioinformatics 21, 15 (2005), 3301–3307.
[28] E. J. Molinelli, A. Korkut, W. Wang, M. L. Miller, N. P. Gauthier, X. Jing, P. Kaushik,
Q. He, G. Mills, D. B. Solit, et al. 2013. Perturbation biology: inferring signaling
networks in cellular systems. PLoS computational biology 9, 12 (2013), e1003290.
[29] P. Orchard, F. Agakov, and A. Storkey. 2013. Bayesian inference in sparse Gaussian

graphical models. arXiv preprint arXiv:1309.7311 (2013).

[30] P. Ravikumar, M.J. Wainwright, G. Raskutti, B. Yu, et al. 2011. High-dimensional
covariance estimation by minimizing ℓ1s-penalized log-determinant divergence.
Electronic Journal of Statistics 5 (2011), 935–980.

[31] J. Snoek, H. Larochelle, and R. P. Adams. 2012. Practical Bayesian optimization

of machine learning algorithms. In NIPS. 2951–2959.

[32] D. M. Witten and R. Tibshirani. 2009. Covariance-regularized regression and
classification for high dimensional problems. Journal of the Royal Statistical
Society. Series B: Statistical Methodology 71, 3 (jun 2009), 615–636.

[33] M. Yuan. 2012. Discussion: Latent variable graphical model selection via convex

optimization. Ann. Statist. 40, 4 (08 2012), 1968–1972.

graphical model. Biometrika (2007), 19–35.

Figure 5. Temporal deviation for stock market data. We observed
two peaks, in correspondence of late 2007 and late 2008, when the
financial crisis happened.

7 CONCLUSIONS AND FUTURE WORK
In this work, we developed a novel method for graphical modelling
of multivariate time-series. The model considers simultaneously
the contribution of latent factors and time consistency in evolving
systems. Indeed, our work is an attempt to generalise both latent
variable and dynamical network inference. To this aim, we impose
prior knowledge on the problem through penalty terms that force
precision and latent matrices to be consistent in time. The choice
of proper penalty terms maintains the convexity of the minimised
functional and, along with the coercivity given by the regularisers,
it guarantees global convergence of the proposed minimisation
algorithm. Our experiments demonstrate the ability of LTGL in
the graphical modelling of synthetic and real-world data, where
the possibility to decompose the total network into two separated
components allows for a better understanding of the underlying
phenomenon.
We emphasise that our framework is modular in the choice of the
penalties, allowing for precise modelling of different and complex
behaviours of the system. This allows for a straightforward inclu-
sion of additional penalty terms, based on the prior knowledge on
the problem at hand. Possible extensions may involve alternative
evolutionary models for different complex systems, e.g. forcing sub-
groups of variables to behave consistently in time [6]. These could
lead to interesting results in time-series clustering and pattern dis-
covery. Further investigations may also head to the inference of the
exact contribution of latent factors starting from the L matrices we
are estimating, possibly using matrix factorisation methods [13].
Such developments may increase the expression power of the
method, leading to advances in data mining and to potential appli-
cations in diverse science fields.

Acknowledgments. The authors thank the anonymous reviewers
for their valuable comments.

REFERENCES
[1] R. Albert. 2007. Network inference, analysis, and modeling in systems biology.

The Plant cell 19, 11 (nov 2007), 3327–38.

bayesian networks with latent variables. In ICML. 249–257.

[3] J. Bai and S. Ng. 2006. Evaluating latent and observed factors in macroeconomics

and finance. Journal of Econometrics 131, 1-2 (2006), 507–537.

[4] E. Bianco-Martinez, N. Rubido, Ch. G. Antonopoulos, and M.S. Baptista. 2016.
Successful network inference from time-series data using mutual information
rate. Chaos: An Interdisciplinary Journal of Nonlinear Science 26, 4 (2016), 043102.
[5] J. Bien and R. J. Tibshirani. 2011. Sparse estimation of a covariance matrix.

Biometrika 98, 4 (dec 2011), 807–820.

[6] A. Bolstad, B. D. Van Veen, and R. Nowak. 2011. Causal network inference via
group sparse regularization. IEEE transactions on signal processing 59, 6 (2011).

[2] A. Anandkumar, D. Hsu, A. Javanmard, and S. Kakade. 2013. Learning linear

[34] M. Yuan and Y. Lin. 2007. Model selection and estimation in the Gaussian

8
1
0
2
 
g
u
A
 
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
7
8
9
3
0
.
2
0
8
1
:
v
i
X
r
a

Latent Variable Time-varying Network Inference

Federico Tomasi∗
Università degli Studi di Genova
16146 Genova, Italy
federico.tomasi@dibris.unige.it

Saverio Salzo
Istituto Italiano di Tecnologia
16163 Genova, Italy
saverio.salzo@iit.it

Veronica Tozzo∗
Università degli Studi di Genova
16146 Genova, Italy
veronica.tozzo@dibris.unige.it

Alessandro Verri
Università degli Studi di Genova
16146 Genova, Italy
alessandro.verri@unige.it

ABSTRACT
In many applications of finance, biology and sociology, complex
systems involve entities interacting with each other. These pro-
cesses have the peculiarity of evolving over time and of comprising
latent factors, which influence the system without being explicitly
measured. In this work we present latent variable time-varying
graphical lasso (LTGL), a method for multivariate time-series graph-
ical modelling that considers the influence of hidden or unmea-
surable factors. The estimation of the contribution of the latent
factors is embedded in the model which produces both sparse and
low-rank components for each time point. In particular, the first
component represents the connectivity structure of observable vari-
ables of the system, while the second represents the influence of
hidden factors, assumed to be few with respect to the observed
variables. Our model includes temporal consistency on both com-
ponents, providing an accurate evolutionary pattern of the system.
We derive a tractable optimisation algorithm based on alternating
direction method of multipliers, and develop a scalable and effi-
cient implementation which exploits proximity operators in closed
form. LTGL is extensively validated on synthetic data, achieving
optimal performance in terms of accuracy, structure learning and
scalability with respect to ground truth and state-of-the-art meth-
ods for graphical inference. We conclude with the application of
LTGL to real case studies, from biology and finance, to illustrate
how our method can be successfully employed to gain insights on
multivariate time-series data.

KEYWORDS
network inference; graphical models; latent variables; time-series;
convex optimization

ACM Reference Format:
Federico Tomasi, Veronica Tozzo, Saverio Salzo, and Alessandro Verri. 2018.
Latent Variable Time-varying Network Inference. In KDD ’18: The 24th ACM

∗These authors equally contributed to the paper.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
KDD ’18, August 19–23, 2018, London, United Kingdom
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5552-0/18/08. . . $15.00
https://doi.org/10.1145/3219819.3220121

SIGKDD International Conference on Knowledge Discovery & Data Mining,
August 19–23, 2018, London, United Kingdom. ACM, New York, NY, USA,
9 pages. https://doi.org/10.1145/3219819.3220121

1 INTRODUCTION
The problem of understanding complex systems arises in many
diverse contexts, such as financial markets [23, 29], social networks
[14] and biology [18, 19]. In such contexts, the goal is to analyse the
system in order to retrieve information on how the components be-
have. This requires accurate and interpretable mathematical models
whose parameters, in practice, need to be estimated from observa-
tions.
Mathematically, a system can be modelled as a network of interac-
tions (edges) between its entities (nodes). However, the underlying
structure of the variables within the system is usually not known a
priori. Nevertheless, observations of the system (i.e., data) incorpo-
rate information on the interactions between variables, since they
provide measurements of such variables acting in the system.
The problem of inferring a network of variable interactions from
data is known as network inference or graphical model selection
[15, 22]. During the last years, the graphical modelling problem
has received much attention, particularly for the availability of
an always increasing number of samples that are required for a
reliable network inference. Nonetheless, structure estimation of
complex systems remains challenging for many reasons. In this
work, we want to tackle two aspects: (i) the presence of global
hidden (or latent) factors, and (ii) the dynamic of systems that
evolve in time. We argue that the inference of a dynamical network
encoding a complex system requires a specific attention to both
aspects to result in a more realistic representation. In particular, a
system may be affected by (latent) factors not encoded in the model.
Such factors, acting in the system, influence how the observable
entities behave and, hence, how they are connected with each other
[10]. The consideration of hidden and unmeasured variables during
the inference process emerges as crucial to avoid misrepresenting
real-world data [26].
At the same time, a complex system depends on a temporal com-
ponent, which drives variable interactions to evolve consistently
during its extent. This means that the structure can either change
or remain stable according to the nature of the system itself. Hence,
the understanding of a complex system is bound to the observation
of its evolution. This is particularly evident in some applications,
such as biology, where the interest could be to understand the
response of the system to perturbation [28].

Related work. Latent variable models have been widely studied
in literature, and shown to outperform graphical models that only
consider observable variables [8, 11, 33]. At the same time, a set of
methods were designed to study the temporal component through
the inference of a dynamical network that incorporates prior knowl-
edge on the behaviour of the system [4, 17]. Time-series with latent
variables are considered to obtain a single graph which represents
the global system [2, 20]. However, to the best of our knowledge,
state-of-the-art methods for regularised network inference do not
consider simultaneously time and latent variables in the inference
of multiple connected networks.

Contribution. In this work we propose latent variable time-varying
graphical lasso (LTGL), a model for dynamical network inference
where the structure is influenced by latent factors. This can be
seen as an attempt to generalise both dynamical and latent variable
network inference under a single unified framework. In partic-
ular, starting from a set of observations of a system at different
time points, LTGL infers an interaction network of the observed
variables under the influence of latent factors, while taking in con-
sideration the temporal evolution of the system. The empirical
interaction network is decomposed into the true underlying struc-
ture of the network and the contribution of latent factors, under
the assumption that both observable variables and latent factors
interdependence follow a temporal non-random behaviour. For
this reason, the model allows to include prior knowledge on the
evolutionary pattern of the system. The imposition of such prior
knowledge benefits inference and subsequent analysis of the net-
work, accentuating precise dynamical patterns. This is particularly
important when the number of samples is low compared to the
number of observed and latent variables in the system. In fact, the
inference of the network at particular time points exploits the de-
pendence between consecutive temporal states. Such advantage
is achieved by a simultaneous inference of all the dynamical sys-
tem, that, mathematically, translates into imposing constraints on
the network behaviour. In this work we provide a set of possible
constraints that can be applied independently on both observed
and latent components, allowing for a wide range of evolutionary
patterns.
Figure 1 provides an example of the theoretical model assumed by
LTGL. Here, observed and latent variables (xi and zi ) are connected
in a slightly different way at each time. Note that the observations
of the system only involve variables xi , while the hidden factors zi
influence the system without being actually observed. Hence, when
analysing samples which are regulated from a dynamical network
with hidden factors, it is infeasible to precisely infer the identity of
latent variables, but only an estimation of their number and their
effect on the global system can be obtained.
Starting from the theoretical model we derived a minimisation
algorithm based on the alternating direction method of multipli-
ers (ADMM) [7]. The algorithm is divided into independent steps
using proximal operators, which can be solved by closed-form so-
lutions favouring a fast and scalable computation [12, 17, 24]. We
also provide the related implementation in a Python framework,
based on the use of highly optimised low-level libraries for nu-
merical computation. Experiments on synthetic data show LTGL
to achieve optimal performance in relation to ground truth and

Figure 1. A dynamical network with latent factors zi and observed
variables xi . At each time ti , all of the connections between la-
lines) and connections among ob-
tent and observed variables (
served variables (
lines) may change according to a specific tem-
poral behaviour.

to state-of-the-art methods for graphical modelling, in terms of
accuracy, structure learning and scalability. Moreover, we show the
computational efficiency of LTGL while increasing the number of
unknowns of the problem and the model complexity. We conclude
with the application of LTGL to real-world data sets to illustrate
how our method can be successfully employed to gain insights on
multivariate time-series data. In particular, we used biological and
financial data sets, to show the use of LTGL in different contexts.
In the first case, we analysed Escherichia coli response to pertur-
bation, correctly identified by our method. In the latter case, we
investigated on a financial data set, to show how the contribution
of latent factors is relevant for the understanding of the behaviour
of the system.

Outline. The paper is organised as follows. Section 2 includes a
background on the reference frameworks for static and dynamical
network inference. Section 3 contains the theoretical formulation
of the problem and the proposed method. Section 4 describes in
details the optimisation algorithm for the minimisation of the func-
tional. Section 5 and Section 6 illustrate the use of our method on
synthetic and real data, respectively. Finally, Section 7 concludes
with a discussion and future directions.

2 PRELIMINARIES
Given a graph G = (V, E), where V = {x1, . . . , xd } is a finite set
of vertices, and E ⊆ V × V is a set of edges, a graphical model
is a multivariate probability distribution on x1, . . . , xd variables
where the conditional independence between two variables xi and
xj given all the others is encoded in G [22]. The two variables xi and
xj are conditionally independent given the others if (xi , xj ) (cid:60) E
and (xj , xi ) (cid:60) E. In what follows, we consider only undirected
Gaussian graphical models (GGMs), where (i) there is no distinc-
tion between an edge (xi , xj ) ∈ E and (xj , xi ), and (ii) variables are
jointly distributed according to a multivariate Gaussian distribution
N (µ, Σ). Without loss of generality, we assume µ to be zero, thus
the distribution depends only on the covariance matrix Σ [11]. The
inverse covariance matrix Θ = Σ−1, called precision matrix, encodes
the conditional independence between pairs of variables. In partic-
ular, the precision matrix has a zero entry in the position i, j only
if (xi , xj ) (cid:60) E [22]. Hence, we can interpret the precision matrix
as the weighted adjacency matrix of G, encoding the dependence
between variables.

Network inference. Consider a series of samples drawn from a
multivariate Gaussian distribution X ∼ N (0, Σ), X ∈ Rn×d . Net-
work inference aims at recovering the graphical model of the d

variables, i.e., the interaction structure Θ, given n observed samples.
The graphical modelling problem has been extensively tackled in
literature by estimating the precision matrix Θ instead of the co-
variance matrix Σ (as, e.g., [5]) [15, 30, 34]. This has been shown
to improve the graphical model inference, particularly for high-
dimensional problems [25]. In such contexts, the assumption is that
a variable is conditionally dependent only on a subset of all the
others. Therefore, the estimation of the precision matrix may be
guided by a sparse prior, in such a way to restrict the number of
possible connections in the network to improve interpretability
and noise reduction. Also, the imposition of a sparse prior on the
problem helps with the identifiability of the graph, especially when
the available number of samples is low compared to the dimension
of the problem. A model for the inference of Θ including the sparse
prior is the graphical lasso [15]:

minimize
Θ

− ℓ(S, Θ) + α ∥Θ ∥od, 1,

(1)

where ℓ is the Gaussian log-likelihood (up to a constant and scaling
factor) defined as ℓ(S, Θ) = log det(Θ)−tr(SΘ) for Θ positive definite
n X ⊤X is the empirical covariance matrix. ∥ · ∥od,1 is the
and S = 1
off-diagonal ℓ1-norm, which promotes sparsity in the precision
matrix (excluding the diagonal).

Latent variable network inference. Often, real-world observa-
tions do not conform exactly to a sparse GGM. This is due to global
hidden factors that influence the system, which introduce spurious
dependencies between observed variables [10, 11]. For this reason,
GGMs can be extended by introducing latent variables able to rep-
resent factors which are not observed in the data. These latent
variables are not principal components, since they do not provide a
low-rank approximation of the graphical model. On the contrary,
such factors are added to the model in order to condition the statis-
tics of the observed variables. In particular, we consider both latent
and observed variables to have a common domain [11].
Let latent variables be indexed by a set H , and observed variables
by a set O. The precision matrix Θ of the joint distribution of both
latent and observed variables may be partitioned into four blocks:

Θ = Σ−1 =

ΘH

ΘH O

ΘO H

ΘO












.












Such blocks represent the conditional dependencies among latent
variables (ΘH ), observed variables (ΘO ), between latent and ob-
served (ΘH O ), and viceversa (ΘO H ). The marginal precision matrix
Σ−1
O of the observed variables is given by the Schur complement
w.r.t. the block ΘH [8]:

ˆΘO = Σ−1
O

= ΘO − ΘO H Θ−1

H ΘH O .

(2)
ΘO specifies the precision matrix of the conditional statistics of the
observed variables given the latent variables, while ΘO H Θ−1
H ΘH O
is a summary of the effect of marginalisation over the latent vari-
ables. Such matrix has a small rank if the number of latent variables
is small compared to the observed variables. Note that the rank is
an indicator of the number of latent variables H [9].
The effect of the marginalisation is scattered over many observed
variables, in such a way not to confound it with the true underlying
conditional sparse structure of ΘO [8]. Typically ˆΘO is not sparse
due to the low-rank term, while, with the addition of the latent
factors contribution, we can recover the true sparse GGM. For this

reason, the graphical lasso in Equation (1) has been extended with
the latent variable graphical lasso that includes the inference of a
low-rank term, using the form (2), as follows [8]:

˜Θ, ˜L = argmin
(Θ, L)
L≽0

− ℓ(S, Θ − L) + α ∥Θ ∥od, 1 + τ ∥L ∥∗ .

(3)

Here ˜Θ provides an estimate of ΘO (precision matrix of the ob-
served variables) while ˜L provides an estimate of ΘO H Θ−1
H ΘH O
(marginalisation over the latent variables). Note that S is the empir-
ical covariance matrix computed only on observed variables, since
no information on the latent ones is available.

Time-varying network inference. Problems (1) and (3) aim at
recovering the structure of the system at fixed time (static network
inference). However, complex systems have temporal dynamics that
regulate their overall functioning [1, 15]. Hence, the modelling
of such complex systems requires a dynamical network inference,
where the states of the network are co-dependent. This naturally
leads to the idea of temporal consistency, which assumes similarities
between consecutive states of the network. In fact, we can assume
that, for sufficiently close time points, a system shows negligible
differences. During the inference of a dynamical network, temporal
consistency may translate into the imposition of similarities among
temporally close networks [16]. In particular, graphical lasso with
temporal consistency results in time-varying graphical lasso [17]:

minimize
(Θ1, . . ., ΘT )

T
(cid:213)

(cid:20)

i =1

− ℓ(Si, Θi ) + α ∥Θi ∥od, 1

+ β

Ψ(Θi +1 − Θi )

(4)

(cid:21)

T −1
(cid:213)

i =1

i j | · |.

where the inference of a network at a single time point i is guided
by the states at adjacent time points. The network is encoded in
a sequence of precision matrices (Θ1, . . . , ΘT ) which model the
system at each time point i = 1, . . . ,T . The type of similarity
imposed to consecutive time points and its strength are specified by
the penalty function Ψ and the parameter β, respectively. Options
when choosing Ψ include the following [17]:
• Lasso penalty (ℓ1) - Ψ = (cid:205)
Encourages few edges to change between subsequent time points,
while the rest of the structure remains the same [12].
• Group lasso penalty - Ψ = (cid:205)
j ∥ ·j ∥2.
Encourages the graph to restructure at some time points and to
stay stable in others [16? ].
• Laplacian penalty - Ψ = (cid:205)
i j (·i j )2.
Encourages smooth transitions over time, for slow changes of the
global structure [? ].
• Max norm penalty (ℓ∞) - Ψ = (cid:205)
Encourages a block of nodes to change their structure with no
additional penalty with respect to the change of a single edge among
such nodes. In fact, ℓ∞ norm is influenced only from the most
changing element for each row.
• Row-column overlap penalty - Ψ = minV :A=V +V ⊤ (cid:205)
Encourages a major change of the network at a specific time, while
in the rest the network is enforced to remain constant. The choice
of p = 2 causes the penalty to be node-based, i.e., the penalty allows
for a perturbation of only some nodes [? ].

j (maxi | ·i j |).

j ∥Vj ∥p .

Dependently from prior assumptions on the problem, one may
choose the most appropriate penalty for the data at hand.

3 MODEL FORMULATION
In this work we propose a new statistical model for the inference
of networks that change consistently in time under the influence
of latent factors. We call such model latent variable time-varying
graphical lasso (LTGL). LTGL infers the dynamical network of com-
plex systems by decomposing the problem into two parts — simi-
larly to what has been done in [8] for static network inference. We
consider two components of the dynamical network: a true under-
lying structure on the observed variables and the contribution of
latent factors. This allows to factor out the contribution of hidden
variables, favouring a reliable modelling of the dynamical system.
The novelty of our method is the simultaneous inference of a dy-
namical network with latent factors that exploits the imposition of
behavioural consistency on both observed variables interactions
and latent influence through the use of penalisation terms. This
allows for an easier interpretation of the evolution of the dynamical
system while, at the same time, improving its graphical modelling.
The two separate (while closely related) components at each time
point are obtained by integrating the network inference with the
information coming from temporally different states of the network.
Formally, let Xi ∈ Rni ×d , for i = 1, . . . ,T , be a set of observations
measured at T different time points composed by ni samples of d
observed variables. (Note that, for each time point i, samples are
assumed to be drawn from the probability distribution on the ob-
served variables conditioned on the latent ones.) Let Si = 1
i Xi
ni
be the empirical covariance matrix at time i. The goal is to retrieve a
set of sparse matrices Θ = (Θ1, . . . , ΘT ) and a set of low-rank matri-
ces L = (L1, . . . , LT ) such that, at each time point i, Θi encodes the
conditional independences between the observed variables, while
Li provides the summary of marginalisation over latent variables
on the observed ones.
Consider Equation (3) at a specific time i. Here we want to im-
pose continuity between the structure and the hidden variables
contribution in time, therefore we enforce the difference between
consecutive graphs to abide certain constraints by adding two pe-
nalisation terms. Our LTGL model takes the following form:

X ⊤

minimize
(Θ, L)
Li ≽0

T
(cid:213)

(cid:20)

i =1

+ β

T −1
(cid:213)

i =1

− ℓ(Si, Θi − Li ) + α ∥Θi ∥od, 1 + τ ∥Li ∥∗

(cid:21)

Ψ(Θi +1 − Θi ) + η

Φ(Li +1 − Li ),

T −1
(cid:213)

i =1

where Ψ and Φ are both penalty functions that force the structure of
the network to change over time according to a certain behaviour,
by acting on Θ and L, respectively. Temporal consistency of both
the structure of the network and latent factors contribution is guar-
anteed by the use of such penalty functions, which benefits the
network inference in particular in presence of few available ob-
servations of the system. Possible choices for Ψ and Φ are listed
in Section 2. Their choice is arbitrary and it is based on the prior
knowledge on the respective components evolution in the system.
Also, note that Ψ and Φ are independent, which allows LTGL to
model a wide range of dynamical behaviours of complex systems.

4 MINIMISATION METHOD
Problem (5) is convex, provided that the penalty functions Ψ and
Φ are convex, and it is coercive because of the regularisers. Thus,

Problem (5) admits solutions. Nonetheless, its optimisation is chal-
lenging in practice due to the high number of unknown matrices
involved (2T , for a total of 2T d (d +1)
unknowns of the problem). A
2
suitable method for the minimisation is ADMM [7]. It allows to
decouple the variables obtaining a separable minimisation problem
which can be efficiently solved in parallel. The sub-problems ex-
ploit proximal operators which are (mostly) solvable in closed-form,
leading to a simple iterative algorithm.
In order to decouple the involved matrices, we define three dual
variables R, Z = (Z1, Z2) and W = (W1,W2) and two projections:

P1 : (Rd ×d )T → (Rd ×d )T −1

P2 : (Rd ×d )T → (Rd ×d )T −1

A (cid:55)→ (A1, . . . , AT −1)

A (cid:55)→ (A2, . . . , AT )

Problem (5) becomes:

minimize
(Θ, L, R, Z ,W )
Li ≽0

T
(cid:213)

(cid:20)

i =1

+ β

T −1
(cid:213)

i =1

− ℓ(Si, Ri ) + α ∥Θi ∥od, 1 + τ ∥Li ∥∗

(cid:21)

Ψ(Z2,i − Z1,i ) + η

Φ(W2,i − W1, i )

(6)

T −1
(cid:213)

i =1

s.t. R = Θ − L, Z1 = P1Θ, Z2 = P2Θ, W1 = P1L, W2 = P2L.

The corresponding augmented Lagrangian is as follows:
Lρ (Θ, L, R, Z, W, U)

− ℓ(Si, Ri ) + α ∥Θi ∥od, 1 + τ ∥Li ∥∗ + I(L ≽ 0)

(cid:21)






+β

Ψ(Z2,i − Z1,i ) + η

Φ(W2,i − W1,i )

T −1
(cid:213)

i =1

∥Ri −Θi +Li +U0,i ∥2 − ∥U0,i ∥2

(cid:21)

∥Θi −Z1,i +U1,i ∥2 − ∥U1,i ∥2 + ∥Θi +1 −Z2,i +U2,i ∥2 − ∥U2,i ∥2

∥Li −W1,i +U3,i ∥2 − ∥U3,i ∥2 + ∥Li +1 −W2,i +U4,i ∥2 − ∥U4,i ∥2

T
(cid:213)

(cid:20)

=

i =1
T −1
(cid:213)

i =1
T
(cid:213)

(cid:20)

i =1
T −1
(cid:213)

(cid:20)

i =1
T −1
(cid:213)

(cid:20)

i =1

+ ρ
2

+ ρ
2

+ ρ
2

(5)

where U = (U0, U1, U2, U3, U4) are the scaled dual variables.
The ADMM algorithm for Problem (6) writes down as follows:

for k = 1, . . .

Rk +1 = argmin

Lρ (Θk, Lk, R, Zk, Wk, Uk )

Θk +1 = argmin

Lρ (Θ, Lk, Rk +1, Zk, Wk, Uk )

R

Θ

Lk +1 = argmin

Lρ (Θk +1, L, Rk +1, Zk, Wk, Uk )

Zk +1 =

Wk +1 =

Uk +1 =

(cid:21)

(cid:21)

+

L
(cid:20) Z k +1
1
Z k +1
2
(cid:20) W k +1
1
W k +1
2
U k
0
U k
1
U k
2
U k
3
U k
4























= argmin
Z

= argmin
W

Lρ (Θk +1, Lk +1, Rk +1, Z, Wk, Uk )

Lρ (Θk +1, Lk +1, Rk +1, Zk +1, W, Uk )

Rk +1 − Θk +1 + Lk +1
P1Θk +1 − Z k +1
P2Θk +1 − Z k +1
P1Lk +1 − W k +1
P2Lk +1 − W k +1

1

1

2

2












.












(cid:21)

(cid:21)

(7)

(8)

4.1 R step
The minimisation problem involving the matrix R in (8) can be
split into parallel updates, since Lρ (Θ, L, R, Z, W, U) is separable
in the variables (R1, . . . , RT ). Therefore, each Ri at iteration k + 1
is given by:
Rk +1
i

∥R − Θk + Lk + U k

0, i ∥2

= argmin
R

= argmin
R

tr(Si R) − log det(R) + ρ
2
tr(Si R) − log det(R) + ρ
2

= argmin
R

tr(Si R) − log det(R) + ρ
2

(cid:13)
(cid:13)
(cid:13)

2

F

i

(cid:13)
(cid:13)R − Ak
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

R −

Ak
i

+ Ak ⊤
i
2

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F

− U k

= Θk
i

− Lk
i

with Ak
0,i . Note that the last equality in (9) follows
i
from the symmetry of R — which also guarantees the log det to be
well-defined. Equation (9) can be explicitly solved. Indeed, Fermat’s
rule yields:

Si − ρ

Ak
i

+ Ak ⊤
i
2

= R−1 − ρR .

Then the solution to Equation (10) is [12, 17, 32]:
(cid:19)

(cid:18)

(cid:113)

V k

−Ek +

(Ek )2 + 4ρI

V k ⊤

Rk +1
i

= 1
2ρ

where V k EkV k ⊤ is the eigenvalue decomposition of Si − ρ

Ak
i

+Ak ⊤
i
2

.

4.2 Θ step
Likewise the R step, the update of Θ in (8) can be done in a parallel
fashion, as follows:

Θk +1
i

= argmin
Θ

α ∥Θ ∥od, 1 + ρ
2

(cid:20) (cid:13)
(cid:13)Rk
(cid:13)

i − Θ + Lk
i

+ U k
0, i

(cid:13)
(cid:13)
(cid:13)

2

F

+ δ iT

(cid:13)
(cid:13)Θ − Z k
(cid:13)

1,i

+ U k
1, i

+ δ i 1

(cid:13)
(cid:13)
(cid:13)

2

F

2,i −1

+ U k

2,i −1

= argmin
Θ

α ∥Θ ∥od, 1 + (1 + δ iT + δ i 1)

(cid:13)
(cid:13)Θ − Z k
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)Θ − Bk
(cid:13)

i

2

F

ρ
2

where δ i j = 1 − δi j , with δi j Kronecker delta and
1,i ) + δ i 1(Z k

+ δ iT (Z k

+ U k
0,i

+ Rk
i

Lk
i

Bk
i

=

1,i − U k
1 + δ iT + δ i 1

2,i −1 − U k

2,i −1)

.

Problem (11) is solved as:

Θk +1
i

(Bk

= proxζ ∥·∥od, 1
, and Sζ (·) element-wise off-diagonal soft-

i ) = Sζ (Bk
i )

with ζ =

α
ρ(1+δ iT +δ i 1)
thresholding function.

(9)

(10)

(cid:21)

(cid:13)
(cid:13)
(cid:13)

2

F

(11)

4.3 L step
The parallel update of L in (8) can be written as:
τ tr(L) + I(L ≽ 0) + ρ
2

i − Θk +1

= argmin
L

Lk +1
i

i

(cid:20) (cid:13)
(cid:13)Rk +1
(cid:13)
(cid:13)
(cid:13)L − W k
(cid:13)

+ δ iT

(cid:13)
(cid:13)L − W k
(cid:13)

1,i

+ U k
3,i

+ δ i 1

(cid:13)
(cid:13)
(cid:13)

2

F

2,i −1

+ U k

4, i −1

τ tr(L) + I(L ≽ 0) + (1 + δ iT + δ i 1)

= argmin
L

= argmin
L

τ tr(L) + I(L ≽ 0) + (1 + δ iT + δ i 1)

+ L + U k
i, 0

2

F

(cid:13)
(cid:13)
(cid:13)
(cid:21)

2

(cid:13)
(cid:13)
(cid:13)

F
(cid:13)
(cid:13)
(cid:13)

2

F

ρ
2

ρ
2

i

(cid:13)
(cid:13)L − C k
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

L −

C k
i

+ C k ⊤
i
2

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(12)

where

C k
i

=

i − Rk +1
Θk +1

i − U k
0,i

+ δ iT (W1,i − U3,i ) + δ i 1(W2,i −1 − U4,i −1)

.

1 + δ iT + δ i 1

Note that the last equality in (12) follows from the symmetry of L.
Then, the solution to Problem (12) is [24]:
= V k ˜EV k ⊤

Lk +1
i
where V k EkV k⊤ is the eigenvalue decomposition of Ck

i , and

(cid:32)

˜Ej j = max

Ek
j j −

τ
ρ(1 + δ iT + δ i1)

(cid:33)

, 0

.

4.4 Z and W step
The dual variables Z and W enforce the network to behave in time
consistently with the choice of Ψ and Φ, respectively. Z is the dual
variable of Θ while W is the dual variable of L. For the sake of
brevity, we show only the steps regarding the update of Z — the
update of W is analogous. The dual variable Z is defined as (Z1, Z2).
Such matrices are not separable in Equation (6), thus they must be
jointly updated. The update of Z in (8) can be rewritten as follows:

(cid:35)

(cid:34)

Z k +1
1,i
Z k +1
2,i

= argmin
Z1, Z2

Let ˆΨ

(cid:21)

(cid:20) Z1
Z2

an unique update [17]:

β Ψ(Z2 − Z1) + ρ
2
+ ρ
2

∥Θk

i − Z1 + X k

1,i ∥2

∥Θk

i +1 − Z2 + X k

2,i ∥2 .

(13)

= Ψ(Z2 − Z1). Then, Problem (13) can be solved with

(cid:21)

(cid:20) Z k +1
1,i
Z k +1
2,i

= prox β
ρ

ˆΨ(·)

(cid:18) (cid:20) Θk
i
Θk
i +1

+ U k
1,i
+ U k
2,i

(cid:21) (cid:19)

.

The same holds for the W step. Hence, the proximal operator for
the update of W1,i and W2,i becomes:
(cid:20) W k +1
1,i
W k +1
2,i

(cid:18) (cid:20) Lk
i
Lk
i +1

+ U k
i, 3
+ U k
i, 4

= prox η
ρ

ˆΦ(·)

(cid:21) (cid:19)

(cid:21)

.

For the particular derivation of different proximal operators,
see [17].

4.5 Termination criterion
According to [7], the algorithm is said to converge if the primal
and dual residuals are sufficiently small, i.e. if ∥r k ∥2
2 ≤ ϵ pri and
∥sk ∥2
2 ≤ ϵ dual. At each iteration k these values are computed as
follows:

∥r k ∥2
2

∥s k ∥2
2

= ∥R k − Θk + Lk ∥2
1 ∥2
+ ∥P1Θk − Z k
F
F
1 ∥2
2 ∥2
+ ∥P2Lk − W k
+ ∥P1Lk − W k
F
F
= ρ (cid:0) ∥R k − R k −1 ∥2
1 − Z k −1
∥2
+ ∥Z k
+ ∥Z k
1
F
F
(cid:1)
2 − W k −1
1 − W k −1
∥2
∥2
2
1
F
F
ϵ pri = c + ϵ rel max(cid:0)D1, D2
ϵ dual = c + ϵ relρ (cid:0) ∥U k

+ ∥W k
(cid:1)

+ ∥W k

+ ∥U k

+ ∥P2Θk − Z k

1 ∥2
F

2 − Z k −1
2

∥2
F

0 ∥2
F

+ ∥U k

1 ∥2
F
where c = ϵ absd(5T − 4)1/2, ϵ abs and ϵ rel are arbitrary tolerance
+ ∥W k
+ ∥Z k
+ ∥W k
∥2
∥2
∥2
∥2
parameters, Dk
1
1
2
2
F
F
F
F
+ ∥P1Θk ∥2
= ∥Θk −Lk ∥2
+ ∥P1Lk ∥2
+ ∥P2Lk ∥2
and Dk
F .
2
F
F
F

+ ∥Z k
1
+ ∥P2Θk ∥2
F

= ∥Rk ∥2
F

+ ∥U k

+ ∥U k

4 ∥2
F

2 ∥2
F

3 ∥2
F

(cid:1)

4.6 Implementation
The minimisation algorithm is available as a Python framework1,
fully compatible with the scikit-learn library of machine learning
algorithms, providing a straightforward and intuitive interface. The
implementation relies on low-level high-performance libraries for
numerical computations and it exploits closed-form solutions for
proximal operators, leading to a fast and scalable minimisation
algorithm even with an increasing number of unknowns.

5 EXPERIMENTS
We performed experiments on synthetic data assessing the perfor-
mance of the method in terms of structure recovery and measure of
latent variables influence. The performance of LTGL was evaluated
with respect to the ground truth and to state-of-the-art methods
for graphical inference. In particular, we assessed two aspects of
such methods, that are modelling performance and scalability, in
separated experiments. Modelling performance was estimated by
comparing the inferred graphical model to the true network un-
derlying the data set. During the scalability experiment, instead,
we assessed the computational time for convergence needed for
increasing problem complexity.

5.1 Modelling performance
We evaluated LTGL modelling performance on two synthetic data
sets. The ground truth sets of matrices Θ = (Θ1, . . . , ΘT ) and
L = (L1, . . . , LT ) were obtained by perturbing initial matrices Θ1
and L1, according to a specific behaviour for T − 1 times, with the
guarantee that Θi − Li ≻ 0 and Li ≽ 0 for i = 1, . . . ,T . The initial
matrices were generated according to [33], following the form (2).
Θ1 and L1 correspond to ΘO and ΘO H Θ−1
H ΘH O , respectively, with
ΘH identity matrix and ΘH O = Θ⊤
O H . Note that, since ΘH has full
rank, the number of latent variables is H . In particular, for d ob-
served variables, n samples and T timestamps, we generated a data
set X ∈ (Rn×d )T sampled from T multivariate normal distributions
Xi ∼ Ni (0, Σi ), for i = 1, . . . ,T , and Σ−1
i
ℓ2
2 perturbation (p2). The first data set was generated by
perturbing the initial matrices with a random matrix of small ℓ2
2
norm. This perturbation assumes the differences between two
consecutive matrices to be small and bounded over time, i.e.,
∥Θi − Θi−1 ∥F ≤ ϵ for i = 2, . . . ,T . The bound ϵ on the norm
is chosen a priori.
The update of Li is done maintaining consistency with the theo-
retical model where Li = ΘO H,i Θ⊤
O H,i . Therefore, the update is
obtained by adding a random matrix with a small norm to ΘO H,i−1.
In this way, the rank of Li remains the same as the number of latent
variables and constant over time. Data were generated in R100 with
10 time stamps, conditioned on 20 latent variables. For each time
stamp, we drew 100 samples from the distribution. For this reason,
in this setting, the contribution of latent factors is predominant
with respect to the network evolution in time.

= Θi − Li .

5.1.1

5.1.2

ℓ1 perturbation (p1). A second data set was generated
according to a different perturbation model. Here, the precision
matrix was updated by randomly choosing an edge and swapping

1LTGL is open-source. The
https://github.com/fdtomasi/regain.

code

is

available under BSD-3-Clause

at

Table 1. Performance in terms of F1 score, accuracy (ACC), mean
rank error (MRE) and mean squared error (MSE) of LTGL with re-
spect to TVGL, LVGLASSO and GL. LTGL and TVGL are employed
with both ℓ2
2 and ℓ1 penalties, to show how the prior on the evolu-
tion of the network affects the outcome.

perturbation method

score

F1

ACC MRE MSE

ℓ2
2 (p2)

ℓ1 (p1)

LTGL (ℓ2
0.926
2)
LTGL (ℓ1)
0.898
TVGL (ℓ2
0.791
2)
TVGL (ℓ1)
0.791
LVGLASSO 0.815
GL
0.745
LTGL (ℓ2
2)
0.842
LTGL (ℓ1)
0.880
TVGL (ℓ2
2)
0.742
TVGL (ℓ1)
0.817
LVGLASSO 0.752
0.748
GL

0.994
0.993
0.980
0.980
0.988
0.974

0.974
0.981
0.950
0.968
0.964
0.951

0.70
0.70
-
-
2.80
-

0.29
0.28
-
-
0.74
-

0.007
0.007
0.003
0.003
0.007
0.004

0.013
0.013
0.009
0.009
0.013
0.007

Figure 2. Distribution of inferred ranks over time. For each method
that considers latent variables, we show the frequency of finding
a specific rank during the network inference. The vertical line in-
dicates the ground truth rank, around which all detected ranks lie.
Note that, in (p2), Li ∈ R100×100, therefore the range of possible ranks
is [0, 100]. For (p2), Li ∈ R50×50, hence the range is [0, 50].

its state, i.e., by removing or adding a connection between two
variables. This allows for a ℓ1-norm evolutionary pattern of the
network. Data were generated in R50 with 100 time stamps, con-
ditioned on 5 latent variables. For each time stamp, we drew 100
samples from the distribution. In this setting, the time consistency
affects the network more than the latent factor contribution.

5.1.3

Scores. We evaluated LTGL performance using different
scores measuring the divergence of the results from the ground
truth. In particular, the performance was evaluated in terms of F1
score, accuracy, mean rank error and mean squared error. We define
as true/false positive the number of correctly/incorrectly existing in-
ferred edges, true/false negative the number of correctly/incorrectly
missing inferred edges [18]. The scores are computed as follows.
• F1 score: indicates the quality of structure inference, as the har-
monic mean of precision and recall.
• Accuracy (ACC): evaluates the number of true existing and missing
connections in the network correctly inferred with respect to the
total number of connections.

Figure 3. Scalability comparison for LTGL in relation to other ADMM-based methods. The compared methods are initialised in the same
manner, i.e., with all variable interactions (not self-interacting) set to zero. For LVGLASSO and TVGL, we used their relative original imple-
mentations. Also, we ignore the computational time required for hyperparameters selection. LTGL outperforms the other methods for each
increasing time and dimensionality of the problem.

• Mean rank error (MRE): estimates the precision on the number of
inferred latent variables, based on the rank of the set of matrices ˜L
in relation to the ground truth. The MRE score is defined as:

MRE = 1
T

T
(cid:213)

i =1

(cid:12)rank(Li ) − rank( ˜Li )(cid:12)
(cid:12)
(cid:12).

A value close to 0 means that we are inferring the true number of
latent variables over time, while, viceversa, a high value indicates a
poor consideration of the contribution of the latent variables.
• Mean squared error (MSE): score how close is the inferred precision
matrix ˜Θ to the ground truth, in terms of the Frobenius norm:

MSE =

2
T d(d − 1)

T
(cid:213)

i =1

(cid:13)
(cid:13)
(cid:13)Θ

(u)
(u)
i − ˜Θ
i

(cid:13)
(cid:13)
(cid:13)F

,

where Θ(u) denotes the upper triangular part of Θ.

5.1.4 Discussion. Table 1 shows the performance of LTGL com-
pared to graphical lasso (GL) [15], latent variable graphical lasso
(LVGLASSO) [8, 24] and time-varying graphical lasso (TVGL) [17]
in terms of F1 score, accuracy, mean rank error (MRE) and mean
squared error (MSE), for both settings with ℓ2
2 (p2) and ℓ1 (p1) per-
turbation. Note that MRE is not available for all the methods since
neither GL or TVGL consider latent factors. LTGL and TVGL are
used with two temporal penalties according to the different per-
turbation models of data generation. In this way, we show how
the correct choice of the penalty for the problem at hand results in
a more accurate network estimation. In both (p2) and (p1), LTGL
outperforms the other methods for graphical modelling. In (p2), in
particular, LTGL correctly infers almost 99,5% of edges in all the dy-
namical network both with the ℓ2
2 and ℓ1 penalties. Nonetheless, the
use of ℓ2
2 penalty enhance the quality of the inference as expected
from the theoretical assumption made during data generation. The
choice of a proper penalty for the problem and the consideration
of time consistency is reflected also in a low MRE, which encom-
passes LVGLASSO ability in detecting latent factors (Figure 2). In
(p2), in fact, the number of latent variables with respect to both
observed variables and samples is high. Therefore, by exploiting
temporal consistency of the network, LTGL is able to improve the
latent factors estimation. Simultaneous consideration of time and
latent variable also positively influences the F1 score, i.e., structure
detection.
Above considerations also hold for the (p1) setting. Here, LTGL
achieves the best results in both F1 score and accuracy, while hav-
ing a low MRE. The adoption of ℓ1 penalty improves structure
estimation and latent factors detection, consistently with the data
generation model. Such settings were designed to show how the

prevalence of latent factors contribution or time consistency af-
fects the outcome of a network inference method. In (p2), where
the latent factors contribution is prevalent, network inference is
more precise when considering latent factors. In (p1), instead, the
number of time points is more relevant than the contribution of
latent factors, hence it is more effective to exploit time consistency
(both for latent and observed variables), evident from the results
of Table 1. LTGL benefits from both aspects, therefore leading to a
noticeable improvement of graphical modelling.

5.2 Scalability
Next, we performed a scalability analysis using LTGL with respect
to different ADMM-based solvers. We evaluated the performance
of our method in relation to LVGLASSO [24] and TVGL [17], both
implemented with closed-form solutions to ADMM subproblems.
In general, the complexity of the three compared solvers is the
same (up to a constant). The implementation of GL [15] was not
included in such experiment, since it is not based on ADMM but on
coordinate descent, and therefore it is not comparable to our method.
As in Section 5.1, we generated different data sets X ∈ (Rn×d )T
with different values of T and d. In particular, d ∈ [10, 400) and
T = {20, 50, 100}. We ran our experiments on a machine provided
with two CPUs (2.4 GHz, 8 cores each).
Figure 3 shows, for the three different time settings, the scalability
of the methods in terms of seconds per convergence considering
different number of unknowns of the problem (i.e., 2T d (d +1)
2 with d
observed variables and T times). In all settings, LTGL outperforms
LVGLASSO and TVGL in terms of seconds per convergence. In
particular, the computational time for convergence remains stable
disregarding the number of time points under consideration. We
emphasise that the most computationally expensive task performed
by our solver is represented by two eigenvalue decompositions,
with a complexity of O(d3), to solve both R and L steps (Section 4).

5.3 Model selection
The hyperparameters of the method have been selected by using a
cross-validation procedure. In particular, we used the Monte Carlo
Cross-Validation (MCCV) [27] that repeatedly splits the n samples
of the data set in two mutually exclusive sets. For each split, n · (1/ν )
samples are labelled as validation set and the remaining n · (1 − 1/ν )
as learning set. For each hyperparameter combination, the model
was trained on the learning set and the likelihood of the model
was estimated on the independent test set. Finally, we selected the
combination of hyperparameters based on the average maximum
likelihood of the model across multiple splits of the data set.

However, the number of possible combinations of LTGL hyperpa-
rameters can be arbitrarily large. In order to avoid the assessment
of a grid of models (which can be computationally expensive), we
used a Gaussian process-based Bayesian optimisation procedure to
choose the best combination of hyperparameters for each analysed
data set, based on the Expected Improvement (EI) strategy [31]. In
practice, assuming the dynamics of a real system to be unknown,
it is possible to select the most appropriate temporal penalty by
exploiting the same principles, i.e., via a model selection procedure
based on the likelihood of different temporal models.

6 APPLICATIONS TO REAL DATA
We applied LTGL to two real data sets, to show how the method
can be employed to infer useful insights on multivariate time-series
data. These data sets measure complex dynamical systems of dif-
ferent (biological and financial) nature, which are usually highly
dimensional and feature complicated interdependences between
variables. This fact makes them ideal candidates for an analysis
using graphical models.

6.1 Metabolomic Data
The physiology of Escherichia coli necessitates rapid changes of
its cellular and molecular network to adapt to environmental con-
ditions. E. coli is widely studied because of the efficiency in its
system response to perturbation. Following the analysis of [21], we
used LTGL on E. coli data to infer network modifications across
different time points evaluated before and after the application of
environmental condition perturbations. We analysed the behaviour
of metabolites, which have been shown to change consistently after
the perturbation. Samples underwent one of two types of stress,
namely cold and heat stress.
Perturbation response detection. We inferred the dynamical
network of E. coli metabolites using LTGL with a group lasso (ℓ2)
penalty on latent variable contribution and a Laplacian (ℓ2
2) penalty
on the observed network. In this way, we allow the latent variables
(which, in our model, could represent the stress or other factors)
to change their global influence at a specific time point, while re-
maining stable in all others. At the same time, by conditioning
the network on the latent variables, we allow the observed net-
work structure to change smoothly in time. Hence, we expect to
see a global shift of the network between the second and third
time points, that is when the perturbation has been introduced in
the system. Figure 4a shows the temporal deviation between time
points, both for Θ, L and the total observed system R = Θ − L.
Latent variables temporal deviation reaches a peak at time t2−3,
right after the application of the perturbation to the system. Instead,
the difference between consecutive Θs remains more stable. Con-
sistently, the difference between the observed networks Rs shows
a major change at the same time point. Hence we can distinguish
the underlying evolving structure of metabolites while detecting
the contribution of the latent variables which affect mostly the
total system. In accordance with [21], we observed a interaction
between isoleucine, threonine, phenylalanine and 2-aminobutyric
acid during the adaptation phase following the stress response (Fig-
ure 4b). Therefore, we can conclude that LTGL successfully inferred
a dynamical network which adjusts in response to perturbation, in
accordance with our prior knowledge about E. coli behaviour.

Figure 4. Structure change of E. coli metabolites subject to stress.
The perturbation happens between time t = 2 and t = 3 (ver-
tical dotted line). (a) Temporal deviation where each point repre-
sents the difference between the network at subsequent time points.
The highest deviation on the observed network R appears when the
stress was applied. This can be decomposed into two parts, the la-
tent factors L and the underlying structure of observed variables Θ.
(b) Structural changes of metabolites interactions before and after
the perturbation.

6.2 Stock market
Finance is another example of a complex dynamical system suitable
to be analysed through a graphical model. Stock prices, in particular,
are highly related to each other and subject to time and environ-
mental changes, i.e., events that modify the system behaviour but
are not directly related to companies share values [3]. Here, the
assumption is that each company, while being part of a global finan-
cial system, is directly dependent from only a subset of others. For
example, it is reasonable to expect that stock prices of a technology
company are not directly influenced by trend of companies on the
primary sector. The modelling power of LTGL allows to detect both
the evolution of relations between companies and environmental
changes happening at a particular point in time. In order to show
this, we analysed stock prices2 during the financial crisis of 2007-
2008. The experiment was designed to consider the latent influence
of the market drop on technology companies interactions.

Global market crisis detection. We used a group lasso (ℓ2)
penalty to detect global shifts of the network. Figure 5 shows two
major changes in both components of the network (latent and ob-
served), in correspondence of late 2007 and late 2008. In particular,
during October 2008 a global crisis of the market occurred, and this
effect is especially evident for the shift of latent variables. Also, the
observed network changes in correspondence of the latent variables
shift or immediately after, caused by the effect of the crisis on the
stock market. The latent factors influence explains how the change
of the network was due to external factors that globally affected the
market, and not to normal evolution of companies relationships.
We further investigated on the causes for the first shift. Indeed,
we found that in late 2007 it happened a drop of a big American
company that was later pointed out as the beginning of the global
crisis of the following year.

2Data are freely available on https://quantquote.com/historical-stock-data.

[7] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. 2010. Distributed Optimiza-
tion and Statistical Learning via the Alternating Direction Method of Multipliers.
Foundations and Trends® in Machine Learning 3, 1 (2010).

[8] V. Chandrasekaran, P. A. Parrilo, and A. S. Willsky. 2010. Latent variable graph-
ical model selection via convex optimization. In Communication, Control, and
Computing (Allerton), 2010 48th Annual Allerton Conference on. IEEE, 1610–1613.
[9] V. Chandrasekaran, S. Sanghavi, P. A. Parrilo, and A. S. Willsky. 2011. Rank-
sparsity incoherence for matrix decomposition. SIAM Journal on Optimization
21, 2 (2011), 572–596.

[10] M. J. Choi, V. Chandrasekaran, and A. S. Willsky. 2010. Gaussian multiresolution
models: Exploiting sparse Markov and covariance structure. IEEE Transactions
on Signal Processing 58, 3 (2010), 1012–1024.

[11] M. J. Choi, V. Y.F. Tan, A. Anandkumar, and A. S. Willsky. 2011. Learning latent

tree graphical models. JMLRh 12, May (2011), 1771–1812.

[12] P. Danaher, P. Wang, and D. M. Witten. 2014. The joint graphical lasso for inverse
covariance estimation across multiple classes. Journal of the Royal Statistical
Society. Series B: Statistical Methodology 76, 2 (2014), 373–397. arXiv:1111.0324
[13] C. Ding, X. He, and H. D. Simon. 2005. On the equivalence of nonnegative matrix
factorization and spectral clustering. In Proceedings of the 2005 SIAM International
Conference on Data Mining. SIAM, 606–610.

[14] A. Farasat, A. Nikolaev, S. N. Srihari, and R. H. Blair. 2015. Probabilistic graphical

models in modern social network analysis. SNAM 5, 1 (2015), 62.

[15] J. Friedman, T. Hastie, and R. Tibshirani. 2008. Sparse inverse covariance estima-

tion with the graphical lasso. Biostatistics 9, 3 (2008), 432–441.

[16] A. J. Gibberd and S. Roy. 2017. Multiple Changepoint Estimation in High-
Dimensional Gaussian Graphical Models. arXiv preprint arXiv:1712.05786 (2017).
[17] D. Hallac, Y. Park, S. Boyd, and J. Leskovec. 2017. Network Inference via the Time-
Varying Graphical Lasso. In Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD ’17). ACM, 205–213.
[18] M. Hecker, S. Lambeck, S. Toepfer, E. Van Someren, and R. Guthke. 2009. Gene
regulatory network inference: Data integration in dynamic models: A review.
Biosystems 96, 1 (2009), 86–103.

[19] L. Huang, L. Liao, and C. H. Wu. 2016. Inference of protein-protein interaction
networks from multiple heterogeneous data. EURASIP Journal on Bioinformatics
and Systems Biology 2016, 1 (2016), 1–9.

[20] A. Jalali and S. Sanghavi. 2011. Learning the dependence graph of time series

with latent factors. arXiv preprint arXiv:1106.1887 (2011).

[21] S. Jozefczuk, S. Klie, G. Catchpole, J. Szymanski, A. Cuadros-Inostroza, D. Stein-
hauser, J. Selbig, and L. Willmitzer. 2010. Metabolomic and transcriptomic stress
response of Escherichia coli. Molecular systems biology 6, 1 (2010), 364.

[22] S. L. Lauritzen. 1996. Graphical models. Vol. 17. Clarendon Press.
[23] H. Liu, F. Han, and C. Zhang. 2012. Transelliptical graphical models. In NIPS.

800–808.

[24] S. Ma, L. Xue, and H. Zou. 2013. Alternating Direction Methods for Latent
Variable Gaussian Graphical Model Selection. Neural Computation 25, 8 (aug
2013), 2172–2198.

[25] N. Meinshausen and P. Bühlmann. 2006. High-dimensional graphs and variable

selection with the lasso. The annals of statistics (2006), 1436–1462.

[26] Z. Meng, B. Eriksson, and A. Hero. 2014. Learning latent variable Gaussian

graphical models. In Proceedings of the 31st ICML. 1269–1277.

[27] A. M. Molinaro, R. Simon, and R. M. Pfeiffer. 2005. Prediction error estimation: a
comparison of resampling methods. Bioinformatics 21, 15 (2005), 3301–3307.
[28] E. J. Molinelli, A. Korkut, W. Wang, M. L. Miller, N. P. Gauthier, X. Jing, P. Kaushik,
Q. He, G. Mills, D. B. Solit, et al. 2013. Perturbation biology: inferring signaling
networks in cellular systems. PLoS computational biology 9, 12 (2013), e1003290.
[29] P. Orchard, F. Agakov, and A. Storkey. 2013. Bayesian inference in sparse Gaussian

graphical models. arXiv preprint arXiv:1309.7311 (2013).

[30] P. Ravikumar, M.J. Wainwright, G. Raskutti, B. Yu, et al. 2011. High-dimensional
covariance estimation by minimizing ℓ1s-penalized log-determinant divergence.
Electronic Journal of Statistics 5 (2011), 935–980.

[31] J. Snoek, H. Larochelle, and R. P. Adams. 2012. Practical Bayesian optimization

of machine learning algorithms. In NIPS. 2951–2959.

[32] D. M. Witten and R. Tibshirani. 2009. Covariance-regularized regression and
classification for high dimensional problems. Journal of the Royal Statistical
Society. Series B: Statistical Methodology 71, 3 (jun 2009), 615–636.

[33] M. Yuan. 2012. Discussion: Latent variable graphical model selection via convex

optimization. Ann. Statist. 40, 4 (08 2012), 1968–1972.

graphical model. Biometrika (2007), 19–35.

Figure 5. Temporal deviation for stock market data. We observed
two peaks, in correspondence of late 2007 and late 2008, when the
financial crisis happened.

7 CONCLUSIONS AND FUTURE WORK
In this work, we developed a novel method for graphical modelling
of multivariate time-series. The model considers simultaneously
the contribution of latent factors and time consistency in evolving
systems. Indeed, our work is an attempt to generalise both latent
variable and dynamical network inference. To this aim, we impose
prior knowledge on the problem through penalty terms that force
precision and latent matrices to be consistent in time. The choice
of proper penalty terms maintains the convexity of the minimised
functional and, along with the coercivity given by the regularisers,
it guarantees global convergence of the proposed minimisation
algorithm. Our experiments demonstrate the ability of LTGL in
the graphical modelling of synthetic and real-world data, where
the possibility to decompose the total network into two separated
components allows for a better understanding of the underlying
phenomenon.
We emphasise that our framework is modular in the choice of the
penalties, allowing for precise modelling of different and complex
behaviours of the system. This allows for a straightforward inclu-
sion of additional penalty terms, based on the prior knowledge on
the problem at hand. Possible extensions may involve alternative
evolutionary models for different complex systems, e.g. forcing sub-
groups of variables to behave consistently in time [6]. These could
lead to interesting results in time-series clustering and pattern dis-
covery. Further investigations may also head to the inference of the
exact contribution of latent factors starting from the L matrices we
are estimating, possibly using matrix factorisation methods [13].
Such developments may increase the expression power of the
method, leading to advances in data mining and to potential appli-
cations in diverse science fields.

Acknowledgments. The authors thank the anonymous reviewers
for their valuable comments.

REFERENCES
[1] R. Albert. 2007. Network inference, analysis, and modeling in systems biology.

The Plant cell 19, 11 (nov 2007), 3327–38.

bayesian networks with latent variables. In ICML. 249–257.

[3] J. Bai and S. Ng. 2006. Evaluating latent and observed factors in macroeconomics

and finance. Journal of Econometrics 131, 1-2 (2006), 507–537.

[4] E. Bianco-Martinez, N. Rubido, Ch. G. Antonopoulos, and M.S. Baptista. 2016.
Successful network inference from time-series data using mutual information
rate. Chaos: An Interdisciplinary Journal of Nonlinear Science 26, 4 (2016), 043102.
[5] J. Bien and R. J. Tibshirani. 2011. Sparse estimation of a covariance matrix.

Biometrika 98, 4 (dec 2011), 807–820.

[6] A. Bolstad, B. D. Van Veen, and R. Nowak. 2011. Causal network inference via
group sparse regularization. IEEE transactions on signal processing 59, 6 (2011).

[2] A. Anandkumar, D. Hsu, A. Javanmard, and S. Kakade. 2013. Learning linear

[34] M. Yuan and Y. Lin. 2007. Model selection and estimation in the Gaussian

8
1
0
2
 
g
u
A
 
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
7
8
9
3
0
.
2
0
8
1
:
v
i
X
r
a

Latent Variable Time-varying Network Inference

Federico Tomasi∗
Università degli Studi di Genova
16146 Genova, Italy
federico.tomasi@dibris.unige.it

Saverio Salzo
Istituto Italiano di Tecnologia
16163 Genova, Italy
saverio.salzo@iit.it

Veronica Tozzo∗
Università degli Studi di Genova
16146 Genova, Italy
veronica.tozzo@dibris.unige.it

Alessandro Verri
Università degli Studi di Genova
16146 Genova, Italy
alessandro.verri@unige.it

ABSTRACT
In many applications of finance, biology and sociology, complex
systems involve entities interacting with each other. These pro-
cesses have the peculiarity of evolving over time and of comprising
latent factors, which influence the system without being explicitly
measured. In this work we present latent variable time-varying
graphical lasso (LTGL), a method for multivariate time-series graph-
ical modelling that considers the influence of hidden or unmea-
surable factors. The estimation of the contribution of the latent
factors is embedded in the model which produces both sparse and
low-rank components for each time point. In particular, the first
component represents the connectivity structure of observable vari-
ables of the system, while the second represents the influence of
hidden factors, assumed to be few with respect to the observed
variables. Our model includes temporal consistency on both com-
ponents, providing an accurate evolutionary pattern of the system.
We derive a tractable optimisation algorithm based on alternating
direction method of multipliers, and develop a scalable and effi-
cient implementation which exploits proximity operators in closed
form. LTGL is extensively validated on synthetic data, achieving
optimal performance in terms of accuracy, structure learning and
scalability with respect to ground truth and state-of-the-art meth-
ods for graphical inference. We conclude with the application of
LTGL to real case studies, from biology and finance, to illustrate
how our method can be successfully employed to gain insights on
multivariate time-series data.

KEYWORDS
network inference; graphical models; latent variables; time-series;
convex optimization

ACM Reference Format:
Federico Tomasi, Veronica Tozzo, Saverio Salzo, and Alessandro Verri. 2018.
Latent Variable Time-varying Network Inference. In KDD ’18: The 24th ACM

∗These authors equally contributed to the paper.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
KDD ’18, August 19–23, 2018, London, United Kingdom
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5552-0/18/08. . . $15.00
https://doi.org/10.1145/3219819.3220121

SIGKDD International Conference on Knowledge Discovery & Data Mining,
August 19–23, 2018, London, United Kingdom. ACM, New York, NY, USA,
9 pages. https://doi.org/10.1145/3219819.3220121

1 INTRODUCTION
The problem of understanding complex systems arises in many
diverse contexts, such as financial markets [23, 29], social networks
[14] and biology [18, 19]. In such contexts, the goal is to analyse the
system in order to retrieve information on how the components be-
have. This requires accurate and interpretable mathematical models
whose parameters, in practice, need to be estimated from observa-
tions.
Mathematically, a system can be modelled as a network of interac-
tions (edges) between its entities (nodes). However, the underlying
structure of the variables within the system is usually not known a
priori. Nevertheless, observations of the system (i.e., data) incorpo-
rate information on the interactions between variables, since they
provide measurements of such variables acting in the system.
The problem of inferring a network of variable interactions from
data is known as network inference or graphical model selection
[15, 22]. During the last years, the graphical modelling problem
has received much attention, particularly for the availability of
an always increasing number of samples that are required for a
reliable network inference. Nonetheless, structure estimation of
complex systems remains challenging for many reasons. In this
work, we want to tackle two aspects: (i) the presence of global
hidden (or latent) factors, and (ii) the dynamic of systems that
evolve in time. We argue that the inference of a dynamical network
encoding a complex system requires a specific attention to both
aspects to result in a more realistic representation. In particular, a
system may be affected by (latent) factors not encoded in the model.
Such factors, acting in the system, influence how the observable
entities behave and, hence, how they are connected with each other
[10]. The consideration of hidden and unmeasured variables during
the inference process emerges as crucial to avoid misrepresenting
real-world data [26].
At the same time, a complex system depends on a temporal com-
ponent, which drives variable interactions to evolve consistently
during its extent. This means that the structure can either change
or remain stable according to the nature of the system itself. Hence,
the understanding of a complex system is bound to the observation
of its evolution. This is particularly evident in some applications,
such as biology, where the interest could be to understand the
response of the system to perturbation [28].

Related work. Latent variable models have been widely studied
in literature, and shown to outperform graphical models that only
consider observable variables [8, 11, 33]. At the same time, a set of
methods were designed to study the temporal component through
the inference of a dynamical network that incorporates prior knowl-
edge on the behaviour of the system [4, 17]. Time-series with latent
variables are considered to obtain a single graph which represents
the global system [2, 20]. However, to the best of our knowledge,
state-of-the-art methods for regularised network inference do not
consider simultaneously time and latent variables in the inference
of multiple connected networks.

Contribution. In this work we propose latent variable time-varying
graphical lasso (LTGL), a model for dynamical network inference
where the structure is influenced by latent factors. This can be
seen as an attempt to generalise both dynamical and latent variable
network inference under a single unified framework. In partic-
ular, starting from a set of observations of a system at different
time points, LTGL infers an interaction network of the observed
variables under the influence of latent factors, while taking in con-
sideration the temporal evolution of the system. The empirical
interaction network is decomposed into the true underlying struc-
ture of the network and the contribution of latent factors, under
the assumption that both observable variables and latent factors
interdependence follow a temporal non-random behaviour. For
this reason, the model allows to include prior knowledge on the
evolutionary pattern of the system. The imposition of such prior
knowledge benefits inference and subsequent analysis of the net-
work, accentuating precise dynamical patterns. This is particularly
important when the number of samples is low compared to the
number of observed and latent variables in the system. In fact, the
inference of the network at particular time points exploits the de-
pendence between consecutive temporal states. Such advantage
is achieved by a simultaneous inference of all the dynamical sys-
tem, that, mathematically, translates into imposing constraints on
the network behaviour. In this work we provide a set of possible
constraints that can be applied independently on both observed
and latent components, allowing for a wide range of evolutionary
patterns.
Figure 1 provides an example of the theoretical model assumed by
LTGL. Here, observed and latent variables (xi and zi ) are connected
in a slightly different way at each time. Note that the observations
of the system only involve variables xi , while the hidden factors zi
influence the system without being actually observed. Hence, when
analysing samples which are regulated from a dynamical network
with hidden factors, it is infeasible to precisely infer the identity of
latent variables, but only an estimation of their number and their
effect on the global system can be obtained.
Starting from the theoretical model we derived a minimisation
algorithm based on the alternating direction method of multipli-
ers (ADMM) [7]. The algorithm is divided into independent steps
using proximal operators, which can be solved by closed-form so-
lutions favouring a fast and scalable computation [12, 17, 24]. We
also provide the related implementation in a Python framework,
based on the use of highly optimised low-level libraries for nu-
merical computation. Experiments on synthetic data show LTGL
to achieve optimal performance in relation to ground truth and

Figure 1. A dynamical network with latent factors zi and observed
variables xi . At each time ti , all of the connections between la-
lines) and connections among ob-
tent and observed variables (
served variables (
lines) may change according to a specific tem-
poral behaviour.

to state-of-the-art methods for graphical modelling, in terms of
accuracy, structure learning and scalability. Moreover, we show the
computational efficiency of LTGL while increasing the number of
unknowns of the problem and the model complexity. We conclude
with the application of LTGL to real-world data sets to illustrate
how our method can be successfully employed to gain insights on
multivariate time-series data. In particular, we used biological and
financial data sets, to show the use of LTGL in different contexts.
In the first case, we analysed Escherichia coli response to pertur-
bation, correctly identified by our method. In the latter case, we
investigated on a financial data set, to show how the contribution
of latent factors is relevant for the understanding of the behaviour
of the system.

Outline. The paper is organised as follows. Section 2 includes a
background on the reference frameworks for static and dynamical
network inference. Section 3 contains the theoretical formulation
of the problem and the proposed method. Section 4 describes in
details the optimisation algorithm for the minimisation of the func-
tional. Section 5 and Section 6 illustrate the use of our method on
synthetic and real data, respectively. Finally, Section 7 concludes
with a discussion and future directions.

2 PRELIMINARIES
Given a graph G = (V, E), where V = {x1, . . . , xd } is a finite set
of vertices, and E ⊆ V × V is a set of edges, a graphical model
is a multivariate probability distribution on x1, . . . , xd variables
where the conditional independence between two variables xi and
xj given all the others is encoded in G [22]. The two variables xi and
xj are conditionally independent given the others if (xi , xj ) (cid:60) E
and (xj , xi ) (cid:60) E. In what follows, we consider only undirected
Gaussian graphical models (GGMs), where (i) there is no distinc-
tion between an edge (xi , xj ) ∈ E and (xj , xi ), and (ii) variables are
jointly distributed according to a multivariate Gaussian distribution
N (µ, Σ). Without loss of generality, we assume µ to be zero, thus
the distribution depends only on the covariance matrix Σ [11]. The
inverse covariance matrix Θ = Σ−1, called precision matrix, encodes
the conditional independence between pairs of variables. In partic-
ular, the precision matrix has a zero entry in the position i, j only
if (xi , xj ) (cid:60) E [22]. Hence, we can interpret the precision matrix
as the weighted adjacency matrix of G, encoding the dependence
between variables.

Network inference. Consider a series of samples drawn from a
multivariate Gaussian distribution X ∼ N (0, Σ), X ∈ Rn×d . Net-
work inference aims at recovering the graphical model of the d

variables, i.e., the interaction structure Θ, given n observed samples.
The graphical modelling problem has been extensively tackled in
literature by estimating the precision matrix Θ instead of the co-
variance matrix Σ (as, e.g., [5]) [15, 30, 34]. This has been shown
to improve the graphical model inference, particularly for high-
dimensional problems [25]. In such contexts, the assumption is that
a variable is conditionally dependent only on a subset of all the
others. Therefore, the estimation of the precision matrix may be
guided by a sparse prior, in such a way to restrict the number of
possible connections in the network to improve interpretability
and noise reduction. Also, the imposition of a sparse prior on the
problem helps with the identifiability of the graph, especially when
the available number of samples is low compared to the dimension
of the problem. A model for the inference of Θ including the sparse
prior is the graphical lasso [15]:

minimize
Θ

− ℓ(S, Θ) + α ∥Θ ∥od, 1,

(1)

where ℓ is the Gaussian log-likelihood (up to a constant and scaling
factor) defined as ℓ(S, Θ) = log det(Θ)−tr(SΘ) for Θ positive definite
n X ⊤X is the empirical covariance matrix. ∥ · ∥od,1 is the
and S = 1
off-diagonal ℓ1-norm, which promotes sparsity in the precision
matrix (excluding the diagonal).

Latent variable network inference. Often, real-world observa-
tions do not conform exactly to a sparse GGM. This is due to global
hidden factors that influence the system, which introduce spurious
dependencies between observed variables [10, 11]. For this reason,
GGMs can be extended by introducing latent variables able to rep-
resent factors which are not observed in the data. These latent
variables are not principal components, since they do not provide a
low-rank approximation of the graphical model. On the contrary,
such factors are added to the model in order to condition the statis-
tics of the observed variables. In particular, we consider both latent
and observed variables to have a common domain [11].
Let latent variables be indexed by a set H , and observed variables
by a set O. The precision matrix Θ of the joint distribution of both
latent and observed variables may be partitioned into four blocks:

Θ = Σ−1 =

ΘH

ΘH O

ΘO H

ΘO












.












Such blocks represent the conditional dependencies among latent
variables (ΘH ), observed variables (ΘO ), between latent and ob-
served (ΘH O ), and viceversa (ΘO H ). The marginal precision matrix
Σ−1
O of the observed variables is given by the Schur complement
w.r.t. the block ΘH [8]:

ˆΘO = Σ−1
O

= ΘO − ΘO H Θ−1

H ΘH O .

(2)
ΘO specifies the precision matrix of the conditional statistics of the
observed variables given the latent variables, while ΘO H Θ−1
H ΘH O
is a summary of the effect of marginalisation over the latent vari-
ables. Such matrix has a small rank if the number of latent variables
is small compared to the observed variables. Note that the rank is
an indicator of the number of latent variables H [9].
The effect of the marginalisation is scattered over many observed
variables, in such a way not to confound it with the true underlying
conditional sparse structure of ΘO [8]. Typically ˆΘO is not sparse
due to the low-rank term, while, with the addition of the latent
factors contribution, we can recover the true sparse GGM. For this

reason, the graphical lasso in Equation (1) has been extended with
the latent variable graphical lasso that includes the inference of a
low-rank term, using the form (2), as follows [8]:

˜Θ, ˜L = argmin
(Θ, L)
L≽0

− ℓ(S, Θ − L) + α ∥Θ ∥od, 1 + τ ∥L ∥∗ .

(3)

Here ˜Θ provides an estimate of ΘO (precision matrix of the ob-
served variables) while ˜L provides an estimate of ΘO H Θ−1
H ΘH O
(marginalisation over the latent variables). Note that S is the empir-
ical covariance matrix computed only on observed variables, since
no information on the latent ones is available.

Time-varying network inference. Problems (1) and (3) aim at
recovering the structure of the system at fixed time (static network
inference). However, complex systems have temporal dynamics that
regulate their overall functioning [1, 15]. Hence, the modelling
of such complex systems requires a dynamical network inference,
where the states of the network are co-dependent. This naturally
leads to the idea of temporal consistency, which assumes similarities
between consecutive states of the network. In fact, we can assume
that, for sufficiently close time points, a system shows negligible
differences. During the inference of a dynamical network, temporal
consistency may translate into the imposition of similarities among
temporally close networks [16]. In particular, graphical lasso with
temporal consistency results in time-varying graphical lasso [17]:

minimize
(Θ1, . . ., ΘT )

T
(cid:213)

(cid:20)

i =1

− ℓ(Si, Θi ) + α ∥Θi ∥od, 1

+ β

Ψ(Θi +1 − Θi )

(4)

(cid:21)

T −1
(cid:213)

i =1

i j | · |.

where the inference of a network at a single time point i is guided
by the states at adjacent time points. The network is encoded in
a sequence of precision matrices (Θ1, . . . , ΘT ) which model the
system at each time point i = 1, . . . ,T . The type of similarity
imposed to consecutive time points and its strength are specified by
the penalty function Ψ and the parameter β, respectively. Options
when choosing Ψ include the following [17]:
• Lasso penalty (ℓ1) - Ψ = (cid:205)
Encourages few edges to change between subsequent time points,
while the rest of the structure remains the same [12].
• Group lasso penalty - Ψ = (cid:205)
j ∥ ·j ∥2.
Encourages the graph to restructure at some time points and to
stay stable in others [16? ].
• Laplacian penalty - Ψ = (cid:205)
i j (·i j )2.
Encourages smooth transitions over time, for slow changes of the
global structure [? ].
• Max norm penalty (ℓ∞) - Ψ = (cid:205)
Encourages a block of nodes to change their structure with no
additional penalty with respect to the change of a single edge among
such nodes. In fact, ℓ∞ norm is influenced only from the most
changing element for each row.
• Row-column overlap penalty - Ψ = minV :A=V +V ⊤ (cid:205)
Encourages a major change of the network at a specific time, while
in the rest the network is enforced to remain constant. The choice
of p = 2 causes the penalty to be node-based, i.e., the penalty allows
for a perturbation of only some nodes [? ].

j (maxi | ·i j |).

j ∥Vj ∥p .

Dependently from prior assumptions on the problem, one may
choose the most appropriate penalty for the data at hand.

3 MODEL FORMULATION
In this work we propose a new statistical model for the inference
of networks that change consistently in time under the influence
of latent factors. We call such model latent variable time-varying
graphical lasso (LTGL). LTGL infers the dynamical network of com-
plex systems by decomposing the problem into two parts — simi-
larly to what has been done in [8] for static network inference. We
consider two components of the dynamical network: a true under-
lying structure on the observed variables and the contribution of
latent factors. This allows to factor out the contribution of hidden
variables, favouring a reliable modelling of the dynamical system.
The novelty of our method is the simultaneous inference of a dy-
namical network with latent factors that exploits the imposition of
behavioural consistency on both observed variables interactions
and latent influence through the use of penalisation terms. This
allows for an easier interpretation of the evolution of the dynamical
system while, at the same time, improving its graphical modelling.
The two separate (while closely related) components at each time
point are obtained by integrating the network inference with the
information coming from temporally different states of the network.
Formally, let Xi ∈ Rni ×d , for i = 1, . . . ,T , be a set of observations
measured at T different time points composed by ni samples of d
observed variables. (Note that, for each time point i, samples are
assumed to be drawn from the probability distribution on the ob-
served variables conditioned on the latent ones.) Let Si = 1
i Xi
ni
be the empirical covariance matrix at time i. The goal is to retrieve a
set of sparse matrices Θ = (Θ1, . . . , ΘT ) and a set of low-rank matri-
ces L = (L1, . . . , LT ) such that, at each time point i, Θi encodes the
conditional independences between the observed variables, while
Li provides the summary of marginalisation over latent variables
on the observed ones.
Consider Equation (3) at a specific time i. Here we want to im-
pose continuity between the structure and the hidden variables
contribution in time, therefore we enforce the difference between
consecutive graphs to abide certain constraints by adding two pe-
nalisation terms. Our LTGL model takes the following form:

X ⊤

minimize
(Θ, L)
Li ≽0

T
(cid:213)

(cid:20)

i =1

+ β

T −1
(cid:213)

i =1

− ℓ(Si, Θi − Li ) + α ∥Θi ∥od, 1 + τ ∥Li ∥∗

(cid:21)

Ψ(Θi +1 − Θi ) + η

Φ(Li +1 − Li ),

T −1
(cid:213)

i =1

where Ψ and Φ are both penalty functions that force the structure of
the network to change over time according to a certain behaviour,
by acting on Θ and L, respectively. Temporal consistency of both
the structure of the network and latent factors contribution is guar-
anteed by the use of such penalty functions, which benefits the
network inference in particular in presence of few available ob-
servations of the system. Possible choices for Ψ and Φ are listed
in Section 2. Their choice is arbitrary and it is based on the prior
knowledge on the respective components evolution in the system.
Also, note that Ψ and Φ are independent, which allows LTGL to
model a wide range of dynamical behaviours of complex systems.

4 MINIMISATION METHOD
Problem (5) is convex, provided that the penalty functions Ψ and
Φ are convex, and it is coercive because of the regularisers. Thus,

Problem (5) admits solutions. Nonetheless, its optimisation is chal-
lenging in practice due to the high number of unknown matrices
involved (2T , for a total of 2T d (d +1)
unknowns of the problem). A
2
suitable method for the minimisation is ADMM [7]. It allows to
decouple the variables obtaining a separable minimisation problem
which can be efficiently solved in parallel. The sub-problems ex-
ploit proximal operators which are (mostly) solvable in closed-form,
leading to a simple iterative algorithm.
In order to decouple the involved matrices, we define three dual
variables R, Z = (Z1, Z2) and W = (W1,W2) and two projections:

P1 : (Rd ×d )T → (Rd ×d )T −1

P2 : (Rd ×d )T → (Rd ×d )T −1

A (cid:55)→ (A1, . . . , AT −1)

A (cid:55)→ (A2, . . . , AT )

Problem (5) becomes:

minimize
(Θ, L, R, Z ,W )
Li ≽0

T
(cid:213)

(cid:20)

i =1

+ β

T −1
(cid:213)

i =1

− ℓ(Si, Ri ) + α ∥Θi ∥od, 1 + τ ∥Li ∥∗

(cid:21)

Ψ(Z2,i − Z1,i ) + η

Φ(W2,i − W1, i )

(6)

T −1
(cid:213)

i =1

s.t. R = Θ − L, Z1 = P1Θ, Z2 = P2Θ, W1 = P1L, W2 = P2L.

The corresponding augmented Lagrangian is as follows:
Lρ (Θ, L, R, Z, W, U)

− ℓ(Si, Ri ) + α ∥Θi ∥od, 1 + τ ∥Li ∥∗ + I(L ≽ 0)

(cid:21)






+β

Ψ(Z2,i − Z1,i ) + η

Φ(W2,i − W1,i )

T −1
(cid:213)

i =1

∥Ri −Θi +Li +U0,i ∥2 − ∥U0,i ∥2

(cid:21)

∥Θi −Z1,i +U1,i ∥2 − ∥U1,i ∥2 + ∥Θi +1 −Z2,i +U2,i ∥2 − ∥U2,i ∥2

∥Li −W1,i +U3,i ∥2 − ∥U3,i ∥2 + ∥Li +1 −W2,i +U4,i ∥2 − ∥U4,i ∥2

T
(cid:213)

(cid:20)

=

i =1
T −1
(cid:213)

i =1
T
(cid:213)

(cid:20)

i =1
T −1
(cid:213)

(cid:20)

i =1
T −1
(cid:213)

(cid:20)

i =1

+ ρ
2

+ ρ
2

+ ρ
2

(5)

where U = (U0, U1, U2, U3, U4) are the scaled dual variables.
The ADMM algorithm for Problem (6) writes down as follows:

for k = 1, . . .

Rk +1 = argmin

Lρ (Θk, Lk, R, Zk, Wk, Uk )

Θk +1 = argmin

Lρ (Θ, Lk, Rk +1, Zk, Wk, Uk )

R

Θ

Lk +1 = argmin

Lρ (Θk +1, L, Rk +1, Zk, Wk, Uk )

Zk +1 =

Wk +1 =

Uk +1 =

(cid:21)

(cid:21)

+

L
(cid:20) Z k +1
1
Z k +1
2
(cid:20) W k +1
1
W k +1
2
U k
0
U k
1
U k
2
U k
3
U k
4























= argmin
Z

= argmin
W

Lρ (Θk +1, Lk +1, Rk +1, Z, Wk, Uk )

Lρ (Θk +1, Lk +1, Rk +1, Zk +1, W, Uk )

Rk +1 − Θk +1 + Lk +1
P1Θk +1 − Z k +1
P2Θk +1 − Z k +1
P1Lk +1 − W k +1
P2Lk +1 − W k +1

1

2

1

2












.












(cid:21)

(cid:21)

(7)

(8)

4.1 R step
The minimisation problem involving the matrix R in (8) can be
split into parallel updates, since Lρ (Θ, L, R, Z, W, U) is separable
in the variables (R1, . . . , RT ). Therefore, each Ri at iteration k + 1
is given by:
Rk +1
i

∥R − Θk + Lk + U k

0, i ∥2

= argmin
R

= argmin
R

tr(Si R) − log det(R) + ρ
2
tr(Si R) − log det(R) + ρ
2

= argmin
R

tr(Si R) − log det(R) + ρ
2

(cid:13)
(cid:13)
(cid:13)

2

F

i

(cid:13)
(cid:13)R − Ak
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

R −

Ak
i

+ Ak ⊤
i
2

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F

− U k

= Θk
i

− Lk
i

with Ak
0,i . Note that the last equality in (9) follows
i
from the symmetry of R — which also guarantees the log det to be
well-defined. Equation (9) can be explicitly solved. Indeed, Fermat’s
rule yields:

Si − ρ

Ak
i

+ Ak ⊤
i
2

= R−1 − ρR .

Then the solution to Equation (10) is [12, 17, 32]:
(cid:19)

(cid:18)

(cid:113)

V k

−Ek +

(Ek )2 + 4ρI

V k ⊤

Rk +1
i

= 1
2ρ

where V k EkV k ⊤ is the eigenvalue decomposition of Si − ρ

Ak
i

+Ak ⊤
i
2

.

4.2 Θ step
Likewise the R step, the update of Θ in (8) can be done in a parallel
fashion, as follows:

Θk +1
i

= argmin
Θ

α ∥Θ ∥od, 1 + ρ
2

(cid:20) (cid:13)
(cid:13)Rk
(cid:13)

i − Θ + Lk
i

+ U k
0, i

(cid:13)
(cid:13)
(cid:13)

2

F

+ δ iT

(cid:13)
(cid:13)Θ − Z k
(cid:13)

1,i

+ U k
1, i

+ δ i 1

(cid:13)
(cid:13)
(cid:13)

2

F

2,i −1

+ U k

2,i −1

= argmin
Θ

α ∥Θ ∥od, 1 + (1 + δ iT + δ i 1)

(cid:13)
(cid:13)Θ − Z k
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)Θ − Bk
(cid:13)

i

2

F

ρ
2

where δ i j = 1 − δi j , with δi j Kronecker delta and
1,i ) + δ i 1(Z k

+ δ iT (Z k

+ U k
0,i

+ Rk
i

Lk
i

Bk
i

=

1,i − U k
1 + δ iT + δ i 1

2,i −1 − U k

2,i −1)

.

Problem (11) is solved as:

Θk +1
i

(Bk

= proxζ ∥·∥od, 1
, and Sζ (·) element-wise off-diagonal soft-

i ) = Sζ (Bk
i )

with ζ =

α
ρ(1+δ iT +δ i 1)
thresholding function.

(9)

(10)

(cid:21)

(cid:13)
(cid:13)
(cid:13)

2

F

(11)

4.3 L step
The parallel update of L in (8) can be written as:
τ tr(L) + I(L ≽ 0) + ρ
2

i − Θk +1

= argmin
L

Lk +1
i

i

(cid:20) (cid:13)
(cid:13)Rk +1
(cid:13)
(cid:13)
(cid:13)L − W k
(cid:13)

+ δ iT

(cid:13)
(cid:13)L − W k
(cid:13)

1,i

+ U k
3,i

+ δ i 1

(cid:13)
(cid:13)
(cid:13)

2

F

2,i −1

+ U k

4, i −1

τ tr(L) + I(L ≽ 0) + (1 + δ iT + δ i 1)

= argmin
L

= argmin
L

τ tr(L) + I(L ≽ 0) + (1 + δ iT + δ i 1)

+ L + U k
i, 0

2

F

(cid:13)
(cid:13)
(cid:13)
(cid:21)

2

(cid:13)
(cid:13)
(cid:13)

F
(cid:13)
(cid:13)
(cid:13)

2

F

ρ
2

ρ
2

i

(cid:13)
(cid:13)L − C k
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

L −

C k
i

+ C k ⊤
i
2

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(12)

where

C k
i

=

i − Rk +1
Θk +1

i − U k
0,i

+ δ iT (W1,i − U3,i ) + δ i 1(W2,i −1 − U4,i −1)

.

1 + δ iT + δ i 1

Note that the last equality in (12) follows from the symmetry of L.
Then, the solution to Problem (12) is [24]:
= V k ˜EV k ⊤

Lk +1
i
where V k EkV k⊤ is the eigenvalue decomposition of Ck

i , and

(cid:32)

˜Ej j = max

Ek
j j −

τ
ρ(1 + δ iT + δ i1)

(cid:33)

, 0

.

4.4 Z and W step
The dual variables Z and W enforce the network to behave in time
consistently with the choice of Ψ and Φ, respectively. Z is the dual
variable of Θ while W is the dual variable of L. For the sake of
brevity, we show only the steps regarding the update of Z — the
update of W is analogous. The dual variable Z is defined as (Z1, Z2).
Such matrices are not separable in Equation (6), thus they must be
jointly updated. The update of Z in (8) can be rewritten as follows:

(cid:35)

(cid:34)

Z k +1
1,i
Z k +1
2,i

= argmin
Z1, Z2

Let ˆΨ

(cid:21)

(cid:20) Z1
Z2

an unique update [17]:

β Ψ(Z2 − Z1) + ρ
2
+ ρ
2

∥Θk

i − Z1 + X k

1,i ∥2

∥Θk

i +1 − Z2 + X k

2,i ∥2 .

(13)

= Ψ(Z2 − Z1). Then, Problem (13) can be solved with

(cid:21)

(cid:20) Z k +1
1,i
Z k +1
2,i

= prox β
ρ

ˆΨ(·)

(cid:18) (cid:20) Θk
i
Θk
i +1

+ U k
1,i
+ U k
2,i

(cid:21) (cid:19)

.

The same holds for the W step. Hence, the proximal operator for
the update of W1,i and W2,i becomes:
(cid:20) W k +1
1,i
W k +1
2,i

(cid:18) (cid:20) Lk
i
Lk
i +1

+ U k
i, 3
+ U k
i, 4

= prox η
ρ

ˆΦ(·)

(cid:21) (cid:19)

(cid:21)

.

For the particular derivation of different proximal operators,
see [17].

4.5 Termination criterion
According to [7], the algorithm is said to converge if the primal
and dual residuals are sufficiently small, i.e. if ∥r k ∥2
2 ≤ ϵ pri and
∥sk ∥2
2 ≤ ϵ dual. At each iteration k these values are computed as
follows:

∥r k ∥2
2

∥s k ∥2
2

= ∥R k − Θk + Lk ∥2
1 ∥2
+ ∥P1Θk − Z k
F
F
1 ∥2
2 ∥2
+ ∥P2Lk − W k
+ ∥P1Lk − W k
F
F
= ρ (cid:0) ∥R k − R k −1 ∥2
1 − Z k −1
∥2
+ ∥Z k
+ ∥Z k
1
F
F
(cid:1)
2 − W k −1
1 − W k −1
∥2
∥2
2
1
F
F
ϵ pri = c + ϵ rel max(cid:0)D1, D2
ϵ dual = c + ϵ relρ (cid:0) ∥U k

+ ∥W k
(cid:1)

+ ∥W k

+ ∥P2Θk − Z k

1 ∥2
F

2 − Z k −1
2

∥2
F

0 ∥2
F

+ ∥U k

+ ∥U k

1 ∥2
F
where c = ϵ absd(5T − 4)1/2, ϵ abs and ϵ rel are arbitrary tolerance
+ ∥W k
+ ∥Z k
+ ∥W k
∥2
∥2
∥2
∥2
parameters, Dk
1
1
2
2
F
F
F
F
+ ∥P1Θk ∥2
= ∥Θk −Lk ∥2
+ ∥P1Lk ∥2
+ ∥P2Lk ∥2
and Dk
F .
2
F
F
F

+ ∥Z k
1
+ ∥P2Θk ∥2
F

= ∥Rk ∥2
F

+ ∥U k

+ ∥U k

4 ∥2
F

2 ∥2
F

3 ∥2
F

(cid:1)

4.6 Implementation
The minimisation algorithm is available as a Python framework1,
fully compatible with the scikit-learn library of machine learning
algorithms, providing a straightforward and intuitive interface. The
implementation relies on low-level high-performance libraries for
numerical computations and it exploits closed-form solutions for
proximal operators, leading to a fast and scalable minimisation
algorithm even with an increasing number of unknowns.

5 EXPERIMENTS
We performed experiments on synthetic data assessing the perfor-
mance of the method in terms of structure recovery and measure of
latent variables influence. The performance of LTGL was evaluated
with respect to the ground truth and to state-of-the-art methods
for graphical inference. In particular, we assessed two aspects of
such methods, that are modelling performance and scalability, in
separated experiments. Modelling performance was estimated by
comparing the inferred graphical model to the true network un-
derlying the data set. During the scalability experiment, instead,
we assessed the computational time for convergence needed for
increasing problem complexity.

5.1 Modelling performance
We evaluated LTGL modelling performance on two synthetic data
sets. The ground truth sets of matrices Θ = (Θ1, . . . , ΘT ) and
L = (L1, . . . , LT ) were obtained by perturbing initial matrices Θ1
and L1, according to a specific behaviour for T − 1 times, with the
guarantee that Θi − Li ≻ 0 and Li ≽ 0 for i = 1, . . . ,T . The initial
matrices were generated according to [33], following the form (2).
Θ1 and L1 correspond to ΘO and ΘO H Θ−1
H ΘH O , respectively, with
ΘH identity matrix and ΘH O = Θ⊤
O H . Note that, since ΘH has full
rank, the number of latent variables is H . In particular, for d ob-
served variables, n samples and T timestamps, we generated a data
set X ∈ (Rn×d )T sampled from T multivariate normal distributions
Xi ∼ Ni (0, Σi ), for i = 1, . . . ,T , and Σ−1
i
ℓ2
2 perturbation (p2). The first data set was generated by
perturbing the initial matrices with a random matrix of small ℓ2
2
norm. This perturbation assumes the differences between two
consecutive matrices to be small and bounded over time, i.e.,
∥Θi − Θi−1 ∥F ≤ ϵ for i = 2, . . . ,T . The bound ϵ on the norm
is chosen a priori.
The update of Li is done maintaining consistency with the theo-
retical model where Li = ΘO H,i Θ⊤
O H,i . Therefore, the update is
obtained by adding a random matrix with a small norm to ΘO H,i−1.
In this way, the rank of Li remains the same as the number of latent
variables and constant over time. Data were generated in R100 with
10 time stamps, conditioned on 20 latent variables. For each time
stamp, we drew 100 samples from the distribution. For this reason,
in this setting, the contribution of latent factors is predominant
with respect to the network evolution in time.

= Θi − Li .

5.1.1

5.1.2

ℓ1 perturbation (p1). A second data set was generated
according to a different perturbation model. Here, the precision
matrix was updated by randomly choosing an edge and swapping

1LTGL is open-source. The
https://github.com/fdtomasi/regain.

code

is

available under BSD-3-Clause

at

Table 1. Performance in terms of F1 score, accuracy (ACC), mean
rank error (MRE) and mean squared error (MSE) of LTGL with re-
spect to TVGL, LVGLASSO and GL. LTGL and TVGL are employed
with both ℓ2
2 and ℓ1 penalties, to show how the prior on the evolu-
tion of the network affects the outcome.

perturbation method

score

F1

ACC MRE MSE

ℓ2
2 (p2)

ℓ1 (p1)

LTGL (ℓ2
0.926
2)
LTGL (ℓ1)
0.898
TVGL (ℓ2
0.791
2)
TVGL (ℓ1)
0.791
LVGLASSO 0.815
GL
0.745
LTGL (ℓ2
2)
0.842
LTGL (ℓ1)
0.880
TVGL (ℓ2
2)
0.742
TVGL (ℓ1)
0.817
LVGLASSO 0.752
0.748
GL

0.994
0.993
0.980
0.980
0.988
0.974

0.974
0.981
0.950
0.968
0.964
0.951

0.70
0.70
-
-
2.80
-

0.29
0.28
-
-
0.74
-

0.007
0.007
0.003
0.003
0.007
0.004

0.013
0.013
0.009
0.009
0.013
0.007

Figure 2. Distribution of inferred ranks over time. For each method
that considers latent variables, we show the frequency of finding
a specific rank during the network inference. The vertical line in-
dicates the ground truth rank, around which all detected ranks lie.
Note that, in (p2), Li ∈ R100×100, therefore the range of possible ranks
is [0, 100]. For (p2), Li ∈ R50×50, hence the range is [0, 50].

its state, i.e., by removing or adding a connection between two
variables. This allows for a ℓ1-norm evolutionary pattern of the
network. Data were generated in R50 with 100 time stamps, con-
ditioned on 5 latent variables. For each time stamp, we drew 100
samples from the distribution. In this setting, the time consistency
affects the network more than the latent factor contribution.

5.1.3

Scores. We evaluated LTGL performance using different
scores measuring the divergence of the results from the ground
truth. In particular, the performance was evaluated in terms of F1
score, accuracy, mean rank error and mean squared error. We define
as true/false positive the number of correctly/incorrectly existing in-
ferred edges, true/false negative the number of correctly/incorrectly
missing inferred edges [18]. The scores are computed as follows.
• F1 score: indicates the quality of structure inference, as the har-
monic mean of precision and recall.
• Accuracy (ACC): evaluates the number of true existing and missing
connections in the network correctly inferred with respect to the
total number of connections.

Figure 3. Scalability comparison for LTGL in relation to other ADMM-based methods. The compared methods are initialised in the same
manner, i.e., with all variable interactions (not self-interacting) set to zero. For LVGLASSO and TVGL, we used their relative original imple-
mentations. Also, we ignore the computational time required for hyperparameters selection. LTGL outperforms the other methods for each
increasing time and dimensionality of the problem.

• Mean rank error (MRE): estimates the precision on the number of
inferred latent variables, based on the rank of the set of matrices ˜L
in relation to the ground truth. The MRE score is defined as:

MRE = 1
T

T
(cid:213)

i =1

(cid:12)rank(Li ) − rank( ˜Li )(cid:12)
(cid:12)
(cid:12).

A value close to 0 means that we are inferring the true number of
latent variables over time, while, viceversa, a high value indicates a
poor consideration of the contribution of the latent variables.
• Mean squared error (MSE): score how close is the inferred precision
matrix ˜Θ to the ground truth, in terms of the Frobenius norm:

MSE =

2
T d(d − 1)

T
(cid:213)

i =1

(cid:13)
(cid:13)
(cid:13)Θ

(u)
(u)
i − ˜Θ
i

(cid:13)
(cid:13)
(cid:13)F

,

where Θ(u) denotes the upper triangular part of Θ.

5.1.4 Discussion. Table 1 shows the performance of LTGL com-
pared to graphical lasso (GL) [15], latent variable graphical lasso
(LVGLASSO) [8, 24] and time-varying graphical lasso (TVGL) [17]
in terms of F1 score, accuracy, mean rank error (MRE) and mean
squared error (MSE), for both settings with ℓ2
2 (p2) and ℓ1 (p1) per-
turbation. Note that MRE is not available for all the methods since
neither GL or TVGL consider latent factors. LTGL and TVGL are
used with two temporal penalties according to the different per-
turbation models of data generation. In this way, we show how
the correct choice of the penalty for the problem at hand results in
a more accurate network estimation. In both (p2) and (p1), LTGL
outperforms the other methods for graphical modelling. In (p2), in
particular, LTGL correctly infers almost 99,5% of edges in all the dy-
namical network both with the ℓ2
2 and ℓ1 penalties. Nonetheless, the
use of ℓ2
2 penalty enhance the quality of the inference as expected
from the theoretical assumption made during data generation. The
choice of a proper penalty for the problem and the consideration
of time consistency is reflected also in a low MRE, which encom-
passes LVGLASSO ability in detecting latent factors (Figure 2). In
(p2), in fact, the number of latent variables with respect to both
observed variables and samples is high. Therefore, by exploiting
temporal consistency of the network, LTGL is able to improve the
latent factors estimation. Simultaneous consideration of time and
latent variable also positively influences the F1 score, i.e., structure
detection.
Above considerations also hold for the (p1) setting. Here, LTGL
achieves the best results in both F1 score and accuracy, while hav-
ing a low MRE. The adoption of ℓ1 penalty improves structure
estimation and latent factors detection, consistently with the data
generation model. Such settings were designed to show how the

prevalence of latent factors contribution or time consistency af-
fects the outcome of a network inference method. In (p2), where
the latent factors contribution is prevalent, network inference is
more precise when considering latent factors. In (p1), instead, the
number of time points is more relevant than the contribution of
latent factors, hence it is more effective to exploit time consistency
(both for latent and observed variables), evident from the results
of Table 1. LTGL benefits from both aspects, therefore leading to a
noticeable improvement of graphical modelling.

5.2 Scalability
Next, we performed a scalability analysis using LTGL with respect
to different ADMM-based solvers. We evaluated the performance
of our method in relation to LVGLASSO [24] and TVGL [17], both
implemented with closed-form solutions to ADMM subproblems.
In general, the complexity of the three compared solvers is the
same (up to a constant). The implementation of GL [15] was not
included in such experiment, since it is not based on ADMM but on
coordinate descent, and therefore it is not comparable to our method.
As in Section 5.1, we generated different data sets X ∈ (Rn×d )T
with different values of T and d. In particular, d ∈ [10, 400) and
T = {20, 50, 100}. We ran our experiments on a machine provided
with two CPUs (2.4 GHz, 8 cores each).
Figure 3 shows, for the three different time settings, the scalability
of the methods in terms of seconds per convergence considering
different number of unknowns of the problem (i.e., 2T d (d +1)
2 with d
observed variables and T times). In all settings, LTGL outperforms
LVGLASSO and TVGL in terms of seconds per convergence. In
particular, the computational time for convergence remains stable
disregarding the number of time points under consideration. We
emphasise that the most computationally expensive task performed
by our solver is represented by two eigenvalue decompositions,
with a complexity of O(d3), to solve both R and L steps (Section 4).

5.3 Model selection
The hyperparameters of the method have been selected by using a
cross-validation procedure. In particular, we used the Monte Carlo
Cross-Validation (MCCV) [27] that repeatedly splits the n samples
of the data set in two mutually exclusive sets. For each split, n · (1/ν )
samples are labelled as validation set and the remaining n · (1 − 1/ν )
as learning set. For each hyperparameter combination, the model
was trained on the learning set and the likelihood of the model
was estimated on the independent test set. Finally, we selected the
combination of hyperparameters based on the average maximum
likelihood of the model across multiple splits of the data set.

However, the number of possible combinations of LTGL hyperpa-
rameters can be arbitrarily large. In order to avoid the assessment
of a grid of models (which can be computationally expensive), we
used a Gaussian process-based Bayesian optimisation procedure to
choose the best combination of hyperparameters for each analysed
data set, based on the Expected Improvement (EI) strategy [31]. In
practice, assuming the dynamics of a real system to be unknown,
it is possible to select the most appropriate temporal penalty by
exploiting the same principles, i.e., via a model selection procedure
based on the likelihood of different temporal models.

6 APPLICATIONS TO REAL DATA
We applied LTGL to two real data sets, to show how the method
can be employed to infer useful insights on multivariate time-series
data. These data sets measure complex dynamical systems of dif-
ferent (biological and financial) nature, which are usually highly
dimensional and feature complicated interdependences between
variables. This fact makes them ideal candidates for an analysis
using graphical models.

6.1 Metabolomic Data
The physiology of Escherichia coli necessitates rapid changes of
its cellular and molecular network to adapt to environmental con-
ditions. E. coli is widely studied because of the efficiency in its
system response to perturbation. Following the analysis of [21], we
used LTGL on E. coli data to infer network modifications across
different time points evaluated before and after the application of
environmental condition perturbations. We analysed the behaviour
of metabolites, which have been shown to change consistently after
the perturbation. Samples underwent one of two types of stress,
namely cold and heat stress.
Perturbation response detection. We inferred the dynamical
network of E. coli metabolites using LTGL with a group lasso (ℓ2)
penalty on latent variable contribution and a Laplacian (ℓ2
2) penalty
on the observed network. In this way, we allow the latent variables
(which, in our model, could represent the stress or other factors)
to change their global influence at a specific time point, while re-
maining stable in all others. At the same time, by conditioning
the network on the latent variables, we allow the observed net-
work structure to change smoothly in time. Hence, we expect to
see a global shift of the network between the second and third
time points, that is when the perturbation has been introduced in
the system. Figure 4a shows the temporal deviation between time
points, both for Θ, L and the total observed system R = Θ − L.
Latent variables temporal deviation reaches a peak at time t2−3,
right after the application of the perturbation to the system. Instead,
the difference between consecutive Θs remains more stable. Con-
sistently, the difference between the observed networks Rs shows
a major change at the same time point. Hence we can distinguish
the underlying evolving structure of metabolites while detecting
the contribution of the latent variables which affect mostly the
total system. In accordance with [21], we observed a interaction
between isoleucine, threonine, phenylalanine and 2-aminobutyric
acid during the adaptation phase following the stress response (Fig-
ure 4b). Therefore, we can conclude that LTGL successfully inferred
a dynamical network which adjusts in response to perturbation, in
accordance with our prior knowledge about E. coli behaviour.

Figure 4. Structure change of E. coli metabolites subject to stress.
The perturbation happens between time t = 2 and t = 3 (ver-
tical dotted line). (a) Temporal deviation where each point repre-
sents the difference between the network at subsequent time points.
The highest deviation on the observed network R appears when the
stress was applied. This can be decomposed into two parts, the la-
tent factors L and the underlying structure of observed variables Θ.
(b) Structural changes of metabolites interactions before and after
the perturbation.

6.2 Stock market
Finance is another example of a complex dynamical system suitable
to be analysed through a graphical model. Stock prices, in particular,
are highly related to each other and subject to time and environ-
mental changes, i.e., events that modify the system behaviour but
are not directly related to companies share values [3]. Here, the
assumption is that each company, while being part of a global finan-
cial system, is directly dependent from only a subset of others. For
example, it is reasonable to expect that stock prices of a technology
company are not directly influenced by trend of companies on the
primary sector. The modelling power of LTGL allows to detect both
the evolution of relations between companies and environmental
changes happening at a particular point in time. In order to show
this, we analysed stock prices2 during the financial crisis of 2007-
2008. The experiment was designed to consider the latent influence
of the market drop on technology companies interactions.

Global market crisis detection. We used a group lasso (ℓ2)
penalty to detect global shifts of the network. Figure 5 shows two
major changes in both components of the network (latent and ob-
served), in correspondence of late 2007 and late 2008. In particular,
during October 2008 a global crisis of the market occurred, and this
effect is especially evident for the shift of latent variables. Also, the
observed network changes in correspondence of the latent variables
shift or immediately after, caused by the effect of the crisis on the
stock market. The latent factors influence explains how the change
of the network was due to external factors that globally affected the
market, and not to normal evolution of companies relationships.
We further investigated on the causes for the first shift. Indeed,
we found that in late 2007 it happened a drop of a big American
company that was later pointed out as the beginning of the global
crisis of the following year.

2Data are freely available on https://quantquote.com/historical-stock-data.

[7] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. 2010. Distributed Optimiza-
tion and Statistical Learning via the Alternating Direction Method of Multipliers.
Foundations and Trends® in Machine Learning 3, 1 (2010).

[8] V. Chandrasekaran, P. A. Parrilo, and A. S. Willsky. 2010. Latent variable graph-
ical model selection via convex optimization. In Communication, Control, and
Computing (Allerton), 2010 48th Annual Allerton Conference on. IEEE, 1610–1613.
[9] V. Chandrasekaran, S. Sanghavi, P. A. Parrilo, and A. S. Willsky. 2011. Rank-
sparsity incoherence for matrix decomposition. SIAM Journal on Optimization
21, 2 (2011), 572–596.

[10] M. J. Choi, V. Chandrasekaran, and A. S. Willsky. 2010. Gaussian multiresolution
models: Exploiting sparse Markov and covariance structure. IEEE Transactions
on Signal Processing 58, 3 (2010), 1012–1024.

[11] M. J. Choi, V. Y.F. Tan, A. Anandkumar, and A. S. Willsky. 2011. Learning latent

tree graphical models. JMLRh 12, May (2011), 1771–1812.

[12] P. Danaher, P. Wang, and D. M. Witten. 2014. The joint graphical lasso for inverse
covariance estimation across multiple classes. Journal of the Royal Statistical
Society. Series B: Statistical Methodology 76, 2 (2014), 373–397. arXiv:1111.0324
[13] C. Ding, X. He, and H. D. Simon. 2005. On the equivalence of nonnegative matrix
factorization and spectral clustering. In Proceedings of the 2005 SIAM International
Conference on Data Mining. SIAM, 606–610.

[14] A. Farasat, A. Nikolaev, S. N. Srihari, and R. H. Blair. 2015. Probabilistic graphical

models in modern social network analysis. SNAM 5, 1 (2015), 62.

[15] J. Friedman, T. Hastie, and R. Tibshirani. 2008. Sparse inverse covariance estima-

tion with the graphical lasso. Biostatistics 9, 3 (2008), 432–441.

[16] A. J. Gibberd and S. Roy. 2017. Multiple Changepoint Estimation in High-
Dimensional Gaussian Graphical Models. arXiv preprint arXiv:1712.05786 (2017).
[17] D. Hallac, Y. Park, S. Boyd, and J. Leskovec. 2017. Network Inference via the Time-
Varying Graphical Lasso. In Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD ’17). ACM, 205–213.
[18] M. Hecker, S. Lambeck, S. Toepfer, E. Van Someren, and R. Guthke. 2009. Gene
regulatory network inference: Data integration in dynamic models: A review.
Biosystems 96, 1 (2009), 86–103.

[19] L. Huang, L. Liao, and C. H. Wu. 2016. Inference of protein-protein interaction
networks from multiple heterogeneous data. EURASIP Journal on Bioinformatics
and Systems Biology 2016, 1 (2016), 1–9.

[20] A. Jalali and S. Sanghavi. 2011. Learning the dependence graph of time series

with latent factors. arXiv preprint arXiv:1106.1887 (2011).

[21] S. Jozefczuk, S. Klie, G. Catchpole, J. Szymanski, A. Cuadros-Inostroza, D. Stein-
hauser, J. Selbig, and L. Willmitzer. 2010. Metabolomic and transcriptomic stress
response of Escherichia coli. Molecular systems biology 6, 1 (2010), 364.

[22] S. L. Lauritzen. 1996. Graphical models. Vol. 17. Clarendon Press.
[23] H. Liu, F. Han, and C. Zhang. 2012. Transelliptical graphical models. In NIPS.

800–808.

[24] S. Ma, L. Xue, and H. Zou. 2013. Alternating Direction Methods for Latent
Variable Gaussian Graphical Model Selection. Neural Computation 25, 8 (aug
2013), 2172–2198.

[25] N. Meinshausen and P. Bühlmann. 2006. High-dimensional graphs and variable

selection with the lasso. The annals of statistics (2006), 1436–1462.

[26] Z. Meng, B. Eriksson, and A. Hero. 2014. Learning latent variable Gaussian

graphical models. In Proceedings of the 31st ICML. 1269–1277.

[27] A. M. Molinaro, R. Simon, and R. M. Pfeiffer. 2005. Prediction error estimation: a
comparison of resampling methods. Bioinformatics 21, 15 (2005), 3301–3307.
[28] E. J. Molinelli, A. Korkut, W. Wang, M. L. Miller, N. P. Gauthier, X. Jing, P. Kaushik,
Q. He, G. Mills, D. B. Solit, et al. 2013. Perturbation biology: inferring signaling
networks in cellular systems. PLoS computational biology 9, 12 (2013), e1003290.
[29] P. Orchard, F. Agakov, and A. Storkey. 2013. Bayesian inference in sparse Gaussian

graphical models. arXiv preprint arXiv:1309.7311 (2013).

[30] P. Ravikumar, M.J. Wainwright, G. Raskutti, B. Yu, et al. 2011. High-dimensional
covariance estimation by minimizing ℓ1s-penalized log-determinant divergence.
Electronic Journal of Statistics 5 (2011), 935–980.

[31] J. Snoek, H. Larochelle, and R. P. Adams. 2012. Practical Bayesian optimization

of machine learning algorithms. In NIPS. 2951–2959.

[32] D. M. Witten and R. Tibshirani. 2009. Covariance-regularized regression and
classification for high dimensional problems. Journal of the Royal Statistical
Society. Series B: Statistical Methodology 71, 3 (jun 2009), 615–636.

[33] M. Yuan. 2012. Discussion: Latent variable graphical model selection via convex

optimization. Ann. Statist. 40, 4 (08 2012), 1968–1972.

graphical model. Biometrika (2007), 19–35.

Figure 5. Temporal deviation for stock market data. We observed
two peaks, in correspondence of late 2007 and late 2008, when the
financial crisis happened.

7 CONCLUSIONS AND FUTURE WORK
In this work, we developed a novel method for graphical modelling
of multivariate time-series. The model considers simultaneously
the contribution of latent factors and time consistency in evolving
systems. Indeed, our work is an attempt to generalise both latent
variable and dynamical network inference. To this aim, we impose
prior knowledge on the problem through penalty terms that force
precision and latent matrices to be consistent in time. The choice
of proper penalty terms maintains the convexity of the minimised
functional and, along with the coercivity given by the regularisers,
it guarantees global convergence of the proposed minimisation
algorithm. Our experiments demonstrate the ability of LTGL in
the graphical modelling of synthetic and real-world data, where
the possibility to decompose the total network into two separated
components allows for a better understanding of the underlying
phenomenon.
We emphasise that our framework is modular in the choice of the
penalties, allowing for precise modelling of different and complex
behaviours of the system. This allows for a straightforward inclu-
sion of additional penalty terms, based on the prior knowledge on
the problem at hand. Possible extensions may involve alternative
evolutionary models for different complex systems, e.g. forcing sub-
groups of variables to behave consistently in time [6]. These could
lead to interesting results in time-series clustering and pattern dis-
covery. Further investigations may also head to the inference of the
exact contribution of latent factors starting from the L matrices we
are estimating, possibly using matrix factorisation methods [13].
Such developments may increase the expression power of the
method, leading to advances in data mining and to potential appli-
cations in diverse science fields.

Acknowledgments. The authors thank the anonymous reviewers
for their valuable comments.

REFERENCES
[1] R. Albert. 2007. Network inference, analysis, and modeling in systems biology.

The Plant cell 19, 11 (nov 2007), 3327–38.

bayesian networks with latent variables. In ICML. 249–257.

[3] J. Bai and S. Ng. 2006. Evaluating latent and observed factors in macroeconomics

and finance. Journal of Econometrics 131, 1-2 (2006), 507–537.

[4] E. Bianco-Martinez, N. Rubido, Ch. G. Antonopoulos, and M.S. Baptista. 2016.
Successful network inference from time-series data using mutual information
rate. Chaos: An Interdisciplinary Journal of Nonlinear Science 26, 4 (2016), 043102.
[5] J. Bien and R. J. Tibshirani. 2011. Sparse estimation of a covariance matrix.

Biometrika 98, 4 (dec 2011), 807–820.

[6] A. Bolstad, B. D. Van Veen, and R. Nowak. 2011. Causal network inference via
group sparse regularization. IEEE transactions on signal processing 59, 6 (2011).

[2] A. Anandkumar, D. Hsu, A. Javanmard, and S. Kakade. 2013. Learning linear

[34] M. Yuan and Y. Lin. 2007. Model selection and estimation in the Gaussian

