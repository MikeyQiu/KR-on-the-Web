9
1
0
2
 
r
p
A
 
3
1
 
 
]

V
C
.
s
c
[
 
 
1
v
3
9
5
6
0
.
4
0
9
1
:
v
i
X
r
a

1

Shakeout: A New Approach to Regularized
Deep Neural Network Training

Guoliang Kang, Jun Li, and Dacheng Tao, Fellow, IEEE

Abstract—Recent years have witnessed the success of deep neural networks in dealing with a plenty of practical problems. Dropout
has played an essential role in many successful deep neural networks, by inducing regularization in the model training. In this paper,
we present a new regularized training approach: Shakeout. Instead of randomly discarding units as Dropout does at the training stage,
Shakeout randomly chooses to enhance or reverse each unit’s contribution to the next layer. This minor modiﬁcation of Dropout has the
statistical trait: the regularizer induced by Shakeout adaptively combines L0, L1 and L2 regularization terms. Our classiﬁcation
experiments with representative deep architectures on image datasets MNIST, CIFAR-10 and ImageNet show that Shakeout deals with
over-ﬁtting effectively and outperforms Dropout. We empirically demonstrate that Shakeout leads to sparser weights under both
unsupervised and supervised settings. Shakeout also leads to the grouping effect of the input units in a layer. Considering the weights
in reﬂecting the importance of connections, Shakeout is superior to Dropout, which is valuable for the deep model compression.
Moreover, we demonstrate that Shakeout can effectively reduce the instability of the training process of the deep architecture.

Index Terms—Shakeout, Dropout, Regularization, Sparsity, Deep Neural Network.

(cid:70)

1 INTRODUCTION

D EEP neural networks have recently achieved impres-

sive success in a number of machine learning and
pattern recognition tasks and been under intensive research
[1], [2], [3], [4], [5], [6], [7]. Hierarchical neural networks
have been known for decades, and there are a number of
essential factors contributing to its recent rising, such as
the availability of big data and powerful computational re-
sources. However, arguably the most important contributor
to the success of deep neural network is the discovery of
efﬁcient training approaches [8], [9], [10], [11], [12].

A particular interesting advance in the training tech-
niques is the invention of Dropout [13]. At the operational
level, Dropout adjusts the network evaluation step (feed-
forward) at the training stage, where a portion of units
are randomly discarded. The effect of this simple trick is
impressive. Dropout enhances the generalization perfor-
mance of neural networks considerably, and is behind many
record-holders of widely recognized benchmarks [2], [14],
[15]. The success has attracted much research attention, and
found applications in a wider range of problems [16], [17],
[18]. Theoretical research from the viewpoint of statistical
learning has pointed out the connections between Dropout
and model regularization, which is the de facto recipe of

• Guoliang Kang, Jun Li are with Center of AI, Faculty of Engineering
and Information Technology, University of Technology Sydney, Ultimo,
NSW, Australia. D. Tao is with the UBTech Sydney Artiﬁcial Intelligence
Institute and the School of Information Technologies in the Faculty of
Engineering and Information Technologies at The University of Sydney,
Darlington, NSW 2008, Australia.
E-mail: Guoliang.Kang@student.uts.edu.au,
dacheng.tao@sydney.edu.au
c(cid:13)2018 IEEE. Personal use of this material is permitted. Permission
from IEEE must be obtained for all other uses, in any current or future
media, including reprinting/republishing this material for advertising
or promotional purposes, creating new collective works, for resale or
redistribution to servers or lists, or reuse of any copyrighted component of
this work in other works.

junjy007@googlemail.com,

reducing over-ﬁtting for complex models in practical ma-
chine learning. For example, Wager et al. [16] showed that
for a generalized linear model (GLM), Dropout implicitly
imposes an adaptive L2 regularizer of the network weights
through an estimation of the inverse diagonal Fisher infor-
mation matrix.

importance in deep learning. It
Sparsity is of vital
is straightforward that
through removing unimportant
weights, deep neural networks perform prediction faster.
Additionally, it is expected to obtain better generalization
performance and reduce the number of examples needed in
the training stage [19]. Recently much evidence has shown
that the accuracy of a trained deep neural network will not
be severely affected by removing a majority of connections
and many researchers focus on the deep model compression
task [20], [21], [22], [23], [24], [25]. One effective way of com-
pression is to train a neural network, prune the connections
and ﬁne-tune the weights iteratively [21], [22]. However, if
we can cut the connections naturally via imposing sparsity-
inducing penalties in the training process of a deep neural
network, the work-ﬂow will be greatly simpliﬁed.

In this paper, we propose a new regularized deep neu-
ral network training approach: Shakeout, which is easy to
implement: randomly choosing to enhance or reverse each
unit’s contribution to the next layer in the training stage.
Note that Dropout can be considered as a special “ﬂat” case
of our approach: randomly keeping (enhance factor is 1) or
discarding (reverse factor is 0) each unit’s contribution to
the next layer. Shakeout enriches the regularization effect.
In theory, we prove that it adaptively combines L0, L1 and
L2 regularization terms. L0 and L1 regularization terms
are known as sparsity-inducing penalties. The combination
of sparsity-inducing penalty and L2 penalty of the model
parameters has shown to be effective in statistical learning:
the Elastic Net [26] has the desirable properties of producing
sparse models while maintaining the grouping effect of the

weights of the model. Because of the randomly “shaking”
process and the regularization characteristic pushing net-
work weights to zero, our new approach is named “Shake-
out”.

As discussed above, it is expected to obtain much sparser
weights using Shakeout than using Dropout because of the
combination of L0 and L1 regularization terms induced
in the training stage. We apply Shakeout on one-hidden-
layer autoencoder and obtain much sparser weights than
that resulted by Dropout. To show the regularization effect
on the classiﬁcation tasks, we conduct the experiments on
image datasets including MNIST, CIFAR-10 and ImageNet
with the representative deep neural network architectures.
In our experiments we ﬁnd that by using Shakeout, the
trained deep neural networks always outperform those by
using Dropout, especially when the data is scarce. Besides
the fact that Shakeout leads to much sparser weights, we
also empirically ﬁnd that it groups the input units of a
layer. Due to the induced L0 and L1 regularization terms,
Shakeout can result in the weights reﬂecting the importance
of the connections between units, which is meaningful for
conducting compression. Moreover, we demonstrate that
Shakeout can effectively reduce the instability of the training
process of the deep architecture.

This journal paper extends our previous work [27] theo-
retically and experimentally. The main extensions are listed
as follows: 1) we derive the analytical formula for the
regularizer induced by Shakeout in the context of GLM and
prove several important properties; 2) we conduct experi-
ments using Wide Residual Network [15] on CIFAR-10 to
show Shakeout outperforms Dropout and standard back-
propagation in promoting the generalization performance
of a much deeper architecture; 3) we conduct experiments
using AlexNet [14] on ImageNet dataset with Shakeout
and Dropout. Shakeout obtains comparable classiﬁcation
performance to Dropout, but with superior regularization
effect; 4) we illustrate that Shakeout can effectively reduce
the instability of the training process of the deep architec-
ture. Moreover, we provide a much clearer and detailed de-
scription of Shakeout, derive the forward-backward update
rule for deep convolutional neural networks with Shakeout,
and give several recommendations to help the practitioners
make full use of Shakeout.

In the rest of the paper, we give a review about the
related work in Section 2. Section 3 presents Shakeout in de-
tail, along with theoretical analysis of the regularization ef-
fect induced by Shakeout. In Section 4, we ﬁrst demonstrate
the regularization effect of Shakeout on the autoencoder
model. The classiﬁcation experiments on MNIST , CIFAR-10
and ImageNet illustrate that Shakeout outperforms Dropout
considering the generalization performance, the regulariza-
tion effect on the weights, and the stabilization effect on the
training process of the deep architecture. Finally, we give
some recommendations for the practitioners to make full
use of Shakeout.

2 RELATED WORK

Deep neural networks have shown their success in a wide
variety of applications. One of the key factors contributes to
this success is the creation of powerful training techniques.

2

The representative power of the network becomes stronger
as the architecture gets deeper [9]. However, millions of pa-
rameters make deep neural networks easily over-ﬁt. Regu-
larization [16], [28] is an effective way to obtain a model that
generalizes well. There exist many approaches to regularize
the training of deep neural networks, like weight decay [29],
early stopping [30], etc. Shakeout belongs to the family of
regularized training techniques.

Among these regularization techniques, our work is
closely related to Dropout [13]. Many subsequent works
were devised to improve the performance of Dropout [31],
[32], [33]. The underlying reason why Dropout improves
performance has also attracted the interest of many re-
searchers. Evidence has shown that Dropout may work
because of its good approximation to model averaging
and regularization on the network weights [34], [35], [36].
Srivastava [34] and Warde-Farley [35] exhibited through
experiments that the weight scaling approximation is an
accurate alternative for the geometric mean over all possible
sub-networks. Gal et al. [37] claimed that training the deep
neural network with Dropout is equivalent to performing
variational inference in a deep Gaussian Process. Dropout
can also be regarded as a way of adding noise into the
neural network. By marginalizing the noise, Srivastava [34]
proved for linear regression that the deterministic version
of Dropout is equivalent to adding an adaptive L2 regular-
ization on the weights. Furthermore, Wager [16] extended
the conclusion to generalized linear models (GLMs) using
a quadratic approximation to the induced regularizer. The
inductive bias of Dropout was studied by Helmbold et al.
[38] to illustrate the properties of the regularizer induced
by Dropout further. In terms of implicitly inducing regu-
larizer of the network weights, Shakeout can be viewed as
a generalization of Dropout. It enriches the regularization
effect of Dropout, i.e. besides the L2 regularization term, it
also induces the L0 and L1 regularization terms, which may
lead to sparse weights of the model.

Due to the implicitly induced L0 and L1 regulariza-
tion terms, Shakeout is also related to sparsity-inducing
approaches. Olshausen et al. [39] introduced the concept
of sparsity in computational neuroscience and proposed
the sparse coding method in the visual system. In machine
learning, the sparsity constraint enables a model to capture
the implicit statistical data structure, performs feature selec-
tion and regularization, compresses the data at a low loss of
the accuracy, and helps us to better understand our models
and explain the obtained results. Sparsity is one of the key
factors underlying many successful deep neural network
architectures [2], [40], [41], [42] and training algorithms [43]
[44]. A Convolutional neural network is much sparser than
the fully-connected one, which results from the concept of
local receptive ﬁeld [40]. Sparsity has been a design princi-
ple and motivation for Inception-series models [2], [41], [42].
Besides working as the heuristic principle of designing a
deep architecture, sparsity often works as a penalty induced
to regularize the training process of a deep neural network.
There exist two kinds of sparsity penalties in deep neural
networks, which lead to the activity sparsity [43] [44] and
the connectivity sparsity [45] respectively. The difference
between Shakeout and these sparsity-inducing approaches
is that for Shakeout, the sparsity is induced through sim-

ple stochastic operations rather than manually designed
architectures or explicit norm-based penalties. This implicit
way enables Shakeout to be easily optimized by stochastic
gradient descent (SGD) − the representative approach for
the optimization of a deep neural network.

3 SHAKEOUT

Shakeout applies on the weights in a linear module. The
linear module, i.e. weighted sum,

θ =

wjxj

p
(cid:88)

j=1

(1)

is arguably the most widely adopted component in data
models. For example, the variables x1, x2, . . . , xp can be
input attributes of a model, e.g. the extracted features for
a GLM, or the intermediate outputs of earlier processing
steps, e.g. the activations of the hidden units in a multilayer
artiﬁcial neural network. Shakeout randomly modiﬁes the
computation in Eq. (1). Speciﬁcally, Shakeout can be realized
by randomly modifying the weights

Step 1: Draw rj, where

(cid:40)

P (rj = 0)
P (rj = 1

= τ
1−τ ) = 1 − τ

.

Step 2: Adjust the weight according to rj,

(cid:40)

˜wj ← −csj,
˜wj ← (wj + cτ sj)/(1 − τ )

if rj = 0
otherwise

(A)
(B)

where sj = sgn(wj) takes ±1 depending on the sign of wj
or takes 0 if wj = 0. As shown above, Shakeout chooses
(randomly by drawing r) between two fundamentally dif-
ferent ways to modify the weights. Modiﬁcation (A) is to set
the weights to constant magnitudes, despite their original
values except for the signs (to be opposite to the original
ones). Modiﬁcation (B) updates the weights by a factor
(1 − τ )−1 and a bias depending on the signs. Note both (A)
and (B) preserve zero values of the weights, i.e. if wj = 0
then ˜wj = 0 with probability 1. Let ˜θ = ˜wT x, and Shakeout
leaves θ unbiased, i.e. E[˜θ] = θ. The hyper-parameters
τ ∈ (0, 1) and c ∈ (0, +∞) conﬁgure the property of
Shakeout.

Shakeout is naturally connected to the widely adopted
operation of Dropout [13], [34]. We will show that Shakeout
has regularization effect on model training similar to but
beyond what is induced by Dropout. From an operational
point of view, Fig. 1 compares Shakeout and Dropout. Note
that Shakeout includes Dropout as a special case when the
hyper-parameter c in Shakeout is set to zero.

When applied at the training stage, Shakeout alters the
objective − the quantity to be minimized − by adjusting the
weights. In particular, we will show that Shakeout (with ex-
pectation over the random switch) induces a regularization
term effectively penalizing the magnitudes of the weights
and leading to sparse weights. Shakeout is an approach
designed for helping model training, when the models are
trained and deployed, one should relieve the disturbance to
allow the model work with its full capacity, i.e. we adopt the
resulting network without any modiﬁcation of the weights
at the test stage.

3

Fig. 1. Comparison between Shakeout and Dropout operations. This
ﬁgure shows how Shakeout and Dropout are applied to the weights in a
linear module. In the original linear module, the output is the summation
of the inputs x weighted by w, while for Dropout and Shakeout, the
weights w are ﬁrst randomly modiﬁed. In detail, a random switch ˆr
controls how each w is modiﬁed. The manipulation of w is illustrated
within the ampliﬁer icons (the red curves, best seen with colors). The
coefﬁcients are α = 1/(1 − τ ) and β(w) = cs(w), where s(w) extracts
the sign of w and c > 0, τ ∈ [0, 1]. Note the sign of β(w) is always
the same as that of w. The magnitudes of coefﬁcients α and β(w) are
determined by the Shakeout hyper-parameters τ and c. Dropout can be
viewed as a special case of Shakeout when c = 0 because β(w) is zero
at this circumstance.

3.1 Regularization Effect of Shakeout

Shakeout randomly modiﬁes the weights in a linear module,
and thus can be regarded as injecting noise into each vari-
able xj, i.e. xj is randomly scaled by γj: ˜xj = γjxj. Note
that γj = rj + c(rj −1)
, the modiﬁcation of xj is actually
|wj |
determined by the random switch rj. Shakeout randomly
chooses to enhance (i.e. when rj = 1
1−τ ) or
reverse (i.e. when rj = 0, γj < 0) each original variable
xj’s contribution to the output at the training stage (see Fig.
1). However, the expectation of ˜xj over the noise remains
unbiased, i.e. Erj [˜xj] = xj.

1−τ , γj > 1

It is well-known that injecting artiﬁcial noise into the
input features will regularize the training objective [16],
[46], [47], i.e. Er[(cid:96)(w, ˜x, y)] = (cid:96)(w, x, y) + π(w), where
˜x is the feature vector randomly modiﬁed by the noise
induced by r. The regularization term π(w) is determined
by the characteristic of the noise. For example, Wager et
al. [16] showed that Dropout, corresponding to inducing
blackout noise to the features, helps introduce an adaptive
L2 penalty on w. In this section we illustrate how Shakeout
helps regularize model parameters w using an example of
GLMs.

Formally, a GLM is a probabilistic model of predicting
target y given features x = [x1, . . . , xp], in terms of the
weighted sum in Eq. (1):

P (y|x, w) = h(y)g(θ)eθy
θ = wT x

(2)

With different h(·) and g(·) functions, GLM can be special-
ized to various useful models or modules, such as logistic
regression model or a layer in a feed-forward neural net-
work. However, roughly speaking, the essence of a GLM
is similar to that of a standard linear model which aims

to ﬁnd weights w1, . . . , wp so that θ = wT x aligns with
y (functions h(·) and g(·) are independent of w and y
respectively). The loss function of a GLM with respect to
w is deﬁned as

We illustrate several properties of Shakeout regularizer
based on Eq. (7). The proof of the following propositions
can be found in the appendices.
Proposition 1. π(0) = 0

4

l(w, x, y) = −θy + A(θ)

A(θ) = − ln[g(θ)]

(3)

(4)

The loss (3) is the negative logarithm of probability (2),
where we keep only terms relevant to w.

Let the loss with Shakeout be

lsko(w, x, y, r) := l(w, ˜x, y)
where r = [r1, . . . , rp]T , and ˜x = [˜x1, . . . , ˜xp]T represents
the features randomly modiﬁed with r.

(5)

Taking expectation over r, the loss with Shakeout be-

Er[lsko(w, x, y, r)] = l(w, x, y) + π(w)

comes

where

π(w) = Er[A(˜θ) − A(θ)]
∞
(cid:88)

=

1
k!

k=1

A(k)(θ)E[(˜θ − θ)k]

(6)

is named Shakeout regularizer. Note that if A(θ) is k-th order
)(θ) = 0 where
derivable, let the k
k
Theorem 1. Let qj = xj(wj + csj), θj− = θ − qj and θj+ =

> k, to make the denotation simple.

order derivative A(k

(cid:48)

(cid:48)

(cid:48)

θ + τ

1−τ qj, then Shakeout regularizer π(w) is

π(w) = τ

A(θj−) + (1 − τ )

A(θj+) − pA(θ) (7)

p
(cid:88)

j=1

p
(cid:88)

j=1

Proof: Note that ˜θ − θ = (cid:80)p

j=1 qj(rj − 1), then for Eq.

(6)

E[(˜θ − θ)k] =

p
(cid:88)

p
(cid:88)

p
(cid:88)

k
(cid:89)

· · ·

k
(cid:89)

qjm

E[

(rjm − 1)]

j1=1

j2=1

jk=1

m=1

m=1

Because arbitrary two random variables rjm1 , rjm2 are inde-
pendent unless jm1 = jm2 and ∀rjm , E[rjm − 1] = 0, then

p
(cid:88)

j=1
p
(cid:88)

j=1

E[(˜θ − θ)k] =

qk
j

E[(rj − 1)k]

= τ

(−qj)k + (1 − τ )

p
(cid:88)

(
j=1

τ
1 − τ

qj)k

Then

π(w) = τ

A(k)(θ)(−qj)k

p
(cid:88)

∞
(cid:88)

j=1

k=1

1
k!

+(1 − τ )

p
(cid:88)

∞
(cid:88)

j=1

k=1

1
k!

A(k)(θ)(

τ
1 − τ

qj)k

Further, let θj− = θ − qj, θj+ = θ + τ

π(w) = τ

A(θj−) + (1 − τ )

p
(cid:88)

j=1

1−τ qj, π(w) becomes
p
(cid:88)

A(θj+) − pA(θ)

j=1

The theorem is proved.

Proposition 2. If A(θ) is convex, π(w) ≥ 0.

Proposition 3. Suppose ∃j, xjwj (cid:54)= 0. If A(θ) is convex,
(θ) > 0, π(w)

π(w) monotonically increases with τ . If A
monotonically increases with c.

(cid:48)(cid:48)

Proposition 3 implies that the hyper-parameters τ and c
relate to the strength of the regularization effect. It is rea-
sonable because higher τ or c means the noise injected into
the features x has larger variance.
Proposition 4. Suppose i) ∀j (cid:54)= j

, xjwj = 0, and ii) xj(cid:48) (cid:54)= 0.

(cid:48)

Then

(cid:48)(cid:48)

i) if A

(θ) > 0,

ii) if lim|θ|→∞ A





(cid:48)(cid:48)

> 0, when wj(cid:48) > 0
< 0, when wj(cid:48) < 0

∂π(w)
∂wj(cid:48)
∂π(w)
∂wj(cid:48)
(θ) = 0, lim|wj(cid:48) |→∞

∂π(w)
∂wj(cid:48)

= 0

Proposition 4 implies that under certain conditions, start-
ing from a zero weight vector, Shakeout regularizer pe-
nalizes the magnitude of wj(cid:48) and its regularization effect
is bounded by a constant value. For example, for logistic
regression, π(w) ≤ τ ln(1+exp(c|xj(cid:48) |)), which is illustrated
in Fig. 2. This bounded property has been proved to be
useful: capped-norm [48] is more robust to outliers than the
traditional L1 or L2 norm.

Based on the Eq. (7), the speciﬁc formulas for the repre-

sentative GLM models can be derived:
i) Linear regression: A(θ) = 1

π(w) =

τ
2(1 − τ )

2 θ2, then
(cid:107)x ◦ (w + cs)(cid:107)2
2

where ◦ denotes
(cid:107)x ◦ (w + cs)(cid:107)2
tion of three components

the element-wise product and the
2 term can be decomposed into the summa-

p
(cid:88)

j=1

p
(cid:88)

j=1

p
(cid:88)

j=1

j w2
x2

j + 2c

j |wj| + c2
x2

x2
j 1wj (cid:54)=0[wj]

(8)

where 1wj (cid:54)=0[wj] is an indicator function which satisﬁes

(cid:40)

1 wj (cid:54)= 0
0 wj = 0

1wj (cid:54)=0[wj] =

. This decomposition implies that

Shakeout regularizer penalizes the combination of L0-norm,
L1-norm and L2-norm of the weights after scaling them
with the square of corresponding features. The L0 and L1
regularization terms can lead to sparse weights.

ii) Logistic regression: A(θ) = ln(1 + exp(θ)), then

π(w) =

ln(

p
(cid:88)

j=1

(1 + exp(θj−))τ (1 + exp(θj+))1−τ
1 + exp(θ)

)

(9)

Fig. 3 illustrates the contour of Shakeout regularizer based
on Eq. (9) in the 2D weight space. On the whole, the con-
tour of Shakeout regularizer indicates that the regularizer
combines L0, L1 and L2 regularization terms. As c goes to
zero, the contour around w = 0 becomes less sharper, which
implies hyper-parameter c relates to the strength of L0 and
L1 components. When c = 0, Shakeout degenerates to

5

(a) Shakeout regularizer: τ = 0.3, c = 0.78

(b) Dropout regularizer: τ = 0.5

Fig. 2. Regularization effect as a function of a single weight when other weights are ﬁxed to zeros for logistic regression model. The corresponding
feature x is ﬁxed at 1.

Dropout, the contour of which implies Dropout regularizer
consists of L2 regularization term.

imposed and leads to much sparser weights of the model.
We will verify this property in our experiment section later.

The difference between Shakeout and Dropout regular-
izers is also illustrated in Fig. 2. We set τ = 0.3, c = 0.78
for Shakeout, and τ = 0.5 for Dropout to make the bounds
of the regularization effects of two regularizers the same. In
this one dimension circumstance, the main difference is that
at w = 0 (see the enlarged snapshot for comparison), Shake-
out regularizer is sharp and discontinuous while Dropout
regularizer is smooth. Thus compared to Dropout, Shakeout
may lead to much sparser weights of the model.

To simplify the analysis and prove the intuition we have
observed in Fig. 3 about the properties of Shakeout regular-
izer, we quadratically approximate Shakeout regularizer of
Eq. (7) by

πapprox(w) =

τ
2(1 − τ )

(cid:48)(cid:48)

A

(θ) (cid:107)x ◦ (w + cs)(cid:107)2
2

(10)

The (cid:107)x ◦ (w + cs)(cid:107)2
2, already shown in Eq. (8), consists
of the combination of L0, L1, L2 regularization terms. It
tends to penalize the weight whose corresponding feature’s
magnitude is large. Meanwhile, the weights whose corre-
sponding features are always zeros are less penalized. The
(θ) is proportional to the variance of prediction y
term A
given x and w. Penalizing A
(θ) encourages the weights to
move towards making the model be more ”conﬁdent” about
its predication, i.e. be more discriminative.

(cid:48)(cid:48)

(cid:48)(cid:48)

Generally speaking, Shakeout regularizer adaptively
combines L0, L1 and L2 regularization terms, the property
of which matches what we have observed in Fig. 3. It
prefers penalizing the weights who have large magnitudes
and encourages the weights to move towards making the
model more discriminative. Moreover, the weights whose
corresponding features are always zeros are less penalized.
The L0 and L1 components can induce sparse weights.

Last but not the least, we want to emphasize that when
τ = 0, the noise is eliminated and the model becomes a
standard GLM. Moreover, Dropout can be viewed as the
special case of Shakeout when c = 0, and a higher value of
τ means a stronger L2 regularization effect imposed on the
weights. Generally, when τ is ﬁxed (τ (cid:54)= 0), a higher value
of c means a stronger effect of the L0 and L1 components

3.2 Shakeout in Multilayer Neural Networks

It has been illustrated that Shakeout regularizes the weights
in linear modules. Linear module is the basic component
of multilayer neural networks. That is, the linear operations
connect the outputs of two successive layers. Thus Shakeout
is readily applicable to the training of multilayer neural
networks.

Considering the forward computation from layer l to
layer l + 1, for a fully-connected layer, the Shakeout forward
computation is as follows

ui =

xj[rjWij + c(rj − 1)Sij] + bi

(11)

(cid:88)

j

(cid:48)
i = f (ui)
x

(12)

where i denotes the index of the output unit of layer l + 1,
and j denotes the index of the output unit of layer l. The
output unit of a layer is represented by x. The weight of the
connection between unit xj and unit x
i is represented as
Wij. The bias for the i-th unit is denoted by bi. The Sij
is the sign of corresponding weight Wij. After Shakeout
operation, the linear combination ui is sent to the activation
function f (·) to obtain the corresponding output x
i. Note
that the weights Wij that connect to the same input unit xj
are controlled by the same random variable rj.

(cid:48)

(cid:48)

During back-propagation, we should compute the gra-
dients with respect to each unit to propagate the error. In
Shakeout, ∂ui
∂xj

takes the form

= rj(Wij + cSij) − cSij

(13)

And the weights are updated following

= xj(rj + c(rj − 1)

(14)

dSij
dWij

)

dSij
dWij

represents the derivative of a sgn function.
where
Because the sgn function is not continuous at zero and

∂ui
∂xj

∂ui
∂Wij

6

Fig. 3. The contour plots of the regularization effect induced by Shakeout in 2D weight space with input x = [1, 1]T . Note that Dropout is a special
case of Shakeout with c = 0.

thus the derivative is not deﬁned, we approximate this
. Empirically we ﬁnd that this
derivative with
approximation works well.

d tanh(Wij )
dWij

Note that

the forward-backward computations with
Shakeout can be easily extended to the convolutional layer.
For a convolutional layer, the Shakeout feed-forward pro-
cess can be formalized as

Ui =

(Xj ◦ Rj) ∗ Wij + c(Xj ◦ (Rj − 1)) ∗ Sij + bi (15)

(cid:88)

j

(cid:48)

X

(16)

i = f (Ui)
where Xj represents the j-th feature map. Rj is the j-th
random mask which has the same spatial structure (i.e. the
same height and width) as the corresponding feature map
Xj. Wij denotes the kernel connecting Xj and Ui. And Sij
is set as sgn(Wij). The symbol * denotes the convolution
operation. And the symbol ◦ means element-wise product.
Correspondingly, during the back-propagation process,
the gradient with respect to a unit of the layer on which
Shakeout is applied takes the form

∂Ui(a, b)
∂Xj(a − a(cid:48), b − b(cid:48))

= Rj(a − a

, b − b

)(Wij(a

(cid:48)

(cid:48)

(cid:48)

(cid:48)

, b

)+

(cid:48)

(cid:48)

cSij(a

, b

)) − cSij(a

(cid:48)

(cid:48)

, b

)

(17)

where (a, b) means the position of a unit in the output
) represents the position
feature map of a layer, and (a
of a weight in the corresponding kernel.

, b

(cid:48)

(cid:48)

The weights are updated following

∂Ui(a, b)
∂Wij(a(cid:48), b(cid:48))

= Xj(a − a

(cid:48)

(cid:48)
, b − b

)(Rj(a − a

, b − b

)

(cid:48)

(cid:48)

+ c(Rj(a − a

(cid:48)

(cid:48)
, b − b

) − 1)

(cid:48)

(cid:48)
, b

)
dSij(a
dWij(a(cid:48), b(cid:48))

)

(18)

4 EXPERIMENTS
In this section, we report empirical evaluations of Shake-
out in training deep neural networks on representative
datasets. The experiments are performed on three kinds of
image datasets: the hand-written image dataset MNIST [40],
the CIFAR-10 image dataset [49] and the ImageNet-2012
dataset [50]. MNIST consists of 60,000+10,000 (training+test)
28×28 images of hand-written digits. CIFAR-10 contains
50,000+10,000 (training+test) 32×32 images of 10 object
classes. ImageNet-2012 consists of 1,281,167+50,000+150,000
(training+validation+test) variable-resolution images of
1000 object classes. We ﬁrst demonstrate that Shakeout leads
to sparse models as our theoretical analysis implies under
the unsupervised setting. Then we show that for the classi-
ﬁcation task, the sparse models have desirable generaliza-
tion performances. Further, we illustrate the regularization
effect of Shakeout on the weights in the classiﬁcation task.
Moreover, the effect of Shakeout on stabilizing the training
processes of the deep architectures is demonstrated. Finally,
we give some practical recommendations of Shakeout. All

the experiments are implemented based on the modiﬁca-
tions of Caffe library [51]. Our code is released on the github:
https://github.com/kgl-prml/shakeout-for-caffe.

4.1 Shakeout and Weight Sparsity

implicitly imposes L0 penalty and L1
Since Shakeout
penalty of the weights, we expect the weights of neural
networks learned by Shakeout contain more zeros than
those learned by the standard back-propagation (BP) [52] or
Dropout [13]. In this experiment, we employ an autoencoder
model for the MNIST hand-written data, train the model
using standard BP, Dropout and Shakeout, respectively, and
compare the degree of sparsity of the weights of the learned
encoders. For the purpose of demonstration, we employ
the simple autoencoder with one hidden layer of 256 units;
Dropout and Shakeout are applied on the input pixels.

To verify the regularization effect, we compare the
weights of the four autoencoders trained under different
settings which correspond to standard BP, Dropout (τ = 0.5)
and Shakeout (τ = 0.5, c = {1, 10}). All the training
methods aim to produce hidden units which can capture
good visual features of the handwritten digits. The statistical
traits of these different resulting weights are shown in Fig. 4.
Moreover, Fig. 5 shows the features captured by each hidden
unit of the autoencoders.

As shown in the Fig. 4, the probability density of weights
around the zero obtained by standard BP training is quite
small compared to the one obtained either by Dropout or
Shakeout. This indicates the strong regularization effect in-
duced by Dropout and Shakeout. Furthermore, the sparsity
level of weights obtained from training by Shakeout is much
higher than the one obtained from training by Dropout.
Using the same τ , increasing c makes the weights much
sparser, which is consistent with the characteristics of L0
penalty and L1 penalty induced by Shakeout. Intuitively,
we can ﬁnd that due to the induced L2 regularization,
the distribution of weights obtained from training by the
Dropout is like a Gaussian, while the one obtained from
training by Shakeout is more like a Laplacian because of
the additionally induced L1 regularization. Fig. 5 shows
that features captured by the hidden units via standard
BP training are not directly interpretable, corresponding to
insigniﬁcant variants in the training data. Both Dropout and
Shakeout suppress irrelevant weights by their regularization
effects, where Shakeout produces much sparser and more
global features thanks to the combination of L0, L1 and L2
regularization terms.

The autoencoder trained by Dropout or Shakeout can
be viewed as the denosing autoencoder, where Dropout
or Shakeout injects special kind of noise into the inputs.
Under this unsupervised setting, the denoising criterion
(i.e. minimizing the error between imaginary images recon-
structed from the noisy inputs and the real images without
noise) is to guide the learning of useful high level feature
representations [11], [12]. To verify that Shakeout helps
learn better feature representations, we adopt the hidden
layer activations as features to train SVM classiﬁers, and the
classiﬁcation accuracies on test set for standard BP, Dropout
and Shakeout are 95.34%, 96.41% and 96.48%, respectively.
We can see that Shakeout leads to much sparser weights
without defeating the main objective.

7

Fig. 4. Distributions of the weights of the autoencoder models learned
by different training approaches. Each curve in the ﬁgure shows the
frequencies of the weights of an autoencoder taking particular values,
i.e. the empirical population densities of the weights. The ﬁve curves
correspond to ﬁve autoencoders learned by standard back-propagation,
Dropout (τ = 0.5), Gaussian Dropout (σ2 = 1) and Shakeout (τ = 0.5,
c = {1, 10}). The sparsity of the weights obtained via Shakeout can be
seen by comparing the curves.

Gaussian Dropout has similar effect on the model train-
ing as standard Dropout [34], which multiplies the acti-
vation of each unit by a Gaussian variable with mean 1
and variance σ2. The relationship between σ2 and τ is
that σ2 = τ
1−τ . The distribution of the weights trained by
Gaussian Dropout (σ2 = 1, i.e. τ = 0.5) is illustrated in
Fig. 4. From Fig. 4, we ﬁnd no notable statistical difference
between two kinds of Dropout implementations which all
exhibit a kind of L2 regularization effect on the weights.
The classiﬁcation performances of SVM classiﬁers on test set
based on the hidden layer activations as extracted features
for both kinds of Dropout implementations are quite similar
(i.e. 96.41% and 96.43% for standard and Gaussian Dropout
respectively). Due to these observations, we conduct the fol-
lowing classiﬁcation experiments using standard Dropout
as a representative implementation (of Dropout) for com-
parison.

4.2 Classiﬁcation Experiments

Sparse models often indicate lower complexity and better
generalization performance [26], [39], [53], [54]. To verify
the effect of L0 and L1 regularization terms induced by
Shakeout on the model performance, we apply Shakeout,
along with Dropout and standard BP, on training represen-
tative deep neural networks for classiﬁcation tasks. In all of
our classiﬁcation experiments, the hyper-parameters τ and
c in Shakeout, and the hyper-parameter τ in Dropout are
determined by validation.

4.2.1 MNIST

We train two different neural networks, a shallow fully-
connected one and a deep convolutional one. For the
fully-connected neural network, a big hidden layer size is
adopted with its value at 4096. The non-linear activation

8

(a) standard BP

(b) Dropout: τ = 0.5

(c) Shakeout: τ = 0.5, c = 0.5

Fig. 5. Features captured by the hidden units of the autoencoder models learned by different training methods. The features captured by a hidden
unit are represented by a group of weights that connect the image pixels with this corresponding hidden unit. One image patch in a sub-graph
corresponds to the features captured by one hidden unit.

TABLE 1
The architecture of convolutional neural network adopted for MNIST
classiﬁcation experiment

TABLE 2
Classiﬁcation on MNIST using training sets of different sizes:
fully-connected neural network

Layer
Type
Channels
Filter size
Conv. stride
Pooling type
Pooling size
Pooling stride
Non-linear

1
conv.
20
5 × 5
1
max
2 × 2
2

2
conv.
50
5 × 5
1
max
2 × 2
2

3
FC
500
-
-
-
-
-

4
FC
10
-
-
-
-
-

ReLU ReLU ReLU Softmax

unit adopted is the rectiﬁer linear unit (ReLU). The deep
convolutional neural network employed is based on the
modiﬁcations of the LeNet [40], which contains two con-
volutional layers and two fully-connected layers. The de-
tailed architecture information of this convolutional neural
network is described in Tab. 1. We separate 10,000 training
samples from original training dataset for validation. The
results are shown in Tab. 2 and Tab. 3. Dropout and Shake-
out are applied on the hidden units of the fully-connected
layer. The table compares the errors of the networks trained
by standard back-propagation, Dropout and Shakeout. The
mean and standard deviation of the classiﬁcation errors
are obtained by 5 runs of the experiment and are shown
in percentage. We can see from the results that when the
training data is not sufﬁcient enough, due to over-ﬁtting, all
the models perform worse. However, the models trained by
Dropout and Shakeout consistently perform better than the
one trained by standard BP. Moreover, when the training
data is scarce, Shakeout leads to superior model perfor-
mance compared to the Dropout. Fig. 6 shows the results
in a more intuitive way.

4.2.2 CIFAR-10

We use the simple convolutional network feature extractor
described in cuda-convnet (layers-80sec.cfg) [55]. We apply
Dropout and Shakeout on the ﬁrst fully-connected layer. We
call this architecture “AlexFastNet” for the convenience of
description. In this experiment, 10,000 colour images are

Size
500
1000
3000
8000
20000
50000

std-BP
13.66±0.66
8.49±0.23
5.54±0.09
3.57±0.14
2.28±0.09
1.55±0.03

Dropout
11.76±0.09
8.05±0.05
4.87±0.06
2.95±0.05
1.82±0.07
1.36±0.03

Shakeout
10.81±0.32
7.19±0.15
4.60±0.07
2.96±0.09
1.92±0.06
1.35±0.07

TABLE 3
Classiﬁcation on MNIST using training sets of different sizes:
convolutional neural network

Size
500
1000
3000
8000
20000
50000

std-BP
9.76±0.26
6.73±0.12
2.93±0.10
1.70±0.03
0.97±0.01
0.78±0.05

Dropout
6.16±0.23
4.01±0.16
2.06±0.06
1.23±0.13
0.83±0.06
0.62±0.04

Shakeout
4.83±0.11
3.43±0.06
1.86±0.13
1.31±0.06
0.77±0.001
0.58±0.10

TABLE 4
Classiﬁcation on CIFAR-10 using training sets of different sizes:
AlexFastNet

Size
300
700
2000
5500
15000
40000

std-BP
68.26±0.57
59.78±0.24
50.73±0.29
41.41±0.52
32.53±0.25
24.48±0.23

Dropout
65.34±0.75
56.04±0.22
46.24±0.49
36.01±0.13
27.28±0.26
20.50±0.32

Shakeout
63.71±0.28
54.66±0.22
44.39±0.41
34.54±0.31
26.53±0.17
20.56±0.12

separated from the training dataset for validation and no
data augmentation is utilized. The per-pixel mean computed
over the training set is subtracted from each image. We ﬁrst
train for 100 epochs with an initial learning rate of 0.001
and then another 50 epochs with the learning rate of 0.0001.
The mean and standard deviation of the classiﬁcation errors
are obtained by 5 runs of the experiment and are shown in
percentage. As shown in Tab. 4, the performances of models
trained by Dropout and Shakeout are consistently superior

9

(a) Fully-connected neural network

(b) Convolutional neural network

Fig. 6. Classiﬁcation of two kinds of neural networks on MNIST using training sets of different sizes. The curves show the performances of the
models trained by standard BP, and those by Dropout and Shakeout applied on the hidden units of the fully-connected layer.

TABLE 5
Classiﬁcation on CIFAR-10 using training sets of different sizes:
WRN-16-4

Size
15000
40000
50000

std-BP Dropout
20.95
15.37
14.39

15.05
9.32
8.03

Shakeout
14.68
9.01
7.97

training set size at 40000. From Tab. 5, we can arrive at
the same conclusion as previous experiments, i.e. the per-
formances of the models trained by Dropout and Shakeout
are consistently superior to the one trained by standard BP.
Moreover, Shakeout outperforms Dropout when the data is
scarce.

4.2.3 Regularization Effect on the Weights

Shakeout is a different way to regularize the training process
of deep neural networks from Dropout. For a GLM model,
we have proved that the regularizer induced by Shakeout
adaptively combines L0, L1 and L1 regularization terms.
In section 4.1, we have demonstrated that for a one-hidden
layer autoencoder, it leads to much sparser weights of the
model. In this section, we will illustrate the regularization
effect of Shakeout on the weights in the classiﬁcation task
and make a comparison to that of Dropout.

The results shown in this section are mainly based on the
experiments conducted on ImageNet-2012 dataset using the
representative deep architecture: AlexNet [14]. For AlexNet,
we apply Dropout or Shakeout on layers FC7 and FC8
which are the last two fully-connected layers. We train the
model from the scratch and obtain the comparable classi-
ﬁcation performances on validation set for Shakeout (top-1
error: 42.88%; top-5 error: 19.85%) and Dropout (top-1 error:
42.99%; top-5 error: 19.60%). The model is trained based on
the same hyper-parameter settings provided by Shelhamer
in Caffe [51] other than the hyper-parameters τ and c for
Shakeout. The initial weights for training by Dropout and
Shakeout are kept the same.

Fig. 8 illustrates the distributions of the magnitude of
weight resulted by Shakeout and Dropout. It can be seen
that the weights learned by Shakeout are much sparser than

Fig. 7. Classiﬁcation on CIFAR-10 using training sets of different sizes.
The curves show the performances of the models trained by standard
BP, and those by Dropout and Shakeout applied on the hidden units of
the fully-connected layer.

to the one trained by standard BP. Furthermore, the model
trained by Shakeout also outperforms the one trained by
Dropout when the training data is scarce. Fig. 7 shows the
results in a more intuitive way.

To test the performance of Shakeout on a much deeper
architecture, we also conduct experiments based on the
Wide Residual Network (WRN) [15]. The conﬁguration of
WRN adopted is WRN-16-4, which means WRN has 16
layers in total and the number of feature maps for the
convolutional layer of each residual block is 4 times as the
corresponding original one [1]. Because the complexity is
much higher than that of “AlexFastNet”, the experiments
are performed on relatively larger training sets with sizes
of 15000, 40000, 50000. Dropout and Shakeout are applied
on the second convolutional layer of each residual block,
following the protocol in [15]. All the training starts from
the same initial weights. Batch Normalization is applied the
same way as [15] to promote the optimization. No data-
augmentation or data pre-processing is adopted. All the
other hyper-parameters other than τ and c are set the same
as [15]. The results are listed in Tab. 5. For the training
of CIFAR-10 with 50000 training samples, we adopt the
same hyper-parameters as those chosen in the training with

10

(a) AlexNet FC7 layer

(b) AlexNet FC8 layer

Fig. 8. Comparison of the distributions of the magnitude of weights trained by Dropout and Shakeout. The experiments are conducted using AlexNet
on ImageNet-2012 dataset. Shakeout or Dropout is applied on the last two fully-connected layers, i.e. FC7 layer and FC8 layer.

(a) AlexNet FC7 layer

(b) AlexNet FC8 layer

Fig. 9. Distributions of the maximum magnitude of the weights connected to the same input unit of a layer. The maximum magnitude of the weights
connected to one input unit can be regarded as a metric of the importance of that unit. The experiments are conducted using AlexNet on ImageNet-
2012 dataset. For Shakeout, the units can be approximately separated into two groups and the one around zero is less important than the other,
whereas for Dropout, the units are more concentrated.

those learned by Dropout, due to the implicitly induced L0
and L1 components.

The regularizer induced by Shakeout not only contains
L0 and L1 regularization terms but also contains L2 reg-
ularization term, the combination of which is expected to
discard a group of weights simultaneously. In Fig. 9, we use
the maximum magnitude of the weights connected to one
input unit of a layer to represent the importance of that unit
for the subsequent output units. From Fig. 9, it can be seen
that for Shakeout, the units can be approximately separated
into two groups according to the maximum magnitudes
of the connected weights and the group around zero can
be discarded, whereas for Dropout, the units are concen-
trated. This implies that compared to Dropout which may
encourage a “distributed code” for the features captured by
the units of a layer, Shakeout tends to discard the useless
features (or units) and award the important ones. This
experiment result veriﬁes the regularization properties of
Shakeout and Dropout further.

As known to us, L0 and L1 regularization terms are
related to performing feature selection [56], [57]. For a deep
architecture, it is expected to obtain a set of weights using
Shakeout suitable for reﬂecting the importance of connec-
tions between units. We perform the following experiment
to verify this effect. After a model is trained, for the layer
on which Dropout or Shakeout is applied, we sort the
magnitudes of the weights increasingly. Then we prune the
ﬁrst m% of the sorted weights and evaluate the performance
of the pruned model again. The pruning ratio m goes from 0
to 1. We calculate the relative accuracy loss (we write R.A.L
for simpliﬁcation) at each pruning ratio m

as

(cid:48)

R.A.L(m

) =

(cid:48)

Accu.(m = 0) − Accu.(m
Accu.(m = 0)

(cid:48)

)

Fig. 10 shows the R.A.L curves for Dropout and Shake-
out based on the AlexNet model on ImageNet-2012 dataset.
The models trained by Dropout and Shakeout are under the
optimal hyper-parameter settings. Apparently, the relative

accuracy loss for Dropout is more severe than that for Shake-
out. For example, the largest margin of the relative accuracy
losses between Dropout and Shakeout is 22.50%, which
occurs at the weight pruning ratio m = 96%. This result
proves that considering the trained weights in reﬂecting the
importance of connections, Shakeout is much better than
Dropout, which beneﬁts from the implicitly induced L0 and
L1 regularization effect. This kind of property is useful for
the popular compression task in deep learning area which
aims to cut the connections or throw units of a deep neural
network to a maximum extent without obvious loss of ac-
curacy. The above experiments illustrate that Shakeout can
play a considerable role in selecting important connections,
which is meaningful for promoting the performance of a
compression task. This is a potential subject for the future
research.

4.3 Stabilization Effect on the Training Process

In both research and production, it is always desirable to
have a level of certainty about how a model’s ﬁtness to
the data improves over optimization iterations, namely, to
have a stable training process. In this section, we show that
Shakeout helps reduce ﬂuctuation in the improvement of
model ﬁtness during training.

The ﬁrst experiment is on the family of Generative
Adversarial Networks (GANs) [58], which is known to be
instable in the training stage [59], [60], [61]. The purpose of
the following tests is to demonstrate the Shakeout’s capa-
bility of stabilizing the training process of neural networks
in a general sense. GAN plays a min-max game between
the generator G and the discriminator D over the expected
log-likelihood of real data x and imaginary data ˆx = G(z)
where z represents the random input

min
G

max
D

V (D, G) = E[log[D(x)] + log[1 − D(G(z))]] (19)

The architecture that we adopt is DCGAN [59]. The numbers
of feature maps of the deconvolutional layers in the gener-
ator are 1024, 64 and 1 respectively, with the corresponding
spatial sizes 7×7, 14×14 and 28×28. We train DCGANs on
MNIST dataset using standard BP, Dropout and Shakeout.
We follow the same experiment protocol described in [59]
except for adopting Dropout or Shakeout on all layers of the
discriminator. The values of −V (D, G) during training are
illustrated in Fig. 11. It can be seen that −V (D, G) during
training by standard BP oscillates greatly, while for Dropout
and Shakeout, the training processes are much steadier.
Compared with Dropout, the training process by Shakeout
has fewer spikes and is smoother. Fig. 12 demonstrates the
minimum and maximum values of −V (D, G) within ﬁxed
length intervals moving from the start to the end of the
training by standard BP, Dropout and Shakeout. It can be
seen that the gaps between the minimum and maximum
values of −V (D, G) trained by Dropout and Shakeout are
much smaller than that trained by standard BP, while that
by Shakeout is the smallest, which implies the stability of
the training process by Shakeout is the best.

The second experiment is based on Wide Residual Net-
work architecture to perform the classiﬁcation task. In the
classiﬁcation task, generalization performance is the main
focus and thus, we compare the validation errors during

11

the training processes by Dropout and Shakeout. Fig. 13
demonstrates the validation error as a function of the
training epoch for Dropout and Shakeout on CIFAR-10
with 40000 training examples. The architecture adopted is
WRN-16-4. The experiment settings are the same as those
described in Section 4.2.2. Considering the generalization
performance, the learning rate schedule adopted is the one
optimized through validation to make the models obtain the
best generalization performances. Under this schedule, we
ﬁnd that the validation error temporarily increases when
lowering the learning rate at the early stage of training,
which has been repeatedly observed by [15]. Nevertheless,
it can be seen from Fig. 13 that the extent of error increase is
less severe for Shakeout than Dropout. Moreover, Shakeout
recovers much faster than Dropout does. At the ﬁnal stage,
both of the validation errors steadily decrease. Shakeout
obtains comparable or even superior generalization perfor-
mance to Dropout. In a word, Shakeout signiﬁcantly stabi-
lizes the entire training process with superior generalization
performance.

4.4 Practical Recommendations

Selection of Hyper-parameters The most practical and pop-
ular way to perform hyper-parameter selection is to parti-
tion the training data into a training set and a validation
set to evaluate the classiﬁcation performance of different
hyper-parameters on it. Due to the expensive cost of time
for training a deep neural network, cross-validation is barely
adopted. There exist many hyper-parameter selection meth-
ods in the domain of deep learning, such as the grid search,
random search [62], Bayesian optimization methods [63],
gradient-based hyper-parameter Optimization [64], etc. For
applying Shakeout on a deep neural network, we need to
decide two hyper-parameters τ and c. From the regular-
ization perspective, we need to decide the most suitable
strength of regularization effect to obtain an optimal trade-
off between model bias and variance. We have pointed out
that in a uniﬁed framework, Dropout is a special case of
Shakeout when Shakeout hyper-parameter c is set to zero.
Empirically we ﬁnd that the optimal τ for Shakeout is not
higher than that for Dropout. After determining the optimal
τ , keeping the order of magnitude of hyper parameter c
N (N represents the number of training
the same as
samples) is an effective choice. If you want to obtain a model
with much sparser weights but meanwhile with superior
or comparable generalization performance to Dropout, a
relatively lower τ and larger c for Shakeout always works.
Shakeout combined with Batch Normalization Batch Nor-
malization [65] is the widely-adopted technique to promote
the optimization of the training process for a deep neural
network. In practice, combining Shakeout with Batch Nor-
malization to train a deep architecture is a good choice.
For example, we observe that the training of WRN-16-4
model on CIFAR-10 is slow to converge without using Batch
Normalization in the training. Moreover, the generalization
performance on the test set for Shakeout combined with
Batch Normalization always outperforms that for standard
BP with Batch Normalization consistently for quite a large
margin, as illustrated in Tab. 5. These results imply the

(cid:113) 1

12

(a) standard BP

(b) Dropout

(c) Shakeout

Fig. 11. The value of −V (D, G) as a function of iteration for the training process of DCGAN. DCGANs are trained using standard BP, Dropout and
Shakeout for comparison. Dropout or Shakeout is applied on the discriminator of GAN.

Fig. 10. Relative accuracy loss as a function of the weight pruning ratio
for Dropout and Shakeout based on AlexNet architecture on ImageNet-
2012. The relative accuracy loss for Dropout is much severe than that for
Shakeout. The largest margin of the relative accuracy losses between
Dropout and Shakeout is 22.50%, which occurs at the weight pruning
ratio m = 96%.

Fig. 13. Validation error as a function of training epoch for Dropout and
Shakeout on CIFAR-10 with training set size at 40000. The architec-
ture adopted is WRN-16-4. “DPO” and “SKO” represent “Dropout” and
“Shakeout” respectively. The following two numbers denote the hyper-
parameters τ and c respectively. The learning rate decays at epoch 60,
120, and 160. After the ﬁrst decay of learning rate, the validation error in-
creases greatly before the steady decrease (see the enlarged snapshot
for training epochs from 60 to 80). It can be seen that the extent of error
increase is less severe for Shakeout than Dropout. Moreover, Shakeout
recovers much faster than Dropout does. At the ﬁnal stage, both of
the validation errors steadily decrease (see the enlarged snapshot for
training epochs from 160 to 200). Shakeout obtains comparable or even
superior generalization performance to Dropout.

important role of Shakeout in reducing over-ﬁtting of a deep
neural network.

Fig. 12. The minimum and maximum values of −V (D, G) within ﬁxed
length intervals moving from the start to the end of the training by stan-
dard BP, Dropout and Shakeout. The optimal value log(4) is obtained
when the imaginary data distribution P (ˆx) matches with the real data
distribution P (x).

5 CONCLUSION

We have proposed Shakeout, which is a new regularized
training approach for deep neural networks. The regularizer
induced by Shakeout is proved to adaptively combine L0,
L1 and L2 regularization terms. Empirically we ﬁnd that

1) Compared to Dropout, Shakeout can afford much
larger models. Or to say, when the data is scarce, Shakeout
outperforms Dropout with a large margin.

2) Shakeout can obtain much sparser weights than
Dropout with superior or comparable generalization per-
formance of the model. While for Dropout, if one wants
to obtain the same level of sparsity as that obtained by
Shakeout, the model may bear a signiﬁcant loss of accuracy.
3) Some deep architectures in nature may result in the
instability of the training process, such as GANs, however,
Shakeout can reduce this instability effectively.

In future, we want to put emphasis on the inductive
bias of Shakeout and attempt to apply Shakeout to the
compression task.

ACKNOWLEDGMENTS

This research is supported by Australian Research Coun-
cil Projects (No. FT-130101457, DP-140102164 and LP-
150100671).

REFERENCES

[1] K. He, X. Zhang, S. Ren, and J. Sun, “Identity mappings in deep
residual networks,” in European Conference on Computer Vision.
Springer, 2016, pp. 630–645.

[2] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, “Inception-
v4, inception-resnet and the impact of residual connections on
learning,” in Proceedings of the Thirty-First AAAI Conference on
Artiﬁcial Intelligence, February 4-9, 2017, San Francisco, California,
USA., 2017, pp. 4278–4284.

[3] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Region-based
convolutional networks for accurate object detection and seg-
mentation,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 38, no. 1, pp. 142–158, Jan 2016.

[4] Y. Sun, X. Wang, and X. Tang, “Hybrid deep learning for face
veriﬁcation,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 38, no. 10, pp. 1997–2009, Oct 2016.

[5] Y. Zheng, Y. J. Zhang, and H. Larochelle, “A deep and autore-
gressive approach for topic modeling of multimodal data,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 38,
no. 6, pp. 1056–1069, June 2016.

[6] Y. Wei, W. Xia, M. Lin, J. Huang, B. Ni, J. Dong, Y. Zhao, and
S. Yan, “Hcp: A ﬂexible cnn framework for multi-label image
classiﬁcation,” IEEE transactions on pattern analysis and machine
intelligence, vol. 38, no. 9, pp. 1901–1907, 2016.

[7] X. Jin, C. Xu, J. Feng, Y. Wei, J. Xiong, and S. Yan, “Deep learning
with s-shaped rectiﬁed linear activation units,” arXiv preprint
arXiv:1512.07030, 2015.

[8] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning algo-
rithm for deep belief nets,” Neural computation, vol. 18, no. 7, pp.
1527–1554, 2006.

[9] Y. Bengio, “Learning deep architectures for AI,” Foundations and

trends R(cid:13) in Machine Learning, vol. 2, no. 1, pp. 1–127, 2009.

[10] Y. Bengio, A. Courville, and P. Vincent, “Representation learning:
A review and new perspectives,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1798–1828, Aug
2013.

[11] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, “Extract-
ing and composing robust features with denoising autoencoders,”
in Proceedings of the 25th international conference on Machine learning.
ACM, 2008, pp. 1096–1103.

[12] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol,
“Stacked denoising autoencoders: Learning useful representations
in a deep network with a local denoising criterion,” The Journal of
Machine Learning Research, vol. 11, pp. 3371–3408, 2010.

[13] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R.
Salakhutdinov, “Improving neural networks by preventing co-
adaptation of feature detectors,” arXiv preprint arXiv:1207.0580,
2012.

13

[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁ-
cation with deep convolutional neural networks,” in Advances in
neural information processing systems, 2012, pp. 1097–1105.

[15] S. Zagoruyko and N. Komodakis, “Wide residual networks,” in
Proceedings of the British Machine Vision Conference 2016, BMVC
2016, York, UK, September 19-22, 2016, 2016.

[16] S. Wager, S. Wang, and P. S. Liang, “Dropout training as adap-
tive regularization,” in Advances in Neural Information Processing
Systems, 2013, pp. 351–359.

[17] N. Chen, J. Zhu, J. Chen, and B. Zhang, “Dropout training for
support vector machines,” in Twenty-Eighth AAAI Conference on
Artiﬁcial Intelligence, 2014.

[18] L. Van Der Maaten, M. Chen, S. Tyree, and K. Q. Weinberger,
“Learning with marginalized corrupted features.” in ICML (1),
2013, pp. 410–418.

[19] Y. LeCun, J. S. Denker, S. A. Solla, R. E. Howard, and L. D. Jackel,
“Optimal brain damage.” in NIPs, vol. 2, 1989, pp. 598–605.
[20] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen,
“Compressing neural networks with the hashing trick,” in Proceed-
ings of the 32nd International Conference on Machine Learning, ICML
2015, Lille, France, 6-11 July 2015, 2015, pp. 2285–2294.

[21] S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights and
connections for efﬁcient neural network,” in Advances in Neural
Information Processing Systems, 2015, pp. 1135–1143.

[22] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing
deep neural network with pruning, trained quantization and
huffman coding,” CoRR, abs/1510.00149, vol. 2, 2015.

[23] M. Denil, B. Shakibi, L. Dinh, N. de Freitas et al., “Predicting
parameters in deep learning,” in Advances in Neural Information
Processing Systems, 2013, pp. 2148–2156.

[24] J. Ba and R. Caruana, “Do deep nets really need to be deep?” in
Advances in neural information processing systems, 2014, pp. 2654–
2662.

[25] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a

neural network,” arXiv preprint arXiv:1503.02531, 2015.

[26] H. Zou and T. Hastie, “Regularization and variable selection via
the elastic net,” Journal of the Royal Statistical Society: Series B
(Statistical Methodology), vol. 67, no. 2, pp. 301–320, 2005.

[27] G. Kang, J. Li, and D. Tao, “Shakeout: A new regularized deep
neural network training scheme,” in Thirtieth AAAI Conference on
Artiﬁcial Intelligence, 2016.

[28] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent,
and S. Bengio, “Why does unsupervised pre-training help deep
learning?” The Journal of Machine Learning Research, vol. 11, pp.
625–660, 2010.

[29] J. Moody, S. Hanson, A. Krogh, and J. A. Hertz, “A simple weight
decay can improve generalization,” Advances in neural information
processing systems, vol. 4, pp. 950–957, 1995.

[30] L. Prechelt, “Automatic early stopping using cross validation:
quantifying the criteria,” Neural Networks, vol. 11, no. 4, pp. 761–
767, 1998.

[31] L. Wan, M. Zeiler, S. Zhang, Y. L. Cun, and R. Fergus, “Regulariza-
tion of neural networks using dropconnect,” in Proceedings of the
30th International Conference on Machine Learning (ICML-13), 2013,
pp. 1058–1066.

[32] J. Ba and B. Frey, “Adaptive dropout for training deep neural
networks,” in Advances in Neural Information Processing Systems,
2013, pp. 3084–3092.

[33] Z. Li, B. Gong, and T. Yang, “Improved dropout for shallow
and deep learning,” in Advances In Neural Information Processing
Systems, 2016, pp. 2523–2531.

[34] N. Srivastava, G. Hinton, A. Krizhevsky,

I. Sutskever, and
R. Salakhutdinov, “Dropout: A simple way to prevent neural net-
works from overﬁtting,” The Journal of Machine Learning Research,
vol. 15, no. 1, pp. 1929–1958, 2014.

[35] D. Warde-Farley, I. J. Goodfellow, A. Courville, and Y. Bengio, “An
empirical analysis of dropout in piecewise linear networks,” arXiv
preprint arXiv:1312.6197, 2013.

[36] P. Baldi and P. J. Sadowski, “Understanding dropout,” in Advances
in Neural Information Processing Systems, 2013, pp. 2814–2822.
[37] Y. Gal and Z. Ghahramani, “Dropout as a bayesian approximation:
Representing model uncertainty in deep learning,” in Proceedings
of the 33nd International Conference on Machine Learning, ICML 2016,
New York City, NY, USA, June 19-24, 2016, 2016, pp. 1050–1059.
[38] D. P. Helmbold and P. M. Long, “On the inductive bias of
dropout,” Journal of Machine Learning Research, vol. 16, pp. 3403–
3454, 2015.

14

[63] J. Snoek, H. Larochelle, and R. P. Adams, “Practical bayesian op-
timization of machine learning algorithms,” in Advances in neural
information processing systems, 2012, pp. 2951–2959.

[64] D. Maclaurin, D. Duvenaud, and R. P. Adams, “Gradient-based
hyperparameter optimization through reversible learning,” in Pro-
ceedings of the 32nd International Conference on Machine Learning,
2015.

[65] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep
network training by reducing internal covariate shift,” in Proceed-
ings of the 32nd International Conference on Machine Learning, ICML
2015, Lille, France, 6-11 July 2015, 2015, pp. 448–456.

[39] B. A. Olshausen and D. J. Field, “Sparse coding with an overcom-
plete basis set: A strategy employed by v1?” Vision research, vol. 37,
no. 23, pp. 3311–3325, 1997.

[40] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based
learning applied to document recognition,” Proceedings of the IEEE,
vol. 86, no. 11, pp. 2278–2324, 1998.

[41] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with
convolutions,” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2015, pp. 1–9.

[42] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna,
“Rethinking the inception architecture for computer vision,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2016, pp. 2818–2826.

[43] Y.-l. Boureau, Y. L. Cun et al., “Sparse feature learning for deep be-
lief networks,” in Advances in neural information processing systems,
2008, pp. 1185–1192.

[44] I. J. Goodfellow, A. Courville, and Y. Bengio, “Spike-and-slab
sparse coding for unsupervised feature discovery,” arXiv preprint
arXiv:1201.3382, 2012.

[45] M. Thom and G. Palm, “Sparse activity and sparse connectivity in
supervised learning,” Journal of Machine Learning Research, vol. 14,
no. Apr, pp. 1091–1143, 2013.

[46] S. Rifai, X. Glorot, Y. Bengio, and P. Vincent, “Adding noise to
the input of a model trained with a regularized objective,” arXiv
preprint arXiv:1104.3250, 2011.

[47] C. M. Bishop, “Training with noise is equivalent to tikhonov
regularization,” Neural computation, vol. 7, no. 1, pp. 108–116, 1995.
[48] W. Jiang, F. Nie, and H. Huang, “Robust dictionary learning with
capped l1-norm,” in Proceedings of the Twenty-Fourth International
Joint Conference on Artiﬁcial Intelligence, IJCAI 2015, Buenos Aires,
Argentina, July 25-31, 2015, 2015, pp. 3590–3596.

[49] A. Krizhevsky and G. Hinton, “Learning multiple layers of fea-
tures from tiny images. Technical report, University of Toronto,”
2009.

[50] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”
International Journal of Computer Vision (IJCV), vol. 115, no. 3, pp.
211–252, 2015.

[51] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture
for fast feature embedding,” in Proceedings of the ACM International
Conference on Multimedia. ACM, 2014, pp. 675–678.

[52] D. R. G. H. R. Williams and G. Hinton, “Learning representations

by back-propagating errors,” Nature, pp. 323–533, 1986.

[53] R. Tibshirani, “Regression shrinkage and selection via the lasso,”
Journal of the Royal Statistical Society. Series B (Methodological), pp.
267–288, 1996.

[54] L. Yuan, J. Liu, and J. Ye, “Efﬁcient methods for overlapping
group lasso,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 35, no. 9, pp. 2104–2116, 2013.

[55] A. Krizhevsky,

“cuda-convnet,”

2012.

[Online]. Available:

https://code.google.com/p/cuda-convnet/

[56] I. Guyon and A. Elisseeff, “An introduction to variable and feature
selection,” Journal of machine learning research, vol. 3, no. Mar, pp.
1157–1182, 2003.

[57] K. Wang, R. He, L. Wang, W. Wang, and T. Tan, “Joint feature
selection and subspace learning for cross-modal retrieval,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 38,
no. 10, pp. 2010–2023, Oct 2016.

[58] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial
nets,” in Advances in neural information processing systems, 2014, pp.
2672–2680.

[59] A. Radford, L. Metz, and S. Chintala, “Unsupervised represen-
tation learning with deep convolutional generative adversarial
networks,” arXiv preprint arXiv:1511.06434, 2015.

[60] M. Arjovsky and L. Bottou, “Towards principled methods for
training generative adversarial networks,” in NIPS 2016 Workshop
on Adversarial Training. In review for ICLR, vol. 2016, 2017.

[61] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” arXiv

preprint arXiv:1701.07875, 2017.

[62] J. Bergstra and Y. Bengio, “Random search for hyper-parameter
optimization,” Journal of Machine Learning Research, vol. 13, no. Feb,
pp. 281–305, 2012.

9
1
0
2
 
r
p
A
 
3
1
 
 
]

V
C
.
s
c
[
 
 
1
v
3
9
5
6
0
.
4
0
9
1
:
v
i
X
r
a

1

Shakeout: A New Approach to Regularized
Deep Neural Network Training

Guoliang Kang, Jun Li, and Dacheng Tao, Fellow, IEEE

Abstract—Recent years have witnessed the success of deep neural networks in dealing with a plenty of practical problems. Dropout
has played an essential role in many successful deep neural networks, by inducing regularization in the model training. In this paper,
we present a new regularized training approach: Shakeout. Instead of randomly discarding units as Dropout does at the training stage,
Shakeout randomly chooses to enhance or reverse each unit’s contribution to the next layer. This minor modiﬁcation of Dropout has the
statistical trait: the regularizer induced by Shakeout adaptively combines L0, L1 and L2 regularization terms. Our classiﬁcation
experiments with representative deep architectures on image datasets MNIST, CIFAR-10 and ImageNet show that Shakeout deals with
over-ﬁtting effectively and outperforms Dropout. We empirically demonstrate that Shakeout leads to sparser weights under both
unsupervised and supervised settings. Shakeout also leads to the grouping effect of the input units in a layer. Considering the weights
in reﬂecting the importance of connections, Shakeout is superior to Dropout, which is valuable for the deep model compression.
Moreover, we demonstrate that Shakeout can effectively reduce the instability of the training process of the deep architecture.

Index Terms—Shakeout, Dropout, Regularization, Sparsity, Deep Neural Network.

(cid:70)

1 INTRODUCTION

D EEP neural networks have recently achieved impres-

sive success in a number of machine learning and
pattern recognition tasks and been under intensive research
[1], [2], [3], [4], [5], [6], [7]. Hierarchical neural networks
have been known for decades, and there are a number of
essential factors contributing to its recent rising, such as
the availability of big data and powerful computational re-
sources. However, arguably the most important contributor
to the success of deep neural network is the discovery of
efﬁcient training approaches [8], [9], [10], [11], [12].

A particular interesting advance in the training tech-
niques is the invention of Dropout [13]. At the operational
level, Dropout adjusts the network evaluation step (feed-
forward) at the training stage, where a portion of units
are randomly discarded. The effect of this simple trick is
impressive. Dropout enhances the generalization perfor-
mance of neural networks considerably, and is behind many
record-holders of widely recognized benchmarks [2], [14],
[15]. The success has attracted much research attention, and
found applications in a wider range of problems [16], [17],
[18]. Theoretical research from the viewpoint of statistical
learning has pointed out the connections between Dropout
and model regularization, which is the de facto recipe of

• Guoliang Kang, Jun Li are with Center of AI, Faculty of Engineering
and Information Technology, University of Technology Sydney, Ultimo,
NSW, Australia. D. Tao is with the UBTech Sydney Artiﬁcial Intelligence
Institute and the School of Information Technologies in the Faculty of
Engineering and Information Technologies at The University of Sydney,
Darlington, NSW 2008, Australia.
E-mail: Guoliang.Kang@student.uts.edu.au,
dacheng.tao@sydney.edu.au
c(cid:13)2018 IEEE. Personal use of this material is permitted. Permission
from IEEE must be obtained for all other uses, in any current or future
media, including reprinting/republishing this material for advertising
or promotional purposes, creating new collective works, for resale or
redistribution to servers or lists, or reuse of any copyrighted component of
this work in other works.

junjy007@googlemail.com,

reducing over-ﬁtting for complex models in practical ma-
chine learning. For example, Wager et al. [16] showed that
for a generalized linear model (GLM), Dropout implicitly
imposes an adaptive L2 regularizer of the network weights
through an estimation of the inverse diagonal Fisher infor-
mation matrix.

importance in deep learning. It
Sparsity is of vital
is straightforward that
through removing unimportant
weights, deep neural networks perform prediction faster.
Additionally, it is expected to obtain better generalization
performance and reduce the number of examples needed in
the training stage [19]. Recently much evidence has shown
that the accuracy of a trained deep neural network will not
be severely affected by removing a majority of connections
and many researchers focus on the deep model compression
task [20], [21], [22], [23], [24], [25]. One effective way of com-
pression is to train a neural network, prune the connections
and ﬁne-tune the weights iteratively [21], [22]. However, if
we can cut the connections naturally via imposing sparsity-
inducing penalties in the training process of a deep neural
network, the work-ﬂow will be greatly simpliﬁed.

In this paper, we propose a new regularized deep neu-
ral network training approach: Shakeout, which is easy to
implement: randomly choosing to enhance or reverse each
unit’s contribution to the next layer in the training stage.
Note that Dropout can be considered as a special “ﬂat” case
of our approach: randomly keeping (enhance factor is 1) or
discarding (reverse factor is 0) each unit’s contribution to
the next layer. Shakeout enriches the regularization effect.
In theory, we prove that it adaptively combines L0, L1 and
L2 regularization terms. L0 and L1 regularization terms
are known as sparsity-inducing penalties. The combination
of sparsity-inducing penalty and L2 penalty of the model
parameters has shown to be effective in statistical learning:
the Elastic Net [26] has the desirable properties of producing
sparse models while maintaining the grouping effect of the

weights of the model. Because of the randomly “shaking”
process and the regularization characteristic pushing net-
work weights to zero, our new approach is named “Shake-
out”.

As discussed above, it is expected to obtain much sparser
weights using Shakeout than using Dropout because of the
combination of L0 and L1 regularization terms induced
in the training stage. We apply Shakeout on one-hidden-
layer autoencoder and obtain much sparser weights than
that resulted by Dropout. To show the regularization effect
on the classiﬁcation tasks, we conduct the experiments on
image datasets including MNIST, CIFAR-10 and ImageNet
with the representative deep neural network architectures.
In our experiments we ﬁnd that by using Shakeout, the
trained deep neural networks always outperform those by
using Dropout, especially when the data is scarce. Besides
the fact that Shakeout leads to much sparser weights, we
also empirically ﬁnd that it groups the input units of a
layer. Due to the induced L0 and L1 regularization terms,
Shakeout can result in the weights reﬂecting the importance
of the connections between units, which is meaningful for
conducting compression. Moreover, we demonstrate that
Shakeout can effectively reduce the instability of the training
process of the deep architecture.

This journal paper extends our previous work [27] theo-
retically and experimentally. The main extensions are listed
as follows: 1) we derive the analytical formula for the
regularizer induced by Shakeout in the context of GLM and
prove several important properties; 2) we conduct experi-
ments using Wide Residual Network [15] on CIFAR-10 to
show Shakeout outperforms Dropout and standard back-
propagation in promoting the generalization performance
of a much deeper architecture; 3) we conduct experiments
using AlexNet [14] on ImageNet dataset with Shakeout
and Dropout. Shakeout obtains comparable classiﬁcation
performance to Dropout, but with superior regularization
effect; 4) we illustrate that Shakeout can effectively reduce
the instability of the training process of the deep architec-
ture. Moreover, we provide a much clearer and detailed de-
scription of Shakeout, derive the forward-backward update
rule for deep convolutional neural networks with Shakeout,
and give several recommendations to help the practitioners
make full use of Shakeout.

In the rest of the paper, we give a review about the
related work in Section 2. Section 3 presents Shakeout in de-
tail, along with theoretical analysis of the regularization ef-
fect induced by Shakeout. In Section 4, we ﬁrst demonstrate
the regularization effect of Shakeout on the autoencoder
model. The classiﬁcation experiments on MNIST , CIFAR-10
and ImageNet illustrate that Shakeout outperforms Dropout
considering the generalization performance, the regulariza-
tion effect on the weights, and the stabilization effect on the
training process of the deep architecture. Finally, we give
some recommendations for the practitioners to make full
use of Shakeout.

2 RELATED WORK

Deep neural networks have shown their success in a wide
variety of applications. One of the key factors contributes to
this success is the creation of powerful training techniques.

2

The representative power of the network becomes stronger
as the architecture gets deeper [9]. However, millions of pa-
rameters make deep neural networks easily over-ﬁt. Regu-
larization [16], [28] is an effective way to obtain a model that
generalizes well. There exist many approaches to regularize
the training of deep neural networks, like weight decay [29],
early stopping [30], etc. Shakeout belongs to the family of
regularized training techniques.

Among these regularization techniques, our work is
closely related to Dropout [13]. Many subsequent works
were devised to improve the performance of Dropout [31],
[32], [33]. The underlying reason why Dropout improves
performance has also attracted the interest of many re-
searchers. Evidence has shown that Dropout may work
because of its good approximation to model averaging
and regularization on the network weights [34], [35], [36].
Srivastava [34] and Warde-Farley [35] exhibited through
experiments that the weight scaling approximation is an
accurate alternative for the geometric mean over all possible
sub-networks. Gal et al. [37] claimed that training the deep
neural network with Dropout is equivalent to performing
variational inference in a deep Gaussian Process. Dropout
can also be regarded as a way of adding noise into the
neural network. By marginalizing the noise, Srivastava [34]
proved for linear regression that the deterministic version
of Dropout is equivalent to adding an adaptive L2 regular-
ization on the weights. Furthermore, Wager [16] extended
the conclusion to generalized linear models (GLMs) using
a quadratic approximation to the induced regularizer. The
inductive bias of Dropout was studied by Helmbold et al.
[38] to illustrate the properties of the regularizer induced
by Dropout further. In terms of implicitly inducing regu-
larizer of the network weights, Shakeout can be viewed as
a generalization of Dropout. It enriches the regularization
effect of Dropout, i.e. besides the L2 regularization term, it
also induces the L0 and L1 regularization terms, which may
lead to sparse weights of the model.

Due to the implicitly induced L0 and L1 regulariza-
tion terms, Shakeout is also related to sparsity-inducing
approaches. Olshausen et al. [39] introduced the concept
of sparsity in computational neuroscience and proposed
the sparse coding method in the visual system. In machine
learning, the sparsity constraint enables a model to capture
the implicit statistical data structure, performs feature selec-
tion and regularization, compresses the data at a low loss of
the accuracy, and helps us to better understand our models
and explain the obtained results. Sparsity is one of the key
factors underlying many successful deep neural network
architectures [2], [40], [41], [42] and training algorithms [43]
[44]. A Convolutional neural network is much sparser than
the fully-connected one, which results from the concept of
local receptive ﬁeld [40]. Sparsity has been a design princi-
ple and motivation for Inception-series models [2], [41], [42].
Besides working as the heuristic principle of designing a
deep architecture, sparsity often works as a penalty induced
to regularize the training process of a deep neural network.
There exist two kinds of sparsity penalties in deep neural
networks, which lead to the activity sparsity [43] [44] and
the connectivity sparsity [45] respectively. The difference
between Shakeout and these sparsity-inducing approaches
is that for Shakeout, the sparsity is induced through sim-

ple stochastic operations rather than manually designed
architectures or explicit norm-based penalties. This implicit
way enables Shakeout to be easily optimized by stochastic
gradient descent (SGD) − the representative approach for
the optimization of a deep neural network.

3 SHAKEOUT

Shakeout applies on the weights in a linear module. The
linear module, i.e. weighted sum,

θ =

wjxj

p
(cid:88)

j=1

(1)

is arguably the most widely adopted component in data
models. For example, the variables x1, x2, . . . , xp can be
input attributes of a model, e.g. the extracted features for
a GLM, or the intermediate outputs of earlier processing
steps, e.g. the activations of the hidden units in a multilayer
artiﬁcial neural network. Shakeout randomly modiﬁes the
computation in Eq. (1). Speciﬁcally, Shakeout can be realized
by randomly modifying the weights

Step 1: Draw rj, where

(cid:40)

P (rj = 0)
P (rj = 1

= τ
1−τ ) = 1 − τ

.

Step 2: Adjust the weight according to rj,

(cid:40)

˜wj ← −csj,
˜wj ← (wj + cτ sj)/(1 − τ )

if rj = 0
otherwise

(A)
(B)

where sj = sgn(wj) takes ±1 depending on the sign of wj
or takes 0 if wj = 0. As shown above, Shakeout chooses
(randomly by drawing r) between two fundamentally dif-
ferent ways to modify the weights. Modiﬁcation (A) is to set
the weights to constant magnitudes, despite their original
values except for the signs (to be opposite to the original
ones). Modiﬁcation (B) updates the weights by a factor
(1 − τ )−1 and a bias depending on the signs. Note both (A)
and (B) preserve zero values of the weights, i.e. if wj = 0
then ˜wj = 0 with probability 1. Let ˜θ = ˜wT x, and Shakeout
leaves θ unbiased, i.e. E[˜θ] = θ. The hyper-parameters
τ ∈ (0, 1) and c ∈ (0, +∞) conﬁgure the property of
Shakeout.

Shakeout is naturally connected to the widely adopted
operation of Dropout [13], [34]. We will show that Shakeout
has regularization effect on model training similar to but
beyond what is induced by Dropout. From an operational
point of view, Fig. 1 compares Shakeout and Dropout. Note
that Shakeout includes Dropout as a special case when the
hyper-parameter c in Shakeout is set to zero.

When applied at the training stage, Shakeout alters the
objective − the quantity to be minimized − by adjusting the
weights. In particular, we will show that Shakeout (with ex-
pectation over the random switch) induces a regularization
term effectively penalizing the magnitudes of the weights
and leading to sparse weights. Shakeout is an approach
designed for helping model training, when the models are
trained and deployed, one should relieve the disturbance to
allow the model work with its full capacity, i.e. we adopt the
resulting network without any modiﬁcation of the weights
at the test stage.

3

Fig. 1. Comparison between Shakeout and Dropout operations. This
ﬁgure shows how Shakeout and Dropout are applied to the weights in a
linear module. In the original linear module, the output is the summation
of the inputs x weighted by w, while for Dropout and Shakeout, the
weights w are ﬁrst randomly modiﬁed. In detail, a random switch ˆr
controls how each w is modiﬁed. The manipulation of w is illustrated
within the ampliﬁer icons (the red curves, best seen with colors). The
coefﬁcients are α = 1/(1 − τ ) and β(w) = cs(w), where s(w) extracts
the sign of w and c > 0, τ ∈ [0, 1]. Note the sign of β(w) is always
the same as that of w. The magnitudes of coefﬁcients α and β(w) are
determined by the Shakeout hyper-parameters τ and c. Dropout can be
viewed as a special case of Shakeout when c = 0 because β(w) is zero
at this circumstance.

3.1 Regularization Effect of Shakeout

Shakeout randomly modiﬁes the weights in a linear module,
and thus can be regarded as injecting noise into each vari-
able xj, i.e. xj is randomly scaled by γj: ˜xj = γjxj. Note
that γj = rj + c(rj −1)
, the modiﬁcation of xj is actually
|wj |
determined by the random switch rj. Shakeout randomly
chooses to enhance (i.e. when rj = 1
1−τ ) or
reverse (i.e. when rj = 0, γj < 0) each original variable
xj’s contribution to the output at the training stage (see Fig.
1). However, the expectation of ˜xj over the noise remains
unbiased, i.e. Erj [˜xj] = xj.

1−τ , γj > 1

It is well-known that injecting artiﬁcial noise into the
input features will regularize the training objective [16],
[46], [47], i.e. Er[(cid:96)(w, ˜x, y)] = (cid:96)(w, x, y) + π(w), where
˜x is the feature vector randomly modiﬁed by the noise
induced by r. The regularization term π(w) is determined
by the characteristic of the noise. For example, Wager et
al. [16] showed that Dropout, corresponding to inducing
blackout noise to the features, helps introduce an adaptive
L2 penalty on w. In this section we illustrate how Shakeout
helps regularize model parameters w using an example of
GLMs.

Formally, a GLM is a probabilistic model of predicting
target y given features x = [x1, . . . , xp], in terms of the
weighted sum in Eq. (1):

P (y|x, w) = h(y)g(θ)eθy
θ = wT x

(2)

With different h(·) and g(·) functions, GLM can be special-
ized to various useful models or modules, such as logistic
regression model or a layer in a feed-forward neural net-
work. However, roughly speaking, the essence of a GLM
is similar to that of a standard linear model which aims

to ﬁnd weights w1, . . . , wp so that θ = wT x aligns with
y (functions h(·) and g(·) are independent of w and y
respectively). The loss function of a GLM with respect to
w is deﬁned as

We illustrate several properties of Shakeout regularizer
based on Eq. (7). The proof of the following propositions
can be found in the appendices.
Proposition 1. π(0) = 0

4

l(w, x, y) = −θy + A(θ)

A(θ) = − ln[g(θ)]

(3)

(4)

The loss (3) is the negative logarithm of probability (2),
where we keep only terms relevant to w.

Let the loss with Shakeout be

lsko(w, x, y, r) := l(w, ˜x, y)
where r = [r1, . . . , rp]T , and ˜x = [˜x1, . . . , ˜xp]T represents
the features randomly modiﬁed with r.

(5)

Taking expectation over r, the loss with Shakeout be-

Er[lsko(w, x, y, r)] = l(w, x, y) + π(w)

comes

where

π(w) = Er[A(˜θ) − A(θ)]
∞
(cid:88)

=

1
k!

k=1

A(k)(θ)E[(˜θ − θ)k]

(6)

is named Shakeout regularizer. Note that if A(θ) is k-th order
)(θ) = 0 where
derivable, let the k
k
Theorem 1. Let qj = xj(wj + csj), θj− = θ − qj and θj+ =

> k, to make the denotation simple.

order derivative A(k

(cid:48)

(cid:48)

(cid:48)

θ + τ

1−τ qj, then Shakeout regularizer π(w) is

π(w) = τ

A(θj−) + (1 − τ )

A(θj+) − pA(θ) (7)

p
(cid:88)

j=1

p
(cid:88)

j=1

Proof: Note that ˜θ − θ = (cid:80)p

j=1 qj(rj − 1), then for Eq.

(6)

E[(˜θ − θ)k] =

p
(cid:88)

p
(cid:88)

p
(cid:88)

k
(cid:89)

· · ·

k
(cid:89)

qjm

E[

(rjm − 1)]

j1=1

j2=1

jk=1

m=1

m=1

Because arbitrary two random variables rjm1 , rjm2 are inde-
pendent unless jm1 = jm2 and ∀rjm , E[rjm − 1] = 0, then

p
(cid:88)

j=1
p
(cid:88)

j=1

E[(˜θ − θ)k] =

qk
j

E[(rj − 1)k]

= τ

(−qj)k + (1 − τ )

p
(cid:88)

(
j=1

τ
1 − τ

qj)k

Then

π(w) = τ

A(k)(θ)(−qj)k

p
(cid:88)

∞
(cid:88)

j=1

k=1

1
k!

+(1 − τ )

p
(cid:88)

∞
(cid:88)

j=1

k=1

1
k!

A(k)(θ)(

τ
1 − τ

qj)k

Further, let θj− = θ − qj, θj+ = θ + τ

π(w) = τ

A(θj−) + (1 − τ )

p
(cid:88)

j=1

1−τ qj, π(w) becomes
p
(cid:88)

A(θj+) − pA(θ)

j=1

The theorem is proved.

Proposition 2. If A(θ) is convex, π(w) ≥ 0.

Proposition 3. Suppose ∃j, xjwj (cid:54)= 0. If A(θ) is convex,
(θ) > 0, π(w)

π(w) monotonically increases with τ . If A
monotonically increases with c.

(cid:48)(cid:48)

Proposition 3 implies that the hyper-parameters τ and c
relate to the strength of the regularization effect. It is rea-
sonable because higher τ or c means the noise injected into
the features x has larger variance.
Proposition 4. Suppose i) ∀j (cid:54)= j

, xjwj = 0, and ii) xj(cid:48) (cid:54)= 0.

(cid:48)

Then

(cid:48)(cid:48)

i) if A

(θ) > 0,

ii) if lim|θ|→∞ A





(cid:48)(cid:48)

> 0, when wj(cid:48) > 0
< 0, when wj(cid:48) < 0

∂π(w)
∂wj(cid:48)
∂π(w)
∂wj(cid:48)
(θ) = 0, lim|wj(cid:48) |→∞

∂π(w)
∂wj(cid:48)

= 0

Proposition 4 implies that under certain conditions, start-
ing from a zero weight vector, Shakeout regularizer pe-
nalizes the magnitude of wj(cid:48) and its regularization effect
is bounded by a constant value. For example, for logistic
regression, π(w) ≤ τ ln(1+exp(c|xj(cid:48) |)), which is illustrated
in Fig. 2. This bounded property has been proved to be
useful: capped-norm [48] is more robust to outliers than the
traditional L1 or L2 norm.

Based on the Eq. (7), the speciﬁc formulas for the repre-

sentative GLM models can be derived:
i) Linear regression: A(θ) = 1

π(w) =

τ
2(1 − τ )

2 θ2, then
(cid:107)x ◦ (w + cs)(cid:107)2
2

where ◦ denotes
(cid:107)x ◦ (w + cs)(cid:107)2
tion of three components

the element-wise product and the
2 term can be decomposed into the summa-

p
(cid:88)

j=1

p
(cid:88)

j=1

p
(cid:88)

j=1

j w2
x2

j + 2c

j |wj| + c2
x2

x2
j 1wj (cid:54)=0[wj]

(8)

where 1wj (cid:54)=0[wj] is an indicator function which satisﬁes

(cid:40)

1 wj (cid:54)= 0
0 wj = 0

1wj (cid:54)=0[wj] =

. This decomposition implies that

Shakeout regularizer penalizes the combination of L0-norm,
L1-norm and L2-norm of the weights after scaling them
with the square of corresponding features. The L0 and L1
regularization terms can lead to sparse weights.

ii) Logistic regression: A(θ) = ln(1 + exp(θ)), then

π(w) =

ln(

p
(cid:88)

j=1

(1 + exp(θj−))τ (1 + exp(θj+))1−τ
1 + exp(θ)

)

(9)

Fig. 3 illustrates the contour of Shakeout regularizer based
on Eq. (9) in the 2D weight space. On the whole, the con-
tour of Shakeout regularizer indicates that the regularizer
combines L0, L1 and L2 regularization terms. As c goes to
zero, the contour around w = 0 becomes less sharper, which
implies hyper-parameter c relates to the strength of L0 and
L1 components. When c = 0, Shakeout degenerates to

5

(a) Shakeout regularizer: τ = 0.3, c = 0.78

(b) Dropout regularizer: τ = 0.5

Fig. 2. Regularization effect as a function of a single weight when other weights are ﬁxed to zeros for logistic regression model. The corresponding
feature x is ﬁxed at 1.

Dropout, the contour of which implies Dropout regularizer
consists of L2 regularization term.

imposed and leads to much sparser weights of the model.
We will verify this property in our experiment section later.

The difference between Shakeout and Dropout regular-
izers is also illustrated in Fig. 2. We set τ = 0.3, c = 0.78
for Shakeout, and τ = 0.5 for Dropout to make the bounds
of the regularization effects of two regularizers the same. In
this one dimension circumstance, the main difference is that
at w = 0 (see the enlarged snapshot for comparison), Shake-
out regularizer is sharp and discontinuous while Dropout
regularizer is smooth. Thus compared to Dropout, Shakeout
may lead to much sparser weights of the model.

To simplify the analysis and prove the intuition we have
observed in Fig. 3 about the properties of Shakeout regular-
izer, we quadratically approximate Shakeout regularizer of
Eq. (7) by

πapprox(w) =

τ
2(1 − τ )

(cid:48)(cid:48)

A

(θ) (cid:107)x ◦ (w + cs)(cid:107)2
2

(10)

The (cid:107)x ◦ (w + cs)(cid:107)2
2, already shown in Eq. (8), consists
of the combination of L0, L1, L2 regularization terms. It
tends to penalize the weight whose corresponding feature’s
magnitude is large. Meanwhile, the weights whose corre-
sponding features are always zeros are less penalized. The
(θ) is proportional to the variance of prediction y
term A
given x and w. Penalizing A
(θ) encourages the weights to
move towards making the model be more ”conﬁdent” about
its predication, i.e. be more discriminative.

(cid:48)(cid:48)

(cid:48)(cid:48)

Generally speaking, Shakeout regularizer adaptively
combines L0, L1 and L2 regularization terms, the property
of which matches what we have observed in Fig. 3. It
prefers penalizing the weights who have large magnitudes
and encourages the weights to move towards making the
model more discriminative. Moreover, the weights whose
corresponding features are always zeros are less penalized.
The L0 and L1 components can induce sparse weights.

Last but not the least, we want to emphasize that when
τ = 0, the noise is eliminated and the model becomes a
standard GLM. Moreover, Dropout can be viewed as the
special case of Shakeout when c = 0, and a higher value of
τ means a stronger L2 regularization effect imposed on the
weights. Generally, when τ is ﬁxed (τ (cid:54)= 0), a higher value
of c means a stronger effect of the L0 and L1 components

3.2 Shakeout in Multilayer Neural Networks

It has been illustrated that Shakeout regularizes the weights
in linear modules. Linear module is the basic component
of multilayer neural networks. That is, the linear operations
connect the outputs of two successive layers. Thus Shakeout
is readily applicable to the training of multilayer neural
networks.

Considering the forward computation from layer l to
layer l + 1, for a fully-connected layer, the Shakeout forward
computation is as follows

ui =

xj[rjWij + c(rj − 1)Sij] + bi

(11)

(cid:88)

j

(cid:48)
i = f (ui)
x

(12)

where i denotes the index of the output unit of layer l + 1,
and j denotes the index of the output unit of layer l. The
output unit of a layer is represented by x. The weight of the
connection between unit xj and unit x
i is represented as
Wij. The bias for the i-th unit is denoted by bi. The Sij
is the sign of corresponding weight Wij. After Shakeout
operation, the linear combination ui is sent to the activation
function f (·) to obtain the corresponding output x
i. Note
that the weights Wij that connect to the same input unit xj
are controlled by the same random variable rj.

(cid:48)

(cid:48)

During back-propagation, we should compute the gra-
dients with respect to each unit to propagate the error. In
Shakeout, ∂ui
∂xj

takes the form

= rj(Wij + cSij) − cSij

(13)

And the weights are updated following

= xj(rj + c(rj − 1)

(14)

dSij
dWij

)

dSij
dWij

represents the derivative of a sgn function.
where
Because the sgn function is not continuous at zero and

∂ui
∂xj

∂ui
∂Wij

6

Fig. 3. The contour plots of the regularization effect induced by Shakeout in 2D weight space with input x = [1, 1]T . Note that Dropout is a special
case of Shakeout with c = 0.

thus the derivative is not deﬁned, we approximate this
. Empirically we ﬁnd that this
derivative with
approximation works well.

d tanh(Wij )
dWij

Note that

the forward-backward computations with
Shakeout can be easily extended to the convolutional layer.
For a convolutional layer, the Shakeout feed-forward pro-
cess can be formalized as

Ui =

(Xj ◦ Rj) ∗ Wij + c(Xj ◦ (Rj − 1)) ∗ Sij + bi (15)

(cid:88)

j

(cid:48)

X

(16)

i = f (Ui)
where Xj represents the j-th feature map. Rj is the j-th
random mask which has the same spatial structure (i.e. the
same height and width) as the corresponding feature map
Xj. Wij denotes the kernel connecting Xj and Ui. And Sij
is set as sgn(Wij). The symbol * denotes the convolution
operation. And the symbol ◦ means element-wise product.
Correspondingly, during the back-propagation process,
the gradient with respect to a unit of the layer on which
Shakeout is applied takes the form

∂Ui(a, b)
∂Xj(a − a(cid:48), b − b(cid:48))

= Rj(a − a

, b − b

)(Wij(a

(cid:48)

(cid:48)

(cid:48)

(cid:48)

, b

)+

(cid:48)

(cid:48)

cSij(a

, b

)) − cSij(a

(cid:48)

(cid:48)

, b

)

(17)

where (a, b) means the position of a unit in the output
) represents the position
feature map of a layer, and (a
of a weight in the corresponding kernel.

, b

(cid:48)

(cid:48)

The weights are updated following

∂Ui(a, b)
∂Wij(a(cid:48), b(cid:48))

= Xj(a − a

(cid:48)

(cid:48)
, b − b

)(Rj(a − a

, b − b

)

(cid:48)

(cid:48)

+ c(Rj(a − a

(cid:48)

(cid:48)
, b − b

) − 1)

(cid:48)

(cid:48)
, b

)
dSij(a
dWij(a(cid:48), b(cid:48))

)

(18)

4 EXPERIMENTS
In this section, we report empirical evaluations of Shake-
out in training deep neural networks on representative
datasets. The experiments are performed on three kinds of
image datasets: the hand-written image dataset MNIST [40],
the CIFAR-10 image dataset [49] and the ImageNet-2012
dataset [50]. MNIST consists of 60,000+10,000 (training+test)
28×28 images of hand-written digits. CIFAR-10 contains
50,000+10,000 (training+test) 32×32 images of 10 object
classes. ImageNet-2012 consists of 1,281,167+50,000+150,000
(training+validation+test) variable-resolution images of
1000 object classes. We ﬁrst demonstrate that Shakeout leads
to sparse models as our theoretical analysis implies under
the unsupervised setting. Then we show that for the classi-
ﬁcation task, the sparse models have desirable generaliza-
tion performances. Further, we illustrate the regularization
effect of Shakeout on the weights in the classiﬁcation task.
Moreover, the effect of Shakeout on stabilizing the training
processes of the deep architectures is demonstrated. Finally,
we give some practical recommendations of Shakeout. All

the experiments are implemented based on the modiﬁca-
tions of Caffe library [51]. Our code is released on the github:
https://github.com/kgl-prml/shakeout-for-caffe.

4.1 Shakeout and Weight Sparsity

implicitly imposes L0 penalty and L1
Since Shakeout
penalty of the weights, we expect the weights of neural
networks learned by Shakeout contain more zeros than
those learned by the standard back-propagation (BP) [52] or
Dropout [13]. In this experiment, we employ an autoencoder
model for the MNIST hand-written data, train the model
using standard BP, Dropout and Shakeout, respectively, and
compare the degree of sparsity of the weights of the learned
encoders. For the purpose of demonstration, we employ
the simple autoencoder with one hidden layer of 256 units;
Dropout and Shakeout are applied on the input pixels.

To verify the regularization effect, we compare the
weights of the four autoencoders trained under different
settings which correspond to standard BP, Dropout (τ = 0.5)
and Shakeout (τ = 0.5, c = {1, 10}). All the training
methods aim to produce hidden units which can capture
good visual features of the handwritten digits. The statistical
traits of these different resulting weights are shown in Fig. 4.
Moreover, Fig. 5 shows the features captured by each hidden
unit of the autoencoders.

As shown in the Fig. 4, the probability density of weights
around the zero obtained by standard BP training is quite
small compared to the one obtained either by Dropout or
Shakeout. This indicates the strong regularization effect in-
duced by Dropout and Shakeout. Furthermore, the sparsity
level of weights obtained from training by Shakeout is much
higher than the one obtained from training by Dropout.
Using the same τ , increasing c makes the weights much
sparser, which is consistent with the characteristics of L0
penalty and L1 penalty induced by Shakeout. Intuitively,
we can ﬁnd that due to the induced L2 regularization,
the distribution of weights obtained from training by the
Dropout is like a Gaussian, while the one obtained from
training by Shakeout is more like a Laplacian because of
the additionally induced L1 regularization. Fig. 5 shows
that features captured by the hidden units via standard
BP training are not directly interpretable, corresponding to
insigniﬁcant variants in the training data. Both Dropout and
Shakeout suppress irrelevant weights by their regularization
effects, where Shakeout produces much sparser and more
global features thanks to the combination of L0, L1 and L2
regularization terms.

The autoencoder trained by Dropout or Shakeout can
be viewed as the denosing autoencoder, where Dropout
or Shakeout injects special kind of noise into the inputs.
Under this unsupervised setting, the denoising criterion
(i.e. minimizing the error between imaginary images recon-
structed from the noisy inputs and the real images without
noise) is to guide the learning of useful high level feature
representations [11], [12]. To verify that Shakeout helps
learn better feature representations, we adopt the hidden
layer activations as features to train SVM classiﬁers, and the
classiﬁcation accuracies on test set for standard BP, Dropout
and Shakeout are 95.34%, 96.41% and 96.48%, respectively.
We can see that Shakeout leads to much sparser weights
without defeating the main objective.

7

Fig. 4. Distributions of the weights of the autoencoder models learned
by different training approaches. Each curve in the ﬁgure shows the
frequencies of the weights of an autoencoder taking particular values,
i.e. the empirical population densities of the weights. The ﬁve curves
correspond to ﬁve autoencoders learned by standard back-propagation,
Dropout (τ = 0.5), Gaussian Dropout (σ2 = 1) and Shakeout (τ = 0.5,
c = {1, 10}). The sparsity of the weights obtained via Shakeout can be
seen by comparing the curves.

Gaussian Dropout has similar effect on the model train-
ing as standard Dropout [34], which multiplies the acti-
vation of each unit by a Gaussian variable with mean 1
and variance σ2. The relationship between σ2 and τ is
that σ2 = τ
1−τ . The distribution of the weights trained by
Gaussian Dropout (σ2 = 1, i.e. τ = 0.5) is illustrated in
Fig. 4. From Fig. 4, we ﬁnd no notable statistical difference
between two kinds of Dropout implementations which all
exhibit a kind of L2 regularization effect on the weights.
The classiﬁcation performances of SVM classiﬁers on test set
based on the hidden layer activations as extracted features
for both kinds of Dropout implementations are quite similar
(i.e. 96.41% and 96.43% for standard and Gaussian Dropout
respectively). Due to these observations, we conduct the fol-
lowing classiﬁcation experiments using standard Dropout
as a representative implementation (of Dropout) for com-
parison.

4.2 Classiﬁcation Experiments

Sparse models often indicate lower complexity and better
generalization performance [26], [39], [53], [54]. To verify
the effect of L0 and L1 regularization terms induced by
Shakeout on the model performance, we apply Shakeout,
along with Dropout and standard BP, on training represen-
tative deep neural networks for classiﬁcation tasks. In all of
our classiﬁcation experiments, the hyper-parameters τ and
c in Shakeout, and the hyper-parameter τ in Dropout are
determined by validation.

4.2.1 MNIST

We train two different neural networks, a shallow fully-
connected one and a deep convolutional one. For the
fully-connected neural network, a big hidden layer size is
adopted with its value at 4096. The non-linear activation

8

(a) standard BP

(b) Dropout: τ = 0.5

(c) Shakeout: τ = 0.5, c = 0.5

Fig. 5. Features captured by the hidden units of the autoencoder models learned by different training methods. The features captured by a hidden
unit are represented by a group of weights that connect the image pixels with this corresponding hidden unit. One image patch in a sub-graph
corresponds to the features captured by one hidden unit.

TABLE 1
The architecture of convolutional neural network adopted for MNIST
classiﬁcation experiment

TABLE 2
Classiﬁcation on MNIST using training sets of different sizes:
fully-connected neural network

Layer
Type
Channels
Filter size
Conv. stride
Pooling type
Pooling size
Pooling stride
Non-linear

1
conv.
20
5 × 5
1
max
2 × 2
2

2
conv.
50
5 × 5
1
max
2 × 2
2

3
FC
500
-
-
-
-
-

4
FC
10
-
-
-
-
-

ReLU ReLU ReLU Softmax

unit adopted is the rectiﬁer linear unit (ReLU). The deep
convolutional neural network employed is based on the
modiﬁcations of the LeNet [40], which contains two con-
volutional layers and two fully-connected layers. The de-
tailed architecture information of this convolutional neural
network is described in Tab. 1. We separate 10,000 training
samples from original training dataset for validation. The
results are shown in Tab. 2 and Tab. 3. Dropout and Shake-
out are applied on the hidden units of the fully-connected
layer. The table compares the errors of the networks trained
by standard back-propagation, Dropout and Shakeout. The
mean and standard deviation of the classiﬁcation errors
are obtained by 5 runs of the experiment and are shown
in percentage. We can see from the results that when the
training data is not sufﬁcient enough, due to over-ﬁtting, all
the models perform worse. However, the models trained by
Dropout and Shakeout consistently perform better than the
one trained by standard BP. Moreover, when the training
data is scarce, Shakeout leads to superior model perfor-
mance compared to the Dropout. Fig. 6 shows the results
in a more intuitive way.

4.2.2 CIFAR-10

We use the simple convolutional network feature extractor
described in cuda-convnet (layers-80sec.cfg) [55]. We apply
Dropout and Shakeout on the ﬁrst fully-connected layer. We
call this architecture “AlexFastNet” for the convenience of
description. In this experiment, 10,000 colour images are

Size
500
1000
3000
8000
20000
50000

std-BP
13.66±0.66
8.49±0.23
5.54±0.09
3.57±0.14
2.28±0.09
1.55±0.03

Dropout
11.76±0.09
8.05±0.05
4.87±0.06
2.95±0.05
1.82±0.07
1.36±0.03

Shakeout
10.81±0.32
7.19±0.15
4.60±0.07
2.96±0.09
1.92±0.06
1.35±0.07

TABLE 3
Classiﬁcation on MNIST using training sets of different sizes:
convolutional neural network

Size
500
1000
3000
8000
20000
50000

std-BP
9.76±0.26
6.73±0.12
2.93±0.10
1.70±0.03
0.97±0.01
0.78±0.05

Dropout
6.16±0.23
4.01±0.16
2.06±0.06
1.23±0.13
0.83±0.06
0.62±0.04

Shakeout
4.83±0.11
3.43±0.06
1.86±0.13
1.31±0.06
0.77±0.001
0.58±0.10

TABLE 4
Classiﬁcation on CIFAR-10 using training sets of different sizes:
AlexFastNet

Size
300
700
2000
5500
15000
40000

std-BP
68.26±0.57
59.78±0.24
50.73±0.29
41.41±0.52
32.53±0.25
24.48±0.23

Dropout
65.34±0.75
56.04±0.22
46.24±0.49
36.01±0.13
27.28±0.26
20.50±0.32

Shakeout
63.71±0.28
54.66±0.22
44.39±0.41
34.54±0.31
26.53±0.17
20.56±0.12

separated from the training dataset for validation and no
data augmentation is utilized. The per-pixel mean computed
over the training set is subtracted from each image. We ﬁrst
train for 100 epochs with an initial learning rate of 0.001
and then another 50 epochs with the learning rate of 0.0001.
The mean and standard deviation of the classiﬁcation errors
are obtained by 5 runs of the experiment and are shown in
percentage. As shown in Tab. 4, the performances of models
trained by Dropout and Shakeout are consistently superior

9

(a) Fully-connected neural network

(b) Convolutional neural network

Fig. 6. Classiﬁcation of two kinds of neural networks on MNIST using training sets of different sizes. The curves show the performances of the
models trained by standard BP, and those by Dropout and Shakeout applied on the hidden units of the fully-connected layer.

TABLE 5
Classiﬁcation on CIFAR-10 using training sets of different sizes:
WRN-16-4

Size
15000
40000
50000

std-BP Dropout
20.95
15.37
14.39

15.05
9.32
8.03

Shakeout
14.68
9.01
7.97

training set size at 40000. From Tab. 5, we can arrive at
the same conclusion as previous experiments, i.e. the per-
formances of the models trained by Dropout and Shakeout
are consistently superior to the one trained by standard BP.
Moreover, Shakeout outperforms Dropout when the data is
scarce.

4.2.3 Regularization Effect on the Weights

Shakeout is a different way to regularize the training process
of deep neural networks from Dropout. For a GLM model,
we have proved that the regularizer induced by Shakeout
adaptively combines L0, L1 and L1 regularization terms.
In section 4.1, we have demonstrated that for a one-hidden
layer autoencoder, it leads to much sparser weights of the
model. In this section, we will illustrate the regularization
effect of Shakeout on the weights in the classiﬁcation task
and make a comparison to that of Dropout.

The results shown in this section are mainly based on the
experiments conducted on ImageNet-2012 dataset using the
representative deep architecture: AlexNet [14]. For AlexNet,
we apply Dropout or Shakeout on layers FC7 and FC8
which are the last two fully-connected layers. We train the
model from the scratch and obtain the comparable classi-
ﬁcation performances on validation set for Shakeout (top-1
error: 42.88%; top-5 error: 19.85%) and Dropout (top-1 error:
42.99%; top-5 error: 19.60%). The model is trained based on
the same hyper-parameter settings provided by Shelhamer
in Caffe [51] other than the hyper-parameters τ and c for
Shakeout. The initial weights for training by Dropout and
Shakeout are kept the same.

Fig. 8 illustrates the distributions of the magnitude of
weight resulted by Shakeout and Dropout. It can be seen
that the weights learned by Shakeout are much sparser than

Fig. 7. Classiﬁcation on CIFAR-10 using training sets of different sizes.
The curves show the performances of the models trained by standard
BP, and those by Dropout and Shakeout applied on the hidden units of
the fully-connected layer.

to the one trained by standard BP. Furthermore, the model
trained by Shakeout also outperforms the one trained by
Dropout when the training data is scarce. Fig. 7 shows the
results in a more intuitive way.

To test the performance of Shakeout on a much deeper
architecture, we also conduct experiments based on the
Wide Residual Network (WRN) [15]. The conﬁguration of
WRN adopted is WRN-16-4, which means WRN has 16
layers in total and the number of feature maps for the
convolutional layer of each residual block is 4 times as the
corresponding original one [1]. Because the complexity is
much higher than that of “AlexFastNet”, the experiments
are performed on relatively larger training sets with sizes
of 15000, 40000, 50000. Dropout and Shakeout are applied
on the second convolutional layer of each residual block,
following the protocol in [15]. All the training starts from
the same initial weights. Batch Normalization is applied the
same way as [15] to promote the optimization. No data-
augmentation or data pre-processing is adopted. All the
other hyper-parameters other than τ and c are set the same
as [15]. The results are listed in Tab. 5. For the training
of CIFAR-10 with 50000 training samples, we adopt the
same hyper-parameters as those chosen in the training with

10

(a) AlexNet FC7 layer

(b) AlexNet FC8 layer

Fig. 8. Comparison of the distributions of the magnitude of weights trained by Dropout and Shakeout. The experiments are conducted using AlexNet
on ImageNet-2012 dataset. Shakeout or Dropout is applied on the last two fully-connected layers, i.e. FC7 layer and FC8 layer.

(a) AlexNet FC7 layer

(b) AlexNet FC8 layer

Fig. 9. Distributions of the maximum magnitude of the weights connected to the same input unit of a layer. The maximum magnitude of the weights
connected to one input unit can be regarded as a metric of the importance of that unit. The experiments are conducted using AlexNet on ImageNet-
2012 dataset. For Shakeout, the units can be approximately separated into two groups and the one around zero is less important than the other,
whereas for Dropout, the units are more concentrated.

those learned by Dropout, due to the implicitly induced L0
and L1 components.

The regularizer induced by Shakeout not only contains
L0 and L1 regularization terms but also contains L2 reg-
ularization term, the combination of which is expected to
discard a group of weights simultaneously. In Fig. 9, we use
the maximum magnitude of the weights connected to one
input unit of a layer to represent the importance of that unit
for the subsequent output units. From Fig. 9, it can be seen
that for Shakeout, the units can be approximately separated
into two groups according to the maximum magnitudes
of the connected weights and the group around zero can
be discarded, whereas for Dropout, the units are concen-
trated. This implies that compared to Dropout which may
encourage a “distributed code” for the features captured by
the units of a layer, Shakeout tends to discard the useless
features (or units) and award the important ones. This
experiment result veriﬁes the regularization properties of
Shakeout and Dropout further.

As known to us, L0 and L1 regularization terms are
related to performing feature selection [56], [57]. For a deep
architecture, it is expected to obtain a set of weights using
Shakeout suitable for reﬂecting the importance of connec-
tions between units. We perform the following experiment
to verify this effect. After a model is trained, for the layer
on which Dropout or Shakeout is applied, we sort the
magnitudes of the weights increasingly. Then we prune the
ﬁrst m% of the sorted weights and evaluate the performance
of the pruned model again. The pruning ratio m goes from 0
to 1. We calculate the relative accuracy loss (we write R.A.L
for simpliﬁcation) at each pruning ratio m

as

(cid:48)

R.A.L(m

) =

(cid:48)

Accu.(m = 0) − Accu.(m
Accu.(m = 0)

(cid:48)

)

Fig. 10 shows the R.A.L curves for Dropout and Shake-
out based on the AlexNet model on ImageNet-2012 dataset.
The models trained by Dropout and Shakeout are under the
optimal hyper-parameter settings. Apparently, the relative

accuracy loss for Dropout is more severe than that for Shake-
out. For example, the largest margin of the relative accuracy
losses between Dropout and Shakeout is 22.50%, which
occurs at the weight pruning ratio m = 96%. This result
proves that considering the trained weights in reﬂecting the
importance of connections, Shakeout is much better than
Dropout, which beneﬁts from the implicitly induced L0 and
L1 regularization effect. This kind of property is useful for
the popular compression task in deep learning area which
aims to cut the connections or throw units of a deep neural
network to a maximum extent without obvious loss of ac-
curacy. The above experiments illustrate that Shakeout can
play a considerable role in selecting important connections,
which is meaningful for promoting the performance of a
compression task. This is a potential subject for the future
research.

4.3 Stabilization Effect on the Training Process

In both research and production, it is always desirable to
have a level of certainty about how a model’s ﬁtness to
the data improves over optimization iterations, namely, to
have a stable training process. In this section, we show that
Shakeout helps reduce ﬂuctuation in the improvement of
model ﬁtness during training.

The ﬁrst experiment is on the family of Generative
Adversarial Networks (GANs) [58], which is known to be
instable in the training stage [59], [60], [61]. The purpose of
the following tests is to demonstrate the Shakeout’s capa-
bility of stabilizing the training process of neural networks
in a general sense. GAN plays a min-max game between
the generator G and the discriminator D over the expected
log-likelihood of real data x and imaginary data ˆx = G(z)
where z represents the random input

min
G

max
D

V (D, G) = E[log[D(x)] + log[1 − D(G(z))]] (19)

The architecture that we adopt is DCGAN [59]. The numbers
of feature maps of the deconvolutional layers in the gener-
ator are 1024, 64 and 1 respectively, with the corresponding
spatial sizes 7×7, 14×14 and 28×28. We train DCGANs on
MNIST dataset using standard BP, Dropout and Shakeout.
We follow the same experiment protocol described in [59]
except for adopting Dropout or Shakeout on all layers of the
discriminator. The values of −V (D, G) during training are
illustrated in Fig. 11. It can be seen that −V (D, G) during
training by standard BP oscillates greatly, while for Dropout
and Shakeout, the training processes are much steadier.
Compared with Dropout, the training process by Shakeout
has fewer spikes and is smoother. Fig. 12 demonstrates the
minimum and maximum values of −V (D, G) within ﬁxed
length intervals moving from the start to the end of the
training by standard BP, Dropout and Shakeout. It can be
seen that the gaps between the minimum and maximum
values of −V (D, G) trained by Dropout and Shakeout are
much smaller than that trained by standard BP, while that
by Shakeout is the smallest, which implies the stability of
the training process by Shakeout is the best.

The second experiment is based on Wide Residual Net-
work architecture to perform the classiﬁcation task. In the
classiﬁcation task, generalization performance is the main
focus and thus, we compare the validation errors during

11

the training processes by Dropout and Shakeout. Fig. 13
demonstrates the validation error as a function of the
training epoch for Dropout and Shakeout on CIFAR-10
with 40000 training examples. The architecture adopted is
WRN-16-4. The experiment settings are the same as those
described in Section 4.2.2. Considering the generalization
performance, the learning rate schedule adopted is the one
optimized through validation to make the models obtain the
best generalization performances. Under this schedule, we
ﬁnd that the validation error temporarily increases when
lowering the learning rate at the early stage of training,
which has been repeatedly observed by [15]. Nevertheless,
it can be seen from Fig. 13 that the extent of error increase is
less severe for Shakeout than Dropout. Moreover, Shakeout
recovers much faster than Dropout does. At the ﬁnal stage,
both of the validation errors steadily decrease. Shakeout
obtains comparable or even superior generalization perfor-
mance to Dropout. In a word, Shakeout signiﬁcantly stabi-
lizes the entire training process with superior generalization
performance.

4.4 Practical Recommendations

Selection of Hyper-parameters The most practical and pop-
ular way to perform hyper-parameter selection is to parti-
tion the training data into a training set and a validation
set to evaluate the classiﬁcation performance of different
hyper-parameters on it. Due to the expensive cost of time
for training a deep neural network, cross-validation is barely
adopted. There exist many hyper-parameter selection meth-
ods in the domain of deep learning, such as the grid search,
random search [62], Bayesian optimization methods [63],
gradient-based hyper-parameter Optimization [64], etc. For
applying Shakeout on a deep neural network, we need to
decide two hyper-parameters τ and c. From the regular-
ization perspective, we need to decide the most suitable
strength of regularization effect to obtain an optimal trade-
off between model bias and variance. We have pointed out
that in a uniﬁed framework, Dropout is a special case of
Shakeout when Shakeout hyper-parameter c is set to zero.
Empirically we ﬁnd that the optimal τ for Shakeout is not
higher than that for Dropout. After determining the optimal
τ , keeping the order of magnitude of hyper parameter c
N (N represents the number of training
the same as
samples) is an effective choice. If you want to obtain a model
with much sparser weights but meanwhile with superior
or comparable generalization performance to Dropout, a
relatively lower τ and larger c for Shakeout always works.
Shakeout combined with Batch Normalization Batch Nor-
malization [65] is the widely-adopted technique to promote
the optimization of the training process for a deep neural
network. In practice, combining Shakeout with Batch Nor-
malization to train a deep architecture is a good choice.
For example, we observe that the training of WRN-16-4
model on CIFAR-10 is slow to converge without using Batch
Normalization in the training. Moreover, the generalization
performance on the test set for Shakeout combined with
Batch Normalization always outperforms that for standard
BP with Batch Normalization consistently for quite a large
margin, as illustrated in Tab. 5. These results imply the

(cid:113) 1

12

(a) standard BP

(b) Dropout

(c) Shakeout

Fig. 11. The value of −V (D, G) as a function of iteration for the training process of DCGAN. DCGANs are trained using standard BP, Dropout and
Shakeout for comparison. Dropout or Shakeout is applied on the discriminator of GAN.

Fig. 10. Relative accuracy loss as a function of the weight pruning ratio
for Dropout and Shakeout based on AlexNet architecture on ImageNet-
2012. The relative accuracy loss for Dropout is much severe than that for
Shakeout. The largest margin of the relative accuracy losses between
Dropout and Shakeout is 22.50%, which occurs at the weight pruning
ratio m = 96%.

Fig. 13. Validation error as a function of training epoch for Dropout and
Shakeout on CIFAR-10 with training set size at 40000. The architec-
ture adopted is WRN-16-4. “DPO” and “SKO” represent “Dropout” and
“Shakeout” respectively. The following two numbers denote the hyper-
parameters τ and c respectively. The learning rate decays at epoch 60,
120, and 160. After the ﬁrst decay of learning rate, the validation error in-
creases greatly before the steady decrease (see the enlarged snapshot
for training epochs from 60 to 80). It can be seen that the extent of error
increase is less severe for Shakeout than Dropout. Moreover, Shakeout
recovers much faster than Dropout does. At the ﬁnal stage, both of
the validation errors steadily decrease (see the enlarged snapshot for
training epochs from 160 to 200). Shakeout obtains comparable or even
superior generalization performance to Dropout.

important role of Shakeout in reducing over-ﬁtting of a deep
neural network.

Fig. 12. The minimum and maximum values of −V (D, G) within ﬁxed
length intervals moving from the start to the end of the training by stan-
dard BP, Dropout and Shakeout. The optimal value log(4) is obtained
when the imaginary data distribution P (ˆx) matches with the real data
distribution P (x).

5 CONCLUSION

We have proposed Shakeout, which is a new regularized
training approach for deep neural networks. The regularizer
induced by Shakeout is proved to adaptively combine L0,
L1 and L2 regularization terms. Empirically we ﬁnd that

1) Compared to Dropout, Shakeout can afford much
larger models. Or to say, when the data is scarce, Shakeout
outperforms Dropout with a large margin.

2) Shakeout can obtain much sparser weights than
Dropout with superior or comparable generalization per-
formance of the model. While for Dropout, if one wants
to obtain the same level of sparsity as that obtained by
Shakeout, the model may bear a signiﬁcant loss of accuracy.
3) Some deep architectures in nature may result in the
instability of the training process, such as GANs, however,
Shakeout can reduce this instability effectively.

In future, we want to put emphasis on the inductive
bias of Shakeout and attempt to apply Shakeout to the
compression task.

ACKNOWLEDGMENTS

This research is supported by Australian Research Coun-
cil Projects (No. FT-130101457, DP-140102164 and LP-
150100671).

REFERENCES

[1] K. He, X. Zhang, S. Ren, and J. Sun, “Identity mappings in deep
residual networks,” in European Conference on Computer Vision.
Springer, 2016, pp. 630–645.

[2] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, “Inception-
v4, inception-resnet and the impact of residual connections on
learning,” in Proceedings of the Thirty-First AAAI Conference on
Artiﬁcial Intelligence, February 4-9, 2017, San Francisco, California,
USA., 2017, pp. 4278–4284.

[3] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Region-based
convolutional networks for accurate object detection and seg-
mentation,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 38, no. 1, pp. 142–158, Jan 2016.

[4] Y. Sun, X. Wang, and X. Tang, “Hybrid deep learning for face
veriﬁcation,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 38, no. 10, pp. 1997–2009, Oct 2016.

[5] Y. Zheng, Y. J. Zhang, and H. Larochelle, “A deep and autore-
gressive approach for topic modeling of multimodal data,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 38,
no. 6, pp. 1056–1069, June 2016.

[6] Y. Wei, W. Xia, M. Lin, J. Huang, B. Ni, J. Dong, Y. Zhao, and
S. Yan, “Hcp: A ﬂexible cnn framework for multi-label image
classiﬁcation,” IEEE transactions on pattern analysis and machine
intelligence, vol. 38, no. 9, pp. 1901–1907, 2016.

[7] X. Jin, C. Xu, J. Feng, Y. Wei, J. Xiong, and S. Yan, “Deep learning
with s-shaped rectiﬁed linear activation units,” arXiv preprint
arXiv:1512.07030, 2015.

[8] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning algo-
rithm for deep belief nets,” Neural computation, vol. 18, no. 7, pp.
1527–1554, 2006.

[9] Y. Bengio, “Learning deep architectures for AI,” Foundations and

trends R(cid:13) in Machine Learning, vol. 2, no. 1, pp. 1–127, 2009.

[10] Y. Bengio, A. Courville, and P. Vincent, “Representation learning:
A review and new perspectives,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1798–1828, Aug
2013.

[11] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, “Extract-
ing and composing robust features with denoising autoencoders,”
in Proceedings of the 25th international conference on Machine learning.
ACM, 2008, pp. 1096–1103.

[12] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol,
“Stacked denoising autoencoders: Learning useful representations
in a deep network with a local denoising criterion,” The Journal of
Machine Learning Research, vol. 11, pp. 3371–3408, 2010.

[13] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R.
Salakhutdinov, “Improving neural networks by preventing co-
adaptation of feature detectors,” arXiv preprint arXiv:1207.0580,
2012.

13

[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁ-
cation with deep convolutional neural networks,” in Advances in
neural information processing systems, 2012, pp. 1097–1105.

[15] S. Zagoruyko and N. Komodakis, “Wide residual networks,” in
Proceedings of the British Machine Vision Conference 2016, BMVC
2016, York, UK, September 19-22, 2016, 2016.

[16] S. Wager, S. Wang, and P. S. Liang, “Dropout training as adap-
tive regularization,” in Advances in Neural Information Processing
Systems, 2013, pp. 351–359.

[17] N. Chen, J. Zhu, J. Chen, and B. Zhang, “Dropout training for
support vector machines,” in Twenty-Eighth AAAI Conference on
Artiﬁcial Intelligence, 2014.

[18] L. Van Der Maaten, M. Chen, S. Tyree, and K. Q. Weinberger,
“Learning with marginalized corrupted features.” in ICML (1),
2013, pp. 410–418.

[19] Y. LeCun, J. S. Denker, S. A. Solla, R. E. Howard, and L. D. Jackel,
“Optimal brain damage.” in NIPs, vol. 2, 1989, pp. 598–605.
[20] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen,
“Compressing neural networks with the hashing trick,” in Proceed-
ings of the 32nd International Conference on Machine Learning, ICML
2015, Lille, France, 6-11 July 2015, 2015, pp. 2285–2294.

[21] S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights and
connections for efﬁcient neural network,” in Advances in Neural
Information Processing Systems, 2015, pp. 1135–1143.

[22] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing
deep neural network with pruning, trained quantization and
huffman coding,” CoRR, abs/1510.00149, vol. 2, 2015.

[23] M. Denil, B. Shakibi, L. Dinh, N. de Freitas et al., “Predicting
parameters in deep learning,” in Advances in Neural Information
Processing Systems, 2013, pp. 2148–2156.

[24] J. Ba and R. Caruana, “Do deep nets really need to be deep?” in
Advances in neural information processing systems, 2014, pp. 2654–
2662.

[25] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a

neural network,” arXiv preprint arXiv:1503.02531, 2015.

[26] H. Zou and T. Hastie, “Regularization and variable selection via
the elastic net,” Journal of the Royal Statistical Society: Series B
(Statistical Methodology), vol. 67, no. 2, pp. 301–320, 2005.

[27] G. Kang, J. Li, and D. Tao, “Shakeout: A new regularized deep
neural network training scheme,” in Thirtieth AAAI Conference on
Artiﬁcial Intelligence, 2016.

[28] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent,
and S. Bengio, “Why does unsupervised pre-training help deep
learning?” The Journal of Machine Learning Research, vol. 11, pp.
625–660, 2010.

[29] J. Moody, S. Hanson, A. Krogh, and J. A. Hertz, “A simple weight
decay can improve generalization,” Advances in neural information
processing systems, vol. 4, pp. 950–957, 1995.

[30] L. Prechelt, “Automatic early stopping using cross validation:
quantifying the criteria,” Neural Networks, vol. 11, no. 4, pp. 761–
767, 1998.

[31] L. Wan, M. Zeiler, S. Zhang, Y. L. Cun, and R. Fergus, “Regulariza-
tion of neural networks using dropconnect,” in Proceedings of the
30th International Conference on Machine Learning (ICML-13), 2013,
pp. 1058–1066.

[32] J. Ba and B. Frey, “Adaptive dropout for training deep neural
networks,” in Advances in Neural Information Processing Systems,
2013, pp. 3084–3092.

[33] Z. Li, B. Gong, and T. Yang, “Improved dropout for shallow
and deep learning,” in Advances In Neural Information Processing
Systems, 2016, pp. 2523–2531.

[34] N. Srivastava, G. Hinton, A. Krizhevsky,

I. Sutskever, and
R. Salakhutdinov, “Dropout: A simple way to prevent neural net-
works from overﬁtting,” The Journal of Machine Learning Research,
vol. 15, no. 1, pp. 1929–1958, 2014.

[35] D. Warde-Farley, I. J. Goodfellow, A. Courville, and Y. Bengio, “An
empirical analysis of dropout in piecewise linear networks,” arXiv
preprint arXiv:1312.6197, 2013.

[36] P. Baldi and P. J. Sadowski, “Understanding dropout,” in Advances
in Neural Information Processing Systems, 2013, pp. 2814–2822.
[37] Y. Gal and Z. Ghahramani, “Dropout as a bayesian approximation:
Representing model uncertainty in deep learning,” in Proceedings
of the 33nd International Conference on Machine Learning, ICML 2016,
New York City, NY, USA, June 19-24, 2016, 2016, pp. 1050–1059.
[38] D. P. Helmbold and P. M. Long, “On the inductive bias of
dropout,” Journal of Machine Learning Research, vol. 16, pp. 3403–
3454, 2015.

14

[63] J. Snoek, H. Larochelle, and R. P. Adams, “Practical bayesian op-
timization of machine learning algorithms,” in Advances in neural
information processing systems, 2012, pp. 2951–2959.

[64] D. Maclaurin, D. Duvenaud, and R. P. Adams, “Gradient-based
hyperparameter optimization through reversible learning,” in Pro-
ceedings of the 32nd International Conference on Machine Learning,
2015.

[65] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep
network training by reducing internal covariate shift,” in Proceed-
ings of the 32nd International Conference on Machine Learning, ICML
2015, Lille, France, 6-11 July 2015, 2015, pp. 448–456.

[39] B. A. Olshausen and D. J. Field, “Sparse coding with an overcom-
plete basis set: A strategy employed by v1?” Vision research, vol. 37,
no. 23, pp. 3311–3325, 1997.

[40] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based
learning applied to document recognition,” Proceedings of the IEEE,
vol. 86, no. 11, pp. 2278–2324, 1998.

[41] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with
convolutions,” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2015, pp. 1–9.

[42] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna,
“Rethinking the inception architecture for computer vision,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2016, pp. 2818–2826.

[43] Y.-l. Boureau, Y. L. Cun et al., “Sparse feature learning for deep be-
lief networks,” in Advances in neural information processing systems,
2008, pp. 1185–1192.

[44] I. J. Goodfellow, A. Courville, and Y. Bengio, “Spike-and-slab
sparse coding for unsupervised feature discovery,” arXiv preprint
arXiv:1201.3382, 2012.

[45] M. Thom and G. Palm, “Sparse activity and sparse connectivity in
supervised learning,” Journal of Machine Learning Research, vol. 14,
no. Apr, pp. 1091–1143, 2013.

[46] S. Rifai, X. Glorot, Y. Bengio, and P. Vincent, “Adding noise to
the input of a model trained with a regularized objective,” arXiv
preprint arXiv:1104.3250, 2011.

[47] C. M. Bishop, “Training with noise is equivalent to tikhonov
regularization,” Neural computation, vol. 7, no. 1, pp. 108–116, 1995.
[48] W. Jiang, F. Nie, and H. Huang, “Robust dictionary learning with
capped l1-norm,” in Proceedings of the Twenty-Fourth International
Joint Conference on Artiﬁcial Intelligence, IJCAI 2015, Buenos Aires,
Argentina, July 25-31, 2015, 2015, pp. 3590–3596.

[49] A. Krizhevsky and G. Hinton, “Learning multiple layers of fea-
tures from tiny images. Technical report, University of Toronto,”
2009.

[50] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”
International Journal of Computer Vision (IJCV), vol. 115, no. 3, pp.
211–252, 2015.

[51] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture
for fast feature embedding,” in Proceedings of the ACM International
Conference on Multimedia. ACM, 2014, pp. 675–678.

[52] D. R. G. H. R. Williams and G. Hinton, “Learning representations

by back-propagating errors,” Nature, pp. 323–533, 1986.

[53] R. Tibshirani, “Regression shrinkage and selection via the lasso,”
Journal of the Royal Statistical Society. Series B (Methodological), pp.
267–288, 1996.

[54] L. Yuan, J. Liu, and J. Ye, “Efﬁcient methods for overlapping
group lasso,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 35, no. 9, pp. 2104–2116, 2013.

[55] A. Krizhevsky,

“cuda-convnet,”

2012.

[Online]. Available:

https://code.google.com/p/cuda-convnet/

[56] I. Guyon and A. Elisseeff, “An introduction to variable and feature
selection,” Journal of machine learning research, vol. 3, no. Mar, pp.
1157–1182, 2003.

[57] K. Wang, R. He, L. Wang, W. Wang, and T. Tan, “Joint feature
selection and subspace learning for cross-modal retrieval,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 38,
no. 10, pp. 2010–2023, Oct 2016.

[58] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial
nets,” in Advances in neural information processing systems, 2014, pp.
2672–2680.

[59] A. Radford, L. Metz, and S. Chintala, “Unsupervised represen-
tation learning with deep convolutional generative adversarial
networks,” arXiv preprint arXiv:1511.06434, 2015.

[60] M. Arjovsky and L. Bottou, “Towards principled methods for
training generative adversarial networks,” in NIPS 2016 Workshop
on Adversarial Training. In review for ICLR, vol. 2016, 2017.

[61] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” arXiv

preprint arXiv:1701.07875, 2017.

[62] J. Bergstra and Y. Bengio, “Random search for hyper-parameter
optimization,” Journal of Machine Learning Research, vol. 13, no. Feb,
pp. 281–305, 2012.

9
1
0
2
 
r
p
A
 
3
1
 
 
]

V
C
.
s
c
[
 
 
1
v
3
9
5
6
0
.
4
0
9
1
:
v
i
X
r
a

1

Shakeout: A New Approach to Regularized
Deep Neural Network Training

Guoliang Kang, Jun Li, and Dacheng Tao, Fellow, IEEE

Abstract—Recent years have witnessed the success of deep neural networks in dealing with a plenty of practical problems. Dropout
has played an essential role in many successful deep neural networks, by inducing regularization in the model training. In this paper,
we present a new regularized training approach: Shakeout. Instead of randomly discarding units as Dropout does at the training stage,
Shakeout randomly chooses to enhance or reverse each unit’s contribution to the next layer. This minor modiﬁcation of Dropout has the
statistical trait: the regularizer induced by Shakeout adaptively combines L0, L1 and L2 regularization terms. Our classiﬁcation
experiments with representative deep architectures on image datasets MNIST, CIFAR-10 and ImageNet show that Shakeout deals with
over-ﬁtting effectively and outperforms Dropout. We empirically demonstrate that Shakeout leads to sparser weights under both
unsupervised and supervised settings. Shakeout also leads to the grouping effect of the input units in a layer. Considering the weights
in reﬂecting the importance of connections, Shakeout is superior to Dropout, which is valuable for the deep model compression.
Moreover, we demonstrate that Shakeout can effectively reduce the instability of the training process of the deep architecture.

Index Terms—Shakeout, Dropout, Regularization, Sparsity, Deep Neural Network.

(cid:70)

1 INTRODUCTION

D EEP neural networks have recently achieved impres-

sive success in a number of machine learning and
pattern recognition tasks and been under intensive research
[1], [2], [3], [4], [5], [6], [7]. Hierarchical neural networks
have been known for decades, and there are a number of
essential factors contributing to its recent rising, such as
the availability of big data and powerful computational re-
sources. However, arguably the most important contributor
to the success of deep neural network is the discovery of
efﬁcient training approaches [8], [9], [10], [11], [12].

A particular interesting advance in the training tech-
niques is the invention of Dropout [13]. At the operational
level, Dropout adjusts the network evaluation step (feed-
forward) at the training stage, where a portion of units
are randomly discarded. The effect of this simple trick is
impressive. Dropout enhances the generalization perfor-
mance of neural networks considerably, and is behind many
record-holders of widely recognized benchmarks [2], [14],
[15]. The success has attracted much research attention, and
found applications in a wider range of problems [16], [17],
[18]. Theoretical research from the viewpoint of statistical
learning has pointed out the connections between Dropout
and model regularization, which is the de facto recipe of

• Guoliang Kang, Jun Li are with Center of AI, Faculty of Engineering
and Information Technology, University of Technology Sydney, Ultimo,
NSW, Australia. D. Tao is with the UBTech Sydney Artiﬁcial Intelligence
Institute and the School of Information Technologies in the Faculty of
Engineering and Information Technologies at The University of Sydney,
Darlington, NSW 2008, Australia.
E-mail: Guoliang.Kang@student.uts.edu.au,
dacheng.tao@sydney.edu.au
c(cid:13)2018 IEEE. Personal use of this material is permitted. Permission
from IEEE must be obtained for all other uses, in any current or future
media, including reprinting/republishing this material for advertising
or promotional purposes, creating new collective works, for resale or
redistribution to servers or lists, or reuse of any copyrighted component of
this work in other works.

junjy007@googlemail.com,

reducing over-ﬁtting for complex models in practical ma-
chine learning. For example, Wager et al. [16] showed that
for a generalized linear model (GLM), Dropout implicitly
imposes an adaptive L2 regularizer of the network weights
through an estimation of the inverse diagonal Fisher infor-
mation matrix.

importance in deep learning. It
Sparsity is of vital
is straightforward that
through removing unimportant
weights, deep neural networks perform prediction faster.
Additionally, it is expected to obtain better generalization
performance and reduce the number of examples needed in
the training stage [19]. Recently much evidence has shown
that the accuracy of a trained deep neural network will not
be severely affected by removing a majority of connections
and many researchers focus on the deep model compression
task [20], [21], [22], [23], [24], [25]. One effective way of com-
pression is to train a neural network, prune the connections
and ﬁne-tune the weights iteratively [21], [22]. However, if
we can cut the connections naturally via imposing sparsity-
inducing penalties in the training process of a deep neural
network, the work-ﬂow will be greatly simpliﬁed.

In this paper, we propose a new regularized deep neu-
ral network training approach: Shakeout, which is easy to
implement: randomly choosing to enhance or reverse each
unit’s contribution to the next layer in the training stage.
Note that Dropout can be considered as a special “ﬂat” case
of our approach: randomly keeping (enhance factor is 1) or
discarding (reverse factor is 0) each unit’s contribution to
the next layer. Shakeout enriches the regularization effect.
In theory, we prove that it adaptively combines L0, L1 and
L2 regularization terms. L0 and L1 regularization terms
are known as sparsity-inducing penalties. The combination
of sparsity-inducing penalty and L2 penalty of the model
parameters has shown to be effective in statistical learning:
the Elastic Net [26] has the desirable properties of producing
sparse models while maintaining the grouping effect of the

weights of the model. Because of the randomly “shaking”
process and the regularization characteristic pushing net-
work weights to zero, our new approach is named “Shake-
out”.

As discussed above, it is expected to obtain much sparser
weights using Shakeout than using Dropout because of the
combination of L0 and L1 regularization terms induced
in the training stage. We apply Shakeout on one-hidden-
layer autoencoder and obtain much sparser weights than
that resulted by Dropout. To show the regularization effect
on the classiﬁcation tasks, we conduct the experiments on
image datasets including MNIST, CIFAR-10 and ImageNet
with the representative deep neural network architectures.
In our experiments we ﬁnd that by using Shakeout, the
trained deep neural networks always outperform those by
using Dropout, especially when the data is scarce. Besides
the fact that Shakeout leads to much sparser weights, we
also empirically ﬁnd that it groups the input units of a
layer. Due to the induced L0 and L1 regularization terms,
Shakeout can result in the weights reﬂecting the importance
of the connections between units, which is meaningful for
conducting compression. Moreover, we demonstrate that
Shakeout can effectively reduce the instability of the training
process of the deep architecture.

This journal paper extends our previous work [27] theo-
retically and experimentally. The main extensions are listed
as follows: 1) we derive the analytical formula for the
regularizer induced by Shakeout in the context of GLM and
prove several important properties; 2) we conduct experi-
ments using Wide Residual Network [15] on CIFAR-10 to
show Shakeout outperforms Dropout and standard back-
propagation in promoting the generalization performance
of a much deeper architecture; 3) we conduct experiments
using AlexNet [14] on ImageNet dataset with Shakeout
and Dropout. Shakeout obtains comparable classiﬁcation
performance to Dropout, but with superior regularization
effect; 4) we illustrate that Shakeout can effectively reduce
the instability of the training process of the deep architec-
ture. Moreover, we provide a much clearer and detailed de-
scription of Shakeout, derive the forward-backward update
rule for deep convolutional neural networks with Shakeout,
and give several recommendations to help the practitioners
make full use of Shakeout.

In the rest of the paper, we give a review about the
related work in Section 2. Section 3 presents Shakeout in de-
tail, along with theoretical analysis of the regularization ef-
fect induced by Shakeout. In Section 4, we ﬁrst demonstrate
the regularization effect of Shakeout on the autoencoder
model. The classiﬁcation experiments on MNIST , CIFAR-10
and ImageNet illustrate that Shakeout outperforms Dropout
considering the generalization performance, the regulariza-
tion effect on the weights, and the stabilization effect on the
training process of the deep architecture. Finally, we give
some recommendations for the practitioners to make full
use of Shakeout.

2 RELATED WORK

Deep neural networks have shown their success in a wide
variety of applications. One of the key factors contributes to
this success is the creation of powerful training techniques.

2

The representative power of the network becomes stronger
as the architecture gets deeper [9]. However, millions of pa-
rameters make deep neural networks easily over-ﬁt. Regu-
larization [16], [28] is an effective way to obtain a model that
generalizes well. There exist many approaches to regularize
the training of deep neural networks, like weight decay [29],
early stopping [30], etc. Shakeout belongs to the family of
regularized training techniques.

Among these regularization techniques, our work is
closely related to Dropout [13]. Many subsequent works
were devised to improve the performance of Dropout [31],
[32], [33]. The underlying reason why Dropout improves
performance has also attracted the interest of many re-
searchers. Evidence has shown that Dropout may work
because of its good approximation to model averaging
and regularization on the network weights [34], [35], [36].
Srivastava [34] and Warde-Farley [35] exhibited through
experiments that the weight scaling approximation is an
accurate alternative for the geometric mean over all possible
sub-networks. Gal et al. [37] claimed that training the deep
neural network with Dropout is equivalent to performing
variational inference in a deep Gaussian Process. Dropout
can also be regarded as a way of adding noise into the
neural network. By marginalizing the noise, Srivastava [34]
proved for linear regression that the deterministic version
of Dropout is equivalent to adding an adaptive L2 regular-
ization on the weights. Furthermore, Wager [16] extended
the conclusion to generalized linear models (GLMs) using
a quadratic approximation to the induced regularizer. The
inductive bias of Dropout was studied by Helmbold et al.
[38] to illustrate the properties of the regularizer induced
by Dropout further. In terms of implicitly inducing regu-
larizer of the network weights, Shakeout can be viewed as
a generalization of Dropout. It enriches the regularization
effect of Dropout, i.e. besides the L2 regularization term, it
also induces the L0 and L1 regularization terms, which may
lead to sparse weights of the model.

Due to the implicitly induced L0 and L1 regulariza-
tion terms, Shakeout is also related to sparsity-inducing
approaches. Olshausen et al. [39] introduced the concept
of sparsity in computational neuroscience and proposed
the sparse coding method in the visual system. In machine
learning, the sparsity constraint enables a model to capture
the implicit statistical data structure, performs feature selec-
tion and regularization, compresses the data at a low loss of
the accuracy, and helps us to better understand our models
and explain the obtained results. Sparsity is one of the key
factors underlying many successful deep neural network
architectures [2], [40], [41], [42] and training algorithms [43]
[44]. A Convolutional neural network is much sparser than
the fully-connected one, which results from the concept of
local receptive ﬁeld [40]. Sparsity has been a design princi-
ple and motivation for Inception-series models [2], [41], [42].
Besides working as the heuristic principle of designing a
deep architecture, sparsity often works as a penalty induced
to regularize the training process of a deep neural network.
There exist two kinds of sparsity penalties in deep neural
networks, which lead to the activity sparsity [43] [44] and
the connectivity sparsity [45] respectively. The difference
between Shakeout and these sparsity-inducing approaches
is that for Shakeout, the sparsity is induced through sim-

ple stochastic operations rather than manually designed
architectures or explicit norm-based penalties. This implicit
way enables Shakeout to be easily optimized by stochastic
gradient descent (SGD) − the representative approach for
the optimization of a deep neural network.

3 SHAKEOUT

Shakeout applies on the weights in a linear module. The
linear module, i.e. weighted sum,

θ =

wjxj

p
(cid:88)

j=1

(1)

is arguably the most widely adopted component in data
models. For example, the variables x1, x2, . . . , xp can be
input attributes of a model, e.g. the extracted features for
a GLM, or the intermediate outputs of earlier processing
steps, e.g. the activations of the hidden units in a multilayer
artiﬁcial neural network. Shakeout randomly modiﬁes the
computation in Eq. (1). Speciﬁcally, Shakeout can be realized
by randomly modifying the weights

Step 1: Draw rj, where

(cid:40)

P (rj = 0)
P (rj = 1

= τ
1−τ ) = 1 − τ

.

Step 2: Adjust the weight according to rj,

(cid:40)

˜wj ← −csj,
˜wj ← (wj + cτ sj)/(1 − τ )

if rj = 0
otherwise

(A)
(B)

where sj = sgn(wj) takes ±1 depending on the sign of wj
or takes 0 if wj = 0. As shown above, Shakeout chooses
(randomly by drawing r) between two fundamentally dif-
ferent ways to modify the weights. Modiﬁcation (A) is to set
the weights to constant magnitudes, despite their original
values except for the signs (to be opposite to the original
ones). Modiﬁcation (B) updates the weights by a factor
(1 − τ )−1 and a bias depending on the signs. Note both (A)
and (B) preserve zero values of the weights, i.e. if wj = 0
then ˜wj = 0 with probability 1. Let ˜θ = ˜wT x, and Shakeout
leaves θ unbiased, i.e. E[˜θ] = θ. The hyper-parameters
τ ∈ (0, 1) and c ∈ (0, +∞) conﬁgure the property of
Shakeout.

Shakeout is naturally connected to the widely adopted
operation of Dropout [13], [34]. We will show that Shakeout
has regularization effect on model training similar to but
beyond what is induced by Dropout. From an operational
point of view, Fig. 1 compares Shakeout and Dropout. Note
that Shakeout includes Dropout as a special case when the
hyper-parameter c in Shakeout is set to zero.

When applied at the training stage, Shakeout alters the
objective − the quantity to be minimized − by adjusting the
weights. In particular, we will show that Shakeout (with ex-
pectation over the random switch) induces a regularization
term effectively penalizing the magnitudes of the weights
and leading to sparse weights. Shakeout is an approach
designed for helping model training, when the models are
trained and deployed, one should relieve the disturbance to
allow the model work with its full capacity, i.e. we adopt the
resulting network without any modiﬁcation of the weights
at the test stage.

3

Fig. 1. Comparison between Shakeout and Dropout operations. This
ﬁgure shows how Shakeout and Dropout are applied to the weights in a
linear module. In the original linear module, the output is the summation
of the inputs x weighted by w, while for Dropout and Shakeout, the
weights w are ﬁrst randomly modiﬁed. In detail, a random switch ˆr
controls how each w is modiﬁed. The manipulation of w is illustrated
within the ampliﬁer icons (the red curves, best seen with colors). The
coefﬁcients are α = 1/(1 − τ ) and β(w) = cs(w), where s(w) extracts
the sign of w and c > 0, τ ∈ [0, 1]. Note the sign of β(w) is always
the same as that of w. The magnitudes of coefﬁcients α and β(w) are
determined by the Shakeout hyper-parameters τ and c. Dropout can be
viewed as a special case of Shakeout when c = 0 because β(w) is zero
at this circumstance.

3.1 Regularization Effect of Shakeout

Shakeout randomly modiﬁes the weights in a linear module,
and thus can be regarded as injecting noise into each vari-
able xj, i.e. xj is randomly scaled by γj: ˜xj = γjxj. Note
that γj = rj + c(rj −1)
, the modiﬁcation of xj is actually
|wj |
determined by the random switch rj. Shakeout randomly
chooses to enhance (i.e. when rj = 1
1−τ ) or
reverse (i.e. when rj = 0, γj < 0) each original variable
xj’s contribution to the output at the training stage (see Fig.
1). However, the expectation of ˜xj over the noise remains
unbiased, i.e. Erj [˜xj] = xj.

1−τ , γj > 1

It is well-known that injecting artiﬁcial noise into the
input features will regularize the training objective [16],
[46], [47], i.e. Er[(cid:96)(w, ˜x, y)] = (cid:96)(w, x, y) + π(w), where
˜x is the feature vector randomly modiﬁed by the noise
induced by r. The regularization term π(w) is determined
by the characteristic of the noise. For example, Wager et
al. [16] showed that Dropout, corresponding to inducing
blackout noise to the features, helps introduce an adaptive
L2 penalty on w. In this section we illustrate how Shakeout
helps regularize model parameters w using an example of
GLMs.

Formally, a GLM is a probabilistic model of predicting
target y given features x = [x1, . . . , xp], in terms of the
weighted sum in Eq. (1):

P (y|x, w) = h(y)g(θ)eθy
θ = wT x

(2)

With different h(·) and g(·) functions, GLM can be special-
ized to various useful models or modules, such as logistic
regression model or a layer in a feed-forward neural net-
work. However, roughly speaking, the essence of a GLM
is similar to that of a standard linear model which aims

to ﬁnd weights w1, . . . , wp so that θ = wT x aligns with
y (functions h(·) and g(·) are independent of w and y
respectively). The loss function of a GLM with respect to
w is deﬁned as

We illustrate several properties of Shakeout regularizer
based on Eq. (7). The proof of the following propositions
can be found in the appendices.
Proposition 1. π(0) = 0

4

l(w, x, y) = −θy + A(θ)

A(θ) = − ln[g(θ)]

(3)

(4)

The loss (3) is the negative logarithm of probability (2),
where we keep only terms relevant to w.

Let the loss with Shakeout be

lsko(w, x, y, r) := l(w, ˜x, y)
where r = [r1, . . . , rp]T , and ˜x = [˜x1, . . . , ˜xp]T represents
the features randomly modiﬁed with r.

(5)

Taking expectation over r, the loss with Shakeout be-

Er[lsko(w, x, y, r)] = l(w, x, y) + π(w)

comes

where

π(w) = Er[A(˜θ) − A(θ)]
∞
(cid:88)

=

1
k!

k=1

A(k)(θ)E[(˜θ − θ)k]

(6)

is named Shakeout regularizer. Note that if A(θ) is k-th order
)(θ) = 0 where
derivable, let the k
k
Theorem 1. Let qj = xj(wj + csj), θj− = θ − qj and θj+ =

> k, to make the denotation simple.

order derivative A(k

(cid:48)

(cid:48)

(cid:48)

θ + τ

1−τ qj, then Shakeout regularizer π(w) is

π(w) = τ

A(θj−) + (1 − τ )

A(θj+) − pA(θ) (7)

p
(cid:88)

j=1

p
(cid:88)

j=1

Proof: Note that ˜θ − θ = (cid:80)p

j=1 qj(rj − 1), then for Eq.

(6)

E[(˜θ − θ)k] =

p
(cid:88)

p
(cid:88)

p
(cid:88)

k
(cid:89)

· · ·

k
(cid:89)

qjm

E[

(rjm − 1)]

j1=1

j2=1

jk=1

m=1

m=1

Because arbitrary two random variables rjm1 , rjm2 are inde-
pendent unless jm1 = jm2 and ∀rjm , E[rjm − 1] = 0, then

p
(cid:88)

j=1
p
(cid:88)

j=1

E[(˜θ − θ)k] =

qk
j

E[(rj − 1)k]

= τ

(−qj)k + (1 − τ )

p
(cid:88)

(
j=1

τ
1 − τ

qj)k

Then

π(w) = τ

A(k)(θ)(−qj)k

p
(cid:88)

∞
(cid:88)

j=1

k=1

1
k!

+(1 − τ )

p
(cid:88)

∞
(cid:88)

j=1

k=1

1
k!

A(k)(θ)(

τ
1 − τ

qj)k

Further, let θj− = θ − qj, θj+ = θ + τ

π(w) = τ

A(θj−) + (1 − τ )

p
(cid:88)

j=1

1−τ qj, π(w) becomes
p
(cid:88)

A(θj+) − pA(θ)

j=1

The theorem is proved.

Proposition 2. If A(θ) is convex, π(w) ≥ 0.

Proposition 3. Suppose ∃j, xjwj (cid:54)= 0. If A(θ) is convex,
(θ) > 0, π(w)

π(w) monotonically increases with τ . If A
monotonically increases with c.

(cid:48)(cid:48)

Proposition 3 implies that the hyper-parameters τ and c
relate to the strength of the regularization effect. It is rea-
sonable because higher τ or c means the noise injected into
the features x has larger variance.
Proposition 4. Suppose i) ∀j (cid:54)= j

, xjwj = 0, and ii) xj(cid:48) (cid:54)= 0.

(cid:48)

Then

(cid:48)(cid:48)

i) if A

(θ) > 0,

ii) if lim|θ|→∞ A





(cid:48)(cid:48)

> 0, when wj(cid:48) > 0
< 0, when wj(cid:48) < 0

∂π(w)
∂wj(cid:48)
∂π(w)
∂wj(cid:48)
(θ) = 0, lim|wj(cid:48) |→∞

∂π(w)
∂wj(cid:48)

= 0

Proposition 4 implies that under certain conditions, start-
ing from a zero weight vector, Shakeout regularizer pe-
nalizes the magnitude of wj(cid:48) and its regularization effect
is bounded by a constant value. For example, for logistic
regression, π(w) ≤ τ ln(1+exp(c|xj(cid:48) |)), which is illustrated
in Fig. 2. This bounded property has been proved to be
useful: capped-norm [48] is more robust to outliers than the
traditional L1 or L2 norm.

Based on the Eq. (7), the speciﬁc formulas for the repre-

sentative GLM models can be derived:
i) Linear regression: A(θ) = 1

π(w) =

τ
2(1 − τ )

2 θ2, then
(cid:107)x ◦ (w + cs)(cid:107)2
2

where ◦ denotes
(cid:107)x ◦ (w + cs)(cid:107)2
tion of three components

the element-wise product and the
2 term can be decomposed into the summa-

p
(cid:88)

j=1

p
(cid:88)

j=1

p
(cid:88)

j=1

j w2
x2

j + 2c

j |wj| + c2
x2

x2
j 1wj (cid:54)=0[wj]

(8)

where 1wj (cid:54)=0[wj] is an indicator function which satisﬁes

(cid:40)

1 wj (cid:54)= 0
0 wj = 0

1wj (cid:54)=0[wj] =

. This decomposition implies that

Shakeout regularizer penalizes the combination of L0-norm,
L1-norm and L2-norm of the weights after scaling them
with the square of corresponding features. The L0 and L1
regularization terms can lead to sparse weights.

ii) Logistic regression: A(θ) = ln(1 + exp(θ)), then

π(w) =

ln(

p
(cid:88)

j=1

(1 + exp(θj−))τ (1 + exp(θj+))1−τ
1 + exp(θ)

)

(9)

Fig. 3 illustrates the contour of Shakeout regularizer based
on Eq. (9) in the 2D weight space. On the whole, the con-
tour of Shakeout regularizer indicates that the regularizer
combines L0, L1 and L2 regularization terms. As c goes to
zero, the contour around w = 0 becomes less sharper, which
implies hyper-parameter c relates to the strength of L0 and
L1 components. When c = 0, Shakeout degenerates to

5

(a) Shakeout regularizer: τ = 0.3, c = 0.78

(b) Dropout regularizer: τ = 0.5

Fig. 2. Regularization effect as a function of a single weight when other weights are ﬁxed to zeros for logistic regression model. The corresponding
feature x is ﬁxed at 1.

Dropout, the contour of which implies Dropout regularizer
consists of L2 regularization term.

imposed and leads to much sparser weights of the model.
We will verify this property in our experiment section later.

The difference between Shakeout and Dropout regular-
izers is also illustrated in Fig. 2. We set τ = 0.3, c = 0.78
for Shakeout, and τ = 0.5 for Dropout to make the bounds
of the regularization effects of two regularizers the same. In
this one dimension circumstance, the main difference is that
at w = 0 (see the enlarged snapshot for comparison), Shake-
out regularizer is sharp and discontinuous while Dropout
regularizer is smooth. Thus compared to Dropout, Shakeout
may lead to much sparser weights of the model.

To simplify the analysis and prove the intuition we have
observed in Fig. 3 about the properties of Shakeout regular-
izer, we quadratically approximate Shakeout regularizer of
Eq. (7) by

πapprox(w) =

τ
2(1 − τ )

(cid:48)(cid:48)

A

(θ) (cid:107)x ◦ (w + cs)(cid:107)2
2

(10)

The (cid:107)x ◦ (w + cs)(cid:107)2
2, already shown in Eq. (8), consists
of the combination of L0, L1, L2 regularization terms. It
tends to penalize the weight whose corresponding feature’s
magnitude is large. Meanwhile, the weights whose corre-
sponding features are always zeros are less penalized. The
(θ) is proportional to the variance of prediction y
term A
given x and w. Penalizing A
(θ) encourages the weights to
move towards making the model be more ”conﬁdent” about
its predication, i.e. be more discriminative.

(cid:48)(cid:48)

(cid:48)(cid:48)

Generally speaking, Shakeout regularizer adaptively
combines L0, L1 and L2 regularization terms, the property
of which matches what we have observed in Fig. 3. It
prefers penalizing the weights who have large magnitudes
and encourages the weights to move towards making the
model more discriminative. Moreover, the weights whose
corresponding features are always zeros are less penalized.
The L0 and L1 components can induce sparse weights.

Last but not the least, we want to emphasize that when
τ = 0, the noise is eliminated and the model becomes a
standard GLM. Moreover, Dropout can be viewed as the
special case of Shakeout when c = 0, and a higher value of
τ means a stronger L2 regularization effect imposed on the
weights. Generally, when τ is ﬁxed (τ (cid:54)= 0), a higher value
of c means a stronger effect of the L0 and L1 components

3.2 Shakeout in Multilayer Neural Networks

It has been illustrated that Shakeout regularizes the weights
in linear modules. Linear module is the basic component
of multilayer neural networks. That is, the linear operations
connect the outputs of two successive layers. Thus Shakeout
is readily applicable to the training of multilayer neural
networks.

Considering the forward computation from layer l to
layer l + 1, for a fully-connected layer, the Shakeout forward
computation is as follows

ui =

xj[rjWij + c(rj − 1)Sij] + bi

(11)

(cid:88)

j

(cid:48)
i = f (ui)
x

(12)

where i denotes the index of the output unit of layer l + 1,
and j denotes the index of the output unit of layer l. The
output unit of a layer is represented by x. The weight of the
connection between unit xj and unit x
i is represented as
Wij. The bias for the i-th unit is denoted by bi. The Sij
is the sign of corresponding weight Wij. After Shakeout
operation, the linear combination ui is sent to the activation
function f (·) to obtain the corresponding output x
i. Note
that the weights Wij that connect to the same input unit xj
are controlled by the same random variable rj.

(cid:48)

(cid:48)

During back-propagation, we should compute the gra-
dients with respect to each unit to propagate the error. In
Shakeout, ∂ui
∂xj

takes the form

= rj(Wij + cSij) − cSij

(13)

And the weights are updated following

= xj(rj + c(rj − 1)

(14)

dSij
dWij

)

dSij
dWij

represents the derivative of a sgn function.
where
Because the sgn function is not continuous at zero and

∂ui
∂xj

∂ui
∂Wij

6

Fig. 3. The contour plots of the regularization effect induced by Shakeout in 2D weight space with input x = [1, 1]T . Note that Dropout is a special
case of Shakeout with c = 0.

thus the derivative is not deﬁned, we approximate this
. Empirically we ﬁnd that this
derivative with
approximation works well.

d tanh(Wij )
dWij

Note that

the forward-backward computations with
Shakeout can be easily extended to the convolutional layer.
For a convolutional layer, the Shakeout feed-forward pro-
cess can be formalized as

Ui =

(Xj ◦ Rj) ∗ Wij + c(Xj ◦ (Rj − 1)) ∗ Sij + bi (15)

(cid:88)

j

(cid:48)

X

(16)

i = f (Ui)
where Xj represents the j-th feature map. Rj is the j-th
random mask which has the same spatial structure (i.e. the
same height and width) as the corresponding feature map
Xj. Wij denotes the kernel connecting Xj and Ui. And Sij
is set as sgn(Wij). The symbol * denotes the convolution
operation. And the symbol ◦ means element-wise product.
Correspondingly, during the back-propagation process,
the gradient with respect to a unit of the layer on which
Shakeout is applied takes the form

∂Ui(a, b)
∂Xj(a − a(cid:48), b − b(cid:48))

= Rj(a − a

, b − b

)(Wij(a

(cid:48)

(cid:48)

(cid:48)

(cid:48)

, b

)+

(cid:48)

(cid:48)

cSij(a

, b

)) − cSij(a

(cid:48)

(cid:48)

, b

)

(17)

where (a, b) means the position of a unit in the output
) represents the position
feature map of a layer, and (a
of a weight in the corresponding kernel.

, b

(cid:48)

(cid:48)

The weights are updated following

∂Ui(a, b)
∂Wij(a(cid:48), b(cid:48))

= Xj(a − a

(cid:48)

(cid:48)
, b − b

)(Rj(a − a

, b − b

)

(cid:48)

(cid:48)

+ c(Rj(a − a

(cid:48)

(cid:48)
, b − b

) − 1)

(cid:48)

(cid:48)
, b

)
dSij(a
dWij(a(cid:48), b(cid:48))

)

(18)

4 EXPERIMENTS
In this section, we report empirical evaluations of Shake-
out in training deep neural networks on representative
datasets. The experiments are performed on three kinds of
image datasets: the hand-written image dataset MNIST [40],
the CIFAR-10 image dataset [49] and the ImageNet-2012
dataset [50]. MNIST consists of 60,000+10,000 (training+test)
28×28 images of hand-written digits. CIFAR-10 contains
50,000+10,000 (training+test) 32×32 images of 10 object
classes. ImageNet-2012 consists of 1,281,167+50,000+150,000
(training+validation+test) variable-resolution images of
1000 object classes. We ﬁrst demonstrate that Shakeout leads
to sparse models as our theoretical analysis implies under
the unsupervised setting. Then we show that for the classi-
ﬁcation task, the sparse models have desirable generaliza-
tion performances. Further, we illustrate the regularization
effect of Shakeout on the weights in the classiﬁcation task.
Moreover, the effect of Shakeout on stabilizing the training
processes of the deep architectures is demonstrated. Finally,
we give some practical recommendations of Shakeout. All

the experiments are implemented based on the modiﬁca-
tions of Caffe library [51]. Our code is released on the github:
https://github.com/kgl-prml/shakeout-for-caffe.

4.1 Shakeout and Weight Sparsity

implicitly imposes L0 penalty and L1
Since Shakeout
penalty of the weights, we expect the weights of neural
networks learned by Shakeout contain more zeros than
those learned by the standard back-propagation (BP) [52] or
Dropout [13]. In this experiment, we employ an autoencoder
model for the MNIST hand-written data, train the model
using standard BP, Dropout and Shakeout, respectively, and
compare the degree of sparsity of the weights of the learned
encoders. For the purpose of demonstration, we employ
the simple autoencoder with one hidden layer of 256 units;
Dropout and Shakeout are applied on the input pixels.

To verify the regularization effect, we compare the
weights of the four autoencoders trained under different
settings which correspond to standard BP, Dropout (τ = 0.5)
and Shakeout (τ = 0.5, c = {1, 10}). All the training
methods aim to produce hidden units which can capture
good visual features of the handwritten digits. The statistical
traits of these different resulting weights are shown in Fig. 4.
Moreover, Fig. 5 shows the features captured by each hidden
unit of the autoencoders.

As shown in the Fig. 4, the probability density of weights
around the zero obtained by standard BP training is quite
small compared to the one obtained either by Dropout or
Shakeout. This indicates the strong regularization effect in-
duced by Dropout and Shakeout. Furthermore, the sparsity
level of weights obtained from training by Shakeout is much
higher than the one obtained from training by Dropout.
Using the same τ , increasing c makes the weights much
sparser, which is consistent with the characteristics of L0
penalty and L1 penalty induced by Shakeout. Intuitively,
we can ﬁnd that due to the induced L2 regularization,
the distribution of weights obtained from training by the
Dropout is like a Gaussian, while the one obtained from
training by Shakeout is more like a Laplacian because of
the additionally induced L1 regularization. Fig. 5 shows
that features captured by the hidden units via standard
BP training are not directly interpretable, corresponding to
insigniﬁcant variants in the training data. Both Dropout and
Shakeout suppress irrelevant weights by their regularization
effects, where Shakeout produces much sparser and more
global features thanks to the combination of L0, L1 and L2
regularization terms.

The autoencoder trained by Dropout or Shakeout can
be viewed as the denosing autoencoder, where Dropout
or Shakeout injects special kind of noise into the inputs.
Under this unsupervised setting, the denoising criterion
(i.e. minimizing the error between imaginary images recon-
structed from the noisy inputs and the real images without
noise) is to guide the learning of useful high level feature
representations [11], [12]. To verify that Shakeout helps
learn better feature representations, we adopt the hidden
layer activations as features to train SVM classiﬁers, and the
classiﬁcation accuracies on test set for standard BP, Dropout
and Shakeout are 95.34%, 96.41% and 96.48%, respectively.
We can see that Shakeout leads to much sparser weights
without defeating the main objective.

7

Fig. 4. Distributions of the weights of the autoencoder models learned
by different training approaches. Each curve in the ﬁgure shows the
frequencies of the weights of an autoencoder taking particular values,
i.e. the empirical population densities of the weights. The ﬁve curves
correspond to ﬁve autoencoders learned by standard back-propagation,
Dropout (τ = 0.5), Gaussian Dropout (σ2 = 1) and Shakeout (τ = 0.5,
c = {1, 10}). The sparsity of the weights obtained via Shakeout can be
seen by comparing the curves.

Gaussian Dropout has similar effect on the model train-
ing as standard Dropout [34], which multiplies the acti-
vation of each unit by a Gaussian variable with mean 1
and variance σ2. The relationship between σ2 and τ is
that σ2 = τ
1−τ . The distribution of the weights trained by
Gaussian Dropout (σ2 = 1, i.e. τ = 0.5) is illustrated in
Fig. 4. From Fig. 4, we ﬁnd no notable statistical difference
between two kinds of Dropout implementations which all
exhibit a kind of L2 regularization effect on the weights.
The classiﬁcation performances of SVM classiﬁers on test set
based on the hidden layer activations as extracted features
for both kinds of Dropout implementations are quite similar
(i.e. 96.41% and 96.43% for standard and Gaussian Dropout
respectively). Due to these observations, we conduct the fol-
lowing classiﬁcation experiments using standard Dropout
as a representative implementation (of Dropout) for com-
parison.

4.2 Classiﬁcation Experiments

Sparse models often indicate lower complexity and better
generalization performance [26], [39], [53], [54]. To verify
the effect of L0 and L1 regularization terms induced by
Shakeout on the model performance, we apply Shakeout,
along with Dropout and standard BP, on training represen-
tative deep neural networks for classiﬁcation tasks. In all of
our classiﬁcation experiments, the hyper-parameters τ and
c in Shakeout, and the hyper-parameter τ in Dropout are
determined by validation.

4.2.1 MNIST

We train two different neural networks, a shallow fully-
connected one and a deep convolutional one. For the
fully-connected neural network, a big hidden layer size is
adopted with its value at 4096. The non-linear activation

8

(a) standard BP

(b) Dropout: τ = 0.5

(c) Shakeout: τ = 0.5, c = 0.5

Fig. 5. Features captured by the hidden units of the autoencoder models learned by different training methods. The features captured by a hidden
unit are represented by a group of weights that connect the image pixels with this corresponding hidden unit. One image patch in a sub-graph
corresponds to the features captured by one hidden unit.

TABLE 1
The architecture of convolutional neural network adopted for MNIST
classiﬁcation experiment

TABLE 2
Classiﬁcation on MNIST using training sets of different sizes:
fully-connected neural network

Layer
Type
Channels
Filter size
Conv. stride
Pooling type
Pooling size
Pooling stride
Non-linear

1
conv.
20
5 × 5
1
max
2 × 2
2

2
conv.
50
5 × 5
1
max
2 × 2
2

3
FC
500
-
-
-
-
-

4
FC
10
-
-
-
-
-

ReLU ReLU ReLU Softmax

unit adopted is the rectiﬁer linear unit (ReLU). The deep
convolutional neural network employed is based on the
modiﬁcations of the LeNet [40], which contains two con-
volutional layers and two fully-connected layers. The de-
tailed architecture information of this convolutional neural
network is described in Tab. 1. We separate 10,000 training
samples from original training dataset for validation. The
results are shown in Tab. 2 and Tab. 3. Dropout and Shake-
out are applied on the hidden units of the fully-connected
layer. The table compares the errors of the networks trained
by standard back-propagation, Dropout and Shakeout. The
mean and standard deviation of the classiﬁcation errors
are obtained by 5 runs of the experiment and are shown
in percentage. We can see from the results that when the
training data is not sufﬁcient enough, due to over-ﬁtting, all
the models perform worse. However, the models trained by
Dropout and Shakeout consistently perform better than the
one trained by standard BP. Moreover, when the training
data is scarce, Shakeout leads to superior model perfor-
mance compared to the Dropout. Fig. 6 shows the results
in a more intuitive way.

4.2.2 CIFAR-10

We use the simple convolutional network feature extractor
described in cuda-convnet (layers-80sec.cfg) [55]. We apply
Dropout and Shakeout on the ﬁrst fully-connected layer. We
call this architecture “AlexFastNet” for the convenience of
description. In this experiment, 10,000 colour images are

Size
500
1000
3000
8000
20000
50000

std-BP
13.66±0.66
8.49±0.23
5.54±0.09
3.57±0.14
2.28±0.09
1.55±0.03

Dropout
11.76±0.09
8.05±0.05
4.87±0.06
2.95±0.05
1.82±0.07
1.36±0.03

Shakeout
10.81±0.32
7.19±0.15
4.60±0.07
2.96±0.09
1.92±0.06
1.35±0.07

TABLE 3
Classiﬁcation on MNIST using training sets of different sizes:
convolutional neural network

Size
500
1000
3000
8000
20000
50000

std-BP
9.76±0.26
6.73±0.12
2.93±0.10
1.70±0.03
0.97±0.01
0.78±0.05

Dropout
6.16±0.23
4.01±0.16
2.06±0.06
1.23±0.13
0.83±0.06
0.62±0.04

Shakeout
4.83±0.11
3.43±0.06
1.86±0.13
1.31±0.06
0.77±0.001
0.58±0.10

TABLE 4
Classiﬁcation on CIFAR-10 using training sets of different sizes:
AlexFastNet

Size
300
700
2000
5500
15000
40000

std-BP
68.26±0.57
59.78±0.24
50.73±0.29
41.41±0.52
32.53±0.25
24.48±0.23

Dropout
65.34±0.75
56.04±0.22
46.24±0.49
36.01±0.13
27.28±0.26
20.50±0.32

Shakeout
63.71±0.28
54.66±0.22
44.39±0.41
34.54±0.31
26.53±0.17
20.56±0.12

separated from the training dataset for validation and no
data augmentation is utilized. The per-pixel mean computed
over the training set is subtracted from each image. We ﬁrst
train for 100 epochs with an initial learning rate of 0.001
and then another 50 epochs with the learning rate of 0.0001.
The mean and standard deviation of the classiﬁcation errors
are obtained by 5 runs of the experiment and are shown in
percentage. As shown in Tab. 4, the performances of models
trained by Dropout and Shakeout are consistently superior

9

(a) Fully-connected neural network

(b) Convolutional neural network

Fig. 6. Classiﬁcation of two kinds of neural networks on MNIST using training sets of different sizes. The curves show the performances of the
models trained by standard BP, and those by Dropout and Shakeout applied on the hidden units of the fully-connected layer.

TABLE 5
Classiﬁcation on CIFAR-10 using training sets of different sizes:
WRN-16-4

Size
15000
40000
50000

std-BP Dropout
20.95
15.37
14.39

15.05
9.32
8.03

Shakeout
14.68
9.01
7.97

training set size at 40000. From Tab. 5, we can arrive at
the same conclusion as previous experiments, i.e. the per-
formances of the models trained by Dropout and Shakeout
are consistently superior to the one trained by standard BP.
Moreover, Shakeout outperforms Dropout when the data is
scarce.

4.2.3 Regularization Effect on the Weights

Shakeout is a different way to regularize the training process
of deep neural networks from Dropout. For a GLM model,
we have proved that the regularizer induced by Shakeout
adaptively combines L0, L1 and L1 regularization terms.
In section 4.1, we have demonstrated that for a one-hidden
layer autoencoder, it leads to much sparser weights of the
model. In this section, we will illustrate the regularization
effect of Shakeout on the weights in the classiﬁcation task
and make a comparison to that of Dropout.

The results shown in this section are mainly based on the
experiments conducted on ImageNet-2012 dataset using the
representative deep architecture: AlexNet [14]. For AlexNet,
we apply Dropout or Shakeout on layers FC7 and FC8
which are the last two fully-connected layers. We train the
model from the scratch and obtain the comparable classi-
ﬁcation performances on validation set for Shakeout (top-1
error: 42.88%; top-5 error: 19.85%) and Dropout (top-1 error:
42.99%; top-5 error: 19.60%). The model is trained based on
the same hyper-parameter settings provided by Shelhamer
in Caffe [51] other than the hyper-parameters τ and c for
Shakeout. The initial weights for training by Dropout and
Shakeout are kept the same.

Fig. 8 illustrates the distributions of the magnitude of
weight resulted by Shakeout and Dropout. It can be seen
that the weights learned by Shakeout are much sparser than

Fig. 7. Classiﬁcation on CIFAR-10 using training sets of different sizes.
The curves show the performances of the models trained by standard
BP, and those by Dropout and Shakeout applied on the hidden units of
the fully-connected layer.

to the one trained by standard BP. Furthermore, the model
trained by Shakeout also outperforms the one trained by
Dropout when the training data is scarce. Fig. 7 shows the
results in a more intuitive way.

To test the performance of Shakeout on a much deeper
architecture, we also conduct experiments based on the
Wide Residual Network (WRN) [15]. The conﬁguration of
WRN adopted is WRN-16-4, which means WRN has 16
layers in total and the number of feature maps for the
convolutional layer of each residual block is 4 times as the
corresponding original one [1]. Because the complexity is
much higher than that of “AlexFastNet”, the experiments
are performed on relatively larger training sets with sizes
of 15000, 40000, 50000. Dropout and Shakeout are applied
on the second convolutional layer of each residual block,
following the protocol in [15]. All the training starts from
the same initial weights. Batch Normalization is applied the
same way as [15] to promote the optimization. No data-
augmentation or data pre-processing is adopted. All the
other hyper-parameters other than τ and c are set the same
as [15]. The results are listed in Tab. 5. For the training
of CIFAR-10 with 50000 training samples, we adopt the
same hyper-parameters as those chosen in the training with

10

(a) AlexNet FC7 layer

(b) AlexNet FC8 layer

Fig. 8. Comparison of the distributions of the magnitude of weights trained by Dropout and Shakeout. The experiments are conducted using AlexNet
on ImageNet-2012 dataset. Shakeout or Dropout is applied on the last two fully-connected layers, i.e. FC7 layer and FC8 layer.

(a) AlexNet FC7 layer

(b) AlexNet FC8 layer

Fig. 9. Distributions of the maximum magnitude of the weights connected to the same input unit of a layer. The maximum magnitude of the weights
connected to one input unit can be regarded as a metric of the importance of that unit. The experiments are conducted using AlexNet on ImageNet-
2012 dataset. For Shakeout, the units can be approximately separated into two groups and the one around zero is less important than the other,
whereas for Dropout, the units are more concentrated.

those learned by Dropout, due to the implicitly induced L0
and L1 components.

The regularizer induced by Shakeout not only contains
L0 and L1 regularization terms but also contains L2 reg-
ularization term, the combination of which is expected to
discard a group of weights simultaneously. In Fig. 9, we use
the maximum magnitude of the weights connected to one
input unit of a layer to represent the importance of that unit
for the subsequent output units. From Fig. 9, it can be seen
that for Shakeout, the units can be approximately separated
into two groups according to the maximum magnitudes
of the connected weights and the group around zero can
be discarded, whereas for Dropout, the units are concen-
trated. This implies that compared to Dropout which may
encourage a “distributed code” for the features captured by
the units of a layer, Shakeout tends to discard the useless
features (or units) and award the important ones. This
experiment result veriﬁes the regularization properties of
Shakeout and Dropout further.

As known to us, L0 and L1 regularization terms are
related to performing feature selection [56], [57]. For a deep
architecture, it is expected to obtain a set of weights using
Shakeout suitable for reﬂecting the importance of connec-
tions between units. We perform the following experiment
to verify this effect. After a model is trained, for the layer
on which Dropout or Shakeout is applied, we sort the
magnitudes of the weights increasingly. Then we prune the
ﬁrst m% of the sorted weights and evaluate the performance
of the pruned model again. The pruning ratio m goes from 0
to 1. We calculate the relative accuracy loss (we write R.A.L
for simpliﬁcation) at each pruning ratio m

as

(cid:48)

R.A.L(m

) =

(cid:48)

Accu.(m = 0) − Accu.(m
Accu.(m = 0)

(cid:48)

)

Fig. 10 shows the R.A.L curves for Dropout and Shake-
out based on the AlexNet model on ImageNet-2012 dataset.
The models trained by Dropout and Shakeout are under the
optimal hyper-parameter settings. Apparently, the relative

accuracy loss for Dropout is more severe than that for Shake-
out. For example, the largest margin of the relative accuracy
losses between Dropout and Shakeout is 22.50%, which
occurs at the weight pruning ratio m = 96%. This result
proves that considering the trained weights in reﬂecting the
importance of connections, Shakeout is much better than
Dropout, which beneﬁts from the implicitly induced L0 and
L1 regularization effect. This kind of property is useful for
the popular compression task in deep learning area which
aims to cut the connections or throw units of a deep neural
network to a maximum extent without obvious loss of ac-
curacy. The above experiments illustrate that Shakeout can
play a considerable role in selecting important connections,
which is meaningful for promoting the performance of a
compression task. This is a potential subject for the future
research.

4.3 Stabilization Effect on the Training Process

In both research and production, it is always desirable to
have a level of certainty about how a model’s ﬁtness to
the data improves over optimization iterations, namely, to
have a stable training process. In this section, we show that
Shakeout helps reduce ﬂuctuation in the improvement of
model ﬁtness during training.

The ﬁrst experiment is on the family of Generative
Adversarial Networks (GANs) [58], which is known to be
instable in the training stage [59], [60], [61]. The purpose of
the following tests is to demonstrate the Shakeout’s capa-
bility of stabilizing the training process of neural networks
in a general sense. GAN plays a min-max game between
the generator G and the discriminator D over the expected
log-likelihood of real data x and imaginary data ˆx = G(z)
where z represents the random input

min
G

max
D

V (D, G) = E[log[D(x)] + log[1 − D(G(z))]] (19)

The architecture that we adopt is DCGAN [59]. The numbers
of feature maps of the deconvolutional layers in the gener-
ator are 1024, 64 and 1 respectively, with the corresponding
spatial sizes 7×7, 14×14 and 28×28. We train DCGANs on
MNIST dataset using standard BP, Dropout and Shakeout.
We follow the same experiment protocol described in [59]
except for adopting Dropout or Shakeout on all layers of the
discriminator. The values of −V (D, G) during training are
illustrated in Fig. 11. It can be seen that −V (D, G) during
training by standard BP oscillates greatly, while for Dropout
and Shakeout, the training processes are much steadier.
Compared with Dropout, the training process by Shakeout
has fewer spikes and is smoother. Fig. 12 demonstrates the
minimum and maximum values of −V (D, G) within ﬁxed
length intervals moving from the start to the end of the
training by standard BP, Dropout and Shakeout. It can be
seen that the gaps between the minimum and maximum
values of −V (D, G) trained by Dropout and Shakeout are
much smaller than that trained by standard BP, while that
by Shakeout is the smallest, which implies the stability of
the training process by Shakeout is the best.

The second experiment is based on Wide Residual Net-
work architecture to perform the classiﬁcation task. In the
classiﬁcation task, generalization performance is the main
focus and thus, we compare the validation errors during

11

the training processes by Dropout and Shakeout. Fig. 13
demonstrates the validation error as a function of the
training epoch for Dropout and Shakeout on CIFAR-10
with 40000 training examples. The architecture adopted is
WRN-16-4. The experiment settings are the same as those
described in Section 4.2.2. Considering the generalization
performance, the learning rate schedule adopted is the one
optimized through validation to make the models obtain the
best generalization performances. Under this schedule, we
ﬁnd that the validation error temporarily increases when
lowering the learning rate at the early stage of training,
which has been repeatedly observed by [15]. Nevertheless,
it can be seen from Fig. 13 that the extent of error increase is
less severe for Shakeout than Dropout. Moreover, Shakeout
recovers much faster than Dropout does. At the ﬁnal stage,
both of the validation errors steadily decrease. Shakeout
obtains comparable or even superior generalization perfor-
mance to Dropout. In a word, Shakeout signiﬁcantly stabi-
lizes the entire training process with superior generalization
performance.

4.4 Practical Recommendations

Selection of Hyper-parameters The most practical and pop-
ular way to perform hyper-parameter selection is to parti-
tion the training data into a training set and a validation
set to evaluate the classiﬁcation performance of different
hyper-parameters on it. Due to the expensive cost of time
for training a deep neural network, cross-validation is barely
adopted. There exist many hyper-parameter selection meth-
ods in the domain of deep learning, such as the grid search,
random search [62], Bayesian optimization methods [63],
gradient-based hyper-parameter Optimization [64], etc. For
applying Shakeout on a deep neural network, we need to
decide two hyper-parameters τ and c. From the regular-
ization perspective, we need to decide the most suitable
strength of regularization effect to obtain an optimal trade-
off between model bias and variance. We have pointed out
that in a uniﬁed framework, Dropout is a special case of
Shakeout when Shakeout hyper-parameter c is set to zero.
Empirically we ﬁnd that the optimal τ for Shakeout is not
higher than that for Dropout. After determining the optimal
τ , keeping the order of magnitude of hyper parameter c
N (N represents the number of training
the same as
samples) is an effective choice. If you want to obtain a model
with much sparser weights but meanwhile with superior
or comparable generalization performance to Dropout, a
relatively lower τ and larger c for Shakeout always works.
Shakeout combined with Batch Normalization Batch Nor-
malization [65] is the widely-adopted technique to promote
the optimization of the training process for a deep neural
network. In practice, combining Shakeout with Batch Nor-
malization to train a deep architecture is a good choice.
For example, we observe that the training of WRN-16-4
model on CIFAR-10 is slow to converge without using Batch
Normalization in the training. Moreover, the generalization
performance on the test set for Shakeout combined with
Batch Normalization always outperforms that for standard
BP with Batch Normalization consistently for quite a large
margin, as illustrated in Tab. 5. These results imply the

(cid:113) 1

12

(a) standard BP

(b) Dropout

(c) Shakeout

Fig. 11. The value of −V (D, G) as a function of iteration for the training process of DCGAN. DCGANs are trained using standard BP, Dropout and
Shakeout for comparison. Dropout or Shakeout is applied on the discriminator of GAN.

Fig. 10. Relative accuracy loss as a function of the weight pruning ratio
for Dropout and Shakeout based on AlexNet architecture on ImageNet-
2012. The relative accuracy loss for Dropout is much severe than that for
Shakeout. The largest margin of the relative accuracy losses between
Dropout and Shakeout is 22.50%, which occurs at the weight pruning
ratio m = 96%.

Fig. 13. Validation error as a function of training epoch for Dropout and
Shakeout on CIFAR-10 with training set size at 40000. The architec-
ture adopted is WRN-16-4. “DPO” and “SKO” represent “Dropout” and
“Shakeout” respectively. The following two numbers denote the hyper-
parameters τ and c respectively. The learning rate decays at epoch 60,
120, and 160. After the ﬁrst decay of learning rate, the validation error in-
creases greatly before the steady decrease (see the enlarged snapshot
for training epochs from 60 to 80). It can be seen that the extent of error
increase is less severe for Shakeout than Dropout. Moreover, Shakeout
recovers much faster than Dropout does. At the ﬁnal stage, both of
the validation errors steadily decrease (see the enlarged snapshot for
training epochs from 160 to 200). Shakeout obtains comparable or even
superior generalization performance to Dropout.

important role of Shakeout in reducing over-ﬁtting of a deep
neural network.

Fig. 12. The minimum and maximum values of −V (D, G) within ﬁxed
length intervals moving from the start to the end of the training by stan-
dard BP, Dropout and Shakeout. The optimal value log(4) is obtained
when the imaginary data distribution P (ˆx) matches with the real data
distribution P (x).

5 CONCLUSION

We have proposed Shakeout, which is a new regularized
training approach for deep neural networks. The regularizer
induced by Shakeout is proved to adaptively combine L0,
L1 and L2 regularization terms. Empirically we ﬁnd that

1) Compared to Dropout, Shakeout can afford much
larger models. Or to say, when the data is scarce, Shakeout
outperforms Dropout with a large margin.

2) Shakeout can obtain much sparser weights than
Dropout with superior or comparable generalization per-
formance of the model. While for Dropout, if one wants
to obtain the same level of sparsity as that obtained by
Shakeout, the model may bear a signiﬁcant loss of accuracy.
3) Some deep architectures in nature may result in the
instability of the training process, such as GANs, however,
Shakeout can reduce this instability effectively.

In future, we want to put emphasis on the inductive
bias of Shakeout and attempt to apply Shakeout to the
compression task.

ACKNOWLEDGMENTS

This research is supported by Australian Research Coun-
cil Projects (No. FT-130101457, DP-140102164 and LP-
150100671).

REFERENCES

[1] K. He, X. Zhang, S. Ren, and J. Sun, “Identity mappings in deep
residual networks,” in European Conference on Computer Vision.
Springer, 2016, pp. 630–645.

[2] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, “Inception-
v4, inception-resnet and the impact of residual connections on
learning,” in Proceedings of the Thirty-First AAAI Conference on
Artiﬁcial Intelligence, February 4-9, 2017, San Francisco, California,
USA., 2017, pp. 4278–4284.

[3] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Region-based
convolutional networks for accurate object detection and seg-
mentation,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 38, no. 1, pp. 142–158, Jan 2016.

[4] Y. Sun, X. Wang, and X. Tang, “Hybrid deep learning for face
veriﬁcation,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 38, no. 10, pp. 1997–2009, Oct 2016.

[5] Y. Zheng, Y. J. Zhang, and H. Larochelle, “A deep and autore-
gressive approach for topic modeling of multimodal data,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 38,
no. 6, pp. 1056–1069, June 2016.

[6] Y. Wei, W. Xia, M. Lin, J. Huang, B. Ni, J. Dong, Y. Zhao, and
S. Yan, “Hcp: A ﬂexible cnn framework for multi-label image
classiﬁcation,” IEEE transactions on pattern analysis and machine
intelligence, vol. 38, no. 9, pp. 1901–1907, 2016.

[7] X. Jin, C. Xu, J. Feng, Y. Wei, J. Xiong, and S. Yan, “Deep learning
with s-shaped rectiﬁed linear activation units,” arXiv preprint
arXiv:1512.07030, 2015.

[8] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning algo-
rithm for deep belief nets,” Neural computation, vol. 18, no. 7, pp.
1527–1554, 2006.

[9] Y. Bengio, “Learning deep architectures for AI,” Foundations and

trends R(cid:13) in Machine Learning, vol. 2, no. 1, pp. 1–127, 2009.

[10] Y. Bengio, A. Courville, and P. Vincent, “Representation learning:
A review and new perspectives,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1798–1828, Aug
2013.

[11] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, “Extract-
ing and composing robust features with denoising autoencoders,”
in Proceedings of the 25th international conference on Machine learning.
ACM, 2008, pp. 1096–1103.

[12] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol,
“Stacked denoising autoencoders: Learning useful representations
in a deep network with a local denoising criterion,” The Journal of
Machine Learning Research, vol. 11, pp. 3371–3408, 2010.

[13] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R.
Salakhutdinov, “Improving neural networks by preventing co-
adaptation of feature detectors,” arXiv preprint arXiv:1207.0580,
2012.

13

[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁ-
cation with deep convolutional neural networks,” in Advances in
neural information processing systems, 2012, pp. 1097–1105.

[15] S. Zagoruyko and N. Komodakis, “Wide residual networks,” in
Proceedings of the British Machine Vision Conference 2016, BMVC
2016, York, UK, September 19-22, 2016, 2016.

[16] S. Wager, S. Wang, and P. S. Liang, “Dropout training as adap-
tive regularization,” in Advances in Neural Information Processing
Systems, 2013, pp. 351–359.

[17] N. Chen, J. Zhu, J. Chen, and B. Zhang, “Dropout training for
support vector machines,” in Twenty-Eighth AAAI Conference on
Artiﬁcial Intelligence, 2014.

[18] L. Van Der Maaten, M. Chen, S. Tyree, and K. Q. Weinberger,
“Learning with marginalized corrupted features.” in ICML (1),
2013, pp. 410–418.

[19] Y. LeCun, J. S. Denker, S. A. Solla, R. E. Howard, and L. D. Jackel,
“Optimal brain damage.” in NIPs, vol. 2, 1989, pp. 598–605.
[20] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen,
“Compressing neural networks with the hashing trick,” in Proceed-
ings of the 32nd International Conference on Machine Learning, ICML
2015, Lille, France, 6-11 July 2015, 2015, pp. 2285–2294.

[21] S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights and
connections for efﬁcient neural network,” in Advances in Neural
Information Processing Systems, 2015, pp. 1135–1143.

[22] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing
deep neural network with pruning, trained quantization and
huffman coding,” CoRR, abs/1510.00149, vol. 2, 2015.

[23] M. Denil, B. Shakibi, L. Dinh, N. de Freitas et al., “Predicting
parameters in deep learning,” in Advances in Neural Information
Processing Systems, 2013, pp. 2148–2156.

[24] J. Ba and R. Caruana, “Do deep nets really need to be deep?” in
Advances in neural information processing systems, 2014, pp. 2654–
2662.

[25] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a

neural network,” arXiv preprint arXiv:1503.02531, 2015.

[26] H. Zou and T. Hastie, “Regularization and variable selection via
the elastic net,” Journal of the Royal Statistical Society: Series B
(Statistical Methodology), vol. 67, no. 2, pp. 301–320, 2005.

[27] G. Kang, J. Li, and D. Tao, “Shakeout: A new regularized deep
neural network training scheme,” in Thirtieth AAAI Conference on
Artiﬁcial Intelligence, 2016.

[28] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent,
and S. Bengio, “Why does unsupervised pre-training help deep
learning?” The Journal of Machine Learning Research, vol. 11, pp.
625–660, 2010.

[29] J. Moody, S. Hanson, A. Krogh, and J. A. Hertz, “A simple weight
decay can improve generalization,” Advances in neural information
processing systems, vol. 4, pp. 950–957, 1995.

[30] L. Prechelt, “Automatic early stopping using cross validation:
quantifying the criteria,” Neural Networks, vol. 11, no. 4, pp. 761–
767, 1998.

[31] L. Wan, M. Zeiler, S. Zhang, Y. L. Cun, and R. Fergus, “Regulariza-
tion of neural networks using dropconnect,” in Proceedings of the
30th International Conference on Machine Learning (ICML-13), 2013,
pp. 1058–1066.

[32] J. Ba and B. Frey, “Adaptive dropout for training deep neural
networks,” in Advances in Neural Information Processing Systems,
2013, pp. 3084–3092.

[33] Z. Li, B. Gong, and T. Yang, “Improved dropout for shallow
and deep learning,” in Advances In Neural Information Processing
Systems, 2016, pp. 2523–2531.

[34] N. Srivastava, G. Hinton, A. Krizhevsky,

I. Sutskever, and
R. Salakhutdinov, “Dropout: A simple way to prevent neural net-
works from overﬁtting,” The Journal of Machine Learning Research,
vol. 15, no. 1, pp. 1929–1958, 2014.

[35] D. Warde-Farley, I. J. Goodfellow, A. Courville, and Y. Bengio, “An
empirical analysis of dropout in piecewise linear networks,” arXiv
preprint arXiv:1312.6197, 2013.

[36] P. Baldi and P. J. Sadowski, “Understanding dropout,” in Advances
in Neural Information Processing Systems, 2013, pp. 2814–2822.
[37] Y. Gal and Z. Ghahramani, “Dropout as a bayesian approximation:
Representing model uncertainty in deep learning,” in Proceedings
of the 33nd International Conference on Machine Learning, ICML 2016,
New York City, NY, USA, June 19-24, 2016, 2016, pp. 1050–1059.
[38] D. P. Helmbold and P. M. Long, “On the inductive bias of
dropout,” Journal of Machine Learning Research, vol. 16, pp. 3403–
3454, 2015.

14

[63] J. Snoek, H. Larochelle, and R. P. Adams, “Practical bayesian op-
timization of machine learning algorithms,” in Advances in neural
information processing systems, 2012, pp. 2951–2959.

[64] D. Maclaurin, D. Duvenaud, and R. P. Adams, “Gradient-based
hyperparameter optimization through reversible learning,” in Pro-
ceedings of the 32nd International Conference on Machine Learning,
2015.

[65] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep
network training by reducing internal covariate shift,” in Proceed-
ings of the 32nd International Conference on Machine Learning, ICML
2015, Lille, France, 6-11 July 2015, 2015, pp. 448–456.

[39] B. A. Olshausen and D. J. Field, “Sparse coding with an overcom-
plete basis set: A strategy employed by v1?” Vision research, vol. 37,
no. 23, pp. 3311–3325, 1997.

[40] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based
learning applied to document recognition,” Proceedings of the IEEE,
vol. 86, no. 11, pp. 2278–2324, 1998.

[41] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with
convolutions,” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2015, pp. 1–9.

[42] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna,
“Rethinking the inception architecture for computer vision,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2016, pp. 2818–2826.

[43] Y.-l. Boureau, Y. L. Cun et al., “Sparse feature learning for deep be-
lief networks,” in Advances in neural information processing systems,
2008, pp. 1185–1192.

[44] I. J. Goodfellow, A. Courville, and Y. Bengio, “Spike-and-slab
sparse coding for unsupervised feature discovery,” arXiv preprint
arXiv:1201.3382, 2012.

[45] M. Thom and G. Palm, “Sparse activity and sparse connectivity in
supervised learning,” Journal of Machine Learning Research, vol. 14,
no. Apr, pp. 1091–1143, 2013.

[46] S. Rifai, X. Glorot, Y. Bengio, and P. Vincent, “Adding noise to
the input of a model trained with a regularized objective,” arXiv
preprint arXiv:1104.3250, 2011.

[47] C. M. Bishop, “Training with noise is equivalent to tikhonov
regularization,” Neural computation, vol. 7, no. 1, pp. 108–116, 1995.
[48] W. Jiang, F. Nie, and H. Huang, “Robust dictionary learning with
capped l1-norm,” in Proceedings of the Twenty-Fourth International
Joint Conference on Artiﬁcial Intelligence, IJCAI 2015, Buenos Aires,
Argentina, July 25-31, 2015, 2015, pp. 3590–3596.

[49] A. Krizhevsky and G. Hinton, “Learning multiple layers of fea-
tures from tiny images. Technical report, University of Toronto,”
2009.

[50] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”
International Journal of Computer Vision (IJCV), vol. 115, no. 3, pp.
211–252, 2015.

[51] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture
for fast feature embedding,” in Proceedings of the ACM International
Conference on Multimedia. ACM, 2014, pp. 675–678.

[52] D. R. G. H. R. Williams and G. Hinton, “Learning representations

by back-propagating errors,” Nature, pp. 323–533, 1986.

[53] R. Tibshirani, “Regression shrinkage and selection via the lasso,”
Journal of the Royal Statistical Society. Series B (Methodological), pp.
267–288, 1996.

[54] L. Yuan, J. Liu, and J. Ye, “Efﬁcient methods for overlapping
group lasso,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 35, no. 9, pp. 2104–2116, 2013.

[55] A. Krizhevsky,

“cuda-convnet,”

2012.

[Online]. Available:

https://code.google.com/p/cuda-convnet/

[56] I. Guyon and A. Elisseeff, “An introduction to variable and feature
selection,” Journal of machine learning research, vol. 3, no. Mar, pp.
1157–1182, 2003.

[57] K. Wang, R. He, L. Wang, W. Wang, and T. Tan, “Joint feature
selection and subspace learning for cross-modal retrieval,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 38,
no. 10, pp. 2010–2023, Oct 2016.

[58] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial
nets,” in Advances in neural information processing systems, 2014, pp.
2672–2680.

[59] A. Radford, L. Metz, and S. Chintala, “Unsupervised represen-
tation learning with deep convolutional generative adversarial
networks,” arXiv preprint arXiv:1511.06434, 2015.

[60] M. Arjovsky and L. Bottou, “Towards principled methods for
training generative adversarial networks,” in NIPS 2016 Workshop
on Adversarial Training. In review for ICLR, vol. 2016, 2017.

[61] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” arXiv

preprint arXiv:1701.07875, 2017.

[62] J. Bergstra and Y. Bengio, “Random search for hyper-parameter
optimization,” Journal of Machine Learning Research, vol. 13, no. Feb,
pp. 281–305, 2012.

9
1
0
2
 
r
p
A
 
3
1
 
 
]

V
C
.
s
c
[
 
 
1
v
3
9
5
6
0
.
4
0
9
1
:
v
i
X
r
a

1

Shakeout: A New Approach to Regularized
Deep Neural Network Training

Guoliang Kang, Jun Li, and Dacheng Tao, Fellow, IEEE

Abstract—Recent years have witnessed the success of deep neural networks in dealing with a plenty of practical problems. Dropout
has played an essential role in many successful deep neural networks, by inducing regularization in the model training. In this paper,
we present a new regularized training approach: Shakeout. Instead of randomly discarding units as Dropout does at the training stage,
Shakeout randomly chooses to enhance or reverse each unit’s contribution to the next layer. This minor modiﬁcation of Dropout has the
statistical trait: the regularizer induced by Shakeout adaptively combines L0, L1 and L2 regularization terms. Our classiﬁcation
experiments with representative deep architectures on image datasets MNIST, CIFAR-10 and ImageNet show that Shakeout deals with
over-ﬁtting effectively and outperforms Dropout. We empirically demonstrate that Shakeout leads to sparser weights under both
unsupervised and supervised settings. Shakeout also leads to the grouping effect of the input units in a layer. Considering the weights
in reﬂecting the importance of connections, Shakeout is superior to Dropout, which is valuable for the deep model compression.
Moreover, we demonstrate that Shakeout can effectively reduce the instability of the training process of the deep architecture.

Index Terms—Shakeout, Dropout, Regularization, Sparsity, Deep Neural Network.

(cid:70)

1 INTRODUCTION

D EEP neural networks have recently achieved impres-

sive success in a number of machine learning and
pattern recognition tasks and been under intensive research
[1], [2], [3], [4], [5], [6], [7]. Hierarchical neural networks
have been known for decades, and there are a number of
essential factors contributing to its recent rising, such as
the availability of big data and powerful computational re-
sources. However, arguably the most important contributor
to the success of deep neural network is the discovery of
efﬁcient training approaches [8], [9], [10], [11], [12].

A particular interesting advance in the training tech-
niques is the invention of Dropout [13]. At the operational
level, Dropout adjusts the network evaluation step (feed-
forward) at the training stage, where a portion of units
are randomly discarded. The effect of this simple trick is
impressive. Dropout enhances the generalization perfor-
mance of neural networks considerably, and is behind many
record-holders of widely recognized benchmarks [2], [14],
[15]. The success has attracted much research attention, and
found applications in a wider range of problems [16], [17],
[18]. Theoretical research from the viewpoint of statistical
learning has pointed out the connections between Dropout
and model regularization, which is the de facto recipe of

• Guoliang Kang, Jun Li are with Center of AI, Faculty of Engineering
and Information Technology, University of Technology Sydney, Ultimo,
NSW, Australia. D. Tao is with the UBTech Sydney Artiﬁcial Intelligence
Institute and the School of Information Technologies in the Faculty of
Engineering and Information Technologies at The University of Sydney,
Darlington, NSW 2008, Australia.
E-mail: Guoliang.Kang@student.uts.edu.au,
dacheng.tao@sydney.edu.au
c(cid:13)2018 IEEE. Personal use of this material is permitted. Permission
from IEEE must be obtained for all other uses, in any current or future
media, including reprinting/republishing this material for advertising
or promotional purposes, creating new collective works, for resale or
redistribution to servers or lists, or reuse of any copyrighted component of
this work in other works.

junjy007@googlemail.com,

reducing over-ﬁtting for complex models in practical ma-
chine learning. For example, Wager et al. [16] showed that
for a generalized linear model (GLM), Dropout implicitly
imposes an adaptive L2 regularizer of the network weights
through an estimation of the inverse diagonal Fisher infor-
mation matrix.

importance in deep learning. It
Sparsity is of vital
is straightforward that
through removing unimportant
weights, deep neural networks perform prediction faster.
Additionally, it is expected to obtain better generalization
performance and reduce the number of examples needed in
the training stage [19]. Recently much evidence has shown
that the accuracy of a trained deep neural network will not
be severely affected by removing a majority of connections
and many researchers focus on the deep model compression
task [20], [21], [22], [23], [24], [25]. One effective way of com-
pression is to train a neural network, prune the connections
and ﬁne-tune the weights iteratively [21], [22]. However, if
we can cut the connections naturally via imposing sparsity-
inducing penalties in the training process of a deep neural
network, the work-ﬂow will be greatly simpliﬁed.

In this paper, we propose a new regularized deep neu-
ral network training approach: Shakeout, which is easy to
implement: randomly choosing to enhance or reverse each
unit’s contribution to the next layer in the training stage.
Note that Dropout can be considered as a special “ﬂat” case
of our approach: randomly keeping (enhance factor is 1) or
discarding (reverse factor is 0) each unit’s contribution to
the next layer. Shakeout enriches the regularization effect.
In theory, we prove that it adaptively combines L0, L1 and
L2 regularization terms. L0 and L1 regularization terms
are known as sparsity-inducing penalties. The combination
of sparsity-inducing penalty and L2 penalty of the model
parameters has shown to be effective in statistical learning:
the Elastic Net [26] has the desirable properties of producing
sparse models while maintaining the grouping effect of the

weights of the model. Because of the randomly “shaking”
process and the regularization characteristic pushing net-
work weights to zero, our new approach is named “Shake-
out”.

As discussed above, it is expected to obtain much sparser
weights using Shakeout than using Dropout because of the
combination of L0 and L1 regularization terms induced
in the training stage. We apply Shakeout on one-hidden-
layer autoencoder and obtain much sparser weights than
that resulted by Dropout. To show the regularization effect
on the classiﬁcation tasks, we conduct the experiments on
image datasets including MNIST, CIFAR-10 and ImageNet
with the representative deep neural network architectures.
In our experiments we ﬁnd that by using Shakeout, the
trained deep neural networks always outperform those by
using Dropout, especially when the data is scarce. Besides
the fact that Shakeout leads to much sparser weights, we
also empirically ﬁnd that it groups the input units of a
layer. Due to the induced L0 and L1 regularization terms,
Shakeout can result in the weights reﬂecting the importance
of the connections between units, which is meaningful for
conducting compression. Moreover, we demonstrate that
Shakeout can effectively reduce the instability of the training
process of the deep architecture.

This journal paper extends our previous work [27] theo-
retically and experimentally. The main extensions are listed
as follows: 1) we derive the analytical formula for the
regularizer induced by Shakeout in the context of GLM and
prove several important properties; 2) we conduct experi-
ments using Wide Residual Network [15] on CIFAR-10 to
show Shakeout outperforms Dropout and standard back-
propagation in promoting the generalization performance
of a much deeper architecture; 3) we conduct experiments
using AlexNet [14] on ImageNet dataset with Shakeout
and Dropout. Shakeout obtains comparable classiﬁcation
performance to Dropout, but with superior regularization
effect; 4) we illustrate that Shakeout can effectively reduce
the instability of the training process of the deep architec-
ture. Moreover, we provide a much clearer and detailed de-
scription of Shakeout, derive the forward-backward update
rule for deep convolutional neural networks with Shakeout,
and give several recommendations to help the practitioners
make full use of Shakeout.

In the rest of the paper, we give a review about the
related work in Section 2. Section 3 presents Shakeout in de-
tail, along with theoretical analysis of the regularization ef-
fect induced by Shakeout. In Section 4, we ﬁrst demonstrate
the regularization effect of Shakeout on the autoencoder
model. The classiﬁcation experiments on MNIST , CIFAR-10
and ImageNet illustrate that Shakeout outperforms Dropout
considering the generalization performance, the regulariza-
tion effect on the weights, and the stabilization effect on the
training process of the deep architecture. Finally, we give
some recommendations for the practitioners to make full
use of Shakeout.

2 RELATED WORK

Deep neural networks have shown their success in a wide
variety of applications. One of the key factors contributes to
this success is the creation of powerful training techniques.

2

The representative power of the network becomes stronger
as the architecture gets deeper [9]. However, millions of pa-
rameters make deep neural networks easily over-ﬁt. Regu-
larization [16], [28] is an effective way to obtain a model that
generalizes well. There exist many approaches to regularize
the training of deep neural networks, like weight decay [29],
early stopping [30], etc. Shakeout belongs to the family of
regularized training techniques.

Among these regularization techniques, our work is
closely related to Dropout [13]. Many subsequent works
were devised to improve the performance of Dropout [31],
[32], [33]. The underlying reason why Dropout improves
performance has also attracted the interest of many re-
searchers. Evidence has shown that Dropout may work
because of its good approximation to model averaging
and regularization on the network weights [34], [35], [36].
Srivastava [34] and Warde-Farley [35] exhibited through
experiments that the weight scaling approximation is an
accurate alternative for the geometric mean over all possible
sub-networks. Gal et al. [37] claimed that training the deep
neural network with Dropout is equivalent to performing
variational inference in a deep Gaussian Process. Dropout
can also be regarded as a way of adding noise into the
neural network. By marginalizing the noise, Srivastava [34]
proved for linear regression that the deterministic version
of Dropout is equivalent to adding an adaptive L2 regular-
ization on the weights. Furthermore, Wager [16] extended
the conclusion to generalized linear models (GLMs) using
a quadratic approximation to the induced regularizer. The
inductive bias of Dropout was studied by Helmbold et al.
[38] to illustrate the properties of the regularizer induced
by Dropout further. In terms of implicitly inducing regu-
larizer of the network weights, Shakeout can be viewed as
a generalization of Dropout. It enriches the regularization
effect of Dropout, i.e. besides the L2 regularization term, it
also induces the L0 and L1 regularization terms, which may
lead to sparse weights of the model.

Due to the implicitly induced L0 and L1 regulariza-
tion terms, Shakeout is also related to sparsity-inducing
approaches. Olshausen et al. [39] introduced the concept
of sparsity in computational neuroscience and proposed
the sparse coding method in the visual system. In machine
learning, the sparsity constraint enables a model to capture
the implicit statistical data structure, performs feature selec-
tion and regularization, compresses the data at a low loss of
the accuracy, and helps us to better understand our models
and explain the obtained results. Sparsity is one of the key
factors underlying many successful deep neural network
architectures [2], [40], [41], [42] and training algorithms [43]
[44]. A Convolutional neural network is much sparser than
the fully-connected one, which results from the concept of
local receptive ﬁeld [40]. Sparsity has been a design princi-
ple and motivation for Inception-series models [2], [41], [42].
Besides working as the heuristic principle of designing a
deep architecture, sparsity often works as a penalty induced
to regularize the training process of a deep neural network.
There exist two kinds of sparsity penalties in deep neural
networks, which lead to the activity sparsity [43] [44] and
the connectivity sparsity [45] respectively. The difference
between Shakeout and these sparsity-inducing approaches
is that for Shakeout, the sparsity is induced through sim-

ple stochastic operations rather than manually designed
architectures or explicit norm-based penalties. This implicit
way enables Shakeout to be easily optimized by stochastic
gradient descent (SGD) − the representative approach for
the optimization of a deep neural network.

3 SHAKEOUT

Shakeout applies on the weights in a linear module. The
linear module, i.e. weighted sum,

θ =

wjxj

p
(cid:88)

j=1

(1)

is arguably the most widely adopted component in data
models. For example, the variables x1, x2, . . . , xp can be
input attributes of a model, e.g. the extracted features for
a GLM, or the intermediate outputs of earlier processing
steps, e.g. the activations of the hidden units in a multilayer
artiﬁcial neural network. Shakeout randomly modiﬁes the
computation in Eq. (1). Speciﬁcally, Shakeout can be realized
by randomly modifying the weights

Step 1: Draw rj, where

(cid:40)

P (rj = 0)
P (rj = 1

= τ
1−τ ) = 1 − τ

.

Step 2: Adjust the weight according to rj,

(cid:40)

˜wj ← −csj,
˜wj ← (wj + cτ sj)/(1 − τ )

if rj = 0
otherwise

(A)
(B)

where sj = sgn(wj) takes ±1 depending on the sign of wj
or takes 0 if wj = 0. As shown above, Shakeout chooses
(randomly by drawing r) between two fundamentally dif-
ferent ways to modify the weights. Modiﬁcation (A) is to set
the weights to constant magnitudes, despite their original
values except for the signs (to be opposite to the original
ones). Modiﬁcation (B) updates the weights by a factor
(1 − τ )−1 and a bias depending on the signs. Note both (A)
and (B) preserve zero values of the weights, i.e. if wj = 0
then ˜wj = 0 with probability 1. Let ˜θ = ˜wT x, and Shakeout
leaves θ unbiased, i.e. E[˜θ] = θ. The hyper-parameters
τ ∈ (0, 1) and c ∈ (0, +∞) conﬁgure the property of
Shakeout.

Shakeout is naturally connected to the widely adopted
operation of Dropout [13], [34]. We will show that Shakeout
has regularization effect on model training similar to but
beyond what is induced by Dropout. From an operational
point of view, Fig. 1 compares Shakeout and Dropout. Note
that Shakeout includes Dropout as a special case when the
hyper-parameter c in Shakeout is set to zero.

When applied at the training stage, Shakeout alters the
objective − the quantity to be minimized − by adjusting the
weights. In particular, we will show that Shakeout (with ex-
pectation over the random switch) induces a regularization
term effectively penalizing the magnitudes of the weights
and leading to sparse weights. Shakeout is an approach
designed for helping model training, when the models are
trained and deployed, one should relieve the disturbance to
allow the model work with its full capacity, i.e. we adopt the
resulting network without any modiﬁcation of the weights
at the test stage.

3

Fig. 1. Comparison between Shakeout and Dropout operations. This
ﬁgure shows how Shakeout and Dropout are applied to the weights in a
linear module. In the original linear module, the output is the summation
of the inputs x weighted by w, while for Dropout and Shakeout, the
weights w are ﬁrst randomly modiﬁed. In detail, a random switch ˆr
controls how each w is modiﬁed. The manipulation of w is illustrated
within the ampliﬁer icons (the red curves, best seen with colors). The
coefﬁcients are α = 1/(1 − τ ) and β(w) = cs(w), where s(w) extracts
the sign of w and c > 0, τ ∈ [0, 1]. Note the sign of β(w) is always
the same as that of w. The magnitudes of coefﬁcients α and β(w) are
determined by the Shakeout hyper-parameters τ and c. Dropout can be
viewed as a special case of Shakeout when c = 0 because β(w) is zero
at this circumstance.

3.1 Regularization Effect of Shakeout

Shakeout randomly modiﬁes the weights in a linear module,
and thus can be regarded as injecting noise into each vari-
able xj, i.e. xj is randomly scaled by γj: ˜xj = γjxj. Note
that γj = rj + c(rj −1)
, the modiﬁcation of xj is actually
|wj |
determined by the random switch rj. Shakeout randomly
chooses to enhance (i.e. when rj = 1
1−τ ) or
reverse (i.e. when rj = 0, γj < 0) each original variable
xj’s contribution to the output at the training stage (see Fig.
1). However, the expectation of ˜xj over the noise remains
unbiased, i.e. Erj [˜xj] = xj.

1−τ , γj > 1

It is well-known that injecting artiﬁcial noise into the
input features will regularize the training objective [16],
[46], [47], i.e. Er[(cid:96)(w, ˜x, y)] = (cid:96)(w, x, y) + π(w), where
˜x is the feature vector randomly modiﬁed by the noise
induced by r. The regularization term π(w) is determined
by the characteristic of the noise. For example, Wager et
al. [16] showed that Dropout, corresponding to inducing
blackout noise to the features, helps introduce an adaptive
L2 penalty on w. In this section we illustrate how Shakeout
helps regularize model parameters w using an example of
GLMs.

Formally, a GLM is a probabilistic model of predicting
target y given features x = [x1, . . . , xp], in terms of the
weighted sum in Eq. (1):

P (y|x, w) = h(y)g(θ)eθy
θ = wT x

(2)

With different h(·) and g(·) functions, GLM can be special-
ized to various useful models or modules, such as logistic
regression model or a layer in a feed-forward neural net-
work. However, roughly speaking, the essence of a GLM
is similar to that of a standard linear model which aims

to ﬁnd weights w1, . . . , wp so that θ = wT x aligns with
y (functions h(·) and g(·) are independent of w and y
respectively). The loss function of a GLM with respect to
w is deﬁned as

We illustrate several properties of Shakeout regularizer
based on Eq. (7). The proof of the following propositions
can be found in the appendices.
Proposition 1. π(0) = 0

4

l(w, x, y) = −θy + A(θ)

A(θ) = − ln[g(θ)]

(3)

(4)

The loss (3) is the negative logarithm of probability (2),
where we keep only terms relevant to w.

Let the loss with Shakeout be

lsko(w, x, y, r) := l(w, ˜x, y)
where r = [r1, . . . , rp]T , and ˜x = [˜x1, . . . , ˜xp]T represents
the features randomly modiﬁed with r.

(5)

Taking expectation over r, the loss with Shakeout be-

Er[lsko(w, x, y, r)] = l(w, x, y) + π(w)

comes

where

π(w) = Er[A(˜θ) − A(θ)]
∞
(cid:88)

=

1
k!

k=1

A(k)(θ)E[(˜θ − θ)k]

(6)

is named Shakeout regularizer. Note that if A(θ) is k-th order
)(θ) = 0 where
derivable, let the k
k
Theorem 1. Let qj = xj(wj + csj), θj− = θ − qj and θj+ =

> k, to make the denotation simple.

order derivative A(k

(cid:48)

(cid:48)

(cid:48)

θ + τ

1−τ qj, then Shakeout regularizer π(w) is

π(w) = τ

A(θj−) + (1 − τ )

A(θj+) − pA(θ) (7)

p
(cid:88)

j=1

p
(cid:88)

j=1

Proof: Note that ˜θ − θ = (cid:80)p

j=1 qj(rj − 1), then for Eq.

(6)

E[(˜θ − θ)k] =

p
(cid:88)

p
(cid:88)

p
(cid:88)

k
(cid:89)

· · ·

k
(cid:89)

qjm

E[

(rjm − 1)]

j1=1

j2=1

jk=1

m=1

m=1

Because arbitrary two random variables rjm1 , rjm2 are inde-
pendent unless jm1 = jm2 and ∀rjm , E[rjm − 1] = 0, then

p
(cid:88)

j=1
p
(cid:88)

j=1

E[(˜θ − θ)k] =

qk
j

E[(rj − 1)k]

= τ

(−qj)k + (1 − τ )

p
(cid:88)

(
j=1

τ
1 − τ

qj)k

Then

π(w) = τ

A(k)(θ)(−qj)k

p
(cid:88)

∞
(cid:88)

j=1

k=1

1
k!

+(1 − τ )

p
(cid:88)

∞
(cid:88)

j=1

k=1

1
k!

A(k)(θ)(

τ
1 − τ

qj)k

Further, let θj− = θ − qj, θj+ = θ + τ

π(w) = τ

A(θj−) + (1 − τ )

p
(cid:88)

j=1

1−τ qj, π(w) becomes
p
(cid:88)

A(θj+) − pA(θ)

j=1

The theorem is proved.

Proposition 2. If A(θ) is convex, π(w) ≥ 0.

Proposition 3. Suppose ∃j, xjwj (cid:54)= 0. If A(θ) is convex,
(θ) > 0, π(w)

π(w) monotonically increases with τ . If A
monotonically increases with c.

(cid:48)(cid:48)

Proposition 3 implies that the hyper-parameters τ and c
relate to the strength of the regularization effect. It is rea-
sonable because higher τ or c means the noise injected into
the features x has larger variance.
Proposition 4. Suppose i) ∀j (cid:54)= j

, xjwj = 0, and ii) xj(cid:48) (cid:54)= 0.

(cid:48)

Then

(cid:48)(cid:48)

i) if A

(θ) > 0,

ii) if lim|θ|→∞ A





(cid:48)(cid:48)

> 0, when wj(cid:48) > 0
< 0, when wj(cid:48) < 0

∂π(w)
∂wj(cid:48)
∂π(w)
∂wj(cid:48)
(θ) = 0, lim|wj(cid:48) |→∞

∂π(w)
∂wj(cid:48)

= 0

Proposition 4 implies that under certain conditions, start-
ing from a zero weight vector, Shakeout regularizer pe-
nalizes the magnitude of wj(cid:48) and its regularization effect
is bounded by a constant value. For example, for logistic
regression, π(w) ≤ τ ln(1+exp(c|xj(cid:48) |)), which is illustrated
in Fig. 2. This bounded property has been proved to be
useful: capped-norm [48] is more robust to outliers than the
traditional L1 or L2 norm.

Based on the Eq. (7), the speciﬁc formulas for the repre-

sentative GLM models can be derived:
i) Linear regression: A(θ) = 1

π(w) =

τ
2(1 − τ )

2 θ2, then
(cid:107)x ◦ (w + cs)(cid:107)2
2

where ◦ denotes
(cid:107)x ◦ (w + cs)(cid:107)2
tion of three components

the element-wise product and the
2 term can be decomposed into the summa-

p
(cid:88)

j=1

p
(cid:88)

j=1

p
(cid:88)

j=1

j w2
x2

j + 2c

j |wj| + c2
x2

x2
j 1wj (cid:54)=0[wj]

(8)

where 1wj (cid:54)=0[wj] is an indicator function which satisﬁes

(cid:40)

1 wj (cid:54)= 0
0 wj = 0

1wj (cid:54)=0[wj] =

. This decomposition implies that

Shakeout regularizer penalizes the combination of L0-norm,
L1-norm and L2-norm of the weights after scaling them
with the square of corresponding features. The L0 and L1
regularization terms can lead to sparse weights.

ii) Logistic regression: A(θ) = ln(1 + exp(θ)), then

π(w) =

ln(

p
(cid:88)

j=1

(1 + exp(θj−))τ (1 + exp(θj+))1−τ
1 + exp(θ)

)

(9)

Fig. 3 illustrates the contour of Shakeout regularizer based
on Eq. (9) in the 2D weight space. On the whole, the con-
tour of Shakeout regularizer indicates that the regularizer
combines L0, L1 and L2 regularization terms. As c goes to
zero, the contour around w = 0 becomes less sharper, which
implies hyper-parameter c relates to the strength of L0 and
L1 components. When c = 0, Shakeout degenerates to

5

(a) Shakeout regularizer: τ = 0.3, c = 0.78

(b) Dropout regularizer: τ = 0.5

Fig. 2. Regularization effect as a function of a single weight when other weights are ﬁxed to zeros for logistic regression model. The corresponding
feature x is ﬁxed at 1.

Dropout, the contour of which implies Dropout regularizer
consists of L2 regularization term.

imposed and leads to much sparser weights of the model.
We will verify this property in our experiment section later.

The difference between Shakeout and Dropout regular-
izers is also illustrated in Fig. 2. We set τ = 0.3, c = 0.78
for Shakeout, and τ = 0.5 for Dropout to make the bounds
of the regularization effects of two regularizers the same. In
this one dimension circumstance, the main difference is that
at w = 0 (see the enlarged snapshot for comparison), Shake-
out regularizer is sharp and discontinuous while Dropout
regularizer is smooth. Thus compared to Dropout, Shakeout
may lead to much sparser weights of the model.

To simplify the analysis and prove the intuition we have
observed in Fig. 3 about the properties of Shakeout regular-
izer, we quadratically approximate Shakeout regularizer of
Eq. (7) by

πapprox(w) =

τ
2(1 − τ )

(cid:48)(cid:48)

A

(θ) (cid:107)x ◦ (w + cs)(cid:107)2
2

(10)

The (cid:107)x ◦ (w + cs)(cid:107)2
2, already shown in Eq. (8), consists
of the combination of L0, L1, L2 regularization terms. It
tends to penalize the weight whose corresponding feature’s
magnitude is large. Meanwhile, the weights whose corre-
sponding features are always zeros are less penalized. The
(θ) is proportional to the variance of prediction y
term A
given x and w. Penalizing A
(θ) encourages the weights to
move towards making the model be more ”conﬁdent” about
its predication, i.e. be more discriminative.

(cid:48)(cid:48)

(cid:48)(cid:48)

Generally speaking, Shakeout regularizer adaptively
combines L0, L1 and L2 regularization terms, the property
of which matches what we have observed in Fig. 3. It
prefers penalizing the weights who have large magnitudes
and encourages the weights to move towards making the
model more discriminative. Moreover, the weights whose
corresponding features are always zeros are less penalized.
The L0 and L1 components can induce sparse weights.

Last but not the least, we want to emphasize that when
τ = 0, the noise is eliminated and the model becomes a
standard GLM. Moreover, Dropout can be viewed as the
special case of Shakeout when c = 0, and a higher value of
τ means a stronger L2 regularization effect imposed on the
weights. Generally, when τ is ﬁxed (τ (cid:54)= 0), a higher value
of c means a stronger effect of the L0 and L1 components

3.2 Shakeout in Multilayer Neural Networks

It has been illustrated that Shakeout regularizes the weights
in linear modules. Linear module is the basic component
of multilayer neural networks. That is, the linear operations
connect the outputs of two successive layers. Thus Shakeout
is readily applicable to the training of multilayer neural
networks.

Considering the forward computation from layer l to
layer l + 1, for a fully-connected layer, the Shakeout forward
computation is as follows

ui =

xj[rjWij + c(rj − 1)Sij] + bi

(11)

(cid:88)

j

(cid:48)
i = f (ui)
x

(12)

where i denotes the index of the output unit of layer l + 1,
and j denotes the index of the output unit of layer l. The
output unit of a layer is represented by x. The weight of the
connection between unit xj and unit x
i is represented as
Wij. The bias for the i-th unit is denoted by bi. The Sij
is the sign of corresponding weight Wij. After Shakeout
operation, the linear combination ui is sent to the activation
function f (·) to obtain the corresponding output x
i. Note
that the weights Wij that connect to the same input unit xj
are controlled by the same random variable rj.

(cid:48)

(cid:48)

During back-propagation, we should compute the gra-
dients with respect to each unit to propagate the error. In
Shakeout, ∂ui
∂xj

takes the form

= rj(Wij + cSij) − cSij

(13)

And the weights are updated following

= xj(rj + c(rj − 1)

(14)

dSij
dWij

)

dSij
dWij

represents the derivative of a sgn function.
where
Because the sgn function is not continuous at zero and

∂ui
∂xj

∂ui
∂Wij

6

Fig. 3. The contour plots of the regularization effect induced by Shakeout in 2D weight space with input x = [1, 1]T . Note that Dropout is a special
case of Shakeout with c = 0.

thus the derivative is not deﬁned, we approximate this
. Empirically we ﬁnd that this
derivative with
approximation works well.

d tanh(Wij )
dWij

Note that

the forward-backward computations with
Shakeout can be easily extended to the convolutional layer.
For a convolutional layer, the Shakeout feed-forward pro-
cess can be formalized as

Ui =

(Xj ◦ Rj) ∗ Wij + c(Xj ◦ (Rj − 1)) ∗ Sij + bi (15)

(cid:88)

j

(cid:48)

X

(16)

i = f (Ui)
where Xj represents the j-th feature map. Rj is the j-th
random mask which has the same spatial structure (i.e. the
same height and width) as the corresponding feature map
Xj. Wij denotes the kernel connecting Xj and Ui. And Sij
is set as sgn(Wij). The symbol * denotes the convolution
operation. And the symbol ◦ means element-wise product.
Correspondingly, during the back-propagation process,
the gradient with respect to a unit of the layer on which
Shakeout is applied takes the form

∂Ui(a, b)
∂Xj(a − a(cid:48), b − b(cid:48))

= Rj(a − a

, b − b

)(Wij(a

(cid:48)

(cid:48)

(cid:48)

(cid:48)

, b

)+

(cid:48)

(cid:48)

cSij(a

, b

)) − cSij(a

(cid:48)

(cid:48)

, b

)

(17)

where (a, b) means the position of a unit in the output
) represents the position
feature map of a layer, and (a
of a weight in the corresponding kernel.

, b

(cid:48)

(cid:48)

The weights are updated following

∂Ui(a, b)
∂Wij(a(cid:48), b(cid:48))

= Xj(a − a

(cid:48)

(cid:48)
, b − b

)(Rj(a − a

, b − b

)

(cid:48)

(cid:48)

+ c(Rj(a − a

(cid:48)

(cid:48)
, b − b

) − 1)

(cid:48)

(cid:48)
, b

)
dSij(a
dWij(a(cid:48), b(cid:48))

)

(18)

4 EXPERIMENTS
In this section, we report empirical evaluations of Shake-
out in training deep neural networks on representative
datasets. The experiments are performed on three kinds of
image datasets: the hand-written image dataset MNIST [40],
the CIFAR-10 image dataset [49] and the ImageNet-2012
dataset [50]. MNIST consists of 60,000+10,000 (training+test)
28×28 images of hand-written digits. CIFAR-10 contains
50,000+10,000 (training+test) 32×32 images of 10 object
classes. ImageNet-2012 consists of 1,281,167+50,000+150,000
(training+validation+test) variable-resolution images of
1000 object classes. We ﬁrst demonstrate that Shakeout leads
to sparse models as our theoretical analysis implies under
the unsupervised setting. Then we show that for the classi-
ﬁcation task, the sparse models have desirable generaliza-
tion performances. Further, we illustrate the regularization
effect of Shakeout on the weights in the classiﬁcation task.
Moreover, the effect of Shakeout on stabilizing the training
processes of the deep architectures is demonstrated. Finally,
we give some practical recommendations of Shakeout. All

the experiments are implemented based on the modiﬁca-
tions of Caffe library [51]. Our code is released on the github:
https://github.com/kgl-prml/shakeout-for-caffe.

4.1 Shakeout and Weight Sparsity

implicitly imposes L0 penalty and L1
Since Shakeout
penalty of the weights, we expect the weights of neural
networks learned by Shakeout contain more zeros than
those learned by the standard back-propagation (BP) [52] or
Dropout [13]. In this experiment, we employ an autoencoder
model for the MNIST hand-written data, train the model
using standard BP, Dropout and Shakeout, respectively, and
compare the degree of sparsity of the weights of the learned
encoders. For the purpose of demonstration, we employ
the simple autoencoder with one hidden layer of 256 units;
Dropout and Shakeout are applied on the input pixels.

To verify the regularization effect, we compare the
weights of the four autoencoders trained under different
settings which correspond to standard BP, Dropout (τ = 0.5)
and Shakeout (τ = 0.5, c = {1, 10}). All the training
methods aim to produce hidden units which can capture
good visual features of the handwritten digits. The statistical
traits of these different resulting weights are shown in Fig. 4.
Moreover, Fig. 5 shows the features captured by each hidden
unit of the autoencoders.

As shown in the Fig. 4, the probability density of weights
around the zero obtained by standard BP training is quite
small compared to the one obtained either by Dropout or
Shakeout. This indicates the strong regularization effect in-
duced by Dropout and Shakeout. Furthermore, the sparsity
level of weights obtained from training by Shakeout is much
higher than the one obtained from training by Dropout.
Using the same τ , increasing c makes the weights much
sparser, which is consistent with the characteristics of L0
penalty and L1 penalty induced by Shakeout. Intuitively,
we can ﬁnd that due to the induced L2 regularization,
the distribution of weights obtained from training by the
Dropout is like a Gaussian, while the one obtained from
training by Shakeout is more like a Laplacian because of
the additionally induced L1 regularization. Fig. 5 shows
that features captured by the hidden units via standard
BP training are not directly interpretable, corresponding to
insigniﬁcant variants in the training data. Both Dropout and
Shakeout suppress irrelevant weights by their regularization
effects, where Shakeout produces much sparser and more
global features thanks to the combination of L0, L1 and L2
regularization terms.

The autoencoder trained by Dropout or Shakeout can
be viewed as the denosing autoencoder, where Dropout
or Shakeout injects special kind of noise into the inputs.
Under this unsupervised setting, the denoising criterion
(i.e. minimizing the error between imaginary images recon-
structed from the noisy inputs and the real images without
noise) is to guide the learning of useful high level feature
representations [11], [12]. To verify that Shakeout helps
learn better feature representations, we adopt the hidden
layer activations as features to train SVM classiﬁers, and the
classiﬁcation accuracies on test set for standard BP, Dropout
and Shakeout are 95.34%, 96.41% and 96.48%, respectively.
We can see that Shakeout leads to much sparser weights
without defeating the main objective.

7

Fig. 4. Distributions of the weights of the autoencoder models learned
by different training approaches. Each curve in the ﬁgure shows the
frequencies of the weights of an autoencoder taking particular values,
i.e. the empirical population densities of the weights. The ﬁve curves
correspond to ﬁve autoencoders learned by standard back-propagation,
Dropout (τ = 0.5), Gaussian Dropout (σ2 = 1) and Shakeout (τ = 0.5,
c = {1, 10}). The sparsity of the weights obtained via Shakeout can be
seen by comparing the curves.

Gaussian Dropout has similar effect on the model train-
ing as standard Dropout [34], which multiplies the acti-
vation of each unit by a Gaussian variable with mean 1
and variance σ2. The relationship between σ2 and τ is
that σ2 = τ
1−τ . The distribution of the weights trained by
Gaussian Dropout (σ2 = 1, i.e. τ = 0.5) is illustrated in
Fig. 4. From Fig. 4, we ﬁnd no notable statistical difference
between two kinds of Dropout implementations which all
exhibit a kind of L2 regularization effect on the weights.
The classiﬁcation performances of SVM classiﬁers on test set
based on the hidden layer activations as extracted features
for both kinds of Dropout implementations are quite similar
(i.e. 96.41% and 96.43% for standard and Gaussian Dropout
respectively). Due to these observations, we conduct the fol-
lowing classiﬁcation experiments using standard Dropout
as a representative implementation (of Dropout) for com-
parison.

4.2 Classiﬁcation Experiments

Sparse models often indicate lower complexity and better
generalization performance [26], [39], [53], [54]. To verify
the effect of L0 and L1 regularization terms induced by
Shakeout on the model performance, we apply Shakeout,
along with Dropout and standard BP, on training represen-
tative deep neural networks for classiﬁcation tasks. In all of
our classiﬁcation experiments, the hyper-parameters τ and
c in Shakeout, and the hyper-parameter τ in Dropout are
determined by validation.

4.2.1 MNIST

We train two different neural networks, a shallow fully-
connected one and a deep convolutional one. For the
fully-connected neural network, a big hidden layer size is
adopted with its value at 4096. The non-linear activation

8

(a) standard BP

(b) Dropout: τ = 0.5

(c) Shakeout: τ = 0.5, c = 0.5

Fig. 5. Features captured by the hidden units of the autoencoder models learned by different training methods. The features captured by a hidden
unit are represented by a group of weights that connect the image pixels with this corresponding hidden unit. One image patch in a sub-graph
corresponds to the features captured by one hidden unit.

TABLE 1
The architecture of convolutional neural network adopted for MNIST
classiﬁcation experiment

TABLE 2
Classiﬁcation on MNIST using training sets of different sizes:
fully-connected neural network

Layer
Type
Channels
Filter size
Conv. stride
Pooling type
Pooling size
Pooling stride
Non-linear

1
conv.
20
5 × 5
1
max
2 × 2
2

2
conv.
50
5 × 5
1
max
2 × 2
2

3
FC
500
-
-
-
-
-

4
FC
10
-
-
-
-
-

ReLU ReLU ReLU Softmax

unit adopted is the rectiﬁer linear unit (ReLU). The deep
convolutional neural network employed is based on the
modiﬁcations of the LeNet [40], which contains two con-
volutional layers and two fully-connected layers. The de-
tailed architecture information of this convolutional neural
network is described in Tab. 1. We separate 10,000 training
samples from original training dataset for validation. The
results are shown in Tab. 2 and Tab. 3. Dropout and Shake-
out are applied on the hidden units of the fully-connected
layer. The table compares the errors of the networks trained
by standard back-propagation, Dropout and Shakeout. The
mean and standard deviation of the classiﬁcation errors
are obtained by 5 runs of the experiment and are shown
in percentage. We can see from the results that when the
training data is not sufﬁcient enough, due to over-ﬁtting, all
the models perform worse. However, the models trained by
Dropout and Shakeout consistently perform better than the
one trained by standard BP. Moreover, when the training
data is scarce, Shakeout leads to superior model perfor-
mance compared to the Dropout. Fig. 6 shows the results
in a more intuitive way.

4.2.2 CIFAR-10

We use the simple convolutional network feature extractor
described in cuda-convnet (layers-80sec.cfg) [55]. We apply
Dropout and Shakeout on the ﬁrst fully-connected layer. We
call this architecture “AlexFastNet” for the convenience of
description. In this experiment, 10,000 colour images are

Size
500
1000
3000
8000
20000
50000

std-BP
13.66±0.66
8.49±0.23
5.54±0.09
3.57±0.14
2.28±0.09
1.55±0.03

Dropout
11.76±0.09
8.05±0.05
4.87±0.06
2.95±0.05
1.82±0.07
1.36±0.03

Shakeout
10.81±0.32
7.19±0.15
4.60±0.07
2.96±0.09
1.92±0.06
1.35±0.07

TABLE 3
Classiﬁcation on MNIST using training sets of different sizes:
convolutional neural network

Size
500
1000
3000
8000
20000
50000

std-BP
9.76±0.26
6.73±0.12
2.93±0.10
1.70±0.03
0.97±0.01
0.78±0.05

Dropout
6.16±0.23
4.01±0.16
2.06±0.06
1.23±0.13
0.83±0.06
0.62±0.04

Shakeout
4.83±0.11
3.43±0.06
1.86±0.13
1.31±0.06
0.77±0.001
0.58±0.10

TABLE 4
Classiﬁcation on CIFAR-10 using training sets of different sizes:
AlexFastNet

Size
300
700
2000
5500
15000
40000

std-BP
68.26±0.57
59.78±0.24
50.73±0.29
41.41±0.52
32.53±0.25
24.48±0.23

Dropout
65.34±0.75
56.04±0.22
46.24±0.49
36.01±0.13
27.28±0.26
20.50±0.32

Shakeout
63.71±0.28
54.66±0.22
44.39±0.41
34.54±0.31
26.53±0.17
20.56±0.12

separated from the training dataset for validation and no
data augmentation is utilized. The per-pixel mean computed
over the training set is subtracted from each image. We ﬁrst
train for 100 epochs with an initial learning rate of 0.001
and then another 50 epochs with the learning rate of 0.0001.
The mean and standard deviation of the classiﬁcation errors
are obtained by 5 runs of the experiment and are shown in
percentage. As shown in Tab. 4, the performances of models
trained by Dropout and Shakeout are consistently superior

9

(a) Fully-connected neural network

(b) Convolutional neural network

Fig. 6. Classiﬁcation of two kinds of neural networks on MNIST using training sets of different sizes. The curves show the performances of the
models trained by standard BP, and those by Dropout and Shakeout applied on the hidden units of the fully-connected layer.

TABLE 5
Classiﬁcation on CIFAR-10 using training sets of different sizes:
WRN-16-4

Size
15000
40000
50000

std-BP Dropout
20.95
15.37
14.39

15.05
9.32
8.03

Shakeout
14.68
9.01
7.97

training set size at 40000. From Tab. 5, we can arrive at
the same conclusion as previous experiments, i.e. the per-
formances of the models trained by Dropout and Shakeout
are consistently superior to the one trained by standard BP.
Moreover, Shakeout outperforms Dropout when the data is
scarce.

4.2.3 Regularization Effect on the Weights

Shakeout is a different way to regularize the training process
of deep neural networks from Dropout. For a GLM model,
we have proved that the regularizer induced by Shakeout
adaptively combines L0, L1 and L1 regularization terms.
In section 4.1, we have demonstrated that for a one-hidden
layer autoencoder, it leads to much sparser weights of the
model. In this section, we will illustrate the regularization
effect of Shakeout on the weights in the classiﬁcation task
and make a comparison to that of Dropout.

The results shown in this section are mainly based on the
experiments conducted on ImageNet-2012 dataset using the
representative deep architecture: AlexNet [14]. For AlexNet,
we apply Dropout or Shakeout on layers FC7 and FC8
which are the last two fully-connected layers. We train the
model from the scratch and obtain the comparable classi-
ﬁcation performances on validation set for Shakeout (top-1
error: 42.88%; top-5 error: 19.85%) and Dropout (top-1 error:
42.99%; top-5 error: 19.60%). The model is trained based on
the same hyper-parameter settings provided by Shelhamer
in Caffe [51] other than the hyper-parameters τ and c for
Shakeout. The initial weights for training by Dropout and
Shakeout are kept the same.

Fig. 8 illustrates the distributions of the magnitude of
weight resulted by Shakeout and Dropout. It can be seen
that the weights learned by Shakeout are much sparser than

Fig. 7. Classiﬁcation on CIFAR-10 using training sets of different sizes.
The curves show the performances of the models trained by standard
BP, and those by Dropout and Shakeout applied on the hidden units of
the fully-connected layer.

to the one trained by standard BP. Furthermore, the model
trained by Shakeout also outperforms the one trained by
Dropout when the training data is scarce. Fig. 7 shows the
results in a more intuitive way.

To test the performance of Shakeout on a much deeper
architecture, we also conduct experiments based on the
Wide Residual Network (WRN) [15]. The conﬁguration of
WRN adopted is WRN-16-4, which means WRN has 16
layers in total and the number of feature maps for the
convolutional layer of each residual block is 4 times as the
corresponding original one [1]. Because the complexity is
much higher than that of “AlexFastNet”, the experiments
are performed on relatively larger training sets with sizes
of 15000, 40000, 50000. Dropout and Shakeout are applied
on the second convolutional layer of each residual block,
following the protocol in [15]. All the training starts from
the same initial weights. Batch Normalization is applied the
same way as [15] to promote the optimization. No data-
augmentation or data pre-processing is adopted. All the
other hyper-parameters other than τ and c are set the same
as [15]. The results are listed in Tab. 5. For the training
of CIFAR-10 with 50000 training samples, we adopt the
same hyper-parameters as those chosen in the training with

10

(a) AlexNet FC7 layer

(b) AlexNet FC8 layer

Fig. 8. Comparison of the distributions of the magnitude of weights trained by Dropout and Shakeout. The experiments are conducted using AlexNet
on ImageNet-2012 dataset. Shakeout or Dropout is applied on the last two fully-connected layers, i.e. FC7 layer and FC8 layer.

(a) AlexNet FC7 layer

(b) AlexNet FC8 layer

Fig. 9. Distributions of the maximum magnitude of the weights connected to the same input unit of a layer. The maximum magnitude of the weights
connected to one input unit can be regarded as a metric of the importance of that unit. The experiments are conducted using AlexNet on ImageNet-
2012 dataset. For Shakeout, the units can be approximately separated into two groups and the one around zero is less important than the other,
whereas for Dropout, the units are more concentrated.

those learned by Dropout, due to the implicitly induced L0
and L1 components.

The regularizer induced by Shakeout not only contains
L0 and L1 regularization terms but also contains L2 reg-
ularization term, the combination of which is expected to
discard a group of weights simultaneously. In Fig. 9, we use
the maximum magnitude of the weights connected to one
input unit of a layer to represent the importance of that unit
for the subsequent output units. From Fig. 9, it can be seen
that for Shakeout, the units can be approximately separated
into two groups according to the maximum magnitudes
of the connected weights and the group around zero can
be discarded, whereas for Dropout, the units are concen-
trated. This implies that compared to Dropout which may
encourage a “distributed code” for the features captured by
the units of a layer, Shakeout tends to discard the useless
features (or units) and award the important ones. This
experiment result veriﬁes the regularization properties of
Shakeout and Dropout further.

As known to us, L0 and L1 regularization terms are
related to performing feature selection [56], [57]. For a deep
architecture, it is expected to obtain a set of weights using
Shakeout suitable for reﬂecting the importance of connec-
tions between units. We perform the following experiment
to verify this effect. After a model is trained, for the layer
on which Dropout or Shakeout is applied, we sort the
magnitudes of the weights increasingly. Then we prune the
ﬁrst m% of the sorted weights and evaluate the performance
of the pruned model again. The pruning ratio m goes from 0
to 1. We calculate the relative accuracy loss (we write R.A.L
for simpliﬁcation) at each pruning ratio m

as

(cid:48)

R.A.L(m

) =

(cid:48)

Accu.(m = 0) − Accu.(m
Accu.(m = 0)

(cid:48)

)

Fig. 10 shows the R.A.L curves for Dropout and Shake-
out based on the AlexNet model on ImageNet-2012 dataset.
The models trained by Dropout and Shakeout are under the
optimal hyper-parameter settings. Apparently, the relative

accuracy loss for Dropout is more severe than that for Shake-
out. For example, the largest margin of the relative accuracy
losses between Dropout and Shakeout is 22.50%, which
occurs at the weight pruning ratio m = 96%. This result
proves that considering the trained weights in reﬂecting the
importance of connections, Shakeout is much better than
Dropout, which beneﬁts from the implicitly induced L0 and
L1 regularization effect. This kind of property is useful for
the popular compression task in deep learning area which
aims to cut the connections or throw units of a deep neural
network to a maximum extent without obvious loss of ac-
curacy. The above experiments illustrate that Shakeout can
play a considerable role in selecting important connections,
which is meaningful for promoting the performance of a
compression task. This is a potential subject for the future
research.

4.3 Stabilization Effect on the Training Process

In both research and production, it is always desirable to
have a level of certainty about how a model’s ﬁtness to
the data improves over optimization iterations, namely, to
have a stable training process. In this section, we show that
Shakeout helps reduce ﬂuctuation in the improvement of
model ﬁtness during training.

The ﬁrst experiment is on the family of Generative
Adversarial Networks (GANs) [58], which is known to be
instable in the training stage [59], [60], [61]. The purpose of
the following tests is to demonstrate the Shakeout’s capa-
bility of stabilizing the training process of neural networks
in a general sense. GAN plays a min-max game between
the generator G and the discriminator D over the expected
log-likelihood of real data x and imaginary data ˆx = G(z)
where z represents the random input

min
G

max
D

V (D, G) = E[log[D(x)] + log[1 − D(G(z))]] (19)

The architecture that we adopt is DCGAN [59]. The numbers
of feature maps of the deconvolutional layers in the gener-
ator are 1024, 64 and 1 respectively, with the corresponding
spatial sizes 7×7, 14×14 and 28×28. We train DCGANs on
MNIST dataset using standard BP, Dropout and Shakeout.
We follow the same experiment protocol described in [59]
except for adopting Dropout or Shakeout on all layers of the
discriminator. The values of −V (D, G) during training are
illustrated in Fig. 11. It can be seen that −V (D, G) during
training by standard BP oscillates greatly, while for Dropout
and Shakeout, the training processes are much steadier.
Compared with Dropout, the training process by Shakeout
has fewer spikes and is smoother. Fig. 12 demonstrates the
minimum and maximum values of −V (D, G) within ﬁxed
length intervals moving from the start to the end of the
training by standard BP, Dropout and Shakeout. It can be
seen that the gaps between the minimum and maximum
values of −V (D, G) trained by Dropout and Shakeout are
much smaller than that trained by standard BP, while that
by Shakeout is the smallest, which implies the stability of
the training process by Shakeout is the best.

The second experiment is based on Wide Residual Net-
work architecture to perform the classiﬁcation task. In the
classiﬁcation task, generalization performance is the main
focus and thus, we compare the validation errors during

11

the training processes by Dropout and Shakeout. Fig. 13
demonstrates the validation error as a function of the
training epoch for Dropout and Shakeout on CIFAR-10
with 40000 training examples. The architecture adopted is
WRN-16-4. The experiment settings are the same as those
described in Section 4.2.2. Considering the generalization
performance, the learning rate schedule adopted is the one
optimized through validation to make the models obtain the
best generalization performances. Under this schedule, we
ﬁnd that the validation error temporarily increases when
lowering the learning rate at the early stage of training,
which has been repeatedly observed by [15]. Nevertheless,
it can be seen from Fig. 13 that the extent of error increase is
less severe for Shakeout than Dropout. Moreover, Shakeout
recovers much faster than Dropout does. At the ﬁnal stage,
both of the validation errors steadily decrease. Shakeout
obtains comparable or even superior generalization perfor-
mance to Dropout. In a word, Shakeout signiﬁcantly stabi-
lizes the entire training process with superior generalization
performance.

4.4 Practical Recommendations

Selection of Hyper-parameters The most practical and pop-
ular way to perform hyper-parameter selection is to parti-
tion the training data into a training set and a validation
set to evaluate the classiﬁcation performance of different
hyper-parameters on it. Due to the expensive cost of time
for training a deep neural network, cross-validation is barely
adopted. There exist many hyper-parameter selection meth-
ods in the domain of deep learning, such as the grid search,
random search [62], Bayesian optimization methods [63],
gradient-based hyper-parameter Optimization [64], etc. For
applying Shakeout on a deep neural network, we need to
decide two hyper-parameters τ and c. From the regular-
ization perspective, we need to decide the most suitable
strength of regularization effect to obtain an optimal trade-
off between model bias and variance. We have pointed out
that in a uniﬁed framework, Dropout is a special case of
Shakeout when Shakeout hyper-parameter c is set to zero.
Empirically we ﬁnd that the optimal τ for Shakeout is not
higher than that for Dropout. After determining the optimal
τ , keeping the order of magnitude of hyper parameter c
N (N represents the number of training
the same as
samples) is an effective choice. If you want to obtain a model
with much sparser weights but meanwhile with superior
or comparable generalization performance to Dropout, a
relatively lower τ and larger c for Shakeout always works.
Shakeout combined with Batch Normalization Batch Nor-
malization [65] is the widely-adopted technique to promote
the optimization of the training process for a deep neural
network. In practice, combining Shakeout with Batch Nor-
malization to train a deep architecture is a good choice.
For example, we observe that the training of WRN-16-4
model on CIFAR-10 is slow to converge without using Batch
Normalization in the training. Moreover, the generalization
performance on the test set for Shakeout combined with
Batch Normalization always outperforms that for standard
BP with Batch Normalization consistently for quite a large
margin, as illustrated in Tab. 5. These results imply the

(cid:113) 1

12

(a) standard BP

(b) Dropout

(c) Shakeout

Fig. 11. The value of −V (D, G) as a function of iteration for the training process of DCGAN. DCGANs are trained using standard BP, Dropout and
Shakeout for comparison. Dropout or Shakeout is applied on the discriminator of GAN.

Fig. 10. Relative accuracy loss as a function of the weight pruning ratio
for Dropout and Shakeout based on AlexNet architecture on ImageNet-
2012. The relative accuracy loss for Dropout is much severe than that for
Shakeout. The largest margin of the relative accuracy losses between
Dropout and Shakeout is 22.50%, which occurs at the weight pruning
ratio m = 96%.

Fig. 13. Validation error as a function of training epoch for Dropout and
Shakeout on CIFAR-10 with training set size at 40000. The architec-
ture adopted is WRN-16-4. “DPO” and “SKO” represent “Dropout” and
“Shakeout” respectively. The following two numbers denote the hyper-
parameters τ and c respectively. The learning rate decays at epoch 60,
120, and 160. After the ﬁrst decay of learning rate, the validation error in-
creases greatly before the steady decrease (see the enlarged snapshot
for training epochs from 60 to 80). It can be seen that the extent of error
increase is less severe for Shakeout than Dropout. Moreover, Shakeout
recovers much faster than Dropout does. At the ﬁnal stage, both of
the validation errors steadily decrease (see the enlarged snapshot for
training epochs from 160 to 200). Shakeout obtains comparable or even
superior generalization performance to Dropout.

important role of Shakeout in reducing over-ﬁtting of a deep
neural network.

Fig. 12. The minimum and maximum values of −V (D, G) within ﬁxed
length intervals moving from the start to the end of the training by stan-
dard BP, Dropout and Shakeout. The optimal value log(4) is obtained
when the imaginary data distribution P (ˆx) matches with the real data
distribution P (x).

5 CONCLUSION

We have proposed Shakeout, which is a new regularized
training approach for deep neural networks. The regularizer
induced by Shakeout is proved to adaptively combine L0,
L1 and L2 regularization terms. Empirically we ﬁnd that

1) Compared to Dropout, Shakeout can afford much
larger models. Or to say, when the data is scarce, Shakeout
outperforms Dropout with a large margin.

2) Shakeout can obtain much sparser weights than
Dropout with superior or comparable generalization per-
formance of the model. While for Dropout, if one wants
to obtain the same level of sparsity as that obtained by
Shakeout, the model may bear a signiﬁcant loss of accuracy.
3) Some deep architectures in nature may result in the
instability of the training process, such as GANs, however,
Shakeout can reduce this instability effectively.

In future, we want to put emphasis on the inductive
bias of Shakeout and attempt to apply Shakeout to the
compression task.

ACKNOWLEDGMENTS

This research is supported by Australian Research Coun-
cil Projects (No. FT-130101457, DP-140102164 and LP-
150100671).

REFERENCES

[1] K. He, X. Zhang, S. Ren, and J. Sun, “Identity mappings in deep
residual networks,” in European Conference on Computer Vision.
Springer, 2016, pp. 630–645.

[2] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, “Inception-
v4, inception-resnet and the impact of residual connections on
learning,” in Proceedings of the Thirty-First AAAI Conference on
Artiﬁcial Intelligence, February 4-9, 2017, San Francisco, California,
USA., 2017, pp. 4278–4284.

[3] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Region-based
convolutional networks for accurate object detection and seg-
mentation,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 38, no. 1, pp. 142–158, Jan 2016.

[4] Y. Sun, X. Wang, and X. Tang, “Hybrid deep learning for face
veriﬁcation,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 38, no. 10, pp. 1997–2009, Oct 2016.

[5] Y. Zheng, Y. J. Zhang, and H. Larochelle, “A deep and autore-
gressive approach for topic modeling of multimodal data,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 38,
no. 6, pp. 1056–1069, June 2016.

[6] Y. Wei, W. Xia, M. Lin, J. Huang, B. Ni, J. Dong, Y. Zhao, and
S. Yan, “Hcp: A ﬂexible cnn framework for multi-label image
classiﬁcation,” IEEE transactions on pattern analysis and machine
intelligence, vol. 38, no. 9, pp. 1901–1907, 2016.

[7] X. Jin, C. Xu, J. Feng, Y. Wei, J. Xiong, and S. Yan, “Deep learning
with s-shaped rectiﬁed linear activation units,” arXiv preprint
arXiv:1512.07030, 2015.

[8] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning algo-
rithm for deep belief nets,” Neural computation, vol. 18, no. 7, pp.
1527–1554, 2006.

[9] Y. Bengio, “Learning deep architectures for AI,” Foundations and

trends R(cid:13) in Machine Learning, vol. 2, no. 1, pp. 1–127, 2009.

[10] Y. Bengio, A. Courville, and P. Vincent, “Representation learning:
A review and new perspectives,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1798–1828, Aug
2013.

[11] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, “Extract-
ing and composing robust features with denoising autoencoders,”
in Proceedings of the 25th international conference on Machine learning.
ACM, 2008, pp. 1096–1103.

[12] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol,
“Stacked denoising autoencoders: Learning useful representations
in a deep network with a local denoising criterion,” The Journal of
Machine Learning Research, vol. 11, pp. 3371–3408, 2010.

[13] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R.
Salakhutdinov, “Improving neural networks by preventing co-
adaptation of feature detectors,” arXiv preprint arXiv:1207.0580,
2012.

13

[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁ-
cation with deep convolutional neural networks,” in Advances in
neural information processing systems, 2012, pp. 1097–1105.

[15] S. Zagoruyko and N. Komodakis, “Wide residual networks,” in
Proceedings of the British Machine Vision Conference 2016, BMVC
2016, York, UK, September 19-22, 2016, 2016.

[16] S. Wager, S. Wang, and P. S. Liang, “Dropout training as adap-
tive regularization,” in Advances in Neural Information Processing
Systems, 2013, pp. 351–359.

[17] N. Chen, J. Zhu, J. Chen, and B. Zhang, “Dropout training for
support vector machines,” in Twenty-Eighth AAAI Conference on
Artiﬁcial Intelligence, 2014.

[18] L. Van Der Maaten, M. Chen, S. Tyree, and K. Q. Weinberger,
“Learning with marginalized corrupted features.” in ICML (1),
2013, pp. 410–418.

[19] Y. LeCun, J. S. Denker, S. A. Solla, R. E. Howard, and L. D. Jackel,
“Optimal brain damage.” in NIPs, vol. 2, 1989, pp. 598–605.
[20] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen,
“Compressing neural networks with the hashing trick,” in Proceed-
ings of the 32nd International Conference on Machine Learning, ICML
2015, Lille, France, 6-11 July 2015, 2015, pp. 2285–2294.

[21] S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights and
connections for efﬁcient neural network,” in Advances in Neural
Information Processing Systems, 2015, pp. 1135–1143.

[22] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing
deep neural network with pruning, trained quantization and
huffman coding,” CoRR, abs/1510.00149, vol. 2, 2015.

[23] M. Denil, B. Shakibi, L. Dinh, N. de Freitas et al., “Predicting
parameters in deep learning,” in Advances in Neural Information
Processing Systems, 2013, pp. 2148–2156.

[24] J. Ba and R. Caruana, “Do deep nets really need to be deep?” in
Advances in neural information processing systems, 2014, pp. 2654–
2662.

[25] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a

neural network,” arXiv preprint arXiv:1503.02531, 2015.

[26] H. Zou and T. Hastie, “Regularization and variable selection via
the elastic net,” Journal of the Royal Statistical Society: Series B
(Statistical Methodology), vol. 67, no. 2, pp. 301–320, 2005.

[27] G. Kang, J. Li, and D. Tao, “Shakeout: A new regularized deep
neural network training scheme,” in Thirtieth AAAI Conference on
Artiﬁcial Intelligence, 2016.

[28] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent,
and S. Bengio, “Why does unsupervised pre-training help deep
learning?” The Journal of Machine Learning Research, vol. 11, pp.
625–660, 2010.

[29] J. Moody, S. Hanson, A. Krogh, and J. A. Hertz, “A simple weight
decay can improve generalization,” Advances in neural information
processing systems, vol. 4, pp. 950–957, 1995.

[30] L. Prechelt, “Automatic early stopping using cross validation:
quantifying the criteria,” Neural Networks, vol. 11, no. 4, pp. 761–
767, 1998.

[31] L. Wan, M. Zeiler, S. Zhang, Y. L. Cun, and R. Fergus, “Regulariza-
tion of neural networks using dropconnect,” in Proceedings of the
30th International Conference on Machine Learning (ICML-13), 2013,
pp. 1058–1066.

[32] J. Ba and B. Frey, “Adaptive dropout for training deep neural
networks,” in Advances in Neural Information Processing Systems,
2013, pp. 3084–3092.

[33] Z. Li, B. Gong, and T. Yang, “Improved dropout for shallow
and deep learning,” in Advances In Neural Information Processing
Systems, 2016, pp. 2523–2531.

[34] N. Srivastava, G. Hinton, A. Krizhevsky,

I. Sutskever, and
R. Salakhutdinov, “Dropout: A simple way to prevent neural net-
works from overﬁtting,” The Journal of Machine Learning Research,
vol. 15, no. 1, pp. 1929–1958, 2014.

[35] D. Warde-Farley, I. J. Goodfellow, A. Courville, and Y. Bengio, “An
empirical analysis of dropout in piecewise linear networks,” arXiv
preprint arXiv:1312.6197, 2013.

[36] P. Baldi and P. J. Sadowski, “Understanding dropout,” in Advances
in Neural Information Processing Systems, 2013, pp. 2814–2822.
[37] Y. Gal and Z. Ghahramani, “Dropout as a bayesian approximation:
Representing model uncertainty in deep learning,” in Proceedings
of the 33nd International Conference on Machine Learning, ICML 2016,
New York City, NY, USA, June 19-24, 2016, 2016, pp. 1050–1059.
[38] D. P. Helmbold and P. M. Long, “On the inductive bias of
dropout,” Journal of Machine Learning Research, vol. 16, pp. 3403–
3454, 2015.

14

[63] J. Snoek, H. Larochelle, and R. P. Adams, “Practical bayesian op-
timization of machine learning algorithms,” in Advances in neural
information processing systems, 2012, pp. 2951–2959.

[64] D. Maclaurin, D. Duvenaud, and R. P. Adams, “Gradient-based
hyperparameter optimization through reversible learning,” in Pro-
ceedings of the 32nd International Conference on Machine Learning,
2015.

[65] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep
network training by reducing internal covariate shift,” in Proceed-
ings of the 32nd International Conference on Machine Learning, ICML
2015, Lille, France, 6-11 July 2015, 2015, pp. 448–456.

[39] B. A. Olshausen and D. J. Field, “Sparse coding with an overcom-
plete basis set: A strategy employed by v1?” Vision research, vol. 37,
no. 23, pp. 3311–3325, 1997.

[40] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based
learning applied to document recognition,” Proceedings of the IEEE,
vol. 86, no. 11, pp. 2278–2324, 1998.

[41] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with
convolutions,” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2015, pp. 1–9.

[42] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna,
“Rethinking the inception architecture for computer vision,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2016, pp. 2818–2826.

[43] Y.-l. Boureau, Y. L. Cun et al., “Sparse feature learning for deep be-
lief networks,” in Advances in neural information processing systems,
2008, pp. 1185–1192.

[44] I. J. Goodfellow, A. Courville, and Y. Bengio, “Spike-and-slab
sparse coding for unsupervised feature discovery,” arXiv preprint
arXiv:1201.3382, 2012.

[45] M. Thom and G. Palm, “Sparse activity and sparse connectivity in
supervised learning,” Journal of Machine Learning Research, vol. 14,
no. Apr, pp. 1091–1143, 2013.

[46] S. Rifai, X. Glorot, Y. Bengio, and P. Vincent, “Adding noise to
the input of a model trained with a regularized objective,” arXiv
preprint arXiv:1104.3250, 2011.

[47] C. M. Bishop, “Training with noise is equivalent to tikhonov
regularization,” Neural computation, vol. 7, no. 1, pp. 108–116, 1995.
[48] W. Jiang, F. Nie, and H. Huang, “Robust dictionary learning with
capped l1-norm,” in Proceedings of the Twenty-Fourth International
Joint Conference on Artiﬁcial Intelligence, IJCAI 2015, Buenos Aires,
Argentina, July 25-31, 2015, 2015, pp. 3590–3596.

[49] A. Krizhevsky and G. Hinton, “Learning multiple layers of fea-
tures from tiny images. Technical report, University of Toronto,”
2009.

[50] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”
International Journal of Computer Vision (IJCV), vol. 115, no. 3, pp.
211–252, 2015.

[51] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture
for fast feature embedding,” in Proceedings of the ACM International
Conference on Multimedia. ACM, 2014, pp. 675–678.

[52] D. R. G. H. R. Williams and G. Hinton, “Learning representations

by back-propagating errors,” Nature, pp. 323–533, 1986.

[53] R. Tibshirani, “Regression shrinkage and selection via the lasso,”
Journal of the Royal Statistical Society. Series B (Methodological), pp.
267–288, 1996.

[54] L. Yuan, J. Liu, and J. Ye, “Efﬁcient methods for overlapping
group lasso,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 35, no. 9, pp. 2104–2116, 2013.

[55] A. Krizhevsky,

“cuda-convnet,”

2012.

[Online]. Available:

https://code.google.com/p/cuda-convnet/

[56] I. Guyon and A. Elisseeff, “An introduction to variable and feature
selection,” Journal of machine learning research, vol. 3, no. Mar, pp.
1157–1182, 2003.

[57] K. Wang, R. He, L. Wang, W. Wang, and T. Tan, “Joint feature
selection and subspace learning for cross-modal retrieval,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 38,
no. 10, pp. 2010–2023, Oct 2016.

[58] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial
nets,” in Advances in neural information processing systems, 2014, pp.
2672–2680.

[59] A. Radford, L. Metz, and S. Chintala, “Unsupervised represen-
tation learning with deep convolutional generative adversarial
networks,” arXiv preprint arXiv:1511.06434, 2015.

[60] M. Arjovsky and L. Bottou, “Towards principled methods for
training generative adversarial networks,” in NIPS 2016 Workshop
on Adversarial Training. In review for ICLR, vol. 2016, 2017.

[61] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” arXiv

preprint arXiv:1701.07875, 2017.

[62] J. Bergstra and Y. Bengio, “Random search for hyper-parameter
optimization,” Journal of Machine Learning Research, vol. 13, no. Feb,
pp. 281–305, 2012.

