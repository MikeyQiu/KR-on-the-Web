Grad-CAM: Why did you say that?

Ramprasaath R. Selvaraju

Ramakrishna Vedantam

Michael Cogswell

Abhishek Das
Devi Parikh

Dhruv Batra

Virginia Tech
{ram21, abhshkdz, vrama91, cogswell, parikh, dbatra}@vt.edu

7
1
0
2
 
n
a
J
 
5
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
0
5
4
7
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

We propose a technique for making Convolutional Neural
Network (CNN)-based models more transparent by visu-
alizing input regions that are ‘important’ for predictions
– producing visual explanations. Our approach, called
Gradient-weighted Class Activation Mapping (Grad-CAM),
uses class-speciﬁc gradient information to localize impor-
tant regions. These localizations are combined with existing
pixel-space visualizations to create a novel high-resolution
and class-discriminative visualization called Guided Grad-
CAM. These methods help better understand CNN-based
models, including image captioning and visual question an-
swering (VQA) models. We evaluate our visual explana-
tions by measuring their ability to discriminate between
classes, to inspire trust in humans, and their correlation
with occlusion maps. Grad-CAM provides a new way to
understand CNN-based models.

We have released code, an online demo hosted on
CloudCV [1], and the full paper [8].1
1. Introduction
Convolutional Neural Networks (CNNs) and other deep net-
works have enabled unprecedented breakthroughs in a vari-
ety of computer vision tasks, from image classiﬁcation to
image captioning and visual question answering. While
these deep neural networks enable superior performance,
their lack of decomposability into intuitive and understand-
able components makes them hard to interpret. Conse-
quently, when today’s intelligent systems fail,
they fail
spectacularly disgracefully, without warning or explanation,
leaving a user staring at incoherent output, wondering why
the system did what it did. In order to build trust in intelle-
gent systems and move towards their meaningful integration
into our everyday lives, it is clear that we must build ‘trans-
parent’ models that explain why they predict what they do.

However, there is a trade-off between accuracy and sim-
plicity/interpretability. Classical rule-based or expert sys-

1 code: https://github.com/ramprs/grad-cam/ demo:
http://gradcam.cloudcv.org paper: https://arxiv.org/
abs/1610.02391

tems were highly interpretable but not very accurate (or ro-
bust). Decomposable pipelines where each stage is hand-
designed are thought to be more interpretable as each indi-
vidual component assumes a natural intuitive explanation.
This tradeoff is realized in more recent work, like Class Ac-
tivation Mapping (CAM) [12], which allows explanations
for a speciﬁc class of image classiﬁcation CNNs. By us-
ing deep models, we sacriﬁce a degree of interpretability in
pipeline modules in order to achieve greater performance
through greater abstraction (more layers) and tighter inte-
gration (end-to-end training).

What makes a good visual explanation? Consider im-
age classiﬁcation – a ‘good’ visual explanation from the
model justifying a predicted class should be (a) class-
discriminative (i.e. localize the category in the image) and
(b) high-resolution (i.e. capture ﬁne-grained detail). To be
concrete we introduce Grad-CAM using the notion ‘class’
from image classiﬁcation (e.g., cat or dog), but visual ex-
planations can be considered for any differentiable node in
a computational graph, including words from a caption or
the answer to a question.

This is illustrated in Fig. 1, where we visualize the ‘tiger cat’
class. Pixel-space gradient visualizations such as Guided
Backpropagation [9], seen at the top of Fig. 1, are high-
resolution and highlight ﬁne-grained details in the image,
but are not class-discriminative. Both the cat and the dog
are highlighted despite ‘tiger cat’ being the class of in-
terest (in fact, the Guided Backpropagation visualizations
of ‘boxer’ dog ‘tiger cat’ are indistinguishable). However,
the Grad-CAM visualization is low-resolution and does not
contain ﬁne details. A high-res visualization like Guided
Grad-CAM helps show these details by highlighting stripes
in the cat in addition to localizing the cat. We combine the
best of both worlds by fusing existing pixel-space gradient
visualizations with our novel localization method – called
Grad-CAM – to create Guided Grad-CAM visualizations,
which are both high-resolution and class-discriminative.

In this abstract: (1) We propose Gradient-weighted Class
Activation Mapping (Grad-CAM) to generate visual expla-
nations from any CNN-based network without requiring ar-

1

to obtain the class-discriminative localization map Grad-
Grad-CAM ∈ Ru×v in generic CNN-based architec-
CAM Lc
tures, we ﬁrst compute the gradient of yc with respect to
feature maps A of a convolutional layer, i.e. ∂yc
. These
∂Ak
ij
gradients are global-average-pooled to obtain weights αc
k:

i

j

αc

(cid:88)

(cid:88)

1
Z

k =

∂yc
∂Ak
ij
This weight αc
k represents a partial linearization of the deep
network downstream from A, and captures the ‘importance’
of feature map k for a target class c. In general, yc need not
be a class score, but could be any differentiable activation.
As in CAM, our Grad-CAM heat-map is a weighted combi-
nation of feature maps, but we follow this by a ReLU:

(2)

Lc

Grad-CAM = ReLU

kAk
αc

(3)

(cid:33)

(cid:32)

(cid:88)

k

This results in a coarse heat-map, which is normalized for
visualization. Other than the ReLU in (3), Grad-CAM is a
generalization of CAM (wc
k where CAM
can be applied) to any CNN-based architecture (CNNs with
fully-connected-layers, ResNets, CNNs stacked with Re-
current Neural Networks (RNNs) etc.).

k are precisely αc

Guided Grad-CAM. In order to combine the class-
discriminative nature of Grad-CAM and the high-resolution
nature of Guided Backpropagation, we fuse them via point-
wise multiplication to create Guided Grad-CAM, shown on
the left of Fig. 1. We expect the last convolutional layers to
have the best compromise between high-level semantics and
detailed spatial information, so we use these feature maps to
compute Grad-CAM and Guided Grad-CAM.
3. Experiments
We evaluate our visualization then show image captioning
and visual question answering examples.

3.1. Evaluating Visualizations

Evaluating Class Discrimination. Intuitively, a good pre-
diction explanation is one that produces discriminative vi-
sualizations for the class of interest. We select images from
PASCAL VOC 2007 val set that contain exactly two an-
notated categories, and create visualizations for one of the
classes. These are shown to workers on Amazon Mechan-
ical Turk (AMT), who are asked “Which of the two object
categories is depicted in the image?” and presented with the
two categories present in the original image as options. As
shown in Table. 1 column 1, human subjects can correctly
identify the category being visualized substantially more of-
ten when using Grad-CAM. This makes Guided Backprop-
agation more class-discriminative.

Evaluating Trust. Given explanations from two different
models, we want to evaluate which of them seems more
trustworthy. We use AlexNet and VGG-16 to compare

Figure 1: Grad-CAM overview: Given an image, and a category (‘tiger cat’) as
input, we foward propagate the image through the model to obtain the raw class
scores before softmax. The gradients are set to zero for all classes except the de-
sired class (tiger cat), which is set to 1. This signal is then backpropagated to the
rectiﬁed convolutional feature map of interest, where we can compute the coarse
Grad-CAM localization (blue heatmap). Finally, we pointwise multiply the heatmap
with guided backpropagation to get Guided Grad-CAM visualizations which are both
high-resolution and class-discriminative.
chitectural changes. (2) To illustrate the broad applicabil-
ity of our technique across tasks, we apply Grad-CAM to
state-of-the-art image captioning and visual question an-
swering models. (3) We design and conduct human stud-
ies to show that Guided Grad-CAM explanations are class-
discriminative and help humans not only establish trust, but
also help untrained users successfully discern a ‘stronger’
deep network from a ‘weaker’ one even when both networks
make identical predictions, simply on the basis of their vi-
sual explanations. (4) Our code and demos of Grad-CAM
are released to help others apply Grad-CAM to interpret
their own models.
2. Approach

We review Class Activation Mapping (CAM) [12], propose
Grad-CAM, and combine Grad-CAM with high-resolution
visualizations to form Guided Grad-CAM. This is summa-
rized by Fig. 1.

Class Activation Mapping (CAM). CAM [12] produces
a localization map from image classiﬁcation CNNs where
global-average-pooled convolutional feature maps are fed
directly into a softmax. Speciﬁcally, let the penultimate
CNN layer produce K feature maps Ak ∈ Ru×v of width u
and height v. These feature maps are then spatially pooled
using Global Average Pooling (GAP) and linearly trans-
formed to produce a score yc for each class c

yc =

(cid:88)

wc
k

1
Z

(cid:88)

(cid:88)

Ak
ij.

(1)

j

k

k wc

i
CAM ∈ Ru×v for class c,
To produce a localization map Lc
CAM computes the linear combination of the ﬁnal feature
maps using the learned weights of the ﬁnal layer: Lc
CAM =
(cid:80)
kAk. This is normalized to lie between 0 and 1 for vi-
sualization purposes. CAM can not be applied to networks
which use multiple fully-connected layers before the output
layer, so fully-connected layers are replaced with convolu-
tional ones and the network is re-trained. In comparison,
our approach can be applied directly to any CNN-based dif-
ferentiable architecture as is without re-training.

Gradient-weighted Class Activation Mapping. In order

2

Guided Backpropagation and Guided Grad-CAM visualiza-
tions, noting that VGG-16 is known to be more reliable than
AlexNet. In order to tease apart the efﬁcacy of the visual-
ization from the accuracy of the model being visualized, we
consider only those instances where both models made the
same prediction as ground truth. Given a visualization from
AlexNet, one from VGG-16, and the name of the object
predicted by both the networks, workers are instructed to
rate which model is more reliable. Results are shown in the
Relative Reliability column of Table. 1, where scores range
from -2 to +2 and positive scores indicate VGG is judged
to be more reliable than AlexNet. With Guided Backprop-
agation, humans score VGG as slightly more reliable than
AlexNet, while with Guided Grad-CAM they score VGG as
clearly more reliable than AlexNet. Thus our Guided Grad-
CAM visualization can help users place trust in a model that
can generalize better, based on individual prediction expla-
nations.

Faithfulness vs. Interpretability. Faithfulness of a vi-
sualization to a model is its ability to accurately explain
the function learned by the model. Naturally, there exists
a tradeoff between the interpretability and faithfulness for
complex models operating on highly compositional inputs.
A more faithful visualization might describe the model in
precise detail yet be completely opaque to human inspec-
tion. Here we are only interested in local ﬁdelity; the visu-
alization need only explain the parts of the model relevant
to that image. That is, in the vicinity of the input data point,
our explanation should be faithful to the model [7].

For comparison, we need a reference explanation with high
local-faithfulness. One obvious choice for such a visual-
ization is image occlusion [11], where we measure the dif-
ference in CNN scores when patches of the input image
Interestingly, patches which change the
are masked out.
CNN score are also patches to which Guided Grad-CAM
assigns high intensity, as shown by measuring rank corre-
lation between patch intensities in Table. 1 (3rd column).
This shows that Guided Grad-CAM is more faithful to the
original model than Guided Backpropagation.

Method

Human Classiﬁca-
tion Accuracy

Relative Re-
liability

Rank Correlation
w/ Occlusion

Guided Backpropagation
Guided Grad-CAM

44.44
61.23

+1.00
+1.27

0.168
0.261

Table 1: Quantitative Visualization Evaluation. Guided Grad-CAM enables hu-
mans to differentiate between visualizations of different classes (Human Classiﬁca-
tion Accuracy) and pick more reliable models (Relative Reliability). It also accurately
reﬂects the behavior of the model (Rank Correlation w/ Occlusion).
3.2. Analyzing Failure Modes for VGG-16
We use Guided Grad-CAM to analyze failure modes of the
VGG-16 CNN on ImageNet classiﬁcation [2].

In order to see what mistakes a network is making we
ﬁrst get a list of examples that the network (VGG-16)
fails to classify correctly. For the misclassiﬁed exam-
ples, we use Guided Grad-CAM to visualize both the cor-

(a)

(b)

(c)

(d)

Figure 2: In these cases the model (VGG-16) failed to predict the correct class as
its top 1 prediction, but it even failed to predict the correct class in its top 5 for ﬁgure
b. All of these errors are due in part to class ambiguity. (a) For example, the network
predicts ‘sandbar’ based on the foreground of (a), but it also knows where the correct
label ‘volcano’ is located. (c-d) In other cases, these errors are still reasonable but
not immediately apparent. For example, humans would ﬁnd it hard to explain the
predicted ‘syringes’ in (c) without looking at the visualization for the predicted class.
rect class and the predicted class. A major advantage of
Guided Grad-CAM over other methods is its ability to
more usefully investigate and explain classiﬁcation mis-
takes, since our visualizations are high-resolution and more
class-discriminative. As seen in Fig. 2, some failures are
due to ambiguities inherent in ImageNet classiﬁcation. We
can also see that seemingly unreasonable predictions have
reasonable explanations, which is a similar observation to
HOGgles [10].
3.3. Image Captioning and VQA

(a) Image captioning explanations

(b) Comparison to DenseCap
Figure 3: Interpreting image captioning models: We use our class-discriminative
localization technique, Grad-CAM to ﬁnd spatial support regions for captions in im-
ages. Fig. 3a Visual explanations from image captioning model [4] highlighting im-
age regions considered to be important for producing the captions. Fig. 3b Grad-
CAM localizations of a global or holistic captioning model for captions generated
by a dense captioning model [3] for the three bounding box proposals marked on the
left. We can see that we get back Grad-CAM localizations (right) that agree with
those bounding boxes – even though the captioning model and Grad-CAM do not use
any bounding box annotations.
Image Captioning. We visualize spatial support for a sim-
ple image captioning model [4] 2 (without attention) using

2‘neuraltalk2’, publicly available at https://github.com/

3

Grad-CAM visualizations. Given a caption, we compute
the gradient of its log probability w.r.t. units in the last con-
volutional layer of the CNN (conv5_3 for VGG-16) and
generate Grad-CAM visualizations as described in Section
2. Results are shown in Fig. 3a. In the ﬁrst example, the
Grad-CAM maps for the generated caption localize every
occurrence of both the kites and people in spite of their
relatively small size. In the top right example, Grad-CAM
correctly highlights the pizza and the man, but ignores the
woman nearby, since ‘woman’ is not mentioned in the cap-
tion. As described in Fig. 3b, if we generate captions for
speciﬁc bounding boxes in an image then Grad-CAM high-
lights only regions within those bounding boxes.

Visual Question Answering. Typical VQA pipelines con-
sist of a CNN to model images and an RNN language model
for questions. The image and the question representations
are fused to predict the answer, typically with a 1000-way
classiﬁcation. Since this is a classiﬁcation problem, we pick

(a) Visualizing baseline VQA model from [5]

(b) Visualizing ResNet based Hierarchical co-attention VQA model from [6]

Figure 4: VQA visualizations: (a) Given the image on the left and the question
“What color is the ﬁrehydrant?”, we visualize Grad-CAMs and Guided Grad-CAMs
for the answers “red", “yellow" and “yellow and red". Our visualizations are highly
interpretable and help explain the model’s predictions – for “red”, the model focuses
on the bottom red part of the ﬁrehydrant; when forced to answer “yellow”, the model
concentrates on it‘s top yellow cap, and when forced to answer “yellow and red", it
looks at the whole ﬁrehydrant! (b) Grad-CAM for ResNet-based VQA model.
an answer (the score yc in (1)) and use its score to compute
Grad-CAM. Despite the complexity of the task, involving
both visual and language components, the explanations (of
the baseline VQA model from [5] and a ResNet based hi-
erarchical co-attention model from [6]) described in Fig. 4
are suprisingly intuitive and informative.

karpathy/neuraltalk2

4

4. Conclusion
In this work, we proposed a novel class-discriminative lo-
calization technique – Gradient-weighted Class Activation
Mapping (Grad-CAM) – and combined it with existing
high-resolution visualizations to produce visual explana-
tions for CNN-based models. Human studies reveal that
our localization-augmented visualizations can discriminate
between classes more accurately and better reveal the trust-
worthiness of a classiﬁer. In addition to the image caption-
ing and VQA examples shown here, the full version of our
paper [8] evaluates Grad-CAM on the ImageNet localiza-
tion challenge, analyzes failure modes of VGG-16 on Ima-
geNet classiﬁcation using Grad-CAM, measures correlation
between VQA Grad-CAM and human attention maps, de-
scribes ablation studies and provides many more examples.
Grad-CAM provides a new way to understand any CNN-
based model.
References

[1] H. Agrawal, C. S. Mathialagan, Y. Goyal, N. Chavali,
P. Banik, A. Mohapatra, A. Osman, and D. Batra. CloudCV:
Large Scale Distributed Computer Vision as a Cloud Service.
In Mobile Cloud Visual Media Computing, pages 265–290.
Springer, 2015. 1

[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
CVPR, 2009. 3

[3] J. Johnson, A. Karpathy, and L. Fei-Fei. DenseCap: Fully
Convolutional Localization Networks for Dense Captioning.
In CVPR, 2016. 3

[4] A. Karpathy and L. Fei-Fei. Deep visual-semantic align-
In CVPR, 2015.

ments for generating image descriptions.
3

[5] J. Lu, X. Lin, D. Batra, and D. Parikh.

Deeper
LSTM and normalized CNN Visual Question Answering
https://github.com/VT-vision-lab/
model.
VQA_LSTM_CNN, 2015. 4

[6] J. Lu, J. Yang, D. Batra, and D. Parikh. Hierarchical
Question-Image Co-Attention for Visual Question Answer-
ing. In NIPS, 2016. 4

[7] M. T. Ribeiro, S. Singh, and C. Guestrin. "Why Should I
Trust You?": Explaining the Predictions of Any Classiﬁer.
In SIGKDD, 2016. 3

[8] R. Selvaraju, A. Das, R. Vedantam, M. Cogswell, D. Parikh,
and D. Batra. Grad-CAM: Why did you say that? Visual
Explanations from Deep Networks via Gradient-based Lo-
calization. CoRR, abs/1610.02391, 2016. 1, 4

[9] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. A. Ried-
miller. Striving for Simplicity: The All Convolutional Net.
CoRR, abs/1412.6806, 2014. 1

[10] C. Vondrick, A. Khosla, T. Malisiewicz, and A. Torralba.
ICCV,

HOGgles: Visualizing Object Detection Features.
2013. 3

[11] M. D. Zeiler and R. Fergus. Visualizing and understanding

convolutional networks. In ECCV, 2014. 3

[12] B. Zhou, A. Khosla, L. A., A. Oliva, and A. Torralba. Learn-
ing Deep Features for Discriminative Localization. In CVPR,
2016. 1, 2

Grad-CAM: Why did you say that?

Ramprasaath R. Selvaraju

Ramakrishna Vedantam

Michael Cogswell

Abhishek Das
Devi Parikh

Dhruv Batra

Virginia Tech
{ram21, abhshkdz, vrama91, cogswell, parikh, dbatra}@vt.edu

7
1
0
2
 
n
a
J
 
5
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
0
5
4
7
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

We propose a technique for making Convolutional Neural
Network (CNN)-based models more transparent by visu-
alizing input regions that are ‘important’ for predictions
– producing visual explanations. Our approach, called
Gradient-weighted Class Activation Mapping (Grad-CAM),
uses class-speciﬁc gradient information to localize impor-
tant regions. These localizations are combined with existing
pixel-space visualizations to create a novel high-resolution
and class-discriminative visualization called Guided Grad-
CAM. These methods help better understand CNN-based
models, including image captioning and visual question an-
swering (VQA) models. We evaluate our visual explana-
tions by measuring their ability to discriminate between
classes, to inspire trust in humans, and their correlation
with occlusion maps. Grad-CAM provides a new way to
understand CNN-based models.

We have released code, an online demo hosted on
CloudCV [1], and the full paper [8].1
1. Introduction
Convolutional Neural Networks (CNNs) and other deep net-
works have enabled unprecedented breakthroughs in a vari-
ety of computer vision tasks, from image classiﬁcation to
image captioning and visual question answering. While
these deep neural networks enable superior performance,
their lack of decomposability into intuitive and understand-
able components makes them hard to interpret. Conse-
quently, when today’s intelligent systems fail,
they fail
spectacularly disgracefully, without warning or explanation,
leaving a user staring at incoherent output, wondering why
the system did what it did. In order to build trust in intelle-
gent systems and move towards their meaningful integration
into our everyday lives, it is clear that we must build ‘trans-
parent’ models that explain why they predict what they do.

However, there is a trade-off between accuracy and sim-
plicity/interpretability. Classical rule-based or expert sys-

1 code: https://github.com/ramprs/grad-cam/ demo:
http://gradcam.cloudcv.org paper: https://arxiv.org/
abs/1610.02391

tems were highly interpretable but not very accurate (or ro-
bust). Decomposable pipelines where each stage is hand-
designed are thought to be more interpretable as each indi-
vidual component assumes a natural intuitive explanation.
This tradeoff is realized in more recent work, like Class Ac-
tivation Mapping (CAM) [12], which allows explanations
for a speciﬁc class of image classiﬁcation CNNs. By us-
ing deep models, we sacriﬁce a degree of interpretability in
pipeline modules in order to achieve greater performance
through greater abstraction (more layers) and tighter inte-
gration (end-to-end training).

What makes a good visual explanation? Consider im-
age classiﬁcation – a ‘good’ visual explanation from the
model justifying a predicted class should be (a) class-
discriminative (i.e. localize the category in the image) and
(b) high-resolution (i.e. capture ﬁne-grained detail). To be
concrete we introduce Grad-CAM using the notion ‘class’
from image classiﬁcation (e.g., cat or dog), but visual ex-
planations can be considered for any differentiable node in
a computational graph, including words from a caption or
the answer to a question.

This is illustrated in Fig. 1, where we visualize the ‘tiger cat’
class. Pixel-space gradient visualizations such as Guided
Backpropagation [9], seen at the top of Fig. 1, are high-
resolution and highlight ﬁne-grained details in the image,
but are not class-discriminative. Both the cat and the dog
are highlighted despite ‘tiger cat’ being the class of in-
terest (in fact, the Guided Backpropagation visualizations
of ‘boxer’ dog ‘tiger cat’ are indistinguishable). However,
the Grad-CAM visualization is low-resolution and does not
contain ﬁne details. A high-res visualization like Guided
Grad-CAM helps show these details by highlighting stripes
in the cat in addition to localizing the cat. We combine the
best of both worlds by fusing existing pixel-space gradient
visualizations with our novel localization method – called
Grad-CAM – to create Guided Grad-CAM visualizations,
which are both high-resolution and class-discriminative.

In this abstract: (1) We propose Gradient-weighted Class
Activation Mapping (Grad-CAM) to generate visual expla-
nations from any CNN-based network without requiring ar-

1

to obtain the class-discriminative localization map Grad-
Grad-CAM ∈ Ru×v in generic CNN-based architec-
CAM Lc
tures, we ﬁrst compute the gradient of yc with respect to
feature maps A of a convolutional layer, i.e. ∂yc
. These
∂Ak
ij
gradients are global-average-pooled to obtain weights αc
k:

i

j

αc

(cid:88)

(cid:88)

1
Z

k =

∂yc
∂Ak
ij
This weight αc
k represents a partial linearization of the deep
network downstream from A, and captures the ‘importance’
of feature map k for a target class c. In general, yc need not
be a class score, but could be any differentiable activation.
As in CAM, our Grad-CAM heat-map is a weighted combi-
nation of feature maps, but we follow this by a ReLU:

(2)

Lc

Grad-CAM = ReLU

kAk
αc

(3)

(cid:33)

(cid:32)

(cid:88)

k

This results in a coarse heat-map, which is normalized for
visualization. Other than the ReLU in (3), Grad-CAM is a
generalization of CAM (wc
k where CAM
can be applied) to any CNN-based architecture (CNNs with
fully-connected-layers, ResNets, CNNs stacked with Re-
current Neural Networks (RNNs) etc.).

k are precisely αc

Guided Grad-CAM. In order to combine the class-
discriminative nature of Grad-CAM and the high-resolution
nature of Guided Backpropagation, we fuse them via point-
wise multiplication to create Guided Grad-CAM, shown on
the left of Fig. 1. We expect the last convolutional layers to
have the best compromise between high-level semantics and
detailed spatial information, so we use these feature maps to
compute Grad-CAM and Guided Grad-CAM.
3. Experiments
We evaluate our visualization then show image captioning
and visual question answering examples.

3.1. Evaluating Visualizations

Evaluating Class Discrimination. Intuitively, a good pre-
diction explanation is one that produces discriminative vi-
sualizations for the class of interest. We select images from
PASCAL VOC 2007 val set that contain exactly two an-
notated categories, and create visualizations for one of the
classes. These are shown to workers on Amazon Mechan-
ical Turk (AMT), who are asked “Which of the two object
categories is depicted in the image?” and presented with the
two categories present in the original image as options. As
shown in Table. 1 column 1, human subjects can correctly
identify the category being visualized substantially more of-
ten when using Grad-CAM. This makes Guided Backprop-
agation more class-discriminative.

Evaluating Trust. Given explanations from two different
models, we want to evaluate which of them seems more
trustworthy. We use AlexNet and VGG-16 to compare

Figure 1: Grad-CAM overview: Given an image, and a category (‘tiger cat’) as
input, we foward propagate the image through the model to obtain the raw class
scores before softmax. The gradients are set to zero for all classes except the de-
sired class (tiger cat), which is set to 1. This signal is then backpropagated to the
rectiﬁed convolutional feature map of interest, where we can compute the coarse
Grad-CAM localization (blue heatmap). Finally, we pointwise multiply the heatmap
with guided backpropagation to get Guided Grad-CAM visualizations which are both
high-resolution and class-discriminative.
chitectural changes. (2) To illustrate the broad applicabil-
ity of our technique across tasks, we apply Grad-CAM to
state-of-the-art image captioning and visual question an-
swering models. (3) We design and conduct human stud-
ies to show that Guided Grad-CAM explanations are class-
discriminative and help humans not only establish trust, but
also help untrained users successfully discern a ‘stronger’
deep network from a ‘weaker’ one even when both networks
make identical predictions, simply on the basis of their vi-
sual explanations. (4) Our code and demos of Grad-CAM
are released to help others apply Grad-CAM to interpret
their own models.
2. Approach

We review Class Activation Mapping (CAM) [12], propose
Grad-CAM, and combine Grad-CAM with high-resolution
visualizations to form Guided Grad-CAM. This is summa-
rized by Fig. 1.

Class Activation Mapping (CAM). CAM [12] produces
a localization map from image classiﬁcation CNNs where
global-average-pooled convolutional feature maps are fed
directly into a softmax. Speciﬁcally, let the penultimate
CNN layer produce K feature maps Ak ∈ Ru×v of width u
and height v. These feature maps are then spatially pooled
using Global Average Pooling (GAP) and linearly trans-
formed to produce a score yc for each class c

yc =

(cid:88)

wc
k

1
Z

(cid:88)

(cid:88)

Ak
ij.

(1)

j

k

k wc

i
CAM ∈ Ru×v for class c,
To produce a localization map Lc
CAM computes the linear combination of the ﬁnal feature
maps using the learned weights of the ﬁnal layer: Lc
CAM =
(cid:80)
kAk. This is normalized to lie between 0 and 1 for vi-
sualization purposes. CAM can not be applied to networks
which use multiple fully-connected layers before the output
layer, so fully-connected layers are replaced with convolu-
tional ones and the network is re-trained. In comparison,
our approach can be applied directly to any CNN-based dif-
ferentiable architecture as is without re-training.

Gradient-weighted Class Activation Mapping. In order

2

Guided Backpropagation and Guided Grad-CAM visualiza-
tions, noting that VGG-16 is known to be more reliable than
AlexNet. In order to tease apart the efﬁcacy of the visual-
ization from the accuracy of the model being visualized, we
consider only those instances where both models made the
same prediction as ground truth. Given a visualization from
AlexNet, one from VGG-16, and the name of the object
predicted by both the networks, workers are instructed to
rate which model is more reliable. Results are shown in the
Relative Reliability column of Table. 1, where scores range
from -2 to +2 and positive scores indicate VGG is judged
to be more reliable than AlexNet. With Guided Backprop-
agation, humans score VGG as slightly more reliable than
AlexNet, while with Guided Grad-CAM they score VGG as
clearly more reliable than AlexNet. Thus our Guided Grad-
CAM visualization can help users place trust in a model that
can generalize better, based on individual prediction expla-
nations.

Faithfulness vs. Interpretability. Faithfulness of a vi-
sualization to a model is its ability to accurately explain
the function learned by the model. Naturally, there exists
a tradeoff between the interpretability and faithfulness for
complex models operating on highly compositional inputs.
A more faithful visualization might describe the model in
precise detail yet be completely opaque to human inspec-
tion. Here we are only interested in local ﬁdelity; the visu-
alization need only explain the parts of the model relevant
to that image. That is, in the vicinity of the input data point,
our explanation should be faithful to the model [7].

For comparison, we need a reference explanation with high
local-faithfulness. One obvious choice for such a visual-
ization is image occlusion [11], where we measure the dif-
ference in CNN scores when patches of the input image
Interestingly, patches which change the
are masked out.
CNN score are also patches to which Guided Grad-CAM
assigns high intensity, as shown by measuring rank corre-
lation between patch intensities in Table. 1 (3rd column).
This shows that Guided Grad-CAM is more faithful to the
original model than Guided Backpropagation.

Method

Human Classiﬁca-
tion Accuracy

Relative Re-
liability

Rank Correlation
w/ Occlusion

Guided Backpropagation
Guided Grad-CAM

44.44
61.23

+1.00
+1.27

0.168
0.261

Table 1: Quantitative Visualization Evaluation. Guided Grad-CAM enables hu-
mans to differentiate between visualizations of different classes (Human Classiﬁca-
tion Accuracy) and pick more reliable models (Relative Reliability). It also accurately
reﬂects the behavior of the model (Rank Correlation w/ Occlusion).
3.2. Analyzing Failure Modes for VGG-16
We use Guided Grad-CAM to analyze failure modes of the
VGG-16 CNN on ImageNet classiﬁcation [2].

In order to see what mistakes a network is making we
ﬁrst get a list of examples that the network (VGG-16)
fails to classify correctly. For the misclassiﬁed exam-
ples, we use Guided Grad-CAM to visualize both the cor-

(a)

(b)

(c)

(d)

Figure 2: In these cases the model (VGG-16) failed to predict the correct class as
its top 1 prediction, but it even failed to predict the correct class in its top 5 for ﬁgure
b. All of these errors are due in part to class ambiguity. (a) For example, the network
predicts ‘sandbar’ based on the foreground of (a), but it also knows where the correct
label ‘volcano’ is located. (c-d) In other cases, these errors are still reasonable but
not immediately apparent. For example, humans would ﬁnd it hard to explain the
predicted ‘syringes’ in (c) without looking at the visualization for the predicted class.
rect class and the predicted class. A major advantage of
Guided Grad-CAM over other methods is its ability to
more usefully investigate and explain classiﬁcation mis-
takes, since our visualizations are high-resolution and more
class-discriminative. As seen in Fig. 2, some failures are
due to ambiguities inherent in ImageNet classiﬁcation. We
can also see that seemingly unreasonable predictions have
reasonable explanations, which is a similar observation to
HOGgles [10].
3.3. Image Captioning and VQA

(a) Image captioning explanations

(b) Comparison to DenseCap
Figure 3: Interpreting image captioning models: We use our class-discriminative
localization technique, Grad-CAM to ﬁnd spatial support regions for captions in im-
ages. Fig. 3a Visual explanations from image captioning model [4] highlighting im-
age regions considered to be important for producing the captions. Fig. 3b Grad-
CAM localizations of a global or holistic captioning model for captions generated
by a dense captioning model [3] for the three bounding box proposals marked on the
left. We can see that we get back Grad-CAM localizations (right) that agree with
those bounding boxes – even though the captioning model and Grad-CAM do not use
any bounding box annotations.
Image Captioning. We visualize spatial support for a sim-
ple image captioning model [4] 2 (without attention) using

2‘neuraltalk2’, publicly available at https://github.com/

3

Grad-CAM visualizations. Given a caption, we compute
the gradient of its log probability w.r.t. units in the last con-
volutional layer of the CNN (conv5_3 for VGG-16) and
generate Grad-CAM visualizations as described in Section
2. Results are shown in Fig. 3a. In the ﬁrst example, the
Grad-CAM maps for the generated caption localize every
occurrence of both the kites and people in spite of their
relatively small size. In the top right example, Grad-CAM
correctly highlights the pizza and the man, but ignores the
woman nearby, since ‘woman’ is not mentioned in the cap-
tion. As described in Fig. 3b, if we generate captions for
speciﬁc bounding boxes in an image then Grad-CAM high-
lights only regions within those bounding boxes.

Visual Question Answering. Typical VQA pipelines con-
sist of a CNN to model images and an RNN language model
for questions. The image and the question representations
are fused to predict the answer, typically with a 1000-way
classiﬁcation. Since this is a classiﬁcation problem, we pick

(a) Visualizing baseline VQA model from [5]

(b) Visualizing ResNet based Hierarchical co-attention VQA model from [6]

Figure 4: VQA visualizations: (a) Given the image on the left and the question
“What color is the ﬁrehydrant?”, we visualize Grad-CAMs and Guided Grad-CAMs
for the answers “red", “yellow" and “yellow and red". Our visualizations are highly
interpretable and help explain the model’s predictions – for “red”, the model focuses
on the bottom red part of the ﬁrehydrant; when forced to answer “yellow”, the model
concentrates on it‘s top yellow cap, and when forced to answer “yellow and red", it
looks at the whole ﬁrehydrant! (b) Grad-CAM for ResNet-based VQA model.
an answer (the score yc in (1)) and use its score to compute
Grad-CAM. Despite the complexity of the task, involving
both visual and language components, the explanations (of
the baseline VQA model from [5] and a ResNet based hi-
erarchical co-attention model from [6]) described in Fig. 4
are suprisingly intuitive and informative.

karpathy/neuraltalk2

4

4. Conclusion
In this work, we proposed a novel class-discriminative lo-
calization technique – Gradient-weighted Class Activation
Mapping (Grad-CAM) – and combined it with existing
high-resolution visualizations to produce visual explana-
tions for CNN-based models. Human studies reveal that
our localization-augmented visualizations can discriminate
between classes more accurately and better reveal the trust-
worthiness of a classiﬁer. In addition to the image caption-
ing and VQA examples shown here, the full version of our
paper [8] evaluates Grad-CAM on the ImageNet localiza-
tion challenge, analyzes failure modes of VGG-16 on Ima-
geNet classiﬁcation using Grad-CAM, measures correlation
between VQA Grad-CAM and human attention maps, de-
scribes ablation studies and provides many more examples.
Grad-CAM provides a new way to understand any CNN-
based model.
References

[1] H. Agrawal, C. S. Mathialagan, Y. Goyal, N. Chavali,
P. Banik, A. Mohapatra, A. Osman, and D. Batra. CloudCV:
Large Scale Distributed Computer Vision as a Cloud Service.
In Mobile Cloud Visual Media Computing, pages 265–290.
Springer, 2015. 1

[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
CVPR, 2009. 3

[3] J. Johnson, A. Karpathy, and L. Fei-Fei. DenseCap: Fully
Convolutional Localization Networks for Dense Captioning.
In CVPR, 2016. 3

[4] A. Karpathy and L. Fei-Fei. Deep visual-semantic align-
In CVPR, 2015.

ments for generating image descriptions.
3

[5] J. Lu, X. Lin, D. Batra, and D. Parikh.

Deeper
LSTM and normalized CNN Visual Question Answering
https://github.com/VT-vision-lab/
model.
VQA_LSTM_CNN, 2015. 4

[6] J. Lu, J. Yang, D. Batra, and D. Parikh. Hierarchical
Question-Image Co-Attention for Visual Question Answer-
ing. In NIPS, 2016. 4

[7] M. T. Ribeiro, S. Singh, and C. Guestrin. "Why Should I
Trust You?": Explaining the Predictions of Any Classiﬁer.
In SIGKDD, 2016. 3

[8] R. Selvaraju, A. Das, R. Vedantam, M. Cogswell, D. Parikh,
and D. Batra. Grad-CAM: Why did you say that? Visual
Explanations from Deep Networks via Gradient-based Lo-
calization. CoRR, abs/1610.02391, 2016. 1, 4

[9] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. A. Ried-
miller. Striving for Simplicity: The All Convolutional Net.
CoRR, abs/1412.6806, 2014. 1

[10] C. Vondrick, A. Khosla, T. Malisiewicz, and A. Torralba.
ICCV,

HOGgles: Visualizing Object Detection Features.
2013. 3

[11] M. D. Zeiler and R. Fergus. Visualizing and understanding

convolutional networks. In ECCV, 2014. 3

[12] B. Zhou, A. Khosla, L. A., A. Oliva, and A. Torralba. Learn-
ing Deep Features for Discriminative Localization. In CVPR,
2016. 1, 2

Grad-CAM: Why did you say that?

Ramprasaath R. Selvaraju

Ramakrishna Vedantam

Michael Cogswell

Abhishek Das
Devi Parikh

Dhruv Batra

Virginia Tech
{ram21, abhshkdz, vrama91, cogswell, parikh, dbatra}@vt.edu

7
1
0
2
 
n
a
J
 
5
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
0
5
4
7
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

We propose a technique for making Convolutional Neural
Network (CNN)-based models more transparent by visu-
alizing input regions that are ‘important’ for predictions
– producing visual explanations. Our approach, called
Gradient-weighted Class Activation Mapping (Grad-CAM),
uses class-speciﬁc gradient information to localize impor-
tant regions. These localizations are combined with existing
pixel-space visualizations to create a novel high-resolution
and class-discriminative visualization called Guided Grad-
CAM. These methods help better understand CNN-based
models, including image captioning and visual question an-
swering (VQA) models. We evaluate our visual explana-
tions by measuring their ability to discriminate between
classes, to inspire trust in humans, and their correlation
with occlusion maps. Grad-CAM provides a new way to
understand CNN-based models.

We have released code, an online demo hosted on
CloudCV [1], and the full paper [8].1
1. Introduction
Convolutional Neural Networks (CNNs) and other deep net-
works have enabled unprecedented breakthroughs in a vari-
ety of computer vision tasks, from image classiﬁcation to
image captioning and visual question answering. While
these deep neural networks enable superior performance,
their lack of decomposability into intuitive and understand-
able components makes them hard to interpret. Conse-
quently, when today’s intelligent systems fail,
they fail
spectacularly disgracefully, without warning or explanation,
leaving a user staring at incoherent output, wondering why
the system did what it did. In order to build trust in intelle-
gent systems and move towards their meaningful integration
into our everyday lives, it is clear that we must build ‘trans-
parent’ models that explain why they predict what they do.

However, there is a trade-off between accuracy and sim-
plicity/interpretability. Classical rule-based or expert sys-

1 code: https://github.com/ramprs/grad-cam/ demo:
http://gradcam.cloudcv.org paper: https://arxiv.org/
abs/1610.02391

tems were highly interpretable but not very accurate (or ro-
bust). Decomposable pipelines where each stage is hand-
designed are thought to be more interpretable as each indi-
vidual component assumes a natural intuitive explanation.
This tradeoff is realized in more recent work, like Class Ac-
tivation Mapping (CAM) [12], which allows explanations
for a speciﬁc class of image classiﬁcation CNNs. By us-
ing deep models, we sacriﬁce a degree of interpretability in
pipeline modules in order to achieve greater performance
through greater abstraction (more layers) and tighter inte-
gration (end-to-end training).

What makes a good visual explanation? Consider im-
age classiﬁcation – a ‘good’ visual explanation from the
model justifying a predicted class should be (a) class-
discriminative (i.e. localize the category in the image) and
(b) high-resolution (i.e. capture ﬁne-grained detail). To be
concrete we introduce Grad-CAM using the notion ‘class’
from image classiﬁcation (e.g., cat or dog), but visual ex-
planations can be considered for any differentiable node in
a computational graph, including words from a caption or
the answer to a question.

This is illustrated in Fig. 1, where we visualize the ‘tiger cat’
class. Pixel-space gradient visualizations such as Guided
Backpropagation [9], seen at the top of Fig. 1, are high-
resolution and highlight ﬁne-grained details in the image,
but are not class-discriminative. Both the cat and the dog
are highlighted despite ‘tiger cat’ being the class of in-
terest (in fact, the Guided Backpropagation visualizations
of ‘boxer’ dog ‘tiger cat’ are indistinguishable). However,
the Grad-CAM visualization is low-resolution and does not
contain ﬁne details. A high-res visualization like Guided
Grad-CAM helps show these details by highlighting stripes
in the cat in addition to localizing the cat. We combine the
best of both worlds by fusing existing pixel-space gradient
visualizations with our novel localization method – called
Grad-CAM – to create Guided Grad-CAM visualizations,
which are both high-resolution and class-discriminative.

In this abstract: (1) We propose Gradient-weighted Class
Activation Mapping (Grad-CAM) to generate visual expla-
nations from any CNN-based network without requiring ar-

1

to obtain the class-discriminative localization map Grad-
Grad-CAM ∈ Ru×v in generic CNN-based architec-
CAM Lc
tures, we ﬁrst compute the gradient of yc with respect to
feature maps A of a convolutional layer, i.e. ∂yc
. These
∂Ak
ij
gradients are global-average-pooled to obtain weights αc
k:

i

j

αc

(cid:88)

(cid:88)

1
Z

k =

∂yc
∂Ak
ij
This weight αc
k represents a partial linearization of the deep
network downstream from A, and captures the ‘importance’
of feature map k for a target class c. In general, yc need not
be a class score, but could be any differentiable activation.
As in CAM, our Grad-CAM heat-map is a weighted combi-
nation of feature maps, but we follow this by a ReLU:

(2)

Lc

Grad-CAM = ReLU

kAk
αc

(3)

(cid:33)

(cid:32)

(cid:88)

k

This results in a coarse heat-map, which is normalized for
visualization. Other than the ReLU in (3), Grad-CAM is a
generalization of CAM (wc
k where CAM
can be applied) to any CNN-based architecture (CNNs with
fully-connected-layers, ResNets, CNNs stacked with Re-
current Neural Networks (RNNs) etc.).

k are precisely αc

Guided Grad-CAM. In order to combine the class-
discriminative nature of Grad-CAM and the high-resolution
nature of Guided Backpropagation, we fuse them via point-
wise multiplication to create Guided Grad-CAM, shown on
the left of Fig. 1. We expect the last convolutional layers to
have the best compromise between high-level semantics and
detailed spatial information, so we use these feature maps to
compute Grad-CAM and Guided Grad-CAM.
3. Experiments
We evaluate our visualization then show image captioning
and visual question answering examples.

3.1. Evaluating Visualizations

Evaluating Class Discrimination. Intuitively, a good pre-
diction explanation is one that produces discriminative vi-
sualizations for the class of interest. We select images from
PASCAL VOC 2007 val set that contain exactly two an-
notated categories, and create visualizations for one of the
classes. These are shown to workers on Amazon Mechan-
ical Turk (AMT), who are asked “Which of the two object
categories is depicted in the image?” and presented with the
two categories present in the original image as options. As
shown in Table. 1 column 1, human subjects can correctly
identify the category being visualized substantially more of-
ten when using Grad-CAM. This makes Guided Backprop-
agation more class-discriminative.

Evaluating Trust. Given explanations from two different
models, we want to evaluate which of them seems more
trustworthy. We use AlexNet and VGG-16 to compare

Figure 1: Grad-CAM overview: Given an image, and a category (‘tiger cat’) as
input, we foward propagate the image through the model to obtain the raw class
scores before softmax. The gradients are set to zero for all classes except the de-
sired class (tiger cat), which is set to 1. This signal is then backpropagated to the
rectiﬁed convolutional feature map of interest, where we can compute the coarse
Grad-CAM localization (blue heatmap). Finally, we pointwise multiply the heatmap
with guided backpropagation to get Guided Grad-CAM visualizations which are both
high-resolution and class-discriminative.
chitectural changes. (2) To illustrate the broad applicabil-
ity of our technique across tasks, we apply Grad-CAM to
state-of-the-art image captioning and visual question an-
swering models. (3) We design and conduct human stud-
ies to show that Guided Grad-CAM explanations are class-
discriminative and help humans not only establish trust, but
also help untrained users successfully discern a ‘stronger’
deep network from a ‘weaker’ one even when both networks
make identical predictions, simply on the basis of their vi-
sual explanations. (4) Our code and demos of Grad-CAM
are released to help others apply Grad-CAM to interpret
their own models.
2. Approach

We review Class Activation Mapping (CAM) [12], propose
Grad-CAM, and combine Grad-CAM with high-resolution
visualizations to form Guided Grad-CAM. This is summa-
rized by Fig. 1.

Class Activation Mapping (CAM). CAM [12] produces
a localization map from image classiﬁcation CNNs where
global-average-pooled convolutional feature maps are fed
directly into a softmax. Speciﬁcally, let the penultimate
CNN layer produce K feature maps Ak ∈ Ru×v of width u
and height v. These feature maps are then spatially pooled
using Global Average Pooling (GAP) and linearly trans-
formed to produce a score yc for each class c

yc =

(cid:88)

wc
k

1
Z

(cid:88)

(cid:88)

Ak
ij.

(1)

j

k

k wc

i
CAM ∈ Ru×v for class c,
To produce a localization map Lc
CAM computes the linear combination of the ﬁnal feature
maps using the learned weights of the ﬁnal layer: Lc
CAM =
(cid:80)
kAk. This is normalized to lie between 0 and 1 for vi-
sualization purposes. CAM can not be applied to networks
which use multiple fully-connected layers before the output
layer, so fully-connected layers are replaced with convolu-
tional ones and the network is re-trained. In comparison,
our approach can be applied directly to any CNN-based dif-
ferentiable architecture as is without re-training.

Gradient-weighted Class Activation Mapping. In order

2

Guided Backpropagation and Guided Grad-CAM visualiza-
tions, noting that VGG-16 is known to be more reliable than
AlexNet. In order to tease apart the efﬁcacy of the visual-
ization from the accuracy of the model being visualized, we
consider only those instances where both models made the
same prediction as ground truth. Given a visualization from
AlexNet, one from VGG-16, and the name of the object
predicted by both the networks, workers are instructed to
rate which model is more reliable. Results are shown in the
Relative Reliability column of Table. 1, where scores range
from -2 to +2 and positive scores indicate VGG is judged
to be more reliable than AlexNet. With Guided Backprop-
agation, humans score VGG as slightly more reliable than
AlexNet, while with Guided Grad-CAM they score VGG as
clearly more reliable than AlexNet. Thus our Guided Grad-
CAM visualization can help users place trust in a model that
can generalize better, based on individual prediction expla-
nations.

Faithfulness vs. Interpretability. Faithfulness of a vi-
sualization to a model is its ability to accurately explain
the function learned by the model. Naturally, there exists
a tradeoff between the interpretability and faithfulness for
complex models operating on highly compositional inputs.
A more faithful visualization might describe the model in
precise detail yet be completely opaque to human inspec-
tion. Here we are only interested in local ﬁdelity; the visu-
alization need only explain the parts of the model relevant
to that image. That is, in the vicinity of the input data point,
our explanation should be faithful to the model [7].

For comparison, we need a reference explanation with high
local-faithfulness. One obvious choice for such a visual-
ization is image occlusion [11], where we measure the dif-
ference in CNN scores when patches of the input image
Interestingly, patches which change the
are masked out.
CNN score are also patches to which Guided Grad-CAM
assigns high intensity, as shown by measuring rank corre-
lation between patch intensities in Table. 1 (3rd column).
This shows that Guided Grad-CAM is more faithful to the
original model than Guided Backpropagation.

Method

Human Classiﬁca-
tion Accuracy

Relative Re-
liability

Rank Correlation
w/ Occlusion

Guided Backpropagation
Guided Grad-CAM

44.44
61.23

+1.00
+1.27

0.168
0.261

Table 1: Quantitative Visualization Evaluation. Guided Grad-CAM enables hu-
mans to differentiate between visualizations of different classes (Human Classiﬁca-
tion Accuracy) and pick more reliable models (Relative Reliability). It also accurately
reﬂects the behavior of the model (Rank Correlation w/ Occlusion).
3.2. Analyzing Failure Modes for VGG-16
We use Guided Grad-CAM to analyze failure modes of the
VGG-16 CNN on ImageNet classiﬁcation [2].

In order to see what mistakes a network is making we
ﬁrst get a list of examples that the network (VGG-16)
fails to classify correctly. For the misclassiﬁed exam-
ples, we use Guided Grad-CAM to visualize both the cor-

(a)

(b)

(c)

(d)

Figure 2: In these cases the model (VGG-16) failed to predict the correct class as
its top 1 prediction, but it even failed to predict the correct class in its top 5 for ﬁgure
b. All of these errors are due in part to class ambiguity. (a) For example, the network
predicts ‘sandbar’ based on the foreground of (a), but it also knows where the correct
label ‘volcano’ is located. (c-d) In other cases, these errors are still reasonable but
not immediately apparent. For example, humans would ﬁnd it hard to explain the
predicted ‘syringes’ in (c) without looking at the visualization for the predicted class.
rect class and the predicted class. A major advantage of
Guided Grad-CAM over other methods is its ability to
more usefully investigate and explain classiﬁcation mis-
takes, since our visualizations are high-resolution and more
class-discriminative. As seen in Fig. 2, some failures are
due to ambiguities inherent in ImageNet classiﬁcation. We
can also see that seemingly unreasonable predictions have
reasonable explanations, which is a similar observation to
HOGgles [10].
3.3. Image Captioning and VQA

(a) Image captioning explanations

(b) Comparison to DenseCap
Figure 3: Interpreting image captioning models: We use our class-discriminative
localization technique, Grad-CAM to ﬁnd spatial support regions for captions in im-
ages. Fig. 3a Visual explanations from image captioning model [4] highlighting im-
age regions considered to be important for producing the captions. Fig. 3b Grad-
CAM localizations of a global or holistic captioning model for captions generated
by a dense captioning model [3] for the three bounding box proposals marked on the
left. We can see that we get back Grad-CAM localizations (right) that agree with
those bounding boxes – even though the captioning model and Grad-CAM do not use
any bounding box annotations.
Image Captioning. We visualize spatial support for a sim-
ple image captioning model [4] 2 (without attention) using

2‘neuraltalk2’, publicly available at https://github.com/

3

Grad-CAM visualizations. Given a caption, we compute
the gradient of its log probability w.r.t. units in the last con-
volutional layer of the CNN (conv5_3 for VGG-16) and
generate Grad-CAM visualizations as described in Section
2. Results are shown in Fig. 3a. In the ﬁrst example, the
Grad-CAM maps for the generated caption localize every
occurrence of both the kites and people in spite of their
relatively small size. In the top right example, Grad-CAM
correctly highlights the pizza and the man, but ignores the
woman nearby, since ‘woman’ is not mentioned in the cap-
tion. As described in Fig. 3b, if we generate captions for
speciﬁc bounding boxes in an image then Grad-CAM high-
lights only regions within those bounding boxes.

Visual Question Answering. Typical VQA pipelines con-
sist of a CNN to model images and an RNN language model
for questions. The image and the question representations
are fused to predict the answer, typically with a 1000-way
classiﬁcation. Since this is a classiﬁcation problem, we pick

(a) Visualizing baseline VQA model from [5]

(b) Visualizing ResNet based Hierarchical co-attention VQA model from [6]

Figure 4: VQA visualizations: (a) Given the image on the left and the question
“What color is the ﬁrehydrant?”, we visualize Grad-CAMs and Guided Grad-CAMs
for the answers “red", “yellow" and “yellow and red". Our visualizations are highly
interpretable and help explain the model’s predictions – for “red”, the model focuses
on the bottom red part of the ﬁrehydrant; when forced to answer “yellow”, the model
concentrates on it‘s top yellow cap, and when forced to answer “yellow and red", it
looks at the whole ﬁrehydrant! (b) Grad-CAM for ResNet-based VQA model.
an answer (the score yc in (1)) and use its score to compute
Grad-CAM. Despite the complexity of the task, involving
both visual and language components, the explanations (of
the baseline VQA model from [5] and a ResNet based hi-
erarchical co-attention model from [6]) described in Fig. 4
are suprisingly intuitive and informative.

karpathy/neuraltalk2

4

4. Conclusion
In this work, we proposed a novel class-discriminative lo-
calization technique – Gradient-weighted Class Activation
Mapping (Grad-CAM) – and combined it with existing
high-resolution visualizations to produce visual explana-
tions for CNN-based models. Human studies reveal that
our localization-augmented visualizations can discriminate
between classes more accurately and better reveal the trust-
worthiness of a classiﬁer. In addition to the image caption-
ing and VQA examples shown here, the full version of our
paper [8] evaluates Grad-CAM on the ImageNet localiza-
tion challenge, analyzes failure modes of VGG-16 on Ima-
geNet classiﬁcation using Grad-CAM, measures correlation
between VQA Grad-CAM and human attention maps, de-
scribes ablation studies and provides many more examples.
Grad-CAM provides a new way to understand any CNN-
based model.
References

[1] H. Agrawal, C. S. Mathialagan, Y. Goyal, N. Chavali,
P. Banik, A. Mohapatra, A. Osman, and D. Batra. CloudCV:
Large Scale Distributed Computer Vision as a Cloud Service.
In Mobile Cloud Visual Media Computing, pages 265–290.
Springer, 2015. 1

[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
CVPR, 2009. 3

[3] J. Johnson, A. Karpathy, and L. Fei-Fei. DenseCap: Fully
Convolutional Localization Networks for Dense Captioning.
In CVPR, 2016. 3

[4] A. Karpathy and L. Fei-Fei. Deep visual-semantic align-
In CVPR, 2015.

ments for generating image descriptions.
3

[5] J. Lu, X. Lin, D. Batra, and D. Parikh.

Deeper
LSTM and normalized CNN Visual Question Answering
https://github.com/VT-vision-lab/
model.
VQA_LSTM_CNN, 2015. 4

[6] J. Lu, J. Yang, D. Batra, and D. Parikh. Hierarchical
Question-Image Co-Attention for Visual Question Answer-
ing. In NIPS, 2016. 4

[7] M. T. Ribeiro, S. Singh, and C. Guestrin. "Why Should I
Trust You?": Explaining the Predictions of Any Classiﬁer.
In SIGKDD, 2016. 3

[8] R. Selvaraju, A. Das, R. Vedantam, M. Cogswell, D. Parikh,
and D. Batra. Grad-CAM: Why did you say that? Visual
Explanations from Deep Networks via Gradient-based Lo-
calization. CoRR, abs/1610.02391, 2016. 1, 4

[9] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. A. Ried-
miller. Striving for Simplicity: The All Convolutional Net.
CoRR, abs/1412.6806, 2014. 1

[10] C. Vondrick, A. Khosla, T. Malisiewicz, and A. Torralba.
ICCV,

HOGgles: Visualizing Object Detection Features.
2013. 3

[11] M. D. Zeiler and R. Fergus. Visualizing and understanding

convolutional networks. In ECCV, 2014. 3

[12] B. Zhou, A. Khosla, L. A., A. Oliva, and A. Torralba. Learn-
ing Deep Features for Discriminative Localization. In CVPR,
2016. 1, 2

Grad-CAM: Why did you say that?

Ramprasaath R. Selvaraju

Ramakrishna Vedantam

Michael Cogswell

Abhishek Das
Devi Parikh

Dhruv Batra

Virginia Tech
{ram21, abhshkdz, vrama91, cogswell, parikh, dbatra}@vt.edu

7
1
0
2
 
n
a
J
 
5
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
0
5
4
7
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

We propose a technique for making Convolutional Neural
Network (CNN)-based models more transparent by visu-
alizing input regions that are ‘important’ for predictions
– producing visual explanations. Our approach, called
Gradient-weighted Class Activation Mapping (Grad-CAM),
uses class-speciﬁc gradient information to localize impor-
tant regions. These localizations are combined with existing
pixel-space visualizations to create a novel high-resolution
and class-discriminative visualization called Guided Grad-
CAM. These methods help better understand CNN-based
models, including image captioning and visual question an-
swering (VQA) models. We evaluate our visual explana-
tions by measuring their ability to discriminate between
classes, to inspire trust in humans, and their correlation
with occlusion maps. Grad-CAM provides a new way to
understand CNN-based models.

We have released code, an online demo hosted on
CloudCV [1], and the full paper [8].1
1. Introduction
Convolutional Neural Networks (CNNs) and other deep net-
works have enabled unprecedented breakthroughs in a vari-
ety of computer vision tasks, from image classiﬁcation to
image captioning and visual question answering. While
these deep neural networks enable superior performance,
their lack of decomposability into intuitive and understand-
able components makes them hard to interpret. Conse-
quently, when today’s intelligent systems fail,
they fail
spectacularly disgracefully, without warning or explanation,
leaving a user staring at incoherent output, wondering why
the system did what it did. In order to build trust in intelle-
gent systems and move towards their meaningful integration
into our everyday lives, it is clear that we must build ‘trans-
parent’ models that explain why they predict what they do.

However, there is a trade-off between accuracy and sim-
plicity/interpretability. Classical rule-based or expert sys-

1 code: https://github.com/ramprs/grad-cam/ demo:
http://gradcam.cloudcv.org paper: https://arxiv.org/
abs/1610.02391

tems were highly interpretable but not very accurate (or ro-
bust). Decomposable pipelines where each stage is hand-
designed are thought to be more interpretable as each indi-
vidual component assumes a natural intuitive explanation.
This tradeoff is realized in more recent work, like Class Ac-
tivation Mapping (CAM) [12], which allows explanations
for a speciﬁc class of image classiﬁcation CNNs. By us-
ing deep models, we sacriﬁce a degree of interpretability in
pipeline modules in order to achieve greater performance
through greater abstraction (more layers) and tighter inte-
gration (end-to-end training).

What makes a good visual explanation? Consider im-
age classiﬁcation – a ‘good’ visual explanation from the
model justifying a predicted class should be (a) class-
discriminative (i.e. localize the category in the image) and
(b) high-resolution (i.e. capture ﬁne-grained detail). To be
concrete we introduce Grad-CAM using the notion ‘class’
from image classiﬁcation (e.g., cat or dog), but visual ex-
planations can be considered for any differentiable node in
a computational graph, including words from a caption or
the answer to a question.

This is illustrated in Fig. 1, where we visualize the ‘tiger cat’
class. Pixel-space gradient visualizations such as Guided
Backpropagation [9], seen at the top of Fig. 1, are high-
resolution and highlight ﬁne-grained details in the image,
but are not class-discriminative. Both the cat and the dog
are highlighted despite ‘tiger cat’ being the class of in-
terest (in fact, the Guided Backpropagation visualizations
of ‘boxer’ dog ‘tiger cat’ are indistinguishable). However,
the Grad-CAM visualization is low-resolution and does not
contain ﬁne details. A high-res visualization like Guided
Grad-CAM helps show these details by highlighting stripes
in the cat in addition to localizing the cat. We combine the
best of both worlds by fusing existing pixel-space gradient
visualizations with our novel localization method – called
Grad-CAM – to create Guided Grad-CAM visualizations,
which are both high-resolution and class-discriminative.

In this abstract: (1) We propose Gradient-weighted Class
Activation Mapping (Grad-CAM) to generate visual expla-
nations from any CNN-based network without requiring ar-

1

to obtain the class-discriminative localization map Grad-
Grad-CAM ∈ Ru×v in generic CNN-based architec-
CAM Lc
tures, we ﬁrst compute the gradient of yc with respect to
feature maps A of a convolutional layer, i.e. ∂yc
. These
∂Ak
ij
gradients are global-average-pooled to obtain weights αc
k:

i

j

αc

(cid:88)

(cid:88)

1
Z

k =

∂yc
∂Ak
ij
This weight αc
k represents a partial linearization of the deep
network downstream from A, and captures the ‘importance’
of feature map k for a target class c. In general, yc need not
be a class score, but could be any differentiable activation.
As in CAM, our Grad-CAM heat-map is a weighted combi-
nation of feature maps, but we follow this by a ReLU:

(2)

Lc

Grad-CAM = ReLU

kAk
αc

(3)

(cid:33)

(cid:32)

(cid:88)

k

This results in a coarse heat-map, which is normalized for
visualization. Other than the ReLU in (3), Grad-CAM is a
generalization of CAM (wc
k where CAM
can be applied) to any CNN-based architecture (CNNs with
fully-connected-layers, ResNets, CNNs stacked with Re-
current Neural Networks (RNNs) etc.).

k are precisely αc

Guided Grad-CAM. In order to combine the class-
discriminative nature of Grad-CAM and the high-resolution
nature of Guided Backpropagation, we fuse them via point-
wise multiplication to create Guided Grad-CAM, shown on
the left of Fig. 1. We expect the last convolutional layers to
have the best compromise between high-level semantics and
detailed spatial information, so we use these feature maps to
compute Grad-CAM and Guided Grad-CAM.
3. Experiments
We evaluate our visualization then show image captioning
and visual question answering examples.

3.1. Evaluating Visualizations

Evaluating Class Discrimination. Intuitively, a good pre-
diction explanation is one that produces discriminative vi-
sualizations for the class of interest. We select images from
PASCAL VOC 2007 val set that contain exactly two an-
notated categories, and create visualizations for one of the
classes. These are shown to workers on Amazon Mechan-
ical Turk (AMT), who are asked “Which of the two object
categories is depicted in the image?” and presented with the
two categories present in the original image as options. As
shown in Table. 1 column 1, human subjects can correctly
identify the category being visualized substantially more of-
ten when using Grad-CAM. This makes Guided Backprop-
agation more class-discriminative.

Evaluating Trust. Given explanations from two different
models, we want to evaluate which of them seems more
trustworthy. We use AlexNet and VGG-16 to compare

Figure 1: Grad-CAM overview: Given an image, and a category (‘tiger cat’) as
input, we foward propagate the image through the model to obtain the raw class
scores before softmax. The gradients are set to zero for all classes except the de-
sired class (tiger cat), which is set to 1. This signal is then backpropagated to the
rectiﬁed convolutional feature map of interest, where we can compute the coarse
Grad-CAM localization (blue heatmap). Finally, we pointwise multiply the heatmap
with guided backpropagation to get Guided Grad-CAM visualizations which are both
high-resolution and class-discriminative.
chitectural changes. (2) To illustrate the broad applicabil-
ity of our technique across tasks, we apply Grad-CAM to
state-of-the-art image captioning and visual question an-
swering models. (3) We design and conduct human stud-
ies to show that Guided Grad-CAM explanations are class-
discriminative and help humans not only establish trust, but
also help untrained users successfully discern a ‘stronger’
deep network from a ‘weaker’ one even when both networks
make identical predictions, simply on the basis of their vi-
sual explanations. (4) Our code and demos of Grad-CAM
are released to help others apply Grad-CAM to interpret
their own models.
2. Approach

We review Class Activation Mapping (CAM) [12], propose
Grad-CAM, and combine Grad-CAM with high-resolution
visualizations to form Guided Grad-CAM. This is summa-
rized by Fig. 1.

Class Activation Mapping (CAM). CAM [12] produces
a localization map from image classiﬁcation CNNs where
global-average-pooled convolutional feature maps are fed
directly into a softmax. Speciﬁcally, let the penultimate
CNN layer produce K feature maps Ak ∈ Ru×v of width u
and height v. These feature maps are then spatially pooled
using Global Average Pooling (GAP) and linearly trans-
formed to produce a score yc for each class c

yc =

(cid:88)

wc
k

1
Z

(cid:88)

(cid:88)

Ak
ij.

(1)

j

k

k wc

i
CAM ∈ Ru×v for class c,
To produce a localization map Lc
CAM computes the linear combination of the ﬁnal feature
maps using the learned weights of the ﬁnal layer: Lc
CAM =
(cid:80)
kAk. This is normalized to lie between 0 and 1 for vi-
sualization purposes. CAM can not be applied to networks
which use multiple fully-connected layers before the output
layer, so fully-connected layers are replaced with convolu-
tional ones and the network is re-trained. In comparison,
our approach can be applied directly to any CNN-based dif-
ferentiable architecture as is without re-training.

Gradient-weighted Class Activation Mapping. In order

2

Guided Backpropagation and Guided Grad-CAM visualiza-
tions, noting that VGG-16 is known to be more reliable than
AlexNet. In order to tease apart the efﬁcacy of the visual-
ization from the accuracy of the model being visualized, we
consider only those instances where both models made the
same prediction as ground truth. Given a visualization from
AlexNet, one from VGG-16, and the name of the object
predicted by both the networks, workers are instructed to
rate which model is more reliable. Results are shown in the
Relative Reliability column of Table. 1, where scores range
from -2 to +2 and positive scores indicate VGG is judged
to be more reliable than AlexNet. With Guided Backprop-
agation, humans score VGG as slightly more reliable than
AlexNet, while with Guided Grad-CAM they score VGG as
clearly more reliable than AlexNet. Thus our Guided Grad-
CAM visualization can help users place trust in a model that
can generalize better, based on individual prediction expla-
nations.

Faithfulness vs. Interpretability. Faithfulness of a vi-
sualization to a model is its ability to accurately explain
the function learned by the model. Naturally, there exists
a tradeoff between the interpretability and faithfulness for
complex models operating on highly compositional inputs.
A more faithful visualization might describe the model in
precise detail yet be completely opaque to human inspec-
tion. Here we are only interested in local ﬁdelity; the visu-
alization need only explain the parts of the model relevant
to that image. That is, in the vicinity of the input data point,
our explanation should be faithful to the model [7].

For comparison, we need a reference explanation with high
local-faithfulness. One obvious choice for such a visual-
ization is image occlusion [11], where we measure the dif-
ference in CNN scores when patches of the input image
Interestingly, patches which change the
are masked out.
CNN score are also patches to which Guided Grad-CAM
assigns high intensity, as shown by measuring rank corre-
lation between patch intensities in Table. 1 (3rd column).
This shows that Guided Grad-CAM is more faithful to the
original model than Guided Backpropagation.

Method

Human Classiﬁca-
tion Accuracy

Relative Re-
liability

Rank Correlation
w/ Occlusion

Guided Backpropagation
Guided Grad-CAM

44.44
61.23

+1.00
+1.27

0.168
0.261

Table 1: Quantitative Visualization Evaluation. Guided Grad-CAM enables hu-
mans to differentiate between visualizations of different classes (Human Classiﬁca-
tion Accuracy) and pick more reliable models (Relative Reliability). It also accurately
reﬂects the behavior of the model (Rank Correlation w/ Occlusion).
3.2. Analyzing Failure Modes for VGG-16
We use Guided Grad-CAM to analyze failure modes of the
VGG-16 CNN on ImageNet classiﬁcation [2].

In order to see what mistakes a network is making we
ﬁrst get a list of examples that the network (VGG-16)
fails to classify correctly. For the misclassiﬁed exam-
ples, we use Guided Grad-CAM to visualize both the cor-

(a)

(b)

(c)

(d)

Figure 2: In these cases the model (VGG-16) failed to predict the correct class as
its top 1 prediction, but it even failed to predict the correct class in its top 5 for ﬁgure
b. All of these errors are due in part to class ambiguity. (a) For example, the network
predicts ‘sandbar’ based on the foreground of (a), but it also knows where the correct
label ‘volcano’ is located. (c-d) In other cases, these errors are still reasonable but
not immediately apparent. For example, humans would ﬁnd it hard to explain the
predicted ‘syringes’ in (c) without looking at the visualization for the predicted class.
rect class and the predicted class. A major advantage of
Guided Grad-CAM over other methods is its ability to
more usefully investigate and explain classiﬁcation mis-
takes, since our visualizations are high-resolution and more
class-discriminative. As seen in Fig. 2, some failures are
due to ambiguities inherent in ImageNet classiﬁcation. We
can also see that seemingly unreasonable predictions have
reasonable explanations, which is a similar observation to
HOGgles [10].
3.3. Image Captioning and VQA

(a) Image captioning explanations

(b) Comparison to DenseCap
Figure 3: Interpreting image captioning models: We use our class-discriminative
localization technique, Grad-CAM to ﬁnd spatial support regions for captions in im-
ages. Fig. 3a Visual explanations from image captioning model [4] highlighting im-
age regions considered to be important for producing the captions. Fig. 3b Grad-
CAM localizations of a global or holistic captioning model for captions generated
by a dense captioning model [3] for the three bounding box proposals marked on the
left. We can see that we get back Grad-CAM localizations (right) that agree with
those bounding boxes – even though the captioning model and Grad-CAM do not use
any bounding box annotations.
Image Captioning. We visualize spatial support for a sim-
ple image captioning model [4] 2 (without attention) using

2‘neuraltalk2’, publicly available at https://github.com/

3

Grad-CAM visualizations. Given a caption, we compute
the gradient of its log probability w.r.t. units in the last con-
volutional layer of the CNN (conv5_3 for VGG-16) and
generate Grad-CAM visualizations as described in Section
2. Results are shown in Fig. 3a. In the ﬁrst example, the
Grad-CAM maps for the generated caption localize every
occurrence of both the kites and people in spite of their
relatively small size. In the top right example, Grad-CAM
correctly highlights the pizza and the man, but ignores the
woman nearby, since ‘woman’ is not mentioned in the cap-
tion. As described in Fig. 3b, if we generate captions for
speciﬁc bounding boxes in an image then Grad-CAM high-
lights only regions within those bounding boxes.

Visual Question Answering. Typical VQA pipelines con-
sist of a CNN to model images and an RNN language model
for questions. The image and the question representations
are fused to predict the answer, typically with a 1000-way
classiﬁcation. Since this is a classiﬁcation problem, we pick

(a) Visualizing baseline VQA model from [5]

(b) Visualizing ResNet based Hierarchical co-attention VQA model from [6]

Figure 4: VQA visualizations: (a) Given the image on the left and the question
“What color is the ﬁrehydrant?”, we visualize Grad-CAMs and Guided Grad-CAMs
for the answers “red", “yellow" and “yellow and red". Our visualizations are highly
interpretable and help explain the model’s predictions – for “red”, the model focuses
on the bottom red part of the ﬁrehydrant; when forced to answer “yellow”, the model
concentrates on it‘s top yellow cap, and when forced to answer “yellow and red", it
looks at the whole ﬁrehydrant! (b) Grad-CAM for ResNet-based VQA model.
an answer (the score yc in (1)) and use its score to compute
Grad-CAM. Despite the complexity of the task, involving
both visual and language components, the explanations (of
the baseline VQA model from [5] and a ResNet based hi-
erarchical co-attention model from [6]) described in Fig. 4
are suprisingly intuitive and informative.

karpathy/neuraltalk2

4

4. Conclusion
In this work, we proposed a novel class-discriminative lo-
calization technique – Gradient-weighted Class Activation
Mapping (Grad-CAM) – and combined it with existing
high-resolution visualizations to produce visual explana-
tions for CNN-based models. Human studies reveal that
our localization-augmented visualizations can discriminate
between classes more accurately and better reveal the trust-
worthiness of a classiﬁer. In addition to the image caption-
ing and VQA examples shown here, the full version of our
paper [8] evaluates Grad-CAM on the ImageNet localiza-
tion challenge, analyzes failure modes of VGG-16 on Ima-
geNet classiﬁcation using Grad-CAM, measures correlation
between VQA Grad-CAM and human attention maps, de-
scribes ablation studies and provides many more examples.
Grad-CAM provides a new way to understand any CNN-
based model.
References

[1] H. Agrawal, C. S. Mathialagan, Y. Goyal, N. Chavali,
P. Banik, A. Mohapatra, A. Osman, and D. Batra. CloudCV:
Large Scale Distributed Computer Vision as a Cloud Service.
In Mobile Cloud Visual Media Computing, pages 265–290.
Springer, 2015. 1

[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
CVPR, 2009. 3

[3] J. Johnson, A. Karpathy, and L. Fei-Fei. DenseCap: Fully
Convolutional Localization Networks for Dense Captioning.
In CVPR, 2016. 3

[4] A. Karpathy and L. Fei-Fei. Deep visual-semantic align-
In CVPR, 2015.

ments for generating image descriptions.
3

[5] J. Lu, X. Lin, D. Batra, and D. Parikh.

Deeper
LSTM and normalized CNN Visual Question Answering
https://github.com/VT-vision-lab/
model.
VQA_LSTM_CNN, 2015. 4

[6] J. Lu, J. Yang, D. Batra, and D. Parikh. Hierarchical
Question-Image Co-Attention for Visual Question Answer-
ing. In NIPS, 2016. 4

[7] M. T. Ribeiro, S. Singh, and C. Guestrin. "Why Should I
Trust You?": Explaining the Predictions of Any Classiﬁer.
In SIGKDD, 2016. 3

[8] R. Selvaraju, A. Das, R. Vedantam, M. Cogswell, D. Parikh,
and D. Batra. Grad-CAM: Why did you say that? Visual
Explanations from Deep Networks via Gradient-based Lo-
calization. CoRR, abs/1610.02391, 2016. 1, 4

[9] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. A. Ried-
miller. Striving for Simplicity: The All Convolutional Net.
CoRR, abs/1412.6806, 2014. 1

[10] C. Vondrick, A. Khosla, T. Malisiewicz, and A. Torralba.
ICCV,

HOGgles: Visualizing Object Detection Features.
2013. 3

[11] M. D. Zeiler and R. Fergus. Visualizing and understanding

convolutional networks. In ECCV, 2014. 3

[12] B. Zhou, A. Khosla, L. A., A. Oliva, and A. Torralba. Learn-
ing Deep Features for Discriminative Localization. In CVPR,
2016. 1, 2

