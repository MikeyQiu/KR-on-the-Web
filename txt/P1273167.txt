Deep Convolutional Neural Networks for Breast
Cancer Histology Image Analysis

Alexander Rakhlin1, Alexey Shvets2, Vladimir Iglovikov3,
and Alexandr A. Kalinin4

1 Neuromation, St. Petersburg 191025, Russia
rakhlin@neuromation.io
2 Massachusetts Institute of Technology, Cambridge, MA 02142, USA
shvets@mit.edu
3 Lyft Inc., San Francisco, CA 94107, USA
iglovikov@gmail.com
4 University of Michigan, Ann Arbor, MI 48109, USA
akalinin@umich.edu

Abstract. Breast cancer is one of the main causes of cancer death
worldwide. Early diagnostics signiÔ¨Åcantly increases the chances of correct
treatment and survival, but this process is tedious and often leads to a
disagreement between pathologists. Computer-aided diagnosis systems
showed potential for improving the diagnostic accuracy. In this work, we
develop the computational approach based on deep convolution neural
networks for breast cancer histology image classiÔ¨Åcation. Hematoxylin and
eosin stained breast histology microscopy image dataset is provided as a
part of the ICIAR 2018 Grand Challenge on Breast Cancer Histology Im-
ages. Our approach utilizes several deep neural network architectures and
gradient boosted trees classiÔ¨Åer. For 4-class classiÔ¨Åcation task, we report
87.2% accuracy. For 2-class classiÔ¨Åcation task to detect carcinomas we re-
port 93.8% accuracy, AUC 97.3%, and sensitivity/speciÔ¨Åcity 96.5/88.0%
at the high-sensitivity operating point. To our knowledge, this approach
outperforms other common methods in automated histopathological im-
age classiÔ¨Åcation. The source code for our approach is made publicly
available at https://github.com/alexander-rakhlin/ICIAR2018

Keywords: Medical imaging, Computer-aided diagnosis (CAD), Computer vi-
sion, Image recognition, Deep learning

1

Introduction

Breast cancer is the most common cancer diagnosed among US women (excluding
skin cancers), accounting for 30% of all new cancer diagnoses in women in the
United States [1]. Breast tissue biopsies allow the pathologists to histologically
assess the microscopic structure and elements of the tissue. Histopathology aims
to distinguish between normal tissue, non-malignant (benign) and malignant
lesions (carcinomas) and to perform a prognostic evaluation [2]. A combination of

8
1
0
2
 
r
p
A
 
3
 
 
]

V
C
.
s
c
[
 
 
2
v
2
5
7
0
0
.
2
0
8
1
:
v
i
X
r
a

2

Rakhlin et al.

hematoxylin and eosin (H&E) is the principal stain of tissue specimens for routine
histopathological diagnostics. There are multiple types of breast carcinomas that
embody characteristic tissue morphology, see Fig. 1. Breast carcinomas arise from
the mammary epithelium and cause a pre-malignant epithelial proliferation within
the ducts, called ductal carcinoma in situ. Invasive carcinoma is characterized by
the cancer cells gaining the capacity to break through the basal membrane of
the duct walls and inÔ¨Åltrate into surrounding tissues [3].

Morphology of tissue, cells, and subcellular compartments is regulated by
complex biological mechanisms related to cell diÔ¨Äerentiation, development, and
cancer [4]. Traditionally, morphological assessment and tumor grading were
visually performed by the pathologist, however, this process is tedious and
subjective, causing inter-observer variations even among senior pathologists [5, 6].
The subjectivity of the application of morphological criteria in visual classiÔ¨Åcation
motivates the use of computer-aided diagnosis (CAD) systems to improve the
diagnosis accuracy, reduce human error, increase the level of inter-observer
agreement, and increased reproducibility [3].

There are many methods developed for the digital pathology image analysis,
from rule-based to applications of machine learning [3]. Recently, deep learning
based approaches were shown to outperform conventional machine learning
methods in many image analysis task, automating end-to-end processing [7‚Äì9]. In
the domain of medical imaging, convolutional neural networks (CNN) have been
successfully used for diabetic retinopathy screening [10], bone disease prediction
[11] and age assessment [12], and other problems [7]. Previous deep learning-based
applications in histological microscopic image analysis have demonstrated their
potential to provide utility in diagnosing breast cancer [3, 13‚Äì15].

In this paper, we present an approach for histology microscopy image analysis
for breast cancer type classiÔ¨Åcation. Our approach utilizes deep CNNs for feature
extraction and gradient boosted trees for classiÔ¨Åcation and, to our knowledge,
outperforms other similar solutions.

2 Methods

2.1 Dataset

The image dataset is an extension of the dataset from [13] and consists of 400
H&E stain images (2048 √ó 1536 pixels). All the images are digitized with the
same acquisition conditions, with a magniÔ¨Åcation of 200√ó and pixel size of
0.42ùúáùëö √ó 0.42ùúáùëö. Each image is labeled with one of the four balanced classes:
normal, benign, in situ carcinoma, and invasive carcinoma, where class is deÔ¨Åned
as a predominant cancer type in the image, see Fig. 1. The image-wise annotation
was performed by two medical experts [16]. The goal of the challenge is to provide
an automatic classiÔ¨Åcation of each input image.

2.2 Approach overview

The limited size of the dataset (400 images of 4 classes) poses a signiÔ¨Åcant chal-
lenge for the training of a deep learning model [7]. Very deep CNN architectures

Deep Learning for Breast Cancer Histology

3

Fig. 1: Examples of microscopic biopsy images in the dataset: (A) normal; (B)
benign; (C) in situ carcinoma; and (D) invasive carcinoma

that contain millions of parameters such as VGG, Inception and ResNet have
achieved the state-of-the-art results in many computer vision tasks [17]. However,
training these neural networks from scratch requires a large number of images,
as training on a small dataset leads to overÔ¨Åtting i.e. inability to generalize
knowledge. A typical remedy in these circumstances is called Ô¨Åne-tuning when
only a part of the pre-trained neural network is being Ô¨Åtted to a new dataset.
However, in our experiments, Ô¨Åne-tuning approach did not demonstrate good
performance on this task. Therefore, we employed a diÔ¨Äerent approach known as
deep convolutional feature representation [18]. To this end, deep CNNs, trained
on large and general datasets like ImageNet (10M images, 20K classes) [19], are
used for unsupervised feature representation extraction. In this study, breast
histology images are encoded with the state-of-the-art, general purpose networks
to obtain sparse descriptors of low dimensionality (1408 or 2048). This unsuper-
vised dimensionality reduction step signiÔ¨Åcantly reduces the risk of overÔ¨Åtting on
the next stage of supervised learning.

We use LightGBM as a fast, distributed, high performance implementation of
gradient boosted trees for supervised classiÔ¨Åcation [20]. Gradient boosting models
are being extensively used in machine learning due to their speed, accuracy, and
robustness against overÔ¨Åtting [21].

2.3 Data pre-processing and augmentation

To bring the microscopy images into a common space to enable improved quan-
titative analysis, we normalize the amount of H&E stained on the tissue as
described in [22]. For each image, we perform 50 random color augmentations.
Following [23] the amount of H&E is adjusted by decomposing the RGB color
of the tissue into H&E color space, followed by multiplying the magnitude of
H&E of every pixel by two random uniform variables from the range [0.7, 1.3].
Furthermore, in our initial experiments, we used diÔ¨Äerent image scales, the orig-
inal 2048 √ó 1536 pixels and downscaled in half to 1024 √ó 768 pixels. From the
images of the original size we extract random crops of two sizes 800 √ó 800 and
1300 √ó 1300. From the downscaled images we extract crops of 400 √ó 400 pixels and
650 √ó 650 pixels. Lately, we found downscaled images is enough. Thereby, each
image is represented by 20 crops. The crops are then encoded into 20 descriptors.

4

Rakhlin et al.

Fig. 2: An overview of the pre-processing pipeline.

Then, the set of 20 descriptors is combined through 3-norm pooling [24] into a
single descriptor:

dùëùùëúùëúùëô =

(Ô∏É

1
ùëÅ

ùëÅ
‚àëÔ∏Å

ùëñ=1

)Ô∏É 1

ùëù

(dùëñ)ùëù

,

(1)

where the hyperparameter ùëù = 3 as suggested in [24, 25], ùëÅ is the number of
crops, dùëñ is descriptor of a crop and dùëùùëúùëúùëô is pooled descriptor of the image. The
p-norm of a vector gives the average for ùëù = 1 and the max for ùëù ‚Üí ‚àû. As a
result, for each original image, we obtain 50 (number of color augmentations) √ó2
(crop sizes) √ó3 (CNN encoders) = 300 descriptors.

2.4 Feature extraction

Overall pre-processing pipeline is depicted in Fig. 2. For features extraction, we
use standard pre-trained ResNet-50, InceptionV3 and VGG-16 networks from
Keras distribution [26]. We remove fully connected layers from each model to
allow the networks to consume images of an arbitrary size. In ResNet-50 and
InceptionV3, we convert the last convolutional layer consisting of 2048 channels
via GlobalAveragePooling into a one-dimensional feature vector with a length of
2048. With VGG-16 we apply the GlobalAveragePooling operation to the four
internal convolutional layers: block2, block3, block4, block5 with 128, 256, 512,
512 channels respectively. We concatenate them into one vector with a length of
1408, see Fig. 3.

Deep Learning for Breast Cancer Histology

5

Fig. 3: Schematic overview of the network architecture for deep feature extraction.

2.5 Training

We split the data into 10 stratiÔ¨Åed folds to preserve class distribution. Augmen-
tations increase the size of the dataset √ó300 (2 patch sizes x 3 encoders x 50
color/aÔ¨Éne augmentations). Nevertheless, the descriptors of a given image remain
correlated. To prevent information leakage, all descriptors of the same image
must be contained in the same fold. For each combination of the encoder, crop
size and scale we train 10 gradient boosting models with 10-fold cross-validation.
In addition to obtaining cross-validated results, this allows us to increase the
diversity of the models with limited data (bagging). Furthermore, we recycle each
dataset 5 times with diÔ¨Äerent random seeds in LightGBM adding augmentation
on the model level. As a result, we train 10 (number of folds) √ó5 (seeds) √ó4
(scale and crop) √ó3 (CNN encoders) = 600 gradient boosting models. At the
cross-validation stage, we predict every fold only with the models not trained
on this fold. For the test data, we similarly extract 300 descriptors for each
image and use them with all models trained for particular patch size and encoder.
The predictions are averaged over all augmentations and models. Finally, the
predicted class is deÔ¨Åned by the maximum probability score.

3 Results

To validate the approach we use 10-fold stratiÔ¨Åed cross-validation.

For 2-class non-carcinomas (normal and benign) vs. carcinomas (in situ and
invasive) classiÔ¨Åcation accuracy was 93.8¬±2.3%, the area under the ROC curve

6

Rakhlin et al.

Fig. 4: a) Non-carcinoma vs. carcinoma classiÔ¨Åcation, ROC. High sensitivity set-
point=0.33 (green): 96.5% sensitivity and 88.0% speciÔ¨Åcity to detect carcinomas.
Setpoint=0.50 (blue): 93.0% sensitivity and 94.5% speciÔ¨Åcity b) Confusion ma-
trix, without normalization. Vertical axis - ground truth, horizontal - predictions.

Table 1: Accuracy (%) and standard deviation for 4-class classiÔ¨Åcation evaluated
over 10 folds via cross-validation. Results for the blended model is in the bot-
tom. Model name represented as ‚ü®CNN‚ü©-‚ü®crop size‚ü©, thereby VGG-650 denotes
LightGBM trained on deep features extracted from 650x650 crops with VGG-16
encoder. Each column in the table corresponds to the fold number.

f1

f2

f3

f4

f5

f6

f7

f8

f9

f10 mean std

92.0 77.5 86.5 87.5 79.5 84.0 85.0 83.0 84.0 82.5 84.2 4.2
ResNet-400
91.0 77.5 86.0 89.5 81.0 74.0 85.5 83.0 84.5 82.5 83.5 5.2
ResNet-650
87.5 83.0 81.5 84.0 84.0 82.5 80.5 82.0 87.5 83.0 83.6 2.9
VGG-400
VGG-650
4.4
89.5 85.5 78.5 85.0 81.0 78.0 81.5 85.5 89.0 80.5 83.4
Inception-400 93.0 86.0 71.5 92.0 85.0 84.5 82.5 79.0 79.5 76.5 83.0 6.5
Inception-650 91.0 84.5 73.5 90.0 84.0 81.0 82.0 84.5 78.0 77.0 82.5 5.5
std
Model fusion 92.5 82.5 87.5 87.5 87.5 90.0 85.0 87.5 87.5 85.0 87.2 2.6

2.8 2.0 3.7

1.8 3.5 5.7

3.9 2.7

1.8 2.1

3.0

-

was 0.973, see Fig.4a. At high sensitivity setpoint 0.33 the sensitivity of the
model to detect carcinomas was 96.5% and speciÔ¨Åcity 88.0%. At the setpoint
0.50 the sensitivity of the model was 93.0% and speciÔ¨Åcity 94.5%, Fig. 4a. Out
of 200 carcinomas cases only 9 in situ and 5 invasive were missed, Fig.4b.

Table 1 shows classiÔ¨Åcation accuracy for 4-class classiÔ¨Åcation. Accuracy av-
eraged across all folds was 87.2¬±2.6%. Finally, the importance of strong aug-
mentation and model fusion we use is particularly evident from the Table 1. The
fused model accuracy is by 4-5% higher than any of its individual constituents.

Deep Learning for Breast Cancer Histology

7

The standard deviation of the ensemble across 10 folds is twice as low than the
average standard deviation of the individual models. Moreover, all our results in
the Table 1 are slightly improved by averaging across 5 seeded models.

4 Conclusions

In this paper, we propose a simple and eÔ¨Äective method for the classiÔ¨Åcation
of H&E stained histological breast cancer images in the situation of very small
training data (few hundred samples). To increase the robustness of the classiÔ¨Åer
we use strong data augmentation and deep convolutional features extracted at
diÔ¨Äerent scales with publicly available CNNs pretrained on ImageNet. On top of it,
we apply highly accurate and prone to overÔ¨Åtting implementation of the gradient
boosting algorithm. Unlike some previous works, we purposely avoid training
neural networks on this amount of data to prevent suboptimal generalization.

To our knowledge, the reported results are superior to the automated analysis

of breast cancer images reported in literature [13‚Äì15].

Acknowledgments The authors thank the Open Data Science community [27]
for useful suggestions and other help aiding the development of this work.

References

1. Rebecca Siegel, Kimberly D. Miller and Ahmedin Jemal, Cancer statistics, 2018,

CA: A Cancer Journal for Clinicians, 68 (1), 7‚Äì30, 2018.

2. Christopher W. Elston and Ian O. Ellis, Pathological prognostic factors in breast
cancer. I. The value of histological grade in breast cancer: experience from a large
study with long-term follow-up, Histopathology, 19 (5), 403‚Äì410, 1991.

3. Stephanie Robertson, Hossein Azizpour, Kevin Smith and Johan Hartman, Digital
image analysis in breast pathology‚Äîfrom image processing techniques to artiÔ¨Åcial
intelligence, Translational Research", 1931-5244, 2017.

4. Alexandr A. Kalinin, Ari Allyn-Feuer, Alex Ade, Gordon-Victor Fon, Walter Meixner,
David Dilworth, R JeÔ¨Ärey, Gerald A Higgins, Gen Zheng, Amy Creekmore and others
3D cell nuclear morphology: microscopy imaging dataset and voxel-based morphometry
classiÔ¨Åcation results, bioRxiv, 208207, 2017.

5. John Meyer, Alvarez, Consuelo, Clara Milikowski and Neal Olson, Irma Russo, Jose
Russo, Andrew Glass, Barbara Zehnbauer, Karen Lister and Reza Parwaresch, Breast
carcinoma malignancy grading by Bloom‚ÄìRichardson system vs proliferation index:
reproducibility of grade and advantages of proliferation index, Modern pathology, 18
(8), 1067, 2005.

6. Joann G. Elmore, Gary M. Longton, Patricia A. Carney, Berta M. Geller, and Tracy
Onega, Anna NA Tosteson, Heidi D. Nelson, Margaret S. Pepe, Kimberly H. Allison,
Stuart J. Schnitt and others Diagnostic concordance among pathologists interpreting
breast biopsy specimens, JAMA, 313 (11), 1122‚Äì1132, 2015.

7. Travers Ching, Daniel S. Himmelstein, Brett K. Beaulieu-Jones, Alexandr A. Kalinin,
Brian T. Do, Gregory P. Way, Enrico Ferrero, Paul-Michael Agapow, Wei Xie, Gail
L. Rosen, and others, Opportunities And Obstacles For Deep Learning In Biology
And Medicine, bioRxiv, 142760, 2017.

8

Rakhlin et al.

8. Vladimir Iglovikov, Sergey Mushinskiy and Vladimir Osin, Satellite Imagery Fea-
ture Detection using Deep Convolutional Neural Network: A Kaggle Competition,
arXiv:1706.06169, 2017

9. Vladimir Iglovikov and Alexey Shvets, TernausNet: U-Net with VGG11 Encoder

Pre-Trained on ImageNet for Image Segmentation, arXiv:1801.05746, 2018.

10. Alexander Rakhlin, Diabetic Retinopathy detection through integration of Deep

Learning classiÔ¨Åcation framework, bioRxiv, 225508, 2017

11. Aleksei Tiulpin, J√©r√¥me Thevenot, Esa Rahtu, Petri Lehenkari and Simo Saarakkala,
Automatic Knee Osteoarthritis Diagnosis from Plain Radiographs: A Deep Learning-
Based Approach, ScientiÔ¨Åc Reports, 8, 1727, 2018.

12. Vladimir Iglovikov, Alexander Rakhlin, Alexandr A. Kalinin and Alexey Shvets,
Pediatric Bone Age Assessment Using Deep Convolutional Neural Networks, arXiv
preprint arXiv:1712.05053, 2017.

13. Teresa Ara√∫jo, Guilherme Aresta, Eduardo Castro, Jos√© Rouco, Paulo Aguiar,
Catarina Eloy, Ant√≥nio Pol√≥nia and Aur√©lio Campilho, ClassiÔ¨Åcation of breast cancer
histology images using Convolutional Neural Networks, PloS one, 12, 6, e0177544,
2017.

14. Fabio Alexandre Spanhol, Luiz S. Oliveira, Caroline Petitjean and Laurent Heutte,
Breast cancer histopathological image classiÔ¨Åcation using convolutional neural net-
works, Neural Networks (IJCNN), 2560‚Äì2567, 2016.

15. Babak Ehteshami Bejnordi, Mitko Veta, Paul Johannes van Diest, Bram van
Ginneken, Nico Karssemeijer, Geert Litjens, Jeroen AWM van der Laak, Meyke
Hermsen, Quirine F Manson, Maschenka Balkenhol and others, Diagnostic assessment
of deep learning algorithms for detection of lymph node metastases in women with
breast cancer, JAMA, 318, 22, 2199‚Äì2210, 2017.

16. ICIAR 2018 Grand Challenge on Breast Cancer Histology Images, https://iciar2018-

challenge.grand-challenge.org/.

17. Christian Szegedy, Sergey IoÔ¨Äe, Vincent Vanhoucke and Alexander A. Alemi,
Inception-v4, inception-resnet and the impact of residual connections on learning,
arXiv:1602.07261v2, 2016

18. Yanming Guo, Yu Liu, Ard Oerlemans, Songyang Lao, Song. Wu and Michael S.
Lew, Deep learning for visual understanding: A review, Neurocomputing, 187, 27‚Äì48,
2016.

19. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li and Li Fei-Fei, Imagenet: A
large-scale hierarchical image database, Computer Vision and Pattern Recognition,
248‚Äì255, 2009.

20. Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei
Ye and Tie-Yan Liu, LightGBM: A highly eÔ¨Écient gradient boosting decision tree,
Advances in Neural Information Processing Systems, 3149‚Äì3157, 2017.

21. Alexey Natekin and Alois Knoll, Gradient boosting machines, a tutorial, Frontiers

in neurorobotics, 7, 21, 2013

22. Marc Macenko, Marc Niethammer, JS Marron, David Borland, John T. Woosley,
Xiaojun Guan, Charles Schmitt and Nancy E. Thomas, A method for normalizing
histology slides for quantitative analysis, Biomedical Imaging: From Nano to Macro
(ISBI‚Äô09), 1107‚Äì1110, 2009.

23. Arnout C. Ruifrok, Dennis A. Johnston and others, QuantiÔ¨Åcation of histochemical
staining by color deconvolution, Analytical and Quantitative Cytology and Histology,
23 (4), 291‚Äì299, 2001.

24. Y-Lan Boureau, Jean Ponce and Yann LeCun, A theoretical analysis of feature
pooling in visual recognition, Proceedings of the 27th international conference on
machine learning (ICML-10), 111‚Äì118, 2010.

Deep Learning for Breast Cancer Histology

9

25. Yan Xu, Zhipeng Jia, Yuqing Ai, Fang Zhang, Maode Lai, I Eric and Chao Chang,
Deep convolutional activation features for large scale brain tumor histopathology
image classiÔ¨Åcation and segmentation, Acoustics, Speech and Signal Processing
(ICASSP), 947‚Äì951, 2015.

26. Fran√ßois Chollet, Deep Learning with Python, Manning Publications, 2017.
27. Open Data Science (ODS), https://ods.ai.

Deep Convolutional Neural Networks for Breast
Cancer Histology Image Analysis

Alexander Rakhlin1, Alexey Shvets2, Vladimir Iglovikov3,
and Alexandr A. Kalinin4

1 Neuromation, St. Petersburg 191025, Russia
rakhlin@neuromation.io
2 Massachusetts Institute of Technology, Cambridge, MA 02142, USA
shvets@mit.edu
3 Lyft Inc., San Francisco, CA 94107, USA
iglovikov@gmail.com
4 University of Michigan, Ann Arbor, MI 48109, USA
akalinin@umich.edu

Abstract. Breast cancer is one of the main causes of cancer death
worldwide. Early diagnostics signiÔ¨Åcantly increases the chances of correct
treatment and survival, but this process is tedious and often leads to a
disagreement between pathologists. Computer-aided diagnosis systems
showed potential for improving the diagnostic accuracy. In this work, we
develop the computational approach based on deep convolution neural
networks for breast cancer histology image classiÔ¨Åcation. Hematoxylin and
eosin stained breast histology microscopy image dataset is provided as a
part of the ICIAR 2018 Grand Challenge on Breast Cancer Histology Im-
ages. Our approach utilizes several deep neural network architectures and
gradient boosted trees classiÔ¨Åer. For 4-class classiÔ¨Åcation task, we report
87.2% accuracy. For 2-class classiÔ¨Åcation task to detect carcinomas we re-
port 93.8% accuracy, AUC 97.3%, and sensitivity/speciÔ¨Åcity 96.5/88.0%
at the high-sensitivity operating point. To our knowledge, this approach
outperforms other common methods in automated histopathological im-
age classiÔ¨Åcation. The source code for our approach is made publicly
available at https://github.com/alexander-rakhlin/ICIAR2018

Keywords: Medical imaging, Computer-aided diagnosis (CAD), Computer vi-
sion, Image recognition, Deep learning

1

Introduction

Breast cancer is the most common cancer diagnosed among US women (excluding
skin cancers), accounting for 30% of all new cancer diagnoses in women in the
United States [1]. Breast tissue biopsies allow the pathologists to histologically
assess the microscopic structure and elements of the tissue. Histopathology aims
to distinguish between normal tissue, non-malignant (benign) and malignant
lesions (carcinomas) and to perform a prognostic evaluation [2]. A combination of

8
1
0
2
 
r
p
A
 
3
 
 
]

V
C
.
s
c
[
 
 
2
v
2
5
7
0
0
.
2
0
8
1
:
v
i
X
r
a

2

Rakhlin et al.

hematoxylin and eosin (H&E) is the principal stain of tissue specimens for routine
histopathological diagnostics. There are multiple types of breast carcinomas that
embody characteristic tissue morphology, see Fig. 1. Breast carcinomas arise from
the mammary epithelium and cause a pre-malignant epithelial proliferation within
the ducts, called ductal carcinoma in situ. Invasive carcinoma is characterized by
the cancer cells gaining the capacity to break through the basal membrane of
the duct walls and inÔ¨Åltrate into surrounding tissues [3].

Morphology of tissue, cells, and subcellular compartments is regulated by
complex biological mechanisms related to cell diÔ¨Äerentiation, development, and
cancer [4]. Traditionally, morphological assessment and tumor grading were
visually performed by the pathologist, however, this process is tedious and
subjective, causing inter-observer variations even among senior pathologists [5, 6].
The subjectivity of the application of morphological criteria in visual classiÔ¨Åcation
motivates the use of computer-aided diagnosis (CAD) systems to improve the
diagnosis accuracy, reduce human error, increase the level of inter-observer
agreement, and increased reproducibility [3].

There are many methods developed for the digital pathology image analysis,
from rule-based to applications of machine learning [3]. Recently, deep learning
based approaches were shown to outperform conventional machine learning
methods in many image analysis task, automating end-to-end processing [7‚Äì9]. In
the domain of medical imaging, convolutional neural networks (CNN) have been
successfully used for diabetic retinopathy screening [10], bone disease prediction
[11] and age assessment [12], and other problems [7]. Previous deep learning-based
applications in histological microscopic image analysis have demonstrated their
potential to provide utility in diagnosing breast cancer [3, 13‚Äì15].

In this paper, we present an approach for histology microscopy image analysis
for breast cancer type classiÔ¨Åcation. Our approach utilizes deep CNNs for feature
extraction and gradient boosted trees for classiÔ¨Åcation and, to our knowledge,
outperforms other similar solutions.

2 Methods

2.1 Dataset

The image dataset is an extension of the dataset from [13] and consists of 400
H&E stain images (2048 √ó 1536 pixels). All the images are digitized with the
same acquisition conditions, with a magniÔ¨Åcation of 200√ó and pixel size of
0.42ùúáùëö √ó 0.42ùúáùëö. Each image is labeled with one of the four balanced classes:
normal, benign, in situ carcinoma, and invasive carcinoma, where class is deÔ¨Åned
as a predominant cancer type in the image, see Fig. 1. The image-wise annotation
was performed by two medical experts [16]. The goal of the challenge is to provide
an automatic classiÔ¨Åcation of each input image.

2.2 Approach overview

The limited size of the dataset (400 images of 4 classes) poses a signiÔ¨Åcant chal-
lenge for the training of a deep learning model [7]. Very deep CNN architectures

Deep Learning for Breast Cancer Histology

3

Fig. 1: Examples of microscopic biopsy images in the dataset: (A) normal; (B)
benign; (C) in situ carcinoma; and (D) invasive carcinoma

that contain millions of parameters such as VGG, Inception and ResNet have
achieved the state-of-the-art results in many computer vision tasks [17]. However,
training these neural networks from scratch requires a large number of images,
as training on a small dataset leads to overÔ¨Åtting i.e. inability to generalize
knowledge. A typical remedy in these circumstances is called Ô¨Åne-tuning when
only a part of the pre-trained neural network is being Ô¨Åtted to a new dataset.
However, in our experiments, Ô¨Åne-tuning approach did not demonstrate good
performance on this task. Therefore, we employed a diÔ¨Äerent approach known as
deep convolutional feature representation [18]. To this end, deep CNNs, trained
on large and general datasets like ImageNet (10M images, 20K classes) [19], are
used for unsupervised feature representation extraction. In this study, breast
histology images are encoded with the state-of-the-art, general purpose networks
to obtain sparse descriptors of low dimensionality (1408 or 2048). This unsuper-
vised dimensionality reduction step signiÔ¨Åcantly reduces the risk of overÔ¨Åtting on
the next stage of supervised learning.

We use LightGBM as a fast, distributed, high performance implementation of
gradient boosted trees for supervised classiÔ¨Åcation [20]. Gradient boosting models
are being extensively used in machine learning due to their speed, accuracy, and
robustness against overÔ¨Åtting [21].

2.3 Data pre-processing and augmentation

To bring the microscopy images into a common space to enable improved quan-
titative analysis, we normalize the amount of H&E stained on the tissue as
described in [22]. For each image, we perform 50 random color augmentations.
Following [23] the amount of H&E is adjusted by decomposing the RGB color
of the tissue into H&E color space, followed by multiplying the magnitude of
H&E of every pixel by two random uniform variables from the range [0.7, 1.3].
Furthermore, in our initial experiments, we used diÔ¨Äerent image scales, the orig-
inal 2048 √ó 1536 pixels and downscaled in half to 1024 √ó 768 pixels. From the
images of the original size we extract random crops of two sizes 800 √ó 800 and
1300 √ó 1300. From the downscaled images we extract crops of 400 √ó 400 pixels and
650 √ó 650 pixels. Lately, we found downscaled images is enough. Thereby, each
image is represented by 20 crops. The crops are then encoded into 20 descriptors.

4

Rakhlin et al.

Fig. 2: An overview of the pre-processing pipeline.

Then, the set of 20 descriptors is combined through 3-norm pooling [24] into a
single descriptor:

dùëùùëúùëúùëô =

(Ô∏É

1
ùëÅ

ùëÅ
‚àëÔ∏Å

ùëñ=1

)Ô∏É 1

ùëù

(dùëñ)ùëù

,

(1)

where the hyperparameter ùëù = 3 as suggested in [24, 25], ùëÅ is the number of
crops, dùëñ is descriptor of a crop and dùëùùëúùëúùëô is pooled descriptor of the image. The
p-norm of a vector gives the average for ùëù = 1 and the max for ùëù ‚Üí ‚àû. As a
result, for each original image, we obtain 50 (number of color augmentations) √ó2
(crop sizes) √ó3 (CNN encoders) = 300 descriptors.

2.4 Feature extraction

Overall pre-processing pipeline is depicted in Fig. 2. For features extraction, we
use standard pre-trained ResNet-50, InceptionV3 and VGG-16 networks from
Keras distribution [26]. We remove fully connected layers from each model to
allow the networks to consume images of an arbitrary size. In ResNet-50 and
InceptionV3, we convert the last convolutional layer consisting of 2048 channels
via GlobalAveragePooling into a one-dimensional feature vector with a length of
2048. With VGG-16 we apply the GlobalAveragePooling operation to the four
internal convolutional layers: block2, block3, block4, block5 with 128, 256, 512,
512 channels respectively. We concatenate them into one vector with a length of
1408, see Fig. 3.

Deep Learning for Breast Cancer Histology

5

Fig. 3: Schematic overview of the network architecture for deep feature extraction.

2.5 Training

We split the data into 10 stratiÔ¨Åed folds to preserve class distribution. Augmen-
tations increase the size of the dataset √ó300 (2 patch sizes x 3 encoders x 50
color/aÔ¨Éne augmentations). Nevertheless, the descriptors of a given image remain
correlated. To prevent information leakage, all descriptors of the same image
must be contained in the same fold. For each combination of the encoder, crop
size and scale we train 10 gradient boosting models with 10-fold cross-validation.
In addition to obtaining cross-validated results, this allows us to increase the
diversity of the models with limited data (bagging). Furthermore, we recycle each
dataset 5 times with diÔ¨Äerent random seeds in LightGBM adding augmentation
on the model level. As a result, we train 10 (number of folds) √ó5 (seeds) √ó4
(scale and crop) √ó3 (CNN encoders) = 600 gradient boosting models. At the
cross-validation stage, we predict every fold only with the models not trained
on this fold. For the test data, we similarly extract 300 descriptors for each
image and use them with all models trained for particular patch size and encoder.
The predictions are averaged over all augmentations and models. Finally, the
predicted class is deÔ¨Åned by the maximum probability score.

3 Results

To validate the approach we use 10-fold stratiÔ¨Åed cross-validation.

For 2-class non-carcinomas (normal and benign) vs. carcinomas (in situ and
invasive) classiÔ¨Åcation accuracy was 93.8¬±2.3%, the area under the ROC curve

6

Rakhlin et al.

Fig. 4: a) Non-carcinoma vs. carcinoma classiÔ¨Åcation, ROC. High sensitivity set-
point=0.33 (green): 96.5% sensitivity and 88.0% speciÔ¨Åcity to detect carcinomas.
Setpoint=0.50 (blue): 93.0% sensitivity and 94.5% speciÔ¨Åcity b) Confusion ma-
trix, without normalization. Vertical axis - ground truth, horizontal - predictions.

Table 1: Accuracy (%) and standard deviation for 4-class classiÔ¨Åcation evaluated
over 10 folds via cross-validation. Results for the blended model is in the bot-
tom. Model name represented as ‚ü®CNN‚ü©-‚ü®crop size‚ü©, thereby VGG-650 denotes
LightGBM trained on deep features extracted from 650x650 crops with VGG-16
encoder. Each column in the table corresponds to the fold number.

f1

f2

f3

f4

f5

f6

f7

f8

f9

f10 mean std

92.0 77.5 86.5 87.5 79.5 84.0 85.0 83.0 84.0 82.5 84.2 4.2
ResNet-400
91.0 77.5 86.0 89.5 81.0 74.0 85.5 83.0 84.5 82.5 83.5 5.2
ResNet-650
87.5 83.0 81.5 84.0 84.0 82.5 80.5 82.0 87.5 83.0 83.6 2.9
VGG-400
VGG-650
4.4
89.5 85.5 78.5 85.0 81.0 78.0 81.5 85.5 89.0 80.5 83.4
Inception-400 93.0 86.0 71.5 92.0 85.0 84.5 82.5 79.0 79.5 76.5 83.0 6.5
Inception-650 91.0 84.5 73.5 90.0 84.0 81.0 82.0 84.5 78.0 77.0 82.5 5.5
std
Model fusion 92.5 82.5 87.5 87.5 87.5 90.0 85.0 87.5 87.5 85.0 87.2 2.6

2.8 2.0 3.7

1.8 3.5 5.7

3.9 2.7

1.8 2.1

3.0

-

was 0.973, see Fig.4a. At high sensitivity setpoint 0.33 the sensitivity of the
model to detect carcinomas was 96.5% and speciÔ¨Åcity 88.0%. At the setpoint
0.50 the sensitivity of the model was 93.0% and speciÔ¨Åcity 94.5%, Fig. 4a. Out
of 200 carcinomas cases only 9 in situ and 5 invasive were missed, Fig.4b.

Table 1 shows classiÔ¨Åcation accuracy for 4-class classiÔ¨Åcation. Accuracy av-
eraged across all folds was 87.2¬±2.6%. Finally, the importance of strong aug-
mentation and model fusion we use is particularly evident from the Table 1. The
fused model accuracy is by 4-5% higher than any of its individual constituents.

Deep Learning for Breast Cancer Histology

7

The standard deviation of the ensemble across 10 folds is twice as low than the
average standard deviation of the individual models. Moreover, all our results in
the Table 1 are slightly improved by averaging across 5 seeded models.

4 Conclusions

In this paper, we propose a simple and eÔ¨Äective method for the classiÔ¨Åcation
of H&E stained histological breast cancer images in the situation of very small
training data (few hundred samples). To increase the robustness of the classiÔ¨Åer
we use strong data augmentation and deep convolutional features extracted at
diÔ¨Äerent scales with publicly available CNNs pretrained on ImageNet. On top of it,
we apply highly accurate and prone to overÔ¨Åtting implementation of the gradient
boosting algorithm. Unlike some previous works, we purposely avoid training
neural networks on this amount of data to prevent suboptimal generalization.

To our knowledge, the reported results are superior to the automated analysis

of breast cancer images reported in literature [13‚Äì15].

Acknowledgments The authors thank the Open Data Science community [27]
for useful suggestions and other help aiding the development of this work.

References

1. Rebecca Siegel, Kimberly D. Miller and Ahmedin Jemal, Cancer statistics, 2018,

CA: A Cancer Journal for Clinicians, 68 (1), 7‚Äì30, 2018.

2. Christopher W. Elston and Ian O. Ellis, Pathological prognostic factors in breast
cancer. I. The value of histological grade in breast cancer: experience from a large
study with long-term follow-up, Histopathology, 19 (5), 403‚Äì410, 1991.

3. Stephanie Robertson, Hossein Azizpour, Kevin Smith and Johan Hartman, Digital
image analysis in breast pathology‚Äîfrom image processing techniques to artiÔ¨Åcial
intelligence, Translational Research", 1931-5244, 2017.

4. Alexandr A. Kalinin, Ari Allyn-Feuer, Alex Ade, Gordon-Victor Fon, Walter Meixner,
David Dilworth, R JeÔ¨Ärey, Gerald A Higgins, Gen Zheng, Amy Creekmore and others
3D cell nuclear morphology: microscopy imaging dataset and voxel-based morphometry
classiÔ¨Åcation results, bioRxiv, 208207, 2017.

5. John Meyer, Alvarez, Consuelo, Clara Milikowski and Neal Olson, Irma Russo, Jose
Russo, Andrew Glass, Barbara Zehnbauer, Karen Lister and Reza Parwaresch, Breast
carcinoma malignancy grading by Bloom‚ÄìRichardson system vs proliferation index:
reproducibility of grade and advantages of proliferation index, Modern pathology, 18
(8), 1067, 2005.

6. Joann G. Elmore, Gary M. Longton, Patricia A. Carney, Berta M. Geller, and Tracy
Onega, Anna NA Tosteson, Heidi D. Nelson, Margaret S. Pepe, Kimberly H. Allison,
Stuart J. Schnitt and others Diagnostic concordance among pathologists interpreting
breast biopsy specimens, JAMA, 313 (11), 1122‚Äì1132, 2015.

7. Travers Ching, Daniel S. Himmelstein, Brett K. Beaulieu-Jones, Alexandr A. Kalinin,
Brian T. Do, Gregory P. Way, Enrico Ferrero, Paul-Michael Agapow, Wei Xie, Gail
L. Rosen, and others, Opportunities And Obstacles For Deep Learning In Biology
And Medicine, bioRxiv, 142760, 2017.

8

Rakhlin et al.

8. Vladimir Iglovikov, Sergey Mushinskiy and Vladimir Osin, Satellite Imagery Fea-
ture Detection using Deep Convolutional Neural Network: A Kaggle Competition,
arXiv:1706.06169, 2017

9. Vladimir Iglovikov and Alexey Shvets, TernausNet: U-Net with VGG11 Encoder

Pre-Trained on ImageNet for Image Segmentation, arXiv:1801.05746, 2018.

10. Alexander Rakhlin, Diabetic Retinopathy detection through integration of Deep

Learning classiÔ¨Åcation framework, bioRxiv, 225508, 2017

11. Aleksei Tiulpin, J√©r√¥me Thevenot, Esa Rahtu, Petri Lehenkari and Simo Saarakkala,
Automatic Knee Osteoarthritis Diagnosis from Plain Radiographs: A Deep Learning-
Based Approach, ScientiÔ¨Åc Reports, 8, 1727, 2018.

12. Vladimir Iglovikov, Alexander Rakhlin, Alexandr A. Kalinin and Alexey Shvets,
Pediatric Bone Age Assessment Using Deep Convolutional Neural Networks, arXiv
preprint arXiv:1712.05053, 2017.

13. Teresa Ara√∫jo, Guilherme Aresta, Eduardo Castro, Jos√© Rouco, Paulo Aguiar,
Catarina Eloy, Ant√≥nio Pol√≥nia and Aur√©lio Campilho, ClassiÔ¨Åcation of breast cancer
histology images using Convolutional Neural Networks, PloS one, 12, 6, e0177544,
2017.

14. Fabio Alexandre Spanhol, Luiz S. Oliveira, Caroline Petitjean and Laurent Heutte,
Breast cancer histopathological image classiÔ¨Åcation using convolutional neural net-
works, Neural Networks (IJCNN), 2560‚Äì2567, 2016.

15. Babak Ehteshami Bejnordi, Mitko Veta, Paul Johannes van Diest, Bram van
Ginneken, Nico Karssemeijer, Geert Litjens, Jeroen AWM van der Laak, Meyke
Hermsen, Quirine F Manson, Maschenka Balkenhol and others, Diagnostic assessment
of deep learning algorithms for detection of lymph node metastases in women with
breast cancer, JAMA, 318, 22, 2199‚Äì2210, 2017.

16. ICIAR 2018 Grand Challenge on Breast Cancer Histology Images, https://iciar2018-

challenge.grand-challenge.org/.

17. Christian Szegedy, Sergey IoÔ¨Äe, Vincent Vanhoucke and Alexander A. Alemi,
Inception-v4, inception-resnet and the impact of residual connections on learning,
arXiv:1602.07261v2, 2016

18. Yanming Guo, Yu Liu, Ard Oerlemans, Songyang Lao, Song. Wu and Michael S.
Lew, Deep learning for visual understanding: A review, Neurocomputing, 187, 27‚Äì48,
2016.

19. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li and Li Fei-Fei, Imagenet: A
large-scale hierarchical image database, Computer Vision and Pattern Recognition,
248‚Äì255, 2009.

20. Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei
Ye and Tie-Yan Liu, LightGBM: A highly eÔ¨Écient gradient boosting decision tree,
Advances in Neural Information Processing Systems, 3149‚Äì3157, 2017.

21. Alexey Natekin and Alois Knoll, Gradient boosting machines, a tutorial, Frontiers

in neurorobotics, 7, 21, 2013

22. Marc Macenko, Marc Niethammer, JS Marron, David Borland, John T. Woosley,
Xiaojun Guan, Charles Schmitt and Nancy E. Thomas, A method for normalizing
histology slides for quantitative analysis, Biomedical Imaging: From Nano to Macro
(ISBI‚Äô09), 1107‚Äì1110, 2009.

23. Arnout C. Ruifrok, Dennis A. Johnston and others, QuantiÔ¨Åcation of histochemical
staining by color deconvolution, Analytical and Quantitative Cytology and Histology,
23 (4), 291‚Äì299, 2001.

24. Y-Lan Boureau, Jean Ponce and Yann LeCun, A theoretical analysis of feature
pooling in visual recognition, Proceedings of the 27th international conference on
machine learning (ICML-10), 111‚Äì118, 2010.

Deep Learning for Breast Cancer Histology

9

25. Yan Xu, Zhipeng Jia, Yuqing Ai, Fang Zhang, Maode Lai, I Eric and Chao Chang,
Deep convolutional activation features for large scale brain tumor histopathology
image classiÔ¨Åcation and segmentation, Acoustics, Speech and Signal Processing
(ICASSP), 947‚Äì951, 2015.

26. Fran√ßois Chollet, Deep Learning with Python, Manning Publications, 2017.
27. Open Data Science (ODS), https://ods.ai.

