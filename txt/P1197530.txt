5
1
0
2
 
n
u
J
 
9
2

 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
0
5
5
2
0
.
6
0
5
1
:
v
i
X
r
a

JMLR: Workshop and Conference Proceedings vol 40:1–27, 2015

Regret Lower Bound and Optimal Algorithm
in Dueling Bandit Problem

Junpei Komiyama
The University of Tokyo
Junya Honda
The University of Tokyo
Hisashi Kashima
Kyoto University
Hiroshi Nakagawa
The University of Tokyo

JUNPEI@KOMIYAMA.INFO

HONDA@STAT.T.U-TOKYO.AC.JP

KASHIMA@I.KYOTO-U.AC.JP

NAKAGAWA@DL.ITC.U-TOKYO.AC.JP

Abstract
We study the K-armed dueling bandit problem, a variation of the standard stochastic bandit prob-
lem where the feedback is limited to relative comparisons of a pair of arms. We introduce a tight
asymptotic regret lower bound that is based on the information divergence. An algorithm that is
inspired by the Deterministic Minimum Empirical Divergence algorithm (Honda and Takemura,
2010) is proposed, and its regret is analyzed. The proposed algorithm is found to be the ﬁrst one
with a regret upper bound that matches the lower bound. Experimental comparisons of dueling
bandit algorithms show that the proposed algorithm signiﬁcantly outperforms existing ones.
Keywords: multi-armed bandit problem, dueling bandit problem, online learning

1. Introduction

A multi-armed bandit problem is a crystallized instance of a sequential decision-making problem
in an uncertain environment, and it can model many real-world scenarios. This problem involves
conceptual entities called arms, and a forecaster who tries to identify good arms from bad ones. At
each round, the forecaster draws one of the K arms and receives a corresponding reward. The aim
of the forecaster is to maximize the cumulative reward over rounds, which is achieved by running an
algorithm that balances the exploration (acquisition of information) and the exploitation (utilization
of information).

While it is desirable to obtain direct feedback from an arm, in some cases such direct feedback is
not available. In this paper, we consider a version of the standard stochastic bandit problem called
the K-armed dueling bandit problem (Yue et al., 2009), in which the forecaster receives relative
feedback, which speciﬁes which of two arms is preferred. Although the original motivation of the
dueling bandit problem arose in the ﬁeld of information retrieval, learning under relative feedback
is universal to many ﬁelds, such as recommender systems (Gemmis et al., 2009), graphical design
(Brochu et al., 2010), and natural language processing (Zaidan and Callison-Burch, 2011), which
involve explicit or implicit feedback provided by humans.

c(cid:13) 2015 J. Komiyama, J. Honda, H. Kashima & H. Nakagawa.

KOMIYAMA HONDA KASHIMA NAKAGAWA

Related work: Here, we brieﬂy discuss the literature of the K-armed dueling bandit problem. The
RK×K, whose ij entry µi,j corresponds to the
problem involves a preference matrix M =
µi,j} ∈
{
probability that arm i is preferred to arm j.

j
j

max

≻
≻

⇔
≻

µ1,j, µj,k}
{

Most algorithms assume that the preference matrix has certain properties. Interleaved Filter (IF)
(Yue et al., 2012) and Beat the Mean Bandit (BTM) (Yue and Joachims, 2011), early algorithms
proposed for solving the dueling bandit problem, require the arms to be totally ordered, that is,
µi,j > 1/2. Moreover, IF assumes stochastic transitivity: for any triple (i, j, k) with
i
. Unfortunately, stochastic transitivity does not hold in many
k, µi,k ≥
i
µi,j, µj,k}
{
real-world settings (Yue and Joachims, 2011). BTM relaxes this assumption by introducing relaxed
k, γµ1,k ≥
stochastic transitivity: there exists γ
holds. The drawback of BTM is that it requires the explicit value of γ on which
max
the performance of the algorithm depends. Urvoy et al. (2013) considered a wide class of sequential
learning problems with bandit feedback that includes the dueling bandit problem. They proposed the
Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) algorithm, which empirically
outperforms IF and BTM for moderate K. Among the several versions of SAVAGE, the one called
Condorcet SAVAGE makes the Condorcet assumption and performed the best in their experiment.
The Condorcet assumption is that there is a unique arm that is superior to the others. Unlike the two
transitivity assumptions, the Condorcet assumption does not require the arms to be totally ordered
and is less restrictive. IF, BTM, and SAVAGE either explicitly require the number of rounds T , or
implicitly require T to determine the conﬁdence level δ.

1 such that for all pairs (j, k) with 1

≻

≥

≻

j

Recently, an algorithm called Relative Upper Conﬁdence Bound (RUCB) (Zoghi et al., 2014b)
was proven to have an O(K log T ) regret bound under the Condorcet assumption. RUCB is based on
the upper conﬁdence bound index (Lai and Robbins, 1985; Agrawal, 1995; Auer et al., 2002) that is
widely used in the ﬁeld of bandit problems. RUCB is horizonless: it does not require T beforehand
and runs for any duration. Zoghi et al. (2015) extended RUCB into the mergeRUCB algorithm
under the Condorcet assumption as well as the assumption that a portion of the preference matrix is
informative (i.e., different from 1/2). They reported that mergeRUCB outperformed RUCB when K
was large. Ailon et al. (2014) proposed three algorithms named Doubler, MultiSBM, and Sparring.
MultiSBM is endowed with an O(K log T ) regret bound and Sparring was reported to outperform
IF and BTM in their simulation. These algorithms assume that the pairwise feedback is generated
from the non-observable utilities of the selected arms. The existence of the utility distributions
associated with individual arms restricts the structure of the preference matrix.

In summary, most algorithms either has O(K 2 log T ) regret under the Condorcet assumption
(SAVAGE) or require additional assumptions to achieve O(K log T ) regret (IF, BTM, MultiSBM,
and mergeRUCB). To the best of our knowledge, RUCB is the only algorithm with an O(K log T )
regret bound1. The main difﬁculty of the dueling bandit problem lies in that, there are K
1
candidates of actions to test “how good” each arm i is. A naive use of the conﬁdence bound requires
every pair of arms to be compared O(log T ) times and yields an O(K 2 log T ) regret bound.
Contribution: In this paper, we propose an algorithm called Relative Minimum Empirical Diver-
gence (RMED). This paper contributes to our understanding of the dueling bandit problem in the
following three respects.

−

•

The regret lower bound: Some studies (e.g., Yue et al., 2012) have shown that the K-armed
dueling bandit problem has a Ω(K log T ) regret lower bound. In this paper, we further ana-

1. Zoghi et al. (2013) ﬁrst proposed RUCB with an O(K 2 log T ) regret bound and later modiﬁed it by adding a ran-

domization procedure to assure O(K log T ) regret in Zoghi et al. (2014b).

2

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

lyze this lower bound to obtain the optimal constant factor for models satisfying the Condorcet
assumption. Furthermore, we show that the lower bound is the same under the total order as-
sumption. This means that optimal algorithms under the Condorcet assumption also achieve
a lower bound of regret under the total order assumption even though such algorithms do not
know that the arms are totally ordered.
An optimal algorithm: The regret of RMED is not only O(K log T ), but also optimal in
the sense that its constant factor matches the asymptotic lower bound under the Condorcet
assumption. RMED is the ﬁrst optimal algorithm in the study of the dueling bandit problem.
Empirical performance assessment: The performance of RMED is extensively evaluated by
using ﬁve datasets: two synthetic datasets, one including preference data, and two including
ranker evaluations in the information retrieval domain.

•

•

2. Problem Setup

∈

.
The K-armed dueling bandit problem involves K arms that are indexed as [K] =
}
RK×K be a preference matrix whose ij entry µi,j corresponds to the probability that
Let M
arm i is preferred to arm j. At each round t = 1, 2, . . . , T , the forecaster selects a pair of arms
[K]2, then receives a relative feedback ˆXl(t),m(t)(t)
Bernoulli(µl(t),m(t)) that
(l(t), m(t))
indicates which of (l(t), m(t)) is preferred. By deﬁnition, µi,j = 1
[K]
and µi,i = 1/2.

∼
µj,i holds for any i, j

1, 2, . . . , K
{

−

∈

∈

Let Ni,j(t) be the number of comparisons of pair (i, j) and ˆµi,j(t) be the empirical estimate of
µi,j at round t. In building statistics by using the feedback, we treat pairs without taking their order
+ 1
t′=1(1
l(t′) =
l(t′) = i, m(t′) = j
into consideration. Therefore, for i
{
}
{
t−1
l(t′) = i, m(t′) = j, ˆXl(t′ ),m(t′)(t′) = 1
+ 1
t′=1(1
j, m(t′) = i
l(t′) =
) and µi,j = (
P
{
}
}
{
j, m(t′) = i, ˆXl(t′),m(t′)(t′) = 0
))/Ni,j (t), where 1[
= i, let
] is the indicator function. For j
P
·
}
Ni>j(t) be the number of times i is preferred over j. Then, ˆµi,j(t) = Ni>j(t)/Ni,j(t), where we
set 0/0 = 1/2 here. Let ˆµi,i(t) = 1/2.

= j, Ni,j(t) =

t−1

Throughout this paper, we will assume that the preference matrix has a Condorcet winner
.
(Urvoy et al., 2013). Here we call an arm i the Condorcet winner if µi,j > 1/2 for any j
i
}
\{
Without loss of generality, we will assume that arm 1 is the Condorcet winner. The set of preference
matrices which have a Condorcet winner is denoted by
MC. We also deﬁne the set of preference
matrices satisfying the total order by
µi,j < 1/2 induces
⇔
a total order iff

Mo ⊂ MC; that is, the relation i

[K]

≺

∈

j

Let ∆i,j = µi,j −

1/2. We deﬁne the regret per round as r(t) = (∆1,i + ∆1,j)/2 when the
pair (i, j) is compared. The expectation of the cumulative regret, E[R(T )] = E
is used
to measure the performance of an algorithm. The regret increases at each round unless the selected
pair is (l(t), m(t)) = (1, 1).

T
t=1 r(t)
i

hP

µi,j} ∈ Mo.
{

2.1. Regret lower bound in the K-armed dueling bandits

In this section we provide an asymptotic regret lower bound when T
. Let the superiors of
, that is, the set of arms that is preferred to i on average.
arm i be a set
[K], µi,j < 1/2
}
The essence of the K-armed dueling bandit problem is how to eliminate each arm i
by
making sure that arm i is not the Condorcet winner. To do so, the algorithm uses some of the arms
in

1
}
\ {

Oi =

→ ∞

j
{

[K]

j
|

∈

∈

Oi and compares i with them.

3

KOMIYAMA HONDA KASHIMA NAKAGAWA

A dueling bandit algorithm is strongly consistent for model

M ⊂ MC iff it has E[R(T )] =
. The following lemma is on the number of comparisons

o(T a) regret for any a > 0 and any M
of suboptimal arm pairs.

∈ M

Lemma 1 (The lower bound on the number of suboptimal arm draws) (i) Let an arm i
and preference matrix M
MC, we have

1
}
\ {
∈ MC be arbitrary. Given any strongly consistent algorithm for model

[K]

∈

E



q + (1


Xj∈Oi

−
Mo.

d(µi,j, 1/2)Ni,j (T )

(1

o(1)) log T,

(1)

≥




−

p) log 1−p
where d(p, q) = p log p
with parameters p and q. (ii) Furthermore, inequality (1) holds for any M
consistent algorithm for

1−q is the KL divergence between two Bernoulli distributions
∈ Mo given any strongly



∈ Oi, an algorithm needs to make log T /d(µi,j, 1/2)
Lemma 1 states that, for arbitrary arm j
comparisons between arms i and j to be convinced that arm i is inferior to arm j and thus i is not the
Condorcet winner. Since the regret increase per round of comparing arm i with j is (∆1,i + ∆1,j)/2,
eliminating arm i by comparing it with j incurs a regret of

(∆1,i + ∆1,j) log T
2d(µi,j, 1/2)

.

(2)

(3)

Therefore, the total regret is bounded from below by comparing each arm i with an arm j that
minimizes (2) and the regret lower bound is formalized in the following theorem.

Theorem 2 (The regret lower bound) (i) Let the preference matrix M
strongly consistent algorithm for model

∈ MC be arbitrary. For any

MC,

lim inf
T →∞

E[R(T )]

log T ≥

min
j∈Oi

∆1,i + ∆1,j
2d(µi,j, 1/2)

Xi∈[K]\{1}
holds. (ii) Furthermore, inequality (3) holds for any M
rithm for

Mo.

∈ Mo given any strongly consistent algo-

The proof of Lemma 1 and Theorem 2 can be found in Appendix B. The proof of Lemma 1 is
similar to that of Lai and Robbins (1985, Theorem 1) for the standard multi-armed bandit problem
but differs in the following point that is characteristic to the dueling bandit. To achieve a small regret
in the dueling bandit, it is necessary to compare the arm i with itself if i is the Condorcet winner.
However, we trivially know that µi,i = 1/2 without sampling and such a comparison yields no
information to distinguish possible preference matrices. We can avoid this difﬁculty by evaluating
Ni,j and Ni,i in different ways.

3. RMED1 Algorithm

In this section, we ﬁrst introduce the notion of empirical divergence. Then, on the basis of the
empirical divergence, we formulate the RMED1 algorithm.

4

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

Algorithm 1 Relative Minimum Empirical Divergence (RMED) Algorithm

1: Input: K arms, f (K)

0. α > 0 (RMED2FH, RMED2). T (RMED2FH).

2: L

1
← (
α log log T
⌈

⌉

≥
(RMED1, RMED2)
(RMED2FH)

.

For each arm i

3: Initial phase: draw each pair of arms L times. At the end of this phase, t = L(K
4: if RMED2FH then
5:
6: end if
7: LC, LR ←
8: while t
≤
9:

∈
.
[K], LN ← ∅
T do

[K], ﬁx ˆb⋆(i) by (6).

if RMED2 then

1)K/2.

−

Draw all pairs (i, j) until it reaches Ni,j(t)

α log log t. t

t + 1 for each draw.

≥

←

end if
for l(t)

∈

LC in an arbitrarily ﬁxed order do

Select m(t) by using

Algorithm 2
Algorithm 3

(RMED1)
(RMED2, RMED2FH)

.

(

.
l(t)
LR \ {
}
(without a duplicate) for any j /
j
LN ∪ {
∈

}

LR such that

Jj(t) holds.

Draw arm pair (l(t), m(t)).
LR ←
LN ←
t
←
end for
LC, LR ←

.
LN , LN ← ∅

t + 1.

19:
20: end while

10:

11:

12:

13:

14:

15:

16:

17:

18:

Algorithm 2 RMED1 subroutine for selecting m(t)
1: ˆ
j
Ol(t)(t)
[K]
l(t)
\ {
← {
∈
}|
ˆ
Ol(t)(t) or ˆ
2: if i∗(t)
Ol(t)(t) =
∈
i∗(t).
3: m(t)
←
4: else
5: m(t)
6: end if

arg minj6=l(t) ˆµl(t),j(t).

ˆµl(t),j(t)
then

1/2
}

←

≤

∅

3.1. Empirical divergence and likelihood function

In inequality (1) of Section 2.1, we have seen that
d(µi,j, 1/2)Ni,j (T ), the sum of the di-
vergence between µi,j and 1/2 multiplied by the number of comparisons between i and j, is the
characteristic value that deﬁnes the minimum number of comparisons. The empirical estimate of
this value is fundamentally useful for evaluating how unlikely arm i is to be the Condorcet winner.
Let the opponents of arm i at round t be the set ˆ
. Note that,
, ˆµi,j (t)
i
1/2
Oi(t) =
\{
}
}
unlike the superiors
Oi(t) for each arm i are deﬁned in terms of the empirical
averages, and thus the algorithms know who the opponents are. Let the empirical divergence be

Oi, the opponents ˆ

j
{

[K]

j∈Oi

j
|

P

≤

∈

Ii(t) =

Ni,j(t)d(ˆµi,j(t), 1/2).

Xj∈ ˆOi(t)

5

KOMIYAMA HONDA KASHIMA NAKAGAWA

−

Ii(t)) can be considered as the “likelihood” that arm i is the Condorcet winner. Let

The value exp (
i∗(t) = arg mini∈[K] Ii(t) (ties are broken arbitrarily) and I ∗(t) = Ii∗(t)(t). By deﬁnition, I ∗(t)
≥
0. RMED is inspired by the Deterministic Minimum Empirical Divergence (DMED) algorithm
(Honda and Takemura, 2010). DMED, which is designed for solving the standard K-armed bandit
problem, draws arms that may be the best one with probability Ω(1/t), whereas RMED in the
dueling bandit problem draws arms that are likely to be the Condorcet winner with probability
Ω(1/t). Namely, any arm i that satisﬁes

Ji(t) =

Ii(t)
{

−

I ∗(t)

log t + f (K)
}

≤

(4)

is the candidate of the Condorcet winner and will be drawn soon. Here, f (K) can be any non-
negative function of K that is independent of t. Algorithm 1 lists the main routine of RMED.
There are several versions of RMED. First, we introduce RMED1. RMED1 initially compares all
pairs once (initial phase). Let Tinit = (K
1)K/2 be the last round of the initial phase. From
t = Tinit + 1, it selects the arm by using a loop. LC = LC(t) is the set of arms in the current loop,
and LR = LR(t)
LC(t) is the remaining arms of LC that have not been drawn yet in the current
loop. LN = LN (t) is the set of arms that are going to be drawn in the next loop. An arm i is put
. By deﬁnition, at least one arm (i.e. i∗(t) at the end
into LN when it satisﬁes
of the current loop) is put into LN in each loop. For arm l(t) in the current loop, RMED1 selects
m(t) (i.e. the comparison target of l(t)) determined by Algorithm 2.

{Ji(t)

LR(t)

i /
∈

∩ {

}}

−

⊂

The following theorem, which is proven in Section 5, describes a regret bound of RMED1.

Theorem 3 For any sufﬁciently small δ > 0, the regret of RMED1 is bounded as:

E[R(T )]

≤

((1 + δ) log T + f (K))∆1,i
2d(µi,1, 1/2)

+ O(K 2) + O

+ O(KeAK−f (K)),

Xi∈[K]\{1}
µi,j}i,j∈[K]) is a constant as a function of T . Therefore, by letting δ = log−1/3 T
where A = A(
{
and choosing an f (K) = cK 1+ǫ for arbitrary c, ǫ > 0, we obtain

K
δ2

(cid:18)

(cid:19)

E[R(T )]

≤

Xi∈[K]\{1}

∆1,i log T
2d(µi,1, 1/2)

+ O(K 2+ǫ) + O(K log2/3 T ).

3.2. Gap between the constant factor of RMED1 and the lower bound
From the lower bound of Theorem 2, the O(K log T ) regret bound of RMED1 is optimal up to
a constant factor. Moreover, the constant factor matches the regret lower bound of Theorem 2 if
b⋆(i) = 1 for all i

where

[K]

∈

1
}
\ {

b⋆(i) = arg min

j∈Oi

∆1,i + ∆1,j
d(µi,j, 1/2)

.

(5)

Here we deﬁne d+(p, q) = d(p, q) if p < q and 0 otherwise, and x/0 = +
. Note that, there
can be ties that minimize the RHS of (5). In that case, we may choose any of the ties as b⋆(i) to
eliminate arm i. For ease of explanation, we henceforth will assume that b⋆(i) is unique, but our
results can be easily extended to the case of ties.

∞

We claim that b⋆(i) = 1 holds in many cases for the following mathematical and practical
= 1, is (∆1,i + ∆1,j)/2, whereas it is simply

reasons. (i) The regret of drawing a pair (i, j), j

6

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

Algorithm 3 Subroutine for selecting m(t) in RMED2 and RMED2FH

Update ˆb⋆(l(t)) by (6).

1: if RMED2 then
2:
3: end if
4: ˆ
Ol(t)(t)
5: if ˆb⋆(l(t))

j
← {

[K]

l(t)

∈
\ {
ˆ
Ol(t)(t) and
∈
ˆb⋆(l(t)).

←

6: m(t)
7: else
8:
9: end if

Select m(t) by using Algorithm 2.

≤

ˆµl(t),j(t)
}|
Nl(t),i∗(t)(t)
Nl(t),i∗(t)(t)

.
1/2
}
Nl(t),ˆb⋆(l(t))(t)/ log log t
Nl(t),ˆb⋆(l(t))(t)/ log log T (RMED2FH)

(RMED2)

(

≥

≥

then

≥

∆1,i/2 for the pair (i, 1). Thus, d+(µi,j, 1/2) has to be much larger than d+(µi,1, 1/2) in order to
satisfy b⋆(i) = j. (ii) The Condorcet winner usually wins over the other arms by a large margin,
and therefore, d+(µi,1, 1/2)
d+(µi,j, 1/2). For example, in the preference matrix of Example 1
(Table 1(a)), b⋆(3) = 1 as long as q < 0.79. Example 2 (Table 1(b)) is a preference matrix based
on six retrieval functions in the full-text search engine of ArXiv.org (Yue and Joachims, 2011)2. In
Example 2, b⋆(i) = 1 holds for all i, even though µ1,4 < µ2,4. In the case of a 16-ranker evalua-
tion based on the Microsoft Learning to Rank dataset (details are given in Section 4), occasionally
= 1 occurs, but the difference between the regrets of drawing arm 1 and b⋆(i) is fairly small
b⋆(i)
(smaller than 1.2% on average). Nevertheless, there are some cases in which comparing arm i with
1 is not such a clever idea. Example 3 (Table 1(c)) is a toy example in which comparing arm i with
b⋆(i)
= 1 makes a large difference. In Example 3, it is clearly better to draw pairs (2, 4), (3, 2) and
(4, 3) to eliminate arms 2, 3, and 4, respectively. Accordingly, it is still interesting to consider an
algorithm that reduces regret by comparing arm i with b⋆(i).

Table 1: Three preference matrices. In each example, the value at row i, column j is µi,j.

(a) Example 1
3
2
0.7
0.7
0.5
q
0.5
1-q

1
0.5
0.3
0.3

1
2
3

(b) Example 2

1
0.50
0.45
0.45
0.46
0.39
0.39

2
0.55
0.50
0.45
0.45
0.42
0.40

3
0.55
0.55
0.50
0.46
0.49
0.44

4
0.54
0.55
0.54
0.50
0.46
0.50

1
2
3
4
5
6

5
0.61
0.58
0.51
0.54
0.50
0.49

6
0.61
0.60
0.56
0.50
0.51
0.50

(c) Example 3

1
0.5
0.4
0.4
0.4

2
0.6
0.5
0.1
0.9

3
0.6
0.9
0.5
0.1

4
0.6
0.1
0.9
0.5

1
2
3
4

3.3. RMED2 Algorithm

We here propose RMED2, which gracefully estimates b⋆(i) during a bandit game and compares
arm i with b⋆(i). RMED2 and RMED1 share the main routine (Algorithm 1). The subroutine of
RMED2 for selecting m(t) is shown in Algorithm 3. Unlike RMED1, RMED2 keeps drawing pairs
of arms (i, j) at least α log log t times (Line 10 in Algorithm 1). The regret of this exploration is
insigniﬁcant since O(log log T ) = o(log T ). Once all pairs have been explored more than α log log t

2. In the original preference matrix of Yue and Joachims (2011), µ2,4 6= 1 − µ4,2. To satisfy µ2,4 = 1 − µ4,2, we

replaced µ2,4 and µ4,2 of the original with (µ2,4 − µ4,2 + 1)/2 and (µ4,2 − µ2,4 + 1)/2, respectively.

7

KOMIYAMA HONDA KASHIMA NAKAGAWA

times, RMED2 goes to the main loop. RMED2 determines m(t) by using Algorithm 2 based on the
estimate of b⋆(i) given by

ˆb⋆(i) = arg min
j∈[K]\{i}

ˆ∆i∗(t),i + ˆ∆i∗(t),j
d+(ˆµi,j(t), 1/2)

,

(6)

where ties are broken arbitrarily, ˆ∆i,j = 1/2
. Intuitively, RMED2
tries to select m(t) = ˆb⋆(i) for most rounds, and occasionally explores i∗(t) in order to reduce the
regret increase when RMED2 fails to estimate the true b⋆(i) correctly.

ˆµi,j and we set x/0 = +

∞

−

3.4. RMED2FH algorithm
Although we believe that the regret of RMED2 is optimal, the analysis of RMED2 is a little bit
complicated since it sometimes breaks the main loop and explores from time to time. For ease of
analysis, we here propose RMED2 Fixed Horizon (RMED2FH, Algorithm 1 and 3), which is a
“static” version of RMED2. Essentially, RMED2 and RMED2FH have the same mechanism. The
differences are that (i) RMED2FH conducts an α log log T exploration in the initial phase. After
the initial phase (ii) ˆb⋆(i) for each i is ﬁxed throughout the game. Note that, unlike RMED1 and
RMED2, RMED2FH requires the number of rounds T beforehand to conduct the initial α log log T
draws of each pair. The following Theorem shows the regret of RMED2FH that matches the lower
bound of Theorem 2.

Theorem 4 For any sufﬁciently small δ > 0, the regret of RMED2FH is bounded as:

E[R(T )]

≤

Xi∈[K]\{1}

(∆1,i + ∆1,b⋆(i))((1 + δ) log T )
2d(µi,b⋆(i), 1/2)

+ O(αK 2 log log T ) + O(KeAK−f (K))

K log T
log log T

+ O

(cid:18)

K
δ2

(cid:19)

(cid:18)

(cid:19)

+ O

+ O (Kf (K)) ,

(7)

where A = A(
µi,j}
{
choosing an f (K) = cK 1+ǫ (c, ǫ > 0) we obtain

) > 0 is a constant as a function of T . By setting δ = O((log T )−1/3) and

E[R(T )]

≤

Xi∈[K]\{1}

(∆1,i + ∆1,b⋆(i)) log T
2d(µi,b⋆(i), 1/2)

+ O(αK 2 log log T ) + O

+ O

K 2+ǫ

.

K log T
log log T

(cid:18)

(cid:19)

(cid:0)

(cid:1)
(8)

Note that all terms except the ﬁrst one in (8) are o(log T ). From Theorems 2 and 4 we see that (i)
RMED2FH is asymptotically optimal under the Condorcet assumption and (ii) the logarithmic term
on the regret bound of RMED2FH cannot be improved even if the arms are totally ordered and the
forecaster knows of the existence of the total order. The proof sketch of Theorem 4 is in Section 5.

4. Experimental Evaluation

To evaluate the empirical performance of RMED, we conducted simulations3 with ﬁve bandit
datasets (preference matrices). The datasets are as follows:

3. The source code of the simulations is available at https://github.com/jkomiyama/duelingbanditlib.

8

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

4

10

t
e
r
g
e
r
 
:
)
t
(
R

3

10

2

10

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

4

10

t
e
r
g
e
r
 
:
)
t
(
R

3

10

2

10

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

4

10

3

10

2

10

1

10

t
e
r
g
e
r
 
:
)
t
(
R

5

10

4

10

3

10

t
e
r
g
e
r
 
:
)
t
(
R

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

4

10

t
e
r
g
e
r
 
:
)
t
(
R

3

10

2

10

6

10

5

10

4

10

t
e
r
g
e
r
 
:
)
t
(
R

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

1

10

2

10

3

10

4

10
t: round

(a) Six rankers

5

10

6

10

2

10

3

10

5

10

6

10

4

10
t: round

(b) Cyclic

1

10

2

10

3

10

4

10
t: round

5

10

6

10

(c) Arithmetic

1

10

2

10

3

10

4

10

5

10

2

10

3

10

4

10

5

10
t: round

6

10

7

10

3

10

3

10

4

10

5

10
t: round

6

10

7

10

(e) MSLR K = 16

(f ) MSLR K = 64

t: round

(d) Sushi

Figure 1: Regret-round log-log plots of algorithms.

Six rankers is the preference matrix based on the six retrieval functions in the full-text search engine
of ArXiv.org (Table 1(b)).
Cyclic is the artiﬁcial preference matrix shown in Table 1(c). This matrix is designed so that the
comparison of i with 1 is not optimal.
Arithmetic dataset involves eight arms with µi,j = 0.5 + 0.05(j
Sushi dataset is based on the Sushi preference dataset (Kamishima, 2003) that contains the pref-
erences of 5, 000 Japanese users as regards 100 types of sushi. We extracted the 16 most popular
types of sushi and converted them into arms with µi,j corresponding to the ratio of users who prefer
sushi i over j. The Condorcet winner is the mildly-fatty tuna (chu-toro).
MSLR: We tested submatrices of a 136
136 preference matrix from Zoghi et al. (2015), which is
derived from the Microsoft Learning to Rank (MSLR) dataset (Microsoft Research, 2010; Qin et al.,
2010) that consists of relevance information between queries and documents with more than 30K
queries. Zoghi et al. (2015) created a ﬁnite set of rankers, each of which corresponds to a ranking
feature in the base dataset. The value µi,j is the probability that the ranker i beats ranker j based on
the navigational click model (Hofmann et al., 2013). We randomly extracted K = 16, 64 rankers
in our experiments and made sub preference matrices. The probability that the Condorcet winner

i) and has a total order.

−

×

9

KOMIYAMA HONDA KASHIMA NAKAGAWA

500

400

300

200

100

t
e
r
g
e
r
 
:
)
t
(
R

RMED1
RMED2
RMED2FH a=1
RMED2FH a=3
RMED2FH a=10
LB1
TrueLB

100

80

60

40

20

t
e
r
g
e
r
 
:
)
t
(
R

RMED1
RMED2
RMED2FH a=1
RMED2FH a=3
RMED2FH a=10
LB1
TrueLB

2000

1500

1000

500

t
e
r
g
e
r
 
:
)
t
(
R

RMED1
RMED2
RMED2FH a=1
RMED2FH a=3
RMED2FH a=10
LB1
TrueLB

0
10

2

3

10

4

10
t: round

5

10

6

10

0
10

2

3

10

4

10
t: round

5

10

6

10

0
10

2

3

10

5

4

10
10
t: round

6

10

7

10

(a) Six rankers

(b) Cyclic

(c) MSLR K = 16

Figure 2: Regret-round semilog plots of RMED compared with theoretical bounds. We set f (K) =

0.3K 1.01 for all algorithms, and α = 3 for RMED2.

exists in the subset of the rankers is high (more than 90%, c.f. Figure 1 in Zoghi et al. (2014a)), and
we excluded the relatively small case where the Condorcet winner does not exist.

A Condorcet winner exists in all datasets. In the experiments, the regrets of the algorithms were

averaged over 1, 000 runs (Six rankers, Cyclic, Arithmetic, and Sushi), or 100 runs (MSLR).

4.1. Comparison among algorithms

We compared the IF, BTM with γ = 1.2, RUCB with α = 0.51, Condorcet SAVAGE with δ = 1/T ,
MultiSBM and Sparring with α = 3, and RMED algorithms. There are two versions of RUCB:
the one that uses a randomizer in choosing l(t) (Zoghi et al., 2014b), and the one that does not
(Zoghi et al., 2013). We implemented both and found that the two perform quite similarly: we show
the result of the former one in this paper. We set f (K) = 0.3K 1.01 for all RMED algorithms and
set α = 3 for RMED2 and RMED2FH. The effect of f (K) is studied in Appendix A. Note that IF
and BTM assume a total order among arms, which is not the case with the Cyclic, Sushi, and MSLR
datasets. MultiSBM and Sparring assume the existence of the utility of each arm, which does not
allow a cyclic preference that appears in the Cyclic dataset.

Figure 1 plots the regrets of the algorithms. In all datasets RMED signiﬁcantly outperforms
RUCB, the next best excluding the different versions of RMED. Notice that the plots are on a base
10 log-log scale. In particular, regret of RMED1 is more than twice smaller than RUCB on all
datasets other than Cyclic, in which RMED2 performs much better. Among the RMED algorithms,
RMED1 outperforms RMED2 and RMED2FH on all datasets except for Cyclic, in which comparing
arm i
= 1 with arm 1 is inefﬁcient. RMED2 outperforms RMED2FH in the ﬁve of six datasets: this
could be due to the fact that RMED2FH does not update ˆb⋆(i) for ease of analysis.

4.2. RMED and asymptotic bound

Figure 2 compares the regret of RMED with two asymptotic bounds. LB1 denotes the regret bound
of RMED1. TrueLB is the asymptotic regret lower bound given by Theorem 2.
, the slope of RMED1 should converge to LB1, and the ones
RMED1 and RMED2: When T
of RMED2 and RMED2FH should converge to TrueLB. On Six rankers, LB1 is exactly the same

→ ∞

10

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

as TrueLB, and the slope of RMED1 converges to this TrueLB. In Cyclic, the slope of RMED2
converges to TrueLB, whereas that of RMED1 converges to LB1, from which we see that RMED2
is actually able to estimate b⋆(i)
= 1 correctly. In MSLR K = 16, LB1 and TrueLB are very close
(the difference is less than 1.2%), and RMED1 and RMED2 converge to these lower bounds.
RMED2FH with different values of α: We also tested RMED2FH with several values of α. On
the one hand, with α = 1, the initial phase of RMED2FH is too short to identify b⋆(i); as a result
it performs poorly on the Cyclic dataset. On the other hand, with α = 10, the initial phase was
too long, which incurs a practically non-negligible regret on the MSLR K = 16 dataset. We also
tested several values of parameter α in RMED2FH. We omit plots of RMED2 with α = 1, 10 for
the sake of readability, but we note that in our datasets the performance of RMED2 is always better
than or comparable with the one of RMED2FH under the same choice of α, although the optimality
of RMED2 is not proved unlike RMED2FH.

5. Regret Analysis

This section provides two lemmas essential for the regret analysis of RMED algorithms and proves
the asymptotic optimality of RMED1 based on these lemmas. A proof sketch on the optimal regret
of RMED2FH is also given.

The crucial property of RMED is that, by constantly comparing arms with the opponents, the

true Condorcet winner (arm 1) actually beats all the other arms with high probability. Let

U

(t) =

.
ˆµ1,i(t) > 1/2
}
{
\i∈[K]\{1}
ˆµ1,i(t) < 1/2 for all i

[K]

U

(t), ˆµi,1(t) = 1

Under
(t)
∈
implies that i∗(t) = arg mini∈[K] Ii(t) is unique with i∗(t) = 1 and I ∗(t) = I1(t) = 0. Lemma
c(t) occurs is constant in T , where the
5 below shows that the average number of rounds that
superscript c denotes the complement.

, and thus, Ii(t) > 0. Therefore,

1
}
\ {

−

U

U

Lemma 5 When RMED1 or RMED2FH is run, the following inequality holds:

T

E


Xt=Tinit+1


c(t)

1

{U

}



= O(eAK−f (K)),

(9)

where A = A(
µi,j}
{

) > 0 is a constant as a function of T .

times in the initial phase, we deﬁne
Note that, since RMED2FH draws each pair
1)K/2 for RMED2FH. We give a proof of this lemma in Appendix C.
(K
Tinit =
⌉
Intuitively, this lemma can be proved from the facts that arm 1 is drawn within roughly eI1(t)−f (K)
rounds and I1(t) is not very large with high probability.

α log log T
⌈

α log log T
⌈

−

⌉

Next, for i

[K]

∈

1
}
\ {

and j

∈ Oi, let
i,j (δ) =

N Suf

(1 + δ) log T + f (K)
d(µi,j, 1/2)

+ 1,

which is a sufﬁcient number of comparisons of i with j to be convinced that the arm i is not the
Condorcet winner. The following lemma states that if pair (i, j) is drawn N Suf
i,j (δ) times then i is
rarely selected as l(t) again.

11

KOMIYAMA HONDA KASHIMA NAKAGAWA

Lemma 6 When RMED1 or RMED2FH is run, for i

[K]

∈

, j

1
}
\ {

∈ Oi,

T

E


Xt=Tinit+1


1
l(t) = i, Ni,j(t)
{

≥

N Suf

i,j (δ)

= O

+ O(eAK−f (K)) + K.

1
δ2

(cid:18)

(cid:19)

}



We prove this lemma in Appendix D based on the Chernoff bound.

Now we can derive the regret bound of RMED1 based on these lemmas.

Proof of Theorem 3: Since
be decomposed as:

U

(t) implies m(t) = 1 in RMED1, the regret increase per round can

r(t) = 1

c(t)
}

+

{U

∆1,i
2

1
l(t) = i, m(t) = 1,
{

.
(t)
}

U

(10)

Xi∈[K]\{1}

Using Lemmas 5 and 6, we obtain

E[R(T )]

Tinit +

≤

K(K
2

−

≤

1)

+E



1)

K(K
2

−

≤

T

[r(t)]

Xt=Tinit+1
T

c(t)

1

{U

Xt=Tinit+1
+ O(eAK−f (K)) +



∆1,i
2  

+
Xi∈[K]\{1}

}



∆1,i
2

(cid:18)

Xi∈[K]\{1}

T

t=1
X

1
δ2

(cid:18)

(cid:19)

N Suf

i,1 (δ)+

1[l(t) = i, m(t) = 1, Ni,1(t)

N Suf

i,1 (δ)]

!

N Suf

i,1 (δ) + O

+ O(eAK−f (K)) + K

,

≥

(cid:19)

which immediately completes the proof of Theorem 3.

We also prove Theorem 4 on the optimality of RMED2FH based on Lemmas 5 and 6. Because

∈

[K]

1
}
\ {

. There exists C2 > 0 such that, for each l(t) = i, (i) with probability 1

c(t) does
(t), we decompose the regret into the contributions

the full proof in Appendix E is a little bit lengthy, here we give its brief sketch.
Proof sketch of Theorem 4 (RMED2FH): Similar to Theorem 3, we use the fact that the
not occur very often (i.e., Lemma 5). Under
of each arm i
−
O((log T )−C2) RMED2FH successfully estimates ˆb⋆(i) = b⋆(i) and selects m(t) = b⋆(i) for most
rounds. The optimal O(log T ) term comes from the comparison of i and b⋆(i). Arm 1 is also drawn
for O(log T / log log T ) = o(log T ) times. On the other hand, (ii) with probability O((log T )−C2),
RMED2FH fails to estimate b⋆(i) correctly. By occasionally comparing arm i with arm 1, we
can bound the regret increase by O(log T log log T ). Since O((log T )−C2
log T log log T ) =
o(log T ), this regret does not affect the O(log T ) factor.

×

U

U

6. Discussion

We proved the regret lower bound in the dueling bandit problem. The RMED algorithm is based
on the likelihood that the arm is the Condorcet winner. RMED is proven to have the matching
regret upper bound. The empirical evaluation revealed that RMED signiﬁcantly outperforms the
state-of-the-art algorithms. To conclude this paper, we mention three directions of future work.

12

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

First, when a Condorcet winner does not necessarily exist, the Copeland bandits (Urvoy et al.,
2013) are a natural extension of our problem. Thus, seeking an effective algorithm for solving this
problem will be interesting. As is well known in the ﬁeld of voting theory, there are several other
criteria of winners that are incompatible with the Condorcet / Copeland bandits, such as the Borda
winner (Urvoy et al., 2013). Comparing several criteria or developing an algorithm that outputs
more than one of these winners should be interesting directions of future work.

Second, another direction is sequential preference elicitation problems under relative feedback
that goes beyond the binary preference over pairs, such as multiscale feedback and/or preferences
among three or more items.

Third, in the standard bandit problem, it is reported that KL-UCB+ (Lai, 1987; Garivier and Capp´e,

2011) performs better than DMED. A study of a UCB-based optimal algorithm for the dueling ban-
dits can yield an algorithm that outperforms RMED.

Acknowledgements

We thank the anonymous reviewers for their useful comments. This work was supported in part by
JSPS KAKENHI Grant Number 26106506.

13

KOMIYAMA HONDA KASHIMA NAKAGAWA

References

R. Agrawal. Sample mean based index policies with O(log n) regret for the multi-armed bandit

problem. Advances in Applied Probability, 27:1054–1078, 1995.

Nir Ailon, Zohar Shay Karnin, and Thorsten Joachims. Reducing dueling bandits to cardinal ban-

dits. In ICML, pages 856–864, 2014.

Peter Auer, Nicol´o Cesa-bianchi, and Paul Fischer. Finite-time Analysis of the Multiarmed Bandit

Problem. Machine Learning, 47:235–256, 2002.

Eric Brochu, Tyson Brochu, and Nando de Freitas. A bayesian interactive optimization approach
In Proceedings of the 2010 Eurographics/ACM SIGGRAPH

to procedural animation design.
Symposium on Computer Animation, SCA 2010, Madrid, Spain, 2010, pages 103–112, 2010.

S´ebastien Bubeck. Bandits Games and Clustering Foundations. Theses, Universit´e des Sciences et

Technologie de Lille - Lille I, June 2010.

Aur´elien Garivier and Olivier Capp´e. The KL-UCB algorithm for bounded stochastic bandits and

beyond. In COLT, pages 359–376, 2011.

Marco De Gemmis, Leo Iaquinta, Pasquale Lops, Cataldo Musto, Fedelucio Narducci, and Giovanni
In In Preference Learning (PL-09)

Semeraro. Preference learning in recommender systems.
ECML/PKDD-09 Workshop, 2009.

Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. Fidelity, soundness, and efﬁciency of
interleaved comparison methods. Transactions on Information Systems, 31(4):17:1–43, 2013.

Junya Honda and Akimichi Takemura. An Asymptotically Optimal Bandit Algorithm for Bounded

Support Models. In COLT, pages 67–79, 2010.

Toshihiro Kamishima. Nantonac collaborative ﬁltering: recommendation based on order responses.

In KDD, pages 583–588, 2003.

1091–1114, 09 1987.

T. L. Lai. Adaptive treatment allocation and the multi-armed bandit problem. Ann. Statist., 15(3):

T. L. Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in

Applied Mathematics, 6(1):4–22, 1985.

Microsoft Research.

Microsoft Learning
to Rank Datasets,
http://research.microsoft.com/en-us/projects/mslr/.

2010.

URL

Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. LETOR: A benchmark collection for research on

learning to rank for information retrieval. Inf. Retr., 13(4):346–374, 2010.

Tanguy Urvoy, Fabrice Cl´erot, Rapha¨el Feraud, and Sami Naamane. Generic exploration and k-

armed voting bandits. In ICML, pages 91–99, 2013.

Yisong Yue and Thorsten Joachims. Beat the mean bandit. In ICML, pages 241–248, 2011.

14

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits

problem. In COLT, 2009.

Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits

problem. J. Comput. Syst. Sci., 78(5):1538–1556, 2012.

Omar Zaidan and Chris Callison-Burch. Crowdsourcing translation: Professional quality from non-
In The 49th Annual Meeting of the Association for Computational Linguistics
professionals.
(ACL): Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Port-
land, Oregon, USA, pages 1220–1229, 2011.

Masrour Zoghi, Shimon Whiteson, R´emi Munos, and Maarten de Rijke. Relative upper con-
ﬁdence bound for the k-armed dueling bandit problem. CoRR, abs/1312.3393, 2013. URL
http://arxiv.org/abs/1312.3393.

Masrour Zoghi, Shimon Whiteson, Maarten de Rijke, and R´emi Munos. Relative conﬁdence sam-

pling for efﬁcient on-line ranker evaluation. In WSDM, pages 73–82, 2014a.

Masrour Zoghi, Shimon Whiteson, R´emi Munos, and Maarten de Rijke. Relative upper conﬁdence

bound for the k-armed dueling bandit problem. In ICML, pages 10–18, 2014b.

Masrour Zoghi, Shimon Whiteson, and Maarten de Rijke. MergeRUCB: A method for large-scale

online ranker evaluation. In WSDM, 2015.

15

KOMIYAMA HONDA KASHIMA NAKAGAWA

c=0
c=0.1
c=0.3
c=1

5

10

4

10

3

10

M
0
1
=
T
 
t
a
 
)
T
(
R

10

2
16

32
64
K: # of arms

128

Figure 3: Performance of RMED1 algorithm with several values of c. The plot shows the regret at

T = 107 in the MSLR dataset with K = 16, 32, 64, and 128.

Appendix A. Experiment: Dependence on f (K)

P

c(t) implies a failure in identifying the Condorcet winner (i.e., 1

= i∗(t)). Although
The event
U
T
E[
c(t)] = O(eAK−f (K)) is a constant function of T for any non-negative f (K), this term
t=1 U
is not negligible with large K. To evaluate the effect of f (K), we set f (K) = cK 1.01 and studied
several values of c with the MSLR dataset (Figure 3). In the case of c = 0, the regret for K = 128
becomes 100 times that for K = 16, which implies that the exponential dependence O(eAK ) may
not be an artifact of the proof. On the other hand, the results for c = 0.1, 0.3, and 1 indicate that
this term can be much improved by simply letting c be a small positive value.

Appendix B. Proofs on Regret Lower Bound

B.1. Proof of Lemma 1

∈

[K]

be arbitrary and M =

Let i
be an arbitrary preference matrix. We consider a
modiﬁed preference matrix M ′ in which the probabilities related to arm i are different from M . Let
′
i, ij element
j
Oi ∪ {

1
\ {
}
[K], µi,j ≤

. For j
[K], µi,j = 1/2
}

, that is,
1/2
}

µi,j}
{

′
i =

′
i =

∈ O

j
|

O

∈

j
j
O
|
{
of M ′ is µ′

∈
i,j such that

d+(µi,j, µ′

i,j) = d(µi,j, 1/2) + ǫ.

(11)

Such a µ′
of the KL divergence. For j /
the modiﬁed bandit problem the Condorcet winner is not arm 1 but arm i. Moreover, if M
then M ′

i,j > 1/2 uniquely exists for sufﬁciently small ǫ > 0 by the monotonicity and continuity
i,j = µi,j. Note that, unlike the original bandit problem, in
∈ Mo

i, let µ′
′

∈ O

∈ Mo.

16

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

Notation: now, let ˆX m

i,j ∈ {

0, 1
}

be the result of m-th draw of the pair (i, j),

n

KLj(n) =

log

ˆX m
ˆX m

i,jµi,j + (1
i,jµ′
i,j + (1

 

−

ˆX m
ˆX m

i,j)(1
i,j)(1

−

µi,j)
µ′
i,j) !

,

Xm=1

−
KLj(Ni,j(T )), and P′, E′ be the probability and the expectation with respect to

c

−

KL =

and
the modiﬁed bandit game. Then, for any event

j∈O′
i

P

c

c

,

E
1

holds. Let us deﬁne the events

) = E

P′(
E

exp

KL

{E}

h

−

(cid:16)

(cid:17)i

c

(12)

Ni,j(T )d(µi,j, µ′

i,j) < (1

ǫ) log T, Ni,i(T ) < √T

,

−






D1 =

D2 =
D12 =
D1\2 =




i

Xj∈O′
KL


≤
n
(cid:16)
D1 ∩ D2,
c
c
2.
D1 ∩ D

ǫ
2

1

−

log T

,

(cid:17)

o

First step (P

= o(1)): From (12),

{D12}
P′(

D12)
By using this we have

E

1
{D12}

exp

≥

h

ǫ
2

1

−

−

(cid:16)

(cid:16)

log T

= T −(1−ǫ/2)P

.
{D12}

(cid:17)

(cid:17)i

(13)

P

{D12} ≤

T (1−ǫ/2)P′(

T (1−ǫ/2)P′

D12)
Ni,i(T ) < √T

≤

≤

≤

T (1−ǫ/2)P′

o
Ni,i(T ) > T

n

T
E′[T
n
T

−

−
−

Ni,i(T )]
√T

T (1−ǫ/2)

√T

−

o

(by the Markov inequality).

(14)

Since this algorithm is strongly consistent, E′[T
o(T a) for any a > 0. Therefore, the
−
RHS of the last line of (14) is o(T a−ǫ/2), which, by choosing sufﬁciently small a, converges to zero
as T
→ ∞
Second step (P

= o(1).
= o(1)): We have

. In summary, P

Ni,i(T )]

{D12}

→

{D1\2}

P

{D1\2}




Xj∈O′

i

= P

Ni,j(T )d(µi,j, µ′

i,j) < (1

ǫ) log T, Ni,i(T ) < √T ,

KLj(Ni,j(T )) >

1

log T

−

ǫ
2

−

(cid:16)

(cid:17)






i

Xj∈O′
ǫ
2

−

1

c

(cid:17)

(cid:16)

log T

.
)

P


(

≤

{nj }∈N|O′

i

|,Pj∈O′

i

max
nj d(µi,j ,µ′

i,j)<(1−ǫ) log T

KLj(nj) >

Xj∈O′

i

17

c

KOMIYAMA HONDA KASHIMA NAKAGAWA

Note that

max
1≤n≤N

KLj(n) = max
1≤n≤N

n

log

ˆX m
ˆX m

i,jµi,j + (1
i,jµ′
i,j + (1

 

ˆX m
ˆX m

i,j)(1
i,j)(1

−

−

µi,j)
µ′
i,j) !

,

−

−

m=1
X

is the maximum of the sum of positive-mean random variables, and thus converges to is average
(c.f., Lemma 10.5 in Bubeck, 2010). Namely,

c

lim
N→∞

max
1≤n≤N

KLj(n)
N

c

= d(µi,j, µ′

i,j)

a.s.

(15)

Let δ > 0 be sufﬁciently small. We have,

max

{nj }∈N|O′

i

|,Pj∈O′

i

njd(µi,j ,µ′

i,j )<(1−ǫ) log T

KLj(nj)

j∈O′
i

log T

P

c

max

{nj}∈N|O′

i

|,Pj∈O′

i

:nj >δ log T nj d(µi,j ,µ′

i,j )<(1−ǫ) log T

≤

log T

KLj(nj)

j∈O′
i

P

c

+

minj∈O′

i

δK
d(µi,j, µ′

.

i,j)

Combining this with the fact that (15) holds for any j, we have

max

{nj}∈N|O′

i

|,Pj∈O′

i

:nj >δ log T nj d(µi,j ,µ′

i,j )<(1−ǫ) log T

log T

KLj(nj)

j∈O′
i

P

c

1

ǫ

a.s.,

≤

−

max

{nj }∈N|O′

i

|,Pj∈O′

i

njd(µi,j ,µ′

i,j)<(1−ǫ) log T

KLj(nj)

j∈O′
i

log T

P

c

≤

−

1

ǫ + Θ(δ)

a.s. (16)

By using the fact that (16) holds almost surely for any sufﬁciently small δ > 0 and 1
we have

−

ǫ/2 > 1

ǫ,

−



{nj }∈N|O′

i

|,Pj∈O′

i

max
njd(µi,j ,µ′

i,j)<(1−ǫ) log T

P



Xj∈O′

i

c

KLj(nj) >

1

log T

= o(1).

ǫ
2

−

(cid:16)

(cid:17)





In summary, we obtain P
Last step: We here have

= o(1).

D1\2

(cid:8)

(cid:9)

D1 =

Ni,j(T )d(µi,j, µ′

i,j) < (1

ǫ) log T

Ni,i(T ) < √T

−

∩




n

o

=

Ni,j(T )(d(µi,j, 1/2) + ǫ) < (1

Ni,i(T ) < √T

(By (11))

lim sup
N→∞

and thus

lim sup
T →∞

Ni,j(T )(d(µi,j, 1/2) + ǫ) +

(1

−

ǫ) log T
√T


Ni,i(T ) < (1

ǫ) log T

,

−

(17)


ǫ) log T

−

∩




n

18

o













Xj∈O′

i

Xj∈O′

i

Xj∈O′

i


⊇ 




REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

where we used the fact that
Note that, by using the result of the previous steps, P
the complementary of this fact,

A < C
{

B < C

} ∩ {

} ⊇ {

A + B < C
}
= P
{D12}

{D1}

for A, B > 0 in the last line.
+ P
= o(1). By using

{D1\2}

Ni,j(T )(d(µi,j, 1/2) + ǫ) +

(1

−

ǫ) log T
√T

Ni,i(T )

(1

ǫ) log T

≥

−

P

≥

{D

c
1}

= 1

o(1).

−






Using the Markov inequality yields



P




Xj∈O′

i

E




Xj∈O′

i

Ni,j(T )(d(µi,j, 1/2) + ǫ) +

Ni,i(T )

(1

ǫ)(1

o(1)) log T.

(18)

(1

−

ǫ) log T
√T

≥




−

−



Because E[Ni,i(T )] is subpolynomial as a function of T due to the consistency, the second term in
LHS of (18) is o(1) and thus negligible. Lemma 1 follows from the fact that (18) holds for sufﬁ-
ciently small ǫ.



B.2. Proof of Theorem 2

We have

R(T ) =

1
2

∆1,i + ∆1,j
2

Ni,j(T ) +

∆1,i + ∆1,i
2

Ni,i(T )

Xi∈[K] Xj∈[K]\{i}

≥

≥

=

Xi,j∈[K]:µi,j<1/2

Xi∈[K]\{1} Xj∈Oi

Xi∈[K]\{1} Xj∈Oi

∆1,i + ∆1,j
2

Ni,j(T ) +

∆1,i + ∆1,i
2

Ni,i(T )

Xi∈[K]

Xi∈[K]

∆1,i + ∆1,j
2

Ni,j(T )

∆1,i + ∆1,j
2d(µi,j, 1/2)

d(µi,j, 1/2)Ni,j (T ).

Taking the expectation on both sides and using Lemma 1 yield

E[R(T )]

≥

Xi∈[K]\{1}

min
j∈Oi

∆1,i + ∆1,j
2d(µi,j, 1/2)

(1

o(1)) log T.

−

Appendix C. Proof of Lemma 5

This lemma essentially states that, the expected number of the rounds in which arm 1 is underesti-
mated is O(1). We show this by bounding the expected number of rounds before arm 1 is compared,

19

KOMIYAMA HONDA KASHIMA NAKAGAWA

for each ﬁxed set of
16 in Honda and Takemura (2010). Note that

N1,s(t)
}
{

and summing over

N1,s(t)
{

. This technique is inspired by Lemma
}

c(t) =

U

[S∈2[K]\{1}\{∅} (

\s∈S

ˆµ1,s(t)
{

≤

1/2

} ∩

ˆµ1,s(t) > 1/2
{

.

})

(19)

\s /∈S

Now we bound the number of rounds that the event

ˆµ1,s(t)
{

≤

1/2

} ∩

ˆµ1,s(t) > 1/2
}
{

\s∈S
occurs. Let N be the set of non-zero natural numbers, ns ∈
for each s
∈
1/2, d+(ˆµns
1,s, 1/2) = xs, N1,s(t) = ns}

N and xs ∈
i,j be the empirical estimate of µi,j at n-th draw of pair (i, j). If
S and ˆµ1,s(t) > 1/2 holds for s /
∈

S. Let ˆµn

holds for s

\s /∈S

∈

[0, log 2] be arbitrary

ˆµns
1,s ≤
{
S then

I1(t) =

nsd+(ˆµ1,s(t), 1/2)

Xs∈S

and therefore

J1(t) holds for any

exp

t

≥

 

nsd+(ˆµ1,s(t), 1/2)

f (K)

.

−

!

Xs∈S
J1(t) occurs, then arm 1 is in LN of the next loop, and thus for some s

If
within 2K rounds. Therefore we have

∈

S, N1,s is incremented

T

1

Xt=Tinit+1

"
\s∈S

ˆµ1,s(t)
{

1/2, N1,s(t) = ns} ∩

≤

ˆµ1,s(t) > 1/2
{

}#

\s /∈S

exp

≤

 

Xs∈S

nsd+(ˆµns

1,s, 1/2)

f (K)

+ 2K.

−

!

20

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

Letting Ps(xs) = Pr[ˆµns

1/2, d+(ˆµns

1,s, 1/2)

xs], we have

1,s ≤

≥

E

T

1


Xt=Tinit+1

=

"

\s∈S

Z{xs}∈[0,log 2]|S|  

ˆµ1,s(t)
{

≤

1/2, N1,s(t) = ns} ∩

ˆµ1,s(t) > 1/2
{

exp

 

Xs∈S

nsxs −

f (K)

+ 2K

!

d(

Ps(xs))

−

\s /∈S

!

Ys∈S

}#


(

(

(

(

(

= e−f (K)

2K

Ps(0) +

ensxsd(

Ps(xs))

−

)

= e−f (K)

2K

Ps(0) +

ensxsPs(xs)]log 2

0 +

Ys∈S

Ys∈S

Ys∈S Zxs∈[0,log 2]
[
−

Ys∈S  

(integration by parts)

nsensxsPs(xs)dxs

!)

Zxs∈[0,log 2]

≤

≤

e−f (K)

(1 + 2K)

Ps(0) +

nsensxse−ns(xs+C1(µ1,s,1/2))dxs

(by the Chernoff bound and Fact 10, where C1(µ, µ2) = (µ

Ys∈S Zxs∈[0,log 2]

e−f (K)

(1 + 2K)

e−nsd(1/2,µ1,s) +

µ2)2/(2µ(1

−

−
nse−nsC1(µ1,s,1/2)dxs

)

µ2)))

)

Ys∈S Zxs∈[0,log 2]

Ys∈S

)

= e−f (K)

(1 + 2K)

e−nsd(1/2,µ1,s) +

(log 2)nse−nsC1(µ1,s,1/2)

.

(20)

Ys∈S

Ys∈S

Ys∈S
,
ns}
{

By summing (20) over

T

P

Xt=Tinit+1

"
\s∈S

ˆµ1,s(t)
{

≤

1/2

} ∩

ˆµ1,s(t) > 1/2
{

}#

e−f (K)

(1 + 2K)

e−nsd(1/2,µ1,s) +

(log 2)nse−nsC1(µ1,s,1/2)

· · ·
X{ns}∈N|S|  
X

e−f (K)

(1 + 2K)

(

ed(1/2,µ1,s )

1

+ (log 2)|S|

where we used the fact that
(19) and the union bound over all S

P

∈

2[K]\{1}

, we obtain
P

\ {∅}

Ys∈S
−
∞
n=1 e−nx = 1/(ex + 1) and

Ys∈S
−
∞
n=1 ne−nx = ex/(ex + 1)2. Using

!

Ys∈S

eC1(µ1,s,1/2)

(eC1(µ1,s,1/2)

,

1)2 )

≤

≤

T

\s /∈S

Ys∈S
1

E


Xt=Tinit+1

< e−f (K)

c(t)

1

{U

}


(1 + 2K)






= O(eAK−f (K)),

1 +

1

ed(1/2,µ1,s )

1

(cid:19)

−

Ys∈[K]\{1} (cid:18)

+ (log 2)K−1

1 +

eC1(µ1,s,1/2)

Ys∈[K]\{1}  

(eC1(µ1,s,1/2)

1)2 !


−

(21)



21

KOMIYAMA HONDA KASHIMA NAKAGAWA

where A = log

maxs∈[K]\{1} max

1 +

1
ed(1/2,µ1,s )−1

, log 2

1 + eC1(µ1,s,1/2)

(eC1(µ1,s,1/2)−1)2

(cid:26)

(cid:18)

(cid:18)

.
(cid:19)(cid:19)(cid:27)

Appendix D. Proof of Lemma 6

Tinit + K + 1 (i.e., after
Except for the ﬁrst loop, arm i must put into LN before
the ﬁrst loop), let τ (t) < t be the round in the previous loop in which arm l(t) is put into LN . In the
round,
Tinit + K + 1 such
= τ (t2) holds because τ (t1) and τ (t2) belong to different
that l(t1) = l(t2) = i, t1 6
loops. By using τ (t), we obtain

Jl(t)(τ (t)) is satisﬁed. With this deﬁnition, for any two rounds t1, t2 ≥

. For t
l(t) = i
}
{

= t2 ⇒

τ (t1)

≥

T

1[l(t) = i, Ni,j(t)

N Suf

i,j (δ)]

≥

U

Xt=Tinit+1

K +

T

≤

≤

K +

Xt=Tinit+K+1
T
1[
U

Xt=Tinit+1

T

Xt=Tinit+K+1
1[l(t) = i,

c(t)] +

Xt=Tinit+K+1

1[l(t) = i,

c(τ (t))] +

1[l(t) = i,

(τ (t)), Ni,j(t)

U

N Suf

i,j (δ)]

≥

T

(τ (t)), Ni,j(t)

U

N Suf

i,j (δ)].

≥

Note that the expectation of term
t, the only round in which pair (i, j) can be compared is the round of
P
once, and thus Ni,j(t)
Ni,j(τ (t))

c(t)] is bounded by Lemma 5. Between τ (t) and
that occurs at most
l(t) = j
{
1. By using this fact, we obtain

T
t=Tinit+1

1[
U

}

−

T

Xt=Tinit+K+1

T

≤

U

≤

≤

T

Xt=Tinit+K+1
1[
Ji(t),

U

Xt=Tinit+1

1[l(t) = i,

(τ (t)), Ni,j(t)

N Suf

i,j (δ)]

≥

1[l(t) = i,

Ji(τ (t)),

U

(τ (t)), Ni,j(τ (t))

N Suf

i,j (δ)

1]

−

≥

(t), Ni,j (t)

N Suf

i,j (δ)

1].

−

≥

(22)

22

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

1[
Ji(t),

U

(t), Ni,j (t)

N Suf

i,j (δ)

1]

−

≥

We can bound this term via Ii(t) as

T

Xt=Tinit+1

T

T

i,j (δ)−1⌉

Xn=⌈N Suf
T

[t=Tinit+1(cid:16)
T

≤

≤

≤

≤

i,j (δ)−1⌉

Xn=⌈N Suf
T

i,j (δ)−1⌉

Xn=⌈N Suf
T

Xn=⌈N Suf

i,j (δ)−1⌉

1

1

1







h

(cid:20)

[t=Tinit+1(cid:16)


(N Suf

i,j (δ)

−

Ij(t)

log t + f (K), Ni,j(t) = n

≤

(by

(t)

U

⇒

I1(t) = 0)

Ni,j(t) = n, Ni,j(t)d+(ˆµn

i,j, 1/2)

log t + f (K)

(cid:17)





(cid:17)





≤

i

1)d+(ˆµn

i,j, 1/2)

log T + f (K)

1

d+(ˆµn

i,j, 1/2)

d(µi,j, 1/2)
1 + δ

≤

≤

.

(cid:21)

(23)

Therefore, by letting µ

(1/2, µi,j ) be a real number such that d(µ, 1/2) = d(µi,j ,1/2)

, we

1+δ

∈
obtain from the Chernoff bound and the monotonicity of d+(
, 1/2) that
·

T

E


Xt=Tinit+1


1[
Ji(t),

U

(t), Ni,j (t)

N Suf

i,j (δ)

1]

−



≤

≥

T

P

d+(ˆµn
(cid:20)

i,j, 1/2)

d(µi,j, 1/2)
1 + δ

≤

(cid:21)

i,j (δ)−1⌉

Xn=⌈N Suf
T



≤

Xn=⌈N Suf

i,j (δ)−1⌉
1

exp (

d(µ, µi,j)n)

−

≤

exp (d(µ, µi,j))

1

−

<

1
d(µ, µi,j)

.

From the Pinsker’s inequality it is easy to conﬁrm that d(µ, µi,j) = Ω(δ2), which completes the
proof.

Appendix E. Optimal Regret Bound: Full Proof of Theorem 4

Events: Deﬁne

Yi =

ˆµ⌈α log log T ⌉
i,j

{|

µi,j|

< ∆suf
i }

−

for sufﬁciently small but ﬁxed ∆suf
in µi,j that
µi,j}i,j∈[K]. Let also
{

Yi implies ˆb⋆(i) = b⋆(i) when we let ∆suf

\i,j∈[K]
i > 0. It is easy to see from the condinuity of d+(µi,j, 1/2)
i > 0 be sufﬁciently small with respect to

Zi(t) =

.
ˆµi,b⋆(i)(t) < 1/2
}
{

23

KOMIYAMA HONDA KASHIMA NAKAGAWA

First step (regret decomposition): Like RMED1, in RMED2FH E[
bility (i.e., Lemma 5). In the following, we bound the regret under

U
(t): let

(t)] holds with high proba-

U

{Y
(B)

{z

r(t)
Zi(t)
}

+ 1
l(t) = i,
{

U

(t),

c
i ∪ Z

c
i (t)

}}

r(t)

(24)

In the following, we ﬁrst bound the terms (A) and (B), and then summarizing all terms to prove

}

|

}

|

ri(t) = 1
l(t) = i,
{
= 1
l(t) = i,
{

r(t)
(t)
}
Yi,
(t),

U

U

(A)

{z

Theorem 4.
Second step (bounding (A)): Note that,
l(t) = i,
{
ˆb⋆(i) = b⋆(i) and ˆb⋆(i)
ˆ
Oi(t). Therefore,

∈

1
l(t) = i,
{

U

(t),

Yi,

r(t)
Zi(t)
}

(t),

Yi,

Zi(t)
}

U

is a sufﬁcient condition for

1
l(t) = i, Ni,b⋆(i)(t)
{

≥

N Suf

i,b⋆(i)(δ)

∆1,i + ∆1,b⋆(i)
2

+

}

N Suf

i,b⋆(i)(δ) +

∆1,i
2

N Suf

i,b⋆(i)(δ)
log log T

.

By applying Lemma 6 with j = b⋆(i), for sufﬁciently small δ > 0 we have

1
l(t) = i, Ni,b⋆(i)(t)
{

≥

N Suf

i,b⋆(i)(δ)

O

}

≤

+ O(eAK−f (K)) + K.

1
δ2

(cid:18)

(cid:19)



In summary, term (A) is bounded as:

T

Xt=Tinit+1
T

≤

Xt=Tinit+1

T

Xt=Tinit+1

E





T

E


Xt=Tinit+1


1
l(t) = i,
{

U

(t),

Yi,

r(t)
Zi(t)
}



∆1,i + ∆1,b⋆(i)
2

≤

N Suf

i,b⋆(i)(δ) + O



log T
log log T

1
δ2

+ O

+ O(eAK−f (K)) + K.

(25)

(cid:19)
. Under
Third step (bounding (B)): Now we consider the case
(t),
}}
this event ˆb⋆(i) = b⋆(i) does not always hold but we can see that m(t)
still holds.
Furthermore, under this event arm ˆb⋆(i) is selected as m(t) at most (log log T )Ni,1(T ) + 1 times

c
c
i (t)
i ∪ Z
ˆb⋆(i), 1
}

l(t) = i,
{

{Y
∈ {

(cid:18)

(cid:19)

(cid:18)

U

24

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

due to Line 5 of Algorithm 3. By using these facts, we have,

T

E


Xt=Tinit+1

T
E

≤

E

≤


Xt=Tinit+1

T


Xt=Tinit+1


1
l(t) = i,
{

U

(t),

c
i ∪ Z

c
i (t)

}}

{Y

r(t)





T

[t′=Tinit+1

i (t′)
c

Z

}}



1
l(t) = i,
{

U

(t),

c
i ∪

{Y

1
l(t) = i, Ni,1(t)
{

≥

N Suf

i,1 (δ)

}



T

T

+ P

c
i ∪

Y




i (t′)
c

Z

N Suf
(cid:16)




[t′=Tinit+1

i,1 (δ) log log T + 1 + N Suf

i,1 (δ)

(cid:17)

O

≤

1
δ2

(cid:18)

(cid:19)



+ O(eAK−f (K)) + K + P

(by Lemma 6).

c
i ∪

Y






i (t′)
c

Z

O

N Suf
(cid:16)

i,1 (δ) log log T

(cid:17)

[t′=Tinit+1






The following lemma bounds P

Lemma 7 For RMED2FH, there exists C2 = C2(
µi,j}
{

c
i ∪

Y

T
t′=Tinit+1 Z

n

.

c
i (t′)
o
, K, α) > 0 such that

S

T

P

c
i ∪

Y






[t=Tinit+1

c
i (t)

Z

= O((log T )−C2).

In summary, term (B) is bounded as:

1
l(t) = i,
{

U

(t),

c
i ∪ Z

c
i (t)

}}

{Y

r(t)

E


Xt=Tinit+1


O

≤

1
δ2

(cid:18)

(cid:19)

+ O(eAK−f (K)) + K + O

i,1 (δ)(log T )−C2 log log T

.

(26)

N Suf
(cid:16)

(cid:17)










25

P((A) + (B))

(by Lemma 5 and inequality (24))

≤

≤

+

≤

KOMIYAMA HONDA KASHIMA NAKAGAWA

Last step (regret bound):

T

E[R(T )]

Tinit +

≤

P



{U

c(t)
}

+

P

ri(t)
(t), l(t) = i
}



{U

T

Tinit +



Xt=Tinit+1

Xt=Tinit+1


O(eAK−f (K)) +

Xi∈[K]\{1}

Xi∈[K]\{1}


O(αK 2 log log T ) + O(eAK−f (K))

∆1,i + ∆1,b⋆(i)
2

N Suf

i,b⋆(i)(δ) + O

log T
log log T

(cid:18)

(cid:19)

Xi∈[K]\{1}(







1
δ2

(cid:18)
(by (25) and (26))

(cid:19)

+ O

+ O(eAK−f (K)) + 2K + O

i,1 (δ)(log T )−C2 log log T

N Suf
(cid:16)

)

(cid:17)

(∆1,i + ∆1,b⋆(i))((1 + δ) log T )
2d(µi,b⋆(i), 1/2)

Xi∈[K]\{1}

O(αK 2 log log T ) + O(KeAK−f (K)) +

K log T
log log T

+ O

(cid:18)

K
δ2

(cid:19)

(cid:18)

(cid:19)

(cid:0)

+ O

+ O

K(log T )1−C2 log log T

+ O (Kf (K)) .

(27)

Combining (27) with the fact that O

K(log T )1−C2 log log T

completes the proof.

(cid:0)

(cid:1)
= o

K log T
log log T

(cid:1)

(cid:16)

(cid:17)

E.1. Proof of Lemma 7
and P
We bound P

c
i }

{Y

T
t=Tinit+1 Z

c
i (t)
}

separately. On the one hand,

{
S
ˆµ⌈α log log T ⌉
i,j
|

P

c
i }

{Y

= P




[i,j∈[K]

µi,j| ≥

−

∆suf

P

ˆµ⌈α log log T ⌉
i,j

{|

≤

i 

(by the Chernoff bound and Pinsker’s inequality)


Xi,j∈[K]

−

µi,j| ≥

∆suf
i }

2 exp (

2(∆suf

i )2α log log T )

−

2 (log T )−2(∆suf

i )2α = 2K 2 (log T )−2(∆suf

i )2α = O((log T )−Ca),

≤

=


Xi,j∈[K]

Xi,j∈[K]

26

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

where Ca = 2(∆suf

i )2α/K 2 > 0. On the other hand,

T

[t=Tinit+1
T

P





= P

c
i (t)

Z






[t=Tinit+1
∞






Xn=⌈α log log T ⌉
∞

−

Xn=⌈α log log T ⌉
(log T )−αd(1/2,µi,b⋆ (i))

≤

≤

≤

≤

∞

ˆµi,b⋆(i)(t) < 1/2

≤




P





Ni,b⋆(i)(t) = n, ˆµn
{
[n=⌈α log log T ⌉

i,b⋆(i) < 1/2

}



P

Ni,b⋆(i)(t) = n, ˆµn
{



i,b⋆(i) < 1/2
}

exp (

d(1/2, µi,b⋆(i))n)

(by the Chernoff bound)

(log T )−αd(1/2,µi,b⋆ (i))

1 +

= O((log T )−Cb),

∞

Xn=0

(cid:18)

exp (

d(1/2, µi,b⋆(i))n)

−

1

d(1/2, µi,b⋆(i))

1

(cid:19)

−

where Cb = αd(1/2, µi,b⋆(i)) > 0. The proof is completed by letting C2 = min (Ca, Cb) and taking
the union bound of P
c
.
i (t)
}

T
t=Tinit+1 Z

and P

c
i }

{Y

{
S

Appendix F. Facts

Fact 8 (The Chernoff bound)
Let X1, . . . , Xn be i.i.d. binary random variables. Let ˆX = 1
n
any ǫ > 0,

n

i=1 Xi and µ = E[ ˆX]. Then, for

and

P( ˆX

≥

µ + ǫ)

exp (

d(µ + ǫ, µ)n)

≤

−

P

P( ˆX

µ

ǫ)

≤

−

≤

exp (

d(µ

ǫ, µ)n).

−

−

Fact 9 (The Pinsker’s inequality)
For p, q

∈

(0, 1), the KL divergence between two Bernoulli distributions is bounded as:

d(p, q)

2(p

≥

−

q)2.

Fact 10 (A minimum difference between divergences (Lemma 13 in Honda and Takemura, 2010))
For any µ and µ2 satisfying 0 < µ2 < µ < 1. Let C1(µ, µ2) = (µ
µ2)). Then, for
any µ3 ≤

µ2)2/(2µ(1

µ2,

−

−

d(µ3, µ)

d(µ3, µ2)

C1(µ, µ2) > 0.

−

≥

27

5
1
0
2
 
n
u
J
 
9
2

 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
0
5
5
2
0
.
6
0
5
1
:
v
i
X
r
a

JMLR: Workshop and Conference Proceedings vol 40:1–27, 2015

Regret Lower Bound and Optimal Algorithm
in Dueling Bandit Problem

Junpei Komiyama
The University of Tokyo
Junya Honda
The University of Tokyo
Hisashi Kashima
Kyoto University
Hiroshi Nakagawa
The University of Tokyo

JUNPEI@KOMIYAMA.INFO

HONDA@STAT.T.U-TOKYO.AC.JP

KASHIMA@I.KYOTO-U.AC.JP

NAKAGAWA@DL.ITC.U-TOKYO.AC.JP

Abstract
We study the K-armed dueling bandit problem, a variation of the standard stochastic bandit prob-
lem where the feedback is limited to relative comparisons of a pair of arms. We introduce a tight
asymptotic regret lower bound that is based on the information divergence. An algorithm that is
inspired by the Deterministic Minimum Empirical Divergence algorithm (Honda and Takemura,
2010) is proposed, and its regret is analyzed. The proposed algorithm is found to be the ﬁrst one
with a regret upper bound that matches the lower bound. Experimental comparisons of dueling
bandit algorithms show that the proposed algorithm signiﬁcantly outperforms existing ones.
Keywords: multi-armed bandit problem, dueling bandit problem, online learning

1. Introduction

A multi-armed bandit problem is a crystallized instance of a sequential decision-making problem
in an uncertain environment, and it can model many real-world scenarios. This problem involves
conceptual entities called arms, and a forecaster who tries to identify good arms from bad ones. At
each round, the forecaster draws one of the K arms and receives a corresponding reward. The aim
of the forecaster is to maximize the cumulative reward over rounds, which is achieved by running an
algorithm that balances the exploration (acquisition of information) and the exploitation (utilization
of information).

While it is desirable to obtain direct feedback from an arm, in some cases such direct feedback is
not available. In this paper, we consider a version of the standard stochastic bandit problem called
the K-armed dueling bandit problem (Yue et al., 2009), in which the forecaster receives relative
feedback, which speciﬁes which of two arms is preferred. Although the original motivation of the
dueling bandit problem arose in the ﬁeld of information retrieval, learning under relative feedback
is universal to many ﬁelds, such as recommender systems (Gemmis et al., 2009), graphical design
(Brochu et al., 2010), and natural language processing (Zaidan and Callison-Burch, 2011), which
involve explicit or implicit feedback provided by humans.

c(cid:13) 2015 J. Komiyama, J. Honda, H. Kashima & H. Nakagawa.

KOMIYAMA HONDA KASHIMA NAKAGAWA

Related work: Here, we brieﬂy discuss the literature of the K-armed dueling bandit problem. The
RK×K, whose ij entry µi,j corresponds to the
problem involves a preference matrix M =
µi,j} ∈
{
probability that arm i is preferred to arm j.

j
j

max

≻
≻

⇔
≻

µ1,j, µj,k}
{

Most algorithms assume that the preference matrix has certain properties. Interleaved Filter (IF)
(Yue et al., 2012) and Beat the Mean Bandit (BTM) (Yue and Joachims, 2011), early algorithms
proposed for solving the dueling bandit problem, require the arms to be totally ordered, that is,
µi,j > 1/2. Moreover, IF assumes stochastic transitivity: for any triple (i, j, k) with
i
. Unfortunately, stochastic transitivity does not hold in many
k, µi,k ≥
i
µi,j, µj,k}
{
real-world settings (Yue and Joachims, 2011). BTM relaxes this assumption by introducing relaxed
k, γµ1,k ≥
stochastic transitivity: there exists γ
holds. The drawback of BTM is that it requires the explicit value of γ on which
max
the performance of the algorithm depends. Urvoy et al. (2013) considered a wide class of sequential
learning problems with bandit feedback that includes the dueling bandit problem. They proposed the
Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) algorithm, which empirically
outperforms IF and BTM for moderate K. Among the several versions of SAVAGE, the one called
Condorcet SAVAGE makes the Condorcet assumption and performed the best in their experiment.
The Condorcet assumption is that there is a unique arm that is superior to the others. Unlike the two
transitivity assumptions, the Condorcet assumption does not require the arms to be totally ordered
and is less restrictive. IF, BTM, and SAVAGE either explicitly require the number of rounds T , or
implicitly require T to determine the conﬁdence level δ.

1 such that for all pairs (j, k) with 1

≻

≥

≻

j

Recently, an algorithm called Relative Upper Conﬁdence Bound (RUCB) (Zoghi et al., 2014b)
was proven to have an O(K log T ) regret bound under the Condorcet assumption. RUCB is based on
the upper conﬁdence bound index (Lai and Robbins, 1985; Agrawal, 1995; Auer et al., 2002) that is
widely used in the ﬁeld of bandit problems. RUCB is horizonless: it does not require T beforehand
and runs for any duration. Zoghi et al. (2015) extended RUCB into the mergeRUCB algorithm
under the Condorcet assumption as well as the assumption that a portion of the preference matrix is
informative (i.e., different from 1/2). They reported that mergeRUCB outperformed RUCB when K
was large. Ailon et al. (2014) proposed three algorithms named Doubler, MultiSBM, and Sparring.
MultiSBM is endowed with an O(K log T ) regret bound and Sparring was reported to outperform
IF and BTM in their simulation. These algorithms assume that the pairwise feedback is generated
from the non-observable utilities of the selected arms. The existence of the utility distributions
associated with individual arms restricts the structure of the preference matrix.

In summary, most algorithms either has O(K 2 log T ) regret under the Condorcet assumption
(SAVAGE) or require additional assumptions to achieve O(K log T ) regret (IF, BTM, MultiSBM,
and mergeRUCB). To the best of our knowledge, RUCB is the only algorithm with an O(K log T )
regret bound1. The main difﬁculty of the dueling bandit problem lies in that, there are K
1
candidates of actions to test “how good” each arm i is. A naive use of the conﬁdence bound requires
every pair of arms to be compared O(log T ) times and yields an O(K 2 log T ) regret bound.
Contribution: In this paper, we propose an algorithm called Relative Minimum Empirical Diver-
gence (RMED). This paper contributes to our understanding of the dueling bandit problem in the
following three respects.

−

•

The regret lower bound: Some studies (e.g., Yue et al., 2012) have shown that the K-armed
dueling bandit problem has a Ω(K log T ) regret lower bound. In this paper, we further ana-

1. Zoghi et al. (2013) ﬁrst proposed RUCB with an O(K 2 log T ) regret bound and later modiﬁed it by adding a ran-

domization procedure to assure O(K log T ) regret in Zoghi et al. (2014b).

2

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

lyze this lower bound to obtain the optimal constant factor for models satisfying the Condorcet
assumption. Furthermore, we show that the lower bound is the same under the total order as-
sumption. This means that optimal algorithms under the Condorcet assumption also achieve
a lower bound of regret under the total order assumption even though such algorithms do not
know that the arms are totally ordered.
An optimal algorithm: The regret of RMED is not only O(K log T ), but also optimal in
the sense that its constant factor matches the asymptotic lower bound under the Condorcet
assumption. RMED is the ﬁrst optimal algorithm in the study of the dueling bandit problem.
Empirical performance assessment: The performance of RMED is extensively evaluated by
using ﬁve datasets: two synthetic datasets, one including preference data, and two including
ranker evaluations in the information retrieval domain.

•

•

2. Problem Setup

∈

.
The K-armed dueling bandit problem involves K arms that are indexed as [K] =
}
RK×K be a preference matrix whose ij entry µi,j corresponds to the probability that
Let M
arm i is preferred to arm j. At each round t = 1, 2, . . . , T , the forecaster selects a pair of arms
[K]2, then receives a relative feedback ˆXl(t),m(t)(t)
Bernoulli(µl(t),m(t)) that
(l(t), m(t))
indicates which of (l(t), m(t)) is preferred. By deﬁnition, µi,j = 1
[K]
and µi,i = 1/2.

∼
µj,i holds for any i, j

1, 2, . . . , K
{

−

∈

∈

Let Ni,j(t) be the number of comparisons of pair (i, j) and ˆµi,j(t) be the empirical estimate of
µi,j at round t. In building statistics by using the feedback, we treat pairs without taking their order
+ 1
t′=1(1
l(t′) =
l(t′) = i, m(t′) = j
into consideration. Therefore, for i
{
}
{
t−1
l(t′) = i, m(t′) = j, ˆXl(t′ ),m(t′)(t′) = 1
+ 1
t′=1(1
j, m(t′) = i
l(t′) =
) and µi,j = (
P
{
}
}
{
j, m(t′) = i, ˆXl(t′),m(t′)(t′) = 0
))/Ni,j (t), where 1[
= i, let
] is the indicator function. For j
P
·
}
Ni>j(t) be the number of times i is preferred over j. Then, ˆµi,j(t) = Ni>j(t)/Ni,j(t), where we
set 0/0 = 1/2 here. Let ˆµi,i(t) = 1/2.

= j, Ni,j(t) =

t−1

Throughout this paper, we will assume that the preference matrix has a Condorcet winner
.
(Urvoy et al., 2013). Here we call an arm i the Condorcet winner if µi,j > 1/2 for any j
i
}
\{
Without loss of generality, we will assume that arm 1 is the Condorcet winner. The set of preference
matrices which have a Condorcet winner is denoted by
MC. We also deﬁne the set of preference
matrices satisfying the total order by
µi,j < 1/2 induces
⇔
a total order iff

Mo ⊂ MC; that is, the relation i

[K]

≺

∈

j

Let ∆i,j = µi,j −

1/2. We deﬁne the regret per round as r(t) = (∆1,i + ∆1,j)/2 when the
pair (i, j) is compared. The expectation of the cumulative regret, E[R(T )] = E
is used
to measure the performance of an algorithm. The regret increases at each round unless the selected
pair is (l(t), m(t)) = (1, 1).

T
t=1 r(t)
i

hP

µi,j} ∈ Mo.
{

2.1. Regret lower bound in the K-armed dueling bandits

In this section we provide an asymptotic regret lower bound when T
. Let the superiors of
, that is, the set of arms that is preferred to i on average.
arm i be a set
[K], µi,j < 1/2
}
The essence of the K-armed dueling bandit problem is how to eliminate each arm i
by
making sure that arm i is not the Condorcet winner. To do so, the algorithm uses some of the arms
in

1
}
\ {

Oi =

→ ∞

j
{

[K]

j
|

∈

∈

Oi and compares i with them.

3

KOMIYAMA HONDA KASHIMA NAKAGAWA

A dueling bandit algorithm is strongly consistent for model

M ⊂ MC iff it has E[R(T )] =
. The following lemma is on the number of comparisons

o(T a) regret for any a > 0 and any M
of suboptimal arm pairs.

∈ M

Lemma 1 (The lower bound on the number of suboptimal arm draws) (i) Let an arm i
and preference matrix M
MC, we have

1
}
\ {
∈ MC be arbitrary. Given any strongly consistent algorithm for model

[K]

∈

E



q + (1


Xj∈Oi

−
Mo.

d(µi,j, 1/2)Ni,j (T )

(1

o(1)) log T,

(1)

≥




−

p) log 1−p
where d(p, q) = p log p
with parameters p and q. (ii) Furthermore, inequality (1) holds for any M
consistent algorithm for

1−q is the KL divergence between two Bernoulli distributions
∈ Mo given any strongly



∈ Oi, an algorithm needs to make log T /d(µi,j, 1/2)
Lemma 1 states that, for arbitrary arm j
comparisons between arms i and j to be convinced that arm i is inferior to arm j and thus i is not the
Condorcet winner. Since the regret increase per round of comparing arm i with j is (∆1,i + ∆1,j)/2,
eliminating arm i by comparing it with j incurs a regret of

(∆1,i + ∆1,j) log T
2d(µi,j, 1/2)

.

(2)

(3)

Therefore, the total regret is bounded from below by comparing each arm i with an arm j that
minimizes (2) and the regret lower bound is formalized in the following theorem.

Theorem 2 (The regret lower bound) (i) Let the preference matrix M
strongly consistent algorithm for model

∈ MC be arbitrary. For any

MC,

lim inf
T →∞

E[R(T )]

log T ≥

min
j∈Oi

∆1,i + ∆1,j
2d(µi,j, 1/2)

Xi∈[K]\{1}
holds. (ii) Furthermore, inequality (3) holds for any M
rithm for

Mo.

∈ Mo given any strongly consistent algo-

The proof of Lemma 1 and Theorem 2 can be found in Appendix B. The proof of Lemma 1 is
similar to that of Lai and Robbins (1985, Theorem 1) for the standard multi-armed bandit problem
but differs in the following point that is characteristic to the dueling bandit. To achieve a small regret
in the dueling bandit, it is necessary to compare the arm i with itself if i is the Condorcet winner.
However, we trivially know that µi,i = 1/2 without sampling and such a comparison yields no
information to distinguish possible preference matrices. We can avoid this difﬁculty by evaluating
Ni,j and Ni,i in different ways.

3. RMED1 Algorithm

In this section, we ﬁrst introduce the notion of empirical divergence. Then, on the basis of the
empirical divergence, we formulate the RMED1 algorithm.

4

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

Algorithm 1 Relative Minimum Empirical Divergence (RMED) Algorithm

1: Input: K arms, f (K)

0. α > 0 (RMED2FH, RMED2). T (RMED2FH).

2: L

1
← (
α log log T
⌈

⌉

≥
(RMED1, RMED2)
(RMED2FH)

.

For each arm i

3: Initial phase: draw each pair of arms L times. At the end of this phase, t = L(K
4: if RMED2FH then
5:
6: end if
7: LC, LR ←
8: while t
≤
9:

∈
.
[K], LN ← ∅
T do

[K], ﬁx ˆb⋆(i) by (6).

if RMED2 then

1)K/2.

−

Draw all pairs (i, j) until it reaches Ni,j(t)

α log log t. t

t + 1 for each draw.

≥

←

end if
for l(t)

∈

LC in an arbitrarily ﬁxed order do

Select m(t) by using

Algorithm 2
Algorithm 3

(RMED1)
(RMED2, RMED2FH)

.

(

.
l(t)
LR \ {
}
(without a duplicate) for any j /
j
LN ∪ {
∈

LR such that

Jj(t) holds.

Draw arm pair (l(t), m(t)).
LR ←
LN ←
t
←
end for
LC, LR ←

.
LN , LN ← ∅

t + 1.

}

19:
20: end while

10:

11:

12:

13:

14:

15:

16:

17:

18:

Algorithm 2 RMED1 subroutine for selecting m(t)
1: ˆ
j
Ol(t)(t)
[K]
l(t)
\ {
← {
∈
}|
ˆ
Ol(t)(t) or ˆ
2: if i∗(t)
Ol(t)(t) =
∈
i∗(t).
3: m(t)
←
4: else
5: m(t)
6: end if

arg minj6=l(t) ˆµl(t),j(t).

ˆµl(t),j(t)
then

1/2
}

←

≤

∅

3.1. Empirical divergence and likelihood function

In inequality (1) of Section 2.1, we have seen that
d(µi,j, 1/2)Ni,j (T ), the sum of the di-
vergence between µi,j and 1/2 multiplied by the number of comparisons between i and j, is the
characteristic value that deﬁnes the minimum number of comparisons. The empirical estimate of
this value is fundamentally useful for evaluating how unlikely arm i is to be the Condorcet winner.
Let the opponents of arm i at round t be the set ˆ
. Note that,
, ˆµi,j (t)
i
1/2
Oi(t) =
\{
}
}
unlike the superiors
Oi(t) for each arm i are deﬁned in terms of the empirical
averages, and thus the algorithms know who the opponents are. Let the empirical divergence be

Oi, the opponents ˆ

j
{

[K]

j∈Oi

j
|

P

≤

∈

Ii(t) =

Ni,j(t)d(ˆµi,j(t), 1/2).

Xj∈ ˆOi(t)

5

KOMIYAMA HONDA KASHIMA NAKAGAWA

−

Ii(t)) can be considered as the “likelihood” that arm i is the Condorcet winner. Let

The value exp (
i∗(t) = arg mini∈[K] Ii(t) (ties are broken arbitrarily) and I ∗(t) = Ii∗(t)(t). By deﬁnition, I ∗(t)
≥
0. RMED is inspired by the Deterministic Minimum Empirical Divergence (DMED) algorithm
(Honda and Takemura, 2010). DMED, which is designed for solving the standard K-armed bandit
problem, draws arms that may be the best one with probability Ω(1/t), whereas RMED in the
dueling bandit problem draws arms that are likely to be the Condorcet winner with probability
Ω(1/t). Namely, any arm i that satisﬁes

Ji(t) =

Ii(t)
{

−

I ∗(t)

log t + f (K)
}

≤

(4)

is the candidate of the Condorcet winner and will be drawn soon. Here, f (K) can be any non-
negative function of K that is independent of t. Algorithm 1 lists the main routine of RMED.
There are several versions of RMED. First, we introduce RMED1. RMED1 initially compares all
pairs once (initial phase). Let Tinit = (K
1)K/2 be the last round of the initial phase. From
t = Tinit + 1, it selects the arm by using a loop. LC = LC(t) is the set of arms in the current loop,
and LR = LR(t)
LC(t) is the remaining arms of LC that have not been drawn yet in the current
loop. LN = LN (t) is the set of arms that are going to be drawn in the next loop. An arm i is put
. By deﬁnition, at least one arm (i.e. i∗(t) at the end
into LN when it satisﬁes
of the current loop) is put into LN in each loop. For arm l(t) in the current loop, RMED1 selects
m(t) (i.e. the comparison target of l(t)) determined by Algorithm 2.

{Ji(t)

LR(t)

i /
∈

∩ {

}}

−

⊂

The following theorem, which is proven in Section 5, describes a regret bound of RMED1.

Theorem 3 For any sufﬁciently small δ > 0, the regret of RMED1 is bounded as:

E[R(T )]

≤

((1 + δ) log T + f (K))∆1,i
2d(µi,1, 1/2)

+ O(K 2) + O

+ O(KeAK−f (K)),

Xi∈[K]\{1}
µi,j}i,j∈[K]) is a constant as a function of T . Therefore, by letting δ = log−1/3 T
where A = A(
{
and choosing an f (K) = cK 1+ǫ for arbitrary c, ǫ > 0, we obtain

K
δ2

(cid:18)

(cid:19)

E[R(T )]

≤

Xi∈[K]\{1}

∆1,i log T
2d(µi,1, 1/2)

+ O(K 2+ǫ) + O(K log2/3 T ).

3.2. Gap between the constant factor of RMED1 and the lower bound
From the lower bound of Theorem 2, the O(K log T ) regret bound of RMED1 is optimal up to
a constant factor. Moreover, the constant factor matches the regret lower bound of Theorem 2 if
b⋆(i) = 1 for all i

where

[K]

∈

1
}
\ {

b⋆(i) = arg min

j∈Oi

∆1,i + ∆1,j
d(µi,j, 1/2)

.

(5)

Here we deﬁne d+(p, q) = d(p, q) if p < q and 0 otherwise, and x/0 = +
. Note that, there
can be ties that minimize the RHS of (5). In that case, we may choose any of the ties as b⋆(i) to
eliminate arm i. For ease of explanation, we henceforth will assume that b⋆(i) is unique, but our
results can be easily extended to the case of ties.

∞

We claim that b⋆(i) = 1 holds in many cases for the following mathematical and practical
= 1, is (∆1,i + ∆1,j)/2, whereas it is simply

reasons. (i) The regret of drawing a pair (i, j), j

6

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

Algorithm 3 Subroutine for selecting m(t) in RMED2 and RMED2FH

Update ˆb⋆(l(t)) by (6).

1: if RMED2 then
2:
3: end if
4: ˆ
Ol(t)(t)
5: if ˆb⋆(l(t))

j
← {

[K]

l(t)

∈
\ {
ˆ
Ol(t)(t) and
∈
ˆb⋆(l(t)).

←

6: m(t)
7: else
8:
9: end if

Select m(t) by using Algorithm 2.

≤

ˆµl(t),j(t)
}|
Nl(t),i∗(t)(t)
Nl(t),i∗(t)(t)

.
1/2
}
Nl(t),ˆb⋆(l(t))(t)/ log log t
Nl(t),ˆb⋆(l(t))(t)/ log log T (RMED2FH)

(RMED2)

(

≥

≥

then

≥

∆1,i/2 for the pair (i, 1). Thus, d+(µi,j, 1/2) has to be much larger than d+(µi,1, 1/2) in order to
satisfy b⋆(i) = j. (ii) The Condorcet winner usually wins over the other arms by a large margin,
and therefore, d+(µi,1, 1/2)
d+(µi,j, 1/2). For example, in the preference matrix of Example 1
(Table 1(a)), b⋆(3) = 1 as long as q < 0.79. Example 2 (Table 1(b)) is a preference matrix based
on six retrieval functions in the full-text search engine of ArXiv.org (Yue and Joachims, 2011)2. In
Example 2, b⋆(i) = 1 holds for all i, even though µ1,4 < µ2,4. In the case of a 16-ranker evalua-
tion based on the Microsoft Learning to Rank dataset (details are given in Section 4), occasionally
= 1 occurs, but the difference between the regrets of drawing arm 1 and b⋆(i) is fairly small
b⋆(i)
(smaller than 1.2% on average). Nevertheless, there are some cases in which comparing arm i with
1 is not such a clever idea. Example 3 (Table 1(c)) is a toy example in which comparing arm i with
b⋆(i)
= 1 makes a large difference. In Example 3, it is clearly better to draw pairs (2, 4), (3, 2) and
(4, 3) to eliminate arms 2, 3, and 4, respectively. Accordingly, it is still interesting to consider an
algorithm that reduces regret by comparing arm i with b⋆(i).

Table 1: Three preference matrices. In each example, the value at row i, column j is µi,j.

(a) Example 1
3
2
0.7
0.7
0.5
q
0.5
1-q

1
0.5
0.3
0.3

1
2
3

(b) Example 2

1
0.50
0.45
0.45
0.46
0.39
0.39

2
0.55
0.50
0.45
0.45
0.42
0.40

3
0.55
0.55
0.50
0.46
0.49
0.44

4
0.54
0.55
0.54
0.50
0.46
0.50

1
2
3
4
5
6

5
0.61
0.58
0.51
0.54
0.50
0.49

6
0.61
0.60
0.56
0.50
0.51
0.50

(c) Example 3

1
0.5
0.4
0.4
0.4

2
0.6
0.5
0.1
0.9

3
0.6
0.9
0.5
0.1

4
0.6
0.1
0.9
0.5

1
2
3
4

3.3. RMED2 Algorithm

We here propose RMED2, which gracefully estimates b⋆(i) during a bandit game and compares
arm i with b⋆(i). RMED2 and RMED1 share the main routine (Algorithm 1). The subroutine of
RMED2 for selecting m(t) is shown in Algorithm 3. Unlike RMED1, RMED2 keeps drawing pairs
of arms (i, j) at least α log log t times (Line 10 in Algorithm 1). The regret of this exploration is
insigniﬁcant since O(log log T ) = o(log T ). Once all pairs have been explored more than α log log t

2. In the original preference matrix of Yue and Joachims (2011), µ2,4 6= 1 − µ4,2. To satisfy µ2,4 = 1 − µ4,2, we

replaced µ2,4 and µ4,2 of the original with (µ2,4 − µ4,2 + 1)/2 and (µ4,2 − µ2,4 + 1)/2, respectively.

7

KOMIYAMA HONDA KASHIMA NAKAGAWA

times, RMED2 goes to the main loop. RMED2 determines m(t) by using Algorithm 2 based on the
estimate of b⋆(i) given by

ˆb⋆(i) = arg min
j∈[K]\{i}

ˆ∆i∗(t),i + ˆ∆i∗(t),j
d+(ˆµi,j(t), 1/2)

,

(6)

where ties are broken arbitrarily, ˆ∆i,j = 1/2
. Intuitively, RMED2
tries to select m(t) = ˆb⋆(i) for most rounds, and occasionally explores i∗(t) in order to reduce the
regret increase when RMED2 fails to estimate the true b⋆(i) correctly.

ˆµi,j and we set x/0 = +

∞

−

3.4. RMED2FH algorithm
Although we believe that the regret of RMED2 is optimal, the analysis of RMED2 is a little bit
complicated since it sometimes breaks the main loop and explores from time to time. For ease of
analysis, we here propose RMED2 Fixed Horizon (RMED2FH, Algorithm 1 and 3), which is a
“static” version of RMED2. Essentially, RMED2 and RMED2FH have the same mechanism. The
differences are that (i) RMED2FH conducts an α log log T exploration in the initial phase. After
the initial phase (ii) ˆb⋆(i) for each i is ﬁxed throughout the game. Note that, unlike RMED1 and
RMED2, RMED2FH requires the number of rounds T beforehand to conduct the initial α log log T
draws of each pair. The following Theorem shows the regret of RMED2FH that matches the lower
bound of Theorem 2.

Theorem 4 For any sufﬁciently small δ > 0, the regret of RMED2FH is bounded as:

E[R(T )]

≤

Xi∈[K]\{1}

(∆1,i + ∆1,b⋆(i))((1 + δ) log T )
2d(µi,b⋆(i), 1/2)

+ O(αK 2 log log T ) + O(KeAK−f (K))

K log T
log log T

+ O

(cid:18)

K
δ2

(cid:19)

(cid:18)

(cid:19)

+ O

+ O (Kf (K)) ,

(7)

where A = A(
µi,j}
{
choosing an f (K) = cK 1+ǫ (c, ǫ > 0) we obtain

) > 0 is a constant as a function of T . By setting δ = O((log T )−1/3) and

E[R(T )]

≤

Xi∈[K]\{1}

(∆1,i + ∆1,b⋆(i)) log T
2d(µi,b⋆(i), 1/2)

+ O(αK 2 log log T ) + O

+ O

K 2+ǫ

.

K log T
log log T

(cid:18)

(cid:19)

(cid:0)

(cid:1)
(8)

Note that all terms except the ﬁrst one in (8) are o(log T ). From Theorems 2 and 4 we see that (i)
RMED2FH is asymptotically optimal under the Condorcet assumption and (ii) the logarithmic term
on the regret bound of RMED2FH cannot be improved even if the arms are totally ordered and the
forecaster knows of the existence of the total order. The proof sketch of Theorem 4 is in Section 5.

4. Experimental Evaluation

To evaluate the empirical performance of RMED, we conducted simulations3 with ﬁve bandit
datasets (preference matrices). The datasets are as follows:

3. The source code of the simulations is available at https://github.com/jkomiyama/duelingbanditlib.

8

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

4

10

t
e
r
g
e
r
 
:
)
t
(
R

3

10

2

10

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

4

10

t
e
r
g
e
r
 
:
)
t
(
R

3

10

2

10

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

4

10

3

10

2

10

1

10

t
e
r
g
e
r
 
:
)
t
(
R

5

10

4

10

3

10

t
e
r
g
e
r
 
:
)
t
(
R

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

4

10

t
e
r
g
e
r
 
:
)
t
(
R

3

10

2

10

6

10

5

10

4

10

t
e
r
g
e
r
 
:
)
t
(
R

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

1

10

2

10

3

10

4

10
t: round

(a) Six rankers

5

10

6

10

2

10

3

10

5

10

6

10

4

10
t: round

(b) Cyclic

1

10

2

10

3

10

4

10
t: round

5

10

6

10

(c) Arithmetic

1

10

2

10

3

10

4

10

5

10

2

10

3

10

4

10

5

10
t: round

6

10

7

10

3

10

3

10

4

10

5

10
t: round

6

10

7

10

(e) MSLR K = 16

(f ) MSLR K = 64

t: round

(d) Sushi

Figure 1: Regret-round log-log plots of algorithms.

Six rankers is the preference matrix based on the six retrieval functions in the full-text search engine
of ArXiv.org (Table 1(b)).
Cyclic is the artiﬁcial preference matrix shown in Table 1(c). This matrix is designed so that the
comparison of i with 1 is not optimal.
Arithmetic dataset involves eight arms with µi,j = 0.5 + 0.05(j
Sushi dataset is based on the Sushi preference dataset (Kamishima, 2003) that contains the pref-
erences of 5, 000 Japanese users as regards 100 types of sushi. We extracted the 16 most popular
types of sushi and converted them into arms with µi,j corresponding to the ratio of users who prefer
sushi i over j. The Condorcet winner is the mildly-fatty tuna (chu-toro).
MSLR: We tested submatrices of a 136
136 preference matrix from Zoghi et al. (2015), which is
derived from the Microsoft Learning to Rank (MSLR) dataset (Microsoft Research, 2010; Qin et al.,
2010) that consists of relevance information between queries and documents with more than 30K
queries. Zoghi et al. (2015) created a ﬁnite set of rankers, each of which corresponds to a ranking
feature in the base dataset. The value µi,j is the probability that the ranker i beats ranker j based on
the navigational click model (Hofmann et al., 2013). We randomly extracted K = 16, 64 rankers
in our experiments and made sub preference matrices. The probability that the Condorcet winner

i) and has a total order.

−

×

9

KOMIYAMA HONDA KASHIMA NAKAGAWA

500

400

300

200

100

t
e
r
g
e
r
 
:
)
t
(
R

RMED1
RMED2
RMED2FH a=1
RMED2FH a=3
RMED2FH a=10
LB1
TrueLB

100

80

60

40

20

t
e
r
g
e
r
 
:
)
t
(
R

RMED1
RMED2
RMED2FH a=1
RMED2FH a=3
RMED2FH a=10
LB1
TrueLB

2000

1500

1000

500

t
e
r
g
e
r
 
:
)
t
(
R

RMED1
RMED2
RMED2FH a=1
RMED2FH a=3
RMED2FH a=10
LB1
TrueLB

0
10

2

3

10

4

10
t: round

5

10

6

10

0
10

2

3

10

4

10
t: round

5

10

6

10

0
10

2

3

10

5

4

10
10
t: round

6

10

7

10

(a) Six rankers

(b) Cyclic

(c) MSLR K = 16

Figure 2: Regret-round semilog plots of RMED compared with theoretical bounds. We set f (K) =

0.3K 1.01 for all algorithms, and α = 3 for RMED2.

exists in the subset of the rankers is high (more than 90%, c.f. Figure 1 in Zoghi et al. (2014a)), and
we excluded the relatively small case where the Condorcet winner does not exist.

A Condorcet winner exists in all datasets. In the experiments, the regrets of the algorithms were

averaged over 1, 000 runs (Six rankers, Cyclic, Arithmetic, and Sushi), or 100 runs (MSLR).

4.1. Comparison among algorithms

We compared the IF, BTM with γ = 1.2, RUCB with α = 0.51, Condorcet SAVAGE with δ = 1/T ,
MultiSBM and Sparring with α = 3, and RMED algorithms. There are two versions of RUCB:
the one that uses a randomizer in choosing l(t) (Zoghi et al., 2014b), and the one that does not
(Zoghi et al., 2013). We implemented both and found that the two perform quite similarly: we show
the result of the former one in this paper. We set f (K) = 0.3K 1.01 for all RMED algorithms and
set α = 3 for RMED2 and RMED2FH. The effect of f (K) is studied in Appendix A. Note that IF
and BTM assume a total order among arms, which is not the case with the Cyclic, Sushi, and MSLR
datasets. MultiSBM and Sparring assume the existence of the utility of each arm, which does not
allow a cyclic preference that appears in the Cyclic dataset.

Figure 1 plots the regrets of the algorithms. In all datasets RMED signiﬁcantly outperforms
RUCB, the next best excluding the different versions of RMED. Notice that the plots are on a base
10 log-log scale. In particular, regret of RMED1 is more than twice smaller than RUCB on all
datasets other than Cyclic, in which RMED2 performs much better. Among the RMED algorithms,
RMED1 outperforms RMED2 and RMED2FH on all datasets except for Cyclic, in which comparing
arm i
= 1 with arm 1 is inefﬁcient. RMED2 outperforms RMED2FH in the ﬁve of six datasets: this
could be due to the fact that RMED2FH does not update ˆb⋆(i) for ease of analysis.

4.2. RMED and asymptotic bound

Figure 2 compares the regret of RMED with two asymptotic bounds. LB1 denotes the regret bound
of RMED1. TrueLB is the asymptotic regret lower bound given by Theorem 2.
, the slope of RMED1 should converge to LB1, and the ones
RMED1 and RMED2: When T
of RMED2 and RMED2FH should converge to TrueLB. On Six rankers, LB1 is exactly the same

→ ∞

10

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

as TrueLB, and the slope of RMED1 converges to this TrueLB. In Cyclic, the slope of RMED2
converges to TrueLB, whereas that of RMED1 converges to LB1, from which we see that RMED2
is actually able to estimate b⋆(i)
= 1 correctly. In MSLR K = 16, LB1 and TrueLB are very close
(the difference is less than 1.2%), and RMED1 and RMED2 converge to these lower bounds.
RMED2FH with different values of α: We also tested RMED2FH with several values of α. On
the one hand, with α = 1, the initial phase of RMED2FH is too short to identify b⋆(i); as a result
it performs poorly on the Cyclic dataset. On the other hand, with α = 10, the initial phase was
too long, which incurs a practically non-negligible regret on the MSLR K = 16 dataset. We also
tested several values of parameter α in RMED2FH. We omit plots of RMED2 with α = 1, 10 for
the sake of readability, but we note that in our datasets the performance of RMED2 is always better
than or comparable with the one of RMED2FH under the same choice of α, although the optimality
of RMED2 is not proved unlike RMED2FH.

5. Regret Analysis

This section provides two lemmas essential for the regret analysis of RMED algorithms and proves
the asymptotic optimality of RMED1 based on these lemmas. A proof sketch on the optimal regret
of RMED2FH is also given.

The crucial property of RMED is that, by constantly comparing arms with the opponents, the

true Condorcet winner (arm 1) actually beats all the other arms with high probability. Let

U

(t) =

.
ˆµ1,i(t) > 1/2
}
{
\i∈[K]\{1}
ˆµ1,i(t) < 1/2 for all i

[K]

U

(t), ˆµi,1(t) = 1

Under
(t)
∈
implies that i∗(t) = arg mini∈[K] Ii(t) is unique with i∗(t) = 1 and I ∗(t) = I1(t) = 0. Lemma
c(t) occurs is constant in T , where the
5 below shows that the average number of rounds that
superscript c denotes the complement.

, and thus, Ii(t) > 0. Therefore,

1
}
\ {

−

U

U

Lemma 5 When RMED1 or RMED2FH is run, the following inequality holds:

T

E


Xt=Tinit+1


c(t)

1

{U

}



= O(eAK−f (K)),

(9)

where A = A(
µi,j}
{

) > 0 is a constant as a function of T .

times in the initial phase, we deﬁne
Note that, since RMED2FH draws each pair
1)K/2 for RMED2FH. We give a proof of this lemma in Appendix C.
(K
Tinit =
⌉
Intuitively, this lemma can be proved from the facts that arm 1 is drawn within roughly eI1(t)−f (K)
rounds and I1(t) is not very large with high probability.

α log log T
⌈

α log log T
⌈

−

⌉

Next, for i

[K]

∈

1
}
\ {

and j

∈ Oi, let
i,j (δ) =

N Suf

(1 + δ) log T + f (K)
d(µi,j, 1/2)

+ 1,

which is a sufﬁcient number of comparisons of i with j to be convinced that the arm i is not the
Condorcet winner. The following lemma states that if pair (i, j) is drawn N Suf
i,j (δ) times then i is
rarely selected as l(t) again.

11

KOMIYAMA HONDA KASHIMA NAKAGAWA

Lemma 6 When RMED1 or RMED2FH is run, for i

[K]

∈

, j

1
}
\ {

∈ Oi,

T

E


Xt=Tinit+1


1
l(t) = i, Ni,j(t)
{

≥

N Suf

i,j (δ)

= O

+ O(eAK−f (K)) + K.

1
δ2

(cid:18)

(cid:19)

}



We prove this lemma in Appendix D based on the Chernoff bound.

Now we can derive the regret bound of RMED1 based on these lemmas.

Proof of Theorem 3: Since
be decomposed as:

U

(t) implies m(t) = 1 in RMED1, the regret increase per round can

r(t) = 1

c(t)
}

+

{U

∆1,i
2

1
l(t) = i, m(t) = 1,
{

.
(t)
}

U

(10)

Xi∈[K]\{1}

Using Lemmas 5 and 6, we obtain

E[R(T )]

Tinit +

≤

K(K
2

−

≤

1)

+E



1)

K(K
2

−

≤

T

[r(t)]

Xt=Tinit+1
T

c(t)

1

{U

Xt=Tinit+1
+ O(eAK−f (K)) +



∆1,i
2  

+
Xi∈[K]\{1}

}



∆1,i
2

(cid:18)

Xi∈[K]\{1}

T

t=1
X

1
δ2

(cid:18)

(cid:19)

N Suf

i,1 (δ)+

1[l(t) = i, m(t) = 1, Ni,1(t)

N Suf

i,1 (δ)]

!

N Suf

i,1 (δ) + O

+ O(eAK−f (K)) + K

,

≥

(cid:19)

which immediately completes the proof of Theorem 3.

We also prove Theorem 4 on the optimality of RMED2FH based on Lemmas 5 and 6. Because

∈

[K]

1
}
\ {

. There exists C2 > 0 such that, for each l(t) = i, (i) with probability 1

c(t) does
(t), we decompose the regret into the contributions

the full proof in Appendix E is a little bit lengthy, here we give its brief sketch.
Proof sketch of Theorem 4 (RMED2FH): Similar to Theorem 3, we use the fact that the
not occur very often (i.e., Lemma 5). Under
of each arm i
−
O((log T )−C2) RMED2FH successfully estimates ˆb⋆(i) = b⋆(i) and selects m(t) = b⋆(i) for most
rounds. The optimal O(log T ) term comes from the comparison of i and b⋆(i). Arm 1 is also drawn
for O(log T / log log T ) = o(log T ) times. On the other hand, (ii) with probability O((log T )−C2),
RMED2FH fails to estimate b⋆(i) correctly. By occasionally comparing arm i with arm 1, we
can bound the regret increase by O(log T log log T ). Since O((log T )−C2
log T log log T ) =
o(log T ), this regret does not affect the O(log T ) factor.

×

U

U

6. Discussion

We proved the regret lower bound in the dueling bandit problem. The RMED algorithm is based
on the likelihood that the arm is the Condorcet winner. RMED is proven to have the matching
regret upper bound. The empirical evaluation revealed that RMED signiﬁcantly outperforms the
state-of-the-art algorithms. To conclude this paper, we mention three directions of future work.

12

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

First, when a Condorcet winner does not necessarily exist, the Copeland bandits (Urvoy et al.,
2013) are a natural extension of our problem. Thus, seeking an effective algorithm for solving this
problem will be interesting. As is well known in the ﬁeld of voting theory, there are several other
criteria of winners that are incompatible with the Condorcet / Copeland bandits, such as the Borda
winner (Urvoy et al., 2013). Comparing several criteria or developing an algorithm that outputs
more than one of these winners should be interesting directions of future work.

Second, another direction is sequential preference elicitation problems under relative feedback
that goes beyond the binary preference over pairs, such as multiscale feedback and/or preferences
among three or more items.

Third, in the standard bandit problem, it is reported that KL-UCB+ (Lai, 1987; Garivier and Capp´e,

2011) performs better than DMED. A study of a UCB-based optimal algorithm for the dueling ban-
dits can yield an algorithm that outperforms RMED.

Acknowledgements

We thank the anonymous reviewers for their useful comments. This work was supported in part by
JSPS KAKENHI Grant Number 26106506.

13

KOMIYAMA HONDA KASHIMA NAKAGAWA

References

R. Agrawal. Sample mean based index policies with O(log n) regret for the multi-armed bandit

problem. Advances in Applied Probability, 27:1054–1078, 1995.

Nir Ailon, Zohar Shay Karnin, and Thorsten Joachims. Reducing dueling bandits to cardinal ban-

dits. In ICML, pages 856–864, 2014.

Peter Auer, Nicol´o Cesa-bianchi, and Paul Fischer. Finite-time Analysis of the Multiarmed Bandit

Problem. Machine Learning, 47:235–256, 2002.

Eric Brochu, Tyson Brochu, and Nando de Freitas. A bayesian interactive optimization approach
In Proceedings of the 2010 Eurographics/ACM SIGGRAPH

to procedural animation design.
Symposium on Computer Animation, SCA 2010, Madrid, Spain, 2010, pages 103–112, 2010.

S´ebastien Bubeck. Bandits Games and Clustering Foundations. Theses, Universit´e des Sciences et

Technologie de Lille - Lille I, June 2010.

Aur´elien Garivier and Olivier Capp´e. The KL-UCB algorithm for bounded stochastic bandits and

beyond. In COLT, pages 359–376, 2011.

Marco De Gemmis, Leo Iaquinta, Pasquale Lops, Cataldo Musto, Fedelucio Narducci, and Giovanni
In In Preference Learning (PL-09)

Semeraro. Preference learning in recommender systems.
ECML/PKDD-09 Workshop, 2009.

Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. Fidelity, soundness, and efﬁciency of
interleaved comparison methods. Transactions on Information Systems, 31(4):17:1–43, 2013.

Junya Honda and Akimichi Takemura. An Asymptotically Optimal Bandit Algorithm for Bounded

Support Models. In COLT, pages 67–79, 2010.

Toshihiro Kamishima. Nantonac collaborative ﬁltering: recommendation based on order responses.

In KDD, pages 583–588, 2003.

1091–1114, 09 1987.

T. L. Lai. Adaptive treatment allocation and the multi-armed bandit problem. Ann. Statist., 15(3):

T. L. Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in

Applied Mathematics, 6(1):4–22, 1985.

Microsoft Research.

Microsoft Learning
to Rank Datasets,
http://research.microsoft.com/en-us/projects/mslr/.

2010.

URL

Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. LETOR: A benchmark collection for research on

learning to rank for information retrieval. Inf. Retr., 13(4):346–374, 2010.

Tanguy Urvoy, Fabrice Cl´erot, Rapha¨el Feraud, and Sami Naamane. Generic exploration and k-

armed voting bandits. In ICML, pages 91–99, 2013.

Yisong Yue and Thorsten Joachims. Beat the mean bandit. In ICML, pages 241–248, 2011.

14

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits

problem. In COLT, 2009.

Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits

problem. J. Comput. Syst. Sci., 78(5):1538–1556, 2012.

Omar Zaidan and Chris Callison-Burch. Crowdsourcing translation: Professional quality from non-
In The 49th Annual Meeting of the Association for Computational Linguistics
professionals.
(ACL): Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Port-
land, Oregon, USA, pages 1220–1229, 2011.

Masrour Zoghi, Shimon Whiteson, R´emi Munos, and Maarten de Rijke. Relative upper con-
ﬁdence bound for the k-armed dueling bandit problem. CoRR, abs/1312.3393, 2013. URL
http://arxiv.org/abs/1312.3393.

Masrour Zoghi, Shimon Whiteson, Maarten de Rijke, and R´emi Munos. Relative conﬁdence sam-

pling for efﬁcient on-line ranker evaluation. In WSDM, pages 73–82, 2014a.

Masrour Zoghi, Shimon Whiteson, R´emi Munos, and Maarten de Rijke. Relative upper conﬁdence

bound for the k-armed dueling bandit problem. In ICML, pages 10–18, 2014b.

Masrour Zoghi, Shimon Whiteson, and Maarten de Rijke. MergeRUCB: A method for large-scale

online ranker evaluation. In WSDM, 2015.

15

KOMIYAMA HONDA KASHIMA NAKAGAWA

c=0
c=0.1
c=0.3
c=1

5

10

4

10

3

10

M
0
1
=
T
 
t
a
 
)
T
(
R

10

2
16

32
64
K: # of arms

128

Figure 3: Performance of RMED1 algorithm with several values of c. The plot shows the regret at

T = 107 in the MSLR dataset with K = 16, 32, 64, and 128.

Appendix A. Experiment: Dependence on f (K)

P

c(t) implies a failure in identifying the Condorcet winner (i.e., 1

= i∗(t)). Although
The event
U
T
E[
c(t)] = O(eAK−f (K)) is a constant function of T for any non-negative f (K), this term
t=1 U
is not negligible with large K. To evaluate the effect of f (K), we set f (K) = cK 1.01 and studied
several values of c with the MSLR dataset (Figure 3). In the case of c = 0, the regret for K = 128
becomes 100 times that for K = 16, which implies that the exponential dependence O(eAK ) may
not be an artifact of the proof. On the other hand, the results for c = 0.1, 0.3, and 1 indicate that
this term can be much improved by simply letting c be a small positive value.

Appendix B. Proofs on Regret Lower Bound

B.1. Proof of Lemma 1

∈

[K]

be arbitrary and M =

Let i
be an arbitrary preference matrix. We consider a
modiﬁed preference matrix M ′ in which the probabilities related to arm i are different from M . Let
′
i, ij element
j
Oi ∪ {

1
\ {
}
[K], µi,j ≤

. For j
[K], µi,j = 1/2
}

, that is,
1/2
}

µi,j}
{

′
i =

′
i =

∈ O

j
|

O

∈

j
j
O
|
{
of M ′ is µ′

∈
i,j such that

d+(µi,j, µ′

i,j) = d(µi,j, 1/2) + ǫ.

(11)

Such a µ′
of the KL divergence. For j /
the modiﬁed bandit problem the Condorcet winner is not arm 1 but arm i. Moreover, if M
then M ′

i,j > 1/2 uniquely exists for sufﬁciently small ǫ > 0 by the monotonicity and continuity
i,j = µi,j. Note that, unlike the original bandit problem, in
∈ Mo

i, let µ′
′

∈ O

∈ Mo.

16

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

Notation: now, let ˆX m

i,j ∈ {

0, 1
}

be the result of m-th draw of the pair (i, j),

n

KLj(n) =

log

ˆX m
ˆX m

i,jµi,j + (1
i,jµ′
i,j + (1

 

−

ˆX m
ˆX m

i,j)(1
i,j)(1

−

µi,j)
µ′
i,j) !

,

Xm=1

−
KLj(Ni,j(T )), and P′, E′ be the probability and the expectation with respect to

c

−

KL =

and
the modiﬁed bandit game. Then, for any event

j∈O′
i

P

c

c

,

E
1

holds. Let us deﬁne the events

) = E

P′(
E

exp

KL

{E}

h

−

(cid:16)

(cid:17)i

c

(12)

Ni,j(T )d(µi,j, µ′

i,j) < (1

ǫ) log T, Ni,i(T ) < √T

,

−






D1 =

D2 =
D12 =
D1\2 =




i

Xj∈O′
KL


≤
n
(cid:16)
D1 ∩ D2,
c
c
2.
D1 ∩ D

ǫ
2

1

−

log T

,

(cid:17)

o

First step (P

= o(1)): From (12),

{D12}
P′(

D12)
By using this we have

E

1
{D12}

exp

≥

h

ǫ
2

1

−

−

(cid:16)

(cid:16)

log T

= T −(1−ǫ/2)P

.
{D12}

(cid:17)

(cid:17)i

(13)

P

{D12} ≤

T (1−ǫ/2)P′(

T (1−ǫ/2)P′

D12)
Ni,i(T ) < √T

≤

≤

≤

T (1−ǫ/2)P′

o
Ni,i(T ) > T

n

T
E′[T
n
T

−

−
−

Ni,i(T )]
√T

T (1−ǫ/2)

√T

−

o

(by the Markov inequality).

(14)

Since this algorithm is strongly consistent, E′[T
o(T a) for any a > 0. Therefore, the
−
RHS of the last line of (14) is o(T a−ǫ/2), which, by choosing sufﬁciently small a, converges to zero
as T
→ ∞
Second step (P

= o(1).
= o(1)): We have

. In summary, P

Ni,i(T )]

{D12}

→

{D1\2}

P

{D1\2}




Xj∈O′

i

= P

Ni,j(T )d(µi,j, µ′

i,j) < (1

ǫ) log T, Ni,i(T ) < √T ,

KLj(Ni,j(T )) >

1

log T

−

ǫ
2

−

(cid:16)

(cid:17)






i

Xj∈O′
ǫ
2

−

1

c

(cid:17)

(cid:16)

log T

.
)

P


(

≤

{nj }∈N|O′

i

|,Pj∈O′

i

max
nj d(µi,j ,µ′

i,j)<(1−ǫ) log T

KLj(nj) >

Xj∈O′

i

17

c

KOMIYAMA HONDA KASHIMA NAKAGAWA

Note that

max
1≤n≤N

KLj(n) = max
1≤n≤N

n

log

ˆX m
ˆX m

i,jµi,j + (1
i,jµ′
i,j + (1

 

ˆX m
ˆX m

i,j)(1
i,j)(1

−

−

µi,j)
µ′
i,j) !

,

−

−

m=1
X

is the maximum of the sum of positive-mean random variables, and thus converges to is average
(c.f., Lemma 10.5 in Bubeck, 2010). Namely,

c

lim
N→∞

max
1≤n≤N

KLj(n)
N

c

= d(µi,j, µ′

i,j)

a.s.

(15)

Let δ > 0 be sufﬁciently small. We have,

max

{nj }∈N|O′

i

|,Pj∈O′

i

njd(µi,j ,µ′

i,j )<(1−ǫ) log T

KLj(nj)

j∈O′
i

log T

P

c

max

{nj}∈N|O′

i

|,Pj∈O′

i

:nj >δ log T nj d(µi,j ,µ′

i,j )<(1−ǫ) log T

≤

log T

KLj(nj)

j∈O′
i

P

c

+

minj∈O′

i

δK
d(µi,j, µ′

.

i,j)

Combining this with the fact that (15) holds for any j, we have

max

{nj}∈N|O′

i

|,Pj∈O′

i

:nj >δ log T nj d(µi,j ,µ′

i,j )<(1−ǫ) log T

log T

KLj(nj)

j∈O′
i

P

c

1

ǫ

a.s.,

≤

−

max

{nj }∈N|O′

i

|,Pj∈O′

i

njd(µi,j ,µ′

i,j)<(1−ǫ) log T

KLj(nj)

j∈O′
i

log T

P

c

≤

−

1

ǫ + Θ(δ)

a.s. (16)

By using the fact that (16) holds almost surely for any sufﬁciently small δ > 0 and 1
we have

−

ǫ/2 > 1

ǫ,

−



{nj }∈N|O′

i

|,Pj∈O′

i

max
njd(µi,j ,µ′

i,j)<(1−ǫ) log T

P



Xj∈O′

i

c

KLj(nj) >

1

log T

= o(1).

ǫ
2

−

(cid:16)

(cid:17)





In summary, we obtain P
Last step: We here have

= o(1).

D1\2

(cid:8)

(cid:9)

D1 =

Ni,j(T )d(µi,j, µ′

i,j) < (1

ǫ) log T

Ni,i(T ) < √T

−

∩




n

o

=

Ni,j(T )(d(µi,j, 1/2) + ǫ) < (1

Ni,i(T ) < √T

(By (11))

lim sup
N→∞

and thus

lim sup
T →∞

Ni,j(T )(d(µi,j, 1/2) + ǫ) +

(1

−

ǫ) log T
√T


Ni,i(T ) < (1

ǫ) log T

,

−

(17)


ǫ) log T

−

∩




n

18

o













Xj∈O′

i

Xj∈O′

i

Xj∈O′

i


⊇ 




REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

where we used the fact that
Note that, by using the result of the previous steps, P
the complementary of this fact,

A < C
{

B < C

} ∩ {

} ⊇ {

A + B < C
}
= P
{D12}

{D1}

for A, B > 0 in the last line.
+ P
= o(1). By using

{D1\2}

Ni,j(T )(d(µi,j, 1/2) + ǫ) +

(1

−

ǫ) log T
√T

Ni,i(T )

(1

ǫ) log T

≥

−

P

≥

{D

c
1}

= 1

o(1).

−






Using the Markov inequality yields



P




Xj∈O′

i

E




Xj∈O′

i

Ni,j(T )(d(µi,j, 1/2) + ǫ) +

Ni,i(T )

(1

ǫ)(1

o(1)) log T.

(18)

(1

−

ǫ) log T
√T

≥




−

−



Because E[Ni,i(T )] is subpolynomial as a function of T due to the consistency, the second term in
LHS of (18) is o(1) and thus negligible. Lemma 1 follows from the fact that (18) holds for sufﬁ-
ciently small ǫ.



B.2. Proof of Theorem 2

We have

R(T ) =

1
2

∆1,i + ∆1,j
2

Ni,j(T ) +

∆1,i + ∆1,i
2

Ni,i(T )

Xi∈[K] Xj∈[K]\{i}

≥

≥

=

Xi,j∈[K]:µi,j<1/2

Xi∈[K]\{1} Xj∈Oi

Xi∈[K]\{1} Xj∈Oi

∆1,i + ∆1,j
2

Ni,j(T ) +

∆1,i + ∆1,i
2

Ni,i(T )

Xi∈[K]

Xi∈[K]

∆1,i + ∆1,j
2

Ni,j(T )

∆1,i + ∆1,j
2d(µi,j, 1/2)

d(µi,j, 1/2)Ni,j (T ).

Taking the expectation on both sides and using Lemma 1 yield

E[R(T )]

≥

Xi∈[K]\{1}

min
j∈Oi

∆1,i + ∆1,j
2d(µi,j, 1/2)

(1

o(1)) log T.

−

Appendix C. Proof of Lemma 5

This lemma essentially states that, the expected number of the rounds in which arm 1 is underesti-
mated is O(1). We show this by bounding the expected number of rounds before arm 1 is compared,

19

KOMIYAMA HONDA KASHIMA NAKAGAWA

for each ﬁxed set of
16 in Honda and Takemura (2010). Note that

N1,s(t)
}
{

and summing over

N1,s(t)
{

. This technique is inspired by Lemma
}

c(t) =

U

[S∈2[K]\{1}\{∅} (

\s∈S

ˆµ1,s(t)
{

≤

1/2

} ∩

ˆµ1,s(t) > 1/2
{

.

})

(19)

\s /∈S

Now we bound the number of rounds that the event

ˆµ1,s(t)
{

≤

1/2

} ∩

ˆµ1,s(t) > 1/2
}
{

\s∈S
occurs. Let N be the set of non-zero natural numbers, ns ∈
for each s
∈
1/2, d+(ˆµns
1,s, 1/2) = xs, N1,s(t) = ns}

N and xs ∈
i,j be the empirical estimate of µi,j at n-th draw of pair (i, j). If
S and ˆµ1,s(t) > 1/2 holds for s /
∈

S. Let ˆµn

holds for s

\s /∈S

∈

[0, log 2] be arbitrary

ˆµns
1,s ≤
{
S then

I1(t) =

nsd+(ˆµ1,s(t), 1/2)

Xs∈S

and therefore

J1(t) holds for any

exp

t

≥

 

nsd+(ˆµ1,s(t), 1/2)

f (K)

.

−

!

Xs∈S
J1(t) occurs, then arm 1 is in LN of the next loop, and thus for some s

If
within 2K rounds. Therefore we have

∈

S, N1,s is incremented

T

1

Xt=Tinit+1

"
\s∈S

ˆµ1,s(t)
{

1/2, N1,s(t) = ns} ∩

≤

ˆµ1,s(t) > 1/2
{

}#

\s /∈S

exp

≤

 

Xs∈S

nsd+(ˆµns

1,s, 1/2)

f (K)

+ 2K.

−

!

20

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

Letting Ps(xs) = Pr[ˆµns

1/2, d+(ˆµns

1,s, 1/2)

xs], we have

1,s ≤

≥

E

T

1


Xt=Tinit+1

=

"

\s∈S

Z{xs}∈[0,log 2]|S|  

ˆµ1,s(t)
{

≤

1/2, N1,s(t) = ns} ∩

ˆµ1,s(t) > 1/2
{

exp

 

Xs∈S

nsxs −

f (K)

+ 2K

!

d(

Ps(xs))

−

\s /∈S

!

Ys∈S

}#


(

(

(

(

(

= e−f (K)

2K

Ps(0) +

ensxsd(

Ps(xs))

−

)

= e−f (K)

2K

Ps(0) +

ensxsPs(xs)]log 2

0 +

Ys∈S

Ys∈S

Ys∈S Zxs∈[0,log 2]
[
−

Ys∈S  

(integration by parts)

nsensxsPs(xs)dxs

!)

Zxs∈[0,log 2]

≤

≤

e−f (K)

(1 + 2K)

Ps(0) +

nsensxse−ns(xs+C1(µ1,s,1/2))dxs

(by the Chernoff bound and Fact 10, where C1(µ, µ2) = (µ

Ys∈S Zxs∈[0,log 2]

e−f (K)

(1 + 2K)

e−nsd(1/2,µ1,s) +

µ2)2/(2µ(1

−

−
nse−nsC1(µ1,s,1/2)dxs

)

µ2)))

)

Ys∈S Zxs∈[0,log 2]

Ys∈S

)

= e−f (K)

(1 + 2K)

e−nsd(1/2,µ1,s) +

(log 2)nse−nsC1(µ1,s,1/2)

.

(20)

Ys∈S

Ys∈S

Ys∈S
,
ns}
{

By summing (20) over

T

P

Xt=Tinit+1

"
\s∈S

ˆµ1,s(t)
{

≤

1/2

} ∩

ˆµ1,s(t) > 1/2
{

}#

e−f (K)

(1 + 2K)

e−nsd(1/2,µ1,s) +

(log 2)nse−nsC1(µ1,s,1/2)

· · ·
X{ns}∈N|S|  
X

e−f (K)

(1 + 2K)

(

ed(1/2,µ1,s )

1

+ (log 2)|S|

where we used the fact that
(19) and the union bound over all S

P

∈

2[K]\{1}

, we obtain
P

\ {∅}

Ys∈S
−
∞
n=1 e−nx = 1/(ex + 1) and

Ys∈S
−
∞
n=1 ne−nx = ex/(ex + 1)2. Using

!

Ys∈S

eC1(µ1,s,1/2)

(eC1(µ1,s,1/2)

,

1)2 )

≤

≤

T

\s /∈S

Ys∈S
1

E


Xt=Tinit+1

< e−f (K)

c(t)

1

{U

}


(1 + 2K)






= O(eAK−f (K)),

1 +

1

ed(1/2,µ1,s )

1

(cid:19)

−

Ys∈[K]\{1} (cid:18)

+ (log 2)K−1

1 +

eC1(µ1,s,1/2)

Ys∈[K]\{1}  

(eC1(µ1,s,1/2)

1)2 !


−

(21)



21

KOMIYAMA HONDA KASHIMA NAKAGAWA

where A = log

maxs∈[K]\{1} max

1 +

1
ed(1/2,µ1,s )−1

, log 2

1 + eC1(µ1,s,1/2)

(eC1(µ1,s,1/2)−1)2

(cid:26)

(cid:18)

(cid:18)

.
(cid:19)(cid:19)(cid:27)

Appendix D. Proof of Lemma 6

Tinit + K + 1 (i.e., after
Except for the ﬁrst loop, arm i must put into LN before
the ﬁrst loop), let τ (t) < t be the round in the previous loop in which arm l(t) is put into LN . In the
round,
Tinit + K + 1 such
= τ (t2) holds because τ (t1) and τ (t2) belong to different
that l(t1) = l(t2) = i, t1 6
loops. By using τ (t), we obtain

Jl(t)(τ (t)) is satisﬁed. With this deﬁnition, for any two rounds t1, t2 ≥

. For t
l(t) = i
}
{

= t2 ⇒

τ (t1)

≥

T

1[l(t) = i, Ni,j(t)

N Suf

i,j (δ)]

≥

U

Xt=Tinit+1

K +

T

≤

≤

K +

Xt=Tinit+K+1
T
1[
U

Xt=Tinit+1

T

Xt=Tinit+K+1
1[l(t) = i,

c(t)] +

Xt=Tinit+K+1

1[l(t) = i,

c(τ (t))] +

1[l(t) = i,

(τ (t)), Ni,j(t)

U

N Suf

i,j (δ)]

≥

T

(τ (t)), Ni,j(t)

U

N Suf

i,j (δ)].

≥

Note that the expectation of term
t, the only round in which pair (i, j) can be compared is the round of
P
once, and thus Ni,j(t)
Ni,j(τ (t))

c(t)] is bounded by Lemma 5. Between τ (t) and
that occurs at most
l(t) = j
{
1. By using this fact, we obtain

T
t=Tinit+1

1[
U

}

−

T

Xt=Tinit+K+1

T

≤

U

≤

≤

T

Xt=Tinit+K+1
1[
Ji(t),

U

Xt=Tinit+1

1[l(t) = i,

(τ (t)), Ni,j(t)

N Suf

i,j (δ)]

≥

1[l(t) = i,

Ji(τ (t)),

U

(τ (t)), Ni,j(τ (t))

N Suf

i,j (δ)

1]

−

≥

(t), Ni,j (t)

N Suf

i,j (δ)

1].

−

≥

(22)

22

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

1[
Ji(t),

U

(t), Ni,j (t)

N Suf

i,j (δ)

1]

−

≥

We can bound this term via Ii(t) as

T

Xt=Tinit+1

T

T

i,j (δ)−1⌉

Xn=⌈N Suf
T

[t=Tinit+1(cid:16)
T

≤

≤

≤

≤

i,j (δ)−1⌉

Xn=⌈N Suf
T

i,j (δ)−1⌉

Xn=⌈N Suf
T

Xn=⌈N Suf

i,j (δ)−1⌉

1

1

1







h

(cid:20)

[t=Tinit+1(cid:16)


(N Suf

i,j (δ)

−

Ij(t)

log t + f (K), Ni,j(t) = n

≤

(by

(t)

U

⇒

I1(t) = 0)

Ni,j(t) = n, Ni,j(t)d+(ˆµn

i,j, 1/2)

log t + f (K)

(cid:17)





(cid:17)





≤

i

1)d+(ˆµn

i,j, 1/2)

log T + f (K)

1

d+(ˆµn

i,j, 1/2)

d(µi,j, 1/2)
1 + δ

≤

≤

.

(cid:21)

(23)

Therefore, by letting µ

(1/2, µi,j ) be a real number such that d(µ, 1/2) = d(µi,j ,1/2)

, we

1+δ

∈
obtain from the Chernoff bound and the monotonicity of d+(
, 1/2) that
·

T

E


Xt=Tinit+1


1[
Ji(t),

U

(t), Ni,j (t)

N Suf

i,j (δ)

1]

−



≤

≥

T

P

d+(ˆµn
(cid:20)

i,j, 1/2)

d(µi,j, 1/2)
1 + δ

≤

(cid:21)

i,j (δ)−1⌉

Xn=⌈N Suf
T



≤

Xn=⌈N Suf

i,j (δ)−1⌉
1

exp (

d(µ, µi,j)n)

−

≤

exp (d(µ, µi,j))

1

−

<

1
d(µ, µi,j)

.

From the Pinsker’s inequality it is easy to conﬁrm that d(µ, µi,j) = Ω(δ2), which completes the
proof.

Appendix E. Optimal Regret Bound: Full Proof of Theorem 4

Events: Deﬁne

Yi =

ˆµ⌈α log log T ⌉
i,j

{|

µi,j|

< ∆suf
i }

−

for sufﬁciently small but ﬁxed ∆suf
in µi,j that
µi,j}i,j∈[K]. Let also
{

Yi implies ˆb⋆(i) = b⋆(i) when we let ∆suf

\i,j∈[K]
i > 0. It is easy to see from the condinuity of d+(µi,j, 1/2)
i > 0 be sufﬁciently small with respect to

Zi(t) =

.
ˆµi,b⋆(i)(t) < 1/2
}
{

23

KOMIYAMA HONDA KASHIMA NAKAGAWA

First step (regret decomposition): Like RMED1, in RMED2FH E[
bility (i.e., Lemma 5). In the following, we bound the regret under

U
(t): let

(t)] holds with high proba-

U

{Y
(B)

{z

r(t)
Zi(t)
}

+ 1
l(t) = i,
{

U

(t),

c
i ∪ Z

c
i (t)

}}

r(t)

(24)

In the following, we ﬁrst bound the terms (A) and (B), and then summarizing all terms to prove

}

|

}

|

ri(t) = 1
l(t) = i,
{
= 1
l(t) = i,
{

r(t)
(t)
}
Yi,
(t),

U

U

(A)

{z

Theorem 4.
Second step (bounding (A)): Note that,
l(t) = i,
{
ˆb⋆(i) = b⋆(i) and ˆb⋆(i)
ˆ
Oi(t). Therefore,

∈

1
l(t) = i,
{

U

(t),

Yi,

r(t)
Zi(t)
}

(t),

Yi,

Zi(t)
}

U

is a sufﬁcient condition for

1
l(t) = i, Ni,b⋆(i)(t)
{

≥

N Suf

i,b⋆(i)(δ)

∆1,i + ∆1,b⋆(i)
2

+

}

N Suf

i,b⋆(i)(δ) +

∆1,i
2

N Suf

i,b⋆(i)(δ)
log log T

.

By applying Lemma 6 with j = b⋆(i), for sufﬁciently small δ > 0 we have

1
l(t) = i, Ni,b⋆(i)(t)
{

≥

N Suf

i,b⋆(i)(δ)

O

}

≤

+ O(eAK−f (K)) + K.

1
δ2

(cid:18)

(cid:19)



In summary, term (A) is bounded as:

T

Xt=Tinit+1
T

≤

Xt=Tinit+1

T

Xt=Tinit+1

E





T

E


Xt=Tinit+1


1
l(t) = i,
{

U

(t),

Yi,

r(t)
Zi(t)
}



∆1,i + ∆1,b⋆(i)
2

≤

N Suf

i,b⋆(i)(δ) + O



log T
log log T

1
δ2

+ O

+ O(eAK−f (K)) + K.

(25)

(cid:19)
. Under
Third step (bounding (B)): Now we consider the case
(t),
}}
this event ˆb⋆(i) = b⋆(i) does not always hold but we can see that m(t)
still holds.
Furthermore, under this event arm ˆb⋆(i) is selected as m(t) at most (log log T )Ni,1(T ) + 1 times

c
c
i (t)
i ∪ Z
ˆb⋆(i), 1
}

l(t) = i,
{

{Y
∈ {

(cid:18)

(cid:19)

(cid:18)

U

24

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

due to Line 5 of Algorithm 3. By using these facts, we have,

T

E


Xt=Tinit+1

T
E

≤

E

≤


Xt=Tinit+1

T


Xt=Tinit+1


1
l(t) = i,
{

U

(t),

c
i ∪ Z

c
i (t)

}}

{Y

r(t)





T

[t′=Tinit+1

i (t′)
c

Z

}}



1
l(t) = i,
{

U

(t),

c
i ∪

{Y

1
l(t) = i, Ni,1(t)
{

≥

N Suf

i,1 (δ)

}



T

T

+ P

c
i ∪

Y




i (t′)
c

Z

N Suf
(cid:16)




[t′=Tinit+1

i,1 (δ) log log T + 1 + N Suf

i,1 (δ)

(cid:17)

O

≤

1
δ2

(cid:18)

(cid:19)



+ O(eAK−f (K)) + K + P

(by Lemma 6).

c
i ∪

Y






i (t′)
c

Z

O

N Suf
(cid:16)

i,1 (δ) log log T

(cid:17)

[t′=Tinit+1






The following lemma bounds P

Lemma 7 For RMED2FH, there exists C2 = C2(
µi,j}
{

c
i ∪

Y

T
t′=Tinit+1 Z

n

.

c
i (t′)
o
, K, α) > 0 such that

S

T

P

c
i ∪

Y






[t=Tinit+1

c
i (t)

Z

= O((log T )−C2).

In summary, term (B) is bounded as:

1
l(t) = i,
{

U

(t),

c
i ∪ Z

c
i (t)

}}

{Y

r(t)

E


Xt=Tinit+1


O

≤

1
δ2

(cid:18)

(cid:19)

+ O(eAK−f (K)) + K + O

i,1 (δ)(log T )−C2 log log T

.

(26)

N Suf
(cid:16)

(cid:17)










25

P((A) + (B))

(by Lemma 5 and inequality (24))

≤

≤

+

≤

KOMIYAMA HONDA KASHIMA NAKAGAWA

Last step (regret bound):

T

E[R(T )]

Tinit +

≤

P



{U

c(t)
}

+

P

ri(t)
(t), l(t) = i
}



{U

T

Tinit +



Xt=Tinit+1

Xt=Tinit+1


O(eAK−f (K)) +

Xi∈[K]\{1}

Xi∈[K]\{1}


O(αK 2 log log T ) + O(eAK−f (K))

∆1,i + ∆1,b⋆(i)
2

N Suf

i,b⋆(i)(δ) + O

log T
log log T

(cid:18)

(cid:19)

Xi∈[K]\{1}(







1
δ2

(cid:18)
(by (25) and (26))

(cid:19)

+ O

+ O(eAK−f (K)) + 2K + O

i,1 (δ)(log T )−C2 log log T

N Suf
(cid:16)

)

(cid:17)

(∆1,i + ∆1,b⋆(i))((1 + δ) log T )
2d(µi,b⋆(i), 1/2)

Xi∈[K]\{1}

O(αK 2 log log T ) + O(KeAK−f (K)) +

K log T
log log T

+ O

(cid:18)

K
δ2

(cid:19)

(cid:18)

(cid:19)

(cid:0)

+ O

+ O

K(log T )1−C2 log log T

+ O (Kf (K)) .

(27)

Combining (27) with the fact that O

K(log T )1−C2 log log T

completes the proof.

(cid:0)

(cid:1)
= o

K log T
log log T

(cid:1)

(cid:16)

(cid:17)

E.1. Proof of Lemma 7
and P
We bound P

c
i }

{Y

T
t=Tinit+1 Z

c
i (t)
}

separately. On the one hand,

{
S
ˆµ⌈α log log T ⌉
i,j
|

P

c
i }

{Y

= P




[i,j∈[K]

µi,j| ≥

−

∆suf

P

ˆµ⌈α log log T ⌉
i,j

{|

≤

i 

(by the Chernoff bound and Pinsker’s inequality)


Xi,j∈[K]

−

µi,j| ≥

∆suf
i }

2 exp (

2(∆suf

i )2α log log T )

−

2 (log T )−2(∆suf

i )2α = 2K 2 (log T )−2(∆suf

i )2α = O((log T )−Ca),

≤

=


Xi,j∈[K]

Xi,j∈[K]

26

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

where Ca = 2(∆suf

i )2α/K 2 > 0. On the other hand,

T

[t=Tinit+1
T

P





= P

c
i (t)

Z






[t=Tinit+1
∞






Xn=⌈α log log T ⌉
∞

−

Xn=⌈α log log T ⌉
(log T )−αd(1/2,µi,b⋆ (i))

≤

≤

≤

≤

∞

ˆµi,b⋆(i)(t) < 1/2

≤




P





Ni,b⋆(i)(t) = n, ˆµn
{
[n=⌈α log log T ⌉

i,b⋆(i) < 1/2

}



P

Ni,b⋆(i)(t) = n, ˆµn
{



i,b⋆(i) < 1/2
}

exp (

d(1/2, µi,b⋆(i))n)

(by the Chernoff bound)

(log T )−αd(1/2,µi,b⋆ (i))

1 +

= O((log T )−Cb),

∞

Xn=0

(cid:18)

exp (

d(1/2, µi,b⋆(i))n)

−

1

d(1/2, µi,b⋆(i))

1

(cid:19)

−

where Cb = αd(1/2, µi,b⋆(i)) > 0. The proof is completed by letting C2 = min (Ca, Cb) and taking
the union bound of P
c
.
i (t)
}

T
t=Tinit+1 Z

and P

c
i }

{Y

{
S

Appendix F. Facts

Fact 8 (The Chernoff bound)
Let X1, . . . , Xn be i.i.d. binary random variables. Let ˆX = 1
n
any ǫ > 0,

n

i=1 Xi and µ = E[ ˆX]. Then, for

and

P( ˆX

≥

µ + ǫ)

exp (

d(µ + ǫ, µ)n)

≤

−

P

P( ˆX

µ

ǫ)

≤

−

≤

exp (

d(µ

ǫ, µ)n).

−

−

Fact 9 (The Pinsker’s inequality)
For p, q

∈

(0, 1), the KL divergence between two Bernoulli distributions is bounded as:

d(p, q)

2(p

≥

−

q)2.

Fact 10 (A minimum difference between divergences (Lemma 13 in Honda and Takemura, 2010))
For any µ and µ2 satisfying 0 < µ2 < µ < 1. Let C1(µ, µ2) = (µ
µ2)). Then, for
any µ3 ≤

µ2)2/(2µ(1

µ2,

−

−

d(µ3, µ)

d(µ3, µ2)

C1(µ, µ2) > 0.

−

≥

27

5
1
0
2
 
n
u
J
 
9
2

 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
0
5
5
2
0
.
6
0
5
1
:
v
i
X
r
a

JMLR: Workshop and Conference Proceedings vol 40:1–27, 2015

Regret Lower Bound and Optimal Algorithm
in Dueling Bandit Problem

Junpei Komiyama
The University of Tokyo
Junya Honda
The University of Tokyo
Hisashi Kashima
Kyoto University
Hiroshi Nakagawa
The University of Tokyo

JUNPEI@KOMIYAMA.INFO

HONDA@STAT.T.U-TOKYO.AC.JP

KASHIMA@I.KYOTO-U.AC.JP

NAKAGAWA@DL.ITC.U-TOKYO.AC.JP

Abstract
We study the K-armed dueling bandit problem, a variation of the standard stochastic bandit prob-
lem where the feedback is limited to relative comparisons of a pair of arms. We introduce a tight
asymptotic regret lower bound that is based on the information divergence. An algorithm that is
inspired by the Deterministic Minimum Empirical Divergence algorithm (Honda and Takemura,
2010) is proposed, and its regret is analyzed. The proposed algorithm is found to be the ﬁrst one
with a regret upper bound that matches the lower bound. Experimental comparisons of dueling
bandit algorithms show that the proposed algorithm signiﬁcantly outperforms existing ones.
Keywords: multi-armed bandit problem, dueling bandit problem, online learning

1. Introduction

A multi-armed bandit problem is a crystallized instance of a sequential decision-making problem
in an uncertain environment, and it can model many real-world scenarios. This problem involves
conceptual entities called arms, and a forecaster who tries to identify good arms from bad ones. At
each round, the forecaster draws one of the K arms and receives a corresponding reward. The aim
of the forecaster is to maximize the cumulative reward over rounds, which is achieved by running an
algorithm that balances the exploration (acquisition of information) and the exploitation (utilization
of information).

While it is desirable to obtain direct feedback from an arm, in some cases such direct feedback is
not available. In this paper, we consider a version of the standard stochastic bandit problem called
the K-armed dueling bandit problem (Yue et al., 2009), in which the forecaster receives relative
feedback, which speciﬁes which of two arms is preferred. Although the original motivation of the
dueling bandit problem arose in the ﬁeld of information retrieval, learning under relative feedback
is universal to many ﬁelds, such as recommender systems (Gemmis et al., 2009), graphical design
(Brochu et al., 2010), and natural language processing (Zaidan and Callison-Burch, 2011), which
involve explicit or implicit feedback provided by humans.

c(cid:13) 2015 J. Komiyama, J. Honda, H. Kashima & H. Nakagawa.

KOMIYAMA HONDA KASHIMA NAKAGAWA

Related work: Here, we brieﬂy discuss the literature of the K-armed dueling bandit problem. The
RK×K, whose ij entry µi,j corresponds to the
problem involves a preference matrix M =
µi,j} ∈
{
probability that arm i is preferred to arm j.

j
j

max

≻
≻

⇔
≻

µ1,j, µj,k}
{

Most algorithms assume that the preference matrix has certain properties. Interleaved Filter (IF)
(Yue et al., 2012) and Beat the Mean Bandit (BTM) (Yue and Joachims, 2011), early algorithms
proposed for solving the dueling bandit problem, require the arms to be totally ordered, that is,
µi,j > 1/2. Moreover, IF assumes stochastic transitivity: for any triple (i, j, k) with
i
. Unfortunately, stochastic transitivity does not hold in many
k, µi,k ≥
i
µi,j, µj,k}
{
real-world settings (Yue and Joachims, 2011). BTM relaxes this assumption by introducing relaxed
k, γµ1,k ≥
stochastic transitivity: there exists γ
holds. The drawback of BTM is that it requires the explicit value of γ on which
max
the performance of the algorithm depends. Urvoy et al. (2013) considered a wide class of sequential
learning problems with bandit feedback that includes the dueling bandit problem. They proposed the
Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) algorithm, which empirically
outperforms IF and BTM for moderate K. Among the several versions of SAVAGE, the one called
Condorcet SAVAGE makes the Condorcet assumption and performed the best in their experiment.
The Condorcet assumption is that there is a unique arm that is superior to the others. Unlike the two
transitivity assumptions, the Condorcet assumption does not require the arms to be totally ordered
and is less restrictive. IF, BTM, and SAVAGE either explicitly require the number of rounds T , or
implicitly require T to determine the conﬁdence level δ.

1 such that for all pairs (j, k) with 1

≻

≥

≻

j

Recently, an algorithm called Relative Upper Conﬁdence Bound (RUCB) (Zoghi et al., 2014b)
was proven to have an O(K log T ) regret bound under the Condorcet assumption. RUCB is based on
the upper conﬁdence bound index (Lai and Robbins, 1985; Agrawal, 1995; Auer et al., 2002) that is
widely used in the ﬁeld of bandit problems. RUCB is horizonless: it does not require T beforehand
and runs for any duration. Zoghi et al. (2015) extended RUCB into the mergeRUCB algorithm
under the Condorcet assumption as well as the assumption that a portion of the preference matrix is
informative (i.e., different from 1/2). They reported that mergeRUCB outperformed RUCB when K
was large. Ailon et al. (2014) proposed three algorithms named Doubler, MultiSBM, and Sparring.
MultiSBM is endowed with an O(K log T ) regret bound and Sparring was reported to outperform
IF and BTM in their simulation. These algorithms assume that the pairwise feedback is generated
from the non-observable utilities of the selected arms. The existence of the utility distributions
associated with individual arms restricts the structure of the preference matrix.

In summary, most algorithms either has O(K 2 log T ) regret under the Condorcet assumption
(SAVAGE) or require additional assumptions to achieve O(K log T ) regret (IF, BTM, MultiSBM,
and mergeRUCB). To the best of our knowledge, RUCB is the only algorithm with an O(K log T )
regret bound1. The main difﬁculty of the dueling bandit problem lies in that, there are K
1
candidates of actions to test “how good” each arm i is. A naive use of the conﬁdence bound requires
every pair of arms to be compared O(log T ) times and yields an O(K 2 log T ) regret bound.
Contribution: In this paper, we propose an algorithm called Relative Minimum Empirical Diver-
gence (RMED). This paper contributes to our understanding of the dueling bandit problem in the
following three respects.

−

•

The regret lower bound: Some studies (e.g., Yue et al., 2012) have shown that the K-armed
dueling bandit problem has a Ω(K log T ) regret lower bound. In this paper, we further ana-

1. Zoghi et al. (2013) ﬁrst proposed RUCB with an O(K 2 log T ) regret bound and later modiﬁed it by adding a ran-

domization procedure to assure O(K log T ) regret in Zoghi et al. (2014b).

2

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

lyze this lower bound to obtain the optimal constant factor for models satisfying the Condorcet
assumption. Furthermore, we show that the lower bound is the same under the total order as-
sumption. This means that optimal algorithms under the Condorcet assumption also achieve
a lower bound of regret under the total order assumption even though such algorithms do not
know that the arms are totally ordered.
An optimal algorithm: The regret of RMED is not only O(K log T ), but also optimal in
the sense that its constant factor matches the asymptotic lower bound under the Condorcet
assumption. RMED is the ﬁrst optimal algorithm in the study of the dueling bandit problem.
Empirical performance assessment: The performance of RMED is extensively evaluated by
using ﬁve datasets: two synthetic datasets, one including preference data, and two including
ranker evaluations in the information retrieval domain.

•

•

2. Problem Setup

∈

.
The K-armed dueling bandit problem involves K arms that are indexed as [K] =
}
RK×K be a preference matrix whose ij entry µi,j corresponds to the probability that
Let M
arm i is preferred to arm j. At each round t = 1, 2, . . . , T , the forecaster selects a pair of arms
[K]2, then receives a relative feedback ˆXl(t),m(t)(t)
Bernoulli(µl(t),m(t)) that
(l(t), m(t))
indicates which of (l(t), m(t)) is preferred. By deﬁnition, µi,j = 1
[K]
and µi,i = 1/2.

∼
µj,i holds for any i, j

1, 2, . . . , K
{

−

∈

∈

Let Ni,j(t) be the number of comparisons of pair (i, j) and ˆµi,j(t) be the empirical estimate of
µi,j at round t. In building statistics by using the feedback, we treat pairs without taking their order
+ 1
t′=1(1
l(t′) =
l(t′) = i, m(t′) = j
into consideration. Therefore, for i
{
}
{
t−1
l(t′) = i, m(t′) = j, ˆXl(t′ ),m(t′)(t′) = 1
+ 1
t′=1(1
j, m(t′) = i
l(t′) =
) and µi,j = (
P
{
}
}
{
j, m(t′) = i, ˆXl(t′),m(t′)(t′) = 0
))/Ni,j (t), where 1[
= i, let
] is the indicator function. For j
P
·
}
Ni>j(t) be the number of times i is preferred over j. Then, ˆµi,j(t) = Ni>j(t)/Ni,j(t), where we
set 0/0 = 1/2 here. Let ˆµi,i(t) = 1/2.

= j, Ni,j(t) =

t−1

Throughout this paper, we will assume that the preference matrix has a Condorcet winner
.
(Urvoy et al., 2013). Here we call an arm i the Condorcet winner if µi,j > 1/2 for any j
i
}
\{
Without loss of generality, we will assume that arm 1 is the Condorcet winner. The set of preference
matrices which have a Condorcet winner is denoted by
MC. We also deﬁne the set of preference
matrices satisfying the total order by
µi,j < 1/2 induces
⇔
a total order iff

Mo ⊂ MC; that is, the relation i

[K]

≺

∈

j

Let ∆i,j = µi,j −

1/2. We deﬁne the regret per round as r(t) = (∆1,i + ∆1,j)/2 when the
pair (i, j) is compared. The expectation of the cumulative regret, E[R(T )] = E
is used
to measure the performance of an algorithm. The regret increases at each round unless the selected
pair is (l(t), m(t)) = (1, 1).

T
t=1 r(t)
i

hP

µi,j} ∈ Mo.
{

2.1. Regret lower bound in the K-armed dueling bandits

In this section we provide an asymptotic regret lower bound when T
. Let the superiors of
, that is, the set of arms that is preferred to i on average.
arm i be a set
[K], µi,j < 1/2
}
The essence of the K-armed dueling bandit problem is how to eliminate each arm i
by
making sure that arm i is not the Condorcet winner. To do so, the algorithm uses some of the arms
in

1
}
\ {

Oi =

→ ∞

j
{

[K]

j
|

∈

∈

Oi and compares i with them.

3

KOMIYAMA HONDA KASHIMA NAKAGAWA

A dueling bandit algorithm is strongly consistent for model

M ⊂ MC iff it has E[R(T )] =
. The following lemma is on the number of comparisons

o(T a) regret for any a > 0 and any M
of suboptimal arm pairs.

∈ M

Lemma 1 (The lower bound on the number of suboptimal arm draws) (i) Let an arm i
and preference matrix M
MC, we have

1
}
\ {
∈ MC be arbitrary. Given any strongly consistent algorithm for model

[K]

∈

E



q + (1


Xj∈Oi

−
Mo.

d(µi,j, 1/2)Ni,j (T )

(1

o(1)) log T,

(1)

≥




−

p) log 1−p
where d(p, q) = p log p
with parameters p and q. (ii) Furthermore, inequality (1) holds for any M
consistent algorithm for

1−q is the KL divergence between two Bernoulli distributions
∈ Mo given any strongly



∈ Oi, an algorithm needs to make log T /d(µi,j, 1/2)
Lemma 1 states that, for arbitrary arm j
comparisons between arms i and j to be convinced that arm i is inferior to arm j and thus i is not the
Condorcet winner. Since the regret increase per round of comparing arm i with j is (∆1,i + ∆1,j)/2,
eliminating arm i by comparing it with j incurs a regret of

(∆1,i + ∆1,j) log T
2d(µi,j, 1/2)

.

(2)

(3)

Therefore, the total regret is bounded from below by comparing each arm i with an arm j that
minimizes (2) and the regret lower bound is formalized in the following theorem.

Theorem 2 (The regret lower bound) (i) Let the preference matrix M
strongly consistent algorithm for model

∈ MC be arbitrary. For any

MC,

lim inf
T →∞

E[R(T )]

log T ≥

min
j∈Oi

∆1,i + ∆1,j
2d(µi,j, 1/2)

Xi∈[K]\{1}
holds. (ii) Furthermore, inequality (3) holds for any M
rithm for

Mo.

∈ Mo given any strongly consistent algo-

The proof of Lemma 1 and Theorem 2 can be found in Appendix B. The proof of Lemma 1 is
similar to that of Lai and Robbins (1985, Theorem 1) for the standard multi-armed bandit problem
but differs in the following point that is characteristic to the dueling bandit. To achieve a small regret
in the dueling bandit, it is necessary to compare the arm i with itself if i is the Condorcet winner.
However, we trivially know that µi,i = 1/2 without sampling and such a comparison yields no
information to distinguish possible preference matrices. We can avoid this difﬁculty by evaluating
Ni,j and Ni,i in different ways.

3. RMED1 Algorithm

In this section, we ﬁrst introduce the notion of empirical divergence. Then, on the basis of the
empirical divergence, we formulate the RMED1 algorithm.

4

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

Algorithm 1 Relative Minimum Empirical Divergence (RMED) Algorithm

1: Input: K arms, f (K)

0. α > 0 (RMED2FH, RMED2). T (RMED2FH).

2: L

1
← (
α log log T
⌈

⌉

≥
(RMED1, RMED2)
(RMED2FH)

.

For each arm i

3: Initial phase: draw each pair of arms L times. At the end of this phase, t = L(K
4: if RMED2FH then
5:
6: end if
7: LC, LR ←
8: while t
≤
9:

∈
.
[K], LN ← ∅
T do

[K], ﬁx ˆb⋆(i) by (6).

if RMED2 then

1)K/2.

−

Draw all pairs (i, j) until it reaches Ni,j(t)

α log log t. t

t + 1 for each draw.

≥

←

end if
for l(t)

∈

LC in an arbitrarily ﬁxed order do

Select m(t) by using

Algorithm 2
Algorithm 3

(RMED1)
(RMED2, RMED2FH)

.

(

.
l(t)
LR \ {
}
(without a duplicate) for any j /
j
LN ∪ {
∈

LR such that

Jj(t) holds.

Draw arm pair (l(t), m(t)).
LR ←
LN ←
t
←
end for
LC, LR ←

.
LN , LN ← ∅

t + 1.

}

19:
20: end while

10:

11:

12:

13:

14:

15:

16:

17:

18:

Algorithm 2 RMED1 subroutine for selecting m(t)
1: ˆ
j
Ol(t)(t)
[K]
l(t)
\ {
← {
∈
}|
ˆ
Ol(t)(t) or ˆ
2: if i∗(t)
Ol(t)(t) =
∈
i∗(t).
3: m(t)
←
4: else
5: m(t)
6: end if

arg minj6=l(t) ˆµl(t),j(t).

ˆµl(t),j(t)
then

1/2
}

←

≤

∅

3.1. Empirical divergence and likelihood function

In inequality (1) of Section 2.1, we have seen that
d(µi,j, 1/2)Ni,j (T ), the sum of the di-
vergence between µi,j and 1/2 multiplied by the number of comparisons between i and j, is the
characteristic value that deﬁnes the minimum number of comparisons. The empirical estimate of
this value is fundamentally useful for evaluating how unlikely arm i is to be the Condorcet winner.
Let the opponents of arm i at round t be the set ˆ
. Note that,
, ˆµi,j (t)
i
1/2
Oi(t) =
\{
}
}
unlike the superiors
Oi(t) for each arm i are deﬁned in terms of the empirical
averages, and thus the algorithms know who the opponents are. Let the empirical divergence be

Oi, the opponents ˆ

j
{

[K]

j∈Oi

j
|

P

≤

∈

Ii(t) =

Ni,j(t)d(ˆµi,j(t), 1/2).

Xj∈ ˆOi(t)

5

KOMIYAMA HONDA KASHIMA NAKAGAWA

−

Ii(t)) can be considered as the “likelihood” that arm i is the Condorcet winner. Let

The value exp (
i∗(t) = arg mini∈[K] Ii(t) (ties are broken arbitrarily) and I ∗(t) = Ii∗(t)(t). By deﬁnition, I ∗(t)
≥
0. RMED is inspired by the Deterministic Minimum Empirical Divergence (DMED) algorithm
(Honda and Takemura, 2010). DMED, which is designed for solving the standard K-armed bandit
problem, draws arms that may be the best one with probability Ω(1/t), whereas RMED in the
dueling bandit problem draws arms that are likely to be the Condorcet winner with probability
Ω(1/t). Namely, any arm i that satisﬁes

Ji(t) =

Ii(t)
{

−

I ∗(t)

log t + f (K)
}

≤

(4)

is the candidate of the Condorcet winner and will be drawn soon. Here, f (K) can be any non-
negative function of K that is independent of t. Algorithm 1 lists the main routine of RMED.
There are several versions of RMED. First, we introduce RMED1. RMED1 initially compares all
pairs once (initial phase). Let Tinit = (K
1)K/2 be the last round of the initial phase. From
t = Tinit + 1, it selects the arm by using a loop. LC = LC(t) is the set of arms in the current loop,
and LR = LR(t)
LC(t) is the remaining arms of LC that have not been drawn yet in the current
loop. LN = LN (t) is the set of arms that are going to be drawn in the next loop. An arm i is put
. By deﬁnition, at least one arm (i.e. i∗(t) at the end
into LN when it satisﬁes
of the current loop) is put into LN in each loop. For arm l(t) in the current loop, RMED1 selects
m(t) (i.e. the comparison target of l(t)) determined by Algorithm 2.

{Ji(t)

LR(t)

i /
∈

∩ {

}}

−

⊂

The following theorem, which is proven in Section 5, describes a regret bound of RMED1.

Theorem 3 For any sufﬁciently small δ > 0, the regret of RMED1 is bounded as:

E[R(T )]

≤

((1 + δ) log T + f (K))∆1,i
2d(µi,1, 1/2)

+ O(K 2) + O

+ O(KeAK−f (K)),

Xi∈[K]\{1}
µi,j}i,j∈[K]) is a constant as a function of T . Therefore, by letting δ = log−1/3 T
where A = A(
{
and choosing an f (K) = cK 1+ǫ for arbitrary c, ǫ > 0, we obtain

K
δ2

(cid:18)

(cid:19)

E[R(T )]

≤

Xi∈[K]\{1}

∆1,i log T
2d(µi,1, 1/2)

+ O(K 2+ǫ) + O(K log2/3 T ).

3.2. Gap between the constant factor of RMED1 and the lower bound
From the lower bound of Theorem 2, the O(K log T ) regret bound of RMED1 is optimal up to
a constant factor. Moreover, the constant factor matches the regret lower bound of Theorem 2 if
b⋆(i) = 1 for all i

where

[K]

∈

1
}
\ {

b⋆(i) = arg min

j∈Oi

∆1,i + ∆1,j
d(µi,j, 1/2)

.

(5)

Here we deﬁne d+(p, q) = d(p, q) if p < q and 0 otherwise, and x/0 = +
. Note that, there
can be ties that minimize the RHS of (5). In that case, we may choose any of the ties as b⋆(i) to
eliminate arm i. For ease of explanation, we henceforth will assume that b⋆(i) is unique, but our
results can be easily extended to the case of ties.

∞

We claim that b⋆(i) = 1 holds in many cases for the following mathematical and practical
= 1, is (∆1,i + ∆1,j)/2, whereas it is simply

reasons. (i) The regret of drawing a pair (i, j), j

6

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

Algorithm 3 Subroutine for selecting m(t) in RMED2 and RMED2FH

Update ˆb⋆(l(t)) by (6).

1: if RMED2 then
2:
3: end if
4: ˆ
Ol(t)(t)
5: if ˆb⋆(l(t))

j
← {

[K]

l(t)

∈
\ {
ˆ
Ol(t)(t) and
∈
ˆb⋆(l(t)).

←

6: m(t)
7: else
8:
9: end if

Select m(t) by using Algorithm 2.

≤

ˆµl(t),j(t)
}|
Nl(t),i∗(t)(t)
Nl(t),i∗(t)(t)

.
1/2
}
Nl(t),ˆb⋆(l(t))(t)/ log log t
Nl(t),ˆb⋆(l(t))(t)/ log log T (RMED2FH)

(RMED2)

(

≥

≥

then

≥

∆1,i/2 for the pair (i, 1). Thus, d+(µi,j, 1/2) has to be much larger than d+(µi,1, 1/2) in order to
satisfy b⋆(i) = j. (ii) The Condorcet winner usually wins over the other arms by a large margin,
and therefore, d+(µi,1, 1/2)
d+(µi,j, 1/2). For example, in the preference matrix of Example 1
(Table 1(a)), b⋆(3) = 1 as long as q < 0.79. Example 2 (Table 1(b)) is a preference matrix based
on six retrieval functions in the full-text search engine of ArXiv.org (Yue and Joachims, 2011)2. In
Example 2, b⋆(i) = 1 holds for all i, even though µ1,4 < µ2,4. In the case of a 16-ranker evalua-
tion based on the Microsoft Learning to Rank dataset (details are given in Section 4), occasionally
= 1 occurs, but the difference between the regrets of drawing arm 1 and b⋆(i) is fairly small
b⋆(i)
(smaller than 1.2% on average). Nevertheless, there are some cases in which comparing arm i with
1 is not such a clever idea. Example 3 (Table 1(c)) is a toy example in which comparing arm i with
b⋆(i)
= 1 makes a large difference. In Example 3, it is clearly better to draw pairs (2, 4), (3, 2) and
(4, 3) to eliminate arms 2, 3, and 4, respectively. Accordingly, it is still interesting to consider an
algorithm that reduces regret by comparing arm i with b⋆(i).

Table 1: Three preference matrices. In each example, the value at row i, column j is µi,j.

(a) Example 1
3
2
0.7
0.7
0.5
q
0.5
1-q

1
0.5
0.3
0.3

1
2
3

(b) Example 2

1
0.50
0.45
0.45
0.46
0.39
0.39

2
0.55
0.50
0.45
0.45
0.42
0.40

3
0.55
0.55
0.50
0.46
0.49
0.44

4
0.54
0.55
0.54
0.50
0.46
0.50

1
2
3
4
5
6

5
0.61
0.58
0.51
0.54
0.50
0.49

6
0.61
0.60
0.56
0.50
0.51
0.50

(c) Example 3

1
0.5
0.4
0.4
0.4

2
0.6
0.5
0.1
0.9

3
0.6
0.9
0.5
0.1

4
0.6
0.1
0.9
0.5

1
2
3
4

3.3. RMED2 Algorithm

We here propose RMED2, which gracefully estimates b⋆(i) during a bandit game and compares
arm i with b⋆(i). RMED2 and RMED1 share the main routine (Algorithm 1). The subroutine of
RMED2 for selecting m(t) is shown in Algorithm 3. Unlike RMED1, RMED2 keeps drawing pairs
of arms (i, j) at least α log log t times (Line 10 in Algorithm 1). The regret of this exploration is
insigniﬁcant since O(log log T ) = o(log T ). Once all pairs have been explored more than α log log t

2. In the original preference matrix of Yue and Joachims (2011), µ2,4 6= 1 − µ4,2. To satisfy µ2,4 = 1 − µ4,2, we

replaced µ2,4 and µ4,2 of the original with (µ2,4 − µ4,2 + 1)/2 and (µ4,2 − µ2,4 + 1)/2, respectively.

7

KOMIYAMA HONDA KASHIMA NAKAGAWA

times, RMED2 goes to the main loop. RMED2 determines m(t) by using Algorithm 2 based on the
estimate of b⋆(i) given by

ˆb⋆(i) = arg min
j∈[K]\{i}

ˆ∆i∗(t),i + ˆ∆i∗(t),j
d+(ˆµi,j(t), 1/2)

,

(6)

where ties are broken arbitrarily, ˆ∆i,j = 1/2
. Intuitively, RMED2
tries to select m(t) = ˆb⋆(i) for most rounds, and occasionally explores i∗(t) in order to reduce the
regret increase when RMED2 fails to estimate the true b⋆(i) correctly.

ˆµi,j and we set x/0 = +

∞

−

3.4. RMED2FH algorithm
Although we believe that the regret of RMED2 is optimal, the analysis of RMED2 is a little bit
complicated since it sometimes breaks the main loop and explores from time to time. For ease of
analysis, we here propose RMED2 Fixed Horizon (RMED2FH, Algorithm 1 and 3), which is a
“static” version of RMED2. Essentially, RMED2 and RMED2FH have the same mechanism. The
differences are that (i) RMED2FH conducts an α log log T exploration in the initial phase. After
the initial phase (ii) ˆb⋆(i) for each i is ﬁxed throughout the game. Note that, unlike RMED1 and
RMED2, RMED2FH requires the number of rounds T beforehand to conduct the initial α log log T
draws of each pair. The following Theorem shows the regret of RMED2FH that matches the lower
bound of Theorem 2.

Theorem 4 For any sufﬁciently small δ > 0, the regret of RMED2FH is bounded as:

E[R(T )]

≤

Xi∈[K]\{1}

(∆1,i + ∆1,b⋆(i))((1 + δ) log T )
2d(µi,b⋆(i), 1/2)

+ O(αK 2 log log T ) + O(KeAK−f (K))

K log T
log log T

+ O

(cid:18)

K
δ2

(cid:19)

(cid:18)

(cid:19)

+ O

+ O (Kf (K)) ,

(7)

where A = A(
µi,j}
{
choosing an f (K) = cK 1+ǫ (c, ǫ > 0) we obtain

) > 0 is a constant as a function of T . By setting δ = O((log T )−1/3) and

E[R(T )]

≤

Xi∈[K]\{1}

(∆1,i + ∆1,b⋆(i)) log T
2d(µi,b⋆(i), 1/2)

+ O(αK 2 log log T ) + O

+ O

K 2+ǫ

.

K log T
log log T

(cid:18)

(cid:19)

(cid:0)

(cid:1)
(8)

Note that all terms except the ﬁrst one in (8) are o(log T ). From Theorems 2 and 4 we see that (i)
RMED2FH is asymptotically optimal under the Condorcet assumption and (ii) the logarithmic term
on the regret bound of RMED2FH cannot be improved even if the arms are totally ordered and the
forecaster knows of the existence of the total order. The proof sketch of Theorem 4 is in Section 5.

4. Experimental Evaluation

To evaluate the empirical performance of RMED, we conducted simulations3 with ﬁve bandit
datasets (preference matrices). The datasets are as follows:

3. The source code of the simulations is available at https://github.com/jkomiyama/duelingbanditlib.

8

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

4

10

t
e
r
g
e
r
 
:
)
t
(
R

3

10

2

10

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

4

10

t
e
r
g
e
r
 
:
)
t
(
R

3

10

2

10

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

4

10

3

10

2

10

1

10

t
e
r
g
e
r
 
:
)
t
(
R

5

10

4

10

3

10

t
e
r
g
e
r
 
:
)
t
(
R

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

4

10

t
e
r
g
e
r
 
:
)
t
(
R

3

10

2

10

6

10

5

10

4

10

t
e
r
g
e
r
 
:
)
t
(
R

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

1

10

2

10

3

10

4

10
t: round

(a) Six rankers

5

10

6

10

2

10

3

10

5

10

6

10

4

10
t: round

(b) Cyclic

1

10

2

10

3

10

4

10
t: round

5

10

6

10

(c) Arithmetic

1

10

2

10

3

10

4

10

5

10

2

10

3

10

4

10

5

10
t: round

6

10

7

10

3

10

3

10

4

10

5

10
t: round

6

10

7

10

(e) MSLR K = 16

(f ) MSLR K = 64

t: round

(d) Sushi

Figure 1: Regret-round log-log plots of algorithms.

Six rankers is the preference matrix based on the six retrieval functions in the full-text search engine
of ArXiv.org (Table 1(b)).
Cyclic is the artiﬁcial preference matrix shown in Table 1(c). This matrix is designed so that the
comparison of i with 1 is not optimal.
Arithmetic dataset involves eight arms with µi,j = 0.5 + 0.05(j
Sushi dataset is based on the Sushi preference dataset (Kamishima, 2003) that contains the pref-
erences of 5, 000 Japanese users as regards 100 types of sushi. We extracted the 16 most popular
types of sushi and converted them into arms with µi,j corresponding to the ratio of users who prefer
sushi i over j. The Condorcet winner is the mildly-fatty tuna (chu-toro).
MSLR: We tested submatrices of a 136
136 preference matrix from Zoghi et al. (2015), which is
derived from the Microsoft Learning to Rank (MSLR) dataset (Microsoft Research, 2010; Qin et al.,
2010) that consists of relevance information between queries and documents with more than 30K
queries. Zoghi et al. (2015) created a ﬁnite set of rankers, each of which corresponds to a ranking
feature in the base dataset. The value µi,j is the probability that the ranker i beats ranker j based on
the navigational click model (Hofmann et al., 2013). We randomly extracted K = 16, 64 rankers
in our experiments and made sub preference matrices. The probability that the Condorcet winner

i) and has a total order.

−

×

9

KOMIYAMA HONDA KASHIMA NAKAGAWA

500

400

300

200

100

t
e
r
g
e
r
 
:
)
t
(
R

RMED1
RMED2
RMED2FH a=1
RMED2FH a=3
RMED2FH a=10
LB1
TrueLB

100

80

60

40

20

t
e
r
g
e
r
 
:
)
t
(
R

RMED1
RMED2
RMED2FH a=1
RMED2FH a=3
RMED2FH a=10
LB1
TrueLB

2000

1500

1000

500

t
e
r
g
e
r
 
:
)
t
(
R

RMED1
RMED2
RMED2FH a=1
RMED2FH a=3
RMED2FH a=10
LB1
TrueLB

0
10

2

3

10

4

10
t: round

5

10

6

10

0
10

2

3

10

4

10
t: round

5

10

6

10

0
10

2

3

10

5

4

10
10
t: round

6

10

7

10

(a) Six rankers

(b) Cyclic

(c) MSLR K = 16

Figure 2: Regret-round semilog plots of RMED compared with theoretical bounds. We set f (K) =

0.3K 1.01 for all algorithms, and α = 3 for RMED2.

exists in the subset of the rankers is high (more than 90%, c.f. Figure 1 in Zoghi et al. (2014a)), and
we excluded the relatively small case where the Condorcet winner does not exist.

A Condorcet winner exists in all datasets. In the experiments, the regrets of the algorithms were

averaged over 1, 000 runs (Six rankers, Cyclic, Arithmetic, and Sushi), or 100 runs (MSLR).

4.1. Comparison among algorithms

We compared the IF, BTM with γ = 1.2, RUCB with α = 0.51, Condorcet SAVAGE with δ = 1/T ,
MultiSBM and Sparring with α = 3, and RMED algorithms. There are two versions of RUCB:
the one that uses a randomizer in choosing l(t) (Zoghi et al., 2014b), and the one that does not
(Zoghi et al., 2013). We implemented both and found that the two perform quite similarly: we show
the result of the former one in this paper. We set f (K) = 0.3K 1.01 for all RMED algorithms and
set α = 3 for RMED2 and RMED2FH. The effect of f (K) is studied in Appendix A. Note that IF
and BTM assume a total order among arms, which is not the case with the Cyclic, Sushi, and MSLR
datasets. MultiSBM and Sparring assume the existence of the utility of each arm, which does not
allow a cyclic preference that appears in the Cyclic dataset.

Figure 1 plots the regrets of the algorithms. In all datasets RMED signiﬁcantly outperforms
RUCB, the next best excluding the different versions of RMED. Notice that the plots are on a base
10 log-log scale. In particular, regret of RMED1 is more than twice smaller than RUCB on all
datasets other than Cyclic, in which RMED2 performs much better. Among the RMED algorithms,
RMED1 outperforms RMED2 and RMED2FH on all datasets except for Cyclic, in which comparing
arm i
= 1 with arm 1 is inefﬁcient. RMED2 outperforms RMED2FH in the ﬁve of six datasets: this
could be due to the fact that RMED2FH does not update ˆb⋆(i) for ease of analysis.

4.2. RMED and asymptotic bound

Figure 2 compares the regret of RMED with two asymptotic bounds. LB1 denotes the regret bound
of RMED1. TrueLB is the asymptotic regret lower bound given by Theorem 2.
, the slope of RMED1 should converge to LB1, and the ones
RMED1 and RMED2: When T
of RMED2 and RMED2FH should converge to TrueLB. On Six rankers, LB1 is exactly the same

→ ∞

10

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

as TrueLB, and the slope of RMED1 converges to this TrueLB. In Cyclic, the slope of RMED2
converges to TrueLB, whereas that of RMED1 converges to LB1, from which we see that RMED2
is actually able to estimate b⋆(i)
= 1 correctly. In MSLR K = 16, LB1 and TrueLB are very close
(the difference is less than 1.2%), and RMED1 and RMED2 converge to these lower bounds.
RMED2FH with different values of α: We also tested RMED2FH with several values of α. On
the one hand, with α = 1, the initial phase of RMED2FH is too short to identify b⋆(i); as a result
it performs poorly on the Cyclic dataset. On the other hand, with α = 10, the initial phase was
too long, which incurs a practically non-negligible regret on the MSLR K = 16 dataset. We also
tested several values of parameter α in RMED2FH. We omit plots of RMED2 with α = 1, 10 for
the sake of readability, but we note that in our datasets the performance of RMED2 is always better
than or comparable with the one of RMED2FH under the same choice of α, although the optimality
of RMED2 is not proved unlike RMED2FH.

5. Regret Analysis

This section provides two lemmas essential for the regret analysis of RMED algorithms and proves
the asymptotic optimality of RMED1 based on these lemmas. A proof sketch on the optimal regret
of RMED2FH is also given.

The crucial property of RMED is that, by constantly comparing arms with the opponents, the

true Condorcet winner (arm 1) actually beats all the other arms with high probability. Let

U

(t) =

.
ˆµ1,i(t) > 1/2
}
{
\i∈[K]\{1}
ˆµ1,i(t) < 1/2 for all i

U

(t), ˆµi,1(t) = 1

Under
(t)
∈
implies that i∗(t) = arg mini∈[K] Ii(t) is unique with i∗(t) = 1 and I ∗(t) = I1(t) = 0. Lemma
c(t) occurs is constant in T , where the
5 below shows that the average number of rounds that
superscript c denotes the complement.

, and thus, Ii(t) > 0. Therefore,

1
}
\ {

[K]

−

U

U

Lemma 5 When RMED1 or RMED2FH is run, the following inequality holds:

T

E


Xt=Tinit+1


c(t)

1

{U

}



= O(eAK−f (K)),

(9)

where A = A(
µi,j}
{

) > 0 is a constant as a function of T .

times in the initial phase, we deﬁne
Note that, since RMED2FH draws each pair
1)K/2 for RMED2FH. We give a proof of this lemma in Appendix C.
(K
Tinit =
⌉
Intuitively, this lemma can be proved from the facts that arm 1 is drawn within roughly eI1(t)−f (K)
rounds and I1(t) is not very large with high probability.

α log log T
⌈

α log log T
⌈

−

⌉

Next, for i

[K]

∈

1
}
\ {

and j

∈ Oi, let
i,j (δ) =

N Suf

(1 + δ) log T + f (K)
d(µi,j, 1/2)

+ 1,

which is a sufﬁcient number of comparisons of i with j to be convinced that the arm i is not the
Condorcet winner. The following lemma states that if pair (i, j) is drawn N Suf
i,j (δ) times then i is
rarely selected as l(t) again.

11

KOMIYAMA HONDA KASHIMA NAKAGAWA

Lemma 6 When RMED1 or RMED2FH is run, for i

[K]

∈

, j

1
}
\ {

∈ Oi,

T

E


Xt=Tinit+1


1
l(t) = i, Ni,j(t)
{

≥

N Suf

i,j (δ)

= O

+ O(eAK−f (K)) + K.

1
δ2

(cid:18)

(cid:19)

}



We prove this lemma in Appendix D based on the Chernoff bound.

Now we can derive the regret bound of RMED1 based on these lemmas.

Proof of Theorem 3: Since
be decomposed as:

U

(t) implies m(t) = 1 in RMED1, the regret increase per round can

r(t) = 1

c(t)
}

+

{U

∆1,i
2

1
l(t) = i, m(t) = 1,
{

.
(t)
}

U

(10)

Xi∈[K]\{1}

Using Lemmas 5 and 6, we obtain

E[R(T )]

Tinit +

≤

K(K
2

−

≤

1)

+E



1)

K(K
2

−

≤

T

[r(t)]

Xt=Tinit+1
T

c(t)

1

{U

Xt=Tinit+1
+ O(eAK−f (K)) +



∆1,i
2  

+
Xi∈[K]\{1}

}



∆1,i
2

(cid:18)

Xi∈[K]\{1}

T

t=1
X

1
δ2

(cid:18)

(cid:19)

N Suf

i,1 (δ)+

1[l(t) = i, m(t) = 1, Ni,1(t)

N Suf

i,1 (δ)]

!

N Suf

i,1 (δ) + O

+ O(eAK−f (K)) + K

,

≥

(cid:19)

which immediately completes the proof of Theorem 3.

We also prove Theorem 4 on the optimality of RMED2FH based on Lemmas 5 and 6. Because

∈

[K]

1
}
\ {

. There exists C2 > 0 such that, for each l(t) = i, (i) with probability 1

c(t) does
(t), we decompose the regret into the contributions

the full proof in Appendix E is a little bit lengthy, here we give its brief sketch.
Proof sketch of Theorem 4 (RMED2FH): Similar to Theorem 3, we use the fact that the
not occur very often (i.e., Lemma 5). Under
of each arm i
−
O((log T )−C2) RMED2FH successfully estimates ˆb⋆(i) = b⋆(i) and selects m(t) = b⋆(i) for most
rounds. The optimal O(log T ) term comes from the comparison of i and b⋆(i). Arm 1 is also drawn
for O(log T / log log T ) = o(log T ) times. On the other hand, (ii) with probability O((log T )−C2),
RMED2FH fails to estimate b⋆(i) correctly. By occasionally comparing arm i with arm 1, we
can bound the regret increase by O(log T log log T ). Since O((log T )−C2
log T log log T ) =
o(log T ), this regret does not affect the O(log T ) factor.

×

U

U

6. Discussion

We proved the regret lower bound in the dueling bandit problem. The RMED algorithm is based
on the likelihood that the arm is the Condorcet winner. RMED is proven to have the matching
regret upper bound. The empirical evaluation revealed that RMED signiﬁcantly outperforms the
state-of-the-art algorithms. To conclude this paper, we mention three directions of future work.

12

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

First, when a Condorcet winner does not necessarily exist, the Copeland bandits (Urvoy et al.,
2013) are a natural extension of our problem. Thus, seeking an effective algorithm for solving this
problem will be interesting. As is well known in the ﬁeld of voting theory, there are several other
criteria of winners that are incompatible with the Condorcet / Copeland bandits, such as the Borda
winner (Urvoy et al., 2013). Comparing several criteria or developing an algorithm that outputs
more than one of these winners should be interesting directions of future work.

Second, another direction is sequential preference elicitation problems under relative feedback
that goes beyond the binary preference over pairs, such as multiscale feedback and/or preferences
among three or more items.

Third, in the standard bandit problem, it is reported that KL-UCB+ (Lai, 1987; Garivier and Capp´e,

2011) performs better than DMED. A study of a UCB-based optimal algorithm for the dueling ban-
dits can yield an algorithm that outperforms RMED.

Acknowledgements

We thank the anonymous reviewers for their useful comments. This work was supported in part by
JSPS KAKENHI Grant Number 26106506.

13

KOMIYAMA HONDA KASHIMA NAKAGAWA

References

R. Agrawal. Sample mean based index policies with O(log n) regret for the multi-armed bandit

problem. Advances in Applied Probability, 27:1054–1078, 1995.

Nir Ailon, Zohar Shay Karnin, and Thorsten Joachims. Reducing dueling bandits to cardinal ban-

dits. In ICML, pages 856–864, 2014.

Peter Auer, Nicol´o Cesa-bianchi, and Paul Fischer. Finite-time Analysis of the Multiarmed Bandit

Problem. Machine Learning, 47:235–256, 2002.

Eric Brochu, Tyson Brochu, and Nando de Freitas. A bayesian interactive optimization approach
In Proceedings of the 2010 Eurographics/ACM SIGGRAPH

to procedural animation design.
Symposium on Computer Animation, SCA 2010, Madrid, Spain, 2010, pages 103–112, 2010.

S´ebastien Bubeck. Bandits Games and Clustering Foundations. Theses, Universit´e des Sciences et

Technologie de Lille - Lille I, June 2010.

Aur´elien Garivier and Olivier Capp´e. The KL-UCB algorithm for bounded stochastic bandits and

beyond. In COLT, pages 359–376, 2011.

Marco De Gemmis, Leo Iaquinta, Pasquale Lops, Cataldo Musto, Fedelucio Narducci, and Giovanni
In In Preference Learning (PL-09)

Semeraro. Preference learning in recommender systems.
ECML/PKDD-09 Workshop, 2009.

Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. Fidelity, soundness, and efﬁciency of
interleaved comparison methods. Transactions on Information Systems, 31(4):17:1–43, 2013.

Junya Honda and Akimichi Takemura. An Asymptotically Optimal Bandit Algorithm for Bounded

Support Models. In COLT, pages 67–79, 2010.

Toshihiro Kamishima. Nantonac collaborative ﬁltering: recommendation based on order responses.

In KDD, pages 583–588, 2003.

1091–1114, 09 1987.

T. L. Lai. Adaptive treatment allocation and the multi-armed bandit problem. Ann. Statist., 15(3):

T. L. Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in

Applied Mathematics, 6(1):4–22, 1985.

Microsoft Research.

Microsoft Learning
to Rank Datasets,
http://research.microsoft.com/en-us/projects/mslr/.

2010.

URL

Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. LETOR: A benchmark collection for research on

learning to rank for information retrieval. Inf. Retr., 13(4):346–374, 2010.

Tanguy Urvoy, Fabrice Cl´erot, Rapha¨el Feraud, and Sami Naamane. Generic exploration and k-

armed voting bandits. In ICML, pages 91–99, 2013.

Yisong Yue and Thorsten Joachims. Beat the mean bandit. In ICML, pages 241–248, 2011.

14

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits

problem. In COLT, 2009.

Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits

problem. J. Comput. Syst. Sci., 78(5):1538–1556, 2012.

Omar Zaidan and Chris Callison-Burch. Crowdsourcing translation: Professional quality from non-
In The 49th Annual Meeting of the Association for Computational Linguistics
professionals.
(ACL): Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Port-
land, Oregon, USA, pages 1220–1229, 2011.

Masrour Zoghi, Shimon Whiteson, R´emi Munos, and Maarten de Rijke. Relative upper con-
ﬁdence bound for the k-armed dueling bandit problem. CoRR, abs/1312.3393, 2013. URL
http://arxiv.org/abs/1312.3393.

Masrour Zoghi, Shimon Whiteson, Maarten de Rijke, and R´emi Munos. Relative conﬁdence sam-

pling for efﬁcient on-line ranker evaluation. In WSDM, pages 73–82, 2014a.

Masrour Zoghi, Shimon Whiteson, R´emi Munos, and Maarten de Rijke. Relative upper conﬁdence

bound for the k-armed dueling bandit problem. In ICML, pages 10–18, 2014b.

Masrour Zoghi, Shimon Whiteson, and Maarten de Rijke. MergeRUCB: A method for large-scale

online ranker evaluation. In WSDM, 2015.

15

KOMIYAMA HONDA KASHIMA NAKAGAWA

c=0
c=0.1
c=0.3
c=1

5

10

4

10

3

10

M
0
1
=
T
 
t
a
 
)
T
(
R

10

2
16

32
64
K: # of arms

128

Figure 3: Performance of RMED1 algorithm with several values of c. The plot shows the regret at

T = 107 in the MSLR dataset with K = 16, 32, 64, and 128.

Appendix A. Experiment: Dependence on f (K)

P

c(t) implies a failure in identifying the Condorcet winner (i.e., 1

= i∗(t)). Although
The event
U
T
E[
c(t)] = O(eAK−f (K)) is a constant function of T for any non-negative f (K), this term
t=1 U
is not negligible with large K. To evaluate the effect of f (K), we set f (K) = cK 1.01 and studied
several values of c with the MSLR dataset (Figure 3). In the case of c = 0, the regret for K = 128
becomes 100 times that for K = 16, which implies that the exponential dependence O(eAK ) may
not be an artifact of the proof. On the other hand, the results for c = 0.1, 0.3, and 1 indicate that
this term can be much improved by simply letting c be a small positive value.

Appendix B. Proofs on Regret Lower Bound

B.1. Proof of Lemma 1

∈

[K]

be arbitrary and M =

Let i
be an arbitrary preference matrix. We consider a
modiﬁed preference matrix M ′ in which the probabilities related to arm i are different from M . Let
′
i, ij element
j
Oi ∪ {

1
\ {
}
[K], µi,j ≤

. For j
[K], µi,j = 1/2
}

, that is,
1/2
}

µi,j}
{

′
i =

′
i =

∈ O

j
|

O

∈

j
j
O
|
{
of M ′ is µ′

∈
i,j such that

d+(µi,j, µ′

i,j) = d(µi,j, 1/2) + ǫ.

(11)

Such a µ′
of the KL divergence. For j /
the modiﬁed bandit problem the Condorcet winner is not arm 1 but arm i. Moreover, if M
then M ′

i,j > 1/2 uniquely exists for sufﬁciently small ǫ > 0 by the monotonicity and continuity
i,j = µi,j. Note that, unlike the original bandit problem, in
∈ Mo

i, let µ′
′

∈ O

∈ Mo.

16

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

Notation: now, let ˆX m

i,j ∈ {

0, 1
}

be the result of m-th draw of the pair (i, j),

n

KLj(n) =

log

ˆX m
ˆX m

i,jµi,j + (1
i,jµ′
i,j + (1

 

−

ˆX m
ˆX m

i,j)(1
i,j)(1

−

µi,j)
µ′
i,j) !

,

Xm=1

−
KLj(Ni,j(T )), and P′, E′ be the probability and the expectation with respect to

c

−

KL =

and
the modiﬁed bandit game. Then, for any event

j∈O′
i

P

c

c

,

E
1

holds. Let us deﬁne the events

) = E

P′(
E

exp

KL

{E}

h

−

(cid:16)

(cid:17)i

c

(12)

Ni,j(T )d(µi,j, µ′

i,j) < (1

ǫ) log T, Ni,i(T ) < √T

,

−






D1 =

D2 =
D12 =
D1\2 =




i

Xj∈O′
KL


≤
n
(cid:16)
D1 ∩ D2,
c
c
2.
D1 ∩ D

ǫ
2

1

−

log T

,

(cid:17)

o

First step (P

= o(1)): From (12),

{D12}
P′(

D12)
By using this we have

E

1
{D12}

exp

≥

h

ǫ
2

1

−

−

(cid:16)

(cid:16)

log T

= T −(1−ǫ/2)P

.
{D12}

(cid:17)

(cid:17)i

(13)

P

{D12} ≤

T (1−ǫ/2)P′(

T (1−ǫ/2)P′

D12)
Ni,i(T ) < √T

≤

≤

≤

T (1−ǫ/2)P′

o
Ni,i(T ) > T

n

T
E′[T
n
T

−

−
−

Ni,i(T )]
√T

T (1−ǫ/2)

√T

−

o

(by the Markov inequality).

(14)

Since this algorithm is strongly consistent, E′[T
o(T a) for any a > 0. Therefore, the
−
RHS of the last line of (14) is o(T a−ǫ/2), which, by choosing sufﬁciently small a, converges to zero
as T
→ ∞
Second step (P

= o(1).
= o(1)): We have

. In summary, P

Ni,i(T )]

{D12}

→

{D1\2}

P

{D1\2}




Xj∈O′

i

= P

Ni,j(T )d(µi,j, µ′

i,j) < (1

ǫ) log T, Ni,i(T ) < √T ,

KLj(Ni,j(T )) >

1

log T

−

ǫ
2

−

(cid:16)

(cid:17)






i

Xj∈O′
ǫ
2

−

1

c

(cid:17)

(cid:16)

log T

.
)

P


(

≤

{nj }∈N|O′

i

|,Pj∈O′

i

max
nj d(µi,j ,µ′

i,j)<(1−ǫ) log T

KLj(nj) >

Xj∈O′

i

17

c

KOMIYAMA HONDA KASHIMA NAKAGAWA

Note that

max
1≤n≤N

KLj(n) = max
1≤n≤N

n

log

ˆX m
ˆX m

i,jµi,j + (1
i,jµ′
i,j + (1

 

ˆX m
ˆX m

i,j)(1
i,j)(1

−

−

µi,j)
µ′
i,j) !

,

−

−

m=1
X

is the maximum of the sum of positive-mean random variables, and thus converges to is average
(c.f., Lemma 10.5 in Bubeck, 2010). Namely,

c

lim
N→∞

max
1≤n≤N

KLj(n)
N

c

= d(µi,j, µ′

i,j)

a.s.

(15)

Let δ > 0 be sufﬁciently small. We have,

max

{nj }∈N|O′

i

|,Pj∈O′

i

njd(µi,j ,µ′

i,j )<(1−ǫ) log T

KLj(nj)

j∈O′
i

log T

P

c

max

{nj}∈N|O′

i

|,Pj∈O′

i

:nj >δ log T nj d(µi,j ,µ′

i,j )<(1−ǫ) log T

≤

log T

KLj(nj)

j∈O′
i

P

c

+

minj∈O′

i

δK
d(µi,j, µ′

.

i,j)

Combining this with the fact that (15) holds for any j, we have

max

{nj}∈N|O′

i

|,Pj∈O′

i

:nj >δ log T nj d(µi,j ,µ′

i,j )<(1−ǫ) log T

log T

KLj(nj)

j∈O′
i

P

c

1

ǫ

a.s.,

≤

−

max

{nj }∈N|O′

i

|,Pj∈O′

i

njd(µi,j ,µ′

i,j)<(1−ǫ) log T

KLj(nj)

j∈O′
i

log T

P

c

≤

−

1

ǫ + Θ(δ)

a.s. (16)

By using the fact that (16) holds almost surely for any sufﬁciently small δ > 0 and 1
we have

−

ǫ/2 > 1

ǫ,

−



{nj }∈N|O′

i

|,Pj∈O′

i

max
njd(µi,j ,µ′

i,j)<(1−ǫ) log T

P



Xj∈O′

i

c

KLj(nj) >

1

log T

= o(1).

ǫ
2

−

(cid:16)

(cid:17)





In summary, we obtain P
Last step: We here have

= o(1).

D1\2

(cid:8)

(cid:9)

D1 =

Ni,j(T )d(µi,j, µ′

i,j) < (1

ǫ) log T

Ni,i(T ) < √T

−

∩




n

o

=

Ni,j(T )(d(µi,j, 1/2) + ǫ) < (1

Ni,i(T ) < √T

(By (11))

lim sup
N→∞

and thus

lim sup
T →∞

Ni,j(T )(d(µi,j, 1/2) + ǫ) +

(1

−

ǫ) log T
√T


Ni,i(T ) < (1

ǫ) log T

,

−

(17)


ǫ) log T

−

∩




n

18

o













Xj∈O′

i

Xj∈O′

i

Xj∈O′

i


⊇ 




REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

where we used the fact that
Note that, by using the result of the previous steps, P
the complementary of this fact,

A < C
{

B < C

} ∩ {

} ⊇ {

A + B < C
}
= P
{D12}

{D1}

for A, B > 0 in the last line.
+ P
= o(1). By using

{D1\2}

Ni,j(T )(d(µi,j, 1/2) + ǫ) +

(1

−

ǫ) log T
√T

Ni,i(T )

(1

ǫ) log T

≥

−

P

≥

{D

c
1}

= 1

o(1).

−






Using the Markov inequality yields



P




Xj∈O′

i

E




Xj∈O′

i

Ni,j(T )(d(µi,j, 1/2) + ǫ) +

Ni,i(T )

(1

ǫ)(1

o(1)) log T.

(18)

(1

−

ǫ) log T
√T

≥




−

−



Because E[Ni,i(T )] is subpolynomial as a function of T due to the consistency, the second term in
LHS of (18) is o(1) and thus negligible. Lemma 1 follows from the fact that (18) holds for sufﬁ-
ciently small ǫ.



B.2. Proof of Theorem 2

We have

R(T ) =

1
2

∆1,i + ∆1,j
2

Ni,j(T ) +

∆1,i + ∆1,i
2

Ni,i(T )

Xi∈[K] Xj∈[K]\{i}

≥

≥

=

Xi,j∈[K]:µi,j<1/2

Xi∈[K]\{1} Xj∈Oi

Xi∈[K]\{1} Xj∈Oi

∆1,i + ∆1,j
2

Ni,j(T ) +

∆1,i + ∆1,i
2

Ni,i(T )

Xi∈[K]

Xi∈[K]

∆1,i + ∆1,j
2

Ni,j(T )

∆1,i + ∆1,j
2d(µi,j, 1/2)

d(µi,j, 1/2)Ni,j (T ).

Taking the expectation on both sides and using Lemma 1 yield

E[R(T )]

≥

Xi∈[K]\{1}

min
j∈Oi

∆1,i + ∆1,j
2d(µi,j, 1/2)

(1

o(1)) log T.

−

Appendix C. Proof of Lemma 5

This lemma essentially states that, the expected number of the rounds in which arm 1 is underesti-
mated is O(1). We show this by bounding the expected number of rounds before arm 1 is compared,

19

KOMIYAMA HONDA KASHIMA NAKAGAWA

for each ﬁxed set of
16 in Honda and Takemura (2010). Note that

N1,s(t)
}
{

and summing over

N1,s(t)
{

. This technique is inspired by Lemma
}

c(t) =

U

[S∈2[K]\{1}\{∅} (

\s∈S

ˆµ1,s(t)
{

≤

1/2

} ∩

ˆµ1,s(t) > 1/2
{

.

})

(19)

\s /∈S

Now we bound the number of rounds that the event

ˆµ1,s(t)
{

≤

1/2

} ∩

ˆµ1,s(t) > 1/2
}
{

\s∈S
occurs. Let N be the set of non-zero natural numbers, ns ∈
for each s
∈
1/2, d+(ˆµns
1,s, 1/2) = xs, N1,s(t) = ns}

N and xs ∈
i,j be the empirical estimate of µi,j at n-th draw of pair (i, j). If
S and ˆµ1,s(t) > 1/2 holds for s /
∈

S. Let ˆµn

holds for s

\s /∈S

∈

[0, log 2] be arbitrary

ˆµns
1,s ≤
{
S then

I1(t) =

nsd+(ˆµ1,s(t), 1/2)

Xs∈S

and therefore

J1(t) holds for any

exp

t

≥

 

nsd+(ˆµ1,s(t), 1/2)

f (K)

.

−

!

Xs∈S
J1(t) occurs, then arm 1 is in LN of the next loop, and thus for some s

If
within 2K rounds. Therefore we have

∈

S, N1,s is incremented

T

1

Xt=Tinit+1

"
\s∈S

ˆµ1,s(t)
{

1/2, N1,s(t) = ns} ∩

≤

ˆµ1,s(t) > 1/2
{

}#

\s /∈S

exp

≤

 

Xs∈S

nsd+(ˆµns

1,s, 1/2)

f (K)

+ 2K.

−

!

20

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

Letting Ps(xs) = Pr[ˆµns

1/2, d+(ˆµns

1,s, 1/2)

xs], we have

1,s ≤

≥

E

T

1


Xt=Tinit+1

=

"

\s∈S

Z{xs}∈[0,log 2]|S|  

ˆµ1,s(t)
{

≤

1/2, N1,s(t) = ns} ∩

ˆµ1,s(t) > 1/2
{

exp

 

Xs∈S

nsxs −

f (K)

+ 2K

!

d(

Ps(xs))

−

\s /∈S

!

Ys∈S

}#


(

(

(

(

(

= e−f (K)

2K

Ps(0) +

ensxsd(

Ps(xs))

−

)

= e−f (K)

2K

Ps(0) +

ensxsPs(xs)]log 2

0 +

Ys∈S

Ys∈S

Ys∈S Zxs∈[0,log 2]
[
−

Ys∈S  

(integration by parts)

nsensxsPs(xs)dxs

!)

Zxs∈[0,log 2]

≤

≤

e−f (K)

(1 + 2K)

Ps(0) +

nsensxse−ns(xs+C1(µ1,s,1/2))dxs

(by the Chernoff bound and Fact 10, where C1(µ, µ2) = (µ

Ys∈S Zxs∈[0,log 2]

e−f (K)

(1 + 2K)

e−nsd(1/2,µ1,s) +

µ2)2/(2µ(1

−

−
nse−nsC1(µ1,s,1/2)dxs

)

µ2)))

)

Ys∈S Zxs∈[0,log 2]

Ys∈S

)

= e−f (K)

(1 + 2K)

e−nsd(1/2,µ1,s) +

(log 2)nse−nsC1(µ1,s,1/2)

.

(20)

Ys∈S

Ys∈S

Ys∈S
,
ns}
{

By summing (20) over

T

P

Xt=Tinit+1

"
\s∈S

ˆµ1,s(t)
{

≤

1/2

} ∩

ˆµ1,s(t) > 1/2
{

}#

e−f (K)

(1 + 2K)

e−nsd(1/2,µ1,s) +

(log 2)nse−nsC1(µ1,s,1/2)

· · ·
X{ns}∈N|S|  
X

e−f (K)

(1 + 2K)

(

ed(1/2,µ1,s )

1

+ (log 2)|S|

where we used the fact that
(19) and the union bound over all S

P

∈

2[K]\{1}

, we obtain
P

\ {∅}

Ys∈S
−
∞
n=1 e−nx = 1/(ex + 1) and

Ys∈S
−
∞
n=1 ne−nx = ex/(ex + 1)2. Using

!

Ys∈S

eC1(µ1,s,1/2)

(eC1(µ1,s,1/2)

,

1)2 )

≤

≤

T

\s /∈S

Ys∈S
1

E


Xt=Tinit+1

< e−f (K)

c(t)

1

{U

}


(1 + 2K)






= O(eAK−f (K)),

1 +

1

ed(1/2,µ1,s )

1

(cid:19)

−

Ys∈[K]\{1} (cid:18)

+ (log 2)K−1

1 +

eC1(µ1,s,1/2)

Ys∈[K]\{1}  

(eC1(µ1,s,1/2)

1)2 !


−

(21)



21

KOMIYAMA HONDA KASHIMA NAKAGAWA

where A = log

maxs∈[K]\{1} max

1 +

1
ed(1/2,µ1,s )−1

, log 2

1 + eC1(µ1,s,1/2)

(eC1(µ1,s,1/2)−1)2

(cid:26)

(cid:18)

(cid:18)

.
(cid:19)(cid:19)(cid:27)

Appendix D. Proof of Lemma 6

Tinit + K + 1 (i.e., after
Except for the ﬁrst loop, arm i must put into LN before
the ﬁrst loop), let τ (t) < t be the round in the previous loop in which arm l(t) is put into LN . In the
round,
Tinit + K + 1 such
= τ (t2) holds because τ (t1) and τ (t2) belong to different
that l(t1) = l(t2) = i, t1 6
loops. By using τ (t), we obtain

Jl(t)(τ (t)) is satisﬁed. With this deﬁnition, for any two rounds t1, t2 ≥

. For t
l(t) = i
}
{

= t2 ⇒

τ (t1)

≥

T

1[l(t) = i, Ni,j(t)

N Suf

i,j (δ)]

≥

U

Xt=Tinit+1

K +

T

≤

≤

K +

Xt=Tinit+K+1
T
1[
U

Xt=Tinit+1

T

Xt=Tinit+K+1
1[l(t) = i,

c(t)] +

Xt=Tinit+K+1

1[l(t) = i,

c(τ (t))] +

1[l(t) = i,

(τ (t)), Ni,j(t)

U

N Suf

i,j (δ)]

≥

T

(τ (t)), Ni,j(t)

U

N Suf

i,j (δ)].

≥

Note that the expectation of term
t, the only round in which pair (i, j) can be compared is the round of
P
once, and thus Ni,j(t)
Ni,j(τ (t))

c(t)] is bounded by Lemma 5. Between τ (t) and
that occurs at most
l(t) = j
{
1. By using this fact, we obtain

T
t=Tinit+1

1[
U

}

−

T

Xt=Tinit+K+1

T

≤

U

≤

≤

T

Xt=Tinit+K+1
1[
Ji(t),

U

Xt=Tinit+1

1[l(t) = i,

(τ (t)), Ni,j(t)

N Suf

i,j (δ)]

≥

1[l(t) = i,

Ji(τ (t)),

U

(τ (t)), Ni,j(τ (t))

N Suf

i,j (δ)

1]

−

≥

(t), Ni,j (t)

N Suf

i,j (δ)

1].

−

≥

(22)

22

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

1[
Ji(t),

U

(t), Ni,j (t)

N Suf

i,j (δ)

1]

−

≥

We can bound this term via Ii(t) as

T

Xt=Tinit+1

T

T

i,j (δ)−1⌉

Xn=⌈N Suf
T

[t=Tinit+1(cid:16)
T

≤

≤

≤

≤

i,j (δ)−1⌉

Xn=⌈N Suf
T

i,j (δ)−1⌉

Xn=⌈N Suf
T

Xn=⌈N Suf

i,j (δ)−1⌉

1

1

1







h

(cid:20)

[t=Tinit+1(cid:16)


(N Suf

i,j (δ)

−

Ij(t)

log t + f (K), Ni,j(t) = n

≤

(by

(t)

U

⇒

I1(t) = 0)

Ni,j(t) = n, Ni,j(t)d+(ˆµn

i,j, 1/2)

log t + f (K)

(cid:17)





(cid:17)





≤

i

1)d+(ˆµn

i,j, 1/2)

log T + f (K)

1

d+(ˆµn

i,j, 1/2)

d(µi,j, 1/2)
1 + δ

≤

≤

.

(cid:21)

(23)

Therefore, by letting µ

(1/2, µi,j ) be a real number such that d(µ, 1/2) = d(µi,j ,1/2)

, we

1+δ

∈
obtain from the Chernoff bound and the monotonicity of d+(
, 1/2) that
·

T

E


Xt=Tinit+1


1[
Ji(t),

U

(t), Ni,j (t)

N Suf

i,j (δ)

1]

−



≤

≥

T

P

d+(ˆµn
(cid:20)

i,j, 1/2)

d(µi,j, 1/2)
1 + δ

≤

(cid:21)

i,j (δ)−1⌉

Xn=⌈N Suf
T



≤

Xn=⌈N Suf

i,j (δ)−1⌉
1

exp (

d(µ, µi,j)n)

−

≤

exp (d(µ, µi,j))

1

−

<

1
d(µ, µi,j)

.

From the Pinsker’s inequality it is easy to conﬁrm that d(µ, µi,j) = Ω(δ2), which completes the
proof.

Appendix E. Optimal Regret Bound: Full Proof of Theorem 4

Events: Deﬁne

Yi =

ˆµ⌈α log log T ⌉
i,j

{|

µi,j|

< ∆suf
i }

−

for sufﬁciently small but ﬁxed ∆suf
in µi,j that
µi,j}i,j∈[K]. Let also
{

Yi implies ˆb⋆(i) = b⋆(i) when we let ∆suf

\i,j∈[K]
i > 0. It is easy to see from the condinuity of d+(µi,j, 1/2)
i > 0 be sufﬁciently small with respect to

Zi(t) =

.
ˆµi,b⋆(i)(t) < 1/2
}
{

23

KOMIYAMA HONDA KASHIMA NAKAGAWA

First step (regret decomposition): Like RMED1, in RMED2FH E[
bility (i.e., Lemma 5). In the following, we bound the regret under

U
(t): let

(t)] holds with high proba-

U

{Y
(B)

{z

r(t)
Zi(t)
}

+ 1
l(t) = i,
{

U

(t),

c
i ∪ Z

c
i (t)

}}

r(t)

(24)

In the following, we ﬁrst bound the terms (A) and (B), and then summarizing all terms to prove

}

|

}

|

ri(t) = 1
l(t) = i,
{
= 1
l(t) = i,
{

r(t)
(t)
}
Yi,
(t),

U

U

(A)

{z

Theorem 4.
Second step (bounding (A)): Note that,
l(t) = i,
{
ˆb⋆(i) = b⋆(i) and ˆb⋆(i)
ˆ
Oi(t). Therefore,

∈

1
l(t) = i,
{

U

(t),

Yi,

r(t)
Zi(t)
}

(t),

Yi,

Zi(t)
}

U

is a sufﬁcient condition for

1
l(t) = i, Ni,b⋆(i)(t)
{

≥

N Suf

i,b⋆(i)(δ)

∆1,i + ∆1,b⋆(i)
2

+

}

N Suf

i,b⋆(i)(δ) +

∆1,i
2

N Suf

i,b⋆(i)(δ)
log log T

.

By applying Lemma 6 with j = b⋆(i), for sufﬁciently small δ > 0 we have

1
l(t) = i, Ni,b⋆(i)(t)
{

≥

N Suf

i,b⋆(i)(δ)

O

}

≤

+ O(eAK−f (K)) + K.

1
δ2

(cid:18)

(cid:19)



In summary, term (A) is bounded as:

T

Xt=Tinit+1
T

≤

Xt=Tinit+1

T

Xt=Tinit+1

E





T

E


Xt=Tinit+1


1
l(t) = i,
{

U

(t),

Yi,

r(t)
Zi(t)
}



∆1,i + ∆1,b⋆(i)
2

≤

N Suf

i,b⋆(i)(δ) + O



log T
log log T

1
δ2

+ O

+ O(eAK−f (K)) + K.

(25)

(cid:19)
. Under
Third step (bounding (B)): Now we consider the case
(t),
}}
this event ˆb⋆(i) = b⋆(i) does not always hold but we can see that m(t)
still holds.
Furthermore, under this event arm ˆb⋆(i) is selected as m(t) at most (log log T )Ni,1(T ) + 1 times

c
c
i (t)
i ∪ Z
ˆb⋆(i), 1
}

l(t) = i,
{

{Y
∈ {

(cid:18)

(cid:19)

(cid:18)

U

24

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

due to Line 5 of Algorithm 3. By using these facts, we have,

T

E


Xt=Tinit+1

T
E

≤

E

≤


Xt=Tinit+1

T


Xt=Tinit+1


1
l(t) = i,
{

U

(t),

c
i ∪ Z

c
i (t)

}}

{Y

r(t)





T

[t′=Tinit+1

i (t′)
c

Z

}}



1
l(t) = i,
{

U

(t),

c
i ∪

{Y

1
l(t) = i, Ni,1(t)
{

≥

N Suf

i,1 (δ)

}



T

T

+ P

c
i ∪

Y




i (t′)
c

Z

N Suf
(cid:16)




[t′=Tinit+1

i,1 (δ) log log T + 1 + N Suf

i,1 (δ)

(cid:17)

O

≤

1
δ2

(cid:18)

(cid:19)



+ O(eAK−f (K)) + K + P

(by Lemma 6).

c
i ∪

Y






i (t′)
c

Z

O

N Suf
(cid:16)

i,1 (δ) log log T

(cid:17)

[t′=Tinit+1






The following lemma bounds P

Lemma 7 For RMED2FH, there exists C2 = C2(
µi,j}
{

c
i ∪

Y

T
t′=Tinit+1 Z

n

.

c
i (t′)
o
, K, α) > 0 such that

S

T

P

c
i ∪

Y






[t=Tinit+1

c
i (t)

Z

= O((log T )−C2).

In summary, term (B) is bounded as:

1
l(t) = i,
{

U

(t),

c
i ∪ Z

c
i (t)

}}

{Y

r(t)

E


Xt=Tinit+1


O

≤

1
δ2

(cid:18)

(cid:19)

+ O(eAK−f (K)) + K + O

i,1 (δ)(log T )−C2 log log T

.

(26)

N Suf
(cid:16)

(cid:17)










25

P((A) + (B))

(by Lemma 5 and inequality (24))

≤

≤

+

≤

KOMIYAMA HONDA KASHIMA NAKAGAWA

Last step (regret bound):

T

E[R(T )]

Tinit +

≤

P



{U

c(t)
}

+

P

ri(t)
(t), l(t) = i
}



{U

T

Tinit +



Xt=Tinit+1

Xt=Tinit+1


O(eAK−f (K)) +

Xi∈[K]\{1}

Xi∈[K]\{1}


O(αK 2 log log T ) + O(eAK−f (K))

∆1,i + ∆1,b⋆(i)
2

N Suf

i,b⋆(i)(δ) + O

log T
log log T

(cid:18)

(cid:19)

Xi∈[K]\{1}(







1
δ2

(cid:18)
(by (25) and (26))

(cid:19)

+ O

+ O(eAK−f (K)) + 2K + O

i,1 (δ)(log T )−C2 log log T

N Suf
(cid:16)

)

(cid:17)

(∆1,i + ∆1,b⋆(i))((1 + δ) log T )
2d(µi,b⋆(i), 1/2)

Xi∈[K]\{1}

O(αK 2 log log T ) + O(KeAK−f (K)) +

K log T
log log T

+ O

(cid:18)

K
δ2

(cid:19)

(cid:18)

(cid:19)

(cid:0)

+ O

+ O

K(log T )1−C2 log log T

+ O (Kf (K)) .

(27)

Combining (27) with the fact that O

K(log T )1−C2 log log T

completes the proof.

(cid:0)

(cid:1)
= o

K log T
log log T

(cid:1)

(cid:16)

(cid:17)

E.1. Proof of Lemma 7
and P
We bound P

c
i }

{Y

T
t=Tinit+1 Z

c
i (t)
}

separately. On the one hand,

{
S
ˆµ⌈α log log T ⌉
i,j
|

P

c
i }

{Y

= P




[i,j∈[K]

µi,j| ≥

−

∆suf

P

ˆµ⌈α log log T ⌉
i,j

{|

≤

i 

(by the Chernoff bound and Pinsker’s inequality)


Xi,j∈[K]

−

µi,j| ≥

∆suf
i }

2 exp (

2(∆suf

i )2α log log T )

−

2 (log T )−2(∆suf

i )2α = 2K 2 (log T )−2(∆suf

i )2α = O((log T )−Ca),

≤

=


Xi,j∈[K]

Xi,j∈[K]

26

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

where Ca = 2(∆suf

i )2α/K 2 > 0. On the other hand,

T

[t=Tinit+1
T

P





= P

c
i (t)

Z






[t=Tinit+1
∞






Xn=⌈α log log T ⌉
∞

−

Xn=⌈α log log T ⌉
(log T )−αd(1/2,µi,b⋆ (i))

≤

≤

≤

≤

∞

ˆµi,b⋆(i)(t) < 1/2

≤




P





Ni,b⋆(i)(t) = n, ˆµn
{
[n=⌈α log log T ⌉

i,b⋆(i) < 1/2

}



P

Ni,b⋆(i)(t) = n, ˆµn
{



i,b⋆(i) < 1/2
}

exp (

d(1/2, µi,b⋆(i))n)

(by the Chernoff bound)

(log T )−αd(1/2,µi,b⋆ (i))

1 +

= O((log T )−Cb),

∞

Xn=0

(cid:18)

exp (

d(1/2, µi,b⋆(i))n)

−

1

d(1/2, µi,b⋆(i))

1

(cid:19)

−

where Cb = αd(1/2, µi,b⋆(i)) > 0. The proof is completed by letting C2 = min (Ca, Cb) and taking
the union bound of P
c
.
i (t)
}

T
t=Tinit+1 Z

and P

c
i }

{Y

{
S

Appendix F. Facts

Fact 8 (The Chernoff bound)
Let X1, . . . , Xn be i.i.d. binary random variables. Let ˆX = 1
n
any ǫ > 0,

n

i=1 Xi and µ = E[ ˆX]. Then, for

and

P( ˆX

≥

µ + ǫ)

exp (

d(µ + ǫ, µ)n)

≤

−

P

P( ˆX

µ

ǫ)

≤

−

≤

exp (

d(µ

ǫ, µ)n).

−

−

Fact 9 (The Pinsker’s inequality)
For p, q

∈

(0, 1), the KL divergence between two Bernoulli distributions is bounded as:

d(p, q)

2(p

≥

−

q)2.

Fact 10 (A minimum difference between divergences (Lemma 13 in Honda and Takemura, 2010))
For any µ and µ2 satisfying 0 < µ2 < µ < 1. Let C1(µ, µ2) = (µ
µ2)). Then, for
any µ3 ≤

µ2)2/(2µ(1

µ2,

−

−

d(µ3, µ)

d(µ3, µ2)

C1(µ, µ2) > 0.

−

≥

27

5
1
0
2
 
n
u
J
 
9
2

 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
0
5
5
2
0
.
6
0
5
1
:
v
i
X
r
a

JMLR: Workshop and Conference Proceedings vol 40:1–27, 2015

Regret Lower Bound and Optimal Algorithm
in Dueling Bandit Problem

Junpei Komiyama
The University of Tokyo
Junya Honda
The University of Tokyo
Hisashi Kashima
Kyoto University
Hiroshi Nakagawa
The University of Tokyo

JUNPEI@KOMIYAMA.INFO

HONDA@STAT.T.U-TOKYO.AC.JP

KASHIMA@I.KYOTO-U.AC.JP

NAKAGAWA@DL.ITC.U-TOKYO.AC.JP

Abstract
We study the K-armed dueling bandit problem, a variation of the standard stochastic bandit prob-
lem where the feedback is limited to relative comparisons of a pair of arms. We introduce a tight
asymptotic regret lower bound that is based on the information divergence. An algorithm that is
inspired by the Deterministic Minimum Empirical Divergence algorithm (Honda and Takemura,
2010) is proposed, and its regret is analyzed. The proposed algorithm is found to be the ﬁrst one
with a regret upper bound that matches the lower bound. Experimental comparisons of dueling
bandit algorithms show that the proposed algorithm signiﬁcantly outperforms existing ones.
Keywords: multi-armed bandit problem, dueling bandit problem, online learning

1. Introduction

A multi-armed bandit problem is a crystallized instance of a sequential decision-making problem
in an uncertain environment, and it can model many real-world scenarios. This problem involves
conceptual entities called arms, and a forecaster who tries to identify good arms from bad ones. At
each round, the forecaster draws one of the K arms and receives a corresponding reward. The aim
of the forecaster is to maximize the cumulative reward over rounds, which is achieved by running an
algorithm that balances the exploration (acquisition of information) and the exploitation (utilization
of information).

While it is desirable to obtain direct feedback from an arm, in some cases such direct feedback is
not available. In this paper, we consider a version of the standard stochastic bandit problem called
the K-armed dueling bandit problem (Yue et al., 2009), in which the forecaster receives relative
feedback, which speciﬁes which of two arms is preferred. Although the original motivation of the
dueling bandit problem arose in the ﬁeld of information retrieval, learning under relative feedback
is universal to many ﬁelds, such as recommender systems (Gemmis et al., 2009), graphical design
(Brochu et al., 2010), and natural language processing (Zaidan and Callison-Burch, 2011), which
involve explicit or implicit feedback provided by humans.

c(cid:13) 2015 J. Komiyama, J. Honda, H. Kashima & H. Nakagawa.

KOMIYAMA HONDA KASHIMA NAKAGAWA

Related work: Here, we brieﬂy discuss the literature of the K-armed dueling bandit problem. The
RK×K, whose ij entry µi,j corresponds to the
problem involves a preference matrix M =
µi,j} ∈
{
probability that arm i is preferred to arm j.

j
j

max

≻
≻

⇔
≻

µ1,j, µj,k}
{

Most algorithms assume that the preference matrix has certain properties. Interleaved Filter (IF)
(Yue et al., 2012) and Beat the Mean Bandit (BTM) (Yue and Joachims, 2011), early algorithms
proposed for solving the dueling bandit problem, require the arms to be totally ordered, that is,
µi,j > 1/2. Moreover, IF assumes stochastic transitivity: for any triple (i, j, k) with
i
. Unfortunately, stochastic transitivity does not hold in many
k, µi,k ≥
i
µi,j, µj,k}
{
real-world settings (Yue and Joachims, 2011). BTM relaxes this assumption by introducing relaxed
k, γµ1,k ≥
stochastic transitivity: there exists γ
holds. The drawback of BTM is that it requires the explicit value of γ on which
max
the performance of the algorithm depends. Urvoy et al. (2013) considered a wide class of sequential
learning problems with bandit feedback that includes the dueling bandit problem. They proposed the
Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) algorithm, which empirically
outperforms IF and BTM for moderate K. Among the several versions of SAVAGE, the one called
Condorcet SAVAGE makes the Condorcet assumption and performed the best in their experiment.
The Condorcet assumption is that there is a unique arm that is superior to the others. Unlike the two
transitivity assumptions, the Condorcet assumption does not require the arms to be totally ordered
and is less restrictive. IF, BTM, and SAVAGE either explicitly require the number of rounds T , or
implicitly require T to determine the conﬁdence level δ.

1 such that for all pairs (j, k) with 1

≻

≥

≻

j

Recently, an algorithm called Relative Upper Conﬁdence Bound (RUCB) (Zoghi et al., 2014b)
was proven to have an O(K log T ) regret bound under the Condorcet assumption. RUCB is based on
the upper conﬁdence bound index (Lai and Robbins, 1985; Agrawal, 1995; Auer et al., 2002) that is
widely used in the ﬁeld of bandit problems. RUCB is horizonless: it does not require T beforehand
and runs for any duration. Zoghi et al. (2015) extended RUCB into the mergeRUCB algorithm
under the Condorcet assumption as well as the assumption that a portion of the preference matrix is
informative (i.e., different from 1/2). They reported that mergeRUCB outperformed RUCB when K
was large. Ailon et al. (2014) proposed three algorithms named Doubler, MultiSBM, and Sparring.
MultiSBM is endowed with an O(K log T ) regret bound and Sparring was reported to outperform
IF and BTM in their simulation. These algorithms assume that the pairwise feedback is generated
from the non-observable utilities of the selected arms. The existence of the utility distributions
associated with individual arms restricts the structure of the preference matrix.

In summary, most algorithms either has O(K 2 log T ) regret under the Condorcet assumption
(SAVAGE) or require additional assumptions to achieve O(K log T ) regret (IF, BTM, MultiSBM,
and mergeRUCB). To the best of our knowledge, RUCB is the only algorithm with an O(K log T )
regret bound1. The main difﬁculty of the dueling bandit problem lies in that, there are K
1
candidates of actions to test “how good” each arm i is. A naive use of the conﬁdence bound requires
every pair of arms to be compared O(log T ) times and yields an O(K 2 log T ) regret bound.
Contribution: In this paper, we propose an algorithm called Relative Minimum Empirical Diver-
gence (RMED). This paper contributes to our understanding of the dueling bandit problem in the
following three respects.

−

•

The regret lower bound: Some studies (e.g., Yue et al., 2012) have shown that the K-armed
dueling bandit problem has a Ω(K log T ) regret lower bound. In this paper, we further ana-

1. Zoghi et al. (2013) ﬁrst proposed RUCB with an O(K 2 log T ) regret bound and later modiﬁed it by adding a ran-

domization procedure to assure O(K log T ) regret in Zoghi et al. (2014b).

2

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

lyze this lower bound to obtain the optimal constant factor for models satisfying the Condorcet
assumption. Furthermore, we show that the lower bound is the same under the total order as-
sumption. This means that optimal algorithms under the Condorcet assumption also achieve
a lower bound of regret under the total order assumption even though such algorithms do not
know that the arms are totally ordered.
An optimal algorithm: The regret of RMED is not only O(K log T ), but also optimal in
the sense that its constant factor matches the asymptotic lower bound under the Condorcet
assumption. RMED is the ﬁrst optimal algorithm in the study of the dueling bandit problem.
Empirical performance assessment: The performance of RMED is extensively evaluated by
using ﬁve datasets: two synthetic datasets, one including preference data, and two including
ranker evaluations in the information retrieval domain.

•

•

2. Problem Setup

∈

.
The K-armed dueling bandit problem involves K arms that are indexed as [K] =
}
RK×K be a preference matrix whose ij entry µi,j corresponds to the probability that
Let M
arm i is preferred to arm j. At each round t = 1, 2, . . . , T , the forecaster selects a pair of arms
[K]2, then receives a relative feedback ˆXl(t),m(t)(t)
Bernoulli(µl(t),m(t)) that
(l(t), m(t))
indicates which of (l(t), m(t)) is preferred. By deﬁnition, µi,j = 1
[K]
and µi,i = 1/2.

∼
µj,i holds for any i, j

1, 2, . . . , K
{

−

∈

∈

Let Ni,j(t) be the number of comparisons of pair (i, j) and ˆµi,j(t) be the empirical estimate of
µi,j at round t. In building statistics by using the feedback, we treat pairs without taking their order
+ 1
t′=1(1
l(t′) =
l(t′) = i, m(t′) = j
into consideration. Therefore, for i
{
}
{
t−1
l(t′) = i, m(t′) = j, ˆXl(t′ ),m(t′)(t′) = 1
+ 1
t′=1(1
j, m(t′) = i
l(t′) =
) and µi,j = (
P
{
}
}
{
j, m(t′) = i, ˆXl(t′),m(t′)(t′) = 0
))/Ni,j (t), where 1[
= i, let
] is the indicator function. For j
P
·
}
Ni>j(t) be the number of times i is preferred over j. Then, ˆµi,j(t) = Ni>j(t)/Ni,j(t), where we
set 0/0 = 1/2 here. Let ˆµi,i(t) = 1/2.

= j, Ni,j(t) =

t−1

Throughout this paper, we will assume that the preference matrix has a Condorcet winner
.
(Urvoy et al., 2013). Here we call an arm i the Condorcet winner if µi,j > 1/2 for any j
i
}
\{
Without loss of generality, we will assume that arm 1 is the Condorcet winner. The set of preference
matrices which have a Condorcet winner is denoted by
MC. We also deﬁne the set of preference
matrices satisfying the total order by
µi,j < 1/2 induces
⇔
a total order iff

Mo ⊂ MC; that is, the relation i

[K]

≺

∈

j

Let ∆i,j = µi,j −

1/2. We deﬁne the regret per round as r(t) = (∆1,i + ∆1,j)/2 when the
pair (i, j) is compared. The expectation of the cumulative regret, E[R(T )] = E
is used
to measure the performance of an algorithm. The regret increases at each round unless the selected
pair is (l(t), m(t)) = (1, 1).

T
t=1 r(t)
i

hP

µi,j} ∈ Mo.
{

2.1. Regret lower bound in the K-armed dueling bandits

In this section we provide an asymptotic regret lower bound when T
. Let the superiors of
, that is, the set of arms that is preferred to i on average.
arm i be a set
[K], µi,j < 1/2
}
The essence of the K-armed dueling bandit problem is how to eliminate each arm i
by
making sure that arm i is not the Condorcet winner. To do so, the algorithm uses some of the arms
in

1
}
\ {

Oi =

→ ∞

j
{

[K]

j
|

∈

∈

Oi and compares i with them.

3

KOMIYAMA HONDA KASHIMA NAKAGAWA

A dueling bandit algorithm is strongly consistent for model

M ⊂ MC iff it has E[R(T )] =
. The following lemma is on the number of comparisons

o(T a) regret for any a > 0 and any M
of suboptimal arm pairs.

∈ M

Lemma 1 (The lower bound on the number of suboptimal arm draws) (i) Let an arm i
and preference matrix M
MC, we have

1
}
\ {
∈ MC be arbitrary. Given any strongly consistent algorithm for model

[K]

∈

E



q + (1


Xj∈Oi

−
Mo.

d(µi,j, 1/2)Ni,j (T )

(1

o(1)) log T,

(1)

≥




−

p) log 1−p
where d(p, q) = p log p
with parameters p and q. (ii) Furthermore, inequality (1) holds for any M
consistent algorithm for

1−q is the KL divergence between two Bernoulli distributions
∈ Mo given any strongly



∈ Oi, an algorithm needs to make log T /d(µi,j, 1/2)
Lemma 1 states that, for arbitrary arm j
comparisons between arms i and j to be convinced that arm i is inferior to arm j and thus i is not the
Condorcet winner. Since the regret increase per round of comparing arm i with j is (∆1,i + ∆1,j)/2,
eliminating arm i by comparing it with j incurs a regret of

(∆1,i + ∆1,j) log T
2d(µi,j, 1/2)

.

(2)

(3)

Therefore, the total regret is bounded from below by comparing each arm i with an arm j that
minimizes (2) and the regret lower bound is formalized in the following theorem.

Theorem 2 (The regret lower bound) (i) Let the preference matrix M
strongly consistent algorithm for model

∈ MC be arbitrary. For any

MC,

lim inf
T →∞

E[R(T )]

log T ≥

min
j∈Oi

∆1,i + ∆1,j
2d(µi,j, 1/2)

Xi∈[K]\{1}
holds. (ii) Furthermore, inequality (3) holds for any M
rithm for

Mo.

∈ Mo given any strongly consistent algo-

The proof of Lemma 1 and Theorem 2 can be found in Appendix B. The proof of Lemma 1 is
similar to that of Lai and Robbins (1985, Theorem 1) for the standard multi-armed bandit problem
but differs in the following point that is characteristic to the dueling bandit. To achieve a small regret
in the dueling bandit, it is necessary to compare the arm i with itself if i is the Condorcet winner.
However, we trivially know that µi,i = 1/2 without sampling and such a comparison yields no
information to distinguish possible preference matrices. We can avoid this difﬁculty by evaluating
Ni,j and Ni,i in different ways.

3. RMED1 Algorithm

In this section, we ﬁrst introduce the notion of empirical divergence. Then, on the basis of the
empirical divergence, we formulate the RMED1 algorithm.

4

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

Algorithm 1 Relative Minimum Empirical Divergence (RMED) Algorithm

1: Input: K arms, f (K)

0. α > 0 (RMED2FH, RMED2). T (RMED2FH).

2: L

1
← (
α log log T
⌈

⌉

≥
(RMED1, RMED2)
(RMED2FH)

.

For each arm i

3: Initial phase: draw each pair of arms L times. At the end of this phase, t = L(K
4: if RMED2FH then
5:
6: end if
7: LC, LR ←
8: while t
≤
9:

∈
.
[K], LN ← ∅
T do

[K], ﬁx ˆb⋆(i) by (6).

if RMED2 then

1)K/2.

−

Draw all pairs (i, j) until it reaches Ni,j(t)

α log log t. t

t + 1 for each draw.

≥

←

end if
for l(t)

∈

LC in an arbitrarily ﬁxed order do

Select m(t) by using

Algorithm 2
Algorithm 3

(RMED1)
(RMED2, RMED2FH)

.

(

.
l(t)
LR \ {
}
(without a duplicate) for any j /
j
LN ∪ {
∈

LR such that

Jj(t) holds.

Draw arm pair (l(t), m(t)).
LR ←
LN ←
t
←
end for
LC, LR ←

.
LN , LN ← ∅

t + 1.

}

19:
20: end while

10:

11:

12:

13:

14:

15:

16:

17:

18:

Algorithm 2 RMED1 subroutine for selecting m(t)
1: ˆ
j
Ol(t)(t)
[K]
l(t)
\ {
← {
∈
}|
ˆ
Ol(t)(t) or ˆ
2: if i∗(t)
Ol(t)(t) =
∈
i∗(t).
3: m(t)
←
4: else
5: m(t)
6: end if

arg minj6=l(t) ˆµl(t),j(t).

ˆµl(t),j(t)
then

1/2
}

←

≤

∅

3.1. Empirical divergence and likelihood function

In inequality (1) of Section 2.1, we have seen that
d(µi,j, 1/2)Ni,j (T ), the sum of the di-
vergence between µi,j and 1/2 multiplied by the number of comparisons between i and j, is the
characteristic value that deﬁnes the minimum number of comparisons. The empirical estimate of
this value is fundamentally useful for evaluating how unlikely arm i is to be the Condorcet winner.
Let the opponents of arm i at round t be the set ˆ
. Note that,
, ˆµi,j (t)
i
1/2
Oi(t) =
\{
}
}
unlike the superiors
Oi(t) for each arm i are deﬁned in terms of the empirical
averages, and thus the algorithms know who the opponents are. Let the empirical divergence be

Oi, the opponents ˆ

j
{

[K]

j∈Oi

j
|

P

≤

∈

Ii(t) =

Ni,j(t)d(ˆµi,j(t), 1/2).

Xj∈ ˆOi(t)

5

KOMIYAMA HONDA KASHIMA NAKAGAWA

−

Ii(t)) can be considered as the “likelihood” that arm i is the Condorcet winner. Let

The value exp (
i∗(t) = arg mini∈[K] Ii(t) (ties are broken arbitrarily) and I ∗(t) = Ii∗(t)(t). By deﬁnition, I ∗(t)
≥
0. RMED is inspired by the Deterministic Minimum Empirical Divergence (DMED) algorithm
(Honda and Takemura, 2010). DMED, which is designed for solving the standard K-armed bandit
problem, draws arms that may be the best one with probability Ω(1/t), whereas RMED in the
dueling bandit problem draws arms that are likely to be the Condorcet winner with probability
Ω(1/t). Namely, any arm i that satisﬁes

Ji(t) =

Ii(t)
{

−

I ∗(t)

log t + f (K)
}

≤

(4)

is the candidate of the Condorcet winner and will be drawn soon. Here, f (K) can be any non-
negative function of K that is independent of t. Algorithm 1 lists the main routine of RMED.
There are several versions of RMED. First, we introduce RMED1. RMED1 initially compares all
pairs once (initial phase). Let Tinit = (K
1)K/2 be the last round of the initial phase. From
t = Tinit + 1, it selects the arm by using a loop. LC = LC(t) is the set of arms in the current loop,
and LR = LR(t)
LC(t) is the remaining arms of LC that have not been drawn yet in the current
loop. LN = LN (t) is the set of arms that are going to be drawn in the next loop. An arm i is put
. By deﬁnition, at least one arm (i.e. i∗(t) at the end
into LN when it satisﬁes
of the current loop) is put into LN in each loop. For arm l(t) in the current loop, RMED1 selects
m(t) (i.e. the comparison target of l(t)) determined by Algorithm 2.

{Ji(t)

LR(t)

i /
∈

∩ {

}}

−

⊂

The following theorem, which is proven in Section 5, describes a regret bound of RMED1.

Theorem 3 For any sufﬁciently small δ > 0, the regret of RMED1 is bounded as:

E[R(T )]

≤

((1 + δ) log T + f (K))∆1,i
2d(µi,1, 1/2)

+ O(K 2) + O

+ O(KeAK−f (K)),

Xi∈[K]\{1}
µi,j}i,j∈[K]) is a constant as a function of T . Therefore, by letting δ = log−1/3 T
where A = A(
{
and choosing an f (K) = cK 1+ǫ for arbitrary c, ǫ > 0, we obtain

K
δ2

(cid:18)

(cid:19)

E[R(T )]

≤

Xi∈[K]\{1}

∆1,i log T
2d(µi,1, 1/2)

+ O(K 2+ǫ) + O(K log2/3 T ).

3.2. Gap between the constant factor of RMED1 and the lower bound
From the lower bound of Theorem 2, the O(K log T ) regret bound of RMED1 is optimal up to
a constant factor. Moreover, the constant factor matches the regret lower bound of Theorem 2 if
b⋆(i) = 1 for all i

where

[K]

∈

1
}
\ {

b⋆(i) = arg min

j∈Oi

∆1,i + ∆1,j
d(µi,j, 1/2)

.

(5)

Here we deﬁne d+(p, q) = d(p, q) if p < q and 0 otherwise, and x/0 = +
. Note that, there
can be ties that minimize the RHS of (5). In that case, we may choose any of the ties as b⋆(i) to
eliminate arm i. For ease of explanation, we henceforth will assume that b⋆(i) is unique, but our
results can be easily extended to the case of ties.

∞

We claim that b⋆(i) = 1 holds in many cases for the following mathematical and practical
= 1, is (∆1,i + ∆1,j)/2, whereas it is simply

reasons. (i) The regret of drawing a pair (i, j), j

6

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

Algorithm 3 Subroutine for selecting m(t) in RMED2 and RMED2FH

Update ˆb⋆(l(t)) by (6).

1: if RMED2 then
2:
3: end if
4: ˆ
Ol(t)(t)
5: if ˆb⋆(l(t))

j
← {

[K]

l(t)

∈
\ {
ˆ
Ol(t)(t) and
∈
ˆb⋆(l(t)).

←

6: m(t)
7: else
8:
9: end if

Select m(t) by using Algorithm 2.

≤

ˆµl(t),j(t)
}|
Nl(t),i∗(t)(t)
Nl(t),i∗(t)(t)

.
1/2
}
Nl(t),ˆb⋆(l(t))(t)/ log log t
Nl(t),ˆb⋆(l(t))(t)/ log log T (RMED2FH)

(RMED2)

(

≥

≥

then

≥

∆1,i/2 for the pair (i, 1). Thus, d+(µi,j, 1/2) has to be much larger than d+(µi,1, 1/2) in order to
satisfy b⋆(i) = j. (ii) The Condorcet winner usually wins over the other arms by a large margin,
and therefore, d+(µi,1, 1/2)
d+(µi,j, 1/2). For example, in the preference matrix of Example 1
(Table 1(a)), b⋆(3) = 1 as long as q < 0.79. Example 2 (Table 1(b)) is a preference matrix based
on six retrieval functions in the full-text search engine of ArXiv.org (Yue and Joachims, 2011)2. In
Example 2, b⋆(i) = 1 holds for all i, even though µ1,4 < µ2,4. In the case of a 16-ranker evalua-
tion based on the Microsoft Learning to Rank dataset (details are given in Section 4), occasionally
= 1 occurs, but the difference between the regrets of drawing arm 1 and b⋆(i) is fairly small
b⋆(i)
(smaller than 1.2% on average). Nevertheless, there are some cases in which comparing arm i with
1 is not such a clever idea. Example 3 (Table 1(c)) is a toy example in which comparing arm i with
b⋆(i)
= 1 makes a large difference. In Example 3, it is clearly better to draw pairs (2, 4), (3, 2) and
(4, 3) to eliminate arms 2, 3, and 4, respectively. Accordingly, it is still interesting to consider an
algorithm that reduces regret by comparing arm i with b⋆(i).

Table 1: Three preference matrices. In each example, the value at row i, column j is µi,j.

(a) Example 1
3
2
0.7
0.7
0.5
q
0.5
1-q

1
0.5
0.3
0.3

1
2
3

(b) Example 2

1
0.50
0.45
0.45
0.46
0.39
0.39

2
0.55
0.50
0.45
0.45
0.42
0.40

3
0.55
0.55
0.50
0.46
0.49
0.44

4
0.54
0.55
0.54
0.50
0.46
0.50

1
2
3
4
5
6

5
0.61
0.58
0.51
0.54
0.50
0.49

6
0.61
0.60
0.56
0.50
0.51
0.50

(c) Example 3

1
0.5
0.4
0.4
0.4

2
0.6
0.5
0.1
0.9

3
0.6
0.9
0.5
0.1

4
0.6
0.1
0.9
0.5

1
2
3
4

3.3. RMED2 Algorithm

We here propose RMED2, which gracefully estimates b⋆(i) during a bandit game and compares
arm i with b⋆(i). RMED2 and RMED1 share the main routine (Algorithm 1). The subroutine of
RMED2 for selecting m(t) is shown in Algorithm 3. Unlike RMED1, RMED2 keeps drawing pairs
of arms (i, j) at least α log log t times (Line 10 in Algorithm 1). The regret of this exploration is
insigniﬁcant since O(log log T ) = o(log T ). Once all pairs have been explored more than α log log t

2. In the original preference matrix of Yue and Joachims (2011), µ2,4 6= 1 − µ4,2. To satisfy µ2,4 = 1 − µ4,2, we

replaced µ2,4 and µ4,2 of the original with (µ2,4 − µ4,2 + 1)/2 and (µ4,2 − µ2,4 + 1)/2, respectively.

7

KOMIYAMA HONDA KASHIMA NAKAGAWA

times, RMED2 goes to the main loop. RMED2 determines m(t) by using Algorithm 2 based on the
estimate of b⋆(i) given by

ˆb⋆(i) = arg min
j∈[K]\{i}

ˆ∆i∗(t),i + ˆ∆i∗(t),j
d+(ˆµi,j(t), 1/2)

,

(6)

where ties are broken arbitrarily, ˆ∆i,j = 1/2
. Intuitively, RMED2
tries to select m(t) = ˆb⋆(i) for most rounds, and occasionally explores i∗(t) in order to reduce the
regret increase when RMED2 fails to estimate the true b⋆(i) correctly.

ˆµi,j and we set x/0 = +

∞

−

3.4. RMED2FH algorithm
Although we believe that the regret of RMED2 is optimal, the analysis of RMED2 is a little bit
complicated since it sometimes breaks the main loop and explores from time to time. For ease of
analysis, we here propose RMED2 Fixed Horizon (RMED2FH, Algorithm 1 and 3), which is a
“static” version of RMED2. Essentially, RMED2 and RMED2FH have the same mechanism. The
differences are that (i) RMED2FH conducts an α log log T exploration in the initial phase. After
the initial phase (ii) ˆb⋆(i) for each i is ﬁxed throughout the game. Note that, unlike RMED1 and
RMED2, RMED2FH requires the number of rounds T beforehand to conduct the initial α log log T
draws of each pair. The following Theorem shows the regret of RMED2FH that matches the lower
bound of Theorem 2.

Theorem 4 For any sufﬁciently small δ > 0, the regret of RMED2FH is bounded as:

E[R(T )]

≤

Xi∈[K]\{1}

(∆1,i + ∆1,b⋆(i))((1 + δ) log T )
2d(µi,b⋆(i), 1/2)

+ O(αK 2 log log T ) + O(KeAK−f (K))

K log T
log log T

+ O

(cid:18)

K
δ2

(cid:19)

(cid:18)

(cid:19)

+ O

+ O (Kf (K)) ,

(7)

where A = A(
µi,j}
{
choosing an f (K) = cK 1+ǫ (c, ǫ > 0) we obtain

) > 0 is a constant as a function of T . By setting δ = O((log T )−1/3) and

E[R(T )]

≤

Xi∈[K]\{1}

(∆1,i + ∆1,b⋆(i)) log T
2d(µi,b⋆(i), 1/2)

+ O(αK 2 log log T ) + O

+ O

K 2+ǫ

.

K log T
log log T

(cid:18)

(cid:19)

(cid:0)

(cid:1)
(8)

Note that all terms except the ﬁrst one in (8) are o(log T ). From Theorems 2 and 4 we see that (i)
RMED2FH is asymptotically optimal under the Condorcet assumption and (ii) the logarithmic term
on the regret bound of RMED2FH cannot be improved even if the arms are totally ordered and the
forecaster knows of the existence of the total order. The proof sketch of Theorem 4 is in Section 5.

4. Experimental Evaluation

To evaluate the empirical performance of RMED, we conducted simulations3 with ﬁve bandit
datasets (preference matrices). The datasets are as follows:

3. The source code of the simulations is available at https://github.com/jkomiyama/duelingbanditlib.

8

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

4

10

t
e
r
g
e
r
 
:
)
t
(
R

3

10

2

10

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

4

10

t
e
r
g
e
r
 
:
)
t
(
R

3

10

2

10

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

4

10

3

10

2

10

1

10

t
e
r
g
e
r
 
:
)
t
(
R

5

10

4

10

3

10

t
e
r
g
e
r
 
:
)
t
(
R

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

4

10

t
e
r
g
e
r
 
:
)
t
(
R

3

10

2

10

6

10

5

10

4

10

t
e
r
g
e
r
 
:
)
t
(
R

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

RMED1
RMED2
RMED2FH
RUCB
BTM
IF
Savage
Sparring
MultiSBM

1

10

2

10

3

10

4

10
t: round

(a) Six rankers

5

10

6

10

2

10

3

10

5

10

6

10

4

10
t: round

(b) Cyclic

1

10

2

10

3

10

4

10
t: round

5

10

6

10

(c) Arithmetic

1

10

2

10

3

10

4

10

5

10

2

10

3

10

4

10

5

10
t: round

6

10

7

10

3

10

3

10

4

10

5

10
t: round

6

10

7

10

(e) MSLR K = 16

(f ) MSLR K = 64

t: round

(d) Sushi

Figure 1: Regret-round log-log plots of algorithms.

Six rankers is the preference matrix based on the six retrieval functions in the full-text search engine
of ArXiv.org (Table 1(b)).
Cyclic is the artiﬁcial preference matrix shown in Table 1(c). This matrix is designed so that the
comparison of i with 1 is not optimal.
Arithmetic dataset involves eight arms with µi,j = 0.5 + 0.05(j
Sushi dataset is based on the Sushi preference dataset (Kamishima, 2003) that contains the pref-
erences of 5, 000 Japanese users as regards 100 types of sushi. We extracted the 16 most popular
types of sushi and converted them into arms with µi,j corresponding to the ratio of users who prefer
sushi i over j. The Condorcet winner is the mildly-fatty tuna (chu-toro).
MSLR: We tested submatrices of a 136
136 preference matrix from Zoghi et al. (2015), which is
derived from the Microsoft Learning to Rank (MSLR) dataset (Microsoft Research, 2010; Qin et al.,
2010) that consists of relevance information between queries and documents with more than 30K
queries. Zoghi et al. (2015) created a ﬁnite set of rankers, each of which corresponds to a ranking
feature in the base dataset. The value µi,j is the probability that the ranker i beats ranker j based on
the navigational click model (Hofmann et al., 2013). We randomly extracted K = 16, 64 rankers
in our experiments and made sub preference matrices. The probability that the Condorcet winner

i) and has a total order.

−

×

9

KOMIYAMA HONDA KASHIMA NAKAGAWA

500

400

300

200

100

t
e
r
g
e
r
 
:
)
t
(
R

RMED1
RMED2
RMED2FH a=1
RMED2FH a=3
RMED2FH a=10
LB1
TrueLB

100

80

60

40

20

t
e
r
g
e
r
 
:
)
t
(
R

RMED1
RMED2
RMED2FH a=1
RMED2FH a=3
RMED2FH a=10
LB1
TrueLB

2000

1500

1000

500

t
e
r
g
e
r
 
:
)
t
(
R

RMED1
RMED2
RMED2FH a=1
RMED2FH a=3
RMED2FH a=10
LB1
TrueLB

0
10

2

3

10

4

10
t: round

5

10

6

10

0
10

2

3

10

4

10
t: round

5

10

6

10

0
10

2

3

10

5

4

10
10
t: round

6

10

7

10

(a) Six rankers

(b) Cyclic

(c) MSLR K = 16

Figure 2: Regret-round semilog plots of RMED compared with theoretical bounds. We set f (K) =

0.3K 1.01 for all algorithms, and α = 3 for RMED2.

exists in the subset of the rankers is high (more than 90%, c.f. Figure 1 in Zoghi et al. (2014a)), and
we excluded the relatively small case where the Condorcet winner does not exist.

A Condorcet winner exists in all datasets. In the experiments, the regrets of the algorithms were

averaged over 1, 000 runs (Six rankers, Cyclic, Arithmetic, and Sushi), or 100 runs (MSLR).

4.1. Comparison among algorithms

We compared the IF, BTM with γ = 1.2, RUCB with α = 0.51, Condorcet SAVAGE with δ = 1/T ,
MultiSBM and Sparring with α = 3, and RMED algorithms. There are two versions of RUCB:
the one that uses a randomizer in choosing l(t) (Zoghi et al., 2014b), and the one that does not
(Zoghi et al., 2013). We implemented both and found that the two perform quite similarly: we show
the result of the former one in this paper. We set f (K) = 0.3K 1.01 for all RMED algorithms and
set α = 3 for RMED2 and RMED2FH. The effect of f (K) is studied in Appendix A. Note that IF
and BTM assume a total order among arms, which is not the case with the Cyclic, Sushi, and MSLR
datasets. MultiSBM and Sparring assume the existence of the utility of each arm, which does not
allow a cyclic preference that appears in the Cyclic dataset.

Figure 1 plots the regrets of the algorithms. In all datasets RMED signiﬁcantly outperforms
RUCB, the next best excluding the different versions of RMED. Notice that the plots are on a base
10 log-log scale. In particular, regret of RMED1 is more than twice smaller than RUCB on all
datasets other than Cyclic, in which RMED2 performs much better. Among the RMED algorithms,
RMED1 outperforms RMED2 and RMED2FH on all datasets except for Cyclic, in which comparing
arm i
= 1 with arm 1 is inefﬁcient. RMED2 outperforms RMED2FH in the ﬁve of six datasets: this
could be due to the fact that RMED2FH does not update ˆb⋆(i) for ease of analysis.

4.2. RMED and asymptotic bound

Figure 2 compares the regret of RMED with two asymptotic bounds. LB1 denotes the regret bound
of RMED1. TrueLB is the asymptotic regret lower bound given by Theorem 2.
, the slope of RMED1 should converge to LB1, and the ones
RMED1 and RMED2: When T
of RMED2 and RMED2FH should converge to TrueLB. On Six rankers, LB1 is exactly the same

→ ∞

10

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

as TrueLB, and the slope of RMED1 converges to this TrueLB. In Cyclic, the slope of RMED2
converges to TrueLB, whereas that of RMED1 converges to LB1, from which we see that RMED2
is actually able to estimate b⋆(i)
= 1 correctly. In MSLR K = 16, LB1 and TrueLB are very close
(the difference is less than 1.2%), and RMED1 and RMED2 converge to these lower bounds.
RMED2FH with different values of α: We also tested RMED2FH with several values of α. On
the one hand, with α = 1, the initial phase of RMED2FH is too short to identify b⋆(i); as a result
it performs poorly on the Cyclic dataset. On the other hand, with α = 10, the initial phase was
too long, which incurs a practically non-negligible regret on the MSLR K = 16 dataset. We also
tested several values of parameter α in RMED2FH. We omit plots of RMED2 with α = 1, 10 for
the sake of readability, but we note that in our datasets the performance of RMED2 is always better
than or comparable with the one of RMED2FH under the same choice of α, although the optimality
of RMED2 is not proved unlike RMED2FH.

5. Regret Analysis

This section provides two lemmas essential for the regret analysis of RMED algorithms and proves
the asymptotic optimality of RMED1 based on these lemmas. A proof sketch on the optimal regret
of RMED2FH is also given.

The crucial property of RMED is that, by constantly comparing arms with the opponents, the

true Condorcet winner (arm 1) actually beats all the other arms with high probability. Let

U

(t) =

.
ˆµ1,i(t) > 1/2
}
{
\i∈[K]\{1}
ˆµ1,i(t) < 1/2 for all i

[K]

U

(t), ˆµi,1(t) = 1

Under
(t)
∈
implies that i∗(t) = arg mini∈[K] Ii(t) is unique with i∗(t) = 1 and I ∗(t) = I1(t) = 0. Lemma
c(t) occurs is constant in T , where the
5 below shows that the average number of rounds that
superscript c denotes the complement.

, and thus, Ii(t) > 0. Therefore,

1
}
\ {

−

U

U

Lemma 5 When RMED1 or RMED2FH is run, the following inequality holds:

T

E


Xt=Tinit+1


c(t)

1

{U

}



= O(eAK−f (K)),

(9)

where A = A(
µi,j}
{

) > 0 is a constant as a function of T .

times in the initial phase, we deﬁne
Note that, since RMED2FH draws each pair
1)K/2 for RMED2FH. We give a proof of this lemma in Appendix C.
(K
Tinit =
⌉
Intuitively, this lemma can be proved from the facts that arm 1 is drawn within roughly eI1(t)−f (K)
rounds and I1(t) is not very large with high probability.

α log log T
⌈

α log log T
⌈

−

⌉

Next, for i

[K]

∈

1
}
\ {

and j

∈ Oi, let
i,j (δ) =

N Suf

(1 + δ) log T + f (K)
d(µi,j, 1/2)

+ 1,

which is a sufﬁcient number of comparisons of i with j to be convinced that the arm i is not the
Condorcet winner. The following lemma states that if pair (i, j) is drawn N Suf
i,j (δ) times then i is
rarely selected as l(t) again.

11

KOMIYAMA HONDA KASHIMA NAKAGAWA

Lemma 6 When RMED1 or RMED2FH is run, for i

[K]

∈

, j

1
}
\ {

∈ Oi,

T

E


Xt=Tinit+1


1
l(t) = i, Ni,j(t)
{

≥

N Suf

i,j (δ)

= O

+ O(eAK−f (K)) + K.

1
δ2

(cid:18)

(cid:19)

}



We prove this lemma in Appendix D based on the Chernoff bound.

Now we can derive the regret bound of RMED1 based on these lemmas.

Proof of Theorem 3: Since
be decomposed as:

U

(t) implies m(t) = 1 in RMED1, the regret increase per round can

r(t) = 1

c(t)
}

+

{U

∆1,i
2

1
l(t) = i, m(t) = 1,
{

.
(t)
}

U

(10)

Xi∈[K]\{1}

Using Lemmas 5 and 6, we obtain

E[R(T )]

Tinit +

≤

K(K
2

−

≤

1)

+E



1)

K(K
2

−

≤

T

[r(t)]

Xt=Tinit+1
T

c(t)

1

{U

Xt=Tinit+1
+ O(eAK−f (K)) +



∆1,i
2  

+
Xi∈[K]\{1}

}



∆1,i
2

(cid:18)

Xi∈[K]\{1}

T

t=1
X

1
δ2

(cid:18)

(cid:19)

N Suf

i,1 (δ)+

1[l(t) = i, m(t) = 1, Ni,1(t)

N Suf

i,1 (δ)]

!

N Suf

i,1 (δ) + O

+ O(eAK−f (K)) + K

,

≥

(cid:19)

which immediately completes the proof of Theorem 3.

We also prove Theorem 4 on the optimality of RMED2FH based on Lemmas 5 and 6. Because

∈

[K]

1
}
\ {

. There exists C2 > 0 such that, for each l(t) = i, (i) with probability 1

c(t) does
(t), we decompose the regret into the contributions

the full proof in Appendix E is a little bit lengthy, here we give its brief sketch.
Proof sketch of Theorem 4 (RMED2FH): Similar to Theorem 3, we use the fact that the
not occur very often (i.e., Lemma 5). Under
of each arm i
−
O((log T )−C2) RMED2FH successfully estimates ˆb⋆(i) = b⋆(i) and selects m(t) = b⋆(i) for most
rounds. The optimal O(log T ) term comes from the comparison of i and b⋆(i). Arm 1 is also drawn
for O(log T / log log T ) = o(log T ) times. On the other hand, (ii) with probability O((log T )−C2),
RMED2FH fails to estimate b⋆(i) correctly. By occasionally comparing arm i with arm 1, we
can bound the regret increase by O(log T log log T ). Since O((log T )−C2
log T log log T ) =
o(log T ), this regret does not affect the O(log T ) factor.

×

U

U

6. Discussion

We proved the regret lower bound in the dueling bandit problem. The RMED algorithm is based
on the likelihood that the arm is the Condorcet winner. RMED is proven to have the matching
regret upper bound. The empirical evaluation revealed that RMED signiﬁcantly outperforms the
state-of-the-art algorithms. To conclude this paper, we mention three directions of future work.

12

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

First, when a Condorcet winner does not necessarily exist, the Copeland bandits (Urvoy et al.,
2013) are a natural extension of our problem. Thus, seeking an effective algorithm for solving this
problem will be interesting. As is well known in the ﬁeld of voting theory, there are several other
criteria of winners that are incompatible with the Condorcet / Copeland bandits, such as the Borda
winner (Urvoy et al., 2013). Comparing several criteria or developing an algorithm that outputs
more than one of these winners should be interesting directions of future work.

Second, another direction is sequential preference elicitation problems under relative feedback
that goes beyond the binary preference over pairs, such as multiscale feedback and/or preferences
among three or more items.

Third, in the standard bandit problem, it is reported that KL-UCB+ (Lai, 1987; Garivier and Capp´e,

2011) performs better than DMED. A study of a UCB-based optimal algorithm for the dueling ban-
dits can yield an algorithm that outperforms RMED.

Acknowledgements

We thank the anonymous reviewers for their useful comments. This work was supported in part by
JSPS KAKENHI Grant Number 26106506.

13

KOMIYAMA HONDA KASHIMA NAKAGAWA

References

R. Agrawal. Sample mean based index policies with O(log n) regret for the multi-armed bandit

problem. Advances in Applied Probability, 27:1054–1078, 1995.

Nir Ailon, Zohar Shay Karnin, and Thorsten Joachims. Reducing dueling bandits to cardinal ban-

dits. In ICML, pages 856–864, 2014.

Peter Auer, Nicol´o Cesa-bianchi, and Paul Fischer. Finite-time Analysis of the Multiarmed Bandit

Problem. Machine Learning, 47:235–256, 2002.

Eric Brochu, Tyson Brochu, and Nando de Freitas. A bayesian interactive optimization approach
In Proceedings of the 2010 Eurographics/ACM SIGGRAPH

to procedural animation design.
Symposium on Computer Animation, SCA 2010, Madrid, Spain, 2010, pages 103–112, 2010.

S´ebastien Bubeck. Bandits Games and Clustering Foundations. Theses, Universit´e des Sciences et

Technologie de Lille - Lille I, June 2010.

Aur´elien Garivier and Olivier Capp´e. The KL-UCB algorithm for bounded stochastic bandits and

beyond. In COLT, pages 359–376, 2011.

Marco De Gemmis, Leo Iaquinta, Pasquale Lops, Cataldo Musto, Fedelucio Narducci, and Giovanni
In In Preference Learning (PL-09)

Semeraro. Preference learning in recommender systems.
ECML/PKDD-09 Workshop, 2009.

Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. Fidelity, soundness, and efﬁciency of
interleaved comparison methods. Transactions on Information Systems, 31(4):17:1–43, 2013.

Junya Honda and Akimichi Takemura. An Asymptotically Optimal Bandit Algorithm for Bounded

Support Models. In COLT, pages 67–79, 2010.

Toshihiro Kamishima. Nantonac collaborative ﬁltering: recommendation based on order responses.

In KDD, pages 583–588, 2003.

1091–1114, 09 1987.

T. L. Lai. Adaptive treatment allocation and the multi-armed bandit problem. Ann. Statist., 15(3):

T. L. Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in

Applied Mathematics, 6(1):4–22, 1985.

Microsoft Research.

Microsoft Learning
to Rank Datasets,
http://research.microsoft.com/en-us/projects/mslr/.

2010.

URL

Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. LETOR: A benchmark collection for research on

learning to rank for information retrieval. Inf. Retr., 13(4):346–374, 2010.

Tanguy Urvoy, Fabrice Cl´erot, Rapha¨el Feraud, and Sami Naamane. Generic exploration and k-

armed voting bandits. In ICML, pages 91–99, 2013.

Yisong Yue and Thorsten Joachims. Beat the mean bandit. In ICML, pages 241–248, 2011.

14

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits

problem. In COLT, 2009.

Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits

problem. J. Comput. Syst. Sci., 78(5):1538–1556, 2012.

Omar Zaidan and Chris Callison-Burch. Crowdsourcing translation: Professional quality from non-
In The 49th Annual Meeting of the Association for Computational Linguistics
professionals.
(ACL): Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Port-
land, Oregon, USA, pages 1220–1229, 2011.

Masrour Zoghi, Shimon Whiteson, R´emi Munos, and Maarten de Rijke. Relative upper con-
ﬁdence bound for the k-armed dueling bandit problem. CoRR, abs/1312.3393, 2013. URL
http://arxiv.org/abs/1312.3393.

Masrour Zoghi, Shimon Whiteson, Maarten de Rijke, and R´emi Munos. Relative conﬁdence sam-

pling for efﬁcient on-line ranker evaluation. In WSDM, pages 73–82, 2014a.

Masrour Zoghi, Shimon Whiteson, R´emi Munos, and Maarten de Rijke. Relative upper conﬁdence

bound for the k-armed dueling bandit problem. In ICML, pages 10–18, 2014b.

Masrour Zoghi, Shimon Whiteson, and Maarten de Rijke. MergeRUCB: A method for large-scale

online ranker evaluation. In WSDM, 2015.

15

KOMIYAMA HONDA KASHIMA NAKAGAWA

c=0
c=0.1
c=0.3
c=1

5

10

4

10

3

10

M
0
1
=
T
 
t
a
 
)
T
(
R

10

2
16

32
64
K: # of arms

128

Figure 3: Performance of RMED1 algorithm with several values of c. The plot shows the regret at

T = 107 in the MSLR dataset with K = 16, 32, 64, and 128.

Appendix A. Experiment: Dependence on f (K)

P

c(t) implies a failure in identifying the Condorcet winner (i.e., 1

= i∗(t)). Although
The event
U
T
E[
c(t)] = O(eAK−f (K)) is a constant function of T for any non-negative f (K), this term
t=1 U
is not negligible with large K. To evaluate the effect of f (K), we set f (K) = cK 1.01 and studied
several values of c with the MSLR dataset (Figure 3). In the case of c = 0, the regret for K = 128
becomes 100 times that for K = 16, which implies that the exponential dependence O(eAK ) may
not be an artifact of the proof. On the other hand, the results for c = 0.1, 0.3, and 1 indicate that
this term can be much improved by simply letting c be a small positive value.

Appendix B. Proofs on Regret Lower Bound

B.1. Proof of Lemma 1

∈

[K]

be arbitrary and M =

Let i
be an arbitrary preference matrix. We consider a
modiﬁed preference matrix M ′ in which the probabilities related to arm i are different from M . Let
′
i, ij element
j
Oi ∪ {

1
\ {
}
[K], µi,j ≤

. For j
[K], µi,j = 1/2
}

, that is,
1/2
}

µi,j}
{

′
i =

′
i =

∈ O

j
|

O

∈

j
j
O
|
{
of M ′ is µ′

∈
i,j such that

d+(µi,j, µ′

i,j) = d(µi,j, 1/2) + ǫ.

(11)

Such a µ′
of the KL divergence. For j /
the modiﬁed bandit problem the Condorcet winner is not arm 1 but arm i. Moreover, if M
then M ′

i,j > 1/2 uniquely exists for sufﬁciently small ǫ > 0 by the monotonicity and continuity
i,j = µi,j. Note that, unlike the original bandit problem, in
∈ Mo

i, let µ′
′

∈ O

∈ Mo.

16

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

Notation: now, let ˆX m

i,j ∈ {

0, 1
}

be the result of m-th draw of the pair (i, j),

n

KLj(n) =

log

ˆX m
ˆX m

i,jµi,j + (1
i,jµ′
i,j + (1

 

−

ˆX m
ˆX m

i,j)(1
i,j)(1

−

µi,j)
µ′
i,j) !

,

Xm=1

−
KLj(Ni,j(T )), and P′, E′ be the probability and the expectation with respect to

c

−

KL =

and
the modiﬁed bandit game. Then, for any event

j∈O′
i

P

c

c

,

E
1

holds. Let us deﬁne the events

) = E

P′(
E

exp

KL

{E}

h

−

(cid:16)

(cid:17)i

c

(12)

Ni,j(T )d(µi,j, µ′

i,j) < (1

ǫ) log T, Ni,i(T ) < √T

,

−






D1 =

D2 =
D12 =
D1\2 =




i

Xj∈O′
KL


≤
n
(cid:16)
D1 ∩ D2,
c
c
2.
D1 ∩ D

ǫ
2

1

−

log T

,

(cid:17)

o

First step (P

= o(1)): From (12),

{D12}
P′(

D12)
By using this we have

E

1
{D12}

exp

≥

h

ǫ
2

1

−

−

(cid:16)

(cid:16)

log T

= T −(1−ǫ/2)P

.
{D12}

(cid:17)

(cid:17)i

(13)

P

{D12} ≤

T (1−ǫ/2)P′(

T (1−ǫ/2)P′

D12)
Ni,i(T ) < √T

≤

≤

≤

T (1−ǫ/2)P′

o
Ni,i(T ) > T

n

T
E′[T
n
T

−

−
−

Ni,i(T )]
√T

T (1−ǫ/2)

√T

−

o

(by the Markov inequality).

(14)

Since this algorithm is strongly consistent, E′[T
o(T a) for any a > 0. Therefore, the
−
RHS of the last line of (14) is o(T a−ǫ/2), which, by choosing sufﬁciently small a, converges to zero
as T
→ ∞
Second step (P

= o(1).
= o(1)): We have

. In summary, P

Ni,i(T )]

{D12}

→

{D1\2}

P

{D1\2}




Xj∈O′

i

= P

Ni,j(T )d(µi,j, µ′

i,j) < (1

ǫ) log T, Ni,i(T ) < √T ,

KLj(Ni,j(T )) >

1

log T

−

ǫ
2

−

(cid:16)

(cid:17)






i

Xj∈O′
ǫ
2

−

1

c

(cid:17)

(cid:16)

log T

.
)

P


(

≤

{nj }∈N|O′

i

|,Pj∈O′

i

max
nj d(µi,j ,µ′

i,j)<(1−ǫ) log T

KLj(nj) >

Xj∈O′

i

17

c

KOMIYAMA HONDA KASHIMA NAKAGAWA

Note that

max
1≤n≤N

KLj(n) = max
1≤n≤N

n

log

ˆX m
ˆX m

i,jµi,j + (1
i,jµ′
i,j + (1

 

ˆX m
ˆX m

i,j)(1
i,j)(1

−

−

µi,j)
µ′
i,j) !

,

−

−

m=1
X

is the maximum of the sum of positive-mean random variables, and thus converges to is average
(c.f., Lemma 10.5 in Bubeck, 2010). Namely,

c

lim
N→∞

max
1≤n≤N

KLj(n)
N

c

= d(µi,j, µ′

i,j)

a.s.

(15)

Let δ > 0 be sufﬁciently small. We have,

max

{nj }∈N|O′

i

|,Pj∈O′

i

njd(µi,j ,µ′

i,j )<(1−ǫ) log T

KLj(nj)

j∈O′
i

log T

P

c

max

{nj}∈N|O′

i

|,Pj∈O′

i

:nj >δ log T nj d(µi,j ,µ′

i,j )<(1−ǫ) log T

≤

log T

KLj(nj)

j∈O′
i

P

c

+

minj∈O′

i

δK
d(µi,j, µ′

.

i,j)

Combining this with the fact that (15) holds for any j, we have

max

{nj}∈N|O′

i

|,Pj∈O′

i

:nj >δ log T nj d(µi,j ,µ′

i,j )<(1−ǫ) log T

log T

KLj(nj)

j∈O′
i

P

c

1

ǫ

a.s.,

≤

−

max

{nj }∈N|O′

i

|,Pj∈O′

i

njd(µi,j ,µ′

i,j)<(1−ǫ) log T

KLj(nj)

j∈O′
i

log T

P

c

≤

−

1

ǫ + Θ(δ)

a.s. (16)

By using the fact that (16) holds almost surely for any sufﬁciently small δ > 0 and 1
we have

−

ǫ/2 > 1

ǫ,

−



{nj }∈N|O′

i

|,Pj∈O′

i

max
njd(µi,j ,µ′

i,j)<(1−ǫ) log T

P



Xj∈O′

i

c

KLj(nj) >

1

log T

= o(1).

ǫ
2

−

(cid:16)

(cid:17)





In summary, we obtain P
Last step: We here have

= o(1).

D1\2

(cid:8)

(cid:9)

D1 =

Ni,j(T )d(µi,j, µ′

i,j) < (1

ǫ) log T

Ni,i(T ) < √T

−

∩




n

o

=

Ni,j(T )(d(µi,j, 1/2) + ǫ) < (1

Ni,i(T ) < √T

(By (11))

lim sup
N→∞

and thus

lim sup
T →∞

Ni,j(T )(d(µi,j, 1/2) + ǫ) +

(1

−

ǫ) log T
√T


Ni,i(T ) < (1

ǫ) log T

,

−

(17)


ǫ) log T

−

∩




n

18

o













Xj∈O′

i

Xj∈O′

i

Xj∈O′

i


⊇ 




REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

where we used the fact that
Note that, by using the result of the previous steps, P
the complementary of this fact,

A < C
{

B < C

} ∩ {

} ⊇ {

A + B < C
}
= P
{D12}

{D1}

for A, B > 0 in the last line.
+ P
= o(1). By using

{D1\2}

Ni,j(T )(d(µi,j, 1/2) + ǫ) +

(1

−

ǫ) log T
√T

Ni,i(T )

(1

ǫ) log T

≥

−

P

≥

{D

c
1}

= 1

o(1).

−






Using the Markov inequality yields



P




Xj∈O′

i

E




Xj∈O′

i

Ni,j(T )(d(µi,j, 1/2) + ǫ) +

Ni,i(T )

(1

ǫ)(1

o(1)) log T.

(18)

(1

−

ǫ) log T
√T

≥




−

−



Because E[Ni,i(T )] is subpolynomial as a function of T due to the consistency, the second term in
LHS of (18) is o(1) and thus negligible. Lemma 1 follows from the fact that (18) holds for sufﬁ-
ciently small ǫ.



B.2. Proof of Theorem 2

We have

R(T ) =

1
2

∆1,i + ∆1,j
2

Ni,j(T ) +

∆1,i + ∆1,i
2

Ni,i(T )

Xi∈[K] Xj∈[K]\{i}

≥

≥

=

Xi,j∈[K]:µi,j<1/2

Xi∈[K]\{1} Xj∈Oi

Xi∈[K]\{1} Xj∈Oi

∆1,i + ∆1,j
2

Ni,j(T ) +

∆1,i + ∆1,i
2

Ni,i(T )

Xi∈[K]

Xi∈[K]

∆1,i + ∆1,j
2

Ni,j(T )

∆1,i + ∆1,j
2d(µi,j, 1/2)

d(µi,j, 1/2)Ni,j (T ).

Taking the expectation on both sides and using Lemma 1 yield

E[R(T )]

≥

Xi∈[K]\{1}

min
j∈Oi

∆1,i + ∆1,j
2d(µi,j, 1/2)

(1

o(1)) log T.

−

Appendix C. Proof of Lemma 5

This lemma essentially states that, the expected number of the rounds in which arm 1 is underesti-
mated is O(1). We show this by bounding the expected number of rounds before arm 1 is compared,

19

KOMIYAMA HONDA KASHIMA NAKAGAWA

for each ﬁxed set of
16 in Honda and Takemura (2010). Note that

N1,s(t)
}
{

and summing over

N1,s(t)
{

. This technique is inspired by Lemma
}

c(t) =

U

[S∈2[K]\{1}\{∅} (

\s∈S

ˆµ1,s(t)
{

≤

1/2

} ∩

ˆµ1,s(t) > 1/2
{

.

})

(19)

\s /∈S

Now we bound the number of rounds that the event

ˆµ1,s(t)
{

≤

1/2

} ∩

ˆµ1,s(t) > 1/2
}
{

\s∈S
occurs. Let N be the set of non-zero natural numbers, ns ∈
for each s
∈
1/2, d+(ˆµns
1,s, 1/2) = xs, N1,s(t) = ns}

N and xs ∈
i,j be the empirical estimate of µi,j at n-th draw of pair (i, j). If
S and ˆµ1,s(t) > 1/2 holds for s /
∈

S. Let ˆµn

holds for s

\s /∈S

∈

[0, log 2] be arbitrary

ˆµns
1,s ≤
{
S then

I1(t) =

nsd+(ˆµ1,s(t), 1/2)

Xs∈S

and therefore

J1(t) holds for any

exp

t

≥

 

nsd+(ˆµ1,s(t), 1/2)

f (K)

.

−

!

Xs∈S
J1(t) occurs, then arm 1 is in LN of the next loop, and thus for some s

If
within 2K rounds. Therefore we have

∈

S, N1,s is incremented

T

1

Xt=Tinit+1

"
\s∈S

ˆµ1,s(t)
{

1/2, N1,s(t) = ns} ∩

≤

ˆµ1,s(t) > 1/2
{

}#

\s /∈S

exp

≤

 

Xs∈S

nsd+(ˆµns

1,s, 1/2)

f (K)

+ 2K.

−

!

20

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

Letting Ps(xs) = Pr[ˆµns

1/2, d+(ˆµns

1,s, 1/2)

xs], we have

1,s ≤

≥

E

T

1


Xt=Tinit+1

=

"

\s∈S

Z{xs}∈[0,log 2]|S|  

ˆµ1,s(t)
{

≤

1/2, N1,s(t) = ns} ∩

ˆµ1,s(t) > 1/2
{

exp

 

Xs∈S

nsxs −

f (K)

+ 2K

!

d(

Ps(xs))

−

\s /∈S

!

Ys∈S

}#


(

(

(

(

(

= e−f (K)

2K

Ps(0) +

ensxsd(

Ps(xs))

−

)

= e−f (K)

2K

Ps(0) +

ensxsPs(xs)]log 2

0 +

Ys∈S

Ys∈S

Ys∈S Zxs∈[0,log 2]
[
−

Ys∈S  

(integration by parts)

nsensxsPs(xs)dxs

!)

Zxs∈[0,log 2]

≤

≤

e−f (K)

(1 + 2K)

Ps(0) +

nsensxse−ns(xs+C1(µ1,s,1/2))dxs

(by the Chernoff bound and Fact 10, where C1(µ, µ2) = (µ

Ys∈S Zxs∈[0,log 2]

e−f (K)

(1 + 2K)

e−nsd(1/2,µ1,s) +

µ2)2/(2µ(1

−

−
nse−nsC1(µ1,s,1/2)dxs

)

µ2)))

)

Ys∈S Zxs∈[0,log 2]

Ys∈S

)

= e−f (K)

(1 + 2K)

e−nsd(1/2,µ1,s) +

(log 2)nse−nsC1(µ1,s,1/2)

.

(20)

Ys∈S

Ys∈S

Ys∈S
,
ns}
{

By summing (20) over

T

P

Xt=Tinit+1

"
\s∈S

ˆµ1,s(t)
{

≤

1/2

} ∩

ˆµ1,s(t) > 1/2
{

}#

e−f (K)

(1 + 2K)

e−nsd(1/2,µ1,s) +

(log 2)nse−nsC1(µ1,s,1/2)

· · ·
X{ns}∈N|S|  
X

e−f (K)

(1 + 2K)

(

ed(1/2,µ1,s )

1

+ (log 2)|S|

where we used the fact that
(19) and the union bound over all S

P

∈

2[K]\{1}

, we obtain
P

\ {∅}

Ys∈S
−
∞
n=1 e−nx = 1/(ex + 1) and

Ys∈S
−
∞
n=1 ne−nx = ex/(ex + 1)2. Using

!

Ys∈S

eC1(µ1,s,1/2)

(eC1(µ1,s,1/2)

,

1)2 )

≤

≤

T

\s /∈S

Ys∈S
1

E


Xt=Tinit+1

< e−f (K)

c(t)

1

{U

}


(1 + 2K)






= O(eAK−f (K)),

1 +

1

ed(1/2,µ1,s )

1

(cid:19)

−

Ys∈[K]\{1} (cid:18)

+ (log 2)K−1

1 +

eC1(µ1,s,1/2)

Ys∈[K]\{1}  

(eC1(µ1,s,1/2)

1)2 !


−

(21)



21

KOMIYAMA HONDA KASHIMA NAKAGAWA

where A = log

maxs∈[K]\{1} max

1 +

1
ed(1/2,µ1,s )−1

, log 2

1 + eC1(µ1,s,1/2)

(eC1(µ1,s,1/2)−1)2

(cid:26)

(cid:18)

(cid:18)

.
(cid:19)(cid:19)(cid:27)

Appendix D. Proof of Lemma 6

Tinit + K + 1 (i.e., after
Except for the ﬁrst loop, arm i must put into LN before
the ﬁrst loop), let τ (t) < t be the round in the previous loop in which arm l(t) is put into LN . In the
round,
Tinit + K + 1 such
= τ (t2) holds because τ (t1) and τ (t2) belong to different
that l(t1) = l(t2) = i, t1 6
loops. By using τ (t), we obtain

Jl(t)(τ (t)) is satisﬁed. With this deﬁnition, for any two rounds t1, t2 ≥

. For t
l(t) = i
}
{

= t2 ⇒

τ (t1)

≥

T

1[l(t) = i, Ni,j(t)

N Suf

i,j (δ)]

≥

U

Xt=Tinit+1

K +

T

≤

≤

K +

Xt=Tinit+K+1
T
1[
U

Xt=Tinit+1

T

Xt=Tinit+K+1
1[l(t) = i,

c(t)] +

Xt=Tinit+K+1

1[l(t) = i,

c(τ (t))] +

1[l(t) = i,

(τ (t)), Ni,j(t)

U

N Suf

i,j (δ)]

≥

T

(τ (t)), Ni,j(t)

U

N Suf

i,j (δ)].

≥

Note that the expectation of term
t, the only round in which pair (i, j) can be compared is the round of
P
once, and thus Ni,j(t)
Ni,j(τ (t))

c(t)] is bounded by Lemma 5. Between τ (t) and
that occurs at most
l(t) = j
{
1. By using this fact, we obtain

T
t=Tinit+1

1[
U

}

−

T

Xt=Tinit+K+1

T

≤

U

≤

≤

T

Xt=Tinit+K+1
1[
Ji(t),

U

Xt=Tinit+1

1[l(t) = i,

(τ (t)), Ni,j(t)

N Suf

i,j (δ)]

≥

1[l(t) = i,

Ji(τ (t)),

U

(τ (t)), Ni,j(τ (t))

N Suf

i,j (δ)

1]

−

≥

(t), Ni,j (t)

N Suf

i,j (δ)

1].

−

≥

(22)

22

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

1[
Ji(t),

U

(t), Ni,j (t)

N Suf

i,j (δ)

1]

−

≥

We can bound this term via Ii(t) as

T

Xt=Tinit+1

T

T

i,j (δ)−1⌉

Xn=⌈N Suf
T

[t=Tinit+1(cid:16)
T

≤

≤

≤

≤

i,j (δ)−1⌉

Xn=⌈N Suf
T

i,j (δ)−1⌉

Xn=⌈N Suf
T

Xn=⌈N Suf

i,j (δ)−1⌉

1

1

1







h

(cid:20)

[t=Tinit+1(cid:16)


(N Suf

i,j (δ)

−

Ij(t)

log t + f (K), Ni,j(t) = n

≤

(by

(t)

U

⇒

I1(t) = 0)

Ni,j(t) = n, Ni,j(t)d+(ˆµn

i,j, 1/2)

log t + f (K)

(cid:17)





(cid:17)





≤

i

1)d+(ˆµn

i,j, 1/2)

log T + f (K)

1

d+(ˆµn

i,j, 1/2)

d(µi,j, 1/2)
1 + δ

≤

≤

.

(cid:21)

(23)

Therefore, by letting µ

(1/2, µi,j ) be a real number such that d(µ, 1/2) = d(µi,j ,1/2)

, we

1+δ

∈
obtain from the Chernoff bound and the monotonicity of d+(
, 1/2) that
·

T

E


Xt=Tinit+1


1[
Ji(t),

U

(t), Ni,j (t)

N Suf

i,j (δ)

1]

−



≤

≥

T

P

d+(ˆµn
(cid:20)

i,j, 1/2)

d(µi,j, 1/2)
1 + δ

≤

(cid:21)

i,j (δ)−1⌉

Xn=⌈N Suf
T



≤

Xn=⌈N Suf

i,j (δ)−1⌉
1

exp (

d(µ, µi,j)n)

−

≤

exp (d(µ, µi,j))

1

−

<

1
d(µ, µi,j)

.

From the Pinsker’s inequality it is easy to conﬁrm that d(µ, µi,j) = Ω(δ2), which completes the
proof.

Appendix E. Optimal Regret Bound: Full Proof of Theorem 4

Events: Deﬁne

Yi =

ˆµ⌈α log log T ⌉
i,j

{|

µi,j|

< ∆suf
i }

−

for sufﬁciently small but ﬁxed ∆suf
in µi,j that
µi,j}i,j∈[K]. Let also
{

Yi implies ˆb⋆(i) = b⋆(i) when we let ∆suf

\i,j∈[K]
i > 0. It is easy to see from the condinuity of d+(µi,j, 1/2)
i > 0 be sufﬁciently small with respect to

Zi(t) =

.
ˆµi,b⋆(i)(t) < 1/2
}
{

23

KOMIYAMA HONDA KASHIMA NAKAGAWA

First step (regret decomposition): Like RMED1, in RMED2FH E[
bility (i.e., Lemma 5). In the following, we bound the regret under

U
(t): let

(t)] holds with high proba-

U

{Y
(B)

{z

r(t)
Zi(t)
}

+ 1
l(t) = i,
{

U

(t),

c
i ∪ Z

c
i (t)

}}

r(t)

(24)

In the following, we ﬁrst bound the terms (A) and (B), and then summarizing all terms to prove

}

|

}

|

ri(t) = 1
l(t) = i,
{
= 1
l(t) = i,
{

r(t)
(t)
}
Yi,
(t),

U

U

(A)

{z

Theorem 4.
Second step (bounding (A)): Note that,
l(t) = i,
{
ˆb⋆(i) = b⋆(i) and ˆb⋆(i)
ˆ
Oi(t). Therefore,

∈

1
l(t) = i,
{

U

(t),

Yi,

r(t)
Zi(t)
}

(t),

Yi,

Zi(t)
}

U

is a sufﬁcient condition for

1
l(t) = i, Ni,b⋆(i)(t)
{

≥

N Suf

i,b⋆(i)(δ)

∆1,i + ∆1,b⋆(i)
2

+

}

N Suf

i,b⋆(i)(δ) +

∆1,i
2

N Suf

i,b⋆(i)(δ)
log log T

.

By applying Lemma 6 with j = b⋆(i), for sufﬁciently small δ > 0 we have

1
l(t) = i, Ni,b⋆(i)(t)
{

≥

N Suf

i,b⋆(i)(δ)

O

}

≤

+ O(eAK−f (K)) + K.

1
δ2

(cid:18)

(cid:19)



In summary, term (A) is bounded as:

T

Xt=Tinit+1
T

≤

Xt=Tinit+1

T

Xt=Tinit+1

E





T

E


Xt=Tinit+1


1
l(t) = i,
{

U

(t),

Yi,

r(t)
Zi(t)
}



∆1,i + ∆1,b⋆(i)
2

≤

N Suf

i,b⋆(i)(δ) + O



log T
log log T

1
δ2

+ O

+ O(eAK−f (K)) + K.

(25)

(cid:19)
. Under
Third step (bounding (B)): Now we consider the case
(t),
}}
this event ˆb⋆(i) = b⋆(i) does not always hold but we can see that m(t)
still holds.
Furthermore, under this event arm ˆb⋆(i) is selected as m(t) at most (log log T )Ni,1(T ) + 1 times

c
c
i (t)
i ∪ Z
ˆb⋆(i), 1
}

l(t) = i,
{

{Y
∈ {

(cid:18)

(cid:19)

(cid:18)

U

24

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

due to Line 5 of Algorithm 3. By using these facts, we have,

T

E


Xt=Tinit+1

T
E

≤

E

≤


Xt=Tinit+1

T


Xt=Tinit+1


1
l(t) = i,
{

U

(t),

c
i ∪ Z

c
i (t)

}}

{Y

r(t)





T

[t′=Tinit+1

i (t′)
c

Z

}}



1
l(t) = i,
{

U

(t),

c
i ∪

{Y

1
l(t) = i, Ni,1(t)
{

≥

N Suf

i,1 (δ)

}



T

T

+ P

c
i ∪

Y




i (t′)
c

Z

N Suf
(cid:16)




[t′=Tinit+1

i,1 (δ) log log T + 1 + N Suf

i,1 (δ)

(cid:17)

O

≤

1
δ2

(cid:18)

(cid:19)



+ O(eAK−f (K)) + K + P

(by Lemma 6).

c
i ∪

Y






i (t′)
c

Z

O

N Suf
(cid:16)

i,1 (δ) log log T

(cid:17)

[t′=Tinit+1






The following lemma bounds P

Lemma 7 For RMED2FH, there exists C2 = C2(
µi,j}
{

c
i ∪

Y

T
t′=Tinit+1 Z

n

.

c
i (t′)
o
, K, α) > 0 such that

S

T

P

c
i ∪

Y






[t=Tinit+1

c
i (t)

Z

= O((log T )−C2).

In summary, term (B) is bounded as:

1
l(t) = i,
{

U

(t),

c
i ∪ Z

c
i (t)

}}

{Y

r(t)

E


Xt=Tinit+1


O

≤

1
δ2

(cid:18)

(cid:19)

+ O(eAK−f (K)) + K + O

i,1 (δ)(log T )−C2 log log T

.

(26)

N Suf
(cid:16)

(cid:17)










25

P((A) + (B))

(by Lemma 5 and inequality (24))

≤

≤

+

≤

KOMIYAMA HONDA KASHIMA NAKAGAWA

Last step (regret bound):

T

E[R(T )]

Tinit +

≤

P



{U

c(t)
}

+

P

ri(t)
(t), l(t) = i
}



{U

T

Tinit +



Xt=Tinit+1

Xt=Tinit+1


O(eAK−f (K)) +

Xi∈[K]\{1}

Xi∈[K]\{1}


O(αK 2 log log T ) + O(eAK−f (K))

∆1,i + ∆1,b⋆(i)
2

N Suf

i,b⋆(i)(δ) + O

log T
log log T

(cid:18)

(cid:19)

Xi∈[K]\{1}(







1
δ2

(cid:18)
(by (25) and (26))

(cid:19)

+ O

+ O(eAK−f (K)) + 2K + O

i,1 (δ)(log T )−C2 log log T

N Suf
(cid:16)

)

(cid:17)

(∆1,i + ∆1,b⋆(i))((1 + δ) log T )
2d(µi,b⋆(i), 1/2)

Xi∈[K]\{1}

O(αK 2 log log T ) + O(KeAK−f (K)) +

K log T
log log T

+ O

(cid:18)

K
δ2

(cid:19)

(cid:18)

(cid:19)

(cid:0)

+ O

+ O

K(log T )1−C2 log log T

+ O (Kf (K)) .

(27)

Combining (27) with the fact that O

K(log T )1−C2 log log T

completes the proof.

(cid:0)

(cid:1)
= o

K log T
log log T

(cid:1)

(cid:16)

(cid:17)

E.1. Proof of Lemma 7
and P
We bound P

c
i }

{Y

T
t=Tinit+1 Z

c
i (t)
}

separately. On the one hand,

{
S
ˆµ⌈α log log T ⌉
i,j
|

P

c
i }

{Y

= P




[i,j∈[K]

µi,j| ≥

−

∆suf

P

ˆµ⌈α log log T ⌉
i,j

{|

≤

i 

(by the Chernoff bound and Pinsker’s inequality)


Xi,j∈[K]

−

µi,j| ≥

∆suf
i }

2 exp (

2(∆suf

i )2α log log T )

−

2 (log T )−2(∆suf

i )2α = 2K 2 (log T )−2(∆suf

i )2α = O((log T )−Ca),

≤

=


Xi,j∈[K]

Xi,j∈[K]

26

REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM

where Ca = 2(∆suf

i )2α/K 2 > 0. On the other hand,

T

[t=Tinit+1
T

P





= P

c
i (t)

Z






[t=Tinit+1
∞






Xn=⌈α log log T ⌉
∞

−

Xn=⌈α log log T ⌉
(log T )−αd(1/2,µi,b⋆ (i))

≤

≤

≤

≤

∞

ˆµi,b⋆(i)(t) < 1/2

≤




P





Ni,b⋆(i)(t) = n, ˆµn
{
[n=⌈α log log T ⌉

i,b⋆(i) < 1/2

}



P

Ni,b⋆(i)(t) = n, ˆµn
{



i,b⋆(i) < 1/2
}

exp (

d(1/2, µi,b⋆(i))n)

(by the Chernoff bound)

(log T )−αd(1/2,µi,b⋆ (i))

1 +

= O((log T )−Cb),

∞

Xn=0

(cid:18)

exp (

d(1/2, µi,b⋆(i))n)

−

1

d(1/2, µi,b⋆(i))

1

(cid:19)

−

where Cb = αd(1/2, µi,b⋆(i)) > 0. The proof is completed by letting C2 = min (Ca, Cb) and taking
the union bound of P
c
.
i (t)
}

T
t=Tinit+1 Z

and P

c
i }

{Y

{
S

Appendix F. Facts

Fact 8 (The Chernoff bound)
Let X1, . . . , Xn be i.i.d. binary random variables. Let ˆX = 1
n
any ǫ > 0,

n

i=1 Xi and µ = E[ ˆX]. Then, for

and

P( ˆX

≥

µ + ǫ)

exp (

d(µ + ǫ, µ)n)

≤

−

P

P( ˆX

µ

ǫ)

≤

−

≤

exp (

d(µ

ǫ, µ)n).

−

−

Fact 9 (The Pinsker’s inequality)
For p, q

∈

(0, 1), the KL divergence between two Bernoulli distributions is bounded as:

d(p, q)

2(p

≥

−

q)2.

Fact 10 (A minimum difference between divergences (Lemma 13 in Honda and Takemura, 2010))
For any µ and µ2 satisfying 0 < µ2 < µ < 1. Let C1(µ, µ2) = (µ
µ2)). Then, for
any µ3 ≤

µ2)2/(2µ(1

µ2,

−

−

d(µ3, µ)

d(µ3, µ2)

C1(µ, µ2) > 0.

−

≥

27

