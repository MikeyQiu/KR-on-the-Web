Low-Precision Random Fourier Features for Memory-Constrained
Kernel Approximation

9
1
0
2
 
r
a

M
 
0
2
 
 
]

G
L
.
s
c
[
 
 
2
v
5
5
1
0
0
.
1
1
8
1
:
v
i
X
r
a

Jian Zhang∗

Avner May∗

Tri Dao

Christopher R´e

Stanford University

Abstract

We investigate how to train kernel approxi-
mation methods that generalize well under
a memory budget. Building on recent theo-
retical work, we deﬁne a measure of kernel
approximation error which we ﬁnd to be more
predictive of the empirical generalization per-
formance of kernel approximation methods
than conventional metrics. An important con-
sequence of this deﬁnition is that a kernel
approximation matrix must be high rank to
attain close approximation. Because storing a
high-rank approximation is memory intensive,
we propose using a low-precision quantiza-
tion of random Fourier features (LP-RFFs)
to build a high-rank approximation under a
memory budget. Theoretically, we show quan-
tization has a negligible eﬀect on generaliza-
tion performance in important settings. Em-
pirically, we demonstrate across four bench-
mark datasets that LP-RFFs can match the
performance of full-precision RFFs and the
Nystr¨om method, with 3x-10x and 50x-460x
less memory, respectively.

1 INTRODUCTION

Kernel methods are a powerful family of machine learn-
ing methods. A key technique for scaling kernel meth-
ods is to construct feature representations whose inner
products approximate the kernel function, and then
learn a linear model with these features; important ex-
amples of this technique include the Nystr¨om method
(Williams and Seeger, 2000) and random Fourier fea-
tures (RFFs) (Rahimi and Recht, 2007). Unfortunately,
a large number of features are typically needed for at-
taining strong generalization performance with these

∗Equal contribution.

Proceedings of the 22nd International Conference on Ar-
tiﬁcial Intelligence and Statistics (AISTATS) 2019, Naha,
Okinawa, Japan. PMLR: Volume 89. Copyright 2019 by
the author(s).

methods on big datasets (Rahimi and Recht, 2008; Tu
et al., 2016; May et al., 2017). Thus, the memory re-
quired to store these features can become the training
bottleneck for kernel approximation models. In this
paper we work to alleviate this memory bottleneck by
optimizing the generalization performance for these
methods under a ﬁxed memory budget.

To gain insight into how to design more memory-
eﬃcient kernel approximation methods, we ﬁrst in-
vestigate the generalization performance vs. memory
utilization of Nystr¨om and RFFs. While prior work
(Yang et al., 2012) has shown that the Nystr¨om method
generalizes better than RFFs under the the same num-
ber of features, we demonstrate that the opposite is
true under a memory budget. Strikingly, we observe
that 50,000 standard RFFs can achieve the same held-
out accuracy as 20,000 Nystr¨om features with 10x less
memory on the TIMIT classiﬁcation task. Further-
more, this cannot be easily explained by the Frobenius
or spectral norms of the kernel approximation error ma-
trices of these methods, even though these norms are
the most common metrics for evaluating kernel approx-
imation methods (Gittens and Mahoney, 2016; Yang
et al., 2014; Sutherland and Schneider, 2015; Yu et al.,
2016; Dao et al., 2017); the above Nystr¨om features
attain 1.7x smaller Frobenius error and 17x smaller
spectral error compared to the RFFs. This observa-
tion suggests the need for a more reﬁned measure of
kernel approximation error—one which better aligns
with generalization performance, and can thus better
guide the design of new approximation methods.

Building on recent theoretical work (Avron et al., 2017),
we deﬁne a measure of approximation error which we
ﬁnd to be much more predictive of empirical generaliza-
tion performance than the conventional metrics. In par-
ticular, we extend Avron et al.’s deﬁnition of ∆-spectral
approximation to our deﬁnition of (∆1, ∆2)-spectral ap-
proximation by decoupling the two roles played by ∆
in the original deﬁnition.1 This decoupling reveals that

1The original deﬁnition uses the same scalar ∆ to upper
and lower bound the approximate kernel matrix in terms
of the exact kernel matrix in the semideﬁnite order.

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Rd, kernel features
Table 1: Memory utilization for kernel approximation methods. We consider data x
Rm, mini-batch size s, # of classes c (for regression/binary classiﬁcation c = 1). We assume full-precision
z(x)
numbers are 32 bits. We measure a method’s memory utilization as the sum of the three components in this table.

∈

∈

Approximation Method

Nystr¨om
RFFs
Circulant RFFs
Low-precision RFFs, b bits (ours)

Feature generation Feature mini-batch Model parameters
32(md + m2)
32md
32m
32m

32ms
32ms
32ms
bms

32mc
32mc
32mc
32mc

∆1 and ∆2 impact generalization diﬀerently, and can
together much better explain the relative generalization
performance of Nystr¨om and RFFs than the original
∆, or the Frobenius or spectral errors. This (∆1, ∆2)
deﬁnition has an important consequence—in order for
an approximate kernel matrix to be close to the exact
kernel matrix, it is necessary for it to be high rank.

Motivated by the above connection between rank and
generalization performance, we propose using low-
precision random Fourier features (LP-RFFs) to attain
a high-rank approximation under a memory budget.
Speciﬁcally, we store each random Fourier feature in a
low-precision ﬁxed-point representation, thus achieving
a higher-rank approximation with more features in the
same amount of space. Theoretically, we show that
when the quantization noise is much smaller than the
regularization parameter, using low precision has negli-
gible eﬀect on the number of features required for the
approximate kernel matrix to be a (∆1, ∆2)-spectral
approximation of the exact kernel matrix. Empiri-
cally, we demonstrate across four benchmark datasets
(TIMIT, YearPred, CovType, Census) that in the mini-
batch training setting, LP-RFFs can match the perfor-
mance of full-precision RFFs (FP-RFFs) as well as the
Nystr¨om method, with 3x-10x and 50x-460x less mem-
ory, respectively. These results suggest that LP-RFFs
could be an important tool going forward for scaling
kernel methods to larger and more challenging tasks.

The rest of this paper is organized as follows: In Section
2 we compare the performance of the Nystr¨om method
and RFFs in terms of their training memory footprint.
In Section 3 we present a more reﬁned measure of kernel
approximation error to explain the relative performance
of Nystr¨om and RFFs. We introduce the LP-RFF
method and corresponding analysis in Section 4, and
present LP-RFF experiments in Section 5. We review
related work in Section 6, and conclude in Section 7.

2 NYSTR ¨OM VS. RFFS: AN

EMPIRICAL COMPARISON

tion of Nystr¨om and RFFs. We begin by reviewing
the memory utilization for these kernel approximation
methods in the mini-batch training setting; this is a
standard setting for training large-scale kernel approxi-
mation models (Huang et al., 2014; Yang et al., 2015;
May et al., 2017), and it is the setting we will be us-
ing to evaluate the diﬀerent approximation methods
(Sections 2.2, 5.1). We then show that RFFs outper-
form Nystr¨om given the same training memory budget,
even though the opposite is true given a budget for
the number of features (Yang et al., 2012). Lastly, we
demonstrate that the Frobenius and spectral norms of
the kernel approximation error matrix align poorly with
generalization performance, suggesting the need for a
more reﬁned measure of approximation error for eval-
uating the quality of a kernel approximation method;
we investigate this in Section 3.

For background on RFFs and the Nystr¨om method,
and for a summary of our notation, see Appendix A.

2.1 Memory Utilization

The optimization setting we consider is mini-batch
training over kernel approximation features. To un-
derstand the training memory footprint, we present in
Table 1 the memory utilization of the diﬀerent parts
of the training pipeline. The three components are:

1. Feature generation: Computing m RFFs over data
in Rd requires a random projection matrix W
Rm
×
points” ˆxi

∈
d. The Nystr¨om method stores m “landmark
m.

Rd, and a projection matrix in Rm
×

2. Feature mini-batch: Kernel approximation features
Rm for all xi in a mini-batch are stored.2

z(xi)

∈

∈

3. Model parameters: For binary classiﬁcation and
regression, the linear model learned on the z(x) fea-
Rm; for c-class classiﬁcation,
tures is a vector θ
it is a matrix θ
×

∈
Rm

c.

∈

In this work we focus on reducing the memory occupied
by the mini-batches of features, which can occupy a

To inform our design of memory-eﬃcient kernel approx-
imation methods, we ﬁrst perform an empirical study
of the generalization performance vs. memory utiliza-

2For simplicity, we ignore the memory occupied by the
mini-batches of d-dim. inputs and c-dim. outputs, as gener-
ally the number of kernel approx. features m

d, c.

(cid:29)

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

(a)

(b)

(c)

(d)

Figure 1: Generalization performance of full-precision RFFs and Nystr¨om with respect to the number of features
and training memory footprint on TIMIT (a,b). Nystr¨om performs better for a ﬁxed number of features, while
RFFs perform better under a memory budget. We also see that the generalization performance of these methods
does not align well with the Frobenius or spectral norms of their respective kernel approximation error matrices
(c,d). For results on YearPred, CovType, and Census, see Appendix D.2.

signiﬁcant fraction of the training memory. Our work
is thus orthogonal to existing work which has shown
how to reduce the memory utilization of the feature
generation (Le et al., 2013; Yu et al., 2015) and the
model parameters (Sainath et al., 2013a; Sindhwani
et al., 2015; De Sa et al., 2018) (e.g., using structured
matrices or low precision). Throughout this paper, we
measure the memory utilization of a kernel approxima-
tion method as the sum of the above three components.

2.2 Empirical Comparison

We now compare the generalization performance of
RFFs and the Nystr¨om method in terms of their train-
ing memory footprint. We demonstrate that RFFs
can outperform the Nystr¨om method given a memory
budget, and show that the diﬀerence in performance
between these methods cannot be explained by the
Frobenius or spectral norms of their kernel approxima-
tion error matrices.

In experiments across four datasets (TIMIT, YearPred,
CovType, Census (Garofolo et al., 1993; Dheeru and
Karra Taniskidou, 2017)), we use up to 20k Nystr¨om
features and 400k RFFs to approximate the Gaus-
sian kernel;3 we train the models using mini-batch
stochastic gradient descent with early stopping, with
a mini-batch size of 250. We present results averaged
from three random seeds, with error bars indicating
standard deviations (for further experiment details,
see Appendix D.2). In Figure 1(a) we observe that
as a function of the number of kernel approximation
features the Nystr¨om method generally outperforms
RFFs, though the gap narrows as m approaches 20k.
However, we see in Figure 1(b) that RFFs attain better
generalization performance as a function of memory.
Interestingly, the relative performance of these meth-

ods cannot simply be explained by the Frobenius or
spectral norms of the kernel approximation error ma-
trices;4 in Figure 1(c,d) we see that there are many
cases in which the RFFs attain better generalization
performance, in spite of having larger Frobenius or
spectral approximation error. This is a phenomenon
we observe on other datasets as well (Appendix D.2).
This suggests the need for a more reﬁned measure of the
approximation error of a kernel approximation method,
which we discuss in the following section.

3 A REFINED MEASURE OF
KERNEL APPROX. ERROR

To explain the important diﬀerences in performance be-
tween Nystr¨om and RFFs, we deﬁne a more reﬁned mea-
sure of kernel approximation error—(∆1, ∆2)-spectral
approximation. Our deﬁnition is an extension of Avron
et al.’s deﬁnition of ∆-spectral approximation, in which
we decouple the two roles played by ∆ in the original
deﬁnition. This decoupling allows for a more ﬁne-
grained understanding of the factors inﬂuencing the
generalization performance of kernel approximation
methods, both theoretically and empirically. Theoret-
ically, we present a generalization bound for kernel
approximation methods in terms of (∆1, ∆2) (Sec. 3.1),
and show that ∆1 and ∆2 inﬂuence the bound in dif-
ferent ways (Prop. 1). Empirically, we show that ∆1
and ∆2 are more predictive of the Nystr¨om vs. RFF
performance than the ∆ from the original deﬁnition,
and the Frobenius and spectral norms of the kernel
approximation error matrix (Sec. 3.2, Figure 2). An
important consequence of the (∆1, ∆2) deﬁnition is
that attaining a small ∆1 requires a large number of
features; we leverage this insight to motivate our pro-
posed method, low-precision random Fourier features,
in Section 4.

3We consider diﬀerent ranges for the number of Nystr¨om
vs. RFF features because the memory footprint for training
with 400k RFFs is similar to 20k Nystr¨om features.

˜K,
4We consider the Frobenius and spectral norms of K
where K and ˜K are the exact and approximate kernel
matrices for 20k randomly sampled heldout points.

−

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

3.1

(∆1, ∆2)-spectral Approximation

We begin by reviewing what it means for a matrix
A to be a ∆-spectral approximation of a matrix B
(Avron et al., 2017). We then extend this deﬁnition
to (∆1, ∆2)-spectral approximation, and bound the
generalization performance of kernel approximation
methods in terms of ∆1 and ∆2 in the context of ﬁxed
design kernel ridge regression.
0, a symmetric matrix A is a
Deﬁnition 1. For ∆
∆-spectral approximation of another symmetric matrix
(1 + ∆)B.
B if (1

∆)B

≥

A

−

(cid:22)

(cid:22)

We extend this deﬁnition by allowing for diﬀerent values
of ∆ in the left and right inequalities above:
0, a symmetric matrix
Deﬁnition 2. For ∆1, ∆2 ≥
A is a (∆1, ∆2)-spectral approximation of another
(1 + ∆2)B.
symmetric matrix B if (1

−
Throughout the text, we will use ∆ to denote the
variable in Def. 1, and (∆1, ∆2) to denote the variables
In our discussions and experiments, we
in Def. 2.
always consider the smallest ∆, ∆1, ∆2 satisfying the
above deﬁnitions; thus, ∆ = max(∆1, ∆2).

∆1)B

(cid:22)

(cid:22)

A

In the paragraphs that follow we present generalization
bounds for kernel approximation models in terms of
∆1 and ∆2 in the context of ﬁxed design kernel ridge
regression, and demonstrate that ∆1 and ∆2 inﬂuence
generalization in diﬀerent ways (Prop. 1). We consider
the ﬁxed design setting because its expected generaliza-
tion error has a closed-form expression, which allows us
to analyze generalization performance in a ﬁne-grained
fashion. For an overview of ﬁxed design kernel ridge
regression, see Appendix A.3.

×

n, a regularization parameter λ
(xi, yi)

In the ﬁxed design setting, given a kernel matrix K
∈
Rn
0, and a set of
≥
n
labeled points
i=1 where the observed labels
}
yi = ¯yi + (cid:15)i are randomly perturbed versions of the true
(cid:3) = σ2 <
R ((cid:15)i independent, E [(cid:15)i] = 0, E (cid:2)(cid:15)2
labels ¯yi
), it is easy to show (Alaoui and Mahoney, 2015) that

∞
the optimal kernel regressor5 fK has expected error

∈

{

i

(fK) =

R

λ2
n

¯yT (K + λI)−

2 ¯y +

tr

K 2(K + λI)−

2(cid:17)

,

(cid:16)

σ2
n

where ¯y = (¯y1, . . . , ¯yn) is the vector of true labels.

This closed-form expression for generalization error
allows us to bound the expected loss
(f ˜K) of a kernel
ridge regression model f ˜K learned using an approximate
kernel matrix ˜K in place of the exact kernel matrix K.
In particular, if we deﬁne

R

(fK) :=

(cid:98)
R

λ
n

¯yT (K + λI)−

1 ¯y +

tr

K(K + λI)−

1(cid:17)

,

(cid:16)

σ2
n

5fK (x) = (cid:80)

i αik(x, xi) for α = (K + λI)−1y.

∞

(fK), we can bound the

which is an upper bound on
R
expected loss of f ˜K as follows:
Proposition 1. (Extended from (Avron et al., 2017))
Suppose ˜K + λI is (∆1, ∆2)-spectral approximation of
0. Let m denote
[0, 1) and ∆2 ≥
K + λI, for ∆1 ∈
the rank of ˜K, and let fK and f ˜K be the kernel ridge
regression estimators learned using these matrices, with
regularizing constant λ
0 and label noise variance
σ2 <
. Then

≥

(f ˜K)

R

≤

1

1
∆1

(cid:98)
R

−

(fK) +

∆2
1 + ∆2

m
n

σ2.

(1)

We include a proof in Appendix B.1. This result shows
that smaller values for ∆1 and ∆2 imply tighter bounds
on the generalization performance of the model trained
with ˜K. We can see that as ∆1 approaches 1 the bound
diverges, and as ∆2 approaches
the bound plateaus.
We leverage this generalization bound to understand
the diﬀerence in performance between Nystr¨om and
RFFs (Sec. 3.2), and to motivate and analyze our pro-
posed low-precision random Fourier features (Sec. 4).

∞

Remark The generalization bound in Prop. 1 as-
sumes the regressor fK is computed via the closed-
form solution for kernel ridge regression. However, in
Sections 4-5 we focus on stochastic gradient descent
(SGD) training for kernel approximation models. Be-
cause SGD can also ﬁnd the model which minimizes
the regularized empirical loss (Nemirovski et al., 2009),
the generalization results carry over to our setting.

3.2 Revisiting Nystr¨om vs. RFF Comparison

In this section we show that the values of ∆1 and ∆2
such that the approximate kernel matrix is a (∆1, ∆2)-
spectral approximation of the exact kernel matrix cor-
relate better with generalization performance than the
original ∆, and the Frobenius and spectral norms of
the kernel approximation error; we measure correlation
using Spearman’s rank correlation coeﬃcient ρ.

To study the correlation of these metrics with gen-
eralization performance, we train Nystr¨om and RFF
models for many feature dimensions on the Census
regression task, and on a subsampled version of 20k
train and heldout points from the CovType classiﬁca-
tion task. We choose these small datasets to be able
to compute the various measures of kernel approxima-
tion error over the entire heldout set. We measure the
˜K, and the ∆
spectral and Frobenius norms of K
and (∆1, ∆2) values between K + λI and ˜K + λI (λ
chosen via cross-validation), where K and ˜K are the
exact and approximate kernel matrices for the heldout
set. For more details about these experiments and how
we compute ∆ and (∆1, ∆2), see Appendix D.3.

−

In Figure 2, we plot the generalization performance on
these tasks as a function of these metrics; while the

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Figure 2: The correlation between generalization performance and diﬀerent measures of kernel approximation
error for the full-precision RFF and Nystr¨om methods. We see that generalization performance aligns well with
1/(1
∆1) (Spearman rank correlation coeﬃcient ρ = 0.958), while aligning poorly with ∆ and the spectral and
squared Frobenius norms of the kernel approximation error matrix. See Appendix D.3 for results on CovType.

−

1

1

−

−

1
∆1

1
∆1

original ∆ and the Frobenius and spectral norms gener-
ally do not align well with generalization performance,
we see that
attains a
does. Speciﬁcally,
Spearman rank correlation coeﬃcient of ρ = 0.958,
while squared Frobenius norm, spectral norm, and the
original ∆ attain values of 0.594, 0.540, and 0.759.6
In Appendix D.3 we show these trends are robust to
diﬀerent kernel approximation methods and datasets.
For example, we show that while other approximation
methods (e.g., orthogonal RFFs (Yu et al., 2016)), like
Nystr¨om, can attain much lower Frobenius and spectral
error than standard RFFs, this does not translate to
improved ∆1 or heldout performance. These results
mirror the generalization bound in Proposition 1, which
grows linearly with
. For simplicity, we ignore
the role of ∆2 here, as ∆1 appears to be suﬃcient for
explaining the main diﬀerences in performance between
these full-precision methods.7 In Sections 4.2 and 5.2,
however, we show that ∆2 has a large inﬂuence on
generalization performance for low-precision features.

1
∆1

−

1

Now that we have seen that ∆1 has signiﬁcant theoreti-
cal and empirical impact on generalization performance,
it is natural to ask how to construct kernel approxi-
mation matrices that attain small ∆1. An important
consequence of the deﬁnition of ∆1 is that for ˜K +λI to
have small ∆1 relative to K + λI, ˜K must be high-rank ;
λm+1(K)
in particular, a necessary condition is ∆1 ≥
λm+1(K)+λ ,
where m is the rank of ˜K and λi(K) is the ith largest
eigenvalue of K.8 This sets a lower bound on the rank
necessary for ˜K to attain small ∆1 which holds regard-
less of the approximation method used, motivating us
to design high-rank kernel approximation methods.

6 One reason ∆1 correlates better than ∆ is because
when ∆2 > ∆1, ∆ = max(∆1, ∆2) hides the value of ∆1.
This shows why decoupling the two roles of ∆ is important.
∆1) aligns well with performance, it is
not perfect—for a ﬁxed ∆1, Nystr¨om generally performs
slightly better than RFFs. In App. D.3.1 we suggest this is
because Nystr¨om has ∆2 = 0 while RFFs has larger ∆2.

7While 1/(1

−

8By deﬁnition, (K + λI)(1

˜K + λI. By Weyl’s
λi( ˜K) + λ.
inequality this implies
If ˜K is rank m, then λm+1( ˜K) = 0, and the result follows.

−
(cid:22)
i (λi(K) + λ)(1

∆1)

∆1)

−

≤

∀

4 LOW-PRECISION RANDOM

FOURIER FEATURES (LP-RFFS)

Taking inspiration from the above-mentioned connec-
tion between the rank of the kernel approximation
matrix and generalization performance, we propose
low-precision random Fourier features (LP-RFFs) to
create a high-rank approximation matrix under a mem-
ory budget. In particular, we quantize each random
Fourier feature to a low-precision ﬁxed-point represen-
tation, thus allowing us to store more features in the
same amount of space. Theoretically, we show that
when the quantization noise is small relative to the
regularization parameter, using low precision has mini-
mal impact on the number of features required for the
approximate kernel matrix to be a (∆1, ∆2)-spectral
approximation of the exact kernel matrix; by Propo-
sition 1, this implies a bound on the generalization
performance of the model trained on the low-precision
features. At the end of this section (Section 4.3), we
discuss a memory-eﬃcient implementation for training
a full-precision model on top of LP-RFFs.

4.1 Method Details

∈

−

(cid:112)

(cid:112)

(cid:112)

2/m,

i x + ai)

2/m cos(wT

1 sub-intervals of equal size r =

2/m] for the RFF vector z(x)

The core idea behind LP-RFFs is to use b bits to store
each RFF, instead of 32 or 64 bits. We implement
this with a simple stochastic rounding scheme. We use
the parametrization zi(x) =
∈
Rm
[
−
(Rahimi and Recht, 2007), and divide this interval
into 2b
1 . We
−
then randomly round each feature zi(x) to either the
top or bottom of the sub-interval [z, z] containing it,
in such a way that the expected value is equal to zi(x);
speciﬁcally, we round zi(x) to z with probability z
z
−
z
z
−
and to z with probability z
z
z . The variance of this
z
stochastic rounding scheme is at most δ2
b /m, where
b := 2/(2b
δ2
1)2 (Prop. 7 in App. C.2). For each low-
precision feature ˜zi(x) we only need to store the integer
j
2/m + jr, which
−
takes b bits. Letting ˜Z
m denote the matrix

1] such that ˜zi(x) =

2√2/m
2b

[0, 2b

Rn

(cid:112)

−
−

−

−

∈

×

∈

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 3: Empirical validation of Theorem 2. In the left and middle plots (shared legend), we see that as the #
of features grows, LP-RFFs approach ∆1 = 0, but plateau at larger ∆2 values (at most δ2
b /λ, marked by dashed
lines) for very low precisions. In the right plot we see that the larger λ is, the lower the precision at which using
low precision does not impact ∆2. For ∆1 and ∆2 vs. # features plots on CovType, see Appendix D.4.

of quantized features, we call ˜K = ˜Z ˜Z T an m-feature
b-bit LP-RFF approximation of a kernel matrix K.

As a way to further reduce the memory footprint during
training, we leverage existing work on using circulant
random matrices (Yu et al., 2015) for the RFF random
projection matrix to only occupy 32m bits.9 All our
LP-RFF experiments use circulant projections.

4.2 Theoretical Results

In this section we show quantization has minimal im-
pact on the number of features required to guarantee
strong generalization performance in certain settings.
We do this in the following theorem by lower bounding
the probability that ˜K + λI is a (∆1, ∆2)-spectral ap-
proximation of K + λI, for the LP-RFF approximation
˜K using m features and b bits per feature.10
˜K be an m-feature b-bit LP-
Theorem 2. Let
RFF approximation of a kernel matrix K, assume
1)2, and deﬁne a :=
b In)(cid:1). Then for any ∆1 ≥
0,

:= 2/(2b

≥

−

δ2
K
λ
b
(cid:107)
(cid:107) ≥
8 tr (cid:0)(K + λIn)−
1(K + δ2
δ2
b /λ,
∆2 ≥
(cid:104)
P
(1

∆1)(K + λI)

(cid:105)
(1 + ∆2)(K + λI)

˜K + λI

−
(cid:32)
exp

(cid:18)

1

a

−

(cid:22)

(cid:19)

(cid:22)
(cid:32)

+exp

m∆2
1
−
4n
λ (1 + 2
3 ∆1)

m(∆2 −

−
4n
λ (1 + 2

3 (∆2 −

δ2
λ )2
b
δ2
λ ))
b

≥
(cid:33)(cid:33)
.

The proof of Theorem 2 is in Appendix C. To provide
more intuition we present the following corollary:
Corollary 2.1. Assuming ∆1 ≤
(1
∆1)(K+λIn)
(cid:22)
−
(cid:16) a
8n/λ
if m
log
∆2
ρ
1
it follows that ˜K + λIn
probability at least 1

3/2, it follows that
˜K+λIn with probability at least 1
ρ
−
(cid:17)
(cid:2) δ2
(cid:3),
λ , 3
. Similarly, assuming ∆2 ∈
2
(1 + ∆2)(K + λIn) with
8n/λ
b /λ)2 log
δ2

(cid:22)
ρ if m

(cid:16) a
ρ

(∆2

≥

(cid:17)

.

b

−

≥

−

9Technically, m additional bits are needed to store a

vector of Rademacher random variables in

10This theorem extends directly to the quantization of
Rn×m with
2/m].

any kernel approximation feature matrix Z
i.i.d. columns and with entries in [

∈
2/m, (cid:112)

(cid:112)

1, 1

m.
}

{−

−

The above corollary suggests that using low precision
has negligible eﬀect on the number of features necessary
to attain a certain value of ∆1, and also has negligible
eﬀect for ∆2 as long as δ2
b /λ

∆2.

(cid:28)

Validation of Theory We now empirically validate
the following two predictions made by the above theory:
(1) Using low precision has no eﬀect on the asymptotic
behavior of ∆1 as the number of features m approaches
inﬁnity, while having a signiﬁcant eﬀect on ∆2 when
δ2
, ∆1 converges
b /λ is large. Speciﬁcally, as m
to 0 for any precision b, while ∆2 converges to a value
upper bounded by δ2
b /λ.11 (2) If δ2
∆2, using b-
bit precision will have negligible eﬀect on the number of
features required to attain this ∆2. Thus, the larger λ
is, the smaller the impact of using low precision should
be on ∆2.

→ ∞

b /λ

(cid:28)

To validate the ﬁrst prediction, in Figure 3 (left, middle)
we plot ∆1 and ∆2 as a function of the number of
features m, for FP-RFFs and LP-RFFs; we use the
same λ as in the Section 2 Census experiments. We
show that for large m, all methods approach ∆1 = 0;
4 the LP-RFFs converge
in contrast, for precisions b
to a ∆2 value much larger than 0, and slightly less than
δ2
b /λ (marked by dashed lines).

≤

To validate the second prediction, in Figure 3 (right)
we plot ∆2 vs. precision for various values of λ, using
m = 2000 features for all precisions; we do this on
a random subsample of 8000 Census training points.
We see that for large enough precision b, the ∆2 is
very similar to the value from using 32-bit precision.
Furthermore, the larger the value of λ, the smaller the
precision b can be without signiﬁcantly aﬀecting ∆2.

11By Lemma 3 in Appendix C, we know that E

K + D for a diagonal matrix D satisfying 0
where D is independent of m. As m
(K + λI)−1/2D(K + λI)−1/2
to

→ ∞
δ2
b /λ.

(cid:107) ≤

(cid:107)

(cid:104)

˜Z ˜Z T (cid:105)
=
δ2
b In,
D
(cid:22)
, ∆2 converges

(cid:22)

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Figure 4: Generalization performance of FP-Nystr¨om, FP-RFFs, circulant FP-RFFs, and LP-RFFs with respect
to memory (sum of components in Table 1) on TIMIT, YearPred and CovType. LP-RFFs attain the best
performance across a wide range of memory budgets. The same trend holds for Census in Appendix D.5.

Table 2: The compression ratios achieved by LP-RFFs
relative to the best performing full-precision baselines.

FP-RFFs Cir. FP-RFFs Nystr¨om

Census
YearPred
Covtype
TIMIT

2.9x
10.3x
4.7x
5.1x

15.6x
7.6x
3.9x
2.4x

63.2x
461.6x
237.2x
50.9x

4.3

Implementation Considerations

In this paper, we focus on training full-precision models
using mini-batch training over low-precision features.
Here we describe how this mixed-precision optimization
can be implemented in a memory-eﬃcient manner.

Naively, to multiply the low-precision features with the
full-precision model, one could ﬁrst cast the features to
full-precision, requiring signiﬁcant intermediate mem-
ory. We can avoid this by casting in the processor
registers. Speciﬁcally, to perform multiplication with
the full-precision model, the features can be streamed
to the processor registers in low precision, and then cast
to full precision in the registers. In this way, only the
features in the registers exist in full precision. A similar
technique can be applied to avoid intermediate memory
in the low-precision feature computation—after a full-
precision feature is computed in the registers, it can be
directly quantized in-place before it is written back to
main memory. We leave a more thorough investigation
of these systems issues for future work.

5 EXPERIMENTS

In this section, we empirically demonstrate the per-
formance of LP-RFFs under a memory budget, and
show that (∆1, ∆2) are predictive of generalization
performance. We show in Section 5.1 that LP-RFFs
can attain the same performance as FP-RFFs and
Nystr¨om, while using 3x-10x and 50x-460x less memory.
In Section 5.2, we show the strong alignment between
(∆1, ∆2) and generalization performance, once again
validating the importance of this measure.

5.1 Empirical Evaluation of LP-RFFs

1, 2, 4, 8, 16

To empirically demonstrate the generalization perfor-
mance of LP-RFFs, we compare their performance
to FP-RFFs, circulant FP-RFFs, and Nystr¨om fea-
tures for various memory budgets. We use the same
datasets and protocol as the large-scale Nystr¨om vs.
RFF comparisons in Section 2.2; the only signiﬁcant ad-
ditions here are that we also evaluate the performance
of circulant FP-RFFs, and LP-RFFs for precisions
b
. Across our experiments, we com-
}
pute the total memory utilization as the sum of all
the components in Table 1. We note that all our low-
precision experiments are done in simulation, which
means we store the quantized values as full-precision
ﬂoating-point numbers. We report average results from
three random seeds, with error bars showing standard
deviations. For more details about our experiments, see
Appendix D.5. We use the above protocol to validate
the following claims on the performance of LP-RFFs.12

∈ {

LP-RFFs can outperform full-precision fea-
tures under memory budgets.
In Figure 4, we
plot the generalization performance for these experi-
ments as a function of the total training memory for
TIMIT, YearPred, and CovType. We observe that LP-
RFFs attain better generalization performance than
the full-precision baselines under various memory bud-
gets. To see results for all precisions, as well as results
on additional benchmark datasets (Census, Adult, Cod-
RNA, CPU, Forest) from the UCI repository (Dheeru
and Karra Taniskidou, 2017), see Appendix D.5.

LP-RFFs can match the performance of full-
precision features with signiﬁcantly less mem-
ory.
In Table 2 we present the compression ratios we
achieve with LP-RFFs relative to the best performing
baseline methods. For each baseline (FP-RFFs, circu-
lant FP-RFFs, Nystr¨om), we ﬁnd the smallest LP-RFF
model, as well as the smallest baseline model, which
4 relative performance of the best-
attain within 10−
performing baseline model; we then compute the ratio

12 Our code: github.com/HazyResearch/lp_rffs.

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

∆1), ∆2)
Figure 5: Generalization perf. vs. ∆2 (left plots, shared legend), and vs. 1/(1
(right plots, shared legend). Left: heldout performance deteriorates as ∆2 gets larger due to lower precision.
Right: max (1/(1
∆1), ∆2) aligns well with performance across LP-RFF precisions (Spearman rank correlation
coeﬃcient ρ = 0.959), while 1/(1

∆1) aligns poorly (ρ = 0.403). See Appendix D.6 for CovType results.

∆1) and max (1/(1

−

−

−

−

of the memory used by these two models (baseline/LP-
RFF) for three random seeds, and report the aver-
age. We can see that LP-RFFs demonstrate signiﬁcant
memory saving over FP-RFFs, circulant FP-RFFs, and
Nystr¨om, attaining compression ratios of 2.9x-10.3x,
2.4x-15.6x, and 50.9x-461.6x, respectively.

5.2 Generalization Performance vs. (∆1, ∆2)

In this section we show that ∆1 and ∆2 are together
quite predictive of generalization performance across all
the kernel approximation methods we have discussed.
We ﬁrst show that performance deteriorates for larger
∆2 values as we vary the precision of the LP-RFFs,
when keeping the number of features constant (thereby
limiting the inﬂuence of ∆1 on performance). We then
combine this insight with our previous observation
(Section 3.2) that performance scales with
in the
full-precision setting by showing that across precisions
(cid:1). For
the performance aligns well with max (cid:0)
these experiments, we use the same protocol as for the
(∆1, ∆2) experiments in Section 3.2, but additionally
consider LP-RFFs for precisions b

, ∆2

1
∆1

1
∆1

−

−

1

1

.

1, 2, 4, 8, 16
}

∈ {

We show in Figure 5 (left plots) that for a ﬁxed number
of random Fourier features, performance deteriorates
as ∆2 grows. As we have shown in Figure 3 (left), ∆1
is primarily governed by the rank of the approxima-
tion matrix, and thus holding the number of features
constant serves as a proxy for holding ∆1 roughly con-
stant. This allows us to isolate the impact of ∆2 on
performance as we vary the precision.

1

1
∆1

, ∆2

To integrate the inﬂuence of ∆1 and ∆2 on general-
ization performance into a single scalar, we consider
max (cid:0)
(cid:1). In Figure 5 (right plots) we show that
when considering both low-precision and full-precision
features, max (cid:0)
(cid:1) aligns well with performance
(ρ = 0.959, incorporating all precisions), while
aligns poorly (ρ = 0.403).

, ∆2

1
∆1

1
∆1

−

−

−

1

1

In Appendix B we argue that performance scales
roughly as ∆2 instead of as ∆2/(1 + ∆2) (as suggested
by Prop. 1) due to looseness in the Prop. 1 bound.

6 RELATED WORK

Low-Memory Kernel Approximation For RFFs,
there has been work on using structured random pro-
jections (Le et al., 2013; Yu et al., 2015, 2016), and
feature selection (Yen et al., 2014; May et al., 2016) to
reduce memory utilization. Our work is orthogonal, as
LP-RFFs can be used with both. For Nystr¨om, there
has been extensive work on improving the choice of
landmark points, and reducing the memory footprint
in other ways (Kumar et al., 2009; Hsieh et al., 2014;
Si et al., 2014; Musco and Musco, 2017). In our work,
we focus on the eﬀect of quantization on generalization
performance per bit, and note that RFFs are much
more amenable to quantization. For our initial experi-
ments quantizing Nystr¨om features, see Appendix D.7.

Low Precision for Machine Learning There has
been much recent interest in using low precision for
accelerating training and inference of machine learning
models, as well as for model compression (Gupta et al.,
2015; De Sa et al., 2015; Hubara et al., 2016; De Sa
et al., 2018, 2017; Han et al., 2016). There have been
many advances in hardware support for low precision
as well (Jouppi et al., 2017; Caulﬁeld et al., 2017).

This work is inspired by the Nystr¨om vs. RFF exper-
iments in the PhD dissertation of May (2018), and
provides a principled understanding of the prior results.
For more related work discussion, see Appendix E.

7 CONCLUSION

We deﬁned a new measure of kernel approximation error
and demonstrated its close connection to the empirical
and theoretical generalization performance of kernel
approximation methods.
Inspired by this measure,
we proposed LP-RFFs and showed they can attain
improved generalization performance under a memory
budget in theory and in experiments. We believe these
contributions provide fundamental insights into the
generalization performance of kernel approximation
methods, and hope to use these insights to scale kernel
methods to larger and more challenging tasks.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Acknowledgements

We thank Michael Collins for his helpful guidance on
the Nystr¨om vs. RFF experiments in Avner May’s PhD
dissertation (May, 2018), which inspired this work. We
also thank Jared Dunnmon, Albert Gu, Beliz Gunel,
Charles Kuang, Megan Leszczynski, Alex Ratner, Nimit
Sohoni, Paroma Varma, and Sen Wu for their helpful
discussions and feedback on this project.

We gratefully acknowledge the support of DARPA un-
der Nos. FA87501720095 (D3M) and FA86501827865
(SDH), NIH under No. N000141712266 (Mobilize),
NSF under Nos. CCF1763315 (Beyond Sparsity) and
CCF1563078 (Volume to Velocity), ONR under No.
N000141712266 (Unifying Weak Supervision), the
Moore Foundation, NXP, Xilinx, LETI-CEA, Intel,
Google, NEC, Toshiba, TSMC, ARM, Hitachi, BASF,
Accenture, Ericsson, Qualcomm, Analog Devices, the
Okawa Foundation, and American Family Insurance,
and members of the Stanford DAWN project: Intel,
Microsoft, Teradata, Facebook, Google, Ant Finan-
cial, NEC, SAP, and VMWare. The U.S. Government
is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright
notation thereon. Any opinions, ﬁndings, and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reﬂect
the views, policies, or endorsements, either expressed
or implied, of DARPA, NIH, ONR, or the U.S. Gov-
ernment.

References

Ahmed El Alaoui and Michael W. Mahoney. Fast
randomized kernel ridge regression with statistical
guarantees. In NIPS, pages 775–783, 2015.

Haim Avron, Michael Kapralov, Cameron Musco,
Christopher Musco, Ameya Velingker, and Amir
Zandieh. Random Fourier features for kernel ridge
regression: Approximation bounds and statistical
guarantees. In ICML, volume 70 of Proceedings of
Machine Learning Research, pages 253–262. PMLR,
2017.

Adrian M. Caulﬁeld, Eric S. Chung, Andrew Put-
nam, Hari Angepat, Daniel Firestone, Jeremy Fowers,
Michael Haselman, Stephen Heil, Matt Humphrey,
Puneet Kaur, Joo-Young Kim, Daniel Lo, Todd Mas-
sengill, Kalin Ovtcharov, Michael Papamichael, Lisa
Woods, Sitaram Lanka, Derek Chiou, and Doug
Burger. Conﬁgurable clouds. IEEE Micro, 37(3):
52–61, 2017.

Corinna Cortes, Mehryar Mohri, and Ameet Talwalkar.
On the impact of kernel approximation on learning
accuracy. In AISTATS, volume 9 of JMLR Proceed-
ings, pages 113–120. JMLR.org, 2010.

Tri Dao, Christopher De Sa, and Christopher R´e. Gaus-
sian quadrature for kernel features. In NIPS, pages
6109–6119, 2017.

Christopher De Sa, Ce Zhang, Kunle Olukotun, and
Christopher R´e. Taming the wild: A uniﬁed analysis
of Hogwild-style algorithms. In NIPS, pages 2674–
2682, 2015.

Christopher De Sa, Matthew Feldman, Christopher R´e,
and Kunle Olukotun. Understanding and optimiz-
ing asynchronous low-precision stochastic gradient
descent. In ISCA, pages 561–574. ACM, 2017.

Christopher De Sa, Megan Leszczynski, Jian Zhang,
Alana Marzoev, Christopher R. Aberger, Kunle
Olukotun, and Christopher R´e. High-accuracy low-
precision training. arXiv preprint arXiv:1803.03383,
2018.

Dua Dheeru and Eﬁ Karra Taniskidou. UCI machine

learning repository, 2017.

M. J. F. Gales. Maximum likelihood linear transforma-
tions for HMM-based speech recognition. Computer
Speech & Language, 12(2):75–98, 1998.

J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus,
D. S. Pallett, and N. L. Dahlgren. DARPA TIMIT
acoustic phonetic continuous speech corpus CDROM,
1993. URL http://www.ldc.upenn.edu/Catalog/
LDC93S1.html.

Alex Gittens and Michael W. Mahoney. Revisiting the
Nystr¨om method for improved large-scale machine
learning. Journal of Machine Learning Research, 17:
117:1–117:65, 2016.

Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan,
and Pritish Narayanan. Deep learning with limited
numerical precision. In ICML, volume 37 of JMLR
Workshop and Conference Proceedings, pages 1737–
1746, 2015.

Song Han, Huizi Mao, and William J. Dally. Deep
compression: Compressing deep neural network with
pruning, trained quantization and Huﬀman coding.
In Proceedings of the International Conference on
Learning Representations (ICLR), 2016.

Cho-Jui Hsieh, Si Si, and Inderjit S. Dhillon. Fast
prediction for large-scale kernel machines. In NIPS,
pages 3689–3697, 2014.

Jie Chen, Lingfei Wu, Kartik Audhkhasi, Brian Kings-
bury, and Bhuvana Ramabhadrari. Eﬃcient one-vs-
one kernel ridge regression for speech recognition. In
ICASSP, pages 2454–2458. IEEE, 2016.

Po-Sen Huang, Haim Avron, Tara N. Sainath, Vikas
Sindhwani, and Bhuvana Ramabhadran. Kernel
methods match deep neural networks on TIMIT.
In ICASSP, pages 205–209. IEEE, 2014.

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Itay Hubara, Matthieu Courbariaux, Daniel Soudry,
Ran El-Yaniv, and Yoshua Bengio. Binarized neural
networks. In NIPS, pages 4107–4115, 2016.

Alessandro Rudi and Lorenzo Rosasco. Generalization
properties of learning with random features. In NIPS,
pages 3218–3228, 2017.

Norman P. Jouppi, Cliﬀ Young, Nishant Patil, David A.
Patterson, et al. In-datacenter performance analysis
of a tensor processing unit. In ISCA, pages 1–12.
ACM, 2017.

Sanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar.
Ensemble Nystr¨om method. In NIPS, pages 1060–
1068, 2009.

Sanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar.
Sampling methods for the Nystr¨om method. Journal
of Machine Learning Research, 13:981–1006, 2012.

Quoc V. Le, Tam´as Sarl´os, and Alexander J. Smola.
Fastfood - computing Hilbert space expansions in log-
linear time. In ICML, volume 28 of JMLR Workshop
and Conference Proceedings, pages 244–252, 2013.

Zhu Li, Jean-Francois Ton, Dino Oglic, and Dino Sejdi-
novic. A uniﬁed analysis of random Fourier features.
arXiv preprint arXiv:1806.09178, 2018.

Avner May. Kernel Approximation Methods for Speech
Recognition. PhD thesis, Columbia University, 2018.

Avner May, Michael Collins, Daniel J. Hsu, and Brian
Kingsbury. Compact kernel models for acoustic mod-
eling via random feature selection. In ICASSP, pages
2424–2428. IEEE, 2016.

Avner May, Alireza Bagheri Garakani, Zhiyun Lu,
Dong Guo, Kuan Liu, Aur´elien Bellet, Linxi Fan,
Michael Collins, Daniel J. Hsu, Brian Kingsbury,
Michael Picheny, and Fei Sha. Kernel approxima-
tion methods for speech recognition. arXiv preprint
arXiv:1701.03577, 2017.

N. Morgan and H. Bourlard. Generalization and pa-
rameter estimation in feedforward nets: Some exper-
iments. In NIPS, 1990.

Tara N. Sainath, Brian Kingsbury, Vikas Sindhwani,
Ebru Arisoy, and Bhuvana Ramabhadran. Low-rank
matrix factorization for deep neural network training
with high-dimensional output targets. In ICASSP,
pages 6655–6659. IEEE, 2013a.

Tara N. Sainath, Brian Kingsbury, Hagen Soltau, and
Bhuvana Ramabhadran. Optimization techniques to
improve training speed of deep neural networks for
large speech tasks. IEEE Trans. Audio, Speech &
Language Processing, 21(11):2267–2276, 2013b.

Si Si, Cho-Jui Hsieh, and Inderjit S. Dhillon. Mem-
ory eﬃcient kernel approximation. In ICML, vol-
ume 32 of JMLR Workshop and Conference Proceed-
ings, pages 701–709, 2014.

Vikas Sindhwani, Tara N. Sainath, and Sanjiv Kumar.
Structured transforms for small-footprint deep learn-
ing. In NIPS, pages 3088–3096, 2015.

Dougal J. Sutherland and Jeﬀ G. Schneider. On the
In UAI, pages

error of random Fourier features.
862–871. AUAI Press, 2015.

Joel A. Tropp. An introduction to matrix concentration
inequalities. Foundations and Trends in Machine
Learning, 8(1-2):1–230, 2015.

Stephen Tu, Rebecca Roelofs, Shivaram Venkatara-
man, and Benjamin Recht. Large scale kernel learn-
ing using block coordinate descent. arXiv preprint
arXiv:1602.05310, 2016.

Yuting Wei, Fanny Yang, and Martin J. Wainwright.
Early stopping for kernel boosting algorithms: A
general analysis with localized complexities. In NIPS,
pages 6067–6077, 2017.

Cameron Musco and Christopher Musco. Recursive
sampling for the Nystr¨om method. In NIPS, pages
3836–3848, 2017.

Christopher K. I. Williams and Matthias W. Seeger.
Using the Nystr¨om method to speed up kernel ma-
chines. In NIPS, pages 682–688. MIT Press, 2000.

Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan,
and Alexander Shapiro. Robust stochastic approxi-
mation approach to stochastic programming. SIAM
Journal on Optimization, 19(4):1574–1609, 2009.

Tiberiu Popoviciu. Sur les ´equations alg´ebriques ayant
toutes leurs racines r´eelles. Mathematica, 9:129–145,
1935.

Ali Rahimi and Benjamin Recht. Random features for
large-scale kernel machines. In NIPS, pages 1177–
1184, 2007.

Jiyan Yang, Vikas Sindhwani, Haim Avron, and
Michael W. Mahoney. Quasi-Monte Carlo feature
maps for shift-invariant kernels. In Proceedings of the
31th International Conference on Machine Learning,
ICML 2014, Beijing, China, 21-26 June 2014, pages
485–493, 2014.

Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong
Jin, and Zhi-Hua Zhou. Nystr¨om method vs ran-
dom Fourier features: A theoretical and empirical
comparison. In NIPS, pages 485–493, 2012.

Ali Rahimi and Benjamin Recht. Weighted sums of
random kitchen sinks: Replacing minimization with
randomization in learning. In NIPS, pages 1313–1320,
2008.

Zichao Yang, Marcin Moczulski, Misha Denil, Nando
de Freitas, Alexander J. Smola, Le Song, and Ziyu
Wang. Deep fried convnets. In ICCV, pages 1476–
1483. IEEE Computer Society, 2015.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Ian En-Hsu Yen, Ting-Wei Lin, Shou-De Lin, Pradeep
Ravikumar, and Inderjit S. Dhillon. Sparse random
feature algorithm as coordinate descent in Hilbert
space. In NIPS, pages 2456–2464, 2014.

Felix X. Yu, Sanjiv Kumar, Henry A. Rowley, and Shih-
Fu Chang. Compact nonlinear maps and circulant
extensions. arXiv preprint arXiv:1503.03893, 2015.

Felix

X.

Yu,

Ananda

Theertha

Suresh,
Krzysztof Marcin Choromanski, Daniel N.
Holtmann-Rice, and Sanjiv Kumar. Orthogo-
In NIPS, pages 1975–1983,
nal random features.
2016.

Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh,
Ji Liu, and Ce Zhang. Zipml: Training linear models
with end-to-end low precision, and a little bit of deep
learning. In ICML, volume 70 of Proceedings of Ma-
chine Learning Research, pages 4035–4043. PMLR,
2017.

Tong Zhang, Bin Yu, et al. Boosting with early stop-
ping: Convergence and consistency. The Annals of
Statistics, 33(4):1538–1579, 2005.

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

A NOTATION AND BACKGROUND

In this appendix, we ﬁrst discuss the notation we use throughout the paper, and then provide an overview of
random Fourier features (RFFs) (Rahimi and Recht, 2007) and the Nystr¨om method (Williams and Seeger, 2000).
After this, we brieﬂy extend our discussion in Section 3 on ﬁxed design kernel ridge regression.

A.1 Notation

×

∈

×

Rn

}
→

Rd, and yi

n
i=1 to denote a training set, for xi
}

= R for regression, and
We use
(xi, yi)
∈
{
n denote the kernel matrix corresponding to a kernel function
for classiﬁcation. We let K
1, . . . , c
=
Y
{
R, where Kij = k(xi, xj), and let ˜K denote an approximation to K. We let z : Rd
Rm denote
Rd
k : Rd
a feature map for approximating a kernel function, such that ˜Kij = z(xi)T z(xj). We use s to denote the size
of the mini-batches during training, and b to denote the precision used for the random features. We let
(cid:107)2
F denote the spectral and Frobenius norms of a matrix K, respectively; if the subscript is not speciﬁed,
and
(cid:107)
will denote the (cid:96)2 norm of x, unless speciﬁed otherwise. In
K
(cid:107)
A is positive
will denote the n
semideﬁnite. We will use λi(A) to denote the ith largest eigenvalue of A, and λmax(A), λmin(A) to denote the
largest and smallest eigenvalues of A, respectively.

K
(cid:107)
denotes the spectral norm. For vectors x,

n identity matrix. For symmetric matrices A and B, we will say A

, where

B if B

K
(cid:107)

∈ Y

→

−

(cid:22)

×

Y

x

(cid:107)

(cid:107)

(cid:107)

A.2 Kernel Approximation Background

k(x, y).
The core idea behind kernel approximation is to construct a feature map z :
X →
n
Given such a map, one can then learn a linear model on top of
i=1, and this model will approximate
(z(xi), yi)
}
{
the model trained using the exact kernel function. We now review RFFs and the Nystr¨om method, two of the
most widely used and studied methods for kernel approximation.

≈

R such that z(x)T z(y)

Random Fourier features (RFFs) For shift-invariant kernels (k(x, x(cid:48)) = ˆk(x
x(cid:48))), the random Fourier
Rm such that
feature method (Rahimi and Recht, 2007) constructs a random feature representation z(x)
E (cid:2)z(x)T z(x(cid:48))(cid:3) = k(x, x(cid:48)). This construction is based on Bochner’s Theorem, which states that any positive
deﬁnite kernel is equal to the Fourier transform of a nonnegative measure. This allows for performing Monte
Carlo approximations of this Fourier transform in order to approximate the function. The resulting features
have the following functional form: zi(x) =
i x + ai), where wi is drawn from the inverse Fourier
transform of the kernel function ˆk, and ai is drawn uniformly from [0, 2π] (see Appendix A in May et al. (2017)
for a derivation).

2/m cos(wT

(cid:112)

−

∈

One way of reducing the memory required for storing W = [w1, . . . , wm], is to replace W by a structured matrix;
in this work, we let W be a concatenation of many square circulant random matrices (Yu et al., 2015).

(cid:104)

(cid:105) ≈

k(x, x(cid:48)).

z(x), z(x(cid:48))

It does this by picking a set of landmark points

Rm
Nystr¨om method The Nystr¨om method constructs a ﬁnite-dimensional feature representation z(x)
∈
ˆx1, . . . , ˆxm
such that
, and
{
taking the SVD ˆK = U ΛU T of the m by m kernel matrix ˆK corresponding to these landmark points ( ˆKi,j =
1/2U T kx, where kx =
k(ˆxi, ˆxj)). The Nystr¨om representation for a point x
Rm
n, the Nystr¨om method can be thought of
[k(x, ˆx1), . . . , k(x, ˆxm)]T . Letting Km,n = [kx1 , . . . , kxn ]
1/2U T Km,n of the full n by n kernel matrix K
as an eﬃcient low-rank approximation K
m,nU Λ−
n
corresponding to the full dataset
i=1. One can also consider the lower-dimensional Nystr¨om representation
}
Rr, where only the top r eigenvalues and eigenvectors of ˆK are used, instead of all m. In
U T
zr(x) = Λ−
r kx
r
this paper, we will always use m = r, and thus will not specify the subscript r.

is deﬁned as z(x) = Λ−

∈ X
∈
1/2Λ−

} ∈ X

K T

1/2

xi

≈

∈

{

×

A.3 Fixed Design Kernel Ridge Regression

We consider the problem of ﬁxed design kernel ridge regression, which has a closed-form equation for the
generalization error, making it a particularly tractable problem to analyze. In ﬁxed design regression, one is
R, and the (cid:15)i are zero-mean uncorrelated
given a set of labeled points
random variables with shared variance σ2 > 0; here, the ¯yi represent the “true labels.” Given such a sample,
¯yi)2(cid:3) is small. Note that for a ﬁxed
the goal is to learn a regressor f (x) such that
learning method, the learned regressor f can be seen as a random function based on the random label noise (cid:15)i.

Rd, yi = ¯yi + (cid:15)i

n
i=1, where xi

i=1(f (xi)

(f ) = E(cid:15)

(xi, yi)

(cid:2) 1
n

(cid:80)n

R

−

∈

∈

{

}

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

One approach to solving this problem is kernel ridge regression. In kernel ridge regression, one chooses a kernel
Rd
function k : Rd
i αik(x, xi).
n denote the kernel matrix such that Kij = k(xi, xj), and y = (y1, . . . , yn), the closed-form
Letting K
1y. It is then easy
solution for this problem (the one minimizing the regularized empirical loss), is α = (K + λI)−
to show (Alaoui and Mahoney, 2015) that the expected error of this regressor fK under the ﬁxed design setting is

R, and a regularizing constant λ, and learns a function of the form f (x) = (cid:80)

×
Rn
×

→

∈

(fK) =

R

1
n

λ2 ¯yT (K + λI)−

2 ¯y +

σ2T r

K 2(K + λI)−

2(cid:17)

,

(cid:16)

1
n

where ¯y = (¯y1, . . . , ¯yn) is the vector of “true labels.”

B GENERALIZATION BOUNDS FOR FIXED DESIGN REGRESSION

B.1 Generalization Bound in Terms of (∆1, ∆2)

Proposition 1. (Extended from (Avron et al., 2017)) Suppose ˜K + λI is (∆1, ∆2)-spectral approximation of
0. Let m denote the rank of ˜K, and let fK and f ˜K be the kernel ridge regression
K + λI, for ∆1 ∈
. Then
estimators learned using these matrices, with regularizing constant λ

0 and label noise variance σ2 <

[0, 1) and ∆2 ≥

(f ˜K)

R

≤

1

1
∆1

(cid:98)
R

(fK) +

∆2
1 + ∆2

m
n

σ2,

≥

∞

(2)

where

(expected risk) and (cid:98)
R

R

(upper bound on

) are deﬁned in Section 3.1.

−

R

∆) and (1 + ∆) with (1

Proof. This proof closely follows the proof of Lemma 2 in Avron et al. (2017), with the primary diﬀerence being
that we replace (1
We begin by replacing K with ˜K in the deﬁnition for (cid:98)
R
1
n

∆1) and (1 + ∆2), respectively.

λ¯yT ( ˜K + λI)−

˜K( ˜K + λI)−

(f ˜K).

(f ˜K)

(fK):

σ2 tr

1 ¯y +

= (cid:98)
R

1
n

1(cid:17)

R

−

≤

−

(cid:16)

( ˜K + λI)−

We now continue this chain of inequalities, using the fact that A
˜K + λI
bounds the ﬁrst term in the above sum.
We now consider the second term. Let m = rank( ˜K), and let sλ( ˜K) = tr

¯yT ( ˜K + λI)−

1(K + λI)−

∆1)−

1 ¯y

⇒

⇒

(1

(cid:22)

≤

(cid:22)

−

1

1

(1

(cid:16)

−

1
B implies B−

˜K( ˜K + λI)−

1(cid:17)

.

1. Thus, (1
∆1)(K +λI)
A−
(cid:22)
−
1 ¯yT (K + λI)−
1 ¯y. This upper

(cid:22)
∆1)−

(cid:16)

˜K( ˜K + λI)−

1(cid:17)

sλ( ˜K) = tr
m
(cid:88)

=

λi( ˜K)
λi( ˜K) + λ

i=1

= m

−

m

−

≤

m
(cid:88)

i=1
m
(cid:88)

i=1

λ
λi( ˜K) + λ

λ
(1 + ∆2)(λi(K) + λ)
m
(cid:88)

= m

1
(1 + (1 + ∆2)−

1)

−

i=1

λ
λi(K) + λ

= m

λ
λi(K) + λ

+

∆2
1 + ∆2

m
(cid:88)

i=1

λ
λi(K) + λ

n

−

≤

λ
λi(K) + λ

+

∆2
1 + ∆2

m

= sλ(K) +

∆2
1 + ∆2

m

−

−

m
(cid:88)

i=1
n
(cid:88)

i=1

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

≤

1

1
∆1

−

sλ(K) +

∆2
1 + ∆2

m

Combining the above results, we get that:

(f ˜K)

R

1 ¯y +

1
n

σ2sλ( ˜K)
(cid:19)

≤

≤

=

=

1
n
1
n

1

1

λ¯yT ( ˜K + λI)−
(cid:18) 1
λ
1

∆1
−
(cid:18) 1
n

1
∆1
−
1
∆1

−

λ¯yT (K + λI)−

1 ¯y +

(fK) +

(cid:98)
R

∆2
1 + ∆2

m
n

σ2

¯yT (K + λI)−

1 ¯y

+

σ2

sλ(K) +

1
n

(cid:18) 1
1

∆1

−
(cid:19)
σ2sλ(K)

+

1
n

∆2
1 + ∆2

m
n

σ2

(cid:19)

∆2
1 + ∆2

m

Remark Above, ∆1 ∈
0. Note that as ∆1 approaches 1, the above upper bound diverges to
n σ2. This suggests
inﬁnity. Whereas as ∆2 approaches
that choosing ˜K and λ such that ∆1 does not get too close to 1 is very important. A necessary condition for
(1

˜K + λI is for ˜K to be high rank, as discussed in Section 3.

, the second term in the upper bound approaches m

[0, 1] and ∆2 ≥
∞

∆1)(K + λI)

−

(cid:22)

B.2 Heuristic Eﬀort to Better Understand Inﬂuence of (∆1, ∆2) on Generalization Error

Here we present a heuristic argument to explain how ∆1 and ∆2 in the spectral approximation aﬀects the
generalization error. We are particularly interested in demonstrating that ∆2 can have an important inﬂuence on
the bias squared term ( λ2
n ¯yT ( ˜K +
1
λI)−
∆1 (cid:98)
R

2 ¯y) of the generalization error, even though the upper bound λ2

(fK) on the bias squared term (from Proposition 1) is only in terms of ∆1.

n ¯yT ( ˜K + λI)−

2 ¯y

≤

1

−

Suppose that the approximate kernel matrix ˜K is a (∆1, ∆2)-spectral approximation of the true kernel matrix K,
that is:

We focus on the bias squared term of ˜K ( λ2
Following Theorem 15 of Musco and Musco (2017), we ﬁrst bound the bias (not squared):

(1

∆1)(K + λIn)

−

˜K + λIn
n ¯yT ( ˜K + λIn)−

(cid:22)

(1 + ∆2)(K + λIn).

(cid:22)
2 ¯y), and compare it to the bias squared term of K.

( ˜K + λIn)−

1 ¯y

(cid:107)

1 ¯y
(cid:107)
1 ¯y
(cid:107)
1 ¯y
(cid:107)
1 ¯y
(cid:107)
1 ¯y
(cid:107)

(cid:107) ≤ (cid:107)
=

=

≤ (cid:107)
=

(K + λIn)−

(K + λIn)−
(cid:107)
(K + λIn)−
(cid:107)
(K + λIn)−

(K + λIn)−
(cid:107)
( ˜K + λIn)−
(cid:107)

1(K

−

˜K

K

−

(cid:22)

∆2(K + λIn)

+

+

+

+
(cid:16)

(( ˜K + λIn)−
1
(cid:107)
( ˜K + λIn)−
(cid:107)
( ˜K + λIn)−
(cid:107)
( ˜K + λIn)−
(cid:107)
1 +

( ˜K + λIn)−
(cid:107)
˜K)
(cid:107)
∆2

. As ˜K + λIn

( ˜K + λIn)

(cid:22)

(cid:22)

1

∆1

−

(K + λIn)−

−

1)¯y
(cid:107)
( ˜K + λIn))(K + λIn)−

1 ¯y
(cid:107)

1((K + λIn)
1(K
1(K

−

−
˜K)(K + λIn)−
˜K)

1 ¯y
(cid:107)
1 ¯y
(K + λIn)−
(cid:107)

−
1(K

(cid:107)(cid:107)
˜K)
(cid:107)

(cid:17)

.

−

(3)

1 + ∆2
∆1
1

−

(cid:22)

( ˜K + λIn).

Now it reduces to bounding

(1 + ∆2)(K + λIn), we have

Similarly, since K + λIn

1
∆1

( ˜K + λIn),

(cid:22)

1

−

Hence

K

˜K

−

(cid:22)

1

∆1

( ˜K + λIn)

∆1

−

1 + ∆2
∆1
1

−

(cid:22)

( ˜K + λIn).

1 + ∆2
∆1
1

−

−

( ˜K + λIn)

K

(cid:22)

−

˜K

(cid:22)

1 + ∆2
∆1
1

−

( ˜K + λIn).

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

˜K)2 from the bound on
˜K. However, under a restricted setting where K and ˜K have the same eigenvectors, we can square the

t2 is not operator monotone, it is not easy to obtain a bound on (K

−

Because t
K
corresponding eigenvalues to obtain

(cid:55)→

−

Thus

Thus

And hence

( ˜K + λIn)−
(cid:107)

1(K

−

˜K)

(cid:107) ≤

1+∆2
∆1
1

−

. Plugging this into the bound (Eq. 3) yields

˜K)2

(K

−

(cid:22)

(1 + ∆2)2
(1

∆1)2 ( ˜K + λIn)2.

−

( ˜K + λIn)−

1(K

˜K)2( ˜K + λIn)−

1

−

(1 + ∆2)2
∆1)2 .
(1

−

(cid:22)

( ˜K + λIn)−
(cid:107)

1 ¯y

(cid:107) ≤

(cid:18)

1 +

(cid:19)

1 + ∆2
∆1
1

−

(K + λIn)−
(cid:107)

1 ¯y
.
(cid:107)

λ2
n

¯yT ( ˜K + λIn)−

2 ¯y =

( ˜K + λIn)−

λ2
n (cid:107)
(cid:18)

1 +

(cid:18)

1 +

1 ¯y
2
(cid:107)
(cid:19)2 λ2
n (cid:107)
(cid:19)2 λ2
n

1 + ∆2
∆1
1

−
1 + ∆2
∆1
1

−

≤

=

(K + λIn)−

2

1 ¯y
(cid:107)

¯yT (K + λIn)−

2 ¯y.

In other words, in this restricted setting the bias squared of ˜K is at most a factor (1 + 1+∆2
)2 larger than the bias
∆1
squared of K. Though this heuristic analysis only holds when K and ˜K have the same eigenvectors, it reveals
the dependency of the generalization performance on ∆1 and ∆2; in particular, it reveals that ∆2 could have an
important inﬂuence on the bias squared term of the generalization error.

−

1

B.3 The Empirical Inﬂuence of ∆2 on the Bias Squared Term

We now empirically validate that ∆2 can have a large impact on the bias squared term, as suggested by the
theoretical discussion in the previous section. The inﬂuence of ∆2 on the bias squared term helps explain our
empirical observations on the inﬂuence of ∆2 on generalization performance from Section 5.2. Though the
generalization bound in Proposition 1 suggests that performance should scale roughly linearly in ∆2/(1 + ∆2),
we empirically found that generalization performance does not asymptote as ∆2 grows (as ∆2/(1 + ∆2) would
suggest it would). In this section, we empirically validate our hypothesis that this is due to looseness in the
generalization bound. Speciﬁcally, the expected mean squared error for ﬁxed design kernel ridge regression is

(f ˜K) =

R

λ2
n

¯yT ( ˜K + λI)−

2 ¯y +

tr

˜K 2( ˜K + λI)−

2(cid:17)

,

(cid:16)

σ2
n

where ˜K is an approximate kernel matrix. We show in experiments that the bias squared term ( λ2
can be strongly inﬂuenced by ∆2, even though the upper bound on it ( λ2
Proposition 1 is only in terms of ∆1.

n ¯yT ( ˜K + λI)−

2 ¯y

≤

n ¯yT ( ˜K + λI)−
1
∆1 (cid:98)
R

2 ¯y)
(fK)) in

1

−

In our experiments, we compute the value of the bias squared term and ∆2 on the Census dataset. To gain
statistically meaningful insights, we collect and average the value of λ2
2 ¯y and ∆2 using 3 independent
runs with diﬀerent random seeds. In Figure 6, we plot the value of λ2
2 ¯y as a function of ∆2 for
3 diﬀerent numbers of features; by controlling the number of features, we can demonstrate the inﬂuence of ∆2
while ∆1 is held roughly ﬁxed. In each curve in Figure 6, the data points are collected from FP-RFFs, circulant
FP-RFFs, as well as LP-RFFs using
bit precision. We can see that for each number of features, the
value of λ2
2 ¯y grows with ∆2. These results demonstrate that the upper bound on the bias term in
Proposition 1 is quite loose, and is not capturing the inﬂuence of ∆2 properly.

n ¯yT ( ˜K + λI)−

n ¯yT ( ˜K + λI)−

n ¯yT ( ˜K + λI)−

1, 2, 4, 8

{

}

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 6: In the ﬁxed design setting, the bias squared term of the generalization performance grows with ∆2.

C THEORETICAL GUARANTEES FOR LP-RFFS

In this section, we ﬁrst lower bound the probability that using LP-RFFs results in a kernel approximation matrix
that is a (∆1, ∆2)-spectral approximation of the exact kernel matrix (Section C.1). We then present bounds on
the Frobenius kernel approximation error for LP-RFFs (Section C.2).

C.1

(∆1, ∆2)-Spectral Approximation Bounds for LP-RFFs

Rn

×

∈

n denote the kernel matrix, and Z

As usual, let K
is the number of data points and m is the number of features. We can write Z = 1
√m
(cid:112)
the (scaled) columns of Z. Each entry of Z has the form
where d is the dimension of the original dataset. Then E[zizT

m be the random Fourier feature matrix, where n
(cid:3) where zi are
Rm,
Rd and a

2/m cos(wT x + a) for some w, x
i ] = K, so E[ZZ T ] = K.

(cid:2)z1, . . . , zm

∈

∈

∈

×

Rn

≥

1. Then the quantized feature matrix is Z + C for some random C

Now suppose we quantize Z to b bits using the quantization method described in Section 4.1, for some ﬁxed
m whose entries are independent
b
(cid:3) where
conditioned on Z (but not identically distributed) with E[C
ci are the (scaled) columns of C. Moreover, the ci are independent conditioned on Z. Deﬁning δ2
1)2 , the
entries Cij have variance E[C 2
δ2
b /m by Proposition 7 in Appendix C.2. In terms of the vectors ci, we
can also see that E[c2

Rn
×
Z] = 0. We can write C = 1
√m

Zij]
ij |
b , where ci,j denotes the jth element of ci.
δ2
≤

(cid:2)c1, . . . , cm
b :=

i,j |

Zij]

(2b

≤

∈

−

2

|

We ﬁrst analyze the expectation of (Z + C)(Z + C)T (over both the randomness of Z and of C).
Lemma 3. E[(Z +C)(Z +C)T ] = K +D, where D := E[c1cT
D does not depend on the number of random features m.

1 ] = sbIn is a multiple of the identity, for 0

sb

≤

≤

δ2
b .

Proof.

E[(Z + C)(Z + C)T ] = E

(zi + ci)(zi + ci)T

= E[(z1 + c1)(z1 + c1)T ]

(cid:20) 1
m

m
(cid:88)

i=1

(cid:21)

Since Ec1[c1 |

z1] = 0, it follows that

E[(z1 + c1)(z1 + c1)T ] = Ez1

(cid:104)

Ec1[(z1 + c1)(z1 + c1)T
(cid:104)

Ec1[c1cT

1 |

(cid:105)

z1]
(cid:105)

|
z1]

1 ] + Ez1

= Ez1[z1zT
= K + E[c1cT
1 ]

It is clear that D := E[c1cT
variable. It is also easy to see that the jth entry on the diagonal of D is equal to E[c2
argue that each element z1,j = √2 cos(wT

1 ] is a diagonal matrix, because each element of c1 is a zero-mean independent random
δ2
b . Lastly, we
1 xj + a1) has the same distribution, because it is distributed the same

1,j |

z1,j]

≤

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

as √2 cos(a1) for a1 uniform in [0, 2π]. Thus, E[c2
completes the proof.

1,j |

z1,j] is independent of j. Letting sb := E[c2

1,1 |

z1,1]

δ2
b

≤

1 ], we can use matrix concentration to show that the quantized kernel matrix ˜K = (Z +C)(Z +C)T
With D := E[c1cT
is close to its expectation K + D. We ﬁrst strengthen the matrix Bernstein inequality with intrinsic dimension
(Theorem 7.7.1 in Tropp (2015)) by removing the requirement on the deviation.13
Theorem 4 (Matrix Bernstein: Hermitian Case with Intrinsic Dimension). Consider a ﬁnite sequence
random Hermitian matrices of the same size, and assume that

Xk
{

of

}

Introduce the random matrix

E[Xk] = 0 and λmax(Xk)

L for each index k.

≤

(cid:88)

Y =

Xk.

k

Let V be a semideﬁnite upper bound for the matrix-valued variance VAR [Y ]:

Deﬁne the intrinsic dimension bound and variance bound

VAR [Y ] = E[Y 2] =

V

(cid:23)

(cid:88)
k

E[X 2
k ].

Then, for t

0,

≥

intdim(V ) =

and

v =

V
(cid:107)

.
(cid:107)

tr(V )
V
(cid:107)

(cid:107)

P (λmax(Y )

t)

≥

≤

4 intdim(V )

exp

·

(cid:18)

t2/2
v + Lt/3

−

(cid:19)

.

(4)

√v + L/3 is exactly Theorem 7.7.1 in Tropp (2015). We just need to show that the bound

2(v + Lt/3). Indeed, t2

2Lt/3

2v has roots

−

−

≤

Proof. The case of t
is vacuous when 0

≥
t < √v + L/3.

Suppose that 0
(cid:113) L2
L
3 ±

≤

9 + 2v. The condition t2

≤

≤
t < √v + L/3. We show that then t2

2(v + Lt/3) is then equivalent to

L
3 −

(cid:114)

L2
9

+ 2v

t

≤

≤

L
3

+

(cid:114)

L2
9

+ 2v.

The lower bound is negative since v
bound. Thus 0

t < √v + L/3 implies that t2

≥

0, and t < √v + L/3 implies t < L/3 +

L2/9 + 2v, satisfying the upper

(cid:112)

2(v + Lt/3). The bound in equation (4) becomes

≤

≥

(cid:18)

≤
t2/2
v + Lt/3

(cid:19)

−

≥

≤

4 intdim(V ) exp

4 intdim(V ) exp(

1)

4/e > 1,

−

≥

since intdim(V )

1. Thus (4) holds vacuously for 0

t < √v + L/3.

We now present Lemma 5, in which we lower bound the probability that ˜K is “close” to its expectation K + D,
in the speciﬁc sense we describe below.
Lemma 5. Let K be an exact kernel matrix, and ˜K = (Z + C)(Z + C)T be an m-features b-bit LP-RFF
2 and M :=
approximation of K with expectation K + D. For any deterministic matrix B, let L := 2n
(cid:107)
B(K + δ2

B
(cid:107)

0,

b In)BT , then for any t1, t2 ≥
(cid:20)
(cid:16)
P
B

t1In

(Z + C)(Z + C)T

(K + D)

B

t2In

(cid:22)
−
4 tr(M )
M
(cid:107)

(cid:107)

1

≥

−

(cid:20)

(cid:18)

exp

2L(
(cid:107)

−
M
(cid:107)

−

(cid:19)

mt2
1
+ 2t1/3)

+ exp

−
M
2L(
(cid:107)
(cid:107)

mt2
2
+ 2t2/3)

(cid:19)(cid:21)

.

(cid:21)

(cid:17)

(cid:18)

(cid:22)

13Theorem 7.7.1 in Tropp (2015) requires that t

√v + L/3, where t, v, and L are as deﬁned in Theorem 4.

≥

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

=

Si

Proof. Let
=
B (cid:0)(Z + C)(Z + C)T
(K + D)(cid:1) B. We see that E[Si] = 0. We will bound λmax(S) and λmin(S) by
applying the matrix Bernstein inequality for symmetric matrices with intrinsic dimension (Theorem 4). Thus we
need to bound

i=1 Si

(cid:80)m

and

and

=

−

−

S

Si

(cid:0)B(zi + ci)(zi + ci)T BT

B(K + D)BT (cid:1)

(cid:80)m

1
m

(cid:107)
Let ui = B(zi + ci)

(cid:107)

(cid:107)

E[S2
i ]
.
i=1
(cid:107)
Rn, then Si = 1
m (uiuT
2 =
(cid:107)

uiuT
(cid:107)

i (cid:107)

i −

∈

B(zi + ci)
(cid:107)
(cid:107)
where we have used the fact that zi + ci is a vector of length n whose entries are in [
bound on

zi + ci
(cid:107)

≤ (cid:107)

ui

=

B

B

(cid:107)

(cid:107)

(cid:107)

(cid:107)

Si

−

2

2

2,

:
(cid:107)

(cid:107)

Si
(cid:107)

(cid:107)

=

1
m (cid:107)

uiuT

i −

E[uiuT
i ]

1
m (cid:107)

uiuT

(cid:107) ≤

1
m

E
uiuT
(cid:107)

i (cid:107) ≤

4n

2

B
(cid:107)
m

(cid:107)

= 2L/m.

uiuT
(cid:107)
2
(cid:107)

≤

i (cid:107)
2n

E[uiuT

i ]). We ﬁrst bound

. Since this is a rank 1 matrix,

√2, √2]. This gives a

Thus λmax(Si)
Now it’s time to bound E[S2

2L/m and λmax(

−
i ]. We will use

Si) =

≤

−

λmin(Si)

≤

+

i (cid:107)
2L/m.

E[S2

i ] =

1
m2
1
m2

(cid:16)

E(cid:2)(uiuT

i )2(cid:3)

E(cid:2)uiuT

i

−

E[

ui
(cid:107)

(cid:107)

2uiuT
i ]

(cid:22)

2n

2
(cid:107)

B
(cid:107)
m2

(cid:3)2(cid:17)

1
m2
(cid:22)
E[uiuT

i ].

=

E[(uiuT

i )2] =

E[uiuT

i uiuT
i ]

1
m2

Thus

m
(cid:88)

i=1

E[S2
i ]

2n

2
(cid:107)

B
(cid:107)
m

(cid:22)

E[v1vT

1 ] =

2n

2
(cid:107)

B
(cid:107)
m

B(K + D)BT

B(K + δ2

b In)BT = LM/m.

2n

2
(cid:107)

B
(cid:107)
m

(cid:22)

Applying Theorem 4 with S, for any t2 ≥
(cid:16)

(Z + C)(Z + C)T

(cid:20)
λmax(B

P

0, we have

(cid:17)

−

(K + D)

4 tr(M )
M
(cid:107)
S and using the fact that λmax(

t2In

B)

(cid:23)

≤

(cid:107)
S) =

exp

(cid:21)

(cid:21)

Similarly, applying Theorem 4 with

(cid:20)
λmin(B

P

(cid:16)

(Z + C)(Z + C)T

(K + D)

B)

(cid:17)

−

−

t1In

(cid:22) −

≤

−

exp

−
4 tr(M )
M

(cid:107)

(cid:107)

Combining the two bounds with the union bound yields the desired inequality.

(cid:18)

(cid:19)

.

mt2
2
+ 2t2/3)

−
2L(
M
(cid:107)
(cid:107)
λmin(S), for any t1 ≥
(cid:19)
(cid:18)
mt2
1
+ 2t1/3)

2L(

−
M
(cid:107)
(cid:107)

.

0, we have

We are now ready to show that low-precision features yield close spectral approximation to the exact kernel
matrix.
Theorem 2. Let ˜K be an m-feature b-bit LP-RFF approximation of a kernel matrix K, and assume
b := 2/(2b
δ2
δ2
b /λ,
(cid:104)
P

1)2. Then for any ∆1 ≥
˜K + λI
∆1)(K + λI)

0, ∆2 ≥
(1 + ∆2)(K + λI)

K
(cid:107)

(cid:107) ≥

(1

−

≥

λ

(cid:105)

−

(cid:22)

(cid:22)

8 tr (cid:0)(K + λIn)−

1(K + δ2

b In)(cid:1)

exp

1

−

≥

(cid:32)

(cid:18)

(cid:19)

m∆2
1
−
4n
λ (1 + 2
3 ∆1)

+ exp

(cid:32)

m(∆2 −

−
4n
λ (1 + 2

3 (∆2 −

δ2
λ )2
b
δ2
λ ))
b

(cid:33)(cid:33)

.

Proof. We conjugate the desired inequality with B := (K + λIn)−
noting that semideﬁnite ordering is preserved by conjugation:

1/2 (i.e., multiply by B on the left and right),

∆1)(K + λIn)

˜K + λIn

(1 + ∆2)(K + λIn)

(1

(1

−

∆1)In

⇐⇒

⇐⇒ −

⇐⇒ −

⇐⇒ −

−
∆1In

∆1In

∆1In

(cid:22)

(cid:22)

(cid:22)

(cid:22)
B( ˜K + λIn)B

(cid:22)
B( ˜K + λIn)B
B( ˜K + λIn
B( ˜K

−
K)B

−

(cid:22)

−
K

(cid:22)
(1 + ∆2)In

(cid:22)
In

∆2In

(cid:22)
λIn)B

(cid:22)

−
∆2In.

∆2In

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

δ2
b /λ)In implies
(∆2 −
We show that
∆1In
(cid:22)
δ2
1
(K + λIn)−
b /λ because
0 by Lemma 3 and B is symmetric, BDB

K
−
B2
(cid:107)
(cid:107)

−
2 =
(cid:107)

D)B
=

B( ˜K
B
(cid:107)

1/λ and

(cid:22)

−

(cid:107)

(cid:107)
0. Thus the condition

(cid:107) ≤

B( ˜K
∆1In
−
(cid:22)
δ2
b , so BDB
∆1In

−
D

K)B
∆2In. Indeed,
(cid:22)
(δ2
b /λ)In. Moreover,
B( ˜K

D)B

K

−

−

(cid:22)

(cid:22)
(cid:22)

−

(cid:107) ≤
(cid:23)

BDB
(cid:107) ≤
(cid:107)
since D
(cid:23)
δ2
b /λ)In implies:
(∆2 −

B( ˜K
B( ˜K

−

K)B = B( ˜K
K)B = B( ˜K

Hence P

∆1In

(cid:104)
−

K)B

∆2In

−
B( ˜K
B( ˜K

−

−

K

K

(cid:105)

−

−

D)B + BDB

D)B + BDB

(cid:22)

(∆2 −
∆1In + 0 =

(cid:104)

P

∆1In

(cid:23) −
B( ˜K

∆1In.

−

b /λ)In + δ2
δ2

b /λIn = ∆2In,

K

D)B

It remains to

δ2
b /λ)In

(cid:105)

.

(∆2 −

(cid:22)
show that
D)B
(cid:22)
apply Lemma 5 for B := (K + λIn)−

∆1In

K

−

−

(cid:22)

−

−

−
≥
δ2
b /λ)In with the desired probability, by applying Lemma 5. We
(∆2 −
1/2, L := 2n

2n/λ, and M := B(K + δ2

−

(cid:22)

−

(cid:22)

(cid:22)

2

≤
To simplify the bound one gets from applying Lemma 5 with the above B, L, and M , we will use the following
=
expression for tr(M ), and the following upper and lower bounds on

B2(K + δ2

tr(M ) = tr

(cid:16)

(cid:17)

(cid:107)

b In)B.

B
(cid:107)

(cid:17)

b In)

(cid:16)

1(K + δ2

(K + λIn)−

tr
b In)U T U (S + λIn)−
δ2
λ1 ≥
M
(cid:107)

(cid:107) ≤

λ by assumption),

1/2U T = U (S + λIn)−

M
(cid:107)

(cid:107)

= (λ1 + δ2

1(S + δ2
b )/(λ1 + λ)

≥

1. Lemma 5 allows us to conclude the following:

. Letting K = U SU T be the SVD of K, we get that M = U (S + λIn)−

(cid:107)

M

.
(cid:107)

b In)
1/2U T U (S +
b In)U T . Thus, letting λ1 be the largest eigenvalue of K (recall
λ, so

1/2. We also assume that δ2

(λ1 + δ2

b )/(2λ1)

b ≤

≥

(cid:20)

P

(1

−

(cid:20)
= P

∆1)(K + λIn)

˜K + λIn

(cid:22)

∆1In

(cid:22)

B( ˜K

K)B

∆2In

−

(cid:22)

(cid:22)

(cid:21)

(cid:21)
(1 + ∆2)(K + λIn)

≥

≥

(cid:104)

P

−

−

−

1

1

∆1In

B( ˜K
(cid:20)

−
(cid:18)

(cid:22)
4 tr(M )
M
(cid:107)
8 tr (cid:0)(K + λIn)−

exp

(cid:107)

≥

−

(K + D))B

(cid:105)

(cid:22)

(∆2 −
(cid:19)

δ2
b /λ)In
(cid:18)

−
M
(cid:107)

2L(
(cid:107)
1(K + δ2

m∆2
1
+ 2∆1/3)
b In)(cid:1)

(cid:20)

exp

(cid:18)

+ exp

−
2L(
M
(cid:107)
m∆2
1
4n/λ(1 + 2∆1/3)

−

b /λ)2
δ2
m(∆2 −
δ2
b /λ)/3)
+ 2(∆2 −
(cid:107)
(cid:18)
(cid:19)
+ exp

(cid:19)(cid:21)

m(∆2 −

δ2
b /λ)2
δ2
4n/λ(1 + 2(∆2 −
b /λ)/3)

−

(cid:19)(cid:21)

.

There is a bias-variance trade-oﬀ: as we decrease the number of bits b, under a ﬁxed memory budget we can use
more features, and (Z + C)(Z + C)T concentrates more strongly (lower variance) around the expectation K + D
δ2
with 0
b In. However, this expectation is further away from the true kernel matrix K (larger bias). Thus
there should be an optimal number of bits b∗ that balances the bias and the variance.

D

(cid:22)

(cid:22)

Proof of Corollary 2.1. Letting ∆2 → ∞
(cid:105)
˜K + λIn

∆1)(K + λIn)

(cid:104)
(1

P

−

(cid:22)

1

≥

−

in Theorem 2 gives

8 tr (cid:0)(K + λIn)−

1(K + δ2

b In)(cid:1) exp

(cid:18)

m∆2
1
4n/λ(1 + 2∆1/3)

−

(cid:19)

.

Using the assumption that ∆1 ≤
(cid:104)

P

(1

−

∆1)(K + λIn)

3/2, we can simplify the bound:

(cid:105)

˜K + λIn

(cid:22)

1

≥

−

8 tr (cid:0)(K + λIn)−

1(K + δ2

b In)(cid:1) exp

(cid:18)

m∆2
1
−
8n/λ

(cid:19)

.

Letting the RHS be 1

ρ and solving for m yields

−

m

≥

8n/λ
∆2
1

log

(cid:17)

.

(cid:16) a
ρ

Similarly, letting ∆1 → ∞

in Theorem 2 gives

P

(cid:104)
˜K + λIn

(1

(cid:22)

−

(cid:105)
∆2)(K + λIn)

1

≥

−

8 tr (cid:0)(K + λIn)−

1(K + δ2

b In)(cid:1) exp

(cid:18)

m(∆2 −
−

δ2
b /λ)2
δ2
b /λ)/3)
4n/λ(1 + 2(∆2 −

(cid:19)

.

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Using the assumption that ∆1 ≤

3/2, we can simplify the bound:

(cid:104)

P

(1

−

∆1)(K + λIn)

(cid:105)

˜K + λIn

(cid:22)

1

≥

−

8 tr (cid:0)(K + λIn)−

1(K + δ2

b In)(cid:1) exp

(cid:18)

−

m(∆2 −
8n/λ

b /λ)2
δ2

(cid:19)

.

Letting the RHS be 1

ρ and solving for m yields

−

m

≥

(∆2 −

8n/λ
b /λ)2 log
δ2

(cid:17)

.

(cid:16) a
ρ

and set ∆1 = ∆2 = ∆, we get the following corollary, similar to the result

If we let the number of bits b go to
from Avron et al. (2017):
Corollary 5.1. Suppose that ˜K = ZZ T ,

∞

(cid:104)

P

(1

−

∆)(K + λIn)

˜K + λIn

(cid:22)

(cid:22)

(cid:105)
(1 + ∆)(K + λIn)

≥

−

1

16 tr((K + λIn)−

1K) exp

(cid:18)

3m∆2
16n/λ

−

(cid:19)

.

K
(cid:107)

(cid:107) ≥

λ. Then for any ∆

1/2,

≤

Thus if we use m
≥
(1 + ∆)(K + λIn) with probability at least 1

16
3∆2 n/λ log(16 tr((K + λIn)−

ρ.

−

1K)/ρ) features, then (1

∆)(K + λIn)

−

˜K + λIn

(cid:22)

(cid:22)

The constants are slightly diﬀerent from that of Avron et al. (2017) as we use the real features √2 cos(wT x + a)
instead of the complex features exp(iwT x).

From these results, we now know that the number of features required depends linearly on n/λ; more precisely,
n/λ features (for some constant c0 > 0), ˜K + λIn will be a (∆, ∆)-spectral
we know that if we use m
approximation of K + λIn with high probability. Avron et al. (2017) further provide a lower bound, showing that
n/λ (for some other constant c1 > 0), ˜K + λIn will not be a (∆, ∆)-spectral approximation of K + λIn
if m
with high probability. This shows that the number of random Fourier features must depend linearly on n/λ.

c0 ·

c1 ·

≤

≥

C.2 Frobenius Kernel Approximation Error Bounds for LP-RFFs

We begin this section by bounding the variance of the quantization noise C added to the random feature matrix
Z. We prove this as a simple consequence of the following Lemma.14
Lemma 6. For z
probability c
c

be the random variable which with probability z
c
(c

z
z. Then E [X a,c

a
a equals c
−
−
a)2
.
−
4

] = 0, and VAR [X a,c

[a, c], let X a,c

z, and with

] = (c

z)(z

a)

−

z

z

∈
z
a equals a
−
−

−

−

−

≤

Proof.

VAR [X a,c

z

] = (c

E [X a,c
z

] = (c

z)

z
c

·

a
a

+ (a

z)

−

c
c

·

z
a

−
−
c
−
c
−
a))

z
a

z)2

−
z + z

·

−

+ (a

a
−
a
−
a)((c
−
c
a
a)

−

−

= 0.

z)2

·
z)(z

−

−

(c

=

−
−
z
c

−

z)(z

−

= (c
d
dz

−
[
−

=

2z + c + a.

−

d
dz

[VAR [X a,c

z

]] =

z2 + (c + a)z

ac]

−

Now, setting the derivative to 0 gives z∗ = c+a
z)(z

a) = (c

a)2
a) = (c
−
4

2 )( c+a
c+a

.

−

−

2 −

2 . Thus, arg maxz

[a,c](c

z)(z

∈

−

−

a) = c+a

2 , and maxz

[a,c](c

∈

−

14This lemma is also a direct consequence of Popoviciu’s inequality on variances (Popoviciu, 1935). Nonetheless, we

include a stand-alone proof of the lemma here for completeness.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Proposition 7. E[C 2

Zij]

b /m, for δ2
δ2

b :=

ij |

≤

(2b

2

1)2 .
−

Proof. Given a feature Zij

2/m], we quantize it to b bits by dividing this interval into 2b

1

−

1 ), and randomly rounding to the top or bottom of the sub-interval
equally-sized sub-intervals (each of size
containing Zij (in an unbiased manner). Let a, c denote the boundaries of the sub-interval containing Zij, where

−

(cid:112)

2/m,

(cid:112)

[

∈

−

2√2/m
2b

1 . We can now see that Cij = X a,c
−
Zij] = 0. Because c

a =

2√2/m
2b

Zij is the unique random variable such that Zij + Cij
X a,c

1 , it follows from Lemma 6 that E[C 2

Zij] = VAR

(cid:104)

a, c
}
(cid:105)

∈ {
Zij

≤

Zij |

−

−

ij |

c = a +

2√2/m
2b
and E [Cij
a)2
4 = 8/m
(c

4(2b

−

|

−

1)2 = δ2

b /m.

We now prove an upper bound on the expected kernel approximation error for LP-RFFs, which applies for any
quantization function with bounded variance. This error corresponds exactly to the variance of the random
variable which is the product of two quantized random features.
Theorem 8. For x, y
≤
σ2, and that k(x, x) = k(y, y) = 1.15 For any unbiased random quantization function Q with bounded variance
VAR [Q(z)]
≤
2˜σ2 + ˜σ4 + σ2.

˜σ2 for any (ﬁxed) z, it follows that E [Q(Zx)Q(Zy)] = k(x, y), and that VAR [Q(Zx)Q(Zy)]

, assume we have random variables Zx, Zy satisfying E [ZxZy] = k(x, y), VAR [ZxZy]

∈ X

≤

Proof. Let Q(Zx) = Zx + (cid:15)x, and Q(Zy) = Zy + (cid:15)y, where E [(cid:15)x] = E [(cid:15)y] = 0, E (cid:2)(cid:15)2
are independent random variables.

x

(cid:3)

≤

˜σ2, E (cid:2)(cid:15)2

y

(cid:3)

≤

˜σ2, and (cid:15)x,(cid:15)y

E [Q(Zx)Q(Zy)] = E [(Zx + (cid:15)x)(Zy + (cid:15)y)]

= E [ZxZy + Zy(cid:15)x + Zx(cid:15)y + (cid:15)x(cid:15)y]
= E [ZxZy]
= k(x, y).

VAR [Q(Zx)Q(Zy)] = E

Q(Zx)Q(Zy)

k(x, y)

(cid:17)2(cid:21)

−

(cid:20)(cid:16)

(cid:20)(cid:16)

(cid:20)(cid:16)

(cid:20)(cid:16)

= E

= E

= E

(Zx + (cid:15)x)(Zy + (cid:15)y)

k(x, y)

−

(cid:17)2(cid:21)

Zy(cid:15)x + Zx(cid:15)y + (cid:15)x(cid:15)y + ZxZy

k(x, y)

(cid:17)2(cid:21)

−
(cid:20)(cid:16)

(cid:17)2(cid:21)

+ E

(cid:17)2(cid:21)

ZxZy

k(x, y)

−

Zy(cid:15)x + Zx(cid:15)y + (cid:15)x(cid:15)y

≤

y (cid:15)2

x + Z 2

E (cid:2)Z 2
y + (cid:15)2
k(y, y)˜σ2 + k(x, x)˜σ2 + ˜σ4 + σ2

(cid:3) + σ2

x(cid:15)2

x(cid:15)2
y

≤
= 2˜σ2 + ˜σ4 + σ2.

Note that if x = y, for this proof to hold, we would need to randomly quantize Zx twice, giving two independent
quantization noise samples (cid:15)(1)

x and (cid:15)(2)
x .

σ2, quantizing the random features will have negligable eﬀect on the
This theorem suggests that if 2˜σ2 + ˜σ4
variance. In practice, for the random quantization method we use with random Fourier features (described in
Section 4.1), we have ˜σ2 =
1)2 . Thus, for a large enough precision b, the quantization noise will be tiny
relative to the variance inherent to the random features.

(cid:28)

(2b

−

2

We note that the above theorem applies to one-dimensional random features, but can be trivially extended to m
dimensional random features. We show this in the following corollary.

15For example, one speciﬁc instance of the random variables Zx, Zy is given by random Fourier features, where zx =
√2, √2], for random w, b. We need not assume that k(x, x) = k(y, y) = 1,

√2 cos(wT x+b), zy = √2 cos(wT y +b), zx, zy
but this simpliﬁes the ﬁnal expression, as can be seen in the last step of the proof.

−

∈

[

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Table 3: Dataset details. For classiﬁcation tasks, we write the number of classes in parentheses in the “task”
column.

Dataset Task

Train Heldout Test # Features

Reg.
Census
YearPred Reg.
CovType Class. (2)
TIMIT

Class. (147)

16k
417k
418k
2.3M 245k

2k
46k
46k

2k
52k
116k
116k

119
90
54
440

Table 4: The Gaussian kernel bandwidths used, and the search grid for initial learning rate on the Census,
YearPred, Covtype and TIMIT datasets. Optimal learning rate in bold.

Dataset

Census
YearPred
Covtype
TIMIT

1/2σ2

0.0006
0.01
0.6
0.0015

Initial learning rate grid

0.01, 0.05, 0.1, 0.5, 1.0
0.05, 0.1, 0.5, 1.0, 5.0
1.0, 5.0, 10.0, 50.0, 100.0
5.0, 10.0, 50.0, 100.0, 500.0

∈ X

Corollary 8.1. For x, y
VAR [Zx] , VAR [Zy]
≤
bounded variance (E [Q(z)] = z, VAR [Q(z)]
(T1, . . . , Tn) be a random sequence of i.i.d. draws from S and T respectively. Deﬁne ¯Sn = 1
n
¯Tn = 1
n
VAR (cid:2) ¯Sn

, assume we have random variables Zx, Zy satisfying E [ZxZy] = k(x, y), and
σ2, and that k(x, x) = k(y, y) = 1. Let Q be any unbiased quantization function with
˜σ2 for any z). Let S = ZxZy, T = Q(Zx)Q(Zy), and (S1, . . . , Sn),
(cid:80)n
i=1 Si, and
(cid:3) = k(x, y), and that

≤
i=1 Ti, to be the empirical mean of these draws. It follows that E (cid:2) ¯Sn
.

(cid:3) = E (cid:2) ¯Tn

(cid:80)n
(cid:3)

n , and VAR (cid:2) ¯Tn

2˜σ2+˜σ4+σ2
n

σ2

(cid:3)

≤

≤

Proof.

VAR (cid:2) ¯Sn

(cid:3) = VAR

(cid:34)

1
n

VAR

(cid:35)

Si

(cid:21)

Si

n
(cid:88)

i=1
(cid:20) 1
n

=

≤

=

n
(cid:88)

i=1

n

·
σ2
n

σ2
n2

The result for VAR (cid:2) ¯Tn

(cid:3) follows in the same way, using the result from Theorem 8.

D EXPERIMENT DETAILS AND EXTENDED RESULTS

D.1 Datasets and Details Applying to All Experiments

In this work, we present results using FP-Nystr¨om, FP-RFFs, circulant FP-RFFs, and LP-RFFs on the TIMIT,
YearPred, CovType, and Census datasets. These datasets span regression, binary classiﬁcation, and multi-class
classiﬁcation tasks. We present details about these datasets in Table 3. In these experiments we use the Gaussian
kernel with the bandwidth σ speciﬁed in Table 4; we use the same bandwidths as May et al. (2017). To evaluate
the performance of these kernel models, we measure the classiﬁcation error for classiﬁcation tasks, and the mean
squared error (MSE) for regression tasks ( 1
yi)2), on the heldout set. We compute the total
n
memory utilization as the sum of all the components in Table 1.

i=1(f ˜K(xi)

(cid:80)n

−

For all datasets besides TIMIT, we pre-processed the features and labels as follows: We normalized all continuous

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

features to have zero mean and unit variance. We did not normalize the binary features in any way. For regression
datasets, we normalized the labels to have zero mean across the training set.

TIMIT (Garofolo et al., 1993) is a benchmark dataset in the speech recognition community which contains
recordings of 630 speakers, of various English dialects, each reciting ten sentences, for a total of 5.4 hours of
speech. The training set (from which the heldout set is then taken) consists of data from 462 speakers each
reciting 8 sentences (SI and SX sentences). We use 40 dimensional feature space maximum likelihood linear
regression (fMLLR) features (Gales, 1998), and concatenate the 5 neighboring frames in either direction, for a
total of 11 frames and 440 features. This dataset has 147 labels, corresponding to the beginning, middle, and end
of 49 phonemes. For reference, we use the exact same features, labels, and divisions of the dataset, as (Huang
et al., 2014; Chen et al., 2016; May et al., 2017).

We acquired the CovType (binary) and YearPred datasets from the LIBSVM webpage,16 and the Census dataset
from Ali Rahimi’s webpage.17 For these datasets, we randomly set aside 10% of the training data as a heldout set
for tuning the learning rate and kernel bandwidth.

The speciﬁc ﬁles we used were as follows:

•

•

•

CovType: We randomly chose 20% of “covtype.libsvm.binary” as test, and used the rest for training/heldout.

YearPred: We used “YearPredictionMSD” as training/heldout set, and “YearPredictionMSD.t” as test.

Census: We used the included matlab dataset ﬁle “census.mat” from Ali Rahimi’s webpage. This Matlab ﬁle
had already split the data into train and test. We used a random 10% of the training data as heldout.

D.2 Nystr¨om vs. RFFs (Section 2.2)

}

∈ {

. For RFFs, we use m

1250, 2500, 5000, 10000, 20000

We compare the generalization performance of full-precision RFFs and the Nystr¨om method across four
datasets, for various memory budgets. We sweep the following hyperparameters: For Nystr¨om, we use
m
1250, 2500, 5000, 10000, 20000, 50000, 100000,
∈ {
200000, 400000
. We choose these limits diﬀerently because 20k Nystr¨om features have roughly the same memory
}
footprint as 400k FP-RFFs. For all experiments, we use a mini-batch size of 250. We use a single initial learning
rate per dataset across all experiments, which we tune via grid search using 20k Nystr¨om features. We choose
to use Nystr¨om features to tune the initial learning rate in order to avoid biasing the results in favor of RFFs.
We use an automatic early-stopping protocol, as in (Morgan and Bourlard, 1990; Sainath et al., 2013b,a), to
regularize our models (Zhang et al., 2005; Wei et al., 2017) and avoid expensive hyperparameter tuning. It works
as follows: at the end of each epoch, we decay the learning rate in half if the heldout loss is less than 1% better
relative to the previous best model, using MSE for regression and cross entropy for classiﬁcation. Furthermore, if
the model performs worse than the previous best, we revert the model. The training terminates after the learning
rate has been decayed 10 times.

We plot our results comparing the Nystr¨om method to RFFs in Figure 7. We compare the performance of these
methods in terms of their number of features, their training memory footprint, and the squared Frobenius norm
and spectral norm of their kernel approximation error.

D.3 Nystr¨om vs. RFFs Revisited (Section 3.2)

D.3.1 Small-Scale Experiments

In order to better understand what properties of kernel approximation features lead to strong generalization
performance, we perform a more ﬁne-grained analysis on two smaller datasets from the UCI machine learning
repository—we consider the Census regression task, and a subset of the CovType task with 20k randomly sampled
training points and 20k randomly sampled heldout points. The reason we use these smaller datasets is because
computing the spectral norm, as well as (∆1,∆2) are expensive operations, which requires instantiating the kernel
matrices fully, and performing singular value decompositions. For the Census experiments, we use the closed-form
solution for the kernel ridge regression estimator, and choose the λ which gives the best performance on the

16https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/
17https://keysduplicated.com/ ali/random-features/data/

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

(a)

(b)

(c)

(d)

Figure 7: Generalization performance of FP-RFFs and Nystr¨om with respect to # features (a) and training
memory footprint (b) on Census, CovType, Yearpred and TIMIT. Nystr¨om performs better for a ﬁxed number
of features, while FP-RFFs perform better under a memory budget. We also see that the relative performance
between these methods cannot be fully explained by the Frobenius norms (c) or spectral norms (d) of their
respective kernel approximation error matrices; in particular, we see many example of Nystr¨om models that have
lower Frobenius or spectral error, but worse heldout performance, than various RFF models. To quantify the
degree of alignment between these error metrics and generalization performance (right plots), we show in the
ﬁgure titles the Spearman rank correlation coeﬃcients ρ between the corresponding x and y metrics. We see
in Figure 11 that 1/(1
∆1) attains notably higher values of ρ than the Frobenius and spectral norms of the
kernel approximation error. Note that although we plot the average performance across three random seeds for
each experimental setting (error bars indicate standard deviation), when we compute the ρ values we treat each
experimental result independently (without averaging).

−

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

−

Figure 8: The correlation between generalization performance and squared Frobenius norm, spectral norm, ∆,
and 1/(1
∆1) for FP-RFFs and FP-Nystr¨om on the Census and subsampled CovType datasets. To quantify
the alignment between these metrics and downstream performance, we include the Spearman rank correlation
coeﬃcients ρ computed between these metrics and the downstream performance of our trained models in the
ﬁgure titles. Note that although we plot the average performance across ﬁve random seeds for each experimental
setting (error bars indicate standard deviation), when we compute the ρ values we treat each experimental result
independently (without averaging).

Table 5: Number of features used for the diﬀerent kernel approximation methods in the experiments on
generalization performance vs. (∆1, ∆2) in Sections 3.2 and 5.2.

Methods

Number of features

FP-Nystr¨om
FP-RFF
Cir. FP-RFF
LP-RFF 16
LP-RFF 8, 4, 2, 1

25, 50, 100, 200, 500, 1250, 2500, 5000, 10000, 20000
200, 500, 1000, 2000, 5000, 10000, 20000
200, 500, 1000, 2000, 5000, 10000, 20000
500, 1000, 2000, 5000, 10000, 20000, 50000
1000, 2000, 5000, 10000, 20000, 50000

heldout set. For CovType, because there is no closed-form solution for logistic regression, we used the following
training protocol to (approximately) ﬁnd the model which minimizes the regularized training loss (just like the
closed-form ridge regression solution does). For each value of λ, we train the model to (near) convergence using 300
epochs of SGD (mini-batch size 250) at a constant learning rate, and pick the learning rate which gives the lowest
regularized training loss for that λ. We then evaluate this converged model on the heldout set to see which λ gives
the best performance. We pick the best learning rate, as well as regularization parameter, by using 20k Nystr¨om
features as a proxy for the exact kernel (note that because there are only 20k training points, this Nystr¨om
approximation is exact). We choose the learning rate 5.0 from the set
, and
}
the regularization parameter λ = 5e
. For the Census
4
}
. We sweep the number
1e
dataset, we pick λ = 5e
}
{
of features shown in Table 5. For both datasets, we report the average squared Frobenius norm, spectral norm,
∆, (∆1, ∆2) and the average generalization performance, along with standard deviations, using ﬁve diﬀerent
random seeds. The results are shown in Figure 8. As can be seen in Figure 8, 1/(1
∆1) aligns much better with
generalization performance than the other metrics.

0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0
5, 1e
−
1
−

6 from
5, 5e

{
6, 5e
3, 1e

7, 5e
4, 1e

7, 1e
3, 5e

6, 1e
2, 5e

5, 5e
2, 1e

8, 1e
4, 5e

5e
{
5, 1e

4 from

−
−

−
−

−
−

−
−

−
−

−
−

−
−

−

−

−

−

It is important to note that although 1/(1
∆1) aligns quite well with generalization performance (Spearman
rank correlation coeﬃcients ρ equal to 0.958 and 0.948 on Census and CovType, respectively), it does not align
perfectly. In particular, we see that for a ﬁxed value of ∆1, Nystr¨om generally attains better heldout performance
than RFFs. We believe this is largely explained by the fact that Nystr¨om always has ∆2 = 0, while RFFs can have

−

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

−

Figure 9: The correlation between generalization performance and squared Frobenius norm, spectral norm, ∆,
and 1/(1
∆1) for various types of full-precision RFFs. The Spearman rank correlation coeﬃcients ρ between the
x and y metrics of each ﬁgure are included in the ﬁgure titles. For these full-precision RFF experiments, we see
∆1) both align very well with downstream performance
that the original ∆ (Avron et al., 2017) as well as 1/(1
−
(ρ = 0.940 and ρ = 0.938, respectively), while the Frobenius and spectral norms do not (ρ = 0.705 and ρ = 0.652,
respectively). Note that although we plot the average performance across ﬁve random seeds for each experimental
setting (error bars indicate standard deviation), when we compute the ρ values we treat each experimental result
independently (without averaging).

Figure 10: The generalization performance as a function of the number of features (left) and the training memory
footprint (right) for various types of RFFs on the Census dataset. We observe that LP-RFFs can achieve lower
heldout mean-squared error (MSE) than other types of RFFs, included the memory-eﬃcient structured orthogonal
random features (“FP-SORF (SC)”), under the same memory budget. We plot performance averaged over ﬁve
random seeds, with error bars indicating standard deviations.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

relatively large values of ∆2 when the number of features is small (see Figure 3). As we show in the generalization
bound in Proposition 1, in Figure 5, as well as in the empirical and theoretical analysis in Appendix B, we expect
generalization performance to deteriorate as ∆2 increases.

D.3.2 Experiments with Diﬀerent Types of RFFs

In Figure 9 we repeat the above experiment on the Census dataset using a number of variations of RFFs.
Speciﬁcally, we run experiments using the [sin(wT x), cos(wT x)] parameterization of RFFs, which Sutherland
and Schneider (2015) show has lower variance than the cos(wT x + a) parameterization (we denote the [sin, cos]
method by “FP-RFF (SC)”, and the cos method by “FP-RFF (C)”). We additionally use Quasi-Monte Carlo
features (Yang et al., 2014), as well as orthogonal random features and its structural variant (Yu et al., 2016); we
denote these by “FP-QMC (SC)”, “FP-ORF (SC)”, “FP-SORF (SC)” respectively, because we implement them
using the [sin, cos] parameterization. We can see in Figure 9 that although these methods attain meaningful
improvements in the Frobenius and spectral norms of the approximation error, these improvements do not
translate into corresponding gains in heldout performance. Once again we see that 1/(1
∆1) is able to much
better explain the relative performance between diﬀerent kernel approximation methods than Frobenius and
spectral error. In Figure 10 we see that LP-RFFs outperform these other methods in terms of performance under
a memory budget.

−

D.3.3 Large-Scale (∆1, ∆2) Experiments

∆1) on the large-scale experiments from Section 2.2. We measure
In Figure 11 we plot the performance vs. 1/(1
∆1 using the exact and approximate kernel matrices on a random subset of 20k heldout points (except for Census,
where we use the entire heldout set which has approximately 2k points). We pick several λ values between the
smallest and largest eigenvalues of the exact (subsampled) training kernel matrix. The strong alignment between
generalization performance and 1/(1

∆1) is quite robust to the choice of λ.

−

−

D.3.4 Measuring ∆ and (∆1, ∆2)

In order to measure the ∆1 and ∆2 between a kernel matrix K and an approximation ˜K, we ﬁrst observe that
the following statements are equivalent. Note that to get the second statement, we multiply the expressions in
the ﬁrst statement by (K + λIn)−

1/2 on the left and right.

(1

−

∆1)(K + λIn)
∆1)In

(1

−

∆1In

−

∆1In

−
1/2( ˜K

(cid:22)

(cid:22)

(cid:22)

(cid:22)

˜K + λIn
(cid:22)
(K + λIn)−

(K + λIn)−

(1 + ∆2)(K + λIn)
1/2( ˜K + λIn)(K + λIn)−
1/2(cid:16)

˜K + λIn

1/2

(cid:22)

(cid:17)

(1 + ∆2)In
1/2

(K + λIn)

(K + λIn)−

−

∆2In

(cid:22)

(K + λIn)−

1/2( ˜K

K)(K + λIn)−

1/2

∆2In.

−
1/2, this is equivalent to
(cid:0)A(cid:1). Lastly, we note that ∆ = max(∆1, ∆2).

∆1 ≤

λmin

−

(cid:22)

(cid:0)A(cid:1) and λmax

(cid:0)A(cid:1)

∆2. Thus,

≤

For A = (K + λIn)−
we choose ∆1 :=

λmin

K)(K + λIn)−
(cid:0)A(cid:1) and ∆2 := λmax

−

−

D.4 Theory Validation (Section 4.2)

To validate our theory in Section 4.2, we perform two sets of experiments to (1) demonstrate the asymptotic
behavior of ∆1 and ∆2 as the number of features increases, and (2) demonstrate that quantization has negligible
eﬀect on ∆2 when δ2

∆2.

b /λ

(cid:28)

To demonstrate the behavior of ∆1 and ∆2 as a function of the number of features, we collect ∆1, and ∆2 using
Nystr¨om features, circulant FP-RFFs, and LP-RFFs using b
, on both the Census and the sub-sampled
}
CovType datasets (20k random heldout points). For each approximation, we sweep the number of features as
listed in Table 6. We use the same value of λ as we used in Section 3.2 for each dataset. In Figure 12, we plot
the values of ∆1 and ∆2 attained by these methods as a function of the number of features. As discussed in
Section 4.2, ∆1 is primarily determined by the rank of the approximation, and approaches 0 for all the methods
as the number of features grows. ∆2, on the other hand, only approaches 0 for the high-precision methods—for
b

b /λ, marked by dashed lines).

, ∆2 converges to higher values (at most δ2
1, 4
}

1, 4, 8

∈ {

∈ {

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

1
∆1

1

−

, where we measure ∆1 using regularizer strength λ equal to the
Figure 11: Generalization performance vs.
0, 25, 50, 75, or 99 percentile eigenvalues of the exact kernel matrix (0 percentile indicates largest eigenvalue).
Note for the Census dataset, we plot the heldout MSE as a function of 1/(1
1 to avoid cluttering the
data points on the left end of the ﬁgure. For comparison to spectral and Frobenius norm plots, see Figure 7. To
∆1) and generalization performance for these diﬀerent values of
quantify the degree of alignment between 1/(1
λ, we compute the Spearman rank correlation coeﬃcients ρ. We see 1/(1
∆1) generally attains much higher
values of ρ than the Frobenius and spectral approximation errors (Figure 7). Although we plot performance
averaged across three random seeds (error bars indicate standard deviations), when we compute ρ we treat each
experimental result independently.

∆1)

−

−

−

−

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Figure 12: Empirical validation of Theorem 2. As the number of features grows, LP-RFFs approach ∆1 values of
0 (left), but plateau at larger ∆2 for very low precisions (right). These are an extended version of the results
from Figure 3 (left, middle) in Section 4.2. We plot average results across ﬁve random seeds, with error bars
indicating standard deviations.

Table 6: Number of features used for the diﬀerent kernel approximation methods in the theory validation
experiments in Section 4.2 (Figure 3 (left,middle)).

Methods

Number of features

FP-Nystr¨om
Cir. FP-RFF
LP-RFF 8, 4, 2, 1

25, 50, 100, 200, 500, 1250, 2500, 5000, 10000, 20000
200, 500, 1000, 2000, 5000, 10000, 20000, 50000, 100000, 200000, 400000
1000, 2000, 5000, 10000, 20000, 50000, 100000, 200000, 400000

To demonstrate that quantization has negligible eﬀect on ∆2 when δ2
points from the Census dataset. For λ
by the LP-RFFs relative to the exact kernel matrix. We see that for larger λ (corresponding to smaller δ2
Figure 3, lower precisions can be used while not inﬂuencing ∆2 signiﬁcantly; this aligns with the theory.

∆2, we use 8000 random training
, we measure the ∆2 attained
b /λ) in

1, 2, 4, 8, 16, 32
}

, and b
}

4, 100, 104

b /λ

10−

∈ {

∈ {

(cid:28)

D.5 Empirical Evaluation of LP-RFFs (Section 5.1)

To empirically demonstrate the generalization performance of LP-RFFs, we compare LP-RFFs to FP-RFFs,
circulant FP-RFFs, and Nystr¨om features for various memory budgets. We use the same datasets as in Section 2,
including Census and YearPred for regression, as well as CovType and TIMIT for classiﬁcation. We use the same
experimental protocol as in Section 2 (details in Appendix D.2), with the only signiﬁcant additions being that we
also evaluate the performance of circulant FP-RFFs, and LP-RFFs for precisions b
. As noted in
}
the main text, all our LP-RFF experiments are done in simulation; in particular, we represent each low-precision
feature as a 64-bit ﬂoating point number, whose value is one of the 2b values representable in b bits. For our
full-precision experiments we also use 64-bit ﬂoats, but we report the memory utilization of these experiments
as if we used 32 bits, to avoid inﬂating the relative gains of LP-RFFs over the full-precision approaches. We
randomly sample the quantization noise for each mini-batch independently each epoch.

1, 2, 4, 8, 16

∈ {

In Section 5.1 (Figure 4), we demonstrated the generalization performance of LP-RFFs on the TIMIT, YearPred,
and CovType datasets, using 4 bits per feature. In Figure 13, we additionally include results on the Census
dataset, and include results for a larger set of precisions (b
). We also include plots of heldout
performance as a function of the number of features used. We observe that LP-RFFs, using 2-8 bits, systematically
outperform the full-precision baselines under diﬀerent memory budgets.

1, 2, 4, 8, 16

∈ {

}

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 13: Generalization performance of the kernel approximation methods (Nystr¨om, FP-RFFs, circ. FP-RFFs,
LP-RFFs) with respect to the number of features used, as well as with respect to memory used. We observe
that LP-RFFs demonstrate better generalization performance than the full-precision baselines under memory
constraints, with 2-8 bits typically giving the best performance. Importantly, the relative ranking of the methods
changes depending on whether we compare the methods based on their number of features, or their memory
utilization. For example, the Nystr¨om method shows better generalization performance than the RFF-based
approaches with the same number of features. However, the Nystr¨om method often performs signiﬁcantly worse
than the RFF methods under ﬁxed memory budgets. We plot results averaged over three random seeds.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Table 7: Dataset details for the additional datasets from Section D.5. For classiﬁcation tasks, we write the number
of classes in parentheses in the “Task” column.

Dataset

Task

Train Heldout Test # Features

Class. (2)
Forest
Cod-RNA Class. (2)
Class. (2)
Adult
Reg.
CPU

470k
54k
29k
6k

52k
6k
3k
0.7k

58k
272k
16k
0.8k

54
8
123
21

Table 8: The Gaussian kernel bandwidths used, and the search grid for initial learning rate on the Forest,
Cod-RNA, Adult, and CPU datasets. Optimal learning rate in bold.

Dataset

1/2σ2

Initial learning rate grid

Forest
0.5
Cod-RNA 0.4
0.1
Adult
0.03
CPU

5.0, 10.0, 50.0, 100.0, 500.0
10.0, 50.0, 100.0, 500.0, 1000.0
5.0, 10.0, 50.0, 100.0, 500.0, 1000.0
0.05, 0.1, 0.5, 1.0, 5.0

We run on four additional classiﬁcation and regression datasets (Forest, Cod-RNA, Adult, CPU) to compare the
empirical performance of LP-RFFs to full-precision RFFs, circulant RFFs and Nystr¨om. We present the results
in Figure 14, and observe that LP-RFFs can achieve competitive generalization performance to the full-precision
baselines with lower training memory budgets. With these 4 additional datasets, our empirical evaluation of
LP-RFFs now covers all the datasets investigated in Yang et al. (2012). We include details about these datasets
and the hyperparameters we used in Tables 7 and 8.

D.6 Generalization Performance vs. (∆1, ∆2) (Section 5.2)

For our experiments in Section 5.2, we use the same protocol and hyperparameters (learning rate, λ) as for the
(∆1, ∆2) experiments in Section 3.2. However, we additionally run experiments with LP-RFFs for precisions
b
. We use ﬁve random seeds for each experimental setting, and plot the average results, with
error bars indicating standard deviations.

1, 2, 4, 8, 16

∈ {

}

In Figure 15, we plot an extended version of the right plots in Figure 5. We include results for all precisions, and
for both Census and CovType. We observe that on Census, 1/(1
∆1) does not align well with performance
(Spearman rank correlation coeﬃcient ρ = 0.403), because the low-precision features (b = 1 or b = 2) perform
signiﬁcantly worse than the full-precision features of the same dimensions. In this case, when we consider the
impact of ∆2 by taking max (cid:0)1/(1
On CovType, on the other hand, the impact on generalization performance from using low-precision is much less
(cid:1) both align well with performance (ρ = 0.942). Furthermore,
pronounced, so 1/(1
in the case of CovType, ∆2 is generally smaller than 1/(1
∆1), so taking the max does not change the plot
signiﬁcantly.

(cid:1), we see that performance aligns much better (ρ = 0.959).

∆1) and max (cid:0)1/(1

∆1), ∆2

∆1), ∆2

−

−

−

−

−

To compute the Spearman rank correlation coeﬃcients ρ for these plots, we take the union of all the experiments
which are a part of the plot. In particular, we include FP-RFFs, circulant FP-RFFs, FP-Nystr¨om , and LP-RFFs
with precisions b
. Although we plot the average performance across ﬁve random seeds in the plot,
}
to compute ρ we treat each experiment independently.

1, 2, 4, 8, 16

∈ {

D.7 Other Experimental Results

D.7.1 Low-Precision Nystr¨om

We encounter two main obstacles to attaining strong performance with the Nystr¨om method under a memory
budget: (1) For the standard Nystr¨om method, because of the large projection matrix (32m2 space), there are

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 14: Comparison of the performance of LP-RFFs and the full-precision baselines on additional datasets,
with respect to the number of features used, as well as with respect to memory used. We observe that LP-RFFs
can achieve performance competitive with the full-precision baseline methods, with signiﬁcant memory savings.
We plot the average performance across three random seeds, with error bars indicating standard deviations.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

−

Figure 15: Generalization performance vs. diﬀerent kernel approximation metrics on Census and CovType. The
metric max (1/(1
∆1), ∆2) is able to incorporate the inﬂuence of both ∆1 and ∆2 on performance for LP-RFFs,
aligning well with generalization performance on both Census (Spearman rank correlation coeﬃcient ρ = 0.959)
and CovType (ρ = 0.942). 1/(1
∆1), on the other hand, fails to align well on Census (ρ = 0.403), but does
align on CovType (ρ = 0.942). Note that although we plot the average performance across ﬁve random seeds for
each experimental setting (error bars indicate standard deviation), when we compute the ρ values we treat each
experimental result independently (without averaging).

−

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 16: We plot the heldout performance (MSE) for the full-precision Nystr¨om method, the ensemble Nystr¨om
method, and 8-bit LP-RFFs. We also show the best possible performance for the Nystr¨om methods, assuming only
the kernel approximation features are quantized (denoted “ideal”); we compute this by plotting the full-precision
results but without counting the memory occupied by the features. The LP-RFF method signiﬁcantly outperforms
the “ideal” Nystr¨om methods as a function of memory.

(a) LP-Nystr¨om

(b) Ensemble LP-Nystr¨om

Figure 17: The generalization performance of low-precision Nystr¨om and low-precision ensemble Nystr¨om, using
a uniform quantization scheme.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

inherent limits to the memory savings attainable by only quantizing the features, regardless of the quantization
scheme used. (2) While the ensemble Nystr¨om method (Kumar et al., 2009) is a well-known method which can
dramatically reduce the space occupied by the the Nystr¨om projection matrix, it does not attain meaningful gains
in performance under a memory budget. To demonstrate the ﬁrst issue empirically, we plot in Figure 16 the
performance of the full-precision Nystr¨om methods without counting the memory from the feature mini-batches
(denoted “ideal”). This is the best possible performance under any quantization scheme for these Nystr¨om
methods, assuming only the features are quantized. LP-RFFs still outperform these “ideal” methods for ﬁxed
memory budgets. To demonstrate the second issue, in Figure 16 we also plot the generalization performance of
the ensemble method vs. the standard Nystr¨om method, as a function of memory. We can see that the ensemble
method does not attain meaningful gains over the standard Nystr¨om method, as a function of memory.

(cid:112)

(cid:112)

2/m and

Lastly, we mention that quantizing Nystr¨om features is more challenging due to their larger dynamic range.
1 and 1, whereas for RFFs, each feature is between
For Nystr¨om, all we know is that each feature is between
2/m. In Figure 17 we plot our results quantizing Nystr¨om features with the following simple
−
scheme: for each feature, we ﬁnd the maximum and minimum value on the training set, and then uniformly
quantize this interval using b bits. We observe that performance degrades signiﬁcantly with less than or equal to
8 bits compared to full-precision Nystr¨om. Although using the ensemble method reduces the dynamic range by a
factor of √r with r blocks (we use r = 10, a common setting in (Kumar et al., 2012)), and also saves space on
the projection matrix, these strengths do not result in signiﬁcantly better performance for ﬁxed memory budgets.

−

D.7.2 Low-Precision Training for LP-RFFs

As discussed in Section 2.1, there are a number of ways to reduce the memory occupied by the model parameters,
including (1) using a low-rank decomposition of the parameter matrix (Sainath et al., 2013a), (2) using structured
matrices (Sindhwani et al., 2015), and (3) using low-precision (De Sa et al., 2018). Though these methods are
orthogonal to our LP-RFF method, we now present results using a low-precision parameterization of the model,
and show that we can attain similar performance to full-precision training. We use a training algorithm called
LM-HALP (linear model high-accuracy low-precision) (De Sa et al., 2018). By parameterizing the model in low
precision, this algorithm eliminates the need of casting the LP-RFFs back into full precision in order to multiply
them with the model parameters. This approach also allows for these matrix multiplications to be implemented
using fast low-precision matrix operations, and reduces the memory occupied by the model during training.

LM-HALP is based on the stochastic variance-reduced gradient (SVRG) algorithm. The model is parameterized
using a low-precision ﬁxed-point representation during training. In LM-HALP, all of the matrix multiplications
involved in the stochastic model updates are done using low-precision ﬁxed-point operations; however, the periodic
computation of the full gradient is calculated in full precision (this is embarrassingly parallelizable). Importantly,
even though most of training is done in low precision, the ﬁnal model returned by this training algorithm is a
full-precision model. We can further simplify the LM-HALP algorithm by replacing the SVRG updates with
SGD updates, thus eliminating the need for calculating and storing the full gradient. In Figure 18 we present our
results using LM-HALP on TIMIT, our largest and most challenging dataset; we use 8-bit LM-HALP (SVRG
and SGD) on top of 8-bit LP-RFFs, and compare to full-precision SGD training. For LM-HALP based training,
we perform the bit centering and rescaling operation after each training epoch. Under this setting, we show that
when the number of features is at least 10,000, the performance of both versions of HALP closely matches that of
full-precision training.

D.7.3 Double Sampling

We perform some initial experiments using the double sampling method of Zhang et al. (2017). In particular,
we use a diﬀerent random quantization of the LP-RFFs on the “forward pass” of our algorithm than in the
“backward pass.” In our initial experiments with double sampling, as shown in Figure 19, we did not observe
noticeable improvements in performance. These experiments were on the YearPred dataset with the Gaussian
kernel. We leave a more extended investigation of these gradient bias reduction methods for future work.

E EXTENDED RELATED WORK

Generalization Performance of Kernel Approximation Methods From a theoretical perspective, there
has been a lot of work analyzing the generalization performance of kernel approximation methods (Rahimi

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 18: Low-precision vs. full-precision training algorithms on TIMIT using 8-bit LP-RFFs.

Figure 19: Comparison of LP-RFFs with and without double sampling on the YearPred dataset.

and Recht, 2008; Cortes et al., 2010; Sutherland and Schneider, 2015; Rudi and Rosasco, 2017; Avron et al.,
2017; Li et al., 2018). The work most relevant to ours is that of Avron et al. (2017), which deﬁnes ∆-spectral
approximation and bounds the generalization performance of kernel approximation methods in terms of ∆. This
approach diﬀers from works which evaluate kernel approximation methods in terms of the Frobenius or spectral
norms of their kernel approximation matrices (Cortes et al., 2010; Gittens and Mahoney, 2016; Yang et al., 2014;
Sutherland and Schneider, 2015; Yu et al., 2016; Dao et al., 2017). Our work shows the promise of Avron et al.’s
approach, and builds upon it.

Large-Scale Kernel Experiments On the topic of scaling kernel methods to large datasets, there have been
a few notable recent papers. Tu et al. (2016) propose a distributed block coordinate descent method for solving
large-scale least squares problems using the Nystr¨om method or RFFs. The recent work of May et al. (2017) uses
a single GPU to train large RFF models on speech recognition datasets, showing comparable performance to
fully-connected deep neural networks. That work was limited by the number of features that could ﬁt on a single
GPU, and thus our proposed method could help scale these results.

Low-Precision Random Fourier Features for Memory-Constrained
Kernel Approximation

9
1
0
2
 
r
a

M
 
0
2
 
 
]

G
L
.
s
c
[
 
 
2
v
5
5
1
0
0
.
1
1
8
1
:
v
i
X
r
a

Jian Zhang∗

Avner May∗

Tri Dao

Christopher R´e

Stanford University

Abstract

We investigate how to train kernel approxi-
mation methods that generalize well under
a memory budget. Building on recent theo-
retical work, we deﬁne a measure of kernel
approximation error which we ﬁnd to be more
predictive of the empirical generalization per-
formance of kernel approximation methods
than conventional metrics. An important con-
sequence of this deﬁnition is that a kernel
approximation matrix must be high rank to
attain close approximation. Because storing a
high-rank approximation is memory intensive,
we propose using a low-precision quantiza-
tion of random Fourier features (LP-RFFs)
to build a high-rank approximation under a
memory budget. Theoretically, we show quan-
tization has a negligible eﬀect on generaliza-
tion performance in important settings. Em-
pirically, we demonstrate across four bench-
mark datasets that LP-RFFs can match the
performance of full-precision RFFs and the
Nystr¨om method, with 3x-10x and 50x-460x
less memory, respectively.

1 INTRODUCTION

Kernel methods are a powerful family of machine learn-
ing methods. A key technique for scaling kernel meth-
ods is to construct feature representations whose inner
products approximate the kernel function, and then
learn a linear model with these features; important ex-
amples of this technique include the Nystr¨om method
(Williams and Seeger, 2000) and random Fourier fea-
tures (RFFs) (Rahimi and Recht, 2007). Unfortunately,
a large number of features are typically needed for at-
taining strong generalization performance with these

∗Equal contribution.

Proceedings of the 22nd International Conference on Ar-
tiﬁcial Intelligence and Statistics (AISTATS) 2019, Naha,
Okinawa, Japan. PMLR: Volume 89. Copyright 2019 by
the author(s).

methods on big datasets (Rahimi and Recht, 2008; Tu
et al., 2016; May et al., 2017). Thus, the memory re-
quired to store these features can become the training
bottleneck for kernel approximation models. In this
paper we work to alleviate this memory bottleneck by
optimizing the generalization performance for these
methods under a ﬁxed memory budget.

To gain insight into how to design more memory-
eﬃcient kernel approximation methods, we ﬁrst in-
vestigate the generalization performance vs. memory
utilization of Nystr¨om and RFFs. While prior work
(Yang et al., 2012) has shown that the Nystr¨om method
generalizes better than RFFs under the the same num-
ber of features, we demonstrate that the opposite is
true under a memory budget. Strikingly, we observe
that 50,000 standard RFFs can achieve the same held-
out accuracy as 20,000 Nystr¨om features with 10x less
memory on the TIMIT classiﬁcation task. Further-
more, this cannot be easily explained by the Frobenius
or spectral norms of the kernel approximation error ma-
trices of these methods, even though these norms are
the most common metrics for evaluating kernel approx-
imation methods (Gittens and Mahoney, 2016; Yang
et al., 2014; Sutherland and Schneider, 2015; Yu et al.,
2016; Dao et al., 2017); the above Nystr¨om features
attain 1.7x smaller Frobenius error and 17x smaller
spectral error compared to the RFFs. This observa-
tion suggests the need for a more reﬁned measure of
kernel approximation error—one which better aligns
with generalization performance, and can thus better
guide the design of new approximation methods.

Building on recent theoretical work (Avron et al., 2017),
we deﬁne a measure of approximation error which we
ﬁnd to be much more predictive of empirical generaliza-
tion performance than the conventional metrics. In par-
ticular, we extend Avron et al.’s deﬁnition of ∆-spectral
approximation to our deﬁnition of (∆1, ∆2)-spectral ap-
proximation by decoupling the two roles played by ∆
in the original deﬁnition.1 This decoupling reveals that

1The original deﬁnition uses the same scalar ∆ to upper
and lower bound the approximate kernel matrix in terms
of the exact kernel matrix in the semideﬁnite order.

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Rd, kernel features
Table 1: Memory utilization for kernel approximation methods. We consider data x
Rm, mini-batch size s, # of classes c (for regression/binary classiﬁcation c = 1). We assume full-precision
z(x)
numbers are 32 bits. We measure a method’s memory utilization as the sum of the three components in this table.

∈

∈

Approximation Method

Nystr¨om
RFFs
Circulant RFFs
Low-precision RFFs, b bits (ours)

Feature generation Feature mini-batch Model parameters
32(md + m2)
32md
32m
32m

32ms
32ms
32ms
bms

32mc
32mc
32mc
32mc

∆1 and ∆2 impact generalization diﬀerently, and can
together much better explain the relative generalization
performance of Nystr¨om and RFFs than the original
∆, or the Frobenius or spectral errors. This (∆1, ∆2)
deﬁnition has an important consequence—in order for
an approximate kernel matrix to be close to the exact
kernel matrix, it is necessary for it to be high rank.

Motivated by the above connection between rank and
generalization performance, we propose using low-
precision random Fourier features (LP-RFFs) to attain
a high-rank approximation under a memory budget.
Speciﬁcally, we store each random Fourier feature in a
low-precision ﬁxed-point representation, thus achieving
a higher-rank approximation with more features in the
same amount of space. Theoretically, we show that
when the quantization noise is much smaller than the
regularization parameter, using low precision has negli-
gible eﬀect on the number of features required for the
approximate kernel matrix to be a (∆1, ∆2)-spectral
approximation of the exact kernel matrix. Empiri-
cally, we demonstrate across four benchmark datasets
(TIMIT, YearPred, CovType, Census) that in the mini-
batch training setting, LP-RFFs can match the perfor-
mance of full-precision RFFs (FP-RFFs) as well as the
Nystr¨om method, with 3x-10x and 50x-460x less mem-
ory, respectively. These results suggest that LP-RFFs
could be an important tool going forward for scaling
kernel methods to larger and more challenging tasks.

The rest of this paper is organized as follows: In Section
2 we compare the performance of the Nystr¨om method
and RFFs in terms of their training memory footprint.
In Section 3 we present a more reﬁned measure of kernel
approximation error to explain the relative performance
of Nystr¨om and RFFs. We introduce the LP-RFF
method and corresponding analysis in Section 4, and
present LP-RFF experiments in Section 5. We review
related work in Section 6, and conclude in Section 7.

2 NYSTR ¨OM VS. RFFS: AN

EMPIRICAL COMPARISON

tion of Nystr¨om and RFFs. We begin by reviewing
the memory utilization for these kernel approximation
methods in the mini-batch training setting; this is a
standard setting for training large-scale kernel approxi-
mation models (Huang et al., 2014; Yang et al., 2015;
May et al., 2017), and it is the setting we will be us-
ing to evaluate the diﬀerent approximation methods
(Sections 2.2, 5.1). We then show that RFFs outper-
form Nystr¨om given the same training memory budget,
even though the opposite is true given a budget for
the number of features (Yang et al., 2012). Lastly, we
demonstrate that the Frobenius and spectral norms of
the kernel approximation error matrix align poorly with
generalization performance, suggesting the need for a
more reﬁned measure of approximation error for eval-
uating the quality of a kernel approximation method;
we investigate this in Section 3.

For background on RFFs and the Nystr¨om method,
and for a summary of our notation, see Appendix A.

2.1 Memory Utilization

The optimization setting we consider is mini-batch
training over kernel approximation features. To un-
derstand the training memory footprint, we present in
Table 1 the memory utilization of the diﬀerent parts
of the training pipeline. The three components are:

1. Feature generation: Computing m RFFs over data
in Rd requires a random projection matrix W
Rm
×
points” ˆxi

∈
d. The Nystr¨om method stores m “landmark
m.

Rd, and a projection matrix in Rm
×

2. Feature mini-batch: Kernel approximation features
Rm for all xi in a mini-batch are stored.2

z(xi)

∈

∈

3. Model parameters: For binary classiﬁcation and
regression, the linear model learned on the z(x) fea-
Rm; for c-class classiﬁcation,
tures is a vector θ
it is a matrix θ
×

∈
Rm

c.

∈

In this work we focus on reducing the memory occupied
by the mini-batches of features, which can occupy a

To inform our design of memory-eﬃcient kernel approx-
imation methods, we ﬁrst perform an empirical study
of the generalization performance vs. memory utiliza-

2For simplicity, we ignore the memory occupied by the
mini-batches of d-dim. inputs and c-dim. outputs, as gener-
ally the number of kernel approx. features m

d, c.

(cid:29)

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

(a)

(b)

(c)

(d)

Figure 1: Generalization performance of full-precision RFFs and Nystr¨om with respect to the number of features
and training memory footprint on TIMIT (a,b). Nystr¨om performs better for a ﬁxed number of features, while
RFFs perform better under a memory budget. We also see that the generalization performance of these methods
does not align well with the Frobenius or spectral norms of their respective kernel approximation error matrices
(c,d). For results on YearPred, CovType, and Census, see Appendix D.2.

signiﬁcant fraction of the training memory. Our work
is thus orthogonal to existing work which has shown
how to reduce the memory utilization of the feature
generation (Le et al., 2013; Yu et al., 2015) and the
model parameters (Sainath et al., 2013a; Sindhwani
et al., 2015; De Sa et al., 2018) (e.g., using structured
matrices or low precision). Throughout this paper, we
measure the memory utilization of a kernel approxima-
tion method as the sum of the above three components.

2.2 Empirical Comparison

We now compare the generalization performance of
RFFs and the Nystr¨om method in terms of their train-
ing memory footprint. We demonstrate that RFFs
can outperform the Nystr¨om method given a memory
budget, and show that the diﬀerence in performance
between these methods cannot be explained by the
Frobenius or spectral norms of their kernel approxima-
tion error matrices.

In experiments across four datasets (TIMIT, YearPred,
CovType, Census (Garofolo et al., 1993; Dheeru and
Karra Taniskidou, 2017)), we use up to 20k Nystr¨om
features and 400k RFFs to approximate the Gaus-
sian kernel;3 we train the models using mini-batch
stochastic gradient descent with early stopping, with
a mini-batch size of 250. We present results averaged
from three random seeds, with error bars indicating
standard deviations (for further experiment details,
see Appendix D.2). In Figure 1(a) we observe that
as a function of the number of kernel approximation
features the Nystr¨om method generally outperforms
RFFs, though the gap narrows as m approaches 20k.
However, we see in Figure 1(b) that RFFs attain better
generalization performance as a function of memory.
Interestingly, the relative performance of these meth-

ods cannot simply be explained by the Frobenius or
spectral norms of the kernel approximation error ma-
trices;4 in Figure 1(c,d) we see that there are many
cases in which the RFFs attain better generalization
performance, in spite of having larger Frobenius or
spectral approximation error. This is a phenomenon
we observe on other datasets as well (Appendix D.2).
This suggests the need for a more reﬁned measure of the
approximation error of a kernel approximation method,
which we discuss in the following section.

3 A REFINED MEASURE OF
KERNEL APPROX. ERROR

To explain the important diﬀerences in performance be-
tween Nystr¨om and RFFs, we deﬁne a more reﬁned mea-
sure of kernel approximation error—(∆1, ∆2)-spectral
approximation. Our deﬁnition is an extension of Avron
et al.’s deﬁnition of ∆-spectral approximation, in which
we decouple the two roles played by ∆ in the original
deﬁnition. This decoupling allows for a more ﬁne-
grained understanding of the factors inﬂuencing the
generalization performance of kernel approximation
methods, both theoretically and empirically. Theoret-
ically, we present a generalization bound for kernel
approximation methods in terms of (∆1, ∆2) (Sec. 3.1),
and show that ∆1 and ∆2 inﬂuence the bound in dif-
ferent ways (Prop. 1). Empirically, we show that ∆1
and ∆2 are more predictive of the Nystr¨om vs. RFF
performance than the ∆ from the original deﬁnition,
and the Frobenius and spectral norms of the kernel
approximation error matrix (Sec. 3.2, Figure 2). An
important consequence of the (∆1, ∆2) deﬁnition is
that attaining a small ∆1 requires a large number of
features; we leverage this insight to motivate our pro-
posed method, low-precision random Fourier features,
in Section 4.

3We consider diﬀerent ranges for the number of Nystr¨om
vs. RFF features because the memory footprint for training
with 400k RFFs is similar to 20k Nystr¨om features.

˜K,
4We consider the Frobenius and spectral norms of K
where K and ˜K are the exact and approximate kernel
matrices for 20k randomly sampled heldout points.

−

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

3.1

(∆1, ∆2)-spectral Approximation

We begin by reviewing what it means for a matrix
A to be a ∆-spectral approximation of a matrix B
(Avron et al., 2017). We then extend this deﬁnition
to (∆1, ∆2)-spectral approximation, and bound the
generalization performance of kernel approximation
methods in terms of ∆1 and ∆2 in the context of ﬁxed
design kernel ridge regression.
0, a symmetric matrix A is a
Deﬁnition 1. For ∆
∆-spectral approximation of another symmetric matrix
(1 + ∆)B.
B if (1

∆)B

≥

A

−

(cid:22)

(cid:22)

We extend this deﬁnition by allowing for diﬀerent values
of ∆ in the left and right inequalities above:
0, a symmetric matrix
Deﬁnition 2. For ∆1, ∆2 ≥
A is a (∆1, ∆2)-spectral approximation of another
(1 + ∆2)B.
symmetric matrix B if (1

−
Throughout the text, we will use ∆ to denote the
variable in Def. 1, and (∆1, ∆2) to denote the variables
In our discussions and experiments, we
in Def. 2.
always consider the smallest ∆, ∆1, ∆2 satisfying the
above deﬁnitions; thus, ∆ = max(∆1, ∆2).

∆1)B

(cid:22)

(cid:22)

A

In the paragraphs that follow we present generalization
bounds for kernel approximation models in terms of
∆1 and ∆2 in the context of ﬁxed design kernel ridge
regression, and demonstrate that ∆1 and ∆2 inﬂuence
generalization in diﬀerent ways (Prop. 1). We consider
the ﬁxed design setting because its expected generaliza-
tion error has a closed-form expression, which allows us
to analyze generalization performance in a ﬁne-grained
fashion. For an overview of ﬁxed design kernel ridge
regression, see Appendix A.3.

×

n, a regularization parameter λ
(xi, yi)

In the ﬁxed design setting, given a kernel matrix K
∈
Rn
0, and a set of
≥
n
labeled points
i=1 where the observed labels
}
yi = ¯yi + (cid:15)i are randomly perturbed versions of the true
(cid:3) = σ2 <
R ((cid:15)i independent, E [(cid:15)i] = 0, E (cid:2)(cid:15)2
labels ¯yi
), it is easy to show (Alaoui and Mahoney, 2015) that

∞
the optimal kernel regressor5 fK has expected error

∈

{

i

(fK) =

R

λ2
n

¯yT (K + λI)−

2 ¯y +

tr

K 2(K + λI)−

2(cid:17)

,

(cid:16)

σ2
n

where ¯y = (¯y1, . . . , ¯yn) is the vector of true labels.

This closed-form expression for generalization error
allows us to bound the expected loss
(f ˜K) of a kernel
ridge regression model f ˜K learned using an approximate
kernel matrix ˜K in place of the exact kernel matrix K.
In particular, if we deﬁne

R

(fK) :=

(cid:98)
R

λ
n

¯yT (K + λI)−

1 ¯y +

tr

K(K + λI)−

1(cid:17)

,

(cid:16)

σ2
n

5fK (x) = (cid:80)

i αik(x, xi) for α = (K + λI)−1y.

∞

(fK), we can bound the

which is an upper bound on
R
expected loss of f ˜K as follows:
Proposition 1. (Extended from (Avron et al., 2017))
Suppose ˜K + λI is (∆1, ∆2)-spectral approximation of
0. Let m denote
[0, 1) and ∆2 ≥
K + λI, for ∆1 ∈
the rank of ˜K, and let fK and f ˜K be the kernel ridge
regression estimators learned using these matrices, with
regularizing constant λ
0 and label noise variance
σ2 <
. Then

≥

(f ˜K)

R

≤

1

1
∆1

(cid:98)
R

−

(fK) +

∆2
1 + ∆2

m
n

σ2.

(1)

We include a proof in Appendix B.1. This result shows
that smaller values for ∆1 and ∆2 imply tighter bounds
on the generalization performance of the model trained
with ˜K. We can see that as ∆1 approaches 1 the bound
diverges, and as ∆2 approaches
the bound plateaus.
We leverage this generalization bound to understand
the diﬀerence in performance between Nystr¨om and
RFFs (Sec. 3.2), and to motivate and analyze our pro-
posed low-precision random Fourier features (Sec. 4).

∞

Remark The generalization bound in Prop. 1 as-
sumes the regressor fK is computed via the closed-
form solution for kernel ridge regression. However, in
Sections 4-5 we focus on stochastic gradient descent
(SGD) training for kernel approximation models. Be-
cause SGD can also ﬁnd the model which minimizes
the regularized empirical loss (Nemirovski et al., 2009),
the generalization results carry over to our setting.

3.2 Revisiting Nystr¨om vs. RFF Comparison

In this section we show that the values of ∆1 and ∆2
such that the approximate kernel matrix is a (∆1, ∆2)-
spectral approximation of the exact kernel matrix cor-
relate better with generalization performance than the
original ∆, and the Frobenius and spectral norms of
the kernel approximation error; we measure correlation
using Spearman’s rank correlation coeﬃcient ρ.

To study the correlation of these metrics with gen-
eralization performance, we train Nystr¨om and RFF
models for many feature dimensions on the Census
regression task, and on a subsampled version of 20k
train and heldout points from the CovType classiﬁca-
tion task. We choose these small datasets to be able
to compute the various measures of kernel approxima-
tion error over the entire heldout set. We measure the
˜K, and the ∆
spectral and Frobenius norms of K
and (∆1, ∆2) values between K + λI and ˜K + λI (λ
chosen via cross-validation), where K and ˜K are the
exact and approximate kernel matrices for the heldout
set. For more details about these experiments and how
we compute ∆ and (∆1, ∆2), see Appendix D.3.

−

In Figure 2, we plot the generalization performance on
these tasks as a function of these metrics; while the

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Figure 2: The correlation between generalization performance and diﬀerent measures of kernel approximation
error for the full-precision RFF and Nystr¨om methods. We see that generalization performance aligns well with
1/(1
∆1) (Spearman rank correlation coeﬃcient ρ = 0.958), while aligning poorly with ∆ and the spectral and
squared Frobenius norms of the kernel approximation error matrix. See Appendix D.3 for results on CovType.

−

1

1

−

−

1
∆1

1
∆1

original ∆ and the Frobenius and spectral norms gener-
ally do not align well with generalization performance,
we see that
attains a
does. Speciﬁcally,
Spearman rank correlation coeﬃcient of ρ = 0.958,
while squared Frobenius norm, spectral norm, and the
original ∆ attain values of 0.594, 0.540, and 0.759.6
In Appendix D.3 we show these trends are robust to
diﬀerent kernel approximation methods and datasets.
For example, we show that while other approximation
methods (e.g., orthogonal RFFs (Yu et al., 2016)), like
Nystr¨om, can attain much lower Frobenius and spectral
error than standard RFFs, this does not translate to
improved ∆1 or heldout performance. These results
mirror the generalization bound in Proposition 1, which
grows linearly with
. For simplicity, we ignore
the role of ∆2 here, as ∆1 appears to be suﬃcient for
explaining the main diﬀerences in performance between
these full-precision methods.7 In Sections 4.2 and 5.2,
however, we show that ∆2 has a large inﬂuence on
generalization performance for low-precision features.

1
∆1

−

1

Now that we have seen that ∆1 has signiﬁcant theoreti-
cal and empirical impact on generalization performance,
it is natural to ask how to construct kernel approxi-
mation matrices that attain small ∆1. An important
consequence of the deﬁnition of ∆1 is that for ˜K +λI to
have small ∆1 relative to K + λI, ˜K must be high-rank ;
λm+1(K)
in particular, a necessary condition is ∆1 ≥
λm+1(K)+λ ,
where m is the rank of ˜K and λi(K) is the ith largest
eigenvalue of K.8 This sets a lower bound on the rank
necessary for ˜K to attain small ∆1 which holds regard-
less of the approximation method used, motivating us
to design high-rank kernel approximation methods.

6 One reason ∆1 correlates better than ∆ is because
when ∆2 > ∆1, ∆ = max(∆1, ∆2) hides the value of ∆1.
This shows why decoupling the two roles of ∆ is important.
∆1) aligns well with performance, it is
not perfect—for a ﬁxed ∆1, Nystr¨om generally performs
slightly better than RFFs. In App. D.3.1 we suggest this is
because Nystr¨om has ∆2 = 0 while RFFs has larger ∆2.

7While 1/(1

−

8By deﬁnition, (K + λI)(1

˜K + λI. By Weyl’s
λi( ˜K) + λ.
inequality this implies
If ˜K is rank m, then λm+1( ˜K) = 0, and the result follows.

−
(cid:22)
i (λi(K) + λ)(1

∆1)

∆1)

−

≤

∀

4 LOW-PRECISION RANDOM

FOURIER FEATURES (LP-RFFS)

Taking inspiration from the above-mentioned connec-
tion between the rank of the kernel approximation
matrix and generalization performance, we propose
low-precision random Fourier features (LP-RFFs) to
create a high-rank approximation matrix under a mem-
ory budget. In particular, we quantize each random
Fourier feature to a low-precision ﬁxed-point represen-
tation, thus allowing us to store more features in the
same amount of space. Theoretically, we show that
when the quantization noise is small relative to the
regularization parameter, using low precision has mini-
mal impact on the number of features required for the
approximate kernel matrix to be a (∆1, ∆2)-spectral
approximation of the exact kernel matrix; by Propo-
sition 1, this implies a bound on the generalization
performance of the model trained on the low-precision
features. At the end of this section (Section 4.3), we
discuss a memory-eﬃcient implementation for training
a full-precision model on top of LP-RFFs.

4.1 Method Details

∈

−

(cid:112)

(cid:112)

(cid:112)

2/m,

i x + ai)

2/m cos(wT

1 sub-intervals of equal size r =

2/m] for the RFF vector z(x)

The core idea behind LP-RFFs is to use b bits to store
each RFF, instead of 32 or 64 bits. We implement
this with a simple stochastic rounding scheme. We use
the parametrization zi(x) =
∈
Rm
[
−
(Rahimi and Recht, 2007), and divide this interval
into 2b
1 . We
−
then randomly round each feature zi(x) to either the
top or bottom of the sub-interval [z, z] containing it,
in such a way that the expected value is equal to zi(x);
speciﬁcally, we round zi(x) to z with probability z
z
−
z
z
−
and to z with probability z
z
z . The variance of this
z
stochastic rounding scheme is at most δ2
b /m, where
b := 2/(2b
δ2
1)2 (Prop. 7 in App. C.2). For each low-
precision feature ˜zi(x) we only need to store the integer
j
2/m + jr, which
−
takes b bits. Letting ˜Z
m denote the matrix

1] such that ˜zi(x) =

2√2/m
2b

[0, 2b

Rn

(cid:112)

−
−

−

−

∈

×

∈

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 3: Empirical validation of Theorem 2. In the left and middle plots (shared legend), we see that as the #
of features grows, LP-RFFs approach ∆1 = 0, but plateau at larger ∆2 values (at most δ2
b /λ, marked by dashed
lines) for very low precisions. In the right plot we see that the larger λ is, the lower the precision at which using
low precision does not impact ∆2. For ∆1 and ∆2 vs. # features plots on CovType, see Appendix D.4.

of quantized features, we call ˜K = ˜Z ˜Z T an m-feature
b-bit LP-RFF approximation of a kernel matrix K.

As a way to further reduce the memory footprint during
training, we leverage existing work on using circulant
random matrices (Yu et al., 2015) for the RFF random
projection matrix to only occupy 32m bits.9 All our
LP-RFF experiments use circulant projections.

4.2 Theoretical Results

In this section we show quantization has minimal im-
pact on the number of features required to guarantee
strong generalization performance in certain settings.
We do this in the following theorem by lower bounding
the probability that ˜K + λI is a (∆1, ∆2)-spectral ap-
proximation of K + λI, for the LP-RFF approximation
˜K using m features and b bits per feature.10
˜K be an m-feature b-bit LP-
Theorem 2. Let
RFF approximation of a kernel matrix K, assume
1)2, and deﬁne a :=
b In)(cid:1). Then for any ∆1 ≥
0,

:= 2/(2b

≥

−

δ2
K
λ
b
(cid:107)
(cid:107) ≥
8 tr (cid:0)(K + λIn)−
1(K + δ2
δ2
b /λ,
∆2 ≥
(cid:104)
P
(1

∆1)(K + λI)

(cid:105)
(1 + ∆2)(K + λI)

˜K + λI

−
(cid:32)
exp

(cid:18)

1

a

−

(cid:22)

(cid:19)

(cid:22)
(cid:32)

+exp

m∆2
1
−
4n
λ (1 + 2
3 ∆1)

m(∆2 −

−
4n
λ (1 + 2

3 (∆2 −

δ2
λ )2
b
δ2
λ ))
b

≥
(cid:33)(cid:33)
.

The proof of Theorem 2 is in Appendix C. To provide
more intuition we present the following corollary:
Corollary 2.1. Assuming ∆1 ≤
(1
∆1)(K+λIn)
(cid:22)
−
(cid:16) a
8n/λ
if m
log
∆2
ρ
1
it follows that ˜K + λIn
probability at least 1

3/2, it follows that
˜K+λIn with probability at least 1
ρ
−
(cid:17)
(cid:2) δ2
(cid:3),
λ , 3
. Similarly, assuming ∆2 ∈
2
(1 + ∆2)(K + λIn) with
8n/λ
b /λ)2 log
δ2

(cid:22)
ρ if m

(cid:16) a
ρ

(∆2

≥

(cid:17)

.

b

−

≥

−

9Technically, m additional bits are needed to store a

vector of Rademacher random variables in

10This theorem extends directly to the quantization of
Rn×m with
2/m].

any kernel approximation feature matrix Z
i.i.d. columns and with entries in [

∈
2/m, (cid:112)

(cid:112)

1, 1

m.
}

{−

−

The above corollary suggests that using low precision
has negligible eﬀect on the number of features necessary
to attain a certain value of ∆1, and also has negligible
eﬀect for ∆2 as long as δ2
b /λ

∆2.

(cid:28)

Validation of Theory We now empirically validate
the following two predictions made by the above theory:
(1) Using low precision has no eﬀect on the asymptotic
behavior of ∆1 as the number of features m approaches
inﬁnity, while having a signiﬁcant eﬀect on ∆2 when
δ2
, ∆1 converges
b /λ is large. Speciﬁcally, as m
to 0 for any precision b, while ∆2 converges to a value
upper bounded by δ2
b /λ.11 (2) If δ2
∆2, using b-
bit precision will have negligible eﬀect on the number of
features required to attain this ∆2. Thus, the larger λ
is, the smaller the impact of using low precision should
be on ∆2.

→ ∞

b /λ

(cid:28)

To validate the ﬁrst prediction, in Figure 3 (left, middle)
we plot ∆1 and ∆2 as a function of the number of
features m, for FP-RFFs and LP-RFFs; we use the
same λ as in the Section 2 Census experiments. We
show that for large m, all methods approach ∆1 = 0;
4 the LP-RFFs converge
in contrast, for precisions b
to a ∆2 value much larger than 0, and slightly less than
δ2
b /λ (marked by dashed lines).

≤

To validate the second prediction, in Figure 3 (right)
we plot ∆2 vs. precision for various values of λ, using
m = 2000 features for all precisions; we do this on
a random subsample of 8000 Census training points.
We see that for large enough precision b, the ∆2 is
very similar to the value from using 32-bit precision.
Furthermore, the larger the value of λ, the smaller the
precision b can be without signiﬁcantly aﬀecting ∆2.

11By Lemma 3 in Appendix C, we know that E

K + D for a diagonal matrix D satisfying 0
where D is independent of m. As m
(K + λI)−1/2D(K + λI)−1/2
to

→ ∞
δ2
b /λ.

(cid:107) ≤

(cid:107)

(cid:104)

˜Z ˜Z T (cid:105)
=
δ2
b In,
D
(cid:22)
, ∆2 converges

(cid:22)

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Figure 4: Generalization performance of FP-Nystr¨om, FP-RFFs, circulant FP-RFFs, and LP-RFFs with respect
to memory (sum of components in Table 1) on TIMIT, YearPred and CovType. LP-RFFs attain the best
performance across a wide range of memory budgets. The same trend holds for Census in Appendix D.5.

Table 2: The compression ratios achieved by LP-RFFs
relative to the best performing full-precision baselines.

FP-RFFs Cir. FP-RFFs Nystr¨om

Census
YearPred
Covtype
TIMIT

2.9x
10.3x
4.7x
5.1x

15.6x
7.6x
3.9x
2.4x

63.2x
461.6x
237.2x
50.9x

4.3

Implementation Considerations

In this paper, we focus on training full-precision models
using mini-batch training over low-precision features.
Here we describe how this mixed-precision optimization
can be implemented in a memory-eﬃcient manner.

Naively, to multiply the low-precision features with the
full-precision model, one could ﬁrst cast the features to
full-precision, requiring signiﬁcant intermediate mem-
ory. We can avoid this by casting in the processor
registers. Speciﬁcally, to perform multiplication with
the full-precision model, the features can be streamed
to the processor registers in low precision, and then cast
to full precision in the registers. In this way, only the
features in the registers exist in full precision. A similar
technique can be applied to avoid intermediate memory
in the low-precision feature computation—after a full-
precision feature is computed in the registers, it can be
directly quantized in-place before it is written back to
main memory. We leave a more thorough investigation
of these systems issues for future work.

5 EXPERIMENTS

In this section, we empirically demonstrate the per-
formance of LP-RFFs under a memory budget, and
show that (∆1, ∆2) are predictive of generalization
performance. We show in Section 5.1 that LP-RFFs
can attain the same performance as FP-RFFs and
Nystr¨om, while using 3x-10x and 50x-460x less memory.
In Section 5.2, we show the strong alignment between
(∆1, ∆2) and generalization performance, once again
validating the importance of this measure.

5.1 Empirical Evaluation of LP-RFFs

1, 2, 4, 8, 16

To empirically demonstrate the generalization perfor-
mance of LP-RFFs, we compare their performance
to FP-RFFs, circulant FP-RFFs, and Nystr¨om fea-
tures for various memory budgets. We use the same
datasets and protocol as the large-scale Nystr¨om vs.
RFF comparisons in Section 2.2; the only signiﬁcant ad-
ditions here are that we also evaluate the performance
of circulant FP-RFFs, and LP-RFFs for precisions
b
. Across our experiments, we com-
}
pute the total memory utilization as the sum of all
the components in Table 1. We note that all our low-
precision experiments are done in simulation, which
means we store the quantized values as full-precision
ﬂoating-point numbers. We report average results from
three random seeds, with error bars showing standard
deviations. For more details about our experiments, see
Appendix D.5. We use the above protocol to validate
the following claims on the performance of LP-RFFs.12

∈ {

LP-RFFs can outperform full-precision fea-
tures under memory budgets.
In Figure 4, we
plot the generalization performance for these experi-
ments as a function of the total training memory for
TIMIT, YearPred, and CovType. We observe that LP-
RFFs attain better generalization performance than
the full-precision baselines under various memory bud-
gets. To see results for all precisions, as well as results
on additional benchmark datasets (Census, Adult, Cod-
RNA, CPU, Forest) from the UCI repository (Dheeru
and Karra Taniskidou, 2017), see Appendix D.5.

LP-RFFs can match the performance of full-
precision features with signiﬁcantly less mem-
ory.
In Table 2 we present the compression ratios we
achieve with LP-RFFs relative to the best performing
baseline methods. For each baseline (FP-RFFs, circu-
lant FP-RFFs, Nystr¨om), we ﬁnd the smallest LP-RFF
model, as well as the smallest baseline model, which
4 relative performance of the best-
attain within 10−
performing baseline model; we then compute the ratio

12 Our code: github.com/HazyResearch/lp_rffs.

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

∆1), ∆2)
Figure 5: Generalization perf. vs. ∆2 (left plots, shared legend), and vs. 1/(1
(right plots, shared legend). Left: heldout performance deteriorates as ∆2 gets larger due to lower precision.
Right: max (1/(1
∆1), ∆2) aligns well with performance across LP-RFF precisions (Spearman rank correlation
coeﬃcient ρ = 0.959), while 1/(1

∆1) aligns poorly (ρ = 0.403). See Appendix D.6 for CovType results.

∆1) and max (1/(1

−

−

−

−

of the memory used by these two models (baseline/LP-
RFF) for three random seeds, and report the aver-
age. We can see that LP-RFFs demonstrate signiﬁcant
memory saving over FP-RFFs, circulant FP-RFFs, and
Nystr¨om, attaining compression ratios of 2.9x-10.3x,
2.4x-15.6x, and 50.9x-461.6x, respectively.

5.2 Generalization Performance vs. (∆1, ∆2)

In this section we show that ∆1 and ∆2 are together
quite predictive of generalization performance across all
the kernel approximation methods we have discussed.
We ﬁrst show that performance deteriorates for larger
∆2 values as we vary the precision of the LP-RFFs,
when keeping the number of features constant (thereby
limiting the inﬂuence of ∆1 on performance). We then
combine this insight with our previous observation
(Section 3.2) that performance scales with
in the
full-precision setting by showing that across precisions
(cid:1). For
the performance aligns well with max (cid:0)
these experiments, we use the same protocol as for the
(∆1, ∆2) experiments in Section 3.2, but additionally
consider LP-RFFs for precisions b

, ∆2

1
∆1

1
∆1

−

−

1

1

.

1, 2, 4, 8, 16
}

∈ {

We show in Figure 5 (left plots) that for a ﬁxed number
of random Fourier features, performance deteriorates
as ∆2 grows. As we have shown in Figure 3 (left), ∆1
is primarily governed by the rank of the approxima-
tion matrix, and thus holding the number of features
constant serves as a proxy for holding ∆1 roughly con-
stant. This allows us to isolate the impact of ∆2 on
performance as we vary the precision.

1

1
∆1

, ∆2

To integrate the inﬂuence of ∆1 and ∆2 on general-
ization performance into a single scalar, we consider
max (cid:0)
(cid:1). In Figure 5 (right plots) we show that
when considering both low-precision and full-precision
features, max (cid:0)
(cid:1) aligns well with performance
(ρ = 0.959, incorporating all precisions), while
aligns poorly (ρ = 0.403).

, ∆2

1
∆1

1
∆1

−

−

−

1

1

In Appendix B we argue that performance scales
roughly as ∆2 instead of as ∆2/(1 + ∆2) (as suggested
by Prop. 1) due to looseness in the Prop. 1 bound.

6 RELATED WORK

Low-Memory Kernel Approximation For RFFs,
there has been work on using structured random pro-
jections (Le et al., 2013; Yu et al., 2015, 2016), and
feature selection (Yen et al., 2014; May et al., 2016) to
reduce memory utilization. Our work is orthogonal, as
LP-RFFs can be used with both. For Nystr¨om, there
has been extensive work on improving the choice of
landmark points, and reducing the memory footprint
in other ways (Kumar et al., 2009; Hsieh et al., 2014;
Si et al., 2014; Musco and Musco, 2017). In our work,
we focus on the eﬀect of quantization on generalization
performance per bit, and note that RFFs are much
more amenable to quantization. For our initial experi-
ments quantizing Nystr¨om features, see Appendix D.7.

Low Precision for Machine Learning There has
been much recent interest in using low precision for
accelerating training and inference of machine learning
models, as well as for model compression (Gupta et al.,
2015; De Sa et al., 2015; Hubara et al., 2016; De Sa
et al., 2018, 2017; Han et al., 2016). There have been
many advances in hardware support for low precision
as well (Jouppi et al., 2017; Caulﬁeld et al., 2017).

This work is inspired by the Nystr¨om vs. RFF exper-
iments in the PhD dissertation of May (2018), and
provides a principled understanding of the prior results.
For more related work discussion, see Appendix E.

7 CONCLUSION

We deﬁned a new measure of kernel approximation error
and demonstrated its close connection to the empirical
and theoretical generalization performance of kernel
approximation methods.
Inspired by this measure,
we proposed LP-RFFs and showed they can attain
improved generalization performance under a memory
budget in theory and in experiments. We believe these
contributions provide fundamental insights into the
generalization performance of kernel approximation
methods, and hope to use these insights to scale kernel
methods to larger and more challenging tasks.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Acknowledgements

We thank Michael Collins for his helpful guidance on
the Nystr¨om vs. RFF experiments in Avner May’s PhD
dissertation (May, 2018), which inspired this work. We
also thank Jared Dunnmon, Albert Gu, Beliz Gunel,
Charles Kuang, Megan Leszczynski, Alex Ratner, Nimit
Sohoni, Paroma Varma, and Sen Wu for their helpful
discussions and feedback on this project.

We gratefully acknowledge the support of DARPA un-
der Nos. FA87501720095 (D3M) and FA86501827865
(SDH), NIH under No. N000141712266 (Mobilize),
NSF under Nos. CCF1763315 (Beyond Sparsity) and
CCF1563078 (Volume to Velocity), ONR under No.
N000141712266 (Unifying Weak Supervision), the
Moore Foundation, NXP, Xilinx, LETI-CEA, Intel,
Google, NEC, Toshiba, TSMC, ARM, Hitachi, BASF,
Accenture, Ericsson, Qualcomm, Analog Devices, the
Okawa Foundation, and American Family Insurance,
and members of the Stanford DAWN project: Intel,
Microsoft, Teradata, Facebook, Google, Ant Finan-
cial, NEC, SAP, and VMWare. The U.S. Government
is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright
notation thereon. Any opinions, ﬁndings, and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reﬂect
the views, policies, or endorsements, either expressed
or implied, of DARPA, NIH, ONR, or the U.S. Gov-
ernment.

References

Ahmed El Alaoui and Michael W. Mahoney. Fast
randomized kernel ridge regression with statistical
guarantees. In NIPS, pages 775–783, 2015.

Haim Avron, Michael Kapralov, Cameron Musco,
Christopher Musco, Ameya Velingker, and Amir
Zandieh. Random Fourier features for kernel ridge
regression: Approximation bounds and statistical
guarantees. In ICML, volume 70 of Proceedings of
Machine Learning Research, pages 253–262. PMLR,
2017.

Adrian M. Caulﬁeld, Eric S. Chung, Andrew Put-
nam, Hari Angepat, Daniel Firestone, Jeremy Fowers,
Michael Haselman, Stephen Heil, Matt Humphrey,
Puneet Kaur, Joo-Young Kim, Daniel Lo, Todd Mas-
sengill, Kalin Ovtcharov, Michael Papamichael, Lisa
Woods, Sitaram Lanka, Derek Chiou, and Doug
Burger. Conﬁgurable clouds. IEEE Micro, 37(3):
52–61, 2017.

Corinna Cortes, Mehryar Mohri, and Ameet Talwalkar.
On the impact of kernel approximation on learning
accuracy. In AISTATS, volume 9 of JMLR Proceed-
ings, pages 113–120. JMLR.org, 2010.

Tri Dao, Christopher De Sa, and Christopher R´e. Gaus-
sian quadrature for kernel features. In NIPS, pages
6109–6119, 2017.

Christopher De Sa, Ce Zhang, Kunle Olukotun, and
Christopher R´e. Taming the wild: A uniﬁed analysis
of Hogwild-style algorithms. In NIPS, pages 2674–
2682, 2015.

Christopher De Sa, Matthew Feldman, Christopher R´e,
and Kunle Olukotun. Understanding and optimiz-
ing asynchronous low-precision stochastic gradient
descent. In ISCA, pages 561–574. ACM, 2017.

Christopher De Sa, Megan Leszczynski, Jian Zhang,
Alana Marzoev, Christopher R. Aberger, Kunle
Olukotun, and Christopher R´e. High-accuracy low-
precision training. arXiv preprint arXiv:1803.03383,
2018.

Dua Dheeru and Eﬁ Karra Taniskidou. UCI machine

learning repository, 2017.

M. J. F. Gales. Maximum likelihood linear transforma-
tions for HMM-based speech recognition. Computer
Speech & Language, 12(2):75–98, 1998.

J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus,
D. S. Pallett, and N. L. Dahlgren. DARPA TIMIT
acoustic phonetic continuous speech corpus CDROM,
1993. URL http://www.ldc.upenn.edu/Catalog/
LDC93S1.html.

Alex Gittens and Michael W. Mahoney. Revisiting the
Nystr¨om method for improved large-scale machine
learning. Journal of Machine Learning Research, 17:
117:1–117:65, 2016.

Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan,
and Pritish Narayanan. Deep learning with limited
numerical precision. In ICML, volume 37 of JMLR
Workshop and Conference Proceedings, pages 1737–
1746, 2015.

Song Han, Huizi Mao, and William J. Dally. Deep
compression: Compressing deep neural network with
pruning, trained quantization and Huﬀman coding.
In Proceedings of the International Conference on
Learning Representations (ICLR), 2016.

Cho-Jui Hsieh, Si Si, and Inderjit S. Dhillon. Fast
prediction for large-scale kernel machines. In NIPS,
pages 3689–3697, 2014.

Jie Chen, Lingfei Wu, Kartik Audhkhasi, Brian Kings-
bury, and Bhuvana Ramabhadrari. Eﬃcient one-vs-
one kernel ridge regression for speech recognition. In
ICASSP, pages 2454–2458. IEEE, 2016.

Po-Sen Huang, Haim Avron, Tara N. Sainath, Vikas
Sindhwani, and Bhuvana Ramabhadran. Kernel
methods match deep neural networks on TIMIT.
In ICASSP, pages 205–209. IEEE, 2014.

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Itay Hubara, Matthieu Courbariaux, Daniel Soudry,
Ran El-Yaniv, and Yoshua Bengio. Binarized neural
networks. In NIPS, pages 4107–4115, 2016.

Alessandro Rudi and Lorenzo Rosasco. Generalization
properties of learning with random features. In NIPS,
pages 3218–3228, 2017.

Norman P. Jouppi, Cliﬀ Young, Nishant Patil, David A.
Patterson, et al. In-datacenter performance analysis
of a tensor processing unit. In ISCA, pages 1–12.
ACM, 2017.

Sanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar.
Ensemble Nystr¨om method. In NIPS, pages 1060–
1068, 2009.

Sanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar.
Sampling methods for the Nystr¨om method. Journal
of Machine Learning Research, 13:981–1006, 2012.

Quoc V. Le, Tam´as Sarl´os, and Alexander J. Smola.
Fastfood - computing Hilbert space expansions in log-
linear time. In ICML, volume 28 of JMLR Workshop
and Conference Proceedings, pages 244–252, 2013.

Zhu Li, Jean-Francois Ton, Dino Oglic, and Dino Sejdi-
novic. A uniﬁed analysis of random Fourier features.
arXiv preprint arXiv:1806.09178, 2018.

Avner May. Kernel Approximation Methods for Speech
Recognition. PhD thesis, Columbia University, 2018.

Avner May, Michael Collins, Daniel J. Hsu, and Brian
Kingsbury. Compact kernel models for acoustic mod-
eling via random feature selection. In ICASSP, pages
2424–2428. IEEE, 2016.

Avner May, Alireza Bagheri Garakani, Zhiyun Lu,
Dong Guo, Kuan Liu, Aur´elien Bellet, Linxi Fan,
Michael Collins, Daniel J. Hsu, Brian Kingsbury,
Michael Picheny, and Fei Sha. Kernel approxima-
tion methods for speech recognition. arXiv preprint
arXiv:1701.03577, 2017.

N. Morgan and H. Bourlard. Generalization and pa-
rameter estimation in feedforward nets: Some exper-
iments. In NIPS, 1990.

Tara N. Sainath, Brian Kingsbury, Vikas Sindhwani,
Ebru Arisoy, and Bhuvana Ramabhadran. Low-rank
matrix factorization for deep neural network training
with high-dimensional output targets. In ICASSP,
pages 6655–6659. IEEE, 2013a.

Tara N. Sainath, Brian Kingsbury, Hagen Soltau, and
Bhuvana Ramabhadran. Optimization techniques to
improve training speed of deep neural networks for
large speech tasks. IEEE Trans. Audio, Speech &
Language Processing, 21(11):2267–2276, 2013b.

Si Si, Cho-Jui Hsieh, and Inderjit S. Dhillon. Mem-
ory eﬃcient kernel approximation. In ICML, vol-
ume 32 of JMLR Workshop and Conference Proceed-
ings, pages 701–709, 2014.

Vikas Sindhwani, Tara N. Sainath, and Sanjiv Kumar.
Structured transforms for small-footprint deep learn-
ing. In NIPS, pages 3088–3096, 2015.

Dougal J. Sutherland and Jeﬀ G. Schneider. On the
In UAI, pages

error of random Fourier features.
862–871. AUAI Press, 2015.

Joel A. Tropp. An introduction to matrix concentration
inequalities. Foundations and Trends in Machine
Learning, 8(1-2):1–230, 2015.

Stephen Tu, Rebecca Roelofs, Shivaram Venkatara-
man, and Benjamin Recht. Large scale kernel learn-
ing using block coordinate descent. arXiv preprint
arXiv:1602.05310, 2016.

Yuting Wei, Fanny Yang, and Martin J. Wainwright.
Early stopping for kernel boosting algorithms: A
general analysis with localized complexities. In NIPS,
pages 6067–6077, 2017.

Cameron Musco and Christopher Musco. Recursive
sampling for the Nystr¨om method. In NIPS, pages
3836–3848, 2017.

Christopher K. I. Williams and Matthias W. Seeger.
Using the Nystr¨om method to speed up kernel ma-
chines. In NIPS, pages 682–688. MIT Press, 2000.

Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan,
and Alexander Shapiro. Robust stochastic approxi-
mation approach to stochastic programming. SIAM
Journal on Optimization, 19(4):1574–1609, 2009.

Tiberiu Popoviciu. Sur les ´equations alg´ebriques ayant
toutes leurs racines r´eelles. Mathematica, 9:129–145,
1935.

Ali Rahimi and Benjamin Recht. Random features for
large-scale kernel machines. In NIPS, pages 1177–
1184, 2007.

Jiyan Yang, Vikas Sindhwani, Haim Avron, and
Michael W. Mahoney. Quasi-Monte Carlo feature
maps for shift-invariant kernels. In Proceedings of the
31th International Conference on Machine Learning,
ICML 2014, Beijing, China, 21-26 June 2014, pages
485–493, 2014.

Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong
Jin, and Zhi-Hua Zhou. Nystr¨om method vs ran-
dom Fourier features: A theoretical and empirical
comparison. In NIPS, pages 485–493, 2012.

Ali Rahimi and Benjamin Recht. Weighted sums of
random kitchen sinks: Replacing minimization with
randomization in learning. In NIPS, pages 1313–1320,
2008.

Zichao Yang, Marcin Moczulski, Misha Denil, Nando
de Freitas, Alexander J. Smola, Le Song, and Ziyu
Wang. Deep fried convnets. In ICCV, pages 1476–
1483. IEEE Computer Society, 2015.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Ian En-Hsu Yen, Ting-Wei Lin, Shou-De Lin, Pradeep
Ravikumar, and Inderjit S. Dhillon. Sparse random
feature algorithm as coordinate descent in Hilbert
space. In NIPS, pages 2456–2464, 2014.

Felix X. Yu, Sanjiv Kumar, Henry A. Rowley, and Shih-
Fu Chang. Compact nonlinear maps and circulant
extensions. arXiv preprint arXiv:1503.03893, 2015.

Felix

X.

Yu,

Ananda

Theertha

Suresh,
Krzysztof Marcin Choromanski, Daniel N.
Holtmann-Rice, and Sanjiv Kumar. Orthogo-
In NIPS, pages 1975–1983,
nal random features.
2016.

Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh,
Ji Liu, and Ce Zhang. Zipml: Training linear models
with end-to-end low precision, and a little bit of deep
learning. In ICML, volume 70 of Proceedings of Ma-
chine Learning Research, pages 4035–4043. PMLR,
2017.

Tong Zhang, Bin Yu, et al. Boosting with early stop-
ping: Convergence and consistency. The Annals of
Statistics, 33(4):1538–1579, 2005.

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

A NOTATION AND BACKGROUND

In this appendix, we ﬁrst discuss the notation we use throughout the paper, and then provide an overview of
random Fourier features (RFFs) (Rahimi and Recht, 2007) and the Nystr¨om method (Williams and Seeger, 2000).
After this, we brieﬂy extend our discussion in Section 3 on ﬁxed design kernel ridge regression.

A.1 Notation

×

∈

×

Rn

}
→

Rd, and yi

n
i=1 to denote a training set, for xi
}

= R for regression, and
We use
(xi, yi)
∈
{
n denote the kernel matrix corresponding to a kernel function
for classiﬁcation. We let K
1, . . . , c
=
Y
{
R, where Kij = k(xi, xj), and let ˜K denote an approximation to K. We let z : Rd
Rm denote
Rd
k : Rd
a feature map for approximating a kernel function, such that ˜Kij = z(xi)T z(xj). We use s to denote the size
of the mini-batches during training, and b to denote the precision used for the random features. We let
(cid:107)2
F denote the spectral and Frobenius norms of a matrix K, respectively; if the subscript is not speciﬁed,
and
(cid:107)
will denote the (cid:96)2 norm of x, unless speciﬁed otherwise. In
K
(cid:107)
A is positive
will denote the n
semideﬁnite. We will use λi(A) to denote the ith largest eigenvalue of A, and λmax(A), λmin(A) to denote the
largest and smallest eigenvalues of A, respectively.

K
(cid:107)
denotes the spectral norm. For vectors x,

n identity matrix. For symmetric matrices A and B, we will say A

, where

B if B

K
(cid:107)

∈ Y

→

(cid:22)

−

×

Y

x

(cid:107)

(cid:107)

(cid:107)

A.2 Kernel Approximation Background

k(x, y).
The core idea behind kernel approximation is to construct a feature map z :
X →
n
Given such a map, one can then learn a linear model on top of
i=1, and this model will approximate
(z(xi), yi)
}
{
the model trained using the exact kernel function. We now review RFFs and the Nystr¨om method, two of the
most widely used and studied methods for kernel approximation.

≈

R such that z(x)T z(y)

Random Fourier features (RFFs) For shift-invariant kernels (k(x, x(cid:48)) = ˆk(x
x(cid:48))), the random Fourier
Rm such that
feature method (Rahimi and Recht, 2007) constructs a random feature representation z(x)
E (cid:2)z(x)T z(x(cid:48))(cid:3) = k(x, x(cid:48)). This construction is based on Bochner’s Theorem, which states that any positive
deﬁnite kernel is equal to the Fourier transform of a nonnegative measure. This allows for performing Monte
Carlo approximations of this Fourier transform in order to approximate the function. The resulting features
have the following functional form: zi(x) =
i x + ai), where wi is drawn from the inverse Fourier
transform of the kernel function ˆk, and ai is drawn uniformly from [0, 2π] (see Appendix A in May et al. (2017)
for a derivation).

2/m cos(wT

(cid:112)

−

∈

One way of reducing the memory required for storing W = [w1, . . . , wm], is to replace W by a structured matrix;
in this work, we let W be a concatenation of many square circulant random matrices (Yu et al., 2015).

(cid:104)

(cid:105) ≈

k(x, x(cid:48)).

z(x), z(x(cid:48))

It does this by picking a set of landmark points

Rm
Nystr¨om method The Nystr¨om method constructs a ﬁnite-dimensional feature representation z(x)
∈
ˆx1, . . . , ˆxm
such that
, and
{
taking the SVD ˆK = U ΛU T of the m by m kernel matrix ˆK corresponding to these landmark points ( ˆKi,j =
1/2U T kx, where kx =
k(ˆxi, ˆxj)). The Nystr¨om representation for a point x
Rm
n, the Nystr¨om method can be thought of
[k(x, ˆx1), . . . , k(x, ˆxm)]T . Letting Km,n = [kx1 , . . . , kxn ]
1/2U T Km,n of the full n by n kernel matrix K
as an eﬃcient low-rank approximation K
m,nU Λ−
n
corresponding to the full dataset
i=1. One can also consider the lower-dimensional Nystr¨om representation
}
Rr, where only the top r eigenvalues and eigenvectors of ˆK are used, instead of all m. In
U T
zr(x) = Λ−
r kx
r
this paper, we will always use m = r, and thus will not specify the subscript r.

is deﬁned as z(x) = Λ−

∈ X
∈
1/2Λ−

} ∈ X

K T

1/2

xi

≈

∈

{

×

A.3 Fixed Design Kernel Ridge Regression

We consider the problem of ﬁxed design kernel ridge regression, which has a closed-form equation for the
generalization error, making it a particularly tractable problem to analyze. In ﬁxed design regression, one is
R, and the (cid:15)i are zero-mean uncorrelated
given a set of labeled points
random variables with shared variance σ2 > 0; here, the ¯yi represent the “true labels.” Given such a sample,
¯yi)2(cid:3) is small. Note that for a ﬁxed
the goal is to learn a regressor f (x) such that
learning method, the learned regressor f can be seen as a random function based on the random label noise (cid:15)i.

Rd, yi = ¯yi + (cid:15)i

n
i=1, where xi

i=1(f (xi)

(f ) = E(cid:15)

(xi, yi)

(cid:2) 1
n

(cid:80)n

R

−

∈

∈

{

}

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

One approach to solving this problem is kernel ridge regression. In kernel ridge regression, one chooses a kernel
Rd
function k : Rd
i αik(x, xi).
n denote the kernel matrix such that Kij = k(xi, xj), and y = (y1, . . . , yn), the closed-form
Letting K
1y. It is then easy
solution for this problem (the one minimizing the regularized empirical loss), is α = (K + λI)−
to show (Alaoui and Mahoney, 2015) that the expected error of this regressor fK under the ﬁxed design setting is

R, and a regularizing constant λ, and learns a function of the form f (x) = (cid:80)

×
Rn
×

→

∈

(fK) =

R

1
n

λ2 ¯yT (K + λI)−

2 ¯y +

σ2T r

K 2(K + λI)−

2(cid:17)

,

(cid:16)

1
n

where ¯y = (¯y1, . . . , ¯yn) is the vector of “true labels.”

B GENERALIZATION BOUNDS FOR FIXED DESIGN REGRESSION

B.1 Generalization Bound in Terms of (∆1, ∆2)

Proposition 1. (Extended from (Avron et al., 2017)) Suppose ˜K + λI is (∆1, ∆2)-spectral approximation of
0. Let m denote the rank of ˜K, and let fK and f ˜K be the kernel ridge regression
K + λI, for ∆1 ∈
. Then
estimators learned using these matrices, with regularizing constant λ

0 and label noise variance σ2 <

[0, 1) and ∆2 ≥

(f ˜K)

R

≤

1

1
∆1

(cid:98)
R

(fK) +

∆2
1 + ∆2

m
n

σ2,

≥

∞

(2)

where

(expected risk) and (cid:98)
R

R

(upper bound on

) are deﬁned in Section 3.1.

−

R

∆) and (1 + ∆) with (1

Proof. This proof closely follows the proof of Lemma 2 in Avron et al. (2017), with the primary diﬀerence being
that we replace (1
We begin by replacing K with ˜K in the deﬁnition for (cid:98)
R
1
n

∆1) and (1 + ∆2), respectively.

λ¯yT ( ˜K + λI)−

˜K( ˜K + λI)−

(f ˜K).

(f ˜K)

(fK):

σ2 tr

1 ¯y +

= (cid:98)
R

1
n

1(cid:17)

R

−

−

≤

(cid:16)

( ˜K + λI)−

We now continue this chain of inequalities, using the fact that A
˜K + λI
bounds the ﬁrst term in the above sum.
We now consider the second term. Let m = rank( ˜K), and let sλ( ˜K) = tr

¯yT ( ˜K + λI)−

1(K + λI)−

∆1)−

1 ¯y

⇒

⇒

(1

(cid:22)

≤

(cid:22)

−

1

1

(1

(cid:16)

−

1
B implies B−

˜K( ˜K + λI)−

1(cid:17)

.

1. Thus, (1
∆1)(K +λI)
A−
(cid:22)
−
1 ¯yT (K + λI)−
1 ¯y. This upper

(cid:22)
∆1)−

(cid:16)

˜K( ˜K + λI)−

1(cid:17)

sλ( ˜K) = tr
m
(cid:88)

=

λi( ˜K)
λi( ˜K) + λ

i=1

= m

−

m

−

≤

m
(cid:88)

i=1
m
(cid:88)

i=1

λ
λi( ˜K) + λ

λ
(1 + ∆2)(λi(K) + λ)
m
(cid:88)

= m

1
(1 + (1 + ∆2)−

1)

−

i=1

λ
λi(K) + λ

= m

λ
λi(K) + λ

+

∆2
1 + ∆2

m
(cid:88)

i=1

λ
λi(K) + λ

n

−

≤

λ
λi(K) + λ

+

∆2
1 + ∆2

m

= sλ(K) +

∆2
1 + ∆2

m

−

−

m
(cid:88)

i=1
n
(cid:88)

i=1

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

≤

1

1
∆1

−

sλ(K) +

∆2
1 + ∆2

m

Combining the above results, we get that:

(f ˜K)

R

1 ¯y +

1
n

σ2sλ( ˜K)
(cid:19)

≤

≤

=

=

1
n
1
n

1

1

λ¯yT ( ˜K + λI)−
(cid:18) 1
λ
1

∆1
−
(cid:18) 1
n

1
∆1
−
1
∆1

−

λ¯yT (K + λI)−

1 ¯y +

(fK) +

(cid:98)
R

∆2
1 + ∆2

m
n

σ2

¯yT (K + λI)−

1 ¯y

+

σ2

sλ(K) +

1
n

(cid:18) 1
1

∆1

−
(cid:19)
σ2sλ(K)

+

1
n

∆2
1 + ∆2

m
n

σ2

(cid:19)

∆2
1 + ∆2

m

Remark Above, ∆1 ∈
0. Note that as ∆1 approaches 1, the above upper bound diverges to
n σ2. This suggests
inﬁnity. Whereas as ∆2 approaches
that choosing ˜K and λ such that ∆1 does not get too close to 1 is very important. A necessary condition for
(1

˜K + λI is for ˜K to be high rank, as discussed in Section 3.

, the second term in the upper bound approaches m

[0, 1] and ∆2 ≥
∞

∆1)(K + λI)

−

(cid:22)

B.2 Heuristic Eﬀort to Better Understand Inﬂuence of (∆1, ∆2) on Generalization Error

Here we present a heuristic argument to explain how ∆1 and ∆2 in the spectral approximation aﬀects the
generalization error. We are particularly interested in demonstrating that ∆2 can have an important inﬂuence on
the bias squared term ( λ2
n ¯yT ( ˜K +
1
λI)−
∆1 (cid:98)
R

2 ¯y) of the generalization error, even though the upper bound λ2

(fK) on the bias squared term (from Proposition 1) is only in terms of ∆1.

n ¯yT ( ˜K + λI)−

2 ¯y

≤

1

−

Suppose that the approximate kernel matrix ˜K is a (∆1, ∆2)-spectral approximation of the true kernel matrix K,
that is:

We focus on the bias squared term of ˜K ( λ2
Following Theorem 15 of Musco and Musco (2017), we ﬁrst bound the bias (not squared):

(1

∆1)(K + λIn)

−

˜K + λIn
n ¯yT ( ˜K + λIn)−

(cid:22)

(1 + ∆2)(K + λIn).

(cid:22)
2 ¯y), and compare it to the bias squared term of K.

( ˜K + λIn)−

1 ¯y

(cid:107)

1 ¯y
(cid:107)
1 ¯y
(cid:107)
1 ¯y
(cid:107)
1 ¯y
(cid:107)
1 ¯y
(cid:107)

(cid:107) ≤ (cid:107)
=

=

≤ (cid:107)
=

(K + λIn)−

(K + λIn)−
(cid:107)
(K + λIn)−
(cid:107)
(K + λIn)−

(K + λIn)−
(cid:107)
( ˜K + λIn)−
(cid:107)

1(K

−

˜K

K

−

(cid:22)

∆2(K + λIn)

+

+

+

+
(cid:16)

(( ˜K + λIn)−
1
(cid:107)
( ˜K + λIn)−
(cid:107)
( ˜K + λIn)−
(cid:107)
( ˜K + λIn)−
(cid:107)
1 +

( ˜K + λIn)−
(cid:107)
˜K)
(cid:107)
∆2

. As ˜K + λIn

( ˜K + λIn)

(cid:22)

(cid:22)

1

∆1

−

(K + λIn)−

−

1)¯y
(cid:107)
( ˜K + λIn))(K + λIn)−

1 ¯y
(cid:107)

1((K + λIn)
1(K
1(K

−

−
˜K)(K + λIn)−
˜K)

1 ¯y
(cid:107)
1 ¯y
(K + λIn)−
(cid:107)

−
1(K

(cid:107)(cid:107)
˜K)
(cid:107)

(cid:17)

.

−

(3)

1 + ∆2
∆1
1

−

(cid:22)

( ˜K + λIn).

Now it reduces to bounding

(1 + ∆2)(K + λIn), we have

Similarly, since K + λIn

1
∆1

( ˜K + λIn),

(cid:22)

1

−

Hence

K

˜K

−

(cid:22)

1

∆1

( ˜K + λIn)

∆1

−

1 + ∆2
∆1
1

−

(cid:22)

( ˜K + λIn).

1 + ∆2
∆1
1

−

−

( ˜K + λIn)

K

(cid:22)

−

˜K

(cid:22)

1 + ∆2
∆1
1

−

( ˜K + λIn).

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

˜K)2 from the bound on
˜K. However, under a restricted setting where K and ˜K have the same eigenvectors, we can square the

t2 is not operator monotone, it is not easy to obtain a bound on (K

−

Because t
K
corresponding eigenvalues to obtain

(cid:55)→

−

Thus

Thus

And hence

( ˜K + λIn)−
(cid:107)

1(K

−

˜K)

(cid:107) ≤

1+∆2
∆1
1

−

. Plugging this into the bound (Eq. 3) yields

˜K)2

(K

−

(cid:22)

(1 + ∆2)2
(1

∆1)2 ( ˜K + λIn)2.

−

( ˜K + λIn)−

1(K

˜K)2( ˜K + λIn)−

1

−

(1 + ∆2)2
∆1)2 .
(1

−

(cid:22)

( ˜K + λIn)−
(cid:107)

1 ¯y

(cid:107) ≤

(cid:18)

1 +

(cid:19)

1 + ∆2
∆1
1

−

(K + λIn)−
(cid:107)

1 ¯y
.
(cid:107)

λ2
n

¯yT ( ˜K + λIn)−

2 ¯y =

( ˜K + λIn)−

λ2
n (cid:107)
(cid:18)

1 +

(cid:18)

1 +

1 ¯y
2
(cid:107)
(cid:19)2 λ2
n (cid:107)
(cid:19)2 λ2
n

1 + ∆2
∆1
1

−
1 + ∆2
∆1
1

−

≤

=

(K + λIn)−

2

1 ¯y
(cid:107)

¯yT (K + λIn)−

2 ¯y.

In other words, in this restricted setting the bias squared of ˜K is at most a factor (1 + 1+∆2
)2 larger than the bias
∆1
squared of K. Though this heuristic analysis only holds when K and ˜K have the same eigenvectors, it reveals
the dependency of the generalization performance on ∆1 and ∆2; in particular, it reveals that ∆2 could have an
important inﬂuence on the bias squared term of the generalization error.

−

1

B.3 The Empirical Inﬂuence of ∆2 on the Bias Squared Term

We now empirically validate that ∆2 can have a large impact on the bias squared term, as suggested by the
theoretical discussion in the previous section. The inﬂuence of ∆2 on the bias squared term helps explain our
empirical observations on the inﬂuence of ∆2 on generalization performance from Section 5.2. Though the
generalization bound in Proposition 1 suggests that performance should scale roughly linearly in ∆2/(1 + ∆2),
we empirically found that generalization performance does not asymptote as ∆2 grows (as ∆2/(1 + ∆2) would
suggest it would). In this section, we empirically validate our hypothesis that this is due to looseness in the
generalization bound. Speciﬁcally, the expected mean squared error for ﬁxed design kernel ridge regression is

(f ˜K) =

R

λ2
n

¯yT ( ˜K + λI)−

2 ¯y +

tr

˜K 2( ˜K + λI)−

2(cid:17)

,

(cid:16)

σ2
n

where ˜K is an approximate kernel matrix. We show in experiments that the bias squared term ( λ2
can be strongly inﬂuenced by ∆2, even though the upper bound on it ( λ2
Proposition 1 is only in terms of ∆1.

n ¯yT ( ˜K + λI)−

2 ¯y

≤

n ¯yT ( ˜K + λI)−
1
∆1 (cid:98)
R

2 ¯y)
(fK)) in

1

−

In our experiments, we compute the value of the bias squared term and ∆2 on the Census dataset. To gain
statistically meaningful insights, we collect and average the value of λ2
2 ¯y and ∆2 using 3 independent
runs with diﬀerent random seeds. In Figure 6, we plot the value of λ2
2 ¯y as a function of ∆2 for
3 diﬀerent numbers of features; by controlling the number of features, we can demonstrate the inﬂuence of ∆2
while ∆1 is held roughly ﬁxed. In each curve in Figure 6, the data points are collected from FP-RFFs, circulant
FP-RFFs, as well as LP-RFFs using
bit precision. We can see that for each number of features, the
value of λ2
2 ¯y grows with ∆2. These results demonstrate that the upper bound on the bias term in
Proposition 1 is quite loose, and is not capturing the inﬂuence of ∆2 properly.

n ¯yT ( ˜K + λI)−

n ¯yT ( ˜K + λI)−

n ¯yT ( ˜K + λI)−

1, 2, 4, 8

{

}

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 6: In the ﬁxed design setting, the bias squared term of the generalization performance grows with ∆2.

C THEORETICAL GUARANTEES FOR LP-RFFS

In this section, we ﬁrst lower bound the probability that using LP-RFFs results in a kernel approximation matrix
that is a (∆1, ∆2)-spectral approximation of the exact kernel matrix (Section C.1). We then present bounds on
the Frobenius kernel approximation error for LP-RFFs (Section C.2).

C.1

(∆1, ∆2)-Spectral Approximation Bounds for LP-RFFs

Rn

×

∈

n denote the kernel matrix, and Z

As usual, let K
is the number of data points and m is the number of features. We can write Z = 1
√m
(cid:112)
the (scaled) columns of Z. Each entry of Z has the form
where d is the dimension of the original dataset. Then E[zizT

m be the random Fourier feature matrix, where n
(cid:3) where zi are
Rm,
Rd and a

2/m cos(wT x + a) for some w, x
i ] = K, so E[ZZ T ] = K.

(cid:2)z1, . . . , zm

∈

∈

∈

×

Rn

≥

1. Then the quantized feature matrix is Z + C for some random C

Now suppose we quantize Z to b bits using the quantization method described in Section 4.1, for some ﬁxed
m whose entries are independent
b
(cid:3) where
conditioned on Z (but not identically distributed) with E[C
ci are the (scaled) columns of C. Moreover, the ci are independent conditioned on Z. Deﬁning δ2
1)2 , the
entries Cij have variance E[C 2
δ2
b /m by Proposition 7 in Appendix C.2. In terms of the vectors ci, we
can also see that E[c2

Rn
×
Z] = 0. We can write C = 1
√m

Zij]
ij |
b , where ci,j denotes the jth element of ci.
δ2
≤

(cid:2)c1, . . . , cm
b :=

i,j |

Zij]

(2b

≤

∈

−

2

|

We ﬁrst analyze the expectation of (Z + C)(Z + C)T (over both the randomness of Z and of C).
Lemma 3. E[(Z +C)(Z +C)T ] = K +D, where D := E[c1cT
D does not depend on the number of random features m.

1 ] = sbIn is a multiple of the identity, for 0

sb

≤

≤

δ2
b .

Proof.

E[(Z + C)(Z + C)T ] = E

(zi + ci)(zi + ci)T

= E[(z1 + c1)(z1 + c1)T ]

(cid:20) 1
m

m
(cid:88)

i=1

(cid:21)

Since Ec1[c1 |

z1] = 0, it follows that

E[(z1 + c1)(z1 + c1)T ] = Ez1

(cid:104)

Ec1[(z1 + c1)(z1 + c1)T
(cid:104)

Ec1[c1cT

1 |

(cid:105)

z1]
(cid:105)

|
z1]

1 ] + Ez1

= Ez1[z1zT
= K + E[c1cT
1 ]

It is clear that D := E[c1cT
variable. It is also easy to see that the jth entry on the diagonal of D is equal to E[c2
argue that each element z1,j = √2 cos(wT

1 ] is a diagonal matrix, because each element of c1 is a zero-mean independent random
δ2
b . Lastly, we
1 xj + a1) has the same distribution, because it is distributed the same

1,j |

z1,j]

≤

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

as √2 cos(a1) for a1 uniform in [0, 2π]. Thus, E[c2
completes the proof.

1,j |

z1,j] is independent of j. Letting sb := E[c2

1,1 |

z1,1]

δ2
b

≤

1 ], we can use matrix concentration to show that the quantized kernel matrix ˜K = (Z +C)(Z +C)T
With D := E[c1cT
is close to its expectation K + D. We ﬁrst strengthen the matrix Bernstein inequality with intrinsic dimension
(Theorem 7.7.1 in Tropp (2015)) by removing the requirement on the deviation.13
Theorem 4 (Matrix Bernstein: Hermitian Case with Intrinsic Dimension). Consider a ﬁnite sequence
random Hermitian matrices of the same size, and assume that

Xk
{

of

}

Introduce the random matrix

E[Xk] = 0 and λmax(Xk)

L for each index k.

≤

(cid:88)

Y =

Xk.

k

Let V be a semideﬁnite upper bound for the matrix-valued variance VAR [Y ]:

Deﬁne the intrinsic dimension bound and variance bound

VAR [Y ] = E[Y 2] =

V

(cid:23)

(cid:88)
k

E[X 2
k ].

Then, for t

0,

≥

intdim(V ) =

and

v =

V
(cid:107)

.
(cid:107)

tr(V )
V
(cid:107)

(cid:107)

P (λmax(Y )

t)

≥

≤

4 intdim(V )

exp

·

(cid:18)

t2/2
v + Lt/3

−

(cid:19)

.

(4)

√v + L/3 is exactly Theorem 7.7.1 in Tropp (2015). We just need to show that the bound

2(v + Lt/3). Indeed, t2

2Lt/3

2v has roots

−

−

≤

Proof. The case of t
is vacuous when 0

≥
t < √v + L/3.

Suppose that 0
(cid:113) L2
L
3 ±

≤

9 + 2v. The condition t2

≤

≤
t < √v + L/3. We show that then t2

2(v + Lt/3) is then equivalent to

L
3 −

(cid:114)

L2
9

+ 2v

t

≤

≤

L
3

+

(cid:114)

L2
9

+ 2v.

The lower bound is negative since v
bound. Thus 0

t < √v + L/3 implies that t2

≥

0, and t < √v + L/3 implies t < L/3 +

L2/9 + 2v, satisfying the upper

(cid:112)

2(v + Lt/3). The bound in equation (4) becomes

≤

≥

(cid:18)

≤
t2/2
v + Lt/3

(cid:19)

−

≥

≤

4 intdim(V ) exp

4 intdim(V ) exp(

1)

4/e > 1,

−

≥

since intdim(V )

1. Thus (4) holds vacuously for 0

t < √v + L/3.

We now present Lemma 5, in which we lower bound the probability that ˜K is “close” to its expectation K + D,
in the speciﬁc sense we describe below.
Lemma 5. Let K be an exact kernel matrix, and ˜K = (Z + C)(Z + C)T be an m-features b-bit LP-RFF
2 and M :=
approximation of K with expectation K + D. For any deterministic matrix B, let L := 2n
(cid:107)
B(K + δ2

B
(cid:107)

0,

b In)BT , then for any t1, t2 ≥
(cid:20)
(cid:16)
P
B

t1In

(Z + C)(Z + C)T

(K + D)

B

t2In

(cid:22)
−
4 tr(M )
M
(cid:107)

(cid:107)

1

≥

−

(cid:20)

(cid:18)

exp

2L(
(cid:107)

−
M
(cid:107)

−

(cid:19)

mt2
1
+ 2t1/3)

+ exp

−
M
2L(
(cid:107)
(cid:107)

mt2
2
+ 2t2/3)

(cid:19)(cid:21)

.

(cid:21)

(cid:17)

(cid:18)

(cid:22)

13Theorem 7.7.1 in Tropp (2015) requires that t

√v + L/3, where t, v, and L are as deﬁned in Theorem 4.

≥

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

=

Si

Proof. Let
=
B (cid:0)(Z + C)(Z + C)T
(K + D)(cid:1) B. We see that E[Si] = 0. We will bound λmax(S) and λmin(S) by
applying the matrix Bernstein inequality for symmetric matrices with intrinsic dimension (Theorem 4). Thus we
need to bound

i=1 Si

(cid:80)m

and

and

=

−

−

S

Si

(cid:0)B(zi + ci)(zi + ci)T BT

B(K + D)BT (cid:1)

(cid:80)m

1
m

(cid:107)
Let ui = B(zi + ci)

(cid:107)

(cid:107)

E[S2
i ]
.
i=1
(cid:107)
Rn, then Si = 1
m (uiuT
2 =
(cid:107)

uiuT
(cid:107)

i −

∈

B(zi + ci)
(cid:107)
(cid:107)
where we have used the fact that zi + ci is a vector of length n whose entries are in [
bound on

zi + ci
(cid:107)

≤ (cid:107)

i (cid:107)

ui

≤

=

B

B

(cid:107)

(cid:107)

(cid:107)

(cid:107)

Si

−

2

2

2,

:
(cid:107)

(cid:107)

Si
(cid:107)

(cid:107)

=

1
m (cid:107)

uiuT

i −

E[uiuT
i ]

1
m (cid:107)

uiuT

(cid:107) ≤

1
m

E
uiuT
(cid:107)

i (cid:107) ≤

4n

2

B
(cid:107)
m

(cid:107)

= 2L/m.

uiuT
(cid:107)
2
(cid:107)

i (cid:107)
2n

E[uiuT

i ]). We ﬁrst bound

. Since this is a rank 1 matrix,

√2, √2]. This gives a

Thus λmax(Si)
Now it’s time to bound E[S2

2L/m and λmax(

−
i ]. We will use

Si) =

≤

−

λmin(Si)

≤

+

i (cid:107)
2L/m.

E[S2

i ] =

1
m2
1
m2

(cid:16)

E(cid:2)(uiuT

i )2(cid:3)

E(cid:2)uiuT

i

−

E[

ui
(cid:107)

(cid:107)

2uiuT
i ]

(cid:22)

2n

2
(cid:107)

B
(cid:107)
m2

(cid:3)2(cid:17)

1
m2
(cid:22)
E[uiuT

i ].

=

E[(uiuT

i )2] =

E[uiuT

i uiuT
i ]

1
m2

Thus

m
(cid:88)

i=1

E[S2
i ]

2n

2
(cid:107)

B
(cid:107)
m

(cid:22)

E[v1vT

1 ] =

2n

2
(cid:107)

B
(cid:107)
m

B(K + D)BT

B(K + δ2

b In)BT = LM/m.

2n

2
(cid:107)

B
(cid:107)
m

(cid:22)

Applying Theorem 4 with S, for any t2 ≥
(cid:16)

(Z + C)(Z + C)T

(cid:20)
λmax(B

P

0, we have

(cid:17)

−

(K + D)

4 tr(M )
M
(cid:107)
S and using the fact that λmax(

t2In

B)

(cid:23)

≤

(cid:107)
S) =

exp

(cid:21)

(cid:21)

Similarly, applying Theorem 4 with

(cid:20)
λmin(B

P

(cid:16)

(Z + C)(Z + C)T

(K + D)

B)

(cid:17)

−

−

t1In

(cid:22) −

≤

−

exp

−
4 tr(M )
M

(cid:107)

(cid:107)

Combining the two bounds with the union bound yields the desired inequality.

(cid:18)

(cid:19)

.

mt2
2
+ 2t2/3)

−
2L(
M
(cid:107)
(cid:107)
λmin(S), for any t1 ≥
(cid:19)
(cid:18)
mt2
1
+ 2t1/3)

2L(

−
M
(cid:107)
(cid:107)

.

0, we have

We are now ready to show that low-precision features yield close spectral approximation to the exact kernel
matrix.
Theorem 2. Let ˜K be an m-feature b-bit LP-RFF approximation of a kernel matrix K, and assume
b := 2/(2b
δ2
δ2
b /λ,
(cid:104)
P

1)2. Then for any ∆1 ≥
˜K + λI
∆1)(K + λI)

0, ∆2 ≥
(1 + ∆2)(K + λI)

K
(cid:107)

(cid:107) ≥

(1

−

≥

λ

(cid:105)

−

(cid:22)

(cid:22)

8 tr (cid:0)(K + λIn)−

1(K + δ2

b In)(cid:1)

exp

1

−

≥

(cid:32)

(cid:18)

(cid:19)

m∆2
1
−
4n
λ (1 + 2
3 ∆1)

+ exp

(cid:32)

m(∆2 −

−
4n
λ (1 + 2

3 (∆2 −

δ2
λ )2
b
δ2
λ ))
b

(cid:33)(cid:33)

.

Proof. We conjugate the desired inequality with B := (K + λIn)−
noting that semideﬁnite ordering is preserved by conjugation:

1/2 (i.e., multiply by B on the left and right),

∆1)(K + λIn)

˜K + λIn

(1 + ∆2)(K + λIn)

(1

(1

−

∆1)In

⇐⇒

⇐⇒ −

⇐⇒ −

⇐⇒ −

−
∆1In

∆1In

∆1In

(cid:22)

(cid:22)

(cid:22)

(cid:22)
B( ˜K + λIn)B

(cid:22)
B( ˜K + λIn)B
B( ˜K + λIn
B( ˜K

−
K)B

−

(cid:22)

−
K

(cid:22)
(1 + ∆2)In

(cid:22)
In

∆2In

(cid:22)
λIn)B

(cid:22)

−
∆2In.

∆2In

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

δ2
b /λ)In implies
(∆2 −
We show that
∆1In
(cid:22)
δ2
1
(K + λIn)−
b /λ because
0 by Lemma 3 and B is symmetric, BDB

K
−
B2
(cid:107)
(cid:107)

−
2 =
(cid:107)

D)B
=

B( ˜K
B
(cid:107)

1/λ and

(cid:22)

−

(cid:107)

(cid:107)
0. Thus the condition

(cid:107) ≤

B( ˜K
∆1In
−
(cid:22)
δ2
b , so BDB
∆1In

−
D

K)B
∆2In. Indeed,
(cid:22)
(δ2
b /λ)In. Moreover,
B( ˜K

D)B

K

−

−

(cid:22)

(cid:22)
(cid:22)

−

(cid:107) ≤
(cid:23)

BDB
(cid:107) ≤
(cid:107)
since D
(cid:23)
δ2
b /λ)In implies:
(∆2 −

B( ˜K
B( ˜K

−

K)B = B( ˜K
K)B = B( ˜K

Hence P

∆1In

(cid:104)
−

K)B

∆2In

−
B( ˜K
B( ˜K

−

−

K

K

(cid:105)

−

−

D)B + BDB

D)B + BDB

(cid:22)

(∆2 −
∆1In + 0 =

(cid:104)

P

∆1In

(cid:23) −
B( ˜K

∆1In.

−

b /λ)In + δ2
δ2

b /λIn = ∆2In,

K

D)B

It remains to

δ2
b /λ)In

(cid:105)

.

(∆2 −

(cid:22)
show that
D)B
(cid:22)
apply Lemma 5 for B := (K + λIn)−

∆1In

K

−

(cid:22)

−

−

−

−
≥
δ2
b /λ)In with the desired probability, by applying Lemma 5. We
(∆2 −
1/2, L := 2n

2n/λ, and M := B(K + δ2

−

(cid:22)

−

(cid:22)

(cid:22)

2

≤
To simplify the bound one gets from applying Lemma 5 with the above B, L, and M , we will use the following
=
expression for tr(M ), and the following upper and lower bounds on

B2(K + δ2

tr(M ) = tr

(cid:17)

(cid:16)

(cid:107)

b In)B.

B
(cid:107)

(cid:17)

b In)

(cid:16)

1(K + δ2

(K + λIn)−

tr
b In)U T U (S + λIn)−
δ2
λ1 ≥
M
(cid:107)

(cid:107) ≤

λ by assumption),

1/2U T = U (S + λIn)−

M
(cid:107)

(cid:107)

= (λ1 + δ2

1(S + δ2
b )/(λ1 + λ)

≥

1. Lemma 5 allows us to conclude the following:

. Letting K = U SU T be the SVD of K, we get that M = U (S + λIn)−

(cid:107)

M

.
(cid:107)

b In)
1/2U T U (S +
b In)U T . Thus, letting λ1 be the largest eigenvalue of K (recall
λ, so

1/2. We also assume that δ2

(λ1 + δ2

b )/(2λ1)

b ≤

≥

(cid:20)

P

(1

−

(cid:20)
= P

∆1)(K + λIn)

˜K + λIn

(cid:22)

∆1In

(cid:22)

B( ˜K

K)B

∆2In

−

(cid:22)

(cid:22)

(cid:21)

(cid:21)
(1 + ∆2)(K + λIn)

≥

≥

(cid:104)

P

−

−

−

1

1

∆1In

B( ˜K
(cid:20)

−
(cid:18)

(cid:22)
4 tr(M )
M
(cid:107)
8 tr (cid:0)(K + λIn)−

exp

(cid:107)

≥

−

(K + D))B

(cid:105)

(cid:22)

(∆2 −
(cid:19)

δ2
b /λ)In
(cid:18)

−
M
(cid:107)

2L(
(cid:107)
1(K + δ2

m∆2
1
+ 2∆1/3)
b In)(cid:1)

(cid:20)

exp

(cid:18)

+ exp

−
2L(
M
(cid:107)
m∆2
1
4n/λ(1 + 2∆1/3)

−

b /λ)2
δ2
m(∆2 −
δ2
b /λ)/3)
+ 2(∆2 −
(cid:107)
(cid:18)
(cid:19)
+ exp

(cid:19)(cid:21)

m(∆2 −

δ2
b /λ)2
δ2
4n/λ(1 + 2(∆2 −
b /λ)/3)

−

(cid:19)(cid:21)

.

There is a bias-variance trade-oﬀ: as we decrease the number of bits b, under a ﬁxed memory budget we can use
more features, and (Z + C)(Z + C)T concentrates more strongly (lower variance) around the expectation K + D
δ2
with 0
b In. However, this expectation is further away from the true kernel matrix K (larger bias). Thus
there should be an optimal number of bits b∗ that balances the bias and the variance.

D

(cid:22)

(cid:22)

Proof of Corollary 2.1. Letting ∆2 → ∞
(cid:105)
˜K + λIn

∆1)(K + λIn)

(cid:104)
(1

P

−

(cid:22)

1

≥

−

in Theorem 2 gives

8 tr (cid:0)(K + λIn)−

1(K + δ2

b In)(cid:1) exp

(cid:18)

m∆2
1
4n/λ(1 + 2∆1/3)

−

(cid:19)

.

Using the assumption that ∆1 ≤
(cid:104)

P

(1

−

∆1)(K + λIn)

3/2, we can simplify the bound:

(cid:105)

˜K + λIn

(cid:22)

1

≥

−

8 tr (cid:0)(K + λIn)−

1(K + δ2

b In)(cid:1) exp

(cid:18)

m∆2
1
−
8n/λ

(cid:19)

.

Letting the RHS be 1

ρ and solving for m yields

−

m

≥

8n/λ
∆2
1

log

(cid:17)

.

(cid:16) a
ρ

Similarly, letting ∆1 → ∞

in Theorem 2 gives

P

(cid:104)
˜K + λIn

(1

(cid:22)

−

(cid:105)
∆2)(K + λIn)

1

≥

−

8 tr (cid:0)(K + λIn)−

1(K + δ2

b In)(cid:1) exp

(cid:18)

m(∆2 −
−

δ2
b /λ)2
δ2
b /λ)/3)
4n/λ(1 + 2(∆2 −

(cid:19)

.

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Using the assumption that ∆1 ≤

3/2, we can simplify the bound:

(cid:104)

P

(1

−

∆1)(K + λIn)

(cid:105)

˜K + λIn

(cid:22)

1

≥

−

8 tr (cid:0)(K + λIn)−

1(K + δ2

b In)(cid:1) exp

(cid:18)

−

m(∆2 −
8n/λ

b /λ)2
δ2

(cid:19)

.

Letting the RHS be 1

ρ and solving for m yields

−

m

≥

(∆2 −

8n/λ
b /λ)2 log
δ2

(cid:17)

.

(cid:16) a
ρ

and set ∆1 = ∆2 = ∆, we get the following corollary, similar to the result

If we let the number of bits b go to
from Avron et al. (2017):
Corollary 5.1. Suppose that ˜K = ZZ T ,

∞

(cid:104)

P

(1

−

∆)(K + λIn)

˜K + λIn

(cid:22)

(cid:22)

(cid:105)
(1 + ∆)(K + λIn)

≥

−

1

16 tr((K + λIn)−

1K) exp

(cid:18)

3m∆2
16n/λ

−

(cid:19)

.

K
(cid:107)

(cid:107) ≥

λ. Then for any ∆

1/2,

≤

Thus if we use m
≥
(1 + ∆)(K + λIn) with probability at least 1

16
3∆2 n/λ log(16 tr((K + λIn)−

ρ.

−

1K)/ρ) features, then (1

∆)(K + λIn)

−

˜K + λIn

(cid:22)

(cid:22)

The constants are slightly diﬀerent from that of Avron et al. (2017) as we use the real features √2 cos(wT x + a)
instead of the complex features exp(iwT x).

From these results, we now know that the number of features required depends linearly on n/λ; more precisely,
n/λ features (for some constant c0 > 0), ˜K + λIn will be a (∆, ∆)-spectral
we know that if we use m
approximation of K + λIn with high probability. Avron et al. (2017) further provide a lower bound, showing that
n/λ (for some other constant c1 > 0), ˜K + λIn will not be a (∆, ∆)-spectral approximation of K + λIn
if m
with high probability. This shows that the number of random Fourier features must depend linearly on n/λ.

c0 ·

c1 ·

≤

≥

C.2 Frobenius Kernel Approximation Error Bounds for LP-RFFs

We begin this section by bounding the variance of the quantization noise C added to the random feature matrix
Z. We prove this as a simple consequence of the following Lemma.14
Lemma 6. For z
probability c
c

be the random variable which with probability z
c
(c

z
z. Then E [X a,c

a
a equals c
−
−
a)2
.
−
4

] = 0, and VAR [X a,c

[a, c], let X a,c

z, and with

] = (c

z)(z

a)

−

z

z

∈
z
a equals a
−
−

−

−

−

≤

Proof.

VAR [X a,c

z

] = (c

E [X a,c
z

] = (c

z)

z
c

·

a
a

+ (a

z)

−

c
c

·

z
a

−
−
c
−
c
−
a))

z
a

z)2

−
z + z

·

−

+ (a

a
−
a
−
a)((c
−
c
a
a)

−

−

= 0.

z)2

·
z)(z

−

−

(c

=

−
−
z
c

−

z)(z

−

= (c
d
dz

−
[
−

=

2z + c + a.

−

d
dz

[VAR [X a,c

z

]] =

z2 + (c + a)z

ac]

−

Now, setting the derivative to 0 gives z∗ = c+a
z)(z

a) = (c

a)2
a) = (c
−
4

2 )( c+a
c+a

.

−

−

2 −

2 . Thus, arg maxz

[a,c](c

z)(z

∈

−

−

a) = c+a

2 , and maxz

[a,c](c

∈

−

14This lemma is also a direct consequence of Popoviciu’s inequality on variances (Popoviciu, 1935). Nonetheless, we

include a stand-alone proof of the lemma here for completeness.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Proposition 7. E[C 2

Zij]

b /m, for δ2
δ2

b :=

ij |

≤

(2b

2

1)2 .
−

Proof. Given a feature Zij

2/m], we quantize it to b bits by dividing this interval into 2b

1

−

1 ), and randomly rounding to the top or bottom of the sub-interval
equally-sized sub-intervals (each of size
containing Zij (in an unbiased manner). Let a, c denote the boundaries of the sub-interval containing Zij, where

−

(cid:112)

2/m,

(cid:112)

[

∈

−

2√2/m
2b

1 . We can now see that Cij = X a,c
−
Zij] = 0. Because c

a =

2√2/m
2b

Zij is the unique random variable such that Zij + Cij
X a,c

1 , it follows from Lemma 6 that E[C 2

Zij] = VAR

(cid:104)

a, c
}
(cid:105)

∈ {
Zij

≤

Zij |

−

−

ij |

c = a +

2√2/m
2b
and E [Cij
a)2
4 = 8/m
(c

4(2b

−

|

−

1)2 = δ2

b /m.

We now prove an upper bound on the expected kernel approximation error for LP-RFFs, which applies for any
quantization function with bounded variance. This error corresponds exactly to the variance of the random
variable which is the product of two quantized random features.
Theorem 8. For x, y
≤
σ2, and that k(x, x) = k(y, y) = 1.15 For any unbiased random quantization function Q with bounded variance
VAR [Q(z)]
≤
2˜σ2 + ˜σ4 + σ2.

˜σ2 for any (ﬁxed) z, it follows that E [Q(Zx)Q(Zy)] = k(x, y), and that VAR [Q(Zx)Q(Zy)]

, assume we have random variables Zx, Zy satisfying E [ZxZy] = k(x, y), VAR [ZxZy]

∈ X

≤

Proof. Let Q(Zx) = Zx + (cid:15)x, and Q(Zy) = Zy + (cid:15)y, where E [(cid:15)x] = E [(cid:15)y] = 0, E (cid:2)(cid:15)2
are independent random variables.

x

(cid:3)

≤

˜σ2, E (cid:2)(cid:15)2

y

(cid:3)

≤

˜σ2, and (cid:15)x,(cid:15)y

E [Q(Zx)Q(Zy)] = E [(Zx + (cid:15)x)(Zy + (cid:15)y)]

= E [ZxZy + Zy(cid:15)x + Zx(cid:15)y + (cid:15)x(cid:15)y]
= E [ZxZy]
= k(x, y).

VAR [Q(Zx)Q(Zy)] = E

Q(Zx)Q(Zy)

k(x, y)

(cid:17)2(cid:21)

−

(cid:20)(cid:16)

(cid:20)(cid:16)

(cid:20)(cid:16)

(cid:20)(cid:16)

= E

= E

= E

(Zx + (cid:15)x)(Zy + (cid:15)y)

k(x, y)

−

(cid:17)2(cid:21)

Zy(cid:15)x + Zx(cid:15)y + (cid:15)x(cid:15)y + ZxZy

k(x, y)

(cid:17)2(cid:21)

−
(cid:20)(cid:16)

(cid:17)2(cid:21)

+ E

(cid:17)2(cid:21)

ZxZy

k(x, y)

−

Zy(cid:15)x + Zx(cid:15)y + (cid:15)x(cid:15)y

≤

y (cid:15)2

x + Z 2

E (cid:2)Z 2
y + (cid:15)2
k(y, y)˜σ2 + k(x, x)˜σ2 + ˜σ4 + σ2

(cid:3) + σ2

x(cid:15)2

x(cid:15)2
y

≤
= 2˜σ2 + ˜σ4 + σ2.

Note that if x = y, for this proof to hold, we would need to randomly quantize Zx twice, giving two independent
quantization noise samples (cid:15)(1)

x and (cid:15)(2)
x .

σ2, quantizing the random features will have negligable eﬀect on the
This theorem suggests that if 2˜σ2 + ˜σ4
variance. In practice, for the random quantization method we use with random Fourier features (described in
Section 4.1), we have ˜σ2 =
1)2 . Thus, for a large enough precision b, the quantization noise will be tiny
relative to the variance inherent to the random features.

(cid:28)

(2b

−

2

We note that the above theorem applies to one-dimensional random features, but can be trivially extended to m
dimensional random features. We show this in the following corollary.

15For example, one speciﬁc instance of the random variables Zx, Zy is given by random Fourier features, where zx =
√2, √2], for random w, b. We need not assume that k(x, x) = k(y, y) = 1,

√2 cos(wT x+b), zy = √2 cos(wT y +b), zx, zy
but this simpliﬁes the ﬁnal expression, as can be seen in the last step of the proof.

−

∈

[

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Table 3: Dataset details. For classiﬁcation tasks, we write the number of classes in parentheses in the “task”
column.

Dataset Task

Train Heldout Test # Features

Reg.
Census
YearPred Reg.
CovType Class. (2)
TIMIT

Class. (147)

16k
417k
418k
2.3M 245k

2k
46k
46k

2k
52k
116k
116k

119
90
54
440

Table 4: The Gaussian kernel bandwidths used, and the search grid for initial learning rate on the Census,
YearPred, Covtype and TIMIT datasets. Optimal learning rate in bold.

Dataset

Census
YearPred
Covtype
TIMIT

1/2σ2

0.0006
0.01
0.6
0.0015

Initial learning rate grid

0.01, 0.05, 0.1, 0.5, 1.0
0.05, 0.1, 0.5, 1.0, 5.0
1.0, 5.0, 10.0, 50.0, 100.0
5.0, 10.0, 50.0, 100.0, 500.0

∈ X

Corollary 8.1. For x, y
VAR [Zx] , VAR [Zy]
≤
bounded variance (E [Q(z)] = z, VAR [Q(z)]
(T1, . . . , Tn) be a random sequence of i.i.d. draws from S and T respectively. Deﬁne ¯Sn = 1
n
¯Tn = 1
n
VAR (cid:2) ¯Sn

, assume we have random variables Zx, Zy satisfying E [ZxZy] = k(x, y), and
σ2, and that k(x, x) = k(y, y) = 1. Let Q be any unbiased quantization function with
˜σ2 for any z). Let S = ZxZy, T = Q(Zx)Q(Zy), and (S1, . . . , Sn),
(cid:80)n
i=1 Si, and
(cid:3) = k(x, y), and that

≤
i=1 Ti, to be the empirical mean of these draws. It follows that E (cid:2) ¯Sn
.

(cid:3) = E (cid:2) ¯Tn

(cid:80)n
(cid:3)

n , and VAR (cid:2) ¯Tn

2˜σ2+˜σ4+σ2
n

σ2

(cid:3)

≤

≤

Proof.

VAR (cid:2) ¯Sn

(cid:3) = VAR

(cid:34)

1
n

VAR

(cid:35)

Si

(cid:21)

Si

n
(cid:88)

i=1
(cid:20) 1
n

=

≤

=

n
(cid:88)

i=1

n

·
σ2
n

σ2
n2

The result for VAR (cid:2) ¯Tn

(cid:3) follows in the same way, using the result from Theorem 8.

D EXPERIMENT DETAILS AND EXTENDED RESULTS

D.1 Datasets and Details Applying to All Experiments

In this work, we present results using FP-Nystr¨om, FP-RFFs, circulant FP-RFFs, and LP-RFFs on the TIMIT,
YearPred, CovType, and Census datasets. These datasets span regression, binary classiﬁcation, and multi-class
classiﬁcation tasks. We present details about these datasets in Table 3. In these experiments we use the Gaussian
kernel with the bandwidth σ speciﬁed in Table 4; we use the same bandwidths as May et al. (2017). To evaluate
the performance of these kernel models, we measure the classiﬁcation error for classiﬁcation tasks, and the mean
squared error (MSE) for regression tasks ( 1
yi)2), on the heldout set. We compute the total
n
memory utilization as the sum of all the components in Table 1.

i=1(f ˜K(xi)

(cid:80)n

−

For all datasets besides TIMIT, we pre-processed the features and labels as follows: We normalized all continuous

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

features to have zero mean and unit variance. We did not normalize the binary features in any way. For regression
datasets, we normalized the labels to have zero mean across the training set.

TIMIT (Garofolo et al., 1993) is a benchmark dataset in the speech recognition community which contains
recordings of 630 speakers, of various English dialects, each reciting ten sentences, for a total of 5.4 hours of
speech. The training set (from which the heldout set is then taken) consists of data from 462 speakers each
reciting 8 sentences (SI and SX sentences). We use 40 dimensional feature space maximum likelihood linear
regression (fMLLR) features (Gales, 1998), and concatenate the 5 neighboring frames in either direction, for a
total of 11 frames and 440 features. This dataset has 147 labels, corresponding to the beginning, middle, and end
of 49 phonemes. For reference, we use the exact same features, labels, and divisions of the dataset, as (Huang
et al., 2014; Chen et al., 2016; May et al., 2017).

We acquired the CovType (binary) and YearPred datasets from the LIBSVM webpage,16 and the Census dataset
from Ali Rahimi’s webpage.17 For these datasets, we randomly set aside 10% of the training data as a heldout set
for tuning the learning rate and kernel bandwidth.

The speciﬁc ﬁles we used were as follows:

•

•

•

CovType: We randomly chose 20% of “covtype.libsvm.binary” as test, and used the rest for training/heldout.

YearPred: We used “YearPredictionMSD” as training/heldout set, and “YearPredictionMSD.t” as test.

Census: We used the included matlab dataset ﬁle “census.mat” from Ali Rahimi’s webpage. This Matlab ﬁle
had already split the data into train and test. We used a random 10% of the training data as heldout.

D.2 Nystr¨om vs. RFFs (Section 2.2)

}

∈ {

. For RFFs, we use m

1250, 2500, 5000, 10000, 20000

We compare the generalization performance of full-precision RFFs and the Nystr¨om method across four
datasets, for various memory budgets. We sweep the following hyperparameters: For Nystr¨om, we use
m
1250, 2500, 5000, 10000, 20000, 50000, 100000,
∈ {
200000, 400000
. We choose these limits diﬀerently because 20k Nystr¨om features have roughly the same memory
}
footprint as 400k FP-RFFs. For all experiments, we use a mini-batch size of 250. We use a single initial learning
rate per dataset across all experiments, which we tune via grid search using 20k Nystr¨om features. We choose
to use Nystr¨om features to tune the initial learning rate in order to avoid biasing the results in favor of RFFs.
We use an automatic early-stopping protocol, as in (Morgan and Bourlard, 1990; Sainath et al., 2013b,a), to
regularize our models (Zhang et al., 2005; Wei et al., 2017) and avoid expensive hyperparameter tuning. It works
as follows: at the end of each epoch, we decay the learning rate in half if the heldout loss is less than 1% better
relative to the previous best model, using MSE for regression and cross entropy for classiﬁcation. Furthermore, if
the model performs worse than the previous best, we revert the model. The training terminates after the learning
rate has been decayed 10 times.

We plot our results comparing the Nystr¨om method to RFFs in Figure 7. We compare the performance of these
methods in terms of their number of features, their training memory footprint, and the squared Frobenius norm
and spectral norm of their kernel approximation error.

D.3 Nystr¨om vs. RFFs Revisited (Section 3.2)

D.3.1 Small-Scale Experiments

In order to better understand what properties of kernel approximation features lead to strong generalization
performance, we perform a more ﬁne-grained analysis on two smaller datasets from the UCI machine learning
repository—we consider the Census regression task, and a subset of the CovType task with 20k randomly sampled
training points and 20k randomly sampled heldout points. The reason we use these smaller datasets is because
computing the spectral norm, as well as (∆1,∆2) are expensive operations, which requires instantiating the kernel
matrices fully, and performing singular value decompositions. For the Census experiments, we use the closed-form
solution for the kernel ridge regression estimator, and choose the λ which gives the best performance on the

16https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/
17https://keysduplicated.com/ ali/random-features/data/

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

(a)

(b)

(c)

(d)

Figure 7: Generalization performance of FP-RFFs and Nystr¨om with respect to # features (a) and training
memory footprint (b) on Census, CovType, Yearpred and TIMIT. Nystr¨om performs better for a ﬁxed number
of features, while FP-RFFs perform better under a memory budget. We also see that the relative performance
between these methods cannot be fully explained by the Frobenius norms (c) or spectral norms (d) of their
respective kernel approximation error matrices; in particular, we see many example of Nystr¨om models that have
lower Frobenius or spectral error, but worse heldout performance, than various RFF models. To quantify the
degree of alignment between these error metrics and generalization performance (right plots), we show in the
ﬁgure titles the Spearman rank correlation coeﬃcients ρ between the corresponding x and y metrics. We see
in Figure 11 that 1/(1
∆1) attains notably higher values of ρ than the Frobenius and spectral norms of the
kernel approximation error. Note that although we plot the average performance across three random seeds for
each experimental setting (error bars indicate standard deviation), when we compute the ρ values we treat each
experimental result independently (without averaging).

−

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

−

Figure 8: The correlation between generalization performance and squared Frobenius norm, spectral norm, ∆,
and 1/(1
∆1) for FP-RFFs and FP-Nystr¨om on the Census and subsampled CovType datasets. To quantify
the alignment between these metrics and downstream performance, we include the Spearman rank correlation
coeﬃcients ρ computed between these metrics and the downstream performance of our trained models in the
ﬁgure titles. Note that although we plot the average performance across ﬁve random seeds for each experimental
setting (error bars indicate standard deviation), when we compute the ρ values we treat each experimental result
independently (without averaging).

Table 5: Number of features used for the diﬀerent kernel approximation methods in the experiments on
generalization performance vs. (∆1, ∆2) in Sections 3.2 and 5.2.

Methods

Number of features

FP-Nystr¨om
FP-RFF
Cir. FP-RFF
LP-RFF 16
LP-RFF 8, 4, 2, 1

25, 50, 100, 200, 500, 1250, 2500, 5000, 10000, 20000
200, 500, 1000, 2000, 5000, 10000, 20000
200, 500, 1000, 2000, 5000, 10000, 20000
500, 1000, 2000, 5000, 10000, 20000, 50000
1000, 2000, 5000, 10000, 20000, 50000

heldout set. For CovType, because there is no closed-form solution for logistic regression, we used the following
training protocol to (approximately) ﬁnd the model which minimizes the regularized training loss (just like the
closed-form ridge regression solution does). For each value of λ, we train the model to (near) convergence using 300
epochs of SGD (mini-batch size 250) at a constant learning rate, and pick the learning rate which gives the lowest
regularized training loss for that λ. We then evaluate this converged model on the heldout set to see which λ gives
the best performance. We pick the best learning rate, as well as regularization parameter, by using 20k Nystr¨om
features as a proxy for the exact kernel (note that because there are only 20k training points, this Nystr¨om
approximation is exact). We choose the learning rate 5.0 from the set
, and
}
the regularization parameter λ = 5e
. For the Census
4
}
. We sweep the number
1e
dataset, we pick λ = 5e
}
{
of features shown in Table 5. For both datasets, we report the average squared Frobenius norm, spectral norm,
∆, (∆1, ∆2) and the average generalization performance, along with standard deviations, using ﬁve diﬀerent
random seeds. The results are shown in Figure 8. As can be seen in Figure 8, 1/(1
∆1) aligns much better with
generalization performance than the other metrics.

0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0
5, 1e
−
1
−

6 from
5, 5e

{
6, 5e
3, 1e

7, 5e
4, 1e

7, 1e
3, 5e

6, 1e
2, 5e

5, 5e
2, 1e

8, 1e
4, 5e

5e
{
5, 1e

4 from

−
−

−
−

−
−

−
−

−
−

−
−

−
−

−

−

−

−

It is important to note that although 1/(1
∆1) aligns quite well with generalization performance (Spearman
rank correlation coeﬃcients ρ equal to 0.958 and 0.948 on Census and CovType, respectively), it does not align
perfectly. In particular, we see that for a ﬁxed value of ∆1, Nystr¨om generally attains better heldout performance
than RFFs. We believe this is largely explained by the fact that Nystr¨om always has ∆2 = 0, while RFFs can have

−

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

−

Figure 9: The correlation between generalization performance and squared Frobenius norm, spectral norm, ∆,
and 1/(1
∆1) for various types of full-precision RFFs. The Spearman rank correlation coeﬃcients ρ between the
x and y metrics of each ﬁgure are included in the ﬁgure titles. For these full-precision RFF experiments, we see
∆1) both align very well with downstream performance
that the original ∆ (Avron et al., 2017) as well as 1/(1
−
(ρ = 0.940 and ρ = 0.938, respectively), while the Frobenius and spectral norms do not (ρ = 0.705 and ρ = 0.652,
respectively). Note that although we plot the average performance across ﬁve random seeds for each experimental
setting (error bars indicate standard deviation), when we compute the ρ values we treat each experimental result
independently (without averaging).

Figure 10: The generalization performance as a function of the number of features (left) and the training memory
footprint (right) for various types of RFFs on the Census dataset. We observe that LP-RFFs can achieve lower
heldout mean-squared error (MSE) than other types of RFFs, included the memory-eﬃcient structured orthogonal
random features (“FP-SORF (SC)”), under the same memory budget. We plot performance averaged over ﬁve
random seeds, with error bars indicating standard deviations.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

relatively large values of ∆2 when the number of features is small (see Figure 3). As we show in the generalization
bound in Proposition 1, in Figure 5, as well as in the empirical and theoretical analysis in Appendix B, we expect
generalization performance to deteriorate as ∆2 increases.

D.3.2 Experiments with Diﬀerent Types of RFFs

In Figure 9 we repeat the above experiment on the Census dataset using a number of variations of RFFs.
Speciﬁcally, we run experiments using the [sin(wT x), cos(wT x)] parameterization of RFFs, which Sutherland
and Schneider (2015) show has lower variance than the cos(wT x + a) parameterization (we denote the [sin, cos]
method by “FP-RFF (SC)”, and the cos method by “FP-RFF (C)”). We additionally use Quasi-Monte Carlo
features (Yang et al., 2014), as well as orthogonal random features and its structural variant (Yu et al., 2016); we
denote these by “FP-QMC (SC)”, “FP-ORF (SC)”, “FP-SORF (SC)” respectively, because we implement them
using the [sin, cos] parameterization. We can see in Figure 9 that although these methods attain meaningful
improvements in the Frobenius and spectral norms of the approximation error, these improvements do not
translate into corresponding gains in heldout performance. Once again we see that 1/(1
∆1) is able to much
better explain the relative performance between diﬀerent kernel approximation methods than Frobenius and
spectral error. In Figure 10 we see that LP-RFFs outperform these other methods in terms of performance under
a memory budget.

−

D.3.3 Large-Scale (∆1, ∆2) Experiments

∆1) on the large-scale experiments from Section 2.2. We measure
In Figure 11 we plot the performance vs. 1/(1
∆1 using the exact and approximate kernel matrices on a random subset of 20k heldout points (except for Census,
where we use the entire heldout set which has approximately 2k points). We pick several λ values between the
smallest and largest eigenvalues of the exact (subsampled) training kernel matrix. The strong alignment between
generalization performance and 1/(1

∆1) is quite robust to the choice of λ.

−

−

D.3.4 Measuring ∆ and (∆1, ∆2)

In order to measure the ∆1 and ∆2 between a kernel matrix K and an approximation ˜K, we ﬁrst observe that
the following statements are equivalent. Note that to get the second statement, we multiply the expressions in
the ﬁrst statement by (K + λIn)−

1/2 on the left and right.

(1

−

∆1)(K + λIn)
∆1)In

(1

−

∆1In

−

∆1In

−
1/2( ˜K

(cid:22)

(cid:22)

(cid:22)

(cid:22)

˜K + λIn
(cid:22)
(K + λIn)−

(K + λIn)−

(1 + ∆2)(K + λIn)
1/2( ˜K + λIn)(K + λIn)−
1/2(cid:16)

˜K + λIn

1/2

(cid:22)

(cid:17)

(1 + ∆2)In
1/2

(K + λIn)

(K + λIn)−

−

∆2In

(cid:22)

(K + λIn)−

1/2( ˜K

K)(K + λIn)−

1/2

∆2In.

−
1/2, this is equivalent to
(cid:0)A(cid:1). Lastly, we note that ∆ = max(∆1, ∆2).

∆1 ≤

λmin

−

(cid:22)

(cid:0)A(cid:1) and λmax

(cid:0)A(cid:1)

∆2. Thus,

≤

For A = (K + λIn)−
we choose ∆1 :=

λmin

K)(K + λIn)−
(cid:0)A(cid:1) and ∆2 := λmax

−

−

D.4 Theory Validation (Section 4.2)

To validate our theory in Section 4.2, we perform two sets of experiments to (1) demonstrate the asymptotic
behavior of ∆1 and ∆2 as the number of features increases, and (2) demonstrate that quantization has negligible
eﬀect on ∆2 when δ2

∆2.

b /λ

(cid:28)

To demonstrate the behavior of ∆1 and ∆2 as a function of the number of features, we collect ∆1, and ∆2 using
Nystr¨om features, circulant FP-RFFs, and LP-RFFs using b
, on both the Census and the sub-sampled
}
CovType datasets (20k random heldout points). For each approximation, we sweep the number of features as
listed in Table 6. We use the same value of λ as we used in Section 3.2 for each dataset. In Figure 12, we plot
the values of ∆1 and ∆2 attained by these methods as a function of the number of features. As discussed in
Section 4.2, ∆1 is primarily determined by the rank of the approximation, and approaches 0 for all the methods
as the number of features grows. ∆2, on the other hand, only approaches 0 for the high-precision methods—for
b

b /λ, marked by dashed lines).

, ∆2 converges to higher values (at most δ2
1, 4
}

1, 4, 8

∈ {

∈ {

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

1
∆1

1

−

, where we measure ∆1 using regularizer strength λ equal to the
Figure 11: Generalization performance vs.
0, 25, 50, 75, or 99 percentile eigenvalues of the exact kernel matrix (0 percentile indicates largest eigenvalue).
Note for the Census dataset, we plot the heldout MSE as a function of 1/(1
1 to avoid cluttering the
data points on the left end of the ﬁgure. For comparison to spectral and Frobenius norm plots, see Figure 7. To
∆1) and generalization performance for these diﬀerent values of
quantify the degree of alignment between 1/(1
λ, we compute the Spearman rank correlation coeﬃcients ρ. We see 1/(1
∆1) generally attains much higher
values of ρ than the Frobenius and spectral approximation errors (Figure 7). Although we plot performance
averaged across three random seeds (error bars indicate standard deviations), when we compute ρ we treat each
experimental result independently.

∆1)

−

−

−

−

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Figure 12: Empirical validation of Theorem 2. As the number of features grows, LP-RFFs approach ∆1 values of
0 (left), but plateau at larger ∆2 for very low precisions (right). These are an extended version of the results
from Figure 3 (left, middle) in Section 4.2. We plot average results across ﬁve random seeds, with error bars
indicating standard deviations.

Table 6: Number of features used for the diﬀerent kernel approximation methods in the theory validation
experiments in Section 4.2 (Figure 3 (left,middle)).

Methods

Number of features

FP-Nystr¨om
Cir. FP-RFF
LP-RFF 8, 4, 2, 1

25, 50, 100, 200, 500, 1250, 2500, 5000, 10000, 20000
200, 500, 1000, 2000, 5000, 10000, 20000, 50000, 100000, 200000, 400000
1000, 2000, 5000, 10000, 20000, 50000, 100000, 200000, 400000

To demonstrate that quantization has negligible eﬀect on ∆2 when δ2
points from the Census dataset. For λ
by the LP-RFFs relative to the exact kernel matrix. We see that for larger λ (corresponding to smaller δ2
Figure 3, lower precisions can be used while not inﬂuencing ∆2 signiﬁcantly; this aligns with the theory.

∆2, we use 8000 random training
, we measure the ∆2 attained
b /λ) in

1, 2, 4, 8, 16, 32
}

, and b
}

4, 100, 104

b /λ

10−

∈ {

∈ {

(cid:28)

D.5 Empirical Evaluation of LP-RFFs (Section 5.1)

To empirically demonstrate the generalization performance of LP-RFFs, we compare LP-RFFs to FP-RFFs,
circulant FP-RFFs, and Nystr¨om features for various memory budgets. We use the same datasets as in Section 2,
including Census and YearPred for regression, as well as CovType and TIMIT for classiﬁcation. We use the same
experimental protocol as in Section 2 (details in Appendix D.2), with the only signiﬁcant additions being that we
also evaluate the performance of circulant FP-RFFs, and LP-RFFs for precisions b
. As noted in
}
the main text, all our LP-RFF experiments are done in simulation; in particular, we represent each low-precision
feature as a 64-bit ﬂoating point number, whose value is one of the 2b values representable in b bits. For our
full-precision experiments we also use 64-bit ﬂoats, but we report the memory utilization of these experiments
as if we used 32 bits, to avoid inﬂating the relative gains of LP-RFFs over the full-precision approaches. We
randomly sample the quantization noise for each mini-batch independently each epoch.

1, 2, 4, 8, 16

∈ {

In Section 5.1 (Figure 4), we demonstrated the generalization performance of LP-RFFs on the TIMIT, YearPred,
and CovType datasets, using 4 bits per feature. In Figure 13, we additionally include results on the Census
dataset, and include results for a larger set of precisions (b
). We also include plots of heldout
performance as a function of the number of features used. We observe that LP-RFFs, using 2-8 bits, systematically
outperform the full-precision baselines under diﬀerent memory budgets.

1, 2, 4, 8, 16

∈ {

}

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 13: Generalization performance of the kernel approximation methods (Nystr¨om, FP-RFFs, circ. FP-RFFs,
LP-RFFs) with respect to the number of features used, as well as with respect to memory used. We observe
that LP-RFFs demonstrate better generalization performance than the full-precision baselines under memory
constraints, with 2-8 bits typically giving the best performance. Importantly, the relative ranking of the methods
changes depending on whether we compare the methods based on their number of features, or their memory
utilization. For example, the Nystr¨om method shows better generalization performance than the RFF-based
approaches with the same number of features. However, the Nystr¨om method often performs signiﬁcantly worse
than the RFF methods under ﬁxed memory budgets. We plot results averaged over three random seeds.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Table 7: Dataset details for the additional datasets from Section D.5. For classiﬁcation tasks, we write the number
of classes in parentheses in the “Task” column.

Dataset

Task

Train Heldout Test # Features

Class. (2)
Forest
Cod-RNA Class. (2)
Class. (2)
Adult
Reg.
CPU

470k
54k
29k
6k

52k
6k
3k
0.7k

58k
272k
16k
0.8k

54
8
123
21

Table 8: The Gaussian kernel bandwidths used, and the search grid for initial learning rate on the Forest,
Cod-RNA, Adult, and CPU datasets. Optimal learning rate in bold.

Dataset

1/2σ2

Initial learning rate grid

Forest
0.5
Cod-RNA 0.4
0.1
Adult
0.03
CPU

5.0, 10.0, 50.0, 100.0, 500.0
10.0, 50.0, 100.0, 500.0, 1000.0
5.0, 10.0, 50.0, 100.0, 500.0, 1000.0
0.05, 0.1, 0.5, 1.0, 5.0

We run on four additional classiﬁcation and regression datasets (Forest, Cod-RNA, Adult, CPU) to compare the
empirical performance of LP-RFFs to full-precision RFFs, circulant RFFs and Nystr¨om. We present the results
in Figure 14, and observe that LP-RFFs can achieve competitive generalization performance to the full-precision
baselines with lower training memory budgets. With these 4 additional datasets, our empirical evaluation of
LP-RFFs now covers all the datasets investigated in Yang et al. (2012). We include details about these datasets
and the hyperparameters we used in Tables 7 and 8.

D.6 Generalization Performance vs. (∆1, ∆2) (Section 5.2)

For our experiments in Section 5.2, we use the same protocol and hyperparameters (learning rate, λ) as for the
(∆1, ∆2) experiments in Section 3.2. However, we additionally run experiments with LP-RFFs for precisions
b
. We use ﬁve random seeds for each experimental setting, and plot the average results, with
error bars indicating standard deviations.

1, 2, 4, 8, 16

∈ {

}

In Figure 15, we plot an extended version of the right plots in Figure 5. We include results for all precisions, and
for both Census and CovType. We observe that on Census, 1/(1
∆1) does not align well with performance
(Spearman rank correlation coeﬃcient ρ = 0.403), because the low-precision features (b = 1 or b = 2) perform
signiﬁcantly worse than the full-precision features of the same dimensions. In this case, when we consider the
impact of ∆2 by taking max (cid:0)1/(1
On CovType, on the other hand, the impact on generalization performance from using low-precision is much less
(cid:1) both align well with performance (ρ = 0.942). Furthermore,
pronounced, so 1/(1
in the case of CovType, ∆2 is generally smaller than 1/(1
∆1), so taking the max does not change the plot
signiﬁcantly.

(cid:1), we see that performance aligns much better (ρ = 0.959).

∆1) and max (cid:0)1/(1

∆1), ∆2

∆1), ∆2

−

−

−

−

−

To compute the Spearman rank correlation coeﬃcients ρ for these plots, we take the union of all the experiments
which are a part of the plot. In particular, we include FP-RFFs, circulant FP-RFFs, FP-Nystr¨om , and LP-RFFs
with precisions b
. Although we plot the average performance across ﬁve random seeds in the plot,
}
to compute ρ we treat each experiment independently.

1, 2, 4, 8, 16

∈ {

D.7 Other Experimental Results

D.7.1 Low-Precision Nystr¨om

We encounter two main obstacles to attaining strong performance with the Nystr¨om method under a memory
budget: (1) For the standard Nystr¨om method, because of the large projection matrix (32m2 space), there are

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 14: Comparison of the performance of LP-RFFs and the full-precision baselines on additional datasets,
with respect to the number of features used, as well as with respect to memory used. We observe that LP-RFFs
can achieve performance competitive with the full-precision baseline methods, with signiﬁcant memory savings.
We plot the average performance across three random seeds, with error bars indicating standard deviations.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

−

Figure 15: Generalization performance vs. diﬀerent kernel approximation metrics on Census and CovType. The
metric max (1/(1
∆1), ∆2) is able to incorporate the inﬂuence of both ∆1 and ∆2 on performance for LP-RFFs,
aligning well with generalization performance on both Census (Spearman rank correlation coeﬃcient ρ = 0.959)
and CovType (ρ = 0.942). 1/(1
∆1), on the other hand, fails to align well on Census (ρ = 0.403), but does
align on CovType (ρ = 0.942). Note that although we plot the average performance across ﬁve random seeds for
each experimental setting (error bars indicate standard deviation), when we compute the ρ values we treat each
experimental result independently (without averaging).

−

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 16: We plot the heldout performance (MSE) for the full-precision Nystr¨om method, the ensemble Nystr¨om
method, and 8-bit LP-RFFs. We also show the best possible performance for the Nystr¨om methods, assuming only
the kernel approximation features are quantized (denoted “ideal”); we compute this by plotting the full-precision
results but without counting the memory occupied by the features. The LP-RFF method signiﬁcantly outperforms
the “ideal” Nystr¨om methods as a function of memory.

(a) LP-Nystr¨om

(b) Ensemble LP-Nystr¨om

Figure 17: The generalization performance of low-precision Nystr¨om and low-precision ensemble Nystr¨om, using
a uniform quantization scheme.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

inherent limits to the memory savings attainable by only quantizing the features, regardless of the quantization
scheme used. (2) While the ensemble Nystr¨om method (Kumar et al., 2009) is a well-known method which can
dramatically reduce the space occupied by the the Nystr¨om projection matrix, it does not attain meaningful gains
in performance under a memory budget. To demonstrate the ﬁrst issue empirically, we plot in Figure 16 the
performance of the full-precision Nystr¨om methods without counting the memory from the feature mini-batches
(denoted “ideal”). This is the best possible performance under any quantization scheme for these Nystr¨om
methods, assuming only the features are quantized. LP-RFFs still outperform these “ideal” methods for ﬁxed
memory budgets. To demonstrate the second issue, in Figure 16 we also plot the generalization performance of
the ensemble method vs. the standard Nystr¨om method, as a function of memory. We can see that the ensemble
method does not attain meaningful gains over the standard Nystr¨om method, as a function of memory.

(cid:112)

(cid:112)

2/m and

Lastly, we mention that quantizing Nystr¨om features is more challenging due to their larger dynamic range.
1 and 1, whereas for RFFs, each feature is between
For Nystr¨om, all we know is that each feature is between
2/m. In Figure 17 we plot our results quantizing Nystr¨om features with the following simple
−
scheme: for each feature, we ﬁnd the maximum and minimum value on the training set, and then uniformly
quantize this interval using b bits. We observe that performance degrades signiﬁcantly with less than or equal to
8 bits compared to full-precision Nystr¨om. Although using the ensemble method reduces the dynamic range by a
factor of √r with r blocks (we use r = 10, a common setting in (Kumar et al., 2012)), and also saves space on
the projection matrix, these strengths do not result in signiﬁcantly better performance for ﬁxed memory budgets.

−

D.7.2 Low-Precision Training for LP-RFFs

As discussed in Section 2.1, there are a number of ways to reduce the memory occupied by the model parameters,
including (1) using a low-rank decomposition of the parameter matrix (Sainath et al., 2013a), (2) using structured
matrices (Sindhwani et al., 2015), and (3) using low-precision (De Sa et al., 2018). Though these methods are
orthogonal to our LP-RFF method, we now present results using a low-precision parameterization of the model,
and show that we can attain similar performance to full-precision training. We use a training algorithm called
LM-HALP (linear model high-accuracy low-precision) (De Sa et al., 2018). By parameterizing the model in low
precision, this algorithm eliminates the need of casting the LP-RFFs back into full precision in order to multiply
them with the model parameters. This approach also allows for these matrix multiplications to be implemented
using fast low-precision matrix operations, and reduces the memory occupied by the model during training.

LM-HALP is based on the stochastic variance-reduced gradient (SVRG) algorithm. The model is parameterized
using a low-precision ﬁxed-point representation during training. In LM-HALP, all of the matrix multiplications
involved in the stochastic model updates are done using low-precision ﬁxed-point operations; however, the periodic
computation of the full gradient is calculated in full precision (this is embarrassingly parallelizable). Importantly,
even though most of training is done in low precision, the ﬁnal model returned by this training algorithm is a
full-precision model. We can further simplify the LM-HALP algorithm by replacing the SVRG updates with
SGD updates, thus eliminating the need for calculating and storing the full gradient. In Figure 18 we present our
results using LM-HALP on TIMIT, our largest and most challenging dataset; we use 8-bit LM-HALP (SVRG
and SGD) on top of 8-bit LP-RFFs, and compare to full-precision SGD training. For LM-HALP based training,
we perform the bit centering and rescaling operation after each training epoch. Under this setting, we show that
when the number of features is at least 10,000, the performance of both versions of HALP closely matches that of
full-precision training.

D.7.3 Double Sampling

We perform some initial experiments using the double sampling method of Zhang et al. (2017). In particular,
we use a diﬀerent random quantization of the LP-RFFs on the “forward pass” of our algorithm than in the
“backward pass.” In our initial experiments with double sampling, as shown in Figure 19, we did not observe
noticeable improvements in performance. These experiments were on the YearPred dataset with the Gaussian
kernel. We leave a more extended investigation of these gradient bias reduction methods for future work.

E EXTENDED RELATED WORK

Generalization Performance of Kernel Approximation Methods From a theoretical perspective, there
has been a lot of work analyzing the generalization performance of kernel approximation methods (Rahimi

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 18: Low-precision vs. full-precision training algorithms on TIMIT using 8-bit LP-RFFs.

Figure 19: Comparison of LP-RFFs with and without double sampling on the YearPred dataset.

and Recht, 2008; Cortes et al., 2010; Sutherland and Schneider, 2015; Rudi and Rosasco, 2017; Avron et al.,
2017; Li et al., 2018). The work most relevant to ours is that of Avron et al. (2017), which deﬁnes ∆-spectral
approximation and bounds the generalization performance of kernel approximation methods in terms of ∆. This
approach diﬀers from works which evaluate kernel approximation methods in terms of the Frobenius or spectral
norms of their kernel approximation matrices (Cortes et al., 2010; Gittens and Mahoney, 2016; Yang et al., 2014;
Sutherland and Schneider, 2015; Yu et al., 2016; Dao et al., 2017). Our work shows the promise of Avron et al.’s
approach, and builds upon it.

Large-Scale Kernel Experiments On the topic of scaling kernel methods to large datasets, there have been
a few notable recent papers. Tu et al. (2016) propose a distributed block coordinate descent method for solving
large-scale least squares problems using the Nystr¨om method or RFFs. The recent work of May et al. (2017) uses
a single GPU to train large RFF models on speech recognition datasets, showing comparable performance to
fully-connected deep neural networks. That work was limited by the number of features that could ﬁt on a single
GPU, and thus our proposed method could help scale these results.

Low-Precision Random Fourier Features for Memory-Constrained
Kernel Approximation

9
1
0
2
 
r
a

M
 
0
2
 
 
]

G
L
.
s
c
[
 
 
2
v
5
5
1
0
0
.
1
1
8
1
:
v
i
X
r
a

Jian Zhang∗

Avner May∗

Tri Dao

Christopher R´e

Stanford University

Abstract

We investigate how to train kernel approxi-
mation methods that generalize well under
a memory budget. Building on recent theo-
retical work, we deﬁne a measure of kernel
approximation error which we ﬁnd to be more
predictive of the empirical generalization per-
formance of kernel approximation methods
than conventional metrics. An important con-
sequence of this deﬁnition is that a kernel
approximation matrix must be high rank to
attain close approximation. Because storing a
high-rank approximation is memory intensive,
we propose using a low-precision quantiza-
tion of random Fourier features (LP-RFFs)
to build a high-rank approximation under a
memory budget. Theoretically, we show quan-
tization has a negligible eﬀect on generaliza-
tion performance in important settings. Em-
pirically, we demonstrate across four bench-
mark datasets that LP-RFFs can match the
performance of full-precision RFFs and the
Nystr¨om method, with 3x-10x and 50x-460x
less memory, respectively.

1 INTRODUCTION

Kernel methods are a powerful family of machine learn-
ing methods. A key technique for scaling kernel meth-
ods is to construct feature representations whose inner
products approximate the kernel function, and then
learn a linear model with these features; important ex-
amples of this technique include the Nystr¨om method
(Williams and Seeger, 2000) and random Fourier fea-
tures (RFFs) (Rahimi and Recht, 2007). Unfortunately,
a large number of features are typically needed for at-
taining strong generalization performance with these

∗Equal contribution.

Proceedings of the 22nd International Conference on Ar-
tiﬁcial Intelligence and Statistics (AISTATS) 2019, Naha,
Okinawa, Japan. PMLR: Volume 89. Copyright 2019 by
the author(s).

methods on big datasets (Rahimi and Recht, 2008; Tu
et al., 2016; May et al., 2017). Thus, the memory re-
quired to store these features can become the training
bottleneck for kernel approximation models. In this
paper we work to alleviate this memory bottleneck by
optimizing the generalization performance for these
methods under a ﬁxed memory budget.

To gain insight into how to design more memory-
eﬃcient kernel approximation methods, we ﬁrst in-
vestigate the generalization performance vs. memory
utilization of Nystr¨om and RFFs. While prior work
(Yang et al., 2012) has shown that the Nystr¨om method
generalizes better than RFFs under the the same num-
ber of features, we demonstrate that the opposite is
true under a memory budget. Strikingly, we observe
that 50,000 standard RFFs can achieve the same held-
out accuracy as 20,000 Nystr¨om features with 10x less
memory on the TIMIT classiﬁcation task. Further-
more, this cannot be easily explained by the Frobenius
or spectral norms of the kernel approximation error ma-
trices of these methods, even though these norms are
the most common metrics for evaluating kernel approx-
imation methods (Gittens and Mahoney, 2016; Yang
et al., 2014; Sutherland and Schneider, 2015; Yu et al.,
2016; Dao et al., 2017); the above Nystr¨om features
attain 1.7x smaller Frobenius error and 17x smaller
spectral error compared to the RFFs. This observa-
tion suggests the need for a more reﬁned measure of
kernel approximation error—one which better aligns
with generalization performance, and can thus better
guide the design of new approximation methods.

Building on recent theoretical work (Avron et al., 2017),
we deﬁne a measure of approximation error which we
ﬁnd to be much more predictive of empirical generaliza-
tion performance than the conventional metrics. In par-
ticular, we extend Avron et al.’s deﬁnition of ∆-spectral
approximation to our deﬁnition of (∆1, ∆2)-spectral ap-
proximation by decoupling the two roles played by ∆
in the original deﬁnition.1 This decoupling reveals that

1The original deﬁnition uses the same scalar ∆ to upper
and lower bound the approximate kernel matrix in terms
of the exact kernel matrix in the semideﬁnite order.

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Rd, kernel features
Table 1: Memory utilization for kernel approximation methods. We consider data x
Rm, mini-batch size s, # of classes c (for regression/binary classiﬁcation c = 1). We assume full-precision
z(x)
numbers are 32 bits. We measure a method’s memory utilization as the sum of the three components in this table.

∈

∈

Approximation Method

Nystr¨om
RFFs
Circulant RFFs
Low-precision RFFs, b bits (ours)

Feature generation Feature mini-batch Model parameters
32(md + m2)
32md
32m
32m

32ms
32ms
32ms
bms

32mc
32mc
32mc
32mc

∆1 and ∆2 impact generalization diﬀerently, and can
together much better explain the relative generalization
performance of Nystr¨om and RFFs than the original
∆, or the Frobenius or spectral errors. This (∆1, ∆2)
deﬁnition has an important consequence—in order for
an approximate kernel matrix to be close to the exact
kernel matrix, it is necessary for it to be high rank.

Motivated by the above connection between rank and
generalization performance, we propose using low-
precision random Fourier features (LP-RFFs) to attain
a high-rank approximation under a memory budget.
Speciﬁcally, we store each random Fourier feature in a
low-precision ﬁxed-point representation, thus achieving
a higher-rank approximation with more features in the
same amount of space. Theoretically, we show that
when the quantization noise is much smaller than the
regularization parameter, using low precision has negli-
gible eﬀect on the number of features required for the
approximate kernel matrix to be a (∆1, ∆2)-spectral
approximation of the exact kernel matrix. Empiri-
cally, we demonstrate across four benchmark datasets
(TIMIT, YearPred, CovType, Census) that in the mini-
batch training setting, LP-RFFs can match the perfor-
mance of full-precision RFFs (FP-RFFs) as well as the
Nystr¨om method, with 3x-10x and 50x-460x less mem-
ory, respectively. These results suggest that LP-RFFs
could be an important tool going forward for scaling
kernel methods to larger and more challenging tasks.

The rest of this paper is organized as follows: In Section
2 we compare the performance of the Nystr¨om method
and RFFs in terms of their training memory footprint.
In Section 3 we present a more reﬁned measure of kernel
approximation error to explain the relative performance
of Nystr¨om and RFFs. We introduce the LP-RFF
method and corresponding analysis in Section 4, and
present LP-RFF experiments in Section 5. We review
related work in Section 6, and conclude in Section 7.

2 NYSTR ¨OM VS. RFFS: AN

EMPIRICAL COMPARISON

tion of Nystr¨om and RFFs. We begin by reviewing
the memory utilization for these kernel approximation
methods in the mini-batch training setting; this is a
standard setting for training large-scale kernel approxi-
mation models (Huang et al., 2014; Yang et al., 2015;
May et al., 2017), and it is the setting we will be us-
ing to evaluate the diﬀerent approximation methods
(Sections 2.2, 5.1). We then show that RFFs outper-
form Nystr¨om given the same training memory budget,
even though the opposite is true given a budget for
the number of features (Yang et al., 2012). Lastly, we
demonstrate that the Frobenius and spectral norms of
the kernel approximation error matrix align poorly with
generalization performance, suggesting the need for a
more reﬁned measure of approximation error for eval-
uating the quality of a kernel approximation method;
we investigate this in Section 3.

For background on RFFs and the Nystr¨om method,
and for a summary of our notation, see Appendix A.

2.1 Memory Utilization

The optimization setting we consider is mini-batch
training over kernel approximation features. To un-
derstand the training memory footprint, we present in
Table 1 the memory utilization of the diﬀerent parts
of the training pipeline. The three components are:

1. Feature generation: Computing m RFFs over data
in Rd requires a random projection matrix W
Rm
×
points” ˆxi

∈
d. The Nystr¨om method stores m “landmark
m.

Rd, and a projection matrix in Rm
×

2. Feature mini-batch: Kernel approximation features
Rm for all xi in a mini-batch are stored.2

z(xi)

∈

∈

3. Model parameters: For binary classiﬁcation and
regression, the linear model learned on the z(x) fea-
Rm; for c-class classiﬁcation,
tures is a vector θ
it is a matrix θ
×

∈
Rm

c.

∈

In this work we focus on reducing the memory occupied
by the mini-batches of features, which can occupy a

To inform our design of memory-eﬃcient kernel approx-
imation methods, we ﬁrst perform an empirical study
of the generalization performance vs. memory utiliza-

2For simplicity, we ignore the memory occupied by the
mini-batches of d-dim. inputs and c-dim. outputs, as gener-
ally the number of kernel approx. features m

d, c.

(cid:29)

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

(a)

(b)

(c)

(d)

Figure 1: Generalization performance of full-precision RFFs and Nystr¨om with respect to the number of features
and training memory footprint on TIMIT (a,b). Nystr¨om performs better for a ﬁxed number of features, while
RFFs perform better under a memory budget. We also see that the generalization performance of these methods
does not align well with the Frobenius or spectral norms of their respective kernel approximation error matrices
(c,d). For results on YearPred, CovType, and Census, see Appendix D.2.

signiﬁcant fraction of the training memory. Our work
is thus orthogonal to existing work which has shown
how to reduce the memory utilization of the feature
generation (Le et al., 2013; Yu et al., 2015) and the
model parameters (Sainath et al., 2013a; Sindhwani
et al., 2015; De Sa et al., 2018) (e.g., using structured
matrices or low precision). Throughout this paper, we
measure the memory utilization of a kernel approxima-
tion method as the sum of the above three components.

2.2 Empirical Comparison

We now compare the generalization performance of
RFFs and the Nystr¨om method in terms of their train-
ing memory footprint. We demonstrate that RFFs
can outperform the Nystr¨om method given a memory
budget, and show that the diﬀerence in performance
between these methods cannot be explained by the
Frobenius or spectral norms of their kernel approxima-
tion error matrices.

In experiments across four datasets (TIMIT, YearPred,
CovType, Census (Garofolo et al., 1993; Dheeru and
Karra Taniskidou, 2017)), we use up to 20k Nystr¨om
features and 400k RFFs to approximate the Gaus-
sian kernel;3 we train the models using mini-batch
stochastic gradient descent with early stopping, with
a mini-batch size of 250. We present results averaged
from three random seeds, with error bars indicating
standard deviations (for further experiment details,
see Appendix D.2). In Figure 1(a) we observe that
as a function of the number of kernel approximation
features the Nystr¨om method generally outperforms
RFFs, though the gap narrows as m approaches 20k.
However, we see in Figure 1(b) that RFFs attain better
generalization performance as a function of memory.
Interestingly, the relative performance of these meth-

ods cannot simply be explained by the Frobenius or
spectral norms of the kernel approximation error ma-
trices;4 in Figure 1(c,d) we see that there are many
cases in which the RFFs attain better generalization
performance, in spite of having larger Frobenius or
spectral approximation error. This is a phenomenon
we observe on other datasets as well (Appendix D.2).
This suggests the need for a more reﬁned measure of the
approximation error of a kernel approximation method,
which we discuss in the following section.

3 A REFINED MEASURE OF
KERNEL APPROX. ERROR

To explain the important diﬀerences in performance be-
tween Nystr¨om and RFFs, we deﬁne a more reﬁned mea-
sure of kernel approximation error—(∆1, ∆2)-spectral
approximation. Our deﬁnition is an extension of Avron
et al.’s deﬁnition of ∆-spectral approximation, in which
we decouple the two roles played by ∆ in the original
deﬁnition. This decoupling allows for a more ﬁne-
grained understanding of the factors inﬂuencing the
generalization performance of kernel approximation
methods, both theoretically and empirically. Theoret-
ically, we present a generalization bound for kernel
approximation methods in terms of (∆1, ∆2) (Sec. 3.1),
and show that ∆1 and ∆2 inﬂuence the bound in dif-
ferent ways (Prop. 1). Empirically, we show that ∆1
and ∆2 are more predictive of the Nystr¨om vs. RFF
performance than the ∆ from the original deﬁnition,
and the Frobenius and spectral norms of the kernel
approximation error matrix (Sec. 3.2, Figure 2). An
important consequence of the (∆1, ∆2) deﬁnition is
that attaining a small ∆1 requires a large number of
features; we leverage this insight to motivate our pro-
posed method, low-precision random Fourier features,
in Section 4.

3We consider diﬀerent ranges for the number of Nystr¨om
vs. RFF features because the memory footprint for training
with 400k RFFs is similar to 20k Nystr¨om features.

˜K,
4We consider the Frobenius and spectral norms of K
where K and ˜K are the exact and approximate kernel
matrices for 20k randomly sampled heldout points.

−

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

3.1

(∆1, ∆2)-spectral Approximation

We begin by reviewing what it means for a matrix
A to be a ∆-spectral approximation of a matrix B
(Avron et al., 2017). We then extend this deﬁnition
to (∆1, ∆2)-spectral approximation, and bound the
generalization performance of kernel approximation
methods in terms of ∆1 and ∆2 in the context of ﬁxed
design kernel ridge regression.
0, a symmetric matrix A is a
Deﬁnition 1. For ∆
∆-spectral approximation of another symmetric matrix
(1 + ∆)B.
B if (1

∆)B

≥

A

−

(cid:22)

(cid:22)

We extend this deﬁnition by allowing for diﬀerent values
of ∆ in the left and right inequalities above:
0, a symmetric matrix
Deﬁnition 2. For ∆1, ∆2 ≥
A is a (∆1, ∆2)-spectral approximation of another
(1 + ∆2)B.
symmetric matrix B if (1

−
Throughout the text, we will use ∆ to denote the
variable in Def. 1, and (∆1, ∆2) to denote the variables
In our discussions and experiments, we
in Def. 2.
always consider the smallest ∆, ∆1, ∆2 satisfying the
above deﬁnitions; thus, ∆ = max(∆1, ∆2).

∆1)B

(cid:22)

(cid:22)

A

In the paragraphs that follow we present generalization
bounds for kernel approximation models in terms of
∆1 and ∆2 in the context of ﬁxed design kernel ridge
regression, and demonstrate that ∆1 and ∆2 inﬂuence
generalization in diﬀerent ways (Prop. 1). We consider
the ﬁxed design setting because its expected generaliza-
tion error has a closed-form expression, which allows us
to analyze generalization performance in a ﬁne-grained
fashion. For an overview of ﬁxed design kernel ridge
regression, see Appendix A.3.

×

n, a regularization parameter λ
(xi, yi)

In the ﬁxed design setting, given a kernel matrix K
∈
Rn
0, and a set of
≥
n
labeled points
i=1 where the observed labels
}
yi = ¯yi + (cid:15)i are randomly perturbed versions of the true
(cid:3) = σ2 <
R ((cid:15)i independent, E [(cid:15)i] = 0, E (cid:2)(cid:15)2
labels ¯yi
), it is easy to show (Alaoui and Mahoney, 2015) that

∞
the optimal kernel regressor5 fK has expected error

∈

{

i

(fK) =

R

λ2
n

¯yT (K + λI)−

2 ¯y +

tr

K 2(K + λI)−

2(cid:17)

,

(cid:16)

σ2
n

where ¯y = (¯y1, . . . , ¯yn) is the vector of true labels.

This closed-form expression for generalization error
allows us to bound the expected loss
(f ˜K) of a kernel
ridge regression model f ˜K learned using an approximate
kernel matrix ˜K in place of the exact kernel matrix K.
In particular, if we deﬁne

R

(fK) :=

(cid:98)
R

λ
n

¯yT (K + λI)−

1 ¯y +

tr

K(K + λI)−

1(cid:17)

,

(cid:16)

σ2
n

5fK (x) = (cid:80)

i αik(x, xi) for α = (K + λI)−1y.

∞

(fK), we can bound the

which is an upper bound on
R
expected loss of f ˜K as follows:
Proposition 1. (Extended from (Avron et al., 2017))
Suppose ˜K + λI is (∆1, ∆2)-spectral approximation of
0. Let m denote
[0, 1) and ∆2 ≥
K + λI, for ∆1 ∈
the rank of ˜K, and let fK and f ˜K be the kernel ridge
regression estimators learned using these matrices, with
regularizing constant λ
0 and label noise variance
σ2 <
. Then

≥

(f ˜K)

R

≤

1

1
∆1

(cid:98)
R

−

(fK) +

∆2
1 + ∆2

m
n

σ2.

(1)

We include a proof in Appendix B.1. This result shows
that smaller values for ∆1 and ∆2 imply tighter bounds
on the generalization performance of the model trained
with ˜K. We can see that as ∆1 approaches 1 the bound
diverges, and as ∆2 approaches
the bound plateaus.
We leverage this generalization bound to understand
the diﬀerence in performance between Nystr¨om and
RFFs (Sec. 3.2), and to motivate and analyze our pro-
posed low-precision random Fourier features (Sec. 4).

∞

Remark The generalization bound in Prop. 1 as-
sumes the regressor fK is computed via the closed-
form solution for kernel ridge regression. However, in
Sections 4-5 we focus on stochastic gradient descent
(SGD) training for kernel approximation models. Be-
cause SGD can also ﬁnd the model which minimizes
the regularized empirical loss (Nemirovski et al., 2009),
the generalization results carry over to our setting.

3.2 Revisiting Nystr¨om vs. RFF Comparison

In this section we show that the values of ∆1 and ∆2
such that the approximate kernel matrix is a (∆1, ∆2)-
spectral approximation of the exact kernel matrix cor-
relate better with generalization performance than the
original ∆, and the Frobenius and spectral norms of
the kernel approximation error; we measure correlation
using Spearman’s rank correlation coeﬃcient ρ.

To study the correlation of these metrics with gen-
eralization performance, we train Nystr¨om and RFF
models for many feature dimensions on the Census
regression task, and on a subsampled version of 20k
train and heldout points from the CovType classiﬁca-
tion task. We choose these small datasets to be able
to compute the various measures of kernel approxima-
tion error over the entire heldout set. We measure the
˜K, and the ∆
spectral and Frobenius norms of K
and (∆1, ∆2) values between K + λI and ˜K + λI (λ
chosen via cross-validation), where K and ˜K are the
exact and approximate kernel matrices for the heldout
set. For more details about these experiments and how
we compute ∆ and (∆1, ∆2), see Appendix D.3.

−

In Figure 2, we plot the generalization performance on
these tasks as a function of these metrics; while the

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Figure 2: The correlation between generalization performance and diﬀerent measures of kernel approximation
error for the full-precision RFF and Nystr¨om methods. We see that generalization performance aligns well with
1/(1
∆1) (Spearman rank correlation coeﬃcient ρ = 0.958), while aligning poorly with ∆ and the spectral and
squared Frobenius norms of the kernel approximation error matrix. See Appendix D.3 for results on CovType.

−

1

1

−

−

1
∆1

1
∆1

original ∆ and the Frobenius and spectral norms gener-
ally do not align well with generalization performance,
we see that
attains a
does. Speciﬁcally,
Spearman rank correlation coeﬃcient of ρ = 0.958,
while squared Frobenius norm, spectral norm, and the
original ∆ attain values of 0.594, 0.540, and 0.759.6
In Appendix D.3 we show these trends are robust to
diﬀerent kernel approximation methods and datasets.
For example, we show that while other approximation
methods (e.g., orthogonal RFFs (Yu et al., 2016)), like
Nystr¨om, can attain much lower Frobenius and spectral
error than standard RFFs, this does not translate to
improved ∆1 or heldout performance. These results
mirror the generalization bound in Proposition 1, which
grows linearly with
. For simplicity, we ignore
the role of ∆2 here, as ∆1 appears to be suﬃcient for
explaining the main diﬀerences in performance between
these full-precision methods.7 In Sections 4.2 and 5.2,
however, we show that ∆2 has a large inﬂuence on
generalization performance for low-precision features.

1
∆1

−

1

Now that we have seen that ∆1 has signiﬁcant theoreti-
cal and empirical impact on generalization performance,
it is natural to ask how to construct kernel approxi-
mation matrices that attain small ∆1. An important
consequence of the deﬁnition of ∆1 is that for ˜K +λI to
have small ∆1 relative to K + λI, ˜K must be high-rank ;
λm+1(K)
in particular, a necessary condition is ∆1 ≥
λm+1(K)+λ ,
where m is the rank of ˜K and λi(K) is the ith largest
eigenvalue of K.8 This sets a lower bound on the rank
necessary for ˜K to attain small ∆1 which holds regard-
less of the approximation method used, motivating us
to design high-rank kernel approximation methods.

6 One reason ∆1 correlates better than ∆ is because
when ∆2 > ∆1, ∆ = max(∆1, ∆2) hides the value of ∆1.
This shows why decoupling the two roles of ∆ is important.
∆1) aligns well with performance, it is
not perfect—for a ﬁxed ∆1, Nystr¨om generally performs
slightly better than RFFs. In App. D.3.1 we suggest this is
because Nystr¨om has ∆2 = 0 while RFFs has larger ∆2.

7While 1/(1

−

8By deﬁnition, (K + λI)(1

˜K + λI. By Weyl’s
λi( ˜K) + λ.
inequality this implies
If ˜K is rank m, then λm+1( ˜K) = 0, and the result follows.

−
(cid:22)
i (λi(K) + λ)(1

∆1)

∆1)

−

≤

∀

4 LOW-PRECISION RANDOM

FOURIER FEATURES (LP-RFFS)

Taking inspiration from the above-mentioned connec-
tion between the rank of the kernel approximation
matrix and generalization performance, we propose
low-precision random Fourier features (LP-RFFs) to
create a high-rank approximation matrix under a mem-
ory budget. In particular, we quantize each random
Fourier feature to a low-precision ﬁxed-point represen-
tation, thus allowing us to store more features in the
same amount of space. Theoretically, we show that
when the quantization noise is small relative to the
regularization parameter, using low precision has mini-
mal impact on the number of features required for the
approximate kernel matrix to be a (∆1, ∆2)-spectral
approximation of the exact kernel matrix; by Propo-
sition 1, this implies a bound on the generalization
performance of the model trained on the low-precision
features. At the end of this section (Section 4.3), we
discuss a memory-eﬃcient implementation for training
a full-precision model on top of LP-RFFs.

4.1 Method Details

∈

−

(cid:112)

(cid:112)

(cid:112)

2/m,

i x + ai)

2/m cos(wT

1 sub-intervals of equal size r =

2/m] for the RFF vector z(x)

The core idea behind LP-RFFs is to use b bits to store
each RFF, instead of 32 or 64 bits. We implement
this with a simple stochastic rounding scheme. We use
the parametrization zi(x) =
∈
Rm
[
−
(Rahimi and Recht, 2007), and divide this interval
into 2b
1 . We
−
then randomly round each feature zi(x) to either the
top or bottom of the sub-interval [z, z] containing it,
in such a way that the expected value is equal to zi(x);
speciﬁcally, we round zi(x) to z with probability z
z
−
z
z
−
and to z with probability z
z
z . The variance of this
z
stochastic rounding scheme is at most δ2
b /m, where
b := 2/(2b
δ2
1)2 (Prop. 7 in App. C.2). For each low-
precision feature ˜zi(x) we only need to store the integer
j
2/m + jr, which
−
takes b bits. Letting ˜Z
m denote the matrix

1] such that ˜zi(x) =

2√2/m
2b

[0, 2b

Rn

(cid:112)

−
−

−

−

∈

×

∈

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 3: Empirical validation of Theorem 2. In the left and middle plots (shared legend), we see that as the #
of features grows, LP-RFFs approach ∆1 = 0, but plateau at larger ∆2 values (at most δ2
b /λ, marked by dashed
lines) for very low precisions. In the right plot we see that the larger λ is, the lower the precision at which using
low precision does not impact ∆2. For ∆1 and ∆2 vs. # features plots on CovType, see Appendix D.4.

of quantized features, we call ˜K = ˜Z ˜Z T an m-feature
b-bit LP-RFF approximation of a kernel matrix K.

As a way to further reduce the memory footprint during
training, we leverage existing work on using circulant
random matrices (Yu et al., 2015) for the RFF random
projection matrix to only occupy 32m bits.9 All our
LP-RFF experiments use circulant projections.

4.2 Theoretical Results

In this section we show quantization has minimal im-
pact on the number of features required to guarantee
strong generalization performance in certain settings.
We do this in the following theorem by lower bounding
the probability that ˜K + λI is a (∆1, ∆2)-spectral ap-
proximation of K + λI, for the LP-RFF approximation
˜K using m features and b bits per feature.10
˜K be an m-feature b-bit LP-
Theorem 2. Let
RFF approximation of a kernel matrix K, assume
1)2, and deﬁne a :=
b In)(cid:1). Then for any ∆1 ≥
0,

:= 2/(2b

≥

−

δ2
K
λ
b
(cid:107)
(cid:107) ≥
8 tr (cid:0)(K + λIn)−
1(K + δ2
δ2
b /λ,
∆2 ≥
(cid:104)
P
(1

∆1)(K + λI)

(cid:105)
(1 + ∆2)(K + λI)

˜K + λI

−
(cid:32)
exp

(cid:18)

1

a

−

(cid:22)

(cid:19)

(cid:22)
(cid:32)

+exp

m∆2
1
−
4n
λ (1 + 2
3 ∆1)

m(∆2 −

−
4n
λ (1 + 2

3 (∆2 −

δ2
λ )2
b
δ2
λ ))
b

≥
(cid:33)(cid:33)
.

The proof of Theorem 2 is in Appendix C. To provide
more intuition we present the following corollary:
Corollary 2.1. Assuming ∆1 ≤
(1
∆1)(K+λIn)
(cid:22)
−
(cid:16) a
8n/λ
if m
log
∆2
ρ
1
it follows that ˜K + λIn
probability at least 1

3/2, it follows that
˜K+λIn with probability at least 1
ρ
−
(cid:17)
(cid:2) δ2
(cid:3),
λ , 3
. Similarly, assuming ∆2 ∈
2
(1 + ∆2)(K + λIn) with
8n/λ
b /λ)2 log
δ2

(cid:22)
ρ if m

(cid:16) a
ρ

(∆2

≥

(cid:17)

.

b

−

≥

−

9Technically, m additional bits are needed to store a

vector of Rademacher random variables in

10This theorem extends directly to the quantization of
Rn×m with
2/m].

any kernel approximation feature matrix Z
i.i.d. columns and with entries in [

∈
2/m, (cid:112)

(cid:112)

1, 1

m.
}

{−

−

The above corollary suggests that using low precision
has negligible eﬀect on the number of features necessary
to attain a certain value of ∆1, and also has negligible
eﬀect for ∆2 as long as δ2
b /λ

∆2.

(cid:28)

Validation of Theory We now empirically validate
the following two predictions made by the above theory:
(1) Using low precision has no eﬀect on the asymptotic
behavior of ∆1 as the number of features m approaches
inﬁnity, while having a signiﬁcant eﬀect on ∆2 when
δ2
, ∆1 converges
b /λ is large. Speciﬁcally, as m
to 0 for any precision b, while ∆2 converges to a value
upper bounded by δ2
b /λ.11 (2) If δ2
∆2, using b-
bit precision will have negligible eﬀect on the number of
features required to attain this ∆2. Thus, the larger λ
is, the smaller the impact of using low precision should
be on ∆2.

→ ∞

b /λ

(cid:28)

To validate the ﬁrst prediction, in Figure 3 (left, middle)
we plot ∆1 and ∆2 as a function of the number of
features m, for FP-RFFs and LP-RFFs; we use the
same λ as in the Section 2 Census experiments. We
show that for large m, all methods approach ∆1 = 0;
4 the LP-RFFs converge
in contrast, for precisions b
to a ∆2 value much larger than 0, and slightly less than
δ2
b /λ (marked by dashed lines).

≤

To validate the second prediction, in Figure 3 (right)
we plot ∆2 vs. precision for various values of λ, using
m = 2000 features for all precisions; we do this on
a random subsample of 8000 Census training points.
We see that for large enough precision b, the ∆2 is
very similar to the value from using 32-bit precision.
Furthermore, the larger the value of λ, the smaller the
precision b can be without signiﬁcantly aﬀecting ∆2.

11By Lemma 3 in Appendix C, we know that E

K + D for a diagonal matrix D satisfying 0
where D is independent of m. As m
(K + λI)−1/2D(K + λI)−1/2
to

→ ∞
δ2
b /λ.

(cid:107) ≤

(cid:107)

(cid:104)

˜Z ˜Z T (cid:105)
=
δ2
b In,
D
(cid:22)
, ∆2 converges

(cid:22)

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Figure 4: Generalization performance of FP-Nystr¨om, FP-RFFs, circulant FP-RFFs, and LP-RFFs with respect
to memory (sum of components in Table 1) on TIMIT, YearPred and CovType. LP-RFFs attain the best
performance across a wide range of memory budgets. The same trend holds for Census in Appendix D.5.

Table 2: The compression ratios achieved by LP-RFFs
relative to the best performing full-precision baselines.

FP-RFFs Cir. FP-RFFs Nystr¨om

Census
YearPred
Covtype
TIMIT

2.9x
10.3x
4.7x
5.1x

15.6x
7.6x
3.9x
2.4x

63.2x
461.6x
237.2x
50.9x

4.3

Implementation Considerations

In this paper, we focus on training full-precision models
using mini-batch training over low-precision features.
Here we describe how this mixed-precision optimization
can be implemented in a memory-eﬃcient manner.

Naively, to multiply the low-precision features with the
full-precision model, one could ﬁrst cast the features to
full-precision, requiring signiﬁcant intermediate mem-
ory. We can avoid this by casting in the processor
registers. Speciﬁcally, to perform multiplication with
the full-precision model, the features can be streamed
to the processor registers in low precision, and then cast
to full precision in the registers. In this way, only the
features in the registers exist in full precision. A similar
technique can be applied to avoid intermediate memory
in the low-precision feature computation—after a full-
precision feature is computed in the registers, it can be
directly quantized in-place before it is written back to
main memory. We leave a more thorough investigation
of these systems issues for future work.

5 EXPERIMENTS

In this section, we empirically demonstrate the per-
formance of LP-RFFs under a memory budget, and
show that (∆1, ∆2) are predictive of generalization
performance. We show in Section 5.1 that LP-RFFs
can attain the same performance as FP-RFFs and
Nystr¨om, while using 3x-10x and 50x-460x less memory.
In Section 5.2, we show the strong alignment between
(∆1, ∆2) and generalization performance, once again
validating the importance of this measure.

5.1 Empirical Evaluation of LP-RFFs

1, 2, 4, 8, 16

To empirically demonstrate the generalization perfor-
mance of LP-RFFs, we compare their performance
to FP-RFFs, circulant FP-RFFs, and Nystr¨om fea-
tures for various memory budgets. We use the same
datasets and protocol as the large-scale Nystr¨om vs.
RFF comparisons in Section 2.2; the only signiﬁcant ad-
ditions here are that we also evaluate the performance
of circulant FP-RFFs, and LP-RFFs for precisions
b
. Across our experiments, we com-
}
pute the total memory utilization as the sum of all
the components in Table 1. We note that all our low-
precision experiments are done in simulation, which
means we store the quantized values as full-precision
ﬂoating-point numbers. We report average results from
three random seeds, with error bars showing standard
deviations. For more details about our experiments, see
Appendix D.5. We use the above protocol to validate
the following claims on the performance of LP-RFFs.12

∈ {

LP-RFFs can outperform full-precision fea-
tures under memory budgets.
In Figure 4, we
plot the generalization performance for these experi-
ments as a function of the total training memory for
TIMIT, YearPred, and CovType. We observe that LP-
RFFs attain better generalization performance than
the full-precision baselines under various memory bud-
gets. To see results for all precisions, as well as results
on additional benchmark datasets (Census, Adult, Cod-
RNA, CPU, Forest) from the UCI repository (Dheeru
and Karra Taniskidou, 2017), see Appendix D.5.

LP-RFFs can match the performance of full-
precision features with signiﬁcantly less mem-
ory.
In Table 2 we present the compression ratios we
achieve with LP-RFFs relative to the best performing
baseline methods. For each baseline (FP-RFFs, circu-
lant FP-RFFs, Nystr¨om), we ﬁnd the smallest LP-RFF
model, as well as the smallest baseline model, which
4 relative performance of the best-
attain within 10−
performing baseline model; we then compute the ratio

12 Our code: github.com/HazyResearch/lp_rffs.

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

∆1), ∆2)
Figure 5: Generalization perf. vs. ∆2 (left plots, shared legend), and vs. 1/(1
(right plots, shared legend). Left: heldout performance deteriorates as ∆2 gets larger due to lower precision.
Right: max (1/(1
∆1), ∆2) aligns well with performance across LP-RFF precisions (Spearman rank correlation
coeﬃcient ρ = 0.959), while 1/(1

∆1) aligns poorly (ρ = 0.403). See Appendix D.6 for CovType results.

∆1) and max (1/(1

−

−

−

−

of the memory used by these two models (baseline/LP-
RFF) for three random seeds, and report the aver-
age. We can see that LP-RFFs demonstrate signiﬁcant
memory saving over FP-RFFs, circulant FP-RFFs, and
Nystr¨om, attaining compression ratios of 2.9x-10.3x,
2.4x-15.6x, and 50.9x-461.6x, respectively.

5.2 Generalization Performance vs. (∆1, ∆2)

In this section we show that ∆1 and ∆2 are together
quite predictive of generalization performance across all
the kernel approximation methods we have discussed.
We ﬁrst show that performance deteriorates for larger
∆2 values as we vary the precision of the LP-RFFs,
when keeping the number of features constant (thereby
limiting the inﬂuence of ∆1 on performance). We then
combine this insight with our previous observation
(Section 3.2) that performance scales with
in the
full-precision setting by showing that across precisions
(cid:1). For
the performance aligns well with max (cid:0)
these experiments, we use the same protocol as for the
(∆1, ∆2) experiments in Section 3.2, but additionally
consider LP-RFFs for precisions b

, ∆2

1
∆1

1
∆1

−

−

1

1

.

1, 2, 4, 8, 16
}

∈ {

We show in Figure 5 (left plots) that for a ﬁxed number
of random Fourier features, performance deteriorates
as ∆2 grows. As we have shown in Figure 3 (left), ∆1
is primarily governed by the rank of the approxima-
tion matrix, and thus holding the number of features
constant serves as a proxy for holding ∆1 roughly con-
stant. This allows us to isolate the impact of ∆2 on
performance as we vary the precision.

1

1
∆1

, ∆2

To integrate the inﬂuence of ∆1 and ∆2 on general-
ization performance into a single scalar, we consider
max (cid:0)
(cid:1). In Figure 5 (right plots) we show that
when considering both low-precision and full-precision
features, max (cid:0)
(cid:1) aligns well with performance
(ρ = 0.959, incorporating all precisions), while
aligns poorly (ρ = 0.403).

, ∆2

1
∆1

1
∆1

−

−

−

1

1

In Appendix B we argue that performance scales
roughly as ∆2 instead of as ∆2/(1 + ∆2) (as suggested
by Prop. 1) due to looseness in the Prop. 1 bound.

6 RELATED WORK

Low-Memory Kernel Approximation For RFFs,
there has been work on using structured random pro-
jections (Le et al., 2013; Yu et al., 2015, 2016), and
feature selection (Yen et al., 2014; May et al., 2016) to
reduce memory utilization. Our work is orthogonal, as
LP-RFFs can be used with both. For Nystr¨om, there
has been extensive work on improving the choice of
landmark points, and reducing the memory footprint
in other ways (Kumar et al., 2009; Hsieh et al., 2014;
Si et al., 2014; Musco and Musco, 2017). In our work,
we focus on the eﬀect of quantization on generalization
performance per bit, and note that RFFs are much
more amenable to quantization. For our initial experi-
ments quantizing Nystr¨om features, see Appendix D.7.

Low Precision for Machine Learning There has
been much recent interest in using low precision for
accelerating training and inference of machine learning
models, as well as for model compression (Gupta et al.,
2015; De Sa et al., 2015; Hubara et al., 2016; De Sa
et al., 2018, 2017; Han et al., 2016). There have been
many advances in hardware support for low precision
as well (Jouppi et al., 2017; Caulﬁeld et al., 2017).

This work is inspired by the Nystr¨om vs. RFF exper-
iments in the PhD dissertation of May (2018), and
provides a principled understanding of the prior results.
For more related work discussion, see Appendix E.

7 CONCLUSION

We deﬁned a new measure of kernel approximation error
and demonstrated its close connection to the empirical
and theoretical generalization performance of kernel
approximation methods.
Inspired by this measure,
we proposed LP-RFFs and showed they can attain
improved generalization performance under a memory
budget in theory and in experiments. We believe these
contributions provide fundamental insights into the
generalization performance of kernel approximation
methods, and hope to use these insights to scale kernel
methods to larger and more challenging tasks.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Acknowledgements

We thank Michael Collins for his helpful guidance on
the Nystr¨om vs. RFF experiments in Avner May’s PhD
dissertation (May, 2018), which inspired this work. We
also thank Jared Dunnmon, Albert Gu, Beliz Gunel,
Charles Kuang, Megan Leszczynski, Alex Ratner, Nimit
Sohoni, Paroma Varma, and Sen Wu for their helpful
discussions and feedback on this project.

We gratefully acknowledge the support of DARPA un-
der Nos. FA87501720095 (D3M) and FA86501827865
(SDH), NIH under No. N000141712266 (Mobilize),
NSF under Nos. CCF1763315 (Beyond Sparsity) and
CCF1563078 (Volume to Velocity), ONR under No.
N000141712266 (Unifying Weak Supervision), the
Moore Foundation, NXP, Xilinx, LETI-CEA, Intel,
Google, NEC, Toshiba, TSMC, ARM, Hitachi, BASF,
Accenture, Ericsson, Qualcomm, Analog Devices, the
Okawa Foundation, and American Family Insurance,
and members of the Stanford DAWN project: Intel,
Microsoft, Teradata, Facebook, Google, Ant Finan-
cial, NEC, SAP, and VMWare. The U.S. Government
is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright
notation thereon. Any opinions, ﬁndings, and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reﬂect
the views, policies, or endorsements, either expressed
or implied, of DARPA, NIH, ONR, or the U.S. Gov-
ernment.

References

Ahmed El Alaoui and Michael W. Mahoney. Fast
randomized kernel ridge regression with statistical
guarantees. In NIPS, pages 775–783, 2015.

Haim Avron, Michael Kapralov, Cameron Musco,
Christopher Musco, Ameya Velingker, and Amir
Zandieh. Random Fourier features for kernel ridge
regression: Approximation bounds and statistical
guarantees. In ICML, volume 70 of Proceedings of
Machine Learning Research, pages 253–262. PMLR,
2017.

Adrian M. Caulﬁeld, Eric S. Chung, Andrew Put-
nam, Hari Angepat, Daniel Firestone, Jeremy Fowers,
Michael Haselman, Stephen Heil, Matt Humphrey,
Puneet Kaur, Joo-Young Kim, Daniel Lo, Todd Mas-
sengill, Kalin Ovtcharov, Michael Papamichael, Lisa
Woods, Sitaram Lanka, Derek Chiou, and Doug
Burger. Conﬁgurable clouds. IEEE Micro, 37(3):
52–61, 2017.

Corinna Cortes, Mehryar Mohri, and Ameet Talwalkar.
On the impact of kernel approximation on learning
accuracy. In AISTATS, volume 9 of JMLR Proceed-
ings, pages 113–120. JMLR.org, 2010.

Tri Dao, Christopher De Sa, and Christopher R´e. Gaus-
sian quadrature for kernel features. In NIPS, pages
6109–6119, 2017.

Christopher De Sa, Ce Zhang, Kunle Olukotun, and
Christopher R´e. Taming the wild: A uniﬁed analysis
of Hogwild-style algorithms. In NIPS, pages 2674–
2682, 2015.

Christopher De Sa, Matthew Feldman, Christopher R´e,
and Kunle Olukotun. Understanding and optimiz-
ing asynchronous low-precision stochastic gradient
descent. In ISCA, pages 561–574. ACM, 2017.

Christopher De Sa, Megan Leszczynski, Jian Zhang,
Alana Marzoev, Christopher R. Aberger, Kunle
Olukotun, and Christopher R´e. High-accuracy low-
precision training. arXiv preprint arXiv:1803.03383,
2018.

Dua Dheeru and Eﬁ Karra Taniskidou. UCI machine

learning repository, 2017.

M. J. F. Gales. Maximum likelihood linear transforma-
tions for HMM-based speech recognition. Computer
Speech & Language, 12(2):75–98, 1998.

J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus,
D. S. Pallett, and N. L. Dahlgren. DARPA TIMIT
acoustic phonetic continuous speech corpus CDROM,
1993. URL http://www.ldc.upenn.edu/Catalog/
LDC93S1.html.

Alex Gittens and Michael W. Mahoney. Revisiting the
Nystr¨om method for improved large-scale machine
learning. Journal of Machine Learning Research, 17:
117:1–117:65, 2016.

Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan,
and Pritish Narayanan. Deep learning with limited
numerical precision. In ICML, volume 37 of JMLR
Workshop and Conference Proceedings, pages 1737–
1746, 2015.

Song Han, Huizi Mao, and William J. Dally. Deep
compression: Compressing deep neural network with
pruning, trained quantization and Huﬀman coding.
In Proceedings of the International Conference on
Learning Representations (ICLR), 2016.

Cho-Jui Hsieh, Si Si, and Inderjit S. Dhillon. Fast
prediction for large-scale kernel machines. In NIPS,
pages 3689–3697, 2014.

Jie Chen, Lingfei Wu, Kartik Audhkhasi, Brian Kings-
bury, and Bhuvana Ramabhadrari. Eﬃcient one-vs-
one kernel ridge regression for speech recognition. In
ICASSP, pages 2454–2458. IEEE, 2016.

Po-Sen Huang, Haim Avron, Tara N. Sainath, Vikas
Sindhwani, and Bhuvana Ramabhadran. Kernel
methods match deep neural networks on TIMIT.
In ICASSP, pages 205–209. IEEE, 2014.

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Itay Hubara, Matthieu Courbariaux, Daniel Soudry,
Ran El-Yaniv, and Yoshua Bengio. Binarized neural
networks. In NIPS, pages 4107–4115, 2016.

Alessandro Rudi and Lorenzo Rosasco. Generalization
properties of learning with random features. In NIPS,
pages 3218–3228, 2017.

Norman P. Jouppi, Cliﬀ Young, Nishant Patil, David A.
Patterson, et al. In-datacenter performance analysis
of a tensor processing unit. In ISCA, pages 1–12.
ACM, 2017.

Sanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar.
Ensemble Nystr¨om method. In NIPS, pages 1060–
1068, 2009.

Sanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar.
Sampling methods for the Nystr¨om method. Journal
of Machine Learning Research, 13:981–1006, 2012.

Quoc V. Le, Tam´as Sarl´os, and Alexander J. Smola.
Fastfood - computing Hilbert space expansions in log-
linear time. In ICML, volume 28 of JMLR Workshop
and Conference Proceedings, pages 244–252, 2013.

Zhu Li, Jean-Francois Ton, Dino Oglic, and Dino Sejdi-
novic. A uniﬁed analysis of random Fourier features.
arXiv preprint arXiv:1806.09178, 2018.

Avner May. Kernel Approximation Methods for Speech
Recognition. PhD thesis, Columbia University, 2018.

Avner May, Michael Collins, Daniel J. Hsu, and Brian
Kingsbury. Compact kernel models for acoustic mod-
eling via random feature selection. In ICASSP, pages
2424–2428. IEEE, 2016.

Avner May, Alireza Bagheri Garakani, Zhiyun Lu,
Dong Guo, Kuan Liu, Aur´elien Bellet, Linxi Fan,
Michael Collins, Daniel J. Hsu, Brian Kingsbury,
Michael Picheny, and Fei Sha. Kernel approxima-
tion methods for speech recognition. arXiv preprint
arXiv:1701.03577, 2017.

N. Morgan and H. Bourlard. Generalization and pa-
rameter estimation in feedforward nets: Some exper-
iments. In NIPS, 1990.

Tara N. Sainath, Brian Kingsbury, Vikas Sindhwani,
Ebru Arisoy, and Bhuvana Ramabhadran. Low-rank
matrix factorization for deep neural network training
with high-dimensional output targets. In ICASSP,
pages 6655–6659. IEEE, 2013a.

Tara N. Sainath, Brian Kingsbury, Hagen Soltau, and
Bhuvana Ramabhadran. Optimization techniques to
improve training speed of deep neural networks for
large speech tasks. IEEE Trans. Audio, Speech &
Language Processing, 21(11):2267–2276, 2013b.

Si Si, Cho-Jui Hsieh, and Inderjit S. Dhillon. Mem-
ory eﬃcient kernel approximation. In ICML, vol-
ume 32 of JMLR Workshop and Conference Proceed-
ings, pages 701–709, 2014.

Vikas Sindhwani, Tara N. Sainath, and Sanjiv Kumar.
Structured transforms for small-footprint deep learn-
ing. In NIPS, pages 3088–3096, 2015.

Dougal J. Sutherland and Jeﬀ G. Schneider. On the
In UAI, pages

error of random Fourier features.
862–871. AUAI Press, 2015.

Joel A. Tropp. An introduction to matrix concentration
inequalities. Foundations and Trends in Machine
Learning, 8(1-2):1–230, 2015.

Stephen Tu, Rebecca Roelofs, Shivaram Venkatara-
man, and Benjamin Recht. Large scale kernel learn-
ing using block coordinate descent. arXiv preprint
arXiv:1602.05310, 2016.

Yuting Wei, Fanny Yang, and Martin J. Wainwright.
Early stopping for kernel boosting algorithms: A
general analysis with localized complexities. In NIPS,
pages 6067–6077, 2017.

Cameron Musco and Christopher Musco. Recursive
sampling for the Nystr¨om method. In NIPS, pages
3836–3848, 2017.

Christopher K. I. Williams and Matthias W. Seeger.
Using the Nystr¨om method to speed up kernel ma-
chines. In NIPS, pages 682–688. MIT Press, 2000.

Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan,
and Alexander Shapiro. Robust stochastic approxi-
mation approach to stochastic programming. SIAM
Journal on Optimization, 19(4):1574–1609, 2009.

Tiberiu Popoviciu. Sur les ´equations alg´ebriques ayant
toutes leurs racines r´eelles. Mathematica, 9:129–145,
1935.

Ali Rahimi and Benjamin Recht. Random features for
large-scale kernel machines. In NIPS, pages 1177–
1184, 2007.

Jiyan Yang, Vikas Sindhwani, Haim Avron, and
Michael W. Mahoney. Quasi-Monte Carlo feature
maps for shift-invariant kernels. In Proceedings of the
31th International Conference on Machine Learning,
ICML 2014, Beijing, China, 21-26 June 2014, pages
485–493, 2014.

Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong
Jin, and Zhi-Hua Zhou. Nystr¨om method vs ran-
dom Fourier features: A theoretical and empirical
comparison. In NIPS, pages 485–493, 2012.

Ali Rahimi and Benjamin Recht. Weighted sums of
random kitchen sinks: Replacing minimization with
randomization in learning. In NIPS, pages 1313–1320,
2008.

Zichao Yang, Marcin Moczulski, Misha Denil, Nando
de Freitas, Alexander J. Smola, Le Song, and Ziyu
Wang. Deep fried convnets. In ICCV, pages 1476–
1483. IEEE Computer Society, 2015.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Ian En-Hsu Yen, Ting-Wei Lin, Shou-De Lin, Pradeep
Ravikumar, and Inderjit S. Dhillon. Sparse random
feature algorithm as coordinate descent in Hilbert
space. In NIPS, pages 2456–2464, 2014.

Felix X. Yu, Sanjiv Kumar, Henry A. Rowley, and Shih-
Fu Chang. Compact nonlinear maps and circulant
extensions. arXiv preprint arXiv:1503.03893, 2015.

Felix

X.

Yu,

Ananda

Theertha

Suresh,
Krzysztof Marcin Choromanski, Daniel N.
Holtmann-Rice, and Sanjiv Kumar. Orthogo-
In NIPS, pages 1975–1983,
nal random features.
2016.

Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh,
Ji Liu, and Ce Zhang. Zipml: Training linear models
with end-to-end low precision, and a little bit of deep
learning. In ICML, volume 70 of Proceedings of Ma-
chine Learning Research, pages 4035–4043. PMLR,
2017.

Tong Zhang, Bin Yu, et al. Boosting with early stop-
ping: Convergence and consistency. The Annals of
Statistics, 33(4):1538–1579, 2005.

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

A NOTATION AND BACKGROUND

In this appendix, we ﬁrst discuss the notation we use throughout the paper, and then provide an overview of
random Fourier features (RFFs) (Rahimi and Recht, 2007) and the Nystr¨om method (Williams and Seeger, 2000).
After this, we brieﬂy extend our discussion in Section 3 on ﬁxed design kernel ridge regression.

A.1 Notation

×

∈

×

Rn

}
→

Rd, and yi

n
i=1 to denote a training set, for xi
}

= R for regression, and
We use
(xi, yi)
∈
{
n denote the kernel matrix corresponding to a kernel function
for classiﬁcation. We let K
1, . . . , c
=
Y
{
R, where Kij = k(xi, xj), and let ˜K denote an approximation to K. We let z : Rd
Rm denote
Rd
k : Rd
a feature map for approximating a kernel function, such that ˜Kij = z(xi)T z(xj). We use s to denote the size
of the mini-batches during training, and b to denote the precision used for the random features. We let
(cid:107)2
F denote the spectral and Frobenius norms of a matrix K, respectively; if the subscript is not speciﬁed,
and
(cid:107)
will denote the (cid:96)2 norm of x, unless speciﬁed otherwise. In
K
(cid:107)
A is positive
will denote the n
semideﬁnite. We will use λi(A) to denote the ith largest eigenvalue of A, and λmax(A), λmin(A) to denote the
largest and smallest eigenvalues of A, respectively.

K
(cid:107)
denotes the spectral norm. For vectors x,

n identity matrix. For symmetric matrices A and B, we will say A

, where

B if B

K
(cid:107)

∈ Y

→

(cid:22)

−

×

Y

x

(cid:107)

(cid:107)

(cid:107)

A.2 Kernel Approximation Background

k(x, y).
The core idea behind kernel approximation is to construct a feature map z :
X →
n
Given such a map, one can then learn a linear model on top of
i=1, and this model will approximate
(z(xi), yi)
}
{
the model trained using the exact kernel function. We now review RFFs and the Nystr¨om method, two of the
most widely used and studied methods for kernel approximation.

≈

R such that z(x)T z(y)

Random Fourier features (RFFs) For shift-invariant kernels (k(x, x(cid:48)) = ˆk(x
x(cid:48))), the random Fourier
Rm such that
feature method (Rahimi and Recht, 2007) constructs a random feature representation z(x)
E (cid:2)z(x)T z(x(cid:48))(cid:3) = k(x, x(cid:48)). This construction is based on Bochner’s Theorem, which states that any positive
deﬁnite kernel is equal to the Fourier transform of a nonnegative measure. This allows for performing Monte
Carlo approximations of this Fourier transform in order to approximate the function. The resulting features
have the following functional form: zi(x) =
i x + ai), where wi is drawn from the inverse Fourier
transform of the kernel function ˆk, and ai is drawn uniformly from [0, 2π] (see Appendix A in May et al. (2017)
for a derivation).

2/m cos(wT

(cid:112)

−

∈

One way of reducing the memory required for storing W = [w1, . . . , wm], is to replace W by a structured matrix;
in this work, we let W be a concatenation of many square circulant random matrices (Yu et al., 2015).

(cid:104)

(cid:105) ≈

k(x, x(cid:48)).

z(x), z(x(cid:48))

It does this by picking a set of landmark points

Rm
Nystr¨om method The Nystr¨om method constructs a ﬁnite-dimensional feature representation z(x)
∈
ˆx1, . . . , ˆxm
such that
, and
{
taking the SVD ˆK = U ΛU T of the m by m kernel matrix ˆK corresponding to these landmark points ( ˆKi,j =
1/2U T kx, where kx =
k(ˆxi, ˆxj)). The Nystr¨om representation for a point x
Rm
n, the Nystr¨om method can be thought of
[k(x, ˆx1), . . . , k(x, ˆxm)]T . Letting Km,n = [kx1 , . . . , kxn ]
1/2U T Km,n of the full n by n kernel matrix K
as an eﬃcient low-rank approximation K
m,nU Λ−
n
corresponding to the full dataset
i=1. One can also consider the lower-dimensional Nystr¨om representation
}
Rr, where only the top r eigenvalues and eigenvectors of ˆK are used, instead of all m. In
U T
zr(x) = Λ−
r kx
r
this paper, we will always use m = r, and thus will not specify the subscript r.

is deﬁned as z(x) = Λ−

∈ X
∈
1/2Λ−

} ∈ X

K T

1/2

xi

≈

∈

{

×

A.3 Fixed Design Kernel Ridge Regression

We consider the problem of ﬁxed design kernel ridge regression, which has a closed-form equation for the
generalization error, making it a particularly tractable problem to analyze. In ﬁxed design regression, one is
R, and the (cid:15)i are zero-mean uncorrelated
given a set of labeled points
random variables with shared variance σ2 > 0; here, the ¯yi represent the “true labels.” Given such a sample,
¯yi)2(cid:3) is small. Note that for a ﬁxed
the goal is to learn a regressor f (x) such that
learning method, the learned regressor f can be seen as a random function based on the random label noise (cid:15)i.

Rd, yi = ¯yi + (cid:15)i

n
i=1, where xi

i=1(f (xi)

(f ) = E(cid:15)

(xi, yi)

(cid:2) 1
n

(cid:80)n

R

−

∈

∈

{

}

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

One approach to solving this problem is kernel ridge regression. In kernel ridge regression, one chooses a kernel
Rd
function k : Rd
i αik(x, xi).
n denote the kernel matrix such that Kij = k(xi, xj), and y = (y1, . . . , yn), the closed-form
Letting K
1y. It is then easy
solution for this problem (the one minimizing the regularized empirical loss), is α = (K + λI)−
to show (Alaoui and Mahoney, 2015) that the expected error of this regressor fK under the ﬁxed design setting is

R, and a regularizing constant λ, and learns a function of the form f (x) = (cid:80)

×
Rn
×

→

∈

(fK) =

R

1
n

λ2 ¯yT (K + λI)−

2 ¯y +

σ2T r

K 2(K + λI)−

2(cid:17)

,

(cid:16)

1
n

where ¯y = (¯y1, . . . , ¯yn) is the vector of “true labels.”

B GENERALIZATION BOUNDS FOR FIXED DESIGN REGRESSION

B.1 Generalization Bound in Terms of (∆1, ∆2)

Proposition 1. (Extended from (Avron et al., 2017)) Suppose ˜K + λI is (∆1, ∆2)-spectral approximation of
0. Let m denote the rank of ˜K, and let fK and f ˜K be the kernel ridge regression
K + λI, for ∆1 ∈
. Then
estimators learned using these matrices, with regularizing constant λ

0 and label noise variance σ2 <

[0, 1) and ∆2 ≥

(f ˜K)

R

≤

1

1
∆1

(cid:98)
R

(fK) +

∆2
1 + ∆2

m
n

σ2,

≥

∞

(2)

where

(expected risk) and (cid:98)
R

R

(upper bound on

) are deﬁned in Section 3.1.

−

R

∆) and (1 + ∆) with (1

Proof. This proof closely follows the proof of Lemma 2 in Avron et al. (2017), with the primary diﬀerence being
that we replace (1
We begin by replacing K with ˜K in the deﬁnition for (cid:98)
R
1
n

∆1) and (1 + ∆2), respectively.

λ¯yT ( ˜K + λI)−

˜K( ˜K + λI)−

(f ˜K).

(f ˜K)

(fK):

σ2 tr

1 ¯y +

= (cid:98)
R

1
n

1(cid:17)

R

−

−

≤

(cid:16)

( ˜K + λI)−

We now continue this chain of inequalities, using the fact that A
˜K + λI
bounds the ﬁrst term in the above sum.
We now consider the second term. Let m = rank( ˜K), and let sλ( ˜K) = tr

¯yT ( ˜K + λI)−

1(K + λI)−

∆1)−

1 ¯y

⇒

⇒

(1

(cid:22)

≤

(cid:22)

−

1

1

(1

(cid:16)

−

1
B implies B−

˜K( ˜K + λI)−

1(cid:17)

.

1. Thus, (1
∆1)(K +λI)
A−
(cid:22)
−
1 ¯yT (K + λI)−
1 ¯y. This upper

(cid:22)
∆1)−

(cid:16)

˜K( ˜K + λI)−

1(cid:17)

sλ( ˜K) = tr
m
(cid:88)

=

λi( ˜K)
λi( ˜K) + λ

i=1

= m

−

m

−

≤

m
(cid:88)

i=1
m
(cid:88)

i=1

λ
λi( ˜K) + λ

λ
(1 + ∆2)(λi(K) + λ)
m
(cid:88)

= m

1
(1 + (1 + ∆2)−

1)

−

i=1

λ
λi(K) + λ

= m

λ
λi(K) + λ

+

∆2
1 + ∆2

m
(cid:88)

i=1

λ
λi(K) + λ

n

−

≤

λ
λi(K) + λ

+

∆2
1 + ∆2

m

= sλ(K) +

∆2
1 + ∆2

m

−

−

m
(cid:88)

i=1
n
(cid:88)

i=1

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

≤

1

1
∆1

−

sλ(K) +

∆2
1 + ∆2

m

Combining the above results, we get that:

(f ˜K)

R

1 ¯y +

1
n

σ2sλ( ˜K)
(cid:19)

≤

≤

=

=

1
n
1
n

1

1

λ¯yT ( ˜K + λI)−
(cid:18) 1
λ
1

∆1
−
(cid:18) 1
n

1
∆1
−
1
∆1

−

λ¯yT (K + λI)−

1 ¯y +

(fK) +

(cid:98)
R

∆2
1 + ∆2

m
n

σ2

¯yT (K + λI)−

1 ¯y

+

σ2

sλ(K) +

1
n

(cid:18) 1
1

∆1

−
(cid:19)
σ2sλ(K)

+

1
n

∆2
1 + ∆2

m
n

σ2

(cid:19)

∆2
1 + ∆2

m

Remark Above, ∆1 ∈
0. Note that as ∆1 approaches 1, the above upper bound diverges to
n σ2. This suggests
inﬁnity. Whereas as ∆2 approaches
that choosing ˜K and λ such that ∆1 does not get too close to 1 is very important. A necessary condition for
(1

˜K + λI is for ˜K to be high rank, as discussed in Section 3.

, the second term in the upper bound approaches m

[0, 1] and ∆2 ≥
∞

∆1)(K + λI)

−

(cid:22)

B.2 Heuristic Eﬀort to Better Understand Inﬂuence of (∆1, ∆2) on Generalization Error

Here we present a heuristic argument to explain how ∆1 and ∆2 in the spectral approximation aﬀects the
generalization error. We are particularly interested in demonstrating that ∆2 can have an important inﬂuence on
the bias squared term ( λ2
n ¯yT ( ˜K +
1
λI)−
∆1 (cid:98)
R

2 ¯y) of the generalization error, even though the upper bound λ2

(fK) on the bias squared term (from Proposition 1) is only in terms of ∆1.

n ¯yT ( ˜K + λI)−

2 ¯y

≤

1

−

Suppose that the approximate kernel matrix ˜K is a (∆1, ∆2)-spectral approximation of the true kernel matrix K,
that is:

We focus on the bias squared term of ˜K ( λ2
Following Theorem 15 of Musco and Musco (2017), we ﬁrst bound the bias (not squared):

(1

∆1)(K + λIn)

−

˜K + λIn
n ¯yT ( ˜K + λIn)−

(cid:22)

(1 + ∆2)(K + λIn).

(cid:22)
2 ¯y), and compare it to the bias squared term of K.

( ˜K + λIn)−

1 ¯y

(cid:107)

1 ¯y
(cid:107)
1 ¯y
(cid:107)
1 ¯y
(cid:107)
1 ¯y
(cid:107)
1 ¯y
(cid:107)

(cid:107) ≤ (cid:107)
=

=

≤ (cid:107)
=

(K + λIn)−

(K + λIn)−
(cid:107)
(K + λIn)−
(cid:107)
(K + λIn)−

(K + λIn)−
(cid:107)
( ˜K + λIn)−
(cid:107)

1(K

−

˜K

K

−

(cid:22)

∆2(K + λIn)

+

+

+

+
(cid:16)

(( ˜K + λIn)−
1
(cid:107)
( ˜K + λIn)−
(cid:107)
( ˜K + λIn)−
(cid:107)
( ˜K + λIn)−
(cid:107)
1 +

( ˜K + λIn)−
(cid:107)
˜K)
(cid:107)
∆2

. As ˜K + λIn

( ˜K + λIn)

(cid:22)

(cid:22)

1

∆1

−

(K + λIn)−

−

1)¯y
(cid:107)
( ˜K + λIn))(K + λIn)−

1 ¯y
(cid:107)

1((K + λIn)
1(K
1(K

−

−
˜K)(K + λIn)−
˜K)

1 ¯y
(cid:107)
1 ¯y
(K + λIn)−
(cid:107)

−
1(K

(cid:107)(cid:107)
˜K)
(cid:107)

(cid:17)

.

−

(3)

1 + ∆2
∆1
1

−

(cid:22)

( ˜K + λIn).

Now it reduces to bounding

(1 + ∆2)(K + λIn), we have

Similarly, since K + λIn

1
∆1

( ˜K + λIn),

(cid:22)

1

−

Hence

K

˜K

−

(cid:22)

1

∆1

( ˜K + λIn)

∆1

−

1 + ∆2
∆1
1

−

(cid:22)

( ˜K + λIn).

1 + ∆2
∆1
1

−

−

( ˜K + λIn)

K

(cid:22)

−

˜K

(cid:22)

1 + ∆2
∆1
1

−

( ˜K + λIn).

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

˜K)2 from the bound on
˜K. However, under a restricted setting where K and ˜K have the same eigenvectors, we can square the

t2 is not operator monotone, it is not easy to obtain a bound on (K

−

Because t
K
corresponding eigenvalues to obtain

(cid:55)→

−

Thus

Thus

And hence

( ˜K + λIn)−
(cid:107)

1(K

−

˜K)

(cid:107) ≤

1+∆2
∆1
1

−

. Plugging this into the bound (Eq. 3) yields

˜K)2

(K

−

(cid:22)

(1 + ∆2)2
(1

∆1)2 ( ˜K + λIn)2.

−

( ˜K + λIn)−

1(K

˜K)2( ˜K + λIn)−

1

−

(1 + ∆2)2
∆1)2 .
(1

−

(cid:22)

( ˜K + λIn)−
(cid:107)

1 ¯y

(cid:107) ≤

(cid:18)

1 +

(cid:19)

1 + ∆2
∆1
1

−

(K + λIn)−
(cid:107)

1 ¯y
.
(cid:107)

λ2
n

¯yT ( ˜K + λIn)−

2 ¯y =

( ˜K + λIn)−

λ2
n (cid:107)
(cid:18)

1 +

(cid:18)

1 +

1 ¯y
2
(cid:107)
(cid:19)2 λ2
n (cid:107)
(cid:19)2 λ2
n

1 + ∆2
∆1
1

−
1 + ∆2
∆1
1

−

≤

=

(K + λIn)−

2

1 ¯y
(cid:107)

¯yT (K + λIn)−

2 ¯y.

In other words, in this restricted setting the bias squared of ˜K is at most a factor (1 + 1+∆2
)2 larger than the bias
∆1
squared of K. Though this heuristic analysis only holds when K and ˜K have the same eigenvectors, it reveals
the dependency of the generalization performance on ∆1 and ∆2; in particular, it reveals that ∆2 could have an
important inﬂuence on the bias squared term of the generalization error.

−

1

B.3 The Empirical Inﬂuence of ∆2 on the Bias Squared Term

We now empirically validate that ∆2 can have a large impact on the bias squared term, as suggested by the
theoretical discussion in the previous section. The inﬂuence of ∆2 on the bias squared term helps explain our
empirical observations on the inﬂuence of ∆2 on generalization performance from Section 5.2. Though the
generalization bound in Proposition 1 suggests that performance should scale roughly linearly in ∆2/(1 + ∆2),
we empirically found that generalization performance does not asymptote as ∆2 grows (as ∆2/(1 + ∆2) would
suggest it would). In this section, we empirically validate our hypothesis that this is due to looseness in the
generalization bound. Speciﬁcally, the expected mean squared error for ﬁxed design kernel ridge regression is

(f ˜K) =

R

λ2
n

¯yT ( ˜K + λI)−

2 ¯y +

tr

˜K 2( ˜K + λI)−

2(cid:17)

,

(cid:16)

σ2
n

where ˜K is an approximate kernel matrix. We show in experiments that the bias squared term ( λ2
can be strongly inﬂuenced by ∆2, even though the upper bound on it ( λ2
Proposition 1 is only in terms of ∆1.

n ¯yT ( ˜K + λI)−

2 ¯y

≤

n ¯yT ( ˜K + λI)−
1
∆1 (cid:98)
R

2 ¯y)
(fK)) in

1

−

In our experiments, we compute the value of the bias squared term and ∆2 on the Census dataset. To gain
statistically meaningful insights, we collect and average the value of λ2
2 ¯y and ∆2 using 3 independent
runs with diﬀerent random seeds. In Figure 6, we plot the value of λ2
2 ¯y as a function of ∆2 for
3 diﬀerent numbers of features; by controlling the number of features, we can demonstrate the inﬂuence of ∆2
while ∆1 is held roughly ﬁxed. In each curve in Figure 6, the data points are collected from FP-RFFs, circulant
FP-RFFs, as well as LP-RFFs using
bit precision. We can see that for each number of features, the
value of λ2
2 ¯y grows with ∆2. These results demonstrate that the upper bound on the bias term in
Proposition 1 is quite loose, and is not capturing the inﬂuence of ∆2 properly.

n ¯yT ( ˜K + λI)−

n ¯yT ( ˜K + λI)−

n ¯yT ( ˜K + λI)−

1, 2, 4, 8

{

}

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 6: In the ﬁxed design setting, the bias squared term of the generalization performance grows with ∆2.

C THEORETICAL GUARANTEES FOR LP-RFFS

In this section, we ﬁrst lower bound the probability that using LP-RFFs results in a kernel approximation matrix
that is a (∆1, ∆2)-spectral approximation of the exact kernel matrix (Section C.1). We then present bounds on
the Frobenius kernel approximation error for LP-RFFs (Section C.2).

C.1

(∆1, ∆2)-Spectral Approximation Bounds for LP-RFFs

Rn

×

∈

n denote the kernel matrix, and Z

As usual, let K
is the number of data points and m is the number of features. We can write Z = 1
√m
(cid:112)
the (scaled) columns of Z. Each entry of Z has the form
where d is the dimension of the original dataset. Then E[zizT

m be the random Fourier feature matrix, where n
(cid:3) where zi are
Rm,
Rd and a

2/m cos(wT x + a) for some w, x
i ] = K, so E[ZZ T ] = K.

(cid:2)z1, . . . , zm

∈

∈

∈

×

Rn

≥

1. Then the quantized feature matrix is Z + C for some random C

Now suppose we quantize Z to b bits using the quantization method described in Section 4.1, for some ﬁxed
m whose entries are independent
b
(cid:3) where
conditioned on Z (but not identically distributed) with E[C
ci are the (scaled) columns of C. Moreover, the ci are independent conditioned on Z. Deﬁning δ2
1)2 , the
entries Cij have variance E[C 2
δ2
b /m by Proposition 7 in Appendix C.2. In terms of the vectors ci, we
can also see that E[c2

Rn
×
Z] = 0. We can write C = 1
√m

Zij]
ij |
b , where ci,j denotes the jth element of ci.
δ2
≤

(cid:2)c1, . . . , cm
b :=

i,j |

Zij]

(2b

≤

∈

−

2

|

We ﬁrst analyze the expectation of (Z + C)(Z + C)T (over both the randomness of Z and of C).
Lemma 3. E[(Z +C)(Z +C)T ] = K +D, where D := E[c1cT
D does not depend on the number of random features m.

1 ] = sbIn is a multiple of the identity, for 0

sb

≤

≤

δ2
b .

Proof.

E[(Z + C)(Z + C)T ] = E

(zi + ci)(zi + ci)T

= E[(z1 + c1)(z1 + c1)T ]

(cid:20) 1
m

m
(cid:88)

i=1

(cid:21)

Since Ec1[c1 |

z1] = 0, it follows that

E[(z1 + c1)(z1 + c1)T ] = Ez1

(cid:104)

Ec1[(z1 + c1)(z1 + c1)T
(cid:104)

Ec1[c1cT

1 |

(cid:105)

z1]
(cid:105)

|
z1]

1 ] + Ez1

= Ez1[z1zT
= K + E[c1cT
1 ]

It is clear that D := E[c1cT
variable. It is also easy to see that the jth entry on the diagonal of D is equal to E[c2
argue that each element z1,j = √2 cos(wT

1 ] is a diagonal matrix, because each element of c1 is a zero-mean independent random
δ2
b . Lastly, we
1 xj + a1) has the same distribution, because it is distributed the same

1,j |

z1,j]

≤

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

as √2 cos(a1) for a1 uniform in [0, 2π]. Thus, E[c2
completes the proof.

1,j |

z1,j] is independent of j. Letting sb := E[c2

1,1 |

z1,1]

δ2
b

≤

1 ], we can use matrix concentration to show that the quantized kernel matrix ˜K = (Z +C)(Z +C)T
With D := E[c1cT
is close to its expectation K + D. We ﬁrst strengthen the matrix Bernstein inequality with intrinsic dimension
(Theorem 7.7.1 in Tropp (2015)) by removing the requirement on the deviation.13
Theorem 4 (Matrix Bernstein: Hermitian Case with Intrinsic Dimension). Consider a ﬁnite sequence
random Hermitian matrices of the same size, and assume that

Xk
{

of

}

Introduce the random matrix

E[Xk] = 0 and λmax(Xk)

L for each index k.

≤

(cid:88)

Y =

Xk.

k

Let V be a semideﬁnite upper bound for the matrix-valued variance VAR [Y ]:

Deﬁne the intrinsic dimension bound and variance bound

VAR [Y ] = E[Y 2] =

V

(cid:23)

(cid:88)
k

E[X 2
k ].

Then, for t

0,

≥

intdim(V ) =

and

v =

V
(cid:107)

.
(cid:107)

tr(V )
V
(cid:107)

(cid:107)

P (λmax(Y )

t)

≥

≤

4 intdim(V )

exp

·

(cid:18)

t2/2
v + Lt/3

−

(cid:19)

.

(4)

√v + L/3 is exactly Theorem 7.7.1 in Tropp (2015). We just need to show that the bound

2(v + Lt/3). Indeed, t2

2Lt/3

2v has roots

−

−

≤

Proof. The case of t
is vacuous when 0

≥
t < √v + L/3.

Suppose that 0
(cid:113) L2
L
3 ±

≤

9 + 2v. The condition t2

≤

≤
t < √v + L/3. We show that then t2

2(v + Lt/3) is then equivalent to

L
3 −

(cid:114)

L2
9

+ 2v

t

≤

≤

L
3

+

(cid:114)

L2
9

+ 2v.

The lower bound is negative since v
bound. Thus 0

t < √v + L/3 implies that t2

≥

0, and t < √v + L/3 implies t < L/3 +

L2/9 + 2v, satisfying the upper

(cid:112)

2(v + Lt/3). The bound in equation (4) becomes

≤

≥

(cid:18)

≤
t2/2
v + Lt/3

(cid:19)

−

≥

≤

4 intdim(V ) exp

4 intdim(V ) exp(

1)

4/e > 1,

−

≥

since intdim(V )

1. Thus (4) holds vacuously for 0

t < √v + L/3.

We now present Lemma 5, in which we lower bound the probability that ˜K is “close” to its expectation K + D,
in the speciﬁc sense we describe below.
Lemma 5. Let K be an exact kernel matrix, and ˜K = (Z + C)(Z + C)T be an m-features b-bit LP-RFF
2 and M :=
approximation of K with expectation K + D. For any deterministic matrix B, let L := 2n
(cid:107)
B(K + δ2

B
(cid:107)

0,

b In)BT , then for any t1, t2 ≥
(cid:20)
(cid:16)
P
B

t1In

(Z + C)(Z + C)T

(K + D)

B

t2In

(cid:22)
−
4 tr(M )
M
(cid:107)

(cid:107)

1

≥

−

(cid:20)

(cid:18)

exp

2L(
(cid:107)

−
M
(cid:107)

−

(cid:19)

mt2
1
+ 2t1/3)

+ exp

−
M
2L(
(cid:107)
(cid:107)

mt2
2
+ 2t2/3)

(cid:19)(cid:21)

.

(cid:21)

(cid:17)

(cid:18)

(cid:22)

13Theorem 7.7.1 in Tropp (2015) requires that t

√v + L/3, where t, v, and L are as deﬁned in Theorem 4.

≥

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

=

Si

Proof. Let
=
B (cid:0)(Z + C)(Z + C)T
(K + D)(cid:1) B. We see that E[Si] = 0. We will bound λmax(S) and λmin(S) by
applying the matrix Bernstein inequality for symmetric matrices with intrinsic dimension (Theorem 4). Thus we
need to bound

i=1 Si

(cid:80)m

and

and

=

−

−

S

Si

(cid:0)B(zi + ci)(zi + ci)T BT

B(K + D)BT (cid:1)

(cid:80)m

1
m

(cid:107)
Let ui = B(zi + ci)

(cid:107)

(cid:107)

E[S2
i ]
.
i=1
(cid:107)
Rn, then Si = 1
m (uiuT
2 =
(cid:107)

uiuT
(cid:107)

=

i −

∈

B(zi + ci)
(cid:107)
(cid:107)
where we have used the fact that zi + ci is a vector of length n whose entries are in [
bound on

zi + ci
(cid:107)

≤ (cid:107)

i (cid:107)

ui

B

B

(cid:107)

(cid:107)

(cid:107)

(cid:107)

Si

−

2

2

2,

:
(cid:107)

(cid:107)

Si
(cid:107)

(cid:107)

=

1
m (cid:107)

uiuT

i −

E[uiuT
i ]

1
m (cid:107)

uiuT

(cid:107) ≤

1
m

E
uiuT
(cid:107)

i (cid:107) ≤

4n

2

B
(cid:107)
m

(cid:107)

= 2L/m.

uiuT
(cid:107)
2
(cid:107)

≤

i (cid:107)
2n

E[uiuT

i ]). We ﬁrst bound

. Since this is a rank 1 matrix,

√2, √2]. This gives a

Thus λmax(Si)
Now it’s time to bound E[S2

2L/m and λmax(

−
i ]. We will use

Si) =

≤

−

λmin(Si)

≤

+

i (cid:107)
2L/m.

E[S2

i ] =

1
m2
1
m2

(cid:16)

E(cid:2)(uiuT

i )2(cid:3)

E(cid:2)uiuT

i

−

E[

ui
(cid:107)

(cid:107)

2uiuT
i ]

(cid:22)

2n

2
(cid:107)

B
(cid:107)
m2

(cid:3)2(cid:17)

1
m2
(cid:22)
E[uiuT

i ].

=

E[(uiuT

i )2] =

E[uiuT

i uiuT
i ]

1
m2

Thus

m
(cid:88)

i=1

E[S2
i ]

2n

2
(cid:107)

B
(cid:107)
m

(cid:22)

E[v1vT

1 ] =

2n

2
(cid:107)

B
(cid:107)
m

B(K + D)BT

B(K + δ2

b In)BT = LM/m.

2n

2
(cid:107)

B
(cid:107)
m

(cid:22)

Applying Theorem 4 with S, for any t2 ≥
(cid:16)

(Z + C)(Z + C)T

(cid:20)
λmax(B

P

0, we have

(cid:17)

−

(K + D)

4 tr(M )
M
(cid:107)
S and using the fact that λmax(

t2In

B)

(cid:23)

≤

(cid:107)
S) =

exp

(cid:21)

(cid:21)

Similarly, applying Theorem 4 with

(cid:20)
λmin(B

P

(cid:16)

(Z + C)(Z + C)T

(K + D)

B)

(cid:17)

−

−

t1In

(cid:22) −

≤

−

exp

−
4 tr(M )
M

(cid:107)

(cid:107)

Combining the two bounds with the union bound yields the desired inequality.

(cid:18)

(cid:19)

.

mt2
2
+ 2t2/3)

−
2L(
M
(cid:107)
(cid:107)
λmin(S), for any t1 ≥
(cid:19)
(cid:18)
mt2
1
+ 2t1/3)

2L(

−
M
(cid:107)
(cid:107)

.

0, we have

We are now ready to show that low-precision features yield close spectral approximation to the exact kernel
matrix.
Theorem 2. Let ˜K be an m-feature b-bit LP-RFF approximation of a kernel matrix K, and assume
b := 2/(2b
δ2
δ2
b /λ,
(cid:104)
P

1)2. Then for any ∆1 ≥
˜K + λI
∆1)(K + λI)

0, ∆2 ≥
(1 + ∆2)(K + λI)

K
(cid:107)

(cid:107) ≥

(1

−

≥

λ

(cid:105)

−

(cid:22)

(cid:22)

8 tr (cid:0)(K + λIn)−

1(K + δ2

b In)(cid:1)

exp

1

−

≥

(cid:32)

(cid:18)

(cid:19)

m∆2
1
−
4n
λ (1 + 2
3 ∆1)

+ exp

(cid:32)

m(∆2 −

−
4n
λ (1 + 2

3 (∆2 −

δ2
λ )2
b
δ2
λ ))
b

(cid:33)(cid:33)

.

Proof. We conjugate the desired inequality with B := (K + λIn)−
noting that semideﬁnite ordering is preserved by conjugation:

1/2 (i.e., multiply by B on the left and right),

∆1)(K + λIn)

˜K + λIn

(1 + ∆2)(K + λIn)

(1

(1

−

∆1)In

⇐⇒

⇐⇒ −

⇐⇒ −

⇐⇒ −

−
∆1In

∆1In

∆1In

(cid:22)

(cid:22)

(cid:22)

(cid:22)
B( ˜K + λIn)B

(cid:22)
B( ˜K + λIn)B
B( ˜K + λIn
B( ˜K

−
K)B

−

(cid:22)

−
K

(cid:22)
(1 + ∆2)In

(cid:22)
In

∆2In

(cid:22)
λIn)B

(cid:22)

−
∆2In.

∆2In

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

δ2
b /λ)In implies
(∆2 −
We show that
∆1In
(cid:22)
δ2
1
(K + λIn)−
b /λ because
0 by Lemma 3 and B is symmetric, BDB

K
−
B2
(cid:107)
(cid:107)

−
2 =
(cid:107)

D)B
=

B( ˜K
B
(cid:107)

1/λ and

(cid:22)

−

(cid:107)

(cid:107)
0. Thus the condition

(cid:107) ≤

B( ˜K
∆1In
−
(cid:22)
δ2
b , so BDB
∆1In

−
D

K)B
∆2In. Indeed,
(cid:22)
(δ2
b /λ)In. Moreover,
B( ˜K

D)B

K

−

−

(cid:22)

(cid:22)
(cid:22)

−

(cid:107) ≤
(cid:23)

BDB
(cid:107) ≤
(cid:107)
since D
(cid:23)
δ2
b /λ)In implies:
(∆2 −

B( ˜K
B( ˜K

−

K)B = B( ˜K
K)B = B( ˜K

Hence P

∆1In

(cid:104)
−

K)B

∆2In

−
B( ˜K
B( ˜K

−

−

K

K

(cid:105)

−

−

D)B + BDB

D)B + BDB

(cid:22)

(∆2 −
∆1In + 0 =

(cid:104)

P

∆1In

(cid:23) −
B( ˜K

∆1In.

−

b /λ)In + δ2
δ2

b /λIn = ∆2In,

K

D)B

It remains to

δ2
b /λ)In

(cid:105)

.

(∆2 −

(cid:22)
show that
D)B
(cid:22)
apply Lemma 5 for B := (K + λIn)−

∆1In

K

−

(cid:22)

−

−

−

−
≥
δ2
b /λ)In with the desired probability, by applying Lemma 5. We
(∆2 −
1/2, L := 2n

2n/λ, and M := B(K + δ2

−

(cid:22)

−

(cid:22)

(cid:22)

2

≤
To simplify the bound one gets from applying Lemma 5 with the above B, L, and M , we will use the following
=
expression for tr(M ), and the following upper and lower bounds on

B2(K + δ2

tr(M ) = tr

(cid:16)

(cid:17)

(cid:107)

b In)B.

B
(cid:107)

(cid:17)

b In)

(cid:16)

1(K + δ2

(K + λIn)−

tr
b In)U T U (S + λIn)−
δ2
λ1 ≥
M
(cid:107)

(cid:107) ≤

λ by assumption),

1/2U T = U (S + λIn)−

M
(cid:107)

(cid:107)

= (λ1 + δ2

1(S + δ2
b )/(λ1 + λ)

≥

1. Lemma 5 allows us to conclude the following:

. Letting K = U SU T be the SVD of K, we get that M = U (S + λIn)−

(cid:107)

M

.
(cid:107)

b In)
1/2U T U (S +
b In)U T . Thus, letting λ1 be the largest eigenvalue of K (recall
λ, so

1/2. We also assume that δ2

(λ1 + δ2

b )/(2λ1)

b ≤

≥

(cid:20)

P

(1

−

(cid:20)
= P

∆1)(K + λIn)

˜K + λIn

(cid:22)

∆1In

(cid:22)

B( ˜K

K)B

∆2In

−

(cid:22)

(cid:22)

(cid:21)

(cid:21)
(1 + ∆2)(K + λIn)

≥

≥

(cid:104)

P

−

−

−

1

1

∆1In

B( ˜K
(cid:20)

−
(cid:18)

(cid:22)
4 tr(M )
M
(cid:107)
8 tr (cid:0)(K + λIn)−

exp

(cid:107)

≥

−

(K + D))B

(cid:105)

(cid:22)

(∆2 −
(cid:19)

δ2
b /λ)In
(cid:18)

−
M
(cid:107)

2L(
(cid:107)
1(K + δ2

m∆2
1
+ 2∆1/3)
b In)(cid:1)

(cid:20)

exp

(cid:18)

+ exp

−
2L(
M
(cid:107)
m∆2
1
4n/λ(1 + 2∆1/3)

−

b /λ)2
δ2
m(∆2 −
δ2
b /λ)/3)
+ 2(∆2 −
(cid:107)
(cid:18)
(cid:19)
+ exp

(cid:19)(cid:21)

m(∆2 −

δ2
b /λ)2
δ2
4n/λ(1 + 2(∆2 −
b /λ)/3)

−

(cid:19)(cid:21)

.

There is a bias-variance trade-oﬀ: as we decrease the number of bits b, under a ﬁxed memory budget we can use
more features, and (Z + C)(Z + C)T concentrates more strongly (lower variance) around the expectation K + D
δ2
with 0
b In. However, this expectation is further away from the true kernel matrix K (larger bias). Thus
there should be an optimal number of bits b∗ that balances the bias and the variance.

D

(cid:22)

(cid:22)

Proof of Corollary 2.1. Letting ∆2 → ∞
(cid:105)
˜K + λIn

∆1)(K + λIn)

(cid:104)
(1

P

−

(cid:22)

1

≥

−

in Theorem 2 gives

8 tr (cid:0)(K + λIn)−

1(K + δ2

b In)(cid:1) exp

(cid:18)

m∆2
1
4n/λ(1 + 2∆1/3)

−

(cid:19)

.

Using the assumption that ∆1 ≤
(cid:104)

P

(1

−

∆1)(K + λIn)

3/2, we can simplify the bound:

(cid:105)

˜K + λIn

(cid:22)

1

≥

−

8 tr (cid:0)(K + λIn)−

1(K + δ2

b In)(cid:1) exp

(cid:18)

m∆2
1
−
8n/λ

(cid:19)

.

Letting the RHS be 1

ρ and solving for m yields

−

m

≥

8n/λ
∆2
1

log

(cid:17)

.

(cid:16) a
ρ

Similarly, letting ∆1 → ∞

in Theorem 2 gives

P

(cid:104)
˜K + λIn

(1

(cid:22)

−

(cid:105)
∆2)(K + λIn)

1

≥

−

8 tr (cid:0)(K + λIn)−

1(K + δ2

b In)(cid:1) exp

(cid:18)

m(∆2 −
−

δ2
b /λ)2
δ2
b /λ)/3)
4n/λ(1 + 2(∆2 −

(cid:19)

.

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Using the assumption that ∆1 ≤

3/2, we can simplify the bound:

(cid:104)

P

(1

−

∆1)(K + λIn)

(cid:105)

˜K + λIn

(cid:22)

1

≥

−

8 tr (cid:0)(K + λIn)−

1(K + δ2

b In)(cid:1) exp

(cid:18)

−

m(∆2 −
8n/λ

b /λ)2
δ2

(cid:19)

.

Letting the RHS be 1

ρ and solving for m yields

−

m

≥

(∆2 −

8n/λ
b /λ)2 log
δ2

(cid:17)

.

(cid:16) a
ρ

and set ∆1 = ∆2 = ∆, we get the following corollary, similar to the result

If we let the number of bits b go to
from Avron et al. (2017):
Corollary 5.1. Suppose that ˜K = ZZ T ,

∞

(cid:104)

P

(1

−

∆)(K + λIn)

˜K + λIn

(cid:22)

(cid:22)

(cid:105)
(1 + ∆)(K + λIn)

≥

−

1

16 tr((K + λIn)−

1K) exp

(cid:18)

3m∆2
16n/λ

−

(cid:19)

.

K
(cid:107)

(cid:107) ≥

λ. Then for any ∆

1/2,

≤

Thus if we use m
≥
(1 + ∆)(K + λIn) with probability at least 1

16
3∆2 n/λ log(16 tr((K + λIn)−

ρ.

−

1K)/ρ) features, then (1

∆)(K + λIn)

−

˜K + λIn

(cid:22)

(cid:22)

The constants are slightly diﬀerent from that of Avron et al. (2017) as we use the real features √2 cos(wT x + a)
instead of the complex features exp(iwT x).

From these results, we now know that the number of features required depends linearly on n/λ; more precisely,
n/λ features (for some constant c0 > 0), ˜K + λIn will be a (∆, ∆)-spectral
we know that if we use m
approximation of K + λIn with high probability. Avron et al. (2017) further provide a lower bound, showing that
n/λ (for some other constant c1 > 0), ˜K + λIn will not be a (∆, ∆)-spectral approximation of K + λIn
if m
with high probability. This shows that the number of random Fourier features must depend linearly on n/λ.

c0 ·

c1 ·

≤

≥

C.2 Frobenius Kernel Approximation Error Bounds for LP-RFFs

We begin this section by bounding the variance of the quantization noise C added to the random feature matrix
Z. We prove this as a simple consequence of the following Lemma.14
Lemma 6. For z
probability c
c

be the random variable which with probability z
c
(c

z
z. Then E [X a,c

a
a equals c
−
−
a)2
.
−
4

] = 0, and VAR [X a,c

[a, c], let X a,c

z, and with

] = (c

z)(z

a)

−

z

z

∈
z
a equals a
−
−

−

−

−

≤

Proof.

VAR [X a,c

z

] = (c

E [X a,c
z

] = (c

z)

z
c

·

a
a

+ (a

z)

−

c
c

·

z
a

−
−
c
−
c
−
a))

z
a

z)2

−
z + z

·

−

+ (a

a
−
a
−
a)((c
−
c
a
a)

−

−

= 0.

z)2

·
z)(z

−

−

(c

=

−
−
z
c

−

z)(z

−

= (c
d
dz

−
[
−

=

2z + c + a.

−

d
dz

[VAR [X a,c

z

]] =

z2 + (c + a)z

ac]

−

Now, setting the derivative to 0 gives z∗ = c+a
z)(z

a) = (c

a)2
a) = (c
−
4

2 )( c+a
c+a

.

−

−

2 −

2 . Thus, arg maxz

[a,c](c

z)(z

∈

−

−

a) = c+a

2 , and maxz

[a,c](c

∈

−

14This lemma is also a direct consequence of Popoviciu’s inequality on variances (Popoviciu, 1935). Nonetheless, we

include a stand-alone proof of the lemma here for completeness.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Proposition 7. E[C 2

Zij]

b /m, for δ2
δ2

b :=

ij |

≤

(2b

2

1)2 .
−

Proof. Given a feature Zij

2/m], we quantize it to b bits by dividing this interval into 2b

1

−

1 ), and randomly rounding to the top or bottom of the sub-interval
equally-sized sub-intervals (each of size
containing Zij (in an unbiased manner). Let a, c denote the boundaries of the sub-interval containing Zij, where

−

(cid:112)

2/m,

(cid:112)

[

∈

−

2√2/m
2b

1 . We can now see that Cij = X a,c
−
Zij] = 0. Because c

a =

2√2/m
2b

Zij is the unique random variable such that Zij + Cij
X a,c

1 , it follows from Lemma 6 that E[C 2

Zij] = VAR

(cid:104)

a, c
}
(cid:105)

∈ {
Zij

≤

Zij |

−

−

ij |

c = a +

2√2/m
2b
and E [Cij
a)2
4 = 8/m
(c

4(2b

−

|

−

1)2 = δ2

b /m.

We now prove an upper bound on the expected kernel approximation error for LP-RFFs, which applies for any
quantization function with bounded variance. This error corresponds exactly to the variance of the random
variable which is the product of two quantized random features.
Theorem 8. For x, y
≤
σ2, and that k(x, x) = k(y, y) = 1.15 For any unbiased random quantization function Q with bounded variance
VAR [Q(z)]
≤
2˜σ2 + ˜σ4 + σ2.

˜σ2 for any (ﬁxed) z, it follows that E [Q(Zx)Q(Zy)] = k(x, y), and that VAR [Q(Zx)Q(Zy)]

, assume we have random variables Zx, Zy satisfying E [ZxZy] = k(x, y), VAR [ZxZy]

∈ X

≤

Proof. Let Q(Zx) = Zx + (cid:15)x, and Q(Zy) = Zy + (cid:15)y, where E [(cid:15)x] = E [(cid:15)y] = 0, E (cid:2)(cid:15)2
are independent random variables.

x

(cid:3)

≤

˜σ2, E (cid:2)(cid:15)2

y

(cid:3)

≤

˜σ2, and (cid:15)x,(cid:15)y

E [Q(Zx)Q(Zy)] = E [(Zx + (cid:15)x)(Zy + (cid:15)y)]

= E [ZxZy + Zy(cid:15)x + Zx(cid:15)y + (cid:15)x(cid:15)y]
= E [ZxZy]
= k(x, y).

VAR [Q(Zx)Q(Zy)] = E

Q(Zx)Q(Zy)

k(x, y)

(cid:17)2(cid:21)

−

(cid:20)(cid:16)

(cid:20)(cid:16)

(cid:20)(cid:16)

(cid:20)(cid:16)

= E

= E

= E

(Zx + (cid:15)x)(Zy + (cid:15)y)

k(x, y)

−

(cid:17)2(cid:21)

Zy(cid:15)x + Zx(cid:15)y + (cid:15)x(cid:15)y + ZxZy

k(x, y)

(cid:17)2(cid:21)

−
(cid:20)(cid:16)

(cid:17)2(cid:21)

+ E

(cid:17)2(cid:21)

ZxZy

k(x, y)

−

Zy(cid:15)x + Zx(cid:15)y + (cid:15)x(cid:15)y

≤

y (cid:15)2

x + Z 2

E (cid:2)Z 2
y + (cid:15)2
k(y, y)˜σ2 + k(x, x)˜σ2 + ˜σ4 + σ2

(cid:3) + σ2

x(cid:15)2

x(cid:15)2
y

≤
= 2˜σ2 + ˜σ4 + σ2.

Note that if x = y, for this proof to hold, we would need to randomly quantize Zx twice, giving two independent
quantization noise samples (cid:15)(1)

x and (cid:15)(2)
x .

σ2, quantizing the random features will have negligable eﬀect on the
This theorem suggests that if 2˜σ2 + ˜σ4
variance. In practice, for the random quantization method we use with random Fourier features (described in
Section 4.1), we have ˜σ2 =
1)2 . Thus, for a large enough precision b, the quantization noise will be tiny
relative to the variance inherent to the random features.

(cid:28)

(2b

−

2

We note that the above theorem applies to one-dimensional random features, but can be trivially extended to m
dimensional random features. We show this in the following corollary.

15For example, one speciﬁc instance of the random variables Zx, Zy is given by random Fourier features, where zx =
√2, √2], for random w, b. We need not assume that k(x, x) = k(y, y) = 1,

√2 cos(wT x+b), zy = √2 cos(wT y +b), zx, zy
but this simpliﬁes the ﬁnal expression, as can be seen in the last step of the proof.

−

∈

[

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Table 3: Dataset details. For classiﬁcation tasks, we write the number of classes in parentheses in the “task”
column.

Dataset Task

Train Heldout Test # Features

Reg.
Census
YearPred Reg.
CovType Class. (2)
TIMIT

Class. (147)

16k
417k
418k
2.3M 245k

2k
46k
46k

2k
52k
116k
116k

119
90
54
440

Table 4: The Gaussian kernel bandwidths used, and the search grid for initial learning rate on the Census,
YearPred, Covtype and TIMIT datasets. Optimal learning rate in bold.

Dataset

Census
YearPred
Covtype
TIMIT

1/2σ2

0.0006
0.01
0.6
0.0015

Initial learning rate grid

0.01, 0.05, 0.1, 0.5, 1.0
0.05, 0.1, 0.5, 1.0, 5.0
1.0, 5.0, 10.0, 50.0, 100.0
5.0, 10.0, 50.0, 100.0, 500.0

∈ X

Corollary 8.1. For x, y
VAR [Zx] , VAR [Zy]
≤
bounded variance (E [Q(z)] = z, VAR [Q(z)]
(T1, . . . , Tn) be a random sequence of i.i.d. draws from S and T respectively. Deﬁne ¯Sn = 1
n
¯Tn = 1
n
VAR (cid:2) ¯Sn

, assume we have random variables Zx, Zy satisfying E [ZxZy] = k(x, y), and
σ2, and that k(x, x) = k(y, y) = 1. Let Q be any unbiased quantization function with
˜σ2 for any z). Let S = ZxZy, T = Q(Zx)Q(Zy), and (S1, . . . , Sn),
(cid:80)n
i=1 Si, and
(cid:3) = k(x, y), and that

≤
i=1 Ti, to be the empirical mean of these draws. It follows that E (cid:2) ¯Sn
.

(cid:3) = E (cid:2) ¯Tn

(cid:80)n
(cid:3)

n , and VAR (cid:2) ¯Tn

2˜σ2+˜σ4+σ2
n

σ2

(cid:3)

≤

≤

Proof.

VAR (cid:2) ¯Sn

(cid:3) = VAR

(cid:34)

1
n

VAR

(cid:35)

Si

(cid:21)

Si

n
(cid:88)

i=1
(cid:20) 1
n

=

≤

=

n
(cid:88)

i=1

n

·
σ2
n

σ2
n2

The result for VAR (cid:2) ¯Tn

(cid:3) follows in the same way, using the result from Theorem 8.

D EXPERIMENT DETAILS AND EXTENDED RESULTS

D.1 Datasets and Details Applying to All Experiments

In this work, we present results using FP-Nystr¨om, FP-RFFs, circulant FP-RFFs, and LP-RFFs on the TIMIT,
YearPred, CovType, and Census datasets. These datasets span regression, binary classiﬁcation, and multi-class
classiﬁcation tasks. We present details about these datasets in Table 3. In these experiments we use the Gaussian
kernel with the bandwidth σ speciﬁed in Table 4; we use the same bandwidths as May et al. (2017). To evaluate
the performance of these kernel models, we measure the classiﬁcation error for classiﬁcation tasks, and the mean
squared error (MSE) for regression tasks ( 1
yi)2), on the heldout set. We compute the total
n
memory utilization as the sum of all the components in Table 1.

i=1(f ˜K(xi)

(cid:80)n

−

For all datasets besides TIMIT, we pre-processed the features and labels as follows: We normalized all continuous

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

features to have zero mean and unit variance. We did not normalize the binary features in any way. For regression
datasets, we normalized the labels to have zero mean across the training set.

TIMIT (Garofolo et al., 1993) is a benchmark dataset in the speech recognition community which contains
recordings of 630 speakers, of various English dialects, each reciting ten sentences, for a total of 5.4 hours of
speech. The training set (from which the heldout set is then taken) consists of data from 462 speakers each
reciting 8 sentences (SI and SX sentences). We use 40 dimensional feature space maximum likelihood linear
regression (fMLLR) features (Gales, 1998), and concatenate the 5 neighboring frames in either direction, for a
total of 11 frames and 440 features. This dataset has 147 labels, corresponding to the beginning, middle, and end
of 49 phonemes. For reference, we use the exact same features, labels, and divisions of the dataset, as (Huang
et al., 2014; Chen et al., 2016; May et al., 2017).

We acquired the CovType (binary) and YearPred datasets from the LIBSVM webpage,16 and the Census dataset
from Ali Rahimi’s webpage.17 For these datasets, we randomly set aside 10% of the training data as a heldout set
for tuning the learning rate and kernel bandwidth.

The speciﬁc ﬁles we used were as follows:

•

•

•

CovType: We randomly chose 20% of “covtype.libsvm.binary” as test, and used the rest for training/heldout.

YearPred: We used “YearPredictionMSD” as training/heldout set, and “YearPredictionMSD.t” as test.

Census: We used the included matlab dataset ﬁle “census.mat” from Ali Rahimi’s webpage. This Matlab ﬁle
had already split the data into train and test. We used a random 10% of the training data as heldout.

D.2 Nystr¨om vs. RFFs (Section 2.2)

}

∈ {

. For RFFs, we use m

1250, 2500, 5000, 10000, 20000

We compare the generalization performance of full-precision RFFs and the Nystr¨om method across four
datasets, for various memory budgets. We sweep the following hyperparameters: For Nystr¨om, we use
m
1250, 2500, 5000, 10000, 20000, 50000, 100000,
∈ {
200000, 400000
. We choose these limits diﬀerently because 20k Nystr¨om features have roughly the same memory
}
footprint as 400k FP-RFFs. For all experiments, we use a mini-batch size of 250. We use a single initial learning
rate per dataset across all experiments, which we tune via grid search using 20k Nystr¨om features. We choose
to use Nystr¨om features to tune the initial learning rate in order to avoid biasing the results in favor of RFFs.
We use an automatic early-stopping protocol, as in (Morgan and Bourlard, 1990; Sainath et al., 2013b,a), to
regularize our models (Zhang et al., 2005; Wei et al., 2017) and avoid expensive hyperparameter tuning. It works
as follows: at the end of each epoch, we decay the learning rate in half if the heldout loss is less than 1% better
relative to the previous best model, using MSE for regression and cross entropy for classiﬁcation. Furthermore, if
the model performs worse than the previous best, we revert the model. The training terminates after the learning
rate has been decayed 10 times.

We plot our results comparing the Nystr¨om method to RFFs in Figure 7. We compare the performance of these
methods in terms of their number of features, their training memory footprint, and the squared Frobenius norm
and spectral norm of their kernel approximation error.

D.3 Nystr¨om vs. RFFs Revisited (Section 3.2)

D.3.1 Small-Scale Experiments

In order to better understand what properties of kernel approximation features lead to strong generalization
performance, we perform a more ﬁne-grained analysis on two smaller datasets from the UCI machine learning
repository—we consider the Census regression task, and a subset of the CovType task with 20k randomly sampled
training points and 20k randomly sampled heldout points. The reason we use these smaller datasets is because
computing the spectral norm, as well as (∆1,∆2) are expensive operations, which requires instantiating the kernel
matrices fully, and performing singular value decompositions. For the Census experiments, we use the closed-form
solution for the kernel ridge regression estimator, and choose the λ which gives the best performance on the

16https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/
17https://keysduplicated.com/ ali/random-features/data/

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

(a)

(b)

(c)

(d)

Figure 7: Generalization performance of FP-RFFs and Nystr¨om with respect to # features (a) and training
memory footprint (b) on Census, CovType, Yearpred and TIMIT. Nystr¨om performs better for a ﬁxed number
of features, while FP-RFFs perform better under a memory budget. We also see that the relative performance
between these methods cannot be fully explained by the Frobenius norms (c) or spectral norms (d) of their
respective kernel approximation error matrices; in particular, we see many example of Nystr¨om models that have
lower Frobenius or spectral error, but worse heldout performance, than various RFF models. To quantify the
degree of alignment between these error metrics and generalization performance (right plots), we show in the
ﬁgure titles the Spearman rank correlation coeﬃcients ρ between the corresponding x and y metrics. We see
in Figure 11 that 1/(1
∆1) attains notably higher values of ρ than the Frobenius and spectral norms of the
kernel approximation error. Note that although we plot the average performance across three random seeds for
each experimental setting (error bars indicate standard deviation), when we compute the ρ values we treat each
experimental result independently (without averaging).

−

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

−

Figure 8: The correlation between generalization performance and squared Frobenius norm, spectral norm, ∆,
and 1/(1
∆1) for FP-RFFs and FP-Nystr¨om on the Census and subsampled CovType datasets. To quantify
the alignment between these metrics and downstream performance, we include the Spearman rank correlation
coeﬃcients ρ computed between these metrics and the downstream performance of our trained models in the
ﬁgure titles. Note that although we plot the average performance across ﬁve random seeds for each experimental
setting (error bars indicate standard deviation), when we compute the ρ values we treat each experimental result
independently (without averaging).

Table 5: Number of features used for the diﬀerent kernel approximation methods in the experiments on
generalization performance vs. (∆1, ∆2) in Sections 3.2 and 5.2.

Methods

Number of features

FP-Nystr¨om
FP-RFF
Cir. FP-RFF
LP-RFF 16
LP-RFF 8, 4, 2, 1

25, 50, 100, 200, 500, 1250, 2500, 5000, 10000, 20000
200, 500, 1000, 2000, 5000, 10000, 20000
200, 500, 1000, 2000, 5000, 10000, 20000
500, 1000, 2000, 5000, 10000, 20000, 50000
1000, 2000, 5000, 10000, 20000, 50000

heldout set. For CovType, because there is no closed-form solution for logistic regression, we used the following
training protocol to (approximately) ﬁnd the model which minimizes the regularized training loss (just like the
closed-form ridge regression solution does). For each value of λ, we train the model to (near) convergence using 300
epochs of SGD (mini-batch size 250) at a constant learning rate, and pick the learning rate which gives the lowest
regularized training loss for that λ. We then evaluate this converged model on the heldout set to see which λ gives
the best performance. We pick the best learning rate, as well as regularization parameter, by using 20k Nystr¨om
features as a proxy for the exact kernel (note that because there are only 20k training points, this Nystr¨om
approximation is exact). We choose the learning rate 5.0 from the set
, and
}
the regularization parameter λ = 5e
. For the Census
4
}
. We sweep the number
1e
dataset, we pick λ = 5e
}
{
of features shown in Table 5. For both datasets, we report the average squared Frobenius norm, spectral norm,
∆, (∆1, ∆2) and the average generalization performance, along with standard deviations, using ﬁve diﬀerent
random seeds. The results are shown in Figure 8. As can be seen in Figure 8, 1/(1
∆1) aligns much better with
generalization performance than the other metrics.

0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0
5, 1e
−
1
−

6 from
5, 5e

{
6, 5e
3, 1e

7, 5e
4, 1e

7, 1e
3, 5e

6, 1e
2, 5e

5, 5e
2, 1e

8, 1e
4, 5e

5e
{
5, 1e

4 from

−
−

−
−

−
−

−
−

−
−

−
−

−
−

−

−

−

−

It is important to note that although 1/(1
∆1) aligns quite well with generalization performance (Spearman
rank correlation coeﬃcients ρ equal to 0.958 and 0.948 on Census and CovType, respectively), it does not align
perfectly. In particular, we see that for a ﬁxed value of ∆1, Nystr¨om generally attains better heldout performance
than RFFs. We believe this is largely explained by the fact that Nystr¨om always has ∆2 = 0, while RFFs can have

−

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

−

Figure 9: The correlation between generalization performance and squared Frobenius norm, spectral norm, ∆,
and 1/(1
∆1) for various types of full-precision RFFs. The Spearman rank correlation coeﬃcients ρ between the
x and y metrics of each ﬁgure are included in the ﬁgure titles. For these full-precision RFF experiments, we see
∆1) both align very well with downstream performance
that the original ∆ (Avron et al., 2017) as well as 1/(1
−
(ρ = 0.940 and ρ = 0.938, respectively), while the Frobenius and spectral norms do not (ρ = 0.705 and ρ = 0.652,
respectively). Note that although we plot the average performance across ﬁve random seeds for each experimental
setting (error bars indicate standard deviation), when we compute the ρ values we treat each experimental result
independently (without averaging).

Figure 10: The generalization performance as a function of the number of features (left) and the training memory
footprint (right) for various types of RFFs on the Census dataset. We observe that LP-RFFs can achieve lower
heldout mean-squared error (MSE) than other types of RFFs, included the memory-eﬃcient structured orthogonal
random features (“FP-SORF (SC)”), under the same memory budget. We plot performance averaged over ﬁve
random seeds, with error bars indicating standard deviations.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

relatively large values of ∆2 when the number of features is small (see Figure 3). As we show in the generalization
bound in Proposition 1, in Figure 5, as well as in the empirical and theoretical analysis in Appendix B, we expect
generalization performance to deteriorate as ∆2 increases.

D.3.2 Experiments with Diﬀerent Types of RFFs

In Figure 9 we repeat the above experiment on the Census dataset using a number of variations of RFFs.
Speciﬁcally, we run experiments using the [sin(wT x), cos(wT x)] parameterization of RFFs, which Sutherland
and Schneider (2015) show has lower variance than the cos(wT x + a) parameterization (we denote the [sin, cos]
method by “FP-RFF (SC)”, and the cos method by “FP-RFF (C)”). We additionally use Quasi-Monte Carlo
features (Yang et al., 2014), as well as orthogonal random features and its structural variant (Yu et al., 2016); we
denote these by “FP-QMC (SC)”, “FP-ORF (SC)”, “FP-SORF (SC)” respectively, because we implement them
using the [sin, cos] parameterization. We can see in Figure 9 that although these methods attain meaningful
improvements in the Frobenius and spectral norms of the approximation error, these improvements do not
translate into corresponding gains in heldout performance. Once again we see that 1/(1
∆1) is able to much
better explain the relative performance between diﬀerent kernel approximation methods than Frobenius and
spectral error. In Figure 10 we see that LP-RFFs outperform these other methods in terms of performance under
a memory budget.

−

D.3.3 Large-Scale (∆1, ∆2) Experiments

∆1) on the large-scale experiments from Section 2.2. We measure
In Figure 11 we plot the performance vs. 1/(1
∆1 using the exact and approximate kernel matrices on a random subset of 20k heldout points (except for Census,
where we use the entire heldout set which has approximately 2k points). We pick several λ values between the
smallest and largest eigenvalues of the exact (subsampled) training kernel matrix. The strong alignment between
generalization performance and 1/(1

∆1) is quite robust to the choice of λ.

−

−

D.3.4 Measuring ∆ and (∆1, ∆2)

In order to measure the ∆1 and ∆2 between a kernel matrix K and an approximation ˜K, we ﬁrst observe that
the following statements are equivalent. Note that to get the second statement, we multiply the expressions in
the ﬁrst statement by (K + λIn)−

1/2 on the left and right.

(1

−

∆1)(K + λIn)
∆1)In

(1

−

∆1In

−

∆1In

−
1/2( ˜K

(cid:22)

(cid:22)

(cid:22)

(cid:22)

˜K + λIn
(cid:22)
(K + λIn)−

(K + λIn)−

(1 + ∆2)(K + λIn)
1/2( ˜K + λIn)(K + λIn)−
1/2(cid:16)

˜K + λIn

1/2

(cid:22)

(cid:17)

(1 + ∆2)In
1/2

(K + λIn)

(K + λIn)−

−

∆2In

(cid:22)

(K + λIn)−

1/2( ˜K

K)(K + λIn)−

1/2

∆2In.

−
1/2, this is equivalent to
(cid:0)A(cid:1). Lastly, we note that ∆ = max(∆1, ∆2).

∆1 ≤

λmin

−

(cid:22)

(cid:0)A(cid:1) and λmax

(cid:0)A(cid:1)

∆2. Thus,

≤

For A = (K + λIn)−
we choose ∆1 :=

λmin

K)(K + λIn)−
(cid:0)A(cid:1) and ∆2 := λmax

−

−

D.4 Theory Validation (Section 4.2)

To validate our theory in Section 4.2, we perform two sets of experiments to (1) demonstrate the asymptotic
behavior of ∆1 and ∆2 as the number of features increases, and (2) demonstrate that quantization has negligible
eﬀect on ∆2 when δ2

∆2.

b /λ

(cid:28)

To demonstrate the behavior of ∆1 and ∆2 as a function of the number of features, we collect ∆1, and ∆2 using
Nystr¨om features, circulant FP-RFFs, and LP-RFFs using b
, on both the Census and the sub-sampled
}
CovType datasets (20k random heldout points). For each approximation, we sweep the number of features as
listed in Table 6. We use the same value of λ as we used in Section 3.2 for each dataset. In Figure 12, we plot
the values of ∆1 and ∆2 attained by these methods as a function of the number of features. As discussed in
Section 4.2, ∆1 is primarily determined by the rank of the approximation, and approaches 0 for all the methods
as the number of features grows. ∆2, on the other hand, only approaches 0 for the high-precision methods—for
b

b /λ, marked by dashed lines).

, ∆2 converges to higher values (at most δ2
1, 4
}

1, 4, 8

∈ {

∈ {

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

1
∆1

1

−

, where we measure ∆1 using regularizer strength λ equal to the
Figure 11: Generalization performance vs.
0, 25, 50, 75, or 99 percentile eigenvalues of the exact kernel matrix (0 percentile indicates largest eigenvalue).
Note for the Census dataset, we plot the heldout MSE as a function of 1/(1
1 to avoid cluttering the
data points on the left end of the ﬁgure. For comparison to spectral and Frobenius norm plots, see Figure 7. To
∆1) and generalization performance for these diﬀerent values of
quantify the degree of alignment between 1/(1
λ, we compute the Spearman rank correlation coeﬃcients ρ. We see 1/(1
∆1) generally attains much higher
values of ρ than the Frobenius and spectral approximation errors (Figure 7). Although we plot performance
averaged across three random seeds (error bars indicate standard deviations), when we compute ρ we treat each
experimental result independently.

∆1)

−

−

−

−

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Figure 12: Empirical validation of Theorem 2. As the number of features grows, LP-RFFs approach ∆1 values of
0 (left), but plateau at larger ∆2 for very low precisions (right). These are an extended version of the results
from Figure 3 (left, middle) in Section 4.2. We plot average results across ﬁve random seeds, with error bars
indicating standard deviations.

Table 6: Number of features used for the diﬀerent kernel approximation methods in the theory validation
experiments in Section 4.2 (Figure 3 (left,middle)).

Methods

Number of features

FP-Nystr¨om
Cir. FP-RFF
LP-RFF 8, 4, 2, 1

25, 50, 100, 200, 500, 1250, 2500, 5000, 10000, 20000
200, 500, 1000, 2000, 5000, 10000, 20000, 50000, 100000, 200000, 400000
1000, 2000, 5000, 10000, 20000, 50000, 100000, 200000, 400000

To demonstrate that quantization has negligible eﬀect on ∆2 when δ2
points from the Census dataset. For λ
by the LP-RFFs relative to the exact kernel matrix. We see that for larger λ (corresponding to smaller δ2
Figure 3, lower precisions can be used while not inﬂuencing ∆2 signiﬁcantly; this aligns with the theory.

∆2, we use 8000 random training
, we measure the ∆2 attained
b /λ) in

1, 2, 4, 8, 16, 32
}

, and b
}

4, 100, 104

b /λ

10−

∈ {

∈ {

(cid:28)

D.5 Empirical Evaluation of LP-RFFs (Section 5.1)

To empirically demonstrate the generalization performance of LP-RFFs, we compare LP-RFFs to FP-RFFs,
circulant FP-RFFs, and Nystr¨om features for various memory budgets. We use the same datasets as in Section 2,
including Census and YearPred for regression, as well as CovType and TIMIT for classiﬁcation. We use the same
experimental protocol as in Section 2 (details in Appendix D.2), with the only signiﬁcant additions being that we
also evaluate the performance of circulant FP-RFFs, and LP-RFFs for precisions b
. As noted in
}
the main text, all our LP-RFF experiments are done in simulation; in particular, we represent each low-precision
feature as a 64-bit ﬂoating point number, whose value is one of the 2b values representable in b bits. For our
full-precision experiments we also use 64-bit ﬂoats, but we report the memory utilization of these experiments
as if we used 32 bits, to avoid inﬂating the relative gains of LP-RFFs over the full-precision approaches. We
randomly sample the quantization noise for each mini-batch independently each epoch.

1, 2, 4, 8, 16

∈ {

In Section 5.1 (Figure 4), we demonstrated the generalization performance of LP-RFFs on the TIMIT, YearPred,
and CovType datasets, using 4 bits per feature. In Figure 13, we additionally include results on the Census
dataset, and include results for a larger set of precisions (b
). We also include plots of heldout
performance as a function of the number of features used. We observe that LP-RFFs, using 2-8 bits, systematically
outperform the full-precision baselines under diﬀerent memory budgets.

1, 2, 4, 8, 16

∈ {

}

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 13: Generalization performance of the kernel approximation methods (Nystr¨om, FP-RFFs, circ. FP-RFFs,
LP-RFFs) with respect to the number of features used, as well as with respect to memory used. We observe
that LP-RFFs demonstrate better generalization performance than the full-precision baselines under memory
constraints, with 2-8 bits typically giving the best performance. Importantly, the relative ranking of the methods
changes depending on whether we compare the methods based on their number of features, or their memory
utilization. For example, the Nystr¨om method shows better generalization performance than the RFF-based
approaches with the same number of features. However, the Nystr¨om method often performs signiﬁcantly worse
than the RFF methods under ﬁxed memory budgets. We plot results averaged over three random seeds.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Table 7: Dataset details for the additional datasets from Section D.5. For classiﬁcation tasks, we write the number
of classes in parentheses in the “Task” column.

Dataset

Task

Train Heldout Test # Features

Class. (2)
Forest
Cod-RNA Class. (2)
Class. (2)
Adult
Reg.
CPU

470k
54k
29k
6k

52k
6k
3k
0.7k

58k
272k
16k
0.8k

54
8
123
21

Table 8: The Gaussian kernel bandwidths used, and the search grid for initial learning rate on the Forest,
Cod-RNA, Adult, and CPU datasets. Optimal learning rate in bold.

Dataset

1/2σ2

Initial learning rate grid

Forest
0.5
Cod-RNA 0.4
0.1
Adult
0.03
CPU

5.0, 10.0, 50.0, 100.0, 500.0
10.0, 50.0, 100.0, 500.0, 1000.0
5.0, 10.0, 50.0, 100.0, 500.0, 1000.0
0.05, 0.1, 0.5, 1.0, 5.0

We run on four additional classiﬁcation and regression datasets (Forest, Cod-RNA, Adult, CPU) to compare the
empirical performance of LP-RFFs to full-precision RFFs, circulant RFFs and Nystr¨om. We present the results
in Figure 14, and observe that LP-RFFs can achieve competitive generalization performance to the full-precision
baselines with lower training memory budgets. With these 4 additional datasets, our empirical evaluation of
LP-RFFs now covers all the datasets investigated in Yang et al. (2012). We include details about these datasets
and the hyperparameters we used in Tables 7 and 8.

D.6 Generalization Performance vs. (∆1, ∆2) (Section 5.2)

For our experiments in Section 5.2, we use the same protocol and hyperparameters (learning rate, λ) as for the
(∆1, ∆2) experiments in Section 3.2. However, we additionally run experiments with LP-RFFs for precisions
b
. We use ﬁve random seeds for each experimental setting, and plot the average results, with
error bars indicating standard deviations.

1, 2, 4, 8, 16

∈ {

}

In Figure 15, we plot an extended version of the right plots in Figure 5. We include results for all precisions, and
for both Census and CovType. We observe that on Census, 1/(1
∆1) does not align well with performance
(Spearman rank correlation coeﬃcient ρ = 0.403), because the low-precision features (b = 1 or b = 2) perform
signiﬁcantly worse than the full-precision features of the same dimensions. In this case, when we consider the
impact of ∆2 by taking max (cid:0)1/(1
On CovType, on the other hand, the impact on generalization performance from using low-precision is much less
(cid:1) both align well with performance (ρ = 0.942). Furthermore,
pronounced, so 1/(1
in the case of CovType, ∆2 is generally smaller than 1/(1
∆1), so taking the max does not change the plot
signiﬁcantly.

(cid:1), we see that performance aligns much better (ρ = 0.959).

∆1) and max (cid:0)1/(1

∆1), ∆2

∆1), ∆2

−

−

−

−

−

To compute the Spearman rank correlation coeﬃcients ρ for these plots, we take the union of all the experiments
which are a part of the plot. In particular, we include FP-RFFs, circulant FP-RFFs, FP-Nystr¨om , and LP-RFFs
with precisions b
. Although we plot the average performance across ﬁve random seeds in the plot,
}
to compute ρ we treat each experiment independently.

1, 2, 4, 8, 16

∈ {

D.7 Other Experimental Results

D.7.1 Low-Precision Nystr¨om

We encounter two main obstacles to attaining strong performance with the Nystr¨om method under a memory
budget: (1) For the standard Nystr¨om method, because of the large projection matrix (32m2 space), there are

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 14: Comparison of the performance of LP-RFFs and the full-precision baselines on additional datasets,
with respect to the number of features used, as well as with respect to memory used. We observe that LP-RFFs
can achieve performance competitive with the full-precision baseline methods, with signiﬁcant memory savings.
We plot the average performance across three random seeds, with error bars indicating standard deviations.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

−

Figure 15: Generalization performance vs. diﬀerent kernel approximation metrics on Census and CovType. The
metric max (1/(1
∆1), ∆2) is able to incorporate the inﬂuence of both ∆1 and ∆2 on performance for LP-RFFs,
aligning well with generalization performance on both Census (Spearman rank correlation coeﬃcient ρ = 0.959)
and CovType (ρ = 0.942). 1/(1
∆1), on the other hand, fails to align well on Census (ρ = 0.403), but does
align on CovType (ρ = 0.942). Note that although we plot the average performance across ﬁve random seeds for
each experimental setting (error bars indicate standard deviation), when we compute the ρ values we treat each
experimental result independently (without averaging).

−

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 16: We plot the heldout performance (MSE) for the full-precision Nystr¨om method, the ensemble Nystr¨om
method, and 8-bit LP-RFFs. We also show the best possible performance for the Nystr¨om methods, assuming only
the kernel approximation features are quantized (denoted “ideal”); we compute this by plotting the full-precision
results but without counting the memory occupied by the features. The LP-RFF method signiﬁcantly outperforms
the “ideal” Nystr¨om methods as a function of memory.

(a) LP-Nystr¨om

(b) Ensemble LP-Nystr¨om

Figure 17: The generalization performance of low-precision Nystr¨om and low-precision ensemble Nystr¨om, using
a uniform quantization scheme.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

inherent limits to the memory savings attainable by only quantizing the features, regardless of the quantization
scheme used. (2) While the ensemble Nystr¨om method (Kumar et al., 2009) is a well-known method which can
dramatically reduce the space occupied by the the Nystr¨om projection matrix, it does not attain meaningful gains
in performance under a memory budget. To demonstrate the ﬁrst issue empirically, we plot in Figure 16 the
performance of the full-precision Nystr¨om methods without counting the memory from the feature mini-batches
(denoted “ideal”). This is the best possible performance under any quantization scheme for these Nystr¨om
methods, assuming only the features are quantized. LP-RFFs still outperform these “ideal” methods for ﬁxed
memory budgets. To demonstrate the second issue, in Figure 16 we also plot the generalization performance of
the ensemble method vs. the standard Nystr¨om method, as a function of memory. We can see that the ensemble
method does not attain meaningful gains over the standard Nystr¨om method, as a function of memory.

(cid:112)

(cid:112)

2/m and

Lastly, we mention that quantizing Nystr¨om features is more challenging due to their larger dynamic range.
1 and 1, whereas for RFFs, each feature is between
For Nystr¨om, all we know is that each feature is between
2/m. In Figure 17 we plot our results quantizing Nystr¨om features with the following simple
−
scheme: for each feature, we ﬁnd the maximum and minimum value on the training set, and then uniformly
quantize this interval using b bits. We observe that performance degrades signiﬁcantly with less than or equal to
8 bits compared to full-precision Nystr¨om. Although using the ensemble method reduces the dynamic range by a
factor of √r with r blocks (we use r = 10, a common setting in (Kumar et al., 2012)), and also saves space on
the projection matrix, these strengths do not result in signiﬁcantly better performance for ﬁxed memory budgets.

−

D.7.2 Low-Precision Training for LP-RFFs

As discussed in Section 2.1, there are a number of ways to reduce the memory occupied by the model parameters,
including (1) using a low-rank decomposition of the parameter matrix (Sainath et al., 2013a), (2) using structured
matrices (Sindhwani et al., 2015), and (3) using low-precision (De Sa et al., 2018). Though these methods are
orthogonal to our LP-RFF method, we now present results using a low-precision parameterization of the model,
and show that we can attain similar performance to full-precision training. We use a training algorithm called
LM-HALP (linear model high-accuracy low-precision) (De Sa et al., 2018). By parameterizing the model in low
precision, this algorithm eliminates the need of casting the LP-RFFs back into full precision in order to multiply
them with the model parameters. This approach also allows for these matrix multiplications to be implemented
using fast low-precision matrix operations, and reduces the memory occupied by the model during training.

LM-HALP is based on the stochastic variance-reduced gradient (SVRG) algorithm. The model is parameterized
using a low-precision ﬁxed-point representation during training. In LM-HALP, all of the matrix multiplications
involved in the stochastic model updates are done using low-precision ﬁxed-point operations; however, the periodic
computation of the full gradient is calculated in full precision (this is embarrassingly parallelizable). Importantly,
even though most of training is done in low precision, the ﬁnal model returned by this training algorithm is a
full-precision model. We can further simplify the LM-HALP algorithm by replacing the SVRG updates with
SGD updates, thus eliminating the need for calculating and storing the full gradient. In Figure 18 we present our
results using LM-HALP on TIMIT, our largest and most challenging dataset; we use 8-bit LM-HALP (SVRG
and SGD) on top of 8-bit LP-RFFs, and compare to full-precision SGD training. For LM-HALP based training,
we perform the bit centering and rescaling operation after each training epoch. Under this setting, we show that
when the number of features is at least 10,000, the performance of both versions of HALP closely matches that of
full-precision training.

D.7.3 Double Sampling

We perform some initial experiments using the double sampling method of Zhang et al. (2017). In particular,
we use a diﬀerent random quantization of the LP-RFFs on the “forward pass” of our algorithm than in the
“backward pass.” In our initial experiments with double sampling, as shown in Figure 19, we did not observe
noticeable improvements in performance. These experiments were on the YearPred dataset with the Gaussian
kernel. We leave a more extended investigation of these gradient bias reduction methods for future work.

E EXTENDED RELATED WORK

Generalization Performance of Kernel Approximation Methods From a theoretical perspective, there
has been a lot of work analyzing the generalization performance of kernel approximation methods (Rahimi

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 18: Low-precision vs. full-precision training algorithms on TIMIT using 8-bit LP-RFFs.

Figure 19: Comparison of LP-RFFs with and without double sampling on the YearPred dataset.

and Recht, 2008; Cortes et al., 2010; Sutherland and Schneider, 2015; Rudi and Rosasco, 2017; Avron et al.,
2017; Li et al., 2018). The work most relevant to ours is that of Avron et al. (2017), which deﬁnes ∆-spectral
approximation and bounds the generalization performance of kernel approximation methods in terms of ∆. This
approach diﬀers from works which evaluate kernel approximation methods in terms of the Frobenius or spectral
norms of their kernel approximation matrices (Cortes et al., 2010; Gittens and Mahoney, 2016; Yang et al., 2014;
Sutherland and Schneider, 2015; Yu et al., 2016; Dao et al., 2017). Our work shows the promise of Avron et al.’s
approach, and builds upon it.

Large-Scale Kernel Experiments On the topic of scaling kernel methods to large datasets, there have been
a few notable recent papers. Tu et al. (2016) propose a distributed block coordinate descent method for solving
large-scale least squares problems using the Nystr¨om method or RFFs. The recent work of May et al. (2017) uses
a single GPU to train large RFF models on speech recognition datasets, showing comparable performance to
fully-connected deep neural networks. That work was limited by the number of features that could ﬁt on a single
GPU, and thus our proposed method could help scale these results.

Low-Precision Random Fourier Features for Memory-Constrained
Kernel Approximation

9
1
0
2
 
r
a

M
 
0
2
 
 
]

G
L
.
s
c
[
 
 
2
v
5
5
1
0
0
.
1
1
8
1
:
v
i
X
r
a

Jian Zhang∗

Avner May∗

Tri Dao

Christopher R´e

Stanford University

Abstract

We investigate how to train kernel approxi-
mation methods that generalize well under
a memory budget. Building on recent theo-
retical work, we deﬁne a measure of kernel
approximation error which we ﬁnd to be more
predictive of the empirical generalization per-
formance of kernel approximation methods
than conventional metrics. An important con-
sequence of this deﬁnition is that a kernel
approximation matrix must be high rank to
attain close approximation. Because storing a
high-rank approximation is memory intensive,
we propose using a low-precision quantiza-
tion of random Fourier features (LP-RFFs)
to build a high-rank approximation under a
memory budget. Theoretically, we show quan-
tization has a negligible eﬀect on generaliza-
tion performance in important settings. Em-
pirically, we demonstrate across four bench-
mark datasets that LP-RFFs can match the
performance of full-precision RFFs and the
Nystr¨om method, with 3x-10x and 50x-460x
less memory, respectively.

1 INTRODUCTION

Kernel methods are a powerful family of machine learn-
ing methods. A key technique for scaling kernel meth-
ods is to construct feature representations whose inner
products approximate the kernel function, and then
learn a linear model with these features; important ex-
amples of this technique include the Nystr¨om method
(Williams and Seeger, 2000) and random Fourier fea-
tures (RFFs) (Rahimi and Recht, 2007). Unfortunately,
a large number of features are typically needed for at-
taining strong generalization performance with these

∗Equal contribution.

Proceedings of the 22nd International Conference on Ar-
tiﬁcial Intelligence and Statistics (AISTATS) 2019, Naha,
Okinawa, Japan. PMLR: Volume 89. Copyright 2019 by
the author(s).

methods on big datasets (Rahimi and Recht, 2008; Tu
et al., 2016; May et al., 2017). Thus, the memory re-
quired to store these features can become the training
bottleneck for kernel approximation models. In this
paper we work to alleviate this memory bottleneck by
optimizing the generalization performance for these
methods under a ﬁxed memory budget.

To gain insight into how to design more memory-
eﬃcient kernel approximation methods, we ﬁrst in-
vestigate the generalization performance vs. memory
utilization of Nystr¨om and RFFs. While prior work
(Yang et al., 2012) has shown that the Nystr¨om method
generalizes better than RFFs under the the same num-
ber of features, we demonstrate that the opposite is
true under a memory budget. Strikingly, we observe
that 50,000 standard RFFs can achieve the same held-
out accuracy as 20,000 Nystr¨om features with 10x less
memory on the TIMIT classiﬁcation task. Further-
more, this cannot be easily explained by the Frobenius
or spectral norms of the kernel approximation error ma-
trices of these methods, even though these norms are
the most common metrics for evaluating kernel approx-
imation methods (Gittens and Mahoney, 2016; Yang
et al., 2014; Sutherland and Schneider, 2015; Yu et al.,
2016; Dao et al., 2017); the above Nystr¨om features
attain 1.7x smaller Frobenius error and 17x smaller
spectral error compared to the RFFs. This observa-
tion suggests the need for a more reﬁned measure of
kernel approximation error—one which better aligns
with generalization performance, and can thus better
guide the design of new approximation methods.

Building on recent theoretical work (Avron et al., 2017),
we deﬁne a measure of approximation error which we
ﬁnd to be much more predictive of empirical generaliza-
tion performance than the conventional metrics. In par-
ticular, we extend Avron et al.’s deﬁnition of ∆-spectral
approximation to our deﬁnition of (∆1, ∆2)-spectral ap-
proximation by decoupling the two roles played by ∆
in the original deﬁnition.1 This decoupling reveals that

1The original deﬁnition uses the same scalar ∆ to upper
and lower bound the approximate kernel matrix in terms
of the exact kernel matrix in the semideﬁnite order.

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Rd, kernel features
Table 1: Memory utilization for kernel approximation methods. We consider data x
Rm, mini-batch size s, # of classes c (for regression/binary classiﬁcation c = 1). We assume full-precision
z(x)
numbers are 32 bits. We measure a method’s memory utilization as the sum of the three components in this table.

∈

∈

Approximation Method

Nystr¨om
RFFs
Circulant RFFs
Low-precision RFFs, b bits (ours)

Feature generation Feature mini-batch Model parameters
32(md + m2)
32md
32m
32m

32ms
32ms
32ms
bms

32mc
32mc
32mc
32mc

∆1 and ∆2 impact generalization diﬀerently, and can
together much better explain the relative generalization
performance of Nystr¨om and RFFs than the original
∆, or the Frobenius or spectral errors. This (∆1, ∆2)
deﬁnition has an important consequence—in order for
an approximate kernel matrix to be close to the exact
kernel matrix, it is necessary for it to be high rank.

Motivated by the above connection between rank and
generalization performance, we propose using low-
precision random Fourier features (LP-RFFs) to attain
a high-rank approximation under a memory budget.
Speciﬁcally, we store each random Fourier feature in a
low-precision ﬁxed-point representation, thus achieving
a higher-rank approximation with more features in the
same amount of space. Theoretically, we show that
when the quantization noise is much smaller than the
regularization parameter, using low precision has negli-
gible eﬀect on the number of features required for the
approximate kernel matrix to be a (∆1, ∆2)-spectral
approximation of the exact kernel matrix. Empiri-
cally, we demonstrate across four benchmark datasets
(TIMIT, YearPred, CovType, Census) that in the mini-
batch training setting, LP-RFFs can match the perfor-
mance of full-precision RFFs (FP-RFFs) as well as the
Nystr¨om method, with 3x-10x and 50x-460x less mem-
ory, respectively. These results suggest that LP-RFFs
could be an important tool going forward for scaling
kernel methods to larger and more challenging tasks.

The rest of this paper is organized as follows: In Section
2 we compare the performance of the Nystr¨om method
and RFFs in terms of their training memory footprint.
In Section 3 we present a more reﬁned measure of kernel
approximation error to explain the relative performance
of Nystr¨om and RFFs. We introduce the LP-RFF
method and corresponding analysis in Section 4, and
present LP-RFF experiments in Section 5. We review
related work in Section 6, and conclude in Section 7.

2 NYSTR ¨OM VS. RFFS: AN

EMPIRICAL COMPARISON

tion of Nystr¨om and RFFs. We begin by reviewing
the memory utilization for these kernel approximation
methods in the mini-batch training setting; this is a
standard setting for training large-scale kernel approxi-
mation models (Huang et al., 2014; Yang et al., 2015;
May et al., 2017), and it is the setting we will be us-
ing to evaluate the diﬀerent approximation methods
(Sections 2.2, 5.1). We then show that RFFs outper-
form Nystr¨om given the same training memory budget,
even though the opposite is true given a budget for
the number of features (Yang et al., 2012). Lastly, we
demonstrate that the Frobenius and spectral norms of
the kernel approximation error matrix align poorly with
generalization performance, suggesting the need for a
more reﬁned measure of approximation error for eval-
uating the quality of a kernel approximation method;
we investigate this in Section 3.

For background on RFFs and the Nystr¨om method,
and for a summary of our notation, see Appendix A.

2.1 Memory Utilization

The optimization setting we consider is mini-batch
training over kernel approximation features. To un-
derstand the training memory footprint, we present in
Table 1 the memory utilization of the diﬀerent parts
of the training pipeline. The three components are:

1. Feature generation: Computing m RFFs over data
in Rd requires a random projection matrix W
Rm
×
points” ˆxi

∈
d. The Nystr¨om method stores m “landmark
m.

Rd, and a projection matrix in Rm
×

2. Feature mini-batch: Kernel approximation features
Rm for all xi in a mini-batch are stored.2

z(xi)

∈

∈

3. Model parameters: For binary classiﬁcation and
regression, the linear model learned on the z(x) fea-
Rm; for c-class classiﬁcation,
tures is a vector θ
it is a matrix θ
×

∈
Rm

c.

∈

In this work we focus on reducing the memory occupied
by the mini-batches of features, which can occupy a

To inform our design of memory-eﬃcient kernel approx-
imation methods, we ﬁrst perform an empirical study
of the generalization performance vs. memory utiliza-

2For simplicity, we ignore the memory occupied by the
mini-batches of d-dim. inputs and c-dim. outputs, as gener-
ally the number of kernel approx. features m

d, c.

(cid:29)

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

(a)

(b)

(c)

(d)

Figure 1: Generalization performance of full-precision RFFs and Nystr¨om with respect to the number of features
and training memory footprint on TIMIT (a,b). Nystr¨om performs better for a ﬁxed number of features, while
RFFs perform better under a memory budget. We also see that the generalization performance of these methods
does not align well with the Frobenius or spectral norms of their respective kernel approximation error matrices
(c,d). For results on YearPred, CovType, and Census, see Appendix D.2.

signiﬁcant fraction of the training memory. Our work
is thus orthogonal to existing work which has shown
how to reduce the memory utilization of the feature
generation (Le et al., 2013; Yu et al., 2015) and the
model parameters (Sainath et al., 2013a; Sindhwani
et al., 2015; De Sa et al., 2018) (e.g., using structured
matrices or low precision). Throughout this paper, we
measure the memory utilization of a kernel approxima-
tion method as the sum of the above three components.

2.2 Empirical Comparison

We now compare the generalization performance of
RFFs and the Nystr¨om method in terms of their train-
ing memory footprint. We demonstrate that RFFs
can outperform the Nystr¨om method given a memory
budget, and show that the diﬀerence in performance
between these methods cannot be explained by the
Frobenius or spectral norms of their kernel approxima-
tion error matrices.

In experiments across four datasets (TIMIT, YearPred,
CovType, Census (Garofolo et al., 1993; Dheeru and
Karra Taniskidou, 2017)), we use up to 20k Nystr¨om
features and 400k RFFs to approximate the Gaus-
sian kernel;3 we train the models using mini-batch
stochastic gradient descent with early stopping, with
a mini-batch size of 250. We present results averaged
from three random seeds, with error bars indicating
standard deviations (for further experiment details,
see Appendix D.2). In Figure 1(a) we observe that
as a function of the number of kernel approximation
features the Nystr¨om method generally outperforms
RFFs, though the gap narrows as m approaches 20k.
However, we see in Figure 1(b) that RFFs attain better
generalization performance as a function of memory.
Interestingly, the relative performance of these meth-

ods cannot simply be explained by the Frobenius or
spectral norms of the kernel approximation error ma-
trices;4 in Figure 1(c,d) we see that there are many
cases in which the RFFs attain better generalization
performance, in spite of having larger Frobenius or
spectral approximation error. This is a phenomenon
we observe on other datasets as well (Appendix D.2).
This suggests the need for a more reﬁned measure of the
approximation error of a kernel approximation method,
which we discuss in the following section.

3 A REFINED MEASURE OF
KERNEL APPROX. ERROR

To explain the important diﬀerences in performance be-
tween Nystr¨om and RFFs, we deﬁne a more reﬁned mea-
sure of kernel approximation error—(∆1, ∆2)-spectral
approximation. Our deﬁnition is an extension of Avron
et al.’s deﬁnition of ∆-spectral approximation, in which
we decouple the two roles played by ∆ in the original
deﬁnition. This decoupling allows for a more ﬁne-
grained understanding of the factors inﬂuencing the
generalization performance of kernel approximation
methods, both theoretically and empirically. Theoret-
ically, we present a generalization bound for kernel
approximation methods in terms of (∆1, ∆2) (Sec. 3.1),
and show that ∆1 and ∆2 inﬂuence the bound in dif-
ferent ways (Prop. 1). Empirically, we show that ∆1
and ∆2 are more predictive of the Nystr¨om vs. RFF
performance than the ∆ from the original deﬁnition,
and the Frobenius and spectral norms of the kernel
approximation error matrix (Sec. 3.2, Figure 2). An
important consequence of the (∆1, ∆2) deﬁnition is
that attaining a small ∆1 requires a large number of
features; we leverage this insight to motivate our pro-
posed method, low-precision random Fourier features,
in Section 4.

3We consider diﬀerent ranges for the number of Nystr¨om
vs. RFF features because the memory footprint for training
with 400k RFFs is similar to 20k Nystr¨om features.

˜K,
4We consider the Frobenius and spectral norms of K
where K and ˜K are the exact and approximate kernel
matrices for 20k randomly sampled heldout points.

−

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

3.1

(∆1, ∆2)-spectral Approximation

We begin by reviewing what it means for a matrix
A to be a ∆-spectral approximation of a matrix B
(Avron et al., 2017). We then extend this deﬁnition
to (∆1, ∆2)-spectral approximation, and bound the
generalization performance of kernel approximation
methods in terms of ∆1 and ∆2 in the context of ﬁxed
design kernel ridge regression.
0, a symmetric matrix A is a
Deﬁnition 1. For ∆
∆-spectral approximation of another symmetric matrix
(1 + ∆)B.
B if (1

∆)B

≥

A

−

(cid:22)

(cid:22)

We extend this deﬁnition by allowing for diﬀerent values
of ∆ in the left and right inequalities above:
0, a symmetric matrix
Deﬁnition 2. For ∆1, ∆2 ≥
A is a (∆1, ∆2)-spectral approximation of another
(1 + ∆2)B.
symmetric matrix B if (1

−
Throughout the text, we will use ∆ to denote the
variable in Def. 1, and (∆1, ∆2) to denote the variables
In our discussions and experiments, we
in Def. 2.
always consider the smallest ∆, ∆1, ∆2 satisfying the
above deﬁnitions; thus, ∆ = max(∆1, ∆2).

∆1)B

(cid:22)

(cid:22)

A

In the paragraphs that follow we present generalization
bounds for kernel approximation models in terms of
∆1 and ∆2 in the context of ﬁxed design kernel ridge
regression, and demonstrate that ∆1 and ∆2 inﬂuence
generalization in diﬀerent ways (Prop. 1). We consider
the ﬁxed design setting because its expected generaliza-
tion error has a closed-form expression, which allows us
to analyze generalization performance in a ﬁne-grained
fashion. For an overview of ﬁxed design kernel ridge
regression, see Appendix A.3.

×

n, a regularization parameter λ
(xi, yi)

In the ﬁxed design setting, given a kernel matrix K
∈
Rn
0, and a set of
≥
n
labeled points
i=1 where the observed labels
}
yi = ¯yi + (cid:15)i are randomly perturbed versions of the true
(cid:3) = σ2 <
R ((cid:15)i independent, E [(cid:15)i] = 0, E (cid:2)(cid:15)2
labels ¯yi
), it is easy to show (Alaoui and Mahoney, 2015) that

∞
the optimal kernel regressor5 fK has expected error

∈

{

i

(fK) =

R

λ2
n

¯yT (K + λI)−

2 ¯y +

tr

K 2(K + λI)−

2(cid:17)

,

(cid:16)

σ2
n

where ¯y = (¯y1, . . . , ¯yn) is the vector of true labels.

This closed-form expression for generalization error
allows us to bound the expected loss
(f ˜K) of a kernel
ridge regression model f ˜K learned using an approximate
kernel matrix ˜K in place of the exact kernel matrix K.
In particular, if we deﬁne

R

(fK) :=

(cid:98)
R

λ
n

¯yT (K + λI)−

1 ¯y +

tr

K(K + λI)−

1(cid:17)

,

(cid:16)

σ2
n

5fK (x) = (cid:80)

i αik(x, xi) for α = (K + λI)−1y.

∞

(fK), we can bound the

which is an upper bound on
R
expected loss of f ˜K as follows:
Proposition 1. (Extended from (Avron et al., 2017))
Suppose ˜K + λI is (∆1, ∆2)-spectral approximation of
0. Let m denote
[0, 1) and ∆2 ≥
K + λI, for ∆1 ∈
the rank of ˜K, and let fK and f ˜K be the kernel ridge
regression estimators learned using these matrices, with
regularizing constant λ
0 and label noise variance
σ2 <
. Then

≥

(f ˜K)

R

≤

1

1
∆1

(cid:98)
R

−

(fK) +

∆2
1 + ∆2

m
n

σ2.

(1)

We include a proof in Appendix B.1. This result shows
that smaller values for ∆1 and ∆2 imply tighter bounds
on the generalization performance of the model trained
with ˜K. We can see that as ∆1 approaches 1 the bound
diverges, and as ∆2 approaches
the bound plateaus.
We leverage this generalization bound to understand
the diﬀerence in performance between Nystr¨om and
RFFs (Sec. 3.2), and to motivate and analyze our pro-
posed low-precision random Fourier features (Sec. 4).

∞

Remark The generalization bound in Prop. 1 as-
sumes the regressor fK is computed via the closed-
form solution for kernel ridge regression. However, in
Sections 4-5 we focus on stochastic gradient descent
(SGD) training for kernel approximation models. Be-
cause SGD can also ﬁnd the model which minimizes
the regularized empirical loss (Nemirovski et al., 2009),
the generalization results carry over to our setting.

3.2 Revisiting Nystr¨om vs. RFF Comparison

In this section we show that the values of ∆1 and ∆2
such that the approximate kernel matrix is a (∆1, ∆2)-
spectral approximation of the exact kernel matrix cor-
relate better with generalization performance than the
original ∆, and the Frobenius and spectral norms of
the kernel approximation error; we measure correlation
using Spearman’s rank correlation coeﬃcient ρ.

To study the correlation of these metrics with gen-
eralization performance, we train Nystr¨om and RFF
models for many feature dimensions on the Census
regression task, and on a subsampled version of 20k
train and heldout points from the CovType classiﬁca-
tion task. We choose these small datasets to be able
to compute the various measures of kernel approxima-
tion error over the entire heldout set. We measure the
˜K, and the ∆
spectral and Frobenius norms of K
and (∆1, ∆2) values between K + λI and ˜K + λI (λ
chosen via cross-validation), where K and ˜K are the
exact and approximate kernel matrices for the heldout
set. For more details about these experiments and how
we compute ∆ and (∆1, ∆2), see Appendix D.3.

−

In Figure 2, we plot the generalization performance on
these tasks as a function of these metrics; while the

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Figure 2: The correlation between generalization performance and diﬀerent measures of kernel approximation
error for the full-precision RFF and Nystr¨om methods. We see that generalization performance aligns well with
1/(1
∆1) (Spearman rank correlation coeﬃcient ρ = 0.958), while aligning poorly with ∆ and the spectral and
squared Frobenius norms of the kernel approximation error matrix. See Appendix D.3 for results on CovType.

−

1

1

−

−

1
∆1

1
∆1

original ∆ and the Frobenius and spectral norms gener-
ally do not align well with generalization performance,
we see that
attains a
does. Speciﬁcally,
Spearman rank correlation coeﬃcient of ρ = 0.958,
while squared Frobenius norm, spectral norm, and the
original ∆ attain values of 0.594, 0.540, and 0.759.6
In Appendix D.3 we show these trends are robust to
diﬀerent kernel approximation methods and datasets.
For example, we show that while other approximation
methods (e.g., orthogonal RFFs (Yu et al., 2016)), like
Nystr¨om, can attain much lower Frobenius and spectral
error than standard RFFs, this does not translate to
improved ∆1 or heldout performance. These results
mirror the generalization bound in Proposition 1, which
grows linearly with
. For simplicity, we ignore
the role of ∆2 here, as ∆1 appears to be suﬃcient for
explaining the main diﬀerences in performance between
these full-precision methods.7 In Sections 4.2 and 5.2,
however, we show that ∆2 has a large inﬂuence on
generalization performance for low-precision features.

1
∆1

−

1

Now that we have seen that ∆1 has signiﬁcant theoreti-
cal and empirical impact on generalization performance,
it is natural to ask how to construct kernel approxi-
mation matrices that attain small ∆1. An important
consequence of the deﬁnition of ∆1 is that for ˜K +λI to
have small ∆1 relative to K + λI, ˜K must be high-rank ;
λm+1(K)
in particular, a necessary condition is ∆1 ≥
λm+1(K)+λ ,
where m is the rank of ˜K and λi(K) is the ith largest
eigenvalue of K.8 This sets a lower bound on the rank
necessary for ˜K to attain small ∆1 which holds regard-
less of the approximation method used, motivating us
to design high-rank kernel approximation methods.

6 One reason ∆1 correlates better than ∆ is because
when ∆2 > ∆1, ∆ = max(∆1, ∆2) hides the value of ∆1.
This shows why decoupling the two roles of ∆ is important.
∆1) aligns well with performance, it is
not perfect—for a ﬁxed ∆1, Nystr¨om generally performs
slightly better than RFFs. In App. D.3.1 we suggest this is
because Nystr¨om has ∆2 = 0 while RFFs has larger ∆2.

7While 1/(1

−

8By deﬁnition, (K + λI)(1

˜K + λI. By Weyl’s
λi( ˜K) + λ.
inequality this implies
If ˜K is rank m, then λm+1( ˜K) = 0, and the result follows.

−
(cid:22)
i (λi(K) + λ)(1

∆1)

∆1)

−

≤

∀

4 LOW-PRECISION RANDOM

FOURIER FEATURES (LP-RFFS)

Taking inspiration from the above-mentioned connec-
tion between the rank of the kernel approximation
matrix and generalization performance, we propose
low-precision random Fourier features (LP-RFFs) to
create a high-rank approximation matrix under a mem-
ory budget. In particular, we quantize each random
Fourier feature to a low-precision ﬁxed-point represen-
tation, thus allowing us to store more features in the
same amount of space. Theoretically, we show that
when the quantization noise is small relative to the
regularization parameter, using low precision has mini-
mal impact on the number of features required for the
approximate kernel matrix to be a (∆1, ∆2)-spectral
approximation of the exact kernel matrix; by Propo-
sition 1, this implies a bound on the generalization
performance of the model trained on the low-precision
features. At the end of this section (Section 4.3), we
discuss a memory-eﬃcient implementation for training
a full-precision model on top of LP-RFFs.

4.1 Method Details

∈

−

(cid:112)

(cid:112)

(cid:112)

2/m,

i x + ai)

2/m cos(wT

1 sub-intervals of equal size r =

2/m] for the RFF vector z(x)

The core idea behind LP-RFFs is to use b bits to store
each RFF, instead of 32 or 64 bits. We implement
this with a simple stochastic rounding scheme. We use
the parametrization zi(x) =
∈
Rm
[
−
(Rahimi and Recht, 2007), and divide this interval
into 2b
1 . We
−
then randomly round each feature zi(x) to either the
top or bottom of the sub-interval [z, z] containing it,
in such a way that the expected value is equal to zi(x);
speciﬁcally, we round zi(x) to z with probability z
z
−
z
z
−
and to z with probability z
z
z . The variance of this
z
stochastic rounding scheme is at most δ2
b /m, where
b := 2/(2b
δ2
1)2 (Prop. 7 in App. C.2). For each low-
precision feature ˜zi(x) we only need to store the integer
j
2/m + jr, which
−
takes b bits. Letting ˜Z
m denote the matrix

1] such that ˜zi(x) =

2√2/m
2b

[0, 2b

Rn

(cid:112)

−
−

−

−

∈

×

∈

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 3: Empirical validation of Theorem 2. In the left and middle plots (shared legend), we see that as the #
of features grows, LP-RFFs approach ∆1 = 0, but plateau at larger ∆2 values (at most δ2
b /λ, marked by dashed
lines) for very low precisions. In the right plot we see that the larger λ is, the lower the precision at which using
low precision does not impact ∆2. For ∆1 and ∆2 vs. # features plots on CovType, see Appendix D.4.

of quantized features, we call ˜K = ˜Z ˜Z T an m-feature
b-bit LP-RFF approximation of a kernel matrix K.

As a way to further reduce the memory footprint during
training, we leverage existing work on using circulant
random matrices (Yu et al., 2015) for the RFF random
projection matrix to only occupy 32m bits.9 All our
LP-RFF experiments use circulant projections.

4.2 Theoretical Results

In this section we show quantization has minimal im-
pact on the number of features required to guarantee
strong generalization performance in certain settings.
We do this in the following theorem by lower bounding
the probability that ˜K + λI is a (∆1, ∆2)-spectral ap-
proximation of K + λI, for the LP-RFF approximation
˜K using m features and b bits per feature.10
˜K be an m-feature b-bit LP-
Theorem 2. Let
RFF approximation of a kernel matrix K, assume
1)2, and deﬁne a :=
b In)(cid:1). Then for any ∆1 ≥
0,

:= 2/(2b

≥

−

δ2
K
λ
b
(cid:107)
(cid:107) ≥
8 tr (cid:0)(K + λIn)−
1(K + δ2
δ2
b /λ,
∆2 ≥
(cid:104)
P
(1

∆1)(K + λI)

(cid:105)
(1 + ∆2)(K + λI)

˜K + λI

−
(cid:32)
exp

(cid:18)

1

a

−

(cid:22)

(cid:19)

(cid:22)
(cid:32)

+exp

m∆2
1
−
4n
λ (1 + 2
3 ∆1)

m(∆2 −

−
4n
λ (1 + 2

3 (∆2 −

δ2
λ )2
b
δ2
λ ))
b

≥
(cid:33)(cid:33)
.

The proof of Theorem 2 is in Appendix C. To provide
more intuition we present the following corollary:
Corollary 2.1. Assuming ∆1 ≤
(1
∆1)(K+λIn)
(cid:22)
−
(cid:16) a
8n/λ
if m
log
∆2
ρ
1
it follows that ˜K + λIn
probability at least 1

3/2, it follows that
˜K+λIn with probability at least 1
ρ
−
(cid:17)
(cid:2) δ2
(cid:3),
λ , 3
. Similarly, assuming ∆2 ∈
2
(1 + ∆2)(K + λIn) with
8n/λ
b /λ)2 log
δ2

(cid:22)
ρ if m

(cid:16) a
ρ

(∆2

≥

(cid:17)

.

b

−

≥

−

9Technically, m additional bits are needed to store a

vector of Rademacher random variables in

10This theorem extends directly to the quantization of
Rn×m with
2/m].

any kernel approximation feature matrix Z
i.i.d. columns and with entries in [

∈
2/m, (cid:112)

(cid:112)

1, 1

m.
}

{−

−

The above corollary suggests that using low precision
has negligible eﬀect on the number of features necessary
to attain a certain value of ∆1, and also has negligible
eﬀect for ∆2 as long as δ2
b /λ

∆2.

(cid:28)

Validation of Theory We now empirically validate
the following two predictions made by the above theory:
(1) Using low precision has no eﬀect on the asymptotic
behavior of ∆1 as the number of features m approaches
inﬁnity, while having a signiﬁcant eﬀect on ∆2 when
δ2
, ∆1 converges
b /λ is large. Speciﬁcally, as m
to 0 for any precision b, while ∆2 converges to a value
upper bounded by δ2
b /λ.11 (2) If δ2
∆2, using b-
bit precision will have negligible eﬀect on the number of
features required to attain this ∆2. Thus, the larger λ
is, the smaller the impact of using low precision should
be on ∆2.

→ ∞

b /λ

(cid:28)

To validate the ﬁrst prediction, in Figure 3 (left, middle)
we plot ∆1 and ∆2 as a function of the number of
features m, for FP-RFFs and LP-RFFs; we use the
same λ as in the Section 2 Census experiments. We
show that for large m, all methods approach ∆1 = 0;
4 the LP-RFFs converge
in contrast, for precisions b
to a ∆2 value much larger than 0, and slightly less than
δ2
b /λ (marked by dashed lines).

≤

To validate the second prediction, in Figure 3 (right)
we plot ∆2 vs. precision for various values of λ, using
m = 2000 features for all precisions; we do this on
a random subsample of 8000 Census training points.
We see that for large enough precision b, the ∆2 is
very similar to the value from using 32-bit precision.
Furthermore, the larger the value of λ, the smaller the
precision b can be without signiﬁcantly aﬀecting ∆2.

11By Lemma 3 in Appendix C, we know that E

K + D for a diagonal matrix D satisfying 0
where D is independent of m. As m
(K + λI)−1/2D(K + λI)−1/2
to

→ ∞
δ2
b /λ.

(cid:107) ≤

(cid:107)

(cid:104)

˜Z ˜Z T (cid:105)
=
δ2
b In,
D
(cid:22)
, ∆2 converges

(cid:22)

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Figure 4: Generalization performance of FP-Nystr¨om, FP-RFFs, circulant FP-RFFs, and LP-RFFs with respect
to memory (sum of components in Table 1) on TIMIT, YearPred and CovType. LP-RFFs attain the best
performance across a wide range of memory budgets. The same trend holds for Census in Appendix D.5.

Table 2: The compression ratios achieved by LP-RFFs
relative to the best performing full-precision baselines.

FP-RFFs Cir. FP-RFFs Nystr¨om

Census
YearPred
Covtype
TIMIT

2.9x
10.3x
4.7x
5.1x

15.6x
7.6x
3.9x
2.4x

63.2x
461.6x
237.2x
50.9x

4.3

Implementation Considerations

In this paper, we focus on training full-precision models
using mini-batch training over low-precision features.
Here we describe how this mixed-precision optimization
can be implemented in a memory-eﬃcient manner.

Naively, to multiply the low-precision features with the
full-precision model, one could ﬁrst cast the features to
full-precision, requiring signiﬁcant intermediate mem-
ory. We can avoid this by casting in the processor
registers. Speciﬁcally, to perform multiplication with
the full-precision model, the features can be streamed
to the processor registers in low precision, and then cast
to full precision in the registers. In this way, only the
features in the registers exist in full precision. A similar
technique can be applied to avoid intermediate memory
in the low-precision feature computation—after a full-
precision feature is computed in the registers, it can be
directly quantized in-place before it is written back to
main memory. We leave a more thorough investigation
of these systems issues for future work.

5 EXPERIMENTS

In this section, we empirically demonstrate the per-
formance of LP-RFFs under a memory budget, and
show that (∆1, ∆2) are predictive of generalization
performance. We show in Section 5.1 that LP-RFFs
can attain the same performance as FP-RFFs and
Nystr¨om, while using 3x-10x and 50x-460x less memory.
In Section 5.2, we show the strong alignment between
(∆1, ∆2) and generalization performance, once again
validating the importance of this measure.

5.1 Empirical Evaluation of LP-RFFs

1, 2, 4, 8, 16

To empirically demonstrate the generalization perfor-
mance of LP-RFFs, we compare their performance
to FP-RFFs, circulant FP-RFFs, and Nystr¨om fea-
tures for various memory budgets. We use the same
datasets and protocol as the large-scale Nystr¨om vs.
RFF comparisons in Section 2.2; the only signiﬁcant ad-
ditions here are that we also evaluate the performance
of circulant FP-RFFs, and LP-RFFs for precisions
b
. Across our experiments, we com-
}
pute the total memory utilization as the sum of all
the components in Table 1. We note that all our low-
precision experiments are done in simulation, which
means we store the quantized values as full-precision
ﬂoating-point numbers. We report average results from
three random seeds, with error bars showing standard
deviations. For more details about our experiments, see
Appendix D.5. We use the above protocol to validate
the following claims on the performance of LP-RFFs.12

∈ {

LP-RFFs can outperform full-precision fea-
tures under memory budgets.
In Figure 4, we
plot the generalization performance for these experi-
ments as a function of the total training memory for
TIMIT, YearPred, and CovType. We observe that LP-
RFFs attain better generalization performance than
the full-precision baselines under various memory bud-
gets. To see results for all precisions, as well as results
on additional benchmark datasets (Census, Adult, Cod-
RNA, CPU, Forest) from the UCI repository (Dheeru
and Karra Taniskidou, 2017), see Appendix D.5.

LP-RFFs can match the performance of full-
precision features with signiﬁcantly less mem-
ory.
In Table 2 we present the compression ratios we
achieve with LP-RFFs relative to the best performing
baseline methods. For each baseline (FP-RFFs, circu-
lant FP-RFFs, Nystr¨om), we ﬁnd the smallest LP-RFF
model, as well as the smallest baseline model, which
4 relative performance of the best-
attain within 10−
performing baseline model; we then compute the ratio

12 Our code: github.com/HazyResearch/lp_rffs.

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

∆1), ∆2)
Figure 5: Generalization perf. vs. ∆2 (left plots, shared legend), and vs. 1/(1
(right plots, shared legend). Left: heldout performance deteriorates as ∆2 gets larger due to lower precision.
Right: max (1/(1
∆1), ∆2) aligns well with performance across LP-RFF precisions (Spearman rank correlation
coeﬃcient ρ = 0.959), while 1/(1

∆1) aligns poorly (ρ = 0.403). See Appendix D.6 for CovType results.

∆1) and max (1/(1

−

−

−

−

of the memory used by these two models (baseline/LP-
RFF) for three random seeds, and report the aver-
age. We can see that LP-RFFs demonstrate signiﬁcant
memory saving over FP-RFFs, circulant FP-RFFs, and
Nystr¨om, attaining compression ratios of 2.9x-10.3x,
2.4x-15.6x, and 50.9x-461.6x, respectively.

5.2 Generalization Performance vs. (∆1, ∆2)

In this section we show that ∆1 and ∆2 are together
quite predictive of generalization performance across all
the kernel approximation methods we have discussed.
We ﬁrst show that performance deteriorates for larger
∆2 values as we vary the precision of the LP-RFFs,
when keeping the number of features constant (thereby
limiting the inﬂuence of ∆1 on performance). We then
combine this insight with our previous observation
(Section 3.2) that performance scales with
in the
full-precision setting by showing that across precisions
(cid:1). For
the performance aligns well with max (cid:0)
these experiments, we use the same protocol as for the
(∆1, ∆2) experiments in Section 3.2, but additionally
consider LP-RFFs for precisions b

, ∆2

1
∆1

1
∆1

−

−

1

1

.

1, 2, 4, 8, 16
}

∈ {

We show in Figure 5 (left plots) that for a ﬁxed number
of random Fourier features, performance deteriorates
as ∆2 grows. As we have shown in Figure 3 (left), ∆1
is primarily governed by the rank of the approxima-
tion matrix, and thus holding the number of features
constant serves as a proxy for holding ∆1 roughly con-
stant. This allows us to isolate the impact of ∆2 on
performance as we vary the precision.

1

1
∆1

, ∆2

To integrate the inﬂuence of ∆1 and ∆2 on general-
ization performance into a single scalar, we consider
max (cid:0)
(cid:1). In Figure 5 (right plots) we show that
when considering both low-precision and full-precision
features, max (cid:0)
(cid:1) aligns well with performance
(ρ = 0.959, incorporating all precisions), while
aligns poorly (ρ = 0.403).

, ∆2

1
∆1

1
∆1

−

−

−

1

1

In Appendix B we argue that performance scales
roughly as ∆2 instead of as ∆2/(1 + ∆2) (as suggested
by Prop. 1) due to looseness in the Prop. 1 bound.

6 RELATED WORK

Low-Memory Kernel Approximation For RFFs,
there has been work on using structured random pro-
jections (Le et al., 2013; Yu et al., 2015, 2016), and
feature selection (Yen et al., 2014; May et al., 2016) to
reduce memory utilization. Our work is orthogonal, as
LP-RFFs can be used with both. For Nystr¨om, there
has been extensive work on improving the choice of
landmark points, and reducing the memory footprint
in other ways (Kumar et al., 2009; Hsieh et al., 2014;
Si et al., 2014; Musco and Musco, 2017). In our work,
we focus on the eﬀect of quantization on generalization
performance per bit, and note that RFFs are much
more amenable to quantization. For our initial experi-
ments quantizing Nystr¨om features, see Appendix D.7.

Low Precision for Machine Learning There has
been much recent interest in using low precision for
accelerating training and inference of machine learning
models, as well as for model compression (Gupta et al.,
2015; De Sa et al., 2015; Hubara et al., 2016; De Sa
et al., 2018, 2017; Han et al., 2016). There have been
many advances in hardware support for low precision
as well (Jouppi et al., 2017; Caulﬁeld et al., 2017).

This work is inspired by the Nystr¨om vs. RFF exper-
iments in the PhD dissertation of May (2018), and
provides a principled understanding of the prior results.
For more related work discussion, see Appendix E.

7 CONCLUSION

We deﬁned a new measure of kernel approximation error
and demonstrated its close connection to the empirical
and theoretical generalization performance of kernel
approximation methods.
Inspired by this measure,
we proposed LP-RFFs and showed they can attain
improved generalization performance under a memory
budget in theory and in experiments. We believe these
contributions provide fundamental insights into the
generalization performance of kernel approximation
methods, and hope to use these insights to scale kernel
methods to larger and more challenging tasks.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Acknowledgements

We thank Michael Collins for his helpful guidance on
the Nystr¨om vs. RFF experiments in Avner May’s PhD
dissertation (May, 2018), which inspired this work. We
also thank Jared Dunnmon, Albert Gu, Beliz Gunel,
Charles Kuang, Megan Leszczynski, Alex Ratner, Nimit
Sohoni, Paroma Varma, and Sen Wu for their helpful
discussions and feedback on this project.

We gratefully acknowledge the support of DARPA un-
der Nos. FA87501720095 (D3M) and FA86501827865
(SDH), NIH under No. N000141712266 (Mobilize),
NSF under Nos. CCF1763315 (Beyond Sparsity) and
CCF1563078 (Volume to Velocity), ONR under No.
N000141712266 (Unifying Weak Supervision), the
Moore Foundation, NXP, Xilinx, LETI-CEA, Intel,
Google, NEC, Toshiba, TSMC, ARM, Hitachi, BASF,
Accenture, Ericsson, Qualcomm, Analog Devices, the
Okawa Foundation, and American Family Insurance,
and members of the Stanford DAWN project: Intel,
Microsoft, Teradata, Facebook, Google, Ant Finan-
cial, NEC, SAP, and VMWare. The U.S. Government
is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright
notation thereon. Any opinions, ﬁndings, and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reﬂect
the views, policies, or endorsements, either expressed
or implied, of DARPA, NIH, ONR, or the U.S. Gov-
ernment.

References

Ahmed El Alaoui and Michael W. Mahoney. Fast
randomized kernel ridge regression with statistical
guarantees. In NIPS, pages 775–783, 2015.

Haim Avron, Michael Kapralov, Cameron Musco,
Christopher Musco, Ameya Velingker, and Amir
Zandieh. Random Fourier features for kernel ridge
regression: Approximation bounds and statistical
guarantees. In ICML, volume 70 of Proceedings of
Machine Learning Research, pages 253–262. PMLR,
2017.

Adrian M. Caulﬁeld, Eric S. Chung, Andrew Put-
nam, Hari Angepat, Daniel Firestone, Jeremy Fowers,
Michael Haselman, Stephen Heil, Matt Humphrey,
Puneet Kaur, Joo-Young Kim, Daniel Lo, Todd Mas-
sengill, Kalin Ovtcharov, Michael Papamichael, Lisa
Woods, Sitaram Lanka, Derek Chiou, and Doug
Burger. Conﬁgurable clouds. IEEE Micro, 37(3):
52–61, 2017.

Corinna Cortes, Mehryar Mohri, and Ameet Talwalkar.
On the impact of kernel approximation on learning
accuracy. In AISTATS, volume 9 of JMLR Proceed-
ings, pages 113–120. JMLR.org, 2010.

Tri Dao, Christopher De Sa, and Christopher R´e. Gaus-
sian quadrature for kernel features. In NIPS, pages
6109–6119, 2017.

Christopher De Sa, Ce Zhang, Kunle Olukotun, and
Christopher R´e. Taming the wild: A uniﬁed analysis
of Hogwild-style algorithms. In NIPS, pages 2674–
2682, 2015.

Christopher De Sa, Matthew Feldman, Christopher R´e,
and Kunle Olukotun. Understanding and optimiz-
ing asynchronous low-precision stochastic gradient
descent. In ISCA, pages 561–574. ACM, 2017.

Christopher De Sa, Megan Leszczynski, Jian Zhang,
Alana Marzoev, Christopher R. Aberger, Kunle
Olukotun, and Christopher R´e. High-accuracy low-
precision training. arXiv preprint arXiv:1803.03383,
2018.

Dua Dheeru and Eﬁ Karra Taniskidou. UCI machine

learning repository, 2017.

M. J. F. Gales. Maximum likelihood linear transforma-
tions for HMM-based speech recognition. Computer
Speech & Language, 12(2):75–98, 1998.

J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus,
D. S. Pallett, and N. L. Dahlgren. DARPA TIMIT
acoustic phonetic continuous speech corpus CDROM,
1993. URL http://www.ldc.upenn.edu/Catalog/
LDC93S1.html.

Alex Gittens and Michael W. Mahoney. Revisiting the
Nystr¨om method for improved large-scale machine
learning. Journal of Machine Learning Research, 17:
117:1–117:65, 2016.

Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan,
and Pritish Narayanan. Deep learning with limited
numerical precision. In ICML, volume 37 of JMLR
Workshop and Conference Proceedings, pages 1737–
1746, 2015.

Song Han, Huizi Mao, and William J. Dally. Deep
compression: Compressing deep neural network with
pruning, trained quantization and Huﬀman coding.
In Proceedings of the International Conference on
Learning Representations (ICLR), 2016.

Cho-Jui Hsieh, Si Si, and Inderjit S. Dhillon. Fast
prediction for large-scale kernel machines. In NIPS,
pages 3689–3697, 2014.

Jie Chen, Lingfei Wu, Kartik Audhkhasi, Brian Kings-
bury, and Bhuvana Ramabhadrari. Eﬃcient one-vs-
one kernel ridge regression for speech recognition. In
ICASSP, pages 2454–2458. IEEE, 2016.

Po-Sen Huang, Haim Avron, Tara N. Sainath, Vikas
Sindhwani, and Bhuvana Ramabhadran. Kernel
methods match deep neural networks on TIMIT.
In ICASSP, pages 205–209. IEEE, 2014.

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Itay Hubara, Matthieu Courbariaux, Daniel Soudry,
Ran El-Yaniv, and Yoshua Bengio. Binarized neural
networks. In NIPS, pages 4107–4115, 2016.

Alessandro Rudi and Lorenzo Rosasco. Generalization
properties of learning with random features. In NIPS,
pages 3218–3228, 2017.

Norman P. Jouppi, Cliﬀ Young, Nishant Patil, David A.
Patterson, et al. In-datacenter performance analysis
of a tensor processing unit. In ISCA, pages 1–12.
ACM, 2017.

Sanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar.
Ensemble Nystr¨om method. In NIPS, pages 1060–
1068, 2009.

Sanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar.
Sampling methods for the Nystr¨om method. Journal
of Machine Learning Research, 13:981–1006, 2012.

Quoc V. Le, Tam´as Sarl´os, and Alexander J. Smola.
Fastfood - computing Hilbert space expansions in log-
linear time. In ICML, volume 28 of JMLR Workshop
and Conference Proceedings, pages 244–252, 2013.

Zhu Li, Jean-Francois Ton, Dino Oglic, and Dino Sejdi-
novic. A uniﬁed analysis of random Fourier features.
arXiv preprint arXiv:1806.09178, 2018.

Avner May. Kernel Approximation Methods for Speech
Recognition. PhD thesis, Columbia University, 2018.

Avner May, Michael Collins, Daniel J. Hsu, and Brian
Kingsbury. Compact kernel models for acoustic mod-
eling via random feature selection. In ICASSP, pages
2424–2428. IEEE, 2016.

Avner May, Alireza Bagheri Garakani, Zhiyun Lu,
Dong Guo, Kuan Liu, Aur´elien Bellet, Linxi Fan,
Michael Collins, Daniel J. Hsu, Brian Kingsbury,
Michael Picheny, and Fei Sha. Kernel approxima-
tion methods for speech recognition. arXiv preprint
arXiv:1701.03577, 2017.

N. Morgan and H. Bourlard. Generalization and pa-
rameter estimation in feedforward nets: Some exper-
iments. In NIPS, 1990.

Tara N. Sainath, Brian Kingsbury, Vikas Sindhwani,
Ebru Arisoy, and Bhuvana Ramabhadran. Low-rank
matrix factorization for deep neural network training
with high-dimensional output targets. In ICASSP,
pages 6655–6659. IEEE, 2013a.

Tara N. Sainath, Brian Kingsbury, Hagen Soltau, and
Bhuvana Ramabhadran. Optimization techniques to
improve training speed of deep neural networks for
large speech tasks. IEEE Trans. Audio, Speech &
Language Processing, 21(11):2267–2276, 2013b.

Si Si, Cho-Jui Hsieh, and Inderjit S. Dhillon. Mem-
ory eﬃcient kernel approximation. In ICML, vol-
ume 32 of JMLR Workshop and Conference Proceed-
ings, pages 701–709, 2014.

Vikas Sindhwani, Tara N. Sainath, and Sanjiv Kumar.
Structured transforms for small-footprint deep learn-
ing. In NIPS, pages 3088–3096, 2015.

Dougal J. Sutherland and Jeﬀ G. Schneider. On the
In UAI, pages

error of random Fourier features.
862–871. AUAI Press, 2015.

Joel A. Tropp. An introduction to matrix concentration
inequalities. Foundations and Trends in Machine
Learning, 8(1-2):1–230, 2015.

Stephen Tu, Rebecca Roelofs, Shivaram Venkatara-
man, and Benjamin Recht. Large scale kernel learn-
ing using block coordinate descent. arXiv preprint
arXiv:1602.05310, 2016.

Yuting Wei, Fanny Yang, and Martin J. Wainwright.
Early stopping for kernel boosting algorithms: A
general analysis with localized complexities. In NIPS,
pages 6067–6077, 2017.

Cameron Musco and Christopher Musco. Recursive
sampling for the Nystr¨om method. In NIPS, pages
3836–3848, 2017.

Christopher K. I. Williams and Matthias W. Seeger.
Using the Nystr¨om method to speed up kernel ma-
chines. In NIPS, pages 682–688. MIT Press, 2000.

Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan,
and Alexander Shapiro. Robust stochastic approxi-
mation approach to stochastic programming. SIAM
Journal on Optimization, 19(4):1574–1609, 2009.

Tiberiu Popoviciu. Sur les ´equations alg´ebriques ayant
toutes leurs racines r´eelles. Mathematica, 9:129–145,
1935.

Ali Rahimi and Benjamin Recht. Random features for
large-scale kernel machines. In NIPS, pages 1177–
1184, 2007.

Jiyan Yang, Vikas Sindhwani, Haim Avron, and
Michael W. Mahoney. Quasi-Monte Carlo feature
maps for shift-invariant kernels. In Proceedings of the
31th International Conference on Machine Learning,
ICML 2014, Beijing, China, 21-26 June 2014, pages
485–493, 2014.

Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong
Jin, and Zhi-Hua Zhou. Nystr¨om method vs ran-
dom Fourier features: A theoretical and empirical
comparison. In NIPS, pages 485–493, 2012.

Ali Rahimi and Benjamin Recht. Weighted sums of
random kitchen sinks: Replacing minimization with
randomization in learning. In NIPS, pages 1313–1320,
2008.

Zichao Yang, Marcin Moczulski, Misha Denil, Nando
de Freitas, Alexander J. Smola, Le Song, and Ziyu
Wang. Deep fried convnets. In ICCV, pages 1476–
1483. IEEE Computer Society, 2015.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Ian En-Hsu Yen, Ting-Wei Lin, Shou-De Lin, Pradeep
Ravikumar, and Inderjit S. Dhillon. Sparse random
feature algorithm as coordinate descent in Hilbert
space. In NIPS, pages 2456–2464, 2014.

Felix X. Yu, Sanjiv Kumar, Henry A. Rowley, and Shih-
Fu Chang. Compact nonlinear maps and circulant
extensions. arXiv preprint arXiv:1503.03893, 2015.

Felix

X.

Yu,

Ananda

Theertha

Suresh,
Krzysztof Marcin Choromanski, Daniel N.
Holtmann-Rice, and Sanjiv Kumar. Orthogo-
In NIPS, pages 1975–1983,
nal random features.
2016.

Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh,
Ji Liu, and Ce Zhang. Zipml: Training linear models
with end-to-end low precision, and a little bit of deep
learning. In ICML, volume 70 of Proceedings of Ma-
chine Learning Research, pages 4035–4043. PMLR,
2017.

Tong Zhang, Bin Yu, et al. Boosting with early stop-
ping: Convergence and consistency. The Annals of
Statistics, 33(4):1538–1579, 2005.

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

A NOTATION AND BACKGROUND

In this appendix, we ﬁrst discuss the notation we use throughout the paper, and then provide an overview of
random Fourier features (RFFs) (Rahimi and Recht, 2007) and the Nystr¨om method (Williams and Seeger, 2000).
After this, we brieﬂy extend our discussion in Section 3 on ﬁxed design kernel ridge regression.

A.1 Notation

×

∈

×

Rn

}
→

Rd, and yi

n
i=1 to denote a training set, for xi
}

= R for regression, and
We use
(xi, yi)
∈
{
n denote the kernel matrix corresponding to a kernel function
for classiﬁcation. We let K
1, . . . , c
=
Y
{
R, where Kij = k(xi, xj), and let ˜K denote an approximation to K. We let z : Rd
Rm denote
Rd
k : Rd
a feature map for approximating a kernel function, such that ˜Kij = z(xi)T z(xj). We use s to denote the size
of the mini-batches during training, and b to denote the precision used for the random features. We let
(cid:107)2
F denote the spectral and Frobenius norms of a matrix K, respectively; if the subscript is not speciﬁed,
and
(cid:107)
will denote the (cid:96)2 norm of x, unless speciﬁed otherwise. In
K
(cid:107)
A is positive
will denote the n
semideﬁnite. We will use λi(A) to denote the ith largest eigenvalue of A, and λmax(A), λmin(A) to denote the
largest and smallest eigenvalues of A, respectively.

K
(cid:107)
denotes the spectral norm. For vectors x,

n identity matrix. For symmetric matrices A and B, we will say A

, where

B if B

K
(cid:107)

∈ Y

→

(cid:22)

−

×

Y

x

(cid:107)

(cid:107)

(cid:107)

A.2 Kernel Approximation Background

k(x, y).
The core idea behind kernel approximation is to construct a feature map z :
X →
n
Given such a map, one can then learn a linear model on top of
i=1, and this model will approximate
(z(xi), yi)
}
{
the model trained using the exact kernel function. We now review RFFs and the Nystr¨om method, two of the
most widely used and studied methods for kernel approximation.

≈

R such that z(x)T z(y)

Random Fourier features (RFFs) For shift-invariant kernels (k(x, x(cid:48)) = ˆk(x
x(cid:48))), the random Fourier
Rm such that
feature method (Rahimi and Recht, 2007) constructs a random feature representation z(x)
E (cid:2)z(x)T z(x(cid:48))(cid:3) = k(x, x(cid:48)). This construction is based on Bochner’s Theorem, which states that any positive
deﬁnite kernel is equal to the Fourier transform of a nonnegative measure. This allows for performing Monte
Carlo approximations of this Fourier transform in order to approximate the function. The resulting features
have the following functional form: zi(x) =
i x + ai), where wi is drawn from the inverse Fourier
transform of the kernel function ˆk, and ai is drawn uniformly from [0, 2π] (see Appendix A in May et al. (2017)
for a derivation).

2/m cos(wT

(cid:112)

−

∈

One way of reducing the memory required for storing W = [w1, . . . , wm], is to replace W by a structured matrix;
in this work, we let W be a concatenation of many square circulant random matrices (Yu et al., 2015).

(cid:104)

(cid:105) ≈

k(x, x(cid:48)).

z(x), z(x(cid:48))

It does this by picking a set of landmark points

Rm
Nystr¨om method The Nystr¨om method constructs a ﬁnite-dimensional feature representation z(x)
∈
ˆx1, . . . , ˆxm
such that
, and
{
taking the SVD ˆK = U ΛU T of the m by m kernel matrix ˆK corresponding to these landmark points ( ˆKi,j =
1/2U T kx, where kx =
k(ˆxi, ˆxj)). The Nystr¨om representation for a point x
Rm
n, the Nystr¨om method can be thought of
[k(x, ˆx1), . . . , k(x, ˆxm)]T . Letting Km,n = [kx1 , . . . , kxn ]
1/2U T Km,n of the full n by n kernel matrix K
as an eﬃcient low-rank approximation K
m,nU Λ−
n
corresponding to the full dataset
i=1. One can also consider the lower-dimensional Nystr¨om representation
}
Rr, where only the top r eigenvalues and eigenvectors of ˆK are used, instead of all m. In
U T
zr(x) = Λ−
r kx
r
this paper, we will always use m = r, and thus will not specify the subscript r.

is deﬁned as z(x) = Λ−

∈ X
∈
1/2Λ−

} ∈ X

K T

1/2

xi

≈

∈

{

×

A.3 Fixed Design Kernel Ridge Regression

We consider the problem of ﬁxed design kernel ridge regression, which has a closed-form equation for the
generalization error, making it a particularly tractable problem to analyze. In ﬁxed design regression, one is
R, and the (cid:15)i are zero-mean uncorrelated
given a set of labeled points
random variables with shared variance σ2 > 0; here, the ¯yi represent the “true labels.” Given such a sample,
¯yi)2(cid:3) is small. Note that for a ﬁxed
the goal is to learn a regressor f (x) such that
learning method, the learned regressor f can be seen as a random function based on the random label noise (cid:15)i.

Rd, yi = ¯yi + (cid:15)i

n
i=1, where xi

i=1(f (xi)

(f ) = E(cid:15)

(xi, yi)

(cid:2) 1
n

(cid:80)n

R

−

∈

∈

}

{

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

One approach to solving this problem is kernel ridge regression. In kernel ridge regression, one chooses a kernel
Rd
function k : Rd
i αik(x, xi).
n denote the kernel matrix such that Kij = k(xi, xj), and y = (y1, . . . , yn), the closed-form
Letting K
1y. It is then easy
solution for this problem (the one minimizing the regularized empirical loss), is α = (K + λI)−
to show (Alaoui and Mahoney, 2015) that the expected error of this regressor fK under the ﬁxed design setting is

R, and a regularizing constant λ, and learns a function of the form f (x) = (cid:80)

×
Rn
×

→

∈

(fK) =

R

1
n

λ2 ¯yT (K + λI)−

2 ¯y +

σ2T r

K 2(K + λI)−

2(cid:17)

,

(cid:16)

1
n

where ¯y = (¯y1, . . . , ¯yn) is the vector of “true labels.”

B GENERALIZATION BOUNDS FOR FIXED DESIGN REGRESSION

B.1 Generalization Bound in Terms of (∆1, ∆2)

Proposition 1. (Extended from (Avron et al., 2017)) Suppose ˜K + λI is (∆1, ∆2)-spectral approximation of
0. Let m denote the rank of ˜K, and let fK and f ˜K be the kernel ridge regression
K + λI, for ∆1 ∈
. Then
estimators learned using these matrices, with regularizing constant λ

0 and label noise variance σ2 <

[0, 1) and ∆2 ≥

(f ˜K)

R

≤

1

1
∆1

(cid:98)
R

(fK) +

∆2
1 + ∆2

m
n

σ2,

≥

∞

(2)

where

(expected risk) and (cid:98)
R

R

(upper bound on

) are deﬁned in Section 3.1.

−

R

∆) and (1 + ∆) with (1

Proof. This proof closely follows the proof of Lemma 2 in Avron et al. (2017), with the primary diﬀerence being
that we replace (1
We begin by replacing K with ˜K in the deﬁnition for (cid:98)
R
1
n

∆1) and (1 + ∆2), respectively.

λ¯yT ( ˜K + λI)−

˜K( ˜K + λI)−

(f ˜K).

(f ˜K)

(fK):

σ2 tr

1 ¯y +

= (cid:98)
R

1
n

1(cid:17)

R

−

−

≤

(cid:16)

( ˜K + λI)−

We now continue this chain of inequalities, using the fact that A
˜K + λI
bounds the ﬁrst term in the above sum.
We now consider the second term. Let m = rank( ˜K), and let sλ( ˜K) = tr

¯yT ( ˜K + λI)−

1(K + λI)−

∆1)−

1 ¯y

⇒

⇒

(1

≤

(cid:22)

(cid:22)

−

1

1

(1

(cid:16)

−

1
B implies B−

˜K( ˜K + λI)−

1(cid:17)

.

1. Thus, (1
∆1)(K +λI)
A−
(cid:22)
−
1 ¯yT (K + λI)−
1 ¯y. This upper

(cid:22)
∆1)−

(cid:16)

˜K( ˜K + λI)−

1(cid:17)

sλ( ˜K) = tr
m
(cid:88)

=

λi( ˜K)
λi( ˜K) + λ

i=1

= m

−

m

−

≤

m
(cid:88)

i=1
m
(cid:88)

i=1

λ
λi( ˜K) + λ

λ
(1 + ∆2)(λi(K) + λ)
m
(cid:88)

= m

1
(1 + (1 + ∆2)−

1)

−

i=1

λ
λi(K) + λ

= m

λ
λi(K) + λ

+

∆2
1 + ∆2

m
(cid:88)

i=1

λ
λi(K) + λ

n

−

≤

λ
λi(K) + λ

+

∆2
1 + ∆2

m

= sλ(K) +

∆2
1 + ∆2

m

−

−

m
(cid:88)

i=1
n
(cid:88)

i=1

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

≤

1

1
∆1

−

sλ(K) +

∆2
1 + ∆2

m

Combining the above results, we get that:

(f ˜K)

R

1 ¯y +

1
n

σ2sλ( ˜K)
(cid:19)

≤

≤

=

=

1
n
1
n

1

1

λ¯yT ( ˜K + λI)−
(cid:18) 1
λ
1

∆1
−
(cid:18) 1
n

1
∆1
−
1
∆1

−

λ¯yT (K + λI)−

1 ¯y +

(fK) +

(cid:98)
R

∆2
1 + ∆2

m
n

σ2

¯yT (K + λI)−

1 ¯y

+

σ2

sλ(K) +

1
n

(cid:18) 1
1

∆1

−
(cid:19)
σ2sλ(K)

+

1
n

∆2
1 + ∆2

m
n

σ2

(cid:19)

∆2
1 + ∆2

m

Remark Above, ∆1 ∈
0. Note that as ∆1 approaches 1, the above upper bound diverges to
n σ2. This suggests
inﬁnity. Whereas as ∆2 approaches
that choosing ˜K and λ such that ∆1 does not get too close to 1 is very important. A necessary condition for
(1

˜K + λI is for ˜K to be high rank, as discussed in Section 3.

, the second term in the upper bound approaches m

[0, 1] and ∆2 ≥
∞

∆1)(K + λI)

−

(cid:22)

B.2 Heuristic Eﬀort to Better Understand Inﬂuence of (∆1, ∆2) on Generalization Error

Here we present a heuristic argument to explain how ∆1 and ∆2 in the spectral approximation aﬀects the
generalization error. We are particularly interested in demonstrating that ∆2 can have an important inﬂuence on
the bias squared term ( λ2
n ¯yT ( ˜K +
1
λI)−
∆1 (cid:98)
R

2 ¯y) of the generalization error, even though the upper bound λ2

(fK) on the bias squared term (from Proposition 1) is only in terms of ∆1.

n ¯yT ( ˜K + λI)−

2 ¯y

≤

1

−

Suppose that the approximate kernel matrix ˜K is a (∆1, ∆2)-spectral approximation of the true kernel matrix K,
that is:

We focus on the bias squared term of ˜K ( λ2
Following Theorem 15 of Musco and Musco (2017), we ﬁrst bound the bias (not squared):

(1

∆1)(K + λIn)

−

˜K + λIn
n ¯yT ( ˜K + λIn)−

(cid:22)

(1 + ∆2)(K + λIn).

(cid:22)
2 ¯y), and compare it to the bias squared term of K.

( ˜K + λIn)−

1 ¯y

(cid:107)

1 ¯y
(cid:107)
1 ¯y
(cid:107)
1 ¯y
(cid:107)
1 ¯y
(cid:107)
1 ¯y
(cid:107)

(cid:107) ≤ (cid:107)
=

=

≤ (cid:107)
=

(K + λIn)−

(K + λIn)−
(cid:107)
(K + λIn)−
(cid:107)
(K + λIn)−

(K + λIn)−
(cid:107)
( ˜K + λIn)−
(cid:107)

1(K

−

˜K

K

−

(cid:22)

∆2(K + λIn)

+

+

+

+
(cid:16)

(( ˜K + λIn)−
1
(cid:107)
( ˜K + λIn)−
(cid:107)
( ˜K + λIn)−
(cid:107)
( ˜K + λIn)−
(cid:107)
1 +

( ˜K + λIn)−
(cid:107)
˜K)
(cid:107)
∆2

. As ˜K + λIn

( ˜K + λIn)

(cid:22)

(cid:22)

1

∆1

−

(K + λIn)−

−

1)¯y
(cid:107)
( ˜K + λIn))(K + λIn)−

1 ¯y
(cid:107)

1((K + λIn)
1(K
1(K

−

−
˜K)(K + λIn)−
˜K)

1 ¯y
(cid:107)
1 ¯y
(K + λIn)−
(cid:107)

−
1(K

(cid:107)(cid:107)
˜K)
(cid:107)

(cid:17)

.

−

(3)

1 + ∆2
∆1
1

−

(cid:22)

( ˜K + λIn).

Now it reduces to bounding

(1 + ∆2)(K + λIn), we have

Similarly, since K + λIn

1
∆1

( ˜K + λIn),

(cid:22)

1

−

Hence

K

˜K

−

(cid:22)

1

∆1

( ˜K + λIn)

∆1

−

1 + ∆2
∆1
1

−

(cid:22)

( ˜K + λIn).

1 + ∆2
∆1
1

−

−

( ˜K + λIn)

K

(cid:22)

−

˜K

(cid:22)

1 + ∆2
∆1
1

−

( ˜K + λIn).

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

˜K)2 from the bound on
˜K. However, under a restricted setting where K and ˜K have the same eigenvectors, we can square the

t2 is not operator monotone, it is not easy to obtain a bound on (K

−

Because t
K
corresponding eigenvalues to obtain

(cid:55)→

−

Thus

Thus

And hence

( ˜K + λIn)−
(cid:107)

1(K

−

˜K)

(cid:107) ≤

1+∆2
∆1
1

−

. Plugging this into the bound (Eq. 3) yields

˜K)2

(K

−

(cid:22)

(1 + ∆2)2
(1

∆1)2 ( ˜K + λIn)2.

−

( ˜K + λIn)−

1(K

˜K)2( ˜K + λIn)−

1

−

(1 + ∆2)2
∆1)2 .
(1

−

(cid:22)

( ˜K + λIn)−
(cid:107)

1 ¯y

(cid:107) ≤

(cid:18)

1 +

(cid:19)

1 + ∆2
∆1
1

−

(K + λIn)−
(cid:107)

1 ¯y
.
(cid:107)

λ2
n

¯yT ( ˜K + λIn)−

2 ¯y =

( ˜K + λIn)−

λ2
n (cid:107)
(cid:18)

1 +

(cid:18)

1 +

1 ¯y
2
(cid:107)
(cid:19)2 λ2
n (cid:107)
(cid:19)2 λ2
n

1 + ∆2
∆1
1

−
1 + ∆2
∆1
1

−

≤

=

(K + λIn)−

2

1 ¯y
(cid:107)

¯yT (K + λIn)−

2 ¯y.

In other words, in this restricted setting the bias squared of ˜K is at most a factor (1 + 1+∆2
)2 larger than the bias
∆1
squared of K. Though this heuristic analysis only holds when K and ˜K have the same eigenvectors, it reveals
the dependency of the generalization performance on ∆1 and ∆2; in particular, it reveals that ∆2 could have an
important inﬂuence on the bias squared term of the generalization error.

−

1

B.3 The Empirical Inﬂuence of ∆2 on the Bias Squared Term

We now empirically validate that ∆2 can have a large impact on the bias squared term, as suggested by the
theoretical discussion in the previous section. The inﬂuence of ∆2 on the bias squared term helps explain our
empirical observations on the inﬂuence of ∆2 on generalization performance from Section 5.2. Though the
generalization bound in Proposition 1 suggests that performance should scale roughly linearly in ∆2/(1 + ∆2),
we empirically found that generalization performance does not asymptote as ∆2 grows (as ∆2/(1 + ∆2) would
suggest it would). In this section, we empirically validate our hypothesis that this is due to looseness in the
generalization bound. Speciﬁcally, the expected mean squared error for ﬁxed design kernel ridge regression is

(f ˜K) =

R

λ2
n

¯yT ( ˜K + λI)−

2 ¯y +

tr

˜K 2( ˜K + λI)−

2(cid:17)

,

(cid:16)

σ2
n

where ˜K is an approximate kernel matrix. We show in experiments that the bias squared term ( λ2
can be strongly inﬂuenced by ∆2, even though the upper bound on it ( λ2
Proposition 1 is only in terms of ∆1.

n ¯yT ( ˜K + λI)−

2 ¯y

≤

n ¯yT ( ˜K + λI)−
1
∆1 (cid:98)
R

2 ¯y)
(fK)) in

1

−

In our experiments, we compute the value of the bias squared term and ∆2 on the Census dataset. To gain
statistically meaningful insights, we collect and average the value of λ2
2 ¯y and ∆2 using 3 independent
runs with diﬀerent random seeds. In Figure 6, we plot the value of λ2
2 ¯y as a function of ∆2 for
3 diﬀerent numbers of features; by controlling the number of features, we can demonstrate the inﬂuence of ∆2
while ∆1 is held roughly ﬁxed. In each curve in Figure 6, the data points are collected from FP-RFFs, circulant
FP-RFFs, as well as LP-RFFs using
bit precision. We can see that for each number of features, the
value of λ2
2 ¯y grows with ∆2. These results demonstrate that the upper bound on the bias term in
Proposition 1 is quite loose, and is not capturing the inﬂuence of ∆2 properly.

n ¯yT ( ˜K + λI)−

n ¯yT ( ˜K + λI)−

n ¯yT ( ˜K + λI)−

1, 2, 4, 8

{

}

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 6: In the ﬁxed design setting, the bias squared term of the generalization performance grows with ∆2.

C THEORETICAL GUARANTEES FOR LP-RFFS

In this section, we ﬁrst lower bound the probability that using LP-RFFs results in a kernel approximation matrix
that is a (∆1, ∆2)-spectral approximation of the exact kernel matrix (Section C.1). We then present bounds on
the Frobenius kernel approximation error for LP-RFFs (Section C.2).

C.1

(∆1, ∆2)-Spectral Approximation Bounds for LP-RFFs

Rn

×

∈

n denote the kernel matrix, and Z

As usual, let K
is the number of data points and m is the number of features. We can write Z = 1
√m
(cid:112)
the (scaled) columns of Z. Each entry of Z has the form
where d is the dimension of the original dataset. Then E[zizT

m be the random Fourier feature matrix, where n
(cid:3) where zi are
Rm,
Rd and a

2/m cos(wT x + a) for some w, x
i ] = K, so E[ZZ T ] = K.

(cid:2)z1, . . . , zm

∈

∈

∈

×

Rn

≥

1. Then the quantized feature matrix is Z + C for some random C

Now suppose we quantize Z to b bits using the quantization method described in Section 4.1, for some ﬁxed
m whose entries are independent
b
(cid:3) where
conditioned on Z (but not identically distributed) with E[C
ci are the (scaled) columns of C. Moreover, the ci are independent conditioned on Z. Deﬁning δ2
1)2 , the
entries Cij have variance E[C 2
δ2
b /m by Proposition 7 in Appendix C.2. In terms of the vectors ci, we
can also see that E[c2

Rn
×
Z] = 0. We can write C = 1
√m

Zij]
ij |
b , where ci,j denotes the jth element of ci.
δ2
≤

(cid:2)c1, . . . , cm
b :=

i,j |

Zij]

(2b

≤

∈

−

2

|

We ﬁrst analyze the expectation of (Z + C)(Z + C)T (over both the randomness of Z and of C).
Lemma 3. E[(Z +C)(Z +C)T ] = K +D, where D := E[c1cT
D does not depend on the number of random features m.

1 ] = sbIn is a multiple of the identity, for 0

sb

≤

≤

δ2
b .

Proof.

E[(Z + C)(Z + C)T ] = E

(zi + ci)(zi + ci)T

= E[(z1 + c1)(z1 + c1)T ]

(cid:20) 1
m

m
(cid:88)

i=1

(cid:21)

Since Ec1[c1 |

z1] = 0, it follows that

E[(z1 + c1)(z1 + c1)T ] = Ez1

(cid:104)

Ec1[(z1 + c1)(z1 + c1)T
(cid:104)

Ec1[c1cT

1 |

(cid:105)

z1]
(cid:105)

|
z1]

1 ] + Ez1

= Ez1[z1zT
= K + E[c1cT
1 ]

It is clear that D := E[c1cT
variable. It is also easy to see that the jth entry on the diagonal of D is equal to E[c2
argue that each element z1,j = √2 cos(wT

1 ] is a diagonal matrix, because each element of c1 is a zero-mean independent random
δ2
b . Lastly, we
1 xj + a1) has the same distribution, because it is distributed the same

1,j |

z1,j]

≤

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

as √2 cos(a1) for a1 uniform in [0, 2π]. Thus, E[c2
completes the proof.

1,j |

z1,j] is independent of j. Letting sb := E[c2

1,1 |

z1,1]

δ2
b

≤

1 ], we can use matrix concentration to show that the quantized kernel matrix ˜K = (Z +C)(Z +C)T
With D := E[c1cT
is close to its expectation K + D. We ﬁrst strengthen the matrix Bernstein inequality with intrinsic dimension
(Theorem 7.7.1 in Tropp (2015)) by removing the requirement on the deviation.13
Theorem 4 (Matrix Bernstein: Hermitian Case with Intrinsic Dimension). Consider a ﬁnite sequence
random Hermitian matrices of the same size, and assume that

Xk
{

of

}

Introduce the random matrix

E[Xk] = 0 and λmax(Xk)

L for each index k.

≤

(cid:88)

Y =

Xk.

k

Let V be a semideﬁnite upper bound for the matrix-valued variance VAR [Y ]:

Deﬁne the intrinsic dimension bound and variance bound

VAR [Y ] = E[Y 2] =

V

(cid:23)

(cid:88)
k

E[X 2
k ].

Then, for t

0,

≥

intdim(V ) =

and

v =

V
(cid:107)

.
(cid:107)

tr(V )
V
(cid:107)

(cid:107)

P (λmax(Y )

t)

≥

≤

4 intdim(V )

exp

·

(cid:18)

t2/2
v + Lt/3

−

(cid:19)

.

(4)

√v + L/3 is exactly Theorem 7.7.1 in Tropp (2015). We just need to show that the bound

2(v + Lt/3). Indeed, t2

2Lt/3

2v has roots

−

−

≤

Proof. The case of t
is vacuous when 0

≥
t < √v + L/3.

Suppose that 0
(cid:113) L2
L
3 ±

≤

9 + 2v. The condition t2

≤

≤
t < √v + L/3. We show that then t2

2(v + Lt/3) is then equivalent to

L
3 −

(cid:114)

L2
9

+ 2v

t

≤

≤

L
3

+

(cid:114)

L2
9

+ 2v.

The lower bound is negative since v
bound. Thus 0

t < √v + L/3 implies that t2

≥

0, and t < √v + L/3 implies t < L/3 +

L2/9 + 2v, satisfying the upper

(cid:112)

2(v + Lt/3). The bound in equation (4) becomes

≤

≥

(cid:18)

≤
t2/2
v + Lt/3

(cid:19)

−

≥

≤

4 intdim(V ) exp

4 intdim(V ) exp(

1)

4/e > 1,

−

≥

since intdim(V )

1. Thus (4) holds vacuously for 0

t < √v + L/3.

We now present Lemma 5, in which we lower bound the probability that ˜K is “close” to its expectation K + D,
in the speciﬁc sense we describe below.
Lemma 5. Let K be an exact kernel matrix, and ˜K = (Z + C)(Z + C)T be an m-features b-bit LP-RFF
2 and M :=
approximation of K with expectation K + D. For any deterministic matrix B, let L := 2n
(cid:107)
B(K + δ2

B
(cid:107)

0,

b In)BT , then for any t1, t2 ≥
(cid:20)
(cid:16)
P
B

t1In

(Z + C)(Z + C)T

(K + D)

B

t2In

(cid:22)
−
4 tr(M )
M
(cid:107)

(cid:107)

1

≥

−

(cid:20)

(cid:18)

exp

2L(
(cid:107)

−
M
(cid:107)

−

(cid:19)

mt2
1
+ 2t1/3)

+ exp

−
M
2L(
(cid:107)
(cid:107)

mt2
2
+ 2t2/3)

(cid:19)(cid:21)

.

(cid:21)

(cid:17)

(cid:18)

(cid:22)

13Theorem 7.7.1 in Tropp (2015) requires that t

√v + L/3, where t, v, and L are as deﬁned in Theorem 4.

≥

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

=

Si

Proof. Let
=
B (cid:0)(Z + C)(Z + C)T
(K + D)(cid:1) B. We see that E[Si] = 0. We will bound λmax(S) and λmin(S) by
applying the matrix Bernstein inequality for symmetric matrices with intrinsic dimension (Theorem 4). Thus we
need to bound

i=1 Si

(cid:80)m

and

and

=

−

−

S

Si

(cid:0)B(zi + ci)(zi + ci)T BT

B(K + D)BT (cid:1)

(cid:80)m

1
m

(cid:107)
Let ui = B(zi + ci)

(cid:107)

(cid:107)

E[S2
i ]
.
i=1
(cid:107)
Rn, then Si = 1
m (uiuT
2 =
(cid:107)

uiuT
(cid:107)

i −

∈

B(zi + ci)
(cid:107)
(cid:107)
where we have used the fact that zi + ci is a vector of length n whose entries are in [
bound on

zi + ci
(cid:107)

≤ (cid:107)

i (cid:107)

ui

≤

=

B

B

(cid:107)

(cid:107)

(cid:107)

(cid:107)

Si

−

2

2

2,

:
(cid:107)

(cid:107)

Si
(cid:107)

(cid:107)

=

1
m (cid:107)

uiuT

i −

E[uiuT
i ]

1
m (cid:107)

uiuT

(cid:107) ≤

1
m

E
uiuT
(cid:107)

i (cid:107) ≤

4n

2

B
(cid:107)
m

(cid:107)

= 2L/m.

uiuT
(cid:107)
2
(cid:107)

i (cid:107)
2n

E[uiuT

i ]). We ﬁrst bound

. Since this is a rank 1 matrix,

√2, √2]. This gives a

Thus λmax(Si)
Now it’s time to bound E[S2

2L/m and λmax(

−
i ]. We will use

Si) =

≤

−

λmin(Si)

≤

+

i (cid:107)
2L/m.

E[S2

i ] =

1
m2
1
m2

(cid:16)

E(cid:2)(uiuT

i )2(cid:3)

E(cid:2)uiuT

i

−

E[

ui
(cid:107)

(cid:107)

2uiuT
i ]

(cid:22)

2n

2
(cid:107)

B
(cid:107)
m2

(cid:3)2(cid:17)

1
m2
(cid:22)
E[uiuT

i ].

=

E[(uiuT

i )2] =

E[uiuT

i uiuT
i ]

1
m2

Thus

m
(cid:88)

i=1

E[S2
i ]

2n

2
(cid:107)

B
(cid:107)
m

(cid:22)

E[v1vT

1 ] =

2n

2
(cid:107)

B
(cid:107)
m

B(K + D)BT

B(K + δ2

b In)BT = LM/m.

2n

2
(cid:107)

B
(cid:107)
m

(cid:22)

Applying Theorem 4 with S, for any t2 ≥
(cid:16)

(Z + C)(Z + C)T

(cid:20)
λmax(B

P

0, we have

(cid:17)

−

(K + D)

4 tr(M )
M
(cid:107)
S and using the fact that λmax(

t2In

B)

(cid:23)

≤

(cid:107)
S) =

exp

(cid:21)

(cid:21)

Similarly, applying Theorem 4 with

(cid:20)
λmin(B

P

(cid:16)

(Z + C)(Z + C)T

(K + D)

B)

(cid:17)

−

−

t1In

(cid:22) −

≤

−

exp

−
4 tr(M )
M

(cid:107)

(cid:107)

Combining the two bounds with the union bound yields the desired inequality.

(cid:18)

(cid:19)

.

mt2
2
+ 2t2/3)

−
2L(
M
(cid:107)
(cid:107)
λmin(S), for any t1 ≥
(cid:19)
(cid:18)
mt2
1
+ 2t1/3)

2L(

−
M
(cid:107)
(cid:107)

.

0, we have

We are now ready to show that low-precision features yield close spectral approximation to the exact kernel
matrix.
Theorem 2. Let ˜K be an m-feature b-bit LP-RFF approximation of a kernel matrix K, and assume
b := 2/(2b
δ2
δ2
b /λ,
(cid:104)
P

1)2. Then for any ∆1 ≥
˜K + λI
∆1)(K + λI)

0, ∆2 ≥
(1 + ∆2)(K + λI)

K
(cid:107)

(cid:107) ≥

(1

−

≥

λ

(cid:105)

−

(cid:22)

(cid:22)

8 tr (cid:0)(K + λIn)−

1(K + δ2

b In)(cid:1)

exp

1

−

≥

(cid:32)

(cid:18)

(cid:19)

m∆2
1
−
4n
λ (1 + 2
3 ∆1)

+ exp

(cid:32)

m(∆2 −

−
4n
λ (1 + 2

3 (∆2 −

δ2
λ )2
b
δ2
λ ))
b

(cid:33)(cid:33)

.

Proof. We conjugate the desired inequality with B := (K + λIn)−
noting that semideﬁnite ordering is preserved by conjugation:

1/2 (i.e., multiply by B on the left and right),

∆1)(K + λIn)

˜K + λIn

(1 + ∆2)(K + λIn)

(1

(1

−

∆1)In

⇐⇒

⇐⇒ −

⇐⇒ −

⇐⇒ −

−
∆1In

∆1In

∆1In

(cid:22)

(cid:22)

(cid:22)

(cid:22)
B( ˜K + λIn)B

(cid:22)
B( ˜K + λIn)B
B( ˜K + λIn
B( ˜K

−
K)B

−

(cid:22)

−
K

(cid:22)
(1 + ∆2)In

(cid:22)
In

∆2In

(cid:22)
λIn)B

(cid:22)

−
∆2In.

∆2In

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

δ2
b /λ)In implies
(∆2 −
We show that
∆1In
(cid:22)
δ2
1
(K + λIn)−
b /λ because
0 by Lemma 3 and B is symmetric, BDB

K
−
B2
(cid:107)
(cid:107)

−
2 =
(cid:107)

D)B
=

B( ˜K
B
(cid:107)

1/λ and

(cid:22)

−

(cid:107)

(cid:107)
0. Thus the condition

(cid:107) ≤

B( ˜K
∆1In
−
(cid:22)
δ2
b , so BDB
∆1In

−
D

K)B
∆2In. Indeed,
(cid:22)
(δ2
b /λ)In. Moreover,
B( ˜K

D)B

K

−

−

(cid:22)

(cid:22)
(cid:22)

−

(cid:107) ≤
(cid:23)

BDB
(cid:107) ≤
(cid:107)
since D
(cid:23)
δ2
b /λ)In implies:
(∆2 −

B( ˜K
B( ˜K

−

K)B = B( ˜K
K)B = B( ˜K

Hence P

∆1In

(cid:104)
−

K)B

∆2In

−
B( ˜K
B( ˜K

−

−

K

K

(cid:105)

−

−

D)B + BDB

D)B + BDB

(cid:22)

(∆2 −
∆1In + 0 =

(cid:104)

P

∆1In

(cid:23) −
B( ˜K

∆1In.

−

b /λ)In + δ2
δ2

b /λIn = ∆2In,

K

D)B

It remains to

δ2
b /λ)In

(cid:105)

.

(∆2 −

(cid:22)
show that
D)B
(cid:22)
apply Lemma 5 for B := (K + λIn)−

∆1In

K

−

−

(cid:22)

−

−

−
≥
δ2
b /λ)In with the desired probability, by applying Lemma 5. We
(∆2 −
1/2, L := 2n

2n/λ, and M := B(K + δ2

−

−

(cid:22)

(cid:22)

(cid:22)

2

≤
To simplify the bound one gets from applying Lemma 5 with the above B, L, and M , we will use the following
=
expression for tr(M ), and the following upper and lower bounds on

B2(K + δ2

(cid:16)

(cid:17)

(cid:107)

b In)B.

B
(cid:107)

(cid:17)

b In)

(cid:16)

1(K + δ2

(K + λIn)−

tr
b In)U T U (S + λIn)−
δ2
λ1 ≥
M
(cid:107)

(cid:107) ≤

λ by assumption),

1/2U T = U (S + λIn)−

M
(cid:107)

(cid:107)

= (λ1 + δ2

1(S + δ2
b )/(λ1 + λ)

≥

1. Lemma 5 allows us to conclude the following:

. Letting K = U SU T be the SVD of K, we get that M = U (S + λIn)−

(cid:107)

M

.
(cid:107)

tr(M ) = tr

b In)
1/2U T U (S +
b In)U T . Thus, letting λ1 be the largest eigenvalue of K (recall
λ, so

1/2. We also assume that δ2

(λ1 + δ2

b )/(2λ1)

b ≤

≥

(cid:20)

P

(1

−

(cid:20)
= P

∆1)(K + λIn)

˜K + λIn

(cid:22)

∆1In

(cid:22)

B( ˜K

K)B

∆2In

−

(cid:22)

(cid:22)

(cid:21)

(cid:21)
(1 + ∆2)(K + λIn)

≥

≥

(cid:104)

P

−

−

−

1

1

∆1In

B( ˜K
(cid:20)

−
(cid:18)

(cid:22)
4 tr(M )
M
(cid:107)
8 tr (cid:0)(K + λIn)−

exp

(cid:107)

≥

−

(K + D))B

(cid:105)

(cid:22)

(∆2 −
(cid:19)

δ2
b /λ)In
(cid:18)

−
M
(cid:107)

2L(
(cid:107)
1(K + δ2

m∆2
1
+ 2∆1/3)
b In)(cid:1)

(cid:20)

exp

(cid:18)

+ exp

−
2L(
M
(cid:107)
m∆2
1
4n/λ(1 + 2∆1/3)

−

b /λ)2
δ2
m(∆2 −
δ2
b /λ)/3)
+ 2(∆2 −
(cid:107)
(cid:18)
(cid:19)
+ exp

(cid:19)(cid:21)

m(∆2 −

δ2
b /λ)2
δ2
4n/λ(1 + 2(∆2 −
b /λ)/3)

−

(cid:19)(cid:21)

.

There is a bias-variance trade-oﬀ: as we decrease the number of bits b, under a ﬁxed memory budget we can use
more features, and (Z + C)(Z + C)T concentrates more strongly (lower variance) around the expectation K + D
δ2
with 0
b In. However, this expectation is further away from the true kernel matrix K (larger bias). Thus
there should be an optimal number of bits b∗ that balances the bias and the variance.

D

(cid:22)

(cid:22)

Proof of Corollary 2.1. Letting ∆2 → ∞
(cid:105)
˜K + λIn

∆1)(K + λIn)

(cid:104)
(1

P

−

(cid:22)

1

≥

−

in Theorem 2 gives

8 tr (cid:0)(K + λIn)−

1(K + δ2

b In)(cid:1) exp

(cid:18)

m∆2
1
4n/λ(1 + 2∆1/3)

−

(cid:19)

.

Using the assumption that ∆1 ≤
(cid:104)

P

(1

−

∆1)(K + λIn)

3/2, we can simplify the bound:

(cid:105)

˜K + λIn

(cid:22)

1

≥

−

8 tr (cid:0)(K + λIn)−

1(K + δ2

b In)(cid:1) exp

(cid:18)

m∆2
1
−
8n/λ

(cid:19)

.

Letting the RHS be 1

ρ and solving for m yields

−

m

≥

8n/λ
∆2
1

log

(cid:17)

.

(cid:16) a
ρ

Similarly, letting ∆1 → ∞

in Theorem 2 gives

P

(cid:104)
˜K + λIn

(1

(cid:22)

−

(cid:105)
∆2)(K + λIn)

1

≥

−

8 tr (cid:0)(K + λIn)−

1(K + δ2

b In)(cid:1) exp

(cid:18)

m(∆2 −
−

δ2
b /λ)2
δ2
b /λ)/3)
4n/λ(1 + 2(∆2 −

(cid:19)

.

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Using the assumption that ∆1 ≤

3/2, we can simplify the bound:

(cid:104)

P

(1

−

∆1)(K + λIn)

(cid:105)

˜K + λIn

(cid:22)

1

≥

−

8 tr (cid:0)(K + λIn)−

1(K + δ2

b In)(cid:1) exp

(cid:18)

−

m(∆2 −
8n/λ

b /λ)2
δ2

(cid:19)

.

Letting the RHS be 1

ρ and solving for m yields

−

m

≥

(∆2 −

8n/λ
b /λ)2 log
δ2

(cid:17)

.

(cid:16) a
ρ

and set ∆1 = ∆2 = ∆, we get the following corollary, similar to the result

If we let the number of bits b go to
from Avron et al. (2017):
Corollary 5.1. Suppose that ˜K = ZZ T ,

∞

(cid:104)

P

(1

−

∆)(K + λIn)

˜K + λIn

(cid:22)

(cid:22)

(cid:105)
(1 + ∆)(K + λIn)

≥

−

1

16 tr((K + λIn)−

1K) exp

(cid:18)

3m∆2
16n/λ

−

(cid:19)

.

K
(cid:107)

(cid:107) ≥

λ. Then for any ∆

1/2,

≤

Thus if we use m
≥
(1 + ∆)(K + λIn) with probability at least 1

16
3∆2 n/λ log(16 tr((K + λIn)−

ρ.

−

1K)/ρ) features, then (1

∆)(K + λIn)

−

˜K + λIn

(cid:22)

(cid:22)

The constants are slightly diﬀerent from that of Avron et al. (2017) as we use the real features √2 cos(wT x + a)
instead of the complex features exp(iwT x).

From these results, we now know that the number of features required depends linearly on n/λ; more precisely,
n/λ features (for some constant c0 > 0), ˜K + λIn will be a (∆, ∆)-spectral
we know that if we use m
approximation of K + λIn with high probability. Avron et al. (2017) further provide a lower bound, showing that
n/λ (for some other constant c1 > 0), ˜K + λIn will not be a (∆, ∆)-spectral approximation of K + λIn
if m
with high probability. This shows that the number of random Fourier features must depend linearly on n/λ.

c0 ·

c1 ·

≤

≥

C.2 Frobenius Kernel Approximation Error Bounds for LP-RFFs

We begin this section by bounding the variance of the quantization noise C added to the random feature matrix
Z. We prove this as a simple consequence of the following Lemma.14
Lemma 6. For z
probability c
c

be the random variable which with probability z
c
(c

z
z. Then E [X a,c

a
a equals c
−
−
a)2
.
−
4

] = 0, and VAR [X a,c

[a, c], let X a,c

z, and with

] = (c

z)(z

a)

−

z

z

∈
z
a equals a
−
−

−

−

−

≤

Proof.

VAR [X a,c

z

] = (c

E [X a,c
z

] = (c

z)

z
c

·

a
a

+ (a

z)

−

c
c

·

z
a

−
−
c
−
c
−
a))

z
a

z)2

−
z + z

·

−

+ (a

a
−
a
−
a)((c
−
c
a
a)

−

−

= 0.

z)2

·
z)(z

−

−

(c

=

−
−
z
c

−

z)(z

−

= (c
d
dz

−
[
−

=

2z + c + a.

−

d
dz

[VAR [X a,c

z

]] =

z2 + (c + a)z

ac]

−

Now, setting the derivative to 0 gives z∗ = c+a
z)(z

a) = (c

a)2
a) = (c
−
4

2 )( c+a
c+a

.

−

−

2 −

2 . Thus, arg maxz

[a,c](c

z)(z

∈

−

−

a) = c+a

2 , and maxz

[a,c](c

∈

−

14This lemma is also a direct consequence of Popoviciu’s inequality on variances (Popoviciu, 1935). Nonetheless, we

include a stand-alone proof of the lemma here for completeness.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Proposition 7. E[C 2

Zij]

b /m, for δ2
δ2

b :=

ij |

≤

(2b

2

1)2 .
−

Proof. Given a feature Zij

2/m], we quantize it to b bits by dividing this interval into 2b

1

−

1 ), and randomly rounding to the top or bottom of the sub-interval
equally-sized sub-intervals (each of size
containing Zij (in an unbiased manner). Let a, c denote the boundaries of the sub-interval containing Zij, where

−

(cid:112)

2/m,

(cid:112)

[

∈

−

2√2/m
2b

1 . We can now see that Cij = X a,c
−
Zij] = 0. Because c

a =

2√2/m
2b

Zij is the unique random variable such that Zij + Cij
X a,c

1 , it follows from Lemma 6 that E[C 2

Zij] = VAR

(cid:104)

a, c
}
(cid:105)

∈ {
Zij

≤

Zij |

−

−

ij |

c = a +

2√2/m
2b
and E [Cij
a)2
4 = 8/m
(c

4(2b

−

|

−

1)2 = δ2

b /m.

We now prove an upper bound on the expected kernel approximation error for LP-RFFs, which applies for any
quantization function with bounded variance. This error corresponds exactly to the variance of the random
variable which is the product of two quantized random features.
Theorem 8. For x, y
≤
σ2, and that k(x, x) = k(y, y) = 1.15 For any unbiased random quantization function Q with bounded variance
VAR [Q(z)]
≤
2˜σ2 + ˜σ4 + σ2.

˜σ2 for any (ﬁxed) z, it follows that E [Q(Zx)Q(Zy)] = k(x, y), and that VAR [Q(Zx)Q(Zy)]

, assume we have random variables Zx, Zy satisfying E [ZxZy] = k(x, y), VAR [ZxZy]

∈ X

≤

Proof. Let Q(Zx) = Zx + (cid:15)x, and Q(Zy) = Zy + (cid:15)y, where E [(cid:15)x] = E [(cid:15)y] = 0, E (cid:2)(cid:15)2
are independent random variables.

x

(cid:3)

≤

˜σ2, E (cid:2)(cid:15)2

y

(cid:3)

≤

˜σ2, and (cid:15)x,(cid:15)y

E [Q(Zx)Q(Zy)] = E [(Zx + (cid:15)x)(Zy + (cid:15)y)]

= E [ZxZy + Zy(cid:15)x + Zx(cid:15)y + (cid:15)x(cid:15)y]
= E [ZxZy]
= k(x, y).

VAR [Q(Zx)Q(Zy)] = E

Q(Zx)Q(Zy)

k(x, y)

(cid:17)2(cid:21)

−

(cid:20)(cid:16)

(cid:20)(cid:16)

(cid:20)(cid:16)

(cid:20)(cid:16)

= E

= E

= E

(Zx + (cid:15)x)(Zy + (cid:15)y)

k(x, y)

−

(cid:17)2(cid:21)

Zy(cid:15)x + Zx(cid:15)y + (cid:15)x(cid:15)y + ZxZy

k(x, y)

(cid:17)2(cid:21)

−
(cid:20)(cid:16)

(cid:17)2(cid:21)

+ E

(cid:17)2(cid:21)

ZxZy

k(x, y)

−

Zy(cid:15)x + Zx(cid:15)y + (cid:15)x(cid:15)y

≤

y (cid:15)2

x + Z 2

E (cid:2)Z 2
y + (cid:15)2
k(y, y)˜σ2 + k(x, x)˜σ2 + ˜σ4 + σ2

(cid:3) + σ2

x(cid:15)2

x(cid:15)2
y

≤
= 2˜σ2 + ˜σ4 + σ2.

Note that if x = y, for this proof to hold, we would need to randomly quantize Zx twice, giving two independent
quantization noise samples (cid:15)(1)

x and (cid:15)(2)
x .

σ2, quantizing the random features will have negligable eﬀect on the
This theorem suggests that if 2˜σ2 + ˜σ4
variance. In practice, for the random quantization method we use with random Fourier features (described in
Section 4.1), we have ˜σ2 =
1)2 . Thus, for a large enough precision b, the quantization noise will be tiny
relative to the variance inherent to the random features.

(cid:28)

(2b

−

2

We note that the above theorem applies to one-dimensional random features, but can be trivially extended to m
dimensional random features. We show this in the following corollary.

15For example, one speciﬁc instance of the random variables Zx, Zy is given by random Fourier features, where zx =
√2, √2], for random w, b. We need not assume that k(x, x) = k(y, y) = 1,

√2 cos(wT x+b), zy = √2 cos(wT y +b), zx, zy
but this simpliﬁes the ﬁnal expression, as can be seen in the last step of the proof.

−

∈

[

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Table 3: Dataset details. For classiﬁcation tasks, we write the number of classes in parentheses in the “task”
column.

Dataset Task

Train Heldout Test # Features

Reg.
Census
YearPred Reg.
CovType Class. (2)
TIMIT

Class. (147)

16k
417k
418k
2.3M 245k

2k
46k
46k

2k
52k
116k
116k

119
90
54
440

Table 4: The Gaussian kernel bandwidths used, and the search grid for initial learning rate on the Census,
YearPred, Covtype and TIMIT datasets. Optimal learning rate in bold.

Dataset

Census
YearPred
Covtype
TIMIT

1/2σ2

0.0006
0.01
0.6
0.0015

Initial learning rate grid

0.01, 0.05, 0.1, 0.5, 1.0
0.05, 0.1, 0.5, 1.0, 5.0
1.0, 5.0, 10.0, 50.0, 100.0
5.0, 10.0, 50.0, 100.0, 500.0

∈ X

Corollary 8.1. For x, y
VAR [Zx] , VAR [Zy]
≤
bounded variance (E [Q(z)] = z, VAR [Q(z)]
(T1, . . . , Tn) be a random sequence of i.i.d. draws from S and T respectively. Deﬁne ¯Sn = 1
n
¯Tn = 1
n
VAR (cid:2) ¯Sn

, assume we have random variables Zx, Zy satisfying E [ZxZy] = k(x, y), and
σ2, and that k(x, x) = k(y, y) = 1. Let Q be any unbiased quantization function with
˜σ2 for any z). Let S = ZxZy, T = Q(Zx)Q(Zy), and (S1, . . . , Sn),
(cid:80)n
i=1 Si, and
(cid:3) = k(x, y), and that

≤
i=1 Ti, to be the empirical mean of these draws. It follows that E (cid:2) ¯Sn
.

(cid:3) = E (cid:2) ¯Tn

(cid:80)n
(cid:3)

n , and VAR (cid:2) ¯Tn

2˜σ2+˜σ4+σ2
n

σ2

(cid:3)

≤

≤

Proof.

VAR (cid:2) ¯Sn

(cid:3) = VAR

(cid:34)

1
n

VAR

(cid:35)

Si

(cid:21)

Si

n
(cid:88)

i=1
(cid:20) 1
n

=

≤

=

n
(cid:88)

i=1

n

·
σ2
n

σ2
n2

The result for VAR (cid:2) ¯Tn

(cid:3) follows in the same way, using the result from Theorem 8.

D EXPERIMENT DETAILS AND EXTENDED RESULTS

D.1 Datasets and Details Applying to All Experiments

In this work, we present results using FP-Nystr¨om, FP-RFFs, circulant FP-RFFs, and LP-RFFs on the TIMIT,
YearPred, CovType, and Census datasets. These datasets span regression, binary classiﬁcation, and multi-class
classiﬁcation tasks. We present details about these datasets in Table 3. In these experiments we use the Gaussian
kernel with the bandwidth σ speciﬁed in Table 4; we use the same bandwidths as May et al. (2017). To evaluate
the performance of these kernel models, we measure the classiﬁcation error for classiﬁcation tasks, and the mean
squared error (MSE) for regression tasks ( 1
yi)2), on the heldout set. We compute the total
n
memory utilization as the sum of all the components in Table 1.

i=1(f ˜K(xi)

(cid:80)n

−

For all datasets besides TIMIT, we pre-processed the features and labels as follows: We normalized all continuous

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

features to have zero mean and unit variance. We did not normalize the binary features in any way. For regression
datasets, we normalized the labels to have zero mean across the training set.

TIMIT (Garofolo et al., 1993) is a benchmark dataset in the speech recognition community which contains
recordings of 630 speakers, of various English dialects, each reciting ten sentences, for a total of 5.4 hours of
speech. The training set (from which the heldout set is then taken) consists of data from 462 speakers each
reciting 8 sentences (SI and SX sentences). We use 40 dimensional feature space maximum likelihood linear
regression (fMLLR) features (Gales, 1998), and concatenate the 5 neighboring frames in either direction, for a
total of 11 frames and 440 features. This dataset has 147 labels, corresponding to the beginning, middle, and end
of 49 phonemes. For reference, we use the exact same features, labels, and divisions of the dataset, as (Huang
et al., 2014; Chen et al., 2016; May et al., 2017).

We acquired the CovType (binary) and YearPred datasets from the LIBSVM webpage,16 and the Census dataset
from Ali Rahimi’s webpage.17 For these datasets, we randomly set aside 10% of the training data as a heldout set
for tuning the learning rate and kernel bandwidth.

The speciﬁc ﬁles we used were as follows:

•

•

•

CovType: We randomly chose 20% of “covtype.libsvm.binary” as test, and used the rest for training/heldout.

YearPred: We used “YearPredictionMSD” as training/heldout set, and “YearPredictionMSD.t” as test.

Census: We used the included matlab dataset ﬁle “census.mat” from Ali Rahimi’s webpage. This Matlab ﬁle
had already split the data into train and test. We used a random 10% of the training data as heldout.

D.2 Nystr¨om vs. RFFs (Section 2.2)

}

∈ {

. For RFFs, we use m

1250, 2500, 5000, 10000, 20000

We compare the generalization performance of full-precision RFFs and the Nystr¨om method across four
datasets, for various memory budgets. We sweep the following hyperparameters: For Nystr¨om, we use
m
1250, 2500, 5000, 10000, 20000, 50000, 100000,
∈ {
200000, 400000
. We choose these limits diﬀerently because 20k Nystr¨om features have roughly the same memory
}
footprint as 400k FP-RFFs. For all experiments, we use a mini-batch size of 250. We use a single initial learning
rate per dataset across all experiments, which we tune via grid search using 20k Nystr¨om features. We choose
to use Nystr¨om features to tune the initial learning rate in order to avoid biasing the results in favor of RFFs.
We use an automatic early-stopping protocol, as in (Morgan and Bourlard, 1990; Sainath et al., 2013b,a), to
regularize our models (Zhang et al., 2005; Wei et al., 2017) and avoid expensive hyperparameter tuning. It works
as follows: at the end of each epoch, we decay the learning rate in half if the heldout loss is less than 1% better
relative to the previous best model, using MSE for regression and cross entropy for classiﬁcation. Furthermore, if
the model performs worse than the previous best, we revert the model. The training terminates after the learning
rate has been decayed 10 times.

We plot our results comparing the Nystr¨om method to RFFs in Figure 7. We compare the performance of these
methods in terms of their number of features, their training memory footprint, and the squared Frobenius norm
and spectral norm of their kernel approximation error.

D.3 Nystr¨om vs. RFFs Revisited (Section 3.2)

D.3.1 Small-Scale Experiments

In order to better understand what properties of kernel approximation features lead to strong generalization
performance, we perform a more ﬁne-grained analysis on two smaller datasets from the UCI machine learning
repository—we consider the Census regression task, and a subset of the CovType task with 20k randomly sampled
training points and 20k randomly sampled heldout points. The reason we use these smaller datasets is because
computing the spectral norm, as well as (∆1,∆2) are expensive operations, which requires instantiating the kernel
matrices fully, and performing singular value decompositions. For the Census experiments, we use the closed-form
solution for the kernel ridge regression estimator, and choose the λ which gives the best performance on the

16https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/
17https://keysduplicated.com/ ali/random-features/data/

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

(a)

(b)

(c)

(d)

Figure 7: Generalization performance of FP-RFFs and Nystr¨om with respect to # features (a) and training
memory footprint (b) on Census, CovType, Yearpred and TIMIT. Nystr¨om performs better for a ﬁxed number
of features, while FP-RFFs perform better under a memory budget. We also see that the relative performance
between these methods cannot be fully explained by the Frobenius norms (c) or spectral norms (d) of their
respective kernel approximation error matrices; in particular, we see many example of Nystr¨om models that have
lower Frobenius or spectral error, but worse heldout performance, than various RFF models. To quantify the
degree of alignment between these error metrics and generalization performance (right plots), we show in the
ﬁgure titles the Spearman rank correlation coeﬃcients ρ between the corresponding x and y metrics. We see
in Figure 11 that 1/(1
∆1) attains notably higher values of ρ than the Frobenius and spectral norms of the
kernel approximation error. Note that although we plot the average performance across three random seeds for
each experimental setting (error bars indicate standard deviation), when we compute the ρ values we treat each
experimental result independently (without averaging).

−

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

−

Figure 8: The correlation between generalization performance and squared Frobenius norm, spectral norm, ∆,
and 1/(1
∆1) for FP-RFFs and FP-Nystr¨om on the Census and subsampled CovType datasets. To quantify
the alignment between these metrics and downstream performance, we include the Spearman rank correlation
coeﬃcients ρ computed between these metrics and the downstream performance of our trained models in the
ﬁgure titles. Note that although we plot the average performance across ﬁve random seeds for each experimental
setting (error bars indicate standard deviation), when we compute the ρ values we treat each experimental result
independently (without averaging).

Table 5: Number of features used for the diﬀerent kernel approximation methods in the experiments on
generalization performance vs. (∆1, ∆2) in Sections 3.2 and 5.2.

Methods

Number of features

FP-Nystr¨om
FP-RFF
Cir. FP-RFF
LP-RFF 16
LP-RFF 8, 4, 2, 1

25, 50, 100, 200, 500, 1250, 2500, 5000, 10000, 20000
200, 500, 1000, 2000, 5000, 10000, 20000
200, 500, 1000, 2000, 5000, 10000, 20000
500, 1000, 2000, 5000, 10000, 20000, 50000
1000, 2000, 5000, 10000, 20000, 50000

heldout set. For CovType, because there is no closed-form solution for logistic regression, we used the following
training protocol to (approximately) ﬁnd the model which minimizes the regularized training loss (just like the
closed-form ridge regression solution does). For each value of λ, we train the model to (near) convergence using 300
epochs of SGD (mini-batch size 250) at a constant learning rate, and pick the learning rate which gives the lowest
regularized training loss for that λ. We then evaluate this converged model on the heldout set to see which λ gives
the best performance. We pick the best learning rate, as well as regularization parameter, by using 20k Nystr¨om
features as a proxy for the exact kernel (note that because there are only 20k training points, this Nystr¨om
approximation is exact). We choose the learning rate 5.0 from the set
, and
}
the regularization parameter λ = 5e
. For the Census
4
}
. We sweep the number
1e
dataset, we pick λ = 5e
}
{
of features shown in Table 5. For both datasets, we report the average squared Frobenius norm, spectral norm,
∆, (∆1, ∆2) and the average generalization performance, along with standard deviations, using ﬁve diﬀerent
random seeds. The results are shown in Figure 8. As can be seen in Figure 8, 1/(1
∆1) aligns much better with
generalization performance than the other metrics.

0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0
5, 1e
−
1
−

6 from
5, 5e

{
6, 5e
3, 1e

7, 5e
4, 1e

7, 1e
3, 5e

6, 1e
2, 5e

5, 5e
2, 1e

8, 1e
4, 5e

5e
{
5, 1e

4 from

−
−

−
−

−
−

−
−

−
−

−
−

−
−

−

−

−

−

It is important to note that although 1/(1
∆1) aligns quite well with generalization performance (Spearman
rank correlation coeﬃcients ρ equal to 0.958 and 0.948 on Census and CovType, respectively), it does not align
perfectly. In particular, we see that for a ﬁxed value of ∆1, Nystr¨om generally attains better heldout performance
than RFFs. We believe this is largely explained by the fact that Nystr¨om always has ∆2 = 0, while RFFs can have

−

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

−

Figure 9: The correlation between generalization performance and squared Frobenius norm, spectral norm, ∆,
and 1/(1
∆1) for various types of full-precision RFFs. The Spearman rank correlation coeﬃcients ρ between the
x and y metrics of each ﬁgure are included in the ﬁgure titles. For these full-precision RFF experiments, we see
∆1) both align very well with downstream performance
that the original ∆ (Avron et al., 2017) as well as 1/(1
−
(ρ = 0.940 and ρ = 0.938, respectively), while the Frobenius and spectral norms do not (ρ = 0.705 and ρ = 0.652,
respectively). Note that although we plot the average performance across ﬁve random seeds for each experimental
setting (error bars indicate standard deviation), when we compute the ρ values we treat each experimental result
independently (without averaging).

Figure 10: The generalization performance as a function of the number of features (left) and the training memory
footprint (right) for various types of RFFs on the Census dataset. We observe that LP-RFFs can achieve lower
heldout mean-squared error (MSE) than other types of RFFs, included the memory-eﬃcient structured orthogonal
random features (“FP-SORF (SC)”), under the same memory budget. We plot performance averaged over ﬁve
random seeds, with error bars indicating standard deviations.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

relatively large values of ∆2 when the number of features is small (see Figure 3). As we show in the generalization
bound in Proposition 1, in Figure 5, as well as in the empirical and theoretical analysis in Appendix B, we expect
generalization performance to deteriorate as ∆2 increases.

D.3.2 Experiments with Diﬀerent Types of RFFs

In Figure 9 we repeat the above experiment on the Census dataset using a number of variations of RFFs.
Speciﬁcally, we run experiments using the [sin(wT x), cos(wT x)] parameterization of RFFs, which Sutherland
and Schneider (2015) show has lower variance than the cos(wT x + a) parameterization (we denote the [sin, cos]
method by “FP-RFF (SC)”, and the cos method by “FP-RFF (C)”). We additionally use Quasi-Monte Carlo
features (Yang et al., 2014), as well as orthogonal random features and its structural variant (Yu et al., 2016); we
denote these by “FP-QMC (SC)”, “FP-ORF (SC)”, “FP-SORF (SC)” respectively, because we implement them
using the [sin, cos] parameterization. We can see in Figure 9 that although these methods attain meaningful
improvements in the Frobenius and spectral norms of the approximation error, these improvements do not
translate into corresponding gains in heldout performance. Once again we see that 1/(1
∆1) is able to much
better explain the relative performance between diﬀerent kernel approximation methods than Frobenius and
spectral error. In Figure 10 we see that LP-RFFs outperform these other methods in terms of performance under
a memory budget.

−

D.3.3 Large-Scale (∆1, ∆2) Experiments

∆1) on the large-scale experiments from Section 2.2. We measure
In Figure 11 we plot the performance vs. 1/(1
∆1 using the exact and approximate kernel matrices on a random subset of 20k heldout points (except for Census,
where we use the entire heldout set which has approximately 2k points). We pick several λ values between the
smallest and largest eigenvalues of the exact (subsampled) training kernel matrix. The strong alignment between
generalization performance and 1/(1

∆1) is quite robust to the choice of λ.

−

−

D.3.4 Measuring ∆ and (∆1, ∆2)

In order to measure the ∆1 and ∆2 between a kernel matrix K and an approximation ˜K, we ﬁrst observe that
the following statements are equivalent. Note that to get the second statement, we multiply the expressions in
the ﬁrst statement by (K + λIn)−

1/2 on the left and right.

(1

−

∆1)(K + λIn)
∆1)In

(1

−

∆1In

−

∆1In

−
1/2( ˜K

(cid:22)

(cid:22)

(cid:22)

(cid:22)

˜K + λIn
(cid:22)
(K + λIn)−

(K + λIn)−

(1 + ∆2)(K + λIn)
1/2( ˜K + λIn)(K + λIn)−
1/2(cid:16)

˜K + λIn

1/2

(cid:22)

(cid:17)

(1 + ∆2)In
1/2

(K + λIn)

(K + λIn)−

−

∆2In

(cid:22)

(K + λIn)−

1/2( ˜K

K)(K + λIn)−

1/2

∆2In.

−
1/2, this is equivalent to
(cid:0)A(cid:1). Lastly, we note that ∆ = max(∆1, ∆2).

∆1 ≤

λmin

−

(cid:22)

(cid:0)A(cid:1) and λmax

(cid:0)A(cid:1)

∆2. Thus,

≤

For A = (K + λIn)−
we choose ∆1 :=

λmin

K)(K + λIn)−
(cid:0)A(cid:1) and ∆2 := λmax

−

−

D.4 Theory Validation (Section 4.2)

To validate our theory in Section 4.2, we perform two sets of experiments to (1) demonstrate the asymptotic
behavior of ∆1 and ∆2 as the number of features increases, and (2) demonstrate that quantization has negligible
eﬀect on ∆2 when δ2

∆2.

b /λ

(cid:28)

To demonstrate the behavior of ∆1 and ∆2 as a function of the number of features, we collect ∆1, and ∆2 using
Nystr¨om features, circulant FP-RFFs, and LP-RFFs using b
, on both the Census and the sub-sampled
}
CovType datasets (20k random heldout points). For each approximation, we sweep the number of features as
listed in Table 6. We use the same value of λ as we used in Section 3.2 for each dataset. In Figure 12, we plot
the values of ∆1 and ∆2 attained by these methods as a function of the number of features. As discussed in
Section 4.2, ∆1 is primarily determined by the rank of the approximation, and approaches 0 for all the methods
as the number of features grows. ∆2, on the other hand, only approaches 0 for the high-precision methods—for
b

b /λ, marked by dashed lines).

, ∆2 converges to higher values (at most δ2
1, 4
}

1, 4, 8

∈ {

∈ {

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

1
∆1

1

−

, where we measure ∆1 using regularizer strength λ equal to the
Figure 11: Generalization performance vs.
0, 25, 50, 75, or 99 percentile eigenvalues of the exact kernel matrix (0 percentile indicates largest eigenvalue).
Note for the Census dataset, we plot the heldout MSE as a function of 1/(1
1 to avoid cluttering the
data points on the left end of the ﬁgure. For comparison to spectral and Frobenius norm plots, see Figure 7. To
∆1) and generalization performance for these diﬀerent values of
quantify the degree of alignment between 1/(1
λ, we compute the Spearman rank correlation coeﬃcients ρ. We see 1/(1
∆1) generally attains much higher
values of ρ than the Frobenius and spectral approximation errors (Figure 7). Although we plot performance
averaged across three random seeds (error bars indicate standard deviations), when we compute ρ we treat each
experimental result independently.

∆1)

−

−

−

−

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Figure 12: Empirical validation of Theorem 2. As the number of features grows, LP-RFFs approach ∆1 values of
0 (left), but plateau at larger ∆2 for very low precisions (right). These are an extended version of the results
from Figure 3 (left, middle) in Section 4.2. We plot average results across ﬁve random seeds, with error bars
indicating standard deviations.

Table 6: Number of features used for the diﬀerent kernel approximation methods in the theory validation
experiments in Section 4.2 (Figure 3 (left,middle)).

Methods

Number of features

FP-Nystr¨om
Cir. FP-RFF
LP-RFF 8, 4, 2, 1

25, 50, 100, 200, 500, 1250, 2500, 5000, 10000, 20000
200, 500, 1000, 2000, 5000, 10000, 20000, 50000, 100000, 200000, 400000
1000, 2000, 5000, 10000, 20000, 50000, 100000, 200000, 400000

To demonstrate that quantization has negligible eﬀect on ∆2 when δ2
points from the Census dataset. For λ
by the LP-RFFs relative to the exact kernel matrix. We see that for larger λ (corresponding to smaller δ2
Figure 3, lower precisions can be used while not inﬂuencing ∆2 signiﬁcantly; this aligns with the theory.

∆2, we use 8000 random training
, we measure the ∆2 attained
b /λ) in

1, 2, 4, 8, 16, 32
}

, and b
}

4, 100, 104

b /λ

10−

∈ {

∈ {

(cid:28)

D.5 Empirical Evaluation of LP-RFFs (Section 5.1)

To empirically demonstrate the generalization performance of LP-RFFs, we compare LP-RFFs to FP-RFFs,
circulant FP-RFFs, and Nystr¨om features for various memory budgets. We use the same datasets as in Section 2,
including Census and YearPred for regression, as well as CovType and TIMIT for classiﬁcation. We use the same
experimental protocol as in Section 2 (details in Appendix D.2), with the only signiﬁcant additions being that we
also evaluate the performance of circulant FP-RFFs, and LP-RFFs for precisions b
. As noted in
}
the main text, all our LP-RFF experiments are done in simulation; in particular, we represent each low-precision
feature as a 64-bit ﬂoating point number, whose value is one of the 2b values representable in b bits. For our
full-precision experiments we also use 64-bit ﬂoats, but we report the memory utilization of these experiments
as if we used 32 bits, to avoid inﬂating the relative gains of LP-RFFs over the full-precision approaches. We
randomly sample the quantization noise for each mini-batch independently each epoch.

1, 2, 4, 8, 16

∈ {

In Section 5.1 (Figure 4), we demonstrated the generalization performance of LP-RFFs on the TIMIT, YearPred,
and CovType datasets, using 4 bits per feature. In Figure 13, we additionally include results on the Census
dataset, and include results for a larger set of precisions (b
). We also include plots of heldout
performance as a function of the number of features used. We observe that LP-RFFs, using 2-8 bits, systematically
outperform the full-precision baselines under diﬀerent memory budgets.

1, 2, 4, 8, 16

∈ {

}

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 13: Generalization performance of the kernel approximation methods (Nystr¨om, FP-RFFs, circ. FP-RFFs,
LP-RFFs) with respect to the number of features used, as well as with respect to memory used. We observe
that LP-RFFs demonstrate better generalization performance than the full-precision baselines under memory
constraints, with 2-8 bits typically giving the best performance. Importantly, the relative ranking of the methods
changes depending on whether we compare the methods based on their number of features, or their memory
utilization. For example, the Nystr¨om method shows better generalization performance than the RFF-based
approaches with the same number of features. However, the Nystr¨om method often performs signiﬁcantly worse
than the RFF methods under ﬁxed memory budgets. We plot results averaged over three random seeds.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

Table 7: Dataset details for the additional datasets from Section D.5. For classiﬁcation tasks, we write the number
of classes in parentheses in the “Task” column.

Dataset

Task

Train Heldout Test # Features

Class. (2)
Forest
Cod-RNA Class. (2)
Class. (2)
Adult
Reg.
CPU

470k
54k
29k
6k

52k
6k
3k
0.7k

58k
272k
16k
0.8k

54
8
123
21

Table 8: The Gaussian kernel bandwidths used, and the search grid for initial learning rate on the Forest,
Cod-RNA, Adult, and CPU datasets. Optimal learning rate in bold.

Dataset

1/2σ2

Initial learning rate grid

Forest
0.5
Cod-RNA 0.4
0.1
Adult
0.03
CPU

5.0, 10.0, 50.0, 100.0, 500.0
10.0, 50.0, 100.0, 500.0, 1000.0
5.0, 10.0, 50.0, 100.0, 500.0, 1000.0
0.05, 0.1, 0.5, 1.0, 5.0

We run on four additional classiﬁcation and regression datasets (Forest, Cod-RNA, Adult, CPU) to compare the
empirical performance of LP-RFFs to full-precision RFFs, circulant RFFs and Nystr¨om. We present the results
in Figure 14, and observe that LP-RFFs can achieve competitive generalization performance to the full-precision
baselines with lower training memory budgets. With these 4 additional datasets, our empirical evaluation of
LP-RFFs now covers all the datasets investigated in Yang et al. (2012). We include details about these datasets
and the hyperparameters we used in Tables 7 and 8.

D.6 Generalization Performance vs. (∆1, ∆2) (Section 5.2)

For our experiments in Section 5.2, we use the same protocol and hyperparameters (learning rate, λ) as for the
(∆1, ∆2) experiments in Section 3.2. However, we additionally run experiments with LP-RFFs for precisions
b
. We use ﬁve random seeds for each experimental setting, and plot the average results, with
error bars indicating standard deviations.

1, 2, 4, 8, 16

∈ {

}

In Figure 15, we plot an extended version of the right plots in Figure 5. We include results for all precisions, and
for both Census and CovType. We observe that on Census, 1/(1
∆1) does not align well with performance
(Spearman rank correlation coeﬃcient ρ = 0.403), because the low-precision features (b = 1 or b = 2) perform
signiﬁcantly worse than the full-precision features of the same dimensions. In this case, when we consider the
impact of ∆2 by taking max (cid:0)1/(1
On CovType, on the other hand, the impact on generalization performance from using low-precision is much less
(cid:1) both align well with performance (ρ = 0.942). Furthermore,
pronounced, so 1/(1
in the case of CovType, ∆2 is generally smaller than 1/(1
∆1), so taking the max does not change the plot
signiﬁcantly.

(cid:1), we see that performance aligns much better (ρ = 0.959).

∆1) and max (cid:0)1/(1

∆1), ∆2

∆1), ∆2

−

−

−

−

−

To compute the Spearman rank correlation coeﬃcients ρ for these plots, we take the union of all the experiments
which are a part of the plot. In particular, we include FP-RFFs, circulant FP-RFFs, FP-Nystr¨om , and LP-RFFs
with precisions b
. Although we plot the average performance across ﬁve random seeds in the plot,
}
to compute ρ we treat each experiment independently.

1, 2, 4, 8, 16

∈ {

D.7 Other Experimental Results

D.7.1 Low-Precision Nystr¨om

We encounter two main obstacles to attaining strong performance with the Nystr¨om method under a memory
budget: (1) For the standard Nystr¨om method, because of the large projection matrix (32m2 space), there are

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 14: Comparison of the performance of LP-RFFs and the full-precision baselines on additional datasets,
with respect to the number of features used, as well as with respect to memory used. We observe that LP-RFFs
can achieve performance competitive with the full-precision baseline methods, with signiﬁcant memory savings.
We plot the average performance across three random seeds, with error bars indicating standard deviations.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

−

Figure 15: Generalization performance vs. diﬀerent kernel approximation metrics on Census and CovType. The
metric max (1/(1
∆1), ∆2) is able to incorporate the inﬂuence of both ∆1 and ∆2 on performance for LP-RFFs,
aligning well with generalization performance on both Census (Spearman rank correlation coeﬃcient ρ = 0.959)
and CovType (ρ = 0.942). 1/(1
∆1), on the other hand, fails to align well on Census (ρ = 0.403), but does
align on CovType (ρ = 0.942). Note that although we plot the average performance across ﬁve random seeds for
each experimental setting (error bars indicate standard deviation), when we compute the ρ values we treat each
experimental result independently (without averaging).

−

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 16: We plot the heldout performance (MSE) for the full-precision Nystr¨om method, the ensemble Nystr¨om
method, and 8-bit LP-RFFs. We also show the best possible performance for the Nystr¨om methods, assuming only
the kernel approximation features are quantized (denoted “ideal”); we compute this by plotting the full-precision
results but without counting the memory occupied by the features. The LP-RFF method signiﬁcantly outperforms
the “ideal” Nystr¨om methods as a function of memory.

(a) LP-Nystr¨om

(b) Ensemble LP-Nystr¨om

Figure 17: The generalization performance of low-precision Nystr¨om and low-precision ensemble Nystr¨om, using
a uniform quantization scheme.

Jian Zhang∗, Avner May∗, Tri Dao, Christopher R´e

inherent limits to the memory savings attainable by only quantizing the features, regardless of the quantization
scheme used. (2) While the ensemble Nystr¨om method (Kumar et al., 2009) is a well-known method which can
dramatically reduce the space occupied by the the Nystr¨om projection matrix, it does not attain meaningful gains
in performance under a memory budget. To demonstrate the ﬁrst issue empirically, we plot in Figure 16 the
performance of the full-precision Nystr¨om methods without counting the memory from the feature mini-batches
(denoted “ideal”). This is the best possible performance under any quantization scheme for these Nystr¨om
methods, assuming only the features are quantized. LP-RFFs still outperform these “ideal” methods for ﬁxed
memory budgets. To demonstrate the second issue, in Figure 16 we also plot the generalization performance of
the ensemble method vs. the standard Nystr¨om method, as a function of memory. We can see that the ensemble
method does not attain meaningful gains over the standard Nystr¨om method, as a function of memory.

(cid:112)

(cid:112)

2/m and

Lastly, we mention that quantizing Nystr¨om features is more challenging due to their larger dynamic range.
1 and 1, whereas for RFFs, each feature is between
For Nystr¨om, all we know is that each feature is between
2/m. In Figure 17 we plot our results quantizing Nystr¨om features with the following simple
−
scheme: for each feature, we ﬁnd the maximum and minimum value on the training set, and then uniformly
quantize this interval using b bits. We observe that performance degrades signiﬁcantly with less than or equal to
8 bits compared to full-precision Nystr¨om. Although using the ensemble method reduces the dynamic range by a
factor of √r with r blocks (we use r = 10, a common setting in (Kumar et al., 2012)), and also saves space on
the projection matrix, these strengths do not result in signiﬁcantly better performance for ﬁxed memory budgets.

−

D.7.2 Low-Precision Training for LP-RFFs

As discussed in Section 2.1, there are a number of ways to reduce the memory occupied by the model parameters,
including (1) using a low-rank decomposition of the parameter matrix (Sainath et al., 2013a), (2) using structured
matrices (Sindhwani et al., 2015), and (3) using low-precision (De Sa et al., 2018). Though these methods are
orthogonal to our LP-RFF method, we now present results using a low-precision parameterization of the model,
and show that we can attain similar performance to full-precision training. We use a training algorithm called
LM-HALP (linear model high-accuracy low-precision) (De Sa et al., 2018). By parameterizing the model in low
precision, this algorithm eliminates the need of casting the LP-RFFs back into full precision in order to multiply
them with the model parameters. This approach also allows for these matrix multiplications to be implemented
using fast low-precision matrix operations, and reduces the memory occupied by the model during training.

LM-HALP is based on the stochastic variance-reduced gradient (SVRG) algorithm. The model is parameterized
using a low-precision ﬁxed-point representation during training. In LM-HALP, all of the matrix multiplications
involved in the stochastic model updates are done using low-precision ﬁxed-point operations; however, the periodic
computation of the full gradient is calculated in full precision (this is embarrassingly parallelizable). Importantly,
even though most of training is done in low precision, the ﬁnal model returned by this training algorithm is a
full-precision model. We can further simplify the LM-HALP algorithm by replacing the SVRG updates with
SGD updates, thus eliminating the need for calculating and storing the full gradient. In Figure 18 we present our
results using LM-HALP on TIMIT, our largest and most challenging dataset; we use 8-bit LM-HALP (SVRG
and SGD) on top of 8-bit LP-RFFs, and compare to full-precision SGD training. For LM-HALP based training,
we perform the bit centering and rescaling operation after each training epoch. Under this setting, we show that
when the number of features is at least 10,000, the performance of both versions of HALP closely matches that of
full-precision training.

D.7.3 Double Sampling

We perform some initial experiments using the double sampling method of Zhang et al. (2017). In particular,
we use a diﬀerent random quantization of the LP-RFFs on the “forward pass” of our algorithm than in the
“backward pass.” In our initial experiments with double sampling, as shown in Figure 19, we did not observe
noticeable improvements in performance. These experiments were on the YearPred dataset with the Gaussian
kernel. We leave a more extended investigation of these gradient bias reduction methods for future work.

E EXTENDED RELATED WORK

Generalization Performance of Kernel Approximation Methods From a theoretical perspective, there
has been a lot of work analyzing the generalization performance of kernel approximation methods (Rahimi

Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation

Figure 18: Low-precision vs. full-precision training algorithms on TIMIT using 8-bit LP-RFFs.

Figure 19: Comparison of LP-RFFs with and without double sampling on the YearPred dataset.

and Recht, 2008; Cortes et al., 2010; Sutherland and Schneider, 2015; Rudi and Rosasco, 2017; Avron et al.,
2017; Li et al., 2018). The work most relevant to ours is that of Avron et al. (2017), which deﬁnes ∆-spectral
approximation and bounds the generalization performance of kernel approximation methods in terms of ∆. This
approach diﬀers from works which evaluate kernel approximation methods in terms of the Frobenius or spectral
norms of their kernel approximation matrices (Cortes et al., 2010; Gittens and Mahoney, 2016; Yang et al., 2014;
Sutherland and Schneider, 2015; Yu et al., 2016; Dao et al., 2017). Our work shows the promise of Avron et al.’s
approach, and builds upon it.

Large-Scale Kernel Experiments On the topic of scaling kernel methods to large datasets, there have been
a few notable recent papers. Tu et al. (2016) propose a distributed block coordinate descent method for solving
large-scale least squares problems using the Nystr¨om method or RFFs. The recent work of May et al. (2017) uses
a single GPU to train large RFF models on speech recognition datasets, showing comparable performance to
fully-connected deep neural networks. That work was limited by the number of features that could ﬁt on a single
GPU, and thus our proposed method could help scale these results.

