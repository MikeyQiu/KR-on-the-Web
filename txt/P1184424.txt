APRIL: Interactively Learning to Summarise by Combining
Active Preference Learning and Reinforcement Learning

Yang Gao, Christian M. Meyer, Iryna Gurevych
Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universit¨at Darmstadt
https://www.ukp.tu-darmstadt.de/

8
1
0
2
 
g
u
A
 
9
2
 
 
]
L
C
.
s
c
[
 
 
1
v
8
5
6
9
0
.
8
0
8
1
:
v
i
X
r
a

Abstract

We propose a method to perform automatic
document summarisation without using refer-
Instead, our method inter-
ence summaries.
actively learns from users’ preferences. The
merit of preference-based interactive sum-
marisation is that preferences are easier for
users to provide than reference summaries.
Existing preference-based interactive learning
methods suffer from high sample complex-
ity,
i.e. they need to interact with the ora-
cle for many rounds in order to converge.
In this work, we propose a new objective
function, which enables us to leverage ac-
tive learning, preference learning and rein-
forcement learning techniques in order to re-
duce the sample complexity. Both simula-
tion and real-user experiments suggest that
our method signiﬁcantly advances the state
of the art. Our source code is freely avail-
able at https://github.com/UKPLab/
emnlp2018-april.

1

Introduction

With the rapid growth of text-based information
on the Internet, automatic document summarisa-
tion attracts increasing research attention from the
Natural Language Processing (NLP) community
(Nenkova and McKeown, 2012). Most existing
document summarisation techniques require ac-
cess to reference summaries to train their systems.
However, obtaining reference summaries is very
expensive: Lin (2004) reported that 3,000 hours
of human effort were required for a simple evalu-
ation of the summaries for the Document Under-
standing Conferences (DUC). Although previous
work has proposed heuristics-based methods to
summarise without reference summaries (Ryang
and Abekawa, 2012; Rioux et al., 2014), the gap
between their performance and the upper bound is
still large: the ROUGE-2 upper bound of .212 on

DUC’04 (P.V.S. and Meyer, 2017) is, for example,
twice as high as Rioux et al.’s (2014) .114.

The Structured Prediction from Partial Infor-
mation (SPPI) framework has been proposed to
learn to make structured predictions without ac-
cess to gold standard data (Sokolov et al., 2016b).
SPPI is an interactive NLP paradigm:
It inter-
acts with a user for multiple rounds and learns
from the user’s feedback. SPPI can learn from
two forms of feedback: point-based feedback, i.e.
a numeric score for the presented prediction, or
preference-based feedback, i.e. a preference over
a pair of predictions. Providing preference-based
feedback yields a lower cognitive burden for hu-
mans than providing ratings or categorical labels
(Thurstone, 1927; Kendall, 1948; Kingsley and
Brown, 2010; Zopf, 2018). Preference-based SPPI
has been applied to multiple NLP applications, in-
cluding text classiﬁcation, chunking and machine
translation (Sokolov et al., 2016a; Kreutzer et al.,
2017). However, SPPI has prohibitively high sam-
ple complexities in the aforementioned NLP tasks,
as it needs at least hundreds of thousands rounds of
interaction to make near-optimal predictions, even
with simulated “perfect” users. Figure 1a illus-
trates the workﬂow of the preference-based SPPI.
To reduce the sample complexity, in this work,
we propose a novel preference-based interactive
learning framework, called APRIL (Active Prefer-
ence ReInforcement Learning). APRIL goes be-
yond SPPI by proposing a new objective func-
tion, which divides the preference-based interac-
tive learning problem into two phases (illustrated
in Figure 1b): an Active Preference Learning
(APL) phase (the right cycle in Figure 1b), and
a Reinforcement Learning (RL) phase (the left cy-
cle). We show that this separation enables us to
query preferences more effectively and to use the
collected preferences more efﬁciently, so as to re-
duce the sample complexity.

(a) Workﬂow of preference-based SPPI

(b) Workﬂow of APRIL

Figure 1: A comparison of workﬂows of SPPI (a)
and APRIL (b) in the EMDS use case. Notation
details, e.g., ∆x and r(yn), are discussed in §3.

We apply APRIL to Extractive Multi-Document
Summarisation (EMDS). The task of EMDS is to
extract sentences from the original documents to
build a summary under a length constraint. We
accommodate multiple APL and RL techniques
in APRIL and compare their performance under
different simulation settings. We also compare
APRIL to a state-of-the-art SPPI implementation
using both automatic metrics and human evalua-
tion. Our results suggest that APRIL signiﬁcantly
outperforms SPPI.

2 Related Work

RL has been previously used to perform EMDS
without using reference summaries. Ryang and
Abekawa (2012) formulated EMDS as a Markov
Decision Process (MDP), designed a heuristics-
based reward function considering both informa-
tion coverage rate and redundancy level, and used
the Temporal Difference (TD) algorithm (Sutton,
1984) to solve the MDP. In a follow-up work, Ri-
oux et al. (2014) proposed a different reward func-
tion, which also did not require reference sum-
their experiments suggested that using
maries;
their new reward function improved the summary
quality. Henß et al. (2015) proposed a different
RL formulation of EMDS and jointly used super-
vised learning and RL to perform the task. How-
ever, their method requires the access to reference
summaries. More recent works applied encoder-
decoder-based RL to document summarisation
(Ranzato et al., 2015; Narayan et al., 2018; Paulus
et al., 2017; Pasunuru and Bansal, 2018). These
works outperformed standard encoder-decoder as

RL can directly optimise the ROUGE scores and
can tackle the exposure bias problems. However,
these neural RL methods all used ROUGE scores
as their rewards, which in turn relied on reference
summaries. APRIL can accommodate these neu-
ral RL techniques in its RL phase by using a rank-
ing of summaries instead of the ROUGE scores as
rewards. We leave neural APRIL for future study.

P.V.S. and Meyer (2017) proposed a bigram-
based interactive EMDS framework. They asked
users to label important bigrams in candidate sum-
maries and used integer linear programming (ILP)
to extract sentences covering as many important
bigrams as possible. Their method requires no ac-
cess to reference summaries, but it requires con-
siderable human effort during the interaction: in
simulation experiments, their system needed to
collect up to 350 bigram annotations from a (simu-
lated) user. In addition, they did not consider noise
in users’ annotations but simulated perfect oracles.

Preference learning aims at obtaining the rank-
ing (i.e. total ordering) of objects from pairwise
preferences (F¨urnkranz and H¨ullermeier, 2010).
Simpson and Gurevych (2018) proposed to use an
improved Gaussian process preference learning
(Chu and Ghahramani, 2005) for learning to rank
arguments in terms of convincingness from crowd-
sourced annotations. However, such Bayesian
methods can hardly scale and suffer from high
computation time. Zopf (2018) recently proposed
to learn a sentence ranker from preferences. The
resulting ranker can be used to identify the impor-
tant sentences and thus to evaluate the quality of
the summaries. His study also suggests that pro-
viding sentence preferences takes less time than
writing reference summaries. APRIL not only
learns a ranking over summaries from pairwise
preferences, but also uses the ranking to “guide”
our RL agent to generate good summaries.

There is a recent trend in machine learning to
combine active learning, preference learning and
RL, for learning to perform complex tasks from
preferences (Wirth et al., 2017). The resulting al-
gorithm is termed Preference-based RL (PbRL),
and has been used in multiple applications, includ-
ing training robots (Wirth et al., 2016) and Atari-
playing agents (Christiano et al., 2017). SPPI and
APRIL can both be viewed as PbRL algorithms.
But unlike most PbRL methods that learn a utility
function of the predictions (in EMDS, predictions
are summaries) to guide the RL agent, APRIL

is able to directly use a ranking of predictions
to guide the RL agent without making assump-
tions about the underlying structure of the utility
functions. This also enables APRIL to use non-
utility-based preference learning techniques (e.g.,
Maystre and Grossglauser, 2017).

3 Background

In this section, we recap necessary details of SPPI,
RL and preference learning, and adapt them to the
EMDS use case, laying the foundation for APRIL.

3.1 The SPPI Framework

Let X be the input space and let Y(x) be the set
of possible outputs for input x ∈ X . In EMDS,
x ∈ X is a cluster of documents and Y(x) is the
set of all possible summaries for cluster x. The
function ∆x : Y(x)×Y(x) → {0, 1} is the prefer-
ence function such that ∆x(yi, yj) = 1 if the user
believes yj is better than yi (denoted by yj (cid:31) yi or
equivalently yi ≺ yj), and 0 otherwise. Through-
out this paper we assume that users do not equally
prefer two different items. For a given x, the ex-
pected loss is:

LSPPI(w|x) = Epw(yi,yj |x)[∆x(yi, yj)]
=
∆x(yi, yj) pw(yi, yj|x),

(cid:88)

(1)

yi,yj ∈Y(x)

where pw(yi, yj|x) is the probability of querying
the pair (yi, yj). Formally,

pw(yi, yj|x)

=

exp[w(cid:124)(φ(yi|x) − φ(yj|x))]
(cid:80)

exp[w(cid:124)(φ(yp|x) − φ(yq|x))]

,

(2)

yp,yq∈Y(x)

where φ(y|x) is the vector representation of y
given x, and w is the weight vector to be learnt.
Eq. (2) is a Gibbs sampling strategy: w(cid:124)(φ(yi|x)−
φ(yj|x)) can be viewed as the “utility gap” be-
tween yi and yj. The sampling strategy pw en-
courages querying pairs with large utility gaps.

To minimise LSPPI, SPPI uses gradient descent
to update w incrementally. Alg. 1 presents the
pseudo code of our adaptation of SPPI to EMDS.
In the supplementary material, we provide a de-
tailed derivation of ∇wLSPPI(w|x).

3.2 Reinforcement Learning

RL amounts to efﬁcient algorithms for searching
optimal solutions in MDPs. MDPs are widely

Input

: sequence of learning rates γt; query

budget T ; document cluster x

initialise w0;
while t = 0 . . . T do

sample (yi, yj) according to Eq. (2);
obtain feedback ∆x(yi, yj);
wt+1 := wt − γt∇wLSPPI(w|x)

end
(cid:124)
Output: y∗ = arg maxy∈Y (x) w
T +1φ(y, x)
Algorithm 1: SPPI for preference-based in-
teractive document summarisation (adjusted
from Alg. 2 in (Sokolov et al., 2016a)).

used to formulate sequential decision making
problems, which EMDS falls into: in EMDS, the
summariser has to sequentially select sentences
from the original documents and add them to the
draft summary. An (episodic) MDP is a tuple
(S, A, P, R, T ). S is the set of states, A is the set
of actions, P : S × A × S → R is the transition
function with P (s(cid:48)|s, a) yielding the probability of
performing action a in state s and being transited
to a new state s(cid:48). R : S × A → R is the reward
function with R(s, a) giving the immediate reward
for performing action a in state s. T ⊆ S is the set
of terminal states; visiting a terminal state termi-
nates the current episode.

In EMDS, we follow the same MDP formu-
lation as Ryang and Abekawa (2012) and Rioux
et al. (2014). Given a document cluster, a state s
is a draft summary, A includes two types of ac-
tions, concatenate a new sentence to the current
draft summary, or terminate the draft summary
construction. The transition function P in EMDS
is trivial because given the current draft summary
and an action, the next state can be easily inferred.
The reward function R returns an evaluation score
of the summary once the action terminate is per-
formed; otherwise it returns 0 because the sum-
mary is still under construction and thus not ready
to be evaluated. Providing non-zero rewards be-
fore the action terminate can lead to even worse
result, as reported by Rioux et al. (2014).

A policy π : S × A → R in an MDP deﬁnes
how actions are selected: π(s, a) is the probability
of selecting action a in state s. In EMDS, a policy
corresponds to a strategy to build summaries for a
given document cluster. We let Yπ(x) be the set of
all possible summaries the policy π can construct
in the document cluster x, and we slightly abuse
the notation by letting π(y|x) denote the probabil-

ity of policy π generating a summary y in cluster
x. Then the expected reward of a policy is:

RRL(π|x) = Ey∈Yπ(x)R(y|x)

(cid:88)

=

y∈Yπ(x)

π(y|x)R(y|x),

(3)

where R(y|x) is the reward for summary y in doc-
ument cluster x. The goal of an MDP is to ﬁnd the
optimal policy π∗ that has the highest expected re-
ward: π∗ = arg maxπ RRL(π).

Note that the loss function in SPPI (Eq. (1)) and
the expected reward function in RL (Eq. (3)) are in
similar forms: if we view the pair selection proba-
bility pw in Eq. (2) as a policy, and view the pref-
erence function ∆x in Eq. (1) as a negative reward
function, we can view SPPI as an RL problem.
The major difference between SPPI and RL is that
SPPI selects and evaluates pairs of outputs, while
RL selects and evaluates single outputs. We will
exploit their connection to propose our new objec-
tive function and the APRIL framework.

3.3 Preference Learning

The linear Bradley-Terry (BT) model (Bradley
and Terry, 1952) is one of the most widely used
methods in preference learning. Given a set of
items Y, suppose we have observed T preferences:
Q = {q1(y1,1, y1,2), · · · , qT (yT,1, yT,2)}, where
yi,1, yi,2 ∈ Y, and qi ∈ {≺, (cid:31)} is the oracle’s
preference in the ith round. The BT model min-
imises the following cross-entropy loss:

LBT(w) = −

(cid:88)

[ µi,1 log Pw(yi,1 (cid:31) yi,2)

qi(yi,1,yi,2)∈Q

+ µi,2 log Pw(yi,2 (cid:31) yi,1) ],

(4)

where Pw(yi (cid:31) yj) = (1 + exp[w(cid:124)(φ(yj) −
φ(yi))])−1, and µi,1 and µi,2 indicate the direc-
tion of preferences: if yi,1 (cid:31) yi,2 then µi,1 = 1
and µi,2 = 0. Let w∗ = arg minw LBT(w),
then w∗ can be used to rank all items in Y: for
any yi, yj ∈ Y, the ranker prefers yi over yj if
w∗(cid:124)φ(yi) > w∗(cid:124)φ(yj).

4 APRIL: Decomposing SPPI into Active

Preference Learning and RL

A major problem of SPPI is its high sample com-
plexity. We believe this is due to two reasons.
First, SPPI’s sampling strategy is inefﬁcient: From
Eq. (2) we can see that SPPI tends to select pairs
with large quality gaps for querying the user. This

strategy can quickly identify the relatively good
and relatively bad summaries, but needs many
rounds of interaction to ﬁnd the top summaries.
Second, SPPI uses the collected preferences inef-
fectively: In Alg. 1, each preference is used only
once for performing the gradient descent update
and is forgotten afterwards. SPPI does not gener-
alise or re-use collected preferences, wasting the
useful and expensive information.

These two weaknesses of SPPI motivate us to
propose a new learning paradigm that can query
and generalise preferences more efﬁciently. Re-
call that in EMDS, the goal is to ﬁnd the optimal
summary for a given document cluster x, namely
the summary that is preferred over all other pos-
sible summaries in Y(x). Based on this under-
standing, we deﬁne a new expected reward func-
tion RAPRIL for policy π as follows:

1
|Y(x)|

(cid:88)

yi∈Y(x)
(cid:88)

RAPRIL(π|x) = Eyj ∼π[

∆x(yi, yj)]

1
|Y(x)|

(cid:88)

=

=

y∈Yπ(x)

(cid:88)

π(yj|x)

∆x(yi, yj)

yj ∈Yπ(x)

yi∈Y(x)

π(y|x) r(y|x),

(5)

where r(y|x) = (cid:80)
yi∈Y(x) ∆x(yi, yj)/|Y(x)|.
Note that ∆x(yi, yj) equals 1 if yj is preferred
over yi and equals 0 otherwise (see §3.1). Thus,
r(y|x) is the relative position of y in the (ascend-
ing) sorted Y(x), and it can be approximated by
preference learning. The use of preference learn-
ing enables us to generalise the observed prefer-
ences to a ranker (see §3.3), allowing more ef-
fective use of the collected preferences. Also, we
can use active learning to select summary pairs for
querying more effectively. In addition, the resem-
blance of RAPRIL and RL’s reward function RRL
(in Eq. (3)) enables us to use a wide range of RL
algorithms to maximise RAPRIL (see §2).

Based on the new objective function, we split
the preference-based interactive learning into two
an Active Preference Learning (APL)
phases:
phase (the right cycle in Fig. 1b), responsible for
querying preferences from the oracle and approxi-
mating the ranking of summaries, and an RL phase
(the left cycle in Fig. 1b), responsible for learning
to summarise based on the learned ranking. The
resulting framework APRIL allows for integrating
any active preference learning and RL techniques.
Note that only the APL phase is online (i.e. in-

Input

: query budget T ; document cluster x;

RL episode budget N
/* Phase 1: active preference learning */
while t = 0 . . . T do

sample a summary pair (yi, yj) using any
APL strategy;
obtain feedback ∆x(yi, yj);
update ranker according to Eq. (4) ;

end
/* Phase 2: RL-based summarisation */
initialise an arbitrary policy π0;
while n = 0 . . . N do

evaluate policy πn according to Eq. (5);
update policy πn using any RL algorithm;

end
Output: y∗ = arg maxy∈YπN (x) πN (y|x)
Algorithm 2: Pseudo code of APRIL

Dataset

Lang

# Topic

# Doc

# Token/Doc

DUC ’01
DUC ’02
DUC ’04

EN
EN
EN

30
59
50

308
567
500

781
561
587

Table 1: Statistics of the datasets. The target sum-
mary length is 100 tokens in all three datasets.

volving humans in the loop) while the RL phase
can be performed ofﬂine, helping to improve the
real-time responsiveness. Also, the learned ranker
can provide an unlimited number of rewards (i.e.
r(y|x) in Eq. (5)) to the RL agent, enabling us to
perform many episodes of RL training with a small
number of collected preferences – unlike in SPPI
where each collected preference is used to train the
system for one round and is forgotten afterwards.
Alg. 2 shows APRIL in pseudo code.

5 Experimental Setup

Datasets. We perform experiments on DUC ’04
to ﬁnd the best performing APL and RL tech-
niques. Then we combine the best-performing
APL and RL to complete APRIL and compare
it against SPPI on the DUC ’01, DUC ’02 and
DUC ’04 datasets.1
Some statistics of these
datasets are summarised in Table 1.

Simulated Users. Existing preference-based in-
teractive learning techniques assume that the or-
acle has an intrinsic evaluation function U ∗ and
provides preferences consistent with U ∗ by prefer-
ring higher valued candidates. We term this a Per-

1http://duc.nist.gov/

fect Oracle (PO). We believe that assuming a PO
is unrealistic for real-world applications, because
sometimes real users tend to misjudge the prefer-
ence direction, especially when the presented can-
didates have similar quality. In this work, besides
PO, we additionally consider two types of noisy
oracles based on the user-response models pro-
posed by Viappiani and Boutilier (2010):

• Constant noisy oracle (CNO): with prob-
ability c ∈ [0, 1], this oracle randomly se-
lects which summary is preferred; otherwise
it provides preferences consistent with U ∗.
We consider CNOs with c = 0.1 and c = 0.3.

in cluster x,

• Logistic noisy oracle (LNO): for two sum-
maries yi and yj
the or-
acle prefers yi over yj with probability
pU ∗(yi (cid:31) yj|x; m) = (1 + exp[(U ∗(yj|x) −
U ∗(yi|x))/m])−1. This oracle reﬂects the in-
tuition that users are more likely to misjudge
the preference direction when two summaries
have similar quality. Note that the parame-
ter m ∈ R+ controls the “noisiness” of the
user’s responses: higher values of m result
in a less steep sigmoid curve, and the result-
ing oracle is more likely to misjudge. We use
LNOs with m = 0.3 and m = 1.

As for the intrinsic evaluation function U ∗, re-
cent work has suggested that human preferences
over summaries have high correlations to ROUGE
scores (Zopf, 2018). Therefore, we deﬁne:

U ∗(y|x) =

R1(y|x)
0.47

+

R2(y|x)
0.22

+

RS(y|x)
0.18

(6)

where R1, R2 and RS stand for ROUGE-1,
ROUGE-2 and ROUGE-SU4, respectively. The
real values (0.47, 0.22 and 0.18) are used to bal-
ance the weights of the three ROUGE scores. We
choose them to be around the EMDS upper-bound
ROUGE scores reported by P.V.S. and Meyer
(2017). As such, an optimal summary’s U ∗ value
should be around 3.

Implementation. All code is written in Python
and runs on a desktop PC with 8 GB RAM and an
i7-2600 CPU. We use NLTK (Bird et al., 2009) to
perform sentence tokenisation. Our source code
is freely available at https://github.com/
UKPLab/emnlp2018-april.

6 Simulation Results

We ﬁrst study the APL phase (§6.1) and the RL
phase (§6.2)) separately by comparing the perfor-

mance of multiple APL and RL algorithms in each
phase. Then, in §6.3, we combine the best per-
forming APL and RL algorithm to complete Alg.
2 and compare APRIL against SPPI.

6.1 APL Phase Performance

Recall that the task of APL is to output a ranking
of all summaries in a cluster. In this subsection,
we test multiple APL techniques and compare the
quality of their resulting rankings. Two metrics are
used: Kendall’s τ (Kendall, 1948) and Spearman’s
ρ (Spearman, 1904). Both metrics are valued be-
tween −1 and 1, with higher values suggesting
higher rank correlation. Because the number of
possible summaries in a cluster is huge, instead of
evaluating the ranking quality on all possible sum-
maries, we evaluate rankings on 10,000 randomly
sampled summaries, denoted ˆY(x). During query-
ing, all candidate summaries presented to the ora-
cle are also selected from ˆY(x). Sampling ˆY(x) a
priori helps us to reduce the response time to un-
der 500 ms for all APL techniques we test. We
compare four active learning strategies under two
query budgets, T = 10 and T = 100:

• Random Sampling (RND): Randomly se-
lect two summaries from ˆY(x) to query.

• SPPI Sampling (SBT): Select summary
pairs from ˆY(x) according to the SPPI strat-
egy in Eq. (2). After each round, the weight
vector w is updated according to Eq. (4).

• Uncertainty Sampling (Unc): Query the
most uncertain summary pairs. In line with
P.V.S. and Meyer (2017), the uncertainty of
a summary is evaluated as follows: ﬁrst,
we estimate the probability of a summary
y being the optimal summary in cluster x
as popt(y|x) = (1 + exp(−w∗(cid:124)
t φ(x, y)))−1,
where w∗
t is the weights learned by the BT
model (see §3.3) in round t. Given popt(y|x),
we let the uncertainty score unc(y|x) = 1 −
popt(y|x) if popt(y|x) ≥ 0.5 and unc(y|x) =
popt(y|x) otherwise.

• J&N is the robust query selection algorithm
proposed by Jamieson and Nowak (2011). It
assumes that the items’ preferences are de-
pendent on their distances to an unknown ref-
erence point in the embedding space: the far-
ther an item to the reference point, the more
preferred the item is. After each round of
interaction, the algorithm uses all collected

preferences to locate the area where the ref-
erence point may fall into, and identify the
query pairs which can reduce the size of this
area, termed ambiguous query pairs. To com-
bat noise in preferences, the algorithm se-
lects the most-likely-correct ambiguous pair
to query the oracle in each round.

After all preferences are collected, we obtain
the ranker as follows: for any yi, yj ∈ Y(x), the
ranker prefers yi over yj if

αw∗(cid:124)φ(yi|x) + (1 − α)HU (yi|x) >
αw∗(cid:124)φ(yj|x) + (1 − α)HU (yj|x),

(7)

where w∗ is the weights vector learned by the BT
model (see Eq. (4)), HU is the heuristics-based
summary evaluation function proposed by Ryang
and Abekawa (2012), and α ∈ [0, 1] is a param-
eter. The aim of using HU and α is to trade off
between the prior knowledge (i.e. heuristics-based
HU ) and the posterior observation (i.e. the BT-
learnt w∗), so as to combat the cold-start problem.
Based on some preliminary experiments, we set
α = 0.3 when the query budget is 10, and α = 0.7
when the query budget is 100. The intuition is to
put more weight to the posterior with increasing
rounds of interaction. More systematic research
of α can yield better results; we leave it for future
work. For the vector φ(y|x), we use the same bag-
of-bigram embeddings as Rioux et al. (2014), and
we let its length be 200.

In Table 2, we compare the performance of the
four APL methods on the DUC’04 dataset. The
baseline we compared against is the prior rank-
ing. We ﬁnd that Unc signiﬁcantly2 outperforms
all other APL methods, except when the oracle
is LNO-1, where the advantage of Unc to SBT
is not signiﬁcant. Also, both Unc and SBT are
able to signiﬁcantly outperform the baseline un-
der all settings. The competitive performance of
SBT, especially with LNO-1, is due to its unique
sampling strategy: LNO-1 is more likely to mis-
judge the preference direction when the presented
summaries have similar quality, but SBT has high
probability to present summaries with large qual-
ity gaps (see Eq. (2)), effectively reducing the
chance that LNOs misjudge preference directions.
However, SBT is more “conservative” compared
to Unc because it tends to exploit the existing

2In this paper we use double-tailed student t-test to com-

pute p-values, and we let signiﬁcance level be p < 0.01.

Oracle

RND
ρ

τ

SBT
ρ

τ

Unc
ρ

τ

J&N
ρ

τ

Query budget T = 10, α = 0.3:
.211 .310 .241 .353 .253∗ .370∗
PO
CNO-0.1 .208 .307 .231 .339 .240∗ .351∗
CNO-0.3 .210 .309 .218 .320 .229∗ .337∗
LNO-0.3 .210 .309 .216 .318 .231∗ .339∗
.310
LNO-1

.206 .303 .210 .308

.211

Query budget T = 100, α = 0.7:
.258 .377 .340 .490 .418∗ .587∗
PO
CNO-0.1 .248 .363 .317 .459 .386∗ .549∗
CNO-0.3 .212 .312 .271 .396 .330∗ .476∗
LNO-0.3 .231 .339 .277 .404 .324∗ .467∗
.331
LNO-1

.210 .309 .225 .330

.225

Baseline, α = 0, T = 0: τ = .206, ρ = .304

.217 .319
.211 .311
.205 .302
.209 .307
.207 .305

.255 .372
.247 .362
.232 .340
.229 .336
.213 .313

Table 2: Performance of multiple APL algorithms
(columns) using different oracles and query bud-
gets (rows). The baseline is the purely prior rank-
ing. All results except the baseline are averaged
over 50 document clusters in DUC’04. Aster-
isk: signiﬁcant advantage over other active learn-
ing strategies given the same oracle and budget T .

ranking to select one good and one bad summary
to query, while Unc performs more exploration by
querying the summaries that are least conﬁdent ac-
cording to the current ranking. We believe this ex-
plains the strong overall performance of Unc.

Additional experiments suggest that when we
only use the posterior ranking (i.e. letting α = 1),
no APL we test can surpass the baseline when
T = 10. Detailed results are presented in the sup-
plementary material. This observation reﬂects the
severity of the cold-start problem, conﬁrms the ef-
fectiveness of our prior-posterior trade-off mecha-
nism in combating cold-start, and indicates the im-
portance of tuning the α value (see Eq. (7)). This
opens up exciting avenues for future work.

6.2 RL Phase Performance

We compare two RL algorithms: TD(λ) (Sut-
ton, 1984) and LSTD(λ) (Boyan, 1999). TD(λ)
has been used in previous RL-based EMDS work
(Ryang and Abekawa, 2012; Rioux et al., 2014).
LSTD(λ) is chosen, because it is an improved TD
algorithm and has been used in the state-of-the-art
PbRL algorithm by Wirth et al. (2016). We let the
learning round (see Alg. 2) N = 5, 000, which we
found to yield good results in reasonable time (less
than 1 minute to generate a summary for one doc-
ument cluster). Letting N = 3, 000 will result in a
signiﬁcant performance drop, while increasing N
to 10,000 will only bring marginal improvement
at the cost of doubling the runtime. The learn-

Method

TD(λ)
LSTD(λ)
ILP

R1

.484
.458
.470

R2

.184
.159
.212

RL

RSU 4

.388
.366
N/A

.199
.185
.185

Table 3: Upper-bound performance comparison.
Results are averaged over all clusters in DUC’04.

ing parameters we use for TD(λ) are the same as
those by Rioux et al. (2014). For LSTD(λ), we let
λ = 1 and initialise its square matrix as a diag-
onal matrix with random numbers between 0 and
1, as suggested by Lagoudakis and Parr (2003).
The rewards we use are the U ∗ function introduced
in §5. Note that this serves as the upper-bound
performance, because U ∗ relies on the reference
summaries (see Eq. (6)), which are not available
in the interactive setting. As a baseline, we also
present the upper-bound performance of integer
linear programming (ILP) reported by P.V.S. and
Meyer (2017), optimised for bigram coverage.

Table 3 shows the performance of RL and ILP
on the DUC’04 dataset. TD(λ) signiﬁcantly out-
performs LSTD(λ) in terms of all ROUGE scores
we consider. Although the least-square RL algo-
rithms (which LSTD belongs to) have been proved
to achieve better performance than standard TD
methods in large-scale problems (see Lagoudakis
and Parr, 2003), their performance is sensitive to
many factors, e.g., initialisation values in the di-
agonal matrix, regularisation parameters, etc. We
note that a similar observation about the inferior
performance of least-square RL in EMDS is re-
ported by Rioux et al. (2014).

TD(λ) also signiﬁcantly outperforms ILP in
terms of all metrics except ROUGE-2. This is not
surprising, because the bigram-based ILP is opti-
mised for ROUGE-2, whereas our reward function
U ∗ considers other metrics as well (see Eq. (6)).
Since ILP is widely used as a strong baseline for
EMDS, these results conﬁrm the advantage of us-
ing RL for EMDS problems.

6.3 Complete Pipeline Performance

Finally, we combine the best techniques of the
APL and RL phase (namely Unc and TD(λ), re-
spectively) to complete APRIL, and compare it
against SPPI. As a baseline, we use the heuristic-
based rewards HU to train both TD(λ) (ranking-
based training, i.e. using HU to produce r(y|x) in
Eq. (5) to train) and SPPI (preference-based train-
ing, i.e. using HU for generating pairs to train

Oracle

Method

T

R1

R2

RSU 4

R1

R2

RSU 4

R1

R2

RL

RSU 4

DUC ’01
RL

DUC ’02
RL

DUC ’04

PO

CNO-0.1

CNO-0.3

LNO-0.3

LNO-1

Baselines

SPPI
APRIL
SPPI
APRIL

SPPI
APRIL
SPPI
APRIL

SPPI
APRIL
SPPI
APRIL

SPPI
APRIL
SPPI
APRIL

SPPI
APRIL
SPPI
APRIL

SPPI
TD(λ)

10
10
100
100

10
10
100
100

10
10
100
100

10
10
100
100

10
10
100
100

0
0

.332
.357
.353
.363

.331
.351
.350
.353

.320†
.339
.345
.349

.319†
.347
.321†
.350

.314†
.337
.320†
.347

.323
.324

.075
.087
.091
.091

.081
.081
.089
.084

.063†
.076
.079
.081

.067†
.084
.068†
.086

.058†
.072
.064†
.080

.068
.069

.264
.283
.284
.283

.265
.276
.279
.280

.253†
.266
.270
.275

.253†
.275
.252†
.277

.250†
.266
.255†
.274

.259
.256

.104
.119
.119
.118

.103
.112
.117
.115

.096†
.108
.111
.109

.096†
.109
.097†
.123

.092†
.104
.097†
.109

.098
.099

.357
.390
.391
.393

.358
.376
.377
.385

.354†
.370
.373
.376

.354†
.370
.352†
.380

.348†
.362
.351†
.369

.350
.350

.083
.108
.104
.107

.081
.102
.100
.103

.080
.091
.094
.097

.083
.095
.080
.079

.076†
.085
.078†
.089

.077
.081

.280†
.306
.306
.310

.279†
.296
.294
.302

.278†
.290
.295
.296

.280†
.289
.278†
.296

.273†
.286
.273†
.286

.278
.276

.116
.133
.136
.137

.114†
.126
.129
.134

.113†
.124
.125
.127

.113†
.125
.112†
.129

.110†
.119
.113†
.123

.112
.113

.378
.410
.392
.415

.372†
.403
.390
.411

.370†
.394
.386
.404

.375†
.398
.387
.407

.373†
.388
.381
.391

.372
.372

.098
.116
.106
.118

.093†
.111
.107
.117

.093†
.104
.104
.114

.095†
.108
.104
.112

.096†
.102
.099
.101

.093
.086

.299
.325
.312
.325

.295†
.320
.309
.325

.129
.149
.140
.151

.125†
.145
.138
.151

.295† 125†
.138
.312
.136
.307
.146
.320

.294†
.311
.309
.321

.297†
.307
.301
.308

.293
.292

.127†
.141
.136
.147

.126†
.134
.132
.136

.125
.122

Table 4: Comparison of APRIL and SPPI. All results are averaged over all clusters in each dataset.
Baselines: HU -trained SPPI and TD(λ), without any interaction (i.e. T = 0). Boldface: Comparable
(i.e. no signiﬁcant gaps exist) or signiﬁcantly better than SPPI with 100 rounds of interaction, under the
same oracle. Superscript †: Comparable or signiﬁcantly worse than the corresponding baseline.

DUC’01

DUC’02

DUC’04

Overall

APRIL
SPPI

3.57±.30
2.29±.29

4.14±.14
2.14±.14

3.86±.40
3.14±.34

3.86±.17
2.52±.18

Table 5: Human ratings for the summaries gener-
ated by APRIL and SPPI (mean±standard error).

SPPI) for up to 5,000 episodes. The baseline re-
sults are presented in the bottom rows of Table 4.

We make the following observations from Ta-
ble 4. (i) Given the same oracle, the performance
of APRIL with 10 rounds of interaction is com-
parable or even superior than that of SPPI after
100 rounds of interaction (see boldface in Table
4), suggesting the strong advantage of APRIL to
(ii) APRIL can sig-
reduce sample complexity.
niﬁcantly improve the baseline with either 10 or
100 rounds of interaction, but SPPI’s performance
can be even worse than the baseline (marked by †
in Table 4), especially under the high-noise low-
budget settings
(i.e., CNO-0.3, LNO-0.3, and
LNO-1 with T = 10). This is because SPPI lacks
a mechanism to balance between prior and poste-
rior ranking, while APRIL can adjust this trade-off

by tuning α (Eq. (7)). This endows APRIL with
better noise robustness and lower sample com-
plexity in high-noise low-budget settings. Note
that the above observations also hold for the other
two datasets,
indicating the consistently strong
performance of APRIL across different datasets.

As for the overall runtime, when budget T =
100, APRIL on average takes 2 minutes to interact
with an oracle and output a summary, while SPPI
takes around 15 minutes due to its expensive gra-
dient descent computation (see §3.1).

7 Human Evaluation

Finally, we invited real users to compare and eval-
uate the quality of the summaries generated by
SPPI and APRIL. We randomly selected three top-
ics (d19 from DUC’01, d117i from DUC’02 and
d30042 from DUC’04), and let both SPPI and our
best-performing APRIL interact with PO for 10
rounds on these topics. The resulting 100-word
summaries, shown in Figure 2, were presented to
seven users, who had already read two background
texts to familiarize with the topic. The users were
asked to provide their preference on the presented

Topic d30042 (DUC’04), SPPI: After meeting Libyan leader Moammar Gad-
haﬁ in a desert tent, U.N. Secretary-General Koﬁ Annan said he thinks an ar-
rangement for bringing two suspects to trial in the bombing of a Pan Am air-
liner could be secured in the “not too distant future.” TRIPOLI, Libya (AP)
U.N. Secretary-General Koﬁ Annan arrived in Libya Saturday for talks aimed
at bringing to trial two Libyan suspects in the 1988 Pan Am bombing over
Lockerbie, Scotland. Secretary General Koﬁ Annan said Wednesday he was
extending his North African tour to include talks with Libyan authorities. An-
nan’s one-day, 2nd graf pvs During his Algerian stay,

Topic d30042 (DUC’04), APRIL: TRIPOLI, Libya (AP) U.N. Secretary-
General Koﬁ Annan arrived in Libya Saturday for talks aimed at bringing to
trial two Libyan suspects in the 1988 Pan Am bombing over Lockerbie, Scot-
land. Annan’s one-day visit to meet with Libyan leader Col. Moammar Gadhaﬁ
followed reports in the Libyan media that Gadhaﬁ had no authority to hand over
the suspects. The 60-year-old Annan is trying to get Libya to go along with a
U.S.-British plan to try the two suspects before a panel of Scottish judges in the
Netherlands for the Dec. 21, 1988, bombing over Lockerbie, Scotland. Sirte
is 400 kilometers (250 miles) east of the Libyan capital Tripoli. During his
Algerian stay,

Topic d117i (DUC’02), SPPI: The Booker Prize is sponsored by Booker, an
international food and agriculture business. The novel, a story of Scottish low-
life narrated largely in Glaswegian dialect, is unlikely to prove a popular choice
with booksellers, who have damned all six books shortlisted for the prize as
boring, elitist and- worst of all- unsaleable. The shortlist of six for the Pounds
20,000 Booker Prize for ﬁction, announced yesterday, immediately prompted
the question ’Who ? ’ Japanese writer Kazuo Ishiguro won the 1989 Booker
Prize, Britain’s top literary award, for his novel “The Remains of the Day,”
judges announced Thursday. He didn’t win.

Topic d117i (DUC’02), APRIL: Australian novelist Peter Carey was awarded
the coveted Booker Prize for ﬁction Tuesday night for his love story, “Oscar
and Lucinda.” The Booker Prize is sponsored by Booker, an international food
and agriculture business, and administered by The Book Trust. British publish-
ers can submit three new novels by British and Commonwealth writers. Six
novels have been nominated for the Booker Prize, Britain’s most prestigious
ﬁction award, and bookmakers say the favorite is “The Remains of the Day” by
Japanese author Kazuo Ishiguro. On the day of the Big Event, Ladbroke, the
large British betting agency, posted the ﬁnal odds.

Topic d19 (DUC’01), SPPI: The issue cuts across partisan lines in the Senate,
with Minority Leader Bob Dole (R-Kan.) arguing against the White House po-
sition on grounds that including illegal aliens in the census is unfair to Amer-
ican citizens.. Loss of Seats Cited. Shelby’s amendment says only that the
secretary is to “make such adjustments in total population ﬁgures as may be
necessary, using such methods and procedures as the secretary determines fea-
sible and appropriate” to keep illegal aliens from being counted in congres-
sional reapportionment. “Some states will lose congressional seats because of
illegal aliens,” Dole argued. But there’s nothing simple about it.

Topic d19 (DUC’01), APRIL: In a blow to California and other states with
large immigrant populations, the Senate voted Friday to bar the Census Bu-
reau from counting illegal aliens in the 1990 population count. But the Senate
already has voted to force the Census Bureau to exclude illegal immigrants in
preparing tallies for congressional reapportionment. said that Georgia and Indi-
ana both lost House seats after the 1980 Census, and California and New York-
centers of illegal immigration- each gained seats. A majority of the members
of the House of Representatives has signaled support. The national head count
will be taken April 1, 1990.

Figure 2: Summaries generated by SPPI and APRIL used in the human evaluation experiments.

summary pairs and rate the summaries on a 5-
point Likert scale with higher scores for better
summaries. All users are ﬂuent in English.

In all three topics, all users prefer the APRIL-
generated summaries over the SPPI-generated
summaries. Table 5 shows the users’ ratings. The
APRIL-generated summaries consistently receive
higher ratings. These results are consistent with
our simulation experiments and conﬁrm the sig-
niﬁcant advantage of APRIL over SPPI.

8 Conclusion

We propose a novel preference-based interactive
learning formulation named APRIL (Active Pref-
erence ReInforcement Learning), which is able to
make structured predictions without referring to
the gold standard data.
Instead, APRIL learns
from preference-based feedback. We designed a
novel objective function for APRIL, which natu-
rally splits APRIL into an active preference learn-
ing (APL) phase and a reinforcement learning
(RL) phase, enabling us to leverage a wide spec-
trum of active learning, preference learning and
RL algorithms to maximise the output quality with
a limited number of interaction rounds. We ap-
plied APRIL to the Extractive Multi-Document
Summarisation (EMDS) problem, simulated the
users’ preference-giving behaviour using multiple
user-response models, and compared the perfor-
mance of multiple APL and RL techniques. Sim-
ulation experiments indicated that APRIL signif-

icantly improved the summary quality with just
10 rounds of interaction (even with high-noise
oracles), and signiﬁcantly outperformed SPPI in
terms of both sample complexity and noise robust-
ness. Human evaluation results suggested that real
users preferred the APRIL-generated summaries
over the SPPI-generated ones.

We identify two major lines of future work. On
the technical side, we plan to employ more ad-
vanced APL and RL algorithms in APRIL, such as
sample-efﬁcient Bayesian-based APL algorithms
(e.g., Simpson and Gurevych, 2018) and neural
RL algorithms (e.g. Mnih et al., 2015) to further
reduce the sample complexity of APRIL. On the
experimental side, a logical next step is to imple-
ment an interactive user interface for APRIL and
conduct a larger evaluation study comparing the
summary quality before and after the interaction.
We also plan to apply APRIL to more NLP appli-
cations, including machine translation, informa-
tion exploration and semantic parsing.

Acknowledgements

This work has been supported by the German
Research Foundation as part of the QA-EduInf
project (grant GU 798/18-1 and grant RI 803/12-
1). We thank the researchers and students from
TU Darmstadt who participated in our human
evaluation experiments. We also thank Johannes
F¨urnkranz, Christian Wirth and the anonymous re-
viewers for their helpful comments.

References

Steven Bird, Ewan Klein,

and Edward Loper.
2009. Natural Language Processing with Python.
O’Reilly.

Justin A. Boyan. 1999. Least-squares temporal differ-
ence learning. In Proceedings of the Sixteenth Inter-
national Conference on Machine Learning (ICML
1999), June 27–30, 1999, Bled, Slovenia, pages 49–
56.

Ralph Allan Bradley and Milton E Terry. 1952. Rank
analysis of incomplete block designs: I. The method
of paired comparisons. Biometrika, 39(3/4):324–
345.

Paul F. Christiano, Jan Leike, Tom Brown, Miljan Mar-
tic, Shane Legg, and Dario Amodei. 2017. Deep
Reinforcement Learning from Human Preferences.
In Advances in Neural Information Processing Sys-
tems 30: 31st Annual Conference on Neural Infor-
mation Processing Systems, December 4–9, 2017,
Long Beach, CA, USA, pages 4302–4310.

Prefer-
Wei Chu and Zoubin Ghahramani. 2005.
In Ma-
ence learning with Gaussian processes.
chine Learning, Proceedings of the Twenty-Second
International Conference (ICML 2005), August 7–
11, 2005, Bonn, Germany, pages 137–144.

Johannes F¨urnkranz and Eyke H¨ullermeier. 2010. Pref-
In Preference
erence learning: An introduction.
Learning, pages 1–17. Berlin/Heidelberg: Springer.

Stefan Henß, Margot Mieskes, and Iryna Gurevych.
2015. A reinforcement learning approach for adap-
tive single- and multi-document summarization. In
Proceedings of the International Conference of the
German Society for Computational Linguistics and
Language Technology (GSCL 2015), September 30–
October 2, 2015, University of Duisburg-Essen,
Germany, pages 3–12.

Kevin G. Jamieson and Robert D. Nowak. 2011. Ac-
In Ad-
tive ranking using pairwise comparisons.
vances in Neural Information Processing Systems
24: 25th Annual Conference on Neural Informa-
tion Processing Systems, December 12–14, 2011,
Granada, Spain, pages 2240–2248.

M.G. Kendall. 1948. Rank correlation methods. C.

Grifﬁn.

David C Kingsley and Thomas C Brown. 2010. Prefer-
ence uncertainty, preference reﬁnement and paired
comparison choice experiments. Land Economics,
86(3):530–544.

Julia Kreutzer, Artem Sokolov, and Stefan Riezler.
Bandit structured prediction for neural
2017.
In Proceedings of
sequence-to-sequence learning.
the 55th Annual Meeting of the Association for
Computational Linguistics (ACL 2017), Volume 1:
Long Papers, July 30–August 4, 2017, Vancouver,
Canada, pages 1503–1513.

Michail G Lagoudakis and Ronald Parr. 2003. Least-
squares policy iteration. Journal of Machine Learn-
ing Research, 4:1107–1149.

Chin-Yew Lin. 2004. ROUGE: A package for auto-
In Workshop on
matic evaluation of summaries.
Text Summarization Branches Out, Post-Conference
Workshop of ACL, July 21–26, 2004, Barcelona,
Spain, pages 74–81.

Lucas Maystre and Matthias Grossglauser. 2017. Just
sort it! A simple and effective approach to ac-
In Proceedings of the
tive preference learning.
34th International Conference on Machine Learning
(ICML 2017), August 6–11, 2017, Sydney, Australia,
pages 2344–2353.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. 2015. Human-level
control through deep reinforcement learning. Na-
ture, 518(7540):529–533.

Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Ranking sentences for extractive summariza-
tion with reinforcement learning. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies (NAACL-HLT
2018),Volume 1 (Long Papers), June 1-6, 2018, New
Orleans, LA, USA, pages 1747–1759.

Ani Nenkova and Kathleen McKeown. 2012. A survey
of text summarization techniques. In Charu C. Ag-
garwal and ChengXiang Zhai, editors, Mining Text
Data, pages 43–76. Boston: Springer.

Ramakanth Pasunuru and Mohit Bansal. 2018. Multi-
reward reinforced summarization with saliency and
entailment. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT 2018), Volume 2 (Short
Papers), June 1-6, 2018, New Orleans, LA, USA,
pages 646–653.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. CoRR, abs/1705.04304.

Joint
Avinesh P.V.S. and Christian M. Meyer. 2017.
optimization of user-desired content
in multi-
document summaries by learning from user feed-
back. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (ACL
2017): Volume 1: Long Paper, July 30–August 4,
2017, Vancouver, Canada, pages 1353–1363.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2015.
Sequence level
training with recurrent neural networks. CoRR,
abs/1511.06732.

Cody Rioux, Sadid A. Hasan, and Yllias Chali. 2014.
Fear the REAPER: A system for automatic multi-
document summarization with reinforcement learn-
In Proceedings of the 2014 Conference on
ing.
Empirical Methods in Natural Language Processing
(EMNLP 2014), October 25–29, 2014, Doha, Qatar,
pages 681–690.

Markus Zopf. 2018. Estimating summary quality with
pairwise preferences. In Proceedings of the 16th An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies (NAACL-HLT 2018),
Volume 1 (Long Papers), June 1–8, 2018, New Or-
leans, LA, USA, pages 1687–1696.

Seonggi Ryang and Takeshi Abekawa. 2012. Frame-
work of automatic text summarization using rein-
In Proceedings of the 2012
forcement learning.
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL 2012), July
12–14, 2012, Jeju Island, Korea, pages 256–265.

Edwin D. Simpson and Iryna Gurevych. 2018. Finding
convincing arguments using scalable bayesian pref-
erence learning. Transactions of the Association for
Computational Linguistics, 6:357–371.

Artem Sokolov, Julia Kreutzer, Christopher Lo, and
Stefan Riezler. 2016a. Learning structured predic-
tors from bandit feedback for interactive NLP.
In
Proceedings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2016):
Volume 1: Long Papers, August 7–12, 2016, Berlin,
Germany, pages 1610–1620.

Artem Sokolov, Julia Kreutzer, Stefan Riezler, and
Christopher Lo. 2016b. Stochastic structured pre-
diction under bandit feedback. In Advances in Neu-
ral Information Processing Systems 29: 30th An-
nual Conference on Neural Information Processing
Systems, December 5–10, 2016, Barcelona, Spain,
pages 1489–1497.

Charles Spearman. 1904. The proof and measurement
of association between two things. The American
Journal of Psychology, 15(1):72–101.

R. S. Sutton. 1984. Temporal Credit Assignment in Re-
inforcement Learning. Ph.D. thesis, University of
Massachusetts, Amherst.

Louis Leon Thurstone. 1927. A Law of Comparative
Judgement. Psychological Review, 34:278–286.

Paolo Viappiani and Craig Boutilier. 2010. Optimal
Bayesian recommendation sets and myopically op-
In Advances in Neural
timal choice query sets.
Information Processing Systems 23: 24th Annual
Conference on Neural Information Processing Sys-
tems, December 6-9 , Vancouver, British Columbia,
Canada, pages 2352–2360.

Christian Wirth, Riad Akrour, Gerhard Neumann, and
Johannes F¨urnkranz. 2017. A survey of preference-
based reinforcement learning methods. Journal of
Machine Learning Research, 18:4945–4990.

Christian Wirth, Johannes F¨urnkranz, and Gerhard
Neumann. 2016. Model-free preference-based rein-
forcement learning. In Proceedings of the Thirtieth
AAAI Conference on Artiﬁcial Intelligence, Febru-
ary 12–17, 2016, Phoenix, AZ, USA, pages 2222–
2228.

APRIL: Interactively Learning to Summarise by Combining
Active Preference Learning and Reinforcement Learning

Yang Gao, Christian M. Meyer, Iryna Gurevych
Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universit¨at Darmstadt
https://www.ukp.tu-darmstadt.de/

8
1
0
2
 
g
u
A
 
9
2
 
 
]
L
C
.
s
c
[
 
 
1
v
8
5
6
9
0
.
8
0
8
1
:
v
i
X
r
a

Abstract

We propose a method to perform automatic
document summarisation without using refer-
Instead, our method inter-
ence summaries.
actively learns from users’ preferences. The
merit of preference-based interactive sum-
marisation is that preferences are easier for
users to provide than reference summaries.
Existing preference-based interactive learning
methods suffer from high sample complex-
ity,
i.e. they need to interact with the ora-
cle for many rounds in order to converge.
In this work, we propose a new objective
function, which enables us to leverage ac-
tive learning, preference learning and rein-
forcement learning techniques in order to re-
duce the sample complexity. Both simula-
tion and real-user experiments suggest that
our method signiﬁcantly advances the state
of the art. Our source code is freely avail-
able at https://github.com/UKPLab/
emnlp2018-april.

1

Introduction

With the rapid growth of text-based information
on the Internet, automatic document summarisa-
tion attracts increasing research attention from the
Natural Language Processing (NLP) community
(Nenkova and McKeown, 2012). Most existing
document summarisation techniques require ac-
cess to reference summaries to train their systems.
However, obtaining reference summaries is very
expensive: Lin (2004) reported that 3,000 hours
of human effort were required for a simple evalu-
ation of the summaries for the Document Under-
standing Conferences (DUC). Although previous
work has proposed heuristics-based methods to
summarise without reference summaries (Ryang
and Abekawa, 2012; Rioux et al., 2014), the gap
between their performance and the upper bound is
still large: the ROUGE-2 upper bound of .212 on

DUC’04 (P.V.S. and Meyer, 2017) is, for example,
twice as high as Rioux et al.’s (2014) .114.

The Structured Prediction from Partial Infor-
mation (SPPI) framework has been proposed to
learn to make structured predictions without ac-
cess to gold standard data (Sokolov et al., 2016b).
SPPI is an interactive NLP paradigm:
It inter-
acts with a user for multiple rounds and learns
from the user’s feedback. SPPI can learn from
two forms of feedback: point-based feedback, i.e.
a numeric score for the presented prediction, or
preference-based feedback, i.e. a preference over
a pair of predictions. Providing preference-based
feedback yields a lower cognitive burden for hu-
mans than providing ratings or categorical labels
(Thurstone, 1927; Kendall, 1948; Kingsley and
Brown, 2010; Zopf, 2018). Preference-based SPPI
has been applied to multiple NLP applications, in-
cluding text classiﬁcation, chunking and machine
translation (Sokolov et al., 2016a; Kreutzer et al.,
2017). However, SPPI has prohibitively high sam-
ple complexities in the aforementioned NLP tasks,
as it needs at least hundreds of thousands rounds of
interaction to make near-optimal predictions, even
with simulated “perfect” users. Figure 1a illus-
trates the workﬂow of the preference-based SPPI.
To reduce the sample complexity, in this work,
we propose a novel preference-based interactive
learning framework, called APRIL (Active Prefer-
ence ReInforcement Learning). APRIL goes be-
yond SPPI by proposing a new objective func-
tion, which divides the preference-based interac-
tive learning problem into two phases (illustrated
in Figure 1b): an Active Preference Learning
(APL) phase (the right cycle in Figure 1b), and
a Reinforcement Learning (RL) phase (the left cy-
cle). We show that this separation enables us to
query preferences more effectively and to use the
collected preferences more efﬁciently, so as to re-
duce the sample complexity.

(a) Workﬂow of preference-based SPPI

(b) Workﬂow of APRIL

Figure 1: A comparison of workﬂows of SPPI (a)
and APRIL (b) in the EMDS use case. Notation
details, e.g., ∆x and r(yn), are discussed in §3.

We apply APRIL to Extractive Multi-Document
Summarisation (EMDS). The task of EMDS is to
extract sentences from the original documents to
build a summary under a length constraint. We
accommodate multiple APL and RL techniques
in APRIL and compare their performance under
different simulation settings. We also compare
APRIL to a state-of-the-art SPPI implementation
using both automatic metrics and human evalua-
tion. Our results suggest that APRIL signiﬁcantly
outperforms SPPI.

2 Related Work

RL has been previously used to perform EMDS
without using reference summaries. Ryang and
Abekawa (2012) formulated EMDS as a Markov
Decision Process (MDP), designed a heuristics-
based reward function considering both informa-
tion coverage rate and redundancy level, and used
the Temporal Difference (TD) algorithm (Sutton,
1984) to solve the MDP. In a follow-up work, Ri-
oux et al. (2014) proposed a different reward func-
tion, which also did not require reference sum-
their experiments suggested that using
maries;
their new reward function improved the summary
quality. Henß et al. (2015) proposed a different
RL formulation of EMDS and jointly used super-
vised learning and RL to perform the task. How-
ever, their method requires the access to reference
summaries. More recent works applied encoder-
decoder-based RL to document summarisation
(Ranzato et al., 2015; Narayan et al., 2018; Paulus
et al., 2017; Pasunuru and Bansal, 2018). These
works outperformed standard encoder-decoder as

RL can directly optimise the ROUGE scores and
can tackle the exposure bias problems. However,
these neural RL methods all used ROUGE scores
as their rewards, which in turn relied on reference
summaries. APRIL can accommodate these neu-
ral RL techniques in its RL phase by using a rank-
ing of summaries instead of the ROUGE scores as
rewards. We leave neural APRIL for future study.

P.V.S. and Meyer (2017) proposed a bigram-
based interactive EMDS framework. They asked
users to label important bigrams in candidate sum-
maries and used integer linear programming (ILP)
to extract sentences covering as many important
bigrams as possible. Their method requires no ac-
cess to reference summaries, but it requires con-
siderable human effort during the interaction: in
simulation experiments, their system needed to
collect up to 350 bigram annotations from a (simu-
lated) user. In addition, they did not consider noise
in users’ annotations but simulated perfect oracles.

Preference learning aims at obtaining the rank-
ing (i.e. total ordering) of objects from pairwise
preferences (F¨urnkranz and H¨ullermeier, 2010).
Simpson and Gurevych (2018) proposed to use an
improved Gaussian process preference learning
(Chu and Ghahramani, 2005) for learning to rank
arguments in terms of convincingness from crowd-
sourced annotations. However, such Bayesian
methods can hardly scale and suffer from high
computation time. Zopf (2018) recently proposed
to learn a sentence ranker from preferences. The
resulting ranker can be used to identify the impor-
tant sentences and thus to evaluate the quality of
the summaries. His study also suggests that pro-
viding sentence preferences takes less time than
writing reference summaries. APRIL not only
learns a ranking over summaries from pairwise
preferences, but also uses the ranking to “guide”
our RL agent to generate good summaries.

There is a recent trend in machine learning to
combine active learning, preference learning and
RL, for learning to perform complex tasks from
preferences (Wirth et al., 2017). The resulting al-
gorithm is termed Preference-based RL (PbRL),
and has been used in multiple applications, includ-
ing training robots (Wirth et al., 2016) and Atari-
playing agents (Christiano et al., 2017). SPPI and
APRIL can both be viewed as PbRL algorithms.
But unlike most PbRL methods that learn a utility
function of the predictions (in EMDS, predictions
are summaries) to guide the RL agent, APRIL

is able to directly use a ranking of predictions
to guide the RL agent without making assump-
tions about the underlying structure of the utility
functions. This also enables APRIL to use non-
utility-based preference learning techniques (e.g.,
Maystre and Grossglauser, 2017).

3 Background

In this section, we recap necessary details of SPPI,
RL and preference learning, and adapt them to the
EMDS use case, laying the foundation for APRIL.

3.1 The SPPI Framework

Let X be the input space and let Y(x) be the set
of possible outputs for input x ∈ X . In EMDS,
x ∈ X is a cluster of documents and Y(x) is the
set of all possible summaries for cluster x. The
function ∆x : Y(x)×Y(x) → {0, 1} is the prefer-
ence function such that ∆x(yi, yj) = 1 if the user
believes yj is better than yi (denoted by yj (cid:31) yi or
equivalently yi ≺ yj), and 0 otherwise. Through-
out this paper we assume that users do not equally
prefer two different items. For a given x, the ex-
pected loss is:

LSPPI(w|x) = Epw(yi,yj |x)[∆x(yi, yj)]
=
∆x(yi, yj) pw(yi, yj|x),

(cid:88)

(1)

yi,yj ∈Y(x)

where pw(yi, yj|x) is the probability of querying
the pair (yi, yj). Formally,

pw(yi, yj|x)

=

exp[w(cid:124)(φ(yi|x) − φ(yj|x))]
(cid:80)

exp[w(cid:124)(φ(yp|x) − φ(yq|x))]

,

(2)

yp,yq∈Y(x)

where φ(y|x) is the vector representation of y
given x, and w is the weight vector to be learnt.
Eq. (2) is a Gibbs sampling strategy: w(cid:124)(φ(yi|x)−
φ(yj|x)) can be viewed as the “utility gap” be-
tween yi and yj. The sampling strategy pw en-
courages querying pairs with large utility gaps.

To minimise LSPPI, SPPI uses gradient descent
to update w incrementally. Alg. 1 presents the
pseudo code of our adaptation of SPPI to EMDS.
In the supplementary material, we provide a de-
tailed derivation of ∇wLSPPI(w|x).

3.2 Reinforcement Learning

RL amounts to efﬁcient algorithms for searching
optimal solutions in MDPs. MDPs are widely

Input

: sequence of learning rates γt; query

budget T ; document cluster x

initialise w0;
while t = 0 . . . T do

sample (yi, yj) according to Eq. (2);
obtain feedback ∆x(yi, yj);
wt+1 := wt − γt∇wLSPPI(w|x)

end
(cid:124)
Output: y∗ = arg maxy∈Y (x) w
T +1φ(y, x)
Algorithm 1: SPPI for preference-based in-
teractive document summarisation (adjusted
from Alg. 2 in (Sokolov et al., 2016a)).

used to formulate sequential decision making
problems, which EMDS falls into: in EMDS, the
summariser has to sequentially select sentences
from the original documents and add them to the
draft summary. An (episodic) MDP is a tuple
(S, A, P, R, T ). S is the set of states, A is the set
of actions, P : S × A × S → R is the transition
function with P (s(cid:48)|s, a) yielding the probability of
performing action a in state s and being transited
to a new state s(cid:48). R : S × A → R is the reward
function with R(s, a) giving the immediate reward
for performing action a in state s. T ⊆ S is the set
of terminal states; visiting a terminal state termi-
nates the current episode.

In EMDS, we follow the same MDP formu-
lation as Ryang and Abekawa (2012) and Rioux
et al. (2014). Given a document cluster, a state s
is a draft summary, A includes two types of ac-
tions, concatenate a new sentence to the current
draft summary, or terminate the draft summary
construction. The transition function P in EMDS
is trivial because given the current draft summary
and an action, the next state can be easily inferred.
The reward function R returns an evaluation score
of the summary once the action terminate is per-
formed; otherwise it returns 0 because the sum-
mary is still under construction and thus not ready
to be evaluated. Providing non-zero rewards be-
fore the action terminate can lead to even worse
result, as reported by Rioux et al. (2014).

A policy π : S × A → R in an MDP deﬁnes
how actions are selected: π(s, a) is the probability
of selecting action a in state s. In EMDS, a policy
corresponds to a strategy to build summaries for a
given document cluster. We let Yπ(x) be the set of
all possible summaries the policy π can construct
in the document cluster x, and we slightly abuse
the notation by letting π(y|x) denote the probabil-

ity of policy π generating a summary y in cluster
x. Then the expected reward of a policy is:

RRL(π|x) = Ey∈Yπ(x)R(y|x)

(cid:88)

=

y∈Yπ(x)

π(y|x)R(y|x),

(3)

where R(y|x) is the reward for summary y in doc-
ument cluster x. The goal of an MDP is to ﬁnd the
optimal policy π∗ that has the highest expected re-
ward: π∗ = arg maxπ RRL(π).

Note that the loss function in SPPI (Eq. (1)) and
the expected reward function in RL (Eq. (3)) are in
similar forms: if we view the pair selection proba-
bility pw in Eq. (2) as a policy, and view the pref-
erence function ∆x in Eq. (1) as a negative reward
function, we can view SPPI as an RL problem.
The major difference between SPPI and RL is that
SPPI selects and evaluates pairs of outputs, while
RL selects and evaluates single outputs. We will
exploit their connection to propose our new objec-
tive function and the APRIL framework.

3.3 Preference Learning

The linear Bradley-Terry (BT) model (Bradley
and Terry, 1952) is one of the most widely used
methods in preference learning. Given a set of
items Y, suppose we have observed T preferences:
Q = {q1(y1,1, y1,2), · · · , qT (yT,1, yT,2)}, where
yi,1, yi,2 ∈ Y, and qi ∈ {≺, (cid:31)} is the oracle’s
preference in the ith round. The BT model min-
imises the following cross-entropy loss:

LBT(w) = −

(cid:88)

[ µi,1 log Pw(yi,1 (cid:31) yi,2)

qi(yi,1,yi,2)∈Q

+ µi,2 log Pw(yi,2 (cid:31) yi,1) ],

(4)

where Pw(yi (cid:31) yj) = (1 + exp[w(cid:124)(φ(yj) −
φ(yi))])−1, and µi,1 and µi,2 indicate the direc-
tion of preferences: if yi,1 (cid:31) yi,2 then µi,1 = 1
and µi,2 = 0. Let w∗ = arg minw LBT(w),
then w∗ can be used to rank all items in Y: for
any yi, yj ∈ Y, the ranker prefers yi over yj if
w∗(cid:124)φ(yi) > w∗(cid:124)φ(yj).

4 APRIL: Decomposing SPPI into Active

Preference Learning and RL

A major problem of SPPI is its high sample com-
plexity. We believe this is due to two reasons.
First, SPPI’s sampling strategy is inefﬁcient: From
Eq. (2) we can see that SPPI tends to select pairs
with large quality gaps for querying the user. This

strategy can quickly identify the relatively good
and relatively bad summaries, but needs many
rounds of interaction to ﬁnd the top summaries.
Second, SPPI uses the collected preferences inef-
fectively: In Alg. 1, each preference is used only
once for performing the gradient descent update
and is forgotten afterwards. SPPI does not gener-
alise or re-use collected preferences, wasting the
useful and expensive information.

These two weaknesses of SPPI motivate us to
propose a new learning paradigm that can query
and generalise preferences more efﬁciently. Re-
call that in EMDS, the goal is to ﬁnd the optimal
summary for a given document cluster x, namely
the summary that is preferred over all other pos-
sible summaries in Y(x). Based on this under-
standing, we deﬁne a new expected reward func-
tion RAPRIL for policy π as follows:

1
|Y(x)|

(cid:88)

yi∈Y(x)
(cid:88)

RAPRIL(π|x) = Eyj ∼π[

∆x(yi, yj)]

1
|Y(x)|

(cid:88)

=

=

y∈Yπ(x)

(cid:88)

π(yj|x)

∆x(yi, yj)

yj ∈Yπ(x)

yi∈Y(x)

π(y|x) r(y|x),

(5)

where r(y|x) = (cid:80)
yi∈Y(x) ∆x(yi, yj)/|Y(x)|.
Note that ∆x(yi, yj) equals 1 if yj is preferred
over yi and equals 0 otherwise (see §3.1). Thus,
r(y|x) is the relative position of y in the (ascend-
ing) sorted Y(x), and it can be approximated by
preference learning. The use of preference learn-
ing enables us to generalise the observed prefer-
ences to a ranker (see §3.3), allowing more ef-
fective use of the collected preferences. Also, we
can use active learning to select summary pairs for
querying more effectively. In addition, the resem-
blance of RAPRIL and RL’s reward function RRL
(in Eq. (3)) enables us to use a wide range of RL
algorithms to maximise RAPRIL (see §2).

Based on the new objective function, we split
the preference-based interactive learning into two
an Active Preference Learning (APL)
phases:
phase (the right cycle in Fig. 1b), responsible for
querying preferences from the oracle and approxi-
mating the ranking of summaries, and an RL phase
(the left cycle in Fig. 1b), responsible for learning
to summarise based on the learned ranking. The
resulting framework APRIL allows for integrating
any active preference learning and RL techniques.
Note that only the APL phase is online (i.e. in-

Input

: query budget T ; document cluster x;

RL episode budget N
/* Phase 1: active preference learning */
while t = 0 . . . T do

sample a summary pair (yi, yj) using any
APL strategy;
obtain feedback ∆x(yi, yj);
update ranker according to Eq. (4) ;

end
/* Phase 2: RL-based summarisation */
initialise an arbitrary policy π0;
while n = 0 . . . N do

evaluate policy πn according to Eq. (5);
update policy πn using any RL algorithm;

end
Output: y∗ = arg maxy∈YπN (x) πN (y|x)
Algorithm 2: Pseudo code of APRIL

Dataset

Lang

# Topic

# Doc

# Token/Doc

DUC ’01
DUC ’02
DUC ’04

EN
EN
EN

30
59
50

308
567
500

781
561
587

Table 1: Statistics of the datasets. The target sum-
mary length is 100 tokens in all three datasets.

volving humans in the loop) while the RL phase
can be performed ofﬂine, helping to improve the
real-time responsiveness. Also, the learned ranker
can provide an unlimited number of rewards (i.e.
r(y|x) in Eq. (5)) to the RL agent, enabling us to
perform many episodes of RL training with a small
number of collected preferences – unlike in SPPI
where each collected preference is used to train the
system for one round and is forgotten afterwards.
Alg. 2 shows APRIL in pseudo code.

5 Experimental Setup

Datasets. We perform experiments on DUC ’04
to ﬁnd the best performing APL and RL tech-
niques. Then we combine the best-performing
APL and RL to complete APRIL and compare
it against SPPI on the DUC ’01, DUC ’02 and
DUC ’04 datasets.1
Some statistics of these
datasets are summarised in Table 1.

Simulated Users. Existing preference-based in-
teractive learning techniques assume that the or-
acle has an intrinsic evaluation function U ∗ and
provides preferences consistent with U ∗ by prefer-
ring higher valued candidates. We term this a Per-

1http://duc.nist.gov/

fect Oracle (PO). We believe that assuming a PO
is unrealistic for real-world applications, because
sometimes real users tend to misjudge the prefer-
ence direction, especially when the presented can-
didates have similar quality. In this work, besides
PO, we additionally consider two types of noisy
oracles based on the user-response models pro-
posed by Viappiani and Boutilier (2010):

• Constant noisy oracle (CNO): with prob-
ability c ∈ [0, 1], this oracle randomly se-
lects which summary is preferred; otherwise
it provides preferences consistent with U ∗.
We consider CNOs with c = 0.1 and c = 0.3.

in cluster x,

• Logistic noisy oracle (LNO): for two sum-
maries yi and yj
the or-
acle prefers yi over yj with probability
pU ∗(yi (cid:31) yj|x; m) = (1 + exp[(U ∗(yj|x) −
U ∗(yi|x))/m])−1. This oracle reﬂects the in-
tuition that users are more likely to misjudge
the preference direction when two summaries
have similar quality. Note that the parame-
ter m ∈ R+ controls the “noisiness” of the
user’s responses: higher values of m result
in a less steep sigmoid curve, and the result-
ing oracle is more likely to misjudge. We use
LNOs with m = 0.3 and m = 1.

As for the intrinsic evaluation function U ∗, re-
cent work has suggested that human preferences
over summaries have high correlations to ROUGE
scores (Zopf, 2018). Therefore, we deﬁne:

U ∗(y|x) =

R1(y|x)
0.47

+

R2(y|x)
0.22

+

RS(y|x)
0.18

(6)

where R1, R2 and RS stand for ROUGE-1,
ROUGE-2 and ROUGE-SU4, respectively. The
real values (0.47, 0.22 and 0.18) are used to bal-
ance the weights of the three ROUGE scores. We
choose them to be around the EMDS upper-bound
ROUGE scores reported by P.V.S. and Meyer
(2017). As such, an optimal summary’s U ∗ value
should be around 3.

Implementation. All code is written in Python
and runs on a desktop PC with 8 GB RAM and an
i7-2600 CPU. We use NLTK (Bird et al., 2009) to
perform sentence tokenisation. Our source code
is freely available at https://github.com/
UKPLab/emnlp2018-april.

6 Simulation Results

We ﬁrst study the APL phase (§6.1) and the RL
phase (§6.2)) separately by comparing the perfor-

mance of multiple APL and RL algorithms in each
phase. Then, in §6.3, we combine the best per-
forming APL and RL algorithm to complete Alg.
2 and compare APRIL against SPPI.

6.1 APL Phase Performance

Recall that the task of APL is to output a ranking
of all summaries in a cluster. In this subsection,
we test multiple APL techniques and compare the
quality of their resulting rankings. Two metrics are
used: Kendall’s τ (Kendall, 1948) and Spearman’s
ρ (Spearman, 1904). Both metrics are valued be-
tween −1 and 1, with higher values suggesting
higher rank correlation. Because the number of
possible summaries in a cluster is huge, instead of
evaluating the ranking quality on all possible sum-
maries, we evaluate rankings on 10,000 randomly
sampled summaries, denoted ˆY(x). During query-
ing, all candidate summaries presented to the ora-
cle are also selected from ˆY(x). Sampling ˆY(x) a
priori helps us to reduce the response time to un-
der 500 ms for all APL techniques we test. We
compare four active learning strategies under two
query budgets, T = 10 and T = 100:

• Random Sampling (RND): Randomly se-
lect two summaries from ˆY(x) to query.

• SPPI Sampling (SBT): Select summary
pairs from ˆY(x) according to the SPPI strat-
egy in Eq. (2). After each round, the weight
vector w is updated according to Eq. (4).

• Uncertainty Sampling (Unc): Query the
most uncertain summary pairs. In line with
P.V.S. and Meyer (2017), the uncertainty of
a summary is evaluated as follows: ﬁrst,
we estimate the probability of a summary
y being the optimal summary in cluster x
as popt(y|x) = (1 + exp(−w∗(cid:124)
t φ(x, y)))−1,
where w∗
t is the weights learned by the BT
model (see §3.3) in round t. Given popt(y|x),
we let the uncertainty score unc(y|x) = 1 −
popt(y|x) if popt(y|x) ≥ 0.5 and unc(y|x) =
popt(y|x) otherwise.

• J&N is the robust query selection algorithm
proposed by Jamieson and Nowak (2011). It
assumes that the items’ preferences are de-
pendent on their distances to an unknown ref-
erence point in the embedding space: the far-
ther an item to the reference point, the more
preferred the item is. After each round of
interaction, the algorithm uses all collected

preferences to locate the area where the ref-
erence point may fall into, and identify the
query pairs which can reduce the size of this
area, termed ambiguous query pairs. To com-
bat noise in preferences, the algorithm se-
lects the most-likely-correct ambiguous pair
to query the oracle in each round.

After all preferences are collected, we obtain
the ranker as follows: for any yi, yj ∈ Y(x), the
ranker prefers yi over yj if

αw∗(cid:124)φ(yi|x) + (1 − α)HU (yi|x) >
αw∗(cid:124)φ(yj|x) + (1 − α)HU (yj|x),

(7)

where w∗ is the weights vector learned by the BT
model (see Eq. (4)), HU is the heuristics-based
summary evaluation function proposed by Ryang
and Abekawa (2012), and α ∈ [0, 1] is a param-
eter. The aim of using HU and α is to trade off
between the prior knowledge (i.e. heuristics-based
HU ) and the posterior observation (i.e. the BT-
learnt w∗), so as to combat the cold-start problem.
Based on some preliminary experiments, we set
α = 0.3 when the query budget is 10, and α = 0.7
when the query budget is 100. The intuition is to
put more weight to the posterior with increasing
rounds of interaction. More systematic research
of α can yield better results; we leave it for future
work. For the vector φ(y|x), we use the same bag-
of-bigram embeddings as Rioux et al. (2014), and
we let its length be 200.

In Table 2, we compare the performance of the
four APL methods on the DUC’04 dataset. The
baseline we compared against is the prior rank-
ing. We ﬁnd that Unc signiﬁcantly2 outperforms
all other APL methods, except when the oracle
is LNO-1, where the advantage of Unc to SBT
is not signiﬁcant. Also, both Unc and SBT are
able to signiﬁcantly outperform the baseline un-
der all settings. The competitive performance of
SBT, especially with LNO-1, is due to its unique
sampling strategy: LNO-1 is more likely to mis-
judge the preference direction when the presented
summaries have similar quality, but SBT has high
probability to present summaries with large qual-
ity gaps (see Eq. (2)), effectively reducing the
chance that LNOs misjudge preference directions.
However, SBT is more “conservative” compared
to Unc because it tends to exploit the existing

2In this paper we use double-tailed student t-test to com-

pute p-values, and we let signiﬁcance level be p < 0.01.

Oracle

RND
ρ

τ

SBT
ρ

τ

Unc
ρ

τ

J&N
ρ

τ

Query budget T = 10, α = 0.3:
.211 .310 .241 .353 .253∗ .370∗
PO
CNO-0.1 .208 .307 .231 .339 .240∗ .351∗
CNO-0.3 .210 .309 .218 .320 .229∗ .337∗
LNO-0.3 .210 .309 .216 .318 .231∗ .339∗
.310
LNO-1

.206 .303 .210 .308

.211

Query budget T = 100, α = 0.7:
.258 .377 .340 .490 .418∗ .587∗
PO
CNO-0.1 .248 .363 .317 .459 .386∗ .549∗
CNO-0.3 .212 .312 .271 .396 .330∗ .476∗
LNO-0.3 .231 .339 .277 .404 .324∗ .467∗
.331
LNO-1

.210 .309 .225 .330

.225

Baseline, α = 0, T = 0: τ = .206, ρ = .304

.217 .319
.211 .311
.205 .302
.209 .307
.207 .305

.255 .372
.247 .362
.232 .340
.229 .336
.213 .313

Table 2: Performance of multiple APL algorithms
(columns) using different oracles and query bud-
gets (rows). The baseline is the purely prior rank-
ing. All results except the baseline are averaged
over 50 document clusters in DUC’04. Aster-
isk: signiﬁcant advantage over other active learn-
ing strategies given the same oracle and budget T .

ranking to select one good and one bad summary
to query, while Unc performs more exploration by
querying the summaries that are least conﬁdent ac-
cording to the current ranking. We believe this ex-
plains the strong overall performance of Unc.

Additional experiments suggest that when we
only use the posterior ranking (i.e. letting α = 1),
no APL we test can surpass the baseline when
T = 10. Detailed results are presented in the sup-
plementary material. This observation reﬂects the
severity of the cold-start problem, conﬁrms the ef-
fectiveness of our prior-posterior trade-off mecha-
nism in combating cold-start, and indicates the im-
portance of tuning the α value (see Eq. (7)). This
opens up exciting avenues for future work.

6.2 RL Phase Performance

We compare two RL algorithms: TD(λ) (Sut-
ton, 1984) and LSTD(λ) (Boyan, 1999). TD(λ)
has been used in previous RL-based EMDS work
(Ryang and Abekawa, 2012; Rioux et al., 2014).
LSTD(λ) is chosen, because it is an improved TD
algorithm and has been used in the state-of-the-art
PbRL algorithm by Wirth et al. (2016). We let the
learning round (see Alg. 2) N = 5, 000, which we
found to yield good results in reasonable time (less
than 1 minute to generate a summary for one doc-
ument cluster). Letting N = 3, 000 will result in a
signiﬁcant performance drop, while increasing N
to 10,000 will only bring marginal improvement
at the cost of doubling the runtime. The learn-

Method

TD(λ)
LSTD(λ)
ILP

R1

.484
.458
.470

R2

.184
.159
.212

RL

RSU 4

.388
.366
N/A

.199
.185
.185

Table 3: Upper-bound performance comparison.
Results are averaged over all clusters in DUC’04.

ing parameters we use for TD(λ) are the same as
those by Rioux et al. (2014). For LSTD(λ), we let
λ = 1 and initialise its square matrix as a diag-
onal matrix with random numbers between 0 and
1, as suggested by Lagoudakis and Parr (2003).
The rewards we use are the U ∗ function introduced
in §5. Note that this serves as the upper-bound
performance, because U ∗ relies on the reference
summaries (see Eq. (6)), which are not available
in the interactive setting. As a baseline, we also
present the upper-bound performance of integer
linear programming (ILP) reported by P.V.S. and
Meyer (2017), optimised for bigram coverage.

Table 3 shows the performance of RL and ILP
on the DUC’04 dataset. TD(λ) signiﬁcantly out-
performs LSTD(λ) in terms of all ROUGE scores
we consider. Although the least-square RL algo-
rithms (which LSTD belongs to) have been proved
to achieve better performance than standard TD
methods in large-scale problems (see Lagoudakis
and Parr, 2003), their performance is sensitive to
many factors, e.g., initialisation values in the di-
agonal matrix, regularisation parameters, etc. We
note that a similar observation about the inferior
performance of least-square RL in EMDS is re-
ported by Rioux et al. (2014).

TD(λ) also signiﬁcantly outperforms ILP in
terms of all metrics except ROUGE-2. This is not
surprising, because the bigram-based ILP is opti-
mised for ROUGE-2, whereas our reward function
U ∗ considers other metrics as well (see Eq. (6)).
Since ILP is widely used as a strong baseline for
EMDS, these results conﬁrm the advantage of us-
ing RL for EMDS problems.

6.3 Complete Pipeline Performance

Finally, we combine the best techniques of the
APL and RL phase (namely Unc and TD(λ), re-
spectively) to complete APRIL, and compare it
against SPPI. As a baseline, we use the heuristic-
based rewards HU to train both TD(λ) (ranking-
based training, i.e. using HU to produce r(y|x) in
Eq. (5) to train) and SPPI (preference-based train-
ing, i.e. using HU for generating pairs to train

Oracle

Method

T

R1

R2

RSU 4

R1

R2

RSU 4

R1

R2

RL

RSU 4

DUC ’01
RL

DUC ’02
RL

DUC ’04

PO

CNO-0.1

CNO-0.3

LNO-0.3

LNO-1

Baselines

SPPI
APRIL
SPPI
APRIL

SPPI
APRIL
SPPI
APRIL

SPPI
APRIL
SPPI
APRIL

SPPI
APRIL
SPPI
APRIL

SPPI
APRIL
SPPI
APRIL

SPPI
TD(λ)

10
10
100
100

10
10
100
100

10
10
100
100

10
10
100
100

10
10
100
100

0
0

.332
.357
.353
.363

.331
.351
.350
.353

.320†
.339
.345
.349

.319†
.347
.321†
.350

.314†
.337
.320†
.347

.323
.324

.075
.087
.091
.091

.081
.081
.089
.084

.063†
.076
.079
.081

.067†
.084
.068†
.086

.058†
.072
.064†
.080

.068
.069

.264
.283
.284
.283

.265
.276
.279
.280

.253†
.266
.270
.275

.253†
.275
.252†
.277

.250†
.266
.255†
.274

.259
.256

.104
.119
.119
.118

.103
.112
.117
.115

.096†
.108
.111
.109

.096†
.109
.097†
.123

.092†
.104
.097†
.109

.098
.099

.357
.390
.391
.393

.358
.376
.377
.385

.354†
.370
.373
.376

.354†
.370
.352†
.380

.348†
.362
.351†
.369

.350
.350

.083
.108
.104
.107

.081
.102
.100
.103

.080
.091
.094
.097

.083
.095
.080
.079

.076†
.085
.078†
.089

.077
.081

.280†
.306
.306
.310

.279†
.296
.294
.302

.278†
.290
.295
.296

.280†
.289
.278†
.296

.273†
.286
.273†
.286

.278
.276

.116
.133
.136
.137

.114†
.126
.129
.134

.113†
.124
.125
.127

.113†
.125
.112†
.129

.110†
.119
.113†
.123

.112
.113

.378
.410
.392
.415

.372†
.403
.390
.411

.370†
.394
.386
.404

.375†
.398
.387
.407

.373†
.388
.381
.391

.372
.372

.098
.116
.106
.118

.093†
.111
.107
.117

.093†
.104
.104
.114

.095†
.108
.104
.112

.096†
.102
.099
.101

.093
.086

.299
.325
.312
.325

.295†
.320
.309
.325

.129
.149
.140
.151

.125†
.145
.138
.151

.295† 125†
.138
.312
.136
.307
.146
.320

.294†
.311
.309
.321

.297†
.307
.301
.308

.293
.292

.127†
.141
.136
.147

.126†
.134
.132
.136

.125
.122

Table 4: Comparison of APRIL and SPPI. All results are averaged over all clusters in each dataset.
Baselines: HU -trained SPPI and TD(λ), without any interaction (i.e. T = 0). Boldface: Comparable
(i.e. no signiﬁcant gaps exist) or signiﬁcantly better than SPPI with 100 rounds of interaction, under the
same oracle. Superscript †: Comparable or signiﬁcantly worse than the corresponding baseline.

DUC’01

DUC’02

DUC’04

Overall

APRIL
SPPI

3.57±.30
2.29±.29

4.14±.14
2.14±.14

3.86±.40
3.14±.34

3.86±.17
2.52±.18

Table 5: Human ratings for the summaries gener-
ated by APRIL and SPPI (mean±standard error).

SPPI) for up to 5,000 episodes. The baseline re-
sults are presented in the bottom rows of Table 4.

We make the following observations from Ta-
ble 4. (i) Given the same oracle, the performance
of APRIL with 10 rounds of interaction is com-
parable or even superior than that of SPPI after
100 rounds of interaction (see boldface in Table
4), suggesting the strong advantage of APRIL to
(ii) APRIL can sig-
reduce sample complexity.
niﬁcantly improve the baseline with either 10 or
100 rounds of interaction, but SPPI’s performance
can be even worse than the baseline (marked by †
in Table 4), especially under the high-noise low-
budget settings
(i.e., CNO-0.3, LNO-0.3, and
LNO-1 with T = 10). This is because SPPI lacks
a mechanism to balance between prior and poste-
rior ranking, while APRIL can adjust this trade-off

by tuning α (Eq. (7)). This endows APRIL with
better noise robustness and lower sample com-
plexity in high-noise low-budget settings. Note
that the above observations also hold for the other
two datasets,
indicating the consistently strong
performance of APRIL across different datasets.

As for the overall runtime, when budget T =
100, APRIL on average takes 2 minutes to interact
with an oracle and output a summary, while SPPI
takes around 15 minutes due to its expensive gra-
dient descent computation (see §3.1).

7 Human Evaluation

Finally, we invited real users to compare and eval-
uate the quality of the summaries generated by
SPPI and APRIL. We randomly selected three top-
ics (d19 from DUC’01, d117i from DUC’02 and
d30042 from DUC’04), and let both SPPI and our
best-performing APRIL interact with PO for 10
rounds on these topics. The resulting 100-word
summaries, shown in Figure 2, were presented to
seven users, who had already read two background
texts to familiarize with the topic. The users were
asked to provide their preference on the presented

Topic d30042 (DUC’04), SPPI: After meeting Libyan leader Moammar Gad-
haﬁ in a desert tent, U.N. Secretary-General Koﬁ Annan said he thinks an ar-
rangement for bringing two suspects to trial in the bombing of a Pan Am air-
liner could be secured in the “not too distant future.” TRIPOLI, Libya (AP)
U.N. Secretary-General Koﬁ Annan arrived in Libya Saturday for talks aimed
at bringing to trial two Libyan suspects in the 1988 Pan Am bombing over
Lockerbie, Scotland. Secretary General Koﬁ Annan said Wednesday he was
extending his North African tour to include talks with Libyan authorities. An-
nan’s one-day, 2nd graf pvs During his Algerian stay,

Topic d30042 (DUC’04), APRIL: TRIPOLI, Libya (AP) U.N. Secretary-
General Koﬁ Annan arrived in Libya Saturday for talks aimed at bringing to
trial two Libyan suspects in the 1988 Pan Am bombing over Lockerbie, Scot-
land. Annan’s one-day visit to meet with Libyan leader Col. Moammar Gadhaﬁ
followed reports in the Libyan media that Gadhaﬁ had no authority to hand over
the suspects. The 60-year-old Annan is trying to get Libya to go along with a
U.S.-British plan to try the two suspects before a panel of Scottish judges in the
Netherlands for the Dec. 21, 1988, bombing over Lockerbie, Scotland. Sirte
is 400 kilometers (250 miles) east of the Libyan capital Tripoli. During his
Algerian stay,

Topic d117i (DUC’02), SPPI: The Booker Prize is sponsored by Booker, an
international food and agriculture business. The novel, a story of Scottish low-
life narrated largely in Glaswegian dialect, is unlikely to prove a popular choice
with booksellers, who have damned all six books shortlisted for the prize as
boring, elitist and- worst of all- unsaleable. The shortlist of six for the Pounds
20,000 Booker Prize for ﬁction, announced yesterday, immediately prompted
the question ’Who ? ’ Japanese writer Kazuo Ishiguro won the 1989 Booker
Prize, Britain’s top literary award, for his novel “The Remains of the Day,”
judges announced Thursday. He didn’t win.

Topic d117i (DUC’02), APRIL: Australian novelist Peter Carey was awarded
the coveted Booker Prize for ﬁction Tuesday night for his love story, “Oscar
and Lucinda.” The Booker Prize is sponsored by Booker, an international food
and agriculture business, and administered by The Book Trust. British publish-
ers can submit three new novels by British and Commonwealth writers. Six
novels have been nominated for the Booker Prize, Britain’s most prestigious
ﬁction award, and bookmakers say the favorite is “The Remains of the Day” by
Japanese author Kazuo Ishiguro. On the day of the Big Event, Ladbroke, the
large British betting agency, posted the ﬁnal odds.

Topic d19 (DUC’01), SPPI: The issue cuts across partisan lines in the Senate,
with Minority Leader Bob Dole (R-Kan.) arguing against the White House po-
sition on grounds that including illegal aliens in the census is unfair to Amer-
ican citizens.. Loss of Seats Cited. Shelby’s amendment says only that the
secretary is to “make such adjustments in total population ﬁgures as may be
necessary, using such methods and procedures as the secretary determines fea-
sible and appropriate” to keep illegal aliens from being counted in congres-
sional reapportionment. “Some states will lose congressional seats because of
illegal aliens,” Dole argued. But there’s nothing simple about it.

Topic d19 (DUC’01), APRIL: In a blow to California and other states with
large immigrant populations, the Senate voted Friday to bar the Census Bu-
reau from counting illegal aliens in the 1990 population count. But the Senate
already has voted to force the Census Bureau to exclude illegal immigrants in
preparing tallies for congressional reapportionment. said that Georgia and Indi-
ana both lost House seats after the 1980 Census, and California and New York-
centers of illegal immigration- each gained seats. A majority of the members
of the House of Representatives has signaled support. The national head count
will be taken April 1, 1990.

Figure 2: Summaries generated by SPPI and APRIL used in the human evaluation experiments.

summary pairs and rate the summaries on a 5-
point Likert scale with higher scores for better
summaries. All users are ﬂuent in English.

In all three topics, all users prefer the APRIL-
generated summaries over the SPPI-generated
summaries. Table 5 shows the users’ ratings. The
APRIL-generated summaries consistently receive
higher ratings. These results are consistent with
our simulation experiments and conﬁrm the sig-
niﬁcant advantage of APRIL over SPPI.

8 Conclusion

We propose a novel preference-based interactive
learning formulation named APRIL (Active Pref-
erence ReInforcement Learning), which is able to
make structured predictions without referring to
the gold standard data.
Instead, APRIL learns
from preference-based feedback. We designed a
novel objective function for APRIL, which natu-
rally splits APRIL into an active preference learn-
ing (APL) phase and a reinforcement learning
(RL) phase, enabling us to leverage a wide spec-
trum of active learning, preference learning and
RL algorithms to maximise the output quality with
a limited number of interaction rounds. We ap-
plied APRIL to the Extractive Multi-Document
Summarisation (EMDS) problem, simulated the
users’ preference-giving behaviour using multiple
user-response models, and compared the perfor-
mance of multiple APL and RL techniques. Sim-
ulation experiments indicated that APRIL signif-

icantly improved the summary quality with just
10 rounds of interaction (even with high-noise
oracles), and signiﬁcantly outperformed SPPI in
terms of both sample complexity and noise robust-
ness. Human evaluation results suggested that real
users preferred the APRIL-generated summaries
over the SPPI-generated ones.

We identify two major lines of future work. On
the technical side, we plan to employ more ad-
vanced APL and RL algorithms in APRIL, such as
sample-efﬁcient Bayesian-based APL algorithms
(e.g., Simpson and Gurevych, 2018) and neural
RL algorithms (e.g. Mnih et al., 2015) to further
reduce the sample complexity of APRIL. On the
experimental side, a logical next step is to imple-
ment an interactive user interface for APRIL and
conduct a larger evaluation study comparing the
summary quality before and after the interaction.
We also plan to apply APRIL to more NLP appli-
cations, including machine translation, informa-
tion exploration and semantic parsing.

Acknowledgements

This work has been supported by the German
Research Foundation as part of the QA-EduInf
project (grant GU 798/18-1 and grant RI 803/12-
1). We thank the researchers and students from
TU Darmstadt who participated in our human
evaluation experiments. We also thank Johannes
F¨urnkranz, Christian Wirth and the anonymous re-
viewers for their helpful comments.

References

Steven Bird, Ewan Klein,

and Edward Loper.
2009. Natural Language Processing with Python.
O’Reilly.

Justin A. Boyan. 1999. Least-squares temporal differ-
ence learning. In Proceedings of the Sixteenth Inter-
national Conference on Machine Learning (ICML
1999), June 27–30, 1999, Bled, Slovenia, pages 49–
56.

Ralph Allan Bradley and Milton E Terry. 1952. Rank
analysis of incomplete block designs: I. The method
of paired comparisons. Biometrika, 39(3/4):324–
345.

Paul F. Christiano, Jan Leike, Tom Brown, Miljan Mar-
tic, Shane Legg, and Dario Amodei. 2017. Deep
Reinforcement Learning from Human Preferences.
In Advances in Neural Information Processing Sys-
tems 30: 31st Annual Conference on Neural Infor-
mation Processing Systems, December 4–9, 2017,
Long Beach, CA, USA, pages 4302–4310.

Prefer-
Wei Chu and Zoubin Ghahramani. 2005.
In Ma-
ence learning with Gaussian processes.
chine Learning, Proceedings of the Twenty-Second
International Conference (ICML 2005), August 7–
11, 2005, Bonn, Germany, pages 137–144.

Johannes F¨urnkranz and Eyke H¨ullermeier. 2010. Pref-
In Preference
erence learning: An introduction.
Learning, pages 1–17. Berlin/Heidelberg: Springer.

Stefan Henß, Margot Mieskes, and Iryna Gurevych.
2015. A reinforcement learning approach for adap-
tive single- and multi-document summarization. In
Proceedings of the International Conference of the
German Society for Computational Linguistics and
Language Technology (GSCL 2015), September 30–
October 2, 2015, University of Duisburg-Essen,
Germany, pages 3–12.

Kevin G. Jamieson and Robert D. Nowak. 2011. Ac-
In Ad-
tive ranking using pairwise comparisons.
vances in Neural Information Processing Systems
24: 25th Annual Conference on Neural Informa-
tion Processing Systems, December 12–14, 2011,
Granada, Spain, pages 2240–2248.

M.G. Kendall. 1948. Rank correlation methods. C.

Grifﬁn.

David C Kingsley and Thomas C Brown. 2010. Prefer-
ence uncertainty, preference reﬁnement and paired
comparison choice experiments. Land Economics,
86(3):530–544.

Julia Kreutzer, Artem Sokolov, and Stefan Riezler.
Bandit structured prediction for neural
2017.
In Proceedings of
sequence-to-sequence learning.
the 55th Annual Meeting of the Association for
Computational Linguistics (ACL 2017), Volume 1:
Long Papers, July 30–August 4, 2017, Vancouver,
Canada, pages 1503–1513.

Michail G Lagoudakis and Ronald Parr. 2003. Least-
squares policy iteration. Journal of Machine Learn-
ing Research, 4:1107–1149.

Chin-Yew Lin. 2004. ROUGE: A package for auto-
In Workshop on
matic evaluation of summaries.
Text Summarization Branches Out, Post-Conference
Workshop of ACL, July 21–26, 2004, Barcelona,
Spain, pages 74–81.

Lucas Maystre and Matthias Grossglauser. 2017. Just
sort it! A simple and effective approach to ac-
In Proceedings of the
tive preference learning.
34th International Conference on Machine Learning
(ICML 2017), August 6–11, 2017, Sydney, Australia,
pages 2344–2353.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. 2015. Human-level
control through deep reinforcement learning. Na-
ture, 518(7540):529–533.

Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Ranking sentences for extractive summariza-
tion with reinforcement learning. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies (NAACL-HLT
2018),Volume 1 (Long Papers), June 1-6, 2018, New
Orleans, LA, USA, pages 1747–1759.

Ani Nenkova and Kathleen McKeown. 2012. A survey
of text summarization techniques. In Charu C. Ag-
garwal and ChengXiang Zhai, editors, Mining Text
Data, pages 43–76. Boston: Springer.

Ramakanth Pasunuru and Mohit Bansal. 2018. Multi-
reward reinforced summarization with saliency and
entailment. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT 2018), Volume 2 (Short
Papers), June 1-6, 2018, New Orleans, LA, USA,
pages 646–653.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. CoRR, abs/1705.04304.

Joint
Avinesh P.V.S. and Christian M. Meyer. 2017.
optimization of user-desired content
in multi-
document summaries by learning from user feed-
back. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (ACL
2017): Volume 1: Long Paper, July 30–August 4,
2017, Vancouver, Canada, pages 1353–1363.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2015.
Sequence level
training with recurrent neural networks. CoRR,
abs/1511.06732.

Cody Rioux, Sadid A. Hasan, and Yllias Chali. 2014.
Fear the REAPER: A system for automatic multi-
document summarization with reinforcement learn-
In Proceedings of the 2014 Conference on
ing.
Empirical Methods in Natural Language Processing
(EMNLP 2014), October 25–29, 2014, Doha, Qatar,
pages 681–690.

Markus Zopf. 2018. Estimating summary quality with
pairwise preferences. In Proceedings of the 16th An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies (NAACL-HLT 2018),
Volume 1 (Long Papers), June 1–8, 2018, New Or-
leans, LA, USA, pages 1687–1696.

Seonggi Ryang and Takeshi Abekawa. 2012. Frame-
work of automatic text summarization using rein-
In Proceedings of the 2012
forcement learning.
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL 2012), July
12–14, 2012, Jeju Island, Korea, pages 256–265.

Edwin D. Simpson and Iryna Gurevych. 2018. Finding
convincing arguments using scalable bayesian pref-
erence learning. Transactions of the Association for
Computational Linguistics, 6:357–371.

Artem Sokolov, Julia Kreutzer, Christopher Lo, and
Stefan Riezler. 2016a. Learning structured predic-
tors from bandit feedback for interactive NLP.
In
Proceedings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2016):
Volume 1: Long Papers, August 7–12, 2016, Berlin,
Germany, pages 1610–1620.

Artem Sokolov, Julia Kreutzer, Stefan Riezler, and
Christopher Lo. 2016b. Stochastic structured pre-
diction under bandit feedback. In Advances in Neu-
ral Information Processing Systems 29: 30th An-
nual Conference on Neural Information Processing
Systems, December 5–10, 2016, Barcelona, Spain,
pages 1489–1497.

Charles Spearman. 1904. The proof and measurement
of association between two things. The American
Journal of Psychology, 15(1):72–101.

R. S. Sutton. 1984. Temporal Credit Assignment in Re-
inforcement Learning. Ph.D. thesis, University of
Massachusetts, Amherst.

Louis Leon Thurstone. 1927. A Law of Comparative
Judgement. Psychological Review, 34:278–286.

Paolo Viappiani and Craig Boutilier. 2010. Optimal
Bayesian recommendation sets and myopically op-
In Advances in Neural
timal choice query sets.
Information Processing Systems 23: 24th Annual
Conference on Neural Information Processing Sys-
tems, December 6-9 , Vancouver, British Columbia,
Canada, pages 2352–2360.

Christian Wirth, Riad Akrour, Gerhard Neumann, and
Johannes F¨urnkranz. 2017. A survey of preference-
based reinforcement learning methods. Journal of
Machine Learning Research, 18:4945–4990.

Christian Wirth, Johannes F¨urnkranz, and Gerhard
Neumann. 2016. Model-free preference-based rein-
forcement learning. In Proceedings of the Thirtieth
AAAI Conference on Artiﬁcial Intelligence, Febru-
ary 12–17, 2016, Phoenix, AZ, USA, pages 2222–
2228.

APRIL: Interactively Learning to Summarise by Combining
Active Preference Learning and Reinforcement Learning

Yang Gao, Christian M. Meyer, Iryna Gurevych
Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universit¨at Darmstadt
https://www.ukp.tu-darmstadt.de/

8
1
0
2
 
g
u
A
 
9
2
 
 
]
L
C
.
s
c
[
 
 
1
v
8
5
6
9
0
.
8
0
8
1
:
v
i
X
r
a

Abstract

We propose a method to perform automatic
document summarisation without using refer-
Instead, our method inter-
ence summaries.
actively learns from users’ preferences. The
merit of preference-based interactive sum-
marisation is that preferences are easier for
users to provide than reference summaries.
Existing preference-based interactive learning
methods suffer from high sample complex-
ity,
i.e. they need to interact with the ora-
cle for many rounds in order to converge.
In this work, we propose a new objective
function, which enables us to leverage ac-
tive learning, preference learning and rein-
forcement learning techniques in order to re-
duce the sample complexity. Both simula-
tion and real-user experiments suggest that
our method signiﬁcantly advances the state
of the art. Our source code is freely avail-
able at https://github.com/UKPLab/
emnlp2018-april.

1

Introduction

With the rapid growth of text-based information
on the Internet, automatic document summarisa-
tion attracts increasing research attention from the
Natural Language Processing (NLP) community
(Nenkova and McKeown, 2012). Most existing
document summarisation techniques require ac-
cess to reference summaries to train their systems.
However, obtaining reference summaries is very
expensive: Lin (2004) reported that 3,000 hours
of human effort were required for a simple evalu-
ation of the summaries for the Document Under-
standing Conferences (DUC). Although previous
work has proposed heuristics-based methods to
summarise without reference summaries (Ryang
and Abekawa, 2012; Rioux et al., 2014), the gap
between their performance and the upper bound is
still large: the ROUGE-2 upper bound of .212 on

DUC’04 (P.V.S. and Meyer, 2017) is, for example,
twice as high as Rioux et al.’s (2014) .114.

The Structured Prediction from Partial Infor-
mation (SPPI) framework has been proposed to
learn to make structured predictions without ac-
cess to gold standard data (Sokolov et al., 2016b).
SPPI is an interactive NLP paradigm:
It inter-
acts with a user for multiple rounds and learns
from the user’s feedback. SPPI can learn from
two forms of feedback: point-based feedback, i.e.
a numeric score for the presented prediction, or
preference-based feedback, i.e. a preference over
a pair of predictions. Providing preference-based
feedback yields a lower cognitive burden for hu-
mans than providing ratings or categorical labels
(Thurstone, 1927; Kendall, 1948; Kingsley and
Brown, 2010; Zopf, 2018). Preference-based SPPI
has been applied to multiple NLP applications, in-
cluding text classiﬁcation, chunking and machine
translation (Sokolov et al., 2016a; Kreutzer et al.,
2017). However, SPPI has prohibitively high sam-
ple complexities in the aforementioned NLP tasks,
as it needs at least hundreds of thousands rounds of
interaction to make near-optimal predictions, even
with simulated “perfect” users. Figure 1a illus-
trates the workﬂow of the preference-based SPPI.
To reduce the sample complexity, in this work,
we propose a novel preference-based interactive
learning framework, called APRIL (Active Prefer-
ence ReInforcement Learning). APRIL goes be-
yond SPPI by proposing a new objective func-
tion, which divides the preference-based interac-
tive learning problem into two phases (illustrated
in Figure 1b): an Active Preference Learning
(APL) phase (the right cycle in Figure 1b), and
a Reinforcement Learning (RL) phase (the left cy-
cle). We show that this separation enables us to
query preferences more effectively and to use the
collected preferences more efﬁciently, so as to re-
duce the sample complexity.

(a) Workﬂow of preference-based SPPI

(b) Workﬂow of APRIL

Figure 1: A comparison of workﬂows of SPPI (a)
and APRIL (b) in the EMDS use case. Notation
details, e.g., ∆x and r(yn), are discussed in §3.

We apply APRIL to Extractive Multi-Document
Summarisation (EMDS). The task of EMDS is to
extract sentences from the original documents to
build a summary under a length constraint. We
accommodate multiple APL and RL techniques
in APRIL and compare their performance under
different simulation settings. We also compare
APRIL to a state-of-the-art SPPI implementation
using both automatic metrics and human evalua-
tion. Our results suggest that APRIL signiﬁcantly
outperforms SPPI.

2 Related Work

RL has been previously used to perform EMDS
without using reference summaries. Ryang and
Abekawa (2012) formulated EMDS as a Markov
Decision Process (MDP), designed a heuristics-
based reward function considering both informa-
tion coverage rate and redundancy level, and used
the Temporal Difference (TD) algorithm (Sutton,
1984) to solve the MDP. In a follow-up work, Ri-
oux et al. (2014) proposed a different reward func-
tion, which also did not require reference sum-
their experiments suggested that using
maries;
their new reward function improved the summary
quality. Henß et al. (2015) proposed a different
RL formulation of EMDS and jointly used super-
vised learning and RL to perform the task. How-
ever, their method requires the access to reference
summaries. More recent works applied encoder-
decoder-based RL to document summarisation
(Ranzato et al., 2015; Narayan et al., 2018; Paulus
et al., 2017; Pasunuru and Bansal, 2018). These
works outperformed standard encoder-decoder as

RL can directly optimise the ROUGE scores and
can tackle the exposure bias problems. However,
these neural RL methods all used ROUGE scores
as their rewards, which in turn relied on reference
summaries. APRIL can accommodate these neu-
ral RL techniques in its RL phase by using a rank-
ing of summaries instead of the ROUGE scores as
rewards. We leave neural APRIL for future study.

P.V.S. and Meyer (2017) proposed a bigram-
based interactive EMDS framework. They asked
users to label important bigrams in candidate sum-
maries and used integer linear programming (ILP)
to extract sentences covering as many important
bigrams as possible. Their method requires no ac-
cess to reference summaries, but it requires con-
siderable human effort during the interaction: in
simulation experiments, their system needed to
collect up to 350 bigram annotations from a (simu-
lated) user. In addition, they did not consider noise
in users’ annotations but simulated perfect oracles.

Preference learning aims at obtaining the rank-
ing (i.e. total ordering) of objects from pairwise
preferences (F¨urnkranz and H¨ullermeier, 2010).
Simpson and Gurevych (2018) proposed to use an
improved Gaussian process preference learning
(Chu and Ghahramani, 2005) for learning to rank
arguments in terms of convincingness from crowd-
sourced annotations. However, such Bayesian
methods can hardly scale and suffer from high
computation time. Zopf (2018) recently proposed
to learn a sentence ranker from preferences. The
resulting ranker can be used to identify the impor-
tant sentences and thus to evaluate the quality of
the summaries. His study also suggests that pro-
viding sentence preferences takes less time than
writing reference summaries. APRIL not only
learns a ranking over summaries from pairwise
preferences, but also uses the ranking to “guide”
our RL agent to generate good summaries.

There is a recent trend in machine learning to
combine active learning, preference learning and
RL, for learning to perform complex tasks from
preferences (Wirth et al., 2017). The resulting al-
gorithm is termed Preference-based RL (PbRL),
and has been used in multiple applications, includ-
ing training robots (Wirth et al., 2016) and Atari-
playing agents (Christiano et al., 2017). SPPI and
APRIL can both be viewed as PbRL algorithms.
But unlike most PbRL methods that learn a utility
function of the predictions (in EMDS, predictions
are summaries) to guide the RL agent, APRIL

is able to directly use a ranking of predictions
to guide the RL agent without making assump-
tions about the underlying structure of the utility
functions. This also enables APRIL to use non-
utility-based preference learning techniques (e.g.,
Maystre and Grossglauser, 2017).

3 Background

In this section, we recap necessary details of SPPI,
RL and preference learning, and adapt them to the
EMDS use case, laying the foundation for APRIL.

3.1 The SPPI Framework

Let X be the input space and let Y(x) be the set
of possible outputs for input x ∈ X . In EMDS,
x ∈ X is a cluster of documents and Y(x) is the
set of all possible summaries for cluster x. The
function ∆x : Y(x)×Y(x) → {0, 1} is the prefer-
ence function such that ∆x(yi, yj) = 1 if the user
believes yj is better than yi (denoted by yj (cid:31) yi or
equivalently yi ≺ yj), and 0 otherwise. Through-
out this paper we assume that users do not equally
prefer two different items. For a given x, the ex-
pected loss is:

LSPPI(w|x) = Epw(yi,yj |x)[∆x(yi, yj)]
=
∆x(yi, yj) pw(yi, yj|x),

(cid:88)

(1)

yi,yj ∈Y(x)

where pw(yi, yj|x) is the probability of querying
the pair (yi, yj). Formally,

pw(yi, yj|x)

=

exp[w(cid:124)(φ(yi|x) − φ(yj|x))]
(cid:80)

exp[w(cid:124)(φ(yp|x) − φ(yq|x))]

,

(2)

yp,yq∈Y(x)

where φ(y|x) is the vector representation of y
given x, and w is the weight vector to be learnt.
Eq. (2) is a Gibbs sampling strategy: w(cid:124)(φ(yi|x)−
φ(yj|x)) can be viewed as the “utility gap” be-
tween yi and yj. The sampling strategy pw en-
courages querying pairs with large utility gaps.

To minimise LSPPI, SPPI uses gradient descent
to update w incrementally. Alg. 1 presents the
pseudo code of our adaptation of SPPI to EMDS.
In the supplementary material, we provide a de-
tailed derivation of ∇wLSPPI(w|x).

3.2 Reinforcement Learning

RL amounts to efﬁcient algorithms for searching
optimal solutions in MDPs. MDPs are widely

Input

: sequence of learning rates γt; query

budget T ; document cluster x

initialise w0;
while t = 0 . . . T do

sample (yi, yj) according to Eq. (2);
obtain feedback ∆x(yi, yj);
wt+1 := wt − γt∇wLSPPI(w|x)

end
(cid:124)
Output: y∗ = arg maxy∈Y (x) w
T +1φ(y, x)
Algorithm 1: SPPI for preference-based in-
teractive document summarisation (adjusted
from Alg. 2 in (Sokolov et al., 2016a)).

used to formulate sequential decision making
problems, which EMDS falls into: in EMDS, the
summariser has to sequentially select sentences
from the original documents and add them to the
draft summary. An (episodic) MDP is a tuple
(S, A, P, R, T ). S is the set of states, A is the set
of actions, P : S × A × S → R is the transition
function with P (s(cid:48)|s, a) yielding the probability of
performing action a in state s and being transited
to a new state s(cid:48). R : S × A → R is the reward
function with R(s, a) giving the immediate reward
for performing action a in state s. T ⊆ S is the set
of terminal states; visiting a terminal state termi-
nates the current episode.

In EMDS, we follow the same MDP formu-
lation as Ryang and Abekawa (2012) and Rioux
et al. (2014). Given a document cluster, a state s
is a draft summary, A includes two types of ac-
tions, concatenate a new sentence to the current
draft summary, or terminate the draft summary
construction. The transition function P in EMDS
is trivial because given the current draft summary
and an action, the next state can be easily inferred.
The reward function R returns an evaluation score
of the summary once the action terminate is per-
formed; otherwise it returns 0 because the sum-
mary is still under construction and thus not ready
to be evaluated. Providing non-zero rewards be-
fore the action terminate can lead to even worse
result, as reported by Rioux et al. (2014).

A policy π : S × A → R in an MDP deﬁnes
how actions are selected: π(s, a) is the probability
of selecting action a in state s. In EMDS, a policy
corresponds to a strategy to build summaries for a
given document cluster. We let Yπ(x) be the set of
all possible summaries the policy π can construct
in the document cluster x, and we slightly abuse
the notation by letting π(y|x) denote the probabil-

ity of policy π generating a summary y in cluster
x. Then the expected reward of a policy is:

RRL(π|x) = Ey∈Yπ(x)R(y|x)

(cid:88)

=

y∈Yπ(x)

π(y|x)R(y|x),

(3)

where R(y|x) is the reward for summary y in doc-
ument cluster x. The goal of an MDP is to ﬁnd the
optimal policy π∗ that has the highest expected re-
ward: π∗ = arg maxπ RRL(π).

Note that the loss function in SPPI (Eq. (1)) and
the expected reward function in RL (Eq. (3)) are in
similar forms: if we view the pair selection proba-
bility pw in Eq. (2) as a policy, and view the pref-
erence function ∆x in Eq. (1) as a negative reward
function, we can view SPPI as an RL problem.
The major difference between SPPI and RL is that
SPPI selects and evaluates pairs of outputs, while
RL selects and evaluates single outputs. We will
exploit their connection to propose our new objec-
tive function and the APRIL framework.

3.3 Preference Learning

The linear Bradley-Terry (BT) model (Bradley
and Terry, 1952) is one of the most widely used
methods in preference learning. Given a set of
items Y, suppose we have observed T preferences:
Q = {q1(y1,1, y1,2), · · · , qT (yT,1, yT,2)}, where
yi,1, yi,2 ∈ Y, and qi ∈ {≺, (cid:31)} is the oracle’s
preference in the ith round. The BT model min-
imises the following cross-entropy loss:

LBT(w) = −

(cid:88)

[ µi,1 log Pw(yi,1 (cid:31) yi,2)

qi(yi,1,yi,2)∈Q

+ µi,2 log Pw(yi,2 (cid:31) yi,1) ],

(4)

where Pw(yi (cid:31) yj) = (1 + exp[w(cid:124)(φ(yj) −
φ(yi))])−1, and µi,1 and µi,2 indicate the direc-
tion of preferences: if yi,1 (cid:31) yi,2 then µi,1 = 1
and µi,2 = 0. Let w∗ = arg minw LBT(w),
then w∗ can be used to rank all items in Y: for
any yi, yj ∈ Y, the ranker prefers yi over yj if
w∗(cid:124)φ(yi) > w∗(cid:124)φ(yj).

4 APRIL: Decomposing SPPI into Active

Preference Learning and RL

A major problem of SPPI is its high sample com-
plexity. We believe this is due to two reasons.
First, SPPI’s sampling strategy is inefﬁcient: From
Eq. (2) we can see that SPPI tends to select pairs
with large quality gaps for querying the user. This

strategy can quickly identify the relatively good
and relatively bad summaries, but needs many
rounds of interaction to ﬁnd the top summaries.
Second, SPPI uses the collected preferences inef-
fectively: In Alg. 1, each preference is used only
once for performing the gradient descent update
and is forgotten afterwards. SPPI does not gener-
alise or re-use collected preferences, wasting the
useful and expensive information.

These two weaknesses of SPPI motivate us to
propose a new learning paradigm that can query
and generalise preferences more efﬁciently. Re-
call that in EMDS, the goal is to ﬁnd the optimal
summary for a given document cluster x, namely
the summary that is preferred over all other pos-
sible summaries in Y(x). Based on this under-
standing, we deﬁne a new expected reward func-
tion RAPRIL for policy π as follows:

1
|Y(x)|

(cid:88)

yi∈Y(x)
(cid:88)

RAPRIL(π|x) = Eyj ∼π[

∆x(yi, yj)]

1
|Y(x)|

(cid:88)

=

=

y∈Yπ(x)

(cid:88)

π(yj|x)

∆x(yi, yj)

yj ∈Yπ(x)

yi∈Y(x)

π(y|x) r(y|x),

(5)

where r(y|x) = (cid:80)
yi∈Y(x) ∆x(yi, yj)/|Y(x)|.
Note that ∆x(yi, yj) equals 1 if yj is preferred
over yi and equals 0 otherwise (see §3.1). Thus,
r(y|x) is the relative position of y in the (ascend-
ing) sorted Y(x), and it can be approximated by
preference learning. The use of preference learn-
ing enables us to generalise the observed prefer-
ences to a ranker (see §3.3), allowing more ef-
fective use of the collected preferences. Also, we
can use active learning to select summary pairs for
querying more effectively. In addition, the resem-
blance of RAPRIL and RL’s reward function RRL
(in Eq. (3)) enables us to use a wide range of RL
algorithms to maximise RAPRIL (see §2).

Based on the new objective function, we split
the preference-based interactive learning into two
an Active Preference Learning (APL)
phases:
phase (the right cycle in Fig. 1b), responsible for
querying preferences from the oracle and approxi-
mating the ranking of summaries, and an RL phase
(the left cycle in Fig. 1b), responsible for learning
to summarise based on the learned ranking. The
resulting framework APRIL allows for integrating
any active preference learning and RL techniques.
Note that only the APL phase is online (i.e. in-

Input

: query budget T ; document cluster x;

RL episode budget N
/* Phase 1: active preference learning */
while t = 0 . . . T do

sample a summary pair (yi, yj) using any
APL strategy;
obtain feedback ∆x(yi, yj);
update ranker according to Eq. (4) ;

end
/* Phase 2: RL-based summarisation */
initialise an arbitrary policy π0;
while n = 0 . . . N do

evaluate policy πn according to Eq. (5);
update policy πn using any RL algorithm;

end
Output: y∗ = arg maxy∈YπN (x) πN (y|x)
Algorithm 2: Pseudo code of APRIL

Dataset

Lang

# Topic

# Doc

# Token/Doc

DUC ’01
DUC ’02
DUC ’04

EN
EN
EN

30
59
50

308
567
500

781
561
587

Table 1: Statistics of the datasets. The target sum-
mary length is 100 tokens in all three datasets.

volving humans in the loop) while the RL phase
can be performed ofﬂine, helping to improve the
real-time responsiveness. Also, the learned ranker
can provide an unlimited number of rewards (i.e.
r(y|x) in Eq. (5)) to the RL agent, enabling us to
perform many episodes of RL training with a small
number of collected preferences – unlike in SPPI
where each collected preference is used to train the
system for one round and is forgotten afterwards.
Alg. 2 shows APRIL in pseudo code.

5 Experimental Setup

Datasets. We perform experiments on DUC ’04
to ﬁnd the best performing APL and RL tech-
niques. Then we combine the best-performing
APL and RL to complete APRIL and compare
it against SPPI on the DUC ’01, DUC ’02 and
DUC ’04 datasets.1
Some statistics of these
datasets are summarised in Table 1.

Simulated Users. Existing preference-based in-
teractive learning techniques assume that the or-
acle has an intrinsic evaluation function U ∗ and
provides preferences consistent with U ∗ by prefer-
ring higher valued candidates. We term this a Per-

1http://duc.nist.gov/

fect Oracle (PO). We believe that assuming a PO
is unrealistic for real-world applications, because
sometimes real users tend to misjudge the prefer-
ence direction, especially when the presented can-
didates have similar quality. In this work, besides
PO, we additionally consider two types of noisy
oracles based on the user-response models pro-
posed by Viappiani and Boutilier (2010):

• Constant noisy oracle (CNO): with prob-
ability c ∈ [0, 1], this oracle randomly se-
lects which summary is preferred; otherwise
it provides preferences consistent with U ∗.
We consider CNOs with c = 0.1 and c = 0.3.

in cluster x,

• Logistic noisy oracle (LNO): for two sum-
maries yi and yj
the or-
acle prefers yi over yj with probability
pU ∗(yi (cid:31) yj|x; m) = (1 + exp[(U ∗(yj|x) −
U ∗(yi|x))/m])−1. This oracle reﬂects the in-
tuition that users are more likely to misjudge
the preference direction when two summaries
have similar quality. Note that the parame-
ter m ∈ R+ controls the “noisiness” of the
user’s responses: higher values of m result
in a less steep sigmoid curve, and the result-
ing oracle is more likely to misjudge. We use
LNOs with m = 0.3 and m = 1.

As for the intrinsic evaluation function U ∗, re-
cent work has suggested that human preferences
over summaries have high correlations to ROUGE
scores (Zopf, 2018). Therefore, we deﬁne:

U ∗(y|x) =

R1(y|x)
0.47

+

R2(y|x)
0.22

+

RS(y|x)
0.18

(6)

where R1, R2 and RS stand for ROUGE-1,
ROUGE-2 and ROUGE-SU4, respectively. The
real values (0.47, 0.22 and 0.18) are used to bal-
ance the weights of the three ROUGE scores. We
choose them to be around the EMDS upper-bound
ROUGE scores reported by P.V.S. and Meyer
(2017). As such, an optimal summary’s U ∗ value
should be around 3.

Implementation. All code is written in Python
and runs on a desktop PC with 8 GB RAM and an
i7-2600 CPU. We use NLTK (Bird et al., 2009) to
perform sentence tokenisation. Our source code
is freely available at https://github.com/
UKPLab/emnlp2018-april.

6 Simulation Results

We ﬁrst study the APL phase (§6.1) and the RL
phase (§6.2)) separately by comparing the perfor-

mance of multiple APL and RL algorithms in each
phase. Then, in §6.3, we combine the best per-
forming APL and RL algorithm to complete Alg.
2 and compare APRIL against SPPI.

6.1 APL Phase Performance

Recall that the task of APL is to output a ranking
of all summaries in a cluster. In this subsection,
we test multiple APL techniques and compare the
quality of their resulting rankings. Two metrics are
used: Kendall’s τ (Kendall, 1948) and Spearman’s
ρ (Spearman, 1904). Both metrics are valued be-
tween −1 and 1, with higher values suggesting
higher rank correlation. Because the number of
possible summaries in a cluster is huge, instead of
evaluating the ranking quality on all possible sum-
maries, we evaluate rankings on 10,000 randomly
sampled summaries, denoted ˆY(x). During query-
ing, all candidate summaries presented to the ora-
cle are also selected from ˆY(x). Sampling ˆY(x) a
priori helps us to reduce the response time to un-
der 500 ms for all APL techniques we test. We
compare four active learning strategies under two
query budgets, T = 10 and T = 100:

• Random Sampling (RND): Randomly se-
lect two summaries from ˆY(x) to query.

• SPPI Sampling (SBT): Select summary
pairs from ˆY(x) according to the SPPI strat-
egy in Eq. (2). After each round, the weight
vector w is updated according to Eq. (4).

• Uncertainty Sampling (Unc): Query the
most uncertain summary pairs. In line with
P.V.S. and Meyer (2017), the uncertainty of
a summary is evaluated as follows: ﬁrst,
we estimate the probability of a summary
y being the optimal summary in cluster x
as popt(y|x) = (1 + exp(−w∗(cid:124)
t φ(x, y)))−1,
where w∗
t is the weights learned by the BT
model (see §3.3) in round t. Given popt(y|x),
we let the uncertainty score unc(y|x) = 1 −
popt(y|x) if popt(y|x) ≥ 0.5 and unc(y|x) =
popt(y|x) otherwise.

• J&N is the robust query selection algorithm
proposed by Jamieson and Nowak (2011). It
assumes that the items’ preferences are de-
pendent on their distances to an unknown ref-
erence point in the embedding space: the far-
ther an item to the reference point, the more
preferred the item is. After each round of
interaction, the algorithm uses all collected

preferences to locate the area where the ref-
erence point may fall into, and identify the
query pairs which can reduce the size of this
area, termed ambiguous query pairs. To com-
bat noise in preferences, the algorithm se-
lects the most-likely-correct ambiguous pair
to query the oracle in each round.

After all preferences are collected, we obtain
the ranker as follows: for any yi, yj ∈ Y(x), the
ranker prefers yi over yj if

αw∗(cid:124)φ(yi|x) + (1 − α)HU (yi|x) >
αw∗(cid:124)φ(yj|x) + (1 − α)HU (yj|x),

(7)

where w∗ is the weights vector learned by the BT
model (see Eq. (4)), HU is the heuristics-based
summary evaluation function proposed by Ryang
and Abekawa (2012), and α ∈ [0, 1] is a param-
eter. The aim of using HU and α is to trade off
between the prior knowledge (i.e. heuristics-based
HU ) and the posterior observation (i.e. the BT-
learnt w∗), so as to combat the cold-start problem.
Based on some preliminary experiments, we set
α = 0.3 when the query budget is 10, and α = 0.7
when the query budget is 100. The intuition is to
put more weight to the posterior with increasing
rounds of interaction. More systematic research
of α can yield better results; we leave it for future
work. For the vector φ(y|x), we use the same bag-
of-bigram embeddings as Rioux et al. (2014), and
we let its length be 200.

In Table 2, we compare the performance of the
four APL methods on the DUC’04 dataset. The
baseline we compared against is the prior rank-
ing. We ﬁnd that Unc signiﬁcantly2 outperforms
all other APL methods, except when the oracle
is LNO-1, where the advantage of Unc to SBT
is not signiﬁcant. Also, both Unc and SBT are
able to signiﬁcantly outperform the baseline un-
der all settings. The competitive performance of
SBT, especially with LNO-1, is due to its unique
sampling strategy: LNO-1 is more likely to mis-
judge the preference direction when the presented
summaries have similar quality, but SBT has high
probability to present summaries with large qual-
ity gaps (see Eq. (2)), effectively reducing the
chance that LNOs misjudge preference directions.
However, SBT is more “conservative” compared
to Unc because it tends to exploit the existing

2In this paper we use double-tailed student t-test to com-

pute p-values, and we let signiﬁcance level be p < 0.01.

Oracle

RND
ρ

τ

SBT
ρ

τ

Unc
ρ

τ

J&N
ρ

τ

Query budget T = 10, α = 0.3:
.211 .310 .241 .353 .253∗ .370∗
PO
CNO-0.1 .208 .307 .231 .339 .240∗ .351∗
CNO-0.3 .210 .309 .218 .320 .229∗ .337∗
LNO-0.3 .210 .309 .216 .318 .231∗ .339∗
.310
LNO-1

.206 .303 .210 .308

.211

Query budget T = 100, α = 0.7:
.258 .377 .340 .490 .418∗ .587∗
PO
CNO-0.1 .248 .363 .317 .459 .386∗ .549∗
CNO-0.3 .212 .312 .271 .396 .330∗ .476∗
LNO-0.3 .231 .339 .277 .404 .324∗ .467∗
.331
LNO-1

.210 .309 .225 .330

.225

Baseline, α = 0, T = 0: τ = .206, ρ = .304

.217 .319
.211 .311
.205 .302
.209 .307
.207 .305

.255 .372
.247 .362
.232 .340
.229 .336
.213 .313

Table 2: Performance of multiple APL algorithms
(columns) using different oracles and query bud-
gets (rows). The baseline is the purely prior rank-
ing. All results except the baseline are averaged
over 50 document clusters in DUC’04. Aster-
isk: signiﬁcant advantage over other active learn-
ing strategies given the same oracle and budget T .

ranking to select one good and one bad summary
to query, while Unc performs more exploration by
querying the summaries that are least conﬁdent ac-
cording to the current ranking. We believe this ex-
plains the strong overall performance of Unc.

Additional experiments suggest that when we
only use the posterior ranking (i.e. letting α = 1),
no APL we test can surpass the baseline when
T = 10. Detailed results are presented in the sup-
plementary material. This observation reﬂects the
severity of the cold-start problem, conﬁrms the ef-
fectiveness of our prior-posterior trade-off mecha-
nism in combating cold-start, and indicates the im-
portance of tuning the α value (see Eq. (7)). This
opens up exciting avenues for future work.

6.2 RL Phase Performance

We compare two RL algorithms: TD(λ) (Sut-
ton, 1984) and LSTD(λ) (Boyan, 1999). TD(λ)
has been used in previous RL-based EMDS work
(Ryang and Abekawa, 2012; Rioux et al., 2014).
LSTD(λ) is chosen, because it is an improved TD
algorithm and has been used in the state-of-the-art
PbRL algorithm by Wirth et al. (2016). We let the
learning round (see Alg. 2) N = 5, 000, which we
found to yield good results in reasonable time (less
than 1 minute to generate a summary for one doc-
ument cluster). Letting N = 3, 000 will result in a
signiﬁcant performance drop, while increasing N
to 10,000 will only bring marginal improvement
at the cost of doubling the runtime. The learn-

Method

TD(λ)
LSTD(λ)
ILP

R1

.484
.458
.470

R2

.184
.159
.212

RL

RSU 4

.388
.366
N/A

.199
.185
.185

Table 3: Upper-bound performance comparison.
Results are averaged over all clusters in DUC’04.

ing parameters we use for TD(λ) are the same as
those by Rioux et al. (2014). For LSTD(λ), we let
λ = 1 and initialise its square matrix as a diag-
onal matrix with random numbers between 0 and
1, as suggested by Lagoudakis and Parr (2003).
The rewards we use are the U ∗ function introduced
in §5. Note that this serves as the upper-bound
performance, because U ∗ relies on the reference
summaries (see Eq. (6)), which are not available
in the interactive setting. As a baseline, we also
present the upper-bound performance of integer
linear programming (ILP) reported by P.V.S. and
Meyer (2017), optimised for bigram coverage.

Table 3 shows the performance of RL and ILP
on the DUC’04 dataset. TD(λ) signiﬁcantly out-
performs LSTD(λ) in terms of all ROUGE scores
we consider. Although the least-square RL algo-
rithms (which LSTD belongs to) have been proved
to achieve better performance than standard TD
methods in large-scale problems (see Lagoudakis
and Parr, 2003), their performance is sensitive to
many factors, e.g., initialisation values in the di-
agonal matrix, regularisation parameters, etc. We
note that a similar observation about the inferior
performance of least-square RL in EMDS is re-
ported by Rioux et al. (2014).

TD(λ) also signiﬁcantly outperforms ILP in
terms of all metrics except ROUGE-2. This is not
surprising, because the bigram-based ILP is opti-
mised for ROUGE-2, whereas our reward function
U ∗ considers other metrics as well (see Eq. (6)).
Since ILP is widely used as a strong baseline for
EMDS, these results conﬁrm the advantage of us-
ing RL for EMDS problems.

6.3 Complete Pipeline Performance

Finally, we combine the best techniques of the
APL and RL phase (namely Unc and TD(λ), re-
spectively) to complete APRIL, and compare it
against SPPI. As a baseline, we use the heuristic-
based rewards HU to train both TD(λ) (ranking-
based training, i.e. using HU to produce r(y|x) in
Eq. (5) to train) and SPPI (preference-based train-
ing, i.e. using HU for generating pairs to train

Oracle

Method

T

R1

R2

RSU 4

R1

R2

RSU 4

R1

R2

RL

RSU 4

DUC ’01
RL

DUC ’02
RL

DUC ’04

PO

CNO-0.1

CNO-0.3

LNO-0.3

LNO-1

Baselines

SPPI
APRIL
SPPI
APRIL

SPPI
APRIL
SPPI
APRIL

SPPI
APRIL
SPPI
APRIL

SPPI
APRIL
SPPI
APRIL

SPPI
APRIL
SPPI
APRIL

SPPI
TD(λ)

10
10
100
100

10
10
100
100

10
10
100
100

10
10
100
100

10
10
100
100

0
0

.332
.357
.353
.363

.331
.351
.350
.353

.320†
.339
.345
.349

.319†
.347
.321†
.350

.314†
.337
.320†
.347

.323
.324

.075
.087
.091
.091

.081
.081
.089
.084

.063†
.076
.079
.081

.067†
.084
.068†
.086

.058†
.072
.064†
.080

.068
.069

.264
.283
.284
.283

.265
.276
.279
.280

.253†
.266
.270
.275

.253†
.275
.252†
.277

.250†
.266
.255†
.274

.259
.256

.104
.119
.119
.118

.103
.112
.117
.115

.096†
.108
.111
.109

.096†
.109
.097†
.123

.092†
.104
.097†
.109

.098
.099

.357
.390
.391
.393

.358
.376
.377
.385

.354†
.370
.373
.376

.354†
.370
.352†
.380

.348†
.362
.351†
.369

.350
.350

.083
.108
.104
.107

.081
.102
.100
.103

.080
.091
.094
.097

.083
.095
.080
.079

.076†
.085
.078†
.089

.077
.081

.280†
.306
.306
.310

.279†
.296
.294
.302

.278†
.290
.295
.296

.280†
.289
.278†
.296

.273†
.286
.273†
.286

.278
.276

.116
.133
.136
.137

.114†
.126
.129
.134

.113†
.124
.125
.127

.113†
.125
.112†
.129

.110†
.119
.113†
.123

.112
.113

.378
.410
.392
.415

.372†
.403
.390
.411

.370†
.394
.386
.404

.375†
.398
.387
.407

.373†
.388
.381
.391

.372
.372

.098
.116
.106
.118

.093†
.111
.107
.117

.093†
.104
.104
.114

.095†
.108
.104
.112

.096†
.102
.099
.101

.093
.086

.299
.325
.312
.325

.295†
.320
.309
.325

.129
.149
.140
.151

.125†
.145
.138
.151

.295† 125†
.138
.312
.136
.307
.146
.320

.294†
.311
.309
.321

.297†
.307
.301
.308

.293
.292

.127†
.141
.136
.147

.126†
.134
.132
.136

.125
.122

Table 4: Comparison of APRIL and SPPI. All results are averaged over all clusters in each dataset.
Baselines: HU -trained SPPI and TD(λ), without any interaction (i.e. T = 0). Boldface: Comparable
(i.e. no signiﬁcant gaps exist) or signiﬁcantly better than SPPI with 100 rounds of interaction, under the
same oracle. Superscript †: Comparable or signiﬁcantly worse than the corresponding baseline.

DUC’01

DUC’02

DUC’04

Overall

APRIL
SPPI

3.57±.30
2.29±.29

4.14±.14
2.14±.14

3.86±.40
3.14±.34

3.86±.17
2.52±.18

Table 5: Human ratings for the summaries gener-
ated by APRIL and SPPI (mean±standard error).

SPPI) for up to 5,000 episodes. The baseline re-
sults are presented in the bottom rows of Table 4.

We make the following observations from Ta-
ble 4. (i) Given the same oracle, the performance
of APRIL with 10 rounds of interaction is com-
parable or even superior than that of SPPI after
100 rounds of interaction (see boldface in Table
4), suggesting the strong advantage of APRIL to
(ii) APRIL can sig-
reduce sample complexity.
niﬁcantly improve the baseline with either 10 or
100 rounds of interaction, but SPPI’s performance
can be even worse than the baseline (marked by †
in Table 4), especially under the high-noise low-
budget settings
(i.e., CNO-0.3, LNO-0.3, and
LNO-1 with T = 10). This is because SPPI lacks
a mechanism to balance between prior and poste-
rior ranking, while APRIL can adjust this trade-off

by tuning α (Eq. (7)). This endows APRIL with
better noise robustness and lower sample com-
plexity in high-noise low-budget settings. Note
that the above observations also hold for the other
two datasets,
indicating the consistently strong
performance of APRIL across different datasets.

As for the overall runtime, when budget T =
100, APRIL on average takes 2 minutes to interact
with an oracle and output a summary, while SPPI
takes around 15 minutes due to its expensive gra-
dient descent computation (see §3.1).

7 Human Evaluation

Finally, we invited real users to compare and eval-
uate the quality of the summaries generated by
SPPI and APRIL. We randomly selected three top-
ics (d19 from DUC’01, d117i from DUC’02 and
d30042 from DUC’04), and let both SPPI and our
best-performing APRIL interact with PO for 10
rounds on these topics. The resulting 100-word
summaries, shown in Figure 2, were presented to
seven users, who had already read two background
texts to familiarize with the topic. The users were
asked to provide their preference on the presented

Topic d30042 (DUC’04), SPPI: After meeting Libyan leader Moammar Gad-
haﬁ in a desert tent, U.N. Secretary-General Koﬁ Annan said he thinks an ar-
rangement for bringing two suspects to trial in the bombing of a Pan Am air-
liner could be secured in the “not too distant future.” TRIPOLI, Libya (AP)
U.N. Secretary-General Koﬁ Annan arrived in Libya Saturday for talks aimed
at bringing to trial two Libyan suspects in the 1988 Pan Am bombing over
Lockerbie, Scotland. Secretary General Koﬁ Annan said Wednesday he was
extending his North African tour to include talks with Libyan authorities. An-
nan’s one-day, 2nd graf pvs During his Algerian stay,

Topic d30042 (DUC’04), APRIL: TRIPOLI, Libya (AP) U.N. Secretary-
General Koﬁ Annan arrived in Libya Saturday for talks aimed at bringing to
trial two Libyan suspects in the 1988 Pan Am bombing over Lockerbie, Scot-
land. Annan’s one-day visit to meet with Libyan leader Col. Moammar Gadhaﬁ
followed reports in the Libyan media that Gadhaﬁ had no authority to hand over
the suspects. The 60-year-old Annan is trying to get Libya to go along with a
U.S.-British plan to try the two suspects before a panel of Scottish judges in the
Netherlands for the Dec. 21, 1988, bombing over Lockerbie, Scotland. Sirte
is 400 kilometers (250 miles) east of the Libyan capital Tripoli. During his
Algerian stay,

Topic d117i (DUC’02), SPPI: The Booker Prize is sponsored by Booker, an
international food and agriculture business. The novel, a story of Scottish low-
life narrated largely in Glaswegian dialect, is unlikely to prove a popular choice
with booksellers, who have damned all six books shortlisted for the prize as
boring, elitist and- worst of all- unsaleable. The shortlist of six for the Pounds
20,000 Booker Prize for ﬁction, announced yesterday, immediately prompted
the question ’Who ? ’ Japanese writer Kazuo Ishiguro won the 1989 Booker
Prize, Britain’s top literary award, for his novel “The Remains of the Day,”
judges announced Thursday. He didn’t win.

Topic d117i (DUC’02), APRIL: Australian novelist Peter Carey was awarded
the coveted Booker Prize for ﬁction Tuesday night for his love story, “Oscar
and Lucinda.” The Booker Prize is sponsored by Booker, an international food
and agriculture business, and administered by The Book Trust. British publish-
ers can submit three new novels by British and Commonwealth writers. Six
novels have been nominated for the Booker Prize, Britain’s most prestigious
ﬁction award, and bookmakers say the favorite is “The Remains of the Day” by
Japanese author Kazuo Ishiguro. On the day of the Big Event, Ladbroke, the
large British betting agency, posted the ﬁnal odds.

Topic d19 (DUC’01), SPPI: The issue cuts across partisan lines in the Senate,
with Minority Leader Bob Dole (R-Kan.) arguing against the White House po-
sition on grounds that including illegal aliens in the census is unfair to Amer-
ican citizens.. Loss of Seats Cited. Shelby’s amendment says only that the
secretary is to “make such adjustments in total population ﬁgures as may be
necessary, using such methods and procedures as the secretary determines fea-
sible and appropriate” to keep illegal aliens from being counted in congres-
sional reapportionment. “Some states will lose congressional seats because of
illegal aliens,” Dole argued. But there’s nothing simple about it.

Topic d19 (DUC’01), APRIL: In a blow to California and other states with
large immigrant populations, the Senate voted Friday to bar the Census Bu-
reau from counting illegal aliens in the 1990 population count. But the Senate
already has voted to force the Census Bureau to exclude illegal immigrants in
preparing tallies for congressional reapportionment. said that Georgia and Indi-
ana both lost House seats after the 1980 Census, and California and New York-
centers of illegal immigration- each gained seats. A majority of the members
of the House of Representatives has signaled support. The national head count
will be taken April 1, 1990.

Figure 2: Summaries generated by SPPI and APRIL used in the human evaluation experiments.

summary pairs and rate the summaries on a 5-
point Likert scale with higher scores for better
summaries. All users are ﬂuent in English.

In all three topics, all users prefer the APRIL-
generated summaries over the SPPI-generated
summaries. Table 5 shows the users’ ratings. The
APRIL-generated summaries consistently receive
higher ratings. These results are consistent with
our simulation experiments and conﬁrm the sig-
niﬁcant advantage of APRIL over SPPI.

8 Conclusion

We propose a novel preference-based interactive
learning formulation named APRIL (Active Pref-
erence ReInforcement Learning), which is able to
make structured predictions without referring to
the gold standard data.
Instead, APRIL learns
from preference-based feedback. We designed a
novel objective function for APRIL, which natu-
rally splits APRIL into an active preference learn-
ing (APL) phase and a reinforcement learning
(RL) phase, enabling us to leverage a wide spec-
trum of active learning, preference learning and
RL algorithms to maximise the output quality with
a limited number of interaction rounds. We ap-
plied APRIL to the Extractive Multi-Document
Summarisation (EMDS) problem, simulated the
users’ preference-giving behaviour using multiple
user-response models, and compared the perfor-
mance of multiple APL and RL techniques. Sim-
ulation experiments indicated that APRIL signif-

icantly improved the summary quality with just
10 rounds of interaction (even with high-noise
oracles), and signiﬁcantly outperformed SPPI in
terms of both sample complexity and noise robust-
ness. Human evaluation results suggested that real
users preferred the APRIL-generated summaries
over the SPPI-generated ones.

We identify two major lines of future work. On
the technical side, we plan to employ more ad-
vanced APL and RL algorithms in APRIL, such as
sample-efﬁcient Bayesian-based APL algorithms
(e.g., Simpson and Gurevych, 2018) and neural
RL algorithms (e.g. Mnih et al., 2015) to further
reduce the sample complexity of APRIL. On the
experimental side, a logical next step is to imple-
ment an interactive user interface for APRIL and
conduct a larger evaluation study comparing the
summary quality before and after the interaction.
We also plan to apply APRIL to more NLP appli-
cations, including machine translation, informa-
tion exploration and semantic parsing.

Acknowledgements

This work has been supported by the German
Research Foundation as part of the QA-EduInf
project (grant GU 798/18-1 and grant RI 803/12-
1). We thank the researchers and students from
TU Darmstadt who participated in our human
evaluation experiments. We also thank Johannes
F¨urnkranz, Christian Wirth and the anonymous re-
viewers for their helpful comments.

References

Steven Bird, Ewan Klein,

and Edward Loper.
2009. Natural Language Processing with Python.
O’Reilly.

Justin A. Boyan. 1999. Least-squares temporal differ-
ence learning. In Proceedings of the Sixteenth Inter-
national Conference on Machine Learning (ICML
1999), June 27–30, 1999, Bled, Slovenia, pages 49–
56.

Ralph Allan Bradley and Milton E Terry. 1952. Rank
analysis of incomplete block designs: I. The method
of paired comparisons. Biometrika, 39(3/4):324–
345.

Paul F. Christiano, Jan Leike, Tom Brown, Miljan Mar-
tic, Shane Legg, and Dario Amodei. 2017. Deep
Reinforcement Learning from Human Preferences.
In Advances in Neural Information Processing Sys-
tems 30: 31st Annual Conference on Neural Infor-
mation Processing Systems, December 4–9, 2017,
Long Beach, CA, USA, pages 4302–4310.

Prefer-
Wei Chu and Zoubin Ghahramani. 2005.
In Ma-
ence learning with Gaussian processes.
chine Learning, Proceedings of the Twenty-Second
International Conference (ICML 2005), August 7–
11, 2005, Bonn, Germany, pages 137–144.

Johannes F¨urnkranz and Eyke H¨ullermeier. 2010. Pref-
In Preference
erence learning: An introduction.
Learning, pages 1–17. Berlin/Heidelberg: Springer.

Stefan Henß, Margot Mieskes, and Iryna Gurevych.
2015. A reinforcement learning approach for adap-
tive single- and multi-document summarization. In
Proceedings of the International Conference of the
German Society for Computational Linguistics and
Language Technology (GSCL 2015), September 30–
October 2, 2015, University of Duisburg-Essen,
Germany, pages 3–12.

Kevin G. Jamieson and Robert D. Nowak. 2011. Ac-
In Ad-
tive ranking using pairwise comparisons.
vances in Neural Information Processing Systems
24: 25th Annual Conference on Neural Informa-
tion Processing Systems, December 12–14, 2011,
Granada, Spain, pages 2240–2248.

M.G. Kendall. 1948. Rank correlation methods. C.

Grifﬁn.

David C Kingsley and Thomas C Brown. 2010. Prefer-
ence uncertainty, preference reﬁnement and paired
comparison choice experiments. Land Economics,
86(3):530–544.

Julia Kreutzer, Artem Sokolov, and Stefan Riezler.
Bandit structured prediction for neural
2017.
In Proceedings of
sequence-to-sequence learning.
the 55th Annual Meeting of the Association for
Computational Linguistics (ACL 2017), Volume 1:
Long Papers, July 30–August 4, 2017, Vancouver,
Canada, pages 1503–1513.

Michail G Lagoudakis and Ronald Parr. 2003. Least-
squares policy iteration. Journal of Machine Learn-
ing Research, 4:1107–1149.

Chin-Yew Lin. 2004. ROUGE: A package for auto-
In Workshop on
matic evaluation of summaries.
Text Summarization Branches Out, Post-Conference
Workshop of ACL, July 21–26, 2004, Barcelona,
Spain, pages 74–81.

Lucas Maystre and Matthias Grossglauser. 2017. Just
sort it! A simple and effective approach to ac-
In Proceedings of the
tive preference learning.
34th International Conference on Machine Learning
(ICML 2017), August 6–11, 2017, Sydney, Australia,
pages 2344–2353.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. 2015. Human-level
control through deep reinforcement learning. Na-
ture, 518(7540):529–533.

Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Ranking sentences for extractive summariza-
tion with reinforcement learning. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies (NAACL-HLT
2018),Volume 1 (Long Papers), June 1-6, 2018, New
Orleans, LA, USA, pages 1747–1759.

Ani Nenkova and Kathleen McKeown. 2012. A survey
of text summarization techniques. In Charu C. Ag-
garwal and ChengXiang Zhai, editors, Mining Text
Data, pages 43–76. Boston: Springer.

Ramakanth Pasunuru and Mohit Bansal. 2018. Multi-
reward reinforced summarization with saliency and
entailment. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT 2018), Volume 2 (Short
Papers), June 1-6, 2018, New Orleans, LA, USA,
pages 646–653.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. CoRR, abs/1705.04304.

Joint
Avinesh P.V.S. and Christian M. Meyer. 2017.
optimization of user-desired content
in multi-
document summaries by learning from user feed-
back. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (ACL
2017): Volume 1: Long Paper, July 30–August 4,
2017, Vancouver, Canada, pages 1353–1363.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2015.
Sequence level
training with recurrent neural networks. CoRR,
abs/1511.06732.

Cody Rioux, Sadid A. Hasan, and Yllias Chali. 2014.
Fear the REAPER: A system for automatic multi-
document summarization with reinforcement learn-
In Proceedings of the 2014 Conference on
ing.
Empirical Methods in Natural Language Processing
(EMNLP 2014), October 25–29, 2014, Doha, Qatar,
pages 681–690.

Markus Zopf. 2018. Estimating summary quality with
pairwise preferences. In Proceedings of the 16th An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies (NAACL-HLT 2018),
Volume 1 (Long Papers), June 1–8, 2018, New Or-
leans, LA, USA, pages 1687–1696.

Seonggi Ryang and Takeshi Abekawa. 2012. Frame-
work of automatic text summarization using rein-
In Proceedings of the 2012
forcement learning.
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL 2012), July
12–14, 2012, Jeju Island, Korea, pages 256–265.

Edwin D. Simpson and Iryna Gurevych. 2018. Finding
convincing arguments using scalable bayesian pref-
erence learning. Transactions of the Association for
Computational Linguistics, 6:357–371.

Artem Sokolov, Julia Kreutzer, Christopher Lo, and
Stefan Riezler. 2016a. Learning structured predic-
tors from bandit feedback for interactive NLP.
In
Proceedings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2016):
Volume 1: Long Papers, August 7–12, 2016, Berlin,
Germany, pages 1610–1620.

Artem Sokolov, Julia Kreutzer, Stefan Riezler, and
Christopher Lo. 2016b. Stochastic structured pre-
diction under bandit feedback. In Advances in Neu-
ral Information Processing Systems 29: 30th An-
nual Conference on Neural Information Processing
Systems, December 5–10, 2016, Barcelona, Spain,
pages 1489–1497.

Charles Spearman. 1904. The proof and measurement
of association between two things. The American
Journal of Psychology, 15(1):72–101.

R. S. Sutton. 1984. Temporal Credit Assignment in Re-
inforcement Learning. Ph.D. thesis, University of
Massachusetts, Amherst.

Louis Leon Thurstone. 1927. A Law of Comparative
Judgement. Psychological Review, 34:278–286.

Paolo Viappiani and Craig Boutilier. 2010. Optimal
Bayesian recommendation sets and myopically op-
In Advances in Neural
timal choice query sets.
Information Processing Systems 23: 24th Annual
Conference on Neural Information Processing Sys-
tems, December 6-9 , Vancouver, British Columbia,
Canada, pages 2352–2360.

Christian Wirth, Riad Akrour, Gerhard Neumann, and
Johannes F¨urnkranz. 2017. A survey of preference-
based reinforcement learning methods. Journal of
Machine Learning Research, 18:4945–4990.

Christian Wirth, Johannes F¨urnkranz, and Gerhard
Neumann. 2016. Model-free preference-based rein-
forcement learning. In Proceedings of the Thirtieth
AAAI Conference on Artiﬁcial Intelligence, Febru-
ary 12–17, 2016, Phoenix, AZ, USA, pages 2222–
2228.

