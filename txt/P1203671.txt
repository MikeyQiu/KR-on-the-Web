9
1
0
2
 
g
u
A
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
4
9
7
1
0
.
8
0
9
1
:
v
i
X
r
a

Some Developments in Clustering Analysis on
Stochastic Processes

Qidi Peng∗

Nan Rao†

Ran Zhao‡

Abstract

We review some developments on clustering stochastic processes and come
with the conclusion that asymptotically consistent clustering algorithms
can be obtained when the processes are ergodic and the dissimilarity mea-
sure satisﬁes the triangle inequality. Examples are provided when the
processes are distribution ergodic, covariance ergodic and locally asymp-
totically self-similar, respectively.

Keywords:
ergodic processes, local asymptotic self-similarity

stochastic process, unsupervised clustering, stationary

1

Introduction

A stochastic process is an inﬁnite sequence of random variables indexed by
“time”. The time indexes can be either discrete or continuous. Stochastic
process type data have been broadly explored in biological and medical research
(Damian et al., 2007; Zhao et al., 2014; J¨a¨askinen et al., 2014; et al., 2018).
Unsupervised learning on stochastic processes (or time series) has increasingly
attracted people from various areas of research and practice. Among the above
unsupervised learning problems, one subject, called cluster analysis, aims to
discover patterns of stochastic process type data. There is a rich literature in
bioinformatics, biostatistics and genetics on clustering stochastic process type
data. We refer the readers to the review of Aghabozorgi et al. (Aghabozorgi
et al., 2015) for updates of cluster analysis on stochastic processes til 2015.
Recently Khaleghi et al. (Khaleghi et al., 2016; Khaleghi and Ryabko, 2014,
2012) obtained asymptotically consistent algorithms for clustering distribution
stationary ergodic processes, in the case where the correct number of clusters
is known or unknown. In their framework, the key idea is to deﬁne a proper
dissimilarity measure d between any 2 observed processes, which characterizes

∗Institute of Mathematical Sciences, Claremont Graduate University, Claremont, CA

91711. Email: qidi.peng@cgu.edu.

nan.rao@sjtu.edu.cn.

†School of Mathematical Sciences, Shanghai Jiao Tong University, Shanghai, China. Email:

‡Institute of Mathematical Sceinces and Drucker School of Management, Claremont Grad-

uate University, Claremont, CA 91711. Email: ran.zhao@cgu.edu.

1

the features of stationarity and ergodicity. Further Peng et al. (Peng et al.,
2019, 2018) derived consistent algorithms for clustering covariance stationary
ergodic processes and locally asymptotically self-similar processes.

In this framework we review the recent developments in cluster analysis on

the following 3 types of stochastic processes:

Type (1) distribution stationary ergodic processes;

Type (2) covariance stationary ergodic processes;

Type (3) locally asymptotically self-similar processes.

According to the nature of each type of processes, the ground-truths in the 3
clustering problems are deﬁned diﬀerently. In the ground-truth of Type (1), two
processes of identical process distributions are in one cluster; in the ground-truth
of Type (2), two processes having the same means and covariance structures are
in one cluster; for the third type, the pattern is the means and covariance
structures of the tangent processes.

From the summary we conclude that a suﬃcient condition for the clustering
algorithms (provided below) being consistent, is that the corresponding dissimi-
larity measure and its estimates satisfy the triangle inequality and its estimator
are consistent (they converge to the theoretical dissimilarity as the path length
goes up to the inﬁnity).

2 Asymptotically Consistent Algorithms

In (Khaleghi et al., 2016), assuming the correct number of clusters κ is known,
two types of datasets are considered in the cluster analysis: oﬄine dataset
and online dataset.
In the oﬄine dataset, the number of sample paths and
the length of each sample path do not vary with respect to time. However in
the online dataset, both can vary. In (Khaleghi et al., 2016) for each type of
datasets, by using a particular dissimilarity measure, asymptotically consistent
algorithms (Algorithm 1 for oﬄine dataset and Algorithm 2 for online dataset)
have been derived, aiming to cluster distribution stationary ergodic processes.
Here asymptotic consistency means the output clusters from the algorithm con-
verge to the ground-truths either in probability (weak sense) or almost surely
(strong sense). Based on Khaleghi et al.’s approaches, Peng et al. (Peng et al.,
2019, 2018) suggested asymptotically consistent algorithms that are valid for a
more general class of processes and dissimilarity measures.

Let X1, X2 be one of the 3 types of processes in the above section. We denote
by d(X1, X2) a dissimilarity measure between 2 stochastic processes X1, X2,
which satisﬁes the triangle inequality. And we denote by (cid:98)d(x1, x2) the estimate
of d(X1, X2), where for i = 1, 2, xi = (x(i)
ni ) is an observed sample
path of the process Xi, with length ni. Moreover, assume that (cid:98)d also veriﬁes
the triangle inequality and it is consistent: for all x1, x2, sampled from X1, X2

1 , . . . , x(i)

2

respectively,

where
vergence, respectively.

are given below.

(cid:98)d(x1, x2)

P or a.s.
−−−−−−−−−−→
min{n1,n2}→∞

d(X1, X2),

P
−→ and a.s.−−→ denote the convergence in probability and almost sure con-

The clustering algorithms suggested by Peng et al. (Peng et al., 2019, 2018)

Algorithm 1: Oﬄine clustering

Input: sample paths S = {z1, . . . , zN }; number of clusters κ.
(cid:98)d(zi, zj);

1 (c1, c2) ←−

argmax
(i,j)∈{1,...,N }2,i<j

2 C1 ←− {c1}; C2 ←− {c2};
3 for k = 3, . . . , κ do

ck ←− argmax
i=1,...,N

min
j=1,...,k−1

(cid:98)d(zi, zcj )

4

8

5 end
6 Assign the remaining points to the nearest centers:
7 for i = 1, . . . , N do
(cid:110)

(cid:98)d(zi, zj) : j ∈ Ck

(cid:111)
;

k ←− argmin
k∈{1,...,κ}
Ck ←− Ck ∪ {i}

9
10 end

Output: The κ clusters f (S, κ, (cid:98)d) = {C1, C2, . . . , Cκ}.

Theorem 2.1 Algorithms 1 and 2 are asymptotically consistent for the pro-
cesses of Types (1) and (2) respectively, provided that the correct number κ of
clusters is known, and the sample dissimilarity measure (cid:98)d is consistent and both
(cid:98)d and d satisfy the triangle inequality.

Proof. The consistency of Algorithms 1 and 2 applied for clustering processes
of Type (1) is proved in (Khaleghi et al., 2016); the consistency of the two
algorithms applied for clustering processes of Type (2) in proved in (Peng et al.,
2019). (cid:3)

It is worth noting that in the above proof, the key features used are the fact
that both d and (cid:98)d verify the triangle inequality and (cid:98)d is a consistent estimator
of d.

For clustering the processes of Type (3), an additional assumption is needed,

which will be introduced in Section 4.

For clustering the processes of Type (1), the speciﬁc form of d and (cid:98)d are
given in (Khaleghi et al., 2016). Then we mainly introduce the other 2 pairs of
(d, (cid:98)d) for clustering analysis on the processes of Types (2) and (3).

3

Algorithm 2: Online clustering

(cid:110)

Input: Sample paths

S(t) = {zt

1, . . . , zt

N (t)}

; number of

(cid:111)

t

clusters κ.
1 for t = 1, . . . , ∞ do

Obtain new paths: S(t) ←−

(cid:110)

zt
1, . . . , zt

N (t)

(cid:111)
;

Initialize the normalization factor : η ←− 0;
Initialize the ﬁnal clusters: Ck(t) ←− ∅, k = 1, . . . , κ;
Generate N (t) − κ + 1 candidate cluster centers:
for j = κ, . . . , N (t) do
1, . . . , C j
κ

(cid:9) ←− Alg1(cid:0)(cid:8)zt

(cid:9), κ(cid:1);

(cid:8)C j
k ←− min (cid:8)i ∈ C j
cj
γj ←−

k

min
k,k(cid:48)∈{1,...,κ},k(cid:54)=k(cid:48)

1, . . . , zt
j
(cid:9), k = 1, . . . , κ;
(cid:98)d(cid:0)zt
(cid:1);
cj
k

, zt
cj
k(cid:48)

wj ←− 1/j(j + 1);
η ←− η + wjγj

end
Assign each point to a cluster :
for i = 1, . . . , N (t) do

k ←− argmin

N (t)
(cid:80)
k(cid:48)∈{1,...,κ}
j=κ
Ck(t) ←− Ck(t) ∪ {i}

1
η

wjγj (cid:98)d(cid:0)zt

(cid:1);

i, zt
cj
k(cid:48)

end

17
18 end

Output: The κ clusters f (S(t), κ, (cid:98)d) = {C1(t), . . . , Cκ(t)},

t = 1, 2, . . . , ∞.

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

3 Dissimilarity Measure for Covariance Station-

ary Ergodic Processes

The deﬁnition of covariance stationary ergodic process is given below.

Deﬁnition 3.1 A stochastic process {Xt}t∈N is covariance stationary ergodic
if:

• its mean and covariance are invariant subject to any time shift;

• any of its sample autocovariance converges in probability to the theoretical

autocovariance function as the sample length goes to +∞.

The dissimilarity measure d and its sample estimate (cid:98)d suggested in Peng et
al. (Peng et al., 2019) to measure the distance between 2 covariance stationary
ergodic processes are given below:

4

Deﬁnition 3.2 The dissimilarity measure d between a pair of covariance sta-
tionary ergodic processes X (1), X (2) is deﬁned as follows:

Cov(X (1)

l

, . . . , X (1)

l+m−1), Cov(X (2)

l

, . . . , X (2)

(cid:17)
l+m−1)

,

d(X (1), X (2))
∞
(cid:88)

:=

wmwlρ

(cid:16)

m,l=1

where:

ﬁnite.

• The sequence of positive weights {wj} is chosen such that d(X (1), X (2)) is

• The distance ρ between 2 equal-sized covariance matrices M1, M2 is deﬁned
to be ρ(M1, M2) := (cid:107)M1 − M2(cid:107)F , with (cid:107)·(cid:107)F being the Frobenius norm.

Deﬁnition 3.3 For two processes’ paths xj = (X (j)
nj ) for j = 1, 2,
let n = min{n1, n2}, then the empirical covariance-based dissimilarity measure
between x1 and x2 is given by

1 , . . . , X (j)

(cid:98)d(x1, x2) :=

mn(cid:88)

n−m+1
(cid:88)

(cid:16)

wmwlρ

ν(X (1)

l...n, m), ν(X (2)

l...n, m)

(cid:17)

,

m=1

l=1

where:

• mn, chosen to be o(n), denotes the size of covariance matrices considered

in the estimator.

• ν(x, l, m) :=

(cid:80)n−m+1
i=l

(Xi...Xi+m−1)T (Xi...Xi+m−1)

n−m−l+2

tionary covariance matrices.

are the estimators of sta-

4 Dissimilarity Measure for Locally Asymptot-

ically Self-similar Processes

In this section we review the work on clustering processes of Type (3). Lo-
cally asymptotically self-similar processes play a key role in the study of fractal
geometry and wavelet analysis. They are generally not covariance stationary,
however, one can still apply the dissimilarity measure d introduced in Section 3
under some assumption (see (Peng et al., 2018)).

The following deﬁnition of locally asymptotically self-similar process is given

in (Boufoussi et al., 2008).

Deﬁnition 4.1 A continuous-time stochastic process

with its in-

dex H(·) being a continuous function valued in (0, 1), is called locally asymptot-
ically self-similar, if for each t ≥ 0, there exists a non-degenerate self-similar

(cid:110)

Z (H(t))

t

(cid:111)

t≥0

5

process

(cid:110)

Y (H(t))
u

(cid:111)

u≥0

with self-similarity index H(t), such that

(cid:40)

Z (H(t+τ u))

t+τ u

− Z (H(t))
t

(cid:41)

τ H(t)

u≥0

(cid:110)

f.d.d.
−−−−→
τ →0+

Y (H(t))
u

(cid:111)

,

u≥0

(4.1)

f.d.d.
−−−−→ is in the sense of all the ﬁnite dimensional distri-

where the convergence
butions.

u

}u is called the tangent process of {Z (H(t))

Here {Y (H(t))
}t at t (see (Falconer,
2002)). Throughout (Peng et al., 2018) it is assumed that the datasets are
sampled from a known number of processes satisfying the following condition:
Assumption (A): The processes are locally asymptotically self-similar with
distinct functional indexes H(·); their tangent processes’ increment processes
are covariance stationary ergodic.

t

Then from (4.1), Peng et al. (Peng et al., 2018) showed the following: under

Assumption (A), for τ being suﬃciently small,

(cid:110)

Z (H(t+τ (u+h)))

t+τ (u+h)

− Z (H(t+τ u))
t+τ u

(cid:111)

f.d.d.
≈

(cid:110)

τ H(t)X (H(t))

u

(cid:111)

u∈[0,Kh]

,

(4.2)

u∈[0,Kh]

where K is an arbitrary positive integer. Statistically, (4.2) can be interpreted
as: given a discrete-time path Z (H(t1))
with ti = ih∆t for each i ∈
{1, . . . , n}, sampled from a locally asymptotically self-similar process {Z (H(t))
},
its localized increment path with time index around ti, i.e.,

, . . . , Z (H(tn))
tn

t1

t

(cid:16)

z(i) :=

Z (H(ti+1))

ti+1

− Z (H(ti))
ti

, . . . , Z (H(ti+1+K ))
ti+1+K

− Z (H(ti+K ))
ti+K

(cid:17)

,

(4.3)

is approximately distributed as a covariance stationary ergodic increment pro-
∆tH(ti)X (H(ti))
. This fact inspires one
cess of the self-similar process

(cid:110)

(cid:111)

u

u∈[0,Kh]

to deﬁne the sample dissimilarity measure between two paths of locally asymp-
totically self-similar processes z1 and z2 as below:

(cid:98)d∗(z1, z2) :=

1
n − K − 1

n−K−1
(cid:88)

i=1

(cid:98)d(z(i)

1 , z(i)
2 ),

(4.4)

where z(i)

1 , z(i)

2 are the localized increment paths deﬁned as in (4.3).
Accordingly, the consistency of Algorithms 1 and 2 can be expressed in the

following way:

Theorem 4.2 Under Assumption (A), Algorithms 1 and 2 are approximately
asymptotically consistent, if (cid:98)d is replaced with (cid:98)d∗.

In Theorem 4.2, “approximately” is in the sense of Eq. (4.2).

6

5 Simulation Study

In the frameworks of khaleghi et al. (Khaleghi et al., 2016), Peng et al. (Peng
et al., 2019) and Peng et al. (Peng et al., 2018), simulation study are provided.
In (Khaleghi et al., 2016), a distribution stationary ergodic process is simulated
based on random walk; in (Peng et al., 2019) the increment process of fractional
Brownian motion (Ayache and L´evy-V´ehel, 2004) is picked as an example of
covariance stationary ergodic process; in (Peng et al., 2018), simulation study is
performed on the so-called multifractional Brownian motion (Peltier and V´ehel,
1995), which is an excellent example of locally asymptotically self-similar pro-
cess. The simulation study results for clustering distribution stationary ergodic
processes are given in (Khaleghi et al., 2016). Here we summarize the results
for clustering the processes of Types (2) and (3), from Peng et al. (Peng et al.,
2019) and (Peng et al., 2018) respectively.

5.1 Clustering Processes of Type (2): Fractional Brownian

Motion

Fractional Brownian motion (fBm) {BH (s)}s≥0, as a natural extension of the
Brownian motion, was ﬁrst introduced by Kolmogorov in 1940 and then pop-
ularized by Mandelbrot and Taqqu (Mandelbrot and van Ness, 1968; Taqqu,
2013). The inﬂuences made by the fractional Brownian motion model have
been on a great many ﬁelds such as biological science, physical sciences and
economics (see (H¨oﬂing and Franosch, 2013)). As a stationary increment pro-
cess, it is shown that the increment process of the fBm is covariance stationary
ergodic (see Maruyama (1970); `Slezak (2017)).

In (Peng et al., 2019), the clustering algorithms are performed on a dataset
of 100 paths of fBms with κ = 5 clusters. In the sample dissimilarity measure
the so-called log∗-transformation is applied to increase the eﬃciency of the al-
gorithms. One considers the mis-clustering rates to be the measure of ﬁtting
errors. The top ﬁgure in Figure 1 presents the comparison results of the of-
ﬂine and online algorithms, based on the behavior of mis-clustering rates as
time increases. Both algorithms show to be asymptotically consistent as the
mis-clustering rates decrease.

5.2 Clustering Processes of Type (3): Multifractional Brow-

nian Motion

Multifractional Brownian motion (mBm) {WH(t)(t)}t≥0, as a natural general-
ization of the fBm, was introduced in (Peltier and V´ehel, 1995; Ayache et al.,
2000). Then it was quickly applied to describe phenomena in for instance molec-
ular biology (Marquez-Lago et al., 2012), biomedical engineering (Buard et al.,
2010) and biophysics (Humeau et al., 2007).

It can be obtained from Boufoussi et al. (2008) that the mBm is locally

asymptotically self-similar satisfying Assumption (A).

7

The datasets of mBms for testing the 2 clustering algorithms are similar to
those of fBms. The performance of the algorithms are shown in the bottom
ﬁgure in Figure 1. Similar conclusion can be drawn that both oﬄine and online
algorithms are approximately asymptotically consistent.

Figure 1: The top graph illustrates the mis-clustering rates of oﬄine algorithm
(Algorithm 1, solid red line), and online algorithm (Algorithm 2, dashed blue
line) for fBm datasets. The bottom graph illustrates the mis-clustering rates of
oﬄine algorithm (Algorithm 1, solid red line), and online algorithm (Algorithm
2, dashed blue line) for mBm datasets.

8

References

Aghabozorgi, S., Shirkhorshidi, A. S., and Wah, T. Y. (2015). Time-series

clustering - a decade review. Information Systems, 53:16–38.

Ayache, A., Cohen, S., and L´evy-V´ehel, J. (2000). The covariance structure of
multifractional Brownian motion, with application to long range dependence.
In Acoustics, Speech, and Signal Processing, 2000. ICASSP’00. Proceedings.
2000 IEEE International Conference on, volume 6, pages 3810–3813. IEEE.

Ayache, A. and L´evy-V´ehel, J. (2004). On the identiﬁcation of the pointwise
H¨older exponent of the generalized multifractional Brownian motion. Stochas-
tic Processes and their Applications, 111(1):119–156.

Boufoussi, B., Dozzi, M., and Guerbaz, R. (2008). Path properties of a class of
locally asymptotically self-similar processes. Electronic Journal of Probability,
13(29):898–921.

Buard, B., Humeau, A., Rousseau, D., Chapeau-Blondeau, F., and Abra-
hamc, P. (2010). Pointwise H¨older exponents of a model for skin laser
doppler ﬂowmetry signals based on six nonlinear coupled oscillators with
linear and parametric couplings: Comparison with experimental data from
young healthy subjects. IRBM, 31:175–181.

Damian, D., Oreˇsiˇc, M., and Verheij, E. e. a. (2007). Applications of a new sub-
space clustering algorithm (COSA) in medical systems biology. Metabolomics,
3(1):69–77.

et al., M. (2018). Clustering gene expression time series data using an inﬁnite

Gaussian process mixture model. PLoS Comput Biol, 14(1):e1005896.

Falconer, K. (2002). Tangent ﬁelds and the local structure of random ﬁelds.

Journal of Theoretical Probability, 15(3):731–750.

H¨oﬂing, F. and Franosch, T. (2013). Anomalous transport in the crowded world

of biological cells. Reports on Progress in Physics, 76(4):046602.

Humeau, A., Chapeau-Blondeau, F., Rousseau, D., Tartas, M., Fromy, B., and
Abraham, P. (2007). Multifractality in the peripheral cardiovascular system
from pointwise H¨older exponents of laser doppler ﬂowmetry signals. iophysical
Journal, 93(12):L59–L61.

J¨a¨askinen, V., Parkkinen, V., Cheng, L., and Corander, J. (2014). Bayesian
clustering of DNA sequences using markov chains and a stochastic partition
model. Stat. Appl. Genet. Mol. Biol., 13(1):105–121.

Khaleghi, A. and Ryabko, D. (2012). Locating changes in highly dependent data
with unknown number of change points. In Advances in Neural Information
Processing Systems, pages 3086–3094.

9

Khaleghi, A. and Ryabko, D. (2014). Asymptotically consistent estimation of
the number of change points in highly dependent time series. In International
Conference on Machine Learning, pages 539–547.

Khaleghi, A., Ryabko, D., Mari, J., and Preux, P. (2016). Consistent algorithms
for clustering time series. Journal of Machine Learning Research, 17(3):1–32.

Mandelbrot, B. and van Ness, J. W. (1968). Fractional Brownian motions,

fractional noises and applications. SIAM Review, 10(4):422–437.

Marquez-Lago, T. T., Leier, A., and Burrage, K. (2012). Anomalous diﬀu-
sion and multifractional brownian motion: simulating molecular crowding and
physical obstacles in systems biology. IET Systems Biology, 6(4):134–142.

Maruyama, G. (1970). Inﬁnitely divisible processes. Theory of Probability and

its Applications, 15(1):1–22.

Peltier, R.-F. and V´ehel, J. L. (1995). Multifractional Brownian motion: deﬁ-
nition and preliminary results. apport de recherche de l’INRIA, INRIA, no.
2645:239–265.

Peng, Q., Rao, N., and Zhao, R. (2018). Clustering analysis on locally
asymptotically self-similar processes with known number of clusters. arXiv,
1804.06234v1.

Peng, Q., Rao, N., and Zhao, R. (2019). Covariance-based dissimilarity measures
applied to clustering wide-sense stationary ergodic processes. Accepted by
Machine Learning, arXiv, 1801.09049v4.

`Slezak, J. (2017). Asymptotic behaviour of time averages for non-ergodic Gaus-

sian processes. Annals of Physics, 383:285–311.

Taqqu, M. S. (2013). Benoˆıt Mandelbrot and fractional Brownian motion. Sta-

tistical Science, 28(1):131–134.

Zhao, W., Zou, W., and Chen, J. J. (2014). Topic modeling for cluster analysis

of large biological and medical datasets. BMC Bioinformatics, 15:S11.

10

