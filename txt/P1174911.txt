9
1
0
2
 
v
o
N
 
6
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
9
3
3
9
0
.
5
0
7
1
:
v
i
X
r
a

Rejection-Cascade of Gaussians: Real-time
adaptive background subtraction framework

B Ravi Kiran1, Arindam Das2, and Senthil Yogamani3

1 Navya, Paris, France
2 Detection Vision Systems, Valeo India
3 Valeo Vision Systems, Galway, Ireland
ravi.kiran@navya.tech, {arindam.das,senthil.yogamani}@valeo.com

Abstract. Background-Foreground classiﬁcation is a well-studied prob-
lem in computer vision. Due to the pixel-wise nature of modeling and
processing in the algorithm, it is usually diﬃcult to satisfy real-time
constraints. There is a trade-oﬀ between the speed (because of model
complexity) and accuracy. Inspired by the rejection cascade of Viola-
Jones classiﬁer, we decompose the Gaussian Mixture Model (GMM) into
an adaptive cascade of Gaussians(CoG). We achieve a good improvement
in speed without compromising the accuracy with respect to the baseline
GMM model. We demonstrate a speed-up factor of 4-5x and 17 percent
average improvement in accuracy over Wallﬂowers surveillance datasets.
The CoG is then demonstrated to over the latent space representation
of images of a convolutional variational autoencoder(VAE). We provide
initial results over CDW-2014 dataset, which could speed up background
subtraction for deep architectures.

Keywords: Background Subtraction · Rejection Cascade · Real-time

1

Introduction

Background subtraction is critical component of surveillance applications (indoor
and outdoor), action recognition, human computer interactions, tracking, exper-
imental chemical procedures that require signiﬁcant change detection. Work on
background subtraction started since the 1970s and even today it is an active
open problem. There have been a host of methods which have been developed
and below is a short review which will serve to aid understanding our algorithm.
A survey by [5] provides an overview of common methods which includes Frame
diﬀerencing (FD), Running Gaussian average (RGA), Gaussian Mixture Model
(GMM) and Kernel Density Estimation (KDE). We employ these basic methods
in a structured methodology to develop our algorithm.

A survey of variants of GMM, issues and analysis are presented in [2]. In our
work, we focus on solving the variable-rate adaptation problem and improving
the performance. Abstractly, our work tries to fuse several algorithms to achieve
speed and accuracy and we list similar methods here. Similar attempts have been
[7] and [3] used a Hierarchical background
made by the following researchers.

2

Kiran et al.

subtraction method that operates in diﬀerent scales over the image : namely
pixel, region and image level, while their models themselves are not hierarchical.
Authors [14] switch between GMM and RGA models, while choosing a complex
model for complicated backgrounds and simple model for simpler backgrounds.
They use an entropy based measure to switch between the diﬀerent models.
We brieﬂy describe our observations and improvement over the standard GMM
from [6]. We observe in most cases, background subtraction is an asymmetric
classiﬁcation problem with probability of foreground pixel being much lesser than
that of background. This assumption fails in the case of scenes like highways, a
busy street, etc. In our work, we focus mainly on surveillance scenarios where
there is very low foreground occupancy. Our framework exploits this fact and
at the same time handles variable rate changes in background and improves
accuracy. Our key contributions in this paper include: 1. Decomposition of GMM
to form an adaptive cascade of classiﬁers - Cascade of Gaussians (CoG) which
handles complex scenes in an eﬃcient way to obtain real-time performance. 2.
A conﬁdence estimate for each pixels classiﬁcation which would be used to vary
the learning rate and thresholds for the classiﬁers and adaptive sampling. 3.
Learning a time windowed KDE from the training data-set which would act as a
prior to the Adaptive Rejection Cascade and also help the conﬁdence estimate.
The decomposition of the GMM into the cascade is similar to the increasing
true positive detection rate inspired by the Viola Jones Rejection Cascade [9].
Authors
[8] provided an optimized lookup for highly probable colors in the
incoming background pixels thus providing speedup in the access.

2 Components of the Cascade

This section describes the diﬀerent components of the rejection cascade and how
they were determined. The rejection cascade is accompanied by the conﬁdence
measure to make an accurate background classiﬁcation at each level of the cas-
cade.

Scene Prior in Background Model: The process of distinguishing lin-
early varying background and noisy pixels is a challenge and critical since the
background subtraction model intrinsically has no additional attribute to sepa-
rate them. For this scenario, in our approach we introduce a prior probability for
every pixel (eqn 1). The non-parametric probability distribution for the pixels
assuming independent R,G,B channels is now given too. The Scene prior basi-
cally provides an non-parametric estimate of pixel-values value over N frames
during training. The choice of N is empirical and depends on how much dynamic
background and foreground is present in the training frames. To obtain complete
variability we choose as large N as possible. Henceforth we refer to Scene Prior
as the prior. In the training phase we estimate the underlying temporal distri-
bution of pixels by calculating the kernel function that approximates the said
distribution. Our case primarily concentrates on long surveillance videos with
suﬃcient information (minimal foreground) available in the training sequence
that decides N . For the standard GMM model(assuming the covariance matrix

Rejection-Cascade of Gaussians

3

is diagonal) the updates of the parameters include:

K
(cid:88)

i=1

P (In(x, y)) =

ωi,n ∗ η(In(x, y), µi,n, σi,n,

ωn+1,k(x, y) ←−(1 − α)ωn,k(x, y) + α(Mi,n+1)
µn+1,k(x, y) ←−(1 − ρ)µn,k(x, y) + ρIn(x, y)
n+1,k(x, y) ←−(1 − ρ)σ2
σ2
n,k(x, y) + ρIn(x, y)

(1)

Where Kσ, represents the gaussian kernel and σ the scale or bandwidth.
This Kernel function is calculated to provide the modes of the diﬀerent pixels.
Where η represents the pixel mode distribution obtained in equation 1, where
ωi represents the ratio of the component i in the distribution of pixel In(x, y),
and µi,σi are the parameters of the component, M represents 0 or 1 based on a
component match and ﬁnally α represents the learning rate of the pixel model.
The α is intialised for all pixels usually, there has been work in adapting it based
on the pixel entropy. We use the pixel gradient value distribution to do the same.
Determining Learning Rate Hyper-parameters: Besides the kernel
density, we also estimate the dynamic nature of the pixels in the scene. This
is obtained by the clustering the residue between consecutive frames into 3 cat-
egories : into static/drifting, oscillating and dynamic pixels (Fig 2 top right).
This helps resolve a pixel drift versus a pixel jump as shown in example below
in ﬁgure. Once we have the residue Rn(x, y) = In(x, y) − In−1(x, y), n ∈ [1, N ],
we evaluate the normalized histogram over the residue values. We select bins
intervals to extract the 3 classes based on the dynamic nature of pixels. A peaky
ﬁrst bin implies near zero residue, thus a drift or static pixels. A peaky sec-
ond bin implies oscillating pixels and the other cases are considered as dynamic
pixels. Based on these values we choose the weights for the conﬁdence measure
(explained in the next section). This frequency over each bin sets the learning
rate for the pixel. The process of obtaining the right learning rates(conﬁdence
function) from the normalized binned histogram values to determine α, β and γ
test for the learning rates have determined empirically by shape matching the
histograms.

Clustering Similar Background - Spatio-Temporal Grouping: The
next step in the training phase is to determine background regions of pixels, in
the frame that behave similarly in terms of adapted variance, number of modes,
and optimally use fewer parameters and lesser instructions to update this speciﬁc
region’s, pixel models. The problem deﬁnition can be formalized as: We are given
N x(f ramesize) pixels and for each pixel In(x, y) we have a set of matches of the
form (In(x, y), In(x(cid:48), y(cid:48)))tn , which means that pixel In(x, y) correlated with pixel
In(x(cid:48), y(cid:48)) at frame number n . From these N matches, we construct a discrete
time series xi(t) by clustering pixel Fx, yn at time interval t frames. A time series
of the pixel In(x, y) values at frame n0. Intuitively, xi measures the correlation
in behavior of pixels over time window t. For convenience we assume that time
series xi have the same length. We group together pixel value time series so that
similar behavior is captured by similarity of the time series xi(t). This way we can
infer which pixels have a similar temporal pattern variances and modalities, and

4

Kiran et al.

we can then consider the center of each cluster as the representative common
pattern of the group. This helps us cluster similar behaving pixels together.
This is can be seen a spectral clustering problem as described in [1]. We try a
simpler approach here ﬁrst by clustering the adapted pixel variances(matrix V)
and weights(matrix R) of ﬁrst dominant mode of pixels within a mixture model.

1. Get N frames & estimate pixel-wise µ(t), σ(t), ω(t)
2. Form matrix whose rows are adapted variance and ranked weight observa-
tions, while columns are variables V and R, V (tk, i) = I(tk), k = 1 : N

3. Obtain covariance matrices Rcov = Cov(R), Vcov = Cov(V )
4. Perform K-means clustering with K=3 (for temporal pixel residue due to

dynamic, oscillating, or drifting BG).
5. Threshold for pixels within 0.7 − 0.5σ
6. Calculate the KDE of given cluster & the joint occurrence distribution and

associated weight ω1, µ1 and σ1

where µ1 is ﬁrst dominant common cascade level at grouped pixels. This suf-
fers from the setback that the variances chosen temporally do not correspond
to mean values associated with the maximum eigen value as obtained in case of
Spectral Clustering. So we have the pixel variance and adapted weight (dominant
mode) covariance matrices R(xi, yi) = Cov(V ar(In(xi, yi))) and W (xi, yi) =
Cov(V ar(Wn(xi, yi))). A single gaussian is ﬁt over thresholded covariance ma-
trices (Adapted variance and ﬁrst dominant mode weight).

rn =µadvar − σadvar < var(Rcov) < µadvar + σadvar
wn =µadw − σadw < var(Wcov) < µadw + σadw

(2)

The parameters µadvar, σadvar and µadw,σadw represent the mean and stan-
dard deviation of the cluster of pixel variances and adapted weights of the ﬁrst
dominant modes. The fundamental clustering algorithm requires Data set Rcov
and Vcov, number of clusters - quantization of the adapted weights or variances,
Gram matrix [1]. One critical point to note here is that, when we do not choose
to employ spatio-temporal grouping, and reduce the number of parameters and
consequent updates, we can use the Scene Prior covariance estimation to increase
the accuracy of the foreground detection. This is very similar to the background
subtraction based on Co-occurrence of Image Variations.

Conﬁdence Measure : The conﬁdence measure is a latent variable use to
aid the Rejection Cascade to obtain a measure of ﬁtness for the classiﬁcation of
a pixel based on various criteria. The Conﬁdence Cn(x, y) for a pixel In(x, y) is
given by Cn(x, y) = αP (x, y) + β(∆nI(x, y) + γM (In(x, y)).

Here, M () represents the diﬀerence between the current pixel value In(x, y)
and the parameters of the model occurring at the top of the ordered Rejection
cascade described below, while ∆nI(x, y) = In(x, y) − In−1(x, y). As seen in
the ordered tree, the ﬁrst set of parameters would be the ﬁrst dominant mode
- (µ1 + σ1, µ1 − σ1). This is carried out based on the level in which the pixel
gets successfully classiﬁed. P () represents the probability of occurrence of the
pixel from the KDE. The values of α β and γ are determined by the normalized

Rejection-Cascade of Gaussians

5

temporal residue distribution (explained above). The physical signiﬁcance and
implications of α β and γ- α says how conﬁdent the region is and regions that
are stable (for example from the segments from clustering adapted variances and
weights of training phase pixel models) would have high α values. While the value
of β determines how fast the pixel would need to adapt to new incoming values
and this would mean a lower eﬀect of the prior distribution. The ﬁnal parameter
γ determines the consistency of the pixel belonging to a model and this would
change whenever the pixels behavior is much more dynamic (as opposed to a
temporal residue weighting it).

Conﬁdence based temporal sampling: Applying multiple modes of back-
ground classiﬁers and observing the consistency in their model parameters (mean,
variance, and connectivity) we predict the future values of these pixels. A thresh-
old on conﬁdence function value determined by using stable regions(using region
growing) as a reference is used to select the pixels both spatially and temporally.
The description of the conﬁdence measure is given in more detail in section 2.3.
The pixels with low conﬁdence reﬂect regions R over the frame with activity
and thus a high probability of ﬁnding pixels whose label are in transition (FG-
BG). Thus by thresholding the conﬁdence function we sub-sample the incoming
pixels spatio-temporally. This intuition is when pixel values arriving now are
within the ﬁrst dominant mode’s 0.7σ region, and even more so within the CHP
level for a large number of frames, the conﬁdence value saturates. The Region
R(xi, yi) = Cn(xi, yi) > CScenceP rior(xi, yi) is just a thresholded binary map of
this conﬁdence value. This is demonstrated in the analysis in section 3.

Cascade of Gaussians CoG The proposed method can be viewed as a
decomposition of the GMM in an adaptive framework so as to reduce complexity
and improve accuracy using a strong prior to determine the scenarios under
which said gains can be achieved. The prior is used to determine the modality of
the pixels distribution and any new value is treated as a new mean with variance
model. The Cascade can be seen to consist of K Gaussians which are ordered
based on the successful classiﬁcation of the pixel. During steady state the ordered
cascade conforms to the Viola Jones Rejection Cascade with decreasing positive
detection rates.

The cascade is ﬁrst headed by a Consistent Hypothesis Propagation (CHP)
classiﬁer which basically repeats the labeling process on the current pixel if its
value is equal to the previous value (previous frame). This CHP classiﬁer is
then followed by an ordered set of Gaussians ωi.η(µi, σi) including the spatio-
temporally grouped parameters. The tree ordering is diﬀerent for diﬀerent pixel
and the order is decided based on the prior distribution (KDE) of the pixel
and the temporal consistency of the pixel in the diﬀerent levels. When the pixel
values do not belong to any of the dominant modes based on the prior, we have
scenario where the beta weight and gamma weight only considered and alpha is
rejected (Prior Nulliﬁed).

The rejection cascade assumes that the frequency of occurrences of fore-
ground detections is lesser than that of the background. This idea was ﬁrst
[9]. For the rejection cascade the
introduced in the classic Viola-Jones paper

6

Kiran et al.

Fig. 1. Top-left : Elements of CoG : CHP, ﬁrst and second modes of gaussians and
spatio-temporal window of CoG. Top-right : Dynamic Pixel Vs Oscillation Vs Pixel
Drift, Bottom : 1. Pixels in CHP(red), Mode 1(green), Mode 2(blue),Mode 3(vio-
let),Foreground(white) 2. Normalized pixel count over elements of Cascade of Gaus-
sians CHP, ﬁrst and Second modes of Gaussians.

training phase produces a sequence of features with decreasing rates of negative
rejections. In our case we arrange the diﬀerent classiﬁers in increasing complex-
ity to maximize the speed. We observe in practice that, this cascade would also
produce decreasing rates of negative rejections. The critical diﬀerence in this re-
jection cascade is that the classiﬁer in each level of the cascade is evolving over
time. To make adaptation eﬃcient we adapt only the active level of the cascade,
thus resulting in only one active update at a time, and during a transition the
parameters are updated.

The performance of diﬀerent rejection cascade elements is depicted in Figure
2. It depicts cascade elements with increasing complexity (and consequently ac-
curacy) have higher performance. These times were obtained over 4 videos from
the wallﬂowers data set by [7] of diﬀerent types of dynamic background. This
by itself can stand for the possible amount of speedup that can be obtained
when the Rejection Cascade is operated on pixels adaptively based on the na-
ture of the pixel. In a similar observation we saw that the number of pixels (in
each of these 4 videos) was distributed in diﬀerent manner amongst the 4 levels.
This is seen in ﬁgure 2. Thus we see that even though the number of pixels
corresponding to dynamic nature of pixel varies with the nature of the video,
there is greater number of pixels on an average corresponding to low complexity
Cascade elements. The rejection cascade for BG subtraction was formed by de-
termining (same as in [9]) the set of background pixel classiﬁers (or in our case

Rejection-Cascade of Gaussians

7

models like attentional operator in Viola Jones) and is organized as a degener-
ate tree such that it has decreasing false positive rate as we proceed down the
cascade. The performance of diﬀerent rejection cascade elements are depicted
in Figure 2. It depicts cascade elements with increasing complexity (and con-
sequently accuracy) have higher performance. These times were obtained over
diﬀerent types of static and dynamic background. This by itself can stand for the
possible amount of speedup that can be obtained when the Rejection Cascade
is operated on pixels adaptively based on the nature of the pixel. In a similar
observation we saw that the number of pixels (in each of these 4 videos) was
distributed in diﬀerent manner amongst the 4 levels. This is seen in ﬁgure 2.
Thus we see that even though the number of pixels corresponding to dynamic
nature of pixel varies with the nature of the video, there is greater number of
pixels on an average corresponding to low complexity Cascade elements. The
learning rate for the model is calculated as a function of the conﬁdence measure
of the pixels. The abrupt illumination change is detected in the ﬁnal level of the
rejection cascade, by adding a conditional counter. This counter measures the
number of pixels that are not modeled by the penultimate cascade element. If
this value is above a threshold we can assume an abrupt illumination change
scenario. This threshold is around seven tenth of the total number of pixels in
the frame [7].

3 Analysis & VAE-COG

3.1 Scene Prior Analysis

Here we discuss the the Scene Prior and its diﬀerent components. First with
regard to the clustering pixels based on their dynamic nature similarity, we
show results of various clustering methods and their intuitions. The ﬁrst model
considers the time series of variances of said pixels in the N frames of training.
The covariance matrix is calculated for the variances of the pixels. This can
loosely act as the aﬃnity matrix for the describing similar behavior of a pair
of pixels. The weight of the ﬁrst dominant mode is also considered to form the
aﬃnity matrix.

3.2 Cascade Analysis

The CoG is faster on two accounts : Firstly it is cascade of simple-to-complex
classiﬁers, CHP to RGA, and averaging over the performance (seen in ﬁgure), we
see an improvement in speed of operation, since the simpler cases of classiﬁcation
outweigh the complex ones. Secondly it models the image as a spatio-temporal
group of super pixels that needs a single set of parameters to update, even more
so, when the conﬁdence of the pixel saturates, the Cascade updates are halted,
providing huge speedups. Though it is necessary to mention that the window
of sampling is chosen empirically and in scale with the conﬁdence saturation
values. The average speedup of the rejection tree algorithm is calculated as :

8

Kiran et al.

Fig. 2. The CoG rejection cascade over the latent space representation of the
convolutional-VAE. The ﬁlters are all size 3x3.

I(x,y)
where x,y go over all indices of image, ni refers the ratio of background
(cid:80)
i sini
pixels labeled mean or mean with variance w.r.t the total number of background
pixels in the image, si is the normalized ratio of the time it takes for level i BG
model to evaluate and label a pixel as background. The values of n and s were
proﬁled over various videos for diﬀerent durations. Also we show the distribution
of the CHP pixels as well as the ﬁrst 3 dominant modes within diﬀerent frames of
Waving tree and Time of Day videos with 40 frames of training each. We can see
a huge occupancy of Red (CHP) for both background and foreground pixels. Here
we explain the conﬁdence measure and eﬀect on accuracy of the GMM model. We
obtain a speedup of 2x-3x with the use of the Adaptive Rejection cascade based
GMM. This speedup goes up at the eﬀectiveness of accuracy of conﬁdence based
spatio-temporal sampling to 4-5x. This is evident in the Cascade level population
(in ﬁgure 2). We observe a 17% improvement in accuracy over the baseline model
because of adaptive modelling to handle diﬃcult scenarios explicitly using scene
priors.

3.3 Latent space CoG with VAEs

CNNs have become become the state-of-the-art models for various computer
vision tasks. Our proposed framework is generic and can be extended to CNN
models. In this section, we study a possible future extension of the the Rejection
cascade to the Variational AutoEncoder (VAE). There has been recent work on
using auto-encoders to learn dynamic background for the subtraction task [11].
Rejection cascades have also been employed within convolutional neural networks
architectures for object detection [12]. VAEs one of the most interpretable deep
generative models.

VAEs are deep generative models that approximate the distribution for high-
dimensional vectors x that correspond to pixel values in the image domain. Like
a classical auto-encoder. VAEs consists of a probabilistic encoder qφ(x|z) that
reduces the input image to latent space vector z and enforces a Gaussian prior,
and a probabilisic decoder pθ(x|z) that reconstructs these latent vectors back to
the original images. The loss function constitutes of the KL-Divergence regular-
ization term, and the expected negative reconstruction error with an additional
KL-divergence term between the latent space vector and the representation with
a mean vector and a standard deviation vector, that optimizes the variational
lower bound on the marginal log-likelihood of each observation [4]. The classical
cascade : CHP, ordered sequence of modes of GMM (µi, σi), can now be envis-
aged in the latent space for a multivariate 1-Gaussian N (z, 0, I). The future goal

Rejection-Cascade of Gaussians

9

Fig. 3. The input-output pairs and absolute value of residue between input-output
pairs from a Convolutional VAE : top half without foreground bottom half with fore-
ground. We remark that the dynamic background such as the snow has been removed.
The right column demonstrates the 2d-Histogram over the latent space z of the CVAE
(top) and the histogram over the temporal residue over z for the same test sequence.

would be to create Early rejection classiﬁers as in [13] for classiﬁcation tasks,
where within each layer of the probabilistic encoder we are capable of measuring
the log-likelihood of being foreground. Storing previous latent space vectors for
the CHP test would require addition memory aside that assigned to the latent
space mean and variance vectors. VAEs are an ideal extension to the rejection
cascade since the pixel-level tests in CoG are now performed by the VAE in
the latent space, over which a likelihood can be evaluated. We also gain the
invariance to positions, orientations, pixel level perturbations, and deformations
in mid-level features due the convolutional architecture. A convolutional VAE
with latent space of 16 dimensions was trained on the CDW-2014 datasets [10],
preliminary results are show in ﬁgure 3.

4 Conclusion

The CoG was evaluated on the wallﬂower dataset, as well as its autoencoder
counterpart VAE-CoG on the CDW-2014 datasets. We observed a speedup of
4-5x, over the baseline GMM, with an average improvement of 17% in the mis-
classiﬁcation rate. This study has demonstrated conceptually how a GMM can be
re-factored optimally into a prior scene based pixel density and rejection cascade
constituent of simpler models ordered based on the probability of occurrences
of each level of the cascade, the accuracy (and complexity) of each model in the
cascade level.

References

1. Azran, A., Ghahramani, Z.: Spectral methods for automatic multiscale data
clustering. Computer Vision and Pattern Recognition, 2006 IEEE Com-
puter Society Conference on In Computer Vision and Pattern Recogni-
tion, 2006 IEEE Computer Society Conference on 1, 190–197 (dec 2006).
https://doi.org/10.1109/ICDM.2008.88

10

Kiran et al.

2. Bouwmans, T., Baf, F.E., Vachon, B.: Background modeling using mixture of gaus-
sians for foreground detection - a survey. Recent Patents on Computer Science 1(3),
219–237 (Nov 2008)

3. Javed, O., Shaﬁque, K., Shah, M.: A hierarchical approach to robust
background subtraction using color and gradient information. Motion and
Video Computing, 2002. Proceedings. Workshop on pp. 22–27 (2002).
https://doi.org/10.1109/MOTION.2002.1182209

4. Kiran, B.R., Thomas, D.M., Parakkal, R.: An overview of deep learning based
methods for unsupervised and semi-supervised anomaly detection in videos. Jour-
nal of Imaging 4(2), 36 (2018)

5. Piccardi, M.: Background subtraction techniques: a review. Systems, Man and
Cybernetics, 2004 IEEE International Conference on 4(1), 3099–3104 (2004).
https://doi.org/10.1109/ICSMC.2004.1400815

6. Strauﬀer, C., Grimson., W.: Adaptive background mixture models for real-time
tracking. Proceedings of the IEEE Computer Society Conference on Computer
Vision and Pattern Recognition (1999)

7. Toyama, K., Krumm, J., Brumitt, B., Meyers, B.: Wallﬂower: Principles and prac-
tice of background maintenance. Computer Vision, 1999. The Proceedings of the
Seventh IEEE International Conference on 1(1), 255–261 (1999)

8. Valentine, B., Apewokin, S., Wills, L., Wills, S.: An eﬃcient, chromatic
clustering-based background model
for embedded vision platforms. Com-
puter Vision and Image Understanding 114(11), 1152–1163 (Nov 2010).
https://doi.org/10.1016/j.cviu.2010.03.014

9. Viola, P., Jones, M.: Robust

reabouwmansl-time object detection.

Journal

ternational
https://doi.org/10.1023/B:VISI.0000013087.49260.fb,
material tr.pdf

of Computer Vision 57(2),

137–154

supplied

In-
2004).
additional

(May
as

10. Wang, Y., Jodoin, P.M., Porikli, F., Konrad, J., Benezeth, Y., Ishwar, P.: Cdnet
2014: an expanded change detection benchmark dataset. In: Proceedings of the
IEEE conference on computer vision and pattern recognition workshops. pp. 387–
394 (2014)

11. Xu, P., Ye, M., Li, X., Liu, Q., Yang, Y., Ding, J.: Dynamic background learning
through deep auto-encoder networks. In: Proceedings of the 22Nd ACM Interna-
tional Conference on Multimedia. pp. 107–116. ACM (2014)

12. Yang, F., Choi, W., Lin, Y.: Exploit all the layers: Fast and accurate cnn object
detector with scale dependent pooling and cascaded rejection classiﬁers. In: Pro-
ceedings of the IEEE conference on computer vision and pattern recognition. pp.
2129–2137 (2016)

13. Zhang, K., Zhang, Z., Wang, H., Li, Z., Qiao, Y., Liu, W.: Detecting faces using
inside cascaded contextual cnn. In: Proceedings of the IEEE International Confer-
ence on Computer Vision. pp. 3171–3179 (2017)

14. Zuo, J., Pan, Q., Y. Liang, H.Z., Cheng, Y.: Model switching based adaptive back-

ground modeling approach. Acta Automatica Sinica 33(5), 467–473 (2007)

