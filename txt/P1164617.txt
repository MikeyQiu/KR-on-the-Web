Memory-augmented Attention Modelling for Videos

Rasool Fakoor†∗, Abdel-rahman Mohamed,††, Margaret Mitchell‡†, Sing Bing Kang††,
Pushmeet Kohli††
††Microsoft Research †University of Texas at Arlington ‡†Google
†rasool.fakoor@mavs.uta.edu, ††{asamir, singbing.kang, pkohli}@microsoft.com ‡†mmitchellai@google.com,

7
1
0
2
 
r
p
A
 
4
2
 
 
]

V
C
.
s
c
[
 
 
4
v
1
6
2
2
0
.
1
1
6
1
:
v
i
X
r
a

Abstract
We present a method to improve video descrip-
tion generation by modeling higher-order inter-
actions between video frames and described con-
cepts. By storing past visual attention in the
video associated to previously generated words,
the system is able to decide what to look at and
describe in light of what it has already looked
at and described. This enables not only more
effective local attention, but tractable considera-
tion of the video sequence while generating each
word. Evaluation on the challenging and pop-
ular MSVD and Charades datasets demonstrates
that the proposed architecture outperforms previ-
ous video description approaches without requir-
ing external temporal video features. The source
code for this paper is available on https://
github.com/rasoolfa/videocap.

1. Introduction

Deep neural architectures have led to remarkable progress
in computer vision and natural language processing prob-
lems.
Image captioning is one such problem, where the
combination of convolutional structures (Krizhevsky et al.,
2012; LeCun et al., 1998), and sequential recurrent struc-
tures (Sutskever et al., 2014) leads to remarkable improve-
ments over previous work (Fang et al., 2015; Devlin et al.,
2015). One of the emerging modelling paradigms, shared
by models for image captioning as well as related vision-
language problems, is the notion of an attention mechanism
that guides the model to attend to certain parts of the image
while generating (Xu et al., 2015a).

The attention models used for problems such as image cap-
tioning typically depend on the single image under con-
sideration and the partial output generated so far, jointly
capturing one region of an image and the words being gen-
erated. However, such models cannot directly capture the
temporal reasoning necessary to effectively produce words

that refer to actions and events taking place over multiple
frames in a video. For example, in a video depicting “some-
one waving a hand”, the “waving” action can start from any
frame and can continue on for a variable number of follow-
ing frames. At the same time, videos contain many frames
that do not provide additional information over the smaller
set of frames necessary to generate a summarizing descrip-
tion. Given these challenges, it is not surprising that even
with recent advancements in image captioning (Fang et al.,
2015; Xu et al., 2015a; Johnson et al., 2016; Vinyals et al.,
2015; Donahue et al., 2015), video captioning has remained
challenging.

Motivated by these observations, we introduce a memory-
based attention mechanism for video captioning and de-
scription. Our model utilizes memories of past attention
in the video when reasoning about where to attend in a
current time step. This allows the model to not only ef-
fectively leverage local attention, but also to consider the
entire video as it generates each word. This mechanism ef-
fectively binds information from both vision and language
sources into a coherent structure.

Our work shares the same goals as recent work on atten-
tion mechanisms for sequence-to-sequence architectures,
such as Rockt¨aschel et al. (2016) and Yang et al. (2016).
Rockt¨aschel et al. (2016) consider the domain of entailment
relations, where the goal is to determine entailment given
two input sentences. They propose a soft attention model
that is not only focused on the current state, but the previous
as well. In our model, all previous attentions are explicitly
stored into memory, and the system learns to memorize the
encoded version of the input videos conditioned on previ-
ously seen words. Yang et al. (2016) and our work both
try to solve the problem of locality of attention in vision-
to-language, but while Yang et al. (2016) introduce a mem-
ory architecture optimized for single image caption gener-
ation, we introduce a memory architecture that operates on
a streaming video’s temporal sequence.

The contributions of this work include:

∗Corresponding

author:

sool.fakoor@mavs.uta.edu)

Rasool

Fakoor

(

ra-

• A deep learning architecture that represents video
with an explicit model of the video’s temporal struc-

1

ture.

• A method to jointly model the video description and
temporal video sequence, connecting the visual video
space and the language description space.

• A memory-based attention mechanism that learns iter-
ative attention relationships in a simple and effective
sequence-to-sequence memory structure.

• Extensive comparison of this work and previous work
on the video captioning problem on the MSVD (Chen
& Dolan, 2011) and the Charades (Sigurdsson et al.,
2016) datasets.

We focus on the video captioning problem, however, the
proposed model is general enough to be applicable in other
sequence problems where attention models are used (e.g.,
machine translation or recognizing entailment relations).

2. Related Work

One of the primary challenges in learning a mapping from
a visual space (i.e., video or image) to a language space
is learning a representation that not only effectively rep-
resents each of these modalities, but is also able to trans-
late a representation from one space to the other. Rohrbach
et al. (2013) developed a model that generates a seman-
tic representation of visual content that can be used as
the source language for the language generation module.
Venugopalan et al. (2015b) proposed a deep method to
translate a video into a sentence where an entire video is
represented with a single vector based on the mean pool
of frame features. However, it was recognized that repre-
senting a video by an average of its frames loses the tem-
poral structure of the video. To address this problem, re-
cent work (Yao et al., 2015; Pan et al., 2016a; Venugopalan
et al., 2015a; Shin et al., 2016; Pan et al., 2016b; Xu et al.,
2015b; Ballas et al., 2016; Yu et al., 2016) proposed meth-
ods to model the temporal structure of videos as well as
language.

The majority of these methods are inspired by sequence-to-
sequence (Sutskever et al., 2014) and attention (Bahdanau
et al., 2015) models. Sequence learning was proposed to
map the input sequence of a source language to a target
language (Sutskever et al., 2014). Applying this method
with an additional attention mechanism to the problem of
translating a video to a description showed promising ini-
tial results, however, revealed additional challenges. First,
modelling the video content with a ﬁxed-length vector in
order to map it to a language space is a more complex prob-
lem than mapping from a language to a language, given the
complexity of visual content and the difference between the
two modalities. Since not all frames in a video are equally
salient for a short description, and an event can happen
in multiple frames, it is important for a model to identify

which frames are most salient. Further, the models need
additional work to be able to focus on points of interest
within the video frames to select what to talk about. Even
a variable-length vector to represent a video using attention
(Yao et al., 2015) can have some problems.

More speciﬁcally, current attention methods are lo-
cal (Yang et al., 2016), since the attention mechanism
works in a sequential structure, and lack the ability to cap-
ture global structure. Moreover, combining a video and a
description as a sequence-to-sequence problem motivates
using some variant of a recurrent neural network (RNN)
(Hochreiter & Schmidhuber, 1997): Given the limited ca-
pacity of a recurrent network to model very long sequences,
memory networks (Weston et al., 2014; Sukhbaatar et al.,
2015) have been introduced to help the RNN memorize se-
quences. However, one problem these memory networks
suffer from is difﬁculty in training the model. The model
proposed by Weston et al. (2014) requires supervision at
each layer, which makes training with backpropagation a
challenging task. Sukhbaatar et al. (2015) proposed a mem-
ory network that can be trained end-to-end, and the current
work follows this research line to tackle the challenging
problem of modeling vision and language memories for
video description generation.

3. Learning to Attend and Memorize

A main challenge in video description is to ﬁnd a mapping
that can capture the connection between the video frames
and the video description. Sequence-to-sequence models,
which work well at connecting input and output sequences
in machine translation (Sutskever et al., 2014), do not per-
form as well for this task, as there is not the same direct
alignment between a full video sequence and its summariz-
ing description.

Our goal in the video description problem is to create an
architecture that learns which moments to focus on in a
video sequence in order to generate a summarizing natu-
ral language description. The modelling challenges we set
forth for the video description problem are: (1) Processing
the temporal structure of the video; (2) Learning to attend
to important parts of the video; and (3) Generating a de-
scription where each word is relevant to the video. At a
high-level, this can be understood as having three primary
parts: When moments in the video are particularly salient;
what concepts to focus on; and how to talk about them.
We directly address these issues in an end-to-end network
with three primary corresponding components (Figure 1):
A Temporal Model (TEM), An Iterative Attention/Memory
Model (IAM), and a Decoder. In summary:

• When: Frames within the video sequence - The Tem-

poral Model (TEM).

2

Figure 1. Our proposed architecture. Each component of our model is described in 3.1 through 3.3.

• What: Language-grounded concepts depicted in the
video - The Iterative Attention/Memory mechanism
(IAM).

• How: Words that ﬂuently describe the what and when

- The Decoder.

The Temporal Model is in place to capture the temporal
structure of the video: It functions as a when component.
The Iterative Attention/Memory is a main contribution of
this work, functioning as a what component to remember
relationships between words and video frames, and storing
longer term memories. The Decoder generates language,
and functions as the how component to create the ﬁnal de-
scription.

To train the system end to end, we formulate the problem
as sequence learning to maximize the probability of gener-
ating a correct description given a video:

Θ∗ = arg max

(cid:88)

Θ

(S,f1,...,fN )

log p(S|f1, . . . , fN ; Θ)

(1)

where S is the description, f1, f2, . . . , fN are the input
video frames, and Θ is the model parameter vector.
In
the next sections, we will describe each component of the
model, then explain the details of training and inference.

Notational note: Numbered equations use bold face to denote
multi-dimensional learnable parameters, e.g., Wj
p. To distinguish
the two different sets of time steps, one for video frames and one
for words in the description, we use the notation t for video and t(cid:48)
for language. Throughout, the terms description and caption are
used interchangeably.

3.1. Temporal Model (TEM)

The ﬁrst module we introduce encodes the temporal struc-
ture of the input video. A clear framework to use for this

3

is a Recurrent Neural Network (RNN), which has been
shown to be effectual in modelling the temporal struc-
ture of sequential data such as video (Ballas et al., 2016;
Sharma et al., 2015; Venugopalan et al., 2015a) and speech
In order to apply this in video
(Graves & Jaitly, 2014).
sequences to generate a description, we seek to capture
the fact that frame-to-frame temporal variation tends to be
local (Brox & Malik, 2011) and critical in modeling mo-
tion (Ballas et al., 2016). Visual features extracted from
the last fully connected layers of Convolutional Neural Net-
works (CNNs) have been shown to produce state-of-the-art
results in image classiﬁcation and recognition (Simonyan
& Zisserman, 2014; He et al., 2016), and thus seem a good
choice for modeling visual frames. However, these features
tend to discard low level information useful in modeling the
motion in the video (Ballas et al., 2016).

To address these challenges, we implement an RNN we call
the Temporal Model (TEM). At each time step of the TEM, a
video frame encoding from a CNN serves as input. Rather
than extracting video frame features from a fully connected
layer of the pretrained CNN, we extract intermediate con-
volutional maps.

In detail, for a given video X with N frames X =
[X 1, X 2, · · · , X N ], N convolutional maps of size RL×D
are extracted, where L is the number of locations in the in-
put frame and D is the number of dimensions (See TEM
in Figure 1). To enable the network to store the most im-
portant L locations of each frame, we use a soft location at-
tention mechanism, fLatt (Bahdanau et al., 2015; Xu et al.,
2015a; Sharma et al., 2015). We ﬁrst use a softmax to com-
pute L probabilities that specify the importance of different
parts in the frame, and this creates an input map for fLatt.

Formally, given a video frame at time t, X t ∈ RL×D, the

fLatt mechanism is deﬁned as follows:

• Given:

ρt
j =

(cid:80)L

exp(Wj
pht−1
)
v
pht−1
k=1 exp(Wk
v
L
(cid:88)

)

fLatt(X t, ht−1

v

; Wp) =

jX t
ρt
j

j=1

v

∈ RK is the hidden state of the TEM at time
where ht−1
t-1 with K dimensions, and Wp ∈ RL×K. For each video
frame time step, TEM learns a vector representation by ap-
plying location attention on the frame convolution map,
conditioned on all previously seen frames:

F t = fLatt(X t, ht−1
v
v = fv(F t, ht−1
ht
; Θv)

v

; Wp)

where fv can be an RNN/LSTM/GRU cell and Θv is the
parameters of the fv. Due to the fact that vanilla RNNs
have gradient vanishing and exploding problems (Pascanu
et al., 2013), we use gradient clipping, and an LSTM with
the following ﬂow to handle potential vanishing gradients:

(2)

(3)

(4)

(5)

N = Number of frames in a given video
T = Number of words in description
v, ..., hN
Hv = Input video states, [h1
v ]
H t(cid:48)−1

g

= Decoder state hg at time t-1, repeated N times

H t(cid:48)−1

m = Memory state hm at time t-1, repeated N times

Wv, Wg ∈ RK×K
Wm ∈ RM ×K
u ∈ RK
α = Probability over all N frames
Θa = {Wv, Wg, Wm, u}

• Attention update [ ˆF (Θa)]:

QA = tanh(HvWv + H t(cid:48)−1
αt(cid:48) = softmax(QAu)
ˆF = H T

v αt(cid:48)

• Memory update:

m = fm(ht(cid:48)−1
ht(cid:48)

m , ˆF ; Θm)

g Wg + H t(cid:48)−1

m Wm)

(6)
(7)

(8)

(9)

Whi)
T
)
T
)

Whf )

Who )
T

T
)

v

v

it = σ(F tWxi + (ht−1
f t = σ(F tWxf + (ht−1
ot = σ(F tWxo + (ht−1
gt = tanh(F tWxg + (ht−1
v = f t (cid:12) ct−1
ct
v = ot (cid:12) tanh(ct)
ht

v
v + it (cid:12) gt

v

)

Whg )

where Wh∗ ∈ RK×K, Wx∗ ∈ RD×K, and we deﬁne Θv =
{Wh∗, Wx∗}.

3.2. Iterative Attention/Memory (IAM)

A main contribution of this work is a global view for the
video description task: A memory-based attention mecha-
nism that learns iterative attention relationships in an efﬁ-
cient sequence-to-sequence memory structure. We refer to
this as the Iterative Attention/Memory mechanism (IAM),
and it aggregates information from previously generated
words and all input frames.

The IAM component is an iterative memorized attention be-
tween an input video and a description. More speciﬁcally,
it learns a iterative attention structure for where to attend in
a video given all previously generated words (from the De-
coder), and previous states (from the TEM). This functions
as a memory structure, remembering encoded versions of
the video with corresponding language, and in turn, en-
abling the Decoder to access the full encoded video and
previously generated words as it generates new words.

4

Figure 2. Iterative Attention and Memory (IAM) is formulated as
an Attention update and a Memory update.

This component addresses several key issues in generating
a coherent video description. In video description, a sin-
gle word or phrase often describes action spanning multi-
ple frames within the input video. By employing the IAM,
the model can effectively capture the relationship between
a relatively short bit of language and an action that occurs
over multiple frames. This also functions to directly ad-
dress the problem of identifying which parts of the video
are most relevant for description.

The proposed Iterative Attention/Memory mechanism is
formalized with an Attention update and a Memory up-
date, detailed in Figure 2. Figure 1 illustrates where the
IAM sits within the full model, with the Attention module
shown in 1a and the Memory module shown in 1b.
As formalized in Figure 2, the Attention update ˆF (Θa)
computes the set of probabilities in a given time step for
attention within the input video states, the memory state,
and the decoder state. The Memory update stores what has
been attended to and described. This serves as the memo-
rization component, combining the previous memory with
the current iterative attention ˆF . We use an LSTM fm
with the equations described above to enable the network to
learn multi-layer attention over the input video and its cor-
responding language. The output of this function is then

used as input to the Decoder.

3.3. Decoder

In order to generate a new word conditioned on all previous
words and IAM states, a recurrent structure is modelled as
follows:

ht(cid:48)
g = fg(st(cid:48)
ˆst(cid:48)

= softmax((ht(cid:48)

, ht(cid:48)

m, ht(cid:48)−1

g
g )T We)

; Θg)

(10)

(11)

g ∈ RK, st(cid:48)

where ht(cid:48)
is a word vector at time t(cid:48), We ∈
RK×|V |, and |V | is the vocabulary size. In addition, ˆst(cid:48)
assigns a probability to each word in the language. fg is an
LSTM where st(cid:48)
g is the recurrent
state.

m are inputs and ht(cid:48)

and ht(cid:48)

3.4. Training and Optimization

in our network is to predict

The goal
the next word
given all previously seen words and an input video.
to optimize our network parameters Θ =
In order
{Wp, Θv, Θa, Θm, Θg, We}, we minimize a negative log
likelihood loss function:

L(S, X; Θ) = −

i log(ˆst(cid:48)
st(cid:48)

i ) + λ (cid:107) Θ (cid:107)2
2

(12)

T
(cid:88)

|V |
(cid:88)

t(cid:48)

i

where |V | is the vocabulary size. We fully train our net-
work in an end-to-end fashion using ﬁrst-order stochas-
tic gradient-based optimization method with an adaptive
learning rate. More speciﬁcally, in order to optimize our
network parameters, we use Adam (Kingma & Ba, 2015)
with learning rate 2 × 10−5 and set β1, β2 to 0.8 and
0.999, respectively. During training, we use a batch size
of 16. The source code for this paper is available on
https://github.com/rasoolfa/videocap.

4. Experiments

Dataset We evaluate the model on the Charades (Sig-
urdsson et al., 2016) dataset and the Microsoft Video De-
scription Corpus (MSVD) (Chen & Dolan, 2011). Cha-
rades contains 9, 848 videos (in total) and provides 27, 8471
video descriptions. We follow the same train/test splits as
Sigurdsson et al. (2016), with 7569 train, 1, 863 test, and
400 validation. A main difference between this dataset and
others is that it uses a “Hollywood in Homes” approach to
data collection, where “actors” are crowdsourced to act out
different actions. This yields a diverse set of videos, with
each containing a speciﬁc action.

MSVD is a set of YouTube videos annotated by workers
on Mechanical Turk,2 who were asked to pick a video clips
representing an activity.
In this dataset, each clip is an-
notated by multiple workers with a single sentence. The
dataset contains 1, 970 videos and about 80, 000 descrip-
tions, where 1, 200 of the videos are training data, 670 test,
and the rest (100 videos) for validation. In order for the
results to be comparable to other approaches, we follow
the exact training/validation/test splits provided by Venu-
gopalan et al. (2015b).

Evaluation metrics We report results on the video de-
scription generation task. In order to evaluate descriptions
generated by our model, we use model-free automatic eval-
uation metrics. We adopt METEOR, BLEU-N, and CIDEr
metrics available from the Microsoft COCO Caption Eval-
uation code3 to score the system.

Video and Caption preprocessing We preprocess the
captions for both datasets using the Natural Language
Toolkit (NLTK)4 and clip each description up to 30 words,
since the majority have less. We extract sample frames
from each video and pass each frame through VGGnet (Si-
monyan & Zisserman, 2014) without ﬁne-tuning. For the
experiments in this paper, we use the feature maps from
conv5 3 layer after applying ReLU. The feature map in this
layer is 14×14×512. Our TEM component operates on the
ﬂattened 196 × 512 of this feature cubes. For the ablation
studies, features from the fully connected layer with 4096
dimensions are used as well.

use

optimization We

Hyper-parameter
random
search (Bergstra & Bengio, 2012) on the validation set
to select hyper-parameters on both datasets. The word-
embedding size, hidden layer size (for both the TEM and
the Decoder), and memory size of the best model on
Charades are: 237, 1316, and 437, respectively. These
values are 402, 1479, and 797 for the model on the MSVD
dataset. A stack of two LSTMs are used in the Decoder and
TEM. The number of frame samples is a hyperparameter
which is selected among 4, 8, 16, 40 on the validation set.
ATT + NO TEM and NO IAM + TEM get the best results on
the validation set with 40 frames, and we use this as the
number of frames for all models in the ablation study.

4.1. Video Caption Generation

We ﬁrst present an ablation analysis to elucidate the contri-
bution of the different components of our proposed model.
Then, we compare the overall performance of our model to
other recent models.

1Only 16087 out of 27, 847 are used as descriptions for our
evaluation since the 27, 847 refers to script of the video as well as
descriptions.

2https://www.mturk.com/mturk/welcome
3https://github.com/tylin/coco-caption
4http://www.nltk.org/

5

Ablation Analysis

Ablation results are shown in Table 1, evaluating on the
MSVD test set. The ﬁrst (ATT + NO TEM) corresponds
to a simpler version of our model in which we remove the
TEM component and instead pass each frame of the video
through a CNN, extracting features from the last fully-
connected hidden layer. In addition, we replace our IAM
with a simpler version where the model only memorizes
the current step instead of all previous steps. In the next
variation (ATT + TEM), it is same as the ﬁrst one except
we use TEM instead of fully connected CNN features. In
the next ablation (NO IAM + TEM), we remove the IAM
component from our model and keep the rest of the model
as-is. In the next variation (IAM + NO TEM), we remove
the TEM and calculate features for each frame, similar to
ATT + NO TEM. Finally, the last row in the table is our
proposed model (IAM + TEM) with all its components.

The IAM plays a signiﬁcant role in the proposed model, and
removing it causes a large drop in performance, as mea-
sured by both BLEU and METEOR. On the other hand, re-
moving the TEM by itself does not drop performance as
much as dropping the IAM. Putting the two together, they
complement one another to result in overall better perfor-
mance for METEOR. However, further development on the
TEM component in future work is warranted.
In the NO
IAM + TEM condition, an entire video must be represented
with a ﬁxed-length vector, which may contribute to the
lower performance (Bahdanau et al., 2015). This is in con-
trast to the other models, which apply single layer attention
or IAM to search relevant parts of the video aligned with
the description.

Performance Comparison

To extensively evaluate the proposed model, we compare
with state-of-the-art models and baselines for the video
caption generation task on the MSVD dataset. In this ex-
periment, we use 8 frames per video as the inputs to the
TEM module. As shown in Table 2,5 our proposed model
achieves state-of-the-art scores in BLEU-4, and outperforms
almost all systems on METEOR. The closest-scoring com-
parison system, from Pan et al. (2016a), shows a trade-off
between METEOR and BLEU: BLEU prefers descriptions
with short-distance ﬂuency and high lexical overlap with
the observed descriptions, while METEOR permits less di-
rect overlap and longer descriptions. A detailed study of
the generated descriptions between the two systems would
be needed to better understand these differences.

5The symbol − indicates that the score was not reported by
the corresponding paper. The horizontal line in Table 2 separates
models that do/do not use external features for the video represen-
tation.

The improvement over previous work is particularly note-
worthy because we do not use external features for the
video, such as Optical Flow (Brox et al., 2004) (de-
noted Flow), 3-Dimensional Convolutional Network fea-
tures (Tran et al., 2015) (denoted C3D), or ﬁne-tuned CNN
features (denoted FT), which further enhances aspects such
as action recognition by leveraging an external dataset such
as UCF-101. The only system using external features that
outperforms the model proposed here is from Yu et al.
(2016), who uses a slightly different version of the same
dataset6 along with C3D features for a large improvement
in results (compare Table 2 rows 4 and 11); future work
may explore the utility of external visual features for this
work. Here, we demonstrate that the proposed architec-
ture maps visual space to language space with improved
performance over previous work, before addition of further
resources.

We additionally report results on the Charades dataset (Sig-
urdsson et al., 2016), which is challenging to train on be-
cause there are only a few (≈ 2) captions per video. In this
experiment, we use 16 frames per video as the input to the
TEM module. As shown in Table 3, our method achieves
a 10% relative improvement over the Venugopalan et al.
(2015a) model reported by Sigurdsson et al. (2016). It is
worth noting that humans reach a METEOR score of 24 and
a BLEU-4 score of 20, illustrating the low upper bound in
this task.7

Results Discussion

We show some example descriptions generated by our sys-
tem in Figure 3. The model generates mostly correct de-
scriptions, with naturalistic variation from the ground truth.
Errors illustrate a preference to describe items that have a
higher likelihood of being mentioned, even if they appear
in less of the frames. For example, in the “a dog is on
a trampoline” video, our model focuses on the man, who
appears in only a few frames, and generates the incorrect
description “a man is washing a bath”. The errors, along-
side the ablation study shown in Table 1, suggest that the
TEM module in particular may be further improved by fo-
cusing on how frames in the video sequence are captured
and passed to the IAM module.

5. Conclusion

We introduce a general framework for an memory-based
sequence learning model, trained end-to-end. We apply this
framework to the task of describing an input video with a

6Yu et al. (2016) uses the MSVD dataset reported in (Guadar-

rama et al., 2013), which has different preprocessing.

7For comparison, the upper bound BLEU score in machine

translation for English to French is above 30.

6

Method

METEOR

BLEU-1

BLEU-2

BLEU-3

BLEU-4

CIDEr

ATT + NO TEM
ATT + TEM
NO IAM + TEM
IAM + NO TEM
IAM + TEM [40F]
IAM + TEM [8F]

31.20
31.00
30.50
31.00
31.70
31.80

77.90
79.00
78.10
78.70
79.00
79.40

65.10
66.50
65.20
66.90
66.20
67.10

55.30
56.30
55.10
57.40
56.0
56.80

44.90
45.50
44.60
47.00
45.60
46.10

63.90
61.00
60.50
62.10
62.20
62.70

Table 1. Ablation of proposed model with and without the IAM component on the MSVD test set.

Method

METEOR

BLEU-1

BLEU-2

BLEU-3

BLEU-4

CIDEr

Venugopalan et al. (2015b)
Venugopalan et al. (2015a)
Pan et al. (2016b)
Yu et al. (2016)
Pan et al. (2016a)
Our Model

27.7
29.2
29.5
31.10
33.10
31.80

Yao et al. (2015) + C3D
Venugopalan et al. (2015a) + Flow 29.8
Ballas et al. (2016) + FT
Pan et al. (2016b) + C3D
Yu et al. (2016) + C3D

30.75
31.0
32.60

29.60

−
−
74.9
77.30
79.20
79.40

−
−
−
78.80
81.50

−
−
60.9
64.50
66.30
67.10

−
−
−
66.0
70.40

−
−
50.6
54.60
55.10
56.80

−
−
−
55.4
60.4

−
−
40.2
44.30
43.80
46.10

41.92
−
49.0
45.3
49.90

−
−
−
−
−
62.70

51.67
−
59.37
−
−

Table 2. Video captioning evaluation on MSVD (670 videos).

Method

M B@1 B@2 B@3 B@4 C

References

Human
(Sigurdsson et al., 2016)

24

62

43

29

20

53

Sigurdsson et al. (2016) 16
Our Model

49

30

18

11

14

17.6 50 31.1 18.8 11.5 16.7

Table 3. Video captioning evaluation on Charades (1863 videos).
M=METEOR, B=BLEU, C=CIDEr. Sigurdsson et al. (2016) results
use the Venugopalan et al. (2015a) model.

natural language description. Our model utilizes a deep
learning architecture that represents video with an explicit
model of the video’s temporal structure, and jointly mod-
els the video description and the temporal video sequence.
This effectively connects the visual video space and the lan-
guage description space.

A memory-based attention mechanism helps guide where
to attend and what to reason about as the description is gen-
erated. This allows the model to not only reason efﬁciently
about local attention, but also to consider the full sequence
of video frames during the generation of each word. Our
experiments conﬁrm that the memory components in our
architecture, most notably from the IAM module, play a
signiﬁcant role in improving the performance of the entire
network.

Future work should raim to reﬁne the temporal video frame
model, TEM, and explore how to improve performance on
capturing the ideal frames for each description.

Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio,
Yoshua. Neural machine translation by jointly learning
to align and translate. ICLR, 2015.

Ballas, Nicolas, Yao, Li, Pal, Chris, and Courville,
Aaron C. Delving deeper into convolutional networks
for learning video representations. In ICLR, 2016.

Bergstra, James and Bengio, Yoshua. Random search for
hyper-parameter optimization. J. Mach. Learn. Res., 13:
281–305, 2012.

Brox, T. and Malik, J. Large displacement optical ﬂow:
Descriptor matching in variational motion estimation.
TPAMI, 33(3):500–513, March 2011. ISSN 0162-8828.

Brox, T., Bruhn, A., Papenberg, N., and Weickert, J. High
accuracy optical ﬂow estimation based on a theory for
warping. In ECCV, 2004.

Chen, David L. and Dolan, William B. Collecting highly
In ACL, June

parallel data for paraphrase evaluation.
2011.

Devlin, Jacob, Cheng, Hao, Fang, Hao, Gupta, Saurabh,
Deng, Li, He, Xiaodong, Zweig, Geoffrey, and Mitchell,
Margaret. Language models for image captioning: The
quirks and what works. In ACL-IJCNLP, pp. 100–105,
July 2015.

Donahue, Jeff, Hendricks, Lisa Anne, Guadarrama, Sergio,
Rohrbach, Marcus, Venugopalan, Subhashini, Saenko,

7

Figure 3. Example captions generated by our model on MSVD test videos.

Kate, and Darrell, Trevor. Long-term recurrent convo-
lutional networks for visual recognition and description.
In CVPR, 2015.

Fang, Hao, Gupta, Saurabh,

Iandola, Forrest, Srivas-
tava, Rupesh K., Deng, Li, Dollar, Piotr, Gao, Jian-
feng, He, Xiaodong, Mitchell, Margaret, Platt, John C.,
Lawrence Zitnick, C., and Zweig, Geoffrey. From cap-
tions to visual concepts and back. In CVPR, June 2015.

Graves, Alex and Jaitly, Navdeep. Towards end-to-end
speech recognition with recurrent neural networks.
In
ICML-14, pp. 1764–1772, 2014.

Guadarrama, Sergio, Krishnamoorthy, Niveda, Malkar-
nenkar, Girish, Venugopalan, Subhashini, Mooney, Ray-
mond, Darrell, Trevor, and Saenko, Kate. Youtube2text:
Recognizing and describing arbitrary activities using se-
mantic hierarchies and zero-shot recognition. In ICCV,
pp. 2712–2719, 2013.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Deep residual learning for image recognition. In
CVPR, June 2016.

Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-
Neural Comput., 9(8):1735–1780,

term memory.
November 1997. ISSN 0899-7667.

Johnson, Justin, Karpathy, Andrej, and Fei-Fei, Li. Dense-
cap: Fully convolutional localization networks for dense
captioning. In CVPR, 2016.

Kingma, Diederik P. and Ba, Jimmy. Adam: A method for

stochastic optimization. In ICLR, 2015.

8

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classiﬁcation with deep convolutional neural
networks. In Pereira, F., Burges, C. J. C., Bottou, L., and
Weinberger, K. Q. (eds.), NIPS, pp. 1097–1105. 2012.

LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner,
Patrick. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–
2324, 1998.

Pan, Pingbo, Xu, Zhongwen, Yang, Yi, Wu, Fei, and
Zhuang, Yueting. Hierarchical recurrent neural encoder
for video representation with application to captioning.
In CVPR, June 2016a.

Pan, Yingwei, Mei, Tao, Yao, Ting, Li, Houqiang, and Rui,
Jointly modeling embedding and translation to

Yong.
bridge video and language. CVPR, 2016b.

Pascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua.
On the difﬁculty of training recurrent neural networks.
ICML-13, 28:1310–1318, 2013.

Rockt¨aschel, Tim, Grefenstette, Edward, Hermann,
Karl Moritz, Kocisk´y, Tom´as, and Blunsom, Phil. Rea-
soning about entailment with neural attention. In ICLR,
2016.

Rohrbach, M., Qiu, W., Titov, I., Thater, S., Pinkal, M., and
Schiele, B. Translating video content to natural language
descriptions. In ICCV, pp. 433–440, Dec 2013.

Sharma, Shikhar, Kiros, Ryan, and Salakhutdinov, Rus-
lan. Action recognition using visual attention. CoRR,
abs/1511.04119, 2015.

Shin, Andrew, Ohnishi, Katsunori, and Harada, Tatsuya.
Beyond caption to narrative: Video captioning with mul-
tiple sentences. ICIP, 2016.

Sigurdsson, Gunnar A., Varol, G¨ul, Wang, Xiaolong,
Farhadi, Ali, Laptev, Ivan, and Gupta, Abhinav. Hol-
lywood in homes: Crowdsourcing data collection for ac-
tivity understanding. In ECCV, 2016.

Yao, Li, Torabi, Atousa, Cho, Kyunghyun, Ballas, Nico-
las, Pal, Christopher, Larochelle, Hugo, and Courville,
Aaron. Describing videos by exploiting temporal struc-
ture. In ICCV, 2015.

Yu, Haonan, Wang, Jiang, Huang, Zhiheng, Yang, Yi, and
Xu, Wei. Video paragraph captioning using hierarchical
recurrent neural networks. In CVPR, June 2016.

Simonyan, K. and Zisserman, A. Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014.

Sukhbaatar, Sainbayar, Szlam, Arthur, Weston, Jason, and
Fergus, Rob. End-to-end memory networks. NIPS, 2015.

Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence
to sequence learning with neural networks. In NIPS, pp.
3104–3112. 2014.

Tran, Du, Bourdev, Lubomir, Fergus, Rob, Torresani,
Lorenzo, and Paluri, Manohar. Learning spatiotempo-
In ICCV,
ral features with 3d convolutional networks.
2015.

Venugopalan, Subhashini, Rohrbach, Marcus, Donahue,
Jeff, Mooney, Raymond, Darrell, Trevor, and Saenko,
Kate. Sequence to sequence – video to text. In ICCV,
2015a.

Venugopalan, Subhashini, Xu, Huijuan, Donahue, Jeff,
Rohrbach, Marcus, Mooney, Raymond, and Saenko,
Kate. Translating videos to natural language using deep
recurrent neural networks. In NAACL HLT, 2015b.

Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Er-
han, Dumitru. Show and tell: A neural image caption
generator. In CVPR, June 2015.

Weston, Jason, Chopra, Sumit, and Bordes, Antoine. Mem-

ory networks. CoRR, abs/1410.3916, 2014.

Xu, Kelvin, Ba, Jimmy, Kiros, Ryan, Cho, Kyunghyun,
Courville, Aaron, Salakhudinov, Ruslan, Zemel, Rich,
and Bengio, Yoshua. Show, attend and tell: Neural im-
age caption generation with visual attention. In ICML-
15, pp. 2048–2057, 2015a.

Xu, Ran, Xiong, Caiming, Chen, Wei, and Corso, Jason J.
Jointly modeling deep video and compositional text to
bridge vision and language in a uniﬁed framework. In
AAAI, 2015b.

Yang, Zhilin, Yuan, Ye, Wu, Yuexin, Salakhutdinov, Rus-
lan, and Cohen, William W. Encode, review, and de-
code: Reviewer module for caption generation. CoRR,
abs/1605.07912, 2016.

9

Memory-augmented Attention Modelling for Videos

Rasool Fakoor†∗, Abdel-rahman Mohamed,††, Margaret Mitchell‡†, Sing Bing Kang††,
Pushmeet Kohli††
††Microsoft Research †University of Texas at Arlington ‡†Google
†rasool.fakoor@mavs.uta.edu, ††{asamir, singbing.kang, pkohli}@microsoft.com ‡†mmitchellai@google.com,

7
1
0
2
 
r
p
A
 
4
2
 
 
]

V
C
.
s
c
[
 
 
4
v
1
6
2
2
0
.
1
1
6
1
:
v
i
X
r
a

Abstract
We present a method to improve video descrip-
tion generation by modeling higher-order inter-
actions between video frames and described con-
cepts. By storing past visual attention in the
video associated to previously generated words,
the system is able to decide what to look at and
describe in light of what it has already looked
at and described. This enables not only more
effective local attention, but tractable considera-
tion of the video sequence while generating each
word. Evaluation on the challenging and pop-
ular MSVD and Charades datasets demonstrates
that the proposed architecture outperforms previ-
ous video description approaches without requir-
ing external temporal video features. The source
code for this paper is available on https://
github.com/rasoolfa/videocap.

1. Introduction

Deep neural architectures have led to remarkable progress
in computer vision and natural language processing prob-
lems.
Image captioning is one such problem, where the
combination of convolutional structures (Krizhevsky et al.,
2012; LeCun et al., 1998), and sequential recurrent struc-
tures (Sutskever et al., 2014) leads to remarkable improve-
ments over previous work (Fang et al., 2015; Devlin et al.,
2015). One of the emerging modelling paradigms, shared
by models for image captioning as well as related vision-
language problems, is the notion of an attention mechanism
that guides the model to attend to certain parts of the image
while generating (Xu et al., 2015a).

The attention models used for problems such as image cap-
tioning typically depend on the single image under con-
sideration and the partial output generated so far, jointly
capturing one region of an image and the words being gen-
erated. However, such models cannot directly capture the
temporal reasoning necessary to effectively produce words

that refer to actions and events taking place over multiple
frames in a video. For example, in a video depicting “some-
one waving a hand”, the “waving” action can start from any
frame and can continue on for a variable number of follow-
ing frames. At the same time, videos contain many frames
that do not provide additional information over the smaller
set of frames necessary to generate a summarizing descrip-
tion. Given these challenges, it is not surprising that even
with recent advancements in image captioning (Fang et al.,
2015; Xu et al., 2015a; Johnson et al., 2016; Vinyals et al.,
2015; Donahue et al., 2015), video captioning has remained
challenging.

Motivated by these observations, we introduce a memory-
based attention mechanism for video captioning and de-
scription. Our model utilizes memories of past attention
in the video when reasoning about where to attend in a
current time step. This allows the model to not only ef-
fectively leverage local attention, but also to consider the
entire video as it generates each word. This mechanism ef-
fectively binds information from both vision and language
sources into a coherent structure.

Our work shares the same goals as recent work on atten-
tion mechanisms for sequence-to-sequence architectures,
such as Rockt¨aschel et al. (2016) and Yang et al. (2016).
Rockt¨aschel et al. (2016) consider the domain of entailment
relations, where the goal is to determine entailment given
two input sentences. They propose a soft attention model
that is not only focused on the current state, but the previous
as well. In our model, all previous attentions are explicitly
stored into memory, and the system learns to memorize the
encoded version of the input videos conditioned on previ-
ously seen words. Yang et al. (2016) and our work both
try to solve the problem of locality of attention in vision-
to-language, but while Yang et al. (2016) introduce a mem-
ory architecture optimized for single image caption gener-
ation, we introduce a memory architecture that operates on
a streaming video’s temporal sequence.

The contributions of this work include:

∗Corresponding

author:

sool.fakoor@mavs.uta.edu)

Rasool

Fakoor

(

ra-

• A deep learning architecture that represents video
with an explicit model of the video’s temporal struc-

1

ture.

• A method to jointly model the video description and
temporal video sequence, connecting the visual video
space and the language description space.

• A memory-based attention mechanism that learns iter-
ative attention relationships in a simple and effective
sequence-to-sequence memory structure.

• Extensive comparison of this work and previous work
on the video captioning problem on the MSVD (Chen
& Dolan, 2011) and the Charades (Sigurdsson et al.,
2016) datasets.

We focus on the video captioning problem, however, the
proposed model is general enough to be applicable in other
sequence problems where attention models are used (e.g.,
machine translation or recognizing entailment relations).

2. Related Work

One of the primary challenges in learning a mapping from
a visual space (i.e., video or image) to a language space
is learning a representation that not only effectively rep-
resents each of these modalities, but is also able to trans-
late a representation from one space to the other. Rohrbach
et al. (2013) developed a model that generates a seman-
tic representation of visual content that can be used as
the source language for the language generation module.
Venugopalan et al. (2015b) proposed a deep method to
translate a video into a sentence where an entire video is
represented with a single vector based on the mean pool
of frame features. However, it was recognized that repre-
senting a video by an average of its frames loses the tem-
poral structure of the video. To address this problem, re-
cent work (Yao et al., 2015; Pan et al., 2016a; Venugopalan
et al., 2015a; Shin et al., 2016; Pan et al., 2016b; Xu et al.,
2015b; Ballas et al., 2016; Yu et al., 2016) proposed meth-
ods to model the temporal structure of videos as well as
language.

The majority of these methods are inspired by sequence-to-
sequence (Sutskever et al., 2014) and attention (Bahdanau
et al., 2015) models. Sequence learning was proposed to
map the input sequence of a source language to a target
language (Sutskever et al., 2014). Applying this method
with an additional attention mechanism to the problem of
translating a video to a description showed promising ini-
tial results, however, revealed additional challenges. First,
modelling the video content with a ﬁxed-length vector in
order to map it to a language space is a more complex prob-
lem than mapping from a language to a language, given the
complexity of visual content and the difference between the
two modalities. Since not all frames in a video are equally
salient for a short description, and an event can happen
in multiple frames, it is important for a model to identify

which frames are most salient. Further, the models need
additional work to be able to focus on points of interest
within the video frames to select what to talk about. Even
a variable-length vector to represent a video using attention
(Yao et al., 2015) can have some problems.

More speciﬁcally, current attention methods are lo-
cal (Yang et al., 2016), since the attention mechanism
works in a sequential structure, and lack the ability to cap-
ture global structure. Moreover, combining a video and a
description as a sequence-to-sequence problem motivates
using some variant of a recurrent neural network (RNN)
(Hochreiter & Schmidhuber, 1997): Given the limited ca-
pacity of a recurrent network to model very long sequences,
memory networks (Weston et al., 2014; Sukhbaatar et al.,
2015) have been introduced to help the RNN memorize se-
quences. However, one problem these memory networks
suffer from is difﬁculty in training the model. The model
proposed by Weston et al. (2014) requires supervision at
each layer, which makes training with backpropagation a
challenging task. Sukhbaatar et al. (2015) proposed a mem-
ory network that can be trained end-to-end, and the current
work follows this research line to tackle the challenging
problem of modeling vision and language memories for
video description generation.

3. Learning to Attend and Memorize

A main challenge in video description is to ﬁnd a mapping
that can capture the connection between the video frames
and the video description. Sequence-to-sequence models,
which work well at connecting input and output sequences
in machine translation (Sutskever et al., 2014), do not per-
form as well for this task, as there is not the same direct
alignment between a full video sequence and its summariz-
ing description.

Our goal in the video description problem is to create an
architecture that learns which moments to focus on in a
video sequence in order to generate a summarizing natu-
ral language description. The modelling challenges we set
forth for the video description problem are: (1) Processing
the temporal structure of the video; (2) Learning to attend
to important parts of the video; and (3) Generating a de-
scription where each word is relevant to the video. At a
high-level, this can be understood as having three primary
parts: When moments in the video are particularly salient;
what concepts to focus on; and how to talk about them.
We directly address these issues in an end-to-end network
with three primary corresponding components (Figure 1):
A Temporal Model (TEM), An Iterative Attention/Memory
Model (IAM), and a Decoder. In summary:

• When: Frames within the video sequence - The Tem-

poral Model (TEM).

2

Figure 1. Our proposed architecture. Each component of our model is described in 3.1 through 3.3.

• What: Language-grounded concepts depicted in the
video - The Iterative Attention/Memory mechanism
(IAM).

• How: Words that ﬂuently describe the what and when

- The Decoder.

The Temporal Model is in place to capture the temporal
structure of the video: It functions as a when component.
The Iterative Attention/Memory is a main contribution of
this work, functioning as a what component to remember
relationships between words and video frames, and storing
longer term memories. The Decoder generates language,
and functions as the how component to create the ﬁnal de-
scription.

To train the system end to end, we formulate the problem
as sequence learning to maximize the probability of gener-
ating a correct description given a video:

Θ∗ = arg max

(cid:88)

Θ

(S,f1,...,fN )

log p(S|f1, . . . , fN ; Θ)

(1)

where S is the description, f1, f2, . . . , fN are the input
video frames, and Θ is the model parameter vector.
In
the next sections, we will describe each component of the
model, then explain the details of training and inference.

Notational note: Numbered equations use bold face to denote
multi-dimensional learnable parameters, e.g., Wj
p. To distinguish
the two different sets of time steps, one for video frames and one
for words in the description, we use the notation t for video and t(cid:48)
for language. Throughout, the terms description and caption are
used interchangeably.

3.1. Temporal Model (TEM)

The ﬁrst module we introduce encodes the temporal struc-
ture of the input video. A clear framework to use for this

3

is a Recurrent Neural Network (RNN), which has been
shown to be effectual in modelling the temporal struc-
ture of sequential data such as video (Ballas et al., 2016;
Sharma et al., 2015; Venugopalan et al., 2015a) and speech
In order to apply this in video
(Graves & Jaitly, 2014).
sequences to generate a description, we seek to capture
the fact that frame-to-frame temporal variation tends to be
local (Brox & Malik, 2011) and critical in modeling mo-
tion (Ballas et al., 2016). Visual features extracted from
the last fully connected layers of Convolutional Neural Net-
works (CNNs) have been shown to produce state-of-the-art
results in image classiﬁcation and recognition (Simonyan
& Zisserman, 2014; He et al., 2016), and thus seem a good
choice for modeling visual frames. However, these features
tend to discard low level information useful in modeling the
motion in the video (Ballas et al., 2016).

To address these challenges, we implement an RNN we call
the Temporal Model (TEM). At each time step of the TEM, a
video frame encoding from a CNN serves as input. Rather
than extracting video frame features from a fully connected
layer of the pretrained CNN, we extract intermediate con-
volutional maps.

In detail, for a given video X with N frames X =
[X 1, X 2, · · · , X N ], N convolutional maps of size RL×D
are extracted, where L is the number of locations in the in-
put frame and D is the number of dimensions (See TEM
in Figure 1). To enable the network to store the most im-
portant L locations of each frame, we use a soft location at-
tention mechanism, fLatt (Bahdanau et al., 2015; Xu et al.,
2015a; Sharma et al., 2015). We ﬁrst use a softmax to com-
pute L probabilities that specify the importance of different
parts in the frame, and this creates an input map for fLatt.

Formally, given a video frame at time t, X t ∈ RL×D, the

fLatt mechanism is deﬁned as follows:

• Given:

ρt
j =

(cid:80)L

exp(Wj
pht−1
)
v
pht−1
k=1 exp(Wk
v
L
(cid:88)

)

fLatt(X t, ht−1

v

; Wp) =

jX t
ρt
j

j=1

v

∈ RK is the hidden state of the TEM at time
where ht−1
t-1 with K dimensions, and Wp ∈ RL×K. For each video
frame time step, TEM learns a vector representation by ap-
plying location attention on the frame convolution map,
conditioned on all previously seen frames:

F t = fLatt(X t, ht−1
v
v = fv(F t, ht−1
ht
; Θv)

v

; Wp)

where fv can be an RNN/LSTM/GRU cell and Θv is the
parameters of the fv. Due to the fact that vanilla RNNs
have gradient vanishing and exploding problems (Pascanu
et al., 2013), we use gradient clipping, and an LSTM with
the following ﬂow to handle potential vanishing gradients:

(2)

(3)

(4)

(5)

N = Number of frames in a given video
T = Number of words in description
v, ..., hN
Hv = Input video states, [h1
v ]
H t(cid:48)−1

g

= Decoder state hg at time t-1, repeated N times

H t(cid:48)−1

m = Memory state hm at time t-1, repeated N times

Wv, Wg ∈ RK×K
Wm ∈ RM ×K
u ∈ RK
α = Probability over all N frames
Θa = {Wv, Wg, Wm, u}

• Attention update [ ˆF (Θa)]:

QA = tanh(HvWv + H t(cid:48)−1
αt(cid:48) = softmax(QAu)
ˆF = H T

v αt(cid:48)

• Memory update:

m = fm(ht(cid:48)−1
ht(cid:48)

m , ˆF ; Θm)

g Wg + H t(cid:48)−1

m Wm)

(6)
(7)

(8)

(9)

Whi)
T
)
T
)

Whf )

Who )
T

T
)

v

v

it = σ(F tWxi + (ht−1
f t = σ(F tWxf + (ht−1
ot = σ(F tWxo + (ht−1
gt = tanh(F tWxg + (ht−1
v = f t (cid:12) ct−1
ct
v = ot (cid:12) tanh(ct)
ht

v
v + it (cid:12) gt

v

)

Whg )

where Wh∗ ∈ RK×K, Wx∗ ∈ RD×K, and we deﬁne Θv =
{Wh∗, Wx∗}.

3.2. Iterative Attention/Memory (IAM)

A main contribution of this work is a global view for the
video description task: A memory-based attention mecha-
nism that learns iterative attention relationships in an efﬁ-
cient sequence-to-sequence memory structure. We refer to
this as the Iterative Attention/Memory mechanism (IAM),
and it aggregates information from previously generated
words and all input frames.

The IAM component is an iterative memorized attention be-
tween an input video and a description. More speciﬁcally,
it learns a iterative attention structure for where to attend in
a video given all previously generated words (from the De-
coder), and previous states (from the TEM). This functions
as a memory structure, remembering encoded versions of
the video with corresponding language, and in turn, en-
abling the Decoder to access the full encoded video and
previously generated words as it generates new words.

4

Figure 2. Iterative Attention and Memory (IAM) is formulated as
an Attention update and a Memory update.

This component addresses several key issues in generating
a coherent video description. In video description, a sin-
gle word or phrase often describes action spanning multi-
ple frames within the input video. By employing the IAM,
the model can effectively capture the relationship between
a relatively short bit of language and an action that occurs
over multiple frames. This also functions to directly ad-
dress the problem of identifying which parts of the video
are most relevant for description.

The proposed Iterative Attention/Memory mechanism is
formalized with an Attention update and a Memory up-
date, detailed in Figure 2. Figure 1 illustrates where the
IAM sits within the full model, with the Attention module
shown in 1a and the Memory module shown in 1b.
As formalized in Figure 2, the Attention update ˆF (Θa)
computes the set of probabilities in a given time step for
attention within the input video states, the memory state,
and the decoder state. The Memory update stores what has
been attended to and described. This serves as the memo-
rization component, combining the previous memory with
the current iterative attention ˆF . We use an LSTM fm
with the equations described above to enable the network to
learn multi-layer attention over the input video and its cor-
responding language. The output of this function is then

used as input to the Decoder.

3.3. Decoder

In order to generate a new word conditioned on all previous
words and IAM states, a recurrent structure is modelled as
follows:

ht(cid:48)
g = fg(st(cid:48)
ˆst(cid:48)

= softmax((ht(cid:48)

, ht(cid:48)

m, ht(cid:48)−1

g
g )T We)

; Θg)

(10)

(11)

g ∈ RK, st(cid:48)

where ht(cid:48)
is a word vector at time t(cid:48), We ∈
RK×|V |, and |V | is the vocabulary size. In addition, ˆst(cid:48)
assigns a probability to each word in the language. fg is an
LSTM where st(cid:48)
g is the recurrent
state.

m are inputs and ht(cid:48)

and ht(cid:48)

3.4. Training and Optimization

in our network is to predict

The goal
the next word
given all previously seen words and an input video.
to optimize our network parameters Θ =
In order
{Wp, Θv, Θa, Θm, Θg, We}, we minimize a negative log
likelihood loss function:

L(S, X; Θ) = −

i log(ˆst(cid:48)
st(cid:48)

i ) + λ (cid:107) Θ (cid:107)2
2

(12)

T
(cid:88)

|V |
(cid:88)

t(cid:48)

i

where |V | is the vocabulary size. We fully train our net-
work in an end-to-end fashion using ﬁrst-order stochas-
tic gradient-based optimization method with an adaptive
learning rate. More speciﬁcally, in order to optimize our
network parameters, we use Adam (Kingma & Ba, 2015)
with learning rate 2 × 10−5 and set β1, β2 to 0.8 and
0.999, respectively. During training, we use a batch size
of 16. The source code for this paper is available on
https://github.com/rasoolfa/videocap.

4. Experiments

Dataset We evaluate the model on the Charades (Sig-
urdsson et al., 2016) dataset and the Microsoft Video De-
scription Corpus (MSVD) (Chen & Dolan, 2011). Cha-
rades contains 9, 848 videos (in total) and provides 27, 8471
video descriptions. We follow the same train/test splits as
Sigurdsson et al. (2016), with 7569 train, 1, 863 test, and
400 validation. A main difference between this dataset and
others is that it uses a “Hollywood in Homes” approach to
data collection, where “actors” are crowdsourced to act out
different actions. This yields a diverse set of videos, with
each containing a speciﬁc action.

MSVD is a set of YouTube videos annotated by workers
on Mechanical Turk,2 who were asked to pick a video clips
representing an activity.
In this dataset, each clip is an-
notated by multiple workers with a single sentence. The
dataset contains 1, 970 videos and about 80, 000 descrip-
tions, where 1, 200 of the videos are training data, 670 test,
and the rest (100 videos) for validation. In order for the
results to be comparable to other approaches, we follow
the exact training/validation/test splits provided by Venu-
gopalan et al. (2015b).

Evaluation metrics We report results on the video de-
scription generation task. In order to evaluate descriptions
generated by our model, we use model-free automatic eval-
uation metrics. We adopt METEOR, BLEU-N, and CIDEr
metrics available from the Microsoft COCO Caption Eval-
uation code3 to score the system.

Video and Caption preprocessing We preprocess the
captions for both datasets using the Natural Language
Toolkit (NLTK)4 and clip each description up to 30 words,
since the majority have less. We extract sample frames
from each video and pass each frame through VGGnet (Si-
monyan & Zisserman, 2014) without ﬁne-tuning. For the
experiments in this paper, we use the feature maps from
conv5 3 layer after applying ReLU. The feature map in this
layer is 14×14×512. Our TEM component operates on the
ﬂattened 196 × 512 of this feature cubes. For the ablation
studies, features from the fully connected layer with 4096
dimensions are used as well.

use

optimization We

Hyper-parameter
random
search (Bergstra & Bengio, 2012) on the validation set
to select hyper-parameters on both datasets. The word-
embedding size, hidden layer size (for both the TEM and
the Decoder), and memory size of the best model on
Charades are: 237, 1316, and 437, respectively. These
values are 402, 1479, and 797 for the model on the MSVD
dataset. A stack of two LSTMs are used in the Decoder and
TEM. The number of frame samples is a hyperparameter
which is selected among 4, 8, 16, 40 on the validation set.
ATT + NO TEM and NO IAM + TEM get the best results on
the validation set with 40 frames, and we use this as the
number of frames for all models in the ablation study.

4.1. Video Caption Generation

We ﬁrst present an ablation analysis to elucidate the contri-
bution of the different components of our proposed model.
Then, we compare the overall performance of our model to
other recent models.

1Only 16087 out of 27, 847 are used as descriptions for our
evaluation since the 27, 847 refers to script of the video as well as
descriptions.

2https://www.mturk.com/mturk/welcome
3https://github.com/tylin/coco-caption
4http://www.nltk.org/

5

Ablation Analysis

Ablation results are shown in Table 1, evaluating on the
MSVD test set. The ﬁrst (ATT + NO TEM) corresponds
to a simpler version of our model in which we remove the
TEM component and instead pass each frame of the video
through a CNN, extracting features from the last fully-
connected hidden layer. In addition, we replace our IAM
with a simpler version where the model only memorizes
the current step instead of all previous steps. In the next
variation (ATT + TEM), it is same as the ﬁrst one except
we use TEM instead of fully connected CNN features. In
the next ablation (NO IAM + TEM), we remove the IAM
component from our model and keep the rest of the model
as-is. In the next variation (IAM + NO TEM), we remove
the TEM and calculate features for each frame, similar to
ATT + NO TEM. Finally, the last row in the table is our
proposed model (IAM + TEM) with all its components.

The IAM plays a signiﬁcant role in the proposed model, and
removing it causes a large drop in performance, as mea-
sured by both BLEU and METEOR. On the other hand, re-
moving the TEM by itself does not drop performance as
much as dropping the IAM. Putting the two together, they
complement one another to result in overall better perfor-
mance for METEOR. However, further development on the
TEM component in future work is warranted.
In the NO
IAM + TEM condition, an entire video must be represented
with a ﬁxed-length vector, which may contribute to the
lower performance (Bahdanau et al., 2015). This is in con-
trast to the other models, which apply single layer attention
or IAM to search relevant parts of the video aligned with
the description.

Performance Comparison

To extensively evaluate the proposed model, we compare
with state-of-the-art models and baselines for the video
caption generation task on the MSVD dataset. In this ex-
periment, we use 8 frames per video as the inputs to the
TEM module. As shown in Table 2,5 our proposed model
achieves state-of-the-art scores in BLEU-4, and outperforms
almost all systems on METEOR. The closest-scoring com-
parison system, from Pan et al. (2016a), shows a trade-off
between METEOR and BLEU: BLEU prefers descriptions
with short-distance ﬂuency and high lexical overlap with
the observed descriptions, while METEOR permits less di-
rect overlap and longer descriptions. A detailed study of
the generated descriptions between the two systems would
be needed to better understand these differences.

5The symbol − indicates that the score was not reported by
the corresponding paper. The horizontal line in Table 2 separates
models that do/do not use external features for the video represen-
tation.

The improvement over previous work is particularly note-
worthy because we do not use external features for the
video, such as Optical Flow (Brox et al., 2004) (de-
noted Flow), 3-Dimensional Convolutional Network fea-
tures (Tran et al., 2015) (denoted C3D), or ﬁne-tuned CNN
features (denoted FT), which further enhances aspects such
as action recognition by leveraging an external dataset such
as UCF-101. The only system using external features that
outperforms the model proposed here is from Yu et al.
(2016), who uses a slightly different version of the same
dataset6 along with C3D features for a large improvement
in results (compare Table 2 rows 4 and 11); future work
may explore the utility of external visual features for this
work. Here, we demonstrate that the proposed architec-
ture maps visual space to language space with improved
performance over previous work, before addition of further
resources.

We additionally report results on the Charades dataset (Sig-
urdsson et al., 2016), which is challenging to train on be-
cause there are only a few (≈ 2) captions per video. In this
experiment, we use 16 frames per video as the input to the
TEM module. As shown in Table 3, our method achieves
a 10% relative improvement over the Venugopalan et al.
(2015a) model reported by Sigurdsson et al. (2016). It is
worth noting that humans reach a METEOR score of 24 and
a BLEU-4 score of 20, illustrating the low upper bound in
this task.7

Results Discussion

We show some example descriptions generated by our sys-
tem in Figure 3. The model generates mostly correct de-
scriptions, with naturalistic variation from the ground truth.
Errors illustrate a preference to describe items that have a
higher likelihood of being mentioned, even if they appear
in less of the frames. For example, in the “a dog is on
a trampoline” video, our model focuses on the man, who
appears in only a few frames, and generates the incorrect
description “a man is washing a bath”. The errors, along-
side the ablation study shown in Table 1, suggest that the
TEM module in particular may be further improved by fo-
cusing on how frames in the video sequence are captured
and passed to the IAM module.

5. Conclusion

We introduce a general framework for an memory-based
sequence learning model, trained end-to-end. We apply this
framework to the task of describing an input video with a

6Yu et al. (2016) uses the MSVD dataset reported in (Guadar-

rama et al., 2013), which has different preprocessing.

7For comparison, the upper bound BLEU score in machine

translation for English to French is above 30.

6

Method

METEOR

BLEU-1

BLEU-2

BLEU-3

BLEU-4

CIDEr

ATT + NO TEM
ATT + TEM
NO IAM + TEM
IAM + NO TEM
IAM + TEM [40F]
IAM + TEM [8F]

31.20
31.00
30.50
31.00
31.70
31.80

77.90
79.00
78.10
78.70
79.00
79.40

65.10
66.50
65.20
66.90
66.20
67.10

55.30
56.30
55.10
57.40
56.0
56.80

44.90
45.50
44.60
47.00
45.60
46.10

63.90
61.00
60.50
62.10
62.20
62.70

Table 1. Ablation of proposed model with and without the IAM component on the MSVD test set.

Method

METEOR

BLEU-1

BLEU-2

BLEU-3

BLEU-4

CIDEr

Venugopalan et al. (2015b)
Venugopalan et al. (2015a)
Pan et al. (2016b)
Yu et al. (2016)
Pan et al. (2016a)
Our Model

27.7
29.2
29.5
31.10
33.10
31.80

Yao et al. (2015) + C3D
Venugopalan et al. (2015a) + Flow 29.8
Ballas et al. (2016) + FT
Pan et al. (2016b) + C3D
Yu et al. (2016) + C3D

30.75
31.0
32.60

29.60

−
−
74.9
77.30
79.20
79.40

−
−
−
78.80
81.50

−
−
60.9
64.50
66.30
67.10

−
−
−
66.0
70.40

−
−
50.6
54.60
55.10
56.80

−
−
−
55.4
60.4

−
−
40.2
44.30
43.80
46.10

41.92
−
49.0
45.3
49.90

−
−
−
−
−
62.70

51.67
−
59.37
−
−

Table 2. Video captioning evaluation on MSVD (670 videos).

Method

M B@1 B@2 B@3 B@4 C

References

Human
(Sigurdsson et al., 2016)

24

62

43

29

20

53

Sigurdsson et al. (2016) 16
Our Model

49

30

18

11

14

17.6 50 31.1 18.8 11.5 16.7

Table 3. Video captioning evaluation on Charades (1863 videos).
M=METEOR, B=BLEU, C=CIDEr. Sigurdsson et al. (2016) results
use the Venugopalan et al. (2015a) model.

natural language description. Our model utilizes a deep
learning architecture that represents video with an explicit
model of the video’s temporal structure, and jointly mod-
els the video description and the temporal video sequence.
This effectively connects the visual video space and the lan-
guage description space.

A memory-based attention mechanism helps guide where
to attend and what to reason about as the description is gen-
erated. This allows the model to not only reason efﬁciently
about local attention, but also to consider the full sequence
of video frames during the generation of each word. Our
experiments conﬁrm that the memory components in our
architecture, most notably from the IAM module, play a
signiﬁcant role in improving the performance of the entire
network.

Future work should raim to reﬁne the temporal video frame
model, TEM, and explore how to improve performance on
capturing the ideal frames for each description.

Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio,
Yoshua. Neural machine translation by jointly learning
to align and translate. ICLR, 2015.

Ballas, Nicolas, Yao, Li, Pal, Chris, and Courville,
Aaron C. Delving deeper into convolutional networks
for learning video representations. In ICLR, 2016.

Bergstra, James and Bengio, Yoshua. Random search for
hyper-parameter optimization. J. Mach. Learn. Res., 13:
281–305, 2012.

Brox, T. and Malik, J. Large displacement optical ﬂow:
Descriptor matching in variational motion estimation.
TPAMI, 33(3):500–513, March 2011. ISSN 0162-8828.

Brox, T., Bruhn, A., Papenberg, N., and Weickert, J. High
accuracy optical ﬂow estimation based on a theory for
warping. In ECCV, 2004.

Chen, David L. and Dolan, William B. Collecting highly
In ACL, June

parallel data for paraphrase evaluation.
2011.

Devlin, Jacob, Cheng, Hao, Fang, Hao, Gupta, Saurabh,
Deng, Li, He, Xiaodong, Zweig, Geoffrey, and Mitchell,
Margaret. Language models for image captioning: The
quirks and what works. In ACL-IJCNLP, pp. 100–105,
July 2015.

Donahue, Jeff, Hendricks, Lisa Anne, Guadarrama, Sergio,
Rohrbach, Marcus, Venugopalan, Subhashini, Saenko,

7

Figure 3. Example captions generated by our model on MSVD test videos.

Kate, and Darrell, Trevor. Long-term recurrent convo-
lutional networks for visual recognition and description.
In CVPR, 2015.

Fang, Hao, Gupta, Saurabh,

Iandola, Forrest, Srivas-
tava, Rupesh K., Deng, Li, Dollar, Piotr, Gao, Jian-
feng, He, Xiaodong, Mitchell, Margaret, Platt, John C.,
Lawrence Zitnick, C., and Zweig, Geoffrey. From cap-
tions to visual concepts and back. In CVPR, June 2015.

Graves, Alex and Jaitly, Navdeep. Towards end-to-end
speech recognition with recurrent neural networks.
In
ICML-14, pp. 1764–1772, 2014.

Guadarrama, Sergio, Krishnamoorthy, Niveda, Malkar-
nenkar, Girish, Venugopalan, Subhashini, Mooney, Ray-
mond, Darrell, Trevor, and Saenko, Kate. Youtube2text:
Recognizing and describing arbitrary activities using se-
mantic hierarchies and zero-shot recognition. In ICCV,
pp. 2712–2719, 2013.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Deep residual learning for image recognition. In
CVPR, June 2016.

Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-
Neural Comput., 9(8):1735–1780,

term memory.
November 1997. ISSN 0899-7667.

Johnson, Justin, Karpathy, Andrej, and Fei-Fei, Li. Dense-
cap: Fully convolutional localization networks for dense
captioning. In CVPR, 2016.

Kingma, Diederik P. and Ba, Jimmy. Adam: A method for

stochastic optimization. In ICLR, 2015.

8

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classiﬁcation with deep convolutional neural
networks. In Pereira, F., Burges, C. J. C., Bottou, L., and
Weinberger, K. Q. (eds.), NIPS, pp. 1097–1105. 2012.

LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner,
Patrick. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–
2324, 1998.

Pan, Pingbo, Xu, Zhongwen, Yang, Yi, Wu, Fei, and
Zhuang, Yueting. Hierarchical recurrent neural encoder
for video representation with application to captioning.
In CVPR, June 2016a.

Pan, Yingwei, Mei, Tao, Yao, Ting, Li, Houqiang, and Rui,
Jointly modeling embedding and translation to

Yong.
bridge video and language. CVPR, 2016b.

Pascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua.
On the difﬁculty of training recurrent neural networks.
ICML-13, 28:1310–1318, 2013.

Rockt¨aschel, Tim, Grefenstette, Edward, Hermann,
Karl Moritz, Kocisk´y, Tom´as, and Blunsom, Phil. Rea-
soning about entailment with neural attention. In ICLR,
2016.

Rohrbach, M., Qiu, W., Titov, I., Thater, S., Pinkal, M., and
Schiele, B. Translating video content to natural language
descriptions. In ICCV, pp. 433–440, Dec 2013.

Sharma, Shikhar, Kiros, Ryan, and Salakhutdinov, Rus-
lan. Action recognition using visual attention. CoRR,
abs/1511.04119, 2015.

Shin, Andrew, Ohnishi, Katsunori, and Harada, Tatsuya.
Beyond caption to narrative: Video captioning with mul-
tiple sentences. ICIP, 2016.

Sigurdsson, Gunnar A., Varol, G¨ul, Wang, Xiaolong,
Farhadi, Ali, Laptev, Ivan, and Gupta, Abhinav. Hol-
lywood in homes: Crowdsourcing data collection for ac-
tivity understanding. In ECCV, 2016.

Yao, Li, Torabi, Atousa, Cho, Kyunghyun, Ballas, Nico-
las, Pal, Christopher, Larochelle, Hugo, and Courville,
Aaron. Describing videos by exploiting temporal struc-
ture. In ICCV, 2015.

Yu, Haonan, Wang, Jiang, Huang, Zhiheng, Yang, Yi, and
Xu, Wei. Video paragraph captioning using hierarchical
recurrent neural networks. In CVPR, June 2016.

Simonyan, K. and Zisserman, A. Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014.

Sukhbaatar, Sainbayar, Szlam, Arthur, Weston, Jason, and
Fergus, Rob. End-to-end memory networks. NIPS, 2015.

Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence
to sequence learning with neural networks. In NIPS, pp.
3104–3112. 2014.

Tran, Du, Bourdev, Lubomir, Fergus, Rob, Torresani,
Lorenzo, and Paluri, Manohar. Learning spatiotempo-
In ICCV,
ral features with 3d convolutional networks.
2015.

Venugopalan, Subhashini, Rohrbach, Marcus, Donahue,
Jeff, Mooney, Raymond, Darrell, Trevor, and Saenko,
Kate. Sequence to sequence – video to text. In ICCV,
2015a.

Venugopalan, Subhashini, Xu, Huijuan, Donahue, Jeff,
Rohrbach, Marcus, Mooney, Raymond, and Saenko,
Kate. Translating videos to natural language using deep
recurrent neural networks. In NAACL HLT, 2015b.

Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Er-
han, Dumitru. Show and tell: A neural image caption
generator. In CVPR, June 2015.

Weston, Jason, Chopra, Sumit, and Bordes, Antoine. Mem-

ory networks. CoRR, abs/1410.3916, 2014.

Xu, Kelvin, Ba, Jimmy, Kiros, Ryan, Cho, Kyunghyun,
Courville, Aaron, Salakhudinov, Ruslan, Zemel, Rich,
and Bengio, Yoshua. Show, attend and tell: Neural im-
age caption generation with visual attention. In ICML-
15, pp. 2048–2057, 2015a.

Xu, Ran, Xiong, Caiming, Chen, Wei, and Corso, Jason J.
Jointly modeling deep video and compositional text to
bridge vision and language in a uniﬁed framework. In
AAAI, 2015b.

Yang, Zhilin, Yuan, Ye, Wu, Yuexin, Salakhutdinov, Rus-
lan, and Cohen, William W. Encode, review, and de-
code: Reviewer module for caption generation. CoRR,
abs/1605.07912, 2016.

9

Memory-augmented Attention Modelling for Videos

Rasool Fakoor†∗, Abdel-rahman Mohamed,††, Margaret Mitchell‡†, Sing Bing Kang††,
Pushmeet Kohli††
††Microsoft Research †University of Texas at Arlington ‡†Google
†rasool.fakoor@mavs.uta.edu, ††{asamir, singbing.kang, pkohli}@microsoft.com ‡†mmitchellai@google.com,

7
1
0
2
 
r
p
A
 
4
2
 
 
]

V
C
.
s
c
[
 
 
4
v
1
6
2
2
0
.
1
1
6
1
:
v
i
X
r
a

Abstract
We present a method to improve video descrip-
tion generation by modeling higher-order inter-
actions between video frames and described con-
cepts. By storing past visual attention in the
video associated to previously generated words,
the system is able to decide what to look at and
describe in light of what it has already looked
at and described. This enables not only more
effective local attention, but tractable considera-
tion of the video sequence while generating each
word. Evaluation on the challenging and pop-
ular MSVD and Charades datasets demonstrates
that the proposed architecture outperforms previ-
ous video description approaches without requir-
ing external temporal video features. The source
code for this paper is available on https://
github.com/rasoolfa/videocap.

1. Introduction

Deep neural architectures have led to remarkable progress
in computer vision and natural language processing prob-
lems.
Image captioning is one such problem, where the
combination of convolutional structures (Krizhevsky et al.,
2012; LeCun et al., 1998), and sequential recurrent struc-
tures (Sutskever et al., 2014) leads to remarkable improve-
ments over previous work (Fang et al., 2015; Devlin et al.,
2015). One of the emerging modelling paradigms, shared
by models for image captioning as well as related vision-
language problems, is the notion of an attention mechanism
that guides the model to attend to certain parts of the image
while generating (Xu et al., 2015a).

The attention models used for problems such as image cap-
tioning typically depend on the single image under con-
sideration and the partial output generated so far, jointly
capturing one region of an image and the words being gen-
erated. However, such models cannot directly capture the
temporal reasoning necessary to effectively produce words

that refer to actions and events taking place over multiple
frames in a video. For example, in a video depicting “some-
one waving a hand”, the “waving” action can start from any
frame and can continue on for a variable number of follow-
ing frames. At the same time, videos contain many frames
that do not provide additional information over the smaller
set of frames necessary to generate a summarizing descrip-
tion. Given these challenges, it is not surprising that even
with recent advancements in image captioning (Fang et al.,
2015; Xu et al., 2015a; Johnson et al., 2016; Vinyals et al.,
2015; Donahue et al., 2015), video captioning has remained
challenging.

Motivated by these observations, we introduce a memory-
based attention mechanism for video captioning and de-
scription. Our model utilizes memories of past attention
in the video when reasoning about where to attend in a
current time step. This allows the model to not only ef-
fectively leverage local attention, but also to consider the
entire video as it generates each word. This mechanism ef-
fectively binds information from both vision and language
sources into a coherent structure.

Our work shares the same goals as recent work on atten-
tion mechanisms for sequence-to-sequence architectures,
such as Rockt¨aschel et al. (2016) and Yang et al. (2016).
Rockt¨aschel et al. (2016) consider the domain of entailment
relations, where the goal is to determine entailment given
two input sentences. They propose a soft attention model
that is not only focused on the current state, but the previous
as well. In our model, all previous attentions are explicitly
stored into memory, and the system learns to memorize the
encoded version of the input videos conditioned on previ-
ously seen words. Yang et al. (2016) and our work both
try to solve the problem of locality of attention in vision-
to-language, but while Yang et al. (2016) introduce a mem-
ory architecture optimized for single image caption gener-
ation, we introduce a memory architecture that operates on
a streaming video’s temporal sequence.

The contributions of this work include:

∗Corresponding

author:

sool.fakoor@mavs.uta.edu)

Rasool

Fakoor

(

ra-

• A deep learning architecture that represents video
with an explicit model of the video’s temporal struc-

1

ture.

• A method to jointly model the video description and
temporal video sequence, connecting the visual video
space and the language description space.

• A memory-based attention mechanism that learns iter-
ative attention relationships in a simple and effective
sequence-to-sequence memory structure.

• Extensive comparison of this work and previous work
on the video captioning problem on the MSVD (Chen
& Dolan, 2011) and the Charades (Sigurdsson et al.,
2016) datasets.

We focus on the video captioning problem, however, the
proposed model is general enough to be applicable in other
sequence problems where attention models are used (e.g.,
machine translation or recognizing entailment relations).

2. Related Work

One of the primary challenges in learning a mapping from
a visual space (i.e., video or image) to a language space
is learning a representation that not only effectively rep-
resents each of these modalities, but is also able to trans-
late a representation from one space to the other. Rohrbach
et al. (2013) developed a model that generates a seman-
tic representation of visual content that can be used as
the source language for the language generation module.
Venugopalan et al. (2015b) proposed a deep method to
translate a video into a sentence where an entire video is
represented with a single vector based on the mean pool
of frame features. However, it was recognized that repre-
senting a video by an average of its frames loses the tem-
poral structure of the video. To address this problem, re-
cent work (Yao et al., 2015; Pan et al., 2016a; Venugopalan
et al., 2015a; Shin et al., 2016; Pan et al., 2016b; Xu et al.,
2015b; Ballas et al., 2016; Yu et al., 2016) proposed meth-
ods to model the temporal structure of videos as well as
language.

The majority of these methods are inspired by sequence-to-
sequence (Sutskever et al., 2014) and attention (Bahdanau
et al., 2015) models. Sequence learning was proposed to
map the input sequence of a source language to a target
language (Sutskever et al., 2014). Applying this method
with an additional attention mechanism to the problem of
translating a video to a description showed promising ini-
tial results, however, revealed additional challenges. First,
modelling the video content with a ﬁxed-length vector in
order to map it to a language space is a more complex prob-
lem than mapping from a language to a language, given the
complexity of visual content and the difference between the
two modalities. Since not all frames in a video are equally
salient for a short description, and an event can happen
in multiple frames, it is important for a model to identify

which frames are most salient. Further, the models need
additional work to be able to focus on points of interest
within the video frames to select what to talk about. Even
a variable-length vector to represent a video using attention
(Yao et al., 2015) can have some problems.

More speciﬁcally, current attention methods are lo-
cal (Yang et al., 2016), since the attention mechanism
works in a sequential structure, and lack the ability to cap-
ture global structure. Moreover, combining a video and a
description as a sequence-to-sequence problem motivates
using some variant of a recurrent neural network (RNN)
(Hochreiter & Schmidhuber, 1997): Given the limited ca-
pacity of a recurrent network to model very long sequences,
memory networks (Weston et al., 2014; Sukhbaatar et al.,
2015) have been introduced to help the RNN memorize se-
quences. However, one problem these memory networks
suffer from is difﬁculty in training the model. The model
proposed by Weston et al. (2014) requires supervision at
each layer, which makes training with backpropagation a
challenging task. Sukhbaatar et al. (2015) proposed a mem-
ory network that can be trained end-to-end, and the current
work follows this research line to tackle the challenging
problem of modeling vision and language memories for
video description generation.

3. Learning to Attend and Memorize

A main challenge in video description is to ﬁnd a mapping
that can capture the connection between the video frames
and the video description. Sequence-to-sequence models,
which work well at connecting input and output sequences
in machine translation (Sutskever et al., 2014), do not per-
form as well for this task, as there is not the same direct
alignment between a full video sequence and its summariz-
ing description.

Our goal in the video description problem is to create an
architecture that learns which moments to focus on in a
video sequence in order to generate a summarizing natu-
ral language description. The modelling challenges we set
forth for the video description problem are: (1) Processing
the temporal structure of the video; (2) Learning to attend
to important parts of the video; and (3) Generating a de-
scription where each word is relevant to the video. At a
high-level, this can be understood as having three primary
parts: When moments in the video are particularly salient;
what concepts to focus on; and how to talk about them.
We directly address these issues in an end-to-end network
with three primary corresponding components (Figure 1):
A Temporal Model (TEM), An Iterative Attention/Memory
Model (IAM), and a Decoder. In summary:

• When: Frames within the video sequence - The Tem-

poral Model (TEM).

2

Figure 1. Our proposed architecture. Each component of our model is described in 3.1 through 3.3.

• What: Language-grounded concepts depicted in the
video - The Iterative Attention/Memory mechanism
(IAM).

• How: Words that ﬂuently describe the what and when

- The Decoder.

The Temporal Model is in place to capture the temporal
structure of the video: It functions as a when component.
The Iterative Attention/Memory is a main contribution of
this work, functioning as a what component to remember
relationships between words and video frames, and storing
longer term memories. The Decoder generates language,
and functions as the how component to create the ﬁnal de-
scription.

To train the system end to end, we formulate the problem
as sequence learning to maximize the probability of gener-
ating a correct description given a video:

Θ∗ = arg max

(cid:88)

Θ

(S,f1,...,fN )

log p(S|f1, . . . , fN ; Θ)

(1)

where S is the description, f1, f2, . . . , fN are the input
video frames, and Θ is the model parameter vector.
In
the next sections, we will describe each component of the
model, then explain the details of training and inference.

Notational note: Numbered equations use bold face to denote
multi-dimensional learnable parameters, e.g., Wj
p. To distinguish
the two different sets of time steps, one for video frames and one
for words in the description, we use the notation t for video and t(cid:48)
for language. Throughout, the terms description and caption are
used interchangeably.

3.1. Temporal Model (TEM)

The ﬁrst module we introduce encodes the temporal struc-
ture of the input video. A clear framework to use for this

3

is a Recurrent Neural Network (RNN), which has been
shown to be effectual in modelling the temporal struc-
ture of sequential data such as video (Ballas et al., 2016;
Sharma et al., 2015; Venugopalan et al., 2015a) and speech
In order to apply this in video
(Graves & Jaitly, 2014).
sequences to generate a description, we seek to capture
the fact that frame-to-frame temporal variation tends to be
local (Brox & Malik, 2011) and critical in modeling mo-
tion (Ballas et al., 2016). Visual features extracted from
the last fully connected layers of Convolutional Neural Net-
works (CNNs) have been shown to produce state-of-the-art
results in image classiﬁcation and recognition (Simonyan
& Zisserman, 2014; He et al., 2016), and thus seem a good
choice for modeling visual frames. However, these features
tend to discard low level information useful in modeling the
motion in the video (Ballas et al., 2016).

To address these challenges, we implement an RNN we call
the Temporal Model (TEM). At each time step of the TEM, a
video frame encoding from a CNN serves as input. Rather
than extracting video frame features from a fully connected
layer of the pretrained CNN, we extract intermediate con-
volutional maps.

In detail, for a given video X with N frames X =
[X 1, X 2, · · · , X N ], N convolutional maps of size RL×D
are extracted, where L is the number of locations in the in-
put frame and D is the number of dimensions (See TEM
in Figure 1). To enable the network to store the most im-
portant L locations of each frame, we use a soft location at-
tention mechanism, fLatt (Bahdanau et al., 2015; Xu et al.,
2015a; Sharma et al., 2015). We ﬁrst use a softmax to com-
pute L probabilities that specify the importance of different
parts in the frame, and this creates an input map for fLatt.

Formally, given a video frame at time t, X t ∈ RL×D, the

fLatt mechanism is deﬁned as follows:

• Given:

ρt
j =

(cid:80)L

exp(Wj
pht−1
)
v
pht−1
k=1 exp(Wk
v
L
(cid:88)

)

fLatt(X t, ht−1

v

; Wp) =

jX t
ρt
j

j=1

v

∈ RK is the hidden state of the TEM at time
where ht−1
t-1 with K dimensions, and Wp ∈ RL×K. For each video
frame time step, TEM learns a vector representation by ap-
plying location attention on the frame convolution map,
conditioned on all previously seen frames:

F t = fLatt(X t, ht−1
v
v = fv(F t, ht−1
ht
; Θv)

v

; Wp)

where fv can be an RNN/LSTM/GRU cell and Θv is the
parameters of the fv. Due to the fact that vanilla RNNs
have gradient vanishing and exploding problems (Pascanu
et al., 2013), we use gradient clipping, and an LSTM with
the following ﬂow to handle potential vanishing gradients:

(2)

(3)

(4)

(5)

N = Number of frames in a given video
T = Number of words in description
v, ..., hN
Hv = Input video states, [h1
v ]
H t(cid:48)−1

g

= Decoder state hg at time t-1, repeated N times

H t(cid:48)−1

m = Memory state hm at time t-1, repeated N times

Wv, Wg ∈ RK×K
Wm ∈ RM ×K
u ∈ RK
α = Probability over all N frames
Θa = {Wv, Wg, Wm, u}

• Attention update [ ˆF (Θa)]:

QA = tanh(HvWv + H t(cid:48)−1
αt(cid:48) = softmax(QAu)
ˆF = H T

v αt(cid:48)

• Memory update:

m = fm(ht(cid:48)−1
ht(cid:48)

m , ˆF ; Θm)

g Wg + H t(cid:48)−1

m Wm)

(6)
(7)

(8)

(9)

Whi)
T
)
T
)

Whf )

Who )
T

T
)

v

v

it = σ(F tWxi + (ht−1
f t = σ(F tWxf + (ht−1
ot = σ(F tWxo + (ht−1
gt = tanh(F tWxg + (ht−1
v = f t (cid:12) ct−1
ct
v = ot (cid:12) tanh(ct)
ht

v
v + it (cid:12) gt

v

)

Whg )

where Wh∗ ∈ RK×K, Wx∗ ∈ RD×K, and we deﬁne Θv =
{Wh∗, Wx∗}.

3.2. Iterative Attention/Memory (IAM)

A main contribution of this work is a global view for the
video description task: A memory-based attention mecha-
nism that learns iterative attention relationships in an efﬁ-
cient sequence-to-sequence memory structure. We refer to
this as the Iterative Attention/Memory mechanism (IAM),
and it aggregates information from previously generated
words and all input frames.

The IAM component is an iterative memorized attention be-
tween an input video and a description. More speciﬁcally,
it learns a iterative attention structure for where to attend in
a video given all previously generated words (from the De-
coder), and previous states (from the TEM). This functions
as a memory structure, remembering encoded versions of
the video with corresponding language, and in turn, en-
abling the Decoder to access the full encoded video and
previously generated words as it generates new words.

4

Figure 2. Iterative Attention and Memory (IAM) is formulated as
an Attention update and a Memory update.

This component addresses several key issues in generating
a coherent video description. In video description, a sin-
gle word or phrase often describes action spanning multi-
ple frames within the input video. By employing the IAM,
the model can effectively capture the relationship between
a relatively short bit of language and an action that occurs
over multiple frames. This also functions to directly ad-
dress the problem of identifying which parts of the video
are most relevant for description.

The proposed Iterative Attention/Memory mechanism is
formalized with an Attention update and a Memory up-
date, detailed in Figure 2. Figure 1 illustrates where the
IAM sits within the full model, with the Attention module
shown in 1a and the Memory module shown in 1b.
As formalized in Figure 2, the Attention update ˆF (Θa)
computes the set of probabilities in a given time step for
attention within the input video states, the memory state,
and the decoder state. The Memory update stores what has
been attended to and described. This serves as the memo-
rization component, combining the previous memory with
the current iterative attention ˆF . We use an LSTM fm
with the equations described above to enable the network to
learn multi-layer attention over the input video and its cor-
responding language. The output of this function is then

used as input to the Decoder.

3.3. Decoder

In order to generate a new word conditioned on all previous
words and IAM states, a recurrent structure is modelled as
follows:

ht(cid:48)
g = fg(st(cid:48)
ˆst(cid:48)

= softmax((ht(cid:48)

, ht(cid:48)

m, ht(cid:48)−1

g
g )T We)

; Θg)

(10)

(11)

g ∈ RK, st(cid:48)

where ht(cid:48)
is a word vector at time t(cid:48), We ∈
RK×|V |, and |V | is the vocabulary size. In addition, ˆst(cid:48)
assigns a probability to each word in the language. fg is an
LSTM where st(cid:48)
g is the recurrent
state.

m are inputs and ht(cid:48)

and ht(cid:48)

3.4. Training and Optimization

in our network is to predict

The goal
the next word
given all previously seen words and an input video.
to optimize our network parameters Θ =
In order
{Wp, Θv, Θa, Θm, Θg, We}, we minimize a negative log
likelihood loss function:

L(S, X; Θ) = −

i log(ˆst(cid:48)
st(cid:48)

i ) + λ (cid:107) Θ (cid:107)2
2

(12)

T
(cid:88)

|V |
(cid:88)

t(cid:48)

i

where |V | is the vocabulary size. We fully train our net-
work in an end-to-end fashion using ﬁrst-order stochas-
tic gradient-based optimization method with an adaptive
learning rate. More speciﬁcally, in order to optimize our
network parameters, we use Adam (Kingma & Ba, 2015)
with learning rate 2 × 10−5 and set β1, β2 to 0.8 and
0.999, respectively. During training, we use a batch size
of 16. The source code for this paper is available on
https://github.com/rasoolfa/videocap.

4. Experiments

Dataset We evaluate the model on the Charades (Sig-
urdsson et al., 2016) dataset and the Microsoft Video De-
scription Corpus (MSVD) (Chen & Dolan, 2011). Cha-
rades contains 9, 848 videos (in total) and provides 27, 8471
video descriptions. We follow the same train/test splits as
Sigurdsson et al. (2016), with 7569 train, 1, 863 test, and
400 validation. A main difference between this dataset and
others is that it uses a “Hollywood in Homes” approach to
data collection, where “actors” are crowdsourced to act out
different actions. This yields a diverse set of videos, with
each containing a speciﬁc action.

MSVD is a set of YouTube videos annotated by workers
on Mechanical Turk,2 who were asked to pick a video clips
representing an activity.
In this dataset, each clip is an-
notated by multiple workers with a single sentence. The
dataset contains 1, 970 videos and about 80, 000 descrip-
tions, where 1, 200 of the videos are training data, 670 test,
and the rest (100 videos) for validation. In order for the
results to be comparable to other approaches, we follow
the exact training/validation/test splits provided by Venu-
gopalan et al. (2015b).

Evaluation metrics We report results on the video de-
scription generation task. In order to evaluate descriptions
generated by our model, we use model-free automatic eval-
uation metrics. We adopt METEOR, BLEU-N, and CIDEr
metrics available from the Microsoft COCO Caption Eval-
uation code3 to score the system.

Video and Caption preprocessing We preprocess the
captions for both datasets using the Natural Language
Toolkit (NLTK)4 and clip each description up to 30 words,
since the majority have less. We extract sample frames
from each video and pass each frame through VGGnet (Si-
monyan & Zisserman, 2014) without ﬁne-tuning. For the
experiments in this paper, we use the feature maps from
conv5 3 layer after applying ReLU. The feature map in this
layer is 14×14×512. Our TEM component operates on the
ﬂattened 196 × 512 of this feature cubes. For the ablation
studies, features from the fully connected layer with 4096
dimensions are used as well.

use

optimization We

Hyper-parameter
random
search (Bergstra & Bengio, 2012) on the validation set
to select hyper-parameters on both datasets. The word-
embedding size, hidden layer size (for both the TEM and
the Decoder), and memory size of the best model on
Charades are: 237, 1316, and 437, respectively. These
values are 402, 1479, and 797 for the model on the MSVD
dataset. A stack of two LSTMs are used in the Decoder and
TEM. The number of frame samples is a hyperparameter
which is selected among 4, 8, 16, 40 on the validation set.
ATT + NO TEM and NO IAM + TEM get the best results on
the validation set with 40 frames, and we use this as the
number of frames for all models in the ablation study.

4.1. Video Caption Generation

We ﬁrst present an ablation analysis to elucidate the contri-
bution of the different components of our proposed model.
Then, we compare the overall performance of our model to
other recent models.

1Only 16087 out of 27, 847 are used as descriptions for our
evaluation since the 27, 847 refers to script of the video as well as
descriptions.

2https://www.mturk.com/mturk/welcome
3https://github.com/tylin/coco-caption
4http://www.nltk.org/

5

Ablation Analysis

Ablation results are shown in Table 1, evaluating on the
MSVD test set. The ﬁrst (ATT + NO TEM) corresponds
to a simpler version of our model in which we remove the
TEM component and instead pass each frame of the video
through a CNN, extracting features from the last fully-
connected hidden layer. In addition, we replace our IAM
with a simpler version where the model only memorizes
the current step instead of all previous steps. In the next
variation (ATT + TEM), it is same as the ﬁrst one except
we use TEM instead of fully connected CNN features. In
the next ablation (NO IAM + TEM), we remove the IAM
component from our model and keep the rest of the model
as-is. In the next variation (IAM + NO TEM), we remove
the TEM and calculate features for each frame, similar to
ATT + NO TEM. Finally, the last row in the table is our
proposed model (IAM + TEM) with all its components.

The IAM plays a signiﬁcant role in the proposed model, and
removing it causes a large drop in performance, as mea-
sured by both BLEU and METEOR. On the other hand, re-
moving the TEM by itself does not drop performance as
much as dropping the IAM. Putting the two together, they
complement one another to result in overall better perfor-
mance for METEOR. However, further development on the
TEM component in future work is warranted.
In the NO
IAM + TEM condition, an entire video must be represented
with a ﬁxed-length vector, which may contribute to the
lower performance (Bahdanau et al., 2015). This is in con-
trast to the other models, which apply single layer attention
or IAM to search relevant parts of the video aligned with
the description.

Performance Comparison

To extensively evaluate the proposed model, we compare
with state-of-the-art models and baselines for the video
caption generation task on the MSVD dataset. In this ex-
periment, we use 8 frames per video as the inputs to the
TEM module. As shown in Table 2,5 our proposed model
achieves state-of-the-art scores in BLEU-4, and outperforms
almost all systems on METEOR. The closest-scoring com-
parison system, from Pan et al. (2016a), shows a trade-off
between METEOR and BLEU: BLEU prefers descriptions
with short-distance ﬂuency and high lexical overlap with
the observed descriptions, while METEOR permits less di-
rect overlap and longer descriptions. A detailed study of
the generated descriptions between the two systems would
be needed to better understand these differences.

5The symbol − indicates that the score was not reported by
the corresponding paper. The horizontal line in Table 2 separates
models that do/do not use external features for the video represen-
tation.

The improvement over previous work is particularly note-
worthy because we do not use external features for the
video, such as Optical Flow (Brox et al., 2004) (de-
noted Flow), 3-Dimensional Convolutional Network fea-
tures (Tran et al., 2015) (denoted C3D), or ﬁne-tuned CNN
features (denoted FT), which further enhances aspects such
as action recognition by leveraging an external dataset such
as UCF-101. The only system using external features that
outperforms the model proposed here is from Yu et al.
(2016), who uses a slightly different version of the same
dataset6 along with C3D features for a large improvement
in results (compare Table 2 rows 4 and 11); future work
may explore the utility of external visual features for this
work. Here, we demonstrate that the proposed architec-
ture maps visual space to language space with improved
performance over previous work, before addition of further
resources.

We additionally report results on the Charades dataset (Sig-
urdsson et al., 2016), which is challenging to train on be-
cause there are only a few (≈ 2) captions per video. In this
experiment, we use 16 frames per video as the input to the
TEM module. As shown in Table 3, our method achieves
a 10% relative improvement over the Venugopalan et al.
(2015a) model reported by Sigurdsson et al. (2016). It is
worth noting that humans reach a METEOR score of 24 and
a BLEU-4 score of 20, illustrating the low upper bound in
this task.7

Results Discussion

We show some example descriptions generated by our sys-
tem in Figure 3. The model generates mostly correct de-
scriptions, with naturalistic variation from the ground truth.
Errors illustrate a preference to describe items that have a
higher likelihood of being mentioned, even if they appear
in less of the frames. For example, in the “a dog is on
a trampoline” video, our model focuses on the man, who
appears in only a few frames, and generates the incorrect
description “a man is washing a bath”. The errors, along-
side the ablation study shown in Table 1, suggest that the
TEM module in particular may be further improved by fo-
cusing on how frames in the video sequence are captured
and passed to the IAM module.

5. Conclusion

We introduce a general framework for an memory-based
sequence learning model, trained end-to-end. We apply this
framework to the task of describing an input video with a

6Yu et al. (2016) uses the MSVD dataset reported in (Guadar-

rama et al., 2013), which has different preprocessing.

7For comparison, the upper bound BLEU score in machine

translation for English to French is above 30.

6

Method

METEOR

BLEU-1

BLEU-2

BLEU-3

BLEU-4

CIDEr

ATT + NO TEM
ATT + TEM
NO IAM + TEM
IAM + NO TEM
IAM + TEM [40F]
IAM + TEM [8F]

31.20
31.00
30.50
31.00
31.70
31.80

77.90
79.00
78.10
78.70
79.00
79.40

65.10
66.50
65.20
66.90
66.20
67.10

55.30
56.30
55.10
57.40
56.0
56.80

44.90
45.50
44.60
47.00
45.60
46.10

63.90
61.00
60.50
62.10
62.20
62.70

Table 1. Ablation of proposed model with and without the IAM component on the MSVD test set.

Method

METEOR

BLEU-1

BLEU-2

BLEU-3

BLEU-4

CIDEr

Venugopalan et al. (2015b)
Venugopalan et al. (2015a)
Pan et al. (2016b)
Yu et al. (2016)
Pan et al. (2016a)
Our Model

27.7
29.2
29.5
31.10
33.10
31.80

Yao et al. (2015) + C3D
Venugopalan et al. (2015a) + Flow 29.8
Ballas et al. (2016) + FT
Pan et al. (2016b) + C3D
Yu et al. (2016) + C3D

30.75
31.0
32.60

29.60

−
−
74.9
77.30
79.20
79.40

−
−
−
78.80
81.50

−
−
60.9
64.50
66.30
67.10

−
−
−
66.0
70.40

−
−
50.6
54.60
55.10
56.80

−
−
−
55.4
60.4

−
−
40.2
44.30
43.80
46.10

41.92
−
49.0
45.3
49.90

−
−
−
−
−
62.70

51.67
−
59.37
−
−

Table 2. Video captioning evaluation on MSVD (670 videos).

Method

M B@1 B@2 B@3 B@4 C

References

Human
(Sigurdsson et al., 2016)

24

62

43

29

20

53

Sigurdsson et al. (2016) 16
Our Model

49

30

18

11

14

17.6 50 31.1 18.8 11.5 16.7

Table 3. Video captioning evaluation on Charades (1863 videos).
M=METEOR, B=BLEU, C=CIDEr. Sigurdsson et al. (2016) results
use the Venugopalan et al. (2015a) model.

natural language description. Our model utilizes a deep
learning architecture that represents video with an explicit
model of the video’s temporal structure, and jointly mod-
els the video description and the temporal video sequence.
This effectively connects the visual video space and the lan-
guage description space.

A memory-based attention mechanism helps guide where
to attend and what to reason about as the description is gen-
erated. This allows the model to not only reason efﬁciently
about local attention, but also to consider the full sequence
of video frames during the generation of each word. Our
experiments conﬁrm that the memory components in our
architecture, most notably from the IAM module, play a
signiﬁcant role in improving the performance of the entire
network.

Future work should raim to reﬁne the temporal video frame
model, TEM, and explore how to improve performance on
capturing the ideal frames for each description.

Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio,
Yoshua. Neural machine translation by jointly learning
to align and translate. ICLR, 2015.

Ballas, Nicolas, Yao, Li, Pal, Chris, and Courville,
Aaron C. Delving deeper into convolutional networks
for learning video representations. In ICLR, 2016.

Bergstra, James and Bengio, Yoshua. Random search for
hyper-parameter optimization. J. Mach. Learn. Res., 13:
281–305, 2012.

Brox, T. and Malik, J. Large displacement optical ﬂow:
Descriptor matching in variational motion estimation.
TPAMI, 33(3):500–513, March 2011. ISSN 0162-8828.

Brox, T., Bruhn, A., Papenberg, N., and Weickert, J. High
accuracy optical ﬂow estimation based on a theory for
warping. In ECCV, 2004.

Chen, David L. and Dolan, William B. Collecting highly
In ACL, June

parallel data for paraphrase evaluation.
2011.

Devlin, Jacob, Cheng, Hao, Fang, Hao, Gupta, Saurabh,
Deng, Li, He, Xiaodong, Zweig, Geoffrey, and Mitchell,
Margaret. Language models for image captioning: The
quirks and what works. In ACL-IJCNLP, pp. 100–105,
July 2015.

Donahue, Jeff, Hendricks, Lisa Anne, Guadarrama, Sergio,
Rohrbach, Marcus, Venugopalan, Subhashini, Saenko,

7

Figure 3. Example captions generated by our model on MSVD test videos.

Kate, and Darrell, Trevor. Long-term recurrent convo-
lutional networks for visual recognition and description.
In CVPR, 2015.

Fang, Hao, Gupta, Saurabh,

Iandola, Forrest, Srivas-
tava, Rupesh K., Deng, Li, Dollar, Piotr, Gao, Jian-
feng, He, Xiaodong, Mitchell, Margaret, Platt, John C.,
Lawrence Zitnick, C., and Zweig, Geoffrey. From cap-
tions to visual concepts and back. In CVPR, June 2015.

Graves, Alex and Jaitly, Navdeep. Towards end-to-end
speech recognition with recurrent neural networks.
In
ICML-14, pp. 1764–1772, 2014.

Guadarrama, Sergio, Krishnamoorthy, Niveda, Malkar-
nenkar, Girish, Venugopalan, Subhashini, Mooney, Ray-
mond, Darrell, Trevor, and Saenko, Kate. Youtube2text:
Recognizing and describing arbitrary activities using se-
mantic hierarchies and zero-shot recognition. In ICCV,
pp. 2712–2719, 2013.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Deep residual learning for image recognition. In
CVPR, June 2016.

Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-
Neural Comput., 9(8):1735–1780,

term memory.
November 1997. ISSN 0899-7667.

Johnson, Justin, Karpathy, Andrej, and Fei-Fei, Li. Dense-
cap: Fully convolutional localization networks for dense
captioning. In CVPR, 2016.

Kingma, Diederik P. and Ba, Jimmy. Adam: A method for

stochastic optimization. In ICLR, 2015.

8

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classiﬁcation with deep convolutional neural
networks. In Pereira, F., Burges, C. J. C., Bottou, L., and
Weinberger, K. Q. (eds.), NIPS, pp. 1097–1105. 2012.

LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner,
Patrick. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–
2324, 1998.

Pan, Pingbo, Xu, Zhongwen, Yang, Yi, Wu, Fei, and
Zhuang, Yueting. Hierarchical recurrent neural encoder
for video representation with application to captioning.
In CVPR, June 2016a.

Pan, Yingwei, Mei, Tao, Yao, Ting, Li, Houqiang, and Rui,
Jointly modeling embedding and translation to

Yong.
bridge video and language. CVPR, 2016b.

Pascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua.
On the difﬁculty of training recurrent neural networks.
ICML-13, 28:1310–1318, 2013.

Rockt¨aschel, Tim, Grefenstette, Edward, Hermann,
Karl Moritz, Kocisk´y, Tom´as, and Blunsom, Phil. Rea-
soning about entailment with neural attention. In ICLR,
2016.

Rohrbach, M., Qiu, W., Titov, I., Thater, S., Pinkal, M., and
Schiele, B. Translating video content to natural language
descriptions. In ICCV, pp. 433–440, Dec 2013.

Sharma, Shikhar, Kiros, Ryan, and Salakhutdinov, Rus-
lan. Action recognition using visual attention. CoRR,
abs/1511.04119, 2015.

Shin, Andrew, Ohnishi, Katsunori, and Harada, Tatsuya.
Beyond caption to narrative: Video captioning with mul-
tiple sentences. ICIP, 2016.

Sigurdsson, Gunnar A., Varol, G¨ul, Wang, Xiaolong,
Farhadi, Ali, Laptev, Ivan, and Gupta, Abhinav. Hol-
lywood in homes: Crowdsourcing data collection for ac-
tivity understanding. In ECCV, 2016.

Yao, Li, Torabi, Atousa, Cho, Kyunghyun, Ballas, Nico-
las, Pal, Christopher, Larochelle, Hugo, and Courville,
Aaron. Describing videos by exploiting temporal struc-
ture. In ICCV, 2015.

Yu, Haonan, Wang, Jiang, Huang, Zhiheng, Yang, Yi, and
Xu, Wei. Video paragraph captioning using hierarchical
recurrent neural networks. In CVPR, June 2016.

Simonyan, K. and Zisserman, A. Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014.

Sukhbaatar, Sainbayar, Szlam, Arthur, Weston, Jason, and
Fergus, Rob. End-to-end memory networks. NIPS, 2015.

Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence
to sequence learning with neural networks. In NIPS, pp.
3104–3112. 2014.

Tran, Du, Bourdev, Lubomir, Fergus, Rob, Torresani,
Lorenzo, and Paluri, Manohar. Learning spatiotempo-
In ICCV,
ral features with 3d convolutional networks.
2015.

Venugopalan, Subhashini, Rohrbach, Marcus, Donahue,
Jeff, Mooney, Raymond, Darrell, Trevor, and Saenko,
Kate. Sequence to sequence – video to text. In ICCV,
2015a.

Venugopalan, Subhashini, Xu, Huijuan, Donahue, Jeff,
Rohrbach, Marcus, Mooney, Raymond, and Saenko,
Kate. Translating videos to natural language using deep
recurrent neural networks. In NAACL HLT, 2015b.

Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Er-
han, Dumitru. Show and tell: A neural image caption
generator. In CVPR, June 2015.

Weston, Jason, Chopra, Sumit, and Bordes, Antoine. Mem-

ory networks. CoRR, abs/1410.3916, 2014.

Xu, Kelvin, Ba, Jimmy, Kiros, Ryan, Cho, Kyunghyun,
Courville, Aaron, Salakhudinov, Ruslan, Zemel, Rich,
and Bengio, Yoshua. Show, attend and tell: Neural im-
age caption generation with visual attention. In ICML-
15, pp. 2048–2057, 2015a.

Xu, Ran, Xiong, Caiming, Chen, Wei, and Corso, Jason J.
Jointly modeling deep video and compositional text to
bridge vision and language in a uniﬁed framework. In
AAAI, 2015b.

Yang, Zhilin, Yuan, Ye, Wu, Yuexin, Salakhutdinov, Rus-
lan, and Cohen, William W. Encode, review, and de-
code: Reviewer module for caption generation. CoRR,
abs/1605.07912, 2016.

9

