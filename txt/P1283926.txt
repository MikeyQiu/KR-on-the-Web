Decoupled Parallel Backpropagation with Convergence Guarantee

Zhouyuan Huo 1 Bin Gu 1 Qian Yang 1 Heng Huang 1

8
1
0
2
 
l
u
J
 
1
2
 
 
]

G
L
.
s
c
[
 
 
3
v
4
7
5
0
1
.
4
0
8
1
:
v
i
X
r
a

Abstract
Backpropagation algorithm is indispensable for
the training of feedforward neural networks. It
requires propagating error gradients sequentially
from the output layer all the way back to the input
layer. The backward locking in backpropagation
algorithm constrains us from updating network
layers in parallel and fully leveraging the com-
puting resources. Recently, several algorithms
have been proposed for breaking the backward
locking. However, their performances degrade se-
riously when networks are deep. In this paper, we
propose decoupled parallel backpropagation algo-
rithm for deep learning optimization with conver-
gence guarantee. Firstly, we decouple the back-
propagation algorithm using delayed gradients,
and show that the backward locking is removed
when we split the networks into multiple mod-
ules. Then, we utilize decoupled parallel back-
propagation in two stochastic methods and prove
that our method guarantees convergence to crit-
ical points for the non-convex problem. Finally,
we perform experiments for training deep convo-
lutional neural networks on benchmark datasets.
The experimental results not only conﬁrm our the-
oretical analysis, but also demonstrate that the
proposed method can achieve signiﬁcant speedup
without loss of accuracy. Code is available at
https://github.com/slowbull/DDG.

1. Introduction

We have witnessed a series of breakthroughs in computer vi-
sion using deep convolutional neural networks (LeCun et al.,
2015). Most neural networks are trained using stochastic
gradient descent (SGD) or its variants in which the gra-
dients of the networks are computed by backpropagation
algorithm (Rumelhart et al., 1988). As shown in Figure 1,

1Department of Electrical and Computer Engineering, Univer-
sity of Pittsburgh, Pittsburgh, United States. Correspondence to:
Heng Huang <heng.huang@pitt.edu>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

Figure 1. We split a multilayer feedforward neural network into
three modules. Each module is a stack of layers. Backpropagation
algorithm requires running forward pass (from 1 to 3) and back-
ward pass (from 4 to 6) in sequential order. For example, module
A cannot perform step 6 before receiving δt
A which is an output of
step 5 in module B.

the backpropagation algorithm consists of two processes, the
forward pass to compute prediction and the backward pass
to compute gradient and update the model. After computing
prediction in the forward pass, backpropagation algorithm
requires propagating error gradients from the top (output
layer) all the way back to the bottom (input layer). There-
fore, in the backward pass, all layers, or more generally,
modules, of the network are locked until their dependencies
have executed.

The backward locking constrains us from updating models
in parallel and fully leveraging the computing resources.
It has been shown in practice (Krizhevsky et al., 2012; Si-
monyan & Zisserman, 2014; Szegedy et al., 2015; He et al.,
2016; Huang et al., 2016) and in theory (Eldan & Shamir,
2016; Telgarsky, 2016; Bengio et al., 2009) that depth is
one of the most critical factors contributing to the success
of deep learning. From AlexNet with 8 layers (Krizhevsky
et al., 2012) to ResNet-101 with more than one hundred
layers (He et al., 2016), the forward and backward time
grow from (4.31ms and 9.58ms) to (53.38ms and 103.06ms)
when we train the networks on Titan X with the input size of
16×3×224×224 (Johnson, 2017). Therefore, parallelizing
the backward pass can greatly reduce the training time when

Decoupled Parallel Backpropagation with Convergence Guarantee

the backward time is about twice of the forward time. We
can easily split a deep neural network into modules like Fig-
ure 1 and distribute them across multiple GPUs. However,
because of the backward locking, all GPUs are idle before
receiving error gradients from dependent modules in the
backward pass.

There have been several algorithms proposed for breaking
the backward locking. For example, (Jaderberg et al., 2016;
Czarnecki et al., 2017) proposed to remove the lockings in
backpropagation by employing additional neural networks
to approximate error gradients. In the backward pass, all
modules use the synthetic gradients to update weights of
the model without incurring any delay. (Nøkland, 2016;
Balduzzi et al., 2015) broke the local dependencies between
successive layers and made all hidden layers receive error
information from the output layer directly. In (Carreira-
Perpinan & Wang, 2014; Taylor et al., 2016), the authors
loosened the exact connections between layers by introduc-
ing auxiliary variables. In each layer, they imposed equality
constraint between the auxiliary variable and activation,
and optimized the new problem using Alternating Direction
Method which is easy to parallel. However, for the con-
volutional neural network, the performances of all above
methods are much worse than backpropagation algorithm
when the network is deep.

In this paper, we focus on breaking the backward locking in
backpropagtion algorithm for training feedforward neural
networks, such that we can update models in parallel without
loss of accuracy. The main contributions of our work are as
follows:

• Firstly, we decouple the backpropagation using delayed
gradients in Section 3 such that all modules of the
network can be updated in parallel without backward
locking.

• Then, we propose two stochastic algorithms using de-
coupled parallel backpropagation in Section 3 for deep
learning optimization.

• We also provide convergence analysis for the proposed
method in Section 4 and prove that it guarantees con-
vergence to critical points for the non-convex problem.

• Finally, we perform experiments for training deep con-
volutional neural networks in Section 5, experimental
results verifying that the proposed method can signiﬁ-
cantly speed up the training without loss of accuracy.

L layers, each layer taking an input hl−1 and producing
an activation hl = Fl(hl−1; wl) with weight wl. Letting
d be the dimension of weights in the network, we have
w = [w1, w2, ..., wL] ∈ Rd. Thus, the output of the network
can be represented as hL = F (h0; w), where h0 denotes
the input data x. Taking a loss function f and targets y, the
training problem is as follows:

min
w=[w1,...,wL]

f (F (x; w), y).

(1)

In the following context, we use f (w) for simplicity.

Gradients based methods are widely used for deep learning
optimization (Robbins & Monro, 1951; Qian, 1999; Hinton
et al., 2012; Kingma & Ba, 2014). In iteration t, we put a
data sample xi(t) into the network, where i(t) denotes the in-
dex of the sample. According to stochastic gradient descent
(SGD), we update the weights of the network through:
(cid:2)∇fl,xi(t) (wt)(cid:3)

, ∀l ∈ {1, 2, ..., L}

l = wt

l − γt

wt+1

(2)

l

where γt is the stepsize and ∇fl,xi(t)(wt) ∈ Rd is the gra-
dient of the loss function (1) with respect to the weights at
layer l and data sample xi(t), all the coordinates in other
than layer l are 0. We always utilize backpropagation al-
gorithm to compute the gradients (Rumelhart et al., 1988).
The backpropagation algorithm consists of two passes of
the network: in the forward pass, the activations of all layers
are calculated from l = 1 to L as follows:

l = Fl(ht
ht

l−1; wl);

in the backward pass, we apply chain rule for gradients and
repeatedly propagate error gradients through the network
from the output layer l = L to the input layer l = 1:

∂f (wt)
∂wt
l
∂f (wt)
∂ht

l−1

=

=

∂ht
l
∂wt
l
∂ht
l
∂ht

,

∂f (wt)
∂ht
l
∂f (wt)
∂ht
l

l−1

,

∂wt
l

where we let ∇fl,xi(t) (wt) = ∂f (wt)
. From equations (4)
and (5), it is obvious that the computation in layer l is depen-
dent on the error gradient ∂f (wt)
from layer l+1. Therefore,
∂ht
l
the backward locking constrains all layers from updating
before receiving error gradients from the dependent layers.
When the network is very deep or distributed across multiple
resources, the backward locking is the main bottleneck in
the training process.

(3)

(4)

(5)

2. Backgrounds

3. Decoupled Parallel Backpropagation

We begin with a brief overview of the backpropagation
algorithm for the optimization of neural networks. Suppose
that we want to train a feedforward neural network with

In this section, we propose to decouple the backpropagation
algorithm using delayed gradients (DDG). Suppose we split
a L-layer feedforward neural network to K modules, such

Decoupled Parallel Backpropagation with Convergence Guarantee

Figure 2. We split a multilayer feedforward neural network into three modules (A, B and C), where each module is a stack of layers. After
executing the forward pass (from 1 to 3) to predict, our proposed method allows all modules to run backward pass (4) using delayed
gradients without locking. Particularly, module A can perform the backward pass using the stale error gradient δt−2
A . Meanwhile, It also
receives δt−1

A from module B for the update of the next iteration.

that the weights of the network are divided into K groups.
Therefore, we have w = [wG(1), wG(2), ..., wG(K)] where
G(k) denotes layer indices in the group k.

Table 1. Comparisons of computation time when the network is
sequentially distributed across K GPUs. TF and TB denote the
forward and backward time for backpropagation algorithm.

3.1. Backpropagation Using Delayed Gradients

In iteration t, data sample xi(t) is input to the network. We
run the forward pass from module k = 1 to k = K. In each
module, we compute the activations in sequential order as
equation (3). In the backward pass, all modules except the
last one have delayed error gradients in store such that they
can execute the backward computation without locking. The
last module updates with the up-to-date gradients. In par-
ticular, module k keeps the stale error gradient ∂f (wt−K+k)
,
∂ht−K+k
Lk
where Lk denotes the last layer in module k. Therefore, the
backward computation in module k is as follows:

∂f (wt−K+k)
∂wt−K+k
l
∂f (wt−K+k)
∂ht−K+k
l−1

=

=

l

∂ht−K+k
∂wt−K+k
l
∂ht−K+k
∂ht−K+k
l−1

l

,

∂f (wt−K+k)
∂ht−K+k
l
∂f (wt−K+k)
∂ht−K+k

.

l

(6)

(7)

where (cid:96) ∈ G(k). Meanwhile, each module also receives
error gradient from the dependent module for further com-
putation. From (6) and (7), we can know that the stale error
gradients in all modules are of different time delay. From
module k = 1 to k = K, their corresponding time delays
are from K − 1 to 0. Delay 0 indicates that the gradients
are up-to-date. In this way, we break the backward locking
and achieve parallel update in the backward pass. Figure 2
shows an example of the decoupled backpropagation, where
error gradients δ := ∂f (w)
∂h .

3.2. Speedup of Decoupled Parallel Backpropagation

When K = 1, there is no time delay and the proposed
method is equivalent to the backpropagation algorithm.
When K (cid:54)= 1, we can distribute the network across multiple

Method
Backpropagation
DDG

Computation Time
TF + TB
TF + TB
K

GPUs and fully leverage the computing resources. Table 1
lists the computation time when we sequentially allocate the
network across K GPUs. When TF is necessary to compute
accurate predictions, we can accelerate the training by re-
ducing the backward time. Because TB is much large than
TF , we can achieve huge speedup even K is small.

Relation to model parallelism: Model parallelism usually
refers to ﬁlter-wise parallelism (Yadan et al., 2013). For ex-
ample, we split a convolutional layer with N ﬁlters into two
GPUs, each part containing N
2 ﬁlters. Although the ﬁlter-
wise parallelism accelerates the training when we distribute
the workloads across multiple GPUs, it still suffers from
the backward locking. We can think of DDG algorithm as
layer-wise parallelism. It is also easy to combine ﬁlter-wise
parallelism with layer-wise parallelism for further speedup.

3.3. Stochastic Methods Using Delayed Gradients

After computing the gradients of the loss function with re-
spect to the weights of the model, we update the model using
(cid:0)wt−K+k(cid:1) :=
delayed gradients. Letting ∇fG(k),xi(t−K+k)

(cid:80)

l∈G(k)

∂f (wt−K+k)
∂wt−K+k

l






0

otherwise

if t − K + k ≥ 0

,

(8)

for any k ∈ {1, 2, ..., K}, we update the weights in module
k following SGD:

wt+1
G(k) = wt

G(k) − γt[∇fG(k),xi(t−K+k)

(cid:0)wt−K+k(cid:1)]G(k).

(9)

Decoupled Parallel Backpropagation with Convergence Guarantee

Algorithm 1 SGD-DDG

Require:

G(1), ..., w0

Initial weights w0 = [w0
Stepsize sequence {γt};
1: for t = 0, 1, 2, . . . , T − 1 do
2:
3:

for k = 1, . . . , K in parallel do
Compute delayed gradient:
(cid:104)

gt
k ←

∇fG(k),xi(t−K+k)

4:

Update weights:

wt+1

G(k) ← wt

G(k) − γt · gt
k;

end for

5:
6: end for

G(K)] ∈ Rd;

(cid:0)wt−K+k(cid:1)(cid:105)

;

G(k)

where γt denotes stepsize. Different from SGD, we update
the weights with delayed gradients. Besides, the delayed
iteration (t − K + k) for group k is also deterministic. We
summarize the proposed method in Algorithm 1.

Moreover, we can also apply the delayed gradients to other
variants of SGD, for example Adam in Algorithm 2. In
each iteration, we update the weights and moment vectors
with delayed gradients. We analyze the convergence for
Algorithm 1 in Section 4, which is the basis of analysis for
other methods.

4. Convergence Analysis

In this section, we establish the convergence guarantees to
critical points for Algorithm 1 when the problem is non-
convex. Analysis shows that our method admits similar con-
vergence rate to vanilla stochastic gradient descent (Bottou
et al., 2016). Throughout this paper, we make the following
commonly used assumptions:

Assumption 1 (Lipschitz-continuous gradient) The gradi-
ent of f (w) is Lipschitz continuous with Lipschitz constant
L > 0, such that ∀w, v ∈ Rd:

(cid:107)∇f (w) − ∇f (v)(cid:107)2 ≤ L(cid:107)w − v(cid:107)2

(10)

Assumption 2 (Bounded variance) To bound the variance
of the stochastic gradient, we assume the second moment
of the stochastic gradient is upper bounded, such that there
exists constant M ≥ 0, for any sample xi and ∀w ∈ Rd:

(cid:107)∇fxi(w)(cid:107)2

2 ≤ M

(11)

Because of the unnoised stochastic gradient E [∇fxi (w)] =
variance
equation
and
∇f (w)
E (cid:107)∇fxi (w) − ∇f (w)(cid:107)2
2 = E(cid:107)∇fxi(w)(cid:107)2
2 − (cid:107)∇f (w)(cid:107)2
2,
the variance of the stochastic gradient is guaranteed to be
less than M .

regarding

the

Algorithm 2 Adam-DDG

Require:

G(K)] ∈ Rd;

G(1), ..., w0

Initial weights: w0 = [w0
Stepsize: γ; Constant (cid:15) = 10−8;
Exponential decay rates: β1 = 0.9 and β2 = 0.999 ;
First moment vector: m0
G(k) ← 0, ∀k ∈ {1, 2, ..., K};
Second moment vector: v0
G(k) ← 0, ∀k ∈ {1, 2, ..., K};
1: for t = 0, 1, 2, . . . , T − 1 do
2:
3:

for k = 1, . . . , K in parallel do
Compute delayed gradient:
(cid:104)

gt
k ←

∇fG(k),xi(t−K+k)

(cid:0)wt−K+k(cid:1)(cid:105)

;

G(k)

4:

5:

6:

7:

8:

Update biased ﬁrst moment estimate:
G(k) ← β1 · mt

G(k) + (1 − β1) · gt
k

mt+1

Update biased second moment estimate:

vt+1
G(k) ← β2 · vt

k)2
Compute bias-correct ﬁrst moment estimate:

G(k) + (1 − β2) · (gt

ˆmt+1

G(k) ← mt+1

G(k)/(1 − βt+1

1

)

Compute bias-correct second moment estimate:
G(k)/(1 − βt+1

)

2

G(k) ← vt+1
ˆvt+1
Update weights:
wt+1

G(k) ← wt

G(k) −γ · ˆmt+1

G(k)/

(cid:16)(cid:113)

(cid:17)
ˆvt+1
G(k) + (cid:15)

end for

9:
10: end for

Under Assumption 1 and 2, we obtain the following lemma
about the sequence of objective functions.

Lemma 1 Assume Assumption 1 and 2 hold. In addition,
and MK = KM +σK 4M .
we let σ := maxt
The iterations in Algorithm 1 satisfy the following inequality,
for all t ∈ N:

γmax{0,t−K+1}
γt

E (cid:2)f (wt+1)(cid:3) − f (wt) ≤ −

(cid:13)∇f (wt)(cid:13)
(cid:13)
2
2 + γ2
(cid:13)

t LMK (12)

γt
2

From Lemma 1, we can observe that the expected decrease
of the objective function is controlled by the stepsize γt
and MK. Therefore, we can guarantee that the values of
objective functions are decreasing as long as the stepsizes
γt are small enough such that the right-hand side of (12) is
less than zero. Using the lemma above, we can analyze the
convergence property for Algorithm 1.

4.1. Fixed Stepsize γt

Firstly, we analyze the convergence for Algorithm 1 when
γt is ﬁxed and prove that the learned model will converge
sub-linearly to the neighborhood of the critical points.

Theorem 1 Assume Assumption 1 and 2 hold and the ﬁxed
stepsize sequence {γt} satisﬁes γt = γ and γL ≤ 1, ∀t ∈
In addition, we assume w∗ to be the
{0, 1, ..., T − 1}.

Decoupled Parallel Backpropagation with Convergence Guarantee

Figure 3. Training and testing curves regarding epochs for ResNet-8 on CIFAR-10. Upper: Loss function values regarding epochs;
Bottom: Top 1 classiﬁcation accuracies regarding epochs. We split the network into two modules such that there is only one split point in
the network for DNI and DDG.

optimal solution to f (w) and let σ = 1 such that MK =
KM + K 4M . Then, the output of Algorithm 1 satisﬁes
that:

1
T

T −1
(cid:88)

t=0

E (cid:13)

(cid:13)∇f (wt)(cid:13)
2
(cid:13)
2

≤

2 (cid:0)f (w0) − f (w∗)(cid:1)
γT

+ 2γLMK (13)

In Theorem 1, we can observe that when T → ∞, the
average norm of the gradients is upper bounded by 2γLMK.
The number of modules K affects the value of the upper
bound. Selecting a small stepsize γ allows us to get better
neighborhood to the critical points, however it also seriously
decreases the speed of convergence.

4.2. Diminishing Stepsize γt

In this section, we prove that Algorithm 1 with diminishing
stepsizes can guarantee the convergence to critical points
for the non-convex problem.

Theorem 2 Assume Assumption 1 and 2 hold and the di-
minishing stepsize sequence {γt} satisﬁes γt = γ0
1+t and
γtL ≤ 1, ∀t ∈ {0, 1, ..., T − 1}. In addition, we assume
w∗ to be the optimal solution to f (w) and let σ = K such

that MK = KM + K 5M . Setting ΓT =

γt, then the

T −1
(cid:80)
t=0

output of Algorithm 1 satisﬁes that:

1
ΓT

T −1
(cid:88)

t=0

γtE (cid:13)

(cid:13)∇f (wt)(cid:13)
2
2 ≤
(cid:13)

2 (cid:0)f (w0) − f (w∗)(cid:1)
ΓT

γ2
t LMK

2

T −1
(cid:80)
t=0

+

ΓT

(14)

Corollary 1 Since γt = γ0
(Robbins & Monro, 1951) are satisﬁed that:

t+1 , the stepsize requirements in

lim
T →∞

T −1
(cid:88)

t=0

γt = ∞ and

γ2
t < ∞.

(15)

lim
T →∞

T −1
(cid:88)

t=0

Therefore, according to Theorem 2, when T → ∞, the
right-hand side of (14) converges to 0.

Corollary 2 Suppose ws
is chosen randomly from
t=0 with probabilities proportional to {γt}T −1
{wt}T −1
t=0 . Ac-
cording to Theorem 2, we can prove that Algorithm 1 guar-
antees convergence to critical points for the non-convex
problem:

lim
s→∞

E(cid:107)∇f (ws)(cid:107)2

2 = 0

(16)

5. Experiments

In this section, we experiment with ResNet (He et al., 2016)
on image classiﬁcation benchmark datasets: CIFAR-10 and
CIFAR-100 (Krizhevsky & Hinton, 2009). In section 5.1,
we evaluate our method by varying the positions and the
number of the split points in the network; In section 5.2
we use our method to optimize deeper neural networks and
show that its performance is as good as the performance of
backpropagation; ﬁnally, we split and distribute the ResNet-
110 across GPUs in Section 5.3, results showing that the
proposed method achieves a speedup of two times without
loss of accuracy.

Decoupled Parallel Backpropagation with Convergence Guarantee

Figure 4. Training and testing curves regarding epochs for ResNet-8 on CIFAR-10. Upper: Loss function values regarding epochs;
Bottom: Top1 classiﬁcation accuracies regarding epochs. For DNI and DDG, the number of split points in the network ranges from 2 to 4.

Implementation Details: We implement DDG algorithm
using PyTorch library (Paszke et al., 2017). The trained
network is split into K modules where each module is run-
ning on a subprocess. The subprocesses are spawned using
multiprocessing package 1 such that we can fully leverage
multiple processors on a given machine. Running modules
on different subprocesses make the communication very
difﬁcult. To make the communication fast, we utilize the
shared memory objects in the multiprocessing package. As
in Figure 2, every two adjacent modules share a pair of
activation (h) and error gradient (δ).

5.1. Comparison of BP, DNI and DDG

In this section, we train ResNet-8 on CIFAR-10 on a single
Titan X GPU. The architecture of the ResNet-8 is in Table 2.
All experiments are run for 300 epochs and optimized using
Adam optimizer (Kingma & Ba, 2014) with a batch size of
128. The stepsize is initialized at 1 × 10−3. We augment the
dataset with random cropping, random horizontal ﬂipping
and normalize the image using mean and standard deviation.
There are three compared methods in this experiment:

• BP: Adam optimizer in Pytorch uses backpropagation
algorithm with data parallelism (Rumelhart et al., 1988)
to compute gradients.

1https://docs.python.org/3/library/multiprocessing.html#module-

multiprocessing

Table 2. Architectural details. Units denotes the number of resid-
ual units in each group. Each unit is a basic residual block without
bottleneck. Channels indicates the number of ﬁlters used in each
unit in each group.

Architecture
ResNet-8
ResNet-56
ResNet-110

Units
1-1-1
9-9-9
18-18-18

Channels
16-16-32-64
16-16-32-64
16-16-32-64

• DNI: Decoupled neural interface (DNI) in (Jaderberg
et al., 2016). Following (Jaderberg et al., 2016), the
synthetic network is a stack of three convolutional
layers with L 5 × 5 ﬁlters with resolution preserving
padding. The ﬁlter depth L is determined by the posi-
tion of DNI. We also input label information into the
synthetic network to increase ﬁnal accuracy.

• DDG: Adam optimizer using delayed gradients in Al-

gorithm 2.

Impact of split position (depth). The position (depth) of
the split points determines the number of layers using de-
layed gradients. Stale or synthetic gradients will induce
noises in the training process, affecting the convergence of
the objective. Figure 3 exhibits the experimental results
when there is only one split point with varying positions. In
the ﬁrst column, we know that all compared methods have

Decoupled Parallel Backpropagation with Convergence Guarantee

(a)

(b)

Figure 5. Training and testing loss curves for ResNet-110 on CIFAR-10 using multiple GPUs. (5a) Loss function value regarding epochs.
(5b) Loss function value regarding computation time.

Table 3. The best Top 1 classiﬁcation accuracy (%) for ResNet-56
and ResNet-110 on the test data of CIFAR-10 and CIFAR-100.

Architecture

ResNet-56
ResNet-110

CIFAR-10
BP
93.12
93.53

DDG
93.11
93.41

CIFAR-100
DDG
BP
70.17
69.79
71.90
71.39

5.2. Optimizing Deeper Neural Networks

In this section, we employ DDG to optimize two very deep
neural networks (ResNet-56 and ResNet-110) on CIFAR-10
and CIFAR-100. Each network is split into two modules
at the center. We use SGD with the momentum of 0.9 and
the stepsize is initialized to 0.01. Each model is trained for
300 epochs and the stepsize is divided by a factor of 10 at
150 and 225 epochs. The weight decay constant is set to
5 × 10−4. We perform the same data augmentation as in
section 5.1. Experiments are run on a single Titan X GPU.

Figure 7 presents the experimental results of BP and DDG.
We do not compare DNI because its performance is far
worse when models are deep. Figures in the ﬁrst column
present the convergence of loss regarding epochs, showing
that DDG and BP admit similar convergence rates. We can
also observe that DDG converges faster when we compare
the loss regarding computation time in the second column
of Figure 7. In the experiment, the “Volatile GPU Utility” is
about 70% when we train the models with BP. Our method
runs on two subprocesses such that it fully leverages the
computing capacity of the GPU. We can draw similar con-
clusions when we compare the Top 1 accuracy in the third
and fourth columns of Figure 7. In Table 3, we list the best
Top 1 accuracy on the test data of CIFAR-10 and CIFAR-
100. We can observe that DDG can obtain comparable or

Figure 6. Computation time and the best Top 1 accuracy for
ResNet-110 on the test data of CIFAR-10. The most left bar
denotes the computation time using backpropagation algorithm
on a GPU, where the forward time accounts for about 32%. We
normalize the computation time of all optimization settings using
the amount of time required by backpropagation.

similar performances when the split point is at layer 1. DDG
performs consistently well when we place the split point at
deeper positions 3, 5 or 7. On the contrary, the performance
of DNI degrades as we vary the positions and it cannot even
converge when the split point is at layer 7.

Impact of the number of split points. From equation (7),
we know that the maximum time delay is determined by
the number of modules K. Theorem 2 also shows that K
affects the convergence rate. In this experiment, we vary the
number of split points in the network from 2 to 4 and plot the
results in Figure 4. It is easy to observe that DDG performs
as well as BP, regardless of the number of split points in
the network. However, DNI is very unstable when we place
more split points, and cannot even converge sometimes.

Decoupled Parallel Backpropagation with Convergence Guarantee

Figure 7. Training and testing curves for ResNet-56 and ResNet-110 on CIFAR-10 and CIFAR-100. Column 1 and 2 present the loss
function value regrading epochs and computation time respectively; Column 3 and 4 present the Top 1 classiﬁcation accuracy regrading
epochs and computation time. For DDG, there is only one split point at the center of the network.

better accuracy even when the network is deep.

5.3. Scaling the Number of GPUs

In this section, we split ResNet-110 into K modules and
allocate them across K Titan X GPUs sequentially. We do
not consider ﬁlter-wise model parallelism in this experiment.
The selections of the parameters in the experiment are sim-
ilar to Section 5.2. From Figure 5, we know that training
networks in multiple GPUs does not affect the convergence
rate. For comparison, we also count the computation time of
backpropagation algorithm on a single GPU. The computa-
tion time is worse when we run backpropagation algorithm
on multiple GPUs because of the communication overhead.
In Figure 6, we can observe that forward time only accounts
for about 32% of the total computation time for backpropa-

gation algorithm. Therefore, backward locking is the main
bottleneck. In Figure 6, it is obvious that when we increase
the number of GPUs from 2 to 4, our method reduces about
30% to 50% of the total computation time. In other words,
DDG achieves a speedup of about 2 times without loss of
accuracy when we train the networks across 4 GPUs.

6. Conclusion

In this paper, we propose decoupled parallel backpropa-
gation algorithm, which breaks the backward locking in
backpropagation algorithm using delayed gradients. We
then apply the decoupled parallel backpropagation to two
stochastic methods for deep learning optimization. In the
theoretical section, we also provide convergence analysis

Decoupled Parallel Backpropagation with Convergence Guarantee

and prove that the proposed method guarantees convergence
to critical points for the non-convex problem. Finally, we
perform experiments on deep convolutional neural networks,
results verifying that our method can accelerate the training
signiﬁcantly without loss of accuracy.

Acknowledgement

This work was partially supported by U.S. NIH R01
AG049371, NSF IIS 1302675, IIS 1344152, DBI 1356628,
IIS 1619308, IIS 1633753.

References

Balduzzi, D., Vanchinathan, H., and Buhmann, J. M. Kick-
back cuts backprop’s red-tape: Biologically plausible
credit assignment in neural networks. In AAAI, pp. 485–
491, 2015.

Bengio, Y. et al. Learning deep architectures for ai. Foun-
dations and trends R(cid:13) in Machine Learning, 2(1):1–127,
2009.

Bottou, L., Curtis, F. E., and Nocedal, J. Optimization
methods for large-scale machine learning. arXiv preprint
arXiv:1606.04838, 2016.

Carreira-Perpinan, M. and Wang, W. Distributed optimiza-
tion of deeply nested systems. In Artiﬁcial Intelligence
and Statistics, pp. 10–19, 2014.

Czarnecki, W. M., ´Swirszcz, G., Jaderberg, M., Osindero,
S., Vinyals, O., and Kavukcuoglu, K. Understanding
synthetic gradients and decoupled neural interfaces. arXiv
preprint arXiv:1703.00522, 2017.

Eldan, R. and Shamir, O. The power of depth for feedfor-
ward neural networks. In Conference on Learning Theory,
pp. 907–940, 2016.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition,
pp. 770–778, 2016.

Hinton, G., Srivastava, N., and Swersky, K. Neural networks
for machine learning-lecture 6a-overview of mini-batch
gradient descent, 2012.

Huang, G., Liu, Z., Weinberger, K. Q., and van der Maaten,
L. Densely connected convolutional networks. arXiv
preprint arXiv:1608.06993, 2016.

Jaderberg, M., Czarnecki, W. M., Osindero, S., Vinyals,
O., Graves, A., and Kavukcuoglu, K. Decoupled neu-
ral interfaces using synthetic gradients. arXiv preprint
arXiv:1608.05343, 2016.

Johnson, J. Benchmarks for popular cnn models. https:
//github.com/jcjohnson/cnn-benchmarks,
2017.

Kingma, D. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.

Krizhevsky, A. and Hinton, G. Learning multiple layers of

features from tiny images. 2009.

Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
classiﬁcation with deep convolutional neural networks.
In Advances in neural information processing systems,
pp. 1097–1105, 2012.

LeCun, Y., Bengio, Y., and Hinton, G. Deep learning. Na-

ture, 521(7553):436–444, 2015.

Nøkland, A. Direct feedback alignment provides learning in
deep neural networks. In Advances in Neural Information
Processing Systems, pp. 1037–1045, 2016.

Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,
DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,
A. Automatic differentiation in pytorch. 2017.

Qian, N. On the momentum term in gradient descent learn-
ing algorithms. Neural networks, 12(1):145–151, 1999.

Robbins, H. and Monro, S. A stochastic approximation
method. The annals of mathematical statistics, pp. 400–
407, 1951.

Rumelhart, D. E., Hinton, G. E., Williams, R. J., et al. Learn-
ing representations by back-propagating errors. Cognitive
modeling, 5(3):1, 1988.

Simonyan, K. and Zisserman, A. Very deep convolu-
tional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich,
In Proceedings
A. Going deeper with convolutions.
of the IEEE conference on computer vision and pattern
recognition, pp. 1–9, 2015.

Taylor, G., Burmeister, R., Xu, Z., Singh, B., Patel, A., and
Goldstein, T. Training neural networks without gradients:
A scalable admm approach. In International Conference
on Machine Learning, pp. 2722–2731, 2016.

Telgarsky, M. Beneﬁts of depth in neural networks. arXiv

preprint arXiv:1602.04485, 2016.

Yadan, O., Adams, K., Taigman, Y., and Ranzato, M. Multi-
gpu training of convnets. arXiv preprint arXiv:1312.5853,
2013.

Supplementary Materials for Paper “Decoupled Parallel Backpropagation with
Convergence Guarantee”

A. Proof to Lemma 1

Proof: Because the gradient of f (w) is Lipschitz continuous in Assumption 1, the following inequality holds that:

f (wt+1) ≤ f (wt) + ∇f (wt)T (cid:0)wt+1 − wt(cid:1) +

L
2

(cid:13)wt+1 − wt(cid:13)
(cid:13)
2
2 .
(cid:13)

(17)

From the update rule in Algorithm 1, we take expectation on both sides and obtain:

E (cid:2)f (wt+1)(cid:3) ≤ f (wt) − γtE

(cid:34)
∇f (wt)T

(cid:32) K
(cid:88)

k=1

∇fG(k),xi(t−K+k)

(cid:0)wt−K+k(cid:1)

(cid:33)(cid:35)

+

Lγ2
t
2

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

k=1

(cid:13)
2
(cid:13)
∇fG(k),xi(t−K+k)(wt−K+k)
(cid:13)
(cid:13)
(cid:13)
2

K
(cid:88)

K
(cid:88)

k=1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k=1

≤ f (wt) − γt

∇f (wt)T (cid:0)∇fG(k)

(cid:0)wt−K+k(cid:1) + ∇fG(k)

(cid:0)wt(cid:1) − ∇fG(k)

(cid:0)wt(cid:1)(cid:1)

K
(cid:88)

k=1

+

Lγ2
t
2

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

k=1

(cid:13)
2
(cid:13)
∇fG(k),xi(t−K+k)(wt−K+k) − ∇f (wt) + ∇f (wt)
(cid:13)
(cid:13)
(cid:13)
2

= f (wt) − γt

(cid:13)∇f (wt)(cid:13)
(cid:13)
2
2 − γt
(cid:13)

∇f (wt)T (cid:0)∇fG(k)

(cid:0)wt−K+k(cid:1) − ∇fG(k)

(cid:0)wt(cid:1)(cid:1)

(cid:13)∇f (wt)(cid:13)
(cid:13)
2
2 +
(cid:13)

Lγ2
t
2

E

(cid:13)
2
(cid:13)
∇fG(k),xi(t−K+k)(wt−K+k) − ∇f (wt)
(cid:13)
(cid:13)
(cid:13)
2

∇f (wt)T (cid:0)∇fG(k)

(cid:0)wt−K+k(cid:1) − ∇fG(k)

(cid:0)wt(cid:1)(cid:1)

+

Lγ2
t
2

+Lγ2
t

K
(cid:88)

k=1

(cid:18)

= f (wt) −

γt −

(cid:19)

Lγ2
t
2

(cid:13)
(cid:13)∇f (wt)(cid:13)
2
2 +
(cid:13)

Lγ2
t
2

E

(cid:124)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

k=1

(cid:13)
2
(cid:13)
∇fG(k),xi(t−K+k)(wt−K+k) − ∇f (wt)
(cid:13)
(cid:13)
(cid:13)
(cid:125)

−(γt − Lγ2
t )

∇f (wt)T (cid:0)∇fG(k)

(cid:0)wt−K+k(cid:1) − ∇fG(k)

(cid:0)wt(cid:1)(cid:1)

,

(18)

K
(cid:88)

k=1

(cid:124)

(cid:123)(cid:122)
Q2

(cid:123)(cid:122)
Q1

(cid:125)

where the second inequality follows from the unbiased gradient E [∇fxi(w)] = ∇f (w). Because of (cid:107)x + y(cid:107)2

2 ≤ 2(cid:107)x(cid:107)2

2 +

Decoupled Parallel Backpropagation with Convergence Guarantee

2(cid:107)y(cid:107)2

2 and xy ≤ 1

2 + 1

2 (cid:107)y(cid:107)2

2, we have the upper bound of Q1 and Q2 as follows:

Q1 =

Lγ2
t
2

E

∇fG(k),xi(t−K+k) (wt−K+k) − ∇f (wt) −

∇fG(k)(wt−K+k) +

∇fG(k)(wt−K+k)

K
(cid:88)

k=1

K
(cid:88)

k=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

≤ Lγ2
t

E

∇fG(k),xi(t−K+k) (wt−K+k) −

∇fG(k)(wt−K+k)

+Lγ2
t

K
(cid:88)

2 (cid:107)x(cid:107)2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

k=1

k=1

(cid:124)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:125)

K
(cid:88)

k=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:124)

(cid:13)
2
(cid:13)
∇fG(k)(wt−K+k) − ∇f (wt)
(cid:13)
.
(19)
(cid:13)
(cid:13)
2
(cid:125)

(cid:123)(cid:122)
Q4

Q2 = −(γt − Lγ2
t )

∇f (wt)T (cid:0)∇fG(k)

(cid:0)wt−K+k(cid:1) − ∇fG(k)

(cid:0)wt(cid:1)(cid:1)

≤

γt − Lγ2
t
2

(cid:13)∇f (wt)(cid:13)
(cid:13)
2
2 +
(cid:13)

γt − Lγ2
t
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

k=1

∇fG(k)(wt−K+k) − ∇f (wt)

(20)

As per the equation regarding variance E(cid:107)ξ − E[ξ](cid:107)2

2 = E(cid:107)ξ(cid:107)2

2 − (cid:107)E[ξ](cid:107)2

2, we can bound Q3 as follows:

K
(cid:88)

k=1

(cid:123)(cid:122)
Q3

K
(cid:88)

k=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

.

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

Q3 = E

∇fG(k),xi(t−K+k) (wt−K+k) −

∇fG(k)(wt−K+k)

K
(cid:88)

k=1

E

(cid:13)
(cid:13)
2
(cid:13)∇fG(k),xi(t−K+k) (wt−K+k) − ∇fG(k)(wt−K+k)
(cid:13)
(cid:13)
(cid:13)
2

E

(cid:13)
(cid:13)
2
(cid:13)∇fG(k),xi(t−K+k) (wt−K+k)
(cid:13)
(cid:13)
(cid:13)
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

k=1

=

≤

K
(cid:88)

k=1

K
(cid:88)

k=1

≤ KM,

where the equality follows from the deﬁnition of ∇fG(k)(w) such that [∇fG(k)(w)]j = 0, ∀j /∈ G(k) and the last inequality
is from Assumption 2. We can also get the upper bound of Q4:

Q4 =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

k=1

(cid:13)
2
(cid:13)
∇fG(k)(wt−K+k) − ∇f (wt)
(cid:13)
(cid:13)
(cid:13)
2

(cid:13)∇fG(k)(wt−K+k) − ∇fG(k)(wt)(cid:13)
(cid:13)
2
(cid:13)
2

(cid:13)∇f (wt−K+k) − ∇f (wt)(cid:13)
(cid:13)
2
(cid:13)
2

K
(cid:88)

≤ L2

t−1
(cid:88)

(cid:0)wj+1 − wj(cid:1)

k=1

j=max{0,t−K+k}

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

≤

K
(cid:88)

k=1

K
(cid:88)

k=1

≤ L2γ2

max{0,t−K+1}K

∇fG(k),x(j)

(cid:0)wj−K+k(cid:1)

K
(cid:88)

t−1
(cid:88)

k=1

j=max{0,t−K+k}

K
(cid:88)

t−1
(cid:88)

K
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k=1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

k=1

j=max{0,t−K+k}

k=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

∇fG(k),x(j)

(cid:0)wj−K+k(cid:1)

≤ KLγt

γmax{0,t−K+1}
γt

≤ LγtσK 4M,

where the second inequality is from Assumption 1, the fourth inequality follows from that Lγt ≤ 1 and the last inequality
follows from (cid:107)z1 + ... + zr(cid:107)2
. Integrating the upper

2), Assumption 2 and σ := maxt

2 + ... + (cid:107)zr(cid:107)2

2 ≤ r((cid:107)z1(cid:107)2

γmax{0,t−K+1}
γt

(21)

(22)

Decoupled Parallel Backpropagation with Convergence Guarantee

bound of Q1, Q2, Q3 and Q4 in (18), we have:

E (cid:2)f (wt+1)(cid:3) − f (wt) ≤ −

γt
2

(cid:13)∇f (wt)(cid:13)
(cid:13)
2
2 + γ2
t L
(cid:13)

K
(cid:88)

k=1

E

(cid:13)
(cid:13)
2
(cid:13)∇fG(k),xi(t−K+k) (wt−K+k)
(cid:13)
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

γt + Lγ2
t
2
(cid:13)∇f (wt)(cid:13)
(cid:13)
2
(cid:13)

k=1

γt
2

≤ −

(cid:13)
2
(cid:13)
∇fG(k)(wt−K+k) − ∇f (wt)
(cid:13)
(cid:13)
(cid:13)
2

.

+ γ2

t LMK,

where we let MK = KM + σK 4M .

B. Proof to Theorem 1

Proof: When γt is constant and γt = γ, taking total expectation of (12) in Lemma 1, we obtain:

E (cid:2)f (wt+1)(cid:3) − E (cid:2)f (wt)(cid:3) ≤ −

E (cid:13)

(cid:13)∇f (wt)(cid:13)
2
2 + γ2LMK,
(cid:13)

where σ = 1 and MK = KM + K 4M . Summing (24) from t = 0 to T − 1, we have:

E (cid:2)f (wT )(cid:3) − f (w0) ≤ −

E (cid:13)

(cid:13)∇f (wt)(cid:13)
2
2 + T γ2LMK.
(cid:13)

γ
2

γ
2

T −1
(cid:88)

t=0

Suppose w∗ is the optimal solution for f (w), therefore f (w∗) − f (w0) ≤ E (cid:2)f (wT )(cid:3) − f (w0). Above all, the following
inequality is guaranteed that:

1
T

T −1
(cid:88)

t=0

E (cid:13)

(cid:13)∇f (wt)(cid:13)
2
2 ≤
(cid:13)

2 (cid:0)f (w0) − f (w∗)(cid:1)
γT

+ 2γLMK.

C. Proof to Theorem 2

Proof: {γt} is a diminishing sequence and γt = γ0
of (12) in Lemma 1 and summing it from t = 0 to T − 1, we obtain:

1+t , such that σ ≤ K and MK = KM + K 5M . Taking total expectation

E (cid:2)f (wT )(cid:3) − f (w0) ≤ −

γtE (cid:13)

(cid:13)∇f (wt)(cid:13)
2
2 +
(cid:13)

γ2
t LMK.

1
2

T −1
(cid:88)

t=0

T −1
(cid:88)

t=0

Suppose w∗ is the optimal solution for f (w), therefore f (w∗) − f (w0) ≤ E (cid:2)f (wT )(cid:3) − f (w0). Letting ΓT =
have:

1
ΓT

T −1
(cid:88)

t=0

γtE (cid:13)

(cid:13)∇f (wt)(cid:13)
2
2 ≤
(cid:13)

2 (cid:0)f (w0) − f (w∗)(cid:1)
ΓT

+

2

T −1
(cid:80)
t=0

γ2
t LMK

.

ΓT

We complete the proof.

(23)

(24)

(25)

(26)

(27)

T −1
(cid:80)
t=0

γt, we

(28)

Decoupled Parallel Backpropagation with Convergence Guarantee

Zhouyuan Huo 1 Bin Gu 1 Qian Yang 1 Heng Huang 1

8
1
0
2
 
l
u
J
 
1
2
 
 
]

G
L
.
s
c
[
 
 
3
v
4
7
5
0
1
.
4
0
8
1
:
v
i
X
r
a

Abstract
Backpropagation algorithm is indispensable for
the training of feedforward neural networks. It
requires propagating error gradients sequentially
from the output layer all the way back to the input
layer. The backward locking in backpropagation
algorithm constrains us from updating network
layers in parallel and fully leveraging the com-
puting resources. Recently, several algorithms
have been proposed for breaking the backward
locking. However, their performances degrade se-
riously when networks are deep. In this paper, we
propose decoupled parallel backpropagation algo-
rithm for deep learning optimization with conver-
gence guarantee. Firstly, we decouple the back-
propagation algorithm using delayed gradients,
and show that the backward locking is removed
when we split the networks into multiple mod-
ules. Then, we utilize decoupled parallel back-
propagation in two stochastic methods and prove
that our method guarantees convergence to crit-
ical points for the non-convex problem. Finally,
we perform experiments for training deep convo-
lutional neural networks on benchmark datasets.
The experimental results not only conﬁrm our the-
oretical analysis, but also demonstrate that the
proposed method can achieve signiﬁcant speedup
without loss of accuracy. Code is available at
https://github.com/slowbull/DDG.

1. Introduction

We have witnessed a series of breakthroughs in computer vi-
sion using deep convolutional neural networks (LeCun et al.,
2015). Most neural networks are trained using stochastic
gradient descent (SGD) or its variants in which the gra-
dients of the networks are computed by backpropagation
algorithm (Rumelhart et al., 1988). As shown in Figure 1,

1Department of Electrical and Computer Engineering, Univer-
sity of Pittsburgh, Pittsburgh, United States. Correspondence to:
Heng Huang <heng.huang@pitt.edu>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

Figure 1. We split a multilayer feedforward neural network into
three modules. Each module is a stack of layers. Backpropagation
algorithm requires running forward pass (from 1 to 3) and back-
ward pass (from 4 to 6) in sequential order. For example, module
A cannot perform step 6 before receiving δt
A which is an output of
step 5 in module B.

the backpropagation algorithm consists of two processes, the
forward pass to compute prediction and the backward pass
to compute gradient and update the model. After computing
prediction in the forward pass, backpropagation algorithm
requires propagating error gradients from the top (output
layer) all the way back to the bottom (input layer). There-
fore, in the backward pass, all layers, or more generally,
modules, of the network are locked until their dependencies
have executed.

The backward locking constrains us from updating models
in parallel and fully leveraging the computing resources.
It has been shown in practice (Krizhevsky et al., 2012; Si-
monyan & Zisserman, 2014; Szegedy et al., 2015; He et al.,
2016; Huang et al., 2016) and in theory (Eldan & Shamir,
2016; Telgarsky, 2016; Bengio et al., 2009) that depth is
one of the most critical factors contributing to the success
of deep learning. From AlexNet with 8 layers (Krizhevsky
et al., 2012) to ResNet-101 with more than one hundred
layers (He et al., 2016), the forward and backward time
grow from (4.31ms and 9.58ms) to (53.38ms and 103.06ms)
when we train the networks on Titan X with the input size of
16×3×224×224 (Johnson, 2017). Therefore, parallelizing
the backward pass can greatly reduce the training time when

Decoupled Parallel Backpropagation with Convergence Guarantee

the backward time is about twice of the forward time. We
can easily split a deep neural network into modules like Fig-
ure 1 and distribute them across multiple GPUs. However,
because of the backward locking, all GPUs are idle before
receiving error gradients from dependent modules in the
backward pass.

There have been several algorithms proposed for breaking
the backward locking. For example, (Jaderberg et al., 2016;
Czarnecki et al., 2017) proposed to remove the lockings in
backpropagation by employing additional neural networks
to approximate error gradients. In the backward pass, all
modules use the synthetic gradients to update weights of
the model without incurring any delay. (Nøkland, 2016;
Balduzzi et al., 2015) broke the local dependencies between
successive layers and made all hidden layers receive error
information from the output layer directly. In (Carreira-
Perpinan & Wang, 2014; Taylor et al., 2016), the authors
loosened the exact connections between layers by introduc-
ing auxiliary variables. In each layer, they imposed equality
constraint between the auxiliary variable and activation,
and optimized the new problem using Alternating Direction
Method which is easy to parallel. However, for the con-
volutional neural network, the performances of all above
methods are much worse than backpropagation algorithm
when the network is deep.

In this paper, we focus on breaking the backward locking in
backpropagtion algorithm for training feedforward neural
networks, such that we can update models in parallel without
loss of accuracy. The main contributions of our work are as
follows:

• Firstly, we decouple the backpropagation using delayed
gradients in Section 3 such that all modules of the
network can be updated in parallel without backward
locking.

• Then, we propose two stochastic algorithms using de-
coupled parallel backpropagation in Section 3 for deep
learning optimization.

• We also provide convergence analysis for the proposed
method in Section 4 and prove that it guarantees con-
vergence to critical points for the non-convex problem.

• Finally, we perform experiments for training deep con-
volutional neural networks in Section 5, experimental
results verifying that the proposed method can signiﬁ-
cantly speed up the training without loss of accuracy.

L layers, each layer taking an input hl−1 and producing
an activation hl = Fl(hl−1; wl) with weight wl. Letting
d be the dimension of weights in the network, we have
w = [w1, w2, ..., wL] ∈ Rd. Thus, the output of the network
can be represented as hL = F (h0; w), where h0 denotes
the input data x. Taking a loss function f and targets y, the
training problem is as follows:

min
w=[w1,...,wL]

f (F (x; w), y).

(1)

In the following context, we use f (w) for simplicity.

Gradients based methods are widely used for deep learning
optimization (Robbins & Monro, 1951; Qian, 1999; Hinton
et al., 2012; Kingma & Ba, 2014). In iteration t, we put a
data sample xi(t) into the network, where i(t) denotes the in-
dex of the sample. According to stochastic gradient descent
(SGD), we update the weights of the network through:
(cid:2)∇fl,xi(t) (wt)(cid:3)

, ∀l ∈ {1, 2, ..., L}

l = wt

l − γt

wt+1

(2)

l

where γt is the stepsize and ∇fl,xi(t)(wt) ∈ Rd is the gra-
dient of the loss function (1) with respect to the weights at
layer l and data sample xi(t), all the coordinates in other
than layer l are 0. We always utilize backpropagation al-
gorithm to compute the gradients (Rumelhart et al., 1988).
The backpropagation algorithm consists of two passes of
the network: in the forward pass, the activations of all layers
are calculated from l = 1 to L as follows:

l = Fl(ht
ht

l−1; wl);

in the backward pass, we apply chain rule for gradients and
repeatedly propagate error gradients through the network
from the output layer l = L to the input layer l = 1:

∂f (wt)
∂wt
l
∂f (wt)
∂ht

l−1

=

=

∂ht
l
∂wt
l
∂ht
l
∂ht

,

∂f (wt)
∂ht
l
∂f (wt)
∂ht
l

l−1

,

∂wt
l

where we let ∇fl,xi(t) (wt) = ∂f (wt)
. From equations (4)
and (5), it is obvious that the computation in layer l is depen-
dent on the error gradient ∂f (wt)
from layer l+1. Therefore,
∂ht
l
the backward locking constrains all layers from updating
before receiving error gradients from the dependent layers.
When the network is very deep or distributed across multiple
resources, the backward locking is the main bottleneck in
the training process.

(3)

(4)

(5)

2. Backgrounds

3. Decoupled Parallel Backpropagation

We begin with a brief overview of the backpropagation
algorithm for the optimization of neural networks. Suppose
that we want to train a feedforward neural network with

In this section, we propose to decouple the backpropagation
algorithm using delayed gradients (DDG). Suppose we split
a L-layer feedforward neural network to K modules, such

Decoupled Parallel Backpropagation with Convergence Guarantee

Figure 2. We split a multilayer feedforward neural network into three modules (A, B and C), where each module is a stack of layers. After
executing the forward pass (from 1 to 3) to predict, our proposed method allows all modules to run backward pass (4) using delayed
gradients without locking. Particularly, module A can perform the backward pass using the stale error gradient δt−2
A . Meanwhile, It also
receives δt−1

A from module B for the update of the next iteration.

that the weights of the network are divided into K groups.
Therefore, we have w = [wG(1), wG(2), ..., wG(K)] where
G(k) denotes layer indices in the group k.

Table 1. Comparisons of computation time when the network is
sequentially distributed across K GPUs. TF and TB denote the
forward and backward time for backpropagation algorithm.

3.1. Backpropagation Using Delayed Gradients

In iteration t, data sample xi(t) is input to the network. We
run the forward pass from module k = 1 to k = K. In each
module, we compute the activations in sequential order as
equation (3). In the backward pass, all modules except the
last one have delayed error gradients in store such that they
can execute the backward computation without locking. The
last module updates with the up-to-date gradients. In par-
ticular, module k keeps the stale error gradient ∂f (wt−K+k)
,
∂ht−K+k
Lk
where Lk denotes the last layer in module k. Therefore, the
backward computation in module k is as follows:

∂f (wt−K+k)
∂wt−K+k
l
∂f (wt−K+k)
∂ht−K+k
l−1

=

=

l

∂ht−K+k
∂wt−K+k
l
∂ht−K+k
∂ht−K+k
l−1

l

,

∂f (wt−K+k)
∂ht−K+k
l
∂f (wt−K+k)
∂ht−K+k

.

l

(6)

(7)

where (cid:96) ∈ G(k). Meanwhile, each module also receives
error gradient from the dependent module for further com-
putation. From (6) and (7), we can know that the stale error
gradients in all modules are of different time delay. From
module k = 1 to k = K, their corresponding time delays
are from K − 1 to 0. Delay 0 indicates that the gradients
are up-to-date. In this way, we break the backward locking
and achieve parallel update in the backward pass. Figure 2
shows an example of the decoupled backpropagation, where
error gradients δ := ∂f (w)
∂h .

3.2. Speedup of Decoupled Parallel Backpropagation

When K = 1, there is no time delay and the proposed
method is equivalent to the backpropagation algorithm.
When K (cid:54)= 1, we can distribute the network across multiple

Method
Backpropagation
DDG

Computation Time
TF + TB
TF + TB
K

GPUs and fully leverage the computing resources. Table 1
lists the computation time when we sequentially allocate the
network across K GPUs. When TF is necessary to compute
accurate predictions, we can accelerate the training by re-
ducing the backward time. Because TB is much large than
TF , we can achieve huge speedup even K is small.

Relation to model parallelism: Model parallelism usually
refers to ﬁlter-wise parallelism (Yadan et al., 2013). For ex-
ample, we split a convolutional layer with N ﬁlters into two
GPUs, each part containing N
2 ﬁlters. Although the ﬁlter-
wise parallelism accelerates the training when we distribute
the workloads across multiple GPUs, it still suffers from
the backward locking. We can think of DDG algorithm as
layer-wise parallelism. It is also easy to combine ﬁlter-wise
parallelism with layer-wise parallelism for further speedup.

3.3. Stochastic Methods Using Delayed Gradients

After computing the gradients of the loss function with re-
spect to the weights of the model, we update the model using
(cid:0)wt−K+k(cid:1) :=
delayed gradients. Letting ∇fG(k),xi(t−K+k)

(cid:80)

l∈G(k)

∂f (wt−K+k)
∂wt−K+k

l






0

otherwise

if t − K + k ≥ 0

,

(8)

for any k ∈ {1, 2, ..., K}, we update the weights in module
k following SGD:

wt+1
G(k) = wt

G(k) − γt[∇fG(k),xi(t−K+k)

(cid:0)wt−K+k(cid:1)]G(k).

(9)

Decoupled Parallel Backpropagation with Convergence Guarantee

Algorithm 1 SGD-DDG

Require:

G(1), ..., w0

Initial weights w0 = [w0
Stepsize sequence {γt};
1: for t = 0, 1, 2, . . . , T − 1 do
2:
3:

for k = 1, . . . , K in parallel do
Compute delayed gradient:
(cid:104)

gt
k ←

∇fG(k),xi(t−K+k)

4:

Update weights:

wt+1

G(k) ← wt

G(k) − γt · gt
k;

end for

5:
6: end for

G(K)] ∈ Rd;

(cid:0)wt−K+k(cid:1)(cid:105)

;

G(k)

where γt denotes stepsize. Different from SGD, we update
the weights with delayed gradients. Besides, the delayed
iteration (t − K + k) for group k is also deterministic. We
summarize the proposed method in Algorithm 1.

Moreover, we can also apply the delayed gradients to other
variants of SGD, for example Adam in Algorithm 2. In
each iteration, we update the weights and moment vectors
with delayed gradients. We analyze the convergence for
Algorithm 1 in Section 4, which is the basis of analysis for
other methods.

4. Convergence Analysis

In this section, we establish the convergence guarantees to
critical points for Algorithm 1 when the problem is non-
convex. Analysis shows that our method admits similar con-
vergence rate to vanilla stochastic gradient descent (Bottou
et al., 2016). Throughout this paper, we make the following
commonly used assumptions:

Assumption 1 (Lipschitz-continuous gradient) The gradi-
ent of f (w) is Lipschitz continuous with Lipschitz constant
L > 0, such that ∀w, v ∈ Rd:

(cid:107)∇f (w) − ∇f (v)(cid:107)2 ≤ L(cid:107)w − v(cid:107)2

(10)

Assumption 2 (Bounded variance) To bound the variance
of the stochastic gradient, we assume the second moment
of the stochastic gradient is upper bounded, such that there
exists constant M ≥ 0, for any sample xi and ∀w ∈ Rd:

(cid:107)∇fxi(w)(cid:107)2

2 ≤ M

(11)

Because of the unnoised stochastic gradient E [∇fxi (w)] =
variance
equation
and
∇f (w)
E (cid:107)∇fxi (w) − ∇f (w)(cid:107)2
2 = E(cid:107)∇fxi(w)(cid:107)2
2 − (cid:107)∇f (w)(cid:107)2
2,
the variance of the stochastic gradient is guaranteed to be
less than M .

regarding

the

Algorithm 2 Adam-DDG

Require:

G(K)] ∈ Rd;

G(1), ..., w0

Initial weights: w0 = [w0
Stepsize: γ; Constant (cid:15) = 10−8;
Exponential decay rates: β1 = 0.9 and β2 = 0.999 ;
First moment vector: m0
G(k) ← 0, ∀k ∈ {1, 2, ..., K};
Second moment vector: v0
G(k) ← 0, ∀k ∈ {1, 2, ..., K};
1: for t = 0, 1, 2, . . . , T − 1 do
2:
3:

for k = 1, . . . , K in parallel do
Compute delayed gradient:
(cid:104)

gt
k ←

∇fG(k),xi(t−K+k)

(cid:0)wt−K+k(cid:1)(cid:105)

;

G(k)

4:

5:

6:

7:

8:

Update biased ﬁrst moment estimate:
G(k) ← β1 · mt

G(k) + (1 − β1) · gt
k

mt+1

Update biased second moment estimate:

vt+1
G(k) ← β2 · vt

k)2
Compute bias-correct ﬁrst moment estimate:

G(k) + (1 − β2) · (gt

ˆmt+1

G(k) ← mt+1

G(k)/(1 − βt+1

1

)

Compute bias-correct second moment estimate:
G(k)/(1 − βt+1

)

2

G(k) ← vt+1
ˆvt+1
Update weights:
wt+1

G(k) ← wt

G(k) −γ · ˆmt+1

G(k)/

(cid:16)(cid:113)

(cid:17)
ˆvt+1
G(k) + (cid:15)

end for

9:
10: end for

Under Assumption 1 and 2, we obtain the following lemma
about the sequence of objective functions.

Lemma 1 Assume Assumption 1 and 2 hold. In addition,
and MK = KM +σK 4M .
we let σ := maxt
The iterations in Algorithm 1 satisfy the following inequality,
for all t ∈ N:

γmax{0,t−K+1}
γt

E (cid:2)f (wt+1)(cid:3) − f (wt) ≤ −

(cid:13)∇f (wt)(cid:13)
(cid:13)
2
2 + γ2
(cid:13)

t LMK (12)

γt
2

From Lemma 1, we can observe that the expected decrease
of the objective function is controlled by the stepsize γt
and MK. Therefore, we can guarantee that the values of
objective functions are decreasing as long as the stepsizes
γt are small enough such that the right-hand side of (12) is
less than zero. Using the lemma above, we can analyze the
convergence property for Algorithm 1.

4.1. Fixed Stepsize γt

Firstly, we analyze the convergence for Algorithm 1 when
γt is ﬁxed and prove that the learned model will converge
sub-linearly to the neighborhood of the critical points.

Theorem 1 Assume Assumption 1 and 2 hold and the ﬁxed
stepsize sequence {γt} satisﬁes γt = γ and γL ≤ 1, ∀t ∈
In addition, we assume w∗ to be the
{0, 1, ..., T − 1}.

Decoupled Parallel Backpropagation with Convergence Guarantee

Figure 3. Training and testing curves regarding epochs for ResNet-8 on CIFAR-10. Upper: Loss function values regarding epochs;
Bottom: Top 1 classiﬁcation accuracies regarding epochs. We split the network into two modules such that there is only one split point in
the network for DNI and DDG.

optimal solution to f (w) and let σ = 1 such that MK =
KM + K 4M . Then, the output of Algorithm 1 satisﬁes
that:

1
T

T −1
(cid:88)

t=0

E (cid:13)

(cid:13)∇f (wt)(cid:13)
2
(cid:13)
2

≤

2 (cid:0)f (w0) − f (w∗)(cid:1)
γT

+ 2γLMK (13)

In Theorem 1, we can observe that when T → ∞, the
average norm of the gradients is upper bounded by 2γLMK.
The number of modules K affects the value of the upper
bound. Selecting a small stepsize γ allows us to get better
neighborhood to the critical points, however it also seriously
decreases the speed of convergence.

4.2. Diminishing Stepsize γt

In this section, we prove that Algorithm 1 with diminishing
stepsizes can guarantee the convergence to critical points
for the non-convex problem.

Theorem 2 Assume Assumption 1 and 2 hold and the di-
minishing stepsize sequence {γt} satisﬁes γt = γ0
1+t and
γtL ≤ 1, ∀t ∈ {0, 1, ..., T − 1}. In addition, we assume
w∗ to be the optimal solution to f (w) and let σ = K such

that MK = KM + K 5M . Setting ΓT =

γt, then the

T −1
(cid:80)
t=0

output of Algorithm 1 satisﬁes that:

1
ΓT

T −1
(cid:88)

t=0

γtE (cid:13)

(cid:13)∇f (wt)(cid:13)
2
2 ≤
(cid:13)

2 (cid:0)f (w0) − f (w∗)(cid:1)
ΓT

γ2
t LMK

2

T −1
(cid:80)
t=0

+

ΓT

(14)

Corollary 1 Since γt = γ0
(Robbins & Monro, 1951) are satisﬁed that:

t+1 , the stepsize requirements in

lim
T →∞

T −1
(cid:88)

t=0

γt = ∞ and

γ2
t < ∞.

(15)

lim
T →∞

T −1
(cid:88)

t=0

Therefore, according to Theorem 2, when T → ∞, the
right-hand side of (14) converges to 0.

Corollary 2 Suppose ws
is chosen randomly from
t=0 with probabilities proportional to {γt}T −1
{wt}T −1
t=0 . Ac-
cording to Theorem 2, we can prove that Algorithm 1 guar-
antees convergence to critical points for the non-convex
problem:

lim
s→∞

E(cid:107)∇f (ws)(cid:107)2

2 = 0

(16)

5. Experiments

In this section, we experiment with ResNet (He et al., 2016)
on image classiﬁcation benchmark datasets: CIFAR-10 and
CIFAR-100 (Krizhevsky & Hinton, 2009). In section 5.1,
we evaluate our method by varying the positions and the
number of the split points in the network; In section 5.2
we use our method to optimize deeper neural networks and
show that its performance is as good as the performance of
backpropagation; ﬁnally, we split and distribute the ResNet-
110 across GPUs in Section 5.3, results showing that the
proposed method achieves a speedup of two times without
loss of accuracy.

Decoupled Parallel Backpropagation with Convergence Guarantee

Figure 4. Training and testing curves regarding epochs for ResNet-8 on CIFAR-10. Upper: Loss function values regarding epochs;
Bottom: Top1 classiﬁcation accuracies regarding epochs. For DNI and DDG, the number of split points in the network ranges from 2 to 4.

Implementation Details: We implement DDG algorithm
using PyTorch library (Paszke et al., 2017). The trained
network is split into K modules where each module is run-
ning on a subprocess. The subprocesses are spawned using
multiprocessing package 1 such that we can fully leverage
multiple processors on a given machine. Running modules
on different subprocesses make the communication very
difﬁcult. To make the communication fast, we utilize the
shared memory objects in the multiprocessing package. As
in Figure 2, every two adjacent modules share a pair of
activation (h) and error gradient (δ).

5.1. Comparison of BP, DNI and DDG

In this section, we train ResNet-8 on CIFAR-10 on a single
Titan X GPU. The architecture of the ResNet-8 is in Table 2.
All experiments are run for 300 epochs and optimized using
Adam optimizer (Kingma & Ba, 2014) with a batch size of
128. The stepsize is initialized at 1 × 10−3. We augment the
dataset with random cropping, random horizontal ﬂipping
and normalize the image using mean and standard deviation.
There are three compared methods in this experiment:

• BP: Adam optimizer in Pytorch uses backpropagation
algorithm with data parallelism (Rumelhart et al., 1988)
to compute gradients.

1https://docs.python.org/3/library/multiprocessing.html#module-

multiprocessing

Table 2. Architectural details. Units denotes the number of resid-
ual units in each group. Each unit is a basic residual block without
bottleneck. Channels indicates the number of ﬁlters used in each
unit in each group.

Architecture
ResNet-8
ResNet-56
ResNet-110

Units
1-1-1
9-9-9
18-18-18

Channels
16-16-32-64
16-16-32-64
16-16-32-64

• DNI: Decoupled neural interface (DNI) in (Jaderberg
et al., 2016). Following (Jaderberg et al., 2016), the
synthetic network is a stack of three convolutional
layers with L 5 × 5 ﬁlters with resolution preserving
padding. The ﬁlter depth L is determined by the posi-
tion of DNI. We also input label information into the
synthetic network to increase ﬁnal accuracy.

• DDG: Adam optimizer using delayed gradients in Al-

gorithm 2.

Impact of split position (depth). The position (depth) of
the split points determines the number of layers using de-
layed gradients. Stale or synthetic gradients will induce
noises in the training process, affecting the convergence of
the objective. Figure 3 exhibits the experimental results
when there is only one split point with varying positions. In
the ﬁrst column, we know that all compared methods have

Decoupled Parallel Backpropagation with Convergence Guarantee

(a)

(b)

Figure 5. Training and testing loss curves for ResNet-110 on CIFAR-10 using multiple GPUs. (5a) Loss function value regarding epochs.
(5b) Loss function value regarding computation time.

Table 3. The best Top 1 classiﬁcation accuracy (%) for ResNet-56
and ResNet-110 on the test data of CIFAR-10 and CIFAR-100.

Architecture

ResNet-56
ResNet-110

CIFAR-10
BP
93.12
93.53

DDG
93.11
93.41

CIFAR-100
DDG
BP
70.17
69.79
71.90
71.39

5.2. Optimizing Deeper Neural Networks

In this section, we employ DDG to optimize two very deep
neural networks (ResNet-56 and ResNet-110) on CIFAR-10
and CIFAR-100. Each network is split into two modules
at the center. We use SGD with the momentum of 0.9 and
the stepsize is initialized to 0.01. Each model is trained for
300 epochs and the stepsize is divided by a factor of 10 at
150 and 225 epochs. The weight decay constant is set to
5 × 10−4. We perform the same data augmentation as in
section 5.1. Experiments are run on a single Titan X GPU.

Figure 7 presents the experimental results of BP and DDG.
We do not compare DNI because its performance is far
worse when models are deep. Figures in the ﬁrst column
present the convergence of loss regarding epochs, showing
that DDG and BP admit similar convergence rates. We can
also observe that DDG converges faster when we compare
the loss regarding computation time in the second column
of Figure 7. In the experiment, the “Volatile GPU Utility” is
about 70% when we train the models with BP. Our method
runs on two subprocesses such that it fully leverages the
computing capacity of the GPU. We can draw similar con-
clusions when we compare the Top 1 accuracy in the third
and fourth columns of Figure 7. In Table 3, we list the best
Top 1 accuracy on the test data of CIFAR-10 and CIFAR-
100. We can observe that DDG can obtain comparable or

Figure 6. Computation time and the best Top 1 accuracy for
ResNet-110 on the test data of CIFAR-10. The most left bar
denotes the computation time using backpropagation algorithm
on a GPU, where the forward time accounts for about 32%. We
normalize the computation time of all optimization settings using
the amount of time required by backpropagation.

similar performances when the split point is at layer 1. DDG
performs consistently well when we place the split point at
deeper positions 3, 5 or 7. On the contrary, the performance
of DNI degrades as we vary the positions and it cannot even
converge when the split point is at layer 7.

Impact of the number of split points. From equation (7),
we know that the maximum time delay is determined by
the number of modules K. Theorem 2 also shows that K
affects the convergence rate. In this experiment, we vary the
number of split points in the network from 2 to 4 and plot the
results in Figure 4. It is easy to observe that DDG performs
as well as BP, regardless of the number of split points in
the network. However, DNI is very unstable when we place
more split points, and cannot even converge sometimes.

Decoupled Parallel Backpropagation with Convergence Guarantee

Figure 7. Training and testing curves for ResNet-56 and ResNet-110 on CIFAR-10 and CIFAR-100. Column 1 and 2 present the loss
function value regrading epochs and computation time respectively; Column 3 and 4 present the Top 1 classiﬁcation accuracy regrading
epochs and computation time. For DDG, there is only one split point at the center of the network.

better accuracy even when the network is deep.

5.3. Scaling the Number of GPUs

In this section, we split ResNet-110 into K modules and
allocate them across K Titan X GPUs sequentially. We do
not consider ﬁlter-wise model parallelism in this experiment.
The selections of the parameters in the experiment are sim-
ilar to Section 5.2. From Figure 5, we know that training
networks in multiple GPUs does not affect the convergence
rate. For comparison, we also count the computation time of
backpropagation algorithm on a single GPU. The computa-
tion time is worse when we run backpropagation algorithm
on multiple GPUs because of the communication overhead.
In Figure 6, we can observe that forward time only accounts
for about 32% of the total computation time for backpropa-

gation algorithm. Therefore, backward locking is the main
bottleneck. In Figure 6, it is obvious that when we increase
the number of GPUs from 2 to 4, our method reduces about
30% to 50% of the total computation time. In other words,
DDG achieves a speedup of about 2 times without loss of
accuracy when we train the networks across 4 GPUs.

6. Conclusion

In this paper, we propose decoupled parallel backpropa-
gation algorithm, which breaks the backward locking in
backpropagation algorithm using delayed gradients. We
then apply the decoupled parallel backpropagation to two
stochastic methods for deep learning optimization. In the
theoretical section, we also provide convergence analysis

Decoupled Parallel Backpropagation with Convergence Guarantee

and prove that the proposed method guarantees convergence
to critical points for the non-convex problem. Finally, we
perform experiments on deep convolutional neural networks,
results verifying that our method can accelerate the training
signiﬁcantly without loss of accuracy.

Acknowledgement

This work was partially supported by U.S. NIH R01
AG049371, NSF IIS 1302675, IIS 1344152, DBI 1356628,
IIS 1619308, IIS 1633753.

References

Balduzzi, D., Vanchinathan, H., and Buhmann, J. M. Kick-
back cuts backprop’s red-tape: Biologically plausible
credit assignment in neural networks. In AAAI, pp. 485–
491, 2015.

Bengio, Y. et al. Learning deep architectures for ai. Foun-
dations and trends R(cid:13) in Machine Learning, 2(1):1–127,
2009.

Bottou, L., Curtis, F. E., and Nocedal, J. Optimization
methods for large-scale machine learning. arXiv preprint
arXiv:1606.04838, 2016.

Carreira-Perpinan, M. and Wang, W. Distributed optimiza-
tion of deeply nested systems. In Artiﬁcial Intelligence
and Statistics, pp. 10–19, 2014.

Czarnecki, W. M., ´Swirszcz, G., Jaderberg, M., Osindero,
S., Vinyals, O., and Kavukcuoglu, K. Understanding
synthetic gradients and decoupled neural interfaces. arXiv
preprint arXiv:1703.00522, 2017.

Eldan, R. and Shamir, O. The power of depth for feedfor-
ward neural networks. In Conference on Learning Theory,
pp. 907–940, 2016.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition,
pp. 770–778, 2016.

Hinton, G., Srivastava, N., and Swersky, K. Neural networks
for machine learning-lecture 6a-overview of mini-batch
gradient descent, 2012.

Huang, G., Liu, Z., Weinberger, K. Q., and van der Maaten,
L. Densely connected convolutional networks. arXiv
preprint arXiv:1608.06993, 2016.

Jaderberg, M., Czarnecki, W. M., Osindero, S., Vinyals,
O., Graves, A., and Kavukcuoglu, K. Decoupled neu-
ral interfaces using synthetic gradients. arXiv preprint
arXiv:1608.05343, 2016.

Johnson, J. Benchmarks for popular cnn models. https:
//github.com/jcjohnson/cnn-benchmarks,
2017.

Kingma, D. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.

Krizhevsky, A. and Hinton, G. Learning multiple layers of

features from tiny images. 2009.

Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
classiﬁcation with deep convolutional neural networks.
In Advances in neural information processing systems,
pp. 1097–1105, 2012.

LeCun, Y., Bengio, Y., and Hinton, G. Deep learning. Na-

ture, 521(7553):436–444, 2015.

Nøkland, A. Direct feedback alignment provides learning in
deep neural networks. In Advances in Neural Information
Processing Systems, pp. 1037–1045, 2016.

Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,
DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,
A. Automatic differentiation in pytorch. 2017.

Qian, N. On the momentum term in gradient descent learn-
ing algorithms. Neural networks, 12(1):145–151, 1999.

Robbins, H. and Monro, S. A stochastic approximation
method. The annals of mathematical statistics, pp. 400–
407, 1951.

Rumelhart, D. E., Hinton, G. E., Williams, R. J., et al. Learn-
ing representations by back-propagating errors. Cognitive
modeling, 5(3):1, 1988.

Simonyan, K. and Zisserman, A. Very deep convolu-
tional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich,
In Proceedings
A. Going deeper with convolutions.
of the IEEE conference on computer vision and pattern
recognition, pp. 1–9, 2015.

Taylor, G., Burmeister, R., Xu, Z., Singh, B., Patel, A., and
Goldstein, T. Training neural networks without gradients:
A scalable admm approach. In International Conference
on Machine Learning, pp. 2722–2731, 2016.

Telgarsky, M. Beneﬁts of depth in neural networks. arXiv

preprint arXiv:1602.04485, 2016.

Yadan, O., Adams, K., Taigman, Y., and Ranzato, M. Multi-
gpu training of convnets. arXiv preprint arXiv:1312.5853,
2013.

Supplementary Materials for Paper “Decoupled Parallel Backpropagation with
Convergence Guarantee”

A. Proof to Lemma 1

Proof: Because the gradient of f (w) is Lipschitz continuous in Assumption 1, the following inequality holds that:

f (wt+1) ≤ f (wt) + ∇f (wt)T (cid:0)wt+1 − wt(cid:1) +

L
2

(cid:13)wt+1 − wt(cid:13)
(cid:13)
2
2 .
(cid:13)

(17)

From the update rule in Algorithm 1, we take expectation on both sides and obtain:

E (cid:2)f (wt+1)(cid:3) ≤ f (wt) − γtE

(cid:34)
∇f (wt)T

(cid:32) K
(cid:88)

k=1

∇fG(k),xi(t−K+k)

(cid:0)wt−K+k(cid:1)

(cid:33)(cid:35)

+

Lγ2
t
2

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

k=1

(cid:13)
2
(cid:13)
∇fG(k),xi(t−K+k)(wt−K+k)
(cid:13)
(cid:13)
(cid:13)
2

K
(cid:88)

K
(cid:88)

k=1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k=1

≤ f (wt) − γt

∇f (wt)T (cid:0)∇fG(k)

(cid:0)wt−K+k(cid:1) + ∇fG(k)

(cid:0)wt(cid:1) − ∇fG(k)

(cid:0)wt(cid:1)(cid:1)

K
(cid:88)

k=1

+

Lγ2
t
2

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

k=1

(cid:13)
2
(cid:13)
∇fG(k),xi(t−K+k)(wt−K+k) − ∇f (wt) + ∇f (wt)
(cid:13)
(cid:13)
(cid:13)
2

= f (wt) − γt

(cid:13)∇f (wt)(cid:13)
(cid:13)
2
2 − γt
(cid:13)

∇f (wt)T (cid:0)∇fG(k)

(cid:0)wt−K+k(cid:1) − ∇fG(k)

(cid:0)wt(cid:1)(cid:1)

(cid:13)∇f (wt)(cid:13)
(cid:13)
2
2 +
(cid:13)

Lγ2
t
2

E

(cid:13)
2
(cid:13)
∇fG(k),xi(t−K+k)(wt−K+k) − ∇f (wt)
(cid:13)
(cid:13)
(cid:13)
2

∇f (wt)T (cid:0)∇fG(k)

(cid:0)wt−K+k(cid:1) − ∇fG(k)

(cid:0)wt(cid:1)(cid:1)

+

Lγ2
t
2

+Lγ2
t

K
(cid:88)

k=1

(cid:18)

= f (wt) −

γt −

(cid:19)

Lγ2
t
2

(cid:13)
(cid:13)∇f (wt)(cid:13)
2
2 +
(cid:13)

Lγ2
t
2

E

(cid:124)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

k=1

(cid:13)
2
(cid:13)
∇fG(k),xi(t−K+k)(wt−K+k) − ∇f (wt)
(cid:13)
(cid:13)
(cid:13)
(cid:125)

−(γt − Lγ2
t )

∇f (wt)T (cid:0)∇fG(k)

(cid:0)wt−K+k(cid:1) − ∇fG(k)

(cid:0)wt(cid:1)(cid:1)

,

(18)

K
(cid:88)

k=1

(cid:124)

(cid:123)(cid:122)
Q2

(cid:123)(cid:122)
Q1

(cid:125)

where the second inequality follows from the unbiased gradient E [∇fxi(w)] = ∇f (w). Because of (cid:107)x + y(cid:107)2

2 ≤ 2(cid:107)x(cid:107)2

2 +

Decoupled Parallel Backpropagation with Convergence Guarantee

2(cid:107)y(cid:107)2

2 and xy ≤ 1

2 + 1

2 (cid:107)y(cid:107)2

2, we have the upper bound of Q1 and Q2 as follows:

Q1 =

Lγ2
t
2

E

∇fG(k),xi(t−K+k) (wt−K+k) − ∇f (wt) −

∇fG(k)(wt−K+k) +

∇fG(k)(wt−K+k)

K
(cid:88)

k=1

K
(cid:88)

k=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

≤ Lγ2
t

E

∇fG(k),xi(t−K+k) (wt−K+k) −

∇fG(k)(wt−K+k)

+Lγ2
t

K
(cid:88)

2 (cid:107)x(cid:107)2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

k=1

k=1

(cid:124)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:125)

K
(cid:88)

k=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:124)

(cid:13)
2
(cid:13)
∇fG(k)(wt−K+k) − ∇f (wt)
(cid:13)
.
(19)
(cid:13)
(cid:13)
2
(cid:125)

(cid:123)(cid:122)
Q4

Q2 = −(γt − Lγ2
t )

∇f (wt)T (cid:0)∇fG(k)

(cid:0)wt−K+k(cid:1) − ∇fG(k)

(cid:0)wt(cid:1)(cid:1)

≤

γt − Lγ2
t
2

(cid:13)∇f (wt)(cid:13)
(cid:13)
2
2 +
(cid:13)

γt − Lγ2
t
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

k=1

∇fG(k)(wt−K+k) − ∇f (wt)

(20)

As per the equation regarding variance E(cid:107)ξ − E[ξ](cid:107)2

2 = E(cid:107)ξ(cid:107)2

2 − (cid:107)E[ξ](cid:107)2

2, we can bound Q3 as follows:

K
(cid:88)

k=1

(cid:123)(cid:122)
Q3

K
(cid:88)

k=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

.

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

Q3 = E

∇fG(k),xi(t−K+k) (wt−K+k) −

∇fG(k)(wt−K+k)

K
(cid:88)

k=1

E

(cid:13)
(cid:13)
2
(cid:13)∇fG(k),xi(t−K+k) (wt−K+k) − ∇fG(k)(wt−K+k)
(cid:13)
(cid:13)
(cid:13)
2

E

(cid:13)
(cid:13)
2
(cid:13)∇fG(k),xi(t−K+k) (wt−K+k)
(cid:13)
(cid:13)
(cid:13)
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

k=1

=

≤

K
(cid:88)

k=1

K
(cid:88)

k=1

≤ KM,

where the equality follows from the deﬁnition of ∇fG(k)(w) such that [∇fG(k)(w)]j = 0, ∀j /∈ G(k) and the last inequality
is from Assumption 2. We can also get the upper bound of Q4:

Q4 =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

k=1

(cid:13)
2
(cid:13)
∇fG(k)(wt−K+k) − ∇f (wt)
(cid:13)
(cid:13)
(cid:13)
2

(cid:13)∇fG(k)(wt−K+k) − ∇fG(k)(wt)(cid:13)
(cid:13)
2
(cid:13)
2

(cid:13)∇f (wt−K+k) − ∇f (wt)(cid:13)
(cid:13)
2
(cid:13)
2

K
(cid:88)

≤ L2

t−1
(cid:88)

(cid:0)wj+1 − wj(cid:1)

k=1

j=max{0,t−K+k}

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

≤

K
(cid:88)

k=1

K
(cid:88)

k=1

≤ L2γ2

max{0,t−K+1}K

∇fG(k),x(j)

(cid:0)wj−K+k(cid:1)

K
(cid:88)

t−1
(cid:88)

k=1

j=max{0,t−K+k}

K
(cid:88)

t−1
(cid:88)

K
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k=1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

k=1

j=max{0,t−K+k}

k=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

∇fG(k),x(j)

(cid:0)wj−K+k(cid:1)

≤ KLγt

γmax{0,t−K+1}
γt

≤ LγtσK 4M,

where the second inequality is from Assumption 1, the fourth inequality follows from that Lγt ≤ 1 and the last inequality
follows from (cid:107)z1 + ... + zr(cid:107)2
. Integrating the upper

2), Assumption 2 and σ := maxt

2 + ... + (cid:107)zr(cid:107)2

2 ≤ r((cid:107)z1(cid:107)2

γmax{0,t−K+1}
γt

(21)

(22)

Decoupled Parallel Backpropagation with Convergence Guarantee

bound of Q1, Q2, Q3 and Q4 in (18), we have:

E (cid:2)f (wt+1)(cid:3) − f (wt) ≤ −

γt
2

(cid:13)∇f (wt)(cid:13)
(cid:13)
2
2 + γ2
t L
(cid:13)

K
(cid:88)

k=1

E

(cid:13)
(cid:13)
2
(cid:13)∇fG(k),xi(t−K+k) (wt−K+k)
(cid:13)
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

γt + Lγ2
t
2
(cid:13)∇f (wt)(cid:13)
(cid:13)
2
(cid:13)

k=1

γt
2

≤ −

(cid:13)
2
(cid:13)
∇fG(k)(wt−K+k) − ∇f (wt)
(cid:13)
(cid:13)
(cid:13)
2

.

+ γ2

t LMK,

where we let MK = KM + σK 4M .

B. Proof to Theorem 1

Proof: When γt is constant and γt = γ, taking total expectation of (12) in Lemma 1, we obtain:

E (cid:2)f (wt+1)(cid:3) − E (cid:2)f (wt)(cid:3) ≤ −

E (cid:13)

(cid:13)∇f (wt)(cid:13)
2
2 + γ2LMK,
(cid:13)

where σ = 1 and MK = KM + K 4M . Summing (24) from t = 0 to T − 1, we have:

E (cid:2)f (wT )(cid:3) − f (w0) ≤ −

E (cid:13)

(cid:13)∇f (wt)(cid:13)
2
2 + T γ2LMK.
(cid:13)

γ
2

γ
2

T −1
(cid:88)

t=0

Suppose w∗ is the optimal solution for f (w), therefore f (w∗) − f (w0) ≤ E (cid:2)f (wT )(cid:3) − f (w0). Above all, the following
inequality is guaranteed that:

1
T

T −1
(cid:88)

t=0

E (cid:13)

(cid:13)∇f (wt)(cid:13)
2
2 ≤
(cid:13)

2 (cid:0)f (w0) − f (w∗)(cid:1)
γT

+ 2γLMK.

C. Proof to Theorem 2

Proof: {γt} is a diminishing sequence and γt = γ0
of (12) in Lemma 1 and summing it from t = 0 to T − 1, we obtain:

1+t , such that σ ≤ K and MK = KM + K 5M . Taking total expectation

E (cid:2)f (wT )(cid:3) − f (w0) ≤ −

γtE (cid:13)

(cid:13)∇f (wt)(cid:13)
2
2 +
(cid:13)

γ2
t LMK.

1
2

T −1
(cid:88)

t=0

T −1
(cid:88)

t=0

Suppose w∗ is the optimal solution for f (w), therefore f (w∗) − f (w0) ≤ E (cid:2)f (wT )(cid:3) − f (w0). Letting ΓT =
have:

1
ΓT

T −1
(cid:88)

t=0

γtE (cid:13)

(cid:13)∇f (wt)(cid:13)
2
2 ≤
(cid:13)

2 (cid:0)f (w0) − f (w∗)(cid:1)
ΓT

+

2

T −1
(cid:80)
t=0

γ2
t LMK

.

ΓT

We complete the proof.

(23)

(24)

(25)

(26)

(27)

T −1
(cid:80)
t=0

γt, we

(28)

