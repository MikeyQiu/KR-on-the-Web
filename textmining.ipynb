{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the code, several dependant package need to be installed, skip if it already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3 in /Applications/anaconda3/lib/python3.7/site-packages (1.24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/04/f62d5834c2bdf90afcaeb23bb5241033c44e27000de64ad8472253daa4a8/pdfminer.six-20200402-py3-none-any.whl (5.6MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6MB 5.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet; python_version > \"3.0\" in /Applications/anaconda3/lib/python3.7/site-packages (from pdfminer.six) (3.0.4)\n",
      "Requirement already satisfied: pycryptodome in /Applications/anaconda3/lib/python3.7/site-packages (from pdfminer.six) (3.9.7)\n",
      "Requirement already satisfied: sortedcontainers in /Applications/anaconda3/lib/python3.7/site-packages (from pdfminer.six) (2.1.0)\n",
      "Installing collected packages: pdfminer.six\n",
      "Successfully installed pdfminer.six-20200402\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymysql in /Applications/anaconda3/lib/python3.7/site-packages (0.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we make connection with database with pymysql api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Low-shot learning with large-scale diffusion',\n",
       "  4,\n",
       "  'graph construction',\n",
       "  'https://github.com/facebookresearch/low-shot-with-diffusion',\n",
       "  '1706.02332',\n",
       "  'YES'],\n",
       " ['A Neural Temporal Model for Human Motion Prediction',\n",
       "  5,\n",
       "  'motion prediction',\n",
       "  'https://github.com/cr7anand/neural_temporal_models',\n",
       "  '1809.03036',\n",
       "  'YES'],\n",
       " ['Integrating kinematics and environment context into deep inverse reinforcement learning for predicting off-road vehicle trajectories',\n",
       "  5,\n",
       "  'motion prediction',\n",
       "  'https://github.com/yfzhang/vehicle-motion-forecasting',\n",
       "  '1810.07225',\n",
       "  'YES'],\n",
       " ['Spectral Inference Networks: Unifying Deep and Spectral Learning',\n",
       "  5,\n",
       "  'bilevel optimization',\n",
       "  'https://github.com/deepmind/spectral_inference_networks',\n",
       "  '1806.02215',\n",
       "  'YES'],\n",
       " ['Collaborative Motion Prediction via Neural Motion Message Passing',\n",
       "  4,\n",
       "  'motion prediction',\n",
       "  'https://github.com/PhyllisH/NMMP',\n",
       "  '2003.06594',\n",
       "  'YES'],\n",
       " ['Common-Knowledge Concept Recognition for SEVA',\n",
       "  4,\n",
       "  'graph construction',\n",
       "  'https://github.com/jitinkrishnan/NASA-SE',\n",
       "  '2003.11687',\n",
       "  'YES'],\n",
       " ['Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks',\n",
       "  3,\n",
       "  'few-shot regression',\n",
       "  'https://github.com/cbfinn/maml',\n",
       "  '1703.03400',\n",
       "  'YES'],\n",
       " ['ICDM 2019 Knowledge Graph Contest: Team UWA',\n",
       "  3,\n",
       "  'graph construction',\n",
       "  'https://github.com/Michael-Stewart-Webdev/text2kg-visualisation',\n",
       "  '1909.01807',\n",
       "  'YES'],\n",
       " ['Weakly Supervised Action Learning with RNN based Fine-to-coarse Modeling',\n",
       "  3,\n",
       "  'action segmentation',\n",
       "  'https://github.com/alexanderrichard/weakly-sup-action-learning',\n",
       "  '1703.08132',\n",
       "  'YES'],\n",
       " ['PyOD: A Python Toolbox for Scalable Outlier Detection',\n",
       "  3,\n",
       "  'outlier ensembles',\n",
       "  'https://github.com/yzhao062/pyod',\n",
       "  '1901.01588',\n",
       "  'YES']]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymysql\n",
    "#input basic info about database\n",
    "conn=pymysql.connect(host='mysql24.ezhostingserver.com', port=3306, user='krw',password='Y>!V@N_26@]cfJ7(')\n",
    "#cursor is the current point we focus on, which could execute sql command.\n",
    "cursor=conn.cursor()\n",
    "lst=[]\n",
    "#Here krw is the databse name and training_view is a view for requesting data. The execution should be around 30s.\n",
    "cursor.execute(\"SELECT * FROM krw.training_view limit 10;\")\n",
    "for r in cursor:\n",
    "    lst.append(list(r))\n",
    "lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After excution we obtained a 2-dimensional matrix with each row represents a paper. Next, we constract a url based on arXiv id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/2003.06594.pdf\n"
     ]
    }
   ],
   "source": [
    "name=lst[4][4]\n",
    "url=\"https://arxiv.org/pdf/\"+str(name)+\".pdf\"\n",
    "print(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By following command we could access pdf by url and convert pdf 2 txt file and store at local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://cloud.tencent.com/developer/article/1395339\n",
    "import urllib\n",
    "import pdfminer\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.layout import *\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from urllib.request import Request\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def OnlinePdfToTxt(dataIo,new_path):\n",
    "    # Create PDF Parser\n",
    "    parser = PDFParser(dataIo)\n",
    "    # Create PDFDocument\n",
    "    document = PDFDocument(parser)\n",
    "    # Is it okay for extraction?\n",
    "    if not document.is_extractable:\n",
    "        raise PDFTextExtractionNotAllowed\n",
    "    else:\n",
    "        # Create PDF Manager\n",
    "        resmag =PDFResourceManager()\n",
    "        # Setting parameters\n",
    "        laparams=LAParams()\n",
    "        # Createing PDF device\n",
    "        # device=PDFDevice(resmag )\n",
    "        device=PDFPageAggregator(resmag ,laparams=laparams)\n",
    "        # Create PDF explainer\n",
    "        interpreter=PDFPageInterpreter(resmag ,device)\n",
    "        # For each page\n",
    "        for page in PDFPage.create_pages(document):\n",
    "            interpreter.process_page(page)\n",
    "            # accept this page's LTP object\n",
    "            layout=device.get_result()\n",
    "            for y in layout:\n",
    "                try:\n",
    "                    if(isinstance(y,LTTextBoxHorizontal)):\n",
    "                        with open('%s'%(new_path),'a',encoding=\"utf-8\") as f:\n",
    "                            f.write(y.get_text()+'\\n')\n",
    "#                             print(\"Success！\")\n",
    "                except:\n",
    "                    print(\"Failed\")\n",
    "\n",
    "#url = \"https://arxiv.org/pdf/2004.11055.pdf\"\n",
    "html = urllib.request.urlopen(urllib.request.Request(url)).read()\n",
    "dataIo = BytesIO(html)\n",
    "OnlinePdfToTxt(dataIo,'txt/'+str(name)+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collaborative Motion Prediction via Neural Motion Message Passing. . Yue Hu1, Siheng Chen2 (cid:0), Ya Zhang1 (cid:0), and Xiao Gu1. . 1 Cooperative Medianet Innovation Center, Shanghai Jiao Tong University. 2 Mitsubishi Electric Research Laboratories. {18671129361, ya zhang, gugu97} @sjtu.edu.cn, schen@merl.com. . 0. 2. 0. 2.  . r. a. . M.  . 4. 1.  .  . ]. . V. C. .. s. c. [.  .  . 1. v. 4. 9. 5. 6. 0. .. 3. 0. 0. 2. :. v. i. X. r. a. . \n",
      "Collaborative Motion Prediction via Neural Motion Message Passing. . Yue   Siheng     Ya     and Xiao   .   Cooperative Medianet Innovation Center, Shanghai Jiao Tong University.   Mitsubishi Electric Research Laboratories.   ya zhang,   @sjtu.edu.cn, schen@merl.com. .         . r. a. . M. .     . . ]. . V. C. .. s. c. [. . .   v.           ..         :. v. i. X. r. a. .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Yue   Siheng', 'Xiao   ', 'V. C.']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def judge(x):\n",
    "    for string in x:\n",
    "        \n",
    "    return bool(re.search(r'\\d', x))\n",
    "\n",
    "def author_extraction(file_name):\n",
    "    file = open('txt/'+str(file_name)+'.txt',encoding=\"ISO-8859-1\")\n",
    "    strings=file.read()\n",
    "    keyStart = ''\n",
    "    keyEnd = 'Abstract'\n",
    "    pat = re.compile(keyStart+'(.*?)'+keyEnd, re.S)\n",
    "    result = pat.findall(strings)\n",
    "#     if len(result)<10:\n",
    "#         return\n",
    "    #print(result)\n",
    "    file.close()\n",
    "    #format processing\n",
    "    txt=''\n",
    "    txt=txt.join(result)\n",
    "    txt=txt.replace('\\n', '. ').replace('\\r', ' ')\n",
    "    print(txt)\n",
    "    rst=[]\n",
    "    tempList=txt.split()\n",
    "    for x in tempList:\n",
    "        if not judge(x):\n",
    "            rst.append(x)\n",
    "        else:\n",
    "            rst.append(' ')\n",
    "    output=' '.join(rst)\n",
    "    print(output)\n",
    "    #using the pretrained model\n",
    "    doc=nlp(output)\n",
    "    ORG_list=[]\n",
    "    #print([(X.text, X.label_) for X in doc.ents])\n",
    "    for X in doc.ents:\n",
    "        if X.label_=='PERSON':\n",
    "            ORG_list.append(X.text)\n",
    "    return ORG_list\n",
    "author_extraction(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = ['xiaohong', '12', 'adf12', '14..', '晓明']\n",
    "\n",
    "\n",
    "judge(\"...12.\")\n",
    "# 对于python3来说同样还可以使用string.isnumeric()方法\n",
    "# for x in L:\n",
    "#     if not x.isdigit():\n",
    "#         print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the generated txt file, first we try to extract the email address by using regular experssion\n",
    "### TODO: Special format such as {aaa, bbb, ccc}@xxx.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nn@eecs.berkeley.edu']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "\n",
    "#open the generated file as input stream and store in the buffer\n",
    "def email_extraction(file_name):\n",
    "    file = open('txt/'+str(file_name)+'.txt',encoding=\"ISO-8859-1\")\n",
    "    strings=file.read()\n",
    "    matches = []\n",
    "    emailRegex = re.compile(r'''(\n",
    "        [a-zA-Z0-9._%+-]+      # username\n",
    "\n",
    "        @                      # @ symbol\n",
    "\n",
    "        [a-zA-Z0-9.-]+        # domain name\n",
    "\n",
    "        (\\.[a-zA-Z]{2,4}){1,2} # dot-something\n",
    "\n",
    "        )''', re.VERBOSE)\n",
    "    # using RE to match all the patterns in the txt\n",
    "    for groups in emailRegex.findall(strings):\n",
    "        matches.append(groups[0])\n",
    "    # reduced the same entities\n",
    "    list2 = list(set(matches))\n",
    "    list_nums = len(list2)\n",
    "    emailList=[]\n",
    "    for line in range(list_nums):\n",
    "        emailList.append(list2[line])\n",
    "    return emailList\n",
    "\n",
    "email_extraction(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly we match the author name list with the email list by token matching method (Levenshtein)\n",
    "### TODO: HOW can we access author list? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Michael Stewart': 'michael.stewart@research.uwa.edu.au',\n",
       " 'Majigsuren Enkhsaikhan': 'majigsuren.enkhsaikhan@research.uwa.edu.au',\n",
       " 'Wei Liu': 'wei.liu@uwa.edu.au'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Levenshtein import *\n",
    "def author_email_matching(nameList, emailList):\n",
    "    if len(emailList)==0:\n",
    "        return\n",
    "    name_email={}\n",
    "    for name in nameList:\n",
    "        highest=0\n",
    "        index=0\n",
    "        for i in range(len(emailList)):\n",
    "            #using re to extract the first part of email\n",
    "            pat = re.compile(''+'(.*?)'+'@', re.S)\n",
    "            email = pat.findall(emailList[i])\n",
    "            #print(jaro(name, str(email)))\n",
    "            #using jaro to calculate the similarity between two strings\n",
    "            if jaro(name, emailList[i])>highest:\n",
    "                index=i\n",
    "                highest=jaro(name, emailList[i])\n",
    "        #set pair with the highest score\n",
    "        name_email[name]=emailList[index]\n",
    "        #If very sure, remove it from the list to reduce the uncertainty for other pairs\n",
    "        if highest>=0.6:\n",
    "            emailList.remove(emailList[index])\n",
    "    return name_email\n",
    "        \n",
    "\n",
    "nameList=['Michael Stewart', 'Majigsuren Enkhsaikhan', 'Wei Liu']\n",
    "emailList=email_extraction(name)\n",
    "author_email_matching(nameList, emailList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After matching email, we move to the section of acknowledgement, which is not certain for every paper. Here we use named entity recognition to classify orgnization which sponsred the study. Here we used Spacy classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Regular expression to locate the paragraph\n",
    "def ORG_recognition(strings):\n",
    "    keyStart = 'Acknowledg+'\n",
    "    keyEnd = 'References'\n",
    "    pat = re.compile(keyStart+'(.*?)'+keyEnd, re.S)\n",
    "    result = pat.findall(strings)\n",
    "    if len(result)<10:\n",
    "        return\n",
    "    #print(result)\n",
    "    file.close()\n",
    "    #format processing\n",
    "    txt=''\n",
    "    txt=txt.join(result)\n",
    "    txt=txt.replace('\\n', '').replace('\\r', '')\n",
    "    #print(txt)\n",
    "    #using the pretrained model\n",
    "    doc=nlp(txt)\n",
    "    ORG_list=[]\n",
    "    print([(X.text, X.label_) for X in doc.ents])\n",
    "    for X in doc.ents:\n",
    "        if X.label_=='ORG':\n",
    "            ORG_list.append(X.text)\n",
    "    return ORG_list\n",
    "ORG_recognition(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
