{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the code, several dependant package need to be installed, skip if it already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3 in /Applications/anaconda3/lib/python3.7/site-packages (1.24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/04/f62d5834c2bdf90afcaeb23bb5241033c44e27000de64ad8472253daa4a8/pdfminer.six-20200402-py3-none-any.whl (5.6MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6MB 5.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet; python_version > \"3.0\" in /Applications/anaconda3/lib/python3.7/site-packages (from pdfminer.six) (3.0.4)\n",
      "Requirement already satisfied: pycryptodome in /Applications/anaconda3/lib/python3.7/site-packages (from pdfminer.six) (3.9.7)\n",
      "Requirement already satisfied: sortedcontainers in /Applications/anaconda3/lib/python3.7/site-packages (from pdfminer.six) (2.1.0)\n",
      "Installing collected packages: pdfminer.six\n",
      "Successfully installed pdfminer.six-20200402\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymysql in /Applications/anaconda3/lib/python3.7/site-packages (0.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we make connection with database with pymysql api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "#https://cloud.tencent.com/developer/article/1395339\n",
    "import urllib\n",
    "import pdfminer\n",
    "import copy\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.layout import *\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from urllib.request import Request\n",
    "from urllib.request import urlopen\n",
    "#input basic info about database\n",
    "class Processing():\n",
    "    def __init__(self):\n",
    "        self.NUM=3\n",
    "        self.conn=pymysql.connect(host='mysql24.ezhostingserver.com', port=3306, user='krw',password='Y>!V@N_26@]cfJ7(')\n",
    "\n",
    "    def connection(self):\n",
    "        query_result=[]\n",
    "        #cursor is the current point we focus on, which could execute sql command.\n",
    "        cursor=self.conn.cursor()\n",
    "        #Here rw is the databse name and training_view is a view for requesting data. The execution should be around 30s.\n",
    "        cursor.execute(\"SELECT * FROM krw.training_paper_view group by id limit %s;\",self.NUM)\n",
    "        for r in cursor:\n",
    "            query_result.append(list(r))\n",
    "        cursor.close()\n",
    "        return query_result\n",
    "    #lst\n",
    "    def pre_processing(self,index,query_result):\n",
    "        resourceId=query_result[index][2]\n",
    "        authorNum=query_result[index][3]\n",
    "        url=query_result[index][7]\n",
    "        #print(resourceId,url)\n",
    "        #name=lst[NUM][6]\n",
    "        webList=[\"acmweb\",\"aclweb\"]\n",
    "        for web in webList:\n",
    "            if web in url:\n",
    "                #pattern = re.compile(ur'^((https|http|ftp|rtsp|mms)?:\\/\\/)[^\\s]+/')\n",
    "                #str = u''\n",
    "                url=url+\".pdf\"\n",
    "            #name=pattern.search(str)\n",
    "        #url=\"https://arxiv.org/pdf/\"+str(name)+\".pdf\"\n",
    "        print(resourceId,url)\n",
    "        return resourceId,url\n",
    "        \n",
    "    def query(self,index, resourceId):\n",
    "        temp=[]\n",
    "        authorList=[]\n",
    "        #Here krw is the databse name and training_view is a view for requesting data. The execution should be around 30s.\n",
    "        #conn=pymysql.connect(host='mysql24.ezhostingserver.com', port=3306, user='krw',password='Y>!V@N_26@]cfJ7(')\n",
    "        cursor=self.conn.cursor()\n",
    "        #Here krw is the databse name and training_view is a view for requesting data. The execution should be around 30s.\n",
    "        cursor.execute(\"SELECT * FROM krw.training_author_view where paper_id=(%s) group by name;\",resourceId)\n",
    "        cursor.close()\n",
    "        authorPaper={}\n",
    "        authorCitation={}\n",
    "        authorID={}\n",
    "        for r in cursor:\n",
    "            temp.append(list(r))\n",
    "        #print(temp)\n",
    "        for line in temp:\n",
    "            authorList.append(line[1])\n",
    "            authorPaper[line[1]]=[line[3]]\n",
    "            authorCitation[line[1]]=[line[4]]\n",
    "            authorID[line[1]]=[line[0]]\n",
    "        #print(authorList)\n",
    "        return authorList,authorPaper,authorCitation,authorID\n",
    "\n",
    "    def onlinePdfToTxt(self,dataIo,new_path):\n",
    "        # Create PDF Parser\n",
    "        parser = PDFParser(dataIo)\n",
    "        # Create PDFDocument\n",
    "        document = PDFDocument(parser)\n",
    "        # Is it okay for extraction?\n",
    "        if not document.is_extractable:\n",
    "            raise PDFTextExtractionNotAllowed\n",
    "        else:\n",
    "            # Create PDF Manager\n",
    "            resmag =PDFResourceManager()\n",
    "            # Setting parameters\n",
    "            laparams=LAParams()\n",
    "            # Createing PDF device\n",
    "            # device=PDFDevice(resmag )\n",
    "            device=PDFPageAggregator(resmag ,laparams=laparams)\n",
    "            # Create PDF explainer\n",
    "            interpreter=PDFPageInterpreter(resmag ,device)\n",
    "            # For each page\n",
    "            for page in PDFPage.create_pages(document):\n",
    "                interpreter.process_page(page)\n",
    "                # accept this page's LTP object\n",
    "                layout=device.get_result()\n",
    "                for y in layout:\n",
    "                    try:\n",
    "                        if(isinstance(y,LTTextBoxHorizontal)):\n",
    "                            with open('%s'%(new_path),'a',encoding=\"utf-8\") as f:\n",
    "                                f.write(y.get_text()+'\\n')\n",
    "                                #print(\"Success！\")\n",
    "                    except:\n",
    "                        print(\"Failed\")\n",
    "                        \n",
    "    def main(self,index):\n",
    "        query_result=self.connection()\n",
    "#         for index in range(len(NUM)):\n",
    "        resourceId,url=self.pre_processing(index,query_result)\n",
    "        html = urllib.request.urlopen(urllib.request.Request(url)).read()\n",
    "        dataIo = BytesIO(html)\n",
    "        self.onlinePdfToTxt(dataIo,'txt/'+str(resourceId)+'.txt')\n",
    "        authorList,authorPaper,authorCitation,authorID=self.query(index,resourceId)\n",
    "        return resourceId,authorList,authorPaper,authorCitation,authorID\n",
    "    \n",
    "# processing=Processing()\n",
    "# processing.NUM=5\n",
    "# resourceId,authorList,authorPaper,authorCitation,authorID=processing.main(4)\n",
    "# print(authorList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After excution we obtained a 2-dimensional matrix with each row represents a paper. Next, we constract a url based on arXiv id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By following command we could access pdf by url and convert pdf 2 txt file and store at local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import copy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from Levenshtein import *\n",
    "#nlp = en_core_web_sm.load()\n",
    "class AuthorInfoExtract():\n",
    "    STARS=[\"*\",'∗']\n",
    "    CROSS=[\"†\",\"‡\"]\n",
    "    def detectCapitalUse(self, word: str) -> bool:\n",
    "        \n",
    "        is_upper = [c.isupper() for c in word]          # 大写字符判别列表\n",
    "        is_lower = [c.islower() for c in word]          # 小写字符判别列表\n",
    "        \n",
    "#         if all(is_upper) or all(is_lower):              # 如果所有字符都是大写或小写\n",
    "#             return True                         \n",
    "\n",
    "        if any(is_upper):                               # 如果既有大写又有小写\n",
    "            return is_upper[0] and all(is_lower[1:])    # 要求第一个大写其他都小写\n",
    "        \n",
    "    def locateContext(self,resourceId):\n",
    "        file = open('txt/'+str(resourceId)+'.txt',encoding=\"utf-8\")\n",
    "        strings=file.read()\n",
    "        symbolList=[\"Abstract\",'Abstract.','ABSTRACT','ABSTRACT.']\n",
    "        for symbol in symbolList:\n",
    "            #if name in strings:\n",
    "            if strings.find(symbol):\n",
    "                output=strings[0:strings.find(symbol)]\n",
    "                break\n",
    "            else:\n",
    "                print(symbol+\" not found!\")\n",
    "                output==strings[0:3000]\n",
    "        file.close()\n",
    "        output=output.replace('\\n', ' . ').replace('\\r', ' ')\n",
    "        output=output.split(\" \")\n",
    "        return output\n",
    "#     def judge(x):\n",
    "#         return bool(re.search(r'\\d', x))\n",
    "\n",
    "    def symbolJudgement(self,resourceId):\n",
    "        file = open('txt/'+str(resourceId)+'.txt',encoding=\"utf-8\")\n",
    "        strings=file.read()\n",
    "        strings=strings[:3000]\n",
    "        #print(strings)\n",
    "        if strings.find(\"equally\"):\n",
    "            pos= strings.find(\"equally\")\n",
    "            #print(pos)\n",
    "            for star in self.STARS:\n",
    "                if star in strings[pos-50:pos+50]:\n",
    "                    return 1 #star represent equall \n",
    "        if strings.find(\"Corresponding\"):\n",
    "            pos= strings.find(\"Corresponding\")\n",
    "            for star in self.STARS:\n",
    "                if star in strings[pos-50:pos+50]:\n",
    "                    return 2 #star represent correspinding \n",
    "        if \"Alphabetical\" in strings:\n",
    "            return 3\n",
    "        return 0\n",
    "    \n",
    "    def matchAuthor(self,roleList,authorList):\n",
    "        author_role=[]\n",
    "        for role in roleList:\n",
    "            highest=0\n",
    "            index=0\n",
    "            for i in range(len(authorList)):\n",
    "                if jaro(role, authorList[i])>highest:\n",
    "                    index=i\n",
    "                    highest=jaro(role, authorList[i])\n",
    "            #print(highest,authorList[index])\n",
    "            if highest>0.7:\n",
    "                author_role.append(authorList[index])\n",
    "        return author_role\n",
    "    \n",
    "    def specialAuthors(self,output,authorList):\n",
    "        starList=[]\n",
    "        crossList=[]\n",
    "        for i in range(len(output)):\n",
    "            for star in self.STARS:\n",
    "                if star in output[i]:\n",
    "                    starList.append(\" \".join(output[i-2:i+1]))\n",
    "            for cross in self.CROSS:\n",
    "                if cross in output[i]:\n",
    "                    crossList.append(\" \".join(output[i-2:i+1]))\n",
    "        judge=self.symbolJudgement(resourceId)\n",
    "        #print(judge)\n",
    "        if judge==0 or judge==1:\n",
    "            star=self.matchAuthor(starList,authorList)\n",
    "            corre=self.matchAuthor(crossList,authorList) \n",
    "        elif judge==2:\n",
    "            star=self.matchAuthor(starList,authorList)\n",
    "            corre=copy.copy(star)\n",
    "            star=[]\n",
    "        elif judge==3:\n",
    "            star=[]\n",
    "            corre=[]\n",
    "        #print(star)\n",
    "        return star,corre\n",
    "\n",
    "    def sequenceExtratcor(self,output,authorList,parameter=0):\n",
    "        author_sequence={}\n",
    "        author_sequence_list=[]\n",
    "        authorListCopy=copy.copy(authorList)\n",
    "        num=1\n",
    "        for i in range(len(output)):\n",
    "            for name in authorListCopy:\n",
    "                if output[i] in name and len(output[i])>=2 and self.detectCapitalUse(output[i]):\n",
    "                    author_sequence[name]=num\n",
    "                    author_sequence_list.append(name)\n",
    "                    num+=1\n",
    "                    authorListCopy.remove(name)\n",
    "                    break\n",
    "        if len(author_sequence_list)==len(authorList):\n",
    "            authorList=author_sequence_list\n",
    "        if parameter==0:\n",
    "            return author_sequence\n",
    "        else:\n",
    "            return authorList\n",
    "    \n",
    "    def main(self,resourceId,authorList,authorPaper,authorCitation,authorID):\n",
    "        extractor=AuthorInfoExtract()\n",
    "        \n",
    "        output=extractor.locateContext(resourceId)\n",
    "        #print(output)\n",
    "        author_sequence=extractor.sequenceExtratcor(output,authorList)\n",
    "        print(author_sequence)\n",
    "        star,corre=extractor.specialAuthors(output,authorList)\n",
    "        #print(star)\n",
    "#         try:\n",
    "        df=pd.DataFrame.from_dict(author_sequence,orient='index',columns=['sequence'])\n",
    "        df=df.reset_index().rename(columns={'index':'name'})\n",
    "        df['star']=0\n",
    "        for index, row in df.iterrows():\n",
    "            for author in star:\n",
    "                if row[\"name\"]==author:\n",
    "                    df.loc[index,\"star\"] = 1\n",
    "        df['corre']=0\n",
    "        for index, row in df.iterrows():\n",
    "            for author in corre:\n",
    "                #print(row[\"name\"],author,index)\n",
    "                if row[\"name\"]==author:\n",
    "                    df.loc[index,\"corre\"] = 1\n",
    "        df['paperCount']=0\n",
    "        for index, row in df.iterrows():\n",
    "            for k,v in authorPaper.items():\n",
    "                #print(row[\"name\"],author,index)\n",
    "                if row[\"name\"]==k:\n",
    "                    df.loc[index,\"paperCount\"] = v\n",
    "\n",
    "        df['citationCount']=0\n",
    "        for index, row in df.iterrows():\n",
    "            for k,v in authorCitation.items():\n",
    "                #print(row[\"name\"],author,index)\n",
    "                if row[\"name\"]==k:\n",
    "                    df.loc[index,\"citationCount\"] = v\n",
    "        df['authorID']=0\n",
    "        for index, row in df.iterrows():\n",
    "            for k,v in authorID.items():\n",
    "                #print(row[\"name\"],author,index)\n",
    "                if row[\"name\"]==k:\n",
    "                    df.loc[index,\"authorID\"] = v\n",
    "#         except:\n",
    "#             print(\"error occured!\")\n",
    "#             df=pd.DataFrame(columns=['name', 'sequence', 'star', 'corre'])\n",
    "        return df\n",
    "    \n",
    "# extractor=AuthorInfoExtract()\n",
    "# annotation=extractor.main()\n",
    "# annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def softmax(x):\n",
    "#     return np.exp(x)/np.sum(np.exp(x),axis=0)\n",
    "def lookup_id(author,df):\n",
    "    author_id=\"\"\n",
    "#     for index, row in df.iterrows():\n",
    "    for index, row in df.iterrows():\n",
    "        #print(index)\n",
    "        #print(k1,v2)\n",
    "        if author==row[\"name\"]:\n",
    "            author_id=(df.at[index,\"authorID\"])\n",
    "            break\n",
    "    return author_id\n",
    "def roleGenerator(df,resourceId):\n",
    "    ROLES=['Writing_Draft','Writing_Review_Editing','Supervision','Resource','Software']\n",
    "    LEVELS=[10,50,100,200,1000,3000,5000,10000]\n",
    "    df_role = pd.DataFrame(0.0, index=df.name, columns=ROLES)\n",
    "    #sieve1:Sequence\n",
    "    for index in range(len(df)):\n",
    "        df_role.loc[df.name[index],\"Writing_Draft\"] =1/df.sequence[index]\n",
    "        df_role.loc[df.name[index],\"Writing_Review_Editing\"] =(df.sequence[index]/len(df.sequence))-0.1\n",
    "    #Last author is likely to be supervisioner\n",
    "    \n",
    "    #sieve2:Annotation\n",
    "    for index in range(len(df)):\n",
    "        if df.star[index]==1:\n",
    "            df_role.loc[df.name[index],\"Writing_Draft\"] +=1\n",
    "        if (len(df.star[df.star>0]))<=2:\n",
    "            if df.corre[index]==1:\n",
    "                df_role.loc[df.name[index],\"Writing_Review_Editing\"] +=1\n",
    "                df_role.loc[df.name[index],\"Supervision\"] +=0.5\n",
    "                #df_role.loc[df.name[index],\"Supervision\"] +=0.5\n",
    "            \n",
    "    #sieve3:Citation and Paper\n",
    "    for index in range(len(df)):\n",
    "        df_role.Supervision[index]=df.citationCount[index]*0.2/sum(df.citationCount)+df.paperCount[index]*0.8/sum(df.paperCount)\n",
    "#         print(v)\n",
    "    for index in range(len(df)):\n",
    "        num=0.0\n",
    "        for level in LEVELS:\n",
    "            if df.citationCount[index]>level:\n",
    "                num+=0.125\n",
    "            if df.paperCount[index]>level:\n",
    "                num+=0.125\n",
    "        df_role.Resource[index]=num\n",
    "        df_role.Writing_Review_Editing[index]+=num/5\n",
    "        \n",
    "    author_role={}\n",
    "    paper_role={}\n",
    "    result_1=[]\n",
    "    result_2=[]\n",
    "    try:\n",
    "        supervisionMax=max(df_role.Supervision)\n",
    "       # print(supervisionMax)\n",
    "        for index in range(len(df_role)):\n",
    "            author_role={}\n",
    "            author_id=lookup_id(df.name[index],df)\n",
    "            if df_role.loc[df.name[index],\"Writing_Draft\"]>=1:\n",
    "                author_role[author_id] =\"Writing_Draft\"#isContributionByAuthor\n",
    "                paper_role[resourceId]=\"Writing_Draft\"#isContributionToPaper\n",
    "            if df_role.loc[df.name[index],\"Writing_Review_Editing\"]>=1:\n",
    "                author_role[author_id] =\"Writing_Review_Editing\"\n",
    "                paper_role[resourceId]=\"Writing_Review_Editing\"\n",
    "            if df_role.loc[df.name[index],\"Resource\"]>=1:\n",
    "                author_role[author_id] =\"Resource\"\n",
    "                paper_role[resourceId]=\"Resource\"\n",
    "            if df_role.loc[df.name[index],\"Supervision\"]==supervisionMax:\n",
    "                author_role[author_id] =\"Supervision\"\n",
    "                paper_role[resourceId]=\"Supervision\"\n",
    "            if author_role!={}:\n",
    "                result_1.append(author_role)\n",
    "                result_2.append(paper_role)\n",
    "    except:\n",
    "        print(\"fail to generate roles\")\n",
    "#def roleClassification(df):\n",
    "    #print(df_role)\n",
    "    return result_1,result_2\n",
    "#     for index, row in df_role.iterrows():\n",
    "#         for k,v in df.sequence\n",
    "#             #print(row[\"name\"],author,index)\n",
    "#             if row[\"name\"]==k:\n",
    "#                 df_role.loc[index,\"citationCount\"] = v\n",
    "#roleGenerator(annotation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the generated txt file, first we try to extract the email address by using regular experssion\n",
    "### TODO: Special format such as {aaa, bbb, ccc}@xxx.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly we match the author name list with the email list by token matching method (Levenshtein)\n",
    "### TODO: HOW can we access author list? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import *\n",
    "import re\n",
    "class EmailInfoExtract():\n",
    "    def __init__(self):\n",
    "        self.name_email={}\n",
    "        self.result=[]\n",
    "    def get_alpha_str(self,s):\n",
    "        result = ''.join([x for x in s if x.isalpha()])\n",
    "        return result\n",
    "\n",
    "    def email_extraction(self,file_name):\n",
    "        file = open('txt/'+str(file_name)+'.txt',encoding=\"utf-8\")\n",
    "        strings=file.read()\n",
    "        #First page's txt\n",
    "        strings=strings[:3000]\n",
    "        matches = []\n",
    "        matchesGroup=[]\n",
    "        #1. singular email address\n",
    "        emailRegex = re.compile(r'''(\n",
    "            [a-zA-Z0-9._%+-]+      # username\n",
    "            @+                     # @ symbol\n",
    "\n",
    "           [a-zA-Z0-9_-]+\n",
    "           (\\.[a-zA-Z0-9_-]+)+\n",
    "            )''', re.VERBOSE)\n",
    "        # using RE to match all the patterns in the txt\n",
    "        for groups in emailRegex.findall(strings):\n",
    "            matches.append(groups[0])\n",
    "\n",
    "        list2 = matches\n",
    "        list_nums = len(list2)\n",
    "        emailList=[]\n",
    "        for line in range(list_nums):\n",
    "            emailList.append(list2[line])\n",
    "        #2. grouped email list {}\n",
    "        emailRegex2 = re.compile(r'''(\n",
    "        \\{(.+?)\\}+\n",
    "        @                      # @ symbol\n",
    "\n",
    "        [a-zA-Z0-9.-]+        # domain name\n",
    "\n",
    "        (\\.[a-zA-Z]{1,4}){1,2} # dot-something\n",
    "        )''', re.VERBOSE)\n",
    "        for groups in emailRegex2.findall(strings):\n",
    "            #print(groups)\n",
    "            matchesGroup.append(groups[0])\n",
    "        list3 = matchesGroup\n",
    "        if len(matchesGroup)>0:\n",
    "            for gp in list3:\n",
    "                stringGroup = gp\n",
    "                index=stringGroup.find(\"@\", 0)\n",
    "                prefix=stringGroup[:index]\n",
    "                suffix=stringGroup[index:]\n",
    "                tempNameList=prefix[1:-1].split(\",\");\n",
    "                for name in tempNameList:\n",
    "                    emailList.append(name+suffix)\n",
    "                #print(emailList)\n",
    "        #3. convert to sequenced list if possible \n",
    "        sequencedList=[]\n",
    "        temp_dict={}\n",
    "        for mail in emailList:\n",
    "            index=mail.find(\"@\",0)\n",
    "            prefix=mail[:index]\n",
    "            email_index=strings.find(prefix)\n",
    "            temp_dict[mail]=email_index\n",
    "        sort_idct=sorted(temp_dict.items(),key = lambda x:x[1],reverse = False)\n",
    "        for k,v in sort_idct:\n",
    "            sequencedList.append(k)\n",
    "        #print((sequencedList))\n",
    "\n",
    "        if len(sequencedList)==len(emailList):\n",
    "            return sequencedList\n",
    "        else:\n",
    "            return emailList\n",
    "    def lookup_id(self,author,df):\n",
    "        author_id=\"\"\n",
    "    #     for index, row in df.iterrows():\n",
    "        for index, row in df.iterrows():\n",
    "            #print(index)\n",
    "            #print(k1,v2)\n",
    "            if author==row[\"name\"]:\n",
    "                author_id=(df.at[index,\"authorID\"])\n",
    "                break\n",
    "        return author_id\n",
    "\n",
    "    def order_matching(self,nameList, emailList):   \n",
    "        print(\"ORDER MATCHING\")\n",
    "        for i in range(len(nameList)):\n",
    "            pat = re.compile(''+'(.*?)'+'@', re.S)\n",
    "            email = pat.findall(emailList[i])\n",
    "            email=self.get_alpha_str(email)\n",
    "            name=nameList[i]\n",
    "            if (jaro(name, str(email)))<0.4:\n",
    "                return False\n",
    "            else:\n",
    "                self.name_email[name]=email\n",
    "        return name_email\n",
    "\n",
    "    #2.No macthes, need to group based on experience and naive approach\n",
    "    def naive_matching(self,nameList, emailList):\n",
    "        print(\"NAIVE MATCHING\")\n",
    "        for name in nameList:\n",
    "            highest=0\n",
    "            index=0\n",
    "            for i in range(len(emailList)):\n",
    "                #name=filter(lambda x: x.isalpha(), name)\n",
    "                #using re to extract the first part of email\n",
    "                pat = re.compile(''+'(.*?)'+'@', re.S)\n",
    "                email = pat.findall(emailList[i])\n",
    "                email=self.get_alpha_str(email)\n",
    "                #print(str(email))\n",
    "                #print(jaro(name, str(email)))\n",
    "                #using jaro to calculate the similarity between two strings\n",
    "                if jaro(name, str(email))>highest:\n",
    "                    index=i\n",
    "                    highest=jaro(name, str(email))\n",
    "            #set pair with the highest score\n",
    "                    #print(name,highest,emailList[i])\n",
    "            #self.name_email[name]=emailList[index]\n",
    "            #If very sure, remove it from the list to reduce the uncertainty for other pairs\n",
    "            if highest>=0.6:\n",
    "                self.name_email[name]=emailList[index]\n",
    "                emailList.remove(emailList[index])\n",
    "            if highest<=0.4:\n",
    "                self.name_email[name]=''\n",
    "        for email in emailList:\n",
    "            if email not in self.name_email:\n",
    "                for name in self.name_email:\n",
    "                    if self.name_email[name]=='':\n",
    "                        self.name_email[name]=email\n",
    "                        #print(jaro(name, str(email)))\n",
    "        return self.name_email\n",
    "    \n",
    "    def main(self,resourceId,authorList,df):\n",
    "        extractor=AuthorInfoExtract()\n",
    "        output=extractor.locateContext(resourceId)\n",
    "        nameList=extractor.sequenceExtratcor(output,authorList,1)\n",
    "        emailList=self.email_extraction(resourceId)\n",
    "        #print(emailList)\n",
    "        if len(emailList)==0:\n",
    "            return\n",
    "        name_email={}\n",
    "        #1.If sequenced list and length matches\n",
    "        if len(nameList)==len(emailList):\n",
    "            self.name_email=self.order_matching(nameList, emailList)\n",
    "        if not name_email:\n",
    "            self.name_email={}\n",
    "            self.name_email=self.naive_matching(nameList, emailList)\n",
    "        \n",
    "        for k,v in self.name_email.items():\n",
    "            author_id=self.lookup_id(k,df)\n",
    "            #print(author_id)\n",
    "            temp={}\n",
    "            temp[author_id]=v\n",
    "            self.result.append(temp)\n",
    "        return self.result\n",
    "        #return self.name_email\n",
    "    \n",
    "# emailExtractor=EmailInfoExtract()\n",
    "# name_email=emailExtractor.main()\n",
    "\n",
    "# name_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, ConjunctiveGraph, Literal, BNode, Namespace, RDF, URIRef,RDFS\n",
    "from rdflib.namespace import DC, FOAF\n",
    "from owlrl import DeductiveClosure, RDFS_Semantics\n",
    "import pprint\n",
    "def create_triple_email(totalList):\n",
    "    g = Graph()\n",
    "    for lst in totalList:\n",
    "        if lst!=None:\n",
    "            for dic in lst:\n",
    "                for author_id,v1 in dic.items():  \n",
    "                    #author = BNode()\n",
    "#                     print(k1,v1)\n",
    "#                     author_id=lookup_id(k1,df)\n",
    "#                     print(author_id)\n",
    "                    author = URIRef(\"http://www.scoaring.com/ontology#%s\")%author_id\n",
    "                    #print(author)\n",
    "                    g.add((author, FOAF.mbox, Literal(v1)))\n",
    "    print(len(g))\n",
    "    #print(g.serialize(format='turtle').decode())\n",
    "    #g.serialize(destination='output.txt', format='turtle')\n",
    "    \n",
    "def create_triple_role(totalList):\n",
    "    g = Graph()\n",
    "    for lst in totalList:\n",
    "        if lst!=None:\n",
    "            for dic in lst:\n",
    "                for author_id,v1 in dic.items():\n",
    "                    author = URIRef(\"http://www.scoaring.com/ontology#%s\")%author_id\n",
    "                    predict= URIRef(http://www.scoaring.com/ontology)#refersTo\n",
    "                    g.add((author, FOAF.mbox, Literal(v1)))\n",
    "create_triple(totalList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1156351 http://arxiv.org/pdf/1710.01269v1.pdf\n",
      "{'Christian S. Perone': 1, 'Evan Calabrese': 2, 'Julien Cohen-Adad': 3}\n",
      "P1156365 http://arxiv.org/pdf/1803.03095v1.pdf\n",
      "{'Xialei Liu': 1, 'Joost van de Weijer': 2, 'Andrew D. Bagdanov': 3}\n",
      "ORDER MATCHING\n",
      "NAIVE MATCHING\n",
      "P1156386 http://arxiv.org/pdf/1706.03686v3.pdf\n",
      "{'Kang Han': 1, 'Wanggen Wan': 2, 'Haiyan Yao': 3, 'Li Hou': 4}\n",
      "NAIVE MATCHING\n",
      "P1156541 https://arxiv.org/pdf/1806.06371v2.pdf\n",
      "{'Lisa Beinborn': 1, 'Teresa Botschen': 2, 'Iryna Gurevych': 3}\n",
      "P1156614 http://arxiv.org/pdf/1709.04496v2.pdf\n",
      "{'Christian F. Baumgartner': 1, 'Lisa M. Koch': 2, 'Marc Pollefeys': 3, 'Ender Konukoglu': 4}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    processing=Processing()\n",
    "    processing.NUM=5\n",
    "    email=[]\n",
    "    role_1=[]\n",
    "    role_2=[]\n",
    "    for i in range(processing.NUM):\n",
    "        try:\n",
    "            resourceId,authorList,authorPaper,authorCitation,authorID=processing.main(i)\n",
    "            #print(authorList)\n",
    "            extractor=AuthorInfoExtract()\n",
    "            annotation=extractor.main(resourceId,authorList,authorPaper,authorCitation,authorID)\n",
    "            #print(annotation)\n",
    "            author_role,paper_role=roleGenerator(annotation,resourceId)\n",
    "            role_1.append(author_role)\n",
    "            role_2.append(paper_role)\n",
    "            #print(author_role)\n",
    "            emailExtractor=EmailInfoExtract()\n",
    "            name_email=emailExtractor.main(resourceId,authorList,annotation)\n",
    "            email.append(name_email)\n",
    "        except:\n",
    "            continue\n",
    "    file = open(\"email_output2.txt\", \"w\")\n",
    "    file.write(str(email))\n",
    "    file = open(\"role_output2.txt\", \"w\")\n",
    "    file.write(str(role_1))\n",
    "    file.write(str(role_2))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-84-d5c2340c4b2d>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-84-d5c2340c4b2d>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    g.add( (n., RDF.type, Literal(\"Author\")) )\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
