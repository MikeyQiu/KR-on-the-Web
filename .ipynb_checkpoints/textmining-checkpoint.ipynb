{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the code, several dependant package need to be installed, skip if it already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3 in /Applications/anaconda3/lib/python3.7/site-packages (1.24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/04/f62d5834c2bdf90afcaeb23bb5241033c44e27000de64ad8472253daa4a8/pdfminer.six-20200402-py3-none-any.whl (5.6MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6MB 5.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet; python_version > \"3.0\" in /Applications/anaconda3/lib/python3.7/site-packages (from pdfminer.six) (3.0.4)\n",
      "Requirement already satisfied: pycryptodome in /Applications/anaconda3/lib/python3.7/site-packages (from pdfminer.six) (3.9.7)\n",
      "Requirement already satisfied: sortedcontainers in /Applications/anaconda3/lib/python3.7/site-packages (from pdfminer.six) (2.1.0)\n",
      "Installing collected packages: pdfminer.six\n",
      "Successfully installed pdfminer.six-20200402\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymysql in /Applications/anaconda3/lib/python3.7/site-packages (0.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we make connection with database with pymysql api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymysql\n",
    "#input basic info about database\n",
    "conn=pymysql.connect(host='mysql24.ezhostingserver.com', port=3306, user='krw',password='Y>!V@N_26@]cfJ7(')\n",
    "#cursor is the current point we focus on, which could execute sql command.\n",
    "cursor=conn.cursor()\n",
    "lst=[]\n",
    "#Here krw is the databse name and training_view is a view for requesting data. The execution should be around 30s.\n",
    "cursor.execute(\"SELECT * FROM krw.training_paper_view limit 100;\")\n",
    "for r in cursor:\n",
    "    lst.append(list(r))\n",
    "cursor.fetchall()\n",
    "#lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After excution we obtained a 2-dimensional matrix with each row represents a paper. Next, we constract a url based on arXiv id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1214641 http://arxiv.org/pdf/1302.4922v4.pdf\n"
     ]
    }
   ],
   "source": [
    "##### NUM=25\n",
    "NUM=82\n",
    "\n",
    "resourceId=lst[NUM][0]\n",
    "authorNum=lst[NUM][3]\n",
    "url=lst[NUM][5]\n",
    "#name=lst[NUM][6]\n",
    "if \"acmweb\" in url:\n",
    "    #pattern = re.compile(ur'^((https|http|ftp|rtsp|mms)?:\\/\\/)[^\\s]+/')\n",
    "    #str = u''\n",
    "    url=url+\".pdf\"\n",
    "    #name=pattern.search(str)\n",
    "#url=\"https://arxiv.org/pdf/\"+str(name)+\".pdf\"\n",
    "print(resourceId,url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['David Duvenaud',\n",
       " 'James Robert Lloyd',\n",
       " 'Joshua B. Tenenbaum',\n",
       " 'Roger Grosse',\n",
       " 'Zoubin Ghahramani']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authorList0=[]\n",
    "authorList=[]\n",
    "#Here krw is the databse name and training_view is a view for requesting data. The execution should be around 30s.\n",
    "#conn=pymysql.connect(host='mysql24.ezhostingserver.com', port=3306, user='krw',password='Y>!V@N_26@]cfJ7(')\n",
    "cursor=conn.cursor()\n",
    "#Here krw is the databse name and training_view is a view for requesting data. The execution should be around 30s.\n",
    "cursor.execute(\"SELECT name FROM krw.training_author_view where paper_id=(%s) group by name;\",resourceId)\n",
    "for r in cursor:\n",
    "    authorList0.append(list(r))\n",
    "for name in authorList0:\n",
    "    authorList.append(name[0])\n",
    "authorList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By following command we could access pdf by url and convert pdf 2 txt file and store at local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/pdf/1302.4922v4.pdf\n"
     ]
    }
   ],
   "source": [
    "#https://cloud.tencent.com/developer/article/1395339\n",
    "import urllib\n",
    "import pdfminer\n",
    "import copy\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.layout import *\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from urllib.request import Request\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def OnlinePdfToTxt(dataIo,new_path):\n",
    "    # Create PDF Parser\n",
    "    parser = PDFParser(dataIo)\n",
    "    # Create PDFDocument\n",
    "    document = PDFDocument(parser)\n",
    "    # Is it okay for extraction?\n",
    "    if not document.is_extractable:\n",
    "        raise PDFTextExtractionNotAllowed\n",
    "    else:\n",
    "        # Create PDF Manager\n",
    "        resmag =PDFResourceManager()\n",
    "        # Setting parameters\n",
    "        laparams=LAParams()\n",
    "        # Createing PDF device\n",
    "        # device=PDFDevice(resmag )\n",
    "        device=PDFPageAggregator(resmag ,laparams=laparams)\n",
    "        # Create PDF explainer\n",
    "        interpreter=PDFPageInterpreter(resmag ,device)\n",
    "        # For each page\n",
    "        for page in PDFPage.create_pages(document):\n",
    "            interpreter.process_page(page)\n",
    "            # accept this page's LTP object\n",
    "            layout=device.get_result()\n",
    "            for y in layout:\n",
    "                try:\n",
    "                    if(isinstance(y,LTTextBoxHorizontal)):\n",
    "                        with open('%s'%(new_path),'a',encoding=\"utf-8\") as f:\n",
    "                            f.write(y.get_text()+'\\n')\n",
    "                            #print(\"Success！\")\n",
    "                except:\n",
    "                    print(\"Failed\")\n",
    "\n",
    "#url = \"https://arxiv.org/pdf/2004.11055.pdf\"\n",
    "print(url)\n",
    "html = urllib.request.urlopen(urllib.request.Request(url)).read()\n",
    "dataIo = BytesIO(html)\n",
    "OnlinePdfToTxt(dataIo,'txt/'+str(resourceId)+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['. David Duvenaud∗†', 'James Robert Lloyd∗†']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>sequence</th>\n",
       "      <th>star</th>\n",
       "      <th>corre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>David Duvenaud</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>James Robert Lloyd</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Roger Grosse</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Joshua B. Tenenbaum</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Zoubin Ghahramani</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  sequence  star  corre\n",
       "0       David Duvenaud         1     1      1\n",
       "1   James Robert Lloyd         2     1      1\n",
       "2         Roger Grosse         3     0      1\n",
       "3  Joshua B. Tenenbaum         4     0      1\n",
       "4    Zoubin Ghahramani         5     0      1"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import copy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from Levenshtein import *\n",
    "#nlp = en_core_web_sm.load()\n",
    "class AuthorInfoExtract():\n",
    "    STARS=[\"*\",'∗']\n",
    "    CROSS=[\"†\",\"‡\"]\n",
    "    def detectCapitalUse(self, word: str) -> bool:\n",
    "        \n",
    "        is_upper = [c.isupper() for c in word]          # 大写字符判别列表\n",
    "        is_lower = [c.islower() for c in word]          # 小写字符判别列表\n",
    "        \n",
    "#         if all(is_upper) or all(is_lower):              # 如果所有字符都是大写或小写\n",
    "#             return True                         \n",
    "\n",
    "        if any(is_upper):                               # 如果既有大写又有小写\n",
    "            return is_upper[0] and all(is_lower[1:])    # 要求第一个大写其他都小写\n",
    "        \n",
    "    def locateContext(self,resourceId):\n",
    "        file = open('txt/'+str(resourceId)+'.txt',encoding=\"utf-8\")\n",
    "        strings=file.read()\n",
    "        symbolList=[\"Abstract\",'Abstract.','ABSTRACT','ABSTRACT.']\n",
    "        for symbol in symbolList:\n",
    "            #if name in strings:\n",
    "            if strings.find(symbol):\n",
    "                output=strings[0:strings.find(symbol)]\n",
    "                break\n",
    "            else:\n",
    "                print(symbol+\" not found!\")\n",
    "                output==strings[0:500]\n",
    "        file.close()\n",
    "        output=output.replace('\\n', ' . ').replace('\\r', ' ')\n",
    "        output=output.split(\" \")\n",
    "        return output\n",
    "#     def judge(x):\n",
    "#         return bool(re.search(r'\\d', x))\n",
    "    def symbolJudgement(self,resourceId):\n",
    "        file = open('txt/'+str(resourceId)+'.txt',encoding=\"utf-8\")\n",
    "        strings=file.read()\n",
    "        strings=strings[:2000]\n",
    "        #print(strings)\n",
    "        if strings.find(\"equal\"):\n",
    "            pos= strings.find(\"equal\")\n",
    "            for star in self.STARS:\n",
    "                if star in strings[pos-50:pos+50]:\n",
    "                    return 1 #star represent equall \n",
    "        if strings.find(\"Corresponding\"):\n",
    "            pos= strings.find(\"Corresponding\")\n",
    "            for star in self.STARS:\n",
    "                if star in strings[pos-50:pos+50]:\n",
    "                    return 2 #star represent equall \n",
    "        if \"Alphabetical\" in strings:\n",
    "            return 3\n",
    "        return 0\n",
    "    def matchAuthor(self,roleList,authorList):\n",
    "        author_role=[]\n",
    "        for role in roleList:\n",
    "            highest=0\n",
    "            index=0\n",
    "            for i in range(len(authorList)):\n",
    "                if jaro(role, authorList[i])>highest:\n",
    "                    index=i\n",
    "                    highest=jaro(role, authorList[i])\n",
    "            #print(highest,authorList[index])\n",
    "            if highest>0.8:\n",
    "                author_role.append(authorList[index])\n",
    "        return author_role\n",
    "    \n",
    "    def specialAuthors(self,output):\n",
    "        starList=[]\n",
    "        crossList=[]\n",
    "        for i in range(len(output)):\n",
    "            for star in self.STARS:\n",
    "                if star in output[i]:\n",
    "                    starList.append(\" \".join(output[i-2:i+1]))\n",
    "            for cross in self.CROSS:\n",
    "                if cross in output[i]:\n",
    "                    crossList.append(\" \".join(output[i-2:i+1]))\n",
    "        judge=self.symbolJudgement(resourceId)\n",
    "        print(judge)\n",
    "        if judge==0 or judge==1:\n",
    "            star=self.matchAuthor(starList,authorList)\n",
    "            corre=self.matchAuthor(crossList,authorList) \n",
    "        elif judge==2:\n",
    "            star=self.matchAuthor(starList,authorList)\n",
    "            corre=copy.copy(star)\n",
    "            star=[]\n",
    "        elif judge==3:\n",
    "            star=[]\n",
    "            corre=[]\n",
    "        print(starList)\n",
    "        return star,corre\n",
    "\n",
    "    def sequenceExtratcor(self,output,authorList,parameter=0):\n",
    "        author_sequence={}\n",
    "        author_sequence_list=[]\n",
    "        #print(authorList)\n",
    "        authorListCopy=copy.copy(authorList)\n",
    "        num=1\n",
    "        for i in range(len(output)):\n",
    "            for name in authorListCopy:\n",
    "                if output[i] in name and len(output[i])>=2 and self.detectCapitalUse(output[i]):\n",
    "                    #print(output[i],name)\n",
    "                    #print(output[i])\n",
    "                    author_sequence[name]=num\n",
    "                    author_sequence_list.append(name)\n",
    "                    num+=1\n",
    "                    authorListCopy.remove(name)\n",
    "                    break\n",
    "        if len(author_sequence_list)==len(authorList):\n",
    "            authorList=author_sequence_list\n",
    "        if parameter==0:\n",
    "            return author_sequence\n",
    "        else:\n",
    "            return authorList\n",
    "    \n",
    "    def main(self):\n",
    "        extractor=AuthorInfoExtract()\n",
    "        \n",
    "        output=extractor.locateContext(resourceId)\n",
    "        #print(output)\n",
    "        author_sequence=extractor.sequenceExtratcor(output,authorList)\n",
    "        #print(sequenced_author)\n",
    "        star,corre=extractor.specialAuthors(output)\n",
    "        try:\n",
    "            df=pd.DataFrame.from_dict(author_sequence,orient='index',columns=['sequence'])\n",
    "            df=df.reset_index().rename(columns={'index':'name'})\n",
    "            df['star']=0\n",
    "            for index, row in df.iterrows():\n",
    "                for author in star:\n",
    "                    if row[\"name\"]==author:\n",
    "                        df.loc[index,\"star\"] = 1\n",
    "            df['corre']=0\n",
    "            for index, row in df.iterrows():\n",
    "                for author in corre:\n",
    "                    #print(row[\"name\"],author,index)\n",
    "                    if row[\"name\"]==author:\n",
    "                        df.loc[index,\"corre\"] = 1\n",
    "        except:\n",
    "            print(\"error occured!\")\n",
    "            df=pd.DataFrame(columns=['name', 'sequence', 'star', 'corre'])\n",
    "        return df\n",
    "\n",
    "extractor=AuthorInfoExtract()\n",
    "extractor.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Daniel Sedra', 'Gao Huang', 'Kilian Weinberger', 'Yu Sun', 'Zhuang Liu']\n",
      "Gao Huang Gao Huang 0\n",
      "Gao Huang Yu Sun 0\n",
      "Yu Sun Gao Huang 1\n",
      "Yu Sun Yu Sun 1\n",
      "Zhuang Liu Gao Huang 2\n",
      "Zhuang Liu Yu Sun 2\n",
      "Daniel Sedra Gao Huang 3\n",
      "Daniel Sedra Yu Sun 3\n",
      "Kilian Weinberger Gao Huang 4\n",
      "Kilian Weinberger Yu Sun 4\n",
      "Gao Huang Zhuang Liu 0\n",
      "Yu Sun Zhuang Liu 1\n",
      "Zhuang Liu Zhuang Liu 2\n",
      "Daniel Sedra Zhuang Liu 3\n",
      "Kilian Weinberger Zhuang Liu 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>sequence</th>\n",
       "      <th>star</th>\n",
       "      <th>corre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Gao Huang</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Yu Sun</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Zhuang Liu</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Daniel Sedra</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Kilian Weinberger</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name  sequence  star  corre\n",
       "0          Gao Huang         1     1      0\n",
       "1             Yu Sun         2     1      0\n",
       "2         Zhuang Liu         3     0      1\n",
       "3       Daniel Sedra         4     0      0\n",
       "4  Kilian Weinberger         5     0      0"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "author_sequence,star,corre=author_extraction(resourceId)\n",
    "df=pd.DataFrame.from_dict(author_sequence,orient='index',columns=['sequence'])\n",
    "df=df.reset_index().rename(columns={'index':'name'})\n",
    "df['star']=0\n",
    "for index, row in df.iterrows():\n",
    "    for author in star:\n",
    "        print(row[\"name\"],author,index)\n",
    "        if row[\"name\"]==author:\n",
    "            df.loc[index,\"star\"] = 1\n",
    "df['corre']=0\n",
    "for index, row in df.iterrows():\n",
    "    for author in corre:\n",
    "        print(row[\"name\"],author,index)\n",
    "        if row[\"name\"]==author:\n",
    "            df.loc[index,\"corre\"] = 1\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the generated txt file, first we try to extract the email address by using regular experssion\n",
    "### TODO: Special format such as {aaa, bbb, ccc}@xxx.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['David Duvenaud', 'James Robert Lloyd', 'Roger Grosse', 'Joshua B. Tenenbaum', 'Zoubin Ghahramani']\n",
      "['dkd23@cam.ac.uk', 'jrl44@cam.ac.uk', 'rgrosse@mit.edu', 'jbt@mit.edu', 'zoubin@eng.cam.ac.uk']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "\n",
    "#open the generated file as input stream and store in the buffer\n",
    "def email_extraction(file_name):\n",
    "    file = open('txt/'+str(file_name)+'.txt',encoding=\"utf-8\")\n",
    "    strings=file.read()\n",
    "    strings=strings[:2500]\n",
    "#     print(strings[:500])\n",
    "#     strings=\"   \".join(strings)\n",
    "    matches = []\n",
    "#    strings=\"dkd23@cam.ac.uk   ,   jrl44@cam.ac.uk   ,   rgrosse@mit.edu      jbt@mit.edu       zoubin@eng.cam.ac.uk\"\n",
    "    #print(strings)\n",
    "    matchesGroup=[]\n",
    "    emailRegex = re.compile(r'''(\n",
    "        [a-zA-Z0-9._%+-]+      # username\n",
    "        @+                     # @ symbol\n",
    "\n",
    "       [a-zA-Z0-9_-]+\n",
    "       (\\.[a-zA-Z0-9_-]+)+\n",
    "        )''', re.VERBOSE)\n",
    "    # using RE to match all the patterns in the txt\n",
    "    for groups in emailRegex.findall(strings):\n",
    "        matches.append(groups[0])\n",
    "    # reduced the same entities\n",
    "    list2 = matches\n",
    "    list_nums = len(list2)\n",
    "    emailList=[]\n",
    "    for line in range(list_nums):\n",
    "        emailList.append(list2[line])\n",
    "        \n",
    "    emailRegex2 = re.compile(r'''(\n",
    "    \\{(.+?)\\}+\n",
    "    @                      # @ symbol\n",
    "\n",
    "    [a-zA-Z0-9.-]+        # domain name\n",
    "\n",
    "    (\\.[a-zA-Z]{1,4}){1,2} # dot-something\n",
    "    )''', re.VERBOSE)\n",
    "    for groups in emailRegex2.findall(strings):\n",
    "        #print(groups)\n",
    "        matchesGroup.append(groups[0])\n",
    "    list3 = matchesGroup\n",
    "    if len(matchesGroup)>0:\n",
    "        for gp in list3:\n",
    "            #\n",
    "            #print(gp)\n",
    "            stringGroup = gp\n",
    "            index=stringGroup.find(\"@\", 0)\n",
    "            prefix=stringGroup[:index]\n",
    "            suffix=stringGroup[index:]\n",
    "            tempNameList=prefix[1:-1].split(\",\");\n",
    "            for name in tempNameList:\n",
    "                emailList.append(name+suffix)\n",
    "            print(emailList)\n",
    "    return emailList\n",
    "extractor=AuthorInfoExtract()\n",
    "output=extractor.locateContext(resourceId)\n",
    "author_sequence=extractor.sequenceExtratcor(output,authorList,1)\n",
    "print(author_sequence)\n",
    "#output=extractor.locateContext(resourceId)\n",
    "#print(output)\n",
    "#utput='Discriminative Learning of Deep Convolutional Feature Point Descriptors . Edgar Simo-Serra∗,1,5, Eduard Trulls∗,2,5, Luis Ferraz3 . Iasonas Kokkinos4, Pascal Fua2, Francesc Moreno-Noguer5 . 1 Waseda University, Tokyo, Japan, esimo@aoni.waseda.jp . 2 CVLab, ´Ecole Polytechnique F´ed´erale de Lausanne, Switzerland, {eduard.trulls,pascal.fua}@epﬂ.ch . 3 Catchoom Technologies, Barcelona, Spain, luis.ferraz@catchoom.com . 4 CentraleSupelec and INRIA-Saclay, Chatenay-Malabry, France, iasonas.kokkinos@ecp.fr . 5 Institut de Rob`otica i Inform`atica Industrial (CSIC-UPC), Barcelona, Spain, {esimo,etrulls,fmoreno}@iri.upc.edu .'\n",
    "emailList=email_extraction(resourceId)\n",
    "print(emailList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly we match the author name list with the email list by token matching method (Levenshtein)\n",
    "### TODO: HOW can we access author list? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['David Duvenaud', 'James Robert Lloyd', 'Roger Grosse', 'Joshua B. Tenenbaum', 'Zoubin Ghahramani']\n",
      "['dkd23@cam.ac.uk', 'jrl44@cam.ac.uk', 'rgrosse@mit.edu', 'jbt@mit.edu', 'zoubin@eng.cam.ac.uk']\n",
      "['dkd23'] David Duvenaud 0.3941798941798942\n",
      "['jrl44'] James Robert Lloyd 0.0\n",
      "['rgrosse'] Roger Grosse 0.597041847041847\n",
      "['jbt'] Joshua B. Tenenbaum 0.0\n",
      "['zoubin'] Zoubin Ghahramani 0.5980392156862745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'David Duvenaud': 'dkd23@cam.ac.uk',\n",
       " 'James Robert Lloyd': 'jrl44@cam.ac.uk',\n",
       " 'Roger Grosse': 'rgrosse@mit.edu',\n",
       " 'Joshua B. Tenenbaum': 'jbt@mit.edu',\n",
       " 'Zoubin Ghahramani': 'zoubin@eng.cam.ac.uk'}"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Levenshtein import *\n",
    "def author_email_matching(nameList, emailList):\n",
    "    if len(emailList)==0:\n",
    "        return\n",
    "    name_email={}\n",
    "    if len(nameList)==len(emailList):\n",
    "        for i in range(len(nameList)):\n",
    "            pat = re.compile(''+'(.*?)'+'@', re.S)\n",
    "            email = pat.findall(emailList[i])\n",
    "            name=nameList[i]\n",
    "            print(str(email),name,jaro(name, str(email)))\n",
    "            name_email[name]=emailList[i]\n",
    "    return name_email\n",
    "\n",
    "    for name in nameList:\n",
    "        highest=0\n",
    "        index=0\n",
    "        for i in range(len(emailList)):\n",
    "            #using re to extract the first part of email\n",
    "            pat = re.compile(''+'(.*?)'+'@', re.S)\n",
    "            email = pat.findall(emailList[i])\n",
    "            #print(str(email))\n",
    "            #print(jaro(name, str(email)))\n",
    "            #using jaro to calculate the similarity between two strings\n",
    "            if jaro(name, str(email))>highest:\n",
    "                index=i\n",
    "                highest=jaro(name, str(email))\n",
    "        #set pair with the highest score\n",
    "                print(name,highest,emailList[i])\n",
    "        name_email[name]=emailList[index]\n",
    "        #If very sure, remove it from the list to reduce the uncertainty for other pairs\n",
    "        if highest>=0.55:\n",
    "            emailList.remove(emailList[index])\n",
    "        if highest<=0.4:\n",
    "            name_email[name]=''\n",
    "    for email in emailList:\n",
    "        if email not in name_email:\n",
    "            for name in name_email:\n",
    "                if name_email[name]=='':\n",
    "                    name_email[name]=email\n",
    "                    print(jaro(name, str(email)))\n",
    "    return name_email\n",
    "        \n",
    "            \n",
    "#nameList=['Michael Stewart', 'Majigsuren Enkhsaikhan', 'Wei Liu']\n",
    "emailList=email_extraction(resourceId)\n",
    "extractor=AuthorInfoExtract()\n",
    "print(author_sequence)\n",
    "print(emailList)\n",
    "author_email_matching(author_sequence, emailList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After matching email, we move to the section of acknowledgement, which is not certain for every paper. Here we use named entity recognition to classify orgnization which sponsred the study. Here we used Spacy classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1169762\n",
      "1\n",
      "No Matching!\n"
     ]
    }
   ],
   "source": [
    "#using Regular expression to locate the paragraph\n",
    "def PER_recognition(file_name):\n",
    "    file = open('txt/'+str(file_name)+'.txt',encoding=\"ISO-8859-1\")\n",
    "    strings=file.read()\n",
    "    #print(strings)\n",
    "    keyStart = 'ABSTRACT'\n",
    "    keyEnd = '1'\n",
    "    pat = re.compile(keyStart+'(.*?)'+keyEnd, re.S)\n",
    "    result = pat.findall(strings)\n",
    "    #print(result)\n",
    "    print(1)\n",
    "    if len(result)<10:\n",
    "        keyStart1 = 'CONCLUSION'\n",
    "        keyEnd1 = 'Recurrent Neural Network'\n",
    "        pat = re.compile(keyStart1+'(.*?)'+keyEnd1, re.S)\n",
    "        result = pat.findall(strings)\n",
    "        if len(result)<10:\n",
    "            print(\"No Matching!\")\n",
    "            return\n",
    "\n",
    "    file.close()\n",
    "    #format processing\n",
    "    txt=''\n",
    "    txt=txt.join(result)\n",
    "    txt=txt.replace('\\n', '').replace('\\r', '')\n",
    "    #print(txt)\n",
    "    #using the pretrained model\n",
    "    \n",
    "    doc=nlp(txt)\n",
    "    PER_list=[]\n",
    "    print([(X.text, X.label_) for X in doc.ents])\n",
    "    for X in doc.ents:\n",
    "        if X.label_=='PER':\n",
    "            PER_list.append(X.text)\n",
    "    return PER_list\n",
    "print(resourceId)\n",
    "PER_recognition(resourceId)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
